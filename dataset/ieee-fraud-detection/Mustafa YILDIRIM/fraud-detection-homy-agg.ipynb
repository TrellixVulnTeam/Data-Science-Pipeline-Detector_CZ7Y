{"cells":[{"metadata":{},"cell_type":"markdown","source":"# IEEE-CIS Fraud Detection","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![fraud](https://abcountrywide.com.au/wp-content/uploads/2018/04/fraud.jpg)Imagine standing at the check-out counter at the grocery store with a long line behind you and the cashier not-so-quietly announces that your card has been declined. In this moment, you probably aren’t thinking about the data science that determined your fate.\n\nEmbarrassed, and certain you have the funds to cover everything needed for an epic nacho party for 50 of your closest friends, you try your card again. Same result. As you step aside and allow the cashier to tend to the next customer, you receive a text message from your bank. “Press 1 if you really tried to spend $500 on cheddar cheese.”\n\nWhile perhaps cumbersome (and often embarrassing) in the moment, this fraud prevention system is actually saving consumers millions of dollars per year. Researchers from the IEEE Computational Intelligence Society (IEEE-CIS) want to improve this figure, while also improving the customer experience. With higher accuracy fraud detection, you can get on with your chips without the hassle.\n\nIEEE-CIS works across a variety of AI and machine learning areas, including deep neural networks, fuzzy systems, evolutionary computation, and swarm intelligence. Today they’re partnering with the world’s leading payment service company, Vesta Corporation, seeking the best solutions for fraud prevention industry, and now you are invited to join the challenge.\n\nIn this competition, you’ll benchmark machine learning models on a challenging large-scale dataset. The data comes from Vesta's real-world e-commerce transactions and contains a wide range of features from device type to product features. You also have the opportunity to create new features to improve your results.\n\nIf successful, you’ll improve the efficacy of fraudulent transaction alerts for millions of people around the world, helping hundreds of thousands of businesses reduce their fraud loss and increase their revenue. And of course, you will save party people just like you the hassle of false positives.\n\nAcknowledgements:\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://www.xenonstack.com/wp-content/uploads/xenonstack-credit-card-fraud-detection.png)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Competition Objective is to detect fraud in transactions;","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Çalışmanın Hedefi, işlemlerde sahtekarlığı tespit etmektir;","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Kategorik Özellikler - İşlem","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Categorical Features - Transaction\n\n* ProductCD\n* emaildomain\n* card1 - card6\n* addr1, addr2\n* P_emaildomain\n* _emaildomain\n* M1 - M9","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Categorical Features - Identity","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* DeviceType\n* DeviceInfo\n* id_12 - id_38","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"TransactionDT özelliği, belirli bir referans tarih saatinden (gerçek bir zaman damgası değil) bir zaman çizelgesidir.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##  Questions","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Sorular","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I will start exploring based on Categorical Features and Transaction Amounts. The aim is answer some questions like:\n\n* What type of data we have on our data?\n* How many cols, rows, missing values we have?\n* Whats the target distribution?\n* What's the Transactions values distribution of fraud and no fraud transactions?\n* We have predominant fraudulent products?\n* What features or target shows some interesting patterns?\n* And a lot of more questions that will raise trought the exploration.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Kategorik Özelliklere ve İşlem Tutarlarına göre keşfetmeye başlayacağım. Amaç, aşağıdaki gibi bazı soruları cevaplamaktır:\n\n* Verilerimizde ne tür veriler var?\n* Kaç tane kömür, sıra, eksik değer var?\n* Hedef dağılımı nedir?\n* Dolandırıcılık işlemlerinin dolandırıcılık dağılımına değer verdiği ve dolandırıcılık işlemlerinin olmadığı nedir?\n* Baskın hileli ürünlerimiz var mı?\n* Hangi özellikler veya hedefler bazı ilginç desenler gösteriyor?\n* Ve keşfi artıracak daha birçok soru.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Importing necessary libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install lightgbm","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"!pip install chart_studio\n!sudo pip install chart_studio","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install xgboost","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"!pip install plotly==3.10.0","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"!pip install git+https://github.com/hyperopt/hyperopt.git","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport gc\nimport time\nfrom contextlib import contextmanager\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import GridSearchCV\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \nwarnings.filterwarnings(\"ignore\", category=FutureWarning) \nwarnings.filterwarnings(\"ignore\", category=UserWarning) \n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.rcParams.update({'figure.max_open_warning': 200})\n%matplotlib inline\n\nimport matplotlib\nimport matplotlib.pyplot as plt # for plotting\nimport seaborn as sns # for making plots with seaborn\ncolor = sns.color_palette()\nfrom numpy import array\nfrom matplotlib import cm\nfrom sklearn import preprocessing\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport scipy as sp\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Standard plotly imports\n#import plotly.plotly as py\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom plotly.offline import iplot, init_notebook_mode\n#import cufflinks\n#import cufflinks as cf\nimport plotly.figure_factory as ff\n\n# Using plotly + cufflinks in offline mode\ninit_notebook_mode(connected=True)\n#cufflinks.go_offline(connected=True)\n\n# Preprocessing, modelling and evaluating\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix, roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, KFold\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\nimport plotly.plotly as py\nimport plotly.graph_objs as go\n\n\n\nimport os\nimport gc\nprint(os.listdir(\"../input\"))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"warnings.simplefilter(\"ignore\")\nplt.style.use('ggplot')\ncolor_pal = [x['color'] for x in plt.rcParams['axes.prop_cycle']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\ndf_test_identity = pd.read_csv(\"../input/test_identity.csv\")\ndf_test_transaction = pd.read_csv(\"../input/test_transaction.csv\")\ndf_train_identity = pd.read_csv(\"../input/train_identity.csv\")\ndf_train_transaction = pd.read_csv(\"../input/train_transaction.csv\")","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"print('Size of df_sample_submission data', df_sample_submission.shape)\nprint('Size of df_test_identity data', df_test_identity.shape)\nprint('Size of df_test_transaction data', df_test_transaction.shape)\nprint('Size of df_train_identity data', df_train_identity.shape)\nprint('Size of df_train_transaction data', df_train_transaction.shape)","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"df_test_identity.head(5)","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"df_test_transaction.head()","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"df_train_identity.head()","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"df_train_transaction.head()","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"df_sample_submission.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x= df_train_transaction['isFraud'].value_counts().values\nsns.barplot([0,1],x)\nplt.title('Target variable count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There is clearly a class imbalace problem.\n* We will look into methods of solving this issue later in this notebook.\n\n* Açıkça bir sınıf dengesizliği sorunu var.\n* Bu sorunu daha sonra bu not defterinde çözme yöntemlerine bakacağız.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Train vs Test are Time Series Split","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* The TransactionDT feature is a timedelta from a given reference datetime (not an actual timestamp). One early discovery about the data is that the train and test appear to be split by time. There is a slight gap inbetween, but otherwise the training set is from an earlier period of time and test is from a later period of time. This will impact which cross validation techniques should be used.\n\n* We will look into this more when reviewing differences in distribution of features between train and test.\n\n* TransactionDT özelliği, belirli bir referans tarih saatinden (gerçek bir zaman damgası değil) bir zaman çizelgesidir. Verilerle ilgili erken bir keşif, tren ve testin zamana bölünmüş gibi görünmesidir. Arada hafif bir boşluk var, ancak aksi takdirde eğitim seti daha önceki bir zamandan ve test daha sonraki bir zaman diliminden. Bu, hangi çapraz validasyon tekniklerinin kullanılması gerektiğini etkileyecektir.\n\n* Özelliklerin tren ve test arasındaki dağılımındaki farklılıkları incelerken bunu daha ayrıntılı inceleyeceğiz.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* **Of the 394 features/columns in the train_transaction data 15 columns begin in C . The officaila explanation of these columns is.**\n* **C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked**\n* **All C columns are of the numeric data type and summary is as below**\n\n\n* **train_transaction verilerindeki 394 özellik / sütundan 15 sütun C ile başlar. Bu sütunların resmi açıklamasıdır.**\n* **C1-C14: ödeme kartıyla ilişkilendirilen kaç adres gibi sayım, vb. Gerçek anlam maskelenir**\n* **Tüm C sütunları sayısal veri türündedir ve özet aşağıdaki gibidir**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### The graph below shows number of unique values in each of the C Columns as blue bars.Orange bar shows the number of unique values in 96.5% of the data in each of the columns. The difference between the two bars is a measure of how distributed the data is across the range of unique values in the column.Red line is the percentage of missing values in the columns.\n\n* **Aşağıdaki grafik, C Sütunlarının her birindeki benzersiz değerlerin sayısını mavi çubuklar olarak gösterir. Turuncu çubuk, her bir sütundaki verilerin% 96.5'indeki benzersiz değerlerin sayısını gösterir. İki çubuk arasındaki fark, verilerin sütundaki benzersiz değerler aralığında ne kadar dağıldığının bir ölçüsüdür.Kırmızı çizgi, sütunlardaki eksik değerlerin yüzdesidir.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Ccols= df_train_transaction.columns[df_train_transaction.columns.str.startswith('C')]\ndf_train_transaction[Ccols].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_values_count = df_train_transaction.isnull().sum()\nprint (missing_values_count[0:10])\ntotal_cells = np.product(df_train_transaction.shape)\ntotal_missing = missing_values_count.sum()\nprint (\"% of missing data = \",(total_missing/total_cells) * 100)","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"missing_values_count = df_train_identity.isnull().sum()\nprint (missing_values_count[0:10])\ntotal_cells = np.product(df_train_identity.shape)\ntotal_missing = missing_values_count.sum()\nprint (\"% of missing data = \",(total_missing/total_cells) * 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ploting Transaction Amount Values Distribution \n## İşlem Tutarı Değerleri Dağılımı","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_id = pd.read_csv(\"../input/train_identity.csv\")\ndf_trans = pd.read_csv(\"../input/train_transaction.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,12))\nplt.suptitle('Transaction Values Distribution', fontsize=22)\nplt.subplot(221)\ng = sns.distplot(df_trans[df_trans['TransactionAmt'] <= 1000]['TransactionAmt'])\ng.set_title(\"Transaction Amount Distribuition <= 1000\", fontsize=18)\ng.set_xlabel(\"\")\ng.set_ylabel(\"Probability\", fontsize=15)\n\nplt.subplot(222)\ng1 = sns.distplot(np.log(df_trans['TransactionAmt']))\ng1.set_title(\"Transaction Amount (Log) Distribuition\", fontsize=18)\ng1.set_xlabel(\"\")\ng1.set_ylabel(\"Probability\", fontsize=15)\n\nplt.figure(figsize=(16,12))\n\n\nplt.subplot(212)\ng4 = plt.scatter(range(df_trans[df_trans['isFraud'] == 0].shape[0]),\n                 np.sort(df_trans[df_trans['isFraud'] == 0]['TransactionAmt'].values), \n                 label='NoFraud', alpha=.2)\ng4 = plt.scatter(range(df_trans[df_trans['isFraud'] == 1].shape[0]),\n                 np.sort(df_trans[df_trans['isFraud'] == 1]['TransactionAmt'].values), \n                 label='Fraud', alpha=.2)\ng4= plt.title(\"ECDF \\nFRAUD and NO FRAUD Transaction Amount Distribution\", fontsize=18)\ng4 = plt.xlabel(\"Index\")\ng4 = plt.ylabel(\"Amount Distribution\", fontsize=15)\ng4 = plt.legend()\n\nplt.figure(figsize=(16,12))\n\nplt.subplot(321)\ng = plt.scatter(range(df_trans[df_trans['isFraud'] == 1].shape[0]), \n                 np.sort(df_trans[df_trans['isFraud'] == 1]['TransactionAmt'].values), \n                label='isFraud', alpha=.4)\nplt.title(\"FRAUD - Transaction Amount ECDF\", fontsize=18)\nplt.xlabel(\"Index\")\nplt.ylabel(\"Amount Distribution\", fontsize=12)\n\nplt.subplot(322)\ng1 = plt.scatter(range(df_trans[df_trans['isFraud'] == 0].shape[0]),\n                 np.sort(df_trans[df_trans['isFraud'] == 0]['TransactionAmt'].values), \n                 label='NoFraud', alpha=.2)\ng1= plt.title(\"NO FRAUD - Transaction Amount ECDF\", fontsize=18)\ng1 = plt.xlabel(\"Index\")\ng1 = plt.ylabel(\"Amount Distribution\", fontsize=15)\n\nplt.suptitle('Individual ECDF Distribution', fontsize=22)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now, let's known the Product Feature Şimdi Ürün Özelliğini bilelim\n- Distribution Products\n- Distribution of Frauds by Product\n- Has Difference between Transaction Amounts in Products? \n\n\n- Dağıtım Ürünleri\n- Sahtekarlıkların Ürüne Göre Dağılımı\n- Ürünlerde İşlem Tutarları Arasında Fark Var mı?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_transaction['TransactionDT'].plot(kind='hist',\n                                        figsize=(15, 5),\n                                        label='train',\n                                        bins=50,\n                                        title='Train vs Test TransactionDT distribution')\ndf_test_transaction['TransactionDT'].plot(kind='hist',\n                                       label='test',\n                                       bins=50)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = df_train_transaction.plot(x='TransactionDT',\n                       y='TransactionAmt',\n                       kind='scatter',\n                       alpha=0.01,\n                       label='TransactionAmt-train',\n                       title='Train and test Transaction Ammounts by Time (TransactionDT)',\n                       ylim=(0, 5000),\n                       figsize=(15, 5))\ndf_test_transaction.plot(x='TransactionDT',\n                      y='TransactionAmt',\n                      kind='scatter',\n                      label='TransactionAmt-test',\n                      alpha=0.01,\n                      color=color_pal[1],\n                       ylim=(0, 5000),\n                      ax=ax)\n# Plot Fraud as Orange\ndf_train_transaction.loc[df_train_transaction['isFraud'] == 1] \\\n    .plot(x='TransactionDT',\n         y='TransactionAmt',\n         kind='scatter',\n         alpha=0.01,\n         label='TransactionAmt-train',\n         title='Train and test Transaction Ammounts by Time (TransactionDT)',\n         ylim=(0, 5000),\n         color='orange',\n         figsize=(15, 5),\n         ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Distribution of Target in Training Set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('  {:.4f}% of Transactions that are fraud in train '.format(df_train_transaction['isFraud'].mean() * 100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dolandırıcılık işlemleri oran 3.4990% ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_transaction.groupby('isFraud') \\\n    .count()['TransactionID'] \\\n    .plot(kind='barh',\n          title='Distribution of Target in Train',\n          figsize=(15, 3))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TransactionAmt","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### The ammount of transaction. I've taken a log transform in some of these plots to better show the distribution- otherwise the few, very large transactions skew the distribution. Because of the log transfrom, any values between 0 and 1 will appear to be negative.\n\n#### İşlem miktarı. Dağılımını daha iyi göstermek için bu parsellerden bazılarında günlük dönüşümü yaptım, aksi halde birkaç büyük işlem dağıtımı çarpıtır. Günlük aktarımı nedeniyle, 0 ile 1 arasındaki tüm değerler negatif görünecektir.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_transaction['TransactionAmt'] \\\n    .apply(np.log) \\\n    .plot(kind='hist',\n          bins=100,\n          figsize=(15, 5),\n          title='Distribution of Log Transaction Amt')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 6))\ndf_train_transaction.loc[df_train_transaction['isFraud'] == 1] \\\n    ['TransactionAmt'].apply(np.log) \\\n    .plot(kind='hist',\n          bins=100,\n          title='Log Transaction Amt.Gunlk islm kay - Fraud.Dolandiricilik',\n          color=color_pal[1],\n          xlim=(-3, 10),\n         ax= ax1)\ndf_train_transaction.loc[df_train_transaction['isFraud'] == 0] \\\n    ['TransactionAmt'].apply(np.log) \\\n    .plot(kind='hist',\n          bins=100,\n          title='Log Transaction Amt - Not Fraud',\n          color=color_pal[2],\n          xlim=(-3, 10),\n         ax=ax2)\ndf_train_transaction.loc[df_train_transaction['isFraud'] == 1] \\\n    ['TransactionAmt'] \\\n    .plot(kind='hist',\n          bins=100,\n          title='Transaction Amt - Fraud',\n          color=color_pal[1],\n         ax= ax3)\ndf_train_transaction.loc[df_train_transaction['isFraud'] == 0] \\\n    ['TransactionAmt'] \\\n    .plot(kind='hist',\n          bins=100,\n          title='Transaction Amt.İşlem Tutarı - Not Fraud.Sahtekarlık Değil',\n          color=color_pal[2],\n         ax=ax4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Fraudulent charges appear to have a higher average transaction ammount\n* Hileli ücretlerin ortalama işlem tutarı daha yüksek gibi görünüyor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Mean transaction amt for fraud is {:.4f}'.format(df_train_transaction.loc[df_train_transaction['isFraud'] == 1]['TransactionAmt'].mean()))\nprint('Mean transaction amt for non-fraud is {:.4f}'.format(df_train_transaction.loc[df_train_transaction['isFraud'] == 0]['TransactionAmt'].mean()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ProductCD\n- For now we don't know exactly what these values represent.\n- `W` has the most number of observations, `C` the least.\n- ProductCD `C` has the most fraud with >11%\n- ProductCD `W` has the least with ~2%\n\n### ProductCD \n* Şimdilik bu değerlerin tam olarak neyi temsil ettiğini bilmiyoruz. \n* ProductCD W en fazla gözlem, C en az gözlem içerir. \n* ProductCD  C>% 11 ile en fazla dolandırıcılığa sahiptir \n* ProductCDCD W ~% 2 ile en düşük değere sahiptir","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_transaction.groupby('ProductCD') \\\n    ['TransactionID'].count() \\\n    .sort_index() \\\n    .plot(kind='barh',\n          figsize=(15, 3),\n         title='Count of Observations by ProductCD. Urun gozlem sayıları')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_transaction.groupby('ProductCD')['isFraud'] \\\n    .mean() \\\n    .sort_index() \\\n    .plot(kind='barh',\n          figsize=(15, 3),\n         title='Percentage of Fraud by ProductCD.Sahtekarlik %')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Categorical Features - Transaction Kategorik Özellikler - İşlem\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We are told in the data description that the following transaction columns are categorical:\nVeri açıklamasında aşağıdaki işlem sütunlarının kategoriktir.    \n\n* ProductCD\n* emaildomain\n* card1 - card6\n* addr1, addr2\n* P_emaildomain\n* R_emaildomain\n* M1 - M9","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### card1 - card6","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* We are told these are all categorical, even though some appear numeric.\n* Bazılarının sayısal görünse de bunların hepsinin kategorik olduğu söyleniyor.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"card_cols = [c for c in df_train_transaction.columns if 'card' in c]\ndf_train_transaction[card_cols].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"color_idx = 0\nfor c in card_cols:\n    if df_train_transaction[c].dtype in ['float64','int64']:\n        df_train_transaction[c].plot(kind='hist',\n                                      title=c,\n                                      bins=50,\n                                      figsize=(15, 2),\n                                      color=color_pal[color_idx])\n    color_idx += 1\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_transaction_fr = df_train_transaction.loc[df_train_transaction['isFraud'] == 1]\ndf_train_transaction_nofr = df_train_transaction.loc[df_train_transaction['isFraud'] == 0]\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 8))\ndf_train_transaction_fr.groupby('card4')['card4'].count().plot(kind='barh', ax=ax1, title='Count of card4 fraud')\ndf_train_transaction_nofr.groupby('card4')['card4'].count().plot(kind='barh', ax=ax2, title='Count of card4 non-fraud')\ndf_train_transaction_fr.groupby('card6')['card6'].count().plot(kind='barh', ax=ax3, title='Count of card6 fraud')\ndf_train_transaction_nofr.groupby('card6')['card6'].count().plot(kind='barh', ax=ax4, title='Count of card6 non-fraud')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fraud by card types . Kart cinslerine göre sahtecilik","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## addr1 & addr2","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The data description states that these are categorical even though they look numeric. Could they be the address value?\n \n* **Veri açıklaması, sayısal görünseler bile bunların kategorik olduğunu belirtir. Adres değeri olabilir mi?**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(' addr1 - has {} NA values'.format(df_train_transaction['addr1'].isna().sum()))\nprint(' addr2 - has {} NA values'.format(df_train_transaction['addr2'].isna().sum()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_transaction['addr1'].plot(kind='hist', bins=500, figsize=(15, 2), title='addr1 distribution dagilimi')\nplt.show()\ndf_train_transaction['addr2'].plot(kind='hist', bins=500, figsize=(15, 2), title='addr2 distribution dagilimi')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## dist1 & dist2","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# dist1 & dist2\nPlotting with logx to better show the distribution. Possibly this could be the distance of the transaction vs. the card owner's home/work address. This is just a guess.\n# dist1 & dist2\nDağıtımı daha iyi göstermek için logx ile çizim. Muhtemelen bu işlemin kart sahibinin ev / iş adresine olan uzaklığı olabilir. Bu sadece bir tahmin.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_transaction['dist1'].plot(kind='hist',\n                                bins=5000,\n                                figsize=(15, 2),\n                                title='dist1 distribution',\n                                color=color_pal[1],\n                                logx=True)\nplt.show()\ndf_train_transaction['dist2'].plot(kind='hist',\n                                bins=5000,\n                                figsize=(15, 2),\n                                title='dist2 distribution',\n                                color=color_pal[1],\n                                logx=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_id = pd.read_csv(\"../input/train_identity.csv\")\ndf_trans = pd.read_csv(\"../input/train_transaction.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def resumetable(df):\n    print(f\"Dataset Shape: {df.shape}\")\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['First Value'] = df.loc[0].values\n    summary['Second Value'] = df.loc[1].values\n    summary['Third Value'] = df.loc[2].values\n\n    for name in summary['Name'].value_counts().index:\n        summary.loc[summary['Name'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True), base=2),2) \n\n    return summary\n\n## Function to reduce the DF size\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n\ndef CalcOutliers(df_num): \n\n    # calculating mean and std of the array\n    data_mean, data_std = np.mean(df_num), np.std(df_num)\n\n    # seting the cut line to both higher and lower values\n    # You can change this value\n    cut = data_std * 3\n\n    #Calculating the higher and lower cut values\n    lower, upper = data_mean - cut, data_mean + cut\n\n    # creating an array of lower, higher and total outlier values \n    outliers_lower = [x for x in df_num if x < lower]\n    outliers_higher = [x for x in df_num if x > upper]\n    outliers_total = [x for x in df_num if x < lower or x > upper]\n\n    # array without outlier values\n    outliers_removed = [x for x in df_num if x > lower and x < upper]\n    \n    print('Identified lowest outliers: %d' % len(outliers_lower)) # printing total number of values in lower cut of outliers\n    print('Identified upper outliers: %d' % len(outliers_higher)) # printing total number of values in higher cut of outliers\n    print('Total outlier observations: %d' % len(outliers_total)) # printing total number of values outliers of both sides\n    print('Non-outlier observations: %d' % len(outliers_removed)) # printing total number of non outlier values\n    print(\"Total percentual of Outliers: \", round((len(outliers_total) / len(outliers_removed) )*100, 4)) # Percentual of outliers in points\n    \n    return","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Target Distribution","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_trans['TransactionAmt'] = df_trans['TransactionAmt'].astype(float)\ntotal = len(df_trans)\ntotal_amt = df_trans.groupby(['isFraud'])['TransactionAmt'].sum().sum()\nplt.figure(figsize=(16,6))\n\nplt.subplot(121)\ng = sns.countplot(x='isFraud', data=df_trans, )\ng.set_title(\"Fraud Transactions Distribution \\n# 0: No Fraud | 1: Fraud #\", fontsize=22)\ng.set_xlabel(\"Is fraud?\", fontsize=18)\ng.set_ylabel('Count', fontsize=18)\nfor p in g.patches:\n    height = p.get_height()\n    g.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:1.2f}%'.format(height/total*100),\n            ha=\"center\", fontsize=15) \n\nperc_amt = (df_trans.groupby(['isFraud'])['TransactionAmt'].sum())\nperc_amt = perc_amt.reset_index()\nplt.subplot(122)\ng1 = sns.barplot(x='isFraud', y='TransactionAmt',  dodge=True, data=perc_amt)\ng1.set_title(\"% Total Amount in Transaction Amt \\n# 0: No Fraud | 1: Fraud #\", fontsize=22)\ng1.set_xlabel(\"Is fraud?\", fontsize=18)\ng1.set_ylabel('Total Transaction Amount Scalar', fontsize=18)\nfor p in g1.patches:\n    height = p.get_height()\n    g1.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:1.2f}%'.format(height/total_amt * 100),\n            ha=\"center\", fontsize=15) \n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- **We have 3.5% of Fraud transactions in our dataset.I think that it would be interesting to see if the amount percentual is higher or lower than 3.5% of total. I will see it later.We have the same % when considering the Total Transactions Amount by Fraudand No Fraud.Let's explore the Transaction amount further below.**\n\n* **Veri setimizde Sahtekarlık işlemlerinin% 3,5'ine sahibiz. Yüzdelik miktarın toplamın% 3,5'inden yüksek veya düşük olup olmadığını görmek ilginç olacağını düşünüyorum. Daha sonra göreceğim. Sahtekarlık ve Sahtekarlık Olmadan Toplam İşlem Tutarı göz önüne alındığında aynı% oranına sahibiz. İşlem tutarını aşağıda daha ayrıntılı inceleyelim.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_trans['TransactionAmt'] = df_trans['TransactionAmt'].astype(float)\nprint(\"Transaction Amounts Quantiles:\")\nprint(df_trans['TransactionAmt'].quantile([.01, .025, .1, .25, .5, .75, .9, .975, .99]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ploting Transaction Amount Values Distribution- İşlem Tutarı Değerleri Dağılımı","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Seeing the Quantiles of Fraud and No Fraud Transactions-Dolandırıcılık Miktarlarını Görme ve Dolandırıcılık İşlemleri Yok","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pd.concat([df_trans[df_trans['isFraud'] == 1]['TransactionAmt']\\\n                 .quantile([.01, .1, .25, .5, .75, .9, .99])\\\n                 .reset_index(), \n                 df_trans[df_trans['isFraud'] == 0]['TransactionAmt']\\\n                 .quantile([.01, .1, .25, .5, .75, .9, .99])\\\n                 .reset_index()],\n                axis=1, keys=['Fraud', \"No Fraud\"]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Transaction Amount Outliers - İşlem Tutarı Aykırı Değerleri","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- It's considering outlier values that are highest than 3 times the std from the mean\n\n- Ortalamadan 3 kat daha yüksek aykırı değerleri dikkate alıyor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"CalcOutliers(df_trans['TransactionAmt'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Tanımlanmış en düşük aykırı değerler: 0\n* Tanımlanan üst aykırı değerler: 10093\n* Toplam aykırı gözlemler: 10093\n* Aykırı olmayan gözlemler: 580447\n* Aykırı değerlerin toplam yüzdesi: 1.7388","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Yalnızca = 0 ila 800 arasındaki değerleri dikkate alırsak, aykırı değerlerden kaçınırız ve dağıtımımıza daha fazla güveniriz.\n* Toplam satırların% 1.74'ünü temsil eden aykırı değerlere sahip 10 bin satırımız var.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Now, let's known the Product Feature\n* Distribution Products\n* Distribution of Frauds by Product\n* Has Difference between Transaction Amounts in Products?\n\n \n* Dağıtım Ürünleri\n* Sahtekarlıkların Ürüne Göre Dağılımı\n* Ürünlerde İşlem Tutarı Arasındaki Fark Nedir?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp = pd.crosstab(df_trans['ProductCD'], df_trans['isFraud'], normalize='index') * 100\ntmp = tmp.reset_index()\ntmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n\nplt.figure(figsize=(14,10))\nplt.suptitle('ProductCD Distributions', fontsize=22)\n\nplt.subplot(221)\ng = sns.countplot(x='ProductCD', data=df_trans)\n# plt.legend(title='Fraud', loc='upper center', labels=['No', 'Yes'])\n\ng.set_title(\"ProductCD Distribution\", fontsize=19)\ng.set_xlabel(\"ProductCD Name\", fontsize=17)\ng.set_ylabel(\"Count\", fontsize=17)\ng.set_ylim(0,500000)\nfor p in g.patches:\n    height = p.get_height()\n    g.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:1.2f}%'.format(height/total*100),\n            ha=\"center\", fontsize=14) \n\n    plt.subplot(222)\ng1 = sns.countplot(x='ProductCD', hue='isFraud', data=df_trans)\nplt.legend(title='Fraud', loc='best', labels=['No', 'Yes'])\ngt = g1.twinx()\ngt = sns.pointplot(x='ProductCD', y='Fraud', data=tmp, color='black', order=['W', 'H',\"C\", \"S\", \"R\"], legend=False)\ngt.set_ylabel(\"% of Fraud Transactions\", fontsize=16)\n\ng1.set_title(\"Product CD by Target(isFraud)\", fontsize=19)\ng1.set_xlabel(\"ProductCD Name\", fontsize=17)\ng1.set_ylabel(\"Count\", fontsize=17)\n\nplt.subplot(212)\ng3 = sns.boxenplot(x='ProductCD', y='TransactionAmt', hue='isFraud', \n              data=df_trans[df_trans['TransactionAmt'] <= 2000] )\ng3.set_title(\"Transaction Amount Distribuition by ProductCD and Target\", fontsize=20)\ng3.set_xlabel(\"ProductCD Name\", fontsize=17)\ng3.set_ylabel(\"Transaction Values\", fontsize=17)\n\nplt.subplots_adjust(hspace = 0.6, top = 0.85)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Exploring M1-M9 Features- M1-M9 Özelliklerini Keşfetme","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ['M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']:\n    df_trans[col] = df_trans[col].fillna(\"Miss\")\n    \ndef ploting_dist_ratio(df, col, lim=2000):\n    tmp = pd.crosstab(df[col], df['isFraud'], normalize='index') * 100\n    tmp = tmp.reset_index()\n    tmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n\n    plt.figure(figsize=(20,5))\n    plt.suptitle(f'{col} Distributions ', fontsize=22)\n\n    plt.subplot(121)\n    g = sns.countplot(x=col, data=df, order=list(tmp[col].values))\n    # plt.legend(title='Fraud', loc='upper center', labels=['No', 'Yes'])\n    g.set_title(f\"{col} Distribution\\nCound and %Fraud by each category\", fontsize=18)\n    g.set_ylim(0,400000)\n    gt = g.twinx()\n    gt = sns.pointplot(x=col, y='Fraud', data=tmp, order=list(tmp[col].values),\n                       color='black', legend=False, )\n    gt.set_ylim(0,20)\n    gt.set_ylabel(\"% of Fraud Transactions\", fontsize=16)\n    g.set_xlabel(f\"{col} Category Names\", fontsize=16)\n    g.set_ylabel(\"Count\", fontsize=17)\n    for p in gt.patches:\n        height = p.get_height()\n        gt.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.2f}%'.format(height/total*100),\n                ha=\"center\",fontsize=14) \n        \n    perc_amt = (df_trans.groupby(['isFraud',col])['TransactionAmt'].sum() / total_amt * 100).unstack('isFraud')\n    perc_amt = perc_amt.reset_index()\n    perc_amt.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n\n    plt.subplot(122)\n    g1 = sns.boxplot(x=col, y='TransactionAmt', hue='isFraud', \n                     data=df[df['TransactionAmt'] <= lim], order=list(tmp[col].values))\n    g1t = g1.twinx()\n    g1t = sns.pointplot(x=col, y='Fraud', data=perc_amt, order=list(tmp[col].values),\n                       color='black', legend=False, )\n    g1t.set_ylim(0,5)\n    g1t.set_ylabel(\"%Fraud Total Amount\", fontsize=16)\n    g1.set_title(f\"{col} by Transactions dist\", fontsize=18)\n    g1.set_xlabel(f\"{col} Category Names\", fontsize=16)\n    g1.set_ylabel(\"Transaction Amount(U$)\", fontsize=16)\n        \n    plt.subplots_adjust(hspace=.4, wspace = 0.35, top = 0.80)\n    \n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### M distributions:  Count, %Fraud and Transaction Amount distribution","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ['M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']:\n    ploting_dist_ratio(df_trans, col, lim=2500)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **This graphs give us many interesting intuition about the M features.**\n* **Only in M4 the Missing values haven't the highest % of Fraud.**\n** **Bu grafikler bize M özellikleri hakkında birçok ilginç sezgi veriyor.**\n** **Sadece M4'te Eksik değerler Sahtekarlık'ın en yüksek yüzdesine sahip değildir.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Addr1 and Addr2","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Card Features Quantiles: \")\nprint(df_trans[['addr1', 'addr2']].quantile([0.01, .025, .1, .25, .5, .75, .90,.975, .99]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will set all values in Addr1 that has less than 5000 entries to \"Others\"\nIn Addr2 I will set as \"Others\" all values with less than 50 entries\n\nAddr1'de 5000'den az girişi olan tüm değerleri 'Diğerleri' olarak ayarlayacağım\nAddr2'de 50'den az girişi olan tüm değerleri 'Diğerleri' olarak ayarlayacağım","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_trans.loc[df_trans.addr1.isin(df_trans.addr1.value_counts()[df_trans.addr1.value_counts() <= 5000 ].index), 'addr1'] = \"Others\"\ndf_trans.loc[df_trans.addr2.isin(df_trans.addr2.value_counts()[df_trans.addr2.value_counts() <= 50 ].index), 'addr2'] = \"Others\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Addr1 Distributions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":" def ploting_cnt_amt(df, col, lim=2000):\n    tmp = pd.crosstab(df[col], df['isFraud'], normalize='index') * 100\n    tmp = tmp.reset_index()\n    tmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n    \n    plt.figure(figsize=(16,14))    \n    plt.suptitle(f'{col} Distributions ', fontsize=24)\n    \n    plt.subplot(211)\n    g = sns.countplot( x=col,  data=df, order=list(tmp[col].values))\n    gt = g.twinx()\n    gt = sns.pointplot(x=col, y='Fraud', data=tmp, order=list(tmp[col].values),\n                       color='black', legend=False, )\n    gt.set_ylim(0,tmp['Fraud'].max()*1.1)\n    gt.set_ylabel(\"%Fraud Transactions\", fontsize=16)\n    g.set_title(f\"Most Frequent {col} values and % Fraud Transactions\", fontsize=20)\n    g.set_xlabel(f\"{col} Category Names\", fontsize=16)\n    g.set_ylabel(\"Count\", fontsize=17)\n    g.set_xticklabels(g.get_xticklabels(),rotation=45)\n    sizes = []\n    for p in g.patches:\n        height = p.get_height()\n        sizes.append(height)\n        g.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.2f}%'.format(height/total*100),\n                ha=\"center\",fontsize=12) \n        \n    g.set_ylim(0,max(sizes)*1.15)\n    \n    #########################################################################\n    perc_amt = (df.groupby(['isFraud',col])['TransactionAmt'].sum() \\\n                / df.groupby([col])['TransactionAmt'].sum() * 100).unstack('isFraud')\n    perc_amt = perc_amt.reset_index()\n    perc_amt.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n    amt = df.groupby([col])['TransactionAmt'].sum().reset_index()\n    perc_amt = perc_amt.fillna(0)\n    plt.subplot(212)\n    g1 = sns.barplot(x=col, y='TransactionAmt', \n                       data=amt, \n                       order=list(tmp[col].values))\n    g1t = g1.twinx()\n    g1t = sns.pointplot(x=col, y='Fraud', data=perc_amt, \n                        order=list(tmp[col].values),\n                       color='black', legend=False, )\n    g1t.set_ylim(0,perc_amt['Fraud'].max()*1.1)\n    g1t.set_ylabel(\"%Fraud Total Amount\", fontsize=16)\n    g.set_xticklabels(g.get_xticklabels(),rotation=45)\n    g1.set_title(f\"{col} by Transactions Total + %of total and %Fraud Transactions\", fontsize=20)\n    g1.set_xlabel(f\"{col} Category Names\", fontsize=16)\n    g1.set_ylabel(\"Transaction Total Amount(U$)\", fontsize=16)\n    g1.set_xticklabels(g.get_xticklabels(),rotation=45)    \n    \n    for p in g1.patches:\n        height = p.get_height()\n        g1.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.2f}%'.format(height/total_amt*100),\n                ha=\"center\",fontsize=12) \n        \n    plt.subplots_adjust(hspace=.4, top = 0.9)\n    plt.show()\n    \nploting_cnt_amt(df_trans, 'addr1')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We can note interesting patterns on Addr1.\n* Addr1'de ilginç desenler not edebiliriz.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Addr2 Distributions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ploting_cnt_amt(df_trans, 'addr2')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Almost all entries in Addr2 are in the same value.\n* Interestingly in the value 65 , the percent of frauds are almost 60%\n* Altought the value 87 has 88% of total entries, it has 96% of Total Transaction Amounts\n\n\n* Addr2'deki hemen hemen tüm girişler aynı değerdedir. \n* İlginç bir şekilde, 65 değerinde, dolandırıcılıkların yüzdesi neredeyse% 60'tır, ancak 87 değerinin toplam girdilerin% 88'ine sahip olmasına rağmen, \n* Toplam İşlem Tutarlarının% 96'sına sahiptir.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### emaildomain Distributions","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- I will group all e-mail domains by the respective enterprises.\n- Also, I will set as \"Others\" all values with less than 500 entries.\n\n- Tüm e-posta alan adlarını ilgili kuruluşlara göre gruplandıracağım.\n- Ayrıca, 500'den az girişi olan tüm değerleri 'Diğerleri' olarak ayarlayacağım.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_trans.loc[df_trans['P_emaildomain'].isin(['gmail.com', 'gmail']),'P_emaildomain'] = 'Google'\n\ndf_trans.loc[df_trans['P_emaildomain'].isin(['yahoo.com', 'yahoo.com.mx',  'yahoo.co.uk',\n                                         'yahoo.co.jp', 'yahoo.de', 'yahoo.fr',\n                                         'yahoo.es']), 'P_emaildomain'] = 'Yahoo Mail'\ndf_trans.loc[df_trans['P_emaildomain'].isin(['hotmail.com','outlook.com','msn.com', 'live.com.mx', \n                                         'hotmail.es','hotmail.co.uk', 'hotmail.de',\n                                         'outlook.es', 'live.com', 'live.fr',\n                                         'hotmail.fr']), 'P_emaildomain'] = 'Microsoft'\ndf_trans.loc[df_trans.P_emaildomain.isin(df_trans.P_emaildomain\\\n                                         .value_counts()[df_trans.P_emaildomain.value_counts() <= 500 ]\\\n                                         .index), 'P_emaildomain'] = \"Others\"\ndf_trans.P_emaildomain.fillna(\"NoInf\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ploting P-Email Domain","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ploting_cnt_amt(df_trans, 'P_emaildomain')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## R-Email Domain plot distribution\n- I will group all e-mail domains by the respective enterprises.\n- I will set as \"Others\" all values with less than 300 entries.\n\n- Tüm e-posta alan adlarını ilgili kuruluşlara göre gruplandıracağım.\n- 300'den az girişi olan tüm değerleri 'Diğerleri' olarak ayarlayacağım.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_trans.loc[df_trans['R_emaildomain'].isin(['gmail.com', 'gmail']),'R_emaildomain'] = 'Google'\n\ndf_trans.loc[df_trans['R_emaildomain'].isin(['yahoo.com', 'yahoo.com.mx',  'yahoo.co.uk',\n                                             'yahoo.co.jp', 'yahoo.de', 'yahoo.fr',\n                                             'yahoo.es']), 'R_emaildomain'] = 'Yahoo Mail'\ndf_trans.loc[df_trans['R_emaildomain'].isin(['hotmail.com','outlook.com','msn.com', 'live.com.mx', \n                                             'hotmail.es','hotmail.co.uk', 'hotmail.de',\n                                             'outlook.es', 'live.com', 'live.fr',\n                                             'hotmail.fr']), 'R_emaildomain'] = 'Microsoft'\ndf_trans.loc[df_trans.R_emaildomain.isin(df_trans.R_emaildomain\\\n                                         .value_counts()[df_trans.R_emaildomain.value_counts() <= 300 ]\\\n                                         .index), 'R_emaildomain'] = \"Others\"\ndf_trans.R_emaildomain.fillna(\"NoInf\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ploting_cnt_amt(df_trans, 'R_emaildomain')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can see a very similar distribution in both email domain features.\n- It's interesting that we have high values in google and icloud frauds\n\n- Her iki e-posta alan özelliğinde de benzer bir dağılım görebiliriz.\n- Google ve icloud sahtekarlıklarında yüksek değerlere sahip olmamız ilginç","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### C1-C14 features","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- Let's understand what this features are.\n- What's the distributions? \n\n Bu özelliklerin ne olduğunu anlayalım.\n- Dağılımlar nedir?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## REducing memory\ndf_trans = reduce_mem_usage(df_trans)\ndf_id = reduce_mem_usage(df_id)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_trans[['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8',\n                      'C9', 'C10', 'C11', 'C12', 'C13', 'C14']].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_trans.loc[df_trans.C1.isin(df_trans.C1\\\n                              .value_counts()[df_trans.C1.value_counts() <= 400 ]\\\n                              .index), 'C1'] = \"Others\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### C1 Distribution Plot","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ploting_cnt_amt(df_trans, 'C1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_trans.loc[df_trans.C2.isin(df_trans.C2\\\n                              .value_counts()[df_trans.C2.value_counts() <= 350 ]\\\n                              .index), 'C2'] = \"Others\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ploting_cnt_amt(df_trans, 'C2')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Top 3 values are 1, 2 and 3 and is the same on Total Amounts. We see the same pattern on fraud ratios\n* **Top 3 values are 1, 2 and 3 and is the same on Total Amounts. We see the same pattern on fraud ratios**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### TimeDelta Feature","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* **Let's see if the frauds have some specific hour that has highest % of frauds**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Converting to Total Days, Weekdays and Hours","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* **In discussions tab I read an excellent solution to Timedelta column, I will set the link below;\nWe will use the first date as 2017-12-01 and use the delta time to compute datetime features**\n\n* **Tartışmalar sekmesinde Timedelta sütununa mükemmel bir çözüm okudum, aşağıdaki bağlantıyı ayarlayacağım;İlk tarihi 2017-12-01 olarak kullanacağız ve datetime özelliklerini hesaplamak için delta zamanını kullanacağız**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/c/ieee-fraud-detection/discussion/100400#latest-579480\nimport datetime\n\nSTART_DATE = '2017-12-01'\nstartdate = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\")\ndf_trans[\"Date\"] = df_trans['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\n\ndf_trans['_Weekdays'] = df_trans['Date'].dt.dayofweek\ndf_trans['_Hours'] = df_trans['Date'].dt.hour\ndf_trans['_Days'] = df_trans['Date'].dt.day","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Top Days with highest Total Transaction Amount - Toplam İşlem Tutarı en yüksek olan İlk Günler","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from functools import partial\n\nimport os\nimport gc\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ploting_cnt_amt(df_trans, '_Days')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Ploting WeekDays Distributions - Haftaiçi Dağılımlar","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ploting_cnt_amt(df_trans, '_Weekdays')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Ploting Hours Distributions - Çizim Saatleri Dağılımları","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ploting_cnt_amt(df_trans, '_Hours')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_id[['id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18',\n       'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25',\n       'id_26', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', 'id_32',\n       'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38']].describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_trans.merge(df_id, how='left', left_index=True, right_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cat_feat_ploting(df, col):\n    tmp = pd.crosstab(df[col], df['isFraud'], normalize='index') * 100\n    tmp = tmp.reset_index()\n    tmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n\n    plt.figure(figsize=(14,10))\n    plt.suptitle(f'{col} Distributions', fontsize=22)\n\n    plt.subplot(221)\n    g = sns.countplot(x=col, data=df, order=tmp[col].values)\n    # plt.legend(title='Fraud', loc='upper center', labels=['No', 'Yes'])\n\n    g.set_title(f\"{col} Distribution\", fontsize=19)\n    g.set_xlabel(f\"{col} Name\", fontsize=17)\n    g.set_ylabel(\"Count\", fontsize=17)\n    # g.set_ylim(0,500000)\n    for p in g.patches:\n        height = p.get_height()\n        g.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.2f}%'.format(height/total*100),\n                ha=\"center\", fontsize=14) \n\n    plt.subplot(222)\n    g1 = sns.countplot(x=col, hue='isFraud', data=df, order=tmp[col].values)\n    plt.legend(title='Fraud', loc='best', labels=['No', 'Yes'])\n    gt = g1.twinx()\n    gt = sns.pointplot(x=col, y='Fraud', data=tmp, color='black', order=tmp[col].values, legend=False)\n    gt.set_ylabel(\"% of Fraud Transactions\", fontsize=16)\n\n    g1.set_title(f\"{col} by Target(isFraud)\", fontsize=19)\n    g1.set_xlabel(f\"{col} Name\", fontsize=17)\n    g1.set_ylabel(\"Count\", fontsize=17)\n\n    plt.subplot(212)\n    g3 = sns.boxenplot(x=col, y='TransactionAmt', hue='isFraud', \n                       data=df[df['TransactionAmt'] <= 2000], order=tmp[col].values )\n    g3.set_title(\"Transaction Amount Distribuition by ProductCD and Target-\", fontsize=20)\n    g3.set_xlabel(\"ProductCD Name\", fontsize=17)\n    g3.set_ylabel(\"Transaction Values\", fontsize=17)\n\n    plt.subplots_adjust(hspace = 0.4, top = 0.85)\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- **Transaction Amount Distribuition by ProductCD and Target**\n- **Ürün CD'si ve Hedefe Göre İşlem Tutarı Dağılımı**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Ploting columns with few unique values - Birkaç benzersiz değere sahip sütunları çizme","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ['id_12', 'id_15', 'id_16', 'id_23', 'id_27', 'id_28', 'id_29']:\n    df_train[col] = df_train[col].fillna('NaN')\n    cat_feat_ploting(df_train, col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Modelling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_trans = pd.read_csv('../input/train_transaction.csv')\ndf_test_trans = pd.read_csv('../input/test_transaction.csv')\n\ndf_id = pd.read_csv('../input/train_identity.csv')\ndf_test_id = pd.read_csv('../input/test_identity.csv')\n\nsample_submission = pd.read_csv('../input/sample_submission.csv', index_col='TransactionID')\n\ndf_train = df_trans.merge(df_id, how='left', left_index=True, right_index=True, on='TransactionID')\ndf_test = df_test_trans.merge(df_test_id, how='left', left_index=True, right_index=True, on='TransactionID')\n\nprint(df_train.shape)\nprint(df_test.shape)\n\n# y_train = df_train['isFraud'].copy()\ndel df_trans, df_id, df_test_trans, df_test_id\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Mapping emails","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', \n          'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft',\n          'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',\n          'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', \n          'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink',\n          'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other',\n          'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', \n          'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', \n          'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo',\n          'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n          'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft',\n          'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', \n          'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', \n          'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', \n          'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', \n          'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', \n          'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', \n          'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other',\n          'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n\nus_emails = ['gmail', 'net', 'edu']\n\n# https://www.kaggle.com/c/ieee-fraud-detection/discussion/100499#latest-579654\nfor c in ['P_emaildomain', 'R_emaildomain']:\n    df_train[c + '_bin'] = df_train[c].map(emails)\n    df_test[c + '_bin'] = df_test[c].map(emails)\n    \n    df_train[c + '_suffix'] = df_train[c].map(lambda x: str(x).split('.')[-1])\n    df_test[c + '_suffix'] = df_test[c].map(lambda x: str(x).split('.')[-1])\n    \n    df_train[c + '_suffix'] = df_train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n    df_test[c + '_suffix'] = df_test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encoding categorical features","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Libraries","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"!pip install catboost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data analysis libraries:\nimport numpy as np\nimport pandas as pd\n\n# data visualization libraries:\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# to ignore warnings:\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# to display all columns:\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport seaborn as sns\nfrom sklearn.preprocessing import scale \nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.metrics import roc_auc_score,roc_curve\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nimport numpy as np, pandas as pd, os, gc\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn as sns\nimport lightgbm as lgb\nimport gc\nfrom time import time\nimport datetime\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold, TimeSeriesSplit\nfrom sklearn.metrics import roc_auc_score\nwarnings.simplefilter('ignore')\nsns.set()\n%matplotlib inline\nimport gc\nimport time\nfrom contextlib import contextmanager","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Helper Functions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"@contextmanager\ndef timer(title):\n    t0 = time.time()\n    yield\n    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# FREQUENCY ENCODE TOGETHER\ndef encode_FE(df1, df2, cols):\n    for col in cols:\n        df = pd.concat([df1[col],df2[col]])\n        vc = df.value_counts(dropna=True, normalize=True).to_dict()\n        vc[-1] = -1\n        nm = col+'_FE'\n        df1[nm] = df1[col].map(vc)\n        df1[nm] = df1[nm].astype('float32')\n        df2[nm] = df2[col].map(vc)\n        df2[nm] = df2[nm].astype('float32')\n        print(nm,', ',end='')\n        \n# LABEL ENCODE\ndef encode_LE(col,train,test,verbose=False):\n    df_comb = pd.concat([train[col],test[col]],axis=0)\n    df_comb,_ = df_comb.factorize(sort=True)\n    nm = col\n    if df_comb.max()>32000: \n        train[nm] = df_comb[:len(train)].astype('int32')\n        test[nm] = df_comb[len(train):].astype('int32')\n    else:\n        train[nm] = df_comb[:len(train)].astype('int16')\n        test[nm] = df_comb[len(train):].astype('int16')\n    del df_comb; x=gc.collect()\n    if verbose: print(nm,', ',end='')\n        \n# GROUP AGGREGATION MEAN AND STD\n# https://www.kaggle.com/kyakovlev/ieee-fe-with-some-eda\ndef encode_AG(main_columns, uids, aggregations, train_df, test_df, \n              fillna=True, usena=False):\n    # AGGREGATION OF MAIN WITH UID FOR GIVEN STATISTICS\n    for main_column in main_columns:  \n        for col in uids:\n            for agg_type in aggregations:\n                new_col_name = main_column+'_'+col+'_'+agg_type\n                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n                if usena: temp_df.loc[temp_df[main_column]==-1,main_column] = np.nan\n                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n                                                        columns={agg_type: new_col_name})\n\n                temp_df.index = list(temp_df[col])\n                temp_df = temp_df[new_col_name].to_dict()   \n\n                train_df[new_col_name] = train_df[col].map(temp_df).astype('float32')\n                test_df[new_col_name]  = test_df[col].map(temp_df).astype('float32')\n                \n                if fillna:\n                    train_df[new_col_name].fillna(-1,inplace=True)\n                    test_df[new_col_name].fillna(-1,inplace=True)\n                \n                print(\"'\"+new_col_name+\"'\",', ',end='')\n                \n# COMBINE FEATURES\ndef encode_CB(col1,col2,train,test):\n    nm = col1+'_'+col2\n    train[nm] = train[col1].astype(str)+'_'+train[col2].astype(str)\n    test[nm] = test[col1].astype(str)+'_'+test[col2].astype(str) \n    encode_LE(nm,train,test)\n# GROUP AGGREGATION NUNIQUE\ndef encode_AG2(main_columns, uids, train_df, test_df):\n    for main_column in main_columns:  \n        for col in uids:\n            comb = pd.concat([train_df[[col]+[main_column]],test_df[[col]+[main_column]]],axis=0)\n            mp = comb.groupby(col)[main_column].agg(['nunique'])['nunique'].to_dict()\n            train_df[col+'_'+main_column+'_ct'] = train_df[col].map(mp).astype('float32')\n            test_df[col+'_'+main_column+'_ct'] = test_df[col].map(mp).astype('float32')\n            print(col+'_'+main_column+'_ct, ',end='')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# From kernel https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n# WARNING! THIS CAN DAMAGE THE DATA \ndef reduce_mem_usage2(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def comb_mails(emails,us_emails,train,test):\n    for c in ['P_emaildomain', 'R_emaildomain']:\n                train[c + '_bin'] = train[c].map(emails)\n                test[c + '_bin'] = test[c].map(emails)\n\n                train[c + '_suffix'] = train[c].map(lambda x: str(x).split('.')[-1])\n                test[c + '_suffix'] = test[c].map(lambda x: str(x).split('.')[-1])\n\n                train[c + '_suffix'] = train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n                test[c + '_suffix'] = test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install future","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def love(data):\n    print('\\n'.join([''.join([(' I_Love_Data_Science_'[(x-y) % len('I_Love_Data_Science_')] if ((x*0.05)**2+(y*0.1)**2-1)**3-(x*0.05)**2*(y*0.1)**3 <= 0 else ' ') for x in range(-30, 30)]) for y in range(15, -15, -1)]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Combine","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"        train_id= pd.read_csv(\"../input/train_identity.csv\")\n        test_id = pd.read_csv(\"../input/test_identity.csv\")\n        train_tr = pd.read_csv(\"../input/train_transaction.csv\")\n        test_tr = pd.read_csv(\"../input/test_transaction.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def combine():\n     with timer('Combining :'):   \n        print('Combining Start...')\n        # Read train and test data with pd.read_csv():\n        train_id= pd.read_csv(\"../input/train_identity.csv\")\n        test_id = pd.read_csv(\"../input/test_identity.csv\")\n        train_tr = pd.read_csv(\"../input/train_transaction.csv\")\n        test_tr = pd.read_csv(\"../input/test_transaction.csv\")\n        train=pd.merge(train_tr, train_id, on = \"TransactionID\",how='left',left_index=True, right_index=True)\n        test=pd.merge(test_tr, test_id, on = \"TransactionID\",how=\"left\",left_index=True, right_index=True)\n        del train_id, train_tr, test_id, test_tr\n        test.columns=train.columns.drop(\"isFraud\")\n    \n        \n        \n\n        return train,test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_tr.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> # Preprocessing and Feature Engineering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def pre_processing_and_feature_engineering():\n    train,test=combine()   \n    \n    with timer('Preprocessing and Feature Engineering'):\n        print('-' * 30)\n        print('Preprocessing and Feature Engineering start...')\n        print('-' * 10)\n        \n        emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', \n              'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft',\n              'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',     \n              'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', \n              'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink',\n              'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other',\n              'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', \n              'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', \n              'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo',\n              'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n              'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft',\n              'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', \n              'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', \n              'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', \n              'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', \n              'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', \n              'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', \n              'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other',\n              'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n\n        us_emails = ['gmail', 'net', 'edu']\n        comb_mails(emails,us_emails,train,test)\n        print(\"Creating New Features...\")\n        numeric=['TransactionAmt','D4','D9','D10','D15', 'C1' , 'C2' ,\n               'C3' , 'C4' , 'C5' , 'C6' , 'C7' , 'C8' , 'C9' , 'C10' , \n                'C11' , 'C12' , 'C13' , 'C14','V95' ,'V279' , 'V281',\n                'V284' ,'V285' , 'V286' , 'V288' , 'V290', 'V296',\n                 'V297',\"C11\",\"D14\", \"id_05\",\"id_06\",\"C2\", \"id_13\",\"D3\",\"D11\",\n                \"C14\",\"D5\", \"D9\", \"C1\", \"dist2\", \"D1\", \"D2\", \"C13\", \"id_20\", \"id_19\", \"D4\",\n                \"D8\", \"dist1\", \"id_02\", \"D15\", \"addr1\", \"card2\", \"day\", \"id_19\", \"id_02\"]\n        encode_FE(train,test,['addr1','card1','card2','card3','P_emaildomain'])\n        encode_CB('card1','addr1',train,test)\n        train['day'] = train.TransactionDT / (24*60*60)\n        train['uid'] = train.card1_addr1.astype(str)+'_'+np.floor(train.day-train.D1).astype(str)\n\n        test['day'] = test.TransactionDT / (24*60*60)\n        test['uid'] = test.card1_addr1.astype(str)+'_'+np.floor(test.day-test.D1).astype(str)\n        encode_FE(train,test,['uid'])\n        encode_AG(numeric, ['uid'],['mean',\"std\"], train, test)\n        encode_AG2(['P_emaildomain','dist1','id_02',\"M4\",\"P_emaildomain\", \"M5\", \"id_31\"], ['uid'], train, test)\n        # FREQUENCY ENCODE: ADDR1, CARD1, CARD2, CARD3, P_EMAILDOMAIN\n        encode_FE(train,test,['addr1','card1','card2','card3','P_emaildomain'])\n        # COMBINE COLUMNS CARD1+ADDR1+P_EMAILDOMAIN\n        encode_CB('card1_addr1','P_emaildomain',train, test)\n        # FREQUENCY ENOCDE\n        encode_FE(train,test,['card1_addr1','card1_addr1_P_emaildomain'])\n        # GROUP AGGREGATE\n        encode_AG(['TransactionAmt','D9','D11'],['card1','card1_addr1','card1_addr1_P_emaildomain'],['mean','std'],train,test,usena=True)\n        del train['uid'], test['uid']\n        print(\"Creating New Features Finished\")\n        print('-' * 10)\n        print('Label Coding and One Hot Encoding Start...')\n        train_cat = train.select_dtypes(include=['object'])\n        train_cat_columns=train_cat.columns\n        train_columns=train.columns\n        test_cat = test.select_dtypes(include=['object'])\n        test_cat_columns=test_cat.columns\n        del test_cat, train_cat\n        from sklearn import preprocessing\n        for i in train_cat_columns: \n            lbe=preprocessing.LabelEncoder()\n            train[i]=lbe.fit_transform(train[i].astype(str))\n        for i in test_cat_columns:    \n            test[i]=lbe.fit_transform(test[i].astype(str))\n        for i in train_cat_columns:\n            if (test[i].max()== train[i].max())&(train[i].max()<6):\n                    test = pd.get_dummies(test, columns = [i])\n                    train=pd.get_dummies(train, columns = [i])\n          \n        \n        import datetime\n        START_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')\n        train['DT_M'] = train['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n        train['DT_M'] = (train['DT_M'].dt.year-2017)*12 + train['DT_M'].dt.month \n\n        test['DT_M'] = test['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n        test['DT_M'] = (test['DT_M'].dt.year-2017)*12 + test['DT_M'].dt.month \n         \n        print('-' * 10)\n        return train,test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pre_processing_and_feature_engineering():\n    train,test=combine()   \n    \n    with timer('Preprocessing and Feature Engineering'):\n        print('-' * 30)\n        print('Preprocessing and Feature Engineering start...')\n        print('-' * 10)\n        \n        emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', \n              'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft',\n              'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',     \n              'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', \n              'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink',\n              'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other',\n              'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', \n              'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', \n              'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo',\n              'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n              'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft',\n              'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', \n              'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', \n              'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', \n              'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', \n              'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', \n              'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', \n              'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other',\n              'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n\n        us_emails = ['gmail', 'net', 'edu']\n        comb_mails(emails,us_emails,train,test)\n        print(\"Creating New Features...\")\n        numeric=['TransactionAmt','D4','D9','D10','D15', 'C1' , 'C2' ,\n               'C3' , 'C4' , 'C5' , 'C6' , 'C7' , 'C8' , 'C9' , 'C10' , \n                'C11' , 'C12' , 'C13' , 'C14','V95' ,'V279' , 'V281',\n                'V284' ,'V285' , 'V286' , 'V288' , 'V290', 'V296',\n                 'V297',\"C11\",\"D14\", \"id_05\",\"id_06\",\"C2\", \"id_13\",\"D3\",\"D11\",\n                \"C14\",\"D5\", \"D9\", \"C1\", \"dist2\", \"D1\", \"D2\", \"C13\", \"id_20\", \"id_19\", \"D4\",\n                \"D8\", \"dist1\", \"id_02\", \"D15\", \"addr1\", \"card2\", \"day\", \"id_19\", \"id_02\"]\n        encode_FE(train,test,['addr1','card1','card2','card3','P_emaildomain'])\n        encode_CB('card1','addr1',train,test)\n        train['day'] = train.TransactionDT / (24*60*60)\n        train['uid'] = train.card1_addr1.astype(str)+'_'+np.floor(train.day-train.D1).astype(str)\n\n        test['day'] = test.TransactionDT / (24*60*60)\n        test['uid'] = test.card1_addr1.astype(str)+'_'+np.floor(test.day-test.D1).astype(str)\n        encode_FE(train,test,['uid'])\n        encode_AG(numeric, ['uid'],['mean',\"std\"], train, test)\n        encode_AG2(['P_emaildomain','dist1','id_02',\"M4\",\"P_emaildomain\", \"M5\", \"id_31\"], ['uid'], train, test)\n        # FREQUENCY ENCODE: ADDR1, CARD1, CARD2, CARD3, P_EMAILDOMAIN\n        encode_FE(train,test,['addr1','card1','card2','card3','P_emaildomain'])\n        # COMBINE COLUMNS CARD1+ADDR1+P_EMAILDOMAIN\n        encode_CB('card1_addr1','P_emaildomain',train, test)\n        # FREQUENCY ENOCDE\n        encode_FE(train,test,['card1_addr1','card1_addr1_P_emaildomain'])\n        # GROUP AGGREGATE\n        encode_AG(['TransactionAmt','D9','D11'],['card1','card1_addr1','card1_addr1_P_emaildomain'],['mean','std'],train,test,usena=True)\n        del train['uid'], test['uid']\n        print(\"Creating New Features Finished\")\n        print('-' * 10)\n        print('Label Coding and One Hot Encoding Start...')\n        train_cat = train.select_dtypes(include=['object'])\n        train_cat_columns=train_cat.columns\n        train_columns=train.columns\n        test_cat = test.select_dtypes(include=['object'])\n        test_cat_columns=test_cat.columns\n        del test_cat, train_cat\n        from sklearn import preprocessing\n        for i in train_cat_columns: \n            lbe=preprocessing.LabelEncoder()\n            train[i]=lbe.fit_transform(train[i].astype(str))\n        for i in test_cat_columns:    \n            test[i]=lbe.fit_transform(test[i].astype(str))\n        for i in train_cat_columns:\n            if (test[i].max()== train[i].max())&(train[i].max()<6):\n                    test = pd.get_dummies(test, columns = [i])\n                    train=pd.get_dummies(train, columns = [i])\n          \n        \n        import datetime\n        START_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')\n        train['DT_M'] = train['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n        train['DT_M'] = (train['DT_M'].dt.year-2017)*12 + train['DT_M'].dt.month \n\n        test['DT_M'] = test['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n        test['DT_M'] = (test['DT_M'].dt.year-2017)*12 + test['DT_M'].dt.month \n         \n        print('-' * 10)\n        return train,test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> # main","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def main():\n    with timer('Full Model Run '):\n        print(\"Full Model Run Start...\")\n        print('-' * 50)\n        data=feature_importance()\n        love(data)   \n        print('-' * 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if __name__ == \"__main__\":\n     main()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}