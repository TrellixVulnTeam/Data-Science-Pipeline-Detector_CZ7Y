{"cells":[{"metadata":{},"cell_type":"markdown","source":"## 1. Data preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport random\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\nimport category_encoders\nfrom sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.1 Import and merge the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"os.chdir(\"/kaggle/input/ieee-fraud-detection\")\nos.listdir()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = ''\n# import the data\ntrain_identity = pd.read_csv(path + 'train_identity.csv')\ntrain_transaction = pd.read_csv(path + 'train_transaction.csv')\ntest_identity = pd.read_csv(path + 'test_identity.csv')\ntest_transaction = pd.read_csv(path + 'test_transaction.csv')\n\n# merge identity and transaction to one dataframe\ntrain = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\ntest = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')\n# train_raw, test_raw = train.copy(), test.copy()\n\ndel train_identity, train_transaction, test_identity, test_transaction","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The computation on the whole data set is too expensive, so I randomly select a subset of size 5000 for demo. Note that you should not run the code in following cell if you want to train a precise model."},{"metadata":{"trusted":true},"cell_type":"code","source":"sampleIdx = random.sample([i for i in range(train.shape[0])], k=5000)\ntrain = train.iloc[sampleIdx, :]\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Training dataset has {train.shape[0]} observations and {train.shape[1]} features.')\nprint(f'Test dataset has {test.shape[0]} observations and {test.shape[1]} features.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Ytr = train[\"isFraud\"]\nX = train.drop([\"isFraud\",\"TransactionID\", \"TransactionDT\"], axis=1).append(test.drop([\"TransactionID\", \"TransactionDT\"], axis=1))\n# X_raw, Ytr_raw = X.copy(), Ytr.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.2 Handle missing values\n\nAt first, I dropt features with high proporation (70%) of missing values. Then, I filled missing values in categorical variables with their mode, and filled missing values in numerical variables with their mean."},{"metadata":{"trusted":true},"cell_type":"code","source":"# proporation of missing values\nmissPropor = [X[col].isnull().sum() / X.shape[0] for col in X.columns]\nplt.hist(missPropor, bins=30)\nplt.ylabel(\"Frequency\")\nplt.xlabel(\"Proportion of missing values\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# delete features with high proporation of missing values\nmany_null_cols = [X.columns[i] for i in range(X.shape[1]) if missPropor[i] > 0.7]\nX = X.drop(many_null_cols, axis=1)\nprint(f\"After deleting features with high proporation of missing values, there are {X.shape[1]} features.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fill missing values in categorical variables with their mode.\n# fill missing values in numerical variables with their mean.\nfor i in range(X.shape[1]):\n    if missPropor[i] > 0:\n        if X.iloc[:, i].dtype == \"object\":\n            X.iloc[:, i] = X.iloc[:, i].fillna(X.iloc[:, i].mode()[0])\n        elif X.iloc[:, i].dtype in ['int64', 'float64']:\n            X.iloc[:, i] = X.iloc[:, i].fillna(X.iloc[:, i].mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.3 Encode categorical variables\n\nIn the following part, I tried two different ways: numeric encoding and binary encoding. [Here](https://medium.com/data-design/visiting-categorical-features-and-encoding-in-decision-trees-53400fa65931) is the reason why I did that.\n\nNumeric encoding (label encoding) simply assigns a value to each category. Binary encoding hashes the cardinalities into binary values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# numeric encoding (label encoding)\nX_le = X.copy()\nfor f in X.columns:\n    if X_le[f].dtype == 'object': \n        le = preprocessing.LabelEncoder()\n        le.fit(list(X_le[f].values))\n        X_le[f] = le.transform(list(X_le[f].values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# binary encoding\nX_be = X.copy()\nfor f in X.columns:\n    if X_be[f].dtype == 'object': \n        if X_be[f].nunique() <= 2:\n            le = preprocessing.LabelEncoder()\n            le.fit(list(X_be[f].values))\n            X_be[f] = le.transform(list(X_be[f].values))\n        else:\n            be = category_encoders.BinaryEncoder(cols=f)\n            X_be = be.fit_transform(X_be)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.4 PCA to reduce dimension\n\nAlthough PCA can reduce dimension and computation time, the result showed that the prediction performance with PCA is worse. So I will not use the data after PCA to fit the model and make predictions in the modeling part."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_le_pca = X_le.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# standardize the data\nscaler = preprocessing.StandardScaler()\nscaler.fit(X_le_pca)\nX_le_pca = scaler.transform(X_le_pca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# apply PCA\n# choose the minimum number of principal components \n# such that 99% of the variance is retained.\npca = PCA(0.99)\npca.fit(X_le_pca)\nX_le_pca = pca.transform(X_le_pca)\nX_le_pca = pd.DataFrame(X_le_pca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Number of features after PCA is {X_le_pca.shape[1]}.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.5 Save processed data"},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtr_le = X_le.iloc[:train.shape[0], :]\nXte_le = X_le.iloc[train.shape[0]:, :]\nXtr_be = X_be.iloc[:train.shape[0], :]\nXte_be = X_be.iloc[train.shape[0]:, :]\nXtr_le_pca = X_le_pca.iloc[:train.shape[0], :]\nXte_le_pca = X_le_pca.iloc[train.shape[0]:, :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Xtr_le.to_csv(f\"{path}X_train_labelencoding.csv\", index=False)\n# Xte_le.to_csv(f\"{path}X_test_labelencoding.csv\", index=False)\n# Xtr_be.to_csv(f\"{path}X_train_binaryencoding.csv\", index=False)\n# Xte_be.to_csv(f\"{path}X_test_binaryencoding.csv\", index=False)\n# Xtr_le_pca.to_csv(f\"{path}X_train_labelencoding_pca.csv\", index=False)\n# Xte_le_pca.to_csv(f\"{path}X_test_labelencoding_pca.csv\", index=False)\n# Ytr.to_csv(f\"{path}Y_train.csv\", header=\"isFraud\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV\nimport random\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nimport xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# path = '/home/wkm/Documents/Data set/ieee-fraud-detection/'\n# Xtr = pd.read_csv(f\"{path}X_train_binaryencoding.csv\")\n# Xte = pd.read_csv(f\"{path}X_test_binaryencoding.csv\")\n# Ytr = pd.read_csv(f\"{path}Y_train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtr = Xtr_be\nXte = Xte_be","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_transaction = pd.read_csv(path + 'test_transaction.csv')\nsubmission = pd.DataFrame(test_transaction[\"TransactionID\"])\ndel test_transaction","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.1 Logistic regression"},{"metadata":{},"cell_type":"markdown","source":"The prediction score is 0.713617."},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression(penalty='l2', max_iter=500, n_jobs=6, tol=1e-6, solver=\"sag\")\nlr.fit(Xtr, np.ravel(Ytr))\nYhat_lr = lr.predict_proba(Xte)\nsubmission[\"isFraud\"] = Yhat_lr[:, 1]\n# submission.to_csv(f\"{path}Y_hat_logistic.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.2 Bagging trees"},{"metadata":{},"cell_type":"markdown","source":"I use RandomForestClassifier and set max_features=\"auto\", which means max_features=n_features, so it is bagging."},{"metadata":{"trusted":true},"cell_type":"code","source":"treeCount = 100\n\nbagging = RandomForestClassifier(max_features=\"auto\", min_samples_leaf=1, n_estimators=treeCount)\nbagging.fit(Xtr, np.ravel(Ytr))\nYhat_bagging = bagging.predict_proba(Xte)\nsubmission[\"isFraud\"] = Yhat_bagging[:, 1]\n# submission.to_csv(f\"{path}Y_hat_bagging.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3 Random forests"},{"metadata":{},"cell_type":"markdown","source":"Let's first do parameter tunning."},{"metadata":{"trusted":true},"cell_type":"code","source":"# use oob error to find the best max_features\nnFeatures = Xtr.shape[1]\noobErrList = list()\nmList = [m for m in range(10, nFeatures+1, 30)]\n\nfor m in mList:\n    rf = RandomForestClassifier(max_features=m, min_samples_leaf=1,\\\n                                oob_score=True, n_estimators=50)\n    rf.fit(Xtr, np.ravel(Ytr))\n    oobErrList.append(1-rf.oob_score_)\n    print(m, 1-rf.oob_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(oobErrList)\nplt.plot([m for m in range(10, nFeatures+1, 30)], oobErrList)\nplt.ylabel('OOB error with (n_estimators=50)')\nplt.xlabel('m, the number of variables considered at each split')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Other than max_features (the number of variables considered at each split), the parameter n_estimators (the number of trees) is also important. However, the prediction performance will increase with the increase of n_estimators, so we should select the highest n_estimators as long as our machine can compute it. \n\nI tried different parameters (max_features and n_estimators) and get the prediction scores (evaluted by AUC) as following.\n\n|max_features | n_estimators | PCA | prediction score |\n|---|---|---|---|\n| 223 | 100 | 99%  | 0.871838 |\n| 190 | 100 | 100% | 0.892415 |\n| 100 | 100 | 100% | 0.894553 |\n| 50  | 100 | 100% | 0.895868 |\n| 223 | 100 | 100% | 0.896448 |\n| 90  | 200 | 100% | 0.898140 |\n| 100 | 200 | 100% | 0.899070 |\n| 50  | 200 | 100% | 0.900798 |\n| 15  | 1000| 100% | 0.904874 |\n\nFrom the table above, we can see that max_features does not infulence the prediction performance significantly, but n_estimators does."},{"metadata":{"trusted":true},"cell_type":"code","source":"treeCount = 1000\nm = 15\n\nrf = RandomForestClassifier(max_features=m, min_samples_leaf=1, n_estimators=treeCount)\nrf.fit(Xtr, np.ravel(Ytr))\nYhat_rf = rf.predict_proba(Xte)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission[\"isFraud\"] = Yhat_rf[:, 1]\n# submission.to_csv(f\"{path}Y_hat_rf_m{m}_t{treeCount}.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.4 Gradient boosting"},{"metadata":{},"cell_type":"markdown","source":"At first, let's try gradient boosting with default parameters. The prediction scroe on test data set is 0.891210."},{"metadata":{"trusted":true},"cell_type":"code","source":"# use default parameters\ngbm0 = GradientBoostingClassifier()\ngbm0.fit(Xtr, np.ravel(Ytr))\nsubmission[\"isFraud\"] = gbm0.predict_proba(Xte)[:, 1]\n# submission.to_csv(f\"{path}Y_hat_gbm_default.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's do parameter tunning. The parameters n_estimators and learning_rate are corrlated, so we need to tune them together. Let's use grid search to find the best number of weak learners (n_estima****tors) and the best step size (learning_rate). "},{"metadata":{"trusted":true},"cell_type":"code","source":"param_test1 = {'n_estimators':range(100, 1200, 100), 'learning_rate':[0.01, 0.1, 1]}\ngbm_tune1 = GradientBoostingClassifier(max_features='sqrt', min_samples_leaf=0.001, max_depth=4)\n\ngs1 = GridSearchCV(estimator=gbm_tune1, param_grid=param_test1, iid=False, scoring='roc_auc', n_jobs=6, cv=5)\ngs1.fit(Xtr, np.ravel(Ytr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"The best parameters: {gs1.best_params_}, and the highest mean_test_score is {gs1.best_score_}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"However, I found that n_estimators=600 is unaffordable for computation, so I still set it to 100. Now let's tune the tree parameters max_depth and min_samples_leaf."},{"metadata":{"trusted":true},"cell_type":"code","source":"param_test2 = {'max_depth':range(2, 16, 2), 'min_samples_leaf':[10**i for i in range(-5,0)]}\ngbm_tune2 = GradientBoostingClassifier(max_features='sqrt', n_estimators=100, learning_rate=0.1)\n\ngs2 = GridSearchCV(estimator=gbm_tune2, param_grid=param_test2, iid=False, scoring='roc_auc', n_jobs=6, cv=5)\ngs2.fit(Xtr, np.ravel(Ytr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"The best parameters: {gs2.best_params_}, and the highest mean_test_score is {gs2.best_score_}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use tuned parameters\ngbm1 = GradientBoostingClassifier(max_depth=10, min_samples_leaf=0.001, \n                                  learning_rate=0.1, n_estimators=100)\ngbm1.fit(Xtr, np.ravel(Ytr))\nsubmission[\"isFraud\"] = gbm1.predict_proba(Xte)[:, 1]\n# submission.to_csv(f\"{path}Y_hat_gbm_tuned1.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After parameters tuning, the prediction score (by AUC) on test data set is 0.919523. It is beeter than that without parameters tuning."},{"metadata":{},"cell_type":"markdown","source":"### 2.5 XGBoost"},{"metadata":{},"cell_type":"markdown","source":"The prediction scroe on test data set by XGBoost with default parameters is 0.900976, and 0.931355 with the parameters tuned by GBT (see section 2.4 Gradient boosting)."},{"metadata":{"trusted":true},"cell_type":"code","source":"xgbc = xgb.XGBClassifier(n_jobs=4, max_depth=10, min_samples_leaf=0.001, \n                         learning_rate=0.1, n_estimators=100, eval_metric=\"auc\")\nxgbc.fit(Xtr, np.ravel(Ytr))\nsubmission[\"isFraud\"] = xgbc.predict_proba(Xte)[:, 1]\n# submission.to_csv(f\"{path}Y_hat_xgb_tuned5.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}