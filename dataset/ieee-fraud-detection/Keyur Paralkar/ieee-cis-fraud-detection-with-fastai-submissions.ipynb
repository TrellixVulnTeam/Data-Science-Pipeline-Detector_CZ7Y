{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install fastai==0.7.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai import *\nfrom fastai.structured import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pathlib","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import *","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading/Creatintg sub-sample data\n\nWe will start with sub-sample of the training data. We will train the randomForestClassifier on this sub-sampled data and using feature importance we will find out which are the most important features that needs to be preserved and which are redundent can be discarded."},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = '/kaggle/input/ieee-fraud-detection/'\nworking_path = '/kaggle/working/'\n\npath = pathlib.Path(PATH)\npath_w = pathlib.Path(working_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls {PATH}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create a sample transaction and identity dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# !head -n 10000 {path}/train_transaction.csv > {path_w}/sample_train_transaction.csv\n# !head -n 10000 {path}/train_identity.csv > {path_w}/sample_train_identity.csv","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[](http://)From the competition description there are two CSV files. One with transaction details and other with identity of each user."},{"metadata":{"trusted":true},"cell_type":"code","source":"# trans = pd.read_csv(path_w/'sample_train_transaction.csv')\n# trans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# iden = pd.read_csv(path_w/'sample_train_identity.csv')\n# iden","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# len(trans.columns),len(iden.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Merging the two dataframe to form single training dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"# data = pd.merge(left=trans,right=iden,on='TransactionID',how='left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Converting all the object data-type based columns to categorical data-types"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_cats(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using `proc_df` function to fill up missing values present in the dataframe with mean and separating out dependant variable from independant variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_trn, y_trn, nas = proc_df(data,y_fld='isFraud')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def split_vals(a,n): return a[:n],a[n:]\n\n# n_valid = 1200\n# n_train = len(data) - n_valid\n# X_train, X_valid = split_vals(df_trn,n_train)\n# y_train, y_valid = split_vals(y_trn,n_train)\n# raw_train, raw_valid = split_vals(data,n_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# raw_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt \nfrom sklearn.metrics import roc_auc_score, roc_curve, auc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`print_score` function will help us to find roc_auc_score and mean accuracy on training and validation set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# def print_score(m):\n#     results = [roc_auc_score(y_train,m.predict(X_train)),roc_auc_score(y_valid,m.predict(X_valid)),\n#                m.score(X_train,y_train),m.score(X_valid,y_valid)]\n#     print(\"Scores = \",results)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`set_rf_samples` will be used for setting the size of random sub-sampled dataset in each estimators of RandomForestClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# set_rf_samples(5000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model = RandomForestRegressor(n_estimators=10,min_samples_leaf=3,max_features=0.5,n_jobs=-1)\n# model.fit(X_train,y_train)\n# print_score(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %time preds = np.stack([t.predict(X_valid) for t in model.estimators_])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# np.mean(preds[:,0]),np.std(preds[:,0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature importance\n`rf_feat_importance` is one of the coolest functions of fast.ai library for sturctured data. Using this we can get the most important columns with affect the predictions of our model. This is done via. randomly suffling each column and then running the model to find it's accuracy. Over here we are capturing top 10 most important features of the dataset which affect the accuracy of the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# fi = rf_feat_importance(model,df_trn)\n# def plot_fi(fi): return fi.plot('cols','imp','barh',figsize=(12,10),legend=True)\n# plot_fi(fi[:25])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# imps1 = fi[fi['imp'] > 0.015]['cols'].tolist()\n# len(imps1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# imps = ['TransactionID'] + list(filter(lambda x: x!='TransactionID',imps1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# len(imps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_trn_imps = df_trn.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #splitting the data again into train and validation set:\n# X_train, X_valid = split_vals(df_trn_imps, n_train)\n# y_train, y_valid = split_vals(y_trn,n_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set_rf_samples(5000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_imps = RandomForestRegressor(n_estimators=10,min_samples_leaf=3,max_features=0.5,n_jobs=-1)\n# model_imps.fit(X_train,y_train)\n# print_score(model_imps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %time preds_imps = np.stack([t.predict(X_valid) for t in model_imps.estimators_])\n# np.mean(preds_imps[:,0]),np.std(preds_imps[:,0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training on Full-scale dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Training this on a full scale dataset\n\n#Reading the dataset\ntrans_full = pd.read_csv(path/'train_transaction.csv')\niden_full = pd.read_csv(path/'train_identity.csv')\ndata = pd.merge(left=trans_full,right=iden_full,on='TransactionID',how='left')\n\n#converting object data to categorical data:\ntrain_cats(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_vals(a,n): return a[:n],a[n:]\n\ndf_trn, y_trn, nas = proc_df(data,y_fld='isFraud')\n\nn_valid = 118108\nn_train = len(df_trn) - n_valid\n\nX_train, X_valid = split_vals(df_trn,n_train)\ny_train, y_valid = split_vals(y_trn, n_train)\ntrain_raw, valid_raw = split_vals(data, n_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imps = ['TransactionID','C12','C8','card1','C1','TransactionAmt','C13','TransactionDT']\nX_train = X_train[imps]\nX_valid = X_valid[imps]\nX_train.columns,X_valid.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set_rf_samples(50000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploring different hyperparameters\nHere we are going to explore the best hyperparameters for our random forest classifier."},{"metadata":{"trusted":true},"cell_type":"code","source":"# estimators_list = [80,100,120,200,300,400,500]\n# min_samples_leaf = 3\n# max_features = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def return_score(est_list,min_sampls,max_fts):\n#     total_findings = []\n#     for n,est in enumerate(est_list):\n#         m = RandomForestRegressor(n_estimators=est,min_samples_leaf=min_sampls,max_features=max_fts,n_jobs=-1,oob_score=True)\n#         m.fit(X_train,y_train)\n#         results = [roc_auc_score(y_train,m.predict(X_train)),roc_auc_score(y_valid,m.predict(X_valid)),\n#                    m.score(X_train,y_train),m.score(X_valid,y_valid)]\n#         print(f'Estimator {n}: Scores = {results}, OOB_SCORE = {m.oob_score_}')\n#         total_findings.append(results)\n        \n#     return total_findings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def plot_validation_graph(e_list,min_spl,max_fts):\n#     tf_findings = return_score(e_list,min_spl,max_fts)\n#     valid_acc = [t[-1] for t in tf_findings]\n#     valid_auc_score = [t[1] for t in tf_findings]\n    \n#     fig,(ax1,ax2) = plt.subplots(1,2,figsize=(18,7))\n\n#     ax1.plot(estimators_list,valid_acc)\n#     ax1.set_xlabel('Number of estimators')\n#     ax1.set_ylabel('Mean accuracy')\n#     ax1.set_title('Validation set accuracy over estimators')\n\n#     ax2.plot(estimators_list,valid_auc_score)\n#     ax2.set_xlabel('Number of estimators')\n#     ax2.set_ylabel('roc_auc_score')\n#     ax2.set_title('Validation set roc_auc_score over estimators')\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot_validation_graph(estimators_list,3,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot_validation_graph(estimators_list,2,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot_validation_graph(estimators_list,1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot_validation_graph(estimators_list,2,0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot_validation_graph(estimators_list,1,0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt \nfrom sklearn.metrics import roc_auc_score, roc_curve, auc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_score(m):\n    results = [roc_auc_score(y_train,m.predict(X_train)),roc_auc_score(y_valid,m.predict(X_valid)),\n               m.score(X_train,y_train),m.score(X_valid,y_valid)]\n    if(m.oob_score_):\n        print(f'Score = {results}, OOB_SCORE = {m.oob_score_}')\n    else:\n        print(\"Score = \",results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Based on above analysis let us take mni_samples_leaf = 3 and max_features = 1 with n_estimators = 40\nm = RandomForestRegressor(n_estimators=800,min_samples_leaf=3,max_features=1,n_jobs=-1,oob_score=True)\nm.fit(X_train,y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# m_best_model = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n#                        max_depth=None, max_features=1, max_leaf_nodes=None,\n#                        min_impurity_decrease=0.0, min_impurity_split=None,\n#                        min_samples_leaf=3, min_samples_split=2,\n#                        min_weight_fraction_leaf=0.0, n_estimators=800,\n#                        n_jobs=None, oob_score=False, random_state=None,\n#                        verbose=0, warm_start=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# m_best_model.fit(X_train,y_train)\n# print_score(m_best_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print_score(m_best_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %time preds_new = np.stack([t.predict(X_valid) for t in m.estimators_])\n# np.mean(preds_new[:,0]),np.std(preds_new[:,0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_valid.shape, preds_new.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# valid_cpy = valid_raw.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# valid_cpy['preds'] = np.mean(preds_new,axis=0)\n# valid_cpy['preds_std'] = np.std(preds_new,axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# valid_cpy.groupby('C12').mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploring hyperparameters via parameter grid of sklearn"},{"metadata":{},"cell_type":"markdown","source":"First let us get to know about hte parameters the randomForestRegressor uses"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import pprint\n# pp = pprint.PrettyPrinter(indent=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rf = RandomForestRegressor()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pp.pprint(rf.get_params())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.model_selection import RandomizedSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# random_search_grid = {\n#     'n_estimators':[x for x in range(200,2001,200)],\n#     'min_samples_leaf':[0.5,1,3,5,8,10],\n#     'max_features':[0.5,1,3,5,8,10]\n# }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_search_grid, n_iter=10,cv=3,verbose=2,random_state=42,n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rf_random.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reading test set:\nt_trans = pd.read_csv(path/'test_transaction.csv')\nt_iden= pd.read_csv(path/'test_identity.csv')\ntest_data = pd.merge(left=t_trans,right=t_iden,on='TransactionID',how='left')\n\ntrain_cats(test_data)\ntest_df,t, _ = proc_df(test_data,na_dict=nas)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df_cpy = test_df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del m_best_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imps","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df_cpy = test_df_cpy[imps]\ntest_df_cpy.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = m.predict(test_df_cpy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df = pd.DataFrame({'TransactionID':test_df_cpy['TransactionID'],'isFraud': test_preds})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}