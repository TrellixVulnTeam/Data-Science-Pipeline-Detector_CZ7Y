{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"### Import modules"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly as py\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\n\ninit_notebook_mode(connected=True)\n\nfrom tqdm import tqdm_notebook\nfrom sklearn.svm import NuSVR, SVR\nfrom sklearn.metrics import mean_absolute_error\npd.options.display.precision = 15\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport time\nimport datetime\nfrom catboost import CatBoostRegressor\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, RobustScaler\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit\nfrom sklearn import metrics\nfrom sklearn import linear_model\nimport gc\nimport seaborn as sns\nimport eli5\nimport shap\nfrom IPython.display import HTML\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n%env JOBLIB_TEMP_FOLDER=/tmp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To get started I'm going to lift some functions from [Andrew Lukayenko's fantastic kernel](https://www.kaggle.com/artgor/eda-and-models)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage(deep=True).sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                c_prec = df[col].apply(lambda x: np.finfo(x).precision).max()\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max and c_prec == np.finfo(np.float32).precision:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n    \n\ndef missingData(df):\n    total = df.isnull().sum().sort_values(ascending=False)\n    percent = (df.isnull().sum())/df.isnull().count().sort_values(ascending=False)\n    missing_data = pd.concat([total, percent], axis=1, keys=['Total','Percent'], sort=False).sort_values('Total', ascending=False)\n    return missing_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"folder_path = '../input/ieee-fraud-detection/'\ntrain_identity = pd.read_csv(f'{folder_path}train_identity.csv')\ntrain_transaction = pd.read_csv(f'{folder_path}train_transaction.csv')\ntest_identity = pd.read_csv(f'{folder_path}test_identity.csv')\ntest_transaction = pd.read_csv(f'{folder_path}test_transaction.csv')\nsub = pd.read_csv(f'{folder_path}sample_submission.csv')\ntrain = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\ntest = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Train dataset has {train.shape[0]} examples and {train.shape[1]} features.')\nprint(f'Test dataset has {test.shape[0]} examples and {test.shape[1]} features.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_identity.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_identity, train_transaction, test_identity, test_transaction;","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Missing data"},{"metadata":{},"cell_type":"markdown","source":"### Nullity matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"import missingno as msno\n\nmissingdata_df = train.columns[train.isnull().any()].tolist()\nmsno.heatmap(train[missingdata_df], figsize=(20,20))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_data = missingData(train)\nmissing_data.head(35)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Deletions"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_drop = missing_data[missing_data['Percent'] > 0.5].index\ntrain = train.drop(cols_to_drop, axis=1)\ntest = test.drop(cols_to_drop, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Imputations"},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_data = missingData(train)\nnull_cols = missing_data[missing_data['Percent']>0].index\nmissing_data.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in null_cols:\n    #print('Data in column {} has format {}.'.format(col, str(train[col].dtype)))\n    train[col] = train[col].replace(np.nan, train[col].mode()[0])\n    test[col] = test[col].replace(np.nan, train[col].mode()[0])\n    #print('Filled the null values of column {}'.format(col))\n    #print('-----------------------------')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define design matrix [X] and target [y]"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop('isFraud', axis=1)\ny = train['isFraud']\n\nprint('The design has shape {}'.format(X.shape))\nprint('The target has shape {}'.format(y.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define categorical [X_cat] and numerical [X_num] sectors of the design"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_cat = X.select_dtypes(include='object')\nX_num = X.select_dtypes(exclude='object')\n\ncat_cols = X_cat.columns.values\nnum_cols = X_num.columns.values\n\nprint('Categorical columns: ', cat_cols)\nprint('Numerical Columns: ', num_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\n\nfig.add_trace(\n    go.Histogram(\n        x = y,\n        histnorm='probability')\n)\n\nfig.update_layout(height=450, width=400, title = 'Distribution of the target [isFraud]')\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_rows, n_cols = 2, 5\nfig = make_subplots(rows=n_rows, cols=n_cols)\n\ni=0\nfor r in range(1, n_rows+1):\n    for c in range(1, n_cols+1):\n        if i >= len(cat_cols): continue\n        feature = cat_cols[i]\n        fig.add_trace(\n            go.Histogram(\n                x=train[feature],\n                histnorm='probability',\n                name=feature\n            ),            \n            row= r, col = c\n        )\n        i+=1\n\nfig.update_layout(height=900, width=1500, title='Distributions of categorical variables')\n        \nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\n\nfig.add_trace(\n    go.Histogram(\n        x = train['TransactionDT'],\n        histnorm='probability',\n        name = 'Training set')\n)\n\nfig.add_trace(\n    go.Histogram(\n        x = test['TransactionDT'],\n        histnorm='probability',\n        name = 'Test set')\n)\n\nfig.update_layout(height=450, width=900, title = 'Distribution of transaction dates')\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"n_rows, n_cols = 2, 5\nfig = make_subplots(rows=n_rows, cols=n_cols)\n\ncols = ['TransactionID', 'TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3', 'card5', 'addr1', 'addr2']\ni=0\nfor r in range(1, n_rows+1):\n    for c in range(1, n_cols+1):\n        if i >= len(cols): continue\n        feature = cols[i]\n        fig.add_trace(\n            go.Histogram(\n                x=train[feature],\n                histnorm='probability',\n                name=feature\n            ),            \n            row= r, col = c\n        )\n        i+=1\n\nfig.update_layout(height=900, width=1500, title='Distributions of numerical variables')\n        \nfig.show()"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"n_rows, n_cols =  7, 2\n\nfig = make_subplots(rows=n_rows, cols=n_cols)\n\ncols = [ 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14']\ni=0\nfor r in range(1, n_rows+1):\n    for c in range(1, n_cols+1):\n        if i >= len(cols): continue\n        feature = cols[i]\n        fig.add_trace(\n            go.Histogram(\n                x=train[feature],\n                histnorm='probability',\n                name=feature\n            ),            \n            row= r, col = c\n        )\n        i+=1\n\nfig.update_layout(height=2400, width=1500, title='Distributions of categorical variables')\n        \nfig.show()"},{"metadata":{},"cell_type":"markdown","source":"n_rows, n_cols =  7, 2\n\nfig = make_subplots(rows=n_rows, cols=n_cols)\n\ncols = [ 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14']\ni=0\nfor r in range(1, n_rows+1):\n    for c in range(1, n_cols+1):\n        if i >= len(cols): continue\n        feature = cols[i]\n        fig.add_trace(\n            go.Histogram(\n                x=train[feature],\n                histnorm='probability',\n                name=feature\n            ),            \n            row= r, col = c\n        )\n        i+=1\n\nfig.update_layout(height=2400, width=1500, title='Distributions of categorical variables')\n        \nfig.show()"},{"metadata":{},"cell_type":"markdown","source":"## Preprocess input for modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = OneHotEncoder(sparse=True)\nencoder.fit(X_cat)\nX_cat_e = encoder.transform(X_cat)\nX_cat_e","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.sparse import coo_matrix\nX_num_sparse = coo_matrix(X_num)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_num_sparse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy\nfrom scipy.sparse import hstack\nX_sparse = scipy.sparse.hstack((X_cat_e, X_num_sparse))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = RobustScaler(with_centering=False)\nXsc = scaler.fit_transform(X_sparse)\nprint(Xsc.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names = []\nfor arr in encoder.categories_:\n    feature_names += list(arr)\nfeature_names += list(num_cols)\nprint(feature_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Models"},{"metadata":{},"cell_type":"markdown","source":"## LGBoost Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'num_leaves': 491,\n          'min_child_weight': 0.03454472573214212,\n          'feature_fraction': 0.3797454081646243,\n          'bagging_fraction': 0.4181193142567742,\n          'min_data_in_leaf': 106,\n          'objective': 'binary',\n          'max_depth': -1,\n          'learning_rate': 0.006883242363721497,\n          \"boosting_type\": \"gbdt\",\n          \"bagging_seed\": 11,\n          \"metric\": 'auc',\n          \"verbosity\": -1,\n          'reg_alpha': 0.3899927210061127,\n          'reg_lambda': 0.6485237330340494,\n          'random_state': 47\n         }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# folds = TimeSeriesSplit(n_splits=5)\n\n# aucs = list()\n# feature_importances = pd.DataFrame()\n# feature_importances['feature'] = feature_names\n\n# training_start_time = time.time()\n# for fold, (trn_idx, test_idx) in enumerate(folds.split(Xsc, y)):\n#     start_time = time.time()\n#     print('Training on fold {}'.format(fold + 1))\n    \n#     trn_data = lgb.Dataset(Xsc[trn_idx,:], label=y.iloc[trn_idx])\n#     val_data = lgb.Dataset(Xsc[test_idx,:], label=y.iloc[test_idx])\n#     clf = lgb.train(params, trn_data, 10000, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds=500)\n    \n#     feature_importances['fold_{}'.format(fold + 1)] = clf.feature_importance()\n#     aucs.append(clf.best_score['valid_1']['auc'])\n    \n#     print('Fold {} finished in {}'.format(fold + 1, str(datetime.timedelta(seconds=time.time() - start_time))))\n# print('-' * 30)\n# print('Training has finished.')\n# print('Total training time is {}'.format(str(datetime.timedelta(seconds=time.time() - training_start_time))))\n# print('Mean AUC:', np.mean(aucs))\n# print('-' * 30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature_importances['average'] = feature_importances[['fold_{}'.format(fold + 1) for fold in range(folds.n_splits)]].mean(axis=1)\n# feature_importances.to_csv('feature_importances.csv')\n\n# plt.figure(figsize=(16, 16))\n# sns.barplot(data=feature_importances.sort_values(by='average', ascending=False).head(50), x='average', y='feature');\n# plt.title('50 TOP feature importance over {} folds average'.format(folds.n_splits));","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}