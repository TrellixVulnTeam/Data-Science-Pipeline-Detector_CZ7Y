{"cells":[{"metadata":{"_uuid":"7c13bb26-fbb0-4ab2-a227-31cfd48327fc","_cell_guid":"f78aaa3b-2419-419c-a970-f711c156147a","trusted":true},"cell_type":"code","source":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport multiprocessing\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nimport gc\nfrom time import time\nimport datetime\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold, TimeSeriesSplit\nfrom sklearn.metrics import roc_auc_score\nwarnings.simplefilter('ignore')\nsns.set()\n%matplotlib inline\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a90a34a-8d1e-4d0d-9497-840ecd86415c","_cell_guid":"30cef36c-813f-4654-90e5-2fd96b4e202d","trusted":true},"cell_type":"code","source":"# load the train and test data\ntrain_identity=pd.read_csv(\"/kaggle/input/ieee-fraud-detection/train_identity.csv\")\ntrain_transaction=pd.read_csv(\"/kaggle/input/ieee-fraud-detection/train_transaction.csv\")\ntest_identity=pd.read_csv(\"/kaggle/input/ieee-fraud-detection/test_identity.csv\")\ntest_transaction=pd.read_csv(\"/kaggle/input/ieee-fraud-detection/test_transaction.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a6744552-3063-4ef5-8ed0-371281d3e734","_cell_guid":"c2932023-4bfe-404f-85cd-b72c80491f7a","trusted":true},"cell_type":"code","source":"# reduce your memory by conversion\n# convert it to the low memory to fit the RAM\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"425d99af-31f9-47b4-ac73-c3ce558449fd","_cell_guid":"547741b3-a0de-43c2-8b0a-e88be1c0dd26","trusted":true},"cell_type":"code","source":"#merge both the transaction and identity by left\ntrain=pd.merge(train_transaction,train_identity,how=\"left\",on=\"TransactionID\")\ntest=pd.merge(test_transaction,test_identity,how=\"left\",on=\"TransactionID\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ab639a8-7099-43fc-bb68-249bda56ff84","_cell_guid":"ccfe7b85-0b12-4d26-bb47-8eed998c5dfb","trusted":true},"cell_type":"code","source":"#now we should reduce the memory to free the RAM or else we cant fit the model\ntrain=reduce_mem_usage(train)\ntest=reduce_mem_usage(test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3dc29625-2192-47e4-b32d-5999f8f10eae","_cell_guid":"331177cf-592d-4033-8e57-f0b62a8d9cd3","trusted":true},"cell_type":"code","source":"# delete the 4 variables in order to reduce the memory issue\ndel train_identity\ndel test_identity\ndel train_transaction\ndel test_transaction","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33c1d906-fa19-4fb8-b262-b4902cbe5f53","_cell_guid":"4f5eeec3-fc9f-4b81-801b-280acb8cfb57","trusted":true},"cell_type":"code","source":"#Try to explore  all the columns in your dataframe\ntrain.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c2a13f9a-92e6-412d-a4d8-7d7235a517e8","_cell_guid":"05bd69b9-e6b9-43eb-a130-e42e1bb20faa","trusted":true},"cell_type":"code","source":"# category columns\ncategory_column=['id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29',\n            'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo', 'ProductCD', 'card4', 'card6', 'M4','P_emaildomain',\n            'R_emaildomain', 'card1', 'card2', 'card3',  'card5', 'addr1', 'addr2', 'M1', 'M2', 'M3', 'M5', 'M6', 'M7', 'M8', 'M9',\n            'P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3', 'R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3']\nprint(\"no of categorical column:\",len(category_column))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37a69b5d-59a0-460f-9e51-6a7fe106de81","_cell_guid":"fb5ed60a-63cd-4b6f-a0ea-c58dd54b0212","trusted":true},"cell_type":"code","source":"#let us try to check for NAs in each columns\nprint(\"Train data\")\ntrain.isna().sum()\nprint(\"Test data\")\ntest.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7426b603-2495-4354-90cd-c2bb5c4ffc66","_cell_guid":"54d26472-5981-4ab2-bb92-80727df4a6fb","trusted":true},"cell_type":"code","source":"#EDA\n#If there is more than 90% NA's we can remove that no need of that column it was not going to affect that much on the final column\nmore_than_90_NA_or_same_value_train=[]\nmore_than_90_NA_or_same_value_test=[]\nmany_na_train=[]\nmany_na_test=[]\nfor col in train.columns:\n    if train[col].isna().sum()/train.shape[0] >=0.90:\n        many_na_train.append(col) # full of NAs in train\nfor col in test.columns:\n    if test[col].isna().sum()/test.shape[0]>=0.90:\n        many_na_test.append(col) # full of NAs in test\nfor col in train.columns:\n  #  print(col,train[col].value_counts(dropna=False,normalize=True).values[0])\n    if train[col].value_counts(dropna=False,normalize=True).values[0] >= 0.90:\n      #  print(\"More than 90% is NA's or same value so we can delete that columns\")\n        more_than_90_NA_or_same_value_train.append(col) # more unique values in train\nfor col in test.columns:\n    if test[col].value_counts(dropna=False,normalize=True).values[0]>=0.90:\n        more_than_90_NA_or_same_value_test.append(col) #more unique values in test","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7193a8cb-ab95-4436-8250-e38795a573f4","_cell_guid":"726dca52-04d3-41cb-bae5-5326c8706c8c","trusted":true},"cell_type":"code","source":"# store the columns to be dropped separately in train and test\ncols_drop_at_train=list(set(more_than_90_NA_or_same_value_train+many_na_train))\ncols_drop_at_test=list(set(more_than_90_NA_or_same_value_test+many_na_test))\nprint(\"Columns to be dropped in train\",len(cols_drop_at_train))\nprint(\"Columns to be dropped in test\",len(cols_drop_at_test))\nprint(\"columns are @ train:\",cols_drop_at_train)\nprint(\"columns are @ test:\", cols_drop_at_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0bdc4b3-af2f-4695-9f33-a1bf54e1f506","_cell_guid":"3a09165d-b550-441a-ad37-c564709da569","trusted":true},"cell_type":"code","source":"total_drop_cols=list(set(cols_drop_at_train+cols_drop_at_test))\nprint(\"Total no of columns to be deleted to increase your model performance\",len(total_drop_cols))\nprint(\"They are:\",total_drop_cols)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"27ab2366-3863-4836-90ed-b456a8356f07","_cell_guid":"6a7e6120-3e32-480c-aa33-be98e33f62f7","trusted":true},"cell_type":"code","source":"# remove the isFraud\ntotal_drop_cols.remove('isFraud')\nprint(\"You can check thta column is removed:\",total_drop_cols)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2d543d8-905e-4e84-93b3-12b6762c75d5","_cell_guid":"9c9607ed-eaad-446d-957e-f72f8b9bfad6","trusted":true},"cell_type":"code","source":"for col in total_drop_cols:\n    if col not in train.columns:\n        print(\"missing drop column in train\",col)\n    if col not in test.columns:\n        print(\"Missing drop columns in test\",col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n=0\nprint(\"len\",len(total_drop_cols))\nfor col in train.columns:\n    if col in total_drop_cols:\n        n+=1\nprint(n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b345ad5-6abd-4afc-9149-ee1c91eba979","_cell_guid":"d2ccd351-a8f2-4e18-b229-91d9e25f9b8a","trusted":true},"cell_type":"code","source":"#columns after dropping unwanted columns\nprint(\"Total no of columns we have now\",len(train.columns))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b97128a-e129-41c3-ba06-c8b733657584","_cell_guid":"0c21918b-6f40-4857-9261-309dc65c372d","trusted":true},"cell_type":"code","source":"# after dropping columns we need to explore the data distrubtions\n# try to plot the distribution to check it\n# reference:https://towardsdatascience.com/histograms-and-density-plots-in-python-f6bda88f5ac0\n# we can start to analyze from the  TransactionDT\n# timedelta from a given reference datetime (not an actual timestamp)\nsns.distplot(train['TransactionDT'], hist=True, kde=True,bins=40) # its shows histogram along with the density plot\nsns.distplot(test['TransactionDT'],hist=True,kde=True,bins=40)\nplt.title('Density Plot of  TransactionDT  in training data')\nplt.xlabel(' TransactionDT')\nplt.ylabel('Counts')\n   \n# so totally the given test is future of the train so be carefull with the split","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f15a1e2f-9763-4122-9b08-13158f872104","_cell_guid":"cc7e8ac7-d98a-4d5d-aebc-9ed3c5593738","trusted":true},"cell_type":"code","source":"#TransactionAMT: transaction payment amount in USD\nsns.distplot(train['TransactionAmt'], hist=True, kde=True,bins=1) # its shows histogram along with the density plot\nplt.title('Density Plot of  TransactionAMT in training data')\nplt.xlabel(' TransactionAMT')\nplt.ylabel('Counts')\n# most of the amount is less than 5000","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c3ca6674-4e9f-47bd-8939-4eb0b466579b","_cell_guid":"a973a2a5-9952-4ccd-b96c-8e4fb613ab07","trusted":true},"cell_type":"code","source":"#ProductCD --  product code, the product for each transaction\n#sns.catplot(x=\"index\", y=\"ProductCD\", hue=\"index\", kind=\"bar\", data=feature_count); \n# in the above plot we can arrange it\nsns.countplot(x=\"ProductCD\", data=train) # shows the count in each class","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc8a3bff-53ef-4540-93fe-b76481efb8ef","_cell_guid":"c44367cd-d221-4b6c-a13f-02ed7254540c","trusted":true},"cell_type":"code","source":"# how we can start to analyze more about the cards\n#card1 - card6: payment card information, such as card type, card category, issue bank, country, etc.\n# categorical variable -ALL the cards\nfor col in ['card1','card2','card3','card4','card5','card6']:\n    print(\"Feature count of \" + str(col))\n    feature_count=(train[col].value_counts())\n    print(feature_count.head(2)) # its so big so i have plotted only 2 \n# card1- some numerical values\n#card2- some amount with float values\n#card3- same as card 2\n#card4 - card type- [visa,mastercard,american express,discover]\n#card5- same as card2\n#card6- type of the card-[debit,credit,charge card,debit or credit]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7c52fa7-5d3c-4738-9516-6b23dc09403a","_cell_guid":"be6c1cb9-43da-4db4-a403-39346db71478","trusted":true},"cell_type":"code","source":"# card 6-type of card\nsns.countplot(x=train['card6'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6329c645-074a-4536-9142-7d98f5dd4035","_cell_guid":"039b7742-9e83-4ff1-b49c-d487b4db5022","trusted":true},"cell_type":"code","source":"#card4-types of card\nsns.countplot(train['card4'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"96b0ab28-7590-4195-a3cf-a631e5a4777a","_cell_guid":"7366e370-7dfb-4a8e-b193-252543311cd2","trusted":true},"cell_type":"code","source":"# how we can start to check how many transaction amount are in each types of card\n#for the sum it shows infifnite\nprint(train.groupby('card4')['TransactionAmt'].mean()) # the discover has highest mean over all","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a5f356f7-e84b-4cba-a9f9-4e78b12c11fc","_cell_guid":"225d07f5-efc2-42c9-91a2-64e034ae9e92","trusted":true},"cell_type":"code","source":"# we can now check for card6\nprint(train.groupby('card6')['TransactionAmt'].mean()) # the discover has highest mean over all\n#fig, ax = plt.subplots()\n#train.groupby('card6').plot(x='card6', y='TransactionAmt',ax=ax)\n# credit card has more value","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f481836-7950-47dc-a2f6-02092583e0c2","_cell_guid":"c9aaa62c-9fd1-4f85-98ff-463b90d8a883","trusted":true},"cell_type":"code","source":"a4_dims = (20, 20)\nfig, axs = plt.subplots(4,1, figsize=a4_dims, squeeze=False)\ncard_list=['card1','card2','card3','card5']\nco=0\n\nfor r in range(0,4):\n    for c in range(0, 1): \n        feature_count=train[card_list[co]].value_counts().reset_index()\n        feature_count=feature_count.iloc[:40,]\n        #print(len(feature_count.iloc[:40,]))\n        ax=sns.barplot(x='index',y=card_list[co],data=feature_count,errwidth=12,capsize=10,ax=axs[r][c])\n        ax.set_xlabel(card_list[co])\n        ax.set_ylabel('Number of Occurrences')\n        co+=1\n\n\nprint(\"This column has high number of categoricals to print so it will be very slow\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8879c5ee-ba1b-4010-bde2-cac6f608bf55","_cell_guid":"26fee99e-9e0f-4266-85dc-63008dea3d21","trusted":true},"cell_type":"code","source":"# addr: address addr1, addr2- categorical variable\na4_dims = (20, 20)\nfig, axs = plt.subplots(2,1, figsize=a4_dims, squeeze=False)\naddr_list=['addr1','addr2']\nco=0\n\nfor r in range(0,2):\n    for c in range(0, 1): \n        feature_count=train[addr_list[co]].value_counts().reset_index()\n       # feature_count= feature_count.sort_values([addr_list[co]])\n        feature_count=feature_count.iloc[:40,]\n        #print(len(feature_count.iloc[:40,]))\n        \n        ax=sns.barplot(x='index',y=addr_list[co],data=feature_count,errwidth=12,capsize=10,ax=axs[r][c])\n        ax.set_xlabel(addr_list[co])\n        ax.set_ylabel('Number of Occurrences')\n        co+=1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38035ccb-db95-4b41-b92e-6979b838e3b6","_cell_guid":"389e675b-0fc9-4759-9184-fe36699b12b7","trusted":true},"cell_type":"code","source":"# dist: distance is numerical we can analyze it later\n#P_ and (R__) emaildomain: purchaser and recipient email domain its categorical\n#print(train['P_emaildomain'].value_counts())\na4_dims = (20, 20)\nfig, axs = plt.subplots(2,1, figsize=a4_dims, squeeze=False)\naddr_list=['P_emaildomain','R_emaildomain']\nco=0\n\nfor r in range(0,2):\n    for c in range(0, 1): \n        feature_count=train[addr_list[co]].value_counts().reset_index()\n       # feature_count= feature_count.sort_values([addr_list[co]])\n        feature_count=feature_count.iloc[:40,]\n        #print(len(feature_count.iloc[:40,]))\n        \n        ax=sns.barplot(x='index',y=addr_list[co],data=feature_count,errwidth=12,capsize=10,ax=axs[r][c])\n        ax.set_xlabel(addr_list[co])\n        ax.set_ylabel('Number of Occurrences')\n        ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n        co+=1\n\n#in both this case the domain name like .eu and .in are different but they should be same try to preprocess it \n# we can split the given one by '.' and take the fisrt part for the correct mail names","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"36fd1c0d-23df-4be5-b018-27e781f7e05a","_cell_guid":"e87a126f-9c16-4878-b6a8-88491cb66f61","trusted":true},"cell_type":"code","source":"m_list=['M1','M2','M3','M4','M5','M6','M7','M8','M9']\n# check they are caetgorical or not\nfor col in m_list:\n    print(\"For the \" + str(col))\n    print(train[col].value_counts())\n# expect M4 all other are T/F","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"084a6685-5b8d-4a80-bb36-531fb387c231","_cell_guid":"bbba6d96-757f-40a1-93bc-4f2d791b24b8","trusted":true},"cell_type":"code","source":"# M1 - M9 categorical variable that need to analyze\n# the values are match, such as names on card and address, etc.\na4_dims = (20, 20)\nfig, axs = plt.subplots(9,1, figsize=a4_dims, squeeze=False)\nco=0\nm_list=['M1','M2','M3','M4','M5','M6','M7','M8','M9']\nfor r in range(0,9):\n    for c in range(0, 1): \n        feature_count=train[m_list[co]].value_counts().reset_index()\n       # feature_count= feature_count.sort_values([addr_list[co]])\n        feature_count=feature_count.iloc[:40,]\n        #print(len(feature_count.iloc[:40,]))\n        \n        ax=sns.barplot(x='index',y=m_list[co],data=feature_count,errwidth=12,capsize=100,ax=axs[r][c])\n       \n        ax.set_xlabel(m_list[co])\n        ax.set_ylabel('Number of Occurrences')\n        ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n        co+=1\nplt.subplots_adjust(hspace = 0.2)\nplt.tight_layout()\n# I think no need of preprocess for this M1-M9 set of features.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15952b95-0cc3-4327-8c2d-16bc0234b539","_cell_guid":"269acf1e-cdcf-4e59-9043-395db86f909d","trusted":true},"cell_type":"code","source":"#DeviceType\n#train['DeviceType'].value_counts() # only two types # we can check where we get more isFraud \nsns.countplot(x='DeviceType',hue='isFraud',data=train)\n# we have more isFraud  in desktop","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"74293873-ca05-4dc7-831a-99051b751f3f","_cell_guid":"2be56c02-6c36-4ee2-94e3-aa06b7043115","trusted":true},"cell_type":"code","source":"#Deviceinfo\nfeature_count=train['DeviceInfo'].value_counts().reset_index()\nfeature_count.sort_values('DeviceInfo')\nfeature_count=feature_count.iloc[:40,]\n#print(feature_count)\nax=sns.barplot(x=\"index\", y=\"DeviceInfo\", data=feature_count,errwidth=12,capsize=100)\nax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n# we have more isFraud  in desktop","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4e97797-3d2e-40e3-869c-1b15fec67b00","_cell_guid":"1cd5fdbd-dd87-45d8-b8dd-8bb56a81688f","trusted":true},"cell_type":"code","source":"#id12 - id38 we need to analyze this part its categorical variable\nid_list=[]\nfor i in range(12,39):\n    id_list.append('id_'+str(i))\nprint(id_list)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e5ac3f3-1e21-43b8-8192-268f51e048d7","_cell_guid":"08cd400d-17a9-46dd-98c9-63d900285624","trusted":true},"cell_type":"code","source":"#iterate the id_list and visualize it\na4_dims = (20, 20)\nfig, axs = plt.subplots(5,1, figsize=a4_dims, squeeze=False)\n\nco=0\n\nfor r in range(0,5):\n    for c in range(0, 1): \n        feature_count=train[id_list[co]].value_counts().reset_index()\n       # feature_count= feature_count.sort_values([addr_list[co]])\n        feature_count=feature_count.iloc[:40,]\n        #print(len(feature_count.iloc[:40,]))\n        \n        ax=sns.barplot(x='index',y=id_list[co],data=feature_count,errwidth=12,capsize=100,ax=axs[r][c])\n       \n        ax.set_xlabel(id_list[co])\n        ax.set_ylabel('Number of Occurrences')\n        ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n        co+=1\n\nplt.tight_layout()\n#id_12 - found/not_found\n#id_13- many fields are there\n#id_14- many fields are there\n#id_15- found/new/unknown\n#id_16-found/not_found","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f727e76a-85ad-445b-84be-d1c3d19093a8","_cell_guid":"813452b2-dbfc-4ed1-a712-c4935ff63e83","trusted":true},"cell_type":"code","source":"#next 5 features\na4_dims = (20, 20)\nfig, axs = plt.subplots(5,1, figsize=a4_dims, squeeze=False)\n\nco=5\n\nfor r in range(0,5):\n    for c in range(0, 1): \n        feature_count=train[id_list[co]].value_counts().reset_index()\n       # feature_count= feature_count.sort_values([addr_list[co]])\n        feature_count=feature_count.iloc[:40,]\n        #print(len(feature_count.iloc[:40,]))\n        \n        ax=sns.barplot(x='index',y=id_list[co],data=feature_count,errwidth=12,capsize=100,ax=axs[r][c])\n       \n        ax.set_xlabel(id_list[co])\n        ax.set_ylabel('Number of Occurrences')\n        ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n        co+=1\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3eab2f37-1850-4ee5-9f3d-9cd6dda532d1","_cell_guid":"d8ce9ce5-cb81-4833-b0a0-49fc545df5ca","trusted":true},"cell_type":"code","source":"#next 5 features\na4_dims = (20, 20)\nfig, axs = plt.subplots(5,1, figsize=a4_dims, squeeze=False)\n\nco=10\n\nfor r in range(0,5):\n    for c in range(0, 1): \n        feature_count=train[id_list[co]].value_counts().reset_index()\n       # feature_count= feature_count.sort_values([addr_list[co]])\n        feature_count=feature_count.iloc[:40,]\n        #print(len(feature_count.iloc[:40,]))\n        \n        ax=sns.barplot(x='index',y=id_list[co],data=feature_count,errwidth=12,capsize=100,ax=axs[r][c])\n       \n        ax.set_xlabel(id_list[co])\n        ax.set_ylabel('Number of Occurrences')\n        ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n        co+=1\n\nplt.tight_layout()\n#id_23-proxy-transparent/anonymous/hidden","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b1d04cda-15ff-4748-a5b1-f1448801dcce","_cell_guid":"d7851744-c3b8-45e2-86fe-b6a69db7b952","trusted":true},"cell_type":"code","source":"#next 5 features\na4_dims = (20, 20)\nfig, axs = plt.subplots(5,1, figsize=a4_dims, squeeze=False)\n\nco=15\n\nfor r in range(0,5):\n    for c in range(0, 1): \n        feature_count=train[id_list[co]].value_counts().reset_index()\n       # feature_count= feature_count.sort_values([addr_list[co]])\n        feature_count=feature_count.iloc[:40,]\n        #print(len(feature_count.iloc[:40,]))\n        \n        ax=sns.barplot(x='index',y=id_list[co],data=feature_count,errwidth=12,capsize=100,ax=axs[r][c])\n       \n        ax.set_xlabel(id_list[co])\n        ax.set_ylabel('Number of Occurrences')\n        ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n        co+=1\n\nplt.tight_layout()\n#id_27/_29 -found/not-found\n#id_28-found/new","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59d0fd91-bbb4-4c59-8ca4-178cf6f20642","_cell_guid":"bf1c9d7a-31bd-44d0-8168-345192251ec2","trusted":true},"cell_type":"code","source":"#next 5 features\n\n#id_34- match-0,1,2\n#id_35/_36-True/False","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24f29690-a7f3-46ef-8692-922e77d0edc0","_cell_guid":"fded77d0-2240-4278-9267-b077d39dcd82","trusted":true},"cell_type":"code","source":"#we need to analyze the some id columns and numeric columns are left now\n# try to create new feature with the help of the EDA\n# and then try to reduce the dimension by dropping it \n# encode the category data\n# missing value treatment for numeric and category columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# you can check the some of the important parameters here\nsomeFeature_list=['id_36','id_35','id_34','id_28','id_29','id_12','id_15','id_16']\na4_dims = (20, 20)\nco=0\nax=sns.countplot(x=someFeature_list[co],hue='isFraud',data=train)\nax.set_xlabel(someFeature_list[co])\nax.set_ylabel('Number of Occurrences')\nax.set_xticklabels(ax.get_xticklabels(), rotation=45)\nco+=1\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax=sns.countplot(x=someFeature_list[co],hue='isFraud',data=train)\nax.set_xlabel(someFeature_list[co])\nax.set_ylabel('Number of Occurrences')\nax.set_xticklabels(ax.get_xticklabels(), rotation=45)\nco+=1\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax=sns.countplot(x=someFeature_list[co],hue='isFraud',data=train)\nax.set_xlabel(someFeature_list[co])\nax.set_ylabel('Number of Occurrences')\nax.set_xticklabels(ax.get_xticklabels(), rotation=45)\nco+=1\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax=sns.countplot(x=someFeature_list[co],hue='isFraud',data=train)\nax.set_xlabel(someFeature_list[co])\nax.set_ylabel('Number of Occurrences')\nax.set_xticklabels(ax.get_xticklabels(), rotation=45)\nco+=1\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax=sns.countplot(x=someFeature_list[co],hue='isFraud',data=train)\nax.set_xlabel(someFeature_list[co])\nax.set_ylabel('Number of Occurrences')\nax.set_xticklabels(ax.get_xticklabels(), rotation=45)\nco+=1\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax=sns.countplot(x=someFeature_list[co],hue='isFraud',data=train)\nax.set_xlabel(someFeature_list[co])\nax.set_ylabel('Number of Occurrences')\nax.set_xticklabels(ax.get_xticklabels(), rotation=45)\nco+=1\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax=sns.countplot(x=someFeature_list[co],hue='isFraud',data=train)\nax.set_xlabel(someFeature_list[co])\nax.set_ylabel('Number of Occurrences')\nax.set_xticklabels(ax.get_xticklabels(), rotation=45)\nco+=1\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax=sns.countplot(x=someFeature_list[co],hue='isFraud',data=train)\nax.set_xlabel(someFeature_list[co])\nax.set_ylabel('Number of Occurrences')\nax.set_xticklabels(ax.get_xticklabels(), rotation=45)\nco+=1\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a980ac92-acd6-42b3-b416-12df9f8b71a6","_cell_guid":"4796b2d7-5ed5-475b-a9c5-a8e29757160f","trusted":true},"cell_type":"code","source":"print(addr_list)\n# when you do one hot encoding please add both test and train both may have different one\nfor col in addr_list:\n    train[col]=(train[col].str.split(\".\",expand=True)[0])\n    test[col]=(test[col].str.split(\".\",expand=True)[0])\n# now we are done with mails so we do some feature engineering","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9da21bf1-4165-429c-889b-b5746bf757b4","_cell_guid":"c4838809-dfe6-4c26-8816-95c105e27990","trusted":true},"cell_type":"code","source":"#Feature Engineering \n# first we can try to use card features\nfor col in ['card1','card2','card3','card4','card5','card6']:\n    # we are just taking a mean for each group and diving it with the each group Transaction amount to get more information\n    # and also std for each group \n    train['Transactionamt_mean_'+str(col)]=(train['TransactionAmt']/train.groupby(col)['TransactionAmt'].transform('mean'))\n    train['Transactionamt_std_'+str(col)]=(train['TransactionAmt']/train.groupby(col)['TransactionAmt'].transform('std'))\n    test['Transactionamt_mean_'+str(col)]=(test['TransactionAmt']/test.groupby(col)['TransactionAmt'].transform('mean'))\n    test['Transactionamt_std_'+str(col)]=(test['TransactionAmt']/test.groupby(col)['TransactionAmt'].transform('std'))\n#feature Engineering only for Cards alone\n# we also need to check device info and device type,id_30,id_31","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's try to do feature Enginnering based on ProductCD because it has only 4 levels\n# and also for P_emaildomain ,R_emaildomain,DeviceType\n#DeviceInfo,id_15,id_23,id_30,id_31,id_34\n#you guys can do according to your understanding\nfor col in ['ProductCD', 'P_emaildomain','R_emaildomain','DeviceType','DeviceInfo','id_15','id_23','id_30','id_31','id_34']:\n    train['Transactionamt_mean_'+str(col)]=(train['TransactionAmt']/train.groupby(col)['TransactionAmt'].transform('mean'))\n    train['Transactionamt_std_'+str(col)]=(train['TransactionAmt']/train.groupby(col)['TransactionAmt'].transform('std'))\n    test['Transactionamt_mean_'+str(col)]=(test['TransactionAmt']/test.groupby(col)['TransactionAmt'].transform('mean'))\n    test['Transactionamt_std_'+str(col)]=(test['TransactionAmt']/test.groupby(col)['TransactionAmt'].transform('std'))\n# there will be lot of NAN in our columns ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now we can preprocess our data\nprint(\"Total number of columns after Feture Engineering:\",len(train.columns)) #466\n# now we want to drop the unwanted columns\nprint(total_drop_cols)\n#train=train.drop(drop)\nfor col in total_drop_cols:\n    del train[col]\n    del test[col]\nprint(\"Final number of columns after Feature Engineering:\",len(train.columns)) # 384\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now we can do label encoding for categorical variable\n# we can do one hot encoding but it will increase our dimension so its problem\n# so we can try label encoding or any other encoding like frequency encoding .etc\n# i am going to try label encoding\nfrom sklearn  import preprocessing\nfor col in train.columns:\n    if train[col].dtype=='object' :\n      #  print(\"label encoding\",col)\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(train[col].values) + list(test[col].values))\n        train[col] =lbl.transform(list(train[col].values))\n        test[col]=lbl.transform(list(test[col].values))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_inf_nan(df):\n    return df.replace([np.inf, -np.inf], np.nan)   \n\n# Cleaning infinite values to NaN\ntrain = clean_inf_nan(train)\ntest = clean_inf_nan(test ) # replace all nan,inf,-inf to nan so it will be easy to replace\nfor i in train.columns:\n    train[i].fillna(train[i].median(),inplace=True) # fill with median because mean may be affect by outliers.\n#X.isna().sum().sum()\nfor i in test.columns:\n    test[i].fillna(test[i].median(),inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(\"Number of Na's in train\",train.isna().sum().sum())\nprint(\"Number of Na's in test\",test.isna().sum().sum())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now we an split the data and train our model\nX = train.drop(['isFraud', 'TransactionDT', 'TransactionID'], axis=1)\ny = train['isFraud']\n#X_test = test.sort_values('TransactionDT').drop(['TransactionDT', 'TransactionID'], axis=1)\nX_test = test.drop(['TransactionDT', 'TransactionID'], axis=1)\n#del train\ntest = test[['TransactionID']]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n#train and test split\nfrom sklearn.model_selection import train_test_split\nxTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_fold = 5\nfolds = TimeSeriesSplit(n_splits=n_fold)\nfolds = KFold(n_splits=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission=pd.DataFrame()\nsubmission['TransactionID']=test['TransactionID']\nsubmission['isFraud'] = 0\nimport xgboost as xgb\nfrom sklearn.metrics import roc_auc_score\nprint(submission.head(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport time\nimport datetime\nimport json\nimport gc\nfrom numba import jit\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm_notebook\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nfrom sklearn import metrics\n\nfrom itertools import product\n\nimport altair as alt\nfrom altair.vega import v5\ndef train_model_classification(X, X_test, y, params, folds, model_type='lgb', eval_metric='auc', columns=None, plot_feature_importance=False, model=None,\n                               verbose=10000, early_stopping_rounds=200, n_estimators=50000, splits=None, n_folds=3, averaging='usual', n_jobs=-1):\n    \"\"\"\n    A function to train a variety of classification models.\n    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n    \n    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: y - target\n    :params: folds - folds to split data\n    :params: model_type - type of model to use\n    :params: eval_metric - metric to use\n    :params: columns - columns to use. If None - use all columns\n    :params: plot_feature_importance - whether to plot feature importance of LGB\n    :params: model - sklearn model, works only for \"sklearn\" model type\n    \n    \"\"\"\n    columns = X.columns if columns is None else columns\n    n_splits = folds.n_splits if splits is None else n_folds\n    X_test = X_test[columns]\n    \n    # to set up scoring parameters\n    metrics_dict = {'auc': {'lgb_metric_name': eval_auc,\n                        'catboost_metric_name': 'AUC',\n                        'sklearn_scoring_function': metrics.roc_auc_score},\n                    }\n    \n    result_dict = {}\n    if averaging == 'usual':\n        # out-of-fold predictions on train data\n        oof = np.zeros((len(X), 1))\n\n        # averaged predictions on train data\n        prediction = np.zeros((len(X_test), 1))\n        \n    elif averaging == 'rank':\n        # out-of-fold predictions on train data\n        oof = np.zeros((len(X), 1))\n\n        # averaged predictions on train data\n        prediction = np.zeros((len(X_test), 1))\n\n    \n    # list of scores on folds\n    scores = []\n    feature_importance = pd.DataFrame()\n    \n    # split and train on folds\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n        print(f'Fold {fold_n + 1} started at {time.ctime()}')\n        if type(X) == np.ndarray:\n            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n            y_train, y_valid = y[train_index], y[valid_index]\n        else:\n            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n            \n        if model_type == 'lgb':\n            model = lgb.LGBMClassifier(**params, n_estimators=n_estimators, n_jobs = n_jobs)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n            \n            y_pred_valid = model.predict_proba(X_valid)[:, 1]\n            y_pred = model.predict_proba(X_test, num_iteration=model.best_iteration_)[:, 1]\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=n_estimators, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=verbose, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n        \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            \n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n            print('')\n            \n            y_pred = model.predict_proba(X_test)\n        \n        if model_type == 'cat':\n            model = CatBoostClassifier(iterations=n_estimators, eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n                                      loss_function=Logloss)\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test)\n        \n        if averaging == 'usual':\n            \n            oof[valid_index] = y_pred_valid.reshape(-1, 1)\n            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n            \n            prediction += y_pred.reshape(-1, 1)\n\n        elif averaging == 'rank':\n                                  \n            oof[valid_index] = y_pred_valid.reshape(-1, 1)\n            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n                                  \n            prediction += pd.Series(y_pred).rank().values.reshape(-1, 1)        \n        \n        if model_type == 'lgb' and plot_feature_importance:\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction /= n_splits\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    result_dict['oof'] = oof\n    result_dict['prediction'] = prediction\n    result_dict['scores'] = scores\n    \n    if model_type == 'lgb':\n        if plot_feature_importance:\n            feature_importance[\"importance\"] /= n_splits\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n            \n            result_dict['feature_importance'] = feature_importance\n            result_dict['top_columns'] = cols\n        \n    return result_dict\ndef fast_auc(y_true, y_prob):\n    \"\"\"\n    fast roc_auc computation: https://www.kaggle.com/c/microsoft-malware-prediction/discussion/76013\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_true = y_true[np.argsort(y_prob)]\n    nfalse = 0\n    auc = 0\n    n = len(y_true)\n    for i in range(n):\n        y_i = y_true[i]\n        nfalse += (1 - y_i)\n        auc += y_i * nfalse\n    auc /= (nfalse * (n - nfalse))\n    return auc\n\n\ndef eval_auc(y_true, y_pred):\n    \"\"\"\n    Fast auc eval function for lgb.\n    \"\"\"\n    return 'auc', fast_auc(y_true, y_pred), True\n\n\ndef group_mean_log_mae(y_true, y_pred, types, floor=1e-9):\n    \"\"\"\n    Fast metric computation for this competition: https://www.kaggle.com/c/champs-scalar-coupling\n    Code is from this kernel: https://www.kaggle.com/uberkinder/efficient-metric\n    \"\"\"\n    maes = (y_true-y_pred).abs().groupby(types).mean()\n    return np.log(maes.map(lambda x: max(x, floor))).mean()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'num_leaves': 256,\n          'min_child_samples': 79,\n          'objective': 'binary',\n          'max_depth': 13,\n          'learning_rate': 0.03,\n          \"boosting_type\": \"gbdt\",\n          \"subsample_freq\": 3,\n          \"subsample\": 0.9,\n          \"bagging_seed\": 11,\n          \"metric\": 'auc',\n          \"verbosity\": -1,\n          'reg_alpha': 0.3,\n          'reg_lambda': 0.3,\n          'colsample_bytree': 0.9,\n          #'categorical_feature': cat_cols\n         }\nresult_dict_lgb = train_model_classification(X=X, X_test=X_test, y=y, params=params, folds=folds, model_type='lgb', eval_metric='auc', plot_feature_importance=True,\n                                                      verbose=500, early_stopping_rounds=200, n_estimators=5000, averaging='usual', n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['isFraud']=result_dict_lgb['prediction']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('Submission_v2.csv')","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}