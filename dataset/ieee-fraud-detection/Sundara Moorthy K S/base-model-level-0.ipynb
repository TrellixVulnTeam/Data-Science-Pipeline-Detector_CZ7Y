{"cells":[{"metadata":{"_uuid":"7c13bb26-fbb0-4ab2-a227-31cfd48327fc","_cell_guid":"f78aaa3b-2419-419c-a970-f711c156147a","trusted":true},"cell_type":"code","source":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport multiprocessing\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nimport gc\nfrom time import time\nimport datetime\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold, TimeSeriesSplit\nfrom sklearn.metrics import roc_auc_score\nwarnings.simplefilter('ignore')\nsns.set()\n%matplotlib inline\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a90a34a-8d1e-4d0d-9497-840ecd86415c","_cell_guid":"30cef36c-813f-4654-90e5-2fd96b4e202d","trusted":true},"cell_type":"code","source":"# load the train and test data\ntrain_identity=pd.read_csv(\"/kaggle/input/ieee-fraud-detection/train_identity.csv\")\ntrain_transaction=pd.read_csv(\"/kaggle/input/ieee-fraud-detection/train_transaction.csv\")\ntest_identity=pd.read_csv(\"/kaggle/input/ieee-fraud-detection/test_identity.csv\")\ntest_transaction=pd.read_csv(\"/kaggle/input/ieee-fraud-detection/test_transaction.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a6744552-3063-4ef5-8ed0-371281d3e734","_cell_guid":"c2932023-4bfe-404f-85cd-b72c80491f7a","trusted":true},"cell_type":"code","source":"# reduce your memory by conversion\n# convert it to the low memory to fit the RAM\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"425d99af-31f9-47b4-ac73-c3ce558449fd","_cell_guid":"547741b3-a0de-43c2-8b0a-e88be1c0dd26","trusted":true},"cell_type":"code","source":"#merge both the transaction and identity by left\ntrain=pd.merge(train_transaction,train_identity,how=\"left\",on=\"TransactionID\")\ntest=pd.merge(test_transaction,test_identity,how=\"left\",on=\"TransactionID\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ab639a8-7099-43fc-bb68-249bda56ff84","_cell_guid":"ccfe7b85-0b12-4d26-bb47-8eed998c5dfb","trusted":true},"cell_type":"code","source":"#now we should reduce the memory to free the RAM or else we cant fit the model\ntrain=reduce_mem_usage(train)\ntest=reduce_mem_usage(test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3dc29625-2192-47e4-b32d-5999f8f10eae","_cell_guid":"331177cf-592d-4033-8e57-f0b62a8d9cd3","trusted":true},"cell_type":"code","source":"# delete the 4 variables in order to reduce the memory issue\ndel train_identity\ndel test_identity\ndel train_transaction\ndel test_transaction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nnot_in_test=[]\nfor col in train.columns:\n    if col not in test.columns and col !=\"isFraud\":\n        not_in_test.append(col) # missing in test data id_01\n        print(\"The column missing in test\",col)\nnot_in_train=[]\nfor col in test.columns:\n    if col not in train.columns:\n        not_in_train.append(col)\ndi={}\nfor col in not_in_train:\n    d=col.strip().split(\"-\")\n    print(d[0]+\"_\"+d[1])\n    di[col]=d[0]+\"_\"+d[1]\"\"\"\n#test.rename(columns=di)\nre_list=[]\nfor col in test.columns:\n    if col not in train.columns:\n        d=col.strip().split(\"-\")\n        print(d[0]+\"_\"+d[1])\n        re_list.append(d[0]+\"_\"+d[1])\n    else:\n        re_list.append(col)\nprint(\"re_list\",re_list)\n    \n#print(\"The columns that are missed in train\",not_in_train,\"test data\",not_in_test)\n#\"id_15\"\n\"\"\"\ngapminder.rename(columns={,'pop':'population',\n                          'lifeExp':'life_exp',\n                          'gdpPercap':'gdp_per_cap'}, \n                 inplace=True)\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"previous_one=list(test.columns)\ntest.columns=re_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33c1d906-fa19-4fb8-b262-b4902cbe5f53","_cell_guid":"4f5eeec3-fc9f-4b81-801b-280acb8cfb57","trusted":true},"cell_type":"code","source":"#Try to explore  all the columns in your dataframe\ntrain.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()\nfor col in train.columns:\n    if col not in test.columns:\n        print(col)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c2a13f9a-92e6-412d-a4d8-7d7235a517e8","_cell_guid":"05bd69b9-e6b9-43eb-a130-e42e1bb20faa","trusted":true},"cell_type":"code","source":"# category columns\ncategory_column=['id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29',\n            'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo', 'ProductCD', 'card4', 'card6', 'M4','P_emaildomain',\n            'R_emaildomain', 'card1', 'card2', 'card3',  'card5', 'addr1', 'addr2', 'M1', 'M2', 'M3', 'M5', 'M6', 'M7', 'M8', 'M9',\n            'P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3', 'R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3']\nprint(\"no of categorical column:\",len(category_column))\nfor col in category_column:\n    if col not in test.columns:\n        print(\"The columns we are missed in test\",col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(train.columns),len(test.columns))\nfor col in test.columns:\n    if col not in train.columns:\n        print(\"The extra columns we have\",col)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37a69b5d-59a0-460f-9e51-6a7fe106de81","_cell_guid":"fb5ed60a-63cd-4b6f-a0ea-c58dd54b0212","trusted":true},"cell_type":"code","source":"#let us try to check for NAs in each columns\nprint(\"Train data\")\ntrain.isna().sum()\nprint(\"Test data\")\ntest.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7426b603-2495-4354-90cd-c2bb5c4ffc66","_cell_guid":"54d26472-5981-4ab2-bb92-80727df4a6fb","trusted":true},"cell_type":"code","source":"#EDA\n#If there is more than 90% NA's we can remove that no need of that column it was not going to affect that much on the final column\nmore_than_90_NA_or_same_value_train=[]\nmore_than_90_NA_or_same_value_test=[]\nmany_na_train=[]\nmany_na_test=[]\nfor col in train.columns:\n    if train[col].isna().sum()/train.shape[0] >=0.90:\n        many_na_train.append(col) # full of NAs in train\nfor col in test.columns:\n    if test[col].isna().sum()/test.shape[0]>=0.90:\n        many_na_test.append(col) # full of NAs in test\nfor col in train.columns:\n  #  print(col,train[col].value_counts(dropna=False,normalize=True).values[0])\n    if train[col].value_counts(dropna=False,normalize=True).values[0] >= 0.90:\n      #  print(\"More than 90% is NA's or same value so we can delete that columns\")\n        more_than_90_NA_or_same_value_train.append(col) # more unique values in train\nfor col in test.columns:\n    if test[col].value_counts(dropna=False,normalize=True).values[0]>=0.90:\n        more_than_90_NA_or_same_value_test.append(col) #more unique values in test","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7193a8cb-ab95-4436-8250-e38795a573f4","_cell_guid":"726dca52-04d3-41cb-bae5-5326c8706c8c","trusted":true},"cell_type":"code","source":"# store the columns to be dropped separately in train and test\ncols_drop_at_train=list(set(more_than_90_NA_or_same_value_train+many_na_train))\ncols_drop_at_test=list(set(more_than_90_NA_or_same_value_test+many_na_test))\nprint(\"Columns to be dropped in train\",len(cols_drop_at_train))\nprint(\"Columns to be dropped in test\",len(cols_drop_at_test))\nprint(\"columns are @ train:\",cols_drop_at_train)\nprint(\"columns are @ test:\", cols_drop_at_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0bdc4b3-af2f-4695-9f33-a1bf54e1f506","_cell_guid":"3a09165d-b550-441a-ad37-c564709da569","trusted":true},"cell_type":"code","source":"total_drop_cols=list(set(cols_drop_at_train+cols_drop_at_test))\nprint(\"Total no of columns to be deleted to increase your model performance\",len(total_drop_cols))\nprint(\"They are:\",total_drop_cols)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"27ab2366-3863-4836-90ed-b456a8356f07","_cell_guid":"6a7e6120-3e32-480c-aa33-be98e33f62f7","trusted":true},"cell_type":"code","source":"# remove the isFraud\ntotal_drop_cols.remove('isFraud')\nprint(\"You can check thta column is removed:\",total_drop_cols)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2d543d8-905e-4e84-93b3-12b6762c75d5","_cell_guid":"9c9607ed-eaad-446d-957e-f72f8b9bfad6","trusted":true},"cell_type":"code","source":"for col in total_drop_cols:\n    if col not in train.columns:\n        print(\"missing drop column in train\",col)\n    if col not in test.columns:\n        print(\"Missing drop columns in test\",col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n=0\nprint(\"len\",len(total_drop_cols))\nfor col in train.columns:\n    if col in total_drop_cols:\n        n+=1\nprint(n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b345ad5-6abd-4afc-9149-ee1c91eba979","_cell_guid":"d2ccd351-a8f2-4e18-b229-91d9e25f9b8a","trusted":true},"cell_type":"code","source":"#columns after dropping unwanted columns\nprint(\"Total no of columns we have now\",len(train.columns),len(test.columns))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ['ProductCD', 'P_emaildomain','R_emaildomain','DeviceType','DeviceInfo','id_15','id_23','id_30','id_31','id_34']:\n    if col in total_drop_cols :\n        print(col)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b97128a-e129-41c3-ba06-c8b733657584","_cell_guid":"0c21918b-6f40-4857-9261-309dc65c372d","trusted":true},"cell_type":"code","source":"# after dropping columns we need to explore the data distrubtions\n# try to plot the distribution to check it\n# reference:https://towardsdatascience.com/histograms-and-density-plots-in-python-f6bda88f5ac0\n# we can start to analyze from the  TransactionDT\n# timedelta from a given reference datetime (not an actual timestamp)\nsns.distplot(train['TransactionDT'], hist=True, kde=True,bins=40) # its shows histogram along with the density plot\nsns.distplot(test['TransactionDT'],hist=True,kde=True,bins=40)\nplt.title('Density Plot of  TransactionDT  in training data')\nplt.xlabel(' TransactionDT')\nplt.ylabel('Counts')\n   \n# so totally the given test is future of the train so be carefull with the split","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f15a1e2f-9763-4122-9b08-13158f872104","_cell_guid":"cc7e8ac7-d98a-4d5d-aebc-9ed3c5593738","trusted":true},"cell_type":"code","source":"#TransactionAMT: transaction payment amount in USD\nsns.distplot(train['TransactionAmt'], hist=True, kde=True,bins=1) # its shows histogram along with the density plot\nplt.title('Density Plot of  TransactionAMT in training data')\nplt.xlabel(' TransactionAMT')\nplt.ylabel('Counts')\n# most of the amount is less than 5000","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c3ca6674-4e9f-47bd-8939-4eb0b466579b","_cell_guid":"a973a2a5-9952-4ccd-b96c-8e4fb613ab07","trusted":true},"cell_type":"code","source":"#ProductCD --  product code, the product for each transaction\n#sns.catplot(x=\"index\", y=\"ProductCD\", hue=\"index\", kind=\"bar\", data=feature_count); \n# in the above plot we can arrange it\nsns.countplot(x=\"ProductCD\", data=train) # shows the count in each class","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc8a3bff-53ef-4540-93fe-b76481efb8ef","_cell_guid":"c44367cd-d221-4b6c-a13f-02ed7254540c","trusted":true},"cell_type":"code","source":"# how we can start to analyze more about the cards\n#card1 - card6: payment card information, such as card type, card category, issue bank, country, etc.\n# categorical variable -ALL the cards\nfor col in ['card1','card2','card3','card4','card5','card6']:\n    print(\"Feature count of \" + str(col))\n    feature_count=(train[col].value_counts())\n    print(feature_count.head(2)) # its so big so i have plotted only 2 \n# card1- some numerical values\n#card2- some amount with float values\n#card3- same as card 2\n#card4 - card type- [visa,mastercard,american express,discover]\n#card5- same as card2\n#card6- type of the card-[debit,credit,charge card,debit or credit]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7c52fa7-5d3c-4738-9516-6b23dc09403a","_cell_guid":"be6c1cb9-43da-4db4-a403-39346db71478","trusted":true},"cell_type":"code","source":"# card 6-type of card\nsns.countplot(x=train['card6'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6329c645-074a-4536-9142-7d98f5dd4035","_cell_guid":"039b7742-9e83-4ff1-b49c-d487b4db5022","trusted":true},"cell_type":"code","source":"#card4-types of card\nsns.countplot(train['card4'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"96b0ab28-7590-4195-a3cf-a631e5a4777a","_cell_guid":"7366e370-7dfb-4a8e-b193-252543311cd2","trusted":true},"cell_type":"code","source":"# how we can start to check how many transaction amount are in each types of card\n#for the sum it shows infifnite\nprint(train.groupby('card4')['TransactionAmt'].mean()) # the discover has highest mean over all","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a5f356f7-e84b-4cba-a9f9-4e78b12c11fc","_cell_guid":"225d07f5-efc2-42c9-91a2-64e034ae9e92","trusted":true},"cell_type":"code","source":"# we can now check for card6\nprint(train.groupby('card6')['TransactionAmt'].mean()) # the discover has highest mean over all\n#fig, ax = plt.subplots()\n#train.groupby('card6').plot(x='card6', y='TransactionAmt',ax=ax)\n# credit card has more value","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f481836-7950-47dc-a2f6-02092583e0c2","_cell_guid":"c9aaa62c-9fd1-4f85-98ff-463b90d8a883","trusted":true},"cell_type":"code","source":"a4_dims = (20, 20)\nfig, axs = plt.subplots(4,1, figsize=a4_dims, squeeze=False)\ncard_list=['card1','card2','card3','card5']\nco=0\n\nfor r in range(0,4):\n    for c in range(0, 1): \n        feature_count=train[card_list[co]].value_counts().reset_index()\n        feature_count=feature_count.iloc[:40,]\n        #print(len(feature_count.iloc[:40,]))\n        ax=sns.barplot(x='index',y=card_list[co],data=feature_count,errwidth=12,capsize=10,ax=axs[r][c])\n        ax.set_xlabel(card_list[co])\n        ax.set_ylabel('Number of Occurrences')\n        co+=1\n\n\nprint(\"This column has high number of categoricals to print so it will be very slow\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8879c5ee-ba1b-4010-bde2-cac6f608bf55","_cell_guid":"26fee99e-9e0f-4266-85dc-63008dea3d21","trusted":true},"cell_type":"code","source":"# addr: address addr1, addr2- categorical variable\na4_dims = (20, 20)\nfig, axs = plt.subplots(2,1, figsize=a4_dims, squeeze=False)\naddr_list=['addr1','addr2']\nco=0\n\nfor r in range(0,2):\n    for c in range(0, 1): \n        feature_count=train[addr_list[co]].value_counts().reset_index()\n       # feature_count= feature_count.sort_values([addr_list[co]])\n        feature_count=feature_count.iloc[:40,]\n        #print(len(feature_count.iloc[:40,]))\n        \n        ax=sns.barplot(x='index',y=addr_list[co],data=feature_count,errwidth=12,capsize=10,ax=axs[r][c])\n        ax.set_xlabel(addr_list[co])\n        ax.set_ylabel('Number of Occurrences')\n        co+=1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38035ccb-db95-4b41-b92e-6979b838e3b6","_cell_guid":"389e675b-0fc9-4759-9184-fe36699b12b7","trusted":true},"cell_type":"code","source":"# dist: distance is numerical we can analyze it later\n#P_ and (R__) emaildomain: purchaser and recipient email domain its categorical\n#print(train['P_emaildomain'].value_counts())\na4_dims = (20, 20)\nfig, axs = plt.subplots(2,1, figsize=a4_dims, squeeze=False)\naddr_list=['P_emaildomain','R_emaildomain']\nco=0\n\nfor r in range(0,2):\n    for c in range(0, 1): \n        feature_count=train[addr_list[co]].value_counts().reset_index()\n       # feature_count= feature_count.sort_values([addr_list[co]])\n        feature_count=feature_count.iloc[:40,]\n        #print(len(feature_count.iloc[:40,]))\n        \n        ax=sns.barplot(x='index',y=addr_list[co],data=feature_count,errwidth=12,capsize=10,ax=axs[r][c])\n        ax.set_xlabel(addr_list[co])\n        ax.set_ylabel('Number of Occurrences')\n        ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n        co+=1\n\n#in both this case the domain name like .eu and .in are different but they should be same try to preprocess it \n# we can split the given one by '.' and take the fisrt part for the correct mail names","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"36fd1c0d-23df-4be5-b018-27e781f7e05a","_cell_guid":"e87a126f-9c16-4878-b6a8-88491cb66f61","trusted":true},"cell_type":"code","source":"m_list=['M1','M2','M3','M4','M5','M6','M7','M8','M9']\n# check they are caetgorical or not\nfor col in m_list:\n    print(\"For the \" + str(col))\n    print(train[col].value_counts())\n# expect M4 all other are T/F","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"084a6685-5b8d-4a80-bb36-531fb387c231","_cell_guid":"bbba6d96-757f-40a1-93bc-4f2d791b24b8","trusted":true},"cell_type":"code","source":"# M1 - M9 categorical variable that need to analyze\n# the values are match, such as names on card and address, etc.\na4_dims = (20, 20)\nfig, axs = plt.subplots(9,1, figsize=a4_dims, squeeze=False)\nco=0\nm_list=['M1','M2','M3','M4','M5','M6','M7','M8','M9']\nfor r in range(0,9):\n    for c in range(0, 1): \n        feature_count=train[m_list[co]].value_counts().reset_index()\n       # feature_count= feature_count.sort_values([addr_list[co]])\n        feature_count=feature_count.iloc[:40,]\n        #print(len(feature_count.iloc[:40,]))\n        \n        ax=sns.barplot(x='index',y=m_list[co],data=feature_count,errwidth=12,capsize=100,ax=axs[r][c])\n       \n        ax.set_xlabel(m_list[co])\n        ax.set_ylabel('Number of Occurrences')\n        ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n        co+=1\nplt.subplots_adjust(hspace = 0.2)\nplt.tight_layout()\n# I think no need of preprocess for this M1-M9 set of features.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15952b95-0cc3-4327-8c2d-16bc0234b539","_cell_guid":"269acf1e-cdcf-4e59-9043-395db86f909d","trusted":true},"cell_type":"code","source":"#DeviceType\n#train['DeviceType'].value_counts() # only two types # we can check where we get more isFraud \nsns.countplot(x='DeviceType',hue='isFraud',data=train)\n# we have more isFraud  in desktop","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"74293873-ca05-4dc7-831a-99051b751f3f","_cell_guid":"2be56c02-6c36-4ee2-94e3-aa06b7043115","trusted":true},"cell_type":"code","source":"#Deviceinfo\nfeature_count=train['DeviceInfo'].value_counts().reset_index()\nfeature_count.sort_values('DeviceInfo')\nfeature_count=feature_count.iloc[:40,]\n#print(feature_count)\nax=sns.barplot(x=\"index\", y=\"DeviceInfo\", data=feature_count,errwidth=12,capsize=100)\nax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n# we have more isFraud  in desktop","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4e97797-3d2e-40e3-869c-1b15fec67b00","_cell_guid":"1cd5fdbd-dd87-45d8-b8dd-8bb56a81688f","trusted":true},"cell_type":"code","source":"#id12 - id38 we need to analyze this part its categorical variable\nid_list=[]\nfor i in range(12,39):\n    id_list.append('id_'+str(i))\nprint(id_list)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e5ac3f3-1e21-43b8-8192-268f51e048d7","_cell_guid":"08cd400d-17a9-46dd-98c9-63d900285624","trusted":true},"cell_type":"code","source":"#iterate the id_list and visualize it\na4_dims = (20, 20)\nfig, axs = plt.subplots(5,1, figsize=a4_dims, squeeze=False)\n\nco=0\n\nfor r in range(0,5):\n    for c in range(0, 1): \n        feature_count=train[id_list[co]].value_counts().reset_index()\n       # feature_count= feature_count.sort_values([addr_list[co]])\n        feature_count=feature_count.iloc[:40,]\n        #print(len(feature_count.iloc[:40,]))\n        \n        ax=sns.barplot(x='index',y=id_list[co],data=feature_count,errwidth=12,capsize=100,ax=axs[r][c])\n       \n        ax.set_xlabel(id_list[co])\n        ax.set_ylabel('Number of Occurrences')\n        ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n        co+=1\n\nplt.tight_layout()\n#id_12 - found/not_found\n#id_13- many fields are there\n#id_14- many fields are there\n#id_15- found/new/unknown\n#id_16-found/not_found","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f727e76a-85ad-445b-84be-d1c3d19093a8","_cell_guid":"813452b2-dbfc-4ed1-a712-c4935ff63e83","trusted":true},"cell_type":"code","source":"#next 5 features\na4_dims = (20, 20)\nfig, axs = plt.subplots(5,1, figsize=a4_dims, squeeze=False)\n\nco=5\n\nfor r in range(0,5):\n    for c in range(0, 1): \n        feature_count=train[id_list[co]].value_counts().reset_index()\n       # feature_count= feature_count.sort_values([addr_list[co]])\n        feature_count=feature_count.iloc[:40,]\n        #print(len(feature_count.iloc[:40,]))\n        \n        ax=sns.barplot(x='index',y=id_list[co],data=feature_count,errwidth=12,capsize=100,ax=axs[r][c])\n       \n        ax.set_xlabel(id_list[co])\n        ax.set_ylabel('Number of Occurrences')\n        ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n        co+=1\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3eab2f37-1850-4ee5-9f3d-9cd6dda532d1","_cell_guid":"d8ce9ce5-cb81-4833-b0a0-49fc545df5ca","trusted":true},"cell_type":"code","source":"#next 5 features\na4_dims = (20, 20)\nfig, axs = plt.subplots(5,1, figsize=a4_dims, squeeze=False)\n\nco=10\n\nfor r in range(0,5):\n    for c in range(0, 1): \n        feature_count=train[id_list[co]].value_counts().reset_index()\n       # feature_count= feature_count.sort_values([addr_list[co]])\n        feature_count=feature_count.iloc[:40,]\n        #print(len(feature_count.iloc[:40,]))\n        \n        ax=sns.barplot(x='index',y=id_list[co],data=feature_count,errwidth=12,capsize=100,ax=axs[r][c])\n       \n        ax.set_xlabel(id_list[co])\n        ax.set_ylabel('Number of Occurrences')\n        ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n        co+=1\n\nplt.tight_layout()\n#id_23-proxy-transparent/anonymous/hidden","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b1d04cda-15ff-4748-a5b1-f1448801dcce","_cell_guid":"d7851744-c3b8-45e2-86fe-b6a69db7b952","trusted":true},"cell_type":"code","source":"#next 5 features\na4_dims = (20, 20)\nfig, axs = plt.subplots(5,1, figsize=a4_dims, squeeze=False)\n\nco=15\n\nfor r in range(0,5):\n    for c in range(0, 1): \n        feature_count=train[id_list[co]].value_counts().reset_index()\n       # feature_count= feature_count.sort_values([addr_list[co]])\n        feature_count=feature_count.iloc[:40,]\n        #print(len(feature_count.iloc[:40,]))\n        \n        ax=sns.barplot(x='index',y=id_list[co],data=feature_count,errwidth=12,capsize=100,ax=axs[r][c])\n       \n        ax.set_xlabel(id_list[co])\n        ax.set_ylabel('Number of Occurrences')\n        ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n        co+=1\n\nplt.tight_layout()\n#id_27/_29 -found/not-found\n#id_28-found/new","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59d0fd91-bbb4-4c59-8ca4-178cf6f20642","_cell_guid":"bf1c9d7a-31bd-44d0-8168-345192251ec2","trusted":true},"cell_type":"code","source":"#next 5 features\n\n#id_34- match-0,1,2\n#id_35/_36-True/False","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24f29690-a7f3-46ef-8692-922e77d0edc0","_cell_guid":"fded77d0-2240-4278-9267-b077d39dcd82","trusted":true},"cell_type":"code","source":"#we need to analyze the some id columns and numeric columns are left now\n# try to create new feature with the help of the EDA\n# and then try to reduce the dimension by dropping it \n# encode the category data\n# missing value treatment for numeric and category columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# you can check the some of the important parameters here\nsomeFeature_list=['id_36','id_35','id_34','id_28','id_29','id_12','id_15','id_16']\na4_dims = (20, 20)\nco=0\nax=sns.countplot(x=someFeature_list[co],hue='isFraud',data=train)\nax.set_xlabel(someFeature_list[co])\nax.set_ylabel('Number of Occurrences')\nax.set_xticklabels(ax.get_xticklabels(), rotation=45)\nco+=1\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax=sns.countplot(x=someFeature_list[co],hue='isFraud',data=train)\nax.set_xlabel(someFeature_list[co])\nax.set_ylabel('Number of Occurrences')\nax.set_xticklabels(ax.get_xticklabels(), rotation=45)\nco+=1\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax=sns.countplot(x=someFeature_list[co],hue='isFraud',data=train)\nax.set_xlabel(someFeature_list[co])\nax.set_ylabel('Number of Occurrences')\nax.set_xticklabels(ax.get_xticklabels(), rotation=45)\nco+=1\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax=sns.countplot(x=someFeature_list[co],hue='isFraud',data=train)\nax.set_xlabel(someFeature_list[co])\nax.set_ylabel('Number of Occurrences')\nax.set_xticklabels(ax.get_xticklabels(), rotation=45)\nco+=1\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax=sns.countplot(x=someFeature_list[co],hue='isFraud',data=train)\nax.set_xlabel(someFeature_list[co])\nax.set_ylabel('Number of Occurrences')\nax.set_xticklabels(ax.get_xticklabels(), rotation=45)\nco+=1\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax=sns.countplot(x=someFeature_list[co],hue='isFraud',data=train)\nax.set_xlabel(someFeature_list[co])\nax.set_ylabel('Number of Occurrences')\nax.set_xticklabels(ax.get_xticklabels(), rotation=45)\nco+=1\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax=sns.countplot(x=someFeature_list[co],hue='isFraud',data=train)\nax.set_xlabel(someFeature_list[co])\nax.set_ylabel('Number of Occurrences')\nax.set_xticklabels(ax.get_xticklabels(), rotation=45)\nco+=1\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax=sns.countplot(x=someFeature_list[co],hue='isFraud',data=train)\nax.set_xlabel(someFeature_list[co])\nax.set_ylabel('Number of Occurrences')\nax.set_xticklabels(ax.get_xticklabels(), rotation=45)\nco+=1\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a980ac92-acd6-42b3-b416-12df9f8b71a6","_cell_guid":"4796b2d7-5ed5-475b-a9c5-a8e29757160f","trusted":true},"cell_type":"code","source":"print(addr_list)\n# when you do one hot encoding please add both test and train both may have different one\nfor col in addr_list:\n    train[col]=(train[col].str.split(\".\",expand=True)[0])\n    test[col]=(test[col].str.split(\".\",expand=True)[0])\n# now we are done with mails so we do some feature engineering","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9da21bf1-4165-429c-889b-b5746bf757b4","_cell_guid":"c4838809-dfe6-4c26-8816-95c105e27990","trusted":true},"cell_type":"code","source":"#Feature Engineering \n# first we can try to use card features\nfor col in ['card1','card2','card3','card4','card5','card6']:\n    # we are just taking a mean for each group and diving it with the each group Transaction amount to get more information\n    # and also std for each group \n    train['Transactionamt_mean_'+str(col)]=(train['TransactionAmt']/train.groupby(col)['TransactionAmt'].transform('mean'))\n    train['Transactionamt_std_'+str(col)]=(train['TransactionAmt']/train.groupby(col)['TransactionAmt'].transform('std'))\n    test['Transactionamt_mean_'+str(col)]=(test['TransactionAmt']/test.groupby(col)['TransactionAmt'].transform('mean'))\n    test['Transactionamt_std_'+str(col)]=(test['TransactionAmt']/test.groupby(col)['TransactionAmt'].transform('std'))\n#feature Engineering only for Cards alone\n# we also need to check device info and device type,id_30,id_31","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in train.columns:\n    if col not in test.columns:\n        print(\"missed the column \",col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c=0\nfor col in ['ProductCD', 'P_emaildomain','R_emaildomain','DeviceType','DeviceInfo','id_15','id_23','id_30','id_31','id_34']:\n    if col in train.columns:\n        print(\"presented\")\n        #print(col)\n        c+=1\n    else:\n        print(\"missed\",col)\nprint(c)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's try to do feature Enginnering based on ProductCD because it has only 4 levels\n# and also for P_emaildomain ,R_emaildomain,DeviceType\n#DeviceInfo,id_15,id_23,id_30,id_31,id_34\n#you guys can do according to your understanding\nfor col in ['ProductCD', 'P_emaildomain','R_emaildomain','DeviceType','DeviceInfo','id_15','id_23','id_30','id_31','id_34']:\n    train['Transactionamt_mean_'+str(col)]=(train['TransactionAmt']/train.groupby(col)['TransactionAmt'].transform('mean'))\n    train['Transactionamt_std_'+str(col)]=(train['TransactionAmt']/train.groupby(col)['TransactionAmt'].transform('std'))\n    test['Transactionamt_mean_'+str(col)]=(test['TransactionAmt']/test.groupby(col)['TransactionAmt'].transform('mean'))\n    test['Transactionamt_std_'+str(col)]=(test['TransactionAmt']/test.groupby(col)['TransactionAmt'].transform('std'))\n# there will be lot of NAN in our columns ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now we can preprocess our data\nprint(\"Total number of columns after Feture Engineering:\",len(train.columns)) #466\n# now we want to drop the unwanted columns\nprint(total_drop_cols)\n#train=train.drop(drop)\nfor col in total_drop_cols:\n    del train[col]\n    del test[col]\nprint(\"Final number of columns after Feature Engineering:\",len(train.columns)) # 384\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now we can do label encoding for categorical variable\n# we can do one hot encoding but it will increase our dimension so its problem\n# so we can try label encoding or any other encoding like frequency encoding .etc\n# i am going to try label encoding\n\nfrom sklearn  import preprocessing\nfor col in train.columns:\n    if train[col].dtype=='object' :\n      #  print(\"label encoding\",col)\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(train[col].values) + list(test[col].values))\n        train[col] =lbl.transform(list(train[col].values))\n        test[col]=lbl.transform(list(test[col].values))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_inf_nan(df):\n    return df.replace([np.inf, -np.inf], np.nan)   \n\n# Cleaning infinite values to NaN\ntrain = clean_inf_nan(train)\ntest = clean_inf_nan(test ) # replace all nan,inf,-inf to nan so it will be easy to replace\nfor i in train.columns:\n    train[i].fillna(train[i].median(),inplace=True) # fill with median because mean may be affect by outliers.\n#X.isna().sum().sum()\nfor i in test.columns:\n    test[i].fillna(test[i].median(),inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(\"Number of Na's in train\",train.isna().sum().sum())\nprint(\"Number of Na's in test\",test.isna().sum().sum())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now we an split the data and train our model\n\nX = train.drop(['isFraud', 'TransactionDT', 'TransactionID'], axis=1)\ny = train['isFraud']\n# take the train data and split it us train and test because we are not able to test with test data \n\n#X_test = test.sort_values('TransactionDT').drop(['TransactionDT', 'TransactionID'], axis=1)\n#X_test = test.drop(['TransactionDT', 'TransactionID'], axis=1)\n#del train\n#test = test[['TransactionID']]\n# TransactionDT is skewed so try to apply the transformation for better results\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#take 75% for training and 25% for testing the dataset\n#from sklearn.model_selection import train_test_split\n#X_train, X_test, y_train, y_test = train_test_split(\n #  X, y, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pct_index=442905","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test = X[:train_pct_index], X[train_pct_index:]\ny_train, y_test = y[:train_pct_index], y[train_pct_index:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape #147,635‬ - 25% 442,905‬- 75%","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 5100 - isfraud\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n#train and test split\n#from sklearn.model_selection import train_test_split\n#xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport xgboost as xgb\nfrom sklearn.metrics import roc_auc_score\n#print(submission.head(5))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can try to fit the base model \n# we can try logistic regression\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom catboost import CatBoostClassifier \n#model_dt =KNeighborsClassifier(n_neighbors=3)\n#RandomForestClassifier(n_estimators=1000,max_depth=7, random_state=0)\n\n#model_dt = model_dt.fit(X_train, y_train)\n\nmodel_dt=CatBoostClassifier (iterations=100, depth=7, learning_rate=0.1)\nmodel_dt.fit(X_train,y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_res=model_dt.predict(X_test)\n\ntest_proab=model_dt.predict_proba(X_test)\ntrain_res=model_dt.predict(X_train)\n\ntrain_proab=model_dt.predict_proba(X_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nc_m=confusion_matrix(y_test,test_res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test, test_res) # 96% - accuracy - lr #0.9681715040471432- randomforest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c_m","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a=list(y_test)\na.count(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sum(test_proab[:,1] > 0.80)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_test=pd.DataFrame()\nsubmission_test['TransactionID']=train.iloc[train_pct_index:train.shape[0],0]\nsubmission_test['isFraud_test'] = 0\nsubmission_test['proab_1_test']=0.0\nsubmission_test['result_test']=0\n\nsubmission_train=pd.DataFrame()\nsubmission_train['TransactionID']=train.iloc[0:train_pct_index,0]\nsubmission_train['isFraud_train'] = 0\nsubmission_train['proab_1_train']=0.0\nsubmission_train['result_train']=0\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsubmission_test['isFraud_test']=test_res\nsubmission_test['proab_1_test']=test_proab[:,1] #proab\nsubmission_test['result_test']=y_test\n\nsubmission_train['isFraud_train']=train_res\nsubmission_train['proab_1_train']=train_proab[:,1]\nsubmission_train['result_train']=y_train\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_test.to_csv(\"submission_cb_test.csv\")\nsubmission_train.to_csv(\"submission_cb_train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_train.head()","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}