{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nimport xgboost as xgb\nfrom sklearn.model_selection import KFold , GridSearchCV\nfrom sklearn.metrics import roc_auc_score \nimport gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_transaction = pd.read_csv('../input/train_transaction.csv', index_col='TransactionID' ,nrows = 200000)\ntest_transaction = pd.read_csv('../input/test_transaction.csv', index_col='TransactionID' )\n\ntrain_identity = pd.read_csv('../input/train_identity.csv', index_col='TransactionID')\ntest_identity = pd.read_csv('../input/test_identity.csv', index_col='TransactionID')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n\nprint(train.shape)\nprint(test.shape)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nY_train = train['isFraud'].copy()\nX_train = train.drop('isFraud' , axis = 1)\nX_test = test.copy()\ndel train_transaction, train_identity, test_transaction, test_identity\ndel train , test\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in X_train.columns:\n    if X_train[i].dtype == 'object' or X_test[i].dtype == 'object':\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(X_train[i].values) + list(X_test[i].values))\n        X_train[i] = lbl.transform(list(X_train[i].values))\n        X_test[i] = lbl.transform(list(X_test[i].values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in X_train.columns:\n    if X_train[i].dtype == 'object' or X_test[i].dtype == 'object':\n        print(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \nX_train = reduce_mem_usage(X_train)\nX_test = reduce_mem_usage(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# null_data = pd.DataFrame(X_train.isnull().sum()/X_train.shape[0]*100)\n# null_data = pd.DataFrame()\n# null_data = pd.concat([pd.DataFrame(X_train.isnull().sum()/X_train.shape[0]*100 ,columns=['train']) ,pd.DataFrame(X_test.isnull().sum()/X_test.shape[0]*100,columns=['test']) ] ,axis = 1).reset_index()\n# null_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# columns_drop = null_data.sort_values(by = 'train'  , ascending = 0).head(100)['index'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_corr = X_train.corr().abs()*100\ntrain_corr = train_corr.where(np.triu(np.ones(train_corr.shape)).astype(np.bool))\ntrain_corr.values[[np.arange(train_corr.shape[0])]*2] = np.nan\nprint(train_corr.shape)\n# display(train_corr.tail(20))\ncounter =0\ncolumns_drop =[]\ntrain_corr_matrix = train_corr.values\nfor i in range(1 , train_corr.shape[0] ,1 ):\n    for j in range(i , train_corr.shape[0] , 1):\n        if train_corr_matrix[i][j] >= 98:\n            counter+=1\n            columns_drop.append(train_corr.columns[j])\n            if counter%20 ==0:\n                print('Comman Columns pair reached ... ' , counter)\nprint(' Total Common Pair Found .... ',counter)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# columns_drop = list(set(columns_drop))\n# X_train.drop(columns = columns_drop , inplace = True)\n# X_test.drop(columns = columns_drop , inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv('../input/sample_submission.csv', index_col='TransactionID')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"%%time\nparameters = {'n_estimators': [500], \n              'max_depth':[4] ,\n              'learning_rate' :[ 0.005 , 0.01] , \n              'subsample' : [0.2 , 0.3],\n              'colsample_bytree' : [0.2 , 0.5 , 0.9]\n             }\nclf = xgb.XGBClassifier()\ngrid_search = GridSearchCV(estimator=clf, param_grid=parameters, cv=3, n_jobs=-1 , verbose = 6)\ngrid_search.fit(X_train , Y_train)\nprint(\"Best score: %0.5f\" % grid_search.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters=grid_search.best_estimator_.get_params()\nfor param_name in sorted(parameters.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"},{"metadata":{},"cell_type":"markdown","source":"Best score: 0.97750<br>\nBest parameters set:\n* \tlearning_rate: 0.005<br>\n* \tmax_depth: 4<br>\n* \tn_estimators: 500<br>\n* \tsubsample: 0.2<br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_train.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 5\ny_pred = np.zeros(sample_submission.shape[0])\ny_oof = np.zeros(X_train.shape[0])\nkf = KFold(n_splits = EPOCHS , shuffle = True)\nfor x_train_index , x_val_index in kf.split(X_train , Y_train):\n    clf = xgb.XGBClassifier(\n        n_estimators=500,\n        max_depth=4,\n        learning_rate=0.005,\n        subsample=0.2,\n        colsample_bytree = 0.2\n    )\n    x_tr , x_val = X_train.iloc[x_train_index , :] , X_train.iloc[x_val_index,:]\n    y_tr , y_val = Y_train.iloc[x_train_index] , Y_train.iloc[x_val_index]\n    clf.fit(x_tr,y_tr)\n    y_pred_train = clf.predict_proba(x_val)[:,1]\n    y_oof[x_val_index] = y_pred_train\n    print('ROC AUC {}'.format(roc_auc_score(y_val, y_pred_train)))\n    y_pred+= clf.predict_proba(X_test)[:,1] / EPOCHS\n\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = X_train.columns\nimportances = clf.feature_importances_\ndataframe = pd.DataFrame({'col':col , 'importance':importances})\ndataframe = dataframe.sort_values(by=['importance'] ,ascending = False)\ndataframe['importance_ratio'] = dataframe['importance']/dataframe['importance'].max()*100\ndataframe = dataframe.head(35)\ndataframe['col'] = dataframe['col'].apply(lambda x:  str(x))\nplt.figure(figsize=(18,12))\nplt.barh(dataframe['col'], dataframe['importance_ratio'], color='orange' , align='center' ,linewidth =30 )\nplt.yticks(rotation=30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['isFraud'] = y_pred\nsample_submission.to_csv('second_simple_xgboost_Fraud_Baseline.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ROC AUC 0.7886114967821933\n* ROC AUC 0.8359518198481278\n* ROC AUC 0.8312436224489796\n* ROC AUC 0.8269948717948717\n* ROC AUC 0.8615600904653375\n* Result for each new initialisation \n* ROC AUC 0.7696746809314766\n* ROC AUC 0.8463722611468055\n* ROC AUC 0.8169901274903227\n* ROC AUC 0.871218112244898\n* ROC AUC 0.8578023148656989\n* for one model and training it on new data on earlier data\n"},{"metadata":{},"cell_type":"markdown","source":"* So Bascially 93.9 % of data final test set is not fraud "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}