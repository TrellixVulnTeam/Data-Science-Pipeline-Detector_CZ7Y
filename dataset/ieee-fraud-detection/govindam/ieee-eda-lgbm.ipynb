{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# General imports\nimport numpy as np\nimport pandas as pd\nimport os, warnings, datetime, math\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\n\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"########################### Helpers\n#################################################################################\n## -------------------\n## Memory Reducer\n# :df pandas dataframe to reduce size             # type: pd.DataFrame()\n# :verbose                                        # type: bool\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n## -------------------","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### DATA LOAD\n#################################################################################\nprint('Load Data')\ntrain_df = pd.read_csv('../input/train_transaction.csv')\ntest_df = pd.read_csv('../input/test_transaction.csv')\ntest_df['isFraud'] = 0\n\ntrain_identity = pd.read_csv('../input/train_identity.csv')\ntest_identity = pd.read_csv('../input/test_identity.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Base check\n#################################################################################\n\nfor df in [train_df, test_df, train_identity, test_identity]:\n    original = df.copy()\n    df = reduce_mem_usage(df)\n\n    for col in list(df):\n        if df[col].dtype!='O':\n            if (df[col]-original[col]).sum()!=0:\n                df[col] = original[col]\n                #print('Bad transformation', col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_identity.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Understanding data types and missing values present in the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_df = pd.concat([train_df, test_df])\npercent_missing = (temp_df.isnull().mean() * 100) \nprint(percent_missing.sort_values(ascending=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Removing columns greater than a certain threshold\n#thresh = 90\n#reduced_col = (percent_missing[percent_missing<thresh])\n#reduced_col.sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# numnerical columns\nnum_cols = train_df._get_numeric_data().columns\nprint('Total number of numerical columns are ', len(num_cols))\nprint(num_cols)\nprint('-------------------------------------------------------------------------------')\n# categorical columns\ncat_cols = set(train_df.columns) - set(num_cols)\nprint('Total number of categorical columns are', len(cat_cols))\nprint(cat_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plotting different graphs"},{"metadata":{},"cell_type":"markdown","source":"### Plotting Functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting categorical features\ndef make_categorical_plots(Vs):\n    col = 4\n    row = len(Vs)//col+1\n    fig = plt.figure(figsize=(20,row*5))\n    for i,v in enumerate(Vs):\n        ax = plt.subplot(row,col,i+1)\n        g1 = sns.barplot(x=v, y=\"isFraud\", data=train_df, ax = ax)\n        g1.legend()\n        plt.title(v+\" barplot w.r.t target\", fontsize=16)\n        g1.set_xlabel(v+ \" values\", fontsize=16)\n        g1.set_ylabel(\"Probability\", fontsize=16)\n        #plt.title('Column: '+ str(v), fontsize=16)\n        plt.subplots_adjust(hspace = 0.5)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting desity plot for numerical features\ndef make_density_plots(Vs):\n    col = 2\n    row = len(Vs)//col+1\n    fig = plt.figure(figsize=(20,row*5))\n    for i,v in enumerate(Vs):\n        ax = plt.subplot(row,col,i+1)\n        g1 = sns.distplot(train_df[train_df['isFraud'] == 1][v].dropna(), label='Fraud',\n                          ax=ax )\n        g1 = sns.distplot(train_df[train_df['isFraud'] == 0][v].dropna(), label='NoFraud',\n                              ax=ax)\n        g1.legend()\n        plt.title(v+\" values distribution w.r.t target\", fontsize=16)\n        g1.set_xlabel(v+ \" values\", fontsize=16)\n        g1.set_ylabel(\"Probability\", fontsize=16)\n        #plt.title('Column: '+ str(v), fontsize=16)\n        plt.subplots_adjust(hspace = 0.5)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to plot histogram\ndef make_histogram_plots(Vs):\n    col = 4\n    row = len(Vs)//4+1\n    plt.figure(figsize=(20,row*5))\n    for i,v in enumerate(Vs):\n        #print(v)\n        plt.subplot(row,col,i+1)\n        plt.title('Column: '+ str(v))\n        h = plt.hist(train_df[v],bins=100)\n        if len(h[0])>1: plt.ylim((0,np.sort(h[0])[-2]))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plotting 'card' graph"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_col = train_df.columns \n# Finding all columns starting with 'card'\nr = re.compile(\"^card\")\ncard_list = list(filter(r.match, total_col)) # Read Note\nprint(card_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above card_list, card4 and card6 are categorical columns, so we will plot only for ['card1', 'card2', 'card3','card5']\n### Plotting density plots\nPlotting density plot in based on the fraud vs non-fraud"},{"metadata":{"trusted":true},"cell_type":"code","source":"make_density_plots(['card1', 'card2', 'card3','card5'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, the variance for card 1 and card 2 is high and it is possible that they contribute high in the model building"},{"metadata":{},"cell_type":"markdown","source":"### Plotting addr graph"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding all columns starting with 'addr'\nr = re.compile(\"^addr\")\naddr_list = list(filter(r.match, total_col)) # Read Note\nprint(addr_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"make_density_plots(addr_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, addr1 has high change to contribute to the model because high variance"},{"metadata":{},"cell_type":"markdown","source":"### Plotting V data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding all columns starting with 'V'\nr = re.compile(\"^V\")\nV_list = list(filter(r.match, total_col)) # Read Note\nprint(V_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since there are too many columns, let's plot few columns to understand if we can get any significance interpretation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting first 10 values\nmake_density_plots(['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, most of the graphs tends to have same distribution. So, first let's find correlation and plot the graph for the variables which are not highly correlated. "},{"metadata":{},"cell_type":"markdown","source":"### Finding correlation for columns starting with V"},{"metadata":{"trusted":true},"cell_type":"code","source":"# col_corr = set() # Set of all the names of deleted columns\ncorr_matrix = train_df[V_list].corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Removing the columns which are highly correlated"},{"metadata":{"trusted":true},"cell_type":"code","source":"####### Removing multi-correlated columns and retaining only that column which has high unique value, \n#### This conveys that it has high variance, and may be has high impact on the model\n#### Here, first we are making correlation matrix and then finding the correlations of columns with other columns\n#### which are highly correlated. Then removing all other columns except for the columns which has highest unique value. \ncol_corr = set()\nthreshold = 0.80\nfor i in range(len(corr_matrix.columns)):\n    high_corr = []\n    high_corr.append(corr_matrix.columns[i])\n    for j in range(i):\n        mx = 0\n        vx = corr_matrix.columns[i]\n        #print('vx', vx)\n        if (corr_matrix.iloc[i, j] >= threshold) and (corr_matrix.columns[j] not in col_corr):\n            high_corr.append(corr_matrix.columns[j])\n            \n    if len(high_corr)>1:\n        #print('high_corr',high_corr )\n        for col in high_corr:\n            n = train_df[col].nunique()\n            if n>mx:\n                mx = n\n                vx = col\n        #print('vx', vx)        \n        high_corr.remove(str(vx))\n        col_corr.update(set(high_corr))\n        #print('col_corr', col_corr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(list(col_corr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# V_list_ = [x for x in V_list if x not in col_corr]\n# print(\"Length of new V_list is\", len(V_list_))\n# print(V_list_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# with threshold value of .80\n# This is the output after finding the features which are highly correlated. \n# You can find how to get these columns 3 cells above\nV_list_ = ['V1', 'V3', 'V5', 'V7', 'V9', 'V13', 'V20', 'V24', 'V26', 'V28', 'V30', 'V36', 'V38', 'V45',\n           'V47', 'V54', 'V55', 'V56', 'V62', 'V67', 'V76', 'V78', 'V83', 'V87', 'V88', 'V107', 'V109',\n           'V110', 'V113', 'V115', 'V116', 'V119', 'V121', 'V122', 'V125', 'V138', 'V140', 'V142', \n           'V147', 'V158', 'V160', 'V162', 'V166', 'V169', 'V174', 'V185', 'V198', 'V201', 'V209', \n           'V210', 'V220', 'V221', 'V223', 'V239', 'V240', 'V241', 'V251', 'V252', 'V260', 'V262',\n           'V267', 'V271', 'V281', 'V282', 'V283', 'V286', 'V289', 'V291', 'V301', 'V303', 'V305', \n           'V307', 'V310', 'V325', 'V339']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's plot this new V_list_ "},{"metadata":{"trusted":true},"cell_type":"code","source":"#make_density_plots(['V_list_'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting values\nmake_density_plots(['TransactionAmt'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not much to say about the result from the graph above "},{"metadata":{},"cell_type":"markdown","source":"### Plotting D data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding all columns starting with 'D'\nr = re.compile(\"^D\")\nD_list = list(filter(r.match, total_col)) # Read Note\nprint(D_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"make_density_plots(D_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Column D9 seems to be important"},{"metadata":{},"cell_type":"markdown","source":"## Plotting M data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding all columns starting with 'M'\nr = re.compile(\"^M\")\nM_list = list(filter(r.match, total_col)) # Read Note\nprint(M_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"make_categorical_plots(['M1','M2','M3','M4','M5','M6','M7','M8','M9'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plotting addr data"},{"metadata":{"trusted":true},"cell_type":"code","source":"make_categorical_plots(['addr1', 'addr2'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Looking into train_identity.csv"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_identity.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_identity.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_identity_df = pd.concat([train_identity, test_identity])\npercent_missing = (temp_identity_df.isnull().mean() * 100) \nprint(percent_missing.sort_values(ascending=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So many features have missing values close to 96%."},{"metadata":{"trusted":true},"cell_type":"code","source":"# numnerical columns\nnum_identity_cols = train_identity._get_numeric_data().columns\nprint('Total number of numerical columns are ', len(num_identity_cols))\nprint(num_identity_cols)\nprint('-------------------------------------------------------------------------------')\n# categorical columns\ncat_identity_cols = set(train_identity.columns) - set(num_identity_cols)\nprint('Total number of categorical columns are', len(cat_identity_cols))\nprint(cat_identity_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's join the train_identity data with the train_df and then we will plot the numerical and the categorical columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['TransactionAmt'] / train_df.groupby(['card1'])['TransactionAmt'].transform('mean')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the groups\n#train_df.groupby(['card1']).groups","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=\"ProductCD\", data=train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=\"M4\", data=train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['card1'].nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pre-processing"},{"metadata":{},"cell_type":"markdown","source":"Since we have indentified some of the columns through plotting which were important, such as: ['card1','card2','M4','D9','addr1','addr2','dist1','dist2', 'P_emaildomain', 'R_emaildomain']. Most of these are categorical featues. Also, we will consider Transaction Data to make various featues. Let's create some new features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Freq encoding\ni_cols = ['card1','card2','M4','D9',\n          'addr1','addr2','dist1','dist2',\n          'P_emaildomain', 'R_emaildomain'\n         ]\n\nfor col in i_cols:\n    temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n    valid_card = temp_df[col].value_counts().to_dict()   \n    train_df[col+'_fq_enc'] = train_df[col].map(valid_card)\n    test_df[col+'_fq_enc']  = test_df[col].map(valid_card)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(train_df['card1'].astype(str)+'_'+train_df['card2'].astype(str)).nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TARGET = 'isFraud'\nfor col in ['card1','card2','addr1','addr2','M4']:\n    temp_dict = train_df.groupby([col])[TARGET].agg(['mean']).reset_index().rename(\n                                                        columns={'mean': col+'_target_mean'})\n    temp_dict.index = temp_dict[col].values\n    temp_dict = temp_dict[col+'_target_mean'].to_dict()\n\n    train_df[col+'_target_mean'] = train_df[col].map(temp_dict)\n    test_df[col+'_target_mean']  = test_df[col].map(temp_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i_cols = V_list_\n\nfor df in [train_df, test_df]:\n    df['V_sum'] = df[i_cols].sum(axis=1).astype(np.int8)\n    df['V_na'] = df[i_cols].isna().sum(axis=1).astype(np.int8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Binary encoding for M columns\ni_cols = ['M1','M2','M3','M5','M6','M7','M8','M9']\n\nfor df in [train_df, test_df]:\n    df['M_sum'] = df[i_cols].sum(axis=1).astype(np.int8)\n    df['M_na'] = df[i_cols].isna().sum(axis=1).astype(np.int8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's add some kind of client uID based on cardID ad addr columns\n# The value will be very specific for each client so we need to remove it\n# from final feature. But we can use it for aggregations.\ntrain_df['uid'] = train_df['card1'].astype(str)+'_'+train_df['card2'].astype(str)\ntest_df['uid'] = test_df['card1'].astype(str)+'_'+test_df['card2'].astype(str)\n\ntrain_df['uid2'] = train_df['uid'].astype(str)+'_'+train_df['addr1'].astype(str)+'_'+train_df['addr2'].astype(str)\ntest_df['uid2'] = test_df['uid'].astype(str)+'_'+test_df['addr1'].astype(str)+'_'+test_df['addr2'].astype(str)\n\ntrain_df['uid3'] = train_df['uid2'].astype(str)+'_'+train_df['P_emaildomain'].astype(str)+'_'+train_df['R_emaildomain'].astype(str)\ntest_df['uid3'] = test_df['uid2'].astype(str)+'_'+test_df['P_emaildomain'].astype(str)+'_'+test_df['R_emaildomain'].astype(str)\n\n# Check if the Transaction Amount is common or not (we can use freq encoding here)\n# In our dialog with a model we are telling to trust or not to these values   \ntrain_df['TransactionAmt_check'] = np.where(train_df['TransactionAmt'].isin(test_df['TransactionAmt']), 1, 0)\ntest_df['TransactionAmt_check']  = np.where(test_df['TransactionAmt'].isin(train_df['TransactionAmt']), 1, 0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i_cols = ['card1','card2','uid','uid2','uid3']\n\nfor col in i_cols:\n    for agg_type in ['mean','std']:\n        new_col_name = col+'_TransactionAmt_'+agg_type\n        temp_df = pd.concat([train_df[[col, 'TransactionAmt']], test_df[[col,'TransactionAmt']]])\n        #temp_df['TransactionAmt'] = temp_df['TransactionAmt'].astype(int)\n        temp_df = temp_df.groupby([col])['TransactionAmt'].agg([agg_type]).reset_index().rename(\n                                                columns={agg_type: new_col_name})\n        \n        temp_df.index = list(temp_df[col])\n        temp_df = temp_df[new_col_name].to_dict()   \n    \n        train_df[new_col_name] = train_df[col].map(temp_df)\n        test_df[new_col_name]  = test_df[col].map(temp_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"START_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')\nfor df in [train_df, test_df]:\n    \n    # Temporary variables for aggregation\n    df['DT'] = df['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n    df['DT_M'] = ((df['DT'].dt.year-2017)*12 + df['DT'].dt.month).astype(np.int8)\n    df['DT_W'] = ((df['DT'].dt.year-2017)*52 + df['DT'].dt.weekofyear).astype(np.int8)\n    df['DT_D'] = ((df['DT'].dt.year-2017)*365 + df['DT'].dt.dayofyear).astype(np.int16)\n    \n    df['DT_hour'] = (df['DT'].dt.hour).astype(np.int8)\n    df['DT_day_week'] = (df['DT'].dt.dayofweek).astype(np.int8)\n    df['DT_day_month'] = (df['DT'].dt.day).astype(np.int8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[['DT','DT_M','DT_W', 'DT_D','DT_hour', 'DT_day_week', 'DT_day_month' ]].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Total transactions per timeblock\nfor col in ['DT_M','DT_W','DT_D', 'DT_hour', 'DT_day_week', 'DT_day_month']:\n    temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n    fq_encode = temp_df[col].value_counts().to_dict()\n            \n    train_df[col+'_total'] = train_df[col].map(fq_encode)\n    test_df[col+'_total']  = test_df[col].map(fq_encode)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[['DT_M_total','DT_W_total','DT_D_total', 'DT_hour_total', 'DT_day_week_total', 'DT_day_month_total']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"V_col_remove = [col for col in V_list if col not in V_list_]\n#print(V_col_remove)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ['M1','M2','M3','M5','M6','M7','M8','M9']:\n    train_df[col] = train_df[col].map({'T':1, 'F':0})\n    test_df[col]  = test_df[col].map({'T':1, 'F':0})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['ProductCD'].head()\n#.fillna('unseen_before_label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['ProductCD'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['ProductCD'].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['ProductCD'].fillna('unseen_before_label').head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.select_dtypes(include=['object'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in list(train_df):\n    if train_df[col].dtype=='object':\n        print(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Frequency encoding for categorical column and then doing categorical encoding\nfor col in list(train_df):\n    if train_df[col].dtype=='object':\n        print(col)\n        train_df[col] = train_df[col].fillna('NaN')\n        test_df[col]  = test_df[col].fillna('NaN')\n        \n        train_df[col] = train_df[col].astype(str)\n        test_df[col] = test_df[col].astype(str)\n        \n        le = LabelEncoder()\n        le.fit(list(train_df[col])+list(test_df[col]))\n        train_df[col] = le.transform(train_df[col])\n        test_df[col]  = le.transform(test_df[col])\n        \n        train_df[col] = train_df[col].astype('category')\n        test_df[col] = test_df[col].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['R_emaildomain']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rm_cols = [\n\n    'uid','uid2','uid3',            \n    'DT','DT_M', 'DT_W', 'DT_D','DT_hour', 'DT_day_week', 'DT_day_month' # Already we have considered these\n    \n]\n\nrm_cols.extend(V_col_remove)\n#rm_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_columns = [col for col in list(train_df) if col not in rm_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_columns.remove('isFraud')\nfeatures_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Model\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\nimport gc\nSEED = 10\n\ndef make_predictions(train_df, test_df, features_columns, target, lgb_params, NFOLDS=3):\n    N_SPLITS = 10    \n    folds = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n\n    # Main Data\n    X,y = train_df[features_columns], train_df[TARGET]\n\n    # Test Data and expport DF\n    P,P_y  = test_df[features_columns], test_df[TARGET]\n    \n    test_df = test_df[['TransactionID',target]]    \n    predictions = np.zeros(len(test_df))\n\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y, groups=y)):\n        print('Fold:',fold_+1)\n        tr_x, tr_y = X.iloc[trn_idx,:], y[trn_idx]    \n        vl_x, vl_y = X.iloc[val_idx,:], y[val_idx]    \n        train_data = lgb.Dataset(tr_x, label=tr_y)\n        #valid_data = lgb.Dataset(vl_x, label=v_y)  \n        \n        if LOCAL_TEST:\n            valid_data = lgb.Dataset(P, label=P_y) \n        else:\n            valid_data = lgb.Dataset(vl_x, label=vl_y)  \n\n\n        estimator = lgb.train(\n                lgb_params,\n                train_data,\n                valid_sets = [train_data, valid_data],\n                verbose_eval = 1000,\n            )\n        \n        pp_p = estimator.predict(P)\n        predictions += pp_p/NFOLDS\n        \n        if LOCAL_TEST:\n            feature_imp = pd.DataFrame(sorted(zip(estimator.feature_importance(),X.columns)), columns=['Value','Feature'])\n            print(feature_imp)\n        \n       # del tr_x, tr_y, vl_x, vl_y, train_data, valid_data\n       # gc.collect()\n        \n    test_df['prediction']  = predictions\n    \n    return test_df       ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LOCAL_TEST = False\n# Model params\nfrom sklearn import metrics\nlgb_params = {\n                    'objective':'binary',\n                    'boosting_type':'gbdt',\n                    'metric':'auc',\n                    'n_jobs':-1,\n                    'learning_rate':0.01,\n                    'num_leaves': 2**8,\n                    'max_depth':-1,\n                    'tree_learner':'serial',\n                    'colsample_bytree': 0.7,\n                    'subsample_freq':1,\n                    'subsample':0.7,\n                    'n_estimators':800,\n                    'max_bin':255,\n                    'verbose':-1,\n                    'seed': SEED,\n                    'early_stopping_rounds':100, \n                } \n# Model Train\nif LOCAL_TEST:\n    lgb_params['learning_rate'] = 0.01\n    lgb_params['n_estimators'] = 20000\n    lgb_params['early_stopping_rounds'] = 100\n    test_predictions = make_predictions(train_df, test_df, features_columns, TARGET, lgb_params)\n    print(metrics.roc_auc_score(test_predictions[TARGET], test_predictions['prediction']))\nelse:\n    lgb_params['learning_rate'] = 0.01\n    lgb_params['n_estimators'] = 800\n    lgb_params['early_stopping_rounds'] = 100    \n    test_predictions = make_predictions(train_df, test_df, features_columns, TARGET, lgb_params, NFOLDS=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submission\ntest_predictions['isFraud'] = test_predictions['prediction']\ntest_predictions[['TransactionID','isFraud']].to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}