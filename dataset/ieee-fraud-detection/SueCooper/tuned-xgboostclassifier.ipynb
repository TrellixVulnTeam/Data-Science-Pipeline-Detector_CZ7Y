{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is my first notebook and submission to a kaggle competition. I took on this competition for a university project and have tested many different models including, logistic regression, random forest, svm and neural networks. I discovered out of these model tested my best result came from a random forest classifier. I tuned this model and improved it too a auc_roc_score of 0.92. Upon searching for more ways to improve this model I found gradient boosting models and tested both LGB and XGB with XGB providing larger auc scores. From here I began to tune the model by testing different stratified samples and parameters to come up with the result below. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import LabelEncoder,StandardScaler #(encoding and standardising data)\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First Loading in the datasets"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_id = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/train_identity.csv\")\ntrain_tran = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/train_transaction.csv\")\ntest_id = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/test_identity.csv\")\ntest_tran = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/test_transaction.csv\")\nsample_submission = pd.read_csv('/kaggle/input/ieee-fraud-detection/sample_submission.csv', index_col='TransactionID')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Merging the transaction and identity dataset for test and train.\n* Converting to a dataframe\n* Deleting unused datasets to save memory"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.merge(train_tran, train_id, on=\"TransactionID\", how=\"left\")\ntest = pd.merge(test_tran, test_id, on=\"TransactionID\", how=\"left\")\ntrain = pd.DataFrame(train)\ntest = pd.DataFrame(test)\ndel train_id, train_tran, test_id, test_tran","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Splitting the target variables for the trainning set & deleting unused dataset "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_y = train.isFraud\ntrain_x=train.drop([\"isFraud\"], axis=1)\ndel train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To apply the data to xgboost I label encode the data using sklearns labelencoder() method"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical = ['ProductCD','card1' ,'card2' ,'card3' ,'card4' ,'card5' ,'card6', 'addr1', 'addr2', 'P_emaildomain', 'R_emaildomain', 'M1'\n               ,'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'DeviceType', 'DeviceInfo', 'id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', \n               'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30', \n               'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38']\n\n\n\nlb_make = LabelEncoder()\n\nlength_cat = len(categorical)\nfor i in range(length_cat):\n    train_x[categorical[i]] = lb_make.fit_transform(train_x[categorical[i]].astype(str))\n    test[categorical[i]] = lb_make.fit_transform(test[categorical[i]].astype(str))\n   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next I standardise the data to equalise the range and variability of the data "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x = StandardScaler().fit_transform(train_x)\ntest = StandardScaler().fit_transform(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I have tuned a model by selecting optimal, max_depth, min_child_weight, sub_sample and colsample_bytree I selected random forest as my boosting alogrithm as apart of my report process I explored the use of random forest and wanted to apply the xgboosting alogrithm to strengthen its learning ability. This model gave my best result so far. "},{"metadata":{"trusted":true},"cell_type":"code","source":"model = XGBClassifier(boosting=\"gbdt\", max_depth=10, min_child_weight=3, subsample=0.9, colsample_bytree=0.9, gamma=0.4)\nmodel.fit(train_x, train_y)\npred = model.predict_proba(train_x)[:, 1]\nprint(\"Tuned\")\nprint(\"AUC\", roc_auc_score(train_y, pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating a submission for the competition"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['isFraud'] = model.predict_proba(test)[:, 1]\nsample_submission.to_csv('tuned_xgb3.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I am completing a more in-depth report for my university project and this is just the model I have found to be the strongest so far and I will continue to explore different models. Any tips for improving my model or problems please comment as I wish to gain as much knowledge from this experience as possible."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}