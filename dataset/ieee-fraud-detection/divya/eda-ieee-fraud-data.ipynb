{"cells":[{"metadata":{},"cell_type":"markdown","source":"I attempt to develop a model that provides the probability that a certain transaction is a fraud."},{"metadata":{},"cell_type":"markdown","source":"References\nhttps://www.kaggle.com/kabure/extensive-eda-and-modeling-xgb-hyperopt#reducing-memory-usage\nhttps://www.kaggle.com/shahules/tackling-class-imbalance"},{"metadata":{},"cell_type":"markdown","source":"## Load Packages, Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.utils import resample\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score,precision_recall_curve,roc_curve\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transaction CSVs\ntrain_transaction = pd.read_csv('../input/train_transaction.csv')\ntest_transaction = pd.read_csv('../input/test_transaction.csv')\n# Identity CSVs - These will be merged onto the transactions to create additional features\ntrain_identity = pd.read_csv('../input/train_identity.csv')\ntest_identity = pd.read_csv('../input/test_identity.csv')\n# Sample Submissions\nss = pd.read_csv('../input/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Examine data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_identity.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train_transaction.merge(train_identity,how='left',left_index=True,right_index=True)\ny_train = train['isFraud']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test= train_test_split( train.drop('isFraud',axis=1), y_train, test_size=.2 , random_state=1 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isna().sum()[train.isna().sum() > 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check if there's a class imbalance"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction.isFraud.value_counts() / train_transaction.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Next Steps:\n\nThere are a few questions to answer next, especially due to the  class imbalance:\n* Do I select features before or after addressing class imbalance\n* What metrics, beyond classification, should I consider using.\n* Do I need to consider certain algorithms due to the class imbalance?\n* Do I need to do data imputation for the missing data.\n    * particularly the identity data"},{"metadata":{},"cell_type":"markdown","source":"### Metrics to consider\n\nNaturally accuracy can be a misleading metric because we can have 96% accuracy without classifying any of the fraudalent transactions correctly. I'm going to consider the F1 score which is a weighted average of the Precision and Recall metric. To refresh\n* **Precision**: Is the measure of how many that were predicted to be a class were actually the class? In this case, I would look at hte precision of the minority class, fradualent transactions and see how many of the fradualent transactions were correctly classified\n* **Recall**: A measure of how many positive values in the original dataset were classified correctly. Another way to describe this is the number of true positives divided by the number of total positives in the original dataset. \n\n$$ F1 Score = 2 * \\frac{precision * recall}{precision + recall}$$\n\nWhile in some use cases, it may make sense to prioritize precision or recall (a treatment cheap so it's better to identify things as positive to be safe instead of focusing on precision). For the case of project, I am focusing on precision and recall."},{"metadata":{},"cell_type":"markdown","source":"### How to tackle class imbalance\nThere are a variety of techniques include\n* SMOTE\n* Resampling the minority class, or undersampling the majority class\n"},{"metadata":{},"cell_type":"markdown","source":"There may be bias inherently in the dataset due to the fact that only certain types of transcations will be considered fraudalent and caught in retrospect. Maybe there is something to be said about transactions that are fraudalent and not caught."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}