{"cells":[{"metadata":{},"cell_type":"markdown","source":"This kernel edit and blend with 2 kernel, https://www.kaggle.com/vaishvik25/nn-conv1d?scriptVersionId=20924291 and https://www.kaggle.com/abazdyrev/keras-nn-focal-loss-experiments","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Data Loading and Feature Selection <br>\n","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\ntrain = pd.read_csv('../input/train_transaction.csv')\ntest = pd.read_csv('../input/test_transaction.csv')\nsub = pd.read_csv('../input/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"useful_features = list(train.iloc[:, 3:55].columns)\n\ny = train.sort_values('TransactionDT')['isFraud']\nX = train.sort_values('TransactionDT')[useful_features]\nX_test = test[useful_features]\ndel train, test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_features = [\n    'ProductCD',\n    'card1', 'card2', 'card3', 'card4', 'card5', 'card6',\n    'addr1', 'addr2',\n    'P_emaildomain',\n    'R_emaildomain',\n    'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9'\n]\n\ncontinuous_features = list(filter(lambda x: x not in categorical_features, X))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Processing\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class ContinuousFeatureConverter:\n    def __init__(self, name, feature, log_transform):\n        self.name = name\n        self.skew = feature.skew()\n        self.log_transform = log_transform\n        \n    def transform(self, feature):\n        if self.skew > 1:\n            feature = self.log_transform(feature)\n        \n        mean = feature.mean()\n        std = feature.std()\n        return (feature - mean)/(std + 1e-6)        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm.autonotebook import tqdm\n\nfeature_converters = {}\ncontinuous_features_processed = []\ncontinuous_features_processed_test = []\n\nfor f in tqdm(continuous_features):\n    feature = X[f]\n    feature_test = X_test[f]\n    log = lambda x: np.log10(x + 1 - min(0, x.min()))\n    converter = ContinuousFeatureConverter(f, feature, log)\n    feature_converters[f] = converter\n    continuous_features_processed.append(converter.transform(feature))\n    continuous_features_processed_test.append(converter.transform(feature_test))\n    \ncontinuous_train = pd.DataFrame({s.name: s for s in continuous_features_processed}).astype(np.float32)\ncontinuous_test = pd.DataFrame({s.name: s for s in continuous_features_processed_test}).astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"continuous_train['isna_sum'] = continuous_train.isna().sum(axis=1)\ncontinuous_test['isna_sum'] = continuous_test.isna().sum(axis=1)\n\ncontinuous_train['isna_sum'] = (continuous_train['isna_sum'] - continuous_train['isna_sum'].mean())/continuous_train['isna_sum'].std()\ncontinuous_test['isna_sum'] = (continuous_test['isna_sum'] - continuous_test['isna_sum'].mean())/continuous_test['isna_sum'].std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"isna_columns = []\nfor column in tqdm(continuous_features):\n    isna = continuous_train[column].isna()\n    if isna.mean() > 0.:\n        continuous_train[column + '_isna'] = isna.astype(int)\n        continuous_test[column + '_isna'] = continuous_test[column].isna().astype(int)\n        isna_columns.append(column)\n        \ncontinuous_train = continuous_train.fillna(0.)\ncontinuous_test = continuous_test.fillna(0.)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For categorical features we will apply OneHot transformation, but only for most common values for each feature to reduce sparsity. <br>\nAlso there is an embedding approach for categorical features transformation. It was implemented in this kernel https://www.kaggle.com/ryches/keras-nn-starter-w-time-series-split <br>\nWith embedding approach I didn't get any significant improvement comparing to this.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom tqdm.autonotebook import tqdm\n\ndef categorical_encode(df_train, df_test, categorical_features, n_values=50):\n    df_train = df_train[categorical_features].astype(str)\n    df_test = df_test[categorical_features].astype(str)\n    \n    categories = []\n    for column in tqdm(categorical_features):\n        categories.append(list(df_train[column].value_counts().iloc[: n_values - 1].index) + ['Other'])\n        values2use = categories[-1]\n        df_train[column] = df_train[column].apply(lambda x: x if x in values2use else 'Other')\n        df_test[column] = df_test[column].apply(lambda x: x if x in values2use else 'Other')\n        \n    \n    ohe = OneHotEncoder(categories=categories)\n    ohe.fit(pd.concat([df_train, df_test]))\n    df_train = pd.DataFrame(ohe.transform(df_train).toarray()).astype(np.float16)\n    df_test = pd.DataFrame(ohe.transform(df_test).toarray()).astype(np.float16)\n    return df_train, df_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_categorical, test_categorical = categorical_encode(X, X_test, categorical_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pd.concat([continuous_train, train_categorical], axis=1)\ndel continuous_train, train_categorical\nX_test = pd.concat([continuous_test, test_categorical], axis=1)\ndel continuous_test, test_categorical\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\npca = PCA()\npca_fit_train_v = pca.fit_transform(X)\npca_fit_test_v = pca.fit_transform(X_test)\nX = pd.DataFrame(data = pca_fit_train_v)\nX_test = pd.DataFrame(data = pca_fit_test_v)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Validation\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"split_ind = int(X.shape[0]*0.8)\n\nX_tr = X.iloc[:split_ind]\nX_val = X.iloc[split_ind:]\n\ny_tr = y.iloc[:split_ind]\ny_val = y.iloc[split_ind:]\n\ndel X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_tr.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nimport random\nimport tensorflow as tf\nimport keras.backend as K\n\nfrom keras.models import Model\nfrom keras.layers import Dense, Input, Dropout, BatchNormalization, Activation\nfrom keras.utils.generic_utils import get_custom_objects\nfrom keras.optimizers import Adam, Nadam\nfrom keras.callbacks import Callback\nfrom sklearn.metrics import roc_auc_score\n\nnp.random.seed(42) # NumPy\nrandom.seed(42) # Python\ntf.set_random_seed(42) # Tensorflow","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compatible with tensorflow backend\nclass roc_callback(Callback):\n    def __init__(self,training_data,validation_data):\n        self.x = training_data[0]\n        self.y = training_data[1]\n        self.x_val = validation_data[0]\n        self.y_val = validation_data[1]\n\n\n    def on_train_begin(self, logs={}):\n        return\n\n    def on_train_end(self, logs={}):\n        return\n\n    def on_epoch_begin(self, epoch, logs={}):\n        return\n\n    def on_epoch_end(self, epoch, logs={}):\n        y_pred_val = self.model.predict(self.x_val)\n        roc_val = roc_auc_score(self.y_val, y_pred_val)\n        print('\\rroc-auc_val: %s' % (str(round(roc_val,4))),end=100*' '+'\\n')\n        return\n\n    def on_batch_begin(self, batch, logs={}):\n        return\n\n    def on_batch_end(self, batch, logs={}):\n        return\n    \ndef focal_loss(gamma=2., alpha=.25):\n    def focal_loss_fixed(y_true, y_pred):\n        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n        return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(K.epsilon()+pt_1))-K.mean((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0 + K.epsilon()))\n    return focal_loss_fixed\n\ndef custom_gelu(x):\n    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n\nget_custom_objects().update({'custom_gelu': Activation(custom_gelu)})\nget_custom_objects().update({'focal_loss_fn': focal_loss()})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(loss_fn):\n    inps = Input(shape=(X_tr.shape[1],))\n    x = keras.layers.Reshape((X_tr.shape[1],1))(inps)\n    x = keras.layers.Conv1D(32, 5, activation='elu')(x)\n    x = BatchNormalization()(x)\n    x = keras.layers.Conv1D(24,1, activation='elu')(x)\n    x = BatchNormalization()(x)\n    x = keras.layers.Conv1D(16,1, activation='elu')(x)\n    x = BatchNormalization()(x)\n    x = keras.layers.Conv1D(4,1, activation='elu')(x)\n    x = keras.layers.Flatten()(x)\n    x = BatchNormalization()(x)\n    x = Dense(1, activation='sigmoid')(x)\n\n    model = Model(inputs=inps, outputs=x)\n    model.compile(\n        optimizer='adam',\n        loss=[loss_fn],\n        metrics=['accuracy']\n        \n    )\n    #model.summary()\n    return model\n\n  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_bce = create_model('binary_crossentropy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_bce.fit(\n    X_tr, y_tr, epochs=100, batch_size=2048, validation_data=(X_val, y_val), verbose=True, \n    callbacks=[roc_callback(training_data=(X_val, y_tr), validation_data=(X_val, y_val))]\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_preds_bce = model_bce.predict(X_val).flatten()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import rankdata, spearmanr\n\nprint('BCE preds: ', roc_auc_score(y_val, val_preds_bce))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fine-tuning and Predicting","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model_bce.fit(X_val, y_val, epochs=2, batch_size=2048, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_preds_bce = model_bce.predict(X_val).flatten()\nprint('BCE preds: ', roc_auc_score(y_val, val_preds_bce))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.isFraud = rankdata(model_bce.predict(X_test).flatten(), method='dense' )\nsub.isFraud = sub.isFraud/sub.isFraud.max()\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}