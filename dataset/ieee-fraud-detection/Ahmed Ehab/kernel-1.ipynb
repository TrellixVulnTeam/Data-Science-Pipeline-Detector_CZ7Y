{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)`\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data\n\nIn this competition we are predicting the probability that an online transaction is fraudulent, as denoted by the binary target isFraud.\n\nThe data is broken into two files **identity ** and **transaction**, which are joined by TransactionID."},{"metadata":{},"cell_type":"markdown","source":"# code to load data and merge them \nthis cell will be run only once and we then restart the kernel to save some memory space \nafter restarting the kernel we load the merged data directly and start working \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# sample_submission = pd.read_csv(\"../input/ieee-fraud-detection/sample_submission.csv\")\n\ntest_identity = pd.read_csv(\"../input/input/test_identity.csv\" , index_col = 'TransactionID')\n# test_identity = pd.read_csv(\"../input/ieee-fraud-detection/test_identity.csv\" , index_col='TransactionID')\ntest_transaction = pd.read_csv(\"../input/ieee-fraud-detection/test_transaction.csv\",index_col='TransactionID')\ntrain_identity = pd.read_csv(\"../input/ieee-fraud-detection/train_identity.csv\",index_col='TransactionID')\ntrain_transaction = pd.read_csv(\"../input/ieee-fraud-detection/train_transaction.csv\",index_col='TransactionID')\n\ntrain_data = train_transaction.merge(train_identity, how='left' ,left_index=True , right_index=True)\ntest_data  = test_transaction.merge(test_identity,how='left' , left_index=True, right_index=True)\n\n\ntest_data.to_csv('merged_test_data.csv')\ntrain_data.to_csv('merged_train_data.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_identity,train_transaction,test_identity, test_transaction , test_data , train_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Start from here \nafter restarting the kernel start from here to load merged data \n\n     \n\n### Categorical Features - Transaction\n\n    ProductCD\n    emaildomain\n    card1 - card6\n    addr1, addr2\n    P_emaildomain\n    R_emaildomain\n    M1 - M9   \n###  Categorical Features - Identity\n\n    DeviceType\n    DeviceInfo\n    id_12 - id_38\n\nThe TransactionDT feature is a timedelta from a given reference datetime (not an actual timestamp)."},{"metadata":{},"cell_type":"markdown","source":"### import things and functions used "},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef resumetable(df):\n    print(f\"Dataset Shape: {df.shape}\")\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    return summary\n    \n\n# THIS FUNCTION WILL PLOT A CORRELATION HEATMAP WITH A SET THRESHOLD OF 0.9 CORRELATION.\ndef corrfunc(df , col):\n    color = plt.get_cmap('RdYlGn') \n    color.set_bad('green') \n    correalation =df[col].corr()\n    correalation[np.abs(correalation)<.9] = 0 # This will set all correlations less than 0.9 to 0\n    plt.figure(figsize= (len(col),len(col)))\n    sns.heatmap(correalation, yticklabels= True, annot = True, vmin=-1, vmax=1,cmap = color)\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: \n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## code to load the merged data"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndata = pd.read_csv(\"../input/kernel-1/merged_train_data.csv\" , index_col='TransactionID')\nprint(data.shape)\n\ntrain = reduce_mem_usage(data)\n\n\ndata = pd.read_csv(\"../input/kernel-1/merged_test_data.csv\" , index_col='TransactionID')\n\nX_test = reduce_mem_usage(data)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with pd.option_context('display.max_columns', 433):\n    print(train.describe(include='all'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"resumetable(train)[:25]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape , X_test.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# exploring the data "},{"metadata":{},"cell_type":"markdown","source":"## Target distribution (isFraud distripution)"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(9,6))\ng =sns.countplot(x='isFraud' , data= train )\ng.set_title(\"Fraud Transactions Distribution \\n# 0: No Fraud | 1: Fraud #\", fontsize=22)\ng.set_xlabel(\"Is fraud?\", fontsize=18)\ng.set_ylabel('Count', fontsize=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"notice that it is imbalanced "},{"metadata":{},"cell_type":"markdown","source":"## trasaction amount feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Transaction Amounts Quantiles:\")\nprint(train['TransactionAmt'].quantile([.01, .025, .1, .25, .5, .75, .9, .975, .99]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,12))\nplt.suptitle('Transaction Values Distribution', fontsize=22)\nplt.subplot(221)\ng = sns.distplot(train['TransactionAmt'])\ng.set_title(\"Transaction Amount\", fontsize=18)\ng.set_xlabel(\"\")\ng.set_ylabel(\"Probability\", fontsize=15)\n\nplt.subplot(222)\ng1 = sns.distplot(np.log(train['TransactionAmt']))\ng1.set_title(\"Transaction Amount (Log) Distribuition\", fontsize=18)\ng1.set_xlabel(\"\")\ng1.set_ylabel(\"Probability\", fontsize=15)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Product Feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.figure(figsize=(14,10))\nplt.title('ProductCD Distributions', fontsize=22)\nplt.subplot(221)\ng = sns.countplot(x='ProductCD', data=train)\ng.set_title(\"ProductCD Distribution\", fontsize=19)\ng.set_xlabel(\"ProductCD Name\", fontsize=17)\ng.set_ylabel(\"Count\", fontsize=17)\n\nplt.subplot(222)\ng1 = sns.countplot(x='ProductCD', hue='isFraud', data=train)\nplt.legend(title='Fraud', loc='best', labels=['No', 'Yes'])\n\ng1.set_title(\"Product CD by Target(isFraud)\", fontsize=19)\ng1.set_xlabel(\"ProductCD Name\", fontsize=17)\ng1.set_ylabel(\"Count\", fontsize=17)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Card Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"resumetable(train[['card1', 'card2', 'card3','card4', 'card5', 'card6']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corrfunc(train,['card1','card2','card3','card5'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,22))\nplt.subplot(411)\ng = sns.distplot(train[train['isFraud'] == 1]['card1'], label='Fraud')  \ng = sns.distplot(train[train['isFraud'] == 0]['card1'], label='NoFraud')\ng.legend()\ng.set_title(\"Card 1 Values Distribution by Target\", fontsize=16)\ng.set_xlabel(\"Card 1 Values\", fontsize=12)\ng.set_ylabel(\"Probability\", fontsize=18)\n\nplt.subplot(412)\ng1 = sns.distplot(train[train['isFraud'] == 1]['card2'].dropna(), label='Fraud')\ng1 = sns.distplot(train[train['isFraud'] == 0]['card2'].dropna(), label='NoFraud')\ng1.legend()\ng1.set_title(\"Card 2 Values Distribution by Target\", fontsize=18)\ng1.set_xlabel(\"Card 2 Values\", fontsize=12)\ng1.set_ylabel(\"Probability\", fontsize=18)\n\nplt.subplot(413)\ng3 = sns.distplot(train[train['isFraud']==1]['card3'].dropna(),label='Fraud')\ng3 = sns.distplot(train[train['isFraud']==0]['card3'].dropna(),label='NotFraud')\ng3.legend()\ng3.set_title('Card3 values Distibution by Target' , fontsize = 18)\ng3.set_xlabel('Card3 Values' ,fontsize=12)\ng3.set_ylabel('Probability' ,fontsize=18)\n\nplt.subplot(414)\ng4=sns.distplot(train[train['isFraud']==1]['card5'].dropna() , label='Fraud' )\ng4=sns.distplot(train[train['isFraud']==1]['card5'].dropna() , label='Fraud' )\ng4.legend()\ng4.set_title('Card5 values Distibution by Target' , fontsize = 18)\ng4.set_xlabel('Card5 Values' ,fontsize=12)\ng4.set_ylabel('Probability' ,fontsize=18)\n\n\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,12))\nplt.subplot(211)\ng=sns.countplot(x='card4' , data = train)\ng.set_title(\"Card4 Distribution\", fontsize=19)\ng.set_xlabel(\"Card4 Category Names\", fontsize=17)\ng.set_ylabel(\"Count\", fontsize=17)\n\nplt.subplot(212)\ng2=sns.countplot(x='card6' , data = train)\ng2.set_title(\"Card6 Distribution\", fontsize=19)\ng2.set_xlabel(\"Card6 Category Names\", fontsize=17)\ng2.set_ylabel(\"Count\", fontsize=17)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploring M1-M9 Features\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ['M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']:   \n    plt.figure(figsize=(5,5))\n    g =sns.countplot(x=col , data = train)\n    g.set_title(col+ \" Distribution\", fontsize=19)\n    g.set_xlabel(col+ \" Category Names\", fontsize=17)\n    g.set_ylabel(\"Count\", fontsize=17)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## find columns with missing data>80%\n\nso we drop these columns as they many nulls"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_null = train.isnull().sum()/len(train) * 100\ndata_null = data_null.drop(data_null[data_null == 0].index).sort_values(ascending=False)[:500]\n\nmissing_data = pd.DataFrame({'Missing Ratio': data_null})\nprint(missing_data.shape)\nmissing_data.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find attributes with more than 90 percent missing vaules \ndef get_useless_columns(data):\n    \n    too_many_null = [col for col in data.columns if data[col].isnull().sum() / data.shape[0] > 0.80]\n    print(\"More than 80% null columns: \" + str(len(too_many_null)))\n    \n#     too_many_rpeated_values = [col for col in data.columns if data[col].value_counts(dropna=False \n#                 ,normalize =True).values[0] >0.90]\n    \n#     print(\"More than 90% repeated value columns: \" + str(len(too_many_rpeated_values)))\n    \n    cols_to_drop = list(set(too_many_null))# + too_many_rpeated_values))\n   # cols_to_drop.remove('isFraud')\n    return cols_to_drop\n\n\n\ncols_to_drop = get_useless_columns(train)\nprint(cols_to_drop)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# feature engineering "},{"metadata":{},"cell_type":"markdown","source":"## handle email domain "},{"metadata":{"trusted":true},"cell_type":"code","source":"emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', \n          'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft',\n          'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',\n          'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', \n          'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink',\n          'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other',\n          'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', \n          'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', \n          'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo',\n          'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n          'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft',\n          'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', \n          'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', \n          'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', \n          'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', \n          'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', \n          'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', \n          'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other',\n          'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n\nus_emails = ['gmail', 'net', 'edu']\nfor c in ['P_emaildomain', 'R_emaildomain']:\n    train[c + '_bin'] = train[c].map(emails)\n    X_test[c + '_bin'] = X_test[c].map(emails)\n    train[c + '_suffix'] = train[c].map(lambda x: str(x).split('.')[-1])\n    X_test[c + '_suffix'] = X_test[c].map(lambda x: str(x).split('.')[-1])\n    \n    train[c + '_suffix'] = train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n    X_test[c + '_suffix'] = X_test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y =train['isFraud'].copy()\n\ntrain =train.drop(['isFraud'],axis=1)\n\n\nx_train_reduced = train.drop(cols_to_drop , axis=1)\n# x_vaild_reduced = x_valid_full.drop(cols_to_drop, axis=1)\n\nX_test_reduced = X_test.drop(cols_to_drop , axis = 1)\n\n# # del x_train_full\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_train_reduced.shape , X_test_reduced.shape)    \nx_train_reduced.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  separete categorical values from numerical values\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#  separete categorical values from numerical values\n\ncategorical_col = [cname for cname in x_train_reduced.columns if   #x_train_reduced[cname].nunique()<15 and \n                   x_train_reduced[cname].dtype=='object']\n\nprint(categorical_col)\n\n\nnumerical_col = [cname for cname in x_train_reduced.columns \n                 \n                 if x_train_reduced[cname].dtype!='object']\n# print(numerical_col) \nprint(len(numerical_col)+len(categorical_col))\n\n\nmy_col=categorical_col+numerical_col\n\n#  now we have total cols \nX_train = x_train_reduced[my_col].copy()\n# X_vaild=x_vaild_reduced[my_col].copy()\nX_test = X_test_reduced[my_col].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape , X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## encode categorical columns and impute the missing values in numerical columns \nuse label encoder with most frequent values for categorical columns and imputer with mean values for numerical values"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nnumircal_imputer = SimpleImputer(strategy='mean')\n\ncat_encoder= LabelEncoder()\ncat_imputer = SimpleImputer(strategy='most_frequent')\n\n\n#impute numerical values  \n\nx_train_imputed_numerical = pd.DataFrame(numircal_imputer.fit_transform(X_train[numerical_col]))\n# x_vaild_imputed_numerical = pd.DataFrame(numircal_imputer.transform(X_vaild[numerical_col]))\nx_test_imputed_numerical = pd.DataFrame(numircal_imputer.transform(X_test[numerical_col]))\n\nx_train_imputed_numerical.columns= X_train[numerical_col].columns\n# x_vaild_imputed_numerical.columns=X_vaild[numerical_col].columns\nx_test_imputed_numerical.columns=X_test[numerical_col].columns\n\n\n# impute cat values \n\nx_train_imputed_cat = pd.DataFrame(cat_imputer.fit_transform(X_train[categorical_col]))\n# x_vaild_imputed_cat = pd.DataFrame(cat_imputer.transform(X_vaild[categorical_col]))\nx_test_imputed_cat = pd.DataFrame(cat_imputer.transform(X_test[categorical_col]))\n\nx_train_imputed_cat.columns = X_train[categorical_col].columns\n# x_vaild_imputed_cat.columns = X_vaild[categorical_col].columns\nx_test_imputed_cat.columns = X_test[categorical_col].columns\n\n# encode categical variables \n\nmy_encoder = LabelEncoder()\n\n\nfor col in categorical_col:\n    my_encoder.fit(list(x_train_imputed_cat[col].values) + list(x_test_imputed_cat[col].values))\n    x_train_imputed_cat[col] = my_encoder.transform(x_train_imputed_cat[col])\n#     x_vaild_imputed_cat[col] = my_encoder.transform(x_vaild_imputed_cat[col])\n    x_test_imputed_cat[col]  = my_encoder.transform(x_test_imputed_cat[col])\ndel X_train ,X_test\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = pd.concat([x_train_imputed_numerical , x_train_imputed_cat] ,axis=1)\n\nX_test  = pd.concat([x_test_imputed_numerical , x_test_imputed_cat] ,axis=1)\n\nprint(X_train.shape , X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"free some memory space"},{"metadata":{"trusted":true},"cell_type":"code","source":"del X_test_reduced , c , cat_encoder ,cat_imputer , categorical_col , col , cols_to_drop , corrfunc ,data \ndel emails , g , g1 , g2 , g3 ,g4 , get_useless_columns ,gridspec , missing_data\ndel my_encoder , my_col ,numerical_col , numircal_imputer , train\ndel x_test_imputed_cat ,x_test_imputed_numerical ,x_train_imputed_cat ,x_train_reduced\ndel data_null","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"whos","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = reduce_mem_usage(X_train)\nX_test = reduce_mem_usage(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Models for classification "},{"metadata":{"trusted":true},"cell_type":"code","source":"X1_train , X1_valid ,y_train , y_vaild = train_test_split(X_train , Y ,train_size = 0.8 ,test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(n_estimators=100,verbose=1)\nmodel.fit(X1_train , y_train)\n\npred=model.predict(X1_valid)\nprint(accuracy_score(y_vaild, pred))\nsample_submission = pd.read_csv(\"../input/ieee-fraud-detection/sample_submission.csv\",index_col='TransactionID')\nsample_submission['isFraud']=model.predict(X_test)\nsample_submission.to_csv('randomForest.csv')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Multi Layer Perceptron"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.regularizers import l2\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Dense(10, activation=tf.nn.sigmoid , activity_regularizer=l2(0.1)))\nmodel.add(tf.keras.layers.Dense(4, activation=tf.nn.sigmoid , activity_regularizer = l2(0.01)))\nmodel.add(tf.keras.layers.Dense(1, activation=tf.nn.relu))\n\nmodel.compile(optimizer=\"adam\", loss=\"binary_crossentropy\"  , metrics=['accuracy'])\n\nmodel.fit(X1_train.values ,y_train.values, epochs=10 , batch_size=100)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model.evaluate(X1_valid, y_vaild))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['isFraud']=model.predict(X_test)\nsample_submission.to_csv('MLP no PCA with regularization L2-2.csv')\n                         \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"whos","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del LabelEncoder ,RandomForestClassifier ,tf ,model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  fisher linear discriminant analysis LDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nlda = LinearDiscriminantAnalysis()\n\nlda.fit(X1_train ,y_train)\n\npred=lda.predict(X1_valid)\nprint(accuracy_score(y_vaild, pred))\n\nsample_submission['isFraud']=lda.predict(X_test)\nsample_submission.to_csv('LDA.csv')\n                         \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X1_train = lda.transform(X1_train)\nX1_test = lda.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X1_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## support vector machines with LDA\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm\n\nmodel = svm.SVC(kernel='linear' , C=0.3 ,verbose=True)\nmodel.fit(X1_train, y_train)\n# preds = model.predict(X1_valid)\n# print(model.score(y_valid, preds))\nsample_submission['isFraud']=model.predict(X1_test)\nsample_submission.to_csv('SVM.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del lda , X1_test , X1_train ,model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## feature reduction with PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n# from sklearn.preprocessing import StandardScaler\n# scaler = StandardScaler()    # normalize data before PCA\n# train_scaled = scaler.fit_transform(X_train)  \n\n\npca = PCA(n_components=260)\nX_train = pca.fit_transform(X_train)\nprint(pca.n_components_ )\n\nX_train = pd.DataFrame(X_train)\nX_train=reduce_mem_usage(X_train)\n\nX_test = pca.transform(X_test)\nX_test = pd.DataFrame(X_test)\nX_test = reduce_mem_usage(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree with PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"X1_train , X1_valid ,y_train , y_vaild = train_test_split(X_train , Y ,train_size = 0.8 ,test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nmodel = DecisionTreeClassifier()\nmodel.fit(X1_train, y_train)\npreds = model.predict(X1_valid)\nprint(accuracy_score(y_vaild, preds))\n\n# print(model.score(y_vaild, preds))\n# cross_val_score(model, X_train ,Y, cv=10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(y_vaild, preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['isFraud']=model.predict(X_test)\nsample_submission.to_csv('decisionTree with PCA 260.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}