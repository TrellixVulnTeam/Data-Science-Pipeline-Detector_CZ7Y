{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import LabelEncoder\nimport datetime \nimport lightgbm as lgb\n\nimport os\nimport gc\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction = reduce_mem_usage(pd.read_csv('../input/train_transaction.csv', index_col='TransactionID'))\ntest_transaction = reduce_mem_usage(pd.read_csv('../input/test_transaction.csv', index_col='TransactionID'))\n\ntrain_identity = reduce_mem_usage(pd.read_csv('../input/train_identity.csv', index_col='TransactionID'))\ntest_identity = reduce_mem_usage(pd.read_csv('../input/test_identity.csv', index_col='TransactionID'))\n\nsample_submission = pd.read_csv('../input/sample_submission.csv', index_col='TransactionID')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def corret_card_id(x): \n    x=x.replace('.0','')\n    x=x.replace('-999','nan')\n    return x\n\ndef define_indexes(df):\n    \n    # create date column\n    START_DATE = '2017-12-01'\n    startdate = datetime.datetime.strptime(START_DATE, '%Y-%m-%d')\n    df['TransactionDT'] = df['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\n    \n    df['year'] = df['TransactionDT'].dt.year\n    df['month'] = df['TransactionDT'].dt.month\n    df['dow'] = df['TransactionDT'].dt.dayofweek\n    df['hour'] = df['TransactionDT'].dt.hour\n    df['day'] = df['TransactionDT'].dt.day\n   \n    # create card ID \n    cards_cols= ['card1', 'card2', 'card3', 'card5']\n    for card in cards_cols: \n        if '1' in card: \n            df['card_id']= df[card].map(str)\n        else : \n            df['card_id']+= ' '+df[card].map(str)\n    \n    # small correction of the Card_ID\n    df['card_id']=df['card_id'].apply(corret_card_id)\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = define_indexes(train)\ntest = define_indexes(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['TransactionAmt_to_mean_card1'] = train['TransactionAmt'] / train.groupby(['card1'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_mean_card4'] = train['TransactionAmt'] / train.groupby(['card4'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_std_card1'] = train['TransactionAmt'] / train.groupby(['card1'])['TransactionAmt'].transform('std')\ntrain['TransactionAmt_to_std_card4'] = train['TransactionAmt'] / train.groupby(['card4'])['TransactionAmt'].transform('std')\n\ntest['TransactionAmt_to_mean_card1'] = test['TransactionAmt'] / test.groupby(['card1'])['TransactionAmt'].transform('mean')\ntest['TransactionAmt_to_mean_card4'] = test['TransactionAmt'] / test.groupby(['card4'])['TransactionAmt'].transform('mean')\ntest['TransactionAmt_to_std_card1'] = test['TransactionAmt'] / test.groupby(['card1'])['TransactionAmt'].transform('std')\ntest['TransactionAmt_to_std_card4'] = test['TransactionAmt'] / test.groupby(['card4'])['TransactionAmt'].transform('std')\n\ntrain['id_02_to_mean_card1'] = train['id_02'] / train.groupby(['card1'])['id_02'].transform('mean')\ntrain['id_02_to_mean_card4'] = train['id_02'] / train.groupby(['card4'])['id_02'].transform('mean')\ntrain['id_02_to_std_card1'] = train['id_02'] / train.groupby(['card1'])['id_02'].transform('std')\ntrain['id_02_to_std_card4'] = train['id_02'] / train.groupby(['card4'])['id_02'].transform('std')\n\ntest['id_02_to_mean_card1'] = test['id_02'] / test.groupby(['card1'])['id_02'].transform('mean')\ntest['id_02_to_mean_card4'] = test['id_02'] / test.groupby(['card4'])['id_02'].transform('mean')\ntest['id_02_to_std_card1'] = test['id_02'] / test.groupby(['card1'])['id_02'].transform('std')\ntest['id_02_to_std_card4'] = test['id_02'] / test.groupby(['card4'])['id_02'].transform('std')\n\ntrain['D15_to_mean_card1'] = train['D15'] / train.groupby(['card1'])['D15'].transform('mean')\ntrain['D15_to_mean_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('mean')\ntrain['D15_to_std_card1'] = train['D15'] / train.groupby(['card1'])['D15'].transform('std')\ntrain['D15_to_std_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('std')\n\ntest['D15_to_mean_card1'] = test['D15'] / test.groupby(['card1'])['D15'].transform('mean')\ntest['D15_to_mean_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('mean')\ntest['D15_to_std_card1'] = test['D15'] / test.groupby(['card1'])['D15'].transform('std')\ntest['D15_to_std_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('std')\n\ntrain['D15_to_mean_addr1'] = train['D15'] / train.groupby(['addr1'])['D15'].transform('mean')\ntrain['D15_to_mean_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('mean')\ntrain['D15_to_std_addr1'] = train['D15'] / train.groupby(['addr1'])['D15'].transform('std')\ntrain['D15_to_std_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('std')\n\ntest['D15_to_mean_addr1'] = test['D15'] / test.groupby(['addr1'])['D15'].transform('mean')\ntest['D15_to_mean_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('mean')\ntest['D15_to_std_addr1'] = test['D15'] / test.groupby(['addr1'])['D15'].transform('std')\ntest['D15_to_std_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('std')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"one_value_cols = [col for col in train.columns if train[col].nunique() <= 1]\none_value_cols_test = [col for col in test.columns if test[col].nunique() <= 1]\n\nmany_null_cols = [col for col in train.columns if train[col].isnull().sum() / train.shape[0] > 0.9]\nmany_null_cols_test = [col for col in test.columns if test[col].isnull().sum() / test.shape[0] > 0.9]\n\nbig_top_value_cols = [col for col in train.columns if train[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\nbig_top_value_cols_test = [col for col in test.columns if test[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\n\ncols_to_drop = list(set(many_null_cols + many_null_cols_test + big_top_value_cols + big_top_value_cols_test + one_value_cols+ one_value_cols_test))\n\ncols_to_drop.remove('isFraud')\n\ntrain.drop(cols_to_drop, axis=1, inplace=True)\ntest.drop(cols_to_drop, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_transaction, train_identity, test_transaction, test_identity\n\ntarget = train['isFraud'].copy()\n\nX_train = train.drop('isFraud', axis=1)\nX_train.drop('TransactionDT', axis=1, inplace=True)\nX_test = test.drop('TransactionDT', axis=1)\n\ndel train, test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for f in X_train.select_dtypes(include='category').columns.tolist() + X_train.select_dtypes(include='object').columns.tolist():\n    lbl = LabelEncoder()\n    lbl.fit(list(X_train[f].values) + list(X_test[f].values))\n    X_train[f] = lbl.transform(list(X_train[f].values))\n    X_test[f] = lbl.transform(list(X_test[f].values))  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'num_leaves': 491,\n          'min_child_weight': 0.03454472573214212,\n          'feature_fraction': 0.3797454081646243,\n          'bagging_fraction': 0.4181193142567742,\n          'min_data_in_leaf': 106,\n          'objective': 'binary',\n          'max_depth': -1,\n          'learning_rate': 0.006883242363721497,\n          \"boosting_type\": \"gbdt\",\n          \"bagging_seed\": 11,\n          \"metric\": 'auc',\n          \"verbosity\": -1,\n          'reg_alpha': 0.3899927210061127,\n          'reg_lambda': 0.6485237330340494,\n          'random_state': 47\n         }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"splits = 5\nfolds = KFold(n_splits = splits)\noof = np.zeros(len(X_train))\npredictions = np.zeros(len(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train.values, target.values)):\n    print(\"Fold {}\".format(fold_))\n    train_df, y_train_df = X_train.iloc[trn_idx], target.iloc[trn_idx]\n    valid_df, y_valid_df = X_train.iloc[val_idx], target.iloc[val_idx]\n    \n    trn_data = lgb.Dataset(train_df, label=y_train_df)\n    val_data = lgb.Dataset(valid_df, label=y_valid_df)\n    \n    clf = lgb.train(params,\n                    trn_data,\n                    10000,\n                    valid_sets = [trn_data, val_data],\n                    verbose_eval=500,\n                    early_stopping_rounds=500)\n\n    pred = clf.predict(valid_df)\n    oof[val_idx] = pred\n    print( \"  auc = \", roc_auc_score(y_valid_df, pred) )\n    predictions += clf.predict(X_test) / splits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = sample_submission.reset_index()\nsample_submission[\"isFraud\"] = predictions\nsample_submission.to_csv(\"lgb_sub.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}