{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Import Libraries","execution_count":null},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import os\nimport gc\nimport datetime\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\nimport lightgbm as lgb\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Functions","execution_count":null},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\n\ndef import_data(file):\n    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n    df = pd.read_csv(file, parse_dates=True, keep_date_col=True, index_col='TransactionID')\n    df = reduce_mem_usage(df)\n    return df\n\n\ndef aggreg(columns, userid, aggr='mean'):\n    \"\"\"\n       Grouping the selected variables according to \"userids\", taking the averages and \n       assigning them to the new variable according to userids\n       \n    \"\"\"\n    \n    \n    for col in columns:\n        # create the new colunm name\n        new_col_name = col+'_'+userid+'_'+aggr \n        df_temp = pd.concat([X_train[[userid, col]], X_test[[userid,col]]]) \n        df_temp.loc[df_temp[col]==-1,col] = np.nan \n        \n        # grouping column by userid\n        df_temp = df_temp.groupby(userid)[col].agg([aggr]).reset_index().rename(columns={aggr: new_col_name})\n        df_temp.index = list(df_temp[userid]) \n        df_temp = df_temp[new_col_name].to_dict()  \n        \n        # Add these average values to Train and Test sets according to userid with the name \"new_col_name\"\n        X_train[new_col_name] = X_train[userid].map(df_temp).astype('float32')\n        X_test[new_col_name]  = X_test[userid].map(df_temp).astype('float32')\n        \n        # Writes -1 instead of \"nan\" values in newly created variables.\n        X_train[new_col_name].fillna(-1,inplace=True)\n        X_test[new_col_name].fillna(-1,inplace=True)\n      \n\n\n    \ndef aggreg_uniq(columns, userid):\n    \n    \"\"\"\n        Variables in columns are grouped by userid and unique values ​​in this column are counted and\n        the total number of each unique value is assigned across the \"userid\" in Test and Train sets.\n    \"\"\"\n    for col in columns:  \n        df = pd.concat([X_train[[userid,col]],X_test[[userid,col]]],axis=0)\n        uniq = df.groupby(userid)[col].agg(['nunique'])['nunique'].to_dict()\n        \n        X_train[col+'_count'] = X_train[userid].map(uniq).astype('float32')\n        X_test[col+'_count'] = X_test[userid].map(uniq).astype('float32')\n    \n    \ndef num_positiv(X_train,X_test):\n    \n    \"\"\"\n       We increase each value by the minimum value in the Train and Test set, so there is no negative value \n       and the minimum value becomes 0. The purpose in doing this is when we assign -1 to NAN values, \n       it can be perceived as a separate class.\n    \"\"\"\n    for f in X_train.columns:  \n        \n        if f not in ['TransactionAmt','TransactionDT',\"isFraud\"]: \n            mn = np.min((X_train[f].min(),X_test[f].min())) \n            X_train[f] -= np.float32(mn)  \n            X_test[f] -= np.float32(mn)\n            \n            X_train[f].fillna(-1,inplace=True)  \n            X_test[f].fillna(-1,inplace=True)  \n            \n\ndef class_freq(cols):\n    \"\"\" \n       The \"class_freq\" function normalizes the specified columns in the entered data sets,\n       converts their types to \"float32\" and adds them to the data sets as a new variable with \"_freq\" extension.\n    \"\"\"\n    \n    for col in cols:\n        df = pd.concat([X_train[col],X_test[col]])\n        vc = df.value_counts(dropna=True).to_dict()  \n        vc[-1] = -1  \n        nm = col+'_freq' \n        X_train[nm] = X_train[col].map(vc)  \n        X_test[nm] = X_test[col].map(vc) \n        del df; x=gc.collect()\n        \n\n        \ndef factorize_categoric():    \n    \n    \"\"\"\n       Factorizing process is performed for all categoric (object) variables, and\n       factorize function keeps nan values as -1.\n    \"\"\"\n    for col in X_train.select_dtypes(include=['category','object']).columns:\n        df = pd.concat([X_train[col],X_test[col]])\n        df,_ = df.factorize(sort=True)\n        X_train[col] = df[:len(X_train)].astype('int32')\n        X_test[col] = df[len(X_train):].astype('int32')\n        del df; x=gc.collect()        \n        \n\n        \n\ndef user_id(col1,col2):\n    \n    \"\"\"\n       Converts the values ​​in 2 columns to string and combines them \n       with \"_\" to create a string type new variable.\n       \n    \"\"\"\n    us_id = col1+'_'+col2\n    \n    X_train[us_id] = X_train[col1].astype(str)+'_'+X_train[col2].astype(str)\n    X_test[us_id] = X_test[col1].astype(str)+'_'+X_test[col2].astype(str)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nprint('Loading data...')\n\ntrain_id = import_data(\"../input/ieee-fraud-detection/train_identity.csv\")\nprint('\\tSuccessfully loaded train_identity!')\n\nX_train = import_data('../input/ieee-fraud-detection/train_transaction.csv')\nprint('\\tSuccessfully loaded train_transaction!')\nX_train = X_train.merge(train_id, how='left', left_index=True, right_index=True) \n\ntest_id = import_data('../input/ieee-fraud-detection/test_identity.csv')\nprint('\\tSuccessfully loaded test_identity!')\n\nX_test = import_data('../input/ieee-fraud-detection/test_transaction.csv')\nprint('\\tSuccessfully loaded test_transaction!')\n\ntest_id.columns = train_id.columns\nX_test = X_test.merge(test_id, how='left', left_index=True, right_index=True)  \n\npd.set_option('max_columns', None)\n\n# TARGET\ny_train = X_train['isFraud'].copy()  \n\nprint('Data was successfully loaded!\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Grouping the V-variables \n\nWe grouped the V variables according to the NaN values they contained and looked at the correlations of the V variables in the same group. we determined 70% as the correlation value.  Re-grouped those with the same correlations. We selected variables with the highest number of unique value among the variables with the same correlation and removed the others from the data set.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"nan_groups={}\nv_cols = ['V'+str(i) for i in range(1,340)]\nfor i in X_train.columns:\n    nan_sum = X_train[i].isna().sum()\n    try:\n        nan_groups[nan_sum].append(i)\n    except:\n        nan_groups[nan_sum]=[i]\n\nfor i,j in nan_groups.items():\n    print('The Sum of the NaN Values =',i)\n    print(j)\n    \n    \n\nnon_group_list=list()\nfor i,j in nan_groups.items():\n    if len(j)>5:\n        if i != 0:\n            non_group_list.append(i)\n            \n            \n# Variable groups with a correlation value of more than 0.70 within the groups\n\n# V1 - V11 \ngrp1 = [[1],[2,3],[4,5],[6,7],[8,9],[10,11]]\n# V12 - V34\ngrp2 = [[12,13],[14],[15,16,17,18,21,22,31,32,33,34],[19,20],[23,24],[25,26],[27,28],[29,30]]\n# V35 - V52\ngrp3 = [[35,36],[37,38],[39,40,42,43,50,51,52],[41],[44,45],[46,47],[48,49]]\n# V53 - V74\ngrp4 = [[53,54],[55,56],[57,58,59,60,63,64,71,72,73,74],[61,62],[65],[66,67],[68],[69,70]]\n# V74 - V94\ngrp5 = [[75,76],[77,78],[79,80,81,84,85,92,93,94],[82,83],[86,87],[88],[89],[90,91]]\n# V95 - V107\ngrp6 = [[95,96,97,101,102,103,105,106],[98],[99,100],[104]]\n# V107 - V123\ngrp7 = [[107],[108,109,110,114],[111,112,113],[115,116],[117,118,119],[120,122],[121],[123]]\n# V124 - V137\ngrp8 = [[124,125],[126,127,128,132,133,134],[129],[130,131],[135,136,137]]\n# V138 - V163\ngrp9 = [[138],[139,140],[141,142],[146,147],[148,149,153,154,156,157,158],[161,162,163]]\n# V167 - V183\ngrp10 = [[167,168,177,178,179],[172,176],[173],[181,182,183]]\n# V184 - V216\ngrp11 = [[186,187,190,191,192,193,196,199],[202,203,204,211,212,213],[205,206],[207],[214,215,216]]\n# V217 - V238\ngrp12 = [[217,218,219,231,232,233,236,237],[223],[224,225],[226],[228],[229,230],[235]]\n# V240 - V262\ngrp13 = [[240,241],[242,243,244,258],[246,257],[247,248,249,253,254],[252],[260],[261,262]]\n# V263 - V278\ngrp14 = [[263,265,264],[266,269],[267,268],[273,274,275],[276,277,278]]\n# V220 - V272\ngrp15 = [[220],[221,222,227,245,255,256,259],[234],[238,239],[250,251],[270,271,272]]\n# V279 - V299\ngrp16 = [[279,280,293,294,295,298,299],[284],[285,287],[286],[290,291,292],[297]]\n# V302 - V321\ngrp17 = [[302,303,304],[305],[306,307,308,316,317,318],[309,311],[310,312],[319,320,321]]\n# V281 V315\ngrp18 = [[281],[282,283],[288,289],[296],[300,301],[313,314,315]]\n# V322 - V339\ngrp19 = [[322,323,324,326,327,328,329,330,331,332,333],[325],[334,335,336],[337,338,339]]\n\n\ngrp_list = [grp1,grp2,grp3,grp4,grp5,grp6,grp7,grp8,grp9,grp10,\n            grp11,grp12,grp13,grp14,grp15,grp16,grp17,grp18,grp19]\n\n\n\n\ndef clip_group(group,df):\n    \"\"\"\n      Selects the higher number of unique values from the same correlated variables\n      \n    \"\"\"\n    clipped_list = []\n    for i in group:\n        maximum = 0; \n        V_num = i[0]\n        for j in i:\n            n = df['V'+str(j)].value_counts().count()\n            if n>maximum:\n                maximum = n\n                V_num = j\n            \n        clipped_list.append(V_num)\n    \n        \n    print('Variables in the clipped_list: ',clipped_list)\n    return clipped_list\n\n\n\n# V variables that were decided to be used in the model as a result of the correlation were kept in the V_clipped_cols variable.\nV_clipped_cols = list()\nfor i in grp_list:\n    for j in clip_group(i,X_train):\n        V_clipped_cols.append(\"V\"+str(j))\n        \n\nfor i in range (1, 339):\n    name = \"V\"+str(i)\n    if name not in V_clipped_cols:\n        X_train.drop(\"V\"+str(i),axis=1, inplace=True)\n        X_test.drop(\"V\"+str(i),axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering \n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Valid and invalid cards\nCards with less than 2 frequencies are defined as invalid cards while those with more than 2 were defined as valid. If a value is not common in the Train and Test set, these values were named as \"nan\". Finally, we named all invalid and \"nan\" values as \"nan\" while the others were valid.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_card = pd.concat([X_train[['card1']], X_test[['card1']]])\nvalid_card = valid_card['card1'].value_counts()\nvalid_card_std = valid_card.values.std()\n\ninvalid_cards = valid_card[valid_card<=2]\n\nvalid_card = valid_card[valid_card>2]\nvalid_card = list(valid_card.index)\n\nX_train['card1'] = np.where(X_train['card1'].isin(X_test['card1']), X_train['card1'], np.nan)\nX_test['card1']  = np.where(X_test['card1'].isin(X_train['card1']), X_test['card1'], np.nan)\n\nX_train['card1'] = np.where(X_train['card1'].isin(valid_card), X_train['card1'], np.nan)\nX_test['card1']   = np.where(X_test['card1'].isin(valid_card), X_test['card1'], np.nan)\n\n\n# Making values \"nan\" if a value is not common in the Train and Test set\nfor col in ['card2','card3','card4','card5','card6']: \n    X_train[col] = np.where(X_train[col].isin(X_test[col]), X_train[col], np.nan)\n    X_test[col]  = np.where(X_test[col].isin(X_train[col]), X_test[col], np.nan)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating Userid\nDetermining the person to whom each process belongs to similar values by converting the features to string and combining them with \"_\".","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"col_1 = 'card1'\ncol_2 = 'P_emaildomain'\ncol_3 = 'addr1'\n\n\nuser_id(col_1,col_2)\nuser_id(col_1+'_'+col_2,col_3)\nX_train.drop(col_1+'_'+col_2, axis = 1, inplace=True)\nX_test.drop(col_1+'_'+col_2, axis = 1, inplace=True)\n\nus_id = col_1 + '_' + col_2 + '_' + col_3\nX_train.rename(columns={us_id: 'userid'}, inplace=True)\nX_test.rename(columns={us_id: 'userid'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Extracting browser and versions\nIt separates the browser, device and versions of the processes and assigns them to new variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in [X_train,X_test]:\n\n    df['OS_id_30'] = df['id_30'].str.split(' ', expand=True)[0]\n    df['version_id_30'] = df['id_30'].str.split(' ', expand=True)[1]\n\n    df['browser_id_31'] = df['id_31'].str.split(' ', expand=True)[0]\n    df['version_id_31'] = df['id_31'].str.split(' ', expand=True)[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Standardization of TransactionAmt\nConverts the type of TransactionAmt to float32. Subtracts the mean from the values in TransactionAmt and divides it by its standard deviation. In this way, standardization has been made.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in [X_train,X_test]:\n\n    df['TransactionAmt'] = df['TransactionAmt'].astype('float32')\n    df['Trans_min_std'] = (df['TransactionAmt'] - df['TransactionAmt'].mean()) / df['TransactionAmt'].std()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Latest Version Control\nChecking the devices if they have the latest version or not. If they use the latest version, they are assigned as 1 otherwise 0.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train[\"lastest_browser\"] = np.zeros(X_train.shape[0])\nX_test[\"lastest_browser\"] = np.zeros(X_test.shape[0])\n\ndef setBrowser(df):\n    \n    df.loc[df[\"id_31\"]==\"samsung browser 7.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"opera 53.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"mobile safari 10.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"google search application 49.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"firefox 60.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"edge 17.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 69.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 67.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 63.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 63.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0 for ios\",'lastest_browser']=1\n    return df\n\nX_train=setBrowser(X_train)\nX_test=setBrowser(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Country Extraction\n\nDetermining the countries where the transactions are made according to the mail extensions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"us_emails = ['gmail', 'net', 'edu']\n\nfor df in [X_train,X_test]:\n    for c in ['P_emaildomain', 'R_emaildomain']:\n\n        df[c + '_suffix'] = df[c].map(lambda x: str(x).split('.')[-1])\n        df[c + '_suffix'] = df[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting foreign countries by exchange rate\nfor df in [X_train,X_test]:\n    \n    df['TransactionAmt_decimal_lenght'] = df['TransactionAmt'].astype(str).str.split('.', expand=True)[1].str.len()\n    df['cents'] = (df['TransactionAmt'] - np.floor(X_train['TransactionAmt'])).astype('float32')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Matching Receiver and Purchaser Email Domains\n\nR_emaildomain and p_emaildomain were compared. If P_emaildomain and R_emaildomain are the same and are not null, we assign 1. Otherwise we assign 0.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"p = 'P_emaildomain'\nr = 'R_emaildomain'\nunknown = 'email_not_provided'\n\ndef setDomain(df):\n    df[p] = df[p].astype('str')\n    df[r] = df[r].astype('str')\n    \n    df[p] = df[p].fillna(unknown)\n    df[r] = df[r].fillna(unknown)\n    \n    df['email_check'] = np.where((df[p]==df[r])&(df[p]!=unknown),1,0)\n\n    df[p+'_prefix'] = df[p].apply(lambda x: x.split('.')[0])\n    df[r+'_prefix'] = df[r].apply(lambda x: x.split('.')[0])\n    \n    return df\n    \nX_train=setDomain(X_train)\nX_test=setDomain(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Defining Time Variable and US Holidays","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Listing dates between '2017-10-01' and '2019-01-01 \ndates_range = pd.date_range(start='2017-10-01', end='2019-01-01')\n\n# US national holidays are listed between '2017-10-01' and '2019-01-01 \nus_holidays = calendar().holidays(start=dates_range.min(), end=dates_range.max())\n\nSTART_DATE = '2017-12-01'\nstartdate = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\")\n\n\n# The variable of the hour of day, day of week and day of month and month of year were created.\nfor df in [X_train,X_test]:\n    \n    df[\"Date\"] = df['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\n    df['_Weekdays'] = df['Date'].dt.dayofweek\n    df['_Dayhours'] = df['Date'].dt.hour\n    df['_Monthdays'] = df['Date'].dt.day\n    df['_Yearmonths'] = (df['Date'].dt.month).astype(np.int8) \n\n    \n    # Is the transaction done on holiday?\n    df['is_holiday'] = (df['Date'].dt.date.astype('datetime64').isin(us_holidays)).astype(np.int8)\n\n    df.drop(\"Date\", axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Mean Encoding\n\nFraud ratios with respect to ProductCD and M4 categories. By doing this we used the target variable as the basis to generate the new encoded feature. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ['ProductCD','M4']:\n    temp_dict = X_train.groupby([col])['isFraud'].agg(['mean']).reset_index().rename(columns={'mean': col+'_target_mean'})\n    temp_dict.index = temp_dict[col].values\n    temp_dict = temp_dict[col+'_target_mean'].to_dict()\n    \n    if col=='ProductCD':\n        X_train['ProductCD_1'] = X_train[col].map(temp_dict)\n        X_test['ProductCD_1']  = X_test[col].map(temp_dict)\n    else:\n        X_train['M4_1'] = X_train[col].map(temp_dict)\n        X_test['M4_1']  = X_test[col].map(temp_dict)\n        \n        \n# Dropping 'ProductCD' and 'M4'\nX_train.drop(['ProductCD','M4'], axis=1,inplace=True)\nX_test.drop(['ProductCD','M4'], axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Rescaling D columns\n\nThe D Columns are \"time deltas\" belong to the past. Therfore they are normalized to  transform the D Columns into their point in the past.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(1,16):\n    if i in [1,2,3,5,9]:\n        continue\n    X_train['D'+str(i)] =  X_train['D'+str(i)] - X_train.TransactionDT/np.float32(24*60*60)\n    X_test['D'+str(i)] = X_test['D'+str(i)] - X_test.TransactionDT/np.float32(24*60*60)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Rolling Window Aggregations of Last Transactions\n\nRolling window aggregations of window size 10 for the transaction amount are performed using these formulas. These \nfeatures are important as they give almost all the necessary information about the distribution of the user’s last 10\ntransaction amounts. Setting minimum acceptable window size as 1 provides us not to have too many missing values for \nthese features, as there is the observation itself in case all the previous observations are missing.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in [X_train,X_test]:\n\n    df['mean_last'] = df['TransactionAmt'] - df.groupby('userid')['TransactionAmt'].transform(lambda x: x.rolling(10, 1).mean())\n    df['min_last'] = df.groupby('userid')['TransactionAmt'].transform(lambda x: x.rolling(10, 1).min())\n    df['max_last'] = df.groupby('userid')['TransactionAmt'].transform(lambda x: x.rolling(10, 1).max())\n    df['std_last'] = df['mean_last'] / df.groupby('userid')['TransactionAmt'].transform(lambda x: x.rolling(10, 1).std())\n\n    df['mean_last'].fillna(0, inplace=True, )\n    df['std_last'].fillna(0, inplace=True)\n\n    df['TransactionAmt_to_mean_card_id'] = df['TransactionAmt'] - df.groupby(['userid'])['TransactionAmt'].transform('mean')\n    df['TransactionAmt_to_std_card_id'] = df['TransactionAmt_to_mean_card_id'] / df.groupby(['userid'])['TransactionAmt'].transform('std')\n    \n    \n    # Replaces infinite values with 999\n    df = df.replace(np.inf,999)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating New Features by Using Aggregation Functions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"factorize_categoric()\n\nnum_positiv(X_train,X_test)\n\nclass_freq(['addr1','card1','card2','card3','P_emaildomain'])\n\naggreg(['TransactionAmt','D4','D9','D10','D11','D15'],'userid','mean')\n\naggreg(['TransactionAmt','D4','D9','D10','D11','D15','C14'],'userid','std')\n\naggreg(['C'+str(x) for x in range(1,15) if x!=3],'userid','mean')\n\naggreg(['M'+str(x) for x in range(1,10) if x!=4],'userid','mean')\n\naggreg_uniq(['P_emaildomain','dist1','id_02','cents','C13','V314','V127','V136','V309','V307','V320'],'userid')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"START_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')\nX_train['DT_M'] = X_train['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\nX_train['DT_M'] = (X_train['DT_M'].dt.year-2017)*12 + X_train['DT_M'].dt.month \n\nX_test['DT_M'] = X_test['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\nX_test['DT_M'] = (X_test['DT_M'].dt.year-2017)*12 + X_test['DT_M'].dt.month ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reducing the memory usage\nX_train = reduce_mem_usage(X_train)\nX_test = reduce_mem_usage(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping userid to prevent overfitting\nX_train.drop(\"userid\", axis=1, inplace=True)\nX_test.drop(\"userid\", axis=1, inplace=True)\n\n# Dropping target variable from Train set\nX_train.drop(\"isFraud\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model  and Hyperparameters Tuning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting Train set\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X_train, y_train, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parameter Tuning\nmodel = lgb.LGBMClassifier()\nparam_dist = {\"max_depth\": [5,10,15],\n              \"learning_rate\" : [0.1,0.15,0.3],\n              \"num_leaves\": [32,150,200],\n              \"n_estimators\": [300,400],\n              'is_unbalance': [True],\n              'boost_from_average': [False],\n              'device': ['gpu'],\n              'gpu_platform_id': [0],\n              'gpu_device_id': [0],\n              \"random_state\": [2]}\n\n\ngrid_search = GridSearchCV(model, param_grid=param_dist, cv = 3, \n                                   verbose=10, n_jobs=-1)\n\n\n\n\ngrid_search.fit(X_train1, y_train1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best values of the hyperparameters\nmax_depth_best = grid_search.best_estimator_.max_depth\nnum_leaves_best = grid_search.best_estimator_.num_leaves\nn_estimators_best = grid_search.best_estimator_.n_estimators\nlearning_rate_best = grid_search.best_estimator_.learning_rate","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del model \nx=gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_train = X_train.drop(X_train.index[590520:])\n#y_train_drop = y_train.drop(y_train.index[590520:])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"groups = X_train['DT_M']\nkf = KFold(n_splits=6, random_state=42)\nkf.get_n_splits(X_train, y_train, groups)\n\npreds = np.zeros(len(X_test))\ncount = 0\n\nfor train_index, test_index in kf.split(X_train, y_train, groups):\n    print(\"\\nTRAIN:\", train_index, \"TEST:\", test_index)\n    X_train_df, X_test_df = X_train.iloc[train_index], X_train.iloc[test_index]\n    y_train_df, y_test_df = y_train.iloc[train_index], y_train.iloc[test_index]\n    print(X_train_df.shape)\n    print(X_test_df.shape)\n    \n    \n    \n    clf = lgb.LGBMClassifier(max_depth = max_depth_best,\n                          num_leaves = num_leaves_best, \n                          n_estimators = n_estimators_best,\n                          n_jobs = -1 , \n                          verbose = 1,\n                          learning_rate= learning_rate_best,\n                          eval_metric='auc',\n                          nthread=4,\n                          is_unbalance = True,\n                          boost_from_average = False,\n                          device = 'gpu',\n                          gpu_platform_id = 0,\n                          gpu_device_id = 0\n                          )\n    \n    h = clf.fit(X_train_df, y_train_df, eval_set=[(X_test_df,y_test_df)],verbose=100, early_stopping_rounds=200)\n    \n   \n    preds += clf.predict_proba(X_test)[:,1]/kf.n_splits\n    \n \n    \n    count = count + 1\n    if count <=5:\n        del h, clf\n    \n        x = gc.collect()\nprint('#'*20)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion matrix and Classification report\npred1 =  clf.predict(X_test_df)\nfpr, tpr, thresholds = metrics.roc_curve(y_test_df, pred1, pos_label=2)\n\nprint(metrics.auc(fpr, tpr))\n\nprint(metrics.confusion_matrix(y_test_df, pred1))\nprint(metrics.classification_report(y_test_df, pred1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submitting the Predicted Values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv('../input/ieee-fraud-detection/sample_submission.csv')\nsample_submission.isFraud = preds\nsample_submission.to_csv('sub_lgbm_kfold.csv',index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}