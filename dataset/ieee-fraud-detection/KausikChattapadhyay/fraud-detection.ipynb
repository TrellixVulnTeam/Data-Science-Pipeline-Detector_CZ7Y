{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_identity = pd.read_csv('/kaggle/input/ieee-fraud-detection/train_identity.csv',index_col='TransactionID')\ntest_identity  = pd.read_csv('/kaggle/input/ieee-fraud-detection/test_identity.csv',index_col='TransactionID')\ntrain_transaction = pd.read_csv('/kaggle/input/ieee-fraud-detection/train_transaction.csv',index_col='TransactionID')\ntest_transaction  = pd.read_csv('/kaggle/input/ieee-fraud-detection/test_transaction.csv',index_col='TransactionID')\ntrain_identity.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of Train_identity:: \", train_identity.shape)\nprint(\"Shape of Test_identity:: \", test_identity.shape)\nprint(\"Shape of Train_transaction:: \", train_transaction.shape)\nprint(\"Shape of Test_transaction:: \", test_transaction.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction.reset_index()['TransactionID'].isin(train_identity.reset_index()['TransactionID']).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_identity.reset_index()['TransactionID'].isin(train_transaction.reset_index()['TransactionID']).value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data is broken into two files identity and transaction, which are joined by TransactionID. Not all transactions have corresponding identity information."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pd.merge(train_transaction,\n             train_identity,\n             on='TransactionID',\n             how='left')\nprint(\"train_transaction dimensions: {} \".format(train_transaction.shape))\nprint(\"train_identity dimensions:    {} \".format(train_identity.shape))\nprint(\"Merged X dimensions:          {} \".format(X.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = X.isFraud\nX = X.reset_index().drop('isFraud', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Fraud Transaction Count is {}\".format(Y[Y==1].count()))\nprint(\"non-Fraud Transaction Count is {}\".format(Y[Y==0].count()))\nprint(\"% Fraud Transaction Count is {}\".format((Y[Y==1].count()/Y.count())*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y.dropna().plot(kind='hist',bins=10)\nplt.xlabel('ISFraud')\nplt.title('ISFRAUD Histogram Plot')\nplt.xlim(0,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting bthe data to train and validation set\nX_train, X_valid, y_train, y_valid = train_test_split(X, Y, test_size=0.20, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"X_train dimensions:  {}\".format(X_train.shape))\nprint(\"y_train dimensions:  {}\".format(y_train.shape))\nprint(\"X_valid dimensions:  {}\".format(X_valid.shape))\nprint(\"y_valid dimensions:  {}\".format(y_valid.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checking Numerical and Categorical Columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_columns = list(X_train.select_dtypes(exclude='object').columns)\ncategorical_columns = list(X_train.select_dtypes(include='object').columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like some fields say card1 - card6, addr1, addr2 are showing as numeric. Need to convert to categorical.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = ['card1', 'card2', 'card3', 'card5', 'addr1', 'addr2', 'ProductCD',\n 'card4',\n 'card6',\n 'P_emaildomain',\n 'R_emaildomain',\n 'M1',\n 'M2',\n 'M3',\n 'M4',\n 'M5',\n 'M6',\n 'M7',\n 'M8',\n 'M9',\n 'id_12',\n 'id_15',\n 'id_16',\n 'id_23',\n 'id_27',\n 'id_28',\n 'id_29',\n 'id_30',\n 'id_31',\n 'id_33',\n 'id_34',\n 'id_35',\n 'id_36',\n 'id_37',\n 'id_38',\n 'DeviceType',\n 'DeviceInfo']\n\nfor col in columns:\n    X_train[col] = X_train[col].astype('category')\n    X_valid[col] = X_valid[col].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_columns = list(X_train.select_dtypes(exclude='category').columns)\ncategorical_columns = list(X_train.select_dtypes(include='category').columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"white\")\n\ndf_corr= X_train[['C1','C2', 'C3', 'C4','C5','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14']]\n\n# Compute the correlation matrix\ncorr = df_corr.dropna().corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\n#mask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(30, 10))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, square=True, annot=True,linewidths=.5, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"C1-C14 features are highly corelated between them. We can either eliminate the corelated features or combined them."},{"metadata":{},"cell_type":"markdown","source":"How V-feature is?"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.columns[54:393]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"V features have a lot of similar columns.\n\nLet's look at it.\n\nThis is too big table, I am hiding it."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.iloc[:,54:393].corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature engineering\nI picked up these 3 pairs of columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.loc[:,[\"V319\",\"V320\"]].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.loc[:,[\"V109\",\"V110\"]].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.loc[:,[\"V329\",\"V330\"]].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.loc[:,[\"V316\",\"V331\"]].corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"They seem to be similar."},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"diff_V319_V320\"] = np.zeros(train.shape[0])\n\ntrain.loc[train[\"V319\"]!=train[\"V320\"],\"diff_V319_V320\"] = 1\n\ntest[\"diff_V319_V320\"] = np.zeros(test.shape[0])\n\ntest.loc[test[\"V319\"]!=test[\"V320\"],\"diff_V319_V320\"] = 1\n\ntrain[\"diff_V109_V110\"] = np.zeros(train.shape[0])\n\ntrain.loc[train[\"V109\"]!=train[\"V110\"],\"diff_V109_V110\"] = 1\n\ntest[\"diff_V109_V110\"] = np.zeros(test.shape[0])\n\ntest.loc[test[\"V109\"]!=test[\"V110\"],\"diff_V109_V110\"] = 1\n\ntrain[\"diff_V329_V330\"] = np.zeros(train.shape[0])\n\ntrain.loc[train[\"V329\"]!=train[\"V330\"],\"diff_V329_V330\"] = 1\n\ntest[\"diff_V329_V330\"] = np.zeros(test.shape[0])\n\ntest.loc[test[\"V329\"]!=test[\"V330\"],\"diff_V329_V330\"] = 1\n\n\ntrain[\"diff_V316_V331\"] = np.zeros(train.shape[0])\n\ntrain.loc[train[\"V331\"]!=train[\"V316\"],\"diff_V316_V331\"] = 1\n\ntest[\"diff_V316_V331\"] = np.zeros(test.shape[0])\n\ntest.loc[test[\"V316\"]!=test[\"V331\"],\"diff_V316_V331\"] = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## look at appearance of this new feature\nV319-V320"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.bar(train.groupby(\"diff_V319_V320\").mean().isFraud.index,train.groupby(\"diff_V319_V320\").mean().isFraud.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"V109-V110"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.bar(train.groupby(\"diff_V109_V110\").mean().isFraud.index,train.groupby(\"diff_V109_V110\").mean().isFraud.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.bar(train.groupby(\"diff_V329_V330\").mean().isFraud.index,train.groupby(\"diff_V329_V330\").mean().isFraud.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.bar(train.groupby(\"diff_V316_V331\").mean().isFraud.index,train.groupby(\"diff_V316_V331\").mean().isFraud.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There seem to be difference, but the gap of \"diff_V109_V110\" is small.\n\nI found that \"diff_V109_V110\" is not meaningful. I deleted.\n\nI found that \"diff_V329_V330\" is not meaningful. I deleted."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(\"diff_V109_V110\",axis=1)\ntest = test.drop(\"diff_V109_V110\",axis=1)\n\ntrain = train.drop(\"diff_V329_V330\",axis=1)\ntest = test.drop(\"diff_V329_V330\",axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Missing Value Check\nX_train[['D1','D2', 'D3', 'D4','D5','D5','D6','D7','D8','D9','D10','D11','D12','D13','D14','D15']].isnull().sum()/X_train.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train['ProductCD'].dropna().plot(kind='hist',bins=10)\nplt.xlabel('ProductCD')\nplt.title('ProductCD Histogram Plot')\nplt.xlim(0,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train[0:60000].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"white\")\n# Compute the correlation matrix\ncorr = train_transaction.reset_index()[numeric_columns].dropna().corr()\n\ncorr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Missing value check and preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Numeric Missing Numbers: {}\".format(X_train[numeric_columns].isnull().sum()[X_train[numeric_columns].isnull().sum() > 0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Categorical Missing Numbers: {}\".format(X_train[categorical_columns].isnull().sum()[X_train[categorical_columns].isnull().sum() > 0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cardinality check--Unique values for each categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train[categorical_columns].nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will onehotencoding for low cardinal(<10) features, rests will be encoded with labelencoder. Now we need to make sure unique values are same between train and validation set."},{"metadata":{"trusted":true},"cell_type":"code","source":"good_category_columns  = [col for col in categorical_columns if set(X_train[col]) == set(X_valid[col])]\nbad_category_columns  = list(set(categorical_columns) - set(good_category_columns))\nbad_category_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Seperating Low cardinal categorical features.\ncategorical_cols_low = [cname for cname in categorical_columns if\n                    X_train[cname].nunique() < 10 and \n                    X_train[cname].dtype == \"category\"]\ncategorical_cols_high = list(set(categorical_columns) - set(categorical_cols_low))\n\nprint(\"Low cardinal categorical features:  {}\".format(categorical_cols_low))\nprint(\"High cardinal categorical features:  {}\".format(categorical_cols_high))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Keep selected columns only\nmy_cols = categorical_cols_low + numeric_columns\nX_train_1 = X_train[my_cols].copy()\nX_valid_1 = X_valid[my_cols].copy()\n#X_test = X_test[my_cols].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='constant')\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numeric_columns),\n        ('cat', categorical_transformer, categorical_cols_low)\n    ])\n\n# Define model\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\n\n# Bundle preprocessing and modeling code in a pipeline\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', model)\n                     ])\n\n# Preprocessing of training data, fit model \nclf.fit(X_train_1, y_train)\n\n# Preprocessing of validation data, get predictions\npreds = clf.predict(X_valid_1)\n\nprint('R2 Test:',  r2_score(y_valid, preds))\nprint('R2 Train:', r2_score(y_train, clf.predict(X_train)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"data cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_transaction, train_identity, test_transaction, test_identity","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)\nprint(test.shape)\n\ny_train = train['isFraud'].copy()\n\n# Drop target, fill in NaNs\nX_train = train.drop('isFraud', axis=1)\nX_test = test.copy()\nX_train = X_train.fillna(-999)\nX_test = X_test.fillna(-999)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train, test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Label Encoding\nWe cannot use literal features for XGB, so these features are changes.\n\nFor example, [H,G,W,A] →[0,1,2,3]\n\nThe number of the words is often related to the numeral([0,1,2,3])."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\n# Label Encoding\nfor f in X_train.columns:\n    if X_train[f].dtype=='object' or X_test[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(X_train[f].values) + list(X_test[f].values))\n        X_train[f] = lbl.transform(list(X_train[f].values))\n        X_test[f] = lbl.transform(list(X_test[f].values)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nfrom xgboost import XGBClassifier\nclf = xgb.XGBClassifier(n_estimators=500,\n                        n_jobs=4,\n                        max_depth=9,\n                        learning_rate=0.05,\n                        subsample=0.9,\n                        colsample_bytree=0.9,\n                        missing=-999)\n\nclf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting the Test set results\n%time y_pred = classifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\ncm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Explained variance score: 1 is perfect prediction\n%time print('Variance score: %.2f' % classifier.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['isFraud'] = clf.predict_proba(X_test)[:,1]\nsample_submission.to_csv('simple_xgboost.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}