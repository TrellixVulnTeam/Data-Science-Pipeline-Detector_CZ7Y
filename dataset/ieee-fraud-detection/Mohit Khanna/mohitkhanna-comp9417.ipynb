{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Loading Libraries and Data","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os,gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_transaction = pd.read_csv('../input/ieee-fraud-detection/train_transaction.csv')\ntest_transaction = pd.read_csv('../input/ieee-fraud-detection/test_transaction.csv')\n\n# train_id = pd.read_csv('../input/ieee-fraud-detection/train_identity.csv')\n# test_id = pd.read_csv('../input/ieee-fraud-detection/test_identity.csv')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = pd.read_csv('../input/ieee-fraud-detection/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_transaction.shape,test_transaction.shape)\n# print(train_id.shape, test_id.shape)\nprint(sample.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Size of both train and test data is comparable. \n2. Number of features is pretty high.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## DATA PREPROCESSING","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction.info(max_cols=400)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are many columns with large number of missing values.\nWe can drop columns with more than 50% missing values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_to_drop = []\nnum_of_rows = train_transaction.shape[0]\nfor i in train_transaction.columns:\n    count_of_null_values = train_transaction[i].isna().sum()\n    if (count_of_null_values >= num_of_rows/2):\n        columns_to_drop.append(i)\ndel num_of_rows        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping columns with more than 50% missing values.\ntrain_transaction.drop(columns_to_drop, axis=1, inplace=True)\ntest_transaction.drop(columns_to_drop, axis=1, inplace=True)\n\nprint(\"No of columns dropped {}\".format(len(columns_to_drop)))\ndel columns_to_drop\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's preprocess the object type columns first.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"object_columns = train_transaction.select_dtypes(include=object).columns\nprint(\"Number of categorical columns: {}\".format(len(object_columns)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in object_columns:\n    print(\"Column Name : {}\".format(i))\n    print(\"-------------> No of missing values: {}\".format(train_transaction[i].isna().sum()))\n    print(\"-------------> Unique values: {}\".format(train_transaction[i].unique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some insights:\n1. Categorical columns with no missing values : ProductCD\n2. Categorical columns with few missing values : card4,card6\n3. Categorical columns with many missing values : P_emaildomain, M6\n4. Categorical columns with huge number of missing values : M1,M2,M3,M4\n\nLets plot the value counts graphs for these columns and see if we can fill the missing values with the mode value.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(3,3,figsize=(18,18))\nfor k,i in enumerate(object_columns):\n    plt.subplot(3,3,k+1)\n    if(i != 'P_emaildomain'):\n        train_transaction[i].value_counts().plot(kind='bar')\n    else:\n        prob = train_transaction[i].value_counts(normalize=True)\n        threshold = 0.02\n        mask = prob > threshold\n        tail_prob = prob.loc[~mask].sum()\n        prob = prob.loc[mask]\n        prob['other'] = tail_prob\n        prob.plot(kind='bar')\n    plt.title(i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are many interesting things here,\n1. 'W' is the ProductCD in over 400,000 transactions.\n2. card4 type is 'visa' in over 350,000 transactions and 'mastercard' in 200,000 transactions. Other types are rare.\n3. card6 value is 'debit' in approx. 430,000 transactions and 'credit' in approx. 150,000 transactions. Other values are extremely rare.\n4. P_emaildomain type in 'google.com' in over 40% of the transactions and 'yahoo.com' in 20% of the transactions. Some values are comparatively less. Most values are rare.\n5. For M2 and M3, 'T' is the category for most transactions.\n6. For M1, 'F' category is extremely rare.\n7. In case of M4, 'M0' is the most occurred value.\n8. In M6, both 'T' and 'F' occur almost equally. Still, we will fill the missing values with 'F' as it is the mode.\n\nFrom these insights, we can safely fill the missing values with the mode of the object columns.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now, plotting the object columns from test transactions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(3,3,figsize=(18,18))\nfor k,i in enumerate(object_columns):\n    plt.subplot(3,3,k+1)\n    if(i != 'P_emaildomain'):\n        test_transaction[i].value_counts().plot(kind='bar')\n    else:\n        prob = test_transaction[i].value_counts(normalize=True)\n        threshold = 0.02\n        mask = prob > threshold\n        tail_prob = prob.loc[~mask].sum()\n        prob = prob.loc[mask]\n        prob['other'] = tail_prob\n        prob.plot(kind='bar')\n    plt.title(i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Above insights hold true for test data too.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in object_columns:\n    train_transaction[i].fillna(train_transaction[i].mode()[0], inplace=True)\n    test_transaction[i].fillna(test_transaction[i].mode()[0], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's see categorical features with numeric values.\n[From the competition host](https://www.kaggle.com/c/ieee-fraud-detection/discussion/101203)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Categorical Features:\n# ProductCD\n# card1 - card6\n# addr1, addr2\n# Pemaildomain Remaildomain\n# M1 - M9\n\n# We handles few M* features above, others were dropped because of >50% missing values.\ncat_num_features = ['addr1','addr2', 'card1', 'card2', 'card3', 'card5']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in cat_num_features:\n    print(\"Column Name : {}\".format(i))\n    print(\"-------------> No of missing values: {}\".format(train_transaction[i].isna().sum()))\n    print(\"Mode value {} occurred in {} transactions \\n\".format(train_transaction[i].mode()[0], train_transaction[i].value_counts().values[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filling the missing values with mode.\nfor i in cat_num_features:\n    train_transaction[i].fillna(train_transaction[i].mode()[0], inplace=True)\n    test_transaction[i].fillna(test_transaction[i].mode()[0], inplace=True)\ndel cat_num_features\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's create a list of numerical features with missing values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"all_numeric_columns = train_transaction.select_dtypes(include=np.number).columns\nnumeric_missing = []\nfor i in all_numeric_columns:\n    missing = train_transaction[i].isna().sum()\n    if(missing>0):\n        numeric_missing.append(i)\ndel all_numeric_columns        \nprint(len(numeric_missing))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction[numeric_missing].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here were can see that for most V_ columns, median and mode values are same.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for k,i in enumerate(numeric_missing):\n    print(k)\n    print(\"Column {} has {} missing values\".format(i, train_transaction[i].isna().sum()))\n    print(\"Mode value {} occurred in {} transactions\".format(train_transaction[i].mode()[0], train_transaction[i].value_counts().values[0]))\n    print(\"Median value {} \\n\".format(train_transaction[i].median(), train_transaction[i].value_counts().values[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filling the missing values with median.\nfor i in numeric_missing:\n    train_transaction[i].fillna(train_transaction[i].median(), inplace=True)\n    test_transaction[i].fillna(test_transaction[i].median(), inplace=True)\nprint(train_transaction.isna().any().sum(), test_transaction.isna().any().sum())   \ndel numeric_missing\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, there are no missing values in our data. Let's start with handling categorical features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# object_columns\nfor f in object_columns:\n    lbl = LabelEncoder()\n    lbl.fit(list(train_transaction[f].values) + list(test_transaction[f].values))\n    train_transaction[f] = lbl.transform(list(train_transaction[f].values))\n    test_transaction[f] = lbl.transform(list(test_transaction[f].values))    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction[object_columns].head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_transaction[object_columns].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_transaction.select_dtypes(exclude=np.number).sum())\ndel object_columns\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, all columns in the data have numeric values.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## EDA","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now that our data contains no missing values and no categorical values. We can start plotting some graphs to get intuition about the data. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here,\n1. Mean of the 'isFraud' column is 0.034, this tells us that the no. of 0s in the columns is way greater than the number of 1s.\n2. In most of V_ columns, the max value is way greater than the mean and median. Outliers are present.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's plot the histogram of isFraud column.\ntrain_transaction['isFraud'].plot(kind='hist')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data is highly imbalanced. We can downsample the '0' class or upsample '1' class, but for now let's continue with the imbalanced data and check the score.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Number of features is over 200, we can't plot pairplots or heatmaps, It will take up all of kernel's ram. Let's skip to training and get a baseline score.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train,X_val,y_train,y_val = train_test_split(train_transaction.drop(['isFraud'],axis=1), train_transaction['isFraud'], test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_transaction.to_csv(\"train_transaction.csv\",sep= ',',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_transaction.to_csv(\"test_transaction.csv\",sep= ',',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_train = train_transaction['isFraud']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train_transaction.drop(['isFraud'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = test_transaction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I prefer manual tuning for the baseline model, you can use Hyperopt for hyperparamer-tuning.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Score= .7427\nparams = {\n    'objective': 'binary',\n    'n_estimators':300,\n    'learning_rate': 0.1,\n    'subsample':0.8\n}\n# Score= .7306\nparams1 = {\n    'objective': 'binary',\n    'n_estimators': 200,\n    'learning_rate': 0.1,\n}\n#Score= .7446\nparams2 = {\n    'objective': 'binary',\n    'n_estimators':300,\n    'learning_rate': 0.1,\n}\n# Score=.774\nparams3 = {\n    'objective': 'binary',\n    'n_estimators':600,\n    'learning_rate': 0.1\n}\n#Score= .7666\nparams4 = {\n    'objective': 'binary',\n    'n_estimators':500,\n    'learning_rate': 0.1\n}\n#Score= .7711\nparams5 = {\n    'objective': 'binary',\n    'n_estimators':500,\n    'learning_rate': 0.1,\n    'num_leaves' : 50,\n    'max_depth' : 7,\n    'subsample' : 0.9,\n    'colsample_bytree' : 0.9\n}\n#Score=.78109\nparams6 = {\n    'objective': 'binary',\n    'n_estimators':600,\n    'learning_rate': 0.1,\n    'num_leaves' : 50,\n    'max_depth' : 7,\n    'subsample' : 0.9,\n    'colsample_bytree' : 0.9\n}\n#Score=.7863\nparams7 = {\n    'objective': 'binary',\n    'n_estimators':700,\n    'learning_rate': 0.1,\n    'num_leaves' : 50,\n    'max_depth' : 7,\n    'subsample' : 0.9,\n    'colsample_bytree' : 0.9\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = LGBMClassifier(**params7, random_state=108)\nclf.fit(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# preds = clf.predict(X_val)\n# roc_auc_score(y_val, preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = clf.predict_proba(X_test)\nsample['isFraud'] = predictions[:,1]\nsample.to_csv('submission_lgbm.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_clf = SVC(C=1000, kernel=\"linear\", probability=True, gamma = 100, random_state=108, decision_function_shape='ovo')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_clf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_clf.fit(X_train,Y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = svm_clf.predict_proba(X_test)\nsample['isFraud'] = predictions[:,1]\nsample.to_csv('submission_svm.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Code below can be used to download the .csv generated in your Kaggle kernel, this way you can submit the predictions without having to commit the kernel again and again. ![Thanks to Rachel](https://www.kaggle.com/rtatman/download-a-csv-file-from-a-kernel)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# def create_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"):  \n#     csv = df.to_csv(index=False)\n#     b64 = base64.b64encode(csv.encode())\n#     payload = b64.decode()\n#     html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n#     html = html.format(payload=payload,title=title,filename=filename)\n#     return HTML(html)\n# create_download_link(sample)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_transaction = pd.read_csv('../input/cleandata/train_transaction.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_transaction = pd.read_csv('../input/cleandata/test_transaction.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators = 1000,random_state=121,min_samples_split = 2, bootstrap = False, max_depth = 5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf.fit(X_train,Y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.fillna(999, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = rf.predict_proba(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sample = pd.read_csv('../input/ieee-fraud-detection/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample['isFraud'] = predictions[:,1]\nsample.to_csv('submission_rf.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import tree\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DTree = tree.DecisionTreeClassifier(random_state=0, criterion='entropy',max_depth=8,splitter='best', min_samples_split=30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DTree = DTree.fit(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = DTree.predict(X_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample['isFraud'] = predictions\nsample.to_csv('submission_dt.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaling = MinMaxScaler(feature_range=(-1,1)).fit(X_train)\nX_train = scaling.transform(X_train)\nX_test = scaling.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## REFERENCES\n\n1. I haven't seen any kernels from this competition yet, but I would like to thank artgor for his amazing EDA kernels. I have done a basic analysis of some of his kernels and learned a lot from them.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"If you think there are mistakes or improvements can be made, please comment :)","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}