{"cells":[{"metadata":{},"cell_type":"markdown","source":"# XGB Gpyopt Hyperparameter Optimisation - GPU\n_By Nick Brooks_\n\nV1 - 12/08/2019 - First Commit <br>\n\n**Aim:** <br>\nAdapt my LGBM GPYOPT Bayesian optimisation to XGBOOST!\n\n\n**Sources:** <br>\n[GPYOPT Documentation](https://buildmedia.readthedocs.org/media/pdf/gpyopt/latest/gpyopt.pdf) <br>\n[krasserm's Blog Post (Super Awesome)](http://krasserm.github.io/2018/03/21/bayesian-optimization/) <br>\n\n\n**Kaggle:** <br>\n[Vincent Model RoC/PR/Confusion Matrix Evaluation Plots](https://www.kaggle.com/vincentlugat/ieee-lgb-bayesian-opt) <br>\n[Feature Engineering](https://www.kaggle.com/davidcairuz/feature-engineering-lightgbm-w-gpu) <br>\n\n"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Latest Pandas version\n!pip install -q 'pandas==0.25' --force-reinstall\n# Install Gpyopt\n!pip install GPyOpt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nimport pandas as pd\nimport GPyOpt\nfrom GPyOpt.methods import BayesianOptimization\nprint(\"GPyOpt version:\", GPyOpt.__version__)\nprint(\"LGBM version:\", xgb.__version__)\nprint(\"Pandas version:\", pd.__version__)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import time\nnotebookstart = time.time()\n\nimport os\nfrom contextlib import contextmanager\nimport gc; gc.enable()\nimport pprint\n\nimport datetime\nimport csv\nimport random\n\nimport numpy as np\nfrom pandas.io.json import json_normalize\n\n# Viz\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Modeling\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn import preprocessing\nfrom sklearn import metrics\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score, roc_auc_score, f1_score, roc_curve, auc,precision_recall_curve\nfrom scipy import interp\nimport itertools\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nseed = 24\nnp.random.seed(seed)\n\npd.set_option('display.max_columns', 500)\npd.options.display.max_rows = 999\npd.set_option('max_colwidth', 500)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(\"Define DF Schema..\")\n\ntarget_var = 'isFraud'\n\nschema = {\n    \"TransactionDT\":       \"int32\",\n    \"TransactionAmt\":    \"float32\",\n    \"ProductCD\":          \"object\",\n    \"card1\":               \"int16\",\n    \"card2\":             \"float32\",\n    \"card3\":             \"float32\",\n    \"card4\":              \"object\",\n    \"card5\":             \"float32\",\n    \"card6\":              \"object\",\n    \"addr1\":             \"float32\",\n    \"addr2\":             \"float32\",\n    \"dist1\":             \"float32\",\n    \"dist2\":             \"float32\",\n    \"P_emaildomain\":      \"object\",\n    \"R_emaildomain\":      \"object\",\n    \"C1\":                \"float32\",\n    \"C2\":                \"float32\",\n    \"C3\":                \"float32\",\n    \"C4\":                \"float32\",\n    \"C5\":                \"float32\",\n    \"C6\":                \"float32\",\n    \"C7\":                \"float32\",\n    \"C8\":                \"float32\",\n    \"C9\":                \"float32\",\n    \"C10\":               \"float32\",\n    \"C11\":               \"float32\",\n    \"C12\":               \"float32\",\n    \"C13\":               \"float32\",\n    \"C14\":               \"float32\",\n    \"D1\":                \"float32\",\n    \"D2\":                \"float32\",\n    \"D3\":                \"float32\",\n    \"D4\":                \"float32\",\n    \"D5\":                \"float32\",\n    \"D6\":                \"float32\",\n    \"D7\":                \"float32\",\n    \"D8\":                \"float32\",\n    \"D9\":                \"float32\",\n    \"D10\":               \"float32\",\n    \"D11\":               \"float32\",\n    \"D12\":               \"float32\",\n    \"D13\":               \"float32\",\n    \"D14\":               \"float32\",\n    \"D15\":               \"float32\",\n    \"M1\":                 \"object\",\n    \"M2\":                 \"object\",\n    \"M3\":                 \"object\",\n    \"M4\":                 \"object\",\n    \"M5\":                 \"object\",\n    \"M6\":                 \"object\",\n    \"M7\":                 \"object\",\n    \"M8\":                 \"object\",\n    \"M9\":                 \"object\",\n    \"V1\":                \"float32\",\n    \"V2\":                \"float32\",\n    \"V3\":                \"float32\",\n    \"V4\":                \"float32\",\n    \"V5\":                \"float32\",\n    \"V6\":                \"float32\",\n    \"V7\":                \"float32\",\n    \"V8\":                \"float32\",\n    \"V9\":                \"float32\",\n    \"V10\":               \"float32\",\n    \"V11\":               \"float32\",\n    \"V12\":               \"float32\",\n    \"V13\":               \"float32\",\n    \"V14\":               \"float32\",\n    \"V15\":               \"float32\",\n    \"V16\":               \"float32\",\n    \"V17\":               \"float32\",\n    \"V18\":               \"float32\",\n    \"V19\":               \"float32\",\n    \"V20\":               \"float32\",\n    \"V21\":               \"float32\",\n    \"V22\":               \"float32\",\n    \"V23\":               \"float32\",\n    \"V24\":               \"float32\",\n    \"V25\":               \"float32\",\n    \"V26\":               \"float32\",\n    \"V27\":               \"float32\",\n    \"V28\":               \"float32\",\n    \"V29\":               \"float32\",\n    \"V30\":               \"float32\",\n    \"V31\":               \"float32\",\n    \"V32\":               \"float32\",\n    \"V33\":               \"float32\",\n    \"V34\":               \"float32\",\n    \"V35\":               \"float32\",\n    \"V36\":               \"float32\",\n    \"V37\":               \"float32\",\n    \"V38\":               \"float32\",\n    \"V39\":               \"float32\",\n    \"V40\":               \"float32\",\n    \"V41\":               \"float32\",\n    \"V42\":               \"float32\",\n    \"V43\":               \"float32\",\n    \"V44\":               \"float32\",\n    \"V45\":               \"float32\",\n    \"V46\":               \"float32\",\n    \"V47\":               \"float32\",\n    \"V48\":               \"float32\",\n    \"V49\":               \"float32\",\n    \"V50\":               \"float32\",\n    \"V51\":               \"float32\",\n    \"V52\":               \"float32\",\n    \"V53\":               \"float32\",\n    \"V54\":               \"float32\",\n    \"V55\":               \"float32\",\n    \"V56\":               \"float32\",\n    \"V57\":               \"float32\",\n    \"V58\":               \"float32\",\n    \"V59\":               \"float32\",\n    \"V60\":               \"float32\",\n    \"V61\":               \"float32\",\n    \"V62\":               \"float32\",\n    \"V63\":               \"float32\",\n    \"V64\":               \"float32\",\n    \"V65\":               \"float32\",\n    \"V66\":               \"float32\",\n    \"V67\":               \"float32\",\n    \"V68\":               \"float32\",\n    \"V69\":               \"float32\",\n    \"V70\":               \"float32\",\n    \"V71\":               \"float32\",\n    \"V72\":               \"float32\",\n    \"V73\":               \"float32\",\n    \"V74\":               \"float32\",\n    \"V75\":               \"float32\",\n    \"V76\":               \"float32\",\n    \"V77\":               \"float32\",\n    \"V78\":               \"float32\",\n    \"V79\":               \"float32\",\n    \"V80\":               \"float32\",\n    \"V81\":               \"float32\",\n    \"V82\":               \"float32\",\n    \"V83\":               \"float32\",\n    \"V84\":               \"float32\",\n    \"V85\":               \"float32\",\n    \"V86\":               \"float32\",\n    \"V87\":               \"float32\",\n    \"V88\":               \"float32\",\n    \"V89\":               \"float32\",\n    \"V90\":               \"float32\",\n    \"V91\":               \"float32\",\n    \"V92\":               \"float32\",\n    \"V93\":               \"float32\",\n    \"V94\":               \"float32\",\n    \"V95\":               \"float32\",\n    \"V96\":               \"float32\",\n    \"V97\":               \"float32\",\n    \"V98\":               \"float32\",\n    \"V99\":               \"float32\",\n    \"V100\":              \"float32\",\n    \"V101\":              \"float32\",\n    \"V102\":              \"float32\",\n    \"V103\":              \"float32\",\n    \"V104\":              \"float32\",\n    \"V105\":              \"float32\",\n    \"V106\":              \"float32\",\n    \"V107\":              \"float32\",\n    \"V108\":              \"float32\",\n    \"V109\":              \"float32\",\n    \"V110\":              \"float32\",\n    \"V111\":              \"float32\",\n    \"V112\":              \"float32\",\n    \"V113\":              \"float32\",\n    \"V114\":              \"float32\",\n    \"V115\":              \"float32\",\n    \"V116\":              \"float32\",\n    \"V117\":              \"float32\",\n    \"V118\":              \"float32\",\n    \"V119\":              \"float32\",\n    \"V120\":              \"float32\",\n    \"V121\":              \"float32\",\n    \"V122\":              \"float32\",\n    \"V123\":              \"float32\",\n    \"V124\":              \"float32\",\n    \"V125\":              \"float32\",\n    \"V126\":              \"float32\",\n    \"V127\":              \"float32\",\n    \"V128\":              \"float32\",\n    \"V129\":              \"float32\",\n    \"V130\":              \"float32\",\n    \"V131\":              \"float32\",\n    \"V132\":              \"float32\",\n    \"V133\":              \"float32\",\n    \"V134\":              \"float32\",\n    \"V135\":              \"float32\",\n    \"V136\":              \"float32\",\n    \"V137\":              \"float32\",\n    \"V138\":              \"float32\",\n    \"V139\":              \"float32\",\n    \"V140\":              \"float32\",\n    \"V141\":              \"float32\",\n    \"V142\":              \"float32\",\n    \"V143\":              \"float32\",\n    \"V144\":              \"float32\",\n    \"V145\":              \"float32\",\n    \"V146\":              \"float32\",\n    \"V147\":              \"float32\",\n    \"V148\":              \"float32\",\n    \"V149\":              \"float32\",\n    \"V150\":              \"float32\",\n    \"V151\":              \"float32\",\n    \"V152\":              \"float32\",\n    \"V153\":              \"float32\",\n    \"V154\":              \"float32\",\n    \"V155\":              \"float32\",\n    \"V156\":              \"float32\",\n    \"V157\":              \"float32\",\n    \"V158\":              \"float32\",\n    \"V159\":              \"float32\",\n    \"V160\":              \"float32\",\n    \"V161\":              \"float32\",\n    \"V162\":              \"float32\",\n    \"V163\":              \"float32\",\n    \"V164\":              \"float32\",\n    \"V165\":              \"float32\",\n    \"V166\":              \"float32\",\n    \"V167\":              \"float32\",\n    \"V168\":              \"float32\",\n    \"V169\":              \"float32\",\n    \"V170\":              \"float32\",\n    \"V171\":              \"float32\",\n    \"V172\":              \"float32\",\n    \"V173\":              \"float32\",\n    \"V174\":              \"float32\",\n    \"V175\":              \"float32\",\n    \"V176\":              \"float32\",\n    \"V177\":              \"float32\",\n    \"V178\":              \"float32\",\n    \"V179\":              \"float32\",\n    \"V180\":              \"float32\",\n    \"V181\":              \"float32\",\n    \"V182\":              \"float32\",\n    \"V183\":              \"float32\",\n    \"V184\":              \"float32\",\n    \"V185\":              \"float32\",\n    \"V186\":              \"float32\",\n    \"V187\":              \"float32\",\n    \"V188\":              \"float32\",\n    \"V189\":              \"float32\",\n    \"V190\":              \"float32\",\n    \"V191\":              \"float32\",\n    \"V192\":              \"float32\",\n    \"V193\":              \"float32\",\n    \"V194\":              \"float32\",\n    \"V195\":              \"float32\",\n    \"V196\":              \"float32\",\n    \"V197\":              \"float32\",\n    \"V198\":              \"float32\",\n    \"V199\":              \"float32\",\n    \"V200\":              \"float32\",\n    \"V201\":              \"float32\",\n    \"V202\":              \"float32\",\n    \"V203\":              \"float32\",\n    \"V204\":              \"float32\",\n    \"V205\":              \"float32\",\n    \"V206\":              \"float32\",\n    \"V207\":              \"float32\",\n    \"V208\":              \"float32\",\n    \"V209\":              \"float32\",\n    \"V210\":              \"float32\",\n    \"V211\":              \"float32\",\n    \"V212\":              \"float32\",\n    \"V213\":              \"float32\",\n    \"V214\":              \"float32\",\n    \"V215\":              \"float32\",\n    \"V216\":              \"float32\",\n    \"V217\":              \"float32\",\n    \"V218\":              \"float32\",\n    \"V219\":              \"float32\",\n    \"V220\":              \"float32\",\n    \"V221\":              \"float32\",\n    \"V222\":              \"float32\",\n    \"V223\":              \"float32\",\n    \"V224\":              \"float32\",\n    \"V225\":              \"float32\",\n    \"V226\":              \"float32\",\n    \"V227\":              \"float32\",\n    \"V228\":              \"float32\",\n    \"V229\":              \"float32\",\n    \"V230\":              \"float32\",\n    \"V231\":              \"float32\",\n    \"V232\":              \"float32\",\n    \"V233\":              \"float32\",\n    \"V234\":              \"float32\",\n    \"V235\":              \"float32\",\n    \"V236\":              \"float32\",\n    \"V237\":              \"float32\",\n    \"V238\":              \"float32\",\n    \"V239\":              \"float32\",\n    \"V240\":              \"float32\",\n    \"V241\":              \"float32\",\n    \"V242\":              \"float32\",\n    \"V243\":              \"float32\",\n    \"V244\":              \"float32\",\n    \"V245\":              \"float32\",\n    \"V246\":              \"float32\",\n    \"V247\":              \"float32\",\n    \"V248\":              \"float32\",\n    \"V249\":              \"float32\",\n    \"V250\":              \"float32\",\n    \"V251\":              \"float32\",\n    \"V252\":              \"float32\",\n    \"V253\":              \"float32\",\n    \"V254\":              \"float32\",\n    \"V255\":              \"float32\",\n    \"V256\":              \"float32\",\n    \"V257\":              \"float32\",\n    \"V258\":              \"float32\",\n    \"V259\":              \"float32\",\n    \"V260\":              \"float32\",\n    \"V261\":              \"float32\",\n    \"V262\":              \"float32\",\n    \"V263\":              \"float32\",\n    \"V264\":              \"float32\",\n    \"V265\":              \"float32\",\n    \"V266\":              \"float32\",\n    \"V267\":              \"float32\",\n    \"V268\":              \"float32\",\n    \"V269\":              \"float32\",\n    \"V270\":              \"float32\",\n    \"V271\":              \"float32\",\n    \"V272\":              \"float32\",\n    \"V273\":              \"float32\",\n    \"V274\":              \"float32\",\n    \"V275\":              \"float32\",\n    \"V276\":              \"float32\",\n    \"V277\":              \"float32\",\n    \"V278\":              \"float32\",\n    \"V279\":              \"float32\",\n    \"V280\":              \"float32\",\n    \"V281\":              \"float32\",\n    \"V282\":              \"float32\",\n    \"V283\":              \"float32\",\n    \"V284\":              \"float32\",\n    \"V285\":              \"float32\",\n    \"V286\":              \"float32\",\n    \"V287\":              \"float32\",\n    \"V288\":              \"float32\",\n    \"V289\":              \"float32\",\n    \"V290\":              \"float32\",\n    \"V291\":              \"float32\",\n    \"V292\":              \"float32\",\n    \"V293\":              \"float32\",\n    \"V294\":              \"float32\",\n    \"V295\":              \"float32\",\n    \"V296\":              \"float32\",\n    \"V297\":              \"float32\",\n    \"V298\":              \"float32\",\n    \"V299\":              \"float32\",\n    \"V300\":              \"float32\",\n    \"V301\":              \"float32\",\n    \"V302\":              \"float32\",\n    \"V303\":              \"float32\",\n    \"V304\":              \"float32\",\n    \"V305\":              \"float32\",\n    \"V306\":              \"float32\",\n    \"V307\":              \"float32\",\n    \"V308\":              \"float32\",\n    \"V309\":              \"float32\",\n    \"V310\":              \"float32\",\n    \"V311\":              \"float32\",\n    \"V312\":              \"float32\",\n    \"V313\":              \"float32\",\n    \"V314\":              \"float32\",\n    \"V315\":              \"float32\",\n    \"V316\":              \"float32\",\n    \"V317\":              \"float32\",\n    \"V318\":              \"float32\",\n    \"V319\":              \"float32\",\n    \"V320\":              \"float32\",\n    \"V321\":              \"float32\",\n    \"V322\":              \"float32\",\n    \"V323\":              \"float32\",\n    \"V324\":              \"float32\",\n    \"V325\":              \"float32\",\n    \"V326\":              \"float32\",\n    \"V327\":              \"float32\",\n    \"V328\":              \"float32\",\n    \"V329\":              \"float32\",\n    \"V330\":              \"float32\",\n    \"V331\":              \"float32\",\n    \"V332\":              \"float32\",\n    \"V333\":              \"float32\",\n    \"V334\":              \"float32\",\n    \"V335\":              \"float32\",\n    \"V336\":              \"float32\",\n    \"V337\":              \"float32\",\n    \"V338\":              \"float32\",\n    \"V339\":              \"float32\",\n    \"id_01\":             \"float32\",\n    \"id_02\":             \"float32\",\n    \"id_03\":             \"float32\",\n    \"id_04\":             \"float32\",\n    \"id_05\":             \"float32\",\n    \"id_06\":             \"float32\",\n    \"id_07\":             \"float32\",\n    \"id_08\":             \"float32\",\n    \"id_09\":             \"float32\",\n    \"id_10\":             \"float32\",\n    \"id_11\":             \"float32\",\n    \"id_12\":              \"object\",\n    \"id_13\":             \"float32\",\n    \"id_14\":             \"float32\",\n    \"id_15\":              \"object\",\n    \"id_16\":              \"object\",\n    \"id_17\":             \"float32\",\n    \"id_18\":             \"float32\",\n    \"id_19\":             \"float32\",\n    \"id_20\":             \"float32\",\n    \"id_21\":             \"float32\",\n    \"id_22\":             \"float32\",\n    \"id_23\":              \"object\",\n    \"id_24\":             \"float32\",\n    \"id_25\":             \"float32\",\n    \"id_26\":             \"float32\",\n    \"id_27\":              \"object\",\n    \"id_28\":              \"object\",\n    \"id_29\":              \"object\",\n    \"id_30\":              \"object\",\n    \"id_31\":              \"object\",\n    \"id_32\":             \"float32\",\n    \"id_33\":              \"object\",\n    \"id_34\":              \"object\",\n    \"id_35\":              \"object\",\n    \"id_36\":              \"object\",\n    \"id_37\":              \"object\",\n    \"id_38\":              \"object\",\n    \"DeviceType\":         \"object\",\n    \"DeviceInfo\":         \"object\",\n    \"is_fraud\":\t\t\t  \"int8\"\n}\n\nemails = {'gmail': 'google',\n'att.net': 'att',\n'twc.com': 'spectrum',\n'scranton.edu': 'other',\n'optonline.net': 'other',\n'hotmail.co.uk': 'microsoft',\n'comcast.net': 'other',\n'yahoo.com.mx': 'yahoo',\n'yahoo.fr': 'yahoo',\n'yahoo.es': 'yahoo',\n'charter.net': 'spectrum',\n'live.com': 'microsoft',\n'aim.com': 'aol',\n'hotmail.de': 'microsoft',\n'centurylink.net': 'centurylink',\n'gmail.com': 'google',\n'me.com': 'apple',\n'earthlink.net': 'other',\n'gmx.de': 'other',\n'web.de': 'other',\n'cfl.rr.com': 'other',\n'hotmail.com': 'microsoft',\n'protonmail.com': 'other',\n'hotmail.fr': 'microsoft',\n'windstream.net': 'other',\n'outlook.es': 'microsoft',\n'yahoo.co.jp': 'yahoo',\n'yahoo.de': 'yahoo',\n'servicios-ta.com': 'other',\n'netzero.net': 'other',\n'suddenlink.net': 'other',\n'roadrunner.com': 'other',\n'sc.rr.com': 'other',\n'live.fr': 'microsoft',\n'verizon.net': 'yahoo',\n'msn.com': 'microsoft',\n'q.com': 'centurylink',\n'prodigy.net.mx': 'att',\n'frontier.com': 'yahoo',\n'anonymous.com': 'other',\n'rocketmail.com': 'yahoo',\n'sbcglobal.net': 'att',\n'frontiernet.net': 'yahoo',\n'ymail.com': 'yahoo',\n'outlook.com': 'microsoft',\n'mail.com': 'other',\n'bellsouth.net': 'other',\n'embarqmail.com': 'centurylink',\n'cableone.net': 'other',\n'hotmail.es': 'microsoft',\n'mac.com': 'apple',\n'yahoo.co.uk': 'yahoo',\n'netzero.com': 'other',\n'yahoo.com': 'yahoo',\n'live.com.mx': 'microsoft',\n'ptd.net': 'other',\n'cox.net': 'other',\n'aol.com': 'aol',\n'juno.com': 'other',\n'icloud.com': 'apple'}\n\n\nus_emails = ['gmail', 'net', 'edu']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@contextmanager\ndef timer(name):\n    \"\"\"\n    Time Each Process\n    \"\"\"\n    t0 = time.time()\n    yield\n    print('\\n[{}] done in {} Minutes\\n'.format(name, round((time.time() - t0)/60,2)))\n\n# Device Features\ndef id_split(dataframe):\n    # https://www.kaggle.com/davidcairuz/feature-engineering-lightgbm-w-gpu\n    dataframe['device_name'] = dataframe['DeviceInfo'].str.split('/', expand=True)[0]\n    dataframe['device_version'] = dataframe['DeviceInfo'].str.split('/', expand=True)[1]\n\n    dataframe['OS_id_30'] = dataframe['id_30'].str.split(' ', expand=True)[0]\n    dataframe['version_id_30'] = dataframe['id_30'].str.split(' ', expand=True)[1]\n\n    dataframe['browser_id_31'] = dataframe['id_31'].str.split(' ', expand=True)[0]\n    dataframe['version_id_31'] = dataframe['id_31'].str.split(' ', expand=True)[1]\n\n    dataframe['screen_width'] = dataframe['id_33'].str.split('x', expand=True)[0]\n    dataframe['screen_height'] = dataframe['id_33'].str.split('x', expand=True)[1]\n\n    dataframe['id_34'] = dataframe['id_34'].str.split(':', expand=True)[1]\n    dataframe['id_23'] = dataframe['id_23'].str.split(':', expand=True)[1]\n\n    dataframe.loc[dataframe['device_name'].str.contains('SM', na=False), 'device_name'] = 'Samsung'\n    dataframe.loc[dataframe['device_name'].str.contains('SAMSUNG', na=False), 'device_name'] = 'Samsung'\n    dataframe.loc[dataframe['device_name'].str.contains('GT-', na=False), 'device_name'] = 'Samsung'\n    dataframe.loc[dataframe['device_name'].str.contains('Moto G', na=False), 'device_name'] = 'Motorola'\n    dataframe.loc[dataframe['device_name'].str.contains('Moto', na=False), 'device_name'] = 'Motorola'\n    dataframe.loc[dataframe['device_name'].str.contains('moto', na=False), 'device_name'] = 'Motorola'\n    dataframe.loc[dataframe['device_name'].str.contains('LG-', na=False), 'device_name'] = 'LG'\n    dataframe.loc[dataframe['device_name'].str.contains('rv:', na=False), 'device_name'] = 'RV'\n    dataframe.loc[dataframe['device_name'].str.contains('HUAWEI', na=False), 'device_name'] = 'Huawei'\n    dataframe.loc[dataframe['device_name'].str.contains('ALE-', na=False), 'device_name'] = 'Huawei'\n    dataframe.loc[dataframe['device_name'].str.contains('-L', na=False), 'device_name'] = 'Huawei'\n    dataframe.loc[dataframe['device_name'].str.contains('Blade', na=False), 'device_name'] = 'ZTE'\n    dataframe.loc[dataframe['device_name'].str.contains('BLADE', na=False), 'device_name'] = 'ZTE'\n    dataframe.loc[dataframe['device_name'].str.contains('Linux', na=False), 'device_name'] = 'Linux'\n    dataframe.loc[dataframe['device_name'].str.contains('XT', na=False), 'device_name'] = 'Sony'\n    dataframe.loc[dataframe['device_name'].str.contains('HTC', na=False), 'device_name'] = 'HTC'\n    dataframe.loc[dataframe['device_name'].str.contains('ASUS', na=False), 'device_name'] = 'Asus'\n\n    dataframe.loc[dataframe.device_name.isin(dataframe.device_name.value_counts()[dataframe.device_name.value_counts() < 200].index), 'device_name'] = \"Others\"\n    dataframe['had_id'] = 1\n    gc.collect()\n    \n    return dataframe\n    \ndef fraud_preprocessing(debug = None):\n    print(\"Starting Pre-Processing..\")\n    with timer(\"Load Tables\"):\n        train_transaction = pd.read_csv('../input/train_transaction.csv',\n                                        index_col='TransactionID', nrows= debug, dtype = schema)\n        test_transaction = pd.read_csv('../input/test_transaction.csv',\n                                       index_col='TransactionID', nrows= debug, dtype = schema)\n\n        train_identity = pd.read_csv('../input/train_identity.csv', index_col='TransactionID')\n        test_identity = pd.read_csv('../input/test_identity.csv', index_col='TransactionID')\n\n        sample_submission = pd.read_csv('../input/sample_submission.csv',\n                                        index_col='TransactionID',\n                                        nrows= debug)\n\n    with timer(\"Merge Tables\"):\n        train = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\n        test = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n\n        print(\"Train Shape: {} Rows, {} Columns\".format(*train.shape))\n        print(\"Test Shape: {} Rows, {} Columns\".format(*test.shape))\n\n        y = train[target_var].copy()\n        del train_transaction, train_identity, test_transaction, test_identity\n\n        traindex = train.index\n        testdex = test.index\n        \n    with timer(\"Train/Test Split Feature Engineering\"):\n        # Credit https://www.kaggle.com/davidcairuz/feature-engineering-lightgbm-w-gpu\n        train['TransactionAmt_to_mean_card1'] = train['TransactionAmt'] / train.groupby(['card1'])['TransactionAmt'].transform('mean')\n        train['TransactionAmt_to_mean_card4'] = train['TransactionAmt'] / train.groupby(['card4'])['TransactionAmt'].transform('mean')\n        train['TransactionAmt_to_std_card1'] = train['TransactionAmt'] / train.groupby(['card1'])['TransactionAmt'].transform('std')\n        train['TransactionAmt_to_std_card4'] = train['TransactionAmt'] / train.groupby(['card4'])['TransactionAmt'].transform('std')\n\n        test['TransactionAmt_to_mean_card1'] = test['TransactionAmt'] / test.groupby(['card1'])['TransactionAmt'].transform('mean')\n        test['TransactionAmt_to_mean_card4'] = test['TransactionAmt'] / test.groupby(['card4'])['TransactionAmt'].transform('mean')\n        test['TransactionAmt_to_std_card1'] = test['TransactionAmt'] / test.groupby(['card1'])['TransactionAmt'].transform('std')\n        test['TransactionAmt_to_std_card4'] = test['TransactionAmt'] / test.groupby(['card4'])['TransactionAmt'].transform('std')\n\n        train['id_02_to_mean_card1'] = train['id_02'] / train.groupby(['card1'])['id_02'].transform('mean')\n        train['id_02_to_mean_card4'] = train['id_02'] / train.groupby(['card4'])['id_02'].transform('mean')\n        train['id_02_to_std_card1'] = train['id_02'] / train.groupby(['card1'])['id_02'].transform('std')\n        train['id_02_to_std_card4'] = train['id_02'] / train.groupby(['card4'])['id_02'].transform('std')\n\n        test['id_02_to_mean_card1'] = test['id_02'] / test.groupby(['card1'])['id_02'].transform('mean')\n        test['id_02_to_mean_card4'] = test['id_02'] / test.groupby(['card4'])['id_02'].transform('mean')\n        test['id_02_to_std_card1'] = test['id_02'] / test.groupby(['card1'])['id_02'].transform('std')\n        test['id_02_to_std_card4'] = test['id_02'] / test.groupby(['card4'])['id_02'].transform('std')\n\n        train['D15_to_mean_card1'] = train['D15'] / train.groupby(['card1'])['D15'].transform('mean')\n        train['D15_to_mean_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('mean')\n        train['D15_to_std_card1'] = train['D15'] / train.groupby(['card1'])['D15'].transform('std')\n        train['D15_to_std_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('std')\n\n        test['D15_to_mean_card1'] = test['D15'] / test.groupby(['card1'])['D15'].transform('mean')\n        test['D15_to_mean_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('mean')\n        test['D15_to_std_card1'] = test['D15'] / test.groupby(['card1'])['D15'].transform('std')\n        test['D15_to_std_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('std')\n\n        train['D15_to_mean_addr1'] = train['D15'] / train.groupby(['addr1'])['D15'].transform('mean')\n        train['D15_to_mean_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('mean')\n        train['D15_to_std_addr1'] = train['D15'] / train.groupby(['addr1'])['D15'].transform('std')\n        train['D15_to_std_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('std')\n\n        test['D15_to_mean_addr1'] = test['D15'] / test.groupby(['addr1'])['D15'].transform('mean')\n        test['D15_to_mean_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('mean')\n        test['D15_to_std_addr1'] = test['D15'] / test.groupby(['addr1'])['D15'].transform('std')\n        test['D15_to_std_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('std')\n        \n        # New feature - log of transaction amount. ()\n        train['TransactionAmt_Log'] = np.log(train['TransactionAmt'])\n        test['TransactionAmt_Log'] = np.log(test['TransactionAmt'])\n        \n        # Encoding - count encoding for both train and test\n        for feature in ['card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'id_36']:\n            train[feature + '_count_full'] = train[feature].map(pd.concat([train[feature], test[feature]], ignore_index=True).value_counts(dropna=False))\n            test[feature + '_count_full'] = test[feature].map(pd.concat([train[feature], test[feature]], ignore_index=True).value_counts(dropna=False))\n\n        # Encoding - count encoding separately for train and test\n        for feature in ['id_01', 'id_31', 'id_33', 'id_36']:\n            train[feature + '_count_dist'] = train[feature].map(train[feature].value_counts(dropna=False))\n            test[feature + '_count_dist'] = test[feature].map(test[feature].value_counts(dropna=False))\n            \n        # https://www.kaggle.com/c/ieee-fraud-detection/discussion/100499\n        for c in ['P_emaildomain', 'R_emaildomain']:\n            train[c + '_bin'] = train[c].map(emails)\n            test[c + '_bin'] = test[c].map(emails)\n\n            train[c + '_suffix'] = train[c].map(lambda x: str(x).split('.')[-1])\n            test[c + '_suffix'] = test[c].map(lambda x: str(x).split('.')[-1])\n\n            train[c + '_suffix'] = train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n            test[c + '_suffix'] = test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n            \n        # Extract Device Information\n        train = id_split(train)\n        test = id_split(test)\n        \n        # Combine\n        df = pd.concat([train.drop(target_var,axis=1),test],axis = 0)\n        del train, test\n        \n    with timer(\"Whole Feature Engineering\"):\n        START_DATE = '2017-12-01'\n        startdate = datetime.datetime.strptime(START_DATE, '%Y-%m-%d')    \n        df = df.assign(\n                # New feature - decimal part of the transaction amount\n                TransactionAmt_decimal = ((df['TransactionAmt'] - df['TransactionAmt'].astype(int)) * 1000).astype(int),\n\n                # Count encoding for card1 feature. \n                # Explained in this kernel: https://www.kaggle.com/nroman/eda-for-cis-fraud-detection\n                card1_count_full = df['card1'].map(df['card1'].value_counts(dropna=False)),\n\n                # https://www.kaggle.com/fchmiel/day-and-time-powerful-predictive-feature\n                Transaction_day_of_week = np.floor((df['TransactionDT'] / (3600 * 24) - 1) % 7),\n                Transaction_hour = np.floor(df['TransactionDT'] / 3600) % 24,\n\n                TransactionDT = df['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x))),\n            )\n        df = df.assign(\n                # Time of Day\n                year = df['TransactionDT'].dt.year,\n                month = df['TransactionDT'].dt.month,\n                dow = df['TransactionDT'].dt.dayofweek,\n                quarter = df['TransactionDT'].dt.quarter,\n                hour = df['TransactionDT'].dt.hour,\n                day = df['TransactionDT'].dt.day,\n        \n                # All NaN\n                all_group_nan_sum = df.isnull().sum(axis=1) / df.shape[1],\n                all_group_0_count = (df == 0).astype(int).sum(axis=1) / (df.shape[1] - df.isnull().sum(axis=1))\n        )\n        \n        # Create Features based on anonymised prefix groups\n        prefix = ['C','D','Device','M','Transaction','V','addr','card','dist','id']\n        for i, p in enumerate(prefix):\n            column_set = [x for x in df.columns.tolist() if x.startswith(prefix[i])]\n\n            # Take NA count\n            df[p + \"group_nan_sum\"] = df[column_set].isnull().sum(axis=1) / df[column_set].shape[1]\n\n            # Take SUM/Mean if numeric\n            numeric_cols = [x for x in column_set if df[x].dtype != object]\n            if numeric_cols:\n                df[p + \"group_sum\"] = df[column_set].sum(axis=1)\n                df[p + \"group_mean\"] = df[column_set].mean(axis=1)\n                # Zero Count\n                df[p + \"group_0_count\"] = (df[column_set] == 0).astype(int).sum(axis=1) / (df[column_set].shape[1] - df[p + \"group_nan_sum\"])\n\n    with timer(\"Label Encode\"):\n        categorical_cols = []\n        # Label Encoding\n        for f in df.columns:\n            if df[f].dtype=='object': \n                categorical_cols += [f]\n                lbl = preprocessing.LabelEncoder()\n                df[f] = lbl.fit_transform(df[f].astype(str))\n    print(\"Total Shape: {} Rows, {} Columns\".format(*df.shape))\n    return df, y, traindex, testdex, categorical_cols, sample_submission","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Prepare Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"DEBUG = None # None for no debug, else number of rows\n\ndf, y, traindex, testdex, cat_cols, sample_submission = fraud_preprocessing(debug = DEBUG)\ndf.fillna(-999, inplace=True)\nX = df.loc[traindex,:]\nfeature_subset = [x for x in X.columns.tolist() if x not in ['TransactionDT','Fraud', 'traintest', 'yrmth']]\ntest = df.loc[testdex,:]\n# xgtest = xgb.DMatrix(test.loc[:,feature_subset], feature_names = feature_subset)\n\ndel df; gc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"None Fraud: {}%, Fraud: {}%\".format(*y.value_counts(normalize=True)))\nprint(\"Randomness Score AUC: {}\".format(\n    metrics.roc_auc_score(y,np.array([y.value_counts(normalize=True)[0]]*y.shape[0]))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bayesian Hyper Parameter Search"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Optimization objective \ndef XGB_score(para):\n    parameters = para[0]\n#     num_leaves = 2**parameters[0] if 2**parameters[0] < 4095 else 4095\n#     parameters[0] = -1 if parameters[0] == 45 else parameters[0]\n#     num_leaves = int(num_leaves * parameters[1])\n    params = {\n        # Static Variables\n        'objective': 'binary:logistic',\n        'eval_metric': metric,\n        'learning_rate': 0.1, # Multiplication performed on each boosting iteration.\n        'tree_method': 'gpu_hist', # GPU usage.\n        'importance_type': 'weight',\n        'missing': -999,\n\n        # Dynamic Variables\n        # https://xgboost.readthedocs.io/en/latest/parameter.html\n        # https://sites.google.com/view/lauraepp/parameters\n\n        # Bushi-ness Parameters\n        'max_depth': int(parameters[0]),  # -1 means no tree depth limit\n#         'num_leaves': int(parameters[1]), # we should let it be smaller than 2^(max_depth)\n\n        # Tree Depth Regularization\n#         'subsample_for_bin': int(parameters[2]), # Number of samples for constructing bin\n#         'min_data_in_leaf': int(parameters[0]), # Minimum number of data need in a child(min_data_in_leaf) - Must be motified when using a smaller dataset\n    #     'min_gain_to_split': [0], # Prune by minimum loss requirement.\n#         'min_sum_hessian_in_leaf': parameters[3], # Prune by minimum hessian requirement - Minimum sum of instance weight(hessian) needed in a child(leaf)\n\n        # Regularization L1/L2\n        'reg_alpha': parameters[1], # L1 regularization term on weights (0 is no regular)\n        'reg_lambda': parameters[2], # L2 regularization term on weights\n    #     'max_bin': list(range(70, 300, 30)),  # Number of bucketed bin for feature values\n\n        # Row/Column Sampling\n        'subsample': parameters[3], # Subsample ratio of the training instance.\n    #     'subsample_freq': 0, # frequence of subsample, <=0 means no enable\n#         'bagging_fraction': 1.0,# Percentage of rows used per iteration frequency.\n#         'bagging_freq': 5,# Iteration frequency to update the selected rows.\n        'colsample_bytree': parameters[4], # Percentage of columns used per iteration.\n        \n#         'nthread': -1, # Multi-threading\n#         'verbose': -1, # Logging Iteration Progression\n        'seed': seed # Seed for row sampling RNG.\n    }\n    \n    experiment_clf = xgb.XGBClassifier(**params,\n                                  n_estimators = n_estimators)\n    \n    modelstart= time.time()\n    experiment_clf.fit(X_train,\n                       y_train,\n                       eval_set=[(X_valid,y_valid)],\n                       early_stopping_rounds=100,\n                       verbose=0)\n    runtime = (time.time() - modelstart)/60\n\n    rounds = experiment_clf.best_ntree_limit\n    val_pred = experiment_clf.predict_proba(X_valid)[:,1]\n\n    # Get Metrics\n    score = experiment_clf.best_score\n    loss = metrics.log_loss(y_valid, val_pred)\n    params['num_boost_round'] = rounds\n\n    gpyopt_output.append(\n        [\n         loss,\n         score,\n         rounds,\n         params,\n         runtime,\n         experiment_clf.feature_importances_\n        ]\n    )\n    \n    return score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"split_size = 0.4\nn_estimators = 10000\nmetric = 'auc'\nESR = 250\n\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X[feature_subset], y, test_size=split_size,\n    random_state=seed, shuffle=True,stratify=y)\n\nbds = [ {'name': 'max_depth', 'type': 'discrete', 'domain': (5, 15)},\n        {'name': 'reg_alpha', 'type': 'continuous', 'domain': (.05, 0.8)},\n        {'name': 'reg_lambda', 'type': 'continuous', 'domain': (.05, 0.8)},\n        {'name': 'subsample', 'type': 'continuous', 'domain': (0.1, 0.80)},\n        {'name': 'colsample_bytree', 'type': 'continuous', 'domain': (0.1, 0.80)},\n#         {'name': 'num_leaves', 'type': 'discrete', 'domain': (200, 4000)},\n#         {'name': 'subsample_for_bin', 'type': 'discrete', 'domain': (1000, 5000)},\n#         {'name': 'min_sum_hessian_in_leaf', 'type': 'continuous', 'domain': (0.1, 15)},\n      ]\n\nsb_cat_cols = [x for x in cat_cols if x in feature_subset]\n\n\nmax_iter = 30\ninitial_iter = 20\n\ngpyopt_output = []\ngpy_importance = pd.DataFrame()\noptimizer = BayesianOptimization(f=XGB_score, \n                                 domain=bds,\n                                 model_type='GP',\n                                 optimize_restarts = 1,\n                                 initial_design_numdata = initial_iter,\n                                 acquisition_type ='EI',\n                                 acquisition_jitter = 0.2,\n                                 exact_feval=True, \n                                 maximize=True)\n\nwith timer(\"Bayesian Optimisation - {} Iterations\".format(max_iter + initial_iter)):\n    optimizer.run_optimization(max_iter=max_iter)\n    \n# Output\nresults = pd.DataFrame(gpyopt_output,\n        columns = ['logloss','valid_auc',\n                   'boosting_rounds','parameters', 'runtime', 'imp']\n                      )\ngpyimp = results.imp.iloc[-20:].apply(lambda x: pd.Series(x)).T\ngpyimp.index = feature_subset\nog_cols = gpyimp.columns\ngpyimp['mean'] = gpyimp[og_cols].mean(axis = 1)\ngpyimp['std'] = gpyimp[og_cols].std(axis = 1)\n\ndel gpyopt_output; results.drop('imp', axis =1, inplace=True)\nresults.to_csv(\"gpyopt_iterations_output.csv\")\nbest_params = results['parameters'].iloc[np.argmax(results.valid_auc)]\n\n# Visualize Convergence\noptimizer.plot_convergence()\n\n# Trees and Runtime\nresults['TPM'] = results['boosting_rounds'] / results['runtime'] # tree_per_minutes\n\nplt.plot(results['TPM'], '-rx')\nplt.xlabel(\"Bayesian Search Iteration\")\nplt.ylabel(\"TPM\")\nplt.title(\"Tree per Minute through Bayesian Search\")\nplt.show()\n\nprint(\"Best AUC: {}\".format(optimizer.fx_opt))\nprint(\"Best Parameters\")\npprint.pprint(best_params)\n\n# Json to DataFrame\nresults = pd.concat([results.drop('parameters',axis=1).reset_index(drop=True),\n                     json_normalize(results['parameters']).reset_index(drop=True)\n                    ], axis = 1)\n\ndel X_train, X_valid, y_train, y_valid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importance_cutoff = .95\n\n# Decide which features to preserve\ncumu_imp = np.cumsum(gpyimp.sort_values(by=\"mean\", ascending=False)['mean'])\nkeep_features = cumu_imp[cumu_imp < importance_cutoff].index[:200]\n\n# Plot Them\nf, ax = plt.subplots(1,2, figsize = [10,5])\nsns.distplot(gpyimp['mean'], ax =ax[0])\nax[0].set_title(\"Feature Importance Distribution\")\nax[0].set_xlabel(\"Importance\")\n\nax[1].plot(cumu_imp.reset_index(drop=True), color = 'r')\nax[1].set_title(\"Cumulative Feature Importance\")\nax[1].set_xlabel(\"Number of Features\")\nax[1].axvline(len(keep_features), color = 'black')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hyperparameter Relationship Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"display(results.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t_r,t_c = 2, 3\nf, axes = plt.subplots(t_r, t_c, figsize = [15,8],\n                       sharex=False, sharey=True)\nrow,col = 0,0\nparas = [x['name'] for x in bds] \n\nfor var in paras:\n    if col == t_c:\n        col = 0\n        row += 1\n    sns.regplot(x=var, y = \"valid_auc\", data = results,\n                x_estimator=np.mean, logx=True,\n                truncate=True, ax = axes[row,col])\n    axes[row,col].set_title('{} vs AUC'.format(var.title()))\n    axes[row,col].grid(True, lw = 2, ls = '--', c = '.75')\n    axes[row,col].set_ylim(0.8,1)\n    if var == 'min_data_in_leaf':\n        axes[row,col].set_xlim(0,100)\n    col+=1\nplt.tight_layout(pad=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Submit Best Parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_subset = keep_features\ndrop_cols = [x for x in X.columns if x not in keep_features]\n\nX.drop(drop_cols, axis = 1, inplace= True)\ntest.drop(drop_cols, axis = 1, inplace= True)\ngc.collect()\nprint(\"Train Shape: {} Rows, {} Cols\\n\".format(*X[feature_subset].shape))\n\nallmodelstart= time.time()\nEPOCHS = 4\nbest_params['learning_rate'] = 0.1\nkf = KFold(n_splits = EPOCHS, shuffle = True)\ny_preds = np.zeros(sample_submission.shape[0])\ny_oof = np.zeros(X.shape[0])\nf,ax = plt.subplots(1,3,figsize = [15,6])\nsb_cat_cols = [x for x in cat_cols if x in feature_subset]\nall_feature_importance_df  = pd.DataFrame()\n\n# Vincent Lugat - https://www.kaggle.com/vincentlugat/ieee-lgb-bayesian-opt\nmean_fpr = np.linspace(0,1,100)\ncms, tprs, aucs, y_real, y_proba,recalls, roc_aucs,f1_scores, accuracies, precisions = [],[],[],[],[],[],[],[],[],[]\n\n# Run Out of Fold\nfor i, (tr_idx, val_idx) in enumerate(kf.split(X, y)):\n    i += 1\n    modelstart = time.time()\n    sb_cat_cols = [x for x in cat_cols if x in feature_subset]        \n    clf = xgb.XGBClassifier(**best_params,\n                    n_estimators = n_estimators)\n    \n    modelstart= time.time()\n    clf.fit(X.iloc[tr_idx, :][feature_subset],\n               y.iloc[tr_idx],\n               eval_set=[(X.iloc[val_idx, :][feature_subset], y.iloc[val_idx])],\n               early_stopping_rounds=ESR,\n               verbose=500)\n    runtime = (time.time() - modelstart)/60\n    rounds = clf.best_ntree_limit\n    \n    # Model Evaluation\n    y_oof[val_idx] = clf.predict_proba(X.iloc[val_idx, :][feature_subset])[:,1]\n    y_preds += clf.predict_proba(test[feature_subset])[:,1] / EPOCHS\n    \n    # Convergence\n    label = 'valid_{}'.format(i)\n    ax[0].plot(clf.evals_result_['validation_0']['auc'], label = label)\n    \n    # Feature Importance\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = feature_subset\n    fold_importance_df[\"importance\"] = clf.feature_importances_\n    all_feature_importance_df = pd.concat([all_feature_importance_df, fold_importance_df.sort_values(by = 'feature')], axis=0)\n    print(\"Model Runtime: %0.2f Minutes\"%((time.time() - modelstart)/60))\n    \n    del clf\n    \n    # Scores \n    roc_aucs.append(roc_auc_score(y.iloc[val_idx].values,y_oof[val_idx]))\n    accuracies.append(accuracy_score(y.iloc[val_idx].values,y_oof[val_idx].round()))\n    recalls.append(recall_score(y.iloc[val_idx].values,y_oof[val_idx].round()))\n    precisions.append(precision_score(y.iloc[val_idx].values,y_oof[val_idx].round()))\n    f1_scores.append(f1_score(y.iloc[val_idx].values,y_oof[val_idx].round()))\n    \n    # Roc curve by folds\n    fpr, tpr, t = roc_curve(y.iloc[val_idx].values,y_oof[val_idx])\n    tprs.append(interp(mean_fpr, fpr, tpr))\n    roc_auc = auc(fpr, tpr)\n    aucs.append(roc_auc)\n    ax[1].plot(fpr, tpr, lw=2, alpha=0.3, label='ROC fold %d (AUC = %0.4f)' % (i,roc_auc))\n    \n    # Precion recall by folds\n    precision, recall, _ = precision_recall_curve(y.iloc[val_idx].values,y_oof[val_idx])\n    y_real.append(y.iloc[val_idx].values)\n    y_proba.append(y_oof[val_idx])\n    ax[2].plot(recall, precision, lw=2, alpha=0.3, label='P|R fold %d' % (i))\n    \n    # Confusion matrix by folds\n    cms.append(confusion_matrix(y.iloc[val_idx].values,y_oof[val_idx].round()))\n\n# Convergence\nax[0].set_title(\"GPU XGB Metric Convergence over {} Folds\".format(EPOCHS)) \nax[0].set_ylabel(\"AUC\")\nax[0].set_xlabel(\"Boosting Rounds\")\n\n#ROC \nax[1].plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'grey')\nmean_tpr = np.mean(tprs, axis=0)\nmean_auc = auc(mean_fpr, mean_tpr)\nax[1].plot(mean_fpr, mean_tpr, color='blue',\n         label=r'Mean ROC (AUC = %0.4f)' % (np.mean(roc_aucs)),lw=2, alpha=1)\n\nax[1].set_xlabel('False Positive Rate')\nax[1].set_ylabel('True Positive Rate')\nax[1].set_title('LGB ROC curve by folds')\nax[1].legend(loc=\"lower right\")\n\n# PR plt\nax[2].plot([0,1],[1,0],linestyle = '--',lw = 2,color = 'grey')\ny_real = np.concatenate(y_real)\ny_proba = np.concatenate(y_proba)\nprecision, recall, _ = precision_recall_curve(y_real, y_proba)\nax[2].plot(recall, precision, color='blue',\n         label=r'Mean P|R')\nax[2].set_xlabel('Recall')\nax[2].set_ylabel('Precision')\nax[2].set_title('P|R curve by folds')\nax[2].legend(loc=\"lower left\")\n\nplt.tight_layout(pad=0)\nplt.savefig('model_eval.png')\nplt.show()\n\n# Metrics\nprint(\n'CV roc score        : {0:.4f}, std: {1:.4f}.'.format(np.mean(roc_aucs), np.std(roc_aucs)),\n'\\nCV accuracy score   : {0:.4f}, std: {1:.4f}.'.format(np.mean(accuracies), np.std(accuracies)),\n'\\nCV recall score     : {0:.4f}, std: {1:.4f}.'.format(np.mean(recalls), np.std(recalls)),\n'\\nCV precision score  : {0:.4f}, std: {1:.4f}.'.format(np.mean(precisions), np.std(precisions)),\n'\\nCV f1 score         : {0:.4f}, std: {1:.4f}.'.format(np.mean(f1_scores), np.std(f1_scores))\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot Importance\ncols = all_feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n    by=\"importance\", ascending=False)[:50].index\nbest_features = all_feature_importance_df.loc[all_feature_importance_df.feature.isin(cols)]\nplt.figure(figsize=(8,10))\nsns.barplot(x=\"importance\", y=\"feature\", \n            data=best_features.sort_values(by=\"importance\", ascending=False))\nplt.title('XGB Weight (split) Features (avg over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances.png')\nprint(\"All Model Runtime: %0.2f Minutes\"%((time.time() - allmodelstart)/60))\n\n# Confusion matrix \ndef plot_confusion_matrix(cm, classes,\n                          normalize = False,\n                          title = 'Confusion matrix\"',\n                          cmap = plt.cm.Blues) :\n    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation = 0)\n    plt.yticks(tick_marks, classes)\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])) :\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment = 'center',\n                 color = 'white' if cm[i, j] > thresh else 'black')\n \n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Confusion maxtrix & metrics\nplt.rcParams[\"axes.grid\"] = False\n\ncm = np.average(cms, axis=0).round(1)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm, \n                      classes=class_names, \n                      title= 'XGB Confusion matrix [averaged/folds]')\nplt.show()\n\ncm = np.std(cms, axis=0).round(2)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm, \n                      classes=class_names, \n                      title= 'XGB Confusion matrix [STD/folds]')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Short EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['card1_count_full', 'card1','card2','card2_count_full']\nplot_df = pd.concat([X.loc[:,cols], y], axis =1 )\n\nt_r,t_c = 2, 2\nf, axes = plt.subplots(t_r,t_c, figsize = [12,8],sharex=False, sharey=False)\nrow,col = 0,0\nfor c in cols:\n    if col == t_c:\n        col = 0\n        row += 1\n    sns.kdeplot(plot_df.loc[plot_df.isFraud == 0, c], shade = True, alpha = 0.6, color = 'black', ax = axes[row,col], label = 'Not Fraud')\n    sns.kdeplot(plot_df.loc[plot_df.isFraud == 1, c], shade = True, alpha = 0.6, color = 'lime', ax = axes[row,col], label = 'Fraud')\n    axes[row,col].set_title('{} and Fraud Distribution'.format(c.title()))\n    col+=1\n    \nplt.tight_layout(pad=0)\nplt.show()\ndel plot_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Submit"},{"metadata":{"trusted":true},"cell_type":"code","source":"# When doing feature selection, make sure you use the same subset on test set.\n# LGBM will not break, but it will give you broken predictions.. -_-\nassert X[feature_subset].shape[1] == test[feature_subset].shape[1]\n\nsample_submission['isFraud'] = y_preds\nsample_submission.to_csv('{}_feats_{}fold_lgbm_gpu.csv'.format(len(feature_subset),EPOCHS))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Notebook Runtime: %0.2f Hours\"%((time.time() - notebookstart)/60/60))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}