{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nimport os\nimport gc\n\nimport catboost\n\nfrom catboost import CatBoostClassifier","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"In this competition, you’ll benchmark machine learning models on a challenging large-scale dataset. The data comes from Vesta's real-world e-commerce transactions and contains a wide range of features from device type to product features. You also have the opportunity to create new features to improve your results. \n\nIn this competition you are predicting the probability that an online transaction is fraudulent, as denoted by the binary target isFraud.\n\nThe data is broken into two files identity and transaction, which are joined by TransactionID. Not all transactions have corresponding identity information.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Transaction Table**\n\n1. TransactionDT: timedelta from a given reference datetime (not an actual timestamp)\n2. TransactionAMT: transaction payment amount in USD\n3. ProductCD: product code, the product for each transaction\n4. card1 - card6: payment card information, such as card type, card category, issue bank, country, etc.\n5. addr: address\n6. dist: distance\n7. P_ and (R__) emaildomain: purchaser and recipient email domain\n8. C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.\n9. D1-D15: timedelta, such as days between previous transaction, etc.\n10. M1-M9: match, such as names on card and address, etc.\n11. Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.\n\n*Categorical Features:*\n1. ProductCD\n2. card1 - card6\n3. addr1, addr2\n4. Pemaildomain Remaildomain\n5. M1 - M9","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_transaction = pd.read_csv(\"../input/ieee-fraud-detection/train_transaction.csv\")\nX_identity = pd.read_csv(\"../input/ieee-fraud-detection/train_identity.csv\", usecols=[\"TransactionID\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_transaction.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Identity Table**\n\nVariables in this table are identity information – network connection information (IP, ISP, Proxy, etc) and digital signature (UA/browser/os/version, etc) associated with transactions.\nThey're collected by Vesta’s fraud protection system and digital security partners.\n(The field names are masked and pairwise dictionary will not be provided for privacy protection and contract agreement)\n\n*Categorical Features:*\n1. DeviceType\n2. DeviceInfo\n3. id12 - id38","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"id_with_identity = X_identity[\"TransactionID\"].unique().tolist()\nX_transaction[\"with_identity\"] = X_transaction[\"TransactionID\"].isin(id_with_identity)\nX_transaction.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_transaction.with_identity.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_transaction.isFraud.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_transaction[\"isFraud\"] = X_transaction[\"isFraud\"].astype(bool) \nsns.countplot(x=\"with_identity\", data=X_transaction[X_transaction.isFraud])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=\"with_identity\", data=X_transaction[~X_transaction.isFraud])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_ratio = [len(X_transaction[X_transaction.isFraud & X_transaction.with_identity]),\n                len(X_transaction[X_transaction.isFraud & ~X_transaction.with_identity]),\n                len(X_transaction[~X_transaction.isFraud & X_transaction.with_identity]),\n                len(X_transaction[~X_transaction.isFraud & ~X_transaction.with_identity])]\nclass_ratio = class_ratio / np.min(class_ratio)\nclass_ratio","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_transaction[\"isFraud\"] = X_transaction[\"isFraud\"].astype(bool) \nX_transaction = X_transaction.sort_values([\"TransactionDT\"])\n\ntrain_test_idx = int(len(X_transaction)*0.75)\nX_train_ = X_transaction.loc[:train_test_idx,:]\nX_test = X_transaction.loc[train_test_idx:,:]\nprint(\"Training set (with validation):\", len(X_train_), \"Test set:\", len(X_test))\n\ntrain_valid_idx = int(len(X_train_)*0.75)\nX_train = X_train_.loc[:train_valid_idx,:]\nX_valid = X_train_.loc[train_valid_idx:,:]\nprint(\"Training set:\", len(X_train), \"Validation set:\", len(X_valid))\n\ndel X_transaction\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# seed = 1\n# strat_group1 = X_transaction[X_transaction.isFraud & X_transaction.with_identity]\n# strat_group2 = X_transaction[X_transaction.isFraud & ~X_transaction.with_identity]\n# strat_group3 = X_transaction[~X_transaction.isFraud & X_transaction.with_identity]\n# strat_group4 = X_transaction[~X_transaction.isFraud & ~X_transaction.with_identity]\n# X_transaction[\"isFraud\"] = X_transaction[\"isFraud\"].astype(bool) \n# strat_groups = [X_transaction.isFraud, ~X_transaction.isFraud]\n\n# X_train = pd.DataFrame()\n# X_valid = pd.DataFrame()\n\n# for group in strat_groups:\n#     X_train_partial = X_transaction[group].sample(frac=0.75, random_state=seed)\n#     X_valid_partial = X_transaction[group].sample(frac=0.25, random_state=seed)\n    \n#     X_train = pd.concat([X_train, X_train_partial])\n#     X_valid = pd.concat([X_valid, X_valid_partial])\n\n# del X_transaction, strat_groups\n# , strat_group1, strat_group2, strat_group3, strat_group4\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train.reset_index(drop=True)\nX_train = X_train.sort_values([\"TransactionDT\"])\n\nX_valid = X_valid.reset_index(drop=True)\nX_valid = X_valid.sort_values([\"TransactionDT\"])\n\nX_test = X_test.reset_index(drop=True)\nX_test = X_test.sort_values([\"TransactionDT\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_baseline(X_train, X_valid):\n    seed = 1\n    cat_str_features = [\"ProductCD\", \"P_emaildomain\", \"R_emaildomain\", \"card4\", \"card6\"]\n    cat_str_features.extend([\"M\"+str(i+1) for i in range(9)])\n\n    cat_num_features = [\"addr1\", \"addr2\"]\n    cat_num_features.extend([\"card\"+str(i+1) for i in range(6) if (i!=3 and i!=5)])\n\n    cat_features = cat_str_features + cat_num_features\n    \n    X_train[cat_num_features] = X_train[cat_num_features].astype(\"category\")\n    X_valid[cat_num_features] = X_valid[cat_num_features].astype(\"category\")\n\n    X_train[cat_features] = X_train[cat_features].astype(\"str\").fillna(\"nan\").astype(\"category\")\n    X_valid[cat_features] = X_valid[cat_features].astype(\"str\").fillna(\"nan\").astype(\"category\")\n\n    X_train[cat_features].isnull().mean()\n    \n    from sklearn.preprocessing import StandardScaler\n    from sklearn.utils.class_weight import compute_class_weight\n\n    scaler = StandardScaler()\n    num_features = X_train.select_dtypes(\"number\").drop(columns=[\"TransactionID\", \"TransactionDT\"]).columns\n    print(num_features.tolist())\n\n    scaled_features = pd.DataFrame(scaler.fit_transform(X_train[num_features]), columns=num_features)\n    X_train = X_train.drop(columns=num_features)\n    X_train = pd.concat([X_train, scaled_features], axis=1)\n\n    scaled_features = pd.DataFrame(scaler.transform(X_valid[num_features]), columns=num_features)\n    X_valid = X_valid.drop(columns=num_features)\n    X_valid = pd.concat([X_valid, scaled_features], axis=1)\n\n    y_train = X_train.isFraud.astype(\"uint8\").to_numpy()\n    X_train = X_train.drop(columns=[\"TransactionID\", \"TransactionDT\", \"isFraud\"])\n\n    y_valid = X_valid.isFraud.astype(\"uint8\").to_numpy()\n    X_valid = X_valid.drop(columns=[\"TransactionID\", \"TransactionDT\", \"isFraud\"])\n\n    c_weight = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n\n    del scaled_features\n    gc.collect()\n    \n    cboost = CatBoostClassifier(loss_function=\"Logloss\", random_seed=seed, class_weights=c_weight, cat_features=cat_features, iterations=1000)\n    cboost.fit(X_train, y_train, cat_features=cat_features, eval_set=catboost.Pool(X_valid, label=y_valid, cat_features=cat_features), plot=True, early_stopping_rounds=100)\n\n    from sklearn.metrics import classification_report\n    print(classification_report(y_valid, cboost.predict(X_valid).reshape(-1)))\n    \n    from sklearn.metrics import roc_auc_score\n    print(\"ROC-AUC score:\", roc_auc_score(y_valid, cboost.predict(X_valid).reshape(-1), average=\"weighted\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_baseline(X_train.copy(), X_valid.copy())\n#     precision    recall  f1-score   support\n# 0       0.99      0.93      0.96    106379\n# 1       0.29      0.70      0.41      4348\n\n#     accuracy                           0.92    110727\n#    macro avg       0.64      0.81      0.69    110727\n# weighted avg       0.96      0.92      0.94    110727\n\n# ROC-AUC score: 0.8141551715515302","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cboost2.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sorted(tuple(zip(X_train.columns, cboost2.feature_importances_)), key=lambda x: x[1], reverse=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_day_count(X_train, min_transactiondt, day_of_week=True):\n    seconds_per_day = 60*60*24\n    seconds_per_week = seconds_per_day*7\n    seconds_per_semimonthly = seconds_per_day*15\n    seconds_per_month = seconds_per_day*30\n\n    datetime_bin_day = np.arange(min_transactiondt, np.max(X_train[\"TransactionDT\"])+seconds_per_day, seconds_per_day)\n    datetime_bin_week = np.arange(min_transactiondt, np.max(X_train[\"TransactionDT\"])+seconds_per_week, seconds_per_week)\n    datetime_bin_semimonthly = np.arange(min_transactiondt, np.max(X_train[\"TransactionDT\"])+seconds_per_semimonthly, seconds_per_semimonthly)\n    datetime_bin_month = np.arange(min_transactiondt, np.max(X_train[\"TransactionDT\"])+seconds_per_month, seconds_per_month)\n\n    X_train[\"Day\"] = pd.cut(X_train[\"TransactionDT\"], bins=datetime_bin_day, labels=range(len(datetime_bin_day)-1)).astype(np.uint8).add(1)\n    X_train[\"Week\"] = pd.cut(X_train[\"TransactionDT\"],  bins=datetime_bin_week, labels=range(len(datetime_bin_week)-1)).astype(np.uint8).add(1)\n    X_train[\"SemiMonthly\"] = pd.cut(X_train[\"TransactionDT\"],  bins=datetime_bin_semimonthly, labels=range(len(datetime_bin_semimonthly)-1)).astype(np.uint8).add(1)\n    X_train[\"Month\"] = pd.cut(X_train[\"TransactionDT\"],  bins=datetime_bin_month, labels=range(len(datetime_bin_month)-1)).astype(np.uint8).add(1)\n    \n    if day_of_week:\n        DAYS_IN_WEEK = 7 \n        X_train[\"DayOfWeek\"] = DAYS_IN_WEEK\n        for i in range(1, DAYS_IN_WEEK):\n            X_train.loc[(X_train.Day % DAYS_IN_WEEK) == i, \"DayOfWeek\"] = i\n\n        WEEKS_IN_MONTH = 4 \n        X_train[\"WeekOfMonth4\"] = WEEKS_IN_MONTH\n        for i in range(1, WEEKS_IN_MONTH):\n            X_train.loc[(X_train.Week % WEEKS_IN_MONTH) == i, \"WeekOfMonth4\"] = i\n\n        WEEKS_IN_MONTH = WEEKS_IN_MONTH + 1\n        X_train[\"WeekOfMonth5\"] = WEEKS_IN_MONTH\n        for i in range(1, WEEKS_IN_MONTH):\n            X_train.loc[(X_train.Week % WEEKS_IN_MONTH) == i, \"WeekOfMonth5\"] = i\n    \n    return X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MIN_TRANSACTION_DT = np.min(X_train[\"TransactionDT\"])-1\nX_train = add_day_count(X_train, MIN_TRANSACTION_DT)\nX_train.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fraud_occurrences(transactionInterval): \n    fraud_count_week = X_train.groupby([transactionInterval, \"isFraud\"])[\"isFraud\"].count().to_frame()\\\n            .rename(columns={\"isFraud\": \"count\"}).reset_index()\n\n    total_count = fraud_count_week.groupby([transactionInterval])[\"count\"].sum().to_frame()\n    fraud_count_week = fraud_count_week[fraud_count_week.isFraud].reset_index(drop=True)\n    \n    total_num_fraud = np.sum(fraud_count_week[\"count\"])\n    fraud_count_week[\"cum_count\"] = fraud_count_week[\"count\"].expanding().sum()\n    fraud_count_week[\"cum_count\"] = fraud_count_week[\"cum_count\"].divide(total_num_fraud)\n    fraud_count_week[\"count\"] = fraud_count_week[\"count\"].divide(total_count[\"count\"])\n    \n    sns.set_style(\"darkgrid\")\n    plt.figure(figsize=(15,5))\n    ax = sns.lineplot(y=\"count\", x=transactionInterval, data=fraud_count_week)\n    sns.scatterplot(y=\"count\", x=transactionInterval, data=fraud_count_week)\n    ax.set(xlabel=transactionInterval, ylabel=\"Percentage of Fraud\", title=\"Percentage of Fraud per \" + transactionInterval)\n    # percentage of reported fraud overall transactions made\n    \n    plt.figure(figsize=(15,5))\n    ax = sns.lineplot(y=\"cum_count\", x=transactionInterval, data=fraud_count_week)\n    sns.scatterplot(y=\"cum_count\", x=transactionInterval, data=fraud_count_week)\n    ax.set(xlabel=transactionInterval, ylabel=\"Cumulative Percentage of Fraud\", title=\"Cumulative Percentage of total Frauds per \" + transactionInterval)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fraud_occurrences(\"Day\")\nfraud_occurrences(\"Week\")\nfraud_occurrences(\"SemiMonthly\")\nfraud_occurrences(\"Month\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_category_trends(X_train, col):\n    sns.set_style(\"darkgrid\")\n    fig, ax = plt.subplots(1, 3, figsize=(25,5))\n    \n    order = X_train[col].unique().tolist()\n    \n    sns.countplot(x=col, data=X_train, ax=ax[0], order=order)\n    ax[0].set(xlabel=col, ylabel=\"Transaction Count\", title=\"Total Transactions per \" + col)\n    \n    count_per_category = X_train[X_train.isFraud].groupby([col]).TransactionID.count()\n    fraud_per_category = count_per_category / X_train.groupby([col]).TransactionID.count()\n    fraud_per_total = count_per_category / X_train[X_train.isFraud][\"isFraud\"].astype(bool).sum()\n    \n#     print(count_per_category)\n#     print(X_train.groupby([col]).TransactionID.count())\n    \n    sns.barplot(y=fraud_per_category, x=fraud_per_category.index, ax=ax[1], order=order)\n    ax[1].set(xlabel=col, ylabel=\"Percentage of Fraud\", title=\"Percentage of Fraud per \" + col)\n    \n    sns.barplot(y=fraud_per_total, x=fraud_per_total.index, ax=ax[2], order=order)\n    ax[2].set(xlabel=col, ylabel=\"Percentage of Fraud\", title=\"Distribution of Fraud Occurences per \" + col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_category_trends(X_train, \"DayOfWeek\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_category_trends(X_train, \"WeekOfMonth4\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_category_trends(X_train, \"WeekOfMonth5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train[\"TransactionAmt\"].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.sort_values([\"TransactionAmt\"]).tail(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train[\"isFraud\"] = X_train[\"isFraud\"].astype(\"category\")\nplt.figure(figsize=(15,5))\nsns.boxenplot(x=\"TransactionAmt\", y=\"isFraud\", data=X_train[X_train.TransactionAmt < X_train.TransactionAmt.max()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\namount_quantile_99 = X_train[\"TransactionAmt\"].quantile(0.99)\namount_with_less_99q = X_train[X_train.TransactionAmt <= amount_quantile_99]\namount_with_more_99q = X_train[X_train.TransactionAmt > amount_quantile_99]\n\nprint(\"Number of outliers:\", len(amount_with_more_99q))\n\nX_train[\"isFraud\"] = X_train[\"isFraud\"].astype(\"category\")\nsns.boxenplot(x=\"TransactionAmt\", y=\"isFraud\", data=amount_with_less_99q)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.pointplot(x=\"TransactionAmt\", y=\"isFraud\", data=X_train[X_train.TransactionAmt < X_train.TransactionAmt.max()], color=\"b\")\nsns.pointplot(x=\"TransactionAmt\", y=\"isFraud\", data=amount_with_less_99q, color=\"r\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"amount_with_more_99q.isFraud.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting outliers\nplt.figure(figsize=(15,5))\nsns.boxenplot(x=\"TransactionAmt\", y=\"isFraud\", \n              data=amount_with_more_99q[amount_with_more_99q.TransactionAmt < np.max(amount_with_more_99q[\"TransactionAmt\"])])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"amount_with_less_99q[\"isFraud\"] = amount_with_less_99q[\"isFraud\"].astype(bool)\nplt.figure(figsize=(15,5))\nsns.lineplot(y=\"TransactionAmt\", x=\"Week\", data=amount_with_less_99q, color=\"b\", ci=None)\nsns.lineplot(y=\"TransactionAmt\", x=\"Week\", data=amount_with_less_99q[amount_with_less_99q.isFraud], color=\"r\")\nsns.lineplot(y=\"TransactionAmt\", x=\"Week\", data=amount_with_less_99q[~amount_with_less_99q.isFraud], color=\"g\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.lineplot(y=\"TransactionAmt\", x=\"Week\", data=amount_with_less_99q[amount_with_less_99q.isFraud], color=\"r\", ci=\"sd\")\nsns.lineplot(y=\"TransactionAmt\", x=\"Week\", data=amount_with_less_99q[~amount_with_less_99q.isFraud], color=\"g\", ci=\"sd\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,8))\nsns.boxenplot(y=\"TransactionAmt\", x=\"Week\", data=amount_with_less_99q, hue=\"isFraud\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_trx_features(X_train):\n    avg_trx_week = X_train.groupby([\"Week\"]).TransactionAmt.mean().to_dict()\n    X_train[\"avg_trx_week\"] = X_train.Week.map(avg_trx_week).astype(np.float64)\n    X_train[\"offset_from_mean_week\"] = X_train.TransactionAmt.subtract(X_train.avg_trx_week).abs()\n    X_train[\"offset_from_mean_week_2\"] = X_train[\"offset_from_mean_week\"].pow(2)\n\n    std_trx_week = X_train.groupby([\"Week\"]).TransactionAmt.std().to_dict()\n    X_train[\"std_trx_week\"] = X_train.Week.map(std_trx_week).astype(np.float64)\n    X_train[\"std_from_mean_week\"] = X_train.TransactionAmt.divide(X_train.std_trx_week)\n\n    return X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"amount_with_less_99q = add_trx_features(amount_with_less_99q)\namount_with_less_99q[\"isFraud\"] = amount_with_less_99q[\"isFraud\"].astype(\"category\")\nplt.figure(figsize=(15,5))\nsns.boxenplot(x=\"std_from_mean_week\", y=\"isFraud\", data=amount_with_less_99q)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.boxenplot(x=\"offset_from_mean_week\", y=\"isFraud\", data=amount_with_less_99q)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_trx_rolling_features(X_train):\n    avg_num_trx_day = int(X_train.groupby([\"Day\"])[\"isFraud\"].count().mean())\n    avg_num_trx_week = int(X_train.groupby([\"Week\"])[\"isFraud\"].count().mean())\n    X_train[\"rolling_mean_\" + str(avg_num_trx_week)] = X_train.TransactionAmt.rolling(avg_num_trx_week, min_periods=avg_num_trx_day).mean()\n    X_train[\"rolling_std_\" + str(avg_num_trx_week)] = X_train.TransactionAmt.rolling(avg_num_trx_week, min_periods=avg_num_trx_day).std()\n    X_train[\"rolling_min_\" + str(avg_num_trx_week)] = X_train.TransactionAmt.rolling(avg_num_trx_week, min_periods=avg_num_trx_day).min()\n    X_train[\"rolling_max_\" + str(avg_num_trx_week)] = X_train.TransactionAmt.rolling(avg_num_trx_week, min_periods=avg_num_trx_day).max()\n    X_train[\"rolling_sum_\" + str(avg_num_trx_week)] = X_train.TransactionAmt.rolling(avg_num_trx_week, min_periods=avg_num_trx_day).sum()\n\n    X_train[\"rolling_min_diff_\" + str(avg_num_trx_week)] = X_train.TransactionAmt.subtract(X_train[\"rolling_min_\" + str(avg_num_trx_week)]).abs()\n    X_train[\"rolling_max_diff_\" + str(avg_num_trx_week)] = X_train.TransactionAmt.subtract(X_train[\"rolling_max_\" + str(avg_num_trx_week)]).abs()\n    X_train[\"rolling_sum_diff_\" + str(avg_num_trx_week)] = X_train.TransactionAmt.subtract(X_train[\"rolling_sum_\" + str(avg_num_trx_week)]).abs()\n    \n    return X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"avg_num_trx_week = int(amount_with_less_99q.groupby([\"Week\"])[\"isFraud\"].count().mean())\namount_with_less_99q = add_trx_rolling_features(amount_with_less_99q)\nplt.figure(figsize=(15,5))\nsns.boxenplot(x=\"rolling_max_diff_\" + str(avg_num_trx_week), y=\"isFraud\", \n             data=amount_with_less_99q)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.boxenplot(x=\"rolling_min_diff_\" + str(avg_num_trx_week), y=\"isFraud\", \n             data=amount_with_less_99q)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"amount_with_less_99q[[\"rolling_min_diff_\"+ str(avg_num_trx_week), \"rolling_max_diff_\"+ str(avg_num_trx_week)]].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.boxenplot(x=\"rolling_sum_diff_\" + str(avg_num_trx_week), y=\"isFraud\", \n             data=amount_with_less_99q)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"amount_with_less_99q[[\"offset_from_mean_week\", \"std_from_mean_week\", \"avg_trx_week\", \"std_trx_week\"]].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_category_trends(X_train, \"ProductCD\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"productCD = [\"W\", \"H\", \"C\", \"S\", \"R\"]\nplt.figure(figsize=(15,5))\nX_train[\"isFraud\"] = X_train.isFraud.astype(bool)\nsns.pointplot(x=\"ProductCD\", y=\"TransactionAmt\", data=X_train[X_train.isFraud], color=\"r\", order=productCD)\nsns.pointplot(x=\"ProductCD\", y=\"TransactionAmt\", data=X_train[~X_train.isFraud], color=\"b\", order=productCD)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"amount_with_less_99q[\"isFraud\"] = amount_with_less_99q.isFraud.astype(bool)\nplt.figure(figsize=(15,8))\nsns.boxenplot(x=\"ProductCD\", y=\"TransactionAmt\", hue=\"isFraud\", data=amount_with_less_99q, order=productCD)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train[[\"card1\",\"card2\",\"card3\",\"card4\",\"card5\",\"card6\"]].sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import category_encoders as ce\ntarget_enc = ce.TargetEncoder()\ncard_encoded = target_enc.fit_transform(X_train[[\"card1\",\"card2\",\"card3\",\"card4\",\"card5\",\"card6\"]].astype(\"category\"), X_train.isFraud.astype(bool))\ncard_encoded.columns = [col + \"_encoded\" for col in card_encoded.columns]\nX_train = pd.concat([X_train, card_encoded], axis=1)\nX_train.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_cum_curve(X_train, col, percent=0.2):\n    percent_per_cat = (X_train[col].value_counts(ascending=True) / len(X_train[~X_train[col].isnull()])).to_frame()\n    df_curve = percent_per_cat[col].expanding().sum().reset_index(drop=True).to_frame()\n    \n    percent_index = df_curve[df_curve[col] > percent].index[0] - 1\n    \n    percent2 =  np.round(percent_per_cat[col].tolist()[-1], 2)\n    percent_index2 = df_curve[df_curve[col] > percent2].index[0] - 1\n#     print(percent_per_cat.tail(1)[col])\n\n    plt.figure(figsize=(15,4))\n    label_str = str(percent_index) + \" categories at \" + str(percent*100) + \"%\"\n    plt.axvline(percent_index, 0, 1, color=\"red\", label=label_str)\n    \n    label_str2 = str(percent_index2) + \" categories at \" + str(percent2*100) + \"%\"\n    plt.axvline(percent_index2, 0, 1, color=\"green\", label=label_str2)\n    \n    ax = sns.lineplot(y=df_curve[col], x=df_curve.index)\n    ax.set(title=\"Cumulative percentage curve \" + col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_category(X_train, col, percent=0.2):\n    percent_per_cat = (X_train[col].value_counts(ascending=True) / len(X_train[~X_train[col].isnull()])).to_frame()\n    df_curve = percent_per_cat[col].expanding().sum().to_frame().reset_index()\n    \n    percent_index = df_curve[df_curve[col] > percent_per_cat[col].tolist()[-1]].index[0] - 1\n    \n    minor_cat = df_curve.loc[:percent_index, \"index\"].tolist()\n    \n    X_train[col+\"_reduce\"] = X_train[col]\n    max_value = int(\"9\"*len(str(int(X_train[col].max()))))\n    X_train.loc[X_train[col].isin(minor_cat), col+\"_reduce\"] = max_value\n    \n    return X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train[\"isFraud\"] = X_train[\"isFraud\"].astype(\"category\")\nfor i in [1,2,3,5]:    \n    cardstr = \"card\"+str(i)\n    plot_cum_curve(X_train, cardstr)\n    \n    plt.figure(figsize=(15,4))\n    sns.boxenplot(x=cardstr+\"_encoded\", y=\"isFraud\", data=X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.addr1.astype(\"category\").describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.addr2.astype(\"category\").describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_enc2 = ce.TargetEncoder()\naddr_encoded = target_enc2.fit_transform(X_train[[\"addr1\",\"addr2\"]].astype(\"category\"), X_train.isFraud.astype(bool))\naddr_encoded.columns = [col + \"_encoded\" for col in addr_encoded.columns]\nX_train = pd.concat([X_train, addr_encoded], axis=1)\nX_train.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in [1,2]:\n    addrstr = \"addr\"+str(i)\n    plot_cum_curve(X_train, addrstr)\n    plt.figure(figsize=(15,4))\n    sns.boxenplot(x=addrstr+\"_encoded\", y=\"isFraud\", data=X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_enc3 = ce.TargetEncoder()\nemail_encoded = target_enc3.fit_transform(X_train[[\"P_emaildomain\",\"R_emaildomain\"]].astype(\"category\"), X_train.isFraud.astype(bool))\nemail_encoded.columns = [col + \"_encoded\" for col in email_encoded.columns]\nX_train = pd.concat([X_train, email_encoded], axis=1)\nX_train.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in [\"P\",\"R\"]:\n    emailstr = str(i)+\"_emaildomain\"\n    plot_cum_curve(X_train, emailstr)\n    \n    plt.figure(figsize=(15,4))\n    sns.boxenplot(x=emailstr+\"_encoded\", y=\"isFraud\", data=X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train[\"isFraud\"] = X_train[\"isFraud\"].astype(\"category\")\nplt.figure(figsize=(15, 5))\nsns.boxenplot(x=\"dist1\", y=\"isFraud\", data=X_train[X_train.dist1 < X_train.dist1.quantile(0.99)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 5))\nsns.boxenplot(x=\"dist2\", y=\"isFraud\", data=X_train[X_train.dist2 < X_train.dist2.quantile(0.99)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in ['P_emaildomain','R_emaildomain']:\n    plt.figure(figsize=(15, 20))\n    domainlist = amount_with_less_99q[amount_with_less_99q.isFraud][feature].unique().tolist()\n    sns.countplot(y=feature, x=\"TransactionAmt\", data=amount_with_less_99q[amount_with_less_99q[feature].isin(domainlist)], hue=\"isFraud\")\n#     sns.boxenplot(y=feature, x=\"TransactionAmt\", data=amount_with_less_99q[amount_with_less_99q.isFraud], color=\"r\")\n#     sns.boxenplot(y=feature, x=\"TransactionAmt\", data=amount_with_less_99q[~amount_with_less_99q.isFraud & amount_with_less_99q[feature].isin(domainlist)], color=\"g\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in ['card4','card6']:\n    plt.figure(figsize=(15, 5))\n    domainlist = amount_with_less_99q[amount_with_less_99q.isFraud][feature].unique().tolist()\n    sns.boxenplot(x=feature, y=\"TransactionAmt\", data=amount_with_less_99q[amount_with_less_99q[feature].isin(domainlist)], hue=\"isFraud\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.regplot(x=\"TransactionAmt\", y=\"dist1\", data=amount_with_less_99q[amount_with_less_99q.isFraud])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.regplot(x=\"TransactionAmt\", y=\"dist2\", data=amount_with_less_99q[amount_with_less_99q.isFraud])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.pipeline import Pipeline, make_pipeline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CAT_STR_FEATURES = [\"ProductCD\", \"P_emaildomain\", \"R_emaildomain\", \"card4\", \"card6\"]\nCAT_STR_FEATURES.extend([\"M\"+str(i+1) for i in range(9)])\n\nCAT_NUM_FEATURES = [\"addr1\", \"addr2\"]\nCAT_NUM_FEATURES.extend([\"card\"+str(i+1) for i in range(6) if (i!=3 and i!=5)])\nCAT_FEATURES = CAT_STR_FEATURES + CAT_NUM_FEATURES\n\nNUM_FEATURES = [\"TransactionAmt\"]\nNUM_FEATURES.extend([\"dist1\", \"dist2\"])\nNUM_FEATURES.extend([\"C\"+str(i+1) for i in range(14)])\nNUM_FEATURES.extend([\"D\"+str(i+1) for i in range(15)])\n\nID_FEATURES = [\"TransactionID\", \"TransactionDT\"]\nFLAG_FEATURES = [\"with_identity\"]\nNON_VESTA_FEATURES = CAT_FEATURES + NUM_FEATURES + FLAG_FEATURES\n# VESTA_FEATURES = list(set(X_train.drop(columns=[\"isFraud\"]).columns) - set(NON_VESTA_FEATURES) - set(ID_FEATURES))\nVESTA_FEATURES = list(set(X_transaction.drop(columns=[\"isFraud\"]).columns) - set(NON_VESTA_FEATURES) - set(ID_FEATURES))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_day_count(X_train, min_transactiondt, day_of_week=True):\n    seconds_per_day = 60*60*24\n    seconds_per_week = seconds_per_day*7\n    seconds_per_semimonthly = seconds_per_day*15\n    seconds_per_month = seconds_per_day*30\n\n    datetime_bin_day = np.arange(min_transactiondt, np.max(X_train[\"TransactionDT\"])+seconds_per_day, seconds_per_day)\n    datetime_bin_week = np.arange(min_transactiondt, np.max(X_train[\"TransactionDT\"])+seconds_per_week, seconds_per_week)\n    datetime_bin_semimonthly = np.arange(min_transactiondt, np.max(X_train[\"TransactionDT\"])+seconds_per_semimonthly, seconds_per_semimonthly)\n    datetime_bin_month = np.arange(min_transactiondt, np.max(X_train[\"TransactionDT\"])+seconds_per_month, seconds_per_month)\n\n    X_train[\"Day\"] = pd.cut(X_train[\"TransactionDT\"], bins=datetime_bin_day, labels=range(len(datetime_bin_day)-1)).astype(np.uint8).add(1)\n    X_train[\"Week\"] = pd.cut(X_train[\"TransactionDT\"],  bins=datetime_bin_week, labels=range(len(datetime_bin_week)-1)).astype(np.uint8).add(1)\n    \n    if day_of_week:\n        DAYS_IN_WEEK = 7 \n        X_train[\"DayOfWeek\"] = DAYS_IN_WEEK\n        for i in range(1, DAYS_IN_WEEK):\n            X_train.loc[(X_train.Day % DAYS_IN_WEEK) == i, \"DayOfWeek\"] = i\n\n        WEEKS_IN_MONTH = 4 \n        X_train[\"WeekOfMonth4\"] = WEEKS_IN_MONTH\n        for i in range(1, WEEKS_IN_MONTH):\n            X_train.loc[(X_train.Week % WEEKS_IN_MONTH) == i, \"WeekOfMonth4\"] = i\n\n        WEEKS_IN_MONTH = WEEKS_IN_MONTH + 1\n        X_train[\"WeekOfMonth5\"] = WEEKS_IN_MONTH\n        for i in range(1, WEEKS_IN_MONTH):\n            X_train.loc[(X_train.Week % WEEKS_IN_MONTH) == i, \"WeekOfMonth5\"] = i\n    \n    return X_train\n\ndef add_trx_features(X_train):\n    avg_trx_week = X_train.groupby([\"Week\"]).TransactionAmt.mean().to_dict()\n    X_train[\"avg_trx_week\"] = X_train.Week.map(avg_trx_week).astype(np.float64)\n    X_train[\"offset_from_mean_week\"] = X_train.TransactionAmt.subtract(X_train.avg_trx_week).abs()\n    X_train[\"offset_from_mean_week_2\"] = X_train[\"offset_from_mean_week\"].pow(2)\n\n    std_trx_week = X_train.groupby([\"Week\"]).TransactionAmt.std().to_dict()\n    X_train[\"std_trx_week\"] = X_train.Week.map(std_trx_week).astype(np.float64)\n    X_train[\"std_from_mean_week\"] = X_train.TransactionAmt.divide(X_train.std_trx_week)\n\n    return X_train\n\ndef add_trx_rolling_features(X_train, avg_num_trx_week, avg_num_trx_day):\n    X_train[\"rolling_mean_\" + str(avg_num_trx_week)] = X_train.TransactionAmt.rolling(avg_num_trx_week, min_periods=avg_num_trx_day).mean().fillna(-1)\n    X_train[\"rolling_std_\" + str(avg_num_trx_week)] = X_train.TransactionAmt.rolling(avg_num_trx_week, min_periods=avg_num_trx_day).std().fillna(-1)\n    X_train[\"rolling_min_\" + str(avg_num_trx_week)] = X_train.TransactionAmt.rolling(avg_num_trx_week, min_periods=avg_num_trx_day).min().fillna(0)\n    X_train[\"rolling_max_\" + str(avg_num_trx_week)] = X_train.TransactionAmt.rolling(avg_num_trx_week, min_periods=avg_num_trx_day).max().fillna(0)\n    X_train[\"rolling_sum_\" + str(avg_num_trx_week)] = X_train.TransactionAmt.rolling(avg_num_trx_week, min_periods=avg_num_trx_day).sum().fillna(0)\n\n    X_train[\"rolling_min_diff_\" + str(avg_num_trx_week)] = X_train.TransactionAmt.subtract(X_train[\"rolling_min_\" + str(avg_num_trx_week)]).abs()\n    X_train[\"rolling_max_diff_\" + str(avg_num_trx_week)] = X_train.TransactionAmt.subtract(X_train[\"rolling_max_\" + str(avg_num_trx_week)]).abs()\n    X_train[\"rolling_sum_diff_\" + str(avg_num_trx_week)] = X_train.TransactionAmt.subtract(X_train[\"rolling_sum_\" + str(avg_num_trx_week)]).abs()\n    \n    return X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TransactionDtTransformer(TransformerMixin, BaseEstimator):\n    def __init__(self):\n        self.min_trx_dt = -1\n    \n    def fit(self, X, y=None):\n        self.min_trx_dt = np.min(X[\"TransactionDT\"])-1\n        \n        seconds_per_day = 60*60*24\n        seconds_per_week = seconds_per_day*7\n        seconds_per_semimonthly = seconds_per_day*15\n        seconds_per_month = seconds_per_day*30\n\n        datetime_bin_day = np.arange(self.min_trx_dt, np.max(X[\"TransactionDT\"])+seconds_per_day, seconds_per_day)\n        datetime_bin_week = np.arange(self.min_trx_dt, np.max(X[\"TransactionDT\"])+seconds_per_week, seconds_per_week)\n        datetime_bin_semimonthly = np.arange(self.min_trx_dt, np.max(X[\"TransactionDT\"])+seconds_per_semimonthly, seconds_per_semimonthly)\n        datetime_bin_month = np.arange(self.min_trx_dt, np.max(X[\"TransactionDT\"])+seconds_per_month, seconds_per_month)\n\n        X[\"Day\"] = pd.cut(X[\"TransactionDT\"], bins=datetime_bin_day, labels=range(len(datetime_bin_day)-1)).astype(np.uint8).add(1)\n        X[\"Week\"] = pd.cut(X[\"TransactionDT\"],  bins=datetime_bin_week, labels=range(len(datetime_bin_week)-1)).astype(np.uint8).add(1)\n    \n        self.avg_num_trx_day = int(X.groupby([\"Day\"]).TransactionID.count().mean())\n        self.avg_num_trx_week = int(X.groupby([\"Week\"]).TransactionID.count().mean())\n        \n        X = X.drop(columns=[\"Day\", \"Week\"])\n        \n        return self\n        \n    def transform(self, X):\n        X = add_day_count(X, self.min_trx_dt)\n        X = add_trx_features(X)\n        X = add_trx_rolling_features(X, 1000, 100)\n        X = X.drop(columns=[\"Day\", \"Week\"])\n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_fraud_rate(X_train, y):\n    rate_map = dict()\n    for col in CAT_FEATURES:\n        dtype = X_train[col].dtype\n        X_train[col] = X_train[col].astype(\"category\")\n        \n        fraud_count = X_train[y.astype(bool)].groupby([col]).TransactionID.count()\n        fraud_rate = (fraud_count / X_train.groupby([col]).TransactionID.count()).to_frame()\n        contribution_rate = (fraud_count / y.astype(bool).sum()).to_frame()\n        \n        fraud_rate[\"TransactionID\"] = fraud_rate[\"TransactionID\"] / np.linalg.norm(fraud_rate[\"TransactionID\"], ord=1)\n        \n        rate_map[col] = [fraud_rate.fillna(0).TransactionID.to_dict(), \n                         contribution_rate.fillna(0).TransactionID.to_dict()]\n        X_train[col] = X_train[col].astype(dtype)\n        \n    return rate_map","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_amt_features(X_train, rate_map):\n    X_train[\"TransactionAmt2\"] = X_train[\"TransactionAmt\"].pow(2)\n    \n    for colkey in rate_map.keys():\n        col_name = \"TransactionAmt_\"+colkey\n        fraud_rate = rate_map[colkey][0]\n        contribution_rate = rate_map[colkey][1]\n        \n        X_train[col_name] = X_train[colkey].map(fraud_rate).multiply(X_train.TransactionAmt).pow(2)\n        X_train[col_name] = X_train[col_name].add(X_train[colkey].map(contribution_rate).multiply(X_train.TransactionAmt)).fillna(0)\n        \n    return X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TransactionAmtTransformer(TransformerMixin, BaseEstimator):\n    def __init__(self):\n        self.fraud_rate = dict()\n        self.min_trx_dt = -1\n    \n    def fit(self, X, y=None):\n        self.fraud_rate = compute_fraud_rate(X, y)\n        return self\n        \n    def transform(self, X):\n        X = add_amt_features(X, self.fraud_rate)\n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_category(X_train, col, cat_to_replace, same_count_topn):\n    percent_per_cat = (X_train[col].value_counts(ascending=True) / len(X_train[~X_train[col].isnull()])).to_frame()\n    df_curve = percent_per_cat[col].expanding().sum().to_frame().reset_index()\n    \n    percent_index = df_curve[df_curve[col] > percent_per_cat[col].tolist()[-same_count_topn]].index[0] - 1\n    \n    minor_cat = df_curve.loc[:percent_index, \"index\"].tolist()\n    X_train.loc[X_train[col].isin(minor_cat), col] = cat_to_replace\n    \n    return X_train, minor_cat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def agg_category(X_train, col, category_left=15, same_count_topn=1):\n    i = 0\n    minor_cat_dict = dict()\n    total_categories = X_train[col].nunique()\n    \n    while total_categories > category_left + i:\n        max_value = int(\"9\"*len(str(int(X_train[col].max())))) - i\n        X_train, minor_cat = reduce_category(X_train, col, max_value, same_count_topn)\n        \n        total_categories = X_train[col].nunique()\n        for cat in minor_cat:\n            minor_cat_dict[cat] = max_value\n        i = i + 1\n        \n    return minor_cat_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CategoryReducerTransformer(TransformerMixin, BaseEstimator):\n    def __init__(self):\n        self.rare_cat = dict()\n    \n    def fit(self, X, y=None):\n        for col in X.columns:\n            minor_cat_dict = agg_category(X.copy(), col)\n            self.rare_cat[col] = minor_cat_dict\n        return self\n    \n    def transform(self, X):\n        for col in X.columns: \n            X.loc[X[col].isin(self.rare_cat[col].keys()),col] = X[col].map(self.rare_cat[col])\n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import category_encoders as ce\nclass LeaveOneOutTransformer(TransformerMixin, BaseEstimator):\n    def __init__(self):\n        self.loo_encoder = None\n        self.bin_encoder = None\n        \n    def fit(self, X, y=None):\n        self.loo_encoder = ce.LeaveOneOutEncoder(cols=X.columns)\n        self.loo_encoder.fit(X, y)\n        \n        self.bin_encoder = ce.TargetEncoder(cols=X.columns)\n        self.bin_encoder.fit(X, y)\n        return self\n    \n    def transform(self, X):\n        existing_cols = X.columns\n        X[[col + \"_loo\" for col in X.columns]] = self.loo_encoder.transform(X)\n        X = pd.concat([X, self.bin_encoder.transform(X[existing_cols])], axis=1) \n        X = X.drop(columns=existing_cols)\n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class WeightOfEvidenceTransformer(TransformerMixin, BaseEstimator):\n    def __init__(self):\n        self.woe_encoder = None\n        \n    def fit(self, X, y=None):\n        self.woe_encoder = ce.WOEEncoder(cols=X.columns)\n        self.woe_encoder.fit(X, y)\n        return self\n    \n    def transform(self, X):\n        X[[col + \"_woe\" for col in X.columns]] = self.woe_encoder.transform(X)\n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CategoryTransformer(TransformerMixin, BaseEstimator):\n    def __init__(self):\n        pass\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        for col in X.columns:\n            X[col] = X[col].astype(\"Int64\").astype(str).str.split('.', expand=True)[0]\n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\nclass IterativeImputerTransformer(TransformerMixin, BaseEstimator):\n    def __init__(self, n_nearest_features, max_iter):\n        self.n_nearest_features = n_nearest_features\n        self.max_iter = max_iter\n        self.iterative_imputer = None\n    \n    def fit(self, X, y=None):\n        self.iterative_imputer = IterativeImputer(random_state=1, \n                                                  n_nearest_features=self.n_nearest_features, \n                                                  max_iter=self.max_iter, skip_complete=True)\n        self.iterative_imputer.fit(X)\n        return self\n    \n    def transform(self, X):\n        return pd.DataFrame(self.iterative_imputer.transform(X), columns=X.columns, index=X.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nclass SimpleImputerTransformer(TransformerMixin, BaseEstimator):\n    def __init__(self, strategy, fill_value):\n        self.strategy = strategy\n        self.fill_value = fill_value\n        self.simple_imputer = None\n    \n    def fit(self, X, y=None):\n        self.simple_imputer = SimpleImputer(strategy=self.strategy, fill_value=self.fill_value)\n        self.simple_imputer.fit(X)\n        return self\n    \n    def transform(self, X):\n        return pd.DataFrame(self.simple_imputer.transform(X), columns=X.columns, index=X.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import MissingIndicator\nclass MissingIndicatorTransformer(TransformerMixin, BaseEstimator):\n    def __init__(self):\n        self.columns = None\n        self.missing_indicator = None\n    \n    def fit(self, X, y=None):\n        self.columns = X.columns\n        self.missing_indicator = MissingIndicator()\n        self.missing_indicator.fit(X)\n        return self\n    \n    def transform(self, X):\n        return pd.DataFrame(self.missing_indicator.transform(X), columns=X.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use FeatureUnion instead with ColumnSelector, column names are getting swapped\nclass DfColumnTransformer(TransformerMixin, BaseEstimator):\n    def __init__(self,\n                 transformers, *,\n                 remainder='drop',\n                 sparse_threshold=0.3,\n                 n_jobs=None,\n                 transformer_weights=None,\n                 verbose=False):\n        self.col_transformer = ColumnTransformer(\n            transformers=transformers,\n            remainder=remainder,\n            sparse_threshold=sparse_threshold,\n            n_jobs=n_jobs,\n            transformer_weights=transformer_weights,\n            verbose=verbose)\n        \n        params = self.col_transformer.get_params()\n        self.remainder = params[\"remainder\"]    \n        self.index = None\n        self.columns = list()\n    \n        for tf in params[\"transformers\"]:\n            self.columns.extend(tf[2])\n    \n    def fit(self, X, y=None):\n        self.index = X.index\n        self.columns = X.columns if self.remainder==\"passthrough\" else self.columns\n        self.col_transformer.fit(X)\n        return self\n    \n    def transform(self, X):\n        return pd.DataFrame(self.col_transformer.transform(X), columns=self.columns, index=self.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ColumnSelector(TransformerMixin, BaseEstimator):\n    def __init__(self, columns, inverse=False):\n        self.inverse = inverse\n        self.columns = columns\n        self.receieved_cols = None\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        return X[list(set(X.columns) - set(self.columns)) if self.inverse else self.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.externals.joblib import Parallel, delayed\nfrom sklearn.pipeline import FeatureUnion, _fit_transform_one, _transform_one, _name_estimators\nfrom scipy import sparse\n\nclass FeatureUnionDf(FeatureUnion):\n    def fit_transform(self, X, y=None, **fit_params):\n        results = self._parallel_func(X, y, fit_params, _fit_transform_one)\n        if not results:\n            return np.zeros((X.shape[0], 0))\n\n        Xs, transformers = zip(*results)\n        self._update_transformer_list(transformers)\n\n        if any(sparse.issparse(f) for f in Xs):\n            Xs = sparse.hstack(Xs).tocsr()\n        else:\n            Xs = self._merge_dataframe(Xs)\n        return Xs\n    \n    def transform(self, X):\n        Xs = Parallel(n_jobs=self.n_jobs)(\n            delayed(_transform_one)(trans, X, None, weight)\n            for name, trans, weight in self._iter())\n        if not Xs:\n            # All transformers are None\n            return np.zeros((X.shape[0], 0))\n        if any(sparse.issparse(f) for f in Xs):\n            Xs = sparse.hstack(Xs).tocsr()\n        else:\n            Xs = self._merge_dataframe(Xs)\n        print(Xs.shape)\n        return Xs\n    \n    def _merge_dataframe(self, X):\n        return pd.concat(X, axis=\"columns\", copy=False)\n     \ndef make_uniondf(*transformers, **kwargs):\n    n_jobs = kwargs.pop('n_jobs', None)\n    verbose = kwargs.pop('verbose', False)\n    return FeatureUnionDf(_name_estimators(transformers), n_jobs=n_jobs, verbose=verbose)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel, SelectKBest, chi2, mutual_info_classif, f_classif\nclass FeatureSelectorTransformer(TransformerMixin, BaseEstimator):\n    def __init__(self, cat_score_func, n_features):\n        self.n_features = n_features\n        self.cat_score_func = cat_score_func\n        self.kbest_num = None\n        self.kbest_cat = None\n        self.selected_num_cols = None\n        self.selected_cat_cols = None\n        self.num_columns = None\n        self.cat_columns = None\n        \n    def fit(self, X, y=None):\n        self.cat_columns = CAT_FEATURES + [\"DayOfWeek\", \"WeekOfMonth4\", \"WeekOfMonth5\"]\n        self.num_columns = list(set(X.drop(columns=[\"TransactionID\", \"TransactionDT\"]).columns) - set(self.cat_columns))\n        \n        self.kbest_num = SelectKBest(score_func=f_classif, k=int(self.n_features*0.9))\n        self.kbest_num.fit(X[self.num_columns], y)\n        \n        self.kbest_cat = SelectKBest(score_func=self.cat_score_func, k=int(self.n_features*0.1))\n        self.kbest_cat.fit(X[self.cat_columns], y)\n        \n        return self\n        \n    def transform(self, X):\n        self.kbest_num.transform(X[self.num_columns])\n        self.kbest_cat.transform(X[self.cat_columns])\n        self.selected_num_cols = X[self.num_columns].loc[:,self.kbest_num.get_support()].columns.tolist()\n        self.selected_cat_cols = X[self.cat_columns].loc[:,self.kbest_cat.get_support()].columns.tolist()\n        \n        return X.loc[:, self.selected_num_cols+self.selected_cat_cols]\n#DONT FORGET TO DROP TRANSACTION IDS AND DTS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectFromModel\nclass FeatureSelectorTransformer2(TransformerMixin, BaseEstimator):\n    def __init__(self, model, n_features, threshold):\n        self.model = model\n        self.threshold = threshold\n        self.n_features = n_features\n        self.feature_importances = None\n        self.select_from_model = None\n        \n    def fit(self, X, y=None):\n        self.all_columns = X.columns\n        self.select_from_model = SelectFromModel(self.model, max_features=self.n_features, threshold=self.threshold)\n        self.select_from_model.fit(X.drop(columns=[\"TransactionID\", \"TransactionDT\"]), y)\n        return self\n    \n    def transform(self, X):\n        self.feature_importances = sorted(list(zip(X.columns, self.select_from_model.estimator_.feature_importances_)), key=lambda x: x[1], reverse=True) \n        return X.drop(columns=[\"TransactionID\", \"TransactionDT\"]).loc[:,self.select_from_model.get_support()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, MinMaxScaler\nclass StandardScalerTransformer(TransformerMixin, BaseEstimator):\n    def __init__(self):\n        self.scaler = None\n        \n    def fit(self, X, y=None):\n        self.scaler = MinMaxScaler()\n        self.scaler.fit(X)\n        return self\n    \n    def transform(self, X):\n        return pd.DataFrame(self.scaler.transform(X), columns=X.columns, index=X.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\nclass PolynomialTransformer(TransformerMixin, BaseEstimator):\n    def __init__(self):\n        self.poly = None\n        \n    def fit(self, X, y=None):\n        self.poly = PolynomialFeatures(interaction_only=True, include_bias=False)\n        return self\n    \n    def transform(self, X):\n        dfpoly = self.poly.fit_transform(X)\n        dfpoly = pd.DataFrame(dfpoly[:,len(X.columns):], columns=[\"poly\"+str(i) for i in range(dfpoly.shape[1]-len(X.columns))], index=X.index).astype(np.float32)\n        return pd.concat([X, dfpoly], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import FeatureUnion, make_union\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.feature_selection import SelectFromModel, SelectKBest, chi2, mutual_info_classif, f_classif\n\ndef init_pipeline(X_train):\n    X_train_sample = X_train\n    # X_train_sample = pd.concat([X_train, X_valid]).reset_index(drop=True)\n\n    for col in [\"TransactionAmt\"]:\n        if not np.isnan(X_train_sample[col].quantile(0.99)):\n            X_train_sample = X_train_sample.loc[X_train_sample[col] < X_train_sample[col].quantile(0.99), :]\n\n    y = X_train_sample.isFraud.astype(int)\n    X_train_sample = X_train_sample.drop(columns=[\"isFraud\"])\n\n    class_weight = compute_class_weight(\"balanced\", np.unique(y.to_numpy()), y.to_numpy())\n    class_weight = dict(zip(range(y.nunique()), class_weight))\n    feature_model = RandomForestClassifier(n_estimators=100, max_depth=7, random_state=1, class_weight=class_weight, verbose=4, n_jobs=-1)\n\n    pipeline = make_pipeline(\n        make_uniondf(\n            make_pipeline(\n                ColumnSelector(CAT_NUM_FEATURES),\n                CategoryReducerTransformer(),\n                CategoryTransformer(),\n                WeightOfEvidenceTransformer(),\n                LeaveOneOutTransformer(),\n                verbose=True\n            ),\n            make_pipeline(\n                ColumnSelector(CAT_STR_FEATURES),\n                WeightOfEvidenceTransformer(),\n                LeaveOneOutTransformer(),\n                verbose=True\n            ),\n            make_pipeline(\n                ColumnSelector(VESTA_FEATURES + NUM_FEATURES),\n                SimpleImputerTransformer(strategy=\"constant\", fill_value=-1),\n                verbose=True\n            ),\n            make_pipeline(ColumnSelector(CAT_NUM_FEATURES + CAT_STR_FEATURES + VESTA_FEATURES + NUM_FEATURES,\n                                        inverse=True)),\n            verbose=True\n        ),\n        make_uniondf(\n            make_pipeline(\n                ColumnSelector(VESTA_FEATURES, inverse=True),\n                IterativeImputerTransformer(n_nearest_features=10, max_iter=20),\n                verbose=True\n            ),\n            make_pipeline(ColumnSelector(VESTA_FEATURES)),\n            verbose=True\n        ),\n        make_uniondf(\n            make_pipeline(\n                ColumnSelector([\"C\"+str(i+1) for i in range(14)] + [\"TransactionAmt\"]),\n                PolynomialTransformer()\n            ),\n            make_pipeline(ColumnSelector([\"C\"+str(i+1) for i in range(14)] + [\"TransactionAmt\"], inverse=True)),\n            verbose=True\n        ),\n#         TransactionDtTransformer(),\n#         TransactionAmtTransformer(),\n    #     make_uniondf(\n    #         make_pipeline(\n    #             ColumnSelector(CAT_FEATURES + FLAG_FEATURES + VESTA_FEATURES + [\"DayOfWeek\", \"WeekOfMonth4\", \"WeekOfMonth5\"],\n    #                           inverse=True),\n    #             StandardScalerTransformer()\n    #         ),\n    #         make_pipeline(\n    #             ColumnSelector(CAT_FEATURES + FLAG_FEATURES + VESTA_FEATURES + [\"DayOfWeek\", \"WeekOfMonth4\", \"WeekOfMonth5\"]))\n    #     ),\n    #     FeatureSelectorTransformer(mutual_info_classif, n_features=200),\n        FeatureSelectorTransformer2(feature_model, n_features=350, threshold=-np.inf),\n    #     ColumnSelector(VESTA_FEATURES, inverse=True),\n        verbose=True\n    )\n    \n    return pipeline, X_train_sample, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def execute_pipeline(X_train, X_valid):    \n    pipeline, X_train_sample, y = init_pipeline(X_train)\n    X_train_sample = pipeline.fit_transform(X_train_sample, y)\n    \n    X_valid2 = pipeline.transform(X_valid.drop(columns=[\"isFraud\"]))\n    y_valid = X_valid.isFraud.astype(int)\n    \n    class_weight = compute_class_weight(\"balanced\", np.unique(y.to_numpy()), y.to_numpy())\n    cboost2 = CatBoostClassifier(loss_function=\"Logloss\", random_seed=50, class_weights=class_weight, iterations=1000)\n    cboost2.fit(X_train_sample.to_numpy(), y,\n                eval_set=catboost.Pool(X_valid2.to_numpy(), label=y_valid), \n                plot=True, early_stopping_rounds=100)\n    \n    from sklearn.metrics import classification_report\n    print(classification_report(y_valid, cboost2.predict(X_valid2).reshape(-1)))\n    from sklearn.metrics import roc_auc_score\n    print(roc_auc_score(y_valid, cboost2.predict(X_valid2).reshape(-1), average=\"weighted\"))\n    \n    # X_test2 = pipeline.transform(X_test.drop(columns=[\"isFraud\"]).copy())\n    # y_test = X_test.isFraud.astype(int)\n    \n    print(\"Number of features\", len(X_train_sample.columns))\n    print(\"Features\", X_train_sample.columns)\n    \n    del X_valid, X_valid2, cboost2\n    gc.collect()\n    \n    return X_train_sample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sample = execute_pipeline(X_train.copy(), X_valid.copy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#               precision    recall  f1-score   support\n\n#            0       0.99      0.91      0.95    106379\n#            1       0.24      0.72      0.36      4348\n\n#     accuracy                           0.90    110727\n#    macro avg       0.62      0.81      0.66    110727\n# weighted avg       0.96      0.90      0.92    110727\n\n# 0.8148787640462721","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#               precision    recall  f1-score   support\n\n#            0       0.99      0.88      0.93    106379\n#            1       0.21      0.76      0.32      4348\n\n#     accuracy                           0.88    110727\n#    macro avg       0.60      0.82      0.63    110727\n# weighted avg       0.96      0.88      0.91    110727\n\n# 0.8198649641226113","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#               precision    recall  f1-score   support\n#            0       0.99      0.93      0.96    106379\n#            1       0.29      0.70      0.41      4348\n\n#     accuracy                           0.92    110727\n#    macro avg       0.64      0.81      0.69    110727\n# weighted avg       0.96      0.92      0.94    110727\n\n# ROC-AUC score: 0.8141551715515302","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline, X_train_sample, y = init_pipeline(X_transaction.reset_index(drop=True))\nX_train_sample = pipeline.fit_transform(X_train_sample, y)\n\n# X_valid2 = pipeline.transform(X_valid.drop(columns=[\"isFraud\"]))\n# y_valid = X_valid.isFraud.astype(int)\n\nclass_weight = compute_class_weight(\"balanced\", np.unique(y.to_numpy()), y.to_numpy())\ncboost = CatBoostClassifier(loss_function=\"Logloss\", random_seed=50, class_weights=class_weight, iterations=180)\ncboost.fit(X_train_sample.to_numpy(), y,\n            plot=True, early_stopping_rounds=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"../input/ieee-fraud-detection/test_transaction.csv\")\nidentity = pd.read_csv(\"../input/ieee-fraud-detection/test_identity.csv\", usecols=[\"TransactionID\"])\nid_with_identity = identity[\"TransactionID\"].unique().tolist()\ntest[\"with_identity\"] = test[\"TransactionID\"].isin(id_with_identity)\n\nids = test.TransactionID\ntest = pipeline.transform(test)\nsubmission = pd.concat([ids, pd.Series(cboost.predict(test).reshape(-1))])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}