{"cells":[{"metadata":{},"cell_type":"markdown","source":"###### Reference\n\n* https://www.kaggle.com/suoires1/fraud-detection-eda-and-modeling"},{"metadata":{},"cell_type":"markdown","source":"## Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport sys\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport missingno as msno\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"working_directory_path = \"/kaggle/input/ieee-fraud-detection/\"\nout_dir = \"/kaggle/working/\"\ntmp_dir = \"/kaggle/input/tmp/\"\nos.chdir(working_directory_path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Utils"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Timing decorator\nfrom functools import wraps\nfrom time import time\n\ndef timing(f):\n    @wraps(f)\n    def wrapper(*args, **kwargs):\n        start = time()\n        result = f(*args, **kwargs)\n        end = time()\n        print('Elapsed time: {:.2f} Sec'.format(end-start))\n        return result\n    return wrapper","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## DATA Load / Processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Read CSV and return df\n@timing\ndef data_load():\n    train_identity = pd.read_csv(\"train_identity.csv\")\n    train_transaction = pd.read_csv(\"train_transaction.csv\")\n\n    test_identity = pd.read_csv(\"test_identity.csv\")\n    test_transaction = pd.read_csv(\"test_transaction.csv\")\n\n    train_df = pd.merge(train_identity, train_transaction, on = 'TransactionID', how='right')\n    test_df = pd.merge(test_identity, test_transaction, on = 'TransactionID', how='right')\n    \n    print(\"INFO - Data load and merge complete\")\n    print(\"INFO - Train - \", train_df.shape)\n    print(\"INFO - Submission - \", test_df.shape)\n    \n    return train_df, test_df\n    \n# Create Metadata object for df\n@timing\ndef data_df_metadata(df):\n    \n    total_rows = len(df.index)\n    na_count = df.isna().sum().to_list()\n    na_pct =  list(map(lambda x: round(x / total_rows * 100) , na_count))\n    unique_count = list(map(lambda x: len(df[x].unique()) , df.columns))\n    memory = round((df.memory_usage(index=False, deep=False)/1024**2),2).to_list()\n    \n    print(len(df.columns.to_list()), len(df.dtypes.to_list()), len(df.isna().sum().to_list()), len(na_pct), len(unique_count), len(memory))\n    \n    metadata_dict = {'column': df.columns.to_list(), \n                     'dtype': df.dtypes.to_list(),\n                     'na_count': na_count, \n                     'na_pct': na_pct,\n                     'unique_count': unique_count,\n                     'memory': memory}\n    return pd.DataFrame(metadata_dict)\n\n# Print Categorical info\ndef data_print_categorical(df):\n    for col in df.columns:\n        if (df[col].dtype == 'object'):\n            print(\"---------- ---------- ----------\")\n            print(df[col].describe())\n            print(\"----------\")\n            print(df[col].value_counts(dropna=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_train_df, input_test_df = data_load()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_low_imp = ['V111', 'V108', 'V32', 'V16', 'V22', 'V121', 'V116', 'V106', 'V18', 'V299', 'V58', 'V298', 'V104', 'V115', 'V31', 'V72', 'V71', 'V109', 'V14', 'V79']\n\ninput_train_df = input_train_df.drop(cols_low_imp, axis=1)\ninput_test_df = input_test_df.drop(cols_low_imp, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop columns which have > 50% NAN\nmetadata_df = data_df_metadata(input_train_df)\ncolumns_to_drop = metadata_df[metadata_df['na_pct']>50].column.to_list()\ninput_train_df = input_train_df.drop(columns_to_drop, axis=1)\ninput_test_df = input_test_df.drop(columns_to_drop, axis=1)\n\nmetadata_df = data_df_metadata(input_train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df = input_test_df['TransactionID']\nsubmission_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop unwanted columns TransactionID, P_emaildomain\ninput_train_df = input_train_df.drop('TransactionID', axis=1)\ninput_test_df = input_test_df.drop('TransactionID', axis=1)\n\ninput_train_df = input_train_df.drop('P_emaildomain', axis=1)\ninput_test_df = input_test_df.drop('P_emaildomain', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_print_categorical(input_train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from matplotlib import pyplot as plt\n# import seaborn as sns\n# plt.figure(figsize=(16,6))\n\n# # target_0 = input_train_df[input_train_df['isFraud'] == 0].sample(frac=0.3)\n# # target_1 = input_train_df[input_train_df['isFraud'] == 1]\n# col = 'C4'\n\n# # ax = sns.distplot(target_0[col], hist=False, rug=False)\n# # ax = sns.distplot(target_1[col], hist=False, rug=False)\n# # df = input_train_df.sample(frac=0.5)\n# ax = sns.violinplot(x=\"isFraud\", y=col, data=df)\n\n\n# print(target_0[col].count(), target_0[col].mean())\n# print(target_1[col].count(), target_1[col].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_train_df.fillna(-999, inplace=True)\ninput_test_df.fillna(-999, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Label Encode categorical columns\n\nfrom sklearn.preprocessing import LabelEncoder \n\nmetadata_df = data_df_metadata(input_train_df)\ncol_list = metadata_df[metadata_df['dtype'] == 'object'].column.to_list()\nprint(col_list)\n\nencoders = {}\n\n@timing\ndef setup_encoders(df, col_list):\n    encoders = {}\n    for col in col_list:\n        print('processing: ', col)\n        LE = LabelEncoder() \n        encoders[col] = LE.fit(list(df[col].astype(str).values))\n    return encoders\n\n@timing\ndef encode_data(df, col_list, encoders):\n    for col in col_list:\n        print('processing: ', col)\n        LE = encoders[col]\n        df[col] = LE.transform(list(df[col].astype(str).values)) \n    return df\n\nencoders = setup_encoders(input_train_df, col_list)\n\ninput_train_df = encode_data(input_train_df, col_list, encoders)\ninput_test_df = encode_data(input_test_df, col_list, encoders)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(input_train_df.shape)\nprint(input_test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split input dataframe into n sets and create n models\n\nNUM_MODELS = 10\n\n@timing\ndef data_load_multi_split(train_df):\n    positive_train_df = train_df[train_df.isFraud == 1]\n    negative_train_df = train_df[train_df.isFraud == 0].sample(frac=1, random_state=10)\n    \n    negative_train_df_array = np.array_split(negative_train_df,  NUM_MODELS)\n    \n    result_df_array = []\n    \n    for df in negative_train_df_array:\n        negative_train_df_sample = df.sample(n=22000, random_state=10)\n        combine_df = negative_train_df_sample.append(positive_train_df).sample(frac=1, random_state=10)\n        result_df_array.append(combine_df)\n    \n    return result_df_array","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# isFraud==1 samples = 20k, isFraud==1 samples = 560k\n# Create n sets of 40k records with same isFraud==1 and different isFraud==0 records\n\ninput_df_array = data_load_multi_split(input_train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(input_df_array))\nprint(input_df_array[0].shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndef get_train_test_split(df_array):\n    result_array = []\n    for df in df_array:\n        X_train, X_test = train_test_split(df, test_size = 0.4, random_state = 0)\n        Y_train, X_train, Y_test, X_test = X_train['isFraud'], X_train.drop(['isFraud'], axis=1), X_test['isFraud'], X_test.drop(['isFraud'], axis=1)\n        result_array.append((Y_train, X_train, Y_test, X_test))\n        \n    return result_array","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_test_split_array = get_train_test_split(input_df_array)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MODELS"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn import metrics\nimport lightgbm as lgb\n\n@timing\ndef get_model_MNB(X, target, alpha=1):\n    model = MultinomialNB(alpha=alpha).fit(X, target)\n    return model\n\n@timing\ndef get_model_BNB(X, target, alpha=1):\n    model = BernoulliNB(alpha=alpha).fit(X, target)\n    return model\n\n@timing\ndef get_model_GNB(X, target, alpha=1):\n    model = GaussianNB().fit(X, target)\n    return model\n\n@timing\ndef get_model_RF(X, target):\n    model = RandomForestClassifier()\n    return model.fit(X, target)\n\n@timing\ndef get_model_XGB_simple(X, target):\n    model = xgb.XGBClassifier()\n    return model.fit(X, target)\n\n@timing\ndef get_model_XGB_custom(X, target):\n    model = xgb.XGBClassifier(n_estimators=250, max_depth=8, learning_rate=0.2, subsample=0.8, nthread=4)\n    return model.fit(X, target)\n\n@timing\ndef get_model_lgb(lgb_train, lgb_val):\n    parameters = {\n        'application': 'binary',\n        'objective': 'binary',\n        'metric': 'auc',\n        'is_unbalance': 'true',\n        'boosting': 'gbdt',\n        'num_leaves': 31,\n        'feature_fraction': 0.5,\n        'bagging_fraction': 0.5,\n        'bagging_freq': 20,\n        'learning_rate': 0.05,\n        'verbose': 2\n    }\n    model = lgb.train(parameters,\n                       lgb_train,\n                       valid_sets=lgb_val,\n                       num_boost_round=2000,\n                       early_stopping_rounds=100)\n    return model\n\n@timing\ndef get_model_SGD(X, target):\n    model = SGDClassifier(loss='log', penalty='l2',alpha=1e-3, random_state=42,max_iter=5, tol=None)\n    return model.fit(X, target)\n\n@timing\ndef run_exp(model, X_test, Y_test):\n    prediction = model.predict_proba(X_test)\n    score = prediction[:, 1].round(1)\n    plt.hist(score)\n    print(\"ROC_AUC: \", metrics.roc_auc_score(Y_test, score))\n    return model, prediction\n\n@timing\ndef run_exp_lgb(model, X_test, Y_test):\n    prediction = model.predict(X_test, num_iteration=model.best_iteration)\n    print(prediction)\n    plt.hist(prediction)\n    return model, prediction\n\n@timing\ndef run_exp_multi_set(train_test_split_array):\n    set_index = 0\n    models = []\n    \n    for (Y_train, X_train, Y_test, X_test) in train_test_split_array:\n        print(\"processing model for index: \", set_index)\n        \n        model = get_model_XGB_custom(X_train, Y_train)\n        _, pred = run_exp(model, X_test, Y_test)\n        \n        models.append(model)\n        set_index = set_index + 1\n        \n    return models\n\ndef save_submission_multi_set(models, name):\n    \n    output_file_names = []\n    \n    for index, model in enumerate(models):\n        X_test = train_test_split_array[index][3]\n        prediction = model.predict_proba(input_test_df)\n        score = prediction[:, 1].round(3)\n        plt.hist(score)\n        plt.show()\n        submission_dict = {'TransactionID': submission_df, 'isFraud': score}\n        out_df = pd.DataFrame(submission_dict)\n\n        # saving the dataframe \n        os.chdir(out_dir)\n        file_name = name+str(index)+'.csv'\n        out_df.to_csv(file_name, index=False)\n        output_file_names.append(file_name)\n        print('----- processed: ', file_name)\n        \n    return output_file_names\n    \n@timing\ndef save_submission(model, X_test, X_sub, name, isLGB = False):\n    prediction = []\n    if (isLGB):\n        prediction = model.predict(X_test)\n        score = prediction.round(1)\n    else:\n        prediction = model.predict_proba(X_test)\n        score = prediction[:, 1].round(1)\n    print(score)\n    print(len(score))\n    plt.hist(score)\n    submission_dict = {'TransactionID': X_sub, 'isFraud': score}\n    out_df = pd.DataFrame(submission_dict) \n\n    # saving the dataframe \n    os.chdir(out_dir)\n    out_df.to_csv(name, index=False)\n    return score\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We are only using XGB model for this run. check run_exp_multi_set() method above.\nmodels = run_exp_multi_set(train_test_split_array)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import plot_importance, plot_tree\nimport matplotlib.pyplot as plt\n\n# plot feature importance\nfig, ax = plt.subplots(figsize=(14, 40))\nplot_importance(models[0], ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create n various of submission csv. we will combine these outputs later.\n# plot hist to visualize variations in each model.\noutput_file_names = save_submission_multi_set(models, 'multi_set_5OCT_')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read all individual submission csv files and calculate mean for final submission.\n\ndef combine_predictions(output_file_names):\n    sub_dfs = []\n    for file in output_file_names:\n        df = pd.read_csv(file)\n        df = df.set_index('TransactionID')\n        sub_dfs.append(df)\n        \n    combine_df = pd.concat(sub_dfs)\n    combine_df = combine_df.groupby(level=0).mean()\n    combine_df['TransactionID'] = combine_df.index\n    return combine_df\n\ndef save_combine_pred_csv(df, name):\n    # saving the dataframe \n    df.to_csv(name, columns=[\"TransactionID\", \"isFraud\"], index=False)\n    print(\"----- saved submission csv - \", name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combine_pred_df = combine_predictions(output_file_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"save_combine_pred_csv(combine_pred_df, 'final_combine_pred_05OCT_2005.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}