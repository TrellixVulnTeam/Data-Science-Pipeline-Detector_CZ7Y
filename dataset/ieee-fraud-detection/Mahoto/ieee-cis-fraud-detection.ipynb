{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"base_path = \"../input/\"\nstr_trans = 'transaction'\nstr_ident = 'identity'\nstr_train = 'train'\nstr_test = 'test'\n\ntrain_df1 = pd.read_csv(base_path+'%s_%s.csv'%(str_train, str_trans))\ntrain_df2 = pd.read_csv(base_path+'%s_%s.csv'%(str_train, str_ident))\ntest_df1 = pd.read_csv(base_path+'%s_%s.csv'%(str_test, str_trans))\ntest_df2 = pd.read_csv(base_path+'%s_%s.csv'%(str_test, str_ident))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# データの内容を表示する"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df1.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df2.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df1.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df2.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 判定対象のカラムの割合を確認\n学習用として提供されているデータの詐欺取引と判定されている割合は約3.5%となっている。  \n学習用データのレコード数は590540件。  \n詐欺取引と判定されているレコード数は20663件。"},{"metadata":{"trusted":true},"cell_type":"code","source":"fraud = train_df1[train_df1['isFraud'] == 1]\nprint(fraud.shape[0] / train_df1.shape[0])\nprint(fraud.shape[0])\nprint(train_df1.shape[0])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 提供されているデータを結合してモデル構築用に加工する\n* 欠損データが25％を超える場合はそのカラムをドロップする。\n* メモリ効率化のためにint値で16bitと32bit必要なものに型を変更する。"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.merge(train_df1, train_df2, on='TransactionID', how='left')\ntest = pd.merge(test_df1, test_df2, on='TransactionID', how='left')\n\nmiss_val_threshold = 0.25\ncol_to_del = []\n\nfor c in train.columns:\n    if train[c].isnull().sum() > train.shape[0]*miss_val_threshold:\n        col_to_del.append(c)\n\nfor c in test.columns:\n    if train[c].isnull().sum() > test.shape[0]*miss_val_threshold:\n        if c not in col_to_del:\n            col_to_del.append(c)\n\ntrain.drop(columns=col_to_del, inplace = True)\ntest.drop(columns=col_to_del, inplace = True)\n\ntarget = train.pop('isFraud')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.fillna(-1, inplace= True)\ntest.fillna(-1, inplace= True)\n\n# df_train[:15].transpose()\ncol_int32 = ['TransactionID', 'TransactionDT']\n\ncol_int16 = ['TransactionAmt', 'card1', 'card2', 'card3', 'card5', 'addr1', 'addr2',\n           'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14',\n           'D1', 'D10', 'D15', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19',\n           'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30',\n           'V31', 'V32', 'V33', 'V34', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59',\n           'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', \n           'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79',\n           'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89',\n           'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99',\n           'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109',\n           'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119',\n           'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129',\n           'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V279',\n           'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289',\n           'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299',\n           'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309',\n           'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319',\n           'V320', 'V321']\n\nfor c in col_int32:\n    train[c] = train[c].astype(np.int32)\n    test[c] = test[c].astype(np.int32)\n    \nfor c in col_int16:\n    train[c] = train[c].astype(np.int16)\n    test[c] = test[c].astype(np.int16)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# モデル評価のためのデータスプリット関数を作成"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nimport matplotlib.pyplot as plt\n\ndef splitData(X, y):\n    return train_test_split(X, y, test_size=0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# モデル作成用にデータを調整"},{"metadata":{"trusted":true},"cell_type":"code","source":"col_dummies = ['ProductCD', 'card4', 'card6', 'P_emaildomain']\nfor c in col_dummies:\n    train[c] = pd.get_dummies(train[c])\n\ntrain.drop(columns=col_dummies, inplace = True)\ntrain.drop(columns=['TransactionID', 'TransactionDT'], inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RandomForestでモデルを構築してImportanceを算出\n* モデルの精度も表示\n* Importanceは複数回出力して、split時のデータ分布によるImportance値の揺れを軽減させる"},{"metadata":{"trusted":true},"cell_type":"code","source":"for loop in range(3):\n    X_train, X_test, y_train, y_test = splitData(train, target)\n\n    # classifier = GradientBoostingClassifier()\n    classifier = RandomForestClassifier()\n    classifier.fit(X_train, y_train)\n\n    predict = classifier.predict(X_test)\n    print(classification_report(y_test, predict))\n\n    importances = classifier.feature_importances_\n    indices = np.argsort(importances)\n    names = list(X_train.columns)\n    names = [names[idx] for idx in indices[-50:]]\n    print('Importances list')\n    for idx in range(len(indices[-50:])):\n        print('%s: %f'%(names[idx], importances[indices[-50:][idx]]))\n\n    plt.figure(figsize=(15, 10))\n    plt.barh(range(len(names)), importances[indices[-50:]], align='center')\n    plt.yticks(range(len(names)), names)\n    plt.xlabel('Feature importance')\n    plt.ylabel('Feature')\n    plt.xlim(0, 0.2)\n    plt.show()\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}