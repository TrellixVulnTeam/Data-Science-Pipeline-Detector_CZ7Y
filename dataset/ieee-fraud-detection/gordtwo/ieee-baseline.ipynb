{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import LabelEncoder\nimport datetime \nimport lightgbm as lgb\nfrom sklearn import preprocessing\nimport xgboost as xgb\n\nimport catboost as cb\nfrom catboost import CatBoostClassifier, Pool\n\n\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler \nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\n\nimport os\nimport gc\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import and Compress Data"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Alternative: data minification"},{"metadata":{"trusted":true},"cell_type":"code","source":"# for col in train.columns:\n  #  if train[col].dtype=='float64': train[col] = train[col].astype('float32')\n   # if train[col].dtype=='int64': train[col] = train[col].astype('int32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_transaction = reduce_mem_usage(pd.read_csv('../input/ieee-fraud-detection/train_transaction.csv', index_col='TransactionID'))\ntest_transaction = reduce_mem_usage(pd.read_csv('../input/ieee-fraud-detection/test_transaction.csv', index_col='TransactionID'))\n\ntrain_identity = reduce_mem_usage(pd.read_csv('../input/ieee-fraud-detection/train_identity.csv', index_col='TransactionID'))\ntest_identity = reduce_mem_usage(pd.read_csv('../input/ieee-fraud-detection/test_identity.csv', index_col='TransactionID'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"list(train.columns) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train['id_11'].value_counts(dropna=False, normalize=True).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Missing data table"},{"metadata":{"trusted":true},"cell_type":"code","source":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(tt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(missing_data(train), missing_data(test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Drop columns that has more than 90% missing values"},{"metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# potentially removing void columns\n\n#def corret_card_id(x): \n #   x=x.replace('.0','')\n  #  x=x.replace('-999','nan')\n   # return x\n\ndef define_indexes(df):\n    \n    # create date column\n    START_DATE = '2017-12-01'\n    startdate = datetime.datetime.strptime(START_DATE, '%Y-%m-%d')\n    df['TransactionDT'] = df['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\n    \n    df['year'] = df['TransactionDT'].dt.year\n    df['month'] = df['TransactionDT'].dt.month\n    df['dow'] = df['TransactionDT'].dt.dayofweek\n    df['hour'] = df['TransactionDT'].dt.hour\n    df['day'] = df['TransactionDT'].dt.day\n   \n    # create card ID \n    cards_cols= ['card1', 'card2', 'card3', 'card5']\n    for card in cards_cols: \n        if '1' in card: \n            df['card_id']= df[card].map(str)\n        else : \n            df['card_id']+= ' '+df[card].map(str)\n    \n    # small correction of the Card_ID\n    df['card_id']=df['card_id'].apply(corret_card_id)\n\ndefine_indexes(train)\ndefine_indexes(test)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# columns that only have ONE value\none_value_cols = [col for col in train.columns if train[col].nunique() <= 1]\none_value_cols_test = [col for col in test.columns if test[col].nunique() <= 1]\n\n# columns that have more than 90% missing values\nmany_null_cols = [col for col in train.columns if train[col].isnull().sum() / train.shape[0] > 0.9]\nmany_null_cols_test = [col for col in test.columns if test[col].isnull().sum() / test.shape[0] > 0.9]\n\n# columns that have values showing up more than 90% of the time\nbig_top_value_cols = [col for col in train.columns if train[col].value_counts(dropna=False, normalize=True).values[1] > 0.9]\nbig_top_value_cols_test = [col for col in test.columns if test[col].value_counts(dropna=False, normalize=True).values[1] > 0.9]\n\n# AND... we don't want them\ncols_to_drop = list(set(many_null_cols + many_null_cols_test + big_top_value_cols + big_top_value_cols_test + one_value_cols+ one_value_cols_test))\n\ncols_to_drop.remove('isFraud')\n\ntrain.drop(cols_to_drop, axis=1, inplace=True)\ntest.drop(cols_to_drop, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Remove outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"# anything 3 sd away / boxplot\ndef CalcOutliers(df_num): \n\n    # calculating mean and std of the array\n    data_mean, data_std = np.mean(df_num), np.std(df_num)\n\n    # seting the cut line to both higher and lower values\n    # You can change this value\n    cut = data_std * 3\n\n    #Calculating the higher and lower cut values\n    lower, upper = data_mean - cut, data_mean + cut\n\n    # creating an array of lower, higher and total outlier values \n    outliers_lower = [x for x in df_num if x < lower]\n    outliers_higher = [x for x in df_num if x > upper]\n    outliers_total = [x for x in df_num if x < lower or x > upper]\n\n    # array without outlier values\n    outliers_removed = [x for x in df_num if x > lower and x < upper]\n    \n    print('Identified lowest outliers: %d' % len(outliers_lower)) # printing total number of values in lower cut of outliers\n    print('Identified upper outliers: %d' % len(outliers_higher)) # printing total number of values in higher cut of outliers\n    print('Total outlier observations: %d' % len(outliers_total)) # printing total number of values outliers of both sides\n    print('Non-outlier observations: %d' % len(outliers_removed)) # printing total number of non outlier values\n    print(\"Total percentual of Outliers: \", round((len(outliers_total) / len(outliers_removed) )*100, 4)) # Percentual of outliers in points\n    \n    return","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.考虑到缺失值本身可能存在意义，也可以额外增加一列 isnull 特征，从而保留该信息!"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['nulls'] = train.isnull().sum(axis=1)\ntest['nulls'] = test.isnull().sum(axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Filling missing values with MICE or???\ntrain.fillna(-1,inplace=True)\ntest.fillna(-1,inplace=True)\n\n[https://www.kaggle.com/pipboyguy/imputation-of-missing-values-ieee](http://)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from impyute.imputation.cs import mice\n\n#imputed = mice(train.values)\n#mice_ = imputed[:, :]\n# in case the imputed value is smaller than 0\n#for col in train.columns:\n #   if col < 0:\n        \n#id_38 = [0 if id_38 < 0 else id_38 for id_num in id_38]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Label Encoding(is it a good idea?)"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_features = [\"ProductCD\", \"card1\", \"card2\", \"card3\", \"card4\", \"card5\", \"card6\",\n                        \"addr1\", \"addr2\", \"P_emaildomain\", \"R_emaildomain\", \"M1\", \"M2\", \"M3\", \"M4\", \"M5\",\n                        \"M6\", \"M7\", \"M8\", \"M9\", \"DeviceType\", \"DeviceInfo\", \"id_12\", \"id_13\", \"id_14\",\n                        \"id_15\", \"id_16\", \"id_17\", \"id_18\", \"id_19\", \"id_20\", \"id_21\", \"id_22\", \"id_23\",\n                        \"id_24\", \"id_25\", \"id_26\", \"id_27\", \"id_28\", \"id_29\", \"id_30\", \"id_31\", \"id_32\", \n                        \"id_33\", \"id_34\", \"id_35\", \"id_36\", \"id_37\", \"id_38\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in categorical_features:\n    if col in train.columns():\n        le = LabelEncoder()\n        le.fit(list(train[col].astype(str).values) + list(test[col].astype(str).values))\n        train[col] = le.transform(list(train[col].astype(str).values))\n        test[col]  = le.transform(list(test[col].astype(str).values))\n# Alternate code on Kaggle\n#for f in train.columns:\n #   if train[f].dtype=='object' or test[f].dtype=='object': \n  #      lbl = preprocessing.LabelEncoder()\n   #     lbl.fit(list(train[f].values) + list(test[f].values))\n    #    train[f] = lbl.transform(list(train[f].values))\n     #   test[f] = lbl.transform(list(test[f].values)) \n                                \n                                  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **9/20**  \n\n#### Feature Engineering\n\n* Missing value\n* EDA gives insight of \"magic features\"\n* Modeling\n* Hypterparameter tuning \n> Optional: Optimization: Grid search/Bayseian Search\n* Ensembling\n\nBaseline: [](http://)"},{"metadata":{},"cell_type":"markdown","source":"## **Feature Engineering**"},{"metadata":{},"cell_type":"markdown","source":"This is the most common way to scaled down the data. "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"train['TransactionAmt_to_mean_card1'] = train['TransactionAmt'] / train.groupby(['card1'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_mean_card4'] = train['TransactionAmt'] / train.groupby(['card4'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_std_card1'] = train['TransactionAmt'] / train.groupby(['card1'])['TransactionAmt'].transform('std')\ntrain['TransactionAmt_to_std_card4'] = train['TransactionAmt'] / train.groupby(['card4'])['TransactionAmt'].transform('std')\n\ntest['TransactionAmt_to_mean_card1'] = test['TransactionAmt'] / test.groupby(['card1'])['TransactionAmt'].transform('mean')\ntest['TransactionAmt_to_mean_card4'] = test['TransactionAmt'] / test.groupby(['card4'])['TransactionAmt'].transform('mean')\ntest['TransactionAmt_to_std_card1'] = test['TransactionAmt'] / test.groupby(['card1'])['TransactionAmt'].transform('std')\ntest['TransactionAmt_to_std_card4'] = test['TransactionAmt'] / test.groupby(['card4'])['TransactionAmt'].transform('std')\n\ntrain['id_02_to_mean_card1'] = train['id_02'] / train.groupby(['card1'])['id_02'].transform('mean')\ntrain['id_02_to_mean_card4'] = train['id_02'] / train.groupby(['card4'])['id_02'].transform('mean')\ntrain['id_02_to_std_card1'] = train['id_02'] / train.groupby(['card1'])['id_02'].transform('std')\ntrain['id_02_to_std_card4'] = train['id_02'] / train.groupby(['card4'])['id_02'].transform('std')\n\ntest['id_02_to_mean_card1'] = test['id_02'] / test.groupby(['card1'])['id_02'].transform('mean')\ntest['id_02_to_mean_card4'] = test['id_02'] / test.groupby(['card4'])['id_02'].transform('mean')\ntest['id_02_to_std_card1'] = test['id_02'] / test.groupby(['card1'])['id_02'].transform('std')\ntest['id_02_to_std_card4'] = test['id_02'] / test.groupby(['card4'])['id_02'].transform('std')\n\ntrain['D15_to_mean_card1'] = train['D15'] / train.groupby(['card1'])['D15'].transform('mean')\ntrain['D15_to_mean_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('mean')\ntrain['D15_to_std_card1'] = train['D15'] / train.groupby(['card1'])['D15'].transform('std')\ntrain['D15_to_std_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('std')\n\ntest['D15_to_mean_card1'] = test['D15'] / test.groupby(['card1'])['D15'].transform('mean')\ntest['D15_to_mean_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('mean')\ntest['D15_to_std_card1'] = test['D15'] / test.groupby(['card1'])['D15'].transform('std')\ntest['D15_to_std_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('std')\n\ntrain['D15_to_mean_addr1'] = train['D15'] / train.groupby(['addr1'])['D15'].transform('mean')\ntrain['D15_to_mean_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('mean')\ntrain['D15_to_std_addr1'] = train['D15'] / train.groupby(['addr1'])['D15'].transform('std')\ntrain['D15_to_std_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('std')\n\ntest['D15_to_mean_addr1'] = test['D15'] / test.groupby(['addr1'])['D15'].transform('mean')\ntest['D15_to_mean_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('mean')\ntest['D15_to_std_addr1'] = test['D15'] / test.groupby(['addr1'])['D15'].transform('std')\ntest['D15_to_std_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('std')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Selection"},{"metadata":{},"cell_type":"markdown","source":"\nI will use the correlation to drop some features that are high correlated with each other"},{"metadata":{},"cell_type":"markdown","source":"## Model fitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"# In order to save some memory for Kaggle \n\ndel train_transaction, train_identity, test_transaction, test_identity\n\ntarget = train['isFraud'].copy()\n\n# prepare data for our model\nX_train = train.drop('isFraud', axis=1)\nX_train.drop('TransactionDT', axis=1, inplace=True)\nX_test = test.drop('TransactionDT', axis=1)\n\ndel train, test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### Optimizer?\n> https://www.kaggle.com/kabure/extensive-eda-and-modeling-xgb-hyperopt "},{"metadata":{"trusted":true},"cell_type":"code","source":"for f in X_train.select_dtypes(include='category').columns.tolist() + X_train.select_dtypes(include='object').columns.tolist():\n    lbl = LabelEncoder()\n    lbl.fit(list(X_train[f].values) + list(X_test[f].values))\n    X_train[f] = lbl.transform(list(X_train[f].values))\n    X_test[f] = lbl.transform(list(X_test[f].values))  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'num_leaves': 491,\n          'min_child_weight': 0.03454472573214212,\n          'feature_fraction': 0.3797454081646243,\n          'bagging_fraction': 0.4181193142567742,\n          'min_data_in_leaf': 106,\n          'objective': 'binary',\n          'max_depth': -1,\n          'learning_rate': 0.006883242363721497,\n          \"boosting_type\": \"gbdt\",\n          \"bagging_seed\": 11,\n          \"metric\": 'auc',\n          \"verbosity\": -1,\n          'reg_alpha': 0.3899927210061127,\n          'reg_lambda': 0.6485237330340494,\n          'random_state': 47\n         }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"splits = 5\nfolds = KFold(n_splits = splits)\noof = np.zeros(len(X_train))\npredictions = np.zeros(len(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train.values, target.values)):\n    print(\"Fold {}\".format(fold_))\n    train_df, y_train_df = X_train.iloc[trn_idx], target.iloc[trn_idx]\n    valid_df, y_valid_df = X_train.iloc[val_idx], target.iloc[val_idx]\n    \n    trn_data = lgb.Dataset(train_df, label=y_train_df)\n    val_data = lgb.Dataset(valid_df, label=y_valid_df)\n    \n    clf = lgb.train(params,\n                    trn_data,\n                    10000,\n                    valid_sets = [trn_data, val_data],\n                    verbose_eval=500,\n                    early_stopping_rounds=500)\n\n    pred = clf.predict(valid_df)\n    oof[val_idx] = pred\n    print( \"  auc = \", roc_auc_score(y_valid_df, pred) )\n    predictions += clf.predict(X_test) / splits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submission\nsample_submission = pd.read_csv('../input/ieee-fraud-detection/sample_submission.csv', index_col='TransactionID')\nsample_submission = sample_submission.reset_index()\nsample_submission[\"isFraud\"] = predictions\nsample_submission.to_csv(\"lgb_sub_baseline.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}