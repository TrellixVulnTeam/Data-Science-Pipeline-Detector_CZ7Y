{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport os\nimport random\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, regularizers\nfrom sklearn.metrics import roc_auc_score\nimport keras.backend as K\nfrom collections import OrderedDict\nfrom keras.models import Model\nimport seaborn as sns\n\nprint(f'tf={tf.__version__}, keras={keras.__version__}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"SEED = 31\nEPOCHS = 100\nBATCH_SIZE = 128\nTARGET = 'isFraud'\nVALIDATION_PERCENT = 0.08\nDROPOUT_RATE = 0.5  #0.2\nL2 = 0.0001  #0.00001\nLEARNING_RATE = 0.002\nHIDDEN_UNITS = 200  #256\nHIDDEN_LAYERS = 2  #4\nPATIENCE = 4\nDECAY_RATE = 0.5\nDECAY_STEPS = 4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.set_random_seed(seed)\n\n\nseed_everything(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file_folder = '../input/ieee-fraud-detection-preprocess'\ntrain = pd.read_csv(f'{file_folder}/train.csv')\ntest = pd.read_csv(f'{file_folder}/test.csv')\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def group_ratios_and_pcs():\n    excludes = {TARGET}\n    for i in range(1, 340):\n        excludes.add(f'V{i}')\n    cols = set(train.columns.values) - excludes\n    return list(cols)\n\n\ndef _keep(col):\n    if col == TARGET:\n        return False\n    if col.startswith('_pc_'):\n        return False\n    #if '_to_' in col:\n     #   return False\n    return True\n\n\nPREDICTORS = [c for c in train.columns.values if _keep(c)]\n#PREDICTORS = group_ratios_and_pcs()\nprint(f'{len(PREDICTORS)} predictors={PREDICTORS}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_size = int(VALIDATION_PERCENT * len(train))\ntrain, val = train[:-val_size], train[-val_size:]\nprint(f'train={train.shape}, val={val.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train[TARGET]\nx_train = train[PREDICTORS]\ny_val = val[TARGET]\nx_val = val[PREDICTORS]\nx_test = test[PREDICTORS]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Compute ROC-AUC after each epoch\nBased on https://stackoverflow.com/a/46844409/519951"},{"metadata":{"trusted":true},"cell_type":"code","source":"class AucRocCallback(keras.callbacks.Callback):\n    def __init__(self,training_data,validation_data):\n        self.x = training_data[0]\n        self.y = training_data[1]\n        self.x_val = validation_data[0]\n        self.y_val = validation_data[1]\n\n\n    def on_train_begin(self, logs={}):\n        return\n\n    def on_train_end(self, logs={}):\n        return\n\n    def on_epoch_begin(self, epoch, logs={}):\n        return\n\n    def on_epoch_end(self, epoch, logs={}):\n        y_pred = self.model.predict(self.x)[:,1]\n        roc = roc_auc_score(self.y, y_pred)\n        y_pred_val = self.model.predict(self.x_val)[:,1]\n        roc_val = roc_auc_score(self.y_val, y_pred_val)\n        print('epoch=%d, roc-auc: val=%s, train=%s' % (epoch,str(round(roc_val,4)),str(round(roc,4))))\n        return\n\n    def on_batch_begin(self, batch, logs={}):\n        return\n\n    def on_batch_end(self, batch, logs={}):\n        return","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(input_shape):\n  #ini = keras.initializers.he_uniform()\n  ini = keras.initializers.he_normal()\n  reg = regularizers.l2(L2)  \n\n  def _block():\n    res = []\n    for i in range(HIDDEN_LAYERS):\n        if i == 0:\n            res.append(layers.Dense(HIDDEN_UNITS,\n                                    activation=tf.nn.relu,\n                                    kernel_initializer=ini,\n                                    kernel_regularizer=reg,\n                                    input_shape=input_shape,\n                                    name=f'dense_{i}'))\n        else:\n            res.append(layers.Dense(HIDDEN_UNITS,\n                                    activation=tf.nn.relu,\n                                    kernel_initializer=ini,\n                                    kernel_regularizer=reg,\n                                    name=f'dense_{i}'))\n        res.append(layers.BatchNormalization(name=f'batch_norm_{i}'))\n        res.append(layers.Dropout(DROPOUT_RATE, name=f'dropout_{i}'))\n    return res\n  \n  hls = _block()\n  model = keras.Sequential(hls + [\n    layers.Dense(2, activation=tf.nn.softmax, name='output')\n  ])\n  opt = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n  model.compile(optimizer=opt,\n              loss='sparse_categorical_crossentropy',\n              metrics=['acc'])\n  return model\n\n\nmodel = build_model(input_shape=[len(x_train.keys())])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sanity check model is producing output of desired type and shape\nexample_batch = x_train[:10]\nexample_result = model.predict(example_batch)\nexample_result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Log weights"},{"metadata":{"trusted":true},"cell_type":"code","source":"def calc_stats(W):\n    return np.linalg.norm(W, 2), np.mean(W), np.std(W)\n\n\nclass MyDebugWeights(keras.callbacks.Callback):\n    \n    def __init__(self):\n        super(MyDebugWeights, self).__init__()\n        self.weights = []\n        self.tf_session = K.get_session()\n            \n    def on_epoch_end(self, epoch, logs=None):\n        for layer in self.model.layers:\n            name = layer.name\n            for i, w in enumerate(layer.weights):\n                w_value = w.eval(session=self.tf_session)\n                w_norm, w_mean, w_std = calc_stats(np.reshape(w_value, -1))\n                self.weights.append((epoch, \"{:s}/W_{:d}\".format(name, i), \n                                     w_norm, w_mean, w_std))\n    \n    def on_train_end(self, logs=None):\n        for e, k, n, m, s in self.weights:\n            print(\"{:3d} {:30s} {:7.3f} {:7.3f} {:7.3f}\".format(e, k, n, m, s))\n            \n\n\nclass WeightsLogger(keras.callbacks.Callback):\n    \n    def __init__(self):\n        super(WeightsLogger, self).__init__()\n        self.weights = pd.DataFrame(columns=['layer', 'target'])\n        self.tf_session = K.get_session()\n            \n    def on_epoch_end(self, epoch, logs=None):\n        for i, layer in enumerate(self.model.layers):\n            name = layer.name\n            if not name.startswith('dense_'):\n                continue\n            for w in layer.weights:\n                w_value = w.eval(session=self.tf_session)\n                tmp = pd.DataFrame(columns=['layer', 'target'])\n                tmp['target'] = np.reshape(w_value, -1)\n                tmp['layer'] = name\n                self.weights = pd.concat([self.weights, tmp])\n    \n\nweights_logger = WeightsLogger()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Learning rate scheduler"},{"metadata":{"trusted":true},"cell_type":"code","source":"def _learning_rate_scheduler(decay_rate, decay_steps):\n    \n    def _scheduler(epoch, lr):\n        if (epoch + 1) % decay_steps == 0:\n            return lr * decay_rate\n        return lr\n\n    return keras.callbacks.LearningRateScheduler(_scheduler, verbose=0)\n\n\nlr_scheduler = _learning_rate_scheduler(decay_rate=DECAY_RATE, decay_steps=DECAY_STEPS)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train model\nLog gradients, adapted from https://stackoverflow.com/questions/45694344/calculating-gradient-norm-wrt-weights-with-keras"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# The patience parameter is the amount of epochs to check for improvement\nearly_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE)\n\nauc_roc = AucRocCallback(training_data=(x_train, y_train), validation_data=(x_val, y_val))\n\ndef get_gradient_norm_func(model):\n    grads = K.gradients(model.total_loss, model.trainable_weights)\n    #summed_squares = [K.sum(K.square(g)) for g in grads]\n    #norm = K.sqrt(sum(summed_squares))\n    inputs = model._feed_inputs + model._feed_targets + model._feed_sample_weights\n    func = K.function(inputs, [grads])\n    return func\n\n\n# must be placed right before fitting the model\nget_gradient = get_gradient_norm_func(model)\nhistory = model.fit(\n  x_train, y_train,\n  epochs=EPOCHS, verbose=0, batch_size=BATCH_SIZE, validation_data=(x_val, y_val),\n  callbacks=[auc_roc, early_stop, lr_scheduler, weights_logger])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inspect gradients"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.trainable_weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(model.trainable_weights)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ngs = get_gradient([x_train, y_train, np.ones(len(y_train))])[0]\nprint(f'len(gs)={len(gs)}')\ngradients = pd.DataFrame(columns=['layer', 'target'])\nnode_names = [w.name for w in model.trainable_weights]\nfor i, node in enumerate(node_names):\n    if node.startswith('dense_0'):\n        layer = 'h0'\n    elif node.startswith('dense_1'):\n        layer = 'h1'\n    elif node.startswith('dense_2'):\n        layer = 'h2'\n    elif node.startswith('dense_3'):\n        layer = 'h3'\n    elif node.startswith('dense_4'):\n        layer = 'h4'\n    else:\n        continue\n    tmp = pd.DataFrame(columns=['layer', 'target'])\n    tmp['target'] = gs[i].flatten()\n    tmp['layer'] = layer\n    gradients = pd.concat([gradients, tmp])\n    \n    \ngradients.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g_stats = gradients.groupby(['layer'])['target'].agg(['median', 'std', 'min', 'max', 'skew'])\ng_stats.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g_ax = sns.violinplot(x='layer', y='target', data=gradients)\ng_ax.set_title('Gradients')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inspect weights"},{"metadata":{"trusted":true},"cell_type":"code","source":"weights = weights_logger.weights\nweights.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w_stats = weights.groupby(['layer'])['target'].agg(['median', 'std', 'min', 'max', 'skew'])\nw_stats.head(HIDDEN_LAYERS * 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w_ax = sns.violinplot(x='layer', y='target', data=weights)\nw_ax.set_title('Weights')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inspect activations"},{"metadata":{"trusted":true},"cell_type":"code","source":"_sample = train.sample(n=1000)\nx_sample = _sample[PREDICTORS]\ny_sample = _sample[TARGET]\nactivations = pd.DataFrame(columns=['layer', 'target'])\n#layers = ['dense_0', 'dense_1']\nlayers = []\nfor i in range(HIDDEN_LAYERS):\n    layers.append(f'batch_norm_{i}')\n\nfor i, layer in enumerate(layers):\n    intermediate_layer_model = keras.Model(inputs=model.input, outputs=model.get_layer(layer).output)\n    intermediate_output = intermediate_layer_model.predict(x_sample)\n    tmp = pd.DataFrame(columns=['layer', 'target'])\n    tmp['target'] = [a for example in intermediate_output for a in example]\n    tmp['layer'] = f'h{i}'\n    activations = pd.concat([activations, tmp])\n  \nactivations.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stats = activations.groupby(['layer'])['target'].agg(['median', 'std', 'min', 'max', 'skew'])\nstats.head(len(layers))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.violinplot(x='layer', y='target', data=activations)\nax.set_title('Activations')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plot training vs validation errors"},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = pd.DataFrame(history.history)\nhist['epoch'] = history.epoch\nhist.tail(EPOCHS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_history(history):\n  hist = pd.DataFrame(history.history)\n  hist['epoch'] = history.epoch\n  plt.figure()\n  plt.xlabel('Epoch')\n  plt.ylabel('Loss')\n  plt.plot(hist['epoch'], hist['loss'],label='Train')\n  plt.plot(hist['epoch'], hist['val_loss'],label = 'Val')\n  #plt.ylim([0,5])\n  plt.legend()\n  plt.figure()\n  plt.xlabel('Epoch')\n  plt.ylabel('Accuracy')\n  plt.plot(hist['epoch'], hist['acc'],label='Train')\n  plt.plot(hist['epoch'], hist['val_acc'],label = 'Val')\n  #plt.ylim([0,20])\n  plt.legend()\n  plt.figure()\n  plt.xlabel('Learning Rate')\n  plt.ylabel('Loss')\n  plt.plot(hist['lr']*-1, hist['loss'],label='Train')\n  plt.plot(hist['lr']*-1, hist['val_loss'],label = 'Val')\n  plt.legend()\n  plt.show()\n\n\nplot_history(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test = test[PREDICTORS]\nsub = pd.read_csv(f'../input/ieee-fraud-detection/sample_submission.csv')\nsub[TARGET] = model.predict(x_test)[:,1]\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('submission.csv', index=False)\nprint(os.listdir(\".\"))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}