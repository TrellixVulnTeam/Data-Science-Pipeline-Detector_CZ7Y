{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data description\nThreads\n1. https://www.kaggle.com/c/ieee-fraud-detection/discussion/101203"},{"metadata":{},"cell_type":"markdown","source":"# Config"},{"metadata":{"trusted":true},"cell_type":"code","source":"TARGET = 'isFraud'\nPREPROCESS_BLACKLIST = {TARGET, 'TransactionID', 'TransactionDT'}\nFORCE_KEEP = {'dist1'}\nGROUP_CARDINALITY_MAX = 5\nCATEGORICAL_FEATURES = {'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'P_emaildomain', 'R_emaildomain',\n                        'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9',\n                       'id_12', 'id_13', 'id_14', 'id_15', 'id_16',\n       'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_23',\n       'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30',\n       'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37',\n       'id_38', 'DeviceType', 'DeviceInfo'}\n\nPCA_PREFIX = '_pc_'\nPCA_N_COMPONENTS = 19\nPCA_FEATURES = set([f'{PCA_PREFIX}{i}' for i in range(PCA_N_COMPONENTS)])\npca_input = set()\nfor i in range(1, 340):\n    pca_input.add(f'V{i}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Characters such as empty strings '' or numpy.inf are considered NA values\npd.set_option('use_inf_as_na', True)\npd.set_option('display.max_columns', 500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def is_categorical(df, col):\n    return col in CATEGORICAL_FEATURES or pd.api.types.is_string_dtype(df[col])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%%time\nfolder_path = '../input/ieee-fraud-detection'\ntrain_identity = pd.read_csv(f'{folder_path}/train_identity.csv')\ntrain_transaction = pd.read_csv(f'{folder_path}/train_transaction.csv')\ntest_identity = pd.read_csv(f'{folder_path}/test_identity.csv')\ntest_transaction = pd.read_csv(f'{folder_path}/test_transaction.csv')\n\n# let's combine the data and work with the whole dataset\ntrain = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\ntest = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')\ndel train_identity, train_transaction, test_identity, test_transaction","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Set data type for categorical variables\n* Use string instead of `category` dtype. This allows string manipulations later. \n* Imput missing values with UNKNOWN to allow group-by aggregations later."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndef handle_categorical_variables(df, columns):\n    df[columns] = df[columns].astype(str)\n    #df[columns] = df[columns].fillna(imput_value)\n    #df[columns] = df[columns].replace('nan', imput)\n    return df\n\n\ncols = list(CATEGORICAL_FEATURES)\ntrain = handle_categorical_variables(train, cols)\ntest = handle_categorical_variables(test, cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Drop columns that have only one value"},{"metadata":{"trusted":true},"cell_type":"code","source":"one_value_cols = [col for col in train.columns if train[col].nunique() <= 1]\none_value_cols_test = [col for col in test.columns if test[col].nunique() <= 1]\none_value_cols == one_value_cols_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Purchaser address\nConcatenate \"addr1\" and \"addr2\" columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"def combine_address_columns(df):\n    cols = ['addr1', 'addr2']\n    df['addr'] = df[cols].apply(','.join, axis=1)\n    df = df.drop(columns=cols)\n    return df\n\n\ntrain = combine_address_columns(train)\ntest = combine_address_columns(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Handle missing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def _missing_data(df):\n    total = train.isnull().sum().sort_values(ascending=False)\n    percent = (train.isnull().sum()/train.isnull().count()).sort_values(ascending=False)\n    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    return missing_data[missing_data['Total'] > 0]\n\n\nmissing = _missing_data(train)\nmissing.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp = missing.filter(items=['dist1'], axis=0)\ntmp.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = missing[missing['Percent'] > 0.5].index.values\ncols = [col for col in cols if col not in FORCE_KEEP]\nprint(f'drop {len(cols)} columns={cols}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(columns=cols)\ntest = test.drop(columns=cols)\nprint(f'train={train.shape}, test={test.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def imput_mode(col, train, test):\n    imput = train[col].mode()[0]\n    train[col].fillna(imput, inplace = True)\n    test[col].fillna(imput, inplace = True)\n    return imput\n\n\ndef imput_median(col, train, test):\n    imput = train[col].median()\n    train[col].fillna(imput, inplace = True)\n    test[col].fillna(imput, inplace = True)\n    return imput\n    \n\ncols = [col for col in train if col not in PREPROCESS_BLACKLIST and not is_categorical(train, col)]\nfor i, col in enumerate(cols):\n    imput = imput_median(col=col, train=train, test=test)\n    print(f'{i + 1}. \"{col}\" imput median={imput}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sanity check to ensure no missing data\nmissing = _missing_data(train)\nprint(f'[{len(missing) == 0}] train has no missing data')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing = _missing_data(test)\nprint(f'[{len(missing) == 0}] test has no missing data')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Imbalanced class problem"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[TARGET].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test set occurs after train set on the timeline\nUse hold-out validation instead of k-fold. Temporal split."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['TransactionDT'].describe().apply(lambda x: format(x, 'f'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['TransactionDT'].describe().apply(lambda x: format(x, 'f'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create ratios to group statistics"},{"metadata":{"trusted":true},"cell_type":"code","source":"CATEGORICAL_FEATURES.add('addr')\nCATEGORICAL_FEATURES.remove('addr1')\nCATEGORICAL_FEATURES.remove('addr2')\ncmap = {}\nfor cf in CATEGORICAL_FEATURES:\n    cmap[cf] = len(train[cf].unique())\n\nprint(f'cmap={cmap}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"groups = []\nfor k, v in cmap.items():\n    if v <= GROUP_CARDINALITY_MAX:  # max theshold for number of distinct values\n        groups.append([k])\n        \nprint(f'{len(groups)} groups={groups}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndef ratio_to_group(train, test, target_columns, group_columns, statistic):\n    for t in target_columns:\n        d = train.groupby(group_columns)[t].transform(statistic)\n        col = f'{t}_to_{\"_\".join(group_columns)}_{statistic}'\n        train[col] = train[t] / d\n        test[col] = test[t] / d\n        train[col] = train[col].fillna(0)\n        test[col] = test[col].fillna(0)\n    return train, test\n\ncols = ['TransactionAmt', 'dist1']\nstatistics = ['mean', 'std']\nfor g in groups:\n    for s in statistics:\n        train, test = ratio_to_group(train, test, target_columns=cols, group_columns=g, statistic=s)\n\n\ncols = train.columns.values\nprint(f'{len(cols)} columns={cols}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Handle categorical features\nFrequency encoding."},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode(df, col, encoder):\n    df[col] = df[col].map(encoder).fillna(0)\n    assert df[col].isnull().sum() == 0\n\ndef freq_encode(col):\n    encoder = dict(train[col].value_counts(normalize=True))\n    encode(train, col, encoder)\n    encode(test, col, encoder)\n\n\ncols = [col for col in train if col not in PREPROCESS_BLACKLIST and is_categorical(train, col)]\nfe_cols = set(cols)\nprint(f'Frequency encode {len(cols)} columns={cols}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfor col in cols:\n    freq_encode(col)\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Zero-mean, unit-variance normalization"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ncols = set(train.columns.values) - PREPROCESS_BLACKLIST - fe_cols - PCA_FEATURES\ncols = list(cols)\nprint(f'transform {len(cols)} columns={cols}')\npt = PowerTransformer()\npt.fit(train[cols]) \ntrain[cols] = pt.transform(train[cols])\ntest[cols] = pt.transform(test[cols])\n# imput zero for any numerical errors\ntrain = train.fillna(0)\ntest = test.fillna(0)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dimensionality reduction: PCA\nDo standardization before PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndef _pca_features(dfs, cols, n_components, prefix):\n    pca = PCA(n_components=n_components)\n    pca.fit(dfs[0][cols])\n    res = []\n    for df in dfs:\n        pcs = pd.DataFrame(pca.transform(df[cols]))\n        pcs.rename(columns=lambda x: str(prefix)+str(x), inplace=True)\n        df = pd.concat([df, pcs], axis=1)\n        #df.drop(columns=cols, inplace=True)\n        res.append(df)\n    return res\n\n\ncols = pca_input & set(train.columns.values)\ncols = list(cols)\nprint(f'PCA {len(cols)} columns={cols}')\ndfs = _pca_features(dfs=[train, test], cols=cols, n_components=PCA_N_COMPONENTS, prefix=PCA_PREFIX)\ntrain, test = dfs[0], dfs[1]\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sort train by time to allow Temporal split later"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.sort_values(by=['TransactionDT'])\n#val_size = int(0.01 * len(train))\n#val = train[-val_size:]\n#val['isFraud'].value_counts()\n#del val","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Save outputs\nDrop unused columns. Cast to smallest data types possible."},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['TransactionID', 'TransactionDT']\ntrain = train.drop(columns=cols)\ntest = test.drop(columns=cols)\ncols = train.columns.values\nprint(f'{len(cols)} columns={cols}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cdt_map = {TARGET: 'uint8'}\nfor col in test.columns:\n    cdt_map[col] = 'float32'\n\ntrain = train.astype(cdt_map)\ndel cdt_map[TARGET]\ntest = test.astype(cdt_map)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.to_csv('train.csv', index=False)\ntest.to_csv('test.csv', index=False)\nprint(os.listdir(\".\"))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}