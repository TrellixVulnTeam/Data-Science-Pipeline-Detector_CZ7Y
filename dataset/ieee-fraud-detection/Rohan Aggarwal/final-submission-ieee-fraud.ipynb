{"cells":[{"metadata":{},"cell_type":"markdown","source":"**This notebook only contains code for final submission, preprocessing , feature engineering and best parameters for LightGBM can be found here:**\n* https://www.kaggle.com/rohan9889/minify-feature-engineering-ieee-fraud ( Feature Engineering )\n* https://www.kaggle.com/rohan9889/best-params-lightgbm-ieee-fraud ( Best parameters for LightGBM)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport gc\n# importing garbage collector to keep our RAM usgae in check\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Importing data generated using above mentioned kernels.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgbm\ntrain = pd.read_pickle('/kaggle/input/pickle-ieee/Train.pkl')\ntest = pd.read_pickle('/kaggle/input/pickle-ieee/Test.pkl')\ny = train['isFraud']\ndel train['isFraud']\nparams = {\n 'reg_lambda': 0.1,\n 'reg_alpha': 0.1,\n 'num_leaves': 800,\n 'min_data_in_leaf': 100,\n 'learning_rate': 0.05,\n 'feature_fraction': 0.4,\n 'bagging_fraction': 0.1,\n 'verbosity' : -1,\n  'objective' : 'binary',\n  'random_state' : 42,\n  'metric' : 'auc',\n  'max_depth' : -1,\n  'boosting_type': 'gbdt',\n}\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The idea to use sum of average predictions came from notebook - https://www.kaggle.com/tolgahancepel/lightgbm-single-model-and-feature-engineering**\nDo visit this kernel for more details on it"},{"metadata":{},"cell_type":"markdown","source":"We will now use KFolds to split our data, train LGBM model using these folds and predict on our test data.\n\ngc.collect() is called after each fold so that when the refernce to X_train, X_valid, y_train and y_valid are deleted, the orphan memeory can be claimed so that limit on RAM is not exceeded."},{"metadata":{"trusted":true},"cell_type":"code","source":"n_folds = 5\nfolds = KFold(n_splits=n_folds)\ncolumns = train.columns\ny_preds = np.zeros(test.shape[0])\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(train, y)):\n    X_train, X_valid = train[columns].iloc[train_index], train[columns].iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n    \n    temp_train = lgbm.Dataset(X_train, label=y_train)\n    temp_valid = lgbm.Dataset(X_valid, label=y_valid)\n    clf = lgbm.train(params,temp_train, 10000, valid_sets = [temp_train, temp_valid],\n                      verbose_eval=200, early_stopping_rounds=500)\n    \n    y_pred_valid = clf.predict(X_valid)\n    print(\"AUC: \",roc_auc_score(y_valid, y_pred_valid))\n    y_preds += clf.predict(test) / n_folds\n    \n    del X_train, X_valid, y_train, y_valid\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/ieee-fraud-detection/sample_submission.csv')\nsubmission['isFraud'] = y_preds\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}