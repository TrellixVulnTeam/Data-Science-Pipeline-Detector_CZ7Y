{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Hello and welcome to my IEEE fraud detection tutorial!\n\n\n**In this tutorial I will do some basic data preparation, because the data of this competition is not as nice and cleaned up as in other competitions.**\n\n**Afterwards i will handle the missing values, encode categorical variables and finally train and optimize the model and its parameters.**\n\n\n**private score = 0.911683**\n\n**private rank: 3085/6381**\n\n**top 48 %**"},{"metadata":{},"cell_type":"markdown","source":"# Overview\n \n## '1'. Data preparation\n\n## '2'. Analyze features\n\n## '3'. Summary of feature analysis\n\n## '4'. Dropping features with many missing values\n\n## '5'. Impute features with low missing values\n\n## '6'. Impute features with medium missing values\n\n## '7'. Final check if there are still missing values in num. columns\n\n## '8'. Reduce memory usage\n\n## '9'. Dealing with missing values in the cat. features\n\n## '10'. Final check if there are still missing values in cat. columns\n\n## '11'. Concat encoded dataframes\n\n## '12'. Train the model"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import time\n\nstart_time = time.time()\n\nprint(\"loading data takes about 1 minute....\")\n\ntrain_transaction = pd.read_csv('/kaggle/input/ieee-fraud-detection/train_transaction.csv', index_col='TransactionID')\ntest_transaction = pd.read_csv('/kaggle/input/ieee-fraud-detection/test_transaction.csv', index_col='TransactionID')\n\ntrain_identity = pd.read_csv('/kaggle/input/ieee-fraud-detection/train_identity.csv', index_col='TransactionID')\ntest_identity = pd.read_csv('/kaggle/input/ieee-fraud-detection/test_identity.csv', index_col='TransactionID')\n\n#sample_submission = pd.read_csv('/kaggle/input/ieee-fraud-detection/sample_submission.csv', index_col='TransactionID')\n\nprint(\"loading successful!\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Data Preparation"},{"metadata":{},"cell_type":"markdown","source":"**Let's have a look at our data. We will simply do all the basic stuff before we actually modify and prepare the data.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_transaction.info(), \"\\n\")\n#print(train_transaction.describe(), \"\\n\")\n#print(train_transaction.head(), \"\\n\")\nprint(train_transaction.shape, \"\\n\")\nprint(train_transaction.columns, \"\\n\")\n\nprint(test_transaction.shape, \"\\n\")\nprint(test_transaction.columns, \"\\n\")\n\nprint(train_transaction.isFraud, \"\\n\")\n\nprint(train_transaction.isFraud.isnull().sum(), \"\\n\")  # 0 missing values in target\n\nprint(\"percent of fraudulent train-transactions: \", len(train_transaction.loc[train_transaction.isFraud == 1])*100/len(train_transaction))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As we can see,  3.5% in train are fraudulent transactions, so this is a rather imbalanced dataset. We can presume that the percentage of fraudulent transactions in the test data is about the same magnitude.**\n\n**Now we will drop the target from our train_transaction dataframe.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train_transaction[\"isFraud\"]\n\n# drop target column from train dataframe\ntrain_transaction = train_transaction.drop(columns = ['isFraud'])\n\nprint(y_train.shape, \"\\n\")\nprint(train_transaction.shape, \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"print(train_identity.shape, \"\\n\")\nprint(train_identity.columns, \"\\n\")\nprint(train_identity.head(), \"\\n\")\n\nprint(\"\\n\\n\")\n\nprint(test_identity.shape, \"\\n\")\nprint(test_identity.columns, \"\\n\")\nprint(test_identity.head(), \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"train_transaction.index: \\n\", train_transaction.index, \"\\n\")\nprint(\"train_identity.index: \\n\", train_identity.index, \"\\n\")\n\nprint(train_identity.id_01.value_counts(), \"\\n\")    #  the id features seem to have many different values\nprint(train_identity.id_07.value_counts(), \"\\n\")    #  the id features seem to have many different values\nprint(train_identity.DeviceType.value_counts(), \"\\n\")  \nprint(train_identity.DeviceInfo.value_counts(), \"\\n\")  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The 38 different id features seem to have many different values, but the first 10 unique values will cover about 90% of all data (quickly estimated).**\n\n**DeviceType only has 2 different values:  'desktop' and 'mobile'.**\n\n**DeviceInfo has about 5 to 7 different values, that cover about 90% of all data (quickly estimated).**\n\n**Later on we will calculate how many unique values it takes to cover up 90% of data per feature.**\n\n\n\n**Now we will concat our train and test dataframes such that we have one big transaction dataframe and one big identity dataframe.**\n\n\n**This will make the preprocessing of our data much easier compared to doing everything twice on both train and test dataframes.**\n\n**And then before the training process we will simply split the dataframe up into train and test.** "},{"metadata":{"trusted":true},"cell_type":"code","source":"# sadly the id columns of test_identity are called id-01 instead of id_01, which is their name in the train dataframe.\n# hence we must first rename all the 38 id columns in test_identity, before we can concat the dataframes.\n# I simply used print(train_identity.columns) to get the list of correct column names, and now\n# we will just assign them as the column names to the test_identity dataframe.\n\ntest_identity.columns = ['id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07', 'id_08',\n       'id_09', 'id_10', 'id_11', 'id_12', 'id_13', 'id_14', 'id_15', 'id_16',\n       'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24',\n       'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', 'id_32',\n       'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType',\n       'DeviceInfo']\n\n\nprint(\"before concatting: \\n\\n\")\nprint(\"train_transaction.shape: \", train_transaction.shape, \"\\n\")\nprint(\"test_transaction.shape: \", test_transaction.shape, \"\\n\")\nprint(\"train_transaction.index: \", train_transaction.index, \"\\n\")\nprint(\"test_transaction.index: \", test_transaction.index, \"\\n\")\n\nprint(\"train_identity.shape: \", train_identity.shape, \"\\n\")\nprint(\"test_identity.shape: \", test_identity.shape, \"\\n\")\nprint(\"train_identity.index: \", train_identity.index, \"\\n\")\nprint(\"test_identity.index: \", test_identity.index, \"\\n\")\n\n\n\ntransaction_data = pd.concat([train_transaction, test_transaction])\nidentity_data = pd.concat([train_identity, test_identity])\n\n\nprint(\"after concatting: \\n\\n\")\nprint(\"transaction_data.shape: \", transaction_data.shape, \"\\n\")\nprint(\"transaction_data.index: \", transaction_data.index, \"\\n\")\nprint(\"identity_data.shape: \", identity_data.shape, \"\\n\")\nprint(\"identity_data.index: \", identity_data.index, \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets generate some useful lists of columns\n# we want a list of numerical features\n# and a list of categorical features\n\nc = (identity_data.dtypes == 'object')\nn = (identity_data.dtypes != 'object')\ncat_id_cols = list(c[c].index)\nnum_id_cols = list(n[n].index) \n\nprint(cat_id_cols, \"\\n\")\nprint(\"number categorical identity features: \", len(cat_id_cols), \"\\n\\n\")\nprint(num_id_cols, \"\\n\")\nprint(\"number numerical identity features: \", len(num_id_cols))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets generate some useful lists of columns\n# we want a list of numerical features\n# and a list of categorical features\n\nc = (transaction_data.dtypes == 'object')\nn = (transaction_data.dtypes != 'object')\ncat_trans_cols = list(c[c].index)\nnum_trans_cols = list(n[n].index) \n\nprint(cat_trans_cols, \"\\n\")\nprint(\"number categorical transaction features: \", len(cat_trans_cols), \"\\n\\n\")\nprint(num_trans_cols, \"\\n\")\nprint(\"number numerical transaction features: \", len(num_trans_cols))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now we can delete train_transaction, train_identity, test_transaction, test_identity, because we no longer need them.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# we save the shapes in these variables before deleting the dataframes\nshape_of_train_trans = train_transaction.shape\nshape_of_train_id    = train_identity.shape\n\nshape_of_test_trans  = test_transaction.shape\nshape_of_test_id     = test_identity.shape\n\ndel train_transaction\ndel train_identity\ndel test_transaction\ndel test_identity\n\nprint(\"deletion successful!\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Analyze features\n\n## 2.1 Analyze identity features"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(identity_data.id_12, \"\\n\")\nprint(identity_data.id_15, \"\\n\")\nprint(identity_data.id_16, \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As we ca see some id features are categorical features containing strings. The different id features contain different sorts of strings though, let's find out more about these features.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in cat_id_cols:\n    print(identity_data[i].value_counts())\n    print(i, \"missing values: \", identity_data[i].isnull().sum())\n    print(identity_data[i].isnull().sum()*100/ len(identity_data[i]), \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  categorical identity features:\n\n#  id_12:   2 values       0  missing values\n#  id_15:   3 values    8178  missing values\n#  id_16:   2 values   31053  missing values\n#  id_23:   3 values  275909  missing values 96%\n#  id_27:   2 values  275909  missing values 96%\n#  id_28:   2 value     8384  missing values \n#  id_29:   2 values    8384  missing values  \n#  id_30:  75 values  137916  missing values 48%\n#  id_31: 130 values:   9233  missing values  \n#  id_33: 260 values: 142180  missing values 50%\n#  id_34:   4 values: 136160  missing values 47%\n#  id_35:   2 values:   8178  missing values\n#  id_36:   2 values:   8178  missing values\n#  id_37:   2 values:   8178  missing values\n#  id_38:   2 values:   8178  missing values\n#  DeviceType: 2 values 8399  missing values\n#  DeviceInfo: 2799 values,  52417  missing values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"low_missing_cat_id_cols = []      # lower than 15% missing values\nmedium_missing_cat_id_cols = []   # between 15% and 60% missing\nmany_missing_cat_id_cols = []     # more than 60% missing\n\nfor i in cat_id_cols:\n    percentage = identity_data[i].isnull().sum() * 100 / len(identity_data[i])\n    if percentage < 15:\n        low_missing_cat_id_cols.append(i)\n    elif percentage >= 15 and percentage < 60:\n        medium_missing_cat_id_cols.append(i)\n    else:\n        many_missing_cat_id_cols.append(i)\n        \nprint(\"cat_id_cols: \\n\\n\")      \nprint(\"number low missing: \", len(low_missing_cat_id_cols), \"\\n\")\nprint(\"number medium missing: \", len(medium_missing_cat_id_cols), \"\\n\")\nprint(\"number many missing: \", len(many_missing_cat_id_cols), \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in num_id_cols:\n    print(identity_data[i].value_counts())\n    print(i, \"missing values: \", identity_data[i].isnull().sum()) \n    print(identity_data[i].isnull().sum()*100/len(identity_data[i]), \"\\n\") # missing percent","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  numerical identity  features:\n\n#  id_01:       77 values,       0  missing values  \n#  id_02:   115655 values,    8292  missing values \n#  id_03:       24 values,  153335  missing values 54%\n#  id_04:       15 values,  153335  missing values 54%\n#  id_05:       93 values,   14525  missing values \n#  id_06:      101 values,   14525  missing values\n#  id_07:       84 values,  275926  missing values 96%\n#  id_08:       94 values,  275926  missing values 96%\n#  id_09:       46 values,  136876  missing values 48%\n#  id_10:       62 values,  136876  missing values 48%\n#  id_11:      365 values,    8384  missing values\n#  id_13:       54 values,   28534  missing values\n#  id_14:       25 values,  134739  missing values 47%\n#  id_17:      104 values,   10805  missing values\n#  id_18:       18 values,  190152  missing values 66%\n#  id_19:      522 values,   10916  missing values\n#  id_20:      394 values,   11246  missing values\n#  id_21:      490 values,  275922  missing values 96%\n#  id_22:       25 values,  275909  missing values 96%\n#  id_24:       12 values,  276653  missing values 97%\n#  id_25:      341 values,  275969  missing values 96%\n#  id_26:       95 values,  275930  missing values 96%\n#  id_32:        4 values,  137883  missing values 48%","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**So far we have a pretty good overview of our identity features,  but the 378 numerical transaction features are simply too many to evaluate by hand, like we just did with the identity features.**\n\n**Let's have a look at our categorical transaction features.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"low_missing_num_id_cols = []      # lower than 15% missing values\nmedium_missing_num_id_cols = []   # between 15% and 60% missing\nmany_missing_num_id_cols = []     # more than 60% missing\n\nfor i in num_id_cols:\n    percentage = identity_data[i].isnull().sum() * 100 / len(identity_data[i])\n    if percentage < 15:\n        low_missing_num_id_cols.append(i)\n    elif percentage >= 15 and percentage < 60:\n        medium_missing_num_id_cols.append(i)\n    else:\n        many_missing_num_id_cols.append(i)\n        \nprint(\"num_id_cols: \\n\\n\")        \nprint(\"number low missing: \", len(low_missing_num_id_cols), \"\\n\")\nprint(\"number medium missing: \", len(medium_missing_num_id_cols), \"\\n\")\nprint(\"number many missing: \", len(many_missing_num_id_cols), \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Analyze transaction features"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in cat_trans_cols:\n    print(transaction_data[i].value_counts())\n    print(i, transaction_data[i].isnull().sum(), \"missing values\")\n    print(i, transaction_data[i].isnull().sum()*100/len(transaction_data[i]), \"\\n\")  # missing percent","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  categorical transaction features:\n\n#  ProductCD:      5  values,      0 missing values\n#  card4:          4  values,   4663 missing values\n#  card6:          4  values,   4578 missing values \n#  P_emaildomain  59  values, 163648 missing values,15%  \n#  R_emaildomain  60  values, 824070 missing values 75%  \n#  M1:             2  values, 447739 missing values 41%\n#  M2:             2  values, 447739 missing values 41% \n#  M3:             2  values, 447739 missing values 41%\n#  M4:             3  values, 519189 missing values 47%  \n#  M5:             2  values, 660114 missing values 60% \n#  M6:             2  values, 328299 missing values 30% \n#  M7:             2  values, 581283 missing values 53% \n#  M8:             2  values, 581256 missing values 53% \n#  M9:             2  values, 581256 missing values 53% ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**For the 378 numerical cols we have to think of something, because we can not evaluate them by hand.** "},{"metadata":{"trusted":true},"cell_type":"code","source":"low_missing_num_trans_cols = []      # lower than 15% missing values\nmedium_missing_num_trans_cols = []   # between 15% and 60% missing\nmany_missing_num_trans_cols = []     # more than 60% missing\n\nfor i in num_trans_cols:\n    percentage = transaction_data[i].isnull().sum() * 100 / len(transaction_data[i])\n    if percentage < 15:\n        low_missing_num_trans_cols.append(i)\n    elif percentage >= 15 and percentage < 60:\n        medium_missing_num_trans_cols.append(i)\n    else:\n        many_missing_num_trans_cols.append(i)\n        \nprint(\"num_trans_cols: \\n\\n\")        \nprint(\"number low missing: \", len(low_missing_num_trans_cols), \"\\n\")\nprint(\"number medium missing: \", len(medium_missing_num_trans_cols), \"\\n\")\nprint(\"number many missing: \", len(many_missing_num_trans_cols), \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Ok, as we can see there are 155 columns with <15% missing values, we will impute these missing values somehow.**\n\n**For the 56 columns with 15%-60% missing values we will think of something later.**\n\n**For the 167 columns with more than 60% missing values, we will simply drop these columns.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"low_missing_cat_trans_cols = []      # lower than 15% missing values\nmedium_missing_cat_trans_cols = []   # between 15% and 60% missing\nmany_missing_cat_trans_cols = []     # more than 60% missing\n\nfor i in cat_trans_cols:\n    percentage = transaction_data[i].isnull().sum() * 100 / len(transaction_data[i])\n    if percentage < 15:\n        low_missing_cat_trans_cols.append(i)\n    elif percentage >= 15 and percentage < 60:\n        medium_missing_cat_trans_cols.append(i)\n    else:\n        many_missing_cat_trans_cols.append(i)\n        \nprint(\"cat_trans_cols: \\n\\n\")    \nprint(\"number low missing: \", len(low_missing_cat_trans_cols), \"\\n\")\nprint(\"number medium missing: \", len(medium_missing_cat_trans_cols), \"\\n\")\nprint(\"number many missing: \", len(many_missing_cat_trans_cols), \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's summarize what we found out about the features of our 2 dataframes so far:**"},{"metadata":{},"cell_type":"markdown","source":"# 3. Summary of feature analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summary so far:\n\n# we have 2 dataframes:   transaction_data and identity_data\n\n####################################################################\n# features:\n\n# transaction_data:     14 categorical and 378 numerical features\n# identity_data:        17 categorical and  23 numerical features\n####################################################################\n# missing values:\n\n# cat_trans_cols:      4 low,    8 medium,    2 many \n# num_trans_cols:    176 low,   35 medium,  167 many\n\n# cat_id_cols:        11 low,    4 medium,    2 many \n# num_id_cols:         9 low,    6 medium,    8 many\n####################################################################","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We will drop all numerical features with many missing values.**\n\n**We will impute with 'median'  all numerical features with medium missing values.**\n\n**We will impute with 'mean' all numerical features with low missing values.**\n\n**But first let's drop all numerical features with many missing values.**"},{"metadata":{},"cell_type":"markdown","source":"# 4. Dropping features with many missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"shape before dropping num_trans_cols: \", transaction_data.shape, \"\\n\")        \ntransaction_data = transaction_data.drop(columns = many_missing_num_trans_cols)\nprint(\"shape after dropping num_trans_cols: \", transaction_data.shape, \"\\n\\n\")    \n\n\nprint(\"shape before dropping num_id_cols: \", identity_data.shape, \"\\n\")        \nidentity_data = identity_data.drop(columns = many_missing_num_id_cols)\nprint(\"shape after dropping num_id_cols: \", identity_data.shape, \"\\n\")\n\n\n# because we dropped some numerical columns from the dataframe,\n# we must create the list 'num_trans_cols' and\n# 'num_id_cols' again such that the dropped cols are no longer in them\nn = (transaction_data.dtypes != 'object')\nnum_trans_cols = list(n[n].index) \n\nn = (identity_data.dtypes != 'object')\nnum_id_cols = list(n[n].index) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Impute features with low missing values\n\n## 5.1 Impute the transaction features"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\nprint(\"index before imputation: \", transaction_data.index, \"\\n\")\nprint(\"columns before imputation: \", transaction_data.columns, \"\\n\")\n\nprint(\"starting imputation..... \\n\\n\")\nmy_imputer = SimpleImputer(strategy = 'mean') \nmy_imputer.fit(transaction_data[low_missing_num_trans_cols])\n\n#print(\"values before imputing: \", train_transaction[low_missing_num_trans_cols], \"\\n\")\n\ntransaction_data[low_missing_num_trans_cols] = my_imputer.transform(transaction_data[low_missing_num_trans_cols])\n\nprint(\"index after imputation: \", transaction_data.index, \"\\n\")\nprint(\"columns after imputation: \", transaction_data.columns, \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"values after imputing: \", transaction_data[low_missing_num_trans_cols], \"\\n\")\n\nprint(\"As we can see the imputation was successful! \\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.2 Impute the identity features"},{"metadata":{},"cell_type":"markdown","source":"**Now we do the exact same imputation procedure for the train_identity dataframe:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"index before imputation: \", identity_data.index, \"\\n\")\nprint(\"columns before imputation: \", identity_data.columns, \"\\n\")\n\n\nmy_imputer = SimpleImputer(strategy = 'mean') \nmy_imputer.fit(identity_data[low_missing_num_id_cols])\n\nprint(\"starting imputation....\\n\")\nidentity_data[low_missing_num_id_cols] = my_imputer.transform(identity_data[low_missing_num_id_cols])\n\nprint(\"index after imputation: \", identity_data.index, \"\\n\")\nprint(\"columns after imputation: \", identity_data.columns, \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Impute features with medium missing values\n\n## 6.1 Impute the transaction features"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"index before imputation: \", transaction_data.index, \"\\n\")\nprint(\"columns before imputation: \", transaction_data.columns, \"\\n\")\n\nprint(\"values before imputing: \", transaction_data[medium_missing_num_trans_cols], \"\\n\")\n\nprint(\"starting imputation.....\\n\\n\")\nmy_imputer = SimpleImputer(strategy = 'median') \nmy_imputer.fit(transaction_data[medium_missing_num_trans_cols])\n\ntransaction_data[medium_missing_num_trans_cols] = my_imputer.transform(transaction_data[medium_missing_num_trans_cols])\n\nprint(\"index after imputation: \", transaction_data.index, \"\\n\")\nprint(\"columns after imputation: \", transaction_data.columns, \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"values after imputing: \", transaction_data[medium_missing_num_trans_cols], \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6.2 Impute the identity features"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"index before imputation: \", identity_data.index, \"\\n\")\nprint(\"columns before imputation: \", identity_data.columns, \"\\n\")\n\n\nmy_imputer = SimpleImputer(strategy = 'median') \nmy_imputer.fit(identity_data[medium_missing_num_id_cols])\n\nprint(\"values before imputing: \", identity_data[medium_missing_num_id_cols], \"\\n\")\n\nidentity_data[medium_missing_num_id_cols] = my_imputer.transform(identity_data[medium_missing_num_id_cols])\n\nprint(\"index after imputation: \", identity_data.index, \"\\n\")\nprint(\"columns after imputation: \", identity_data.columns, \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7. Final check if there are still missing values in num. columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(transaction_data[num_trans_cols].isnull().sum().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(identity_data[num_id_cols].isnull().sum().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Hooray, no more missing values in our numerical features!**\n\n**Next we will reduce the memory usage of our dataframes.**"},{"metadata":{},"cell_type":"markdown","source":"# 8. Reduce memory usage\n\n**We will maybe run into problems when we dont reduce the memory usage of our dataframes.**\n\n**Sadly one of our dataframe has more than one million rows, hence it really makes sense to try a few methods that are known to reduce the memory usage.**\n\n**I explain these methods in this short tutorial**: https://www.kaggle.com/jonas0/reduce-memory-usage-tutorial"},{"metadata":{},"cell_type":"markdown","source":"\n\n**We will print the memory usage of transaction_data and identity_data and compare with the memory_usage after we have converted the numerical datatypes.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"transaction_data.memory_usage(): \", transaction_data.info(), \"\\n\")  # 1.8 GB\n\nprint(\"identity_data.memory_usage(): \", identity_data.info(), \"\\n\")        #  72 MB","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's get an overview of our features and which datatype they have.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"object_counter = 0\nint_counter = 0\nfloat_counter = 0\n\nnot_detected = []\n\nfor i in transaction_data.columns:\n        if transaction_data[i].dtype == 'object':\n            object_counter += 1\n        elif transaction_data[i].dtype == 'int':\n            int_counter += 1\n        elif transaction_data[i].dtype in ['float', 'float16', 'float32', 'float64']:\n            float_counter += 1\n        else:\n            not_detected.append(i)\n            \nprint(\"transaction_data has \", \"\\n\")\nprint(object_counter, \"object columns, \\n\")\nprint(int_counter, \"int columns, \\n\")\nprint(float_counter, \"float columns \\n\")\n\ntotal = object_counter + int_counter  + float_counter\n\nif total != len(transaction_data.columns):\n    \n    print(\"D DOUBLE DANGER: some columns have not been detected!!\")\n    print(\"these columns have not been detected: \", not_detected)\n    for i in not_detected:\n        print(identity_data[i].dtype, \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"object_counter = 0\nint_counter = 0\nfloat_counter = 0\n\nnot_detected = []\n\nfor i in identity_data.columns:\n        if identity_data[i].dtype == 'object':\n            object_counter += 1\n        elif identity_data[i].dtype == 'int':\n            int_counter += 1\n        elif identity_data[i].dtype in ['float', 'float16', 'float32', 'float64']:\n            float_counter += 1\n        else:\n            not_detected.append(i)\n            \n            \nprint(\"identity_data has \", \"\\n\")\nprint(object_counter, \"object columns, \\n\")\nprint(int_counter, \"int columns, \\n\")\nprint(float_counter, \"float columns \\n\")\n\ntotal = object_counter + int_counter  + float_counter\n\nif total != len(identity_data.columns):\n    \n    print(\"D DOUBLE DANGER: some columns have not been detected!!\")\n    print(\"these columns have not been detected: \", not_detected)    \n    for i in not_detected:\n        print(identity_data[i].dtype, \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the integer datatypes have the following ranges:\n\n#   int8:  -128 to 127, range = 255  \n\n#  int16:  -32,768 to 32,767, range = 65,535\n\n#  int32:  -2,147,483,648 to 2,147,483,647, range = 4,294,967,295\n\n#  int64:  -9,223,372,036,854,775,808 to 9,223,372,036,854,775,807,\n#           range = 18,446,744,073,709,551,615\n\n\n#  By default all numerical columns in pandas are in int64 or float64.\n#  This means that when we find a numerical integer column whose \n#  values do not exceed one of the ranges shown above, we can then\n#  convert this datatype down to a smaller one. ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now we will define a function which takes a list of numerical columns and a dataframe.**\n\n**The function returns a list of lists indicating which columns need to be converted.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#  this function detects all the numerical columns,\n#  that can be converted to a smaller datatype.\n\ndef detect_num_cols_to_shrink(list_of_num_cols, dataframe):\n \n    convert_to_int8 = []\n    convert_to_int16 = []\n    convert_to_int32 = []\n    \n    #  sadly the datatype float8 does not exist\n    convert_to_float16 = []\n    convert_to_float32 = []\n    \n    for col in list_of_num_cols:\n        \n        if dataframe[col].dtype in ['int', 'int8', 'int32', 'int64']:\n            describe_object = dataframe[col].describe()\n            minimum = describe_object[3]\n            maximum = describe_object[7]\n            diff = abs(maximum - minimum)\n\n            if diff < 255:\n                convert_to_int8.append(col)\n            elif diff < 65535:\n                convert_to_int16.append(col)\n            elif diff < 4294967295:\n                convert_to_int32.append(col)   \n                \n        elif dataframe[col].dtype in ['float', 'float16', 'float32', 'float64']:\n            describe_object = dataframe[col].describe()\n            minimum = describe_object[3]\n            maximum = describe_object[7]\n            diff = abs(maximum - minimum)\n\n            if diff < 65535:\n                convert_to_float16.append(col)\n            elif diff < 4294967295:\n                convert_to_float32.append(col) \n        \n    list_of_lists = []\n    list_of_lists.append(convert_to_int8)\n    list_of_lists.append(convert_to_int16)\n    list_of_lists.append(convert_to_int32)\n    list_of_lists.append(convert_to_float16)\n    list_of_lists.append(convert_to_float32)\n    \n    return list_of_lists","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_cols_to_shrink_trans = detect_num_cols_to_shrink(num_trans_cols, transaction_data)\n\nconvert_to_int8 = num_cols_to_shrink_trans[0]\nconvert_to_int16 = num_cols_to_shrink_trans[1]\nconvert_to_int32 = num_cols_to_shrink_trans[2]\n\nconvert_to_float16 = num_cols_to_shrink_trans[3]\nconvert_to_float32 = num_cols_to_shrink_trans[4]\n\nprint(\"convert_to_int8 :\", convert_to_int8, \"\\n\")\nprint(\"convert_to_int16 :\", convert_to_int16, \"\\n\")\nprint(\"convert_to_int32 :\", convert_to_int32, \"\\n\")\n\nprint(\"convert_to_float16 :\", convert_to_float16, \"\\n\")\nprint(\"convert_to_float32 :\", convert_to_float32, \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"starting with converting process....\")\n\nfor col in convert_to_int16:\n    transaction_data[col] = transaction_data[col].astype('int16') \n    \nfor col in convert_to_int32:\n    transaction_data[col] = transaction_data[col].astype('int32') \n\nfor col in convert_to_float16:\n    transaction_data[col] = transaction_data[col].astype('float16')\n    \nfor col in convert_to_float32:\n    transaction_data[col] = transaction_data[col].astype('float32')\n    \nprint(\"successfully converted!\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**And now we do the same with the identity_data.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_cols_to_shrink_id = detect_num_cols_to_shrink(num_id_cols, identity_data)\n\nconvert_to_int8 = num_cols_to_shrink_id[0]\nconvert_to_int16 = num_cols_to_shrink_id[1]\nconvert_to_int32 = num_cols_to_shrink_id[2]\n\nconvert_to_float16 = num_cols_to_shrink_id[3]\nconvert_to_float32 = num_cols_to_shrink_id[4]\n\nprint(\"convert_to_int8 :\", convert_to_int8, \"\\n\")\nprint(\"convert_to_int16 :\", convert_to_int16, \"\\n\")\nprint(\"convert_to_int32 :\", convert_to_int32, \"\\n\")\n\nprint(\"convert_to_float16 :\", convert_to_float16, \"\\n\")\nprint(\"convert_to_float32 :\", convert_to_float32, \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in convert_to_float16:\n    identity_data[col] = identity_data[col].astype('float16')\n    \nfor col in convert_to_float32:\n    identity_data[col] = identity_data[col].astype('float32')\n    \n    \nprint(\"successfully converted!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"transaction_data.memory_usage(): \", transaction_data.info(), \"\\n\")   # now uses 615 MB\n\nprint(\"identity_data.memory_usage(): \", identity_data.info(), \"\\n\")         # now uses 48 MB","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Wow, this really was helpful, since transaction_data went down from 1.8 GB to 615 MB.**\n\n**For identity_data we went down from 72 MB to 48 MB.**"},{"metadata":{},"cell_type":"markdown","source":"# 9. Dealing with missing values in the cat. features"},{"metadata":{},"cell_type":"markdown","source":"**I will try a simple one-hot encoding approach, but it's important that we preprocess the train- and the test-data in the exact same way.**\n\n**Otherwise we cant train our model with the train-data, and then feed the test-data into the model, for this process the train- and test-data\nmust look identical in terms of shape and columns and column-names etc.**\n\n**So far we have only preprocessed our train_transaction and train_identity dataframes, now we will quickly do all the procedures with the test-data:**\n\n**-Drop features with many missing values**\n\n**-Label-encode features with high cardinality**\n\n**-Onehot-Encode features with low cardinality**"},{"metadata":{},"cell_type":"markdown","source":"## 9.1 Drop cat. features with many missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"shape before dropping many_missing_cat_trans_cols: \", transaction_data.shape, \"\\n\")        \ntransaction_data = transaction_data.drop(columns = many_missing_cat_trans_cols)\nprint(\"shape after dropping many_missing_cat_trans_cols: \", transaction_data.shape, \"\\n\\n\")    \n\nprint(\"shape before dropping many_missing_cat_id_cols: \", identity_data.shape, \"\\n\")        \nidentity_data = identity_data.drop(columns = many_missing_cat_id_cols)\nprint(\"shape after dropping many_missing_cat_id_cols: \", identity_data.shape, \"\\n\")\n\n\n# because we dropped some categorical columns from the dataframe,\n# we must create the list 'cat_trans_cols' and\n# 'cat_id_cols' again such that the dropped cols are no longer in them\nc = (transaction_data.dtypes == 'object')\ncat_trans_cols = list(c[c].index) \n\nc = (identity_data.dtypes == 'object')\ncat_id_cols = list(c[c].index) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 9.2 Label-Encode features with high cardinality"},{"metadata":{},"cell_type":"markdown","source":"**First we have to calculate the cardinality of our categorical features of transaction_data.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in cat_trans_cols:\n    print(col, transaction_data[col].nunique(), \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As we can see there two groups of cardinality:  low cardinality with (2,3,4,5) unique values,  and comparibly high cardinality with 60 unique values.**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"low_card_trans_cols = [\"ProductCD\", \"card4\", \"card6\", \"M1\", \"M2\", \"M3\", \"M4\", \"M6\", \"M7\", \"M8\", \"M9\"]\nhigh_card_trans_cols = [\"P_emaildomain\"]\n\nprint(\"lists successfully created!\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**First we will label-encode our high cardinality features.**\n\n**Before we can actually label-encode, we must replace all NaN's with the most frequent value per columns.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in cat_trans_cols:\n    most_frequent_value = transaction_data[i].mode()[0]\n    print(\"For column: \", i, \"the most frequent value is: \", most_frequent_value, \"\\n\")\n    transaction_data[i].fillna(most_frequent_value, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n    \nlabel_encoder = LabelEncoder()\nprint(\"transaction_data.shape before label-encoding: \", transaction_data.shape, \"\\n\")\n\ntransaction_data[high_card_trans_cols] = label_encoder.fit_transform(transaction_data[high_card_trans_cols])\n\nprint(\"transaction_data.shape after label-encoding: \", transaction_data.shape, \"\\n\")\nprint(\"transaction_data[high_card_trans_cols] after label_encoding: \",transaction_data[high_card_trans_cols], \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now we check if the label-encoding was successful:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in cat_id_cols:\n    print(col, identity_data[col].nunique(), \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Here we can also find 2 groups with low and high cardinality.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"low_card_id_cols =  [\"id_12\", \"id_15\", \"id_16\", \"id_28\", \"id_29\", \"id_34\", \"id_35\", \"id_36\", \"id_37\", \"id_38\", \"DeviceType\"]\nhigh_card_id_cols = [\"id_30\", \"id_31\", \"id_33\", \"DeviceInfo\"]\n    \nprint(\"lists successfully created!\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Before we can label-encode, we must remove all NaN's from the dataset.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in cat_id_cols:\n    most_frequent_value = identity_data[i].mode()[0]\n    print(\"For column: \", i, \"the most frequent value is: \", most_frequent_value, \"\\n\")\n    identity_data[i].fillna(most_frequent_value, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_encoder = LabelEncoder()\n\nprint(\"identity_data.shape before label-encoding: \", identity_data.shape, \"\\n\")\n\nfor col in high_card_id_cols:\n    identity_data[col] = label_encoder.fit_transform(identity_data[col])\n\nprint(\"identity_data.shape after label-encoding: \", identity_data.shape, \"\\n\")\nprint(\"identity_data[high_card_id_cols] after label_encoding: \",identity_data[high_card_id_cols], \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(transaction_data.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(identity_data.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 9.3 Onehot-Encode features with low cardinality"},{"metadata":{},"cell_type":"markdown","source":"**For the onehot-encoding-process we sadly have to think a little more than for the imputation-process, because the imputation process did not change the number of columns.**\n\n**When we are going to onehot-encode our low cardinality features, the onehot-encoder will generate many more columns, the column-names, column-values and column-positions will all be generated automatically.**\n\n**Hence we have to create a separate dataframe for each onehot-encoding-process, and then put these dataframes back together afterwards.**\n\n**Due to the generation of extra columns, we can not simply do something like**\n\ndataframe[list_of_columns] = encoder.fit_transform(dataframe[list_of_columns])\n\n**like we did it for the imputation.**  "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"shape before encoding: \", transaction_data.shape, \"\\n\")\nprint(\"columns to encode: \", low_card_trans_cols, \"\\n\")\nprint(\"transaction_data.columns.to_list() before encoding: \", transaction_data.columns.to_list(), \"\\n\")\n\n\n# this line does the onehot encoding\nlow_card_trans_encoded = pd.get_dummies(transaction_data[low_card_trans_cols], dummy_na = False)\ntransaction_data.drop(columns = low_card_trans_cols, inplace = True)\n\nprint(\"shape after encoding: \", transaction_data.shape, \"\\n\\n\")\nprint(\"shape of new dataframe: \", low_card_trans_encoded.shape, \"\\n\\n\")\nprint(\"newly generated columns: \", low_card_trans_encoded.columns, \"\\n\")\nprint(\"low_card_trans_encoded.info(): \", low_card_trans_encoded.info(),\"\\n\")\nprint(\"transaction_data.columns.to_list() after encoding: \", transaction_data.columns.to_list(), \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The cool thing is that the label-encoding and the onehot-encoding process uses uint8/int8 for the integers, hence we do not have to convert everything down to a smaller datatype.**"},{"metadata":{},"cell_type":"markdown","source":"**Now let's do the same thing for the cat. features of the identity dataframe:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"shape before encoding: \", identity_data.shape, \"\\n\")\nprint(\"columns to encode: \", low_card_id_cols, \"\\n\")\n\n# this line does the onehot encoding\nlow_card_id_encoded = pd.get_dummies(identity_data[low_card_id_cols], dummy_na = False)\nidentity_data.drop(columns = low_card_id_cols, inplace = True)\n\n\nprint(\"shape after encoding: \", identity_data.shape, \"\\n\\n\")\nprint(\"shape of new dataframe: \", low_card_id_encoded.shape, \"\\n\\n\")\nprint(\"newly generated columns: \", low_card_id_encoded.columns, \"\\n\")\nprint(\"low_card_id_encoded.info(): \", low_card_id_encoded.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 10. Final check if there are still missing values in cat. columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(transaction_data.isnull().sum().sum(), \"\\n\")\nprint(low_card_trans_encoded.isnull().sum().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(identity_data.isnull().sum().sum(), \"\\n\")\nprint(low_card_id_encoded.isnull().sum().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(transaction_data.info(), \"\\n\")\nprint(low_card_trans_encoded.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(identity_data.info(), \"\\n\")\nprint(low_card_id_encoded.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 11. Concat encoded dataframes"},{"metadata":{},"cell_type":"markdown","source":"**Right now we have our 2 original dataframes transaction_data and identity_data containing the label-encoded categorical features, and our 2 onehot-encoded dataframes:**\n\n1. low_card_trans_encoded\n1. low_card_id_encoded\n\n\n**Now we have to concat these dataframes to one big dataframe, and then we have to split up transaction_data into train_transaction and test_transaction,  and then do the same with identity_data.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"transaction_data.shape before concatting: \", transaction_data.shape, \"\\n\")\nprint(\"low_card_trans_encoded.shape before concatting: \", low_card_trans_encoded.shape, \"\\n\")\n\ntransaction_concatted = pd.concat([transaction_data, low_card_trans_encoded], axis = 1)\n\nprint(\"transaction_concatted.shape after concatting: \", transaction_concatted.shape, \"\\n\")\nprint(\"transaction_concatted.columns after concatting: \", transaction_concatted.columns, \"\\n\")\n\n#del low_card_trans_encoded\n#del transaction_data\n\nprint(transaction_concatted.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now we do the same with the identity_data.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"identity_data.shape before concatting: \", identity_data.shape, \"\\n\")\nprint(\"low_card_id_encoded.shape before concatting: \", low_card_id_encoded.shape, \"\\n\")\n\nidentity_concatted = pd.concat([identity_data, low_card_id_encoded], axis = 1)\n\nprint(\"identity_concatted.shape after concatting: \", identity_concatted.shape, \"\\n\")\nprint(\"identity_concatted.columns after concatting: \", identity_concatted.columns, \"\\n\")\n\n#del low_card_id_encoded\n#del identity_data\n\nprint(identity_concatted.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now we have our 2 finished dataframes  transaction_concatted and identity_concatted, that contains all the features we want to use, all categorical features are encoded and all missing values have been removed.**\n\n**Before we start the training process, we have to split up our transaction_concatted into train and test, and we also have to split up our identity_concatted.**\n\n**The reason for this is shown in the following image:**\n\n![](https://i.imgur.com/9spsvK8.png)\n"},{"metadata":{},"cell_type":"markdown","source":"**In the left part of the image we can see why we cannot concat both dataframes first and then split up.**\n\n**For the splitting process we have to split up our dataframe into two pieces via a horizontal cut.**\n\n**But due to the different shapes of transaction_concatted (1097231, 253)  and identity_concatted  (286140, 55), we cannot concat first and then split up.**\n\n**This would not result in the correct 4 dataframes  train_transaction, test_transaction, train_identity, test_identity, with which we started.**\n\n**In order to get the correct train and test dataframes, we have to split up first,  and then concat.**\n\n**Due to the different shapes of both dataframes there will also be a huge number of generated NaN's. This is caused by the fact that identity_concatted dataframe does not have the same columns as transaction_concatted.  Hence all the empty values generated by the concatting process will be filled with NaN's.  We will not impute these missing values, we will simply tell our model to ignore these values. I will explain this later in more detail.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"transaction_concatted.shape before splitting up: \", transaction_concatted.shape, \"\\n\")\n\n# shape of train_transaction was (590540, 393), \n# shape of test_transaction  was (506691, 392)\ntrain_transaction = transaction_concatted.iloc[0:590540]\ntest_transaction = transaction_concatted.iloc[590540:]\n\nprint(\"train_transaction.shape after splitting up: \", train_transaction.shape, \"\\n\")\nprint(\"test_transaction.shape after splitting up: \", test_transaction.shape, \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"identity_concatted.shape before splitting up: \", identity_concatted.shape, \"\\n\")\n\n# shape of train_identity was  (144233, 40)\n# shape of test_identity  was  (141907, 40)\ntrain_identity = identity_concatted.iloc[0:144233]\ntest_identity = identity_concatted.iloc[144233:]\n\nprint(\"train_identity.shape after splitting up: \", train_identity.shape, \"\\n\")\nprint(\"test_identity.shape after splitting up: \", test_identity.shape, \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"train_transaction.shape before concatting: \", train_transaction.shape, \"\\n\")\nprint(\"train_identity.shape before concatting: \", train_identity.shape, \"\\n\")\n\ntrain_data  = pd.concat([train_transaction, train_identity], axis = 1)\n\nprint(\"train_data.shape: \", train_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"counter = 0\n\nfor i in train_data.columns:\n    \n    summ = train_data[i].isnull().sum()\n    print(i, summ)\n    if summ > 0:\n        counter += 1\n        \nprint(\"\\n number of columns with missing values: \", counter)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As we can see exactly 590540 - 144233 = 446307 values are missing in exactly 44 columns, hence the concatting process went exactly as we expected it to happen.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"test_transaction.shape before concatting: \", test_transaction.shape, \"\\n\")\nprint(\"test_identity.shape before concatting: \", test_identity.shape, \"\\n\")\n\ntest_data  = pd.concat([test_transaction, test_identity], axis = 1)\n\nprint(\"test_data.shape: \", test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"counter = 0\n\nfor i in test_data.columns:\n    \n    summ = test_data[i].isnull().sum()\n    print(i, summ)\n    if summ > 0:\n        counter += 1\n        \nprint(\"\\n number of columns with missing values: \", counter)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's have a look at any random column containing these many NaN's:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_data[\"id_35_F\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The good thing is that we dont have to remove these NaN's, we can simply let our xgb model handle them.**"},{"metadata":{},"cell_type":"markdown","source":"# 12. Train the model"},{"metadata":{},"cell_type":"markdown","source":"**We choose a XGBClassifier as our model and start the training process.**\n\n**I commented the code to save time such that I can save this notebook faster.**\n\n**The given parameters result roughly in a private rank of 3085/6381.**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#from xgboost import XGBClassifier\n\n\n#clf = XGBClassifier(objective = 'binary:logistic',\n#                    gamma = 0.05,\n#                    colsample_bytree = 0.5, \n#                    eval_metric = 'auc',\n#                    n_estimators = 1350,         \n#                    max_depth = 8,\n#                    min_child_weight = 2, \n#                    learning_rate = 0.02,\n#                    subsample = 0.8,\n#                    n_jobs = -1,\n#                    silent = False,\n#                    verbosity = 0)        \n                \n\n#print(\"starting training process..... \\n\") \n#clf.fit(train_data, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now we save the predictions in a .csv file.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#sample_submission = pd.read_csv('/kaggle/input/ieee-fraud-detection/sample_submission.csv', index_col='TransactionID')\n\n#sample_submission['isFraud'] = clf.predict_proba(test_data)[:,1]\n#sample_submission.to_csv('simple_xgboost.csv')\n\n#print(\"saving was successful!\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Thank you for reading my IEEE Fraud detection tutorial!\n\n# Feel free to comment or ask questions :)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}