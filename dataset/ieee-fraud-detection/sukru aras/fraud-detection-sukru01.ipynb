{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd, os, gc\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data visualization libraries:\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# to ignore warnings:\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# to display all columns:\npd.set_option('display.max_columns', None)\n\n# to display all rowss:\npd.set_option('display.max_rows', None)\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %% [markdown]\n# # Libraries\n\n# %% [code]\n# data analysis libraries:\nimport numpy as np\nimport pandas as pd\n\n# data visualization libraries:\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# to ignore warnings:\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# to display all columns:\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport seaborn as sns\nfrom sklearn.preprocessing import scale \nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.metrics import roc_auc_score,roc_curve\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nimport numpy as np, pandas as pd, os, gc\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn as sns\nimport lightgbm as lgb\nimport gc\nfrom time import time\nimport datetime\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold, TimeSeriesSplit\nfrom sklearn.metrics import roc_auc_score\nwarnings.simplefilter('ignore')\nsns.set()\n%matplotlib inline\nimport gc\nimport time\nfrom contextlib import contextmanager","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Read train and test data with pd.read_csv():\n\ntrain_identity = pd.read_csv('/kaggle/input/ieee-fraud-detection/train_identity.csv')\ntrain_transaction = pd.read_csv('/kaggle/input/ieee-fraud-detection/train_transaction.csv')\ntest_identity = pd.read_csv('/kaggle/input/ieee-fraud-detection/test_identity.csv')\ntest_transaction = pd.read_csv('/kaggle/input/ieee-fraud-detection/test_transaction.csv')\nsub = pd.read_csv('/kaggle/input/ieee-fraud-detection/sample_submission.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  combine the data and work with the whole dataset\n\ntrain = pd.merge(train_transaction, train_identity, on='TransactionID', how='left',left_index=True, right_index=True)\ntest = pd.merge(test_transaction, test_identity, on='TransactionID', how='left', left_index=True, right_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# export dataset into computer as excel\n\n#df.to_excel(r'Path where you want to store the exported excel file\\File Name.xlsx', sheet_name='Your sheet name', index = False)\n# df.to_excel (export_file_path, index = False, header=True)\n#train.to_excel(r\"E:\\DATA SCIENCE\\KAGGLE PROJECTS\\fraudetection\\train.xlsx\", index = False )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_identity, train_transaction, test_identity, test_transaction; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().any().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"train.isnull().all()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Function to reduce the DF size\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## REducing memory\ntrain = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"train.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"train.isnull().count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rowsperct=(train.count()/train.isnull().count())*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"rowsperct, gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correlat1=train.corrwith( train['isFraud'], method= 'spearman')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cortable=pd.DataFrame(correlat1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"cortable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# FEATURE SELECTION\n# correlat=train.corrwith( train['isFraud'], method= 'spearman')\n#  correlat1=pd.DataFrame(correlat,columns=[\"corr\"])\n# correlat1.reset_index()\n#  cor_features=correlat1.loc[(correlat1.loc[:,\"corr\"] > 0.11)|(correlat1.loc[:,\"corr\"]<-0.11)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TARGET \nyiftrain =train['isFraud'].copy(); gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yiftrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ELIMINATION UNRELIABLE VARIABLES FROM TRAIN DATASET\n\ntrain1= train.drop(['V1', 'V2', 'V4', 'V6', 'V7', 'V8','V9', 'V10', 'V12', 'V13', 'V14', 'V15','V16', 'V17', 'V18','V19','V20', 'V21', 'V22', 'V24', 'V25', 'V26','V27', 'V28', 'V29', 'V30', \n             'V31', 'V32', 'V35', 'V36','V37','V38', 'V39', 'V40', 'V41', 'V42', 'V43','V45', 'V46', 'V47', 'V49', 'V50', 'V51', 'V53','V54', 'V55', 'V56', 'V57', 'V58', 'V59','V60',\n             'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67','V68', 'V69', 'V71', 'V72', 'V73', 'V75', 'V76','V77', 'V78', 'V79', 'V80', 'V81', 'V82','V83','V84', 'V85', 'V86', 'V88', 'V89',\n             'V91', 'V92', 'V93', 'V95', 'V96', 'V97', 'V98', 'V100','V101', 'V104', 'V105', 'V106', 'V107', 'V108','V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115','V116', 'V117',\n             'V118','V119','V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126','V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136','V137','V138', 'V139', 'V141', 'V142',\n             'V143', 'V144','V145', 'V146','V147', 'V148', 'V149', 'V150', 'V151', 'V152', 'V153','V154', 'V155', 'V157','V160','V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V167',\n             'V168', 'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176','V177', 'V178', 'V179', 'V180', 'V181', 'V182','V183','V184', 'V185', 'V186', 'V187', 'V188', 'V189','V190','V191', \n             'V192', 'V193', 'V194','V195', 'V196', 'V197', 'V198','V199', 'V200','V201', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V208','V209', 'V210', 'V211', 'V212', 'V213', 'V214', 'V215',\n            'V216', 'V217', 'V218','V219', 'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226','V227', 'V228', 'V229', 'V230', 'V231', 'V232', 'V233', 'V234', 'V235', 'V236','V237','V238', 'V239',\n            'V240', 'V241', 'V242', 'V243', 'V244', 'V245', 'V246', 'V247','V248', 'V249', 'V250', 'V251', 'V252', 'V253','V254', 'V255', 'V256', 'V257', 'V259','V260', 'V261', 'V262', 'V263', 'V265',\n            'V266', 'V267','V268', 'V269', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276','V277', 'V278', 'V279', 'V280', 'V281', 'V282','V284', 'V286', 'V287', 'V288', 'V289', 'V291', 'V292', 'V293',\n             'V295', 'V296', 'V297','V298','V299', 'V300', 'V301', 'V302', 'V304', 'V305', 'V306', 'V307', 'V308','V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318','V319',\n            'V320', 'V321', 'V322', 'V323', 'V324', 'V325', 'V326','V327', 'V328', 'V329', 'V330', 'V331', 'V333', 'V334', 'V335', 'V336','V337','V338', 'V339', 'id_33'], axis=1);x = gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ELIMINATION UNRELIABLE VARIABLES FROM TEST DATASET\n\ntest1= test.drop(['V1', 'V2', 'V4', 'V6', 'V7', 'V8','V9', 'V10', 'V12', 'V13', 'V14', 'V15','V16', 'V17', 'V18','V19','V20', 'V21', 'V22', 'V24', 'V25', 'V26','V27', 'V28', 'V29', 'V30', \n             'V31', 'V32', 'V35', 'V36','V37','V38', 'V39', 'V40', 'V41', 'V42', 'V43','V45', 'V46', 'V47', 'V49', 'V50', 'V51', 'V53','V54', 'V55', 'V56', 'V57', 'V58', 'V59','V60',\n             'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67','V68', 'V69', 'V71', 'V72', 'V73', 'V75', 'V76','V77', 'V78', 'V79', 'V80', 'V81', 'V82','V83','V84', 'V85', 'V86', 'V88', 'V89',\n             'V91', 'V92', 'V93', 'V95', 'V96', 'V97', 'V98', 'V100','V101', 'V104', 'V105', 'V106', 'V107', 'V108','V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115','V116', 'V117',\n             'V118','V119','V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126','V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136','V137','V138', 'V139', 'V141', 'V142',\n             'V143', 'V144','V145', 'V146','V147', 'V148', 'V149', 'V150', 'V151', 'V152', 'V153','V154', 'V155', 'V157','V160','V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V167',\n             'V168', 'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176','V177', 'V178', 'V179', 'V180', 'V181', 'V182','V183','V184', 'V185', 'V186', 'V187', 'V188', 'V189','V190','V191', \n             'V192', 'V193', 'V194','V195', 'V196', 'V197', 'V198','V199', 'V200','V201', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V208','V209', 'V210', 'V211', 'V212', 'V213', 'V214', 'V215',\n            'V216', 'V217', 'V218','V219', 'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226','V227', 'V228', 'V229', 'V230', 'V231', 'V232', 'V233', 'V234', 'V235', 'V236','V237','V238', 'V239',\n            'V240', 'V241', 'V242', 'V243', 'V244', 'V245', 'V246', 'V247','V248', 'V249', 'V250', 'V251', 'V252', 'V253','V254', 'V255', 'V256', 'V257', 'V259','V260', 'V261', 'V262', 'V263', 'V265',\n            'V266', 'V267','V268', 'V269', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276','V277', 'V278', 'V279', 'V280', 'V281', 'V282','V284', 'V286', 'V287', 'V288', 'V289', 'V291', 'V292', 'V293',\n             'V295', 'V296', 'V297','V298','V299', 'V300', 'V301', 'V302', 'V304', 'V305', 'V306', 'V307', 'V308','V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318','V319',\n            'V320', 'V321', 'V322', 'V323', 'V324', 'V325', 'V326','V327', 'V328', 'V329', 'V330', 'V331', 'V333', 'V334', 'V335', 'V336','V337','V338', 'V339', 'id-33'], axis=1);x = gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test1.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1.shape, test1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train, test ; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# del train1['isFraud']; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PLOT ORIGINAL D\nplt.figure(figsize=(15,5))\nplt.scatter(train1.TransactionDT,train1.D15)\nplt.title('Original D15')\nplt.xlabel('Time')\nplt.ylabel('D15')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1.shape, test1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NORMALIZE D COLUMNS\nfor i in range(1,16):\n    if i in [1,2,3,5,9]: continue\n    train1['D'+str(i)] =  train1['D'+str(i)] - train1.TransactionDT/np.float32(24*60*60)\n    test1['D'+str(i)] = test1['D'+str(i)] - test1.TransactionDT/np.float32(24*60*60) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PLOT TRANSFORMED D\nplt.figure(figsize=(15,5))\nplt.scatter(train1.TransactionDT,train1.D15)\nplt.title('Transformed D15')\nplt.xlabel('Time')\nplt.ylabel('D15n')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1.shape, test1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yiftrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1= train1.drop(['C1', 'C2', 'C3', 'C6', 'C8','C9', 'C10', 'C11', 'C13', 'C14', 'C14', 'D4', 'D5', 'D6', 'D7','D8', 'D9', 'D10', 'D11', 'D12', 'D13','D14',\n                    'M2', 'M3', 'M5', 'M6','M7', 'M8', 'M9'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test1= test1.drop(['C1', 'C2', 'C3', 'C6', 'C8','C9', 'C10', 'C11', 'C13', 'C14', 'C14', 'D4', 'D5', 'D6', 'D7','D8', 'D9', 'D10', 'D11', 'D12', 'D13','D14',\n                    'M2', 'M3', 'M5', 'M6','M7', 'M8', 'M9'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1.shape, test1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#   Mapping emails\n\nemails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', \n          'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft',\n          'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',\n          'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', \n          'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink',\n          'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other',\n          'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', \n          'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', \n          'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo',\n          'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n          'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft',\n          'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', \n          'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', \n          'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', \n          'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', \n          'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', \n          'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', \n          'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other',\n          'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n\nus_emails = ['gmail', 'net', 'edu']\n\n# https://www.kaggle.com/c/ieee-fraud-detection/discussion/100499#latest-579654\nfor c in ['P_emaildomain', 'R_emaildomain']:\n    train1[c + '_bin'] = train1[c].map(emails)\n    test1[c + '_bin'] = test1[c].map(emails)\n    \n    train1[c + '_suffix'] = train1[c].map(lambda x: str(x).split('.')[-1])\n    test1[c + '_suffix'] = test1[c].map(lambda x: str(x).split('.')[-1])\n    \n    train1[c + '_suffix'] = train1[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n    test1[c + '_suffix'] = test1[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1.shape, test1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# FREQUENCY ENCODE TOGETHER\ndef encode_FE(df1, df2, cols):\n    for col in cols:\n        df = pd.concat([df1[col],df2[col]])\n        vc = df.value_counts(dropna=True, normalize=True).to_dict()\n        vc[-1] = -1\n        nm = col+'_FE'\n        df1[nm] = df1[col].map(vc)\n        df1[nm] = df1[nm].astype('float32')\n        df2[nm] = df2[col].map(vc)\n        df2[nm] = df2[nm].astype('float32')\n        print(nm,', ',end='')\n        \n# LABEL ENCODE\ndef encode_LE(col,train1=train1,test1=test1,verbose=True):\n    df_comb = pd.concat([train1[col],test1[col]],axis=0)\n    df_comb,_ = df_comb.factorize(sort=True)\n    nm = col\n    if df_comb.max()>32000: \n        train1[nm] = df_comb[:len(train1)].astype('int32')\n        test1[nm] = df_comb[len(train1):].astype('int32')\n    else:\n        train1[nm] = df_comb[:len(train1)].astype('int16')\n        test1[nm] = df_comb[len(train1):].astype('int16')\n    del df_comb; x=gc.collect()\n    if verbose: print(nm,', ',end='')\n        \n# GROUP AGGREGATION MEAN AND STD\n# https://www.kaggle.com/kyakovlev/ieee-fe-with-some-eda\ndef encode_AG(main_columns, uids, aggregations=['mean'], train_df=train1, test_df=test1, \n              fillna=True, usena=False):\n    # AGGREGATION OF MAIN WITH UID FOR GIVEN STATISTICS\n    for main_column in main_columns:  \n        for col in uids:\n            for agg_type in aggregations:\n                new_col_name = main_column+'_'+col+'_'+agg_type\n                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n                if usena: temp_df.loc[temp_df[main_column]==-1,main_column] = np.nan\n                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n                                                        columns={agg_type: new_col_name})\n\n                temp_df.index = list(temp_df[col])\n                temp_df = temp_df[new_col_name].to_dict()   \n\n                train_df[new_col_name] = train_df[col].map(temp_df).astype('float32')\n                test_df[new_col_name]  = test_df[col].map(temp_df).astype('float32')\n                \n                if fillna:\n                    train_df[new_col_name].fillna(-1,inplace=True)\n                    test_df[new_col_name].fillna(-1,inplace=True)\n                \n                print(\"'\"+new_col_name+\"'\",', ',end='')        \n# COMBINE FEATURES\ndef encode_CB(col1,col2,df1=train1,df2=test1):\n    nm = col1+'_'+col2\n    df1[nm] = df1[col1].astype(str)+'_'+df1[col2].astype(str)\n    df2[nm] = df2[col1].astype(str)+'_'+df2[col2].astype(str) \n    encode_LE(nm,verbose=False)\n# GROUP AGGREGATION NUNIQUE\ndef encode_AG2(main_columns, uids, train_df=train1, test_df=test1):\n    for main_column in main_columns:  \n        for col in uids:\n            comb = pd.concat([train_df[[col]+[main_column]],test_df[[col]+[main_column]]],axis=0)\n            mp = comb.groupby(col)[main_column].agg(['nunique'])['nunique'].to_dict()\n            train_df[col+'_'+main_column+'_ct'] = train_df[col].map(mp).astype('float32')\n            test_df[col+'_'+main_column+'_ct'] = test_df[col].map(mp).astype('float32')\n            print(col+'_'+main_column+'_ct, ',end='')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encode_CB('card1','addr1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  encode_CB('card3','addr1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1.shape, test1.shape,","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"na_low=[]\nfor col in train1.loc[:,'TransactionAmt':].columns: \n       if sum(train1[col].isnull())/float(len(train1.index)) < 0.30:\n            na_low.append(col)\n            print(\"'\"+col+\"'\",', ',end='')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"na_low=['TransactionAmt' , 'ProductCD' , 'card1' , 'card2' , 'card3' , 'card4' , 'card5' , 'card6' , 'addr1' ,\n        'addr2' , 'P_emaildomain' , 'C4' , 'C5' , 'C7' , 'C12' ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric=train1[na_low]._get_numeric_data().columns\nnumeric","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1.shape, test1.shape,","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test1.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating DAY FEATURE IN TRAIN\n\ntrain1['day'] = train1.TransactionDT / (24*60*60)\ntrain1['uid'] = train1.card1_addr1.astype(str)+'_'+np.floor(train1.day-train1.D1).astype(str)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test1['day'] = test1.TransactionDT / (24*60*60)\ntest1['uid'] = test1.card1_addr1.astype(str)+'_'+np.floor(test1.day-test1.D1).astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1.shape, test1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encode_FE(train1,test1,['addr1','card1','card2','card3','P_emaildomain'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1.shape, test1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test1.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encode_FE(train1,test1,['uid'])\nencode_AG( numeric, ['uid'],['mean',\"std\"], train1, test1, fillna=True, usena=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1= train1.drop(['id_12', 'id_15', 'id_16', 'id_23', 'id_27', 'id_28', 'id_29',\n       'id_30', 'id_31', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test1= test1.drop([ 'id-12', 'id-15', 'id-16', 'id-23', 'id-27', 'id-28', 'id-29',\n       'id-30', 'id-31', 'id-34', 'id-35', 'id-36', 'id-37', 'id-38'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_columns=test1.columns.drop(test1._get_numeric_data().columns)\ncategorical_columns=categorical_columns.drop('uid')\ncategorical_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1.shape, test1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test1.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encode_AG2(categorical_columns, ['uid'], train1, test1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# FREQUENCY ENCODE: ADDR1, CARD1, CARD2, CARD3, P_EMAILDOMAIN\nencode_FE(train1,test1,['addr1','card1','card2','card3','P_emaildomain'])\n# COMBINE COLUMNS CARD1+ADDR1+P_EMAILDOMAIN\nencode_CB('card1_addr1','P_emaildomain')\n# FREQUENCY ENOCDE\nencode_FE(train1,test1,['card1_addr1'])\n# GROUP AGGREGATE\nencode_AG(['TransactionAmt'],['card1','card1_addr1'],['mean','std'],usena=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1.shape, test1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train1['uid'], test1['uid']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1.shape, test1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# From kernel https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n# WARNING! THIS CAN DAMAGE THE DATA \ndef reduce_mem_usage2(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_columns=test1.columns.drop(test1._get_numeric_data().columns)\ncategorical_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nfor i in categorical_columns: \n    lbe=preprocessing.LabelEncoder()\n    train1[i]=lbe.fit_transform(train1[i].astype(str))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in categorical_columns:    \n    test1[i]=lbe.fit_transform(test1[i].astype(str))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_columns=train1.columns\ntrain_columns=train_columns.drop(\"isFraud\")\ntest1.columns=train_columns\ntest1.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1.shape, test1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in categorical_columns:\n    if (test1[i].max()== train1[i].max())&(train1[i].max()<8):\n            test1 = pd.get_dummies(test1, columns = [i])\n            train1=pd.get_dummies(train1, columns = [i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1.shape, test1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_TransactionID= train1[\"TransactionID\"]\ntest_TransactionID=test1[\"TransactionID\"]\nTransactionDT=train1[\"TransactionDT\"]\nX= train1.drop([ 'TransactionDT', 'TransactionID'], axis=1)\ny = train1['isFraud']\ntest1 = test1.sort_values('TransactionDT').drop(['TransactionDT', 'TransactionID'], axis=1)\ndel train1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X=X.drop(feature_drop,axis=1)\n#test=test.drop(feature_drop,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=X.drop(\"isFraud\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"y.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape, test1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'num_leaves': 491,\n          'min_child_weight': 0.03454472573214212,\n          'feature_fraction': 0.3797454081646243,\n          'bagging_fraction': 0.4181193142567742,\n          'min_data_in_leaf': 106,\n          'objective': 'binary',\n          'max_depth': -1,\n          'learning_rate': 0.006883242363721497,\n          \"boosting_type\": \"gbdt\",\n          \"bagging_seed\": 11,\n          \"metric\": 'auc',\n          \"verbosity\": -1,\n          'reg_alpha': 0.3899927210061127,\n          'reg_lambda': 0.6485237330340494,\n          'random_state': 47\n          \n         }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# datetime object containing current date and time\n\nimport datetime\ndatetime.datetime.now()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from time import time\ntime()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"folds = TimeSeriesSplit(n_splits=5)\n\naucs = list()\nfeature_importances = pd.DataFrame()\nfeature_importances['feature'] = X.columns\n\ntraining_start_time = time()\nfor fold, (trn_idx, test_idx) in enumerate(folds.split(X, y)):\n    start_time = time()\n    print('Training on fold {}'.format(fold + 1))\n    \n    trn_data = lgb.Dataset(X.iloc[trn_idx], label=y.iloc[trn_idx])\n    val_data = lgb.Dataset(X.iloc[test_idx], label=y.iloc[test_idx])\n    clf = lgb.train(params, trn_data, 10000, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds=500)\n    \n    feature_importances['fold_{}'.format(fold + 1)] = clf.feature_importance()\n    aucs.append(clf.best_score['valid_1']['auc'])\n    \n    print('Fold {} finished in {}'.format(fold + 1, str(datetime.timedelta(seconds=time() - start_time))))\nprint('-' * 30)\nprint('Training has finished.')\nprint('Total training time is {}'.format(str(datetime.timedelta(seconds=time() - training_start_time))))\nprint('Mean AUC:', np.mean(aucs))\nprint('-' * 30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances['average'] = feature_importances[['fold_{}'.format(fold + 1) for fold in range(folds.n_splits)]].mean(axis=1)\nfeature_importances.to_csv('feature_importances_TimeFold.csv')\n\nplt.figure(figsize=(16, 16))\nsns.barplot(data=feature_importances.sort_values(by='average', ascending=False).head(50), x='average', y='feature');\nplt.title('50 TOP feature importance over {} folds average'.format(folds.n_splits));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#clf right now is the last model, trained with 80% of data and validated with 20%\nbest_iter = clf.best_iteration","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = lgb.LGBMClassifier(**params, num_boost_round=best_iter)\nclf.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#set the output as a dataframe and convert to csv file named submission.csv\npredictions = clf.predict_proba(test1)[:, 1]\noutput = pd.DataFrame({ \"TransactionID\" : test_TransactionID, \"isFraud\": predictions })\noutput.to_csv('submission_lgbm.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}