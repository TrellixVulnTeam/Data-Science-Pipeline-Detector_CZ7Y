{"cells":[{"metadata":{"_uuid":"39a48e67-cf3c-4dcd-b090-64184579d592","_cell_guid":"a4ad9875-0702-4aae-a0f3-8fc2ce3f6400","trusted":true},"cell_type":"markdown","source":"![](https://cdn.cnn.com/cnnnext/dam/assets/140820084625-peanut-butter-stock-super-tease.jpg)"},{"metadata":{"_uuid":"4b419ff5-3c38-424d-8150-a0a83a3cb946","_cell_guid":"ed50b9f0-3c46-407e-871a-ec58e10311d0","trusted":true},"cell_type":"markdown","source":"This is a helper kernel for anyone struggling to deal with all the missing values and unsmooth features. Im sure it does add quite a lot of bias, but for linear models the NaN values are a pain so I hope this helps you.\n# \n# We also encode labels for ease of use\n# \n# If you want to see my further analysis where I build on this data using catboost and Logistic Regression  on this Please see https://www.kaggle.com/pipboyguy/simple-models-smoothing-of-features"},{"metadata":{"_uuid":"c53ef85f-bcda-472a-9db5-418718939d82","_cell_guid":"c9603d2f-381f-4c15-aa38-14be44034467","trusted":true},"cell_type":"code","source":"import gc\nimport os\nimport re\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport scipy.sparse\nimport seaborn as sns\nfrom scipy import stats\nfrom sklearn.impute import SimpleImputer  # for categorical variables\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n\n%matplotlib inline\nsns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"2f40fd40-0f3f-4374-8ba6-2f62e88ae422","_cell_guid":"a86f6605-508e-46ba-a4e2-acd404c15cef","trusted":true},"cell_type":"code","source":"#Helper Functions\n\n# I have given thanks to reduce_mem in one of my previous kernels\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage(deep=True).sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                c_prec = df[col].apply(lambda x: np.finfo(x).precision).max()\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max and c_prec == np.finfo(np.float32).precision:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n\ndef boxcox_Numfeats(frame: pd.DataFrame, columns_to_boxcox: list):\n    \"\"\"\n    Smoothing the numerical-dataframe with box-cox\n    \"\"\"\n    for feature in columns_to_boxcox:\n        print(f\"Processing Feature: {feature}\")\n        \n        # boxcox only takes positive values\n        \n        if frame[feature].min() < 0:\n            \n            rc_bc, bc_params = stats.boxcox(frame[feature]+np.abs(frame[feature].min())+0.0001) \n            frame[feature] = rc_bc\n            \n        else:\n            \n            rc_bc, bc_params  = stats.boxcox(frame[feature]+0.0001) \n            frame[feature] = rc_bc\n            \n        gc.collect()\n    \n    return frame\n\n\n## Thanks to https://www.amazon.com/Feature-Engineering-Machine-Learning-Principles/dp/1491953241 for help on this. I engineered it a bit more ;)\n\ndef plot_boxcoxes(df : pd.DataFrame,feature_names ,number_of_feats_to_plot = 6):\n    \"\"\"\n    Plots a random sample of the transformed features as a histrograme\n    \"\"\"\n    fig, axes = plt.subplots(number_of_feats_to_plot,1, figsize= (10,15))\n    choices = np.random.choice(feature_names, number_of_feats_to_plot, replace= False)\n    \n    for i, feat in enumerate(choices):\n\n        df[feat].hist(ax=axes[i], bins=100, color = \"sandybrown\")\n        axes[i].set_yscale('log')\n        axes[i].tick_params(labelsize=14)\n        axes[i].set_title(feat+ ' Histogram after boxcox', fontsize=14)\n        axes[i].set_xlabel('')\n        axes[i].set_ylabel('Occurrence', fontsize=14)\n        fig.tight_layout()\n        fig.show()","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"ea832c27-6e2e-49c6-a1a6-c74f63fc9d59","_cell_guid":"f7a9ed65-f128-4127-a286-3cdd73cf9e1c","trusted":true},"cell_type":"code","source":"train_df_identity = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/train_identity.csv\")\ntrain_df_transaction = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/train_transaction.csv\")\ntest_df_identity = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/test_identity.csv\")\ntest_df_transaction = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/test_transaction.csv\")\n\nsub_df = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/sample_submission.csv\")\n\ntrain_df = pd.merge(left=train_df_identity, right=train_df_transaction, how='right', on='TransactionID', validate ='one_to_many')\ntest_df = pd.merge(left=test_df_identity, right=test_df_transaction, how='right', on='TransactionID', validate ='one_to_many')\n\ntrain_df.set_index(\"TransactionID\", inplace=True)\n# train_df.drop([\"TransactionID\"], axis=1,inplace=True) #This wont be needed in training set its already in ID\n\ntrain_df.drop(['isFraud'],axis=1, inplace=True) # we don't want to leak this into our predictor features\n\n## The following code aids us in submission later on\nsub_df.set_index(\"TransactionID\", inplace=True)\ntest_df.set_index(\"TransactionID\", inplace=True)\n\n\ntest_df = test_df.reindex(sub_df.index) #So order is similar to submission file\nassert all(test_df.index.values == sub_df.index.values) #Test if this worked\n\ndel train_df_identity,train_df_transaction,test_df_identity,test_df_transaction\n\ngc.collect()","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"b8f82519-ec7f-47dc-9048-ddfbead3eeb9","_cell_guid":"86c8382c-2cf7-4e37-9802-8bc59c90bc24","trusted":true},"cell_type":"code","source":"#according to https://www.kaggle.com/c/ieee-fraud-detection/discussion/101203#latest-607486\n\nCatfeats = ['ProductCD'] + \\\n           [\"card\"+f\"{i+1}\" for i in range(6)] + \\\n           [\"addr\"+f\"{i+1}\" for i in range(2)] + \\\n           [\"P_emaildomain\", \"R_emaildomain\"] + \\\n           [\"M\"+f\"{i+1}\" for i in range(9)] + \\\n           [\"DeviceType\", \"DeviceInfo\"] + \\\n           [\"id_\"+f\"{i}\" for i in range(12, 39)]\n\nNumfeats = list(set(train_df.columns)-set(Catfeats))\n\nassert set(Catfeats+Numfeats) == set(train_df.columns.values)","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"e6bf20a6-08d8-4bbe-adb6-91a1adb9e42f","_cell_guid":"6155afa1-c75c-4b57-ae6d-b0d229425281","trusted":true},"cell_type":"markdown","source":"Lets transform the categorical features that present as floats and text to pandas categorical"},{"metadata":{"_uuid":"513abdf3-63be-4a58-932e-2e60f652469d","_cell_guid":"c5d50d56-5d5f-459c-81c3-ce09446b2690","trusted":true},"cell_type":"code","source":"for col in Catfeats:\n    train_df[col] = train_df[col].astype('str').astype('category')\n    test_df[col] = test_df[col].astype('str').astype('category')\n    \ntrain_df[Catfeats] = train_df[Catfeats].replace('nan', np.nan) # to create a sparse matrix    \ntest_df[Catfeats] = test_df[Catfeats].replace('nan', np.nan) # to create a sparse matrix","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"32bf26f9-2440-4bdd-b572-6913f50ce176","_cell_guid":"b1d5239a-ed3c-4d7d-9712-f71e6044737d","trusted":true},"cell_type":"markdown","source":"We combine train and test to impute values"},{"metadata":{"_uuid":"97b49851-e700-4ed0-85cf-8a21faf97e3f","_cell_guid":"4c5eec0b-73ef-4153-af6e-3c0999a2bcc1","trusted":true},"cell_type":"code","source":"# test_df['isFraud'] = np.NaN \n\n# train and test have same columns so we append\n\ntest_indeces = test_df.index\ntrain_indeces = train_df.index\n\nimputed_df = train_df.append(test_df, ignore_index=False, verify_integrity = True, sort=True)\n\ndel train_df,test_df\n\nimputed_df = reduce_mem_usage(imputed_df)\n\ngc.collect()","execution_count":0,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#######\n# I still need to figure out how to save memory. If we take all data the kernel crashes\n\nimputed_df = imputed_df.sample(frac=0.15)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b4478d7a-7304-414d-8657-fdb6f41c21d2","_cell_guid":"1a491526-0c84-4373-960b-ee58248adf1c","trusted":true},"cell_type":"code","source":"imputed_df.head()","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"8b45533d-2010-487f-9ece-d7e42e7ba5b9","_cell_guid":"91cb6360-fabe-4402-9275-654d3ad8e025","trusted":true},"cell_type":"code","source":"imp = SimpleImputer(strategy=\"most_frequent\", missing_values= np.nan)\n\nfor col in Catfeats:\n    imputed_df[col] = imp.fit_transform(imputed_df[col].as_matrix().reshape(-1,1))","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"e6e4859f-327b-48be-b2cf-0222f2b2a6fe","_cell_guid":"09448fcd-00d2-491a-b2ec-47e34ae206ef","trusted":true},"cell_type":"code","source":"imputed_df[Catfeats].head()","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"ee671f9b-bea4-44ff-bbd9-bce48e322be9","_cell_guid":"10ae81d0-456b-4e06-bd9e-b4c2d46e7ae3","trusted":true},"cell_type":"markdown","source":"Categorical features look good. We will now focus on numerical features  Simple imputation for all features. Iterative imputer is too computationally expensive"},{"metadata":{"_uuid":"0a24cf6d-99d7-42a3-bd26-5f68ebb06c88","_cell_guid":"d85ae765-6c35-4c61-b6a7-da880259834b","trusted":true},"cell_type":"code","source":"# missing_na_perc = (imputed_df[Numfeats].isna().sum()/len(imputed_df[Numfeats])).sort_values(ascending=False)\n\n# print(f\"Missing percentage of values per feature:\\n\\n{missing_na_perc}\")","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"56a4dccf-372b-4de7-b442-559b11584f7e","_cell_guid":"544edeb8-35f2-498f-86ca-9dcbd497f4f8","trusted":true},"cell_type":"code","source":"# sixty_perc_or_less = list(missing_na_perc.loc[missing_na_perc <= .6].index)\n# sixty_perc_or_more = list(missing_na_perc.loc[missing_na_perc > .6].index)","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"c9295f2f-6711-4672-b785-992dfe324a22","_cell_guid":"5ed67706-9c14-43d3-879b-7ac5690c7d7d","trusted":true},"cell_type":"code","source":"# Simple imputation for al features. Iterative umputer is too computationally expensive\nfrom sklearn.impute import SimpleImputer \n\n# to introduce as less bias as we can, we iterate between mean and median for imputer\n\nimpute_method = 'mean'\n\nfor col in Numfeats:\n    if impute_method == 'mean':\n        impute_method = 'median'\n        imp_mean = SimpleImputer(missing_values=np.nan, strategy=impute_method)\n        imputed_df[col] = imp_mean.fit_transform(imputed_df[col].ravel().reshape(-1,1))\n    elif impute_method == 'median':     \n        impute_method = 'mean'\n        imp_mean = SimpleImputer(missing_values=np.nan, strategy=impute_method)\n        imp_mean.fit_transform(imputed_df[col].ravel().reshape(-1,1))\n        imputed_df[col] = imp_mean.fit_transform(imputed_df[col].ravel().reshape(-1,1))\n        \n\ngc.collect()","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"4e135bba-5642-4390-bd54-364103179706","_cell_guid":"c17bd61d-8201-42ef-a7b3-a66f20b365b0","trusted":true},"cell_type":"code","source":"assert imputed_df[Numfeats].isna().sum().sum() == 0, \"We aren't done\"","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"bd4ea781-ca65-4819-8ad3-5a4f538d0562","_cell_guid":"ef1fbd66-3594-4038-9bf5-2b8ddc239b3a","trusted":true},"cell_type":"code","source":"# # Complex imputation of more than 60% of missing values\n# from sklearn.experimental import enable_iterative_imputer  # noqa\n# # now you can import normally from sklearn.impute\n# from sklearn.impute import IterativeImputer # for our numerical outpurs\n\n# finished_cols = []\n\n# for col in sixty_perc_or_more:\n    \n#     imp_iterative = IterativeImputer(max_iter=50, min_value = train_df[col].min(), max_value = train_df[col].max())\n#     train_df[col] = imp_iterative.fit_transform(pd.concat([train_df[sixty_perc_or_less + finished_cols], train_df[col]], axis=1))[:,-1] # we take the last column as that is the \n#                                                                                                                         # one we are imputing using the rest\n        \n#     finished_cols.append(col)\n    \n#     gc.collect()","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"f117a264-03b9-4de2-bd75-73ac5cb5beec","_cell_guid":"cb5302f1-fcf9-48f1-9631-de8bf8f74136","trusted":true},"cell_type":"code","source":"# assert train_df.isna().sum().sum() == 0, \"We aren't done\"","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"54f68157-8c49-43dc-ad8a-b2da8edb1cb1","_cell_guid":"bb323385-5518-40ce-bb82-a09315441d56","trusted":true},"cell_type":"markdown","source":"## Feature smoothing"},{"metadata":{"_uuid":"02ee8619-a764-4ccb-a785-a132816e8143","_cell_guid":"6bf59927-db40-4741-a7eb-c769e2cdc373","trusted":true},"cell_type":"markdown","source":"#### before boxcox"},{"metadata":{"_uuid":"d2ca1740-db4c-4b22-9b87-f77a16225252","_cell_guid":"050a3f23-0add-41e0-aded-af8b967a1174","trusted":true},"cell_type":"code","source":"print(\"before boxcox:\")\nplot_boxcoxes(imputed_df[Numfeats],feature_names = Numfeats)","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"3d41930d-6ad0-49ac-8145-546568d00cba","_cell_guid":"443ff6bc-f391-4132-b4e8-097cf59c8ffa","trusted":true},"cell_type":"code","source":"imputed_df = boxcox_Numfeats(imputed_df, Numfeats)\n\nimputed_df = reduce_mem_usage(imputed_df)\n\ngc.collect()","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"3a877f3d-387f-4814-b3cc-cb3e0ad202de","_cell_guid":"44154d1e-1c57-4d41-82a9-537da2c1a747","trusted":true},"cell_type":"markdown","source":"Plotting a random sample of the box coxed features:"},{"metadata":{"_uuid":"af061fcb-8f43-4f00-a4a7-cc1d3b475104","_cell_guid":"b48c678d-f4cd-447b-957f-3bbc57a03d80","trusted":true},"cell_type":"markdown","source":"#### after boxcox"},{"metadata":{"_uuid":"86b6406b-a3a3-44de-91f4-f1e5757e2540","_cell_guid":"c5657583-570a-48de-9a69-55c134fa8c7e","trusted":true},"cell_type":"code","source":"print(\"after boxcox:\")\nplot_boxcoxes(imputed_df[Numfeats],feature_names = Numfeats)","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"f771fd84-4555-4568-93db-54f5526eab73","_cell_guid":"5841ec37-9dec-43e7-91ba-0e1d50067d9f","trusted":true},"cell_type":"markdown","source":"### Encoding of Categorical Features"},{"metadata":{"_uuid":"14685146-35e9-4b05-ba76-8aea8779d762","_cell_guid":"f599d8b4-d346-4c32-ba00-729a796be4fc","trusted":true},"cell_type":"code","source":"less_than_4_levels = imputed_df[Catfeats].columns[np.where(imputed_df[Catfeats].nunique() <= 4)]\nmore_than_4_levels = imputed_df[Catfeats].columns[np.where(imputed_df[Catfeats].nunique() > 4)]","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"422d0a1e-5917-48ed-b428-a78a408e158f","_cell_guid":"c6602154-6c69-435e-81e3-73bcbeb78a86","trusted":true},"cell_type":"code","source":"## Less or equal than 4 levels we do label encoder:\n\nle = LabelEncoder()\nfor col in less_than_4_levels:\n    imputed_df[col] = le.fit_transform(imputed_df[col].astype(str))\n    gc.collect()","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"0e145f4e-8f20-426d-be01-5ff40d60dde0","_cell_guid":"273cdcad-bf03-4afb-90a9-c9dfb68f0cc2","trusted":true},"cell_type":"code","source":"## More than 4 levels we do one-hot encoder:\n\n# enc = OneHotEncoder(handle_unknown='ignore')\n# OH_encoded_sparse = enc.fit_transform(imputed_df[more_than_4_levels])","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"e44fd561-0549-4cb9-afbd-eb4f4eb64849","_cell_guid":"537507d2-2542-444b-b834-41be35e31c60","trusted":true},"cell_type":"markdown","source":"We combine the sparse matrix to the original (sparsed) imputed_df"},{"metadata":{"_uuid":"13711273-3ebc-4621-bb9a-08b290339a40","_cell_guid":"e34718f4-878b-4b57-a615-99366a1256f0","trusted":true},"cell_type":"code","source":"object_feats = (imputed_df.dtypes == 'O').index\n\nstrings_feats = []\n\nfor obj_f in object_feats:\n    try:\n         imputed_df[obj_f] = imputed_df[obj_f].astype('float')\n    except:\n        strings_feats.append(obj_f)","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"81f4a0b8-67f0-4474-9b69-fbd489236168","_cell_guid":"b6452a24-1522-4805-9b7e-5e008aa4a381","trusted":true},"cell_type":"code","source":"imputed_df[strings_feats]","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"d3183351-ae0a-4b98-9cba-c54a9300c8ef","_cell_guid":"1a7414a0-b95d-4117-a53a-70f01a199312","trusted":true},"cell_type":"markdown","source":"Let's get rid of these pesky critters"},{"metadata":{"_uuid":"b66b21b1-3bd8-4151-a47f-7df836e45e72","_cell_guid":"b1198961-fc12-4d22-adfa-476a095be1d1","trusted":true},"cell_type":"code","source":"for col in strings_feats:\n    imputed_df[col] = le.fit_transform(imputed_df[col].astype(str))\n    gc.collect()","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"caf83694-9cdb-4f84-bfe2-9d49a19440cb","_cell_guid":"ba1bd343-cc78-47c7-8235-14d40c790397","trusted":true},"cell_type":"code","source":"# we will need our indeces to restore the sparse matrix in our next kernel\n# imputed_df.reset_index(inplace=True)\n# imputed_df.head()\n\n# test_csr_indeces = np.where(imputed_df.TransactionID.isin(test_indeces))","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"c2ca447e-6d6e-4e32-89d9-9c850031b7f4","_cell_guid":"0ac82b5e-5a79-483c-bfe6-50b505017b27","trusted":true},"cell_type":"code","source":"# # imputed_df.drop(more_than_4_levels,axis=1, inplace=True)\n# imputed_df = scipy.sparse.csr_matrix(imputed_df)\n\n# #adding one hot encoded features\n# imputed_df = scipy.sparse.hstack((imputed_df, OH_encoded_sparse))\n\n# gc.collect()","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"18d569b9-c0cb-4bf5-bfe0-58a6cb45d106","_cell_guid":"0d1d57b7-7fbf-46c9-ba82-d7cd529585de","trusted":true},"cell_type":"code","source":"# # Now we get our original training and test sets back\n# test_sparse = imputed_df.tocsr()[np.where(imputed_df.TransactionID.isin(test_indeces))]\n# train_sparse_no_target_var = imputed_df.tocsr()\n\n# del imputed_df;gc.collect()","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"11799622-8bdf-4b49-997c-0aa1743fa4bb","_cell_guid":"7faf5008-f2f9-49de-926a-d3b7555eed19","trusted":true},"cell_type":"markdown","source":"## Finally we are done!!\n# \n# I save the finished product. I hope it serves you well ;) Just remember that we introduced a lot of bias by imputing in this fasion## Finally we are done!!\n# \n# I save the finished product. I hope it serves you well ;) Just remember that we introduced a lot of bias by imputing in this fasion\n"},{"metadata":{"_uuid":"8c39f5b3-afa6-44d8-b04f-5dbf19385b83","_cell_guid":"f1ee5102-f6ac-4b10-812b-e483705af75a","trusted":true},"cell_type":"code","source":"# train_sparse_no_target_var.to_pickle(\"train_sparse_no_target_var.pkl\") # You can load it in your kernel using df = pd.read_pickle(\"Imputed_Train.pkl\") \n\n# scipy.sparse.save_npz('/tmp/imputed_df.npz', imputed_df)\n\n# scipy.sparse.save_npz('/tmp/train_sparse_no_target_var.npz', train_sparse_no_target_var)\n\n# You can load it in your kernel using sparse_matrix = scipy.sparse.load_npz('/tmp/sparse_matrix.npz')\n\nimputed_df.to_csv(\"Imputed_df.csv\",index=True)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}