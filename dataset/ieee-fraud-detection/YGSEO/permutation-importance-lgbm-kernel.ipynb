{"cells":[{"metadata":{},"cell_type":"markdown","source":"References:\n* Kaggle Official: https://www.kaggle.com/dansbecker/permutation-importance\n* sarmat https://www.kaggle.com/sarmat/lgbm-permutation-importance"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport random\nimport pprint as pp\nimport gc\nimport os\nimport time\nimport datetime\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport lightgbm as lgb\nfrom scipy import stats\nfrom tqdm import tqdm_notebook\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import KFold,StratifiedKFold, RepeatedKFold\nfrom operator import itemgetter\nimport os\nimport time\nimport datetime\nimport warnings\nwarnings.filterwarnings('ignore')\nprint(os.listdir(\"../input/ieee-base-data-pkl/\"))\n\nimport gc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n\ndef missing_values_table(df):# Function to calculate missing values by column# Funct \n    mis_val = df.isnull().sum() # Total missing values\n    mis_val_pct = 100 * df.isnull().sum() / len(df)# Percentage of missing values\n    mis_val_df = pd.concat([mis_val, mis_val_pct], axis=1)# Make a table with the results\n    mis_val_df_cols = mis_val_df.rename(columns = {0 : 'Missing Values', 1 : '% of Total Values'})# Rename the columns\n    mis_val_df_cols = mis_val_df_cols[mis_val_df_cols.iloc[:,1] != 0].sort_values('% of Total Values', ascending=False).round(1)# Sort the table by percentage of missing descending\n    print (\"Dataframe has \" + str(df.shape[1]) + \" columns.\\n\" \n           \"There are \" + str(mis_val_df_cols.shape[0]) + \" cols having missing values.\")# Print some summary information\n    return mis_val_df_cols # Return the dataframe with missing information\n\ndef Negativedownsampling(train, ratio) :\n    \n\n    # Number of data points in the minority class\n    number_records_fraud = len(train[train.isFraud == 1])\n    fraud_indices = np.array(train[train.isFraud == 1].index)\n\n    # Picking the indices of the normal classes\n    normal_indices = train[train.isFraud == 0].index\n\n    # Out of the indices we picked, randomly select \"x\" number (number_records_fraud)\n    random_normal_indices = np.random.choice(normal_indices, number_records_fraud*ratio, replace = False)\n    random_normal_indices = np.array(random_normal_indices)\n\n    # Appending the 2 indices\n    under_sample_indices = np.concatenate([fraud_indices,random_normal_indices])\n\n    # Under sample dataset\n    under_sample_data = train.iloc[under_sample_indices,:]\n    \n    # Showing ratio\n    print(\"Percentage of normal transactions: \", round(len(under_sample_data[under_sample_data.isFraud == 0])/len(under_sample_data),2)* 100,\"%\")\n    print(\"Percentage of fraud transactions: \", round(len(under_sample_data[under_sample_data.isFraud == 1])/len(under_sample_data),2)* 100,\"%\")\n    print(\"Total number of transactions in resampled data: \", len(under_sample_data))\n    \n    return under_sample_data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = reduce_mem_usage(pd.read_pickle('../input/ieee-base-data-pkl/train_transaction.pkl'))\ntest_df = reduce_mem_usage(pd.read_pickle('../input/ieee-base-data-pkl/test_transaction.pkl'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# NegativeDownSample\n\ndiscussion: https://www.kaggle.com/c/ieee-fraud-detection/discussion/108616#latest-634925\n\nkernel: https://www.kaggle.com/zero92/negative-downsampling-improve-the-score"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = Negativedownsampling(train_df, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df.shape, test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_Y = train_df['isFraud']\n\ntrain_df.drop(['isFraud','TransactionID'],1,inplace=True)\ntest_df.drop(['TransactionID'],1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X = train_df\ntest_X = test_df\ndel train_df; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import eli5\nfrom IPython.display import display\nfrom eli5.permutation_importance import get_score_importances\nfrom eli5.sklearn import PermutationImportance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TO DROP NAN for permutation\n\ncate_drop = train_X.loc[:,train_X.dtypes=='category'].columns.tolist()\n\nprint(train_X.shape)\ntrain_X.drop(cate_drop,1,inplace=True)\ntest_X.drop(cate_drop,1,inplace=True)\ntrain_X.fillna(0, inplace=True)\ntest_X.fillna(0, inplace=True)\nprint(train_X.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MODEL"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_fold = 5\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=42)\n\noof_lgb = np.zeros(len(train_X))\npredictions = np.zeros(len(test_X))\nfeature_importance_df = pd.DataFrame()\npermu_importance_df = pd.DataFrame()\ncv_score_df = []\n\n# Model parameters\nlgb_params = {'num_leaves': 2**8,\n             #'min_data_in_leaf': 10, \n             'objective':'binary',\n             'max_depth': -1,\n             'learning_rate': 0.5,\n             \"boosting\": \"gbdt\",\n             \"feature_fraction\": 1,\n             \"bagging_freq\": 1,\n             \"bagging_fraction\": 1,\n             \"bagging_seed\": 42,\n             \"metric\": 'auc',\n             \"lambda_l1\": 0.0,\n             \"verbosity\": 100,\n             \"nthread\": -1,\n             \"random_state\": 42}\n\nmodel_start = time.time()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train_X)):\n    strLog = \"fold {}\".format(fold_)\n    print(strLog+ \"-\" * 50)\n\n    X_tr, X_val = train_X.iloc[trn_idx], train_X.iloc[val_idx]\n    y_tr, y_val = train_Y.iloc[trn_idx], train_Y.iloc[val_idx]\n\n    model = lgb.LGBMClassifier(**lgb_params, n_estimators = 200000, n_jobs = -1)\n    model.fit(X_tr, \n              y_tr, \n              eval_set=[(X_tr, y_tr), (X_val, y_val)], \n              eval_metric='auc',\n              verbose=10000, \n              early_stopping_rounds=100)\n    oof_lgb[val_idx] = model.predict_proba(X_val, num_iteration=model.best_iteration_)[:,1]\n\n    cv_score_df.append(model.best_score_)\n\n    #feature importance\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = train_X.columns\n    fold_importance_df[\"importance\"] = model.feature_importances_[:len(train_X.columns)]\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    ####### Permutation Importance #######\n    perm = PermutationImportance(model,scoring=None, n_iter=1, random_state=42, cv=None, refit=False).fit(X_val, y_val)\n    tmp = eli5.show_weights(perm)\n    display(eli5.show_weights(perm, top = 10, feature_names = list(train_X.columns)))\n    \n    fold_permu_df = eli5.explain_weights_df(perm, top = len(list(train_X.columns)), feature_names = list(train_X.columns))\n    fold_permu_df[\"fold\"] = fold_ + 1\n    permu_importance_df = pd.concat([permu_importance_df, fold_permu_df], axis=0)\n    ######## predictions\n    predictions += model.predict_proba(test_X, num_iteration=model.best_iteration_)[:,1] / folds.n_splits\n\ncv_score_df = pd.DataFrame.from_dict(cv_score_df)\ncv_score_df = cv_score_df.valid_1.tolist()\ncv_score_df = list(map(itemgetter('auc'),cv_score_df))\n\nprint(\"-\" * 50)\n#print(\"SF   RMSE = {}\".format(oof_score))\nprint(\"Mean AUC\" , \" = {}\".format(np.mean(cv_score_df)))\nprint(\"Std AUC\" , \" = {}\".format(np.std(cv_score_df)))\nlgb.plot_metric(model, metric='auc', title='auc plot', xlabel='Iterations', ylabel='auto', figsize=(10,8), grid=False)\n\nmodel_end = time.time()\nmodel_elapsed = model_end - model_start\nprint('Model elapsed {0:0.2f}'.format(model_elapsed/60), \"minutes.\")\ncols = (feature_importance_df[[\"Feature\", \"importance\"]]\n    .groupby(\"Feature\")\n    .mean()\n    .sort_values(by=\"importance\", ascending=False)[:1000].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\npd.set_option('display.max_rows', 500)\nbest_features = best_features.drop(['fold'],axis=1)\nbest_features = best_features.groupby(['Feature'], as_index = False).mean()\nbest_features['Feature Rank'] = best_features['importance'].rank(ascending=0)\nbest_features = best_features.sort_values('Feature Rank', ascending = True)\nprint(\"Best Feature\")\npp.pprint(best_features.loc[best_features['importance']!=0].head(10))\n\npermu_features = permu_importance_df.loc[permu_importance_df.feature.isin(cols)]\npermu_features = permu_features.drop(['fold'],axis=1)\npermu_features = permu_features.groupby(['feature'], as_index=False).mean()\npermu_features['Feature Rank'] = permu_features['weight'].rank(ascending=0)\npermu_features = permu_features.sort_values('Feature Rank', ascending=True)\nprint(\"Permutation Feature\")\nprint(permu_features.head(10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RESULT"},{"metadata":{"trusted":true},"cell_type":"code","source":"best_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"permu_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_features.to_csv(\"best_features.csv\", index=False)\npermu_features.to_csv(\"permu_features.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}