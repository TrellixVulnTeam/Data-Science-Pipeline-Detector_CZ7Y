{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# General imports\nimport numpy as np\nimport pandas as pd\nimport os, sys, gc, warnings, random, datetime, psutil\n\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split, KFold, GroupKFold\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom catboost import CatBoostClassifier\n\nwarnings.filterwarnings('ignore')","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2021-10-05T13:42:55.850153Z","iopub.execute_input":"2021-10-05T13:42:55.850941Z","iopub.status.idle":"2021-10-05T13:42:55.85818Z","shell.execute_reply.started":"2021-10-05T13:42:55.850883Z","shell.execute_reply":"2021-10-05T13:42:55.857446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########################### Helpers\n#################################################################################\n## Seeder\n# :seed to make all processes deterministic     # type: int\ndef seed_everything(seed=0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    \n## Memory Reducer\n# :df pandas dataframe to reduce size             # type: pd.DataFrame()\n# :verbose                                        # type: bool\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n\n## Simple \"Memory profilers\" to see memory usage\ndef get_memory_usage():\n    return np.round(psutil.Process(os.getpid()).memory_info()[0]/2.**30, 2) \n        \ndef sizeof_fmt(num, suffix='B'):\n    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n        if abs(num) < 1024.0:\n            return \"%3.1f%s%s\" % (num, unit, suffix)\n        num /= 1024.0\n    return \"%.1f%s%s\" % (num, 'Yi', suffix)","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-10-05T13:42:55.860896Z","iopub.execute_input":"2021-10-05T13:42:55.861482Z","iopub.status.idle":"2021-10-05T13:42:55.883784Z","shell.execute_reply.started":"2021-10-05T13:42:55.861261Z","shell.execute_reply":"2021-10-05T13:42:55.883057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########################### Model\ndef make_predictions(tr_df, tt_df, features_columns, target, cat_params, NFOLDS=2, kfold_mode='grouped'):\n    \n    X,y = tr_df[features_columns], tr_df[target]    \n    P,P_y = tt_df[features_columns], tt_df[target]  \n    split_groups = tr_df['DT_M']\n\n    tt_df = tt_df[['TransactionID',target]] \n    tr_df = tr_df[['TransactionID',target]] \n    \n    predictions = np.zeros(len(tt_df))\n    oof = np.zeros(len(tr_df))\n\n    if kfold_mode=='grouped':\n        folds = GroupKFold(n_splits=NFOLDS)\n        folds_split = folds.split(X, y, groups=split_groups)\n    else:\n        folds = KFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n        folds_split = folds.split(X, y)\n\n    for fold_, (trn_idx, val_idx) in enumerate(folds_split):        \n        print('Fold:',fold_)\n        \n        estimator = CatBoostClassifier(**cat_params)        \n        estimator.fit(\n            X.iloc[trn_idx,:],y[trn_idx],\n            eval_set=(X.iloc[val_idx,:], y[val_idx]),\n            cat_features=categorical_features,\n            use_best_model=True,\n            verbose=True)\n        \n        pp_p = estimator.predict_proba(P)[:,1]\n        predictions += pp_p/NFOLDS\n        \n        oof_preds = estimator.predict_proba(X.iloc[val_idx,:])[:,1]\n        oof[val_idx] = (oof_preds - oof_preds.min())/(oof_preds.max() - oof_preds.min())\n        \n        del estimator\n        gc.collect()\n        \n    tt_df['prediction'] = predictions\n    print('OOF AUC:', metrics.roc_auc_score(y, oof))\n    if LOCAL_TEST:\n        print('Holdout AUC:', metrics.roc_auc_score(tt_df[TARGET], tt_df['prediction']))\n    \n    return tt_df\n## -------------------","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-10-05T13:42:55.885304Z","iopub.execute_input":"2021-10-05T13:42:55.885752Z","iopub.status.idle":"2021-10-05T13:42:55.902083Z","shell.execute_reply.started":"2021-10-05T13:42:55.885569Z","shell.execute_reply":"2021-10-05T13:42:55.901431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########################### Vars\nSEED = 42\nseed_everything(SEED)\nLOCAL_TEST = False\nTARGET = 'isFraud'\nSTART_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')","metadata":{"execution":{"iopub.status.busy":"2021-10-05T13:42:55.903981Z","iopub.execute_input":"2021-10-05T13:42:55.90455Z","iopub.status.idle":"2021-10-05T13:42:55.914337Z","shell.execute_reply.started":"2021-10-05T13:42:55.904325Z","shell.execute_reply":"2021-10-05T13:42:55.913607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########################### Model params\ncat_params = {\n                'n_estimators':5000,\n                'learning_rate': 0.07,\n                'eval_metric':'AUC',\n                'loss_function':'Logloss',\n                'random_seed':SEED,\n                'metric_period':500,\n                'od_wait':500,\n                'task_type':'GPU',\n                'depth': 8,\n                #'colsample_bylevel':0.7,\n                } ","metadata":{"execution":{"iopub.status.busy":"2021-10-05T13:42:55.91731Z","iopub.execute_input":"2021-10-05T13:42:55.917837Z","iopub.status.idle":"2021-10-05T13:42:55.924821Z","shell.execute_reply.started":"2021-10-05T13:42:55.917654Z","shell.execute_reply":"2021-10-05T13:42:55.923762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########################### DATA LOAD\n#################################################################################\nprint('Load Data')\n\nif LOCAL_TEST:\n    train_df = pd.read_pickle('../input/ieee-fe-for-local-test/train_df.pkl')\n    test_df = pd.read_pickle('../input/ieee-fe-for-local-test/test_df.pkl')\nelse:\n    train_df = pd.read_pickle('../input/ieee-fe-with-some-eda/train_df.pkl')\n    test_df = pd.read_pickle('../input/ieee-fe-with-some-eda/test_df.pkl')\n    \nremove_features = pd.read_pickle('../input/ieee-fe-with-some-eda/remove_features.pkl')\nremove_features = list(remove_features['features_to_remove'].values)\nprint('Shape control:', train_df.shape, test_df.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-05T13:42:55.926991Z","iopub.execute_input":"2021-10-05T13:42:55.927506Z","iopub.status.idle":"2021-10-05T13:43:45.769597Z","shell.execute_reply.started":"2021-10-05T13:42:55.927292Z","shell.execute_reply":"2021-10-05T13:43:45.768893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########################### Encode NaN goups\nnans_groups = {}\ntemp_df = train_df.isna()\ntemp_df2 = test_df.isna()\nnans_df = pd.concat([temp_df, temp_df2])\n\nfor col in list(nans_df):\n    cur_group = nans_df[col].sum()\n    if cur_group>0:\n        try:\n            nans_groups[cur_group].append(col)\n        except:\n            nans_groups[cur_group]=[col]\n\nadd_category = []\nfor col in nans_groups:\n    if len(nans_groups[col])>1:\n        train_df['nan_group_'+str(col)] = np.where(temp_df[nans_groups[col]].sum(axis=1)>0,1,0).astype(np.int8)\n        test_df['nan_group_'+str(col)]  = np.where(temp_df2[nans_groups[col]].sum(axis=1)>0,1,0).astype(np.int8)\n        add_category.append('nan_group_'+str(col))\n        \ndel temp_df, temp_df2, nans_df, nans_groups","metadata":{"execution":{"iopub.status.busy":"2021-10-05T13:43:45.771701Z","iopub.execute_input":"2021-10-05T13:43:45.771945Z","iopub.status.idle":"2021-10-05T13:43:57.413751Z","shell.execute_reply.started":"2021-10-05T13:43:45.771902Z","shell.execute_reply":"2021-10-05T13:43:57.412871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########################### Copy original Categorical features\ncategorical_features = ['ProductCD','M4',\n                        'card1','card2','card3','card4','card5','card6',\n                        'addr1','addr2','dist1','dist2',\n                        'P_emaildomain','R_emaildomain',\n                       ]\n\no_trans = pd.concat([pd.read_pickle('../input/ieee-data-minification/train_transaction.pkl'),\n                     pd.read_pickle('../input/ieee-data-minification/test_transaction.pkl')])\n\no_ident = pd.concat([pd.read_pickle('../input/ieee-data-minification/train_identity.pkl'),\n                     pd.read_pickle('../input/ieee-data-minification/test_identity.pkl')])\n\no_trans = o_trans.merge(o_ident, on=['TransactionID'], how='left')\no_trans = o_trans[['TransactionID'] + categorical_features]\no_features = categorical_features.copy()\ncategorical_features = [col+'_cat' for col in categorical_features]\no_trans.columns = ['TransactionID'] + categorical_features\ndel o_ident\n\ntemp_df = train_df[['TransactionID']]\ntemp_df = temp_df.merge(o_trans, on=['TransactionID'], how='left')\ndel temp_df['TransactionID']\ntrain_df = pd.concat([train_df, temp_df], axis=1)\n\ntemp_df = test_df[['TransactionID']]\ntemp_df = temp_df.merge(o_trans, on=['TransactionID'], how='left')\ndel temp_df['TransactionID']\ntest_df = pd.concat([test_df, temp_df], axis=1)\ndel temp_df, o_trans\n\nfor col in o_features:\n    if train_df[col].equals(train_df[col+'_cat']):\n        print('No transformation (keep only categorical)', col)\n        del train_df[col], test_df[col]\n        \n    col = col+'_cat'    \n    train_df[col] = train_df[col].fillna(-999)\n    test_df[col]  = test_df[col].fillna(-999)\n\ncategorical_features += add_category","metadata":{"execution":{"iopub.status.busy":"2021-10-05T13:43:57.416106Z","iopub.execute_input":"2021-10-05T13:43:57.416417Z","iopub.status.idle":"2021-10-05T13:44:28.31346Z","shell.execute_reply.started":"2021-10-05T13:43:57.41636Z","shell.execute_reply":"2021-10-05T13:44:28.312578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########################### Transform Heavy Dominated columns\ntotal_items = len(train_df)\nkeep_cols = [TARGET,'C3_fq_enc']\n\nfor col in list(train_df):\n    if train_df[col].dtype.name!='category':\n        cur_dominator = list(train_df[col].fillna(-999).value_counts())[0]\n        if (cur_dominator/total_items > 0.85) and (col not in keep_cols):\n            cur_dominator = train_df[col].fillna(-999).value_counts().index[0]\n            print('Column:', col, ' | Dominator:', cur_dominator)\n            train_df[col] = np.where(train_df[col].fillna(-999)==cur_dominator,1,0)\n            test_df[col] = np.where(test_df[col].fillna(-999)==cur_dominator,1,0)\n\n            train_df[col] = train_df[col].fillna(-999).astype(int)\n            test_df[col] = test_df[col].fillna(-999).astype(int)\n\n            if col not in categorical_features:\n                categorical_features.append(col)\n                \ncategorical_features +=['D8_not_same_day','TransactionAmt_check']","metadata":{"execution":{"iopub.status.busy":"2021-10-05T13:44:28.319226Z","iopub.execute_input":"2021-10-05T13:44:28.32148Z","iopub.status.idle":"2021-10-05T13:45:10.78687Z","shell.execute_reply.started":"2021-10-05T13:44:28.321414Z","shell.execute_reply":"2021-10-05T13:45:10.786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########################### Restore some categorical features\n## These features weren't useful for lgbm\n## but catboost can use it\nrestore_features = [\n                    'uid','uid2','uid3','uid4','uid5','bank_type',\n                    ]\n\nfor col in restore_features:\n    categorical_features.append(col)\n    remove_features.remove(col)","metadata":{"execution":{"iopub.status.busy":"2021-10-05T13:45:10.78932Z","iopub.execute_input":"2021-10-05T13:45:10.789661Z","iopub.status.idle":"2021-10-05T13:45:10.796969Z","shell.execute_reply.started":"2021-10-05T13:45:10.789612Z","shell.execute_reply":"2021-10-05T13:45:10.796081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########################### Remove 100% duplicated columns\ncols_sum = {}\nbad_types = ['datetime64[ns]', 'category','object']\n\nfor col in list(train_df):\n    if train_df[col].dtype.name not in bad_types:\n        cur_col = train_df[col].values\n        cur_sum = cur_col.mean()\n        try:\n            cols_sum[cur_sum].append(col)\n        except:\n            cols_sum[cur_sum] = [col]\n\ncols_sum = {k:v for k,v in cols_sum.items() if len(v)>1}   \n\nfor k,v in cols_sum.items():\n    for col in v[1:]:\n        if train_df[v[0]].equals(train_df[col]):\n            print('Duplicate', col)\n            del train_df[col], test_df[col]","metadata":{"execution":{"iopub.status.busy":"2021-10-05T13:45:10.798271Z","iopub.execute_input":"2021-10-05T13:45:10.798661Z","iopub.status.idle":"2021-10-05T13:45:19.176663Z","shell.execute_reply.started":"2021-10-05T13:45:10.79861Z","shell.execute_reply":"2021-10-05T13:45:19.175829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########################### Encode Str columns\n# As we restored some original features\n# we nned to run LabelEncoder to reduce\n# memory usage and garant that there are no nans\nfor col in list(train_df):\n    if train_df[col].dtype=='O':\n        print(col)\n        train_df[col] = train_df[col].fillna('unseen_before_label')\n        test_df[col]  = test_df[col].fillna('unseen_before_label')\n        \n        train_df[col] = train_df[col].astype(str)\n        test_df[col] = test_df[col].astype(str)\n        \n        le = LabelEncoder()\n        le.fit(list(train_df[col])+list(test_df[col]))\n        train_df[col] = le.transform(train_df[col])\n        test_df[col]  = le.transform(test_df[col])\n    \n    elif col in categorical_features:\n        train_df[col] = train_df[col].astype(float).fillna(-999)\n        test_df[col]  = test_df[col].astype(float).fillna(-999)\n        \n        le = LabelEncoder()\n        le.fit(list(train_df[col])+list(test_df[col]))\n        train_df[col] = le.transform(train_df[col])\n        test_df[col]  = le.transform(test_df[col])","metadata":{"execution":{"iopub.status.busy":"2021-10-05T13:45:19.180175Z","iopub.execute_input":"2021-10-05T13:45:19.180446Z","iopub.status.idle":"2021-10-05T13:46:29.38203Z","shell.execute_reply.started":"2021-10-05T13:45:19.180397Z","shell.execute_reply":"2021-10-05T13:46:29.380933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########################### Final features list\nfeatures_columns = [col for col in list(train_df) if col not in remove_features]\ncategorical_features = [col for col in categorical_features if col in features_columns]\n\n########################### Final Minification\n## I don't like this part as it changes float numbers\n## small change but change.\n## To be able to train catboost without \n## minification we need to do some changes on model\n## we will do it later.\nif not LOCAL_TEST:\n    train_df = reduce_mem_usage(train_df)\n    test_df  = reduce_mem_usage(test_df)\n    \ntrain_df = train_df[['TransactionID','DT_M',TARGET]+features_columns]\ntest_df  = test_df[['TransactionID','DT_M',TARGET]+features_columns]\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-10-05T13:46:29.383657Z","iopub.execute_input":"2021-10-05T13:46:29.383939Z","iopub.status.idle":"2021-10-05T13:47:50.246998Z","shell.execute_reply.started":"2021-10-05T13:46:29.383894Z","shell.execute_reply":"2021-10-05T13:47:50.246362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########################### Cleaning\n# Check what variables consume memory\nfor name, size in sorted(((name, sys.getsizeof(value)) for name,value in locals().items()),\n                         key= lambda x: -x[1])[:10]:\n    print(\"{:>30}: {:>8}\".format(name,sizeof_fmt(size)))\nprint('Memory in Gb', get_memory_usage())\n\n# Confirm thar variable exist\ntemp_df = 0\n\ndel temp_df\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-10-05T13:47:50.248322Z","iopub.execute_input":"2021-10-05T13:47:50.248611Z","iopub.status.idle":"2021-10-05T13:47:50.448905Z","shell.execute_reply.started":"2021-10-05T13:47:50.248565Z","shell.execute_reply":"2021-10-05T13:47:50.448006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########################### Model Train\nif LOCAL_TEST:\n    test_predictions = make_predictions(train_df, test_df, features_columns, TARGET, cat_params, \n                                        NFOLDS=4, kfold_mode='grouped')\n\nelse:    \n    # Why NFOLDS = 6 -> we have 6 months -> let's split it by month))\n    NFOLDS = 6\n    folds = GroupKFold(n_splits=NFOLDS)\n\n    X,y = train_df[features_columns], train_df[TARGET]    \n    P,P_y = test_df[features_columns], test_df[TARGET]  \n    \n    split_groups = train_df['DT_M']\n    # We don't need original sets anymore\n    # let's reduce it\n    train_df = train_df[['TransactionID',TARGET]] \n    test_df = test_df[['TransactionID',TARGET]] \n    test_df['prediction'] = 0\n    gc.collect()\n    \n    oof = np.zeros(len(train_df))\n    predictions = np.zeros(len(test_df))\n\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y, groups=split_groups)):\n        print('Fold:',fold_)\n        \n        estimator = CatBoostClassifier(**cat_params)        \n        estimator.fit(\n            X.iloc[trn_idx,:],y[trn_idx],\n            eval_set=(X.iloc[val_idx,:], y[val_idx]),\n            cat_features=categorical_features,\n            use_best_model=True,\n            verbose=True)\n\n        oof_preds = estimator.predict_proba(X.iloc[val_idx,:])[:,1]\n        oof[val_idx] = (oof_preds - oof_preds.min())/(oof_preds.max() - oof_preds.min())\n        test_df['prediction'] += estimator.predict_proba(P)[:,1]/NFOLDS\n        \n        del estimator\n        gc.collect()\n        \n    print('OOF AUC:', metrics.roc_auc_score(y, oof))","metadata":{"execution":{"iopub.status.busy":"2021-10-05T13:47:50.450546Z","iopub.execute_input":"2021-10-05T13:47:50.450977Z","iopub.status.idle":"2021-10-05T15:41:19.038258Z","shell.execute_reply.started":"2021-10-05T13:47:50.450812Z","shell.execute_reply":"2021-10-05T15:41:19.03749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########################### Export\nif not LOCAL_TEST:\n    test_df['isFraud'] = test_df['prediction']\n    test_df[['TransactionID','isFraud']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-05T15:41:19.039563Z","iopub.execute_input":"2021-10-05T15:41:19.040027Z","iopub.status.idle":"2021-10-05T15:41:21.643308Z","shell.execute_reply.started":"2021-10-05T15:41:19.039975Z","shell.execute_reply":"2021-10-05T15:41:21.64255Z"},"trusted":true},"execution_count":null,"outputs":[]}]}