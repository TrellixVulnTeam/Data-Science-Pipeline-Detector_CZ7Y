{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport gc\nimport warnings\nimport sys\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage_func(df):\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    return df\n   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_iden = pd.read_csv(\"../input/ieee-fraud-detection/train_identity.csv\", index_col = 'TransactionID')\ntrain_trans = pd.read_csv(\"../input/ieee-fraud-detection/train_transaction.csv\", index_col = 'TransactionID')\ntest_iden = pd.read_csv(\"../input/ieee-fraud-detection/test_identity.csv\" , index_col = 'TransactionID')\ntest_trans = pd.read_csv(\"../input/ieee-fraud-detection/sample_submission.csv\" , index_col = 'TransactionID')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ide = reduce_mem_usage_func(train_iden)\ntrain_tran = reduce_mem_usage_func(train_trans)\ntest_tran = reduce_mem_usage_func(test_trans)\ntest_ide = reduce_mem_usage_func(test_iden)\ndel train_trans , test_trans\ndel train_iden , test_iden\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train_tran.merge(train_ide, how='left', left_index=True, right_index=True)\ntest = test_tran.merge(test_ide , how = 'left', left_index = True, right_index = True)\ndel train_tran, train_ide, test_ide, test_tran\ngc.collect()\ntrain.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"id_31\"].fillna(0, inplace = True)\ntrain.head(5)\ntrain[\"id_31\"][train[\"id_31\"] != 0] = 1\ntest[\"id_31\"].fillna(0, inplace = True)\ntrain.head(5)\ntest[\"id_31\"][test[\"id_31\"] != 0] = 1\ntest[\"id_31\"].head(10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in train.columns:\n    print(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"one_value_cols = [col for col in train.columns if train[col].nunique() <= 1]\nmany_null_cols = [col for col in train.columns if train[col].isnull().sum() / train.shape[0] > 0.9]\nbig_top_value_cols = [col for col in train.columns if train[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\none_value_cols_test = [col for col in test.columns if test[col].nunique() <= 1]\nman_null_cols_test = [col for col in test.columns if test[col].isnull().sum() / train.shape[0] > 0.9]\nbig_top_value_cols_test = [col for col in test.columns if test[col].value_counts(dropna = False, normalize = True).values[0] > 0.9]\ncol_drop = list(set(one_value_cols + many_null_cols + big_top_value_cols + one_value_cols_test + man_null_cols_test + big_top_value_cols_test ))\ncol_drop.remove('isFraud')\nprint('{} total number of droped features.'.format(len(col_drop)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain.drop(col_drop , axis = 1)\ntest_col = ['V133','V129','V299', 'V106', 'V301', 'V318', 'V119', 'V296', 'V113', 'V110', 'V112', 'V300', 'V105', 'D7' ,'V108' ,'V320', 'V121', 'V111' ,'V132', 'V117', 'V134' ,'V290', 'V98', 'V101', 'V136', 'V124', 'V297', 'V311', 'V284' ,'V109', 'V122', 'V115' ,'V305' ,'V137', 'V321', 'V281', 'V309', 'V114', 'V123', 'V319', 'V293', 'dist2', 'V125', 'V135', 'V286', 'V298', 'V316', 'V104', 'V103', 'V120' ,'V116', 'V118', 'V295', 'V107', 'C3', 'V102']\ntest_col[2]\nfor i in test_col:\n    print(i)\n    col_drop.remove(i)\n    \nprint(len(col_drop))\ntest.drop(col_drop , axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = 0\nfor col in train.columns: \n    if train[col].dtype == 'object':\n        le = LabelEncoder()\n        le.fit(list(train[col].astype(str).values) )\n        train[col] = le.transform(list(train[col].astype(str).values))\n        m = m + 1\nprint(m)\ndel m\n\nn = 0\nfor col in test.columns: \n    if test[col].dtype == 'object':\n        le = LabelEncoder()\n        le.fit(list(test[col].astype(str).values) )\n        test[col] = le.transform(list(test[col].astype(str).values))\n        n = n + 1\nprint(n)\ndel n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for c in train.columns:\n    if train[c].dtype=='float16' or  train[c].dtype=='float32' or  train[c].dtype=='float64':\n        train[c].fillna(train[c].mean())\n\nfor c in test.columns:\n    if test[c].dtype=='float16' or  test[c].dtype=='float32' or  test[c].dtype=='float64':\n        test[c].fillna(test[c].mean())        \n        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train.iloc[:, 0].values\ntrain = train.drop('isFraud', axis = 1)\nx = train.iloc[:,:].values\n#test = test.iloc[:,:].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nkfold = 5\nskf = StratifiedKFold(n_splits=kfold, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'min_child_weight': 10.0,\n    'objective': 'binary:logistic',\n    'max_depth': 7,\n    'max_delta_step': 1.8,\n    'colsample_bytree': 0.4,\n    'subsample': 0.8,\n    'eta': 0.025,\n    'gamma': 0.65,\n    'num_boost_round' : 700\n    }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gini(actual, pred, cmpcol = 0, sortcol = 1):\n    assert( len(actual) == len(pred) )\n    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n    totalLosses = all[:,0].sum()\n    giniSum = all[:,0].cumsum().sum() / totalLosses\n    \n    giniSum -= (len(actual) + 1) / 2.\n    return giniSum / len(actual)\n \ndef gini_normalized(a, p):\n    return gini(a, p) / gini(a, a)\n\ndef gini_xgb(preds, dtrain):\n    labels = dtrain.get_label()\n    gini_score = gini_normalized(labels, preds)\n    return 'gini', gini_score\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nfor i, (train_index, test_index) in enumerate(skf.split(x, y)):\n    print('[Fold %d/%d]' % (i + 1, kfold))\n    X_train, X_valid = x[train_index], x[test_index]\n    y_train, y_valid = y[train_index], y[test_index]\n    # Convert our data into XGBoost format\n    d_train = xgb.DMatrix(X_train, y_train)\n    d_valid = xgb.DMatrix(X_valid, y_valid)\n    d_test = xgb.DMatrix(test.values)\n    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n\n    # Train the model! We pass in a max of 1,600 rounds (with early stopping after 70)\n    # and the custom metric (maximize=True tells xgb that higher metric is better)\n    mdl = xgb.train(params, d_train, 1600, watchlist, early_stopping_rounds=70, feval=gini_xgb, maximize=True, verbose_eval=100)\n\n    print('[Fold %d/%d Prediciton:]' % (i + 1, kfold))\n    # Predict on our test data\n    p_test = mdl.predict(d_test, ntree_limit=mdl.best_ntree_limit)\n    sub['target'] += p_test/kfold","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}