{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Some models can make more useful feature\nIn this kernel. I will try DeepCtr model such as DeepFm, Xdeepfm, AutoInt and PNN model.\n- fm component of Deepfm can create cross featureï¼š order-2 feature interactions\n- WHile Product component of PNN can create: feature interactions viewd as a series of multiplication operations\n\nIt means PNN can create group feature in this competition.\n- group feature such like \n    ```python\n    newfeature =  df['feature'].groupby(by = ['card1','card2','card3']).count()\n    ```\n\nAnd now, let us show PNN whether is better than other model?<br>\nAnd then, discover whether group feature is more useful than order-2 feature interactions."},{"metadata":{},"cell_type":"markdown","source":"I will use deepctr-torch , because it is really easy to use"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport pickle as pk\nimport torch\n\nfrom sklearn.metrics import log_loss, roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\n\n!pip install deepctr_torch\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom deepctr_torch.models import DeepFM, xDeepFM, AutoInt, PNN\nfrom deepctr_torch.inputs import  SparseFeat, DenseFeat, get_feature_names\n\npd.set_option('display.max_columns', 500)\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> # Prepocess Data\n- make data can be fed into NN model"},{"metadata":{},"cell_type":"markdown","source":"Just use origin feature, dataset is created by merge identity.csv and transaction.csv from train and test "},{"metadata":{"trusted":true},"cell_type":"code","source":"\nwith open('../input/ieeefraud/train.pickle', 'rb') as file:\n    train =pk.load(file)\n    \nwith open('../input/ieeefraud/test.pickle', 'rb') as file:\n    test =pk.load(file)\n    \nfile.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = ['isFraud']\n\nsparse_features = ['ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6',  'addr1', \n'addr2', 'P_emaildomain', 'R_emaildomain',  'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'DeviceType', \n'DeviceInfo', 'id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', \n'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', \n'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38',]\n\nnotfeature = [\"TransactionID\"]\n\ndense_features = [i for i in train.columns if i not in sparse_features+target+notfeature]\n\ntest['isFraud'] = 0\ndata = pd.concat([train, test], axis=0, sort=False)\nprint(train.shape, test.shape, data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train.fillna(-10, inplace=True)\n# test.fillna(-10, inplace=True)\ndata[sparse_features] = data[sparse_features].fillna(\"null\",)\ndata[dense_features] = data[dense_features].fillna(-999,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[sparse_features] = data[sparse_features].astype(\"str\")\n\n# for feat in dense_features:\n#     print(\"start pro \", feat)\n#     if data[feat].dtype == 'O':\n#         print(\"object -> float\", feat)\n#         data[feat] = data[feat].astype(\"float\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#del train, test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1.Label Encoding for sparse features,and do simple Transformation for dense features\n\nfor feat in sparse_features:\n    # print(\"Process: \", feat)\n    lbe = LabelEncoder()\n    data[feat] = lbe.fit_transform(data[feat])\nprint(\"Process sparse finish \")\n\nmms = MinMaxScaler(feature_range=(0, 1))\nfor feat in dense_features:\n    print(\"Process: \", feat)\n    data[feat] = mms.fit_transform(np.array(data[feat]).reshape(-1, 1))\nprint(\"Process dense finish \")\n\n# 2.count #unique features for each sparse field,and record dense feature field name\n\nfixlen_feature_columns = [SparseFeat(feat, data[feat].nunique())\n                       for feat in sparse_features] + [DenseFeat(feat, 1,)\n                      for feat in dense_features]\n\ndnn_feature_columns = fixlen_feature_columns\nlinear_feature_columns = fixlen_feature_columns\n\nfixlen_feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 3.generate input data for model\n\n#train, test = train_test_split(data, test_size=0.2)\ntrain = data[:590540]\ntest = data[590540:]\nprint(train.shape, test.shape, data.shape)\n\ntrain_model_input = [train[name] for name in fixlen_feature_names]\ntest_model_input = [test[name] for name in fixlen_feature_names]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Single Model"},{"metadata":{},"cell_type":"markdown","source":"## auto int\n\n- use self-attention to learn feature iteraction "},{"metadata":{"trusted":true},"cell_type":"code","source":"# import warnings\n# warnings.filterwarnings('ignore')\n\nbs = 1024\ndevice = 'cpu'\nuse_cuda = True\nif use_cuda and torch.cuda.is_available():\n    print('cuda ready...')\n    device = 'cuda:0'\n\nmodel = AutoInt(dnn_feature_columns,task='binary',device=device)\nmodel.compile(\"adam\", \"binary_crossentropy\",\n              metrics=['auc'], )\n\nhistory = model.fit(train_model_input, train[target].values,\n                    batch_size=bs, epochs=10, verbose=2, validation_split=0.2, shuffle=True)\n\nAI_pred_trn = model.predict(train_model_input, batch_size=bs)\nAI_pred_ans = model.predict(test_model_input, batch_size=bs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## xDeepFM"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import warnings\n# warnings.filterwarnings('ignore')\n\n\nmodel = xDeepFM(linear_feature_columns,dnn_feature_columns,task='binary',device=device)\nmodel.compile(\"adam\", \"binary_crossentropy\",\n              metrics=['auc'], )\n\nhistory = model.fit(train_model_input, train[target].values,\n                    batch_size=bs, epochs=10, verbose=2, validation_split=0.2, shuffle=True)\n\nXD_pred_trn = model.predict(train_model_input, batch_size=bs)\nXD_pred_ans = model.predict(test_model_input, batch_size=bs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## DeepFM\n- 0.87"},{"metadata":{"trusted":true},"cell_type":"code","source":" \nbs = 1024\nmodel = DeepFM(linear_feature_columns,dnn_feature_columns,task='binary',device=device)\nmodel.compile(\"adam\", \"binary_crossentropy\", \n              metrics=['auc', 'logloss'], )\n\nhistory = model.fit(train_model_input, train[target].values, \n                    batch_size=bs, epochs=10, verbose=2, validation_split=0.2, shuffle=True)\n\nDF_pred_trn = model.predict(train_model_input, batch_size=bs)\nDF_pred_ans = model.predict(test_model_input, batch_size=bs)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"bs = 1024\nmodel = PNN(dnn_feature_columns,task='binary',device=device)\nmodel.compile(\"adam\", \"binary_crossentropy\", \n              metrics=['auc', 'logloss'], )\n\nhistory = model.fit(train_model_input, train[target].values, \n                    batch_size=bs, epochs=10, verbose=2, validation_split=0.2, shuffle=True)\n\nPN_pred_trn = model.predict(train_model_input, batch_size=bs)\nPN_pred_ans = model.predict(test_model_input, batch_size=bs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stack\n\nstack these for model, and use feature importance to judge good and bad"},{"metadata":{"trusted":true},"cell_type":"code","source":"trn_stack = train[['TransactionID','isFraud']]\ntest['isFraud'] = 0\ntet_stack = test[[\"TransactionID\", \"isFraud\"]]\n\ntrn_stack['AI_pred'] = AI_pred_trn\ntet_stack['AI_pred'] = AI_pred_ans\n\ntrn_stack['XD_pred'] = XD_pred_trn\ntet_stack['XD_pred'] = XD_pred_ans\n\ntrn_stack['DF_pred'] = DF_pred_trn\ntet_stack['DF_pred'] = DF_pred_ans\n\ntrn_stack['PN_pred'] = PN_pred_trn\ntet_stack['PN_pred'] = PN_pred_ans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_fe = [i for i in trn_stack.columns if i not in ['TransactionID', 'isFraud']]\ntrn_stack.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tet_stack.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, KFold, GroupKFold, StratifiedKFold\nimport lightgbm as lgb\nimport gc\n\ndef make_predictions(tr_df, tt_df, features_columns, target, lgb_params, NFOLDS=3):\n    folds = StratifiedKFold(n_splits=NFOLDS, shuffle=True)\n    # tr_df['VLABEL'] = 0\n\n    X, y = tr_df[features_columns], tr_df[target]\n    P, P_y = tt_df[features_columns], tt_df[target]\n\n    tt_df = tt_df[['TransactionID', target]]\n    predictions = np.zeros(len(tt_df))\n    oof = np.zeros((len(tr_df),1))\n\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n        print('Fold:', fold_)\n        tr_x, tr_y = X.iloc[trn_idx, :], y[trn_idx]\n        vl_x, vl_y = X.iloc[val_idx, :], y[val_idx]\n        tr_data = lgb.Dataset(tr_x, label=tr_y)\n        vl_data = lgb.Dataset(vl_x, label=vl_y)\n\n        estimator = lgb.train(\n            lgb_params,\n            tr_data,\n            valid_sets=[vl_data],\n            verbose_eval=200,\n        )\n        \n        oof[val_idx] = estimator.predict(vl_x).reshape(-1, 1)\n        pp_p = estimator.predict(P)\n        # Y_label = estimator.predict(X)\n        predictions += pp_p / NFOLDS\n        # tr_df['VLABEL'] += Y_label / NFOLDS\n\n        del tr_x, tr_y, vl_x, vl_y, tr_data, vl_data\n        gc.collect()\n        \n        feature_imp = pd.DataFrame(sorted(zip(estimator.feature_importance(), X.columns)),\n                                       columns=['Value', 'Feature'])\n        print(feature_imp)\n\n    # tr_df[['TransactionID', 'VLABEL']].to_csv('submission.csv', index=False)\n    tt_df['prediction'] = predictions\n\n    return tt_df, oof","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_params = {\n                    'objective':'binary',\n                    'boosting_type':'gbdt',\n                    'metric':'auc',\n                    'n_jobs':-1,\n                    'learning_rate':0.01,\n                    'n_estimators':800,\n                    'verbose':-1,\n                }\n \ntest_predictions, oof = make_predictions(trn_stack, tet_stack, train_fe, 'isFraud', lgb_params, NFOLDS=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predictions['isFraud'] = test_predictions['prediction']\ntest_predictions.sort_values('TransactionID', inplace=True)\ntest_predictions[['TransactionID','isFraud']].to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predictions.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\nYou can find that PNN model can get the highest importance among these four models"},{"metadata":{},"cell_type":"markdown","source":"# BTW \nOf course, you can make more feature you create, and use my backbone to train.<br>\nI just use origin feature, so the score will not good enough"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":true}},"nbformat":4,"nbformat_minor":1}