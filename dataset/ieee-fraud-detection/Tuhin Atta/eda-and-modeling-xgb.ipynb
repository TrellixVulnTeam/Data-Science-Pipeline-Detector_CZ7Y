{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport scipy as sp\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\n\n# Standard plotly imports\n#import plotly.plotly as py\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom plotly.offline import iplot, init_notebook_mode\n#import cufflinks\n#import cufflinks as cf\nimport plotly.figure_factory as ff\n\n# Using plotly + cufflinks in offline mode\ninit_notebook_mode(connected=True)\n#cufflinks.go_offline(connected=True)\n\n# Preprocessing, modelling and evaluating\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix, roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, KFold\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\n\n## Hyperopt modules\nfrom hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING\nfrom functools import partial\n\nimport os\nimport gc\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing train datasets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_id = pd.read_csv(\"../input/train_identity.csv\")\ndf_trans = pd.read_csv(\"../input/train_transaction.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def resumetable(df):\n    print(f\"Dataset Shape: {df.shape}\")\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['First Value'] = df.loc[0].values\n    summary['Second Value'] = df.loc[1].values\n    summary['Third Value'] = df.loc[2].values\n\n    for name in summary['Name'].value_counts().index:\n        summary.loc[summary['Name'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True), base=2),2) \n\n    return summary\n\n## Function to reduce the DF size\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n\ndef CalcOutliers(df_num): \n\n    # calculating mean and std of the array\n    data_mean, data_std = np.mean(df_num), np.std(df_num)\n\n    # seting the cut line to both higher and lower values\n    # You can change this value\n    cut = data_std * 3\n\n    #Calculating the higher and lower cut values\n    lower, upper = data_mean - cut, data_mean + cut\n\n    # creating an array of lower, higher and total outlier values \n    outliers_lower = [x for x in df_num if x < lower]\n    outliers_higher = [x for x in df_num if x > upper]\n    outliers_total = [x for x in df_num if x < lower or x > upper]\n\n    # array without outlier values\n    outliers_removed = [x for x in df_num if x > lower and x < upper]\n    \n    print('Identified lowest outliers: %d' % len(outliers_lower)) # printing total number of values in lower cut of outliers\n    print('Identified upper outliers: %d' % len(outliers_higher)) # printing total number of values in higher cut of outliers\n    print('Total outlier observations: %d' % len(outliers_total)) # printing total number of values outliers of both sides\n    print('Non-outlier observations: %d' % len(outliers_removed)) # printing total number of non outlier values\n    print(\"Total percentual of Outliers: \", round((len(outliers_total) / len(outliers_removed) )*100, 4)) # Percentual of outliers in points\n    \n    return","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Reducing memory\ndf_trans = reduce_mem_usage(df_trans)\ndf_id = reduce_mem_usage(df_id)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"resumetable(df_trans)[:25]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Target Distribution","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df_trans['TransactionAmt'] = df_trans['TransactionAmt'].astype(float)\ntotal = len(df_trans)\ntotal_amt = df_trans.groupby(['isFraud'])['TransactionAmt'].sum().sum()\nplt.figure(figsize=(16,6))\n\nplt.subplot(121)\ng = sns.countplot(x='isFraud', data=df_trans, )\ng.set_title(\"Fraud Transactions Distribution \\n# 0: No Fraud | 1: Fraud #\", fontsize=22)\ng.set_xlabel(\"Is fraud?\", fontsize=18)\ng.set_ylabel('Count', fontsize=18)\nfor p in g.patches:\n    height = p.get_height()\n    g.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:1.2f}%'.format(height/total*100),\n            ha=\"center\", fontsize=15) \n\nperc_amt = (df_trans.groupby(['isFraud'])['TransactionAmt'].sum())\nperc_amt = perc_amt.reset_index()\nplt.subplot(122)\ng1 = sns.barplot(x='isFraud', y='TransactionAmt',  dodge=True, data=perc_amt)\ng1.set_title(\"% Total Amount in Transaction Amt \\n# 0: No Fraud | 1: Fraud #\", fontsize=22)\ng1.set_xlabel(\"Is fraud?\", fontsize=18)\ng1.set_ylabel('Total Transaction Amount Scalar', fontsize=18)\nfor p in g1.patches:\n    height = p.get_height()\n    g1.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:1.2f}%'.format(height/total_amt * 100),\n            ha=\"center\", fontsize=15) \n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Transaction Amount Quantiles","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df_trans['TransactionAmt'] = df_trans['TransactionAmt'].astype(float)\nprint(\"Transaction Amounts Quantiles:\")\nprint(df_trans['TransactionAmt'].quantile([.01, .025, .1, .25, .5, .75, .9, .975, .99]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ploting Transaction Amount Values Distribution","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(16,12))\nplt.suptitle('Transaction Values Distribution', fontsize=22)\nplt.subplot(221)\ng = sns.distplot(df_trans[df_trans['TransactionAmt'] <= 1000]['TransactionAmt'])\ng.set_title(\"Transaction Amount Distribuition <= 1000\", fontsize=18)\ng.set_xlabel(\"\")\ng.set_ylabel(\"Probability\", fontsize=15)\n\nplt.subplot(222)\ng1 = sns.distplot(np.log(df_trans['TransactionAmt']))\ng1.set_title(\"Transaction Amount (Log) Distribuition\", fontsize=18)\ng1.set_xlabel(\"\")\ng1.set_ylabel(\"Probability\", fontsize=15)\n\nplt.figure(figsize=(16,12))\n\n\nplt.subplot(212)\ng4 = plt.scatter(range(df_trans[df_trans['isFraud'] == 0].shape[0]),\n                 np.sort(df_trans[df_trans['isFraud'] == 0]['TransactionAmt'].values), \n                 label='NoFraud', alpha=.2)\ng4 = plt.scatter(range(df_trans[df_trans['isFraud'] == 1].shape[0]),\n                 np.sort(df_trans[df_trans['isFraud'] == 1]['TransactionAmt'].values), \n                 label='Fraud', alpha=.2)\ng4= plt.title(\"ECDF \\nFRAUD and NO FRAUD Transaction Amount Distribution\", fontsize=18)\ng4 = plt.xlabel(\"Index\")\ng4 = plt.ylabel(\"Amount Distribution\", fontsize=15)\ng4 = plt.legend()\n\nplt.figure(figsize=(16,12))\n\nplt.subplot(321)\ng = plt.scatter(range(df_trans[df_trans['isFraud'] == 1].shape[0]), \n                 np.sort(df_trans[df_trans['isFraud'] == 1]['TransactionAmt'].values), \n                label='isFraud', alpha=.4)\nplt.title(\"FRAUD - Transaction Amount ECDF\", fontsize=18)\nplt.xlabel(\"Index\")\nplt.ylabel(\"Amount Distribution\", fontsize=12)\n\nplt.subplot(322)\ng1 = plt.scatter(range(df_trans[df_trans['isFraud'] == 0].shape[0]),\n                 np.sort(df_trans[df_trans['isFraud'] == 0]['TransactionAmt'].values), \n                 label='NoFraud', alpha=.2)\ng1= plt.title(\"NO FRAUD - Transaction Amount ECDF\", fontsize=18)\ng1 = plt.xlabel(\"Index\")\ng1 = plt.ylabel(\"Amount Distribution\", fontsize=15)\n\nplt.suptitle('Individual ECDF Distribution', fontsize=22)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Seeing the Quantiles of Fraud and No Fraud Transactions","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(pd.concat([df_trans[df_trans['isFraud'] == 1]['TransactionAmt']\\\n                 .quantile([.01, .1, .25, .5, .75, .9, .99])\\\n                 .reset_index(), \n                 df_trans[df_trans['isFraud'] == 0]['TransactionAmt']\\\n                 .quantile([.01, .1, .25, .5, .75, .9, .99])\\\n                 .reset_index()],\n                axis=1, keys=['Fraud', \"No Fraud\"]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Transaction Amount Outliers\n- It's considering outlier values that are highest than 3 times the std from the mean","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"s = 0.0\nfor i in range(len(df_trans['TransactionAmt'])) :\n    s=s+df_trans['TransactionAmt'][i]\n    \nmean=s/(len(df_trans['TransactionAmt']))\nprint (mean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import statistics\n\n# calculate summary statistics\nstd = statistics.stdev(df_trans['TransactionAmt'])\nprint(std)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cut_off = std * 3\nlower, upper = mean - cut_off, mean + cut_off\nlst=[]\n\n# identify outliers and Fraud Distribution\nfor i in range(len(df_trans['TransactionAmt'])):\n    c=df_trans['TransactionAmt'][i]\n    if(c<lower or c>upper):\n        isf=df_trans['isFraud'][i]\n        lst.extend([isf])\n\nones=[x for x in lst if x==1]\nprint(\"Total outliers are\",len(lst))\nprint(\"No of ones are ony\",len(ones))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Product Feature\n ","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"tmp = pd.crosstab(df_trans['ProductCD'], df_trans['isFraud'], normalize='index') * 100\ntmp = tmp.reset_index()\ntmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n\nplt.figure(figsize=(14,10))\nplt.suptitle('ProductCD Distributions', fontsize=22)\n\nplt.subplot(221)\ng = sns.countplot(x='ProductCD', data=df_trans)\n# plt.legend(title='Fraud', loc='upper center', labels=['No', 'Yes'])\n\ng.set_title(\"ProductCD Distribution\", fontsize=19)\ng.set_xlabel(\"ProductCD Name\", fontsize=17)\ng.set_ylabel(\"Count\", fontsize=17)\ng.set_ylim(0,500000)\nfor p in g.patches:\n    height = p.get_height()\n    g.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:1.2f}%'.format(height/total*100),\n            ha=\"center\", fontsize=14) \n\nplt.subplot(222)\ng1 = sns.countplot(x='ProductCD', hue='isFraud', data=df_trans)\nplt.legend(title='Fraud', loc='best', labels=['No', 'Yes'])\ngt = g1.twinx()\ngt = sns.pointplot(x='ProductCD', y='Fraud', data=tmp, color='black', order=['W', 'H',\"C\", \"S\", \"R\"], legend=False)\ngt.set_ylabel(\"% of Fraud Transactions\", fontsize=16)\n\ng1.set_title(\"Product CD by Target(isFraud)\", fontsize=19)\ng1.set_xlabel(\"ProductCD Name\", fontsize=17)\ng1.set_ylabel(\"Count\", fontsize=17)\n\nplt.subplot(212)\ng3 = sns.boxenplot(x='ProductCD', y='TransactionAmt', hue='isFraud', \n              data=df_trans[df_trans['TransactionAmt'] <= 2000] )\ng3.set_title(\"Transaction Amount Distribuition by ProductCD and Target\", fontsize=20)\ng3.set_xlabel(\"ProductCD Name\", fontsize=17)\ng3.set_ylabel(\"Transaction Values\", fontsize=17)\n\nplt.subplots_adjust(hspace = 0.6, top = 0.85)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Card Features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Knowning the Card Features\nresumetable(df_trans[['card1', 'card2', 'card3','card4', 'card5', 'card6']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Numericals Feature Card Quantiles","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(\"Card Features Quantiles: \")\nprint(df_trans[['card1', 'card2', 'card3', 'card5']].quantile([0.01, .025, .1, .25, .5, .75, .975, .99]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Addr1 and Addr2","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Card Features Quantiles: \")\nprint(df_trans[['addr1', 'addr2']].quantile([0.01, .025, .1, .25, .5, .75, .90,.975, .99]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Addr1 Distributions","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":" def ploting_cnt_amt(df, col, lim=2000):\n    tmp = pd.crosstab(df[col], df['isFraud'], normalize='index') * 100\n    tmp = tmp.reset_index()\n    tmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n    \n    plt.figure(figsize=(16,14))    \n    plt.suptitle(f'{col} Distributions ', fontsize=24)\n    \n    plt.subplot(211)\n    g = sns.countplot( x=col,  data=df, order=list(tmp[col].values))\n    gt = g.twinx()\n    gt = sns.pointplot(x=col, y='Fraud', data=tmp, order=list(tmp[col].values),\n                       color='black', legend=False, )\n    gt.set_ylim(0,tmp['Fraud'].max()*1.1)\n    gt.set_ylabel(\"%Fraud Transactions\", fontsize=16)\n    g.set_title(f\"Most Frequent {col} values and % Fraud Transactions\", fontsize=20)\n    g.set_xlabel(f\"{col} Category Names\", fontsize=16)\n    g.set_ylabel(\"Count\", fontsize=17)\n    g.set_xticklabels(g.get_xticklabels(),rotation=45)\n    sizes = []\n    for p in g.patches:\n        height = p.get_height()\n        sizes.append(height)\n        g.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.2f}%'.format(height/total*100),\n                ha=\"center\",fontsize=12) \n        \n    g.set_ylim(0,max(sizes)*1.15)\n    \n    #########################################################################\n    perc_amt = (df.groupby(['isFraud',col])['TransactionAmt'].sum() \\\n                / df.groupby([col])['TransactionAmt'].sum() * 100).unstack('isFraud')\n    perc_amt = perc_amt.reset_index()\n    perc_amt.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n    amt = df.groupby([col])['TransactionAmt'].sum().reset_index()\n    perc_amt = perc_amt.fillna(0)\n    plt.subplot(212)\n    g1 = sns.barplot(x=col, y='TransactionAmt', \n                       data=amt, \n                       order=list(tmp[col].values))\n    g1t = g1.twinx()\n    g1t = sns.pointplot(x=col, y='Fraud', data=perc_amt, \n                        order=list(tmp[col].values),\n                       color='black', legend=False, )\n    g1t.set_ylim(0,perc_amt['Fraud'].max()*1.1)\n    g1t.set_ylabel(\"%Fraud Total Amount\", fontsize=16)\n    g.set_xticklabels(g.get_xticklabels(),rotation=45)\n    g1.set_title(f\"{col} by Transactions Total + %of total and %Fraud Transactions\", fontsize=20)\n    g1.set_xlabel(f\"{col} Category Names\", fontsize=16)\n    g1.set_ylabel(\"Transaction Total Amount(U$)\", fontsize=16)\n    g1.set_xticklabels(g.get_xticklabels(),rotation=45)    \n    \n    for p in g1.patches:\n        height = p.get_height()\n        g1.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.2f}%'.format(height/total_amt*100),\n                ha=\"center\",fontsize=12) \n        \n    plt.subplots_adjust(hspace=.4, top = 0.9)\n    plt.show()\n    \nploting_cnt_amt(df_trans, 'addr1')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# C1-C14 features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"resumetable(df_trans[['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8',\n                      'C9', 'C10', 'C11', 'C12', 'C13', 'C14']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_trans.loc[df_trans.C1.isin(df_trans.C1\\\n                              .value_counts()[df_trans.C1.value_counts() <= 400 ]\\\n                              .index), 'C1'] = \"Others\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## C1 Distribution Plot","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_trans.loc[df_trans.C2.isin(df_trans.C2\\\n                              .value_counts()[df_trans.C2.value_counts() <= 350 ]\\\n                              .index), 'C2'] = \"Others\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TimeDelta Feature\n- Let's see if the frauds have some specific hour that has highest % of frauds ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Converting to Total Days, Weekdays and Hours\nIn discussions tab I read an excellent solution to Timedelta column, I will set the link below; <br>\nWe will use the first date as 2017-12-01 and use the delta time to compute datetime features\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/c/ieee-fraud-detection/discussion/100400#latest-579480\nimport datetime\n\nSTART_DATE = '2017-12-01'\nstartdate = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\")\ndf_trans[\"Date\"] = df_trans['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\n\ndf_trans['_Weekdays'] = df_trans['Date'].dt.dayofweek\ndf_trans['_Hours'] = df_trans['Date'].dt.hour\ndf_trans['_Days'] = df_trans['Date'].dt.day","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Top Days with highest Total Transaction Amount","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ploting_cnt_amt(df_trans, '_Days')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ploting WeekDays Distributions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ploting_cnt_amt(df_trans, '_Weekdays')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We don't have the reference of date but we can see that two days has lower transactions, that we can infer it is weekend days","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Ploting Hours Distributions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ploting_cnt_amt(df_trans, '_Hours')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelling ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_trans = pd.read_csv('../input/train_transaction.csv')\ndf_test_trans = pd.read_csv('../input/test_transaction.csv')\n\ndf_id = pd.read_csv('../input/train_identity.csv')\ndf_test_id = pd.read_csv('../input/test_identity.csv')\n\nsample_submission = pd.read_csv('../input/sample_submission.csv', index_col='TransactionID')\n\ndf_train = df_trans.merge(df_id, how='left', left_index=True, right_index=True, on='TransactionID')\ndf_test = df_test_trans.merge(df_test_id, how='left', left_index=True, right_index=True, on='TransactionID')\n\nprint(df_train.shape)\nprint(df_test.shape)\n\n# y_train = df_train['isFraud'].copy()\ndel df_trans, df_id, df_test_trans, df_test_id\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# reducing memory usage","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = reduce_mem_usage(df_train)\ndf_test = reduce_mem_usage(df_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Mapping emails","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nemails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', \n          'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft',\n          'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',\n          'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', \n          'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink',\n          'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other',\n          'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', \n          'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', \n          'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo',\n          'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n          'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft',\n          'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', \n          'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', \n          'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', \n          'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', \n          'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', \n          'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', \n          'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other',\n          'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n\nus_emails = ['gmail', 'net', 'edu']\n\n# https://www.kaggle.com/c/ieee-fraud-detection/discussion/100499#latest-579654\nfor c in ['P_emaildomain', 'R_emaildomain']:\n    df_train[c + '_bin'] = df_train[c].map(emails)\n    df_test[c + '_bin'] = df_test[c].map(emails)\n    \n    df_train[c + '_suffix'] = df_train[c].map(lambda x: str(x).split('.')[-1])\n    df_test[c + '_suffix'] = df_test[c].map(lambda x: str(x).split('.')[-1])\n    \n    df_train[c + '_suffix'] = df_train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n    df_test[c + '_suffix'] = df_test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Encoding categorical features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#change id- to id_ in test_identity\nfor i in range(9):\n    j=i+1\n    s1=\"id-0\"\n    s2=\"id_0\"\n    ss1=s1+str(j)\n    ss2=s2+str(j)\n    df_test.rename(columns = {ss1:ss2},inplace = True) \n    \nfor i in range(10):\n    j=i\n    s1=\"id-1\"\n    s2=\"id_1\"\n    ss1=s1+str(j)\n    ss2=s2+str(j)\n    df_test.rename(columns = {ss1:ss2},inplace = True) \n    \nfor i in range(10):\n    j=i\n    s1=\"id-2\"\n    s2=\"id_2\"\n    ss1=s1+str(j)\n    ss2=s2+str(j)\n    df_test.rename(columns = {ss1:ss2},inplace = True) \n    \nfor i in range(9):\n    j=i\n    s1=\"id-3\"\n    s2=\"id_3\"\n    ss1=s1+str(j)\n    ss2=s2+str(j)\n    df_test.rename(columns = {ss1:ss2},inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Label Encoding\nfor f in df_train.drop('isFraud', axis=1).columns:\n    if df_train[f].dtype=='object' or df_test[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(df_train[f].values) + list(df_test[f].values))\n        df_train[f] = lbl.transform(list(df_train[f].values))\n        df_test[f] = lbl.transform(list(df_test[f].values)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Some feature engineering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Trans_min_mean'] = df_train['TransactionAmt'] - df_train['TransactionAmt'].mean()\ndf_train['Trans_min_std'] = df_train['Trans_min_mean'] / df_train['TransactionAmt'].std()\ndf_test['Trans_min_mean'] = df_test['TransactionAmt'] - df_test['TransactionAmt'].mean()\ndf_test['Trans_min_std'] = df_test['Trans_min_mean'] / df_test['TransactionAmt'].std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['TransactionAmt_to_mean_card1'] = df_train['TransactionAmt'] / df_train.groupby(['card1'])['TransactionAmt'].transform('mean')\ndf_train['TransactionAmt_to_mean_card4'] = df_train['TransactionAmt'] / df_train.groupby(['card4'])['TransactionAmt'].transform('mean')\ndf_train['TransactionAmt_to_std_card1'] = df_train['TransactionAmt'] / df_train.groupby(['card1'])['TransactionAmt'].transform('std')\ndf_train['TransactionAmt_to_std_card4'] = df_train['TransactionAmt'] / df_train.groupby(['card4'])['TransactionAmt'].transform('std')\n\ndf_test['TransactionAmt_to_mean_card1'] = df_test['TransactionAmt'] / df_test.groupby(['card1'])['TransactionAmt'].transform('mean')\ndf_test['TransactionAmt_to_mean_card4'] = df_test['TransactionAmt'] / df_test.groupby(['card4'])['TransactionAmt'].transform('mean')\ndf_test['TransactionAmt_to_std_card1'] = df_test['TransactionAmt'] / df_test.groupby(['card1'])['TransactionAmt'].transform('std')\ndf_test['TransactionAmt_to_std_card4'] = df_test['TransactionAmt'] / df_test.groupby(['card4'])['TransactionAmt'].transform('std')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['TransactionAmt'] = np.log(df_train['TransactionAmt'])\ndf_test['TransactionAmt'] = np.log(df_test['TransactionAmt'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Time Distribution Pattern**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vals = plt.hist(df_train['TransactionDT'] / (3600*24), bins=1800)\nplt.xlim(70, 78)\nplt.xlabel('Days')\nplt.ylabel('Number of transactions')\nplt.ylim(0,1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_day_feature(df, offset=0, tname='TransactionDT'):\n    \"\"\"\n    Creates a day of the week feature, encoded as 0-6. \n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        df to manipulate.\n    offset : float (default=0)\n        offset (in days) to shift the start/end of a day.\n    tname : str\n        Name of the time column in df.\n    \"\"\"\n    # found a good offset is 0.58\n    days = df[tname] / (3600*24)        \n    encoded_days = np.floor(days-1+offset) % 7\n    return encoded_days\n\ndef make_hour_feature(df, tname='TransactionDT'):\n    \"\"\"\n    Creates an hour of the day feature, encoded as 0-23. \n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        df to manipulate.\n    tname : str\n        Name of the time column in df.\n    \"\"\"\n    hours = df[tname] / (3600)        \n    encoded_hours = np.floor(hours) % 24\n    return encoded_hours","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['weekday'] = make_day_feature(df_train, offset=0.58)\nplt.plot(df_train.groupby('weekday').mean()['isFraud'])\n\nplt.ylim(0, 0.04)\nplt.xlabel('Encoded day')\nplt.ylabel('Fraction of fraudulent transactions')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['hours'] = make_hour_feature(df_train)\nplt.plot(df_train.groupby('hours').mean()['isFraud'], color='k')\nax = plt.gca()\nax2 = ax.twinx()\n_ = ax2.hist(df_train['hours'], alpha=0.3, bins=24)\nax.set_xlabel('Encoded hour')\nax.set_ylabel('Fraction of fraudulent transactions')\nax2.set_ylabel('Number of transactions')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Concating dfs to get PCA of V features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['isFraud'] = 'test'\ndf = pd.concat([df_train, df_test], axis=0, sort=False )\ndf = df.reset_index()\ndf = df.drop('index', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def PCA_change(df, cols, n_components, prefix='PCA_', rand_seed=42):\n    pca = PCA(n_components=n_components, random_state=rand_seed)\n\n    principalComponents = pca.fit_transform(df[cols])\n\n    principalDf = pd.DataFrame(principalComponents)\n\n    df.drop(cols, axis=1, inplace=True)\n\n    principalDf.rename(columns=lambda x: str(prefix)+str(x), inplace=True)\n\n    df = pd.concat([df, principalDf], axis=1)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mas_v = df_train.columns[55:394]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Getting PCA ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import minmax_scale\nfrom sklearn.decomposition import PCA\n# from sklearn.cluster import KMeans\n\nfor col in mas_v:\n    df[col] = df[col].fillna((df[col].min() - 2))\n    df[col] = (minmax_scale(df[col], feature_range=(0,1)))\n\n    \ndf = PCA_change(df, mas_v, prefix='PCA_V_', n_components=30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = reduce_mem_usage(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Setting train and test back","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train, df_test = df[df['isFraud'] != 'test'], df[df['isFraud'] == 'test'].drop('isFraud', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_col = np.zeros(len(df_train['TransactionDT']), dtype = int)\nfor i in range(len(df_train['TransactionDT'])):\n    val=((df_train['TransactionDT'][i])/(3600*24))\n    encoded_days = np.floor(val-1+0.5) % 7\n    new_col[i]=encoded_days\n    \ndf_train['Day_Col']=new_col\n\n\nnew_col = np.zeros(len(df_test['TransactionDT']), dtype = int)\nfor i in range(len(df_test['TransactionDT'])):\n    val=((df_test['TransactionDT'][i+590540])/(3600*24))\n    encoded_days = np.floor(val-1+0.5) % 7\n    new_col[i]=encoded_days\n    \ndf_test['Day_Col']=new_col","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_col = np.zeros(len(df_train['TransactionDT']), dtype = int)\nfor i in range(len(df_train['TransactionDT'])):\n    val=(df_train['TransactionDT'][i])/(3600)\n    encoded_hours = np.floor(val) % 24\n    new_col[i]=encoded_hours\n    \ndf_train['Hours_Col']=new_col\n\nnew_col = np.zeros(len(df_test['TransactionDT']), dtype = int)\nfor i in range(len(df_test['TransactionDT'])):\n    val=(df_test['TransactionDT'][590540+i])/3600\n    encoded_hours = np.floor(val) % 24\n    new_col[i]=encoded_hours\n    \ndf_test['Hours_Col']=new_col","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Setting X and y","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = df_train.sort_values('TransactionDT').drop(['isFraud', \n                                                      'TransactionDT', \n                                                      #'Card_ID'\n                                                     ],\n                                                     axis=1)\ny_train = df_train.sort_values('TransactionDT')['isFraud'].astype(bool)\n\nX_test = df_test.sort_values('TransactionDT').drop(['TransactionDT',\n                                                    #'Card_ID'\n                                                   ], \n                                                   axis=1)\ndel df_train\ndf_test = df_test[[\"TransactionDT\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train=X_train.drop(columns=['Trans_min_mean','Trans_min_std'])\nX_test=X_test.drop(columns=['Trans_min_mean','Trans_min_std'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"card_cols = [c for c in X_train.columns if 'card' in c]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train[card_cols].corr(method ='pearson')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#TransactionAmt_to_mean_card4 & card1 has 0.8618\n#TransactionAmt_to_std_card4 & mean_card1 has 0.8498\n#TransactionAmt_to_std_card4 & mean_card4 has 0.9871\n#Remove mean_1 and std_4\nX_train=X_train.drop(['TransactionAmt_to_mean_card1','TransactionAmt_to_std_card4'],axis=1)\nX_test=X_test.drop(['TransactionAmt_to_mean_card1','TransactionAmt_to_std_card4'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c_cols = [c for c in X_train if c[0] == 'C']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train[c_cols].corr(method ='pearson')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#C1, C2, C4, C6, C8, C10, C11\n#Remove C2,4,6,8,10,11\nX_train=X_train.drop(['C2','C4','C6','C8','C10','C11'], axis=1)\nX_test=X_test.drop(['C2','C4','C6','C8','C10','C11'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d_cols = [c for c in X_train if c[0] == 'D']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(1, 16):\n    print(X_train['D' + str(i)].value_counts(dropna=False, normalize=True).head())\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Nans with >80% dropped\n#D6,7,8,9,12,13,14\nX_train=X_train.drop(['D6','D7','D8','D9','D12','D13','D14'], axis=1)\nX_test=X_test.drop(['D6','D7','D8','D9','D12','D13','D14'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d_cols = [c for c in X_train if c[0] == 'D']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train[d_cols].corr(method ='pearson')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Corelated columns D2\nX_train=X_train.drop(['D2'], axis=1)\nX_test=X_test.drop(['D2'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m_cols = [c for c in X_train if c[0] == 'M']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train[m_cols].corr(method ='pearson')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Corelated columns M2,3,8,9\nX_train=X_train.drop(['M2','M3','M8','M9'], axis=1)\nX_test=X_test.drop(['M2','M3','M8','M9'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"id_cols = [c for c in X_train if c[0] == 'i']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(1, 10):\n    print(X_train['id_0' + str(i)].value_counts(dropna=False, normalize=True).head())\n    print('\\n')\n    \nfor i in range(10, 39):\n    print(X_train['id_' + str(i)].value_counts(dropna=False, normalize=True).head())\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Nans with >80% dropped\n#id_3,4,7,8,9,10,14,18,21,22,24,25,26,32\nX_train=X_train.drop(['id_03','id_04','id_07','id_08','id_09','id_10','id_14','id_18','id_21','id_22','id_24','id_25','id_26','id_32'], axis=1)\nX_test=X_test.drop(['id_03','id_04','id_07','id_08','id_09','id_10','id_14','id_18','id_21','id_22','id_24','id_25','id_26','id_32'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Corelation with >85% dropped\n#id_15,28,36,38\nX_train=X_train.drop(['id_15','id_28','id_36','id_38'], axis=1)\nX_test=X_test.drop(['id_15','id_28','id_36','id_38'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in X_test.columns :\n    print (i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train=X_train.drop(['weekday','hours'], axis=1)\nX_test=X_test.drop(['weekday','hours'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_train=X_train.fillna(X_train.median(axis = 0, skipna = True), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predicting X test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#XG Boost on df\nfrom xgboost import XGBClassifier\ngg = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, gamma=0,\n              learning_rate=0.1, max_delta_step=0, max_depth=2,\n              min_child_weight=1, missing=None, n_estimators=70, n_jobs=-1,\n              nthread=None, objective='binary:logistic', random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n              silent=None, subsample=1, verbosity=1)\ngg.fit(X_train, y_train)\ny_xgb = gg.predict_proba(X_test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outcome = pd.DataFrame(y_xgb)\noutcome.to_csv('y_xgb.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Top 50 Feature importance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_important = gg.get_booster().get_score(importance_type=\"weight\")\nkeys = list(feature_important.keys())\nvalues = list(feature_important.values())\n\ndata = pd.DataFrame(data=values, index=keys, columns=[\"score\"]).sort_values(by = \"score\", ascending=False)\n\n# Top 10 features\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def select_columns(data_frame, column_names):\n    new_frame = data_frame.loc[:, column_names]\n    return new_frame","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_columns = ['C1', 'C14', 'D3','C5','PCA_V_28','R_emaildomain_bin','PCA_V_12','C7','C12','TransactionAmt','PCA_V_8','PCA_V_4','PCA_V_6','TransactionID','card6','M5','D5'\n                   ,'PCA_V_17','C13','D15','PCA_V_14','PCA_V_5','card3','PCA_V_13','PCA_V_9','PCA_V_1','M4','D10','PCA_V_26','PCA_V_0','ProductCD','D1','M6','PCA_V_29','PCA_V_15','PCA_V_7','addr2','P_emaildomain','Day_Col','Hours_Col']\nX_tr = select_columns(X_train, selected_columns)\nX_tt = select_columns(X_test, selected_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Neural Net\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\nclassifier=Sequential()\nclassifier.add(Dense(output_dim=20,init='uniform', activation='relu', input_dim=40))\nclassifier.add(Dense(output_dim=1, init='uniform', activation='sigmoid'))\nclassifier.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])              \nclassifier.fit(X_tr, y_train, validation_split=0.2, batch_size=25, epochs=15, shuffle=True, verbose=2)\ny_ANN=classifier.predict(X_tt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outcome = pd.DataFrame(y_ANN)\noutcome.to_csv('y_ANN.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Light GBM on df\nlgb_train = lgb.Dataset(X_tr, y_train)\nlgb_eval = lgb.Dataset(X_tt, reference=lgb_train)\n\nparams = {'objective': 'binary','feature_fraction': 1,'bagging_fraction': 1,'verbose': -1}\n\ngbm = lgb.train(params,lgb_train,num_boost_round=20)\ny_gbm_50 = gbm.predict(X_tt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outcome = pd.DataFrame(y_gbm_50)\noutcome.to_csv('y_gbm_50.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}