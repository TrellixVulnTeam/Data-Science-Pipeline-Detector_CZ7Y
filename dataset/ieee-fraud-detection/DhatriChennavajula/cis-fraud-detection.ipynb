{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_identity = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/train_identity.csv\")\ntrain_transaction = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/train_transaction.csv\")\ntest_identity = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/test_identity.csv\")\ntest_transaction = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/test_transaction.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_identity.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_identity.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_identity.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_transaction.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_identity.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Check how many fraud cases are there**"},{"metadata":{"trusted":true},"cell_type":"code","source":"fc = train_transaction['isFraud'].value_counts(normalize=True).to_frame()\nfc.plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* there is a class imbalace problem."},{"metadata":{},"cell_type":"markdown","source":"**Merge the transaction and identity data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train=train_transaction.merge(train_identity,how='left',left_index=True,right_index=True)\ny_train=train['isFraud'].astype('uint8')\nprint('Train shape',train.shape)\ndel train_transaction,train_identity\nprint(\"Data set merged \")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Merge the test data **"},{"metadata":{"trusted":true},"cell_type":"code","source":"test=test_transaction.merge(test_identity,how='left',left_index=True,right_index=True)\nprint('Test shape',test.shape)\ndel test_transaction,test_identity\nprint(\"Test Data set merged \")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head(3)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# From kernel https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n# WARNING! THIS CAN DAMAGE THE DATA \ndef reduce_mem_usage2(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain = reduce_mem_usage2(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntest = reduce_mem_usage2(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Imbalance**\n\nOne of the techniques to overcome imbalance is to use \"Oversampling minority clas\"\n\nOversampling is defined as adding more copies of the minority class. Oversampling can be a good choice when you donâ€™t have a ton of data to work with.\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"not_fraud=train[train.isFraud==0]\nfraud=train[train.isFraud==1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import resample\n\n# upsample minority\nfraud_upsampled = resample(fraud,\n                          replace=True, # sample with replacement\n                          n_samples=len(not_fraud), # match number in majority class\n                          random_state=27) # reproducible results\n\n# combine majority and upsampled minority\nX = pd.concat([not_fraud, fraud_upsampled])\n\n# check new class counts\nX.isFraud.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = X['isFraud']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X = X.drop(['isFraud'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Convert TransactionDT to day time**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X[\"TransactionDay\"] = X[\"TransactionDT\"] // (24*60*60)\nX[\"TransactionWeek\"] = X[\"TransactionDay\"] // 7","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](http://)![](http://)**Analyze fraud on ProductCD**"},{"metadata":{"trusted":true},"cell_type":"code","source":"prod = list(set(X[\"ProductCD\"]))\nprint(prod)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.groupby(\"ProductCD\")['isFraud'].value_counts().to_frame().plot.bar()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"more number of frauds are for product **\"C\" and \"W\"**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X.groupby(['card4', 'card3'])['isFraud'].value_counts().to_frame().plot.bar()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.groupby(['card4'])['isFraud'].value_counts().to_frame().plot.bar()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Most number of frauds are for**\n* Visa\n* Mastercard"},{"metadata":{"trusted":true},"cell_type":"code","source":"X.groupby(['card4', 'ProductCD'])['isFraud'].value_counts().to_frame().plot.bar()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" **Most number of fraud cases are seen for **\n   * Visa, Master card for \n   * Product \"C\" and \"W\" "},{"metadata":{},"cell_type":"markdown","source":"**Analyze for Fraud Transaction amount**"},{"metadata":{"trusted":true},"cell_type":"code","source":"fraud = X[X['isFraud'] == 1]\nprint(\"max trans amount happend during fraud:\",max(fraud[\"TransactionAmt\"]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Min trans amount happend during fraud:\",min(fraud[\"TransactionAmt\"]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fraud[fraud[\"TransactionAmt\"] == 0.292]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Analysis on Address**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X['addr1']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X['addr2']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fraud.groupby(['addr1', 'addr2'])['isFraud'].value_counts().to_frame().plot.bar()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" **Columns with null values**\n \n Drop the columns which has more than 50% of null values"},{"metadata":{"trusted":true},"cell_type":"code","source":"null_percent = train.isnull().sum()/train.shape[0]*100\n\ncols_to_drop = np.array(null_percent[null_percent > 50].index)\n\ncols_to_drop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X.drop(cols_to_drop, axis=1)\ntest = test.drop(cols_to_drop,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.columns\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"null_percent = test.isnull().sum()/X.shape[0]*100\nnull_percent[null_percent > 0]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_drop_again = np.array(null_percent[null_percent > 0.001].index)\n\ncols_to_drop_again","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X.drop(cols_to_drop_again, axis=1)\ntest = test.drop(cols_to_drop_again,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(X.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Replace the nan with mode values"},{"metadata":{},"cell_type":"markdown","source":"**Get more visualizations for email domanin, transaction amt,  **"},{"metadata":{},"cell_type":"markdown","source":"** looking at the distribution of Transaction amount\""},{"metadata":{"trusted":true},"cell_type":"code","source":" sns.distplot(a=X[\"TransactionAmt\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is exponential distribution , doing log normal on it"},{"metadata":{"trusted":true},"cell_type":"code","source":"log_trans = X['TransactionAmt'].apply(np.log)\nsns.distplot(a=log_trans)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Applying log , Close to normal distribution, apply it to the column"},{"metadata":{"trusted":true},"cell_type":"code","source":"X['TransactionAmt'] = X['TransactionAmt'].apply(np.log)\ntest['TransactionAmt'] = test['TransactionAmt'].apply(np.log)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_drop = ['TransactionDT','TransactionID_x']\nX = X.drop(cols_to_drop, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Find the categorical and numerical data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_data = X.select_dtypes(include='object')\nnum_data = X.select_dtypes(exclude='object')\n\ncat_cols = cat_data.columns.values\nnum_cols = num_data.columns.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"List of Categorical data"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Label encode the categorical variables**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom tqdm import tqdm\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in tqdm(cat_cols): \n    label = LabelEncoder()\n    label.fit(list(X[i].values)+list(test[i].values))\n    X[i] = label.transform(list(X[i].values))\n    test[i] = label.transform(list(test[i].values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Find the correlaiton between variables**"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = X.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\nsns.heatmap(corr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_corr = set()\nfor i in range(len(corr.columns)):\n    for j in range(i):\n        if (corr.iloc[i, j] >= 0.95) and (corr.columns[j] not in col_corr):\n            colname = corr.columns[i] # getting the name of column\n            col_corr.add(colname)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_corr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Get rid of highly correlated columns "},{"metadata":{"trusted":true},"cell_type":"code","source":"final_columns = []\n\ncols = X.columns\n\nfor i in cols:\n    if i in col_corr:\n        continue\n    else:\n        final_columns.append(i)\n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> **Take the coefficients with less correlation in to new data frame**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_final = X[final_columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_final = test[final_columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_final.shape)\nprint(test_final.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler.fit(X_final)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfX = scaler.transform(X_final)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_name = X_final.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfX = pd.DataFrame(dfX, columns=feature_name)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential, Model\nfrom keras.layers import Dense, BatchNormalization, Dropout, Flatten, Input\nfrom keras import backend as K\nimport keras\n\nn_features = dfX.shape[1]\ndim = 15\n\ndef build_model(dropout_rate=0.15, activation='tanh'):\n    main_input = Input(shape=(n_features, ), name='main_input')\n    \n    x = Dense(dim*2, activation=activation)(main_input)\n    x = BatchNormalization()(x)\n    x = Dropout(dropout_rate)(x)\n    \n    x = Dense(dim*2, activation=activation)(x)\n    x = BatchNormalization()(x)\n    x = Dropout(dropout_rate/2)(x)\n    \n    x = Dense(dim, activation=activation)(x)\n    x = Dropout(dropout_rate/4)(x)\n\n    encoded = Dense(2, activation='tanh')(x)\n\n    input_encoded = Input(shape=(2, ))\n    \n    x = Dense(dim, activation=activation)(input_encoded)\n    x = Dense(dim, activation=activation)(x)\n    x = Dense(dim*2, activation=activation)(x)\n    \n    decoded = x = Dense(n_features, activation='linear')(x)\n\n    encoder = Model(main_input, encoded, name=\"encoder\")\n    decoder = Model(input_encoded, decoded, name=\"decoder\")\n    autoencoder = Model(main_input, decoder(encoder(main_input)), name=\"autoencoder\")\n    return encoder, decoder, autoencoder\n\nK.clear_session()\nc_encoder, c_decoder, c_autoencoder = build_model()\nc_autoencoder.compile(optimizer='nadam', loss='mse')\n\nc_autoencoder.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nepochs = 50\nbatch_size = 9548\nhistory = c_autoencoder.fit(dfX, y,\n                    epochs=epochs,\n                    batch_size=batch_size,\n                    shuffle=True,\n                    verbose=1)\n\nloss_history = history.history['loss']\nplt.figure(figsize=(10, 5))\nplt.plot(loss_history);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'num_leaves': 491,\n          'min_child_weight': 0.03454472573214212,\n          'feature_fraction': 0.3797454081646243,\n          'bagging_fraction': 0.4181193142567742,\n          'min_data_in_leaf': 106,\n          'objective': 'binary',\n          'max_depth': -1,\n          'learning_rate': 0.006883242363721497,\n          \"boosting_type\": \"gbdt\",\n          \"bagging_seed\": 11,\n          \"metric\": 'auc',\n          \"verbosity\": -1,\n          'reg_alpha': 0.3899927210061127,\n          'reg_lambda': 0.6485237330340494,\n          'random_state': 47,\n         }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import confusion_matrix, roc_auc_score ,roc_curve,auc\nfrom sklearn.model_selection import StratifiedKFold\nimport lightgbm as lgb\nseed = 123\nkf = StratifiedKFold(n_splits=5,shuffle=True,random_state=seed)\npred_test_full =0\ncv_score =[]\ni=1\npredictions = np.zeros(test_final.shape[0])\nprint('5 Fold Stratified Cross Validation')\nprint('-----------------------------------')\nfor train_index,test_index in kf.split(X_final,y):\n    print('{} of KFold {}'.format(i,kf.n_splits))\n    xtr,xv = X_final.loc[train_index],X_final.loc[test_index]\n    ytr,yv = y.loc[train_index],y.loc[test_index]\n    \n    #model\n    #clf = LogisticRegression(C=2)\n    #clf.fit(xtr,ytr)\n    #clf = CatBoostClassifier(task_type='GPU', eval_metric='AUC', loss_function='Logloss', use_best_model=True,\n    #                      silent=True, class_weights=[0.05, 0.95],\n    #                     random_state=42, iterations=5000, od_type='Iter', od_wait=200, grow_policy='Lossguide', max_depth=8)\n    dtrain = lgb.Dataset(xtr, label=ytr)\n    dvalid = lgb.Dataset(xv, label=yv)\n\n    clf = lgb.train(params, dtrain, 10000, valid_sets = [dtrain, dvalid], verbose_eval=200, early_stopping_rounds=500)\n    \n    y_pred_valid = clf.predict(xv)\n    score = roc_auc_score(yv,clf.predict(xv))\n    print('ROC AUC score:',score)\n    cv_score.append(score)    \n    pred_test = clf.predict(test_final)/5\n    pred_test_full +=pred_test\n    i+=1\n    print('-------------------------------------')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Mean AUC Score for CatBoost : {}'.format(np.array(score).mean()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('/kaggle/input/ieee-fraud-detection/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['isFraud'] = pred_test_full","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}