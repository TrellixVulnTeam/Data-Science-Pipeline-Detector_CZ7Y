{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this competition a lot has been said and written about feature engineering...and the likely winner and top scores will definitely be achieved by heavy feature engineering.\n\nBut curious to see how far we can get with a minimum of feature engineering. So I created this notebook with just some basic preparation, feature selection, count encoding and NaN elimination\n\nI'll use a simple 1D Convolutional Neural Network to showcase whats possible."},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport gc\nimport time\nimport random\nimport tensorflow as tf\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import preprocessing\nfrom sklearn.impute import SimpleImputer\nfrom keras import backend as K\nfrom keras.models import Sequential, load_model\nfrom keras.optimizers import Adam, Nadam\nfrom keras.initializers import glorot_uniform, lecun_uniform\nfrom keras.layers import Dense, Conv1D, Flatten, MaxPool1D, Dropout, Activation, BatchNormalization\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\n\n# Random Seed\nseed = 12345\nnp.random.seed(seed)\nrandom.seed(seed)\n\n# Constants\nepochs = 25\nbatch_size = 1024\nnumber_of_folds = 6\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next I'll define 2 lists. One with categorical features and one with 'low-score' features. I made a simple LightGBM setup in which I determined the AUC score per feature. This usually gives a nice impression. The list below is a list of the lowest scoring ones. Very quick and dirty determined but I used this already in multiple competitions and usually it works quitte well."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Categorical features\ncat_feats = ['ProductCD', 'addr1', 'addr2', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', \n                        'P_emaildomain', 'R_emaildomain', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']\n\n# Low score Features\nlowscore_feats =   ['V322','V329','V321','V336','V331','V335','V330','V332','V328','V338','V327','V137','V333','V326','V116','V339','V337',\n                    'V334','V114','V115','V163','V298','V162','V142','V141','V325','V129','V138','V161','V100','V296','V112','V105',\n                    'V113','V111','V106','V299','V98','V110','V301','V108','V135','V109','V319','V104','V300','V297','V119','V311',\n                    'V117','V41','V118','V121','V122','V286','V120','V107','V305']\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we load the data and immediately drop the columns we don't need. Alternatively we could create a list of just the columns that we want to load. I prefer the first method as I can now easily modify my list with lowscore_feats."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load Data\ntrain = pd.read_csv('../input/ieee-fraud-detection/train_transaction.csv')\nlabels = train['isFraud']\ntest = pd.read_csv('../input/ieee-fraud-detection/test_transaction.csv')\n\n# Drop Columns\ntest.drop(lowscore_feats, axis = 1, inplace = True)\ntrain.drop(lowscore_feats, axis = 1, inplace = True)\ntrain.drop(['isFraud'], axis=1, inplace = True)\n\n# Summary Shapes\nprint('====== Dataset Shapes')\nprint('Train Transaction: {0}'.format(train.shape))\nprint('Test Transaction: {0}'.format(test.shape))\nprint('Labels: {0}'.format(labels.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'll append the train and test set. Only for the features with less than 10K of missing values will the missing values be replaced by the mean value."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Append Train and Test Datasets\ntrain_len = len(train)\ndf = train.append(test).reset_index()\n\n# Cleanup\ndel train, test\ngc.collect()\n\n# Impute Mean value for features that have less than 10K NaN values\nprocessed_feats = []\nfor feat in [f for f in df.columns if f not in ['index', 'TransactionID', 'TransactionDT'] + cat_feats + lowscore_feats]:\n    if df[feat].isna().sum() < 10000:\n        imputer = SimpleImputer(strategy = 'mean')\n        df[feat] = imputer.fit_transform(df[feat].values.reshape(-1, 1))\n        processed_feats.append(feat)       \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we process TransactionDT into new features 'hour' and 'weekday'."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Process TransactionDT into hour and weekday\ndf['hour'] = df['TransactionDT'].map(lambda x:(x // 3600) % 24)\ndf['weekday'] = df['TransactionDT'].map(lambda x:(x // (3600 * 24)) % 7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All remaining features will be encoded with their value counts. The nan values are included in this to make sure that we don't have any missing values when running the neural net."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Count encode all categorical features\nfor feat in cat_feats:\n    df[feat] = df[feat].map(df[feat].value_counts(dropna = False))\n\n# Count encode all other remaining 'Numerical' Features\nfor feat in [f for f in df.columns if f not in ['index', 'TransactionID', 'TransactionDT'] + cat_feats + processed_feats]:\n    df[feat] = df[feat].map(df[feat].value_counts(dropna = False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As a last processing step I use a MinMaxScaler to scale all the features. For features with a large skew value this will also be corrected."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Get Final Features\nfeats = [f for f in df.columns if f not in ['index', 'TransactionID', 'TransactionDT'] + lowscore_feats]\n\n# Scale and correct extreme skew\nscaler = preprocessing.MinMaxScaler()\nfor feat in feats:\n    # Scale\n    df[feat] = scaler.fit_transform(df[feat].values.reshape(-1, 1))\n    \n    # Correct Skew\n    if df[feat].skew() > 10:\n        df[feat] = np.log10(df[feat] + 1 - min(0, df[feat].min()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Split back to train and test dataset  \ntrain = df[:train_len]\ntest = df[train_len:]\n\n# Final Summary Shapes\nprint('====== Final Dataset Shapes')\nprint('Train Transaction: {0}'.format(train[feats].shape))\nprint('Test Transaction: {0}'.format(test[feats].shape))\nprint('Labels: {0}'.format(labels.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define the EarlyStopping, ModelCheckPoint and the model itself."},{"metadata":{"trusted":false},"cell_type":"code","source":"def EarlyStop(patience):\n    return EarlyStopping(monitor = \"val_loss\",\n                          min_delta = 0,\n                          mode = \"min\",\n                          verbose = 1, \n                          patience = patience)\n\ndef ModelCheckpointFull(model_name):\n    return ModelCheckpoint(model_name, \n                            monitor = 'val_loss', \n                            verbose = 1, \n                            save_best_only = True, \n                            save_weights_only = False, \n                            mode = 'min', \n                            period = 1)\n\n# Input Shape\ninput_shape = train[feats].shape[1]\n\n# Define CNN 1D model\ndef create_model():\n    model = Sequential()\n    model.add(Conv1D(96, 2, activation = 'relu', input_shape=(input_shape, 1), kernel_initializer = glorot_uniform(seed = seed)))\n    model.add(BatchNormalization())       \n    model.add(Dropout(0.25))\n    model.add(Conv1D(96, 1, activation = 'relu', kernel_initializer = glorot_uniform(seed = seed)))\n    model.add(BatchNormalization())       \n    model.add(Flatten())\n    model.add(Dropout(0.25))    \n    model.add(Dense(96, activation = 'relu', kernel_initializer = glorot_uniform(seed = seed)))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.25))\n    model.add(Dense(1, activation = 'sigmoid', kernel_initializer = glorot_uniform(seed = seed)))\n    model.compile(loss = 'binary_crossentropy', optimizer = Adam(lr = 0.001), metrics = ['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As a final step we run a Stratified KFold and generate the submission file. After some trials I found that about 25 epochs is a good value to use. When using more the validation AUC score will increase but the LB score will start to drop slightly."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Reshape\ntrain = train[feats].values.reshape(-1, input_shape, 1)\ntest = test[feats].values.reshape(-1, input_shape, 1)\n\n# CV Folds\nfolds = StratifiedKFold(n_splits = number_of_folds, shuffle = True, random_state = seed)\n\n# Arrays to store predictions\noof_preds = np.zeros(train.shape[0])\nsub_preds = np.zeros(test.shape[0])\n\n# Loop through folds\nfor n_fold, (train_idx, valid_idx) in enumerate(folds.split(train, labels)):\n    train_x, train_y = train[train_idx], labels.iloc[train_idx]\n    valid_x, valid_y = train[valid_idx], labels.iloc[valid_idx]\n\n    print('Running Fold: ' + str(n_fold))\n\n    # CNN 1D model\n    model = create_model()\n    model.fit(train_x, train_y, \n                validation_data=(valid_x, valid_y), \n                epochs=epochs, \n                batch_size=batch_size, \n                verbose=0,\n                callbacks=[EarlyStop(10), ModelCheckpointFull('model.h5')])\n\n    # Delete Model\n    del model\n    gc.collect()\n\n    # Reload Best Saved Model\n    model = load_model('model.h5')\n\n    # OOF Predictions\n    oof_preds[valid_idx] = model.predict(valid_x).reshape(-1,)\n    \n    # Submission Predictions\n    predictions = model.predict(test).reshape(-1,)\n    sub_preds += predictions / number_of_folds\n\n    # Fold AUC Score\n    print('Fold %2d AUC : %.6f' % (n_fold, roc_auc_score(valid_y, oof_preds[valid_idx])))        \n\n    # Cleanup \n    del model, train_x, train_y, valid_y, valid_x\n    K.clear_session()\n    gc.collect\n\nprint('Full AUC score %.6f' % roc_auc_score(labels, oof_preds))\n\n# Generate Submission\nsubmission = pd.read_csv('../input/ieee-fraud-detection/sample_submission.csv')\nsubmission['isFraud'] = sub_preds\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I hope you enjoyed the notebook. Let me know if you have any feedback or questions."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}