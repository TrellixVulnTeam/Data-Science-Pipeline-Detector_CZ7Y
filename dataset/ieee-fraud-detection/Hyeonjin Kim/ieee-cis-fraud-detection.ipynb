{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Load Packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport scipy as sp\nfrom scipy import stats\nimport seaborn as sns\n\nimport gc, os, sys, re, time\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\nfrom collections import Counter, defaultdict\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load CSV data"},{"metadata":{"trusted":true},"cell_type":"code","source":"path='../input/ieee-fraud-detection'\nos.listdir(path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_identity=pd.read_csv(os.path.join(path,'train_identity.csv'))\ntrain_transaction=pd.read_csv(os.path.join(path,'train_transaction.csv'))\nprint('Training Dataset is loaded')\n\ntest_identity=pd.read_csv(os.path.join(path,'test_identity.csv'))\ntest_transaction=pd.read_csv(os.path.join(path,'test_transaction.csv'))\nprint('Test Dataset is loaded')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking the target class"},{"metadata":{},"cell_type":"markdown","source":"Transaction Table<br>\nCategorical Features:\n* ProductCD\n* card1 - card6\n* addr1, addr2\n* P_emaildomain\n* R_emaildomain\n* M1 - M9\n\nIdentity Table<br>\nCategorical Features:\n* DeviceType\n* DeviceInfo\n* id_12 - id_38\n\n[reference discussion](https://www.kaggle.com/c/ieee-fraud-detection/discussion/101203)"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = train_transaction['isFraud'].value_counts().values\n\nax = sns.barplot([0,1],x)\nax.set(title='Class distribution w.rt target variables', xlabel = 'Target Class Count', ylabel='Count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This graph shows the distribution of the target class, Fraud."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge data (transaction and identity)\ntrain = train_transaction.merge(train_identity, how='left',left_index=True, right_index=True)\ny_train = train['isFraud'].astype('uint8')\nprint('Train shape = ', train.shape)\n\n# Delete train identity and transaction for system memory\ndel train_identity, train_transaction\nprint(train.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Also Merge Test data\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n\ndel test_identity, test_transaction","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### For preventing from memory break\nTo reduce memory usage, int value goes to int8 <br>\nfloat value goes to float16<br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_memory_usage(df):\n    start_memory = df.memory_usage().sum() / 1024**2\n    print('Memory Usage in beginning is {:.2f} MB'.format(start_memory))\n    column = df.columns\n    for col in column:\n        coltype = df[col].dtype\n        if coltype != 'object':\n            cmin = df[col].min()\n            cmax = df[col].max()\n            if str(coltype)[:3] == 'int':\n                if cmin>np.iinfo(np.int8).min and cmax < np.iinfo(np.int8).max:\n                    df[col]=df[col].astype(np.int8)\n                elif cmin>np.iinfo(np.int16).min and cmax < np.iinfo(np.int16).max:\n                    df[col]=df[col].astype(np.int16)\n                elif cmin>np.iinfo(np.int32).min and cmax < np.iinfo(np.int32).max:\n                    df[col]=df[col].astype(np.int32)\n                elif cmin>np.iinfo(np.int64).min and cmax < np.iinfo(np.int64).max:\n                    df[col]=df[col].astype(np.int64)  \n            else:\n                if cmin>np.finfo(np.float16).min and cmax < np.finfo(np.float16).max:\n                    df[col]=df[col].astype(np.float16)\n                elif cmin>np.finfo(np.float32).min and cmax < np.finfo(np.float32).max:\n                    df[col]=df[col].astype(np.float32)\n                else:\n                    df[col]=df[col].astype(np.float64)\n        else:\n            df[col]=df[col].astype('category')\n    end_memory=df.memory_usage().sum() / 1024**2\n    diff= start_memory - end_memory\n    print('The memory now is {} MB'.format(end_memory))\n    print('Memory is reduced to a tune of {:.2f}%'.format(100*(diff/start_memory)))\n    return df      ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=reduce_memory_usage(train)\ntest=reduce_memory_usage(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For reducing memory usage, we can save 72% of the memory."},{"metadata":{},"cell_type":"markdown","source":"### Match ID section\nThe ID value has two methods to be represented, id_17, id:17. <br>\nTo match both, it makes finding different column function."},{"metadata":{"trusted":true},"cell_type":"code","source":"def differentcols(df_train,df_test):\n    for i in df_train:\n        if i not in df_test:\n            print(i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 여러번 일일이 써서 맞추는게 싫어서 짠 코든데 안돌아간다...왜지"},{"metadata":{"trusted":true},"cell_type":"code","source":"def differentcols2(df_train, df_test):\n    for i in df_train:\n        if i not in df_test:\n            if i != 'isFraud':\n                print(i)\n                i = i[:2]+\"_\"+i[3:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"differentcols(train,test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test=test.rename(columns={'id-01':'id_01','id-02':'id_02','id-03':'id_03','id-04':'id_04','id-05':'id_05',\n                         'id-06':'id_06','id-07':'id_07','id-08':'id_08','id-09':'id_09',\n                         'id-10':'id_10','id-11':'id_11','id-12':'id_12','id-13':'id_13','id-14':'id_14',\n                         'id-15':'id_15','id-16':'id_16','id-17':'id_17','id-18':'id_18','id-19':'id_19',\n                         'id-20':'id_20','id-21':'id_21','id-22':'id_22','id-23':'id_23','id-24':'id_24',\n                         'id-25':'id_25','id-26':'id_26','id-27':'id_27','id-28':'id_28','id-29':'id_29',\n                         'id-30':'id_30','id-31':'id_31','id-32':'id_32','id-33':'id_33','id-34':'id_34',\n                         'id-35':'id_35','id-36':'id_36','id-37':'id_37','id-38':'id_38'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_id=test['TransactionID_x']\ndifferentcols(train,test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Only isFraud is remained."},{"metadata":{},"cell_type":"markdown","source":"### Missing Value"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_missing_values(data):\n    total = data.isnull().sum().sort_values(ascending=False)\n    percent = (data.isnull().sum() / data.isnull().count()).sort_values(ascending=False)\n    missing_data = pd.concat([total, percent], axis=1, keys=['total', 'percent'])\n    return missing_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_data_train = get_missing_values(train)\nmissing_data_train.head(100).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_data_test=get_missing_values(test)\nmissing_data_test.head(100).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop missing data\ndropped_cols=missing_data_train[missing_data_train['total']>100000].index\n\ntrain=train.drop(dropped_cols,axis=1)\ntest=test.drop(dropped_cols,axis=1)\n\ndropped_cols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most missing values are dropped out.<br>\nHowever, there are still a variety of missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.isnull().sum().sum())\nprint(test.isnull().sum().sum())\n\nprint('Train shape: ', train.shape)\nprint('Test shape: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_data_train=get_missing_values(train)\nmissing_data_test=get_missing_values(test)\ndropped_cols_train=missing_data_train[missing_data_train['percent']>0].index\ndropped_cols_test=missing_data_test[missing_data_test['percent']>0].index","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dropout useless features - email domain"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['P_emaildomain'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop('P_emaildomain',axis=1,inplace = True)\ntest.drop('P_emaildomain',axis=1,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_data_train=get_missing_values(train)\nmissing_data_test=get_missing_values(test)\nmissing_data_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Drop train and test data columns which have more than 15000 null data."},{"metadata":{"trusted":true},"cell_type":"code","source":"dropped_cols_train_1=missing_data_train[missing_data_train['total']>15000].index\ntrain.drop(dropped_cols_train_1,axis=1,inplace=True)\ntest.drop(dropped_cols_train_1,axis=1,inplace=True)\n\nprint('Train shape: ', train.shape)\nprint('Test shape: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Find missing values again after concatnate the train and test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"ntrain=train.shape[0] # Train shape\nntest=test.shape[0] # Test shape\n\nall_data = pd.concat([train, test], axis=0, sort=False)\nall_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data_cols=all_data.columns\nfor i in all_data_cols:\n    if all_data[i].dtype=='object':\n        all_data[i]=all_data[i].fillna(all_data[i].mode()[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_data=get_missing_values(all_data)\nmissing_data.head(100).T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Replace the variable starting with 'C' or 'V' with the mode value\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in all_data_cols:\n    if (i.startswith(\"C\") or (i.startswith(\"V\"))) and all_data[i].isnull().sum() > 0:\n        all_data[i]=all_data[i].fillna(all_data[i].mode()[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_data=get_missing_values(all_data)\nmissing_data.sort_values('total',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The variables which do not have any common names should be filled manually."},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data['card3']=all_data['card3'].fillna(all_data['card3'].mode()[0])\nall_data['D1']=all_data['D1'].fillna(all_data['D1'].mode()[0])\nall_data['card2']=all_data['card2'].fillna(all_data['card2'].mode()[0])\nall_data['card4']=all_data['card4'].fillna(all_data['card4'].mode()[0])\nall_data['card5']=all_data['card5'].fillna(all_data['card5'].mode()[0])\nall_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seperate train and test values using ntrain and ntest."},{"metadata":{"trusted":true},"cell_type":"code","source":"train=all_data[:ntrain]\ntest=all_data[ntrain:]\n\ntest.drop(['isFraud'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preview of train and test dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"ntrain=train.shape[0]\nntest=test.shape[0]\nprint('Train shape : ', ntrain)\nprint('Test shape : ', ntest)\nalldata=pd.concat([train,test],axis=0,sort=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Previous all data shape : ', alldata.shape)\nalldata=pd.get_dummies(alldata)\nprint('After all data shape : ', alldata.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=alldata[:ntrain]\ntest=alldata[ntrain:]\n\nprint('Train shape : ', train.shape)\nprint('Test shape : ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Drop useless feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"target = train['isFraud']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(['TransactionID_x'],axis=1,inplace=True)\ntest.drop(['TransactionID_x'],axis=1,inplace=True)\n\ntrain.drop(['isFraud'],axis=1,inplace=True)\ntest.drop(['isFraud'],axis=1,inplace=True)\n\n\nprint('Train shape : ', train.shape)\nprint('Test shape : ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Total number of feature becomes 120 from 122."},{"metadata":{},"cell_type":"markdown","source":"### Overview of Features\n\n[EDA Reference](https://www.kaggle.com/pavitrasprabhu/eda-on-fraud-detection-dataset-in-python)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set=alldata[:ntrain]\ntest_set=alldata[ntrain:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Cx_plot_train = (train.filter(regex=(\"^C.*\")))\n\ncorr_train = Cx_plot_train.corr(method=\"pearson\")\nfig, ax = plt.subplots(figsize=(7, 4))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(0, 250, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr_train, cmap=cmap, vmax=1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.title(\"Pairwise Correlation heatmap \\n(Cx variables: train set)\")\n\n## Correlation heatmap for test set\nCx_plot_test=(test.filter(regex=(\"^C.*\")))\n\ncorr_test = Cx_plot_test.corr(method=\"pearson\")\nfig, ax = plt.subplots(figsize=(7, 4))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(0, 145, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr_test, cmap=cmap, vmax=1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.title(\"Pairwise Correlation heatmap \\n(Cx variables: test set)\")\n\ndel Cx_plot_test\ndel Cx_plot_train\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"D_ features are delected since they have a lot of missing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"Dx_plot_train = (train.filter(regex=(\"^D.*\")))\n\ncorr_train = Dx_plot_train.corr(method=\"pearson\")\nfig, ax = plt.subplots(figsize=(7, 4))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(0, 250, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr_train, cmap=cmap, vmax=1, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.title(\"Pairwise Correlation heatmap \\n(Dx variables: train set)\")\n\n\n## Correlation heatmap for test set\nDx_plot_test=(test.filter(regex=(\"^D.*\")))\n\ncorr_test = Dx_plot_test.corr(method=\"pearson\")\nfig, ax = plt.subplots(figsize=(7, 4))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(0, 145, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr_test, cmap=cmap, vmax=1, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.title(\"Pairwise Correlation heatmap \\n(Dx variables: test set)\")\n\n\ndel Dx_plot_test\ndel Dx_plot_train\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train\nVxxx_plot_train = (train.filter(regex=(\"^V.*\")).reset_index())\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.impute import SimpleImputer\n\nVxxx_plot_train.fillna(-999,inplace=True)\npca = PCA(n_components=20)\npc = pca.fit_transform(Vxxx_plot_train)\nprint(pca.explained_variance_ratio_)\n\ndel Vxxx_plot_train\ngc.collect()\n\nPC_values = np.arange(pca.n_components_) + 1\nplt.plot(PC_values, pca.explained_variance_ratio_, 'ro-', linewidth=2)\nplt.title('Scree Plot')\nplt.xlabel('Principal Component')\nplt.ylabel('Proportion of Variance Explained')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test\nVxxx_plot_test = (test.filter(regex=(\"^V.*\")).reset_index())\n\nVxxx_plot_test.fillna(-999,inplace=True)\npca = PCA(n_components=20)\npc = pca.fit_transform(Vxxx_plot_test)\nprint(pca.explained_variance_ratio_)\n\ndel Vxxx_plot_test\ngc.collect()\n\nPC_values = np.arange(pca.n_components_) + 1\nplt.plot(PC_values, pca.explained_variance_ratio_, 'ro-', linewidth=2)\nplt.title('Scree Plot')\nplt.xlabel('Principal Component')\nplt.ylabel('Proportion of Variance Explained')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"card_plt = (train_set[['card1','card2','card3','card5','isFraud','TransactionAmt']]\n.melt(id_vars=['isFraud','TransactionAmt']))\n\n# Distribution of Transaction amount of Fraudulent transactions vs legitimate transactions grouped by card details\ng = sns.catplot(data=card_plt,\n                x=\"isFraud\", y=\"value\",kind=\"box\",\n                col = \"variable\",hue=\"isFraud\",sharey=False,sharex=False)\n## Drop 2 Variables\ncard_drop_var = ['card1','card2']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Build XGB model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nxgmodel = xgb.XGBClassifier()\nxgmodel.fit(train, target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred  = xgmodel.predict_proba(test)\nprint('Prediction shape : ', y_pred.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub=pd.DataFrame()\nsub['Transaction Id']=test_id\nsub['isFraud']=y_pred[:,1]\nsub.to_csv('Predictions.csv',index=False)\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Build LGBM model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit\nfrom sklearn import metrics\nfrom sklearn import linear_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_fold = 5\nfols = TimeSeriesSplit(n_splits=n_fold)\nfolds = KFold(n_splits=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from itertools import product\nfrom numba import jit\n\nimport altair as alt\nfrom altair.vega import v5\nfrom IPython.display import HTML\n\n# using ideas from this kernel: https://www.kaggle.com/notslush/altair-visualization-2018-stackoverflow-survey\ndef prepare_altair():\n    \"\"\"\n    Helper function to prepare altair for working.\n    \"\"\"\n\n    vega_url = 'https://cdn.jsdelivr.net/npm/vega@' + v5.SCHEMA_VERSION\n    vega_lib_url = 'https://cdn.jsdelivr.net/npm/vega-lib'\n    vega_lite_url = 'https://cdn.jsdelivr.net/npm/vega-lite@' + alt.SCHEMA_VERSION\n    vega_embed_url = 'https://cdn.jsdelivr.net/npm/vega-embed@3'\n    noext = \"?noext\"\n    \n    paths = {\n        'vega': vega_url + noext,\n        'vega-lib': vega_lib_url + noext,\n        'vega-lite': vega_lite_url + noext,\n        'vega-embed': vega_embed_url + noext\n    }\n    \n    workaround = f\"\"\"    requirejs.config({{\n        baseUrl: 'https://cdn.jsdelivr.net/npm/',\n        paths: {paths}\n    }});\n    \"\"\"\n    \n    return workaround\n    \n\ndef add_autoincrement(render_func):\n    # Keep track of unique <div/> IDs\n    cache = {}\n    def wrapped(chart, id=\"vega-chart\", autoincrement=True):\n        if autoincrement:\n            if id in cache:\n                counter = 1 + cache[id]\n                cache[id] = counter\n            else:\n                cache[id] = 0\n            actual_id = id if cache[id] == 0 else id + '-' + str(cache[id])\n        else:\n            if id not in cache:\n                cache[id] = 0\n            actual_id = id\n        return render_func(chart, id=actual_id)\n    # Cache will stay outside and \n    return wrapped\n           \n\n@add_autoincrement\ndef render(chart, id=\"vega-chart\"):\n    \"\"\"\n    Helper function to plot altair visualizations.\n    \"\"\"\n    chart_str = \"\"\"\n    <div id=\"{id}\"></div><script>\n    require([\"vega-embed\"], function(vg_embed) {{\n        const spec = {chart};     \n        vg_embed(\"#{id}\", spec, {{defaultStyle: true}}).catch(console.warn);\n        console.log(\"anything?\");\n    }});\n    console.log(\"really...anything?\");\n    </script>\n    \"\"\"\n    return HTML(\n        chart_str.format(\n            id=id,\n            chart=json.dumps(chart) if isinstance(chart, dict) else chart.to_json(indent=None)\n        )\n    )\n    \n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage(deep=True).sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                c_prec = df[col].apply(lambda x: np.finfo(x).precision).max()\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max and c_prec == np.finfo(np.float32).precision:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n    \n\n@jit\ndef fast_auc(y_true, y_prob):\n    \"\"\"\n    fast roc_auc computation: https://www.kaggle.com/c/microsoft-malware-prediction/discussion/76013\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_true = y_true[np.argsort(y_prob)]\n    nfalse = 0\n    auc = 0\n    n = len(y_true)\n    for i in range(n):\n        y_i = y_true[i]\n        nfalse += (1 - y_i)\n        auc += y_i * nfalse\n    auc /= (nfalse * (n - nfalse))\n    return auc\n\n\ndef eval_auc(y_true, y_pred):\n    \"\"\"\n    Fast auc eval function for lgb.\n    \"\"\"\n    return 'auc', fast_auc(y_true, y_pred), True\n\n\ndef group_mean_log_mae(y_true, y_pred, types, floor=1e-9):\n    \"\"\"\n    Fast metric computation for this competition: https://www.kaggle.com/c/champs-scalar-coupling\n    Code is from this kernel: https://www.kaggle.com/uberkinder/efficient-metric\n    \"\"\"\n    maes = (y_true-y_pred).abs().groupby(types).mean()\n    return np.log(maes.map(lambda x: max(x, floor))).mean()\n    \n\ndef train_model_regression(X, X_test, y, params, folds=None, model_type='lgb', eval_metric='mae', columns=None, plot_feature_importance=False, model=None,\n                               verbose=10000, early_stopping_rounds=200, n_estimators=50000, splits=None, n_folds=3):\n    \"\"\"\n    A function to train a variety of regression models.\n    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n    \n    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: y - target\n    :params: folds - folds to split data\n    :params: model_type - type of model to use\n    :params: eval_metric - metric to use\n    :params: columns - columns to use. If None - use all columns\n    :params: plot_feature_importance - whether to plot feature importance of LGB\n    :params: model - sklearn model, works only for \"sklearn\" model type\n    \n    \"\"\"\n    columns = X.columns if columns is None else columns\n    X_test = X_test[columns]\n    splits = folds.split(X) if splits is None else splits\n    n_splits = folds.n_splits if splits is None else n_folds\n    \n    # to set up scoring parameters\n    metrics_dict = {'mae': {'lgb_metric_name': 'mae',\n                        'catboost_metric_name': 'MAE',\n                        'sklearn_scoring_function': metrics.mean_absolute_error},\n                    'group_mae': {'lgb_metric_name': 'mae',\n                        'catboost_metric_name': 'MAE',\n                        'scoring_function': group_mean_log_mae},\n                    'mse': {'lgb_metric_name': 'mse',\n                        'catboost_metric_name': 'MSE',\n                        'sklearn_scoring_function': metrics.mean_squared_error}\n                    }\n\n    \n    result_dict = {}\n    \n    # out-of-fold predictions on train data\n    oof = np.zeros(len(X))\n    \n    # averaged predictions on train data\n    prediction = np.zeros(len(X_test))\n    \n    # list of scores on folds\n    scores = []\n    feature_importance = pd.DataFrame()\n    \n    # split and train on folds\n    for fold_n, (train_index, valid_index) in enumerate(splits):\n        if verbose:\n            print(f'Fold {fold_n + 1} started at {time.ctime()}')\n        if type(X) == np.ndarray:\n            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n            y_train, y_valid = y[train_index], y[valid_index]\n        else:\n            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n            \n        if model_type == 'lgb':\n            model = lgb.LGBMRegressor(**params, n_estimators = n_estimators, n_jobs = -1)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n            \n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=verbose, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n        \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            \n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n            print('')\n            \n            y_pred = model.predict(X_test).reshape(-1,)\n        \n        if model_type == 'cat':\n            model = CatBoostRegressor(iterations=20000,  eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n                                      loss_function=metrics_dict[eval_metric]['catboost_metric_name'])\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test)\n        \n        oof[valid_index] = y_pred_valid.reshape(-1,)\n        if eval_metric != 'group_mae':\n            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n        else:\n            scores.append(metrics_dict[eval_metric]['scoring_function'](y_valid, y_pred_valid, X_valid['type']))\n\n        prediction += y_pred    \n        \n        if model_type == 'lgb' and plot_feature_importance:\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction /= n_splits\n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    result_dict['oof'] = oof\n    result_dict['prediction'] = prediction\n    result_dict['scores'] = scores\n    \n    if model_type == 'lgb':\n        if plot_feature_importance:\n            feature_importance[\"importance\"] /= n_splits\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n            \n            result_dict['feature_importance'] = feature_importance\n        \n    return result_dict\n    \n\n\ndef train_model_classification(X, X_test, y, params, folds, model_type='lgb', eval_metric='auc', columns=None, plot_feature_importance=False, model=None,\n                               verbose=10000, early_stopping_rounds=200, n_estimators=50000, splits=None, n_folds=3, averaging='usual', n_jobs=-1):\n    \"\"\"\n    A function to train a variety of classification models.\n    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n    \n    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: y - target\n    :params: folds - folds to split data\n    :params: model_type - type of model to use\n    :params: eval_metric - metric to use\n    :params: columns - columns to use. If None - use all columns\n    :params: plot_feature_importance - whether to plot feature importance of LGB\n    :params: model - sklearn model, works only for \"sklearn\" model type\n    \n    \"\"\"\n    columns = X.columns if columns is None else columns\n    n_splits = folds.n_splits if splits is None else n_folds\n    X_test = X_test[columns]\n    \n    # to set up scoring parameters\n    metrics_dict = {'auc': {'lgb_metric_name': eval_auc,\n                        'catboost_metric_name': 'AUC',\n                        'sklearn_scoring_function': metrics.roc_auc_score},\n                    }\n    \n    result_dict = {}\n    if averaging == 'usual':\n        # out-of-fold predictions on train data\n        oof = np.zeros((len(X), 1))\n\n        # averaged predictions on train data\n        prediction = np.zeros((len(X_test), 1))\n        \n    elif averaging == 'rank':\n        # out-of-fold predictions on train data\n        oof = np.zeros((len(X), 1))\n\n        # averaged predictions on train data\n        prediction = np.zeros((len(X_test), 1))\n\n    \n    # list of scores on folds\n    scores = []\n    feature_importance = pd.DataFrame()\n    \n    # split and train on folds\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n        print(f'Fold {fold_n + 1} started at {time.ctime()}')\n        if type(X) == np.ndarray:\n            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n            y_train, y_valid = y[train_index], y[valid_index]\n        else:\n            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n            \n        if model_type == 'lgb':\n            model = lgb.LGBMClassifier(**params, n_estimators=n_estimators, n_jobs = n_jobs)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n            \n            y_pred_valid = model.predict_proba(X_valid)[:, 1]\n            y_pred = model.predict_proba(X_test, num_iteration=model.best_iteration_)[:, 1]\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=n_estimators, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=verbose, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n        \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            \n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n            print('')\n            \n            y_pred = model.predict_proba(X_test)\n        \n        if model_type == 'cat':\n            model = CatBoostClassifier(iterations=n_estimators, eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n                                      loss_function=Logloss)\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test)\n        \n        if averaging == 'usual':\n            \n            oof[valid_index] = y_pred_valid.reshape(-1, 1)\n            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n            \n            prediction += y_pred.reshape(-1, 1)\n\n        elif averaging == 'rank':\n                                  \n            oof[valid_index] = y_pred_valid.reshape(-1, 1)\n            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n                                  \n            prediction += pd.Series(y_pred).rank().values.reshape(-1, 1)        \n        \n        if model_type == 'lgb' and plot_feature_importance:\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction /= n_splits\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    result_dict['oof'] = oof\n    result_dict['prediction'] = prediction\n    result_dict['scores'] = scores\n    \n    if model_type == 'lgb':\n        if plot_feature_importance:\n            feature_importance[\"importance\"] /= n_splits\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n            \n            result_dict['feature_importance'] = feature_importance\n            result_dict['top_columns'] = cols\n        \n    return result_dict\n\n# setting up altair\nworkaround = prepare_altair()\nHTML(\"\".join((\n    \"<script>\",\n    workaround,\n    \"</script>\",\n)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'num_leaves': 256,\n          'min_child_samples': 79,\n          'objective': 'binary',\n          'max_depth': 13,\n          'learning_rate': 0.03,\n          \"boosting_type\": \"gbdt\",\n          \"subsample_freq\": 3,\n          \"subsample\": 0.9,\n          \"bagging_seed\": 11,\n          \"metric\": 'auc',\n          \"verbosity\": -1,\n          'reg_alpha': 0.3,\n          'reg_lambda': 0.3,\n          'colsample_bytree': 0.9,\n         }\n#result_dict_lgb = train_model_classification(X=train, X_test=test, y=y, params=params, folds=folds, model_type='lgb', eval_metric='auc', plot_feature_importance=True,\n#                                                      verbose=500, early_stopping_rounds=200, n_estimators=5000, averaging='usual', n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sub['isFraud'] = result_dict_lgb['prediction']\n#sub.to_csv('submission.csv', index=False)\n\n#pd.DataFrame(result_dict_lgb['oof']).to_csv('lgb_oof.csv', index=False)\n#sub.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}