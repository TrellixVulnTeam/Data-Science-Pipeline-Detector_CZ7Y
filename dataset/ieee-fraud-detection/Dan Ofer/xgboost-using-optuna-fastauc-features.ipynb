{"cells":[{"metadata":{},"cell_type":"markdown","source":"* XGBOost model hyperparam tuner\n* modified to use temporal split for CV. \n* Added 3 of the features from my features kernel, and datetime : \n    * https://www.kaggle.com/danofer/ieee-fraud-features-xgboost-0-934-lb\n    * https://www.kaggle.com/danofer/ieee-fraud-new-features-export-0-9359-lb\n    \n* Credit for the specific date (vs just 1.1.2018\" goes to : https://www.kaggle.com/kevinbonnes/transactiondt-starting-at-2017-12-01"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nimport xgboost as xgb\n# import lightgbm as lgb\nimport optuna\nimport functools\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve,auc,accuracy_score,confusion_matrix,f1_score\nimport datetime","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction = pd.read_csv('../input/train_transaction.csv', index_col='TransactionID') # ,nrows=12345\ntest_transaction = pd.read_csv('../input/test_transaction.csv', index_col='TransactionID')\n\ntrain_identity = pd.read_csv('../input/train_identity.csv', index_col='TransactionID')\ntest_identity = pd.read_csv('../input/test_identity.csv', index_col='TransactionID')\n\nsample_submission = pd.read_csv('../input/sample_submission.csv', index_col='TransactionID')\n\ntrain = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n\nprint(train.shape)\nprint(test.shape)\n\ny_train = train['isFraud'].copy()\ndel train_transaction, train_identity, test_transaction, test_identity\n\n# Drop target, fill in NaNs\nX_train = train.drop('isFraud', axis=1)\nX_test = test.copy()\n\ndel train, test\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fraud_datetime(df):\n    \"\"\"\n    Credit for picking 31.12\n    \"\"\"\n    START_DATE = '2017-12-01'\n    startdate = datetime.datetime.strptime(START_DATE, '%Y-%m-%d')\n    df['TransactionDT'] = df['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\n\n    # df['month'] = df['TransactionDT'].dt.month\n    df['dow'] = df['TransactionDT'].dt.dayofweek\n    df['hour'] = df['TransactionDT'].dt.hour\n#     df['day'] = df['TransactionDT'].dt.day\n    df.drop(['TransactionDT'],axis=1,inplace=True)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nX_train[\"null_counts\"] = X_train.isna().sum(axis=1)\nX_test[\"null_counts\"] = X_test.isna().sum(axis=1)\n\n\n# nunique appears unstable\n# currently trying wit just numeric cols, may use less mem\nX_train[\"nuniques\"] = X_train.select_dtypes(include=[np.number]).nunique(axis=1)\nX_test[\"nuniques\"] = X_test.select_dtypes(include=[np.number]).nunique(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## note: we also drop the TransactionDT here\nX_train = fraud_datetime(X_train)\nX_test = fraud_datetime(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## probably better to impute NANS, as the distrib changes between train and test\n\n# X_train = X_train.fillna(-999)\n# X_test = X_test.fillna(-999)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Label Encoding\nfor f in X_train.columns:\n    if X_train[f].dtype=='object' or X_test[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(X_train[f].values) + list(X_test[f].values))\n        X_train[f] = lbl.transform(list(X_train[f].values))\n        X_test[f] = lbl.transform(list(X_test[f].values))   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## random split: \n# (X_train,X_eval,y_train,y_eval) = train_test_split(X_train,y_train,test_size=0.15,random_state=0)\n\n## temporal split (assuming data remains sorted): split by last records, e.g. by recordid or TransactionDT\n\n## 80% of data\nTR_ROW_CNT = int(X_train.shape[0]*0.8)\nprint(TR_ROW_CNT)\n\nX_eval = X_train[TR_ROW_CNT:]\nprint(\"eval shape\",X_eval.shape)\n\nX_train = X_train[:TR_ROW_CNT]\nprint(\"new train shape\",X_train.shape)\n\ny_eval = y_train[TR_ROW_CNT:]\ny_train = y_train[:TR_ROW_CNT]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training\n\n* metric to  AUC"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ## fast AUC metric + calc, from : https://www.kaggle.com/c/microsoft-malware-prediction/discussion/76013#latest-556434\n# ### gives errors, doesn't work ? \n\n# # import numpy as np \n# from numba import jit\n\n# @jit\n# def fast_auc(y_true, y_prob):\n#     y_true = np.asarray(y_true)\n#     y_true = y_true[np.argsort(y_prob)]\n#     nfalse = 0\n#     auc = 0\n#     n = len(y_true)\n#     for i in range(n):\n#         y_i = y_true[i]\n#         nfalse += (1 - y_i)\n#         auc += y_i * nfalse\n#     auc /= (nfalse * (n - nfalse))\n#     return auc\n\n# def eval_auc(preds, dtrain):\n#     labels = dtrain.get_label()\n#     return 'auc', fast_auc(labels, preds), True\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.metrics import roc_auc_score\n\ndef opt(X_train, y_train, X_test, y_test, trial):\n    #param_list\n    n_estimators = trial.suggest_int('n_estimators', 400, 900) # may relate to instability of kernel when predicting? \n    max_depth = trial.suggest_int('max_depth', 4, 20)\n    min_child_weight = trial.suggest_int('min_child_weight', 1, 25)\n    #learning_rate = trial.suggest_discrete_uniform('learning_rate', 0.01, 0.1, 0.01)\n    scale_pos_weight = trial.suggest_int('scale_pos_weight', 1, 30)\n    subsample = trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.1)\n    colsample_bytree = trial.suggest_discrete_uniform('colsample_bytree', 0.6, 1.0, 0.1)\n\n    xgboost_tuna = xgb.XGBClassifier(n_jobs=1,\n        random_state=41, \n        tree_method='gpu_hist',\n        n_estimators = n_estimators,\n        max_depth = max_depth,\n        min_child_weight = min_child_weight,\n        #learning_rate = learning_rate,\n        scale_pos_weight = scale_pos_weight,\n        subsample = subsample,\n        colsample_bytree = colsample_bytree\n    )\n    xgboost_tuna.fit(X_train, y_train)\n\n    tuna_pred_test_proba = xgboost_tuna.predict_proba(X_test)[:,1]\n    return (1.0 - (roc_auc_score(y_test, tuna_pred_test_proba)))\n\n#     tuna_pred_test = xgboost_tuna.predict(X_test)\n#     return (1.0 - (accuracy_score(y_test, tuna_pred_test))) # default ,accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"study = optuna.create_study()\nstudy.optimize(functools.partial(opt, X_train, y_train, X_eval, y_eval), n_trials=80)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Best score found:\",1-study.best_value)\nprint(study.best_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = xgb.XGBClassifier(tree_method='gpu_hist',**study.best_params)\nX_train = pd.concat([X_train,X_eval])\ny_train = pd.concat([y_train,y_eval])\ndel X_eval,y_eval\n\nprint(X_train.shape)\nprint(y_train.shape)\nclf.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## plot features importance\nimport matplotlib.pyplot as plt\n\nfi = pd.DataFrame(index=X_train.columns)\nfi['importance'] = clf.feature_importances_\nfi.loc[fi['importance'] > 0.0005].sort_values('importance',ascending=False).head(32).plot(kind='barh', figsize=(8, 24), title='Feature Importance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## kernel tends to run out of memory when doing predictions ?\n\nsample_submission['isFraud'] = clf.predict_proba(X_test)[:,1]\nsample_submission.to_csv('submission.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}