{"cells":[{"metadata":{},"cell_type":"markdown","source":"\n* I've added a few simple & generic fraud/anomaly features, such as anomality, isolation, missing values per group, datetime, occurences count and hash based identity reconstruction.\n* The kernel will be updated with additional features.\n* Isolation forest for anomaly detection feature from: https://www.kaggle.com/danofer/anomaly-detection-for-feature-engineering-v2\n\n* Model code is from the baseline + [gpu/Xhulu's kernel](https://www.kaggle.com/xhlulu/ieee-fraud-xgboost-with-gpu-fit-in-40s)\n\n* Start date set to 2.11.2017 instead of 20.12.2017, based on this kernel : https://www.kaggle.com/terrypham/transactiondt-timeframe-deduction\n\n* To get my ~95 submission, just run this with more XGBoost iterations and depth / hyperparams (e.g. as in my kernel : https://www.kaggle.com/danofer/xgboost-using-optuna-fastauc-features) , also add V,C to the COLUMN_GROUP_PREFIXES\n\n\n\n###### PERFORMANCE notes: If running locally, set the `n_jobs` param in the isolation forests (+- XGBoost) to -1/-2 (it's problematic in kaggle), and if you have a GPU, set XGBoost to use `gpu_hist` - it's much much faster\n ###### If runing locally - `FAST_RUN = False`  , otherwise you'll have poor results "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set this param to false for better performance, at thos ecost of longer run time. \nFAST_RUN = True","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import os\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nimport xgboost as xgb\n\nfrom sklearn.ensemble import IsolationForest\n\nfrom sklearn.model_selection import KFold, TimeSeriesSplit\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.model_selection import cross_val_predict, cross_val_score\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"%%time\nif FAST_RUN:\n    print(\"using sampled data , fast run\")\n    train_transaction = pd.read_csv('../input/train_transaction.csv', index_col='TransactionID').sample(frac=0.3)\nelse:\n    train_transaction = pd.read_csv('../input/train_transaction.csv', index_col='TransactionID')# ,nrows=12345)\n\ntest_transaction = pd.read_csv('../input/test_transaction.csv', index_col='TransactionID')#,nrows=123)\n\ntrain_identity = pd.read_csv('../input/train_identity.csv', index_col='TransactionID')\ntest_identity = pd.read_csv('../input/test_identity.csv', index_col='TransactionID')\n\nsample_submission = pd.read_csv('../input/sample_submission.csv', index_col='TransactionID')\n\ntrain = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n\nprint(train.shape)\nprint(test.shape)\n\ny_train = train['isFraud'].copy()\ndel train_transaction, train_identity, test_transaction, test_identity\n\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ## join train+test for easier feature engineering:\n# df = pd.concat([train,test],sort=False)\n# print(df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Add some features\n* missing values count\n    * TODO: nans per cattegory/group (e..g V columns)\n    * Could be more efficient with this code, but that's aimed at columnar, not row level summation: https://stackoverflow.com/questions/54207038/groupby-columns-on-column-header-prefix\n* Add some of the time series identified in external platform\n* ToDo: anomaly detection features. \n* proxy for lack of an identifier, duplicate values. \n    * TODO: try to understand what could be a proxy for a key/customer/card identifier (then apply features based on that).\n    \n    \n* ToDo: readd feature of identical transactions: this is typically a strong feature, but (surprisingly) gave no signal in this dataset. Both with and without transaction amount (and with transaction time removed ofc).\n\n\n* COLUMN_GROUP_PREFIXES - I don't calculate all due to memory issues/kernel instability. Not a very strogn feature, but can help. "},{"metadata":{"trusted":true},"cell_type":"code","source":"list(train.columns)\n\nif FAST_RUN:\n    COLUMN_GROUP_PREFIXES = [\"card\",\"M\",\"id\"]\n    \nelse: COLUMN_GROUP_PREFIXES = [\"card\",\"C\",\"D\",\"M\",\"V\",\"id\"]  # ,\"V\" # \"C\" , \"V\" # V has many values, slow, but does contribute. \n\ndef column_group_features(df):\n    \"\"\"\n    Note: surprisingly slow! \n    TODO: Check speed, e.g. with `$ pip install line_profiler`\n    \"\"\"\n    df[\"total_missing\"] = df.isna().sum(axis=1)\n    print(\"total_missing\",df[\"total_missing\"].describe(percentiles=[0.5]))\n    df[\"total_unique_values\"] = df.nunique(axis=1,dropna=False)\n    print(\"total_unique_values\",df[\"total_unique_values\"].describe(percentiles=[0.5]))\n    \n    for p in COLUMN_GROUP_PREFIXES:\n        col_group = [col for col in df if col.startswith(p)]\n        print(\"total cols in subset:\", p ,len(col_group))\n        df[p+\"_missing_count\"] = df[col_group].isna().sum(axis=1)\n        print(p+\"_missing_count\", \"mean:\",df[p+\"_missing_count\"].describe(percentiles=[]))\n        df[p+\"_uniques_count\"] = df[col_group].nunique(axis=1,dropna=False)\n        print(p+\"_uniques_count\", \"mean:\",df[p+\"_uniques_count\"].describe(percentiles=[]))\n#         df[p+\"_max_val\"] = df[col_group].max(axis=1)\n#         df[p+\"_min_val\"] = df[col_group].min(axis=1)\n    print(\"done \\n\")\n    return df\n\n\n# From kernel https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n# WARNING! THIS CAN DAMAGE THE DATA \ndef reduce_mem_usage(df,do_categoricals=False):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            if do_categoricals==True:\n                df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n\ntrain = reduce_mem_usage(train,do_categoricals=False)\ntest = reduce_mem_usage(test,do_categoricals=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ntrain = column_group_features(train)\nprint(\"train features generated\")\n\ntest = column_group_features(test)\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## datetime features\n* try to guess date and datetime delta unit, then add features\n* TODO: strong features potential already found offline, need to validate\n* Try 01.12.2017 as start date: https://www.kaggle.com/kevinbonnes/transactiondt-starting-at-2017-12-01\n* ALT : https://www.kaggle.com/terrypham/transactiondt-timeframe-deduction "},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime\n\nSTART_DATE = \"2017-11-02\"\n\n# Preprocess date column\nstartdate = datetime.datetime.strptime(START_DATE, '%Y-%m-%d')\ntrain['time'] = train['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\ntest['time'] = test['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\n\n## check if time of day is morning/early night, and/or weekend/holiday:\ntrain[\"hour_of_day\"] = train['time'].dt.hour\ntest[\"hour_of_day\"] = test['time'].dt.hour\n\n## check if time of day is morning/early night, and/or weekend/holiday: (day of the week with Monday=0, Sunday=6.)\ntrain[\"day_of_week\"] = train['time'].dt.dayofweek\ntest[\"day_of_week\"] = test['time'].dt.dayofweek\n\nprint(train['time'].describe())\nprint(test['time'].describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## no clear correlation, but we expect any such features to be categorical in nature, not ordinal/continous. the model can findi t\ntrain[[\"isFraud\",\"hour_of_day\",\"day_of_week\"]].sample(frac=0.07).corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Identity hash: \"magic feature\"?\n* Top missing feature is user identity (for feature engineering). \n* Previously (not shown), I extracted features based on  duplicate counts using most of the rows, and showed 1% + of rows as being duplicates (with `TransactionDT`, +- `TransactionAmt` excluded) - but this did not give a good feature (surprisingly).\n    * Fraud is often characterized by similar behavior/repeat behavior. \n* I will create a proxy for identity from the identity/ `id` data! \n    * This can doubtless be improved by keeping/dropping some. e.g. DeviceType , DeviceInfo may be too specific.\n    * `id_34` has \"match_status:\" values - may be only feature we need, or metric of noise (in which case we may want to drop it)\n    \n#### We see that due to the sparsity of the identifier data, 75% of the transactions are duplicates!! \n* This is probably why this wasn't used to create  a \"user ID\" in the first place.\n* Presumably, it might still be salvageable using other data\n\n* This kernel looks at just some card features and gets an approximate of missing car numbers: https://www.kaggle.com/smerllo/identify-unique-cards-id"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ID_COLS = [col for col in train if col.startswith(\"id\")] # + (['DeviceType', 'DeviceInfo'])\n\nID_COLS =['id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07', 'id_08', 'id_09', 'id_10', 'id_11', 'id_12', \n 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26',\n 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38',\n 'DeviceType', 'DeviceInfo']\nprint(ID_COLS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[ID_COLS].sample(12).drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['DeviceType'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['DeviceInfo'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"percent of rows with duplicated identities in data: {:.5}% \".format(100*train[ID_COLS].duplicated().sum()/train.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.duplicated(subset=ID_COLS).sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Continued Identity hash: \n* Let's try using some of the card data as well. (Ideally we'd use something like bank account or location, but we lack that :( )\n\n* using transaction amt changes this from ~74% (when addin in card type) to 51% . Still a lot of redundancy, and it's a bad variable (too easy to change/game), but better than nothing ?\n* adding in the addr1/2 and email domains gets us to 30%. (The same without `TransactionAmt` goes back to 60% duplicates)\n    * Adding dist 1/2 gets us to 19%, but i'm scared of them - no way of knowing if they mean distance or location or something else\n    \n    \n    \n    ########### Ideal solution: brute force a half dozen combinations as features, add them all or one by one, check feature importance/feature selection, keep the best one(s). \n    * I leave this for others kernels :) \n    \n    * does hash respect nans? This kernel ensures they're kept : https://www.kaggle.com/smerllo/identify-unique-cards-id"},{"metadata":{"trusted":true},"cell_type":"code","source":"ID_COLS_EXT =['id_01', 'id_02', 'id_03', 'id_04', 'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38',\n 'DeviceType', 'DeviceInfo',\"ProductCD\",\"card1\",\"card2\",\"card3\",\"card4\",\"card5\",\"card6\",\n              'addr1', \n              'addr2'\n      , 'P_emaildomain', \n#               'R_emaildomain'\n              ]\n\n\nID_COLS =[ 'DeviceType',\n          \"ProductCD\",\n          \"card1\",\"card2\",\"card3\",\"card4\",\"card5\",\"card6\",\n       'P_emaildomain', \n#           'addr1', \n              ]\n\n\n# much siompler, basically just \"did they use this trans.amount. Would benefit from loh10 rounding the amount\" : \nMINI_ID_COLS = [\"card1\",\"card2\"] # ,\"card3\",\"card4\",\"card5\",\"card6\",\"ProductCD\"  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[ID_COLS].head(8).drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[MINI_ID_COLS].drop_duplicates().shape[0]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"train[MINI_ID_COLS].head().drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"% EXT rows with duplicated identities: {:.4}% \".format(100*train[ID_COLS_EXT].duplicated().sum()/train.shape[0]))\n\nprint(\"Without TransactionAmt,  % rows with duplicated identities: {:.4}% \".format(100*train[ID_COLS].duplicated().sum()/train.shape[0]))\n\n\nprint(\"% duplicatecard: {:.4}% \".format(100*train[MINI_ID_COLS].duplicated().sum()/train.shape[0]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns[420:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## make concatenated train+Test for this feature \n* delete it after due to memory issues if using kernel\n* can use for anomaly detection features\n\n\n\ntodo: hash for counting duplicates (counts based on group by columns with missing values is tricky. We don't care about collisions much.\n\n* integer (efficient) Hashing : https://stackoverflow.com/questions/25757042/create-hash-value-for-each-row-of-data-with-selected-columns-in-dataframe-in-pyt\n     * `df.apply(lambda x: hash(tuple(x)), axis = 1)`"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all = pd.concat([train,test],sort=True)\nprint(df_all.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_all[\"duplicated_extended\"] = df_all[ID_COLS_EXT].duplicated().astype(int)\n\ndf_all[\"duplicated_base\"] = df_all[ID_COLS].duplicated().astype(int)\n\ndf_all[\"duplicated_card\"] = df_all[MINI_ID_COLS].duplicated().astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n## size includes NaN values, count does not.\n\ndf_all[\"card_hash\"] = df_all[MINI_ID_COLS].apply(lambda x: hash(tuple(x)), axis = 1)\ndf_all[\"card_hash_total_counts\"] = df_all.groupby(\"card_hash\")[\"total_missing\"].transform(\"size\") \n\n\ndf_all[\"multIDcols_hash\"] = df_all[ID_COLS].apply(lambda x: hash(tuple(x)), axis = 1)\ndf_all[\"multIDcols_total_counts\"] = df_all.groupby(ID_COLS)[\"total_missing\"].transform(\"size\") \n\n# df_all[\"hash_PlusTransamt\"] = df_all[ID_COLS_EXT].apply(lambda x: hash(tuple(x)), axis = 1)\n# df_all[\"hash_PlusTransamt_total_counts\"] = df_all.groupby(\"hash_PlusTransamt\")[\"total_missing\"].transform(\"size\") \n\n# # count # transaction amount reoccurred +- trans type\n\n# df_all[\"hash_Transamt\"] = df_all[MINI_ID_COLS].apply(lambda x: hash(tuple(x)), axis = 1)\n# df_all[\"hash_Transamt_total_counts\"] = df_all.groupby(\"hash_Transamt\")[\"total_missing\"].transform(\"size\") \n\n\n\n# # drop some  the hashed ids that uses transaction amount\n# df_all.drop(['hash_PlusTransamt',\"hash_Transamt\"], axis=1, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n## Spending vs mean\ndf_all[\"TransactionAmt_count\"] = df_all.groupby([\"TransactionAmt\"])[\"card1\"].transform(\"size\")\n\n## more (count) feats\n\ndf_all[\"TransactionAmt_count\"] = df_all.groupby([\"TransactionAmt\"])[\"card1\"].transform(\"size\")\n\ndf_all[\"card1_count\"] = df_all.groupby([\"card1\"])[\"ProductCD\"].transform(\"size\")\ndf_all[\"card2_count\"] = df_all.groupby([\"card2\"])[\"ProductCD\"].transform(\"size\")\ndf_all[\"card3_count\"] = df_all.groupby([\"card3\"])[\"ProductCD\"].transform(\"size\")\ndf_all[\"card5_count\"] = df_all.groupby([\"card5\"])[\"ProductCD\"].transform(\"size\")\n\ndf_all[\"R_email_count\"] = df_all.groupby([\"R_emaildomain\"])[\"ProductCD\"].transform(\"size\")\ndf_all[\"P_email_count\"] = df_all.groupby([\"P_emaildomain\"])[\"ProductCD\"].transform(\"size\")\ndf_all[\"addr1_count\"] = df_all.groupby([\"addr1\"])[\"ProductCD\"].transform(\"size\")\ndf_all[\"addr2_count\"] = df_all.groupby([\"addr2\"])[\"ProductCD\"].transform(\"size\")\n# joint column count\ndf_all[\"P_R_emails_count\"] = df_all.groupby([\"P_emaildomain\",\"R_emaildomain\"])[\"ProductCD\"].transform(\"size\")\ndf_all[\"joint_addresses_count\"] = df_all.groupby([\"addr1\",\"addr2\"])[\"ProductCD\"].transform(\"size\")","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"## transactions per hour\n## https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#resampling\n## https://stackoverflow.com/questions/45922291/average-number-of-actions-per-day-of-the-week-using-pandas\n\n## results seem inconsistent between the 2 functions. need to debug. \n# df_all.head(8234).set_index(\"time\").resample(\"H\").size()\n# df_all.head(2234).groupby([df_all.time.dt.hour,df_all.time.dt.dayofyear])[\"ProductCD\"].transform(\"size\")\n\ndf_all[\"events_this_hour_cnt\"] = df_all.groupby([df_all.time.dt.hour,df_all.time.dt.dayofyear])[\"ProductCD\"].transform(\"size\")\n\n####\n\n# mean spend for an hour of day and DoW. could add more aggregations\ndf_all[\"mean_spend_hour_day\"] = df_all.groupby([df_all.time.dt.hour,df_all.time.dt.dayofweek])[\"TransactionAmt\"].transform(\"mean\")\n# spend vs mean\ndf_all[\"normalized_spend_vs_hour_day_mean\"] = df_all[\"TransactionAmt\"].div(df_all[\"mean_spend_hour_day\"])\n\n# spend vs that specific day of year\ndf_all[\"normalized_spend_vs_dayOfYear\"] = df_all[\"TransactionAmt\"].div(df_all.groupby([df_all.time.dt.day])[\"TransactionAmt\"].transform(\"mean\"))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### FE kernel - I forgot to add these originally!\n\nhttps://www.kaggle.com/c/ieee-fraud-detection/discussion/108575\n* add mean transaction (even if I extract it anyway afterwards, externally).\n* card 1 counts.."},{"metadata":{"trusted":true},"cell_type":"code","source":"## Frequency Encoding\ntemp = df_all['card1'].value_counts().to_dict()\ndf_all['card1_counts'] = df_all['card1'].map(temp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Additional mean Aggregations / Group Statistics\ntemp = df_all.groupby('card1')['TransactionAmt'].agg(['median']).rename({'median':'TransactionAmt_card1_median'},axis=1)\ndf_all = pd.merge(df_all,temp,on='card1',how='left')\n\n\ntemp = df_all.groupby('card_hash')['TransactionAmt'].agg(['mean']).rename({'mean':'TransactionAmt_card_hash_mean'},axis=1)\ndf_all = pd.merge(df_all,temp,on='card_hash',how='left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### holiday possible features\n* Depends on our start date\n* Additional possible holidays - cyber monday, black friday... \n* Holiday code from : https://www.kaggle.com/kyakovlev/ieee-fe-for-local-test\n* Original also did datetime aggregations, may be different than the ones I do here/above.. "},{"metadata":{"trusted":true},"cell_type":"code","source":"list(df_all.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### TransactionDT\nfrom pandas.tseries.holiday import USFederalHolidayCalendar as calendar\ndates_range = pd.date_range(start='2017-10-01', end='2019-01-01')\nus_holidays = calendar().holidays(start=dates_range.min(), end=dates_range.max())\n\n## USA holidays\ndf_all['is_holiday'] = (df_all['time'].dt.date.astype('datetime64').isin(us_holidays)).astype(np.int8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.columns[432:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### put our new features back into train, test. \n\n* Warning - requires that we manually add these columns here - could be improved. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"ADDED_FEATS = [\n       'duplicated_base', 'duplicated_card', 'card_hash',\n       'card_hash_total_counts', 'multIDcols_total_counts', 'multIDcols_hash',\n       'TransactionAmt_count', 'card1_count', 'card2_count', 'card3_count',\n       'card5_count', 'R_email_count', 'P_email_count', 'addr1_count',\n       'addr2_count', 'P_R_emails_count', 'joint_addresses_count',\n       'events_this_hour_cnt', 'mean_spend_hour_day',\n       'normalized_spend_vs_hour_day_mean', 'normalized_spend_vs_dayOfYear',\n    'TransactionAmt_card1_median', 'TransactionAmt_card_hash_mean', 'card1_counts'\n    ,'is_holiday']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_all = reduce_mem_usage(df_all,do_categoricals=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)\ntrain = train.join(df_all[ADDED_FEATS])\nprint(train.shape)\nprint()\nprint(test.shape)\ntest = test.join(df_all[ADDED_FEATS])\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check if columns have unary vals\nnunique = test.apply(pd.Series.nunique)\ncols_to_drop = nunique[nunique == 1].index\ntest.drop(cols_to_drop, axis=1,inplace=True)\n\ntrain.drop(cols_to_drop, axis=1,inplace=True)\nprint(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### label-encode & model build\n* TODO: compare to OHE? +- other encoding/embedding methods"},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop target, fill in NaNs ?\n# consider dropping the TransactionDT column as well...\nX_train = train.drop(['isFraud',\"time\",'TransactionDT'], axis=1).copy()\nX_test = test.drop([\"time\",'TransactionDT'], axis=1).copy()\n\ny_train = train['isFraud'].copy() # recopy\n\n\ndel train, test\ngc.collect()\n\n# Label Encoding\nfor f in X_train.columns:\n    if X_train[f].dtype=='object' or X_test[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(X_train[f].values) + list(X_test[f].values))\n        X_train[f] = lbl.transform(list(X_train[f].values))\n        X_test[f] = lbl.transform(list(X_test[f].values))   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### xgboost can handlke nans itself, often better (more efficient memory use) BUT our  test data has diff nan distrib ? \n\nX_train.fillna(-999,inplace=True)\nX_test.fillna(-999,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nX_train = reduce_mem_usage(X_train,do_categoricals=True)\nX_test = reduce_mem_usage(X_test,do_categoricals=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Anomaly detection features\n* Isolation forest approach for now, can easily be improved with semisupervised approach, additional models, TSNE etc'\n* Based on this kernel: https://www.kaggle.com/danofer/anomaly-detection-for-feature-engineering-v2\n\n* Note: potential improvement: train additional model on only positive (non fraud) samples on concatenated train+test. \n\n\n\n##### Isolation forest (anomaly detection)\n* https://www.kaggle.io/svf/1100683/56c8356ed1b0a6efccea8371bc791ba7/__results__.html#Tree-based-techniques )\n* contamination = % of anomalies expected  (fraud class % in our case)\n\n* isolation forest doesn't work on nan values!\n    * TODO: model +- transaction amount. +- nan imputation (at least/especially for important columns)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_all = pd.concat([X_train.dropna(axis=1),X_test.dropna(axis=1)]).drop([\"TransactionDT\"],axis=1).dropna(axis=1)\n# TR_ROWS = X_train.shape[0]\n# NO_NAN_COLS = df_all.columns\n# print(\"num of no nan cols\",len(NO_NAN_COLS))\n# print(\"columns with no missing vals: \\n\",NO_NAN_COLS)\n# print(df_all.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nclf = IsolationForest(random_state=82,  max_samples=0.05, bootstrap=False, n_jobs=2,\n                          n_estimators=100,max_features=0.98,behaviour=\"new\",contamination= 0.035)\nclf.fit(pd.concat([X_train.dropna(axis=1),X_test.dropna(axis=1)]))\n# del (df_all)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## add anomalous feature.\n## Warning! this is brittle! be careful with the columns!!\n\n# X_train[\"isolation_overall_score\"] =clf.decision_function(X_train[NO_NAN_COLS])\n# X_test[\"isolation_overall_score\"] =clf.decision_function(X_test[NO_NAN_COLS])\n\nX_train[\"isolation_overall_score\"] =clf.decision_function(X_train)\nX_test[\"isolation_overall_score\"] =clf.decision_function(X_test)\n\n\nprint(\"Fraud only mean anomaly score\",X_train.loc[y_train==1][\"isolation_overall_score\"].mean())\nprint(\"Non-Fraud only mean anomaly score\",X_train.loc[y_train==0][\"isolation_overall_score\"].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all[\"isolation_overall_score\"] = pd.concat([X_train[\"isolation_overall_score\"],X_test[\"isolation_overall_score\"] ])\n\ndf_all[\"isolation_overall_score\"].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train only on non fraud samples\n\nclf = IsolationForest(random_state=31,  bootstrap=True,  max_samples=0.05,n_jobs=3,\n                          n_estimators=100,behaviour=\"new\",max_features=0.96) #\n# clf.fit(X_train[NO_NAN_COLS].loc[y_train==1].values)\nclf.fit(X_train.loc[y_train==1].values)\n\n# X_train[\"isolation_pos_score\"] =clf.decision_function(X_train[NO_NAN_COLS])\n# X_test[\"isolation_pos_score\"] =clf.decision_function(X_test[NO_NAN_COLS])\n\nX_train[\"isolation_pos_score\"] =clf.decision_function(X_train)\nX_test[\"isolation_pos_score\"] =clf.decision_function(X_test)\n\n# del (clf)\n\nprint(\"Fraud only mean pos-anomaly score\",X_train.loc[y_train==1][\"isolation_pos_score\"].mean())\nprint(\"Non-Fraud only mean pos-anomaly score\",X_train.loc[y_train==0][\"isolation_pos_score\"].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all[\"isolation_pos_score\"] = pd.concat([X_train[\"isolation_pos_score\"],X_test[\"isolation_pos_score\"] ])\n\ndf_all[\"isolation_pos_score\"].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Model training\n\n* todo: do cross_val_predict (sklearn) using sklearn api for convenience\n* Temporal split :  use sklearn's TimeSeriesSplit (or manual) for early stopping/validation + validation\n\n\n* First version - cross validated predictions - ensemble approach\n* secodn appproach - single model"},{"metadata":{"trusted":true},"cell_type":"code","source":"## some hyperparams, made faster for fast runs. \n\nif FAST_RUN:\n    EPOCHS = 2\n    model_num_estimators = 300\n    model_lr = 0.2\nelse: \n    EPOCHS = 4 # use more for better perf, but greater risk of overfitting\n    model_num_estimators = 700\n    model_lr = 0.06\n    \n    \nkf = KFold(n_splits = EPOCHS, shuffle = False)\nkf_time = TimeSeriesSplit(n_splits = EPOCHS) # temporal validation. use this to evaluate performance better , not necessarily as good for OOV ensembling though!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n\n\n# y_preds = np.zeros(sample_submission.shape[0])\n# y_oof = np.zeros(X_train.shape[0])\n# for tr_idx, val_idx in kf.split(X_train, y_train):\n#     clf = xgb.XGBClassifier(#n_jobs=2,\n#         n_estimators=500,  # 500 default\n#         max_depth=9, # 9\n#         learning_rate=0.05,\n#         subsample=0.9,\n#         colsample_bytree=0.9,\n# #         tree_method='gpu_hist' # #'gpu_hist', - faster, less exact , \"gpu_exact\" - better perf\n# #         ,min_child_weight=2 # 1 by default\n#     )\n    \n#     X_tr, X_vl = X_train.iloc[tr_idx, :], X_train.iloc[val_idx, :]\n#     y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n#     clf.fit(X_tr, y_tr)\n#     y_pred_train = clf.predict_proba(X_vl)[:,1]\n#     y_oof[val_idx] = y_pred_train\n#     print('ROC AUC {}'.format(roc_auc_score(y_vl, y_pred_train)))\n    \n#     y_preds+= clf.predict_proba(X_test)[:,1] / EPOCHS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n## second approach:\n\nclf = xgb.XGBClassifier(n_jobs=3,\n    n_estimators=model_num_estimators,  # 500 default\n    max_depth=11, # 9\n    learning_rate=model_lr, # 0.05 better\n    subsample=0.9,\n    colsample_bytree=0.9\n    ,tree_method= 'hist' # #'gpu_hist', - faster, less exact , \"gpu_exact\" - better perf , \"auto\", 'hist' (cpu)\n        ,min_child_weight=2 # 1 by default\n)\n\n\nclf.fit( X_train,y_train)\ny_preds = clf.predict_proba(X_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n# fi = pd.DataFrame(index=clf.feature_names_)\nfi = pd.DataFrame(index=X_train.columns)\nfi['importance'] = clf.feature_importances_\nfi.loc[fi['importance'] > 0.0004].sort_values('importance').head(50).plot(kind='barh', figsize=(14, 32), title='Feature Importance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make submissions\nsample_submission['isFraud'] = y_preds\nsample_submission.to_csv('dan_xgboost_cpu_hist_singlemodel.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"mean AUC\",cross_val_score(clf, X_train,y_train, cv=kf, scoring='roc_auc').mean())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"mean temporal CV AUC\",cross_val_score(clf, X_train,y_train, cv=kf_time, scoring='roc_auc').mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## A model stacking feature\n#### warning - may overfit!!\n* temporal CV self score (vs random CV). "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train[\"random_cv_preds\"] = cross_val_predict(clf, X_train,y_train, method='predict_proba')[:,1]\nX_test[\"random_cv_preds\"] = y_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train[\"temporal_cv_preds\"] = cross_val_predict(clf, X_train,y_train, cv=kf_time.split(X_train),  method='predict_proba')[:,1]\n# X_test[\"temporal_cv_preds\"] = y_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n## second approach:\n\nclf = xgb.XGBClassifier(n_jobs=2,\n    n_estimators=model_num_estimators,  # 500 default\n    max_depth=15, # 19\n    learning_rate=model_lr, # 0.06 is better\n    subsample=0.9,\n    colsample_bylevel=0.9\n    ,tree_method= 'auto' # #'gpu_hist', / \"hist\" - faster, less exact , \"gpu_exact\" - better perf , \"auto\", 'hist' (cpu)\n     ,min_child_weight=3 # 1 by default\n    ,missing=-999\n#     ,scale_pos_weight=3\n)\n\n\nclf.fit( X_train,y_train)\ny_preds = clf.predict_proba(X_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fi = pd.DataFrame(index=X_train.columns)\nfi['importance'] = clf.feature_importances_\nfi.loc[fi['importance'] > 0.0004].sort_values('importance').head(40).plot(kind='barh', figsize=(14, 32), title='Feature Importance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['isFraud'] = y_preds\nsample_submission.to_csv('dan_xgboost_stack_model2.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all[\"random_cv_preds\"] = pd.concat([X_train[\"random_cv_preds\"],X_test[\"random_cv_preds\"] ])\n# df_all[\"temporal_cv_preds\"] = pd.concat([X_train[\"temporal_cv_preds\"],X_test[\"temporal_cv_preds\"] ])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Store the extracted, novel features in a new dataframe for export/sharing\n\n* Store it before any memory saving resizing if possible\n* can concat the anomaly model based features to it, or seperately. \n\n* Remember: `TransactionID` is the index, not a column"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.columns[:65]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.columns[:100]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.columns[100:200]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.columns[200:300]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.columns[300:390]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.columns[390:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df_all.columns[432:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.columns[431:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EXTRA_FEAT_NAMES = df_all.columns[432:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all[EXTRA_FEAT_NAMES].tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all[EXTRA_FEAT_NAMES].to_csv(\"extra_fraud_feats_baseV2.csv\")#.gz\",compression=\"gzip\")\n\n# del df_all","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.drop(['TransactionDT'],axis=1).loc[~df_all['isFraud'].isna()].sample(frac=0.005).to_csv(\"sample_fraud_train_augV1.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.drop(['TransactionDT'],axis=1).loc[~df_all['isFraud'].isna()].to_csv(\"fraud_train_augV1.csv\")\nprint(\"train full saved\")\ndf_all.loc[df_all['isFraud'].isna()].drop(['TransactionDT','isFraud'],axis=1).to_csv(\"fraud_test_augV1.csv\")\nprint(\"test full saved\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### transactions only for TS"},{"metadata":{"trusted":true},"cell_type":"code","source":"TRANSACT_COLS = ['isFraud', 'time', 'card_hash', \n       'multIDcols_hash','normalized_spend_vs_hour_day_mean',\n       'normalized_spend_vs_dayOfYear', 'isolation_overall_score',\n       'isolation_pos_score','TransactionAmt','P_emaildomain', 'ProductCD', 'R_emaildomain','addr1', 'addr2',\n                'id_01', 'id_02', 'id_03', 'id_04', 'id_05',\n       'id_06', 'id_07', 'id_08', 'id_09', 'id_10', 'id_11', 'id_12', 'id_13',\n       'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21',\n       'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29',\n       'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37',\n       'id_38',\"V307\",\"V179\",\"V120\",\"V258\", \"V185\"]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df_all.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.loc[~df_all['isFraud'].isna()][TRANSACT_COLS].to_csv(\"tr_eventsTS.csv.gz\",compression=\"gzip\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.loc[df_all['isFraud'].isna()][TRANSACT_COLS].to_csv(\"test_eventsTS.csv.gz\",compression=\"gzip\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Simple model based feature importance plot\n* TODO: shapley, interactions\n\n* It looks like our grouped missing values are **valuable**, although the datetime features seemingly didn't (likely, some of the anonymized variables already capture them). They may have some marginal contribution.\n    * toDo: check that run models with and without them"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}