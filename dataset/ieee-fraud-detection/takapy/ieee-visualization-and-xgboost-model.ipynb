{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Overview\n\nIn this competition you are predicting the probability that an online transaction is fraudulent, as denoted by the binary target isFraud.\n\nThe data is broken into two files identity and transaction, which are joined by TransactionID.\n\n> Note: Not all transactions have corresponding identity information.\n\n**Categorical Features - Transaction**\n\n- ProductCD\n- emaildomain\n- card1 - card6\n- addr1, addr2\n- P_emaildomain\n- R_emaildomain\n- M1 - M9\n\n**Categorical Features - Identity**\n\n- DeviceType\n- DeviceInfo\n- id_12 - id_38\n\nThe TransactionDT feature is a timedelta from a given reference datetime (not an actual timestamp).\n"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport gc\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nimport pandas_profiling as pdp\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 300)\npd.set_option('display.max_colwidth', 5000)\npd.options.display.float_format = '{:.3f}'.format\n%matplotlib inline\nplt.style.use('fivethirtyeight')\n\nimport os\nprint(os.listdir(\"../input/\"))\nDIR_NAME = \"../input\"\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, KFold, RepeatedKFold, GridSearchCV\nfrom sklearn.metrics import classification_report, roc_auc_score, roc_curve, auc\n\nimport xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# import data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# load csv\ndef load_dir_csv(directory, csv_files=None):\n    if csv_files is None:\n        csv_files = sorted( [ f for f in os.listdir(directory) if f.endswith(\".csv\") ])    \n    csv_vars  = [ filename[:-4] for filename in csv_files ]\n    gdict = globals()\n    for filename, var in zip( csv_files, csv_vars ):\n        print(f\"{var:32s} = pd.read_csv({directory}/{filename})\")\n        gdict[var] = pd.read_csv( f\"{directory}/{filename}\" )\n        print(f\"{'shape ':32s} = \" + str(gdict[var].shape))\n        display(gdict[var].head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nload_dir_csv(DIR_NAME, [\"train_transaction.csv\", \"test_transaction.csv\", \"train_identity.csv\", \"test_identity.csv\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is data of column name that I don't understand well.  \nFor example, CXX, DXX, MXX, VXX of `transaction.csv`."},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n# merge to data\ntrain = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\ntest = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Train dataset has {} rows and {} columns.'.format(train.shape[0], train.shape[1]))\nprint('Test dataset has {} rows and {} columns.'.format(test.shape[0], test.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"del train_transaction, train_identity, test_transaction, test_identity","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{},"cell_type":"markdown","source":"## Basic statistics"},{"metadata":{"trusted":false},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The VXX data seems to have almost zero data."},{"metadata":{},"cell_type":"markdown","source":"## Missing value"},{"metadata":{"trusted":false},"cell_type":"code","source":"def is_integer_num(n):\n    if isinstance(n, int):\n        return True\n    if isinstance(n, float):\n        return n.is_integer()\n    return False\n\ndef missing_values_table_specified_value(df, value=0.5): \n    mis_val = df.isnull().sum()\n    mis_val_percent = 100 * df.isnull().sum()/len(df)\n    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n    mis_val_table_ren_columns = mis_val_table.rename(\n    columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n    \n    if is_integer_num(value):\n        mis_val_table_ren_columns = mis_val_table_ren_columns[mis_val_table_ren_columns['Missing Values'] >= value]\n        print('The number of columns with {} counts missing values is {}.'.format(value, len(mis_val_table_ren_columns)))\n    else:\n        value = value * 100\n        mis_val_table_ren_columns = mis_val_table_ren_columns[mis_val_table_ren_columns['% of Total Values'] >= value]\n        print('The number of columns with {}% missing values is {}.'.format(value, len(mis_val_table_ren_columns)))\n    return mis_val_table_ren_columns \n\ndef missing_values_table(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(np.transpose(tt))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"missing_values_table_specified_value(train, 0.5).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"missing_values_table_specified_value(test, 0.5).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Test data contains fewer columns with 50% or more missing data than train data."},{"metadata":{"trusted":false},"cell_type":"code","source":"display(missing_values_table(train), missing_values_table(test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Looking Fraud"},{"metadata":{"trusted":false},"cell_type":"code","source":"train['isFraud'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.countplot(train['isFraud'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is imbalanced data, which has an overwhelming number of 0"},{"metadata":{"trusted":false},"cell_type":"code","source":"print('{:.4f}% of data that are fraud in train.'.format(train['isFraud'].mean() * 100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Split into Quantitative and Qualitative data"},{"metadata":{"trusted":false},"cell_type":"code","source":"quantitative = [f for f in train.columns if train.dtypes[f] != 'object']\nprint(quantitative)\nprint('Counts: {}'.format(len(quantitative)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"qualitative = [f for f in train.columns if train.dtypes[f] == 'object']\nprint(qualitative)\nprint('Counts: {}'.format(len(qualitative)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Looking qualitative columns"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Get columns with less than n unique values in the type specified in column_type\ndef unique_data_under_n_columns_list(df, column_type='object', n=0):\n    # If n = 0, return all columns\n    if n == 0:\n        n = df.shape[0]\n    \n    columns_list = df.select_dtypes(include=column_type).columns.tolist()\n    unique_n_list = []\n    for colomn in columns_list:\n        unique_count = len(df[colomn].unique())\n        if unique_count < n:\n            unique_n_list.append(colomn)\n        else:\n            print('{} Is excluded because it has a unique value {} greater than {}.'.format(colomn, n, unique_count))\n    return unique_n_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Graph data of specified type\ndef visualization_object_data(df, fig_x=10, fig_y=5):\n    # Get columns type of object\n    object_list = unique_data_under_n_columns_list(df, 'object', 65)\n    for colomn in object_list:\n        print('column name:{} / unique value(axis x):{} / max value:{} / min value:{} / missing value:{} '.format(colomn, len(df[colomn].unique()), max(df[colomn].value_counts()), min(df[colomn].value_counts()), df[colomn].isnull().sum()))\n        # Calculate the Optimal Horizontal Size of Shapes\n        fig_x = len(df[colomn].unique())\n        order = df[colomn].value_counts(ascending=False).index\n        if fig_x <= 8:\n            fig, ax = plt.subplots(1, 3, figsize=(fig_x*6,fig_y))\n            sns.countplot(x=colomn, ax=ax[0], data=df, order=order)\n            ax[0].set_title('All', fontsize=14)\n            sns.countplot(x=colomn, ax=ax[1], data=df.loc[df['isFraud'] == 1], order=order)\n            ax[1].set_title('isFraud = 1', fontsize=14)\n            sns.countplot(x=colomn, ax=ax[2], data=df.loc[df['isFraud'] == 0], order=order)\n            ax[2].set_title('isFraud = 0', fontsize=14)\n        else:\n            fig, ax = plt.subplots(1, 3, figsize=(32,10))\n            sns.countplot(y=colomn, ax=ax[0], data=df, order=order)\n            ax[0].set_title('All', fontsize=14)\n            sns.countplot(y=colomn, ax=ax[1], data=df.loc[df['isFraud'] == 1], order=order)\n            ax[1].set_title('isFraud = 1', fontsize=14)\n            sns.countplot(y=colomn, ax=ax[2], data=df.loc[df['isFraud'] == 0], order=order)\n            ax[2].set_title('isFraud = 0', fontsize=14)\n\n        plt.show()\n        plt.pause(0.05)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"visualization_object_data(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Boosting Model + FE Importance"},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\nX_train = train.drop('isFraud', axis=1)\ny_train = train['isFraud'].copy()\nX_test = test.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train.shape, y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"del train, test\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n# Label Encoding\nfor f in qualitative:\n    lbl = preprocessing.LabelEncoder()\n    lbl.fit(list(X_train[f].values) + list(X_test[f].values))\n    X_train[f] = lbl.transform(list(X_train[f].values))\n    X_test[f] = lbl.transform(list(X_test[f].values))  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Check if it is encoded\nprint(len(X_train.select_dtypes(include='object').columns))\nprint(len(X_test.select_dtypes(include='object').columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\nX_train = reduce_mem_usage(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\nX_test = reduce_mem_usage(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_test.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# model xgboost"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv(DIR_NAME + '/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"NFOLDS = 5\nkf = KFold(n_splits = NFOLDS, shuffle = True)\ny_preds = np.zeros(sub.shape[0])\ny_oof = np.zeros(X_train.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\nfor tr_idx, val_idx in kf.split(X_train, y_train):\n\n    clf = xgb.XGBClassifier(\n        n_estimators=500,\n        max_depth=9,\n        learning_rate=0.05,\n        subsample=0.9,\n        colsample_bytree=0.9,\n        tree_method='gpu_hist'\n    )\n    \n    X_tr, X_vl = X_train.iloc[tr_idx, :], X_train.iloc[val_idx, :]\n    y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n    clf.fit(X_tr, y_tr)\n    y_pred_train = clf.predict_proba(X_vl)[:,1]\n    y_oof[val_idx] = y_pred_train\n    \n    print('ROC AUC {}'.format(roc_auc_score(y_vl, y_pred_train)))\n    \n    y_preds += clf.predict_proba(X_test)[:,1] / NFOLDS","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sub['isFraud'] = y_preds\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.2"}},"nbformat":4,"nbformat_minor":1}