{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# importing necessary libraries\nimport numpy as np\nimport pandas as pd\nimport os\nimport sklearn\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nimport matplotlib.pyplot as plt\n\n# reading the required files\ntr_trns = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/train_transaction.csv\")\ntr_id = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/train_identity.csv\")\ntst_trns = pd.read_csv('/kaggle/input/ieee-fraud-detection/test_transaction.csv')\ntst_id = pd.read_csv('/kaggle/input/ieee-fraud-detection/test_identity.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We'll need to join on the TransactionID field to get a single source of data\ntr_trns['TransactionID'].head()\ntr_id['TransactionID'].head()\n\n\n# Limiting the number of columns for the first 5 questions of the exercise\nlist(tr_trns.columns)\ncols1 = ['TransactionID', 'TransactionDT', 'TransactionAmt', 'ProductCD',\n         'card4', 'card6', 'P_emaildomain', 'R_emaildomain', 'addr1', 'addr2',\n         'dist1', 'dist2', 'isFraud']\ntrns_sub = tr_trns[cols1]\n\n\nlist(tr_id.columns)\ncols2 = ['TransactionID', 'DeviceType', 'DeviceInfo']\nid_sub = tr_id[cols2]\n\n\n# Now joining the two tables\ntrns_join = trns_sub.merge(id_sub, on = 'TransactionID', how = 'left')\ntrns_join.dtypes\ndesc = trns_join.describe()\ntrns_join.isna().sum()          # How many NAs in each feature","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1. Plotting the distribution of some features\n# First separating the dataset into fradulent and nonfradulent\nfraud_dat = trns_join[trns_join['isFraud'] == 1]\nnonfraud_dat = trns_join[trns_join['isFraud'] == 0]\n\n# shape of both datasets\nprint(fraud_dat.shape)\nprint(nonfraud_dat.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## distribution of some features in fraud_dat\n# a. TransactionDT\nprint(fraud_dat['TransactionDT'].describe())\nplt.figure(figsize=(18, 6))\nplt.subplot(1, 2, 1)\nplt.hist(fraud_dat['TransactionDT'])\nplt.title('TransactionDT in fraudulent data')\n\n# a.2.\nplt.subplot(1, 2, 2)\nplt.hist(nonfraud_dat['TransactionDT'])\nplt.title('TransactionDT in the non-fraudulent data')\n\n# Finding the minimum and maximum time\na = np.min(trns_join['TransactionDT'])\nb = np.max(trns_join['TransactionDT'])\n(b-a)/86400\n# So, the transactions have been recorded over a span of about 182 days","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations**: From the first histogram, we can see that the TransactionDT variable is quite even. So the fraudulent transactions happening during the given time frame is quite uniform. As compared to fradulent transactions, nonfradulent transactions were more frequent in the beginning and then the count reduced. However, after that it was more or less quite even. \nBy some simple calculations, we find out that the transactions have been recorded over a span of about 182 days."},{"metadata":{"trusted":true},"cell_type":"code","source":"# b. TransactionAmt\nprint(fraud_dat['TransactionAmt'].describe())\nplt.figure(figsize=(20, 5))\nplt.subplot(1, 4, 1)\nplt.hist(fraud_dat['TransactionAmt'], color='orange')\nplt.title('Distribution of TransactionAmt in Fraudulent data')\nprint(fraud_dat['TransactionAmt'].quantile(0.9))\nlen(fraud_dat[fraud_dat['TransactionAmt'] < 1000])/len(fraud_dat['TransactionAmt'])\n\n# Plotting for less than $500 amount\ntmp = fraud_dat[fraud_dat['TransactionAmt'] < 500]\nplt.subplot(1, 4, 2)\nplt.hist(tmp['TransactionAmt'], color='orange')\nplt.title('Distribution for TransactionAmt < $500')\n# We can also check the distribution of the fraudulent transaction amount\n# during the different time frames.\nplt.subplot(1, 4, 3)\nplt.scatter(tmp['TransactionAmt'], tmp['TransactionDT'], color='orange')\nplt.title('TransactionAmt vs Transaction Time')\n\n# b.2. \nplt.subplot(1, 4, 4)\nplt.hist(nonfraud_dat['TransactionAmt'], color='orange')\nplt.title('Distribution of TransactionAmt in non-Fraudulent data')\nprint(nonfraud_dat['TransactionAmt'].quantile(0.9))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations**: The first plot obtained is highly skewed to the right. It indicates that most of the fraudulent transactions (99%) happen to be less than \\\\$1000. And in fact, 90% of the fraudulent transactions are below $335. We can try plotting for less than 500 to see the distribution. It's still skewed to the right. We also plot TransactionAmt with TransactionDT - turns out, it's not very interesting. Fraudulent transaction amount doesn't seem to depend directly on the Transaction time. The distribution in non-fraudulent data is quite similar to what we saw in the fraudulent transactions. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# c. ProductCD (a categorical variable)\nprint(fraud_dat['ProductCD'].describe())\nplt.figure(figsize=(18, 6))\nplt.subplot(1, 2, 1)\nfraud_dat['ProductCD'].value_counts().plot(kind='bar', color = 'purple')\nplt.title('ProductCD distribution in fraudulent')\n\n# c.2.\nplt.subplot(1, 2, 2)\nnonfraud_dat['ProductCD'].value_counts().plot(kind='bar', color = 'purple')\nplt.title('ProductCD distribution in non-fraudulent')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations**: From the ProductCD bar graph, we see that 'W' and 'C' are the most dominant product codes for fraudulent transactions. The second plot is quite interesting - although 'W' is still dominant among the nonfraudulent transactions, 'C' has waned in importance. The proportion of 'W' vs 'C' is quite different from what we observed in the fraudulent transaction. This could potentially help our model learn better. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# d. card4 and card6 are two more categorical variables\nplt.figure(figsize=(18, 12))\nplt.subplot(2, 2, 1)\nfraud_dat['card4'].value_counts().plot(kind='bar')\nplt.title('card4 distribution in fraudulent')\n\nplt.subplot(2, 2, 2)\nnonfraud_dat['card4'].value_counts().plot(kind='bar')\nplt.title('card4 distribution in non-fraudulent')\n\n# card6 plots\nplt.subplot(2, 2, 3)\nfraud_dat['card6'].value_counts().plot(kind='bar')\nplt.title('card6 distribution in fraudulent')\n\nplt.subplot(2, 2, 4)\nnonfraud_dat['card6'].value_counts().plot(kind='bar')\nplt.title('card6 distribution in non-fraudulent')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations**: Clearly, most frauds are committed using Visa and Mastercards but one probable reason could be that because they are ubiquitous. Can infer that by comparing with the nonfradulent transactions. The distribution in the second plot is very similar to fraudulent dataset. In the third plot, we see that frauds committed using debit and credit cards are nearly the same. The same pattern is not followed in the non-fraudulent transactions, there is a discrepancy between the proportion between debit and credit card usage. Maybe it could mean that credit card are as popular as debit cards in committing frauds."},{"metadata":{"trusted":true},"cell_type":"code","source":"# e. P_emaildomain and R_emaildomain (more categorical variables)\nplt.figure(figsize=(18, 15))\nplt.subplot(2, 2, 1)\nfraud_dat['P_emaildomain'].value_counts().plot(kind='bar')\nplt.title('Purchaser emaildomain in fraudulent')\n# a number of email domains have been used for frauds, however, the most\n# popular ones stands out here as well. Frauds committed using gmail domain\n# are more in number than all the other domains combined.\nplt.subplot(2, 2, 2)\nnonfraud_dat['P_emaildomain'].value_counts().plot(kind='bar')\nplt.title('Purchaser emaildomain in non-fraudulent')\n\n# R_emaildomain\nplt.subplot(2, 2, 3)\nfraud_dat['R_emaildomain'].value_counts().plot(kind='bar')\nplt.title('Recipient emaildomain in fraudulent')\n\nplt.subplot(2, 2, 4)\nnonfraud_dat['R_emaildomain'].value_counts().plot(kind='bar')\nplt.title('Recipient emaildomain in non-fraudulent')\n\n# analyzing the email domains of the purchaser and recipient email domain\na = set(fraud_dat['P_emaildomain'])\nb = set(fraud_dat['R_emaildomain'])\nc = a.difference(b)\nprint(c)\nd = b.difference(a)\nprint(d)\n# so, the recipient email domain is actually a subset of the purchaser email\n# domain. There is no secret domain that the recipient uses but not being\n# used by the purchasers. ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations**: A number of email domains have been used for frauds, however, the most popular ones stands out here as well. Frauds committed using gmail domain are more in number than all the other domains combined. A number of email domains have been used for frauds, however, the most popular ones stands out here as well. Frauds committed using gmail domain are more in number than all the other domains combined. After some more analysis, we come to know that the recipient email domain is actually a subset of the purchaser email domain. There is no secret domain that the recipient uses but not being used by the purchasers. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# f. addr1 and addr2 - both are address codes and hence categorical (i.e.\n# mathematical operations shouldn't be performed on them)\nplt.figure(figsize=(18, 15))\nplt.subplot(2, 2, 1)\nfraud_dat['addr1'].value_counts().plot(kind='pie')\nplt.title('addr1 distribution in fraudulent')\n# Using the pie chart, we are able to easily see the most frequently \n# occurring region codes\nplt.subplot(2, 2, 2)\nfraud_dat['addr2'].value_counts().plot(kind='pie')\nplt.title('addr2 distribution in fraudulent')\n# the country code most frequently appearing is '87', in fact almost all\n# fraudulent transactions are from that country. Can contrast with the \n# whole dataset, if '87' still has the highest number of transactions by \n# same proportion\nplt.subplot(2, 2, 3)\nnonfraud_dat['addr1'].value_counts().plot(kind='pie')\nplt.title('addr1 distribution in non-fraudulent')\nplt.subplot(2, 2, 4)\nnonfraud_dat['addr2'].value_counts().plot(kind='pie')\nplt.title('addr2 distribution in non-fraudulent')\n# The distribution is similar to nonfraudulent transactions, just a few \n# addresses have shifted positions which became more prominent in fraudulent\n# transactions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations**: Both addr1 and addr2 are address codes, and hence categorical (i.e. mathematical operations shouldn't be performed on them). Using the pie chart, we are able to easily see the most frequently occurring region codes. The country code most frequently appearing is '87', in fact almost all fraudulent transactions are from that country. Can contrast with the whole dataset, if '87' still has the highest number of transactions by same proportion. The distribution is similar to nonfraudulent transactions, just a few addresses have shifted positions which became more prominent in fraudulent transactions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# g. dist1 and dist2 - both are continuous, however contain a large number\n# of missing values. Could be missing due to privacy or other legal reasons\n# we can still plot the rest of the 'good' records and see distribution\nplt.figure(figsize=(18, 10))\nplt.subplot(2, 2, 1)\nplt.hist(fraud_dat['dist1'].dropna(), alpha=0.5)\nplt.title('dist1 distribution in fraudulent')\n# Again a skewed distribution meaning most 'good' records lie within the \n# range of (0, 1000) of dist1\nplt.subplot(2, 2, 2)\nplt.hist(nonfraud_dat['dist1'].dropna(), alpha=0.5)\nplt.title('dist1 distribution in non-fraudulent')\n# The distribution is almost identical\nplt.subplot(2, 2, 3)\nplt.hist(fraud_dat['dist2'].dropna(), alpha=0.5, color='red')\nplt.title('dist2 distribution in fraudulent')\nplt.subplot(2, 2, 4)\nplt.hist(nonfraud_dat['dist2'].dropna(), alpha=0.5, color='red')\nplt.title('dist2 distribution in non-fraudulent')\n# both look the same as 'dist1'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations**: dist1 and dist2 - both are continuous, however contain a large number of missing values. Could be missing due to privacy or other legal reasons we can still plot the rest of the 'good' records and see distribution. Again in the first plot, a skewed distribution meaning most 'good' records lie within the range of (0, 1000) of dist1. The distribution of dist2 in both fraudulent and non-fradulent cases are almost identical to dist1."},{"metadata":{"trusted":true},"cell_type":"code","source":"# h. DeviceType - a potentially useful categorical variable\nplt.figure(figsize=(18, 5))\nplt.subplot(1, 2, 1)\nprint(fraud_dat['DeviceType'].value_counts())\nfraud_dat['DeviceType'].value_counts().plot(kind='bar', color='yellow')\nplt.title('DeviceType distribution for fraudulent')\n# both the deviceType users are fairly equally distributed\nplt.subplot(1, 2, 2)\nnonfraud_dat['DeviceType'].value_counts().plot(kind='bar')\nplt.title('DeviceType distribution for non-fraudulent')\n# For nonfraudulent, more desktop devices are used as compared to mobile","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations**: DeviceType - a potentially useful categorical variable. Both the deviceType users are fairly equally distributed as seen from the plot. For nonfraudulent, more desktop devices are used as compared to mobile."},{"metadata":{"trusted":true},"cell_type":"code","source":"# i. DeviceInfo - another potentially useful categorical variable\nprint(fraud_dat['DeviceInfo'].value_counts())\n# There are a lot of categories that are hard to visualize, still I would\n# take an attempt\nplt.figure(figsize=(18, 5))\nplt.subplot(1, 2, 1)\nfraud_dat['DeviceInfo'].value_counts().plot(kind='pie')\nplt.title('DeviceInfo distribution in fraudulent')\n# Although a mess, it's clear that Windows and iOS platforms have the largest\n# share, taking more than 50% of total records\nplt.subplot(1, 2, 2)\nnonfraud_dat['DeviceInfo'].value_counts().plot(kind='pie')\nplt.title('DeviceInfo distribution in non-fraudulent')\n# not much difference between both distribution","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations**: There are a lot of categories that are hard to visualize. Although a mess, it's clear that Windows and iOS platforms have the largest share, taking more than 50% of total records for both fraudulent and non-fraudulent transactions."},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2. Analyzing the frequency distribution of transactions by time for the \n# most frequent code\n# first the most frequent country code\n\ndf = trns_join\ndf['addr2'].value_counts()\n# 87 country code has the highest number of transactions\n#df = df.dropna(subset = ['addr2'])\n#df['addr2'] = df['addr2'].astype(int)\ndf = df[df['addr2'] == 87]\n# now dataframe is just composed of transactions from that country\n# we can take a look at the distribution of TransactionDT feature\ndf['TransactionDT'].head(10)\n# we find the time of the place by dividing with 86400\ndf['Time'] = df['TransactionDT']%86400\ndf['Time'].head()\n# now dividing that with 60*60 or 3600 to get in hours\ndf['Time'] = df['Time']/3600\nplt.figure(figsize=(18, 5))\nplt.subplot(1, 2, 1)\nplt.hist(df['Time'], color = 'green', alpha = 0.7)\nplt.title('Distribution of Time variable')\n\n# We can go one step further to see if the fraudulent transactions also\n# follow the same pattern\nplt.subplot(1, 2, 2)\nplt.hist(df['Time'][df['isFraud'] == 1], color = 'teal', alpha = 0.7)\nplt.title('Distribution of Time variable for fraudulent transactions')\n# It also follows the same pattern","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations**: From the first plot, we can see a very interesting result. Most of the transactions happen starting from 15hrs-24hrs and 00hrs-3hrs relative to the reference point. The most likely explanation for this could be that  the region in the middle where the bars drop is the sleeping time in the country. Hence, the minimum number of transactions at those hours. The waking hours could be 12 hrs relative to the reference time, as this is where the plot starts rising. We can go one step further to see if the fraudulent transactions also follow the same pattern. It also follows the same pattern, as evident from the second plot. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# 3. Analyze the ProductCD variable\n\ndf = trns_join\ndf.columns\ndf['ProductCD'].value_counts()\ndf = df[['ProductCD', 'TransactionAmt']]\ndf.boxplot(by='ProductCD')\n# Here, we can see the Product code W has a couple of points that have very\n# high transaction amount value. Those products are the most expensive, which\n# belong to the product code W. However, most of the products with the code\n# W are still far lower in the amount. We can try to create the boxplot again\n# without the obvious outliers\ndf[df['TransactionAmt'] < 25000].boxplot(by='ProductCD')\n# It again gives us the same picture that there are a lot of outliers \n# especially in product code W. We may rather take mean/median of transaction\n# amount for each product code and compare. \n# Actually median could be preferred as it's not impacted by the outliers\n# but it may not capture the whole idea of most or least expensive products\ndf.groupby('ProductCD').agg('mean')\n# Using mean, we see that products with code 'R' and 'W' have the most \n# expensive products. While code 'C' and 'S' have the cheapest products\ndf.groupby('ProductCD').agg('median')\n# Using median, it's obvious that the 'R' has the most expensive products\n# while 'C' products are the cheapest","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Analysis**: Here, we can see the Product code W has a couple of points that have very high transaction amount value. Those products are the most expensive, which belong to the product code W. However, most of the products with the code W are still far lower in the amount. We can try to create the boxplot again without the obvious outliers. It again gives us the same picture that there are a lot of outliers especially in product code W. We may rather take mean/median of transaction amount for each product code for comparison sake. Actually median could be preferred as it's not impacted by the outliers but it may not capture the whole idea of most or least expensive products. Using mean, we see that products with code 'R' and 'W' have the most expensive products while code 'C' and 'S' have the cheapest products. Using median, it's obvious that the 'R' has the most expensive products while 'C' products are the cheapest."},{"metadata":{"trusted":true},"cell_type":"code","source":"# 4. Plot time of day vs transaction amount\ndf = trns_join\ndf['Time'] = df['TransactionDT']%86400\ndf['Time'].head()\n# now dividing that with 60*60 or 3600 to get in hours\ndf['Time'] = df['Time']/3600\nplt.figure(figsize=(15, 5))\nplt.subplot(1, 2, 1)\nplt.scatter(df['Time'], df['TransactionAmt'])\nplt.title('Time vs TransactionAmt')\n# There is a clear outlier - to view the distribution clearly, we will remove\n# the outlier and replot\ndf = df[df['TransactionAmt'] < 25000]\nplt.subplot(1, 2, 2)\nplt.scatter(df['Time'], df['TransactionAmt'])\nplt.title('With outlier removed')\nz = np.polyfit(df['Time'], df['TransactionAmt'], 1)\np = np.poly1d(z)\nplt.plot(df['Time'], p(df['Time']), 'm-')\nplt.title('With the trend line')\nplt.show()\n# Not an interesting correlation can be observed here. The trendline is\n# also quite constant. Let's find out the correlation coefficients\ndf['Time'].corr(df['TransactionAmt'], method = 'pearson')\n# 0.04538\ndf['Time'].corr(df['TransactionAmt'], method = 'spearman')\n# 0.03808\n# The above numbers imply a very weak correlation between the two variables","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Analysis**: In the first plot, we see there is a clear outlier - to view the distribution clearly, we remove the outlier and replot, which is in the next plot. Not an interesting correlation can be observed here. The trendline is also quite constant. Let's find out the correlation coefficients. \n1. Pearson coefficient = 0.04538\n2. Spearman coefficient = 0.03808"},{"metadata":{},"cell_type":"markdown","source":"**TASK**: Let's identify the top 5 regions with highest proportion of frauds in the country with most frequent transactions ('87'), and see if there is anything interesting about it. \nWe come to know that region code 260 has the highest fraud occurence with 33%, however it reflects just 2 frauds out of 6 total transactions. Other top 10 highest fraudulent regions we can see in tabular format."},{"metadata":{"trusted":true},"cell_type":"code","source":"# 5. Any interesting plot\n# Let's identify the top 5 regions with highest proportion of frauds in \n# the country with most frequent transactions ('87')\n\ndf = df[df['addr2'] == 87]\ndf['addr1'].value_counts()\ndf1 = df[['addr1', 'isFraud']]\nx = df1.groupby(['addr1']).agg('count')\nx['addr1'] = x.index\ndf2 = df1[df1['isFraud'] == 1]\ny = df2.groupby(['addr1']).agg('count')\ny.rename(columns = {'isFraud':'Fraud'}, inplace = True)\ny['addr1'] = y.index\n# now join both x and y\nx = x.reset_index(drop=True)\ny = y.reset_index(drop=True)\nxy = x.merge(y, on = 'addr1', how = 'left')\nxy['Fraud'][xy['Fraud'].isna()] = 0\n# now find the proportion of frauds\nxy['fraud_ratio'] = xy['Fraud']/xy['isFraud']\nxy = xy.sort_values('fraud_ratio', ascending = False)\nxy['addr1'] =xy['addr1'].astype(int).astype(str)\nplt.bar(xy['addr1'][:5], xy['fraud_ratio'][:5], color = 'orange')\n\n# Region code 260 has the highest fraud occurence with 33%, however\n# it reflects just 2 frauds out of 6 total transactions. Other top 10\n# highest fraudulent regions we can see in tabular format:\ncols = xy.columns.tolist()\ncols = [cols[1]] + [cols[0]] + cols[2:]\nxy = xy[cols]\nxy.rename(columns = {'isFraud':'Total'}, inplace = True)\nxy = xy.reset_index(drop=True)\nxy.head(10)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}