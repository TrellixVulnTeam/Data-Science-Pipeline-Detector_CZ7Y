{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# data analysis libraries:\nimport numpy as np\nimport pandas as pd\n\n# data visualization libraries:\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# to ignore warnings:\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# to display all columns:\npd.set_option('display.max_columns', None)\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.gridspec as gridspec\n%matplotlib inline\n\n# Standard plotly imports\n\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom plotly.offline import iplot, init_notebook_mode\nimport cufflinks\nimport cufflinks as cf\nimport plotly.figure_factory as ff\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read train and test data with pd.read_csv():\ntrain_id= pd.read_csv(\"/kaggle/input/ieee-fraud-detection/train_identity.csv\")\ntest_id = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/test_identity.csv\")\ntrain_tr = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/train_transaction.csv\")\ntest_tr = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/test_transaction.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_id.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_id.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tr.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_tr.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_id.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_id.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tr.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tr.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_tr.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_und_cat(data,df):\n    target=data[\"isFraud\"]\n    \n    print(\"describtion of data = \",df.describe())\n    print(\"value counts= \",df.value_counts())\n    print(\"total uniq values= \",df.value_counts().count())\n    print(\"total missing values = \",df.isnull().sum())\n    print(\"% missing values = \",df.isnull().sum()/df.value_counts().sum())\n    df.value_counts().plot.barh().set_title(\"Class Frequencies of Variable\");\n    return data    \n    \n        \n        \n        \n    \n   \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_und_num(data,df):\n    target=data[\"isFraud\"]\n    print(\"describtion of data = \",df.describe())\n    print(\"total missing values = \",df.isnull().sum())\n    print(\"% missing values = \",df.isnull().sum()/df.value_counts().sum())\n    return data    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Missing Values","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"*  Missing Values in train_tr DataFrame. % 41 percent data are missing ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_values_count = train_tr.isnull().sum()\nprint (missing_values_count[0:10])\ntotal_cells = np.product(train_tr.shape)\ntotal_missing = missing_values_count.sum()\nprint (\"% of missing data = \",(total_missing/total_cells) * 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Missing Values in train_id DataFrame. % 35 percent data are missing ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_values_count = train_id.isnull().sum()\nprint (missing_values_count[0:10])\ntotal_cells = np.product(train_id.shape)\ntotal_missing = missing_values_count.sum()\nprint(\"total_missing = \",missing_values_count.sum())\nprint (\"% of missing data = \",(total_missing/total_cells) * 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.merge(train_tr, train_id, on = \"TransactionID\",how='left',left_index=True, right_index=True)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test=pd.merge(test_tr, test_id, on = \"TransactionID\",how='left',left_index=True, right_index=True)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Trans_min_mean'] = (train['TransactionAmt'] - train['TransactionAmt'].mean())\ntrain['Trans_min_std'] = train['Trans_min_mean'] / train['TransactionAmt'].std()\ntest['Trans_min_mean'] = test['TransactionAmt'] - test['TransactionAmt'].mean()\ntest['Trans_min_std'] = test['Trans_min_mean'] / test['TransactionAmt'].std()\n\ntrain['TransactionAmt_to_mean_card1'] = train['TransactionAmt'] / train.groupby(['card1'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_mean_card4'] = train['TransactionAmt'] / train.groupby(['card4'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_std_card1'] = train['TransactionAmt'] / train.groupby(['card1'])['TransactionAmt'].transform('std')\ntrain['TransactionAmt_to_std_card4'] = train['TransactionAmt'] / train.groupby(['card4'])['TransactionAmt'].transform('std')\ntest['TransactionAmt_to_mean_card1'] = test['TransactionAmt'] / test.groupby(['card1'])['TransactionAmt'].transform('mean')\ntest['TransactionAmt_to_mean_card4'] = test['TransactionAmt'] / test.groupby(['card4'])['TransactionAmt'].transform('mean')\ntest['TransactionAmt_to_std_card1'] = test['TransactionAmt'] / test.groupby(['card1'])['TransactionAmt'].transform('std')\ntest['TransactionAmt_to_std_card4'] = test['TransactionAmt'] / test.groupby(['card4'])['TransactionAmt'].transform('std')\n\ntrain['id_02_to_mean_card1'] = train['id_02'] / train.groupby(['card1'])['id_02'].transform('mean')\ntrain['id_02_to_mean_card4'] = train['id_02'] / train.groupby(['card4'])['id_02'].transform('mean')\ntrain['id_02_to_std_card1'] = train['id_02'] / train.groupby(['card1'])['id_02'].transform('std')\ntrain['id_02_to_std_card4'] = train['id_02'] / train.groupby(['card4'])['id_02'].transform('std')\ntest['id_02_to_mean_card1'] = test['id-02'] / test.groupby(['card1'])['id-02'].transform('mean')\ntest['id_02_to_mean_card4'] = test['id-02'] / test.groupby(['card4'])['id-02'].transform('mean')\ntest['id_02_to_std_card1'] = test['id-02'] / test.groupby(['card1'])['id-02'].transform('std')\ntest['id_02_to_std_card4'] = test['id-02'] / test.groupby(['card4'])['id-02'].transform('std')\n\ntrain['D15_to_mean_card1'] = train['D15'] / train.groupby(['card1'])['D15'].transform('mean')\ntrain['D15_to_mean_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('mean')\ntrain['D15_to_std_card1'] = train['D15'] / train.groupby(['card1'])['D15'].transform('std')\ntrain['D15_to_std_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('std')\ntest['D15_to_mean_card1'] = test['D15'] / test.groupby(['card1'])['D15'].transform('mean')\ntest['D15_to_mean_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('mean')\ntest['D15_to_std_card1'] = test['D15'] / test.groupby(['card1'])['D15'].transform('std')\ntest['D15_to_std_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('std')\n\n\n\ntrain['TransactionAmt_to_mean_card1'] = train['TransactionAmt'] / train.groupby(['card1'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_mean_card4'] = train['TransactionAmt'] / train.groupby(['card4'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_std_card1'] = train['TransactionAmt'] / train.groupby(['card1'])['TransactionAmt'].transform('std')\ntrain['TransactionAmt_to_std_card4'] = train['TransactionAmt'] / train.groupby(['card4'])['TransactionAmt'].transform('std')\ntest['TransactionAmt_to_mean_card1'] = test['TransactionAmt'] / test.groupby(['card1'])['TransactionAmt'].transform('mean')\ntest['TransactionAmt_to_mean_card4'] = test['TransactionAmt'] / test.groupby(['card4'])['TransactionAmt'].transform('mean')\ntest['TransactionAmt_to_std_card1'] = test['TransactionAmt'] / test.groupby(['card1'])['TransactionAmt'].transform('std')\ntest['TransactionAmt_to_std_card4'] = test['TransactionAmt'] / test.groupby(['card4'])['TransactionAmt'].transform('std')\n\ntrain['TransactionAmt'] = np.log(train['TransactionAmt'])\ntest['TransactionAmt'] = np.log(test['TransactionAmt'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[['P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3']] = train['P_emaildomain'].str.split('.', expand=True)\ntrain[['R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3']] = train['R_emaildomain'].str.split('.', expand=True)\ntest[['P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3']] = test['P_emaildomain'].str.split('.', expand=True)\ntest[['R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3']] = test['R_emaildomain'].str.split('.', expand=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_id, train_tr, test_id, test_tr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = train[\"isFraud\"]\ntrain_TransactionID= train[\"TransactionID\"]\ntest_TransactionID=test[\"TransactionID\"]\ntrain = train.drop([\"TransactionID\"], axis=1)\ntest=test.drop([\"TransactionID\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cat = train.select_dtypes(include=['object'])\ntrain_cat_columns=train_cat.columns\ntrain_cat_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_cat = test.select_dtypes(include=['object'])\ntest_cat_columns=test_cat.columns\ndel test_cat, train_cat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train_cat_columns]=train[train_cat_columns].astype('category')\ntest[test_cat_columns]=test[test_cat_columns].astype(\"category\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nfor i in train_cat_columns: \n    lbe=preprocessing.LabelEncoder()\n    train[i]=lbe.fit_transform(train[i].astype(str))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in test_cat_columns:    \n    test[i]=lbe.fit_transform(test[i].astype(str))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train.fillna(-1,inplace=True)\n#test.fillna(-1,inplace=True)\n#train=train.dropna(axis=1)\n#test=test.dropna(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test[i].value_counts().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_columns=train.columns\n#train_columns=train_columns.drop(\"isFraud\")\n#test.columns=train_columns\n#test.columns\n#for i in train_cat_columns:\n    #if test[i].value_counts()==train[i].value_counts():\n           # test = pd.get_dummies(test, columns = [i])\n            #train=pd.get_dummies(train, columns = [i])\n            \n\n            \n        \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in train:\n    train[i] = train[i].fillna((train[i].min() - 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correlat=train.corrwith( train['isFraud'], method= 'spearman')\ncorrelat1=pd.DataFrame(correlat,columns=[\"corr\"])\ncorrelat1.reset_index()\ncor_features=correlat1.loc[(correlat1.loc[:,\"corr\"] > 0.07)|(correlat1.loc[:,\"corr\"]<-0.07)]\ncor_features=cor_features.T\ncor_features=cor_features.columns\ntrain=train[cor_features]\ncor_features=cor_features.drop(\"isFraud\")\ntest=test[cor_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test=test[cor_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(['isFraud'], axis=1)\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.preprocessing import StandardScaler\n#train = StandardScaler().fit_transform(train)\n#train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# From kernel https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n# WARNING! THIS CAN DAMAGE THE DATA \ndef reduce_mem_usage2(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain = reduce_mem_usage2(train)\ntest = reduce_mem_usage2(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.decomposition import PCA\n#optimum bilese sayisi\n#pca = PCA().fit(train)\n#plt.plot(np.cumsum(pca.explained_variance_ratio_))\n#plt.xlabel(\"Bileşen Sayısını\")\n#plt.ylabel(\"Kümülatif Varyans Oranı\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#final\n#pca = PCA(n_components = 200)\n#train = pca.fit_transform(train)\n#test = pca.fit_transform(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train = pd.DataFrame(data = train)\n#test = pd.DataFrame(data = test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pca.explained_variance_ratio_.sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport seaborn as sns\nfrom sklearn.preprocessing import scale \nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.metrics import roc_auc_score,roc_curve\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names = [\"LogisticRegression\",\"GBM\",\"LightGBM\"]\n    \n    \nclassifiers = [LogisticRegression(), GradientBoostingClassifier(),LGBMClassifier()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def base_models(train):\n    \n  \n    \n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import accuracy_score\n   \n    #Y = train[\"isFraud\"]\n    #X = train.drop(['isFraud'], axis=1)\n\n    X_train, X_test, y_train, y_test = train_test_split(train, Y, \n                                                    test_size = 0.20, \n                                                    random_state = 42)\n    \n    #results = []\n    \n    names = [\"LogisticRegression\",\"GBM\",\"LightGBM\"]\n    \n    \n    classifiers = [LogisticRegression(), GradientBoostingClassifier(),LGBMClassifier()]\n    \n    \n    for name, clf in zip(names, classifiers):\n\n        model = clf.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        acc = accuracy_score(y_test, y_pred)\n        msg = \"%s: %f\" % (name, acc)\n        print(msg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_models(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Y = train[\"isFraud\"]\n#X = train.drop(['isFraud'], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(train, Y, \n                                                    test_size = 0.20, \n                                                    random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom lightgbm import LGBMClassifier\nlgbm_model = LGBMClassifier(\n          num_leaves= 256,\n          min_child_samples= 79,\n          objective= 'binary',\n          max_depth= 13,\n          learning_rate= 0.03,\n          boosting_type= \"gbdt\",\n          subsample_freq= 3,\n          subsample= 0.9,\n          bagging_seed= 11,\n          metric= 'auc',\n          verbosity= -1,\n          reg_alpha= 0.3,\n          reg_lambda= 0.3,\n          colsample_bytree= 0.9).fit(X_train, y_train)\ny_pred = lgbm_model.predict(X_test)\naccuracy_score(y_test, y_pred)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(lgbm_model.feature_importances_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# sorted(zip(clf.feature_importances_, X.columns), reverse=True)\nfeature_imp = pd.DataFrame(sorted(zip(lgbm_model.feature_importances_,X_train.columns)), columns=['Value','Feature'])\n\nplt.figure(figsize=(30, 80))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-01.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ids = test_TransactionID\npredictions = lgbm_model.predict(test)\n\n#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({ \"TransactionID\" : ids, \"isFraud\": predictions })\noutput.to_csv('submission_son.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns=pd.DataFrame(train.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_imp.to_excel(\"output.xlsx\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns.to_excel(\"columns.xlsx\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_imp.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns.columns=[\"Feature\"]\ncolumns.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"proje=pd.merge(columns,feature_imp, on=\"Feature\",how= \"left\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"proje.to_excel(\"columns.xlsx\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}