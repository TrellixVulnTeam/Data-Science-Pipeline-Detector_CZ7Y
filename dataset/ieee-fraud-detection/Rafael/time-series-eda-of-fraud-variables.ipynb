{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Overview\n\nThis notebook will focus on time series EDA on the variables in our dataset. While the original data table is per transaaction basis, we can derive hourly / daily summary of is_fraud and other predictive variables. This has two purposes: \n* (1) Better understanding of underlying variables \n* (2) Use time series metrics as additional features for our prediction model"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm import tqdm_notebook\npd.options.display.precision = 15\n\nimport time\nimport datetime\nimport gc\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport json\n# import altair as alt\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# alt.renderers.enable('notebook')\n\nimport plotly.graph_objs as go\n# import plotly.plotly as py\nimport plotly.offline as pyo\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly_express as px\ninit_notebook_mode(connected=True)\nfrom matplotlib import cm\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\n\nimport plotly_express as px","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading data"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"folder_path = '../input/ieee-fraud-detection//'\ntrain_identity = pd.read_csv(f'{folder_path}train_identity.csv')\ntrain_transaction = pd.read_csv(f'{folder_path}train_transaction.csv')\n# test_identity = pd.read_csv(f'{folder_path}test_identity.csv')\n# test_transaction = pd.read_csv(f'{folder_path}test_transaction.csv')\nsub = pd.read_csv(f'{folder_path}sample_submission.csv')\n# let's combine the data and work with the whole dataset\n# I will save this for later\n# train = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\n# test = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Train Transaction dataset has {train_transaction.shape[0]} rows and {train_transaction.shape[1]} columns.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I also sample down the dataset since this is an EDA and it will accelerate data processing and visualization"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train_transaction = train_transaction.sample(n=10000)\ntrain_transaction.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Train Transaction dataset has {train_transaction.shape[0]} rows and {train_transaction.shape[1]} columns.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create datetime feature\n\nI use some codes from the following notebooks which provides a handy datetime feature creation. Based on their EDA, the notebook also found that the datetime is likely starting at 1-Dec-2017\n* https://www.kaggle.com/fchmiel/day-and-time-powerful-predictive-feature\n* https://www.kaggle.com/kevinbonnes/transactiondt-starting-at-2017-12-01"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def make_day_feature(df, offset=0, tname='TransactionDT'):\n    \"\"\"\n    Creates a day of the week feature, encoded as 0-6. \n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        df to manipulate.\n    offset : float (default=0)\n        offset (in days) to shift the start/end of a day.\n    tname : str\n        Name of the time column in df.\n    \"\"\"\n    # found a good offset is 0.58\n    days = df[tname] / (3600*24)        \n    encoded_days = np.floor(days-1+offset) % 7\n    return encoded_days\n\ndef make_hour_feature(df, tname='TransactionDT'):\n    \"\"\"\n    Creates an hour of the day feature, encoded as 0-23. \n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        df to manipulate.\n    tname : str\n        Name of the time column in df.\n    \"\"\"\n    hours = df[tname] / (3600)        \n    encoded_hours = np.floor(hours) % 24\n    return encoded_hours\n\nSTART_DATE = '2017-12-01'\nstartdate = datetime.datetime.strptime(START_DATE, '%Y-%m-%d')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train_transaction['TransactionDateTime'] = train_transaction['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\ntrain_transaction['TransactionDate'] = [x.date() for x in train_transaction['TransactionDateTime']]\ntrain_transaction['TransactionHour'] = train_transaction.TransactionDT // 3600\ntrain_transaction['TransactionHourOfDay'] = train_transaction['TransactionHour'] % 24\ntrain_transaction['TransactionDay'] = train_transaction.TransactionDT // (3600 * 24)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"train_transaction.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define groups of metrics"},{"metadata":{"trusted":true},"cell_type":"code","source":"trx_colnames = train_transaction.columns\ntrx_colnames_core_num = ['isFraud', 'TransactionAmt', 'card1','card2', 'card3','card5']\ntrx_colnames_core_cat = ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain']\ntrx_colnames_C = [c for c in trx_colnames if c.startswith(\"C\") ]\ntrx_colnames_V = [c for c in trx_colnames if c.startswith(\"V\") ]\ntrx_colnames_M = [c for c in trx_colnames if c.startswith(\"M\") ]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Time Series EDA based on: "},{"metadata":{},"cell_type":"markdown","source":"# Core Metrics - Hourly Time Series"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"agg_dict = {}\nfor col in trx_colnames_core_num:\n    agg_dict[col] = ['mean','sum']\ntrain_trx_hour = train_transaction.groupby(['TransactionHour']).agg(agg_dict).reset_index()\ntrain_trx_hour.columns = ['_'.join(col).strip() for col in train_trx_hour.columns.values]\ntrain_trx_hour.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import math\ndf = train_trx_hour\nplotted_columns = train_trx_hour.columns[1:]\nlencols = len(plotted_columns)\nfig, axes = plt.subplots(math.ceil(lencols//3),3,figsize=(12,lencols))\nfor i, metrics in enumerate(plotted_columns):\n    df.plot(x='TransactionHour_',y=metrics,title=metrics + \" by Hour\",ax=axes[i//3,i%3])\nplt.tight_layout()\nplt.suptitle(\"Core Metrics on Hourly Basis\",y=\"1.05\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Some interesting observations:\n* Card5 mean have a common cap at $225, and the spiky variation tend to be on the negative direction. So card5 is probably a \"Maximum Balance\" / \"Remaining Balance\" type of variables\n* Card3 mean has a common band between 150 and 185\n* Card1 and Card2 mean hugely varied, and they're at different scales"},{"metadata":{},"cell_type":"markdown","source":"# Core Metrics - Daily time series "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"agg_dict = {}\nfor col in trx_colnames_core_num:\n    agg_dict[col] = ['mean','sum']\ntrain_trx_date = train_transaction.groupby(['TransactionDate']).agg(agg_dict).reset_index()\ntrain_trx_date.columns = ['_'.join(col).strip() for col in train_trx_date.columns.values]\ntrain_trx_date.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"pd.plotting.register_matplotlib_converters()\nimport math\ndf = train_trx_date\nplotted_columns = df.columns[1:]\nlencols = len(plotted_columns)\nfig, axes = plt.subplots(math.ceil(lencols//3),3,figsize=(16,lencols))\nfor i, metrics in enumerate(plotted_columns):\n    df.plot(x='TransactionDate_',y=metrics,title=metrics + \" by Date\",ax=axes[i//3,i%3])\nplt.tight_layout()\nplt.suptitle(\"Core Metrics on Date Basis\",y=\"1.05\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Core Metrics - Seasonal Decomposition using Prophet\n\nNext, I will do seasonal decomposition using Facebook's prophet library. Because the data is not yet 2 years, we couldn't extract yearly seasonality. Nonetheless, we are still able to extract the DayOfWeek seasonality, which is quite significant in some metrics. "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from fbprophet import Prophet\nprophet = Prophet()\n\ndf = train_trx_date[['TransactionDate_','isFraud_sum']]\ndf.columns = ['ds','y']\nprophet = Prophet()\nprophet.fit(df)\nforecast = prophet.predict(df)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from fbprophet import Prophet\n\ndef plot_forecast(df_input,metrics):\n    prophet = Prophet()\n    df = df_input[['TransactionDate_',metrics]]\n    df.columns = ['ds','y']\n    prophet.fit(df)\n    forecast = prophet.predict(df)\n    fig,ax = plt.subplots(1,3,figsize=(20,5),sharey=True)\n    forecast.weekly[:7].plot(ax=ax[0])\n    ax[0].set_title(\"weekly component\")\n    forecast.trend.plot(ax=ax[1])\n    ax[1].set_title(\"trend component\")\n#     ax[1].xticks(forecast.ds)\n    ax[2].plot(forecast.yhat)\n    ax[2].plot(df.y)\n    ax[2].set_title(\"comparing fitted vs. actual\")\n    plt.suptitle(metrics + ' based on: Day Of Week, Long-term Trend, Seasonality vs. Actual')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for metrics in plotted_columns:\n    plot_forecast(train_trx_date,metrics)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# C Metrics - Daily time series "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"agg_dict = {}\nfor col in trx_colnames_C:\n    agg_dict[col] = ['mean','sum']\ntrain_trx_date = train_transaction.groupby(['TransactionDate']).agg(agg_dict).reset_index()\ntrain_trx_date.columns = ['_'.join(col).strip() for col in train_trx_date.columns.values]\ntrain_trx_date.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import math\npd.plotting.register_matplotlib_converters()\ndf = train_trx_date\nplotted_columns = df.columns[1:]\nlencols = len(plotted_columns)\nfig, axes = plt.subplots(math.ceil(lencols//3)+1,3,figsize=(16,lencols))\nfor i, metrics in enumerate(plotted_columns):\n    df.plot(x='TransactionDate_',y=metrics,title=metrics + \" by Date\",ax=axes[i//3,i%3])\nplt.tight_layout()\nplt.suptitle(\"Core Metrics on Date Basis\",y=\"1.05\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The spike is clearly the Christmas holiday (25-Dec). Probably removing those holiday dates would help to generalize the training data. I do wonder what these C variables could be, since some of them have very low mean (<10) in all of the days, except during the holiday\n\n#### Some of these variables also demonstrate spikes on several dates. C3 is the most prominent one, because it is usually 0, except for a couple of days. It is probably some kind of indicator of rare event."},{"metadata":{},"cell_type":"markdown","source":"# C Metrics - Seasonal decomposition"},{"metadata":{},"cell_type":"markdown","source":"I will only be taking the time series for data after 1-Jan, since the holiday will really skew the decomposition. Alternatively, I can add in the holiday date into Prophet specification, but I'm not doing that for now"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import datetime\nt = datetime.date(2018,1,1)\ntrain_trx_date2 = train_trx_date[train_trx_date['TransactionDate_']>t].reset_index(drop=True)\ntrain_trx_date2.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"for metrics in plotted_columns:\n    plot_forecast(train_trx_date2,metrics)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# C Metrics - Hour of Day Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"def percentile(n):\n    def percentile_(x):\n        return np.percentile(x, n)\n    percentile_.__name__ = 'percentile_%s' % n\n    return percentile_\n\nagg_dict = {}\nfor col in trx_colnames_core_num:\n    agg_dict[col] = ['mean','sum']\ntrain_trx_HOD = train_transaction.groupby(['TransactionHourOfDay']).agg(agg_dict).reset_index()\ntrain_trx_HOD.columns = ['_'.join(col).strip() for col in train_trx_HOD.columns.values]\ntrain_trx_HOD.head()\n\nagg_dict = {}\nfor col in trx_colnames_core_num:\n    agg_dict[col] = [percentile(25),'median',percentile(75)]\ntrain_trx_HOD2 = train_transaction.groupby(['TransactionHourOfDay']).agg(agg_dict).reset_index()\ntrain_trx_HOD2.columns = ['_'.join(col).strip() for col in train_trx_HOD2.columns.values]\ntrain_trx_HOD2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.plotting.register_matplotlib_converters()\nimport math\ndf = train_trx_HOD\nplotted_columns = df.columns[1:]\nlencols = len(plotted_columns)\nfig, axes = plt.subplots(math.ceil(lencols//2),2,figsize=(16,lencols))\nfor i, metrics in enumerate(plotted_columns):\n    df.plot(x='TransactionHourOfDay_',y=metrics,title=metrics + \" by HourOfDay\",ax=axes[i//2,i%2])\nplt.tight_layout()\nplt.suptitle(\"Core Metrics by HourOfDay\",y=\"1.05\")\nfig.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}