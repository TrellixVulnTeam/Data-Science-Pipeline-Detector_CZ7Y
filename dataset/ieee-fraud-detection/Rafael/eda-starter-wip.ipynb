{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Overview\n\nFirst dip into this dataset! Will do some EDAs to understand the nature of the dataset and variables. I'm using Pandas Profiling to generate quick standard EDA before deep-diving\n\nReference notebooks:\n* https://www.kaggle.com/artgor/eda-and-models: Join trx and cust from the start. Histograms of variables (except V variables). LGB pred\n* https://www.kaggle.com/jazivxt/safe-box: Uses the1owl package for autoML\n* https://www.kaggle.com/jesucristo/fraud-complete-eda: time series analysis"},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"!pip install pandas-profiling","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import NuSVR, SVR\nfrom sklearn.metrics import mean_absolute_error\npd.options.display.precision = 15\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport time\nimport datetime\nfrom catboost import CatBoostRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit\nfrom sklearn import metrics\nfrom sklearn import linear_model\nimport gc\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport eli5\nimport shap\nfrom IPython.display import HTML\nimport json\n# import altair as alt\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# alt.renderers.enable('notebook')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import pandas_profiling\nimport plotly.graph_objs as go\n# import plotly.plotly as py\nimport plotly.offline as pyo\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly_express as px\ninit_notebook_mode(connected=True)\nfrom matplotlib import cm\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\n\nimport plotly_express as px","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"folder_path = '../input/'\ntrain_identity = pd.read_csv(f'{folder_path}train_identity.csv')\ntrain_transaction = pd.read_csv(f'{folder_path}train_transaction.csv')\n# test_identity = pd.read_csv(f'{folder_path}test_identity.csv')\n# test_transaction = pd.read_csv(f'{folder_path}test_transaction.csv')\nsub = pd.read_csv(f'{folder_path}sample_submission.csv')\n# let's combine the data and work with the whole dataset\n# I will save this for later\n# train = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\n# test = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f'Train Transaction dataset has {train_transaction.shape[0]} rows and {train_transaction.shape[1]} columns.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# print(f'Train dataset has {train.shape[0]} rows and {train.shape[1]} columns.')\n# print(f'Test dataset has {test.shape[0]} rows and {test.shape[1]} columns.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA - Train_Transaction"},{"metadata":{},"cell_type":"markdown","source":"Let's quickly check the transaction table"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train_transaction = train_transaction.sample(n=100000)\ntrain_transaction.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"trx_colnames = train_transaction.columns\nfor i in range(len(trx_colnames)):\n    print(i, ': ',trx_colnames[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"trx_colnames_main = trx_colnames[1:17]\ntrx_colnames_CDM = trx_colnames[17:55]\ntrx_colnames_V = trx_colnames[55:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check out pandas profiling report below. This library generates a summary of dataset, including number of null, distribution of values, correlation, etc. It also suggests which variables should be dropped due to high correlation. I am commenting it out because it consumes tons of memory, so I'm running it offline one-time."},{"metadata":{"trusted":true},"cell_type":"code","source":"# profiler1 = train_transaction[trx_colnames_main].profile_report()\n# profiler1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# profiler2 = train_transaction[trx_colnames_CDM].profile_report()\n# profiler2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA - Transaction - Key Takeaways from Pandas Profiler\n* C: The variables here are highly correlated\n* D: A lot of zeros or missing to deal with\n* M: Half is missing! \n* A lot of them are highlyg skewed, or having a very long tail to the right\n"},{"metadata":{},"cell_type":"markdown","source":"# Util to check memory consumption"},{"metadata":{},"cell_type":"markdown","source":"This is a useful function to check the memory consumption size of our python objects:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\n\n# These are the usual ipython objects, including this one you are creating\nipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n\n# Get a sorted list of the objects and their sizes\nsorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Variable: Cs"},{"metadata":{"trusted":true},"cell_type":"code","source":"trx_colnames_C = [c for c in trx_colnames if c.startswith(\"C\") ]\ntrx_colnames_C","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"trxC = train_transaction[trx_colnames_C]\ntrxClog = np.log1p(trxC)\nfig, ax = plt.subplots(15,3,figsize=(10,20))\nfor i,c in enumerate(trxC.columns):\n    trxC[c].hist(ax=ax[i,0])\n    trxClog[c].hist(ax=ax[i,1])\n    trxClog[c].hist(ax=ax[i,2],cumulative=True,density=True)\nplt.tight_layout()\nplt.suptitle('Distribution of C variables - C, log(C), cumulative log(C)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"X = trxClog\npca = PCA(n_components=len(trx_colnames_C)).fit(X)\n#Plotting the Cumulative Summation of the Explained Variance\nexpvar=np.cumsum(pca.explained_variance_ratio_)\ndata = [go.Scatter(y=expvar)]\nlayout = {'title': 'Review PCA Explained Variance to determine number of components'}\niplot({'data':data,'layout':layout})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"pca = PCA(n_components=4)\nXPCA = pca.fit_transform(X)\nNc = range(1,10)\nkmeans = [KMeans(i) for i in Nc]\nscore = [kmeans[i].fit(XPCA).score(XPCA) for i in range(len(kmeans))]\ndata = [go.Scatter(y=score,x=list(Nc))]\nlayout = {'title':'Review Elbow Curve to determine number of clusters for KMeans'}\niplot({'data':data,'layout':layout})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from yellowbrick.features.pca import PCADecomposition\nvisualizer = PCADecomposition(scale=True)\nvisualizer.fit_transform(trxClog)\nvisualizer.poof()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Variable: Vs"},{"metadata":{"trusted":true},"cell_type":"code","source":"trx_colnames_V = [c for c in trx_colnames if c.startswith(\"V\") ]\ntrx_colnames_V","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"trxV = train_transaction[trx_colnames_V]\ntrxVlog = np.log1p(trxV)\nfig, ax = plt.subplots(15,3,figsize=(10,20))\nfor i,c in enumerate(trxV.columns[:15]):\n    trxV[c].hist(ax=ax[i,0],bins=50)\n    trxVlog[c].hist(ax=ax[i,1],bins=50)\n    trxVlog[c].hist(ax=ax[i,2],bins=50,cumulative=True,density=True,histtype='step')\nplt.tight_layout()\nplt.suptitle('Distribution of V variables - V, log(V), cumulative log(V)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is too much na. Need to strategize around na. May make more sense to convert the V into categorical, and then do embedding."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"X = trxV.fillna(0)\npca = PCA(n_components=10).fit(X)\n#Plotting the Cumulative Summation of the Explained Variance\nexpvar=np.cumsum(pca.explained_variance_ratio_)\ndata = [go.Scatter(y=expvar)]\nlayout = {'title': 'Review PCA Explained Variance to determine number of components'}\niplot({'data':data,'layout':layout})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from yellowbrick.features.pca import PCADecomposition\nvisualizer = PCADecomposition(scale=True)\nvisualizer.fit_transform(trxV.fillna(0))\nvisualizer.poof()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We don't need to do log transformation"},{"metadata":{},"cell_type":"markdown","source":"# Datetime analysis"},{"metadata":{},"cell_type":"markdown","source":"Reference: \n* https://www.kaggle.com/fchmiel/day-and-time-powerful-predictive-feature\n* https://www.kaggle.com/kevinbonnes/transactiondt-starting-at-2017-12-01"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def make_day_feature(df, offset=0, tname='TransactionDT'):\n    \"\"\"\n    Creates a day of the week feature, encoded as 0-6. \n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        df to manipulate.\n    offset : float (default=0)\n        offset (in days) to shift the start/end of a day.\n    tname : str\n        Name of the time column in df.\n    \"\"\"\n    # found a good offset is 0.58\n    days = df[tname] / (3600*24)        \n    encoded_days = np.floor(days-1+offset) % 7\n    return encoded_days\n\ndef make_hour_feature(df, tname='TransactionDT'):\n    \"\"\"\n    Creates an hour of the day feature, encoded as 0-23. \n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        df to manipulate.\n    tname : str\n        Name of the time column in df.\n    \"\"\"\n    hours = df[tname] / (3600)        \n    encoded_hours = np.floor(hours) % 24\n    return encoded_hours\n\nSTART_DATE = '2017-12-01'\nstartdate = datetime.datetime.strptime(START_DATE, '%Y-%m-%d')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train_transaction['TransactionDateTime'] = train_transaction['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\ntrain_transaction['TransactionDate'] = [x.date() for x in train_transaction['TransactionDateTime']]\n# train_transaction['TransactionDateHour'] = train_transaction['TransactionDateTime'].date()\ntrain_transaction['TransactionHour'] = train_transaction.TransactionDT // 3600\ntrain_transaction['TransactionHourOfDay'] = train_transaction['TransactionHour'] % 24\ntrain_transaction['TransactionDay'] = train_transaction.TransactionDT // (3600 * 24)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"agg_dict = {}\nfor col in trx_colnames_C:\n    agg_dict[col] = ['mean','sum']\nprint(agg_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_trx_hour = train_transaction.groupby(['TransactionHour']).agg(agg_dict).reset_index()\ntrain_trx_hour.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_trx_hour.columns.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_trx_hour.columns = ['_'.join(col).strip() for col in train_trx_hour.columns.values]\ntrain_trx_hour.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_trx_hour.columns = ['TrxHourAgg_' + col for col in train_trx_hour.columns.values]\ntrain_trx_hour.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_trx_hour.DShour = pd.to_datetime(train_trx_hour.TransactionHour)\ntrain_trx_hour.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_trx_day = train_transaction.groupby(['TransactionDay'])[\"isFraud\",\"TransactionAmt\"].mean().reset_index()\ntrain_trx_day.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.line(train_trx_hour.iloc[:1000,:],x=\"TransactionHour\",y=\"isFraud\",title='Average Fraud Rate by Hour')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_trx_date = train_transaction.groupby(['TransactionDate']).agg({'isFraud':['mean','sum'],'TransactionAmt':['count','mean','sum']}).reset_index()\ntrain_trx_date.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_trx_date.columns = train_trx_date.columns.get_level_values(0) + '_' + train_trx_date.columns.get_level_values(1)\ntrain_trx_date.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.line(train_trx_date,x=\"TransactionDate_\",y=\"isFraud_sum\",title=\"Total Frauds by Date\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.scatter(train_trx_date,x=\"TransactionAmt_count\",y=\"isFraud_mean\",trendline='lowess',title=\"Relationship between Transaction Amount and Fraud Rate\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.scatter(train_trx_date,x=\"TransactionAmt_count\",y=\"isFraud_sum\",trendline='lowess',title='Relationship between Transaction Amount and Number of Frauds (abs)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_trx_date[\"DayOfWeek\"] = [x.weekday() for x in train_trx_date.TransactionDate_]\ntrain_trx_date[\"DayOfWeek\"] = train_trx_date[\"DayOfWeek\"].apply(str)\ntrain_trx_date.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.scatter(train_trx_date,x=\"TransactionAmt_count\",y=\"isFraud_mean\",color='DayOfWeek',marginal_y='box',trendline='ols',title=\"Fraud rate vs. Transaction Amount\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.scatter(train_trx_date,x=\"TransactionAmt_count\",y=\"isFraud_sum\",color='DayOfWeek',marginal_y='box',trendline='ols',title=\"Fraud Count vs. Transaction Amount\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_trx_day['TransactionDay'].dtype\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datetime.date.fromordinal(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from fbprophet import Prophet\ndf = train_trx_date[['TransactionDate_','isFraud_sum']]\ndf.columns = ['ds','y']\nprophet = Prophet()\nprophet.fit(df)\nforecast = prophet.predict(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"forecast.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig,ax = plt.subplots(1,3,figsize=(20,5))\nforecast.weekly[:7].plot(ax=ax[0],ylim=(-2,40))\nax[0].set_title(\"weekly component\")\nforecast.trend.plot(ax=ax[1],ylim=(-2,40))\nax[1].set_title(\"trend component\")\n# ax[1].xticks(forecast.ds)\nax[2].plot(forecast.yhat)\nax[2].plot(df.y)\nax[2].set_ylim(-2,40)\nax[2].set_title(\"comparing fitted vs. actual\")\nplt.suptitle('Avg Fraud Count based on: Day Of Week, Long-term Trend, Seasonality vs. Actual')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df = train_trx_date[['TransactionDate_','isFraud_mean']]\ndf.columns = ['ds','y']\nprophet = Prophet()\nprophet.fit(df)\nforecast = prophet.predict(df)\n\nfig,ax = plt.subplots(1,3,figsize=(20,5))\nforecast.weekly[:7].plot(ax=ax[0],ylim=(-0.01,0.09))\nax[0].set_title(\"weekly component\")\nforecast.trend.plot(ax=ax[1],ylim=(-0.01,0.09))\nax[1].set_title(\"trend component\")\nax[2].plot(forecast.yhat)\nax[2].plot(df.y)\nax[2].set_ylim(-0.01,0.09)\nax[2].set_title(\"comparing fitted vs. actual\")\nplt.suptitle('Avg Fraud Rate based on: Day Of Week, Long-term Trend, Seasonality vs. Actual')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fraud Rate have relative small weekday seasonality, compared to overall variability"},{"metadata":{},"cell_type":"markdown","source":"# This is still very early. To be continued!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}