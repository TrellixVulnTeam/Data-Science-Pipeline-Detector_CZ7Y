{"cells":[{"metadata":{},"cell_type":"markdown","source":"## IEEE Fraud Detection competition\nIn this kernel I work with [IEEE Fraud Detection competition](https://www.kaggle.com/c/ieee-fraud-detection?rvi=1)\n\n\n# 1. Overview\n\nCheck Kaggle Data Description [IEEE Fraud Detection competition Data Description](https://www.kaggle.com/c/ieee-fraud-detection/data)\n\n# 1-1. Goal\n\n> In this competition you are predicting the probability that an online transaction is fraudulent, as denoted by the binary target isFraud\n\n# 1-2. Overview and Data Description\n- 온라인 사기 거래를 탐지하는 Competition으로 사기거래 데이터에는 `isFraud`에 1로 표시\n- 데이터는 두개의 파일로 나누어져 있다. `identity`와 `transaction`은 `TransactionID`로 매핑시킬 수 있다. 하지만 모든 거래에 신분정보가 있는 것은 아님으로 주의할 것. transaction에 비해 identity에는 24% 정도의 trasactionID만 존재\n- `TransactionDT`은 실제 timestamp가 아님\n- 데이터에 대한 더 많은 정보는 `Lynn@Vesta`가 Discussion에 올려준 리스트를 참고함 [Data Description (Details and Discussion)](https://www.kaggle.com/c/ieee-fraud-detection/discussion/101203)\n- train_identity 테이블에는 총 41개의 컬럼과 train_transaction 테이블에는 393개의 컬럼이 있음\n\n\n## (1). Tansaction Table\n\n|  \t| Transaction Table \t| Is Categorical Feature \t|\n|--------------------------\t|---------------------------------------------------------------------------------------------------------------------------\t|------------------------\t|\n| TransactionDT \t| timedelta from a given reference datetime (not an actual timestamp) \t| False \t|\n| TransactionAMT \t| transaction payment amount in USD \t| False \t|\n| ProductCD \t| product code, the product for each transaction \t| True \t|\n| card1 - card6 \t| payment card information, such as card type, card category, issue bank, country, etc. \t| True \t|\n| addr \t| address \t| True \t|\n| dist \t| distance \t| False \t|\n| P_ and (R__) emaildomain \t| purchaser and recipient email domain \t| True \t|\n| C1-C14 \t| counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked. \t| False \t|\n| D1-D15 \t| timedelta, such as days between previous transaction, etc. \t| False \t|\n| M1-M9 \t| match, such as names on card and address, etc. \t| True \t|\n| Vxxx \t| Vesta engineered rich features, including ranking, counting, and other entity relations. \t| False \t|\n\n\n\n\n<br>\n<br>\n\n\n\n## (2). Identity Table\n\n\n----\n\n> Variables in this table are identity information - network connection information (IP, IPS, Proxy, etc) and digital signature (UA/Browser/os/version, etc) associated with transactions.<br>They're collected by Vesta's fraud protection system and digital security partners.<br>(The field names are masked and pairwise dictionary will not be provided for privacy protection and contract agreement) \n\n\n\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Following refer\n- (1) https://www.kaggle.com/jesucristo/fraud-complete-eda\n- (2) https://www.kaggle.com/nroman/eda-for-cis-fraud-detection\n- (3) https://www.kaggle.com/kabure/extensive-eda-and-modeling-xgb-hyperopt\n- (4) https://www.kaggle.com/c/ieee-fraud-detection/discussion/100167#latest-577688","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport math\nfrom scipy import stats\nimport time\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import NuSVR, SVR\nfrom sklearn.metrics import mean_absolute_error\npd.options.display.precision = 15\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport time\nimport datetime\nfrom catboost import CatBoostRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit\nfrom sklearn import metrics\nfrom sklearn import linear_model\nimport gc\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport eli5\nimport shap\nfrom IPython.display import HTML\nimport json\nimport altair as alt\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. EDA\n## Load Data","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nimport pandas as pd\ntest_identity = pd.read_csv('/kaggle/input/ieee-fraud-detection/test_identity.csv')\ntrain_identity = pd.read_csv('/kaggle/input/ieee-fraud-detection/train_identity.csv')\ntest_transaction = pd.read_csv('/kaggle/input/ieee-fraud-detection/test_transaction.csv')\nsample_submission = pd.read_csv('/kaggle/input/ieee-fraud-detection/sample_submission.csv')\ntrain_transaction = pd.read_csv('/kaggle/input/ieee-fraud-detection/train_transaction.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def check_dataframe(df):\n    \n    # excution time\n    start_time = time.time()\n\n    dict_check_value = {}\n    list_check_value = []\n    total_rows = len(df)\n    for col in df.columns:\n    #     print(col)\n    \n        value_count_base = df[col].value_counts()\n        value_count_index = list(value_count_base.index)\n        value_count_value = list(value_count_base.values)\n        null_value = df[col].isnull().sum()\n        unique_value = len(value_count_index)\n        unique_value_exam = value_count_index[:5]\n#         unique_value_exam = df[col][~df[col].isnull()].unique()\n#         unique_value = len(unique_value_exam)\n#         unique_value_exam = unique_value_exam[:5]\n        value_type = df[col].dtype\n    \n        # include null value\n        in_null_entropy = round(stats.entropy(df[col].value_counts()/total_rows), 4)\n        \n        # except null value\n        except_null_entropy = round(stats.entropy(df[col].value_counts()/total_rows-null_value), 4)\n\n\n        list_check_value = [unique_value, null_value, unique_value_exam, value_type, in_null_entropy, except_null_entropy]\n\n\n        dict_check_value[col] = list_check_value\n    \n    new_df = pd.DataFrame.from_dict(dict_check_value, orient='index', columns=['uniques', 'missing', 'values_exam_top5', 'dtypes', 'total_entropy', 'exp_null_entropy'])\n    \n    print(f'The Excution Time(minutes) is {(time.time()-start_time)/60}')\n    return new_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# temp_df = train_transaction[:1000].copy(deep=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### References Study:\n- (1) scipy.stats.entropy : [About Entropy](https://datascienceschool.net/view-notebook/d3ecf5cc7027441c8509c0cae7fea088/)\n- (2) heatmap : [Better Heatmaps and Correlation Matrix Plots in Python](https://towardsdatascience.com/better-heatmaps-and-correlation-matrix-plots-in-python-41445d0f2bec)\n- (3) missng value : [missingno github](https://github.com/ResidentMario/missingno), [How to Handle Missing Data](https://towardsdatascience.com/how-to-handle-missing-data-8646b18db0d4)","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":true},"cell_type":"code","source":"train_transaction_chk = check_dataframe(train_transaction)\ntrain_transaction_chk","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"'....'.join([str(k)+\" Data Types: \"+str(v)+'' for (k, v) in train_transaction_chk.groupby('dtypes').size().to_dict().items()])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Exploratory Transaction Table\n\n- Data Type 체크\n> int64 Data Types: 4....float64 Data Types: 376....object Data Types: 14\n\n\n\n**위와 같은 결과에 따라, 상대적으로 적은 갯수를 차지하는 Object부터 살펴볼 것**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 2-1. Object Feature","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"\n# Object Data Types\n\n\n# basic_df \n# cross_check_df\n# columns\n# target columns\n\n\ndef check_obj_col(df, column):\n    \"\"\"\n    # From: https://www.kaggle.com/kabure/extensive-eda-and-modeling-xgb-hyperopt\n    parameter : df, column\n    df(dataframe): Reference Dataframe \n    column(string): column to be based on\n    \"\"\"\n    total = len(df)\n    df[col] = df[column].fillna(\"Miss\")\n    cross_df = pd.crosstab(df[column], df['isFraud'], normalize='index')*100\n    cross_df = cross_df.reset_index()\n    cross_df.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n\n    plt.figure(figsize=(14,10))\n    subtile_str = '{} Distributions'.format(column)\n    plt.suptitle(subtile_str, fontsize=22)\n    plt.subplot(221)\n    g = sns.countplot(x=column, data=df)\n    \n    g.set_title(subtile_str, fontsize=19)\n    g.set_xlabel(\"{} Name\".format(column), fontsize=17)\n    g.set_ylabel(\"Count\", fontsize=17)\n    g.set_ylim(0,500000)\n    for p in g.patches:\n        height = p.get_height()\n        g.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.2f}%'.format(height/total*100),\n                ha=\"center\", fontsize=14)\n        \n    plt.subplot(222)\n    g1 = sns.countplot(x=column, hue='isFraud', data=df)\n    plt.legend(title='Fraud', loc='best', labels=['No', 'Yes'])\n    gt = g1.twinx()\n    \n    order_xcol = [t.get_text()  for t in g.get_xticklabels()]\n    \n    gt = sns.pointplot(x=column, y='Fraud', data=cross_df, color='black', order=order_xcol, legend=False)\n    gt.set_ylabel(\"% of Fraud Transactions\", fontsize=16)\n\n    g1.set_title(\"{} by Target(isFraud)\".format(column), fontsize=19)\n    g1.set_xlabel(\"{} Name\".format(column), fontsize=17)\n    g1.set_ylabel(\"Count\", fontsize=17)\n\n    plt.subplot(212)\n    g3 = sns.boxenplot(x=column, y='TransactionAmt', hue='isFraud', \n                  data=df[df['TransactionAmt'] <= 2000] )\n    g3.set_title(\"Transaction Amount Distribuition by {} and Target\".format(column), fontsize=20)\n    g3.set_xlabel(\"{} Name\".format(column), fontsize=17)\n    g3.set_ylabel(\"Transaction Values\", fontsize=17)\n    \n    plt.subplots_adjust(hspace = 0.6, top = 0.85)\n    \n    return plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"object_col = [col for col in list(train_transaction_chk[train_transaction_chk['dtypes']=='object'].index) if col not in ['P_emaildomain', 'R_emaildomain']]\n# print(object_col) # unique 개수가 너무 많은 컬럼 제외\n\n# check_obj_col(train_transaction, column='ProductCD')\n\n# From: https://www.kaggle.com/kabure/extensive-eda-and-modeling-xgb-hyperopt\nfor col in object_col:\n    check_obj_col(train_transaction, column=col)\n    \n    \nprint(object_col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* `ProductCD` : **W**가 70%이상을 차지하고, 다른 4개의 item들은 10%전후로 거의 비슷하다고 할 수 있다. 단, **C**가 Fraud비율은 가장 높다.\n* `card4` : **Visa**가 60%이상을 차지하며, 그 뒤로는 mastercard가 뒤를 잇는다. 그 외 discover, american express, Miss가 비슷한 비율로 나타났다. Fraud의 비율은 discover가 가장 높았다. \n* `card6` : **debit**이 70%이상으로 가장 많은 부분을 차지하고, 그 다음으로 credit으로 25%, debit or credit, change card, Miss 부분은 3%이하 부분을 차지한다. debit이 훨씬 많은 비율을 차지하지만, credit이 Fraud 비율이 가장 높다. \n* `M1` : T, F, miss로 True랑 Miss 부분이 엇비슷하고, Fraud비율은 Miss가 높음\n* `M2` : T, F, miss, True랑 Miss 부분이 엇비슷, F비율이 6%로 가장 낮다. 하지만 Miss가 Fraud 비율이 가장 높고, False Fraud 비율이 그 다음으로 높다.\n* `M3` : T, F, miss, True랑 Miss 엇비슷하고, False 비율은 11%이다. Fraud 비율은 Miss, Fals순이다. \n* `M4` : M2, M0, Miss, M1,Fraud 비율이 M2가 가장 높음\n* `M5`: Miss가 50% 이상이고, False, True가 비슷한 비율을 차지한다. Fraud 비율은 True랑 Miss가 비슷","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train_transaction_chk[(train_transaction_chk.index == 'P_emaildomain') | (train_transaction_chk.index == 'R_emaildomain')]\n# num_col_chk = [col for col in train_transaction_chk[train_transaction_chk['dtypes']!='object'].index if col not in ['TransactionID', 'isFraud', 'TransactionDT']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train_transaction[['P_emaildomain', 'R_emaildomain']].head()\n# NaN, NaN\n# value, NaN\n# value, value\n# NaN, value","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* P_emaildomain 보다 R_emaindomain이 기입이 안되어 있는 경우가 많다.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"total = len(train_transaction)\nprint(f'p_emaildomain은 {(total-94456)/total}%로 값이 존재')\nprint(f'r_emaildomain은 {(total-453249)/total}%로 값이 존재')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"\n\n# P is Null, R is Not Null\ntrain_transaction[(train_transaction['P_emaildomain'].isnull()) & (train_transaction['R_emaildomain'].notnull())].groupby('isFraud').size()\np_null_r_notnull = np.array(train_transaction[(train_transaction['P_emaildomain'].isnull()) & (train_transaction['R_emaildomain'].notnull())].groupby('isFraud').size())\n\ntrain_transaction[(train_transaction['P_emaildomain'].notnull()) & (train_transaction['R_emaildomain'].isnull())].groupby('isFraud').size()\np_notnull_r_null = np.array(train_transaction[(train_transaction['P_emaildomain'].notnull()) & (train_transaction['R_emaildomain'].isnull())].groupby('isFraud').size())\n\ntrain_transaction[(train_transaction['P_emaildomain'].isnull()) & (train_transaction['R_emaildomain'].isnull())].groupby('isFraud').size()\np_null_r_null = np.array(train_transaction[(train_transaction['P_emaildomain'].isnull()) & (train_transaction['R_emaildomain'].isnull())].groupby('isFraud').size())\n\ntrain_transaction[(train_transaction['P_emaildomain'].notnull()) & (train_transaction['R_emaildomain'].notnull())].groupby('isFraud').size()\np_notnull_r_notnull = np.array(train_transaction[(train_transaction['P_emaildomain'].notnull()) & (train_transaction['R_emaildomain'].notnull())].groupby('isFraud').size())\n\nemaildomain_df = pd.DataFrame(np.stack((p_null_r_notnull, p_notnull_r_null, p_null_r_null, p_notnull_r_notnull)), columns=['Normal', 'isFraud'], index=['p_null_r_notnull', 'p_notnull_r_null', 'p_null_r_null', 'p_notnull_r_notnull'])\n\nemaildomain_df['Normal_ratio'], emaildomain_df['isFraud_ratio'] = list(zip(*emaildomain_df.apply(lambda x: (x['Normal']/total, x['isFraud']/total), axis=1)))\nemaildomain_df['isFraud_ratio'] = emaildomain_df.apply(lambda x: x['isFraud']/(x['Normal']+x['isFraud']), axis=1)\n\nfraud_total = len(train_transaction[train_transaction['isFraud']==1])\nemaildomain_df['fraudRatio_per_Fraud'] = emaildomain_df.apply(lambda x: x['isFraud']/fraud_total, axis=1)\nemaildomain_df","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# from https://www.kaggle.com/jesucristo/fraud-complete-eda\nfig, ax = plt.subplots(1, 3, figsize=(32,10))\n\nsns.countplot(y=\"P_emaildomain\", ax=ax[0], data=train_transaction)\nax[0].set_title('P_emaildomain', fontsize=14)\nsns.countplot(y=\"P_emaildomain\", ax=ax[1], data=train_transaction.loc[train_transaction['isFraud'] == 1])\nax[1].set_title('P_emaildomain isFraud = 1', fontsize=14)\nsns.countplot(y=\"P_emaildomain\", ax=ax[2], data=train_transaction.loc[train_transaction['isFraud'] == 0])\nax[2].set_title('P_emaildomain isFraud = 0', fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# from https://www.kaggle.com/jesucristo/fraud-complete-eda\nfig, ax = plt.subplots(1, 3, figsize=(32,10))\n\nsns.countplot(y=\"R_emaildomain\", ax=ax[0], data=train_transaction)\nax[0].set_title('R_emaildomain', fontsize=14)\nsns.countplot(y=\"R_emaildomain\", ax=ax[1], data=train_transaction.loc[train_transaction['isFraud'] == 1])\nax[1].set_title('R_emaildomain isFraud = 1', fontsize=14)\nsns.countplot(y=\"R_emaildomain\", ax=ax[2], data=train_transaction.loc[train_transaction['isFraud'] == 0])\nax[2].set_title('R_emaildomain isFraud = 0', fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 사기행각을 벌일 때, 기입하는 이메일은 우리가 일반적으로 사용하는 Email을 기입한다는 것을 알 수 있다. ","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train_transaction[(train_transaction['P_emaildomain'].notnull()) & (train_transaction['R_emaildomain'].notnull()) & (train_transaction['isFraud']==1)].apply(lambda x: (x['P_emaildomain']+'-'+x['R_emaildomain']), axis=1).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train_transaction[(train_transaction['P_emaildomain'].notnull()) & (train_transaction['R_emaildomain'].notnull()) & (train_transaction['isFraud']==0)].apply(lambda x: (x['P_emaildomain']+'-'+x['R_emaildomain']), axis=1).value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* p_emaildomain은 거의 데이터가 있다. \n* p_emaildomain이 값이 있을 때, r_emaildomain이 Null값인 경우가 가장 많지만, Fraud 비율이 가장 높은건 p_emaildomin이 기입되어 있고, r_emaildomain도 기입되어 있는 경우이다.\n\n> `일반적인 Email 사용`에서 사기행각이 많이 일어나는 것과, p_email이 있는 경우 사기행각 비율이 높은 것고, `특히 r_emaildomain도 있는 경우에 사기행각 비율이 높은 것`으로 봐서, emaildomain은 위조되기 쉬운 것으로 보인다.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 2-2. TimeDelta Feature\n* 사기행각이 시간에 영향을 받는지 알아본다\n\n[Reference Kaggle](https://www.kaggle.com/kabure/extensive-eda-and-modeling-xgb-hyperopt)","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/kabure/extensive-eda-and-modeling-xgb-hyperopt \ndef ploting_cnt_amt(df, col, lim=2000):\n    tmp = pd.crosstab(df[col], df['isFraud'], normalize='index') * 100\n    tmp = tmp.reset_index()\n    tmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n    \n    plt.figure(figsize=(16,14))    \n    plt.suptitle(f'{col} Distributions ', fontsize=24)\n    \n    plt.subplot(211)\n    g = sns.countplot( x=col,  data=df, order=list(tmp[col].values))\n    gt = g.twinx()\n    gt = sns.pointplot(x=col, y='Fraud', data=tmp, order=list(tmp[col].values),\n                       color='black', legend=False, )\n    gt.set_ylim(0,tmp['Fraud'].max()*1.1)\n    gt.set_ylabel(\"%Fraud Transactions\", fontsize=16)\n    g.set_title(f\"Most Frequent {col} values and % Fraud Transactions\", fontsize=20)\n    g.set_xlabel(f\"{col} Category Names\", fontsize=16)\n    g.set_ylabel(\"Count\", fontsize=17)\n    g.set_xticklabels(g.get_xticklabels(),rotation=45)\n    sizes = []\n    for p in g.patches:\n        height = p.get_height()\n        sizes.append(height)\n        g.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.2f}%'.format(height/total*100),\n                ha=\"center\",fontsize=12) \n        \n    g.set_ylim(0,max(sizes)*1.15)\n    \n    #########################################################################\n    perc_amt = (df.groupby(['isFraud',col])['TransactionAmt'].sum() \\\n                / df.groupby([col])['TransactionAmt'].sum() * 100).unstack('isFraud')\n    perc_amt = perc_amt.reset_index()\n    perc_amt.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n    amt = df.groupby([col])['TransactionAmt'].sum().reset_index()\n    perc_amt = perc_amt.fillna(0)\n    plt.subplot(212)\n    g1 = sns.barplot(x=col, y='TransactionAmt', \n                       data=amt, \n                       order=list(tmp[col].values))\n    g1t = g1.twinx()\n    g1t = sns.pointplot(x=col, y='Fraud', data=perc_amt, \n                        order=list(tmp[col].values),\n                       color='black', legend=False, )\n    g1t.set_ylim(0,perc_amt['Fraud'].max()*1.1)\n    g1t.set_ylabel(\"%Fraud Total Amount\", fontsize=16)\n    g.set_xticklabels(g.get_xticklabels(),rotation=45)\n    g1.set_title(f\"{col} by Transactions Total + %of total and %Fraud Transactions\", fontsize=20)\n    g1.set_xlabel(f\"{col} Category Names\", fontsize=16)\n    g1.set_ylabel(\"Transaction Total Amount(U$)\", fontsize=16)\n    g1.set_xticklabels(g.get_xticklabels(),rotation=45)    \n    \n    for p in g1.patches:\n        height = p.get_height()\n        g1.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.2f}%'.format(height/total_amt*100),\n                ha=\"center\",fontsize=12) \n        \n    plt.subplots_adjust(hspace=.4, top = 0.9)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/c/ieee-fraud-detection/discussion/100400#latest-579480\nimport datetime\n\nSTART_DATE = '2017-12-01'\nstartdate = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\")\ntrain_transaction[\"Date\"] = train_transaction['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\n\ntrain_transaction['_Hours'] = train_transaction['Date'].dt.hour\n\ntrain_transaction['_Weekdays'] = train_transaction['Date'].dt.dayofweek","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# total_amt = train_transaction.groupby(['isFraud'])['TransactionAmt'].sum().sum()\ntotal_amt = train_transaction['TransactionAmt'].sum()\nploting_cnt_amt(train_transaction, '_Hours')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"ploting_cnt_amt(train_transaction, '_Weekdays')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 특정시간에 그 시간대의 카운트에 비해 사기행각이 높아지는 것을 볼 수 있다\n* 요일별로 살펴보았을 때 약간의 차이는 있지만 차이는 많이 나지 않음을 볼 수 있다. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction.drop(['_Weekdays', 'Date'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2-3. Numeric Feature\n* 거의 대부분이 `Numeric` Data에 해당","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# print(train_transaction_chk[train_transaction_chk['dtypes']=!'object'])\nprint('Numeric Columns {} in train_transaction'.format(len(train_transaction.select_dtypes(['float', 'int']).columns)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(train_transaction.select_dtypes(['float', 'int']).columns)\n# TransactionID, isFraud, TransactionDT, TransactionAmt,_Hours 제외","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eda_col_list = [col for col in list(train_transaction.select_dtypes(['float', 'int']).columns) if col not in ['TransactionID', 'isFraud', 'TransactionDT', '_Hours', 'TransactionAmt']]\n# sns.pairplot(train_transaction[eda_col_list], kind=\"scatter\", hue='type', corner=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":true},"cell_type":"code","source":"train_transaction_chk.loc[eda_col_list, :] # dfObj.loc[ 'b' , : ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"train_transaction_chk.loc[eda_col_list, :].apply(lambda x: (total-x['missing'])/total, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Numeric Data를 살펴보고, NaN값의 비율도 살펴본다\n* int Value는 `card1`뿐, 나머지는 Float 형식을 띔\n* `card1`과 `C*`는 값이 모두 있다. \n* `card*`도 거의 값이 있고, `addr*`도 많이 값이 있음\n* `dist*`는 값이 50% 없고, `V*`는 값이 거의 있는 것부터 10%만 값이 있는 경우로 다양하다","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# This code from https://www.kaggle.com/nroman/eda-for-cis-fraud-detection\n# def plot_numerical(feature):\n#     \"\"\"\n#     Plot some information about a numerical feature for both train and test set.\n#     Args:\n#         feature (str): name of the column in DataFrame\n#     \"\"\"\n#     fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(16, 18))\n#     sns.kdeplot(train[feature], ax=axes[0][0], label='Train');\n#     sns.kdeplot(test[feature], ax=axes[0][0], label='Test');\n\n#     sns.kdeplot(train[train['isFraud']==0][feature], ax=axes[0][1], label='isFraud 0')\n#     sns.kdeplot(train[train['isFraud']==1][feature], ax=axes[0][1], label='isFraud 1')\n\n#     test[feature].index += len(train)\n#     axes[1][0].plot(train[feature], '.', label='Train');\n#     axes[1][0].plot(test[feature], '.', label='Test');\n#     axes[1][0].set_xlabel('row index');\n#     axes[1][0].legend()\n#     test[feature].index -= len(train)\n\n#     axes[1][1].plot(train[train['isFraud']==0][feature], '.', label='isFraud 0');\n#     axes[1][1].plot(train[train['isFraud']==1][feature], '.', label='isFraud 1');\n#     axes[1][1].set_xlabel('row index');\n#     axes[1][1].legend()\n\n#     pd.DataFrame({'train': [train[feature].isnull().sum()], 'test': [test[feature].isnull().sum()]}).plot(kind='bar', rot=0, ax=axes[2][0]);\n#     pd.DataFrame({'isFraud 0': [train[(train['isFraud']==0) & (train[feature].isnull())][feature].shape[0]],\n#                   'isFraud 1': [train[(train['isFraud']==1) & (train[feature].isnull())][feature].shape[0]]}).plot(kind='bar', rot=0, ax=axes[2][1]);\n\n#     fig.suptitle(feature, fontsize=18);\n#     axes[0][0].set_title('Train/Test KDE distribution');\n#     axes[0][1].set_title('Target value KDE distribution');\n#     axes[1][0].set_title('Index versus value: Train/Test distribution');\n#     axes[1][1].set_title('Index versus value: Target distribution');\n#     axes[2][0].set_title('Number of NaNs');\n#     axes[2][1].set_title('Target value distribution among NaN values');\n    \n\ndef plot_numerical(train, test, feature):\n    \"\"\"\n    Fix a little code from https://www.kaggle.com/nroman/eda-for-cis-fraud-detection\n    Plot some information about a numerical feature for both train and test set.\n    Args:\n        train : train DataFrame\n        test : test Dataframe\n        feature (str): name of the column in DataFrame\n    \"\"\"\n    fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(16, 18))\n    sns.kdeplot(train[feature], ax=axes[0][0], label='Train');\n    sns.kdeplot(test[feature], ax=axes[0][0], label='Test');\n\n    sns.kdeplot(train[train['isFraud']==0][feature], ax=axes[0][1], label='isFraud 0')\n    sns.kdeplot(train[train['isFraud']==1][feature], ax=axes[0][1], label='isFraud 1')\n\n    test[feature].index += len(train)\n    axes[1][0].plot(train[feature], '.', label='Train');\n    axes[1][0].plot(test[feature], '.', label='Test');\n    axes[1][0].set_xlabel('row index');\n    axes[1][0].legend()\n    test[feature].index -= len(train)\n\n    axes[1][1].plot(train[train['isFraud']==0][feature], '.', label='isFraud 0');\n    axes[1][1].plot(train[train['isFraud']==1][feature], '.', label='isFraud 1');\n    axes[1][1].set_xlabel('row index');\n    axes[1][1].legend()\n\n    pd.DataFrame({'train': [train[feature].isnull().sum()], 'test': [test[feature].isnull().sum()]}).plot(kind='bar', rot=0, ax=axes[2][0]);\n    pd.DataFrame({'isFraud 0': [train[(train['isFraud']==0) & (train[feature].isnull())][feature].shape[0]],\n                  'isFraud 1': [train[(train['isFraud']==1) & (train[feature].isnull())][feature].shape[0]]}).plot(kind='bar', rot=0, ax=axes[2][1]);\n\n    fig.suptitle(feature, fontsize=18);\n    axes[0][0].set_title('Train/Test KDE distribution');\n    axes[0][1].set_title('Target value KDE distribution');\n    axes[1][0].set_title('Index versus value: Train/Test distribution');\n    axes[1][1].set_title('Index versus value: Target distribution');\n    axes[2][0].set_title('Number of NaNs');\n    axes[2][1].set_title('Target value distribution among NaN values');\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`card*`는 카테고리컬 데이터라고 쓰여있지만 value type은 Int와 Float였다. Unique 갯수도 높은것으로 봐서, 실제 Numeric 데이터로 봐야할지를 살펴봐야 함","execution_count":null},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# https://stackoverflow.com/questions/11350770/select-by-partial-string-from-a-pandas-dataframe\n# Error가 나므로 이후 수정 필요\nfor col in [col for col in train_transaction.columns if ('card' in col) & (col not in ['card4', 'card6'])]:\n    print(col)\n    try:\n        plot_numerical(train_transaction, test_transaction, col)\n    except ValueError as e:\n        print(e, col)\n        continue","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_numerical(train_transaction, test_transaction, 'card1')\n# https://www.kaggle.com/c/ieee-fraud-detection/discussion/100340#latest-578626","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_numerical(train_transaction, test_transaction, 'card2')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_numerical(train_transaction, test_transaction, 'card5')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Target value에 따른, Feature의 isnull 값의 비율\n\n# temp_cols = [col for col in train_transaction.columns if ('card' in col) & (col not in ['card4', 'card6'])]\n\ntemp_arr = np.empty([0, 3])\n\n# np.array(pd.crosstab(train_transaction['card5'].fillna('missing'), train_transaction['isFraud'].fillna('missing'), margins=True).loc['missing', :])\nfor col in train_transaction.columns:\n    print(col)\n    try:\n        new_arr = np.array(pd.crosstab(train_transaction[col].fillna('missing'), train_transaction['isFraud'].fillna('missing'), margins=True).loc['missing', :])\n    except KeyError:\n        new_arr = np.array([0, 0, 0])\n#     print(pd.crosstab(train_transaction[col].fillna('missing'), train_transaction['isFraud'].fillna('missing'), margins=True))\n#     print(np.array(pd.crosstab(train_transaction[col].fillna('missing'), train_transaction['isFraud'].fillna('missing'), margins=True).loc['missing', :]))\n    temp_arr = np.vstack((temp_arr, new_arr))\n\nnull_df = pd.DataFrame(temp_arr, columns=['isFraud0', 'isFraud1', 'Null_Total'], index=train_transaction.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\ndef fraudNull_perTotalNull(a, b):\n    val = a/b*100\n    if math.isnan(val):\n        val = 0\n    return val\n# null_df.apply(lambda x: x['isFraud1']/x['Null_Total']*100, axis=1)\nnull_df['fraudNull_perTotalNull'] = null_df.apply(lambda x: fraudNull_perTotalNull(x['isFraud1'], x['Null_Total']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# null값은 의미가 있을까?\n# Null값중 Fraud가 차지하는 비율\nnull_df['fraudNull_perTotalFraud'] = null_df.apply(lambda x: np.ceil(x['isFraud1']/fraud_total*100), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction_chk = pd.concat([null_df[['isFraud1', 'fraudNull_perTotalNull', 'fraudNull_perTotalFraud']], train_transaction_chk], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"train_transaction_chk.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"train_transaction_chk","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`C*` Columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"[col for col in train_transaction.columns if ('C' in col) & ('ProductCD' not in col)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_numerical(train_transaction, test_transaction, 'C1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_numerical(train_transaction, test_transaction, 'C2')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_numerical(train_transaction, test_transaction, 'C6')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_numerical(train_transaction, test_transaction, 'C9')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_numerical(train_transaction, test_transaction, 'C11')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_numerical(train_transaction, test_transaction, 'C13')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_numerical(train_transaction, test_transaction, 'C14')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* `D*` Columns","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"for col in [col for col in train_transaction.columns if ('D' in col) & (col not in ['TransactionID', 'TransactionDT', 'ProductCD', 'Date'])]:\n    try:\n        plot_numerical(train_transaction, test_transaction, col)\n    except ValueError:\n        print(\"{} Could not convert data to an integer.\".format(col))\n        continue\n    except Exception as e:\n        print(\"error\", e)\n        continue","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set columns\neda_col_list = [col for col in list(train.columns) if col not in ['id', 'fiberID', 'type']]\n\n# plot\nfig, ax = plt.subplots(figsize=(12, 15), sharex=True)\nsns.despine(left=True)\n\nnum_fig = len(eda_col_list)\nncols_fig = 3\nnrows_fig = math.ceil(num_fig/ncols_fig)\n\ngs = gridspec.GridSpec(nrows_fig, ncols_fig)\n\nfig.legend([plt.plot([], [], c=c)[0] for c in new_colors], unique_labels, loc='upper right', bbox_to_anchor=(1.2, 0.5))\n\n\nfor i, n in enumerate(range(num_fig)):\n  # if i < 5:\n    ax = fig.add_subplot(gs[n])\n\n  # Plot a kernel density estimate and rug plot\n    sns.distplot(df[eda_col_list[n]], hist=False, rug=True)\n\n    ax.set_title(eda_col_list[n])\n  # else:\n    # break\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set columns\neda_col_list = [col for col in list(train.columns) if col not in ['type']]\n\n# Box Plot\nnum_fig = len(eda_col_list)\nncols_fig = 4\nnrows_fig = math.ceil(num_fig/4)\ngs = gridspec.GridSpec(nrows_fig, ncols_fig)\nfig, axs = plt.subplots(figsize=(15,20))\nred_square = dict(markerfacecolor='r', marker='s')\n\nfor n in range(num_fig):\n    ax = fig.add_subplot(gs[n])\n    ax.boxplot(df[eda_col_list[n]], flierprops=red_square, notch='True',patch_artist=True)\n    ax.set_title(eda_col_list[n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import itertools\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\nimport math\nimport numpy as np\n\n# ! pip3 install colorspacious\nfrom matplotlib import cm\nfrom colorspacious import cspace_converter\nfrom collections import OrderedDict\n\n\neda_col_list = [col for col in train.columns if col not in ['fiberID', 'type', 'id']]\n\nprint(eda_col_list)\n\nunique_labels = list(train['type'].unique())\n\n# for Color\n# https://stackoverflow.com/questions/53283813/legend-in-separate-subplot-and-grid\nlabel2idx = {val: i for i, val in enumerate(unique_labels)}\nnew_colors = ['C'+str(label2idx[label]) for label in unique_labels]\n\n# plot\nfig, ax = plt.subplots(figsize=(12, 15))\n\nnum_fig = len(eda_col_list)\nncols_fig = 3\nnrows_fig = math.ceil(num_fig/ncols_fig)\n\n# gs = gridspec.GridSpec(rows, cols)\ngs = gridspec.GridSpec(nrows_fig, ncols_fig)\n# figure = plt.figure()\n# plt.clf()\n\nfig.legend([plt.plot([], [], c=c)[0] for c in new_colors], unique_labels, loc='upper right')\n\n\nfor i, n in enumerate(range(num_fig)):\n  # if i < 5:\n  ax = fig.add_subplot(gs[n])\n      ax.scatter(train.id, train[eda_col_list[n]], c=['C'+str(label2idx[label]) for label in train.type.values], cmap=plt.cm.RdYlGn)\n  # ax.text(train.id+.03, train[eda_col_list[n]]+.03, train['type'])\n  ax.set_title(eda_col_list[n])\n  # else:\n  #   break\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colormap = plt.cm.viridis\nplt.figure(figsize=(25,25))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_transaction.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# len(train_transaction_chk[train_transaction_chk['dtypes']!='object'].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_transaction.select_dtypes('object').columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_transaction.select_dtypes(['float', 'int']).columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction_chk[train_transaction_chk['dtypes']=='int64']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# max_val로 numeric value 체크\ntrain_transaction[[col for col in train_transaction_chk[train_transaction_chk['dtypes']!='object'].index if col not in ['TransactionID', 'isFraud', 'TransactionDT']]].max()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"num_col_chk = [col for col in train_transaction_chk[train_transaction_chk['dtypes']!='object'].index if col not in ['TransactionID', 'isFraud', 'TransactionDT']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(train_transaction[num_col_chk].max().values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# avg - max\n# avg - std\n# median - iqr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.percentile(train_transaction[num_col_chk], q = 75)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction[num_col_chk].agg([np.percentile(75)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction[num_col_chk].describe(percentiles =[0.75, 0.25])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def percentile(n):\n    def percentile_(x):\n        return np.quantile(n)\n    percentile_.__name__ = 'percentile_{:2.0f}'.format(n*100)\n    return percentile_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction[num_col_chk].quantile([.25, .75])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction[num_col_chk].quantile([.75-.25])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction[num_col_chk].agg(['quantile'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction[num_col_chk].quantile([.25, .75]).T.apply(lambda x: x[0.75]-x[0.25], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.quantile()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scatter_df = train_transaction[num_col_chk].agg(['max', 'mean']).T\ng = sns.scatterplot(x='mean', y='max', data=scatter_df)\n\n\n\nfor i in range(len(scatter_df)):\n    \n    g.text(scatter_df['mean'][i], scatter_df['max'][i], scatter_df.index[i], rotation=45)\n\n\n\n# (idea) 각 fraud와 non-fraud에서 갖는 값들을 비교.. 일정이상 차이가 나는 것과 안나는 것과 다른 labeling\n## 참고 : https://seaborn.pydata.org/generated/seaborn.scatterplot.html","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction_chk[train_transaction_chk.index=='V160']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction['V160'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction['card1'].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfor i in range(len(scatter_df)):\n#     if i > 10:\n#         break\n#     print(i)\n#     print(scatter_df['max'][i])\n#     print(scatter_df['mean'][i])\n#     print(scatter_df.index[i])\n    \n    g.text(scatter_df['mean'][i], scatter_df['max'][i], scatter_df.index[i])\n\n    \n    \n# for p in g.patches:\n#     height = p.get_height()\n#     g.text(p.get_x()+p.get_width()/2., height + 3, '{:1.2f}%'.format(height/total*100),\n#           ha=\"center\", fontsize=10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction[num_col_chk].agg(['max', 'mean']).T.plot.scatter(x='mean', y='max',c='DarkBlue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction[num_col_chk].agg(['std', 'mean']).T.plot.scatter(x='std', y='mean',c='DarkBlue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction[num_col_chk].std().max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction[num_col_chk].std().min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction[num_col_chk].max().max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# max_val로 numeric value 체크\nsns.distplot(train_transaction[[col for col in train_transaction_chk[train_transaction_chk['dtypes']!='object'].index if col not in ['TransactionID', 'isFraud', 'TransactionDT']]].max().values, hist=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction_chk[train_transaction_chk['dtypes']=='int64'].index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Using From this kernael : https://www.kaggle.com/artgor/eda-and-models\nimport os\nimport time\nimport datetime\nimport json\nimport gc\nfrom numba import jit\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm_notebook\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nfrom sklearn import metrics\n\nfrom itertools import product\n\nimport altair as alt\nfrom altair.vega import v5\nfrom IPython.display import HTML\n\n# using ideas from this kernel: https://www.kaggle.com/notslush/altair-visualization-2018-stackoverflow-survey\n\n# using ideas from this kernel: https://www.kaggle.com/notslush/altair-visualization-2018-stackoverflow-survey\ndef prepare_altair():\n    \"\"\"\n    Helper function to prepare altair for working.\n    \"\"\"\n\n    vega_url = 'https://cdn.jsdelivr.net/npm/vega@' + v5.SCHEMA_VERSION\n    vega_lib_url = 'https://cdn.jsdelivr.net/npm/vega-lib'\n    vega_lite_url = 'https://cdn.jsdelivr.net/npm/vega-lite@' + alt.SCHEMA_VERSION\n    vega_embed_url = 'https://cdn.jsdelivr.net/npm/vega-embed@3'\n    noext = \"?noext\"\n    \n    paths = {\n        'vega': vega_url + noext,\n        'vega-lib': vega_lib_url + noext,\n        'vega-lite': vega_lite_url + noext,\n        'vega-embed': vega_embed_url + noext\n    }\n    \n    workaround = f\"\"\"    requirejs.config({{\n        baseUrl: 'https://cdn.jsdelivr.net/npm/',\n        paths: {paths}\n    }});\n    \"\"\"\n    \n    return workaround\n\n\n\n\n\ndef add_autoincrement(render_func):\n    # Keep track of unique <div/> IDs\n    cache = {}\n    def wrapped(chart, id=\"vega-chart\", autoincrement=True):\n        if autoincrement:\n            if id in cache:\n                counter = 1 + cache[id]\n                cache[id] = counter\n            else:\n                cache[id] = 0\n            actual_id = id if cache[id] == 0 else id + '-' + str(cache[id])\n        else:\n            if id not in cache:\n                cache[id] = 0\n            actual_id = id\n        return render_func(chart, id=actual_id)\n    # Cache will stay outside and \n    return wrapped\n\n\n\n\n@add_autoincrement\ndef render(chart, id=\"vega-chart\"):\n    \"\"\"\n    Helper function to plot altair visualizations.\n    \"\"\"\n    chart_str = \"\"\"\n    <div id=\"{id}\"></div><script>\n    require([\"vega-embed\"], function(vg_embed) {{\n        const spec = {chart};     \n        vg_embed(\"#{id}\", spec, {{defaultStyle: true}}).catch(console.warn);\n        console.log(\"anything?\");\n    }});\n    console.log(\"really...anything?\");\n    </script>\n    \"\"\"\n    return HTML(\n        chart_str.format(\n            id=id,\n            chart=json.dumps(chart) if isinstance(chart, dict) else chart.to_json(indent=None)\n        )\n    )\n    \n    \ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage(deep=True).sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                c_prec = df[col].apply(lambda x: np.finfo(x).precision).max()\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max and c_prec == np.finfo(np.float32).precision:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n\n\n@jit\ndef fast_auc(y_true, y_prob):\n    \"\"\"\n    fast roc_auc computation: https://www.kaggle.com/c/microsoft-malware-prediction/discussion/76013\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_true = y_true[np.argsort(y_prob)]\n    nfalse = 0\n    auc = 0\n    n = len(y_true)\n    for i in range(n):\n        y_i = y_true[i]\n        nfalse += (1 - y_i)\n        auc += y_i * nfalse\n    auc /= (nfalse * (n - nfalse))\n    return auc\n\n\ndef eval_auc(y_true, y_pred):\n    \"\"\"\n    Fast auc eval function for lgb.\n    \"\"\"\n    return 'auc', fast_auc(y_true, y_pred), True\n\n\ndef group_mean_log_mae(y_true, y_pred, types, floor=1e-9):\n    \"\"\"\n    Fast metric computation for this competition: https://www.kaggle.com/c/champs-scalar-coupling\n    Code is from this kernel: https://www.kaggle.com/uberkinder/efficient-metric\n    \"\"\"\n    maes = (y_true-y_pred).abs().groupby(types).mean()\n    return np.log(maes.map(lambda x: max(x, floor))).mean()\n\n\n\ndef train_model_regression(X, X_test, y, params, folds=None, model_type='lgb', eval_metric='mae', columns=None, plot_feature_importance=False, model=None,\n                               verbose=10000, early_stopping_rounds=200, n_estimators=50000, splits=None, n_folds=3):\n    \"\"\"\n    A function to train a variety of regression models.\n    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n    \n    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: y - target\n    :params: folds - folds to split data\n    :params: model_type - type of model to use\n    :params: eval_metric - metric to use\n    :params: columns - columns to use. If None - use all columns\n    :params: plot_feature_importance - whether to plot feature importance of LGB\n    :params: model - sklearn model, works only for \"sklearn\" model type\n    \n    \"\"\"\n    columns = X.columns if columns is None else columns\n    X_test = X_test[columns]\n    splits = folds.split(X) if splits is None else splits\n    n_splits = folds.n_splits if splits is None else n_folds\n    \n    # to set up scoring parameters\n    metrics_dict = {'mae': {'lgb_metric_name': 'mae',\n                        'catboost_metric_name': 'MAE',\n                        'sklearn_scoring_function': metrics.mean_absolute_error},\n                    'group_mae': {'lgb_metric_name': 'mae',\n                        'catboost_metric_name': 'MAE',\n                        'scoring_function': group_mean_log_mae},\n                    'mse': {'lgb_metric_name': 'mse',\n                        'catboost_metric_name': 'MSE',\n                        'sklearn_scoring_function': metrics.mean_squared_error}\n                    }\n\n    \n    result_dict = {}\n    \n    # out-of-fold predictions on train data\n    oof = np.zeros(len(X))\n    \n    # averaged predictions on train data\n    prediction = np.zeros(len(X_test))\n    \n    # list of scores on folds\n    scores = []\n    feature_importance = pd.DataFrame()\n    \n    # split and train on folds\n    for fold_n, (train_index, valid_index) in enumerate(splits):\n        if verbose:\n            print(f'Fold {fold_n + 1} started at {time.ctime()}')\n        if type(X) == np.ndarray:\n            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n            y_train, y_valid = y[train_index], y[valid_index]\n        else:\n            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n            \n        if model_type == 'lgb':\n            model = lgb.LGBMRegressor(**params, n_estimators = n_estimators, n_jobs = -1)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n            \n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=verbose, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n        \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            \n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n            print('')\n            \n            y_pred = model.predict(X_test).reshape(-1,)\n        \n        if model_type == 'cat':\n            model = CatBoostRegressor(iterations=20000,  eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n                                      loss_function=metrics_dict[eval_metric]['catboost_metric_name'])\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test)\n        \n        oof[valid_index] = y_pred_valid.reshape(-1,)\n        if eval_metric != 'group_mae':\n            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n        else:\n            scores.append(metrics_dict[eval_metric]['scoring_function'](y_valid, y_pred_valid, X_valid['type']))\n\n        prediction += y_pred    \n        \n        if model_type == 'lgb' and plot_feature_importance:\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction /= n_splits\n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    result_dict['oof'] = oof\n    result_dict['prediction'] = prediction\n    result_dict['scores'] = scores\n    \n    if model_type == 'lgb':\n        if plot_feature_importance:\n            feature_importance[\"importance\"] /= n_splits\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n            \n            result_dict['feature_importance'] = feature_importance\n        \n    return result_dict\n\n\n\n\n\ndef train_model_classification(X, X_test, y, params, folds, model_type='lgb', eval_metric='auc', columns=None, plot_feature_importance=False, model=None,\n                               verbose=10000, early_stopping_rounds=200, n_estimators=50000, splits=None, n_folds=3, averaging='usual', n_jobs=-1):\n    \"\"\"\n    A function to train a variety of classification models.\n    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n    \n    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: y - target\n    :params: folds - folds to split data\n    :params: model_type - type of model to use\n    :params: eval_metric - metric to use\n    :params: columns - columns to use. If None - use all columns\n    :params: plot_feature_importance - whether to plot feature importance of LGB\n    :params: model - sklearn model, works only for \"sklearn\" model type\n    \n    \"\"\"\n    columns = X.columns if columns is None else columns\n    n_splits = folds.n_splits if splits is None else n_folds\n    X_test = X_test[columns]\n    \n    # to set up scoring parameters\n    metrics_dict = {'auc': {'lgb_metric_name': eval_auc,\n                        'catboost_metric_name': 'AUC',\n                        'sklearn_scoring_function': metrics.roc_auc_score},\n                    }\n    \n    result_dict = {}\n    if averaging == 'usual':\n        # out-of-fold predictions on train data\n        oof = np.zeros((len(X), 1))\n\n        # averaged predictions on train data\n        prediction = np.zeros((len(X_test), 1))\n        \n    elif averaging == 'rank':\n        # out-of-fold predictions on train data\n        oof = np.zeros((len(X), 1))\n\n        # averaged predictions on train data\n        prediction = np.zeros((len(X_test), 1))\n\n    \n    # list of scores on folds\n    scores = []\n    feature_importance = pd.DataFrame()\n    \n    # split and train on folds\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n        print(f'Fold {fold_n + 1} started at {time.ctime()}')\n        if type(X) == np.ndarray:\n            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n            y_train, y_valid = y[train_index], y[valid_index]\n        else:\n            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n            \n        if model_type == 'lgb':\n            model = lgb.LGBMClassifier(**params, n_estimators=n_estimators, n_jobs = n_jobs)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n            \n            y_pred_valid = model.predict_proba(X_valid)[:, 1]\n            y_pred = model.predict_proba(X_test, num_iteration=model.best_iteration_)[:, 1]\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=n_estimators, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=verbose, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n        \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            \n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n            print('')\n            \n            y_pred = model.predict_proba(X_test)\n        \n        if model_type == 'cat':\n            model = CatBoostClassifier(iterations=n_estimators, eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n                                      loss_function=Logloss)\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test)\n        \n        if averaging == 'usual':\n            \n            oof[valid_index] = y_pred_valid.reshape(-1, 1)\n            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n            \n            prediction += y_pred.reshape(-1, 1)\n\n        elif averaging == 'rank':\n                                  \n            oof[valid_index] = y_pred_valid.reshape(-1, 1)\n            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n                                  \n            prediction += pd.Series(y_pred).rank().values.reshape(-1, 1)        \n        \n        if model_type == 'lgb' and plot_feature_importance:\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction /= n_splits\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    result_dict['oof'] = oof\n    result_dict['prediction'] = prediction\n    result_dict['scores'] = scores\n    \n    if model_type == 'lgb':\n        if plot_feature_importance:\n            feature_importance[\"importance\"] /= n_splits\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n            \n            result_dict['feature_importance'] = feature_importance\n            result_dict['top_columns'] = cols\n        \n    return result_dict\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nimport pandas as pd\ntest_identity = pd.read_csv('/kaggle/input/ieee-fraud-detection/test_identity.csv')\ntrain_identity = pd.read_csv('/kaggle/input/ieee-fraud-detection/train_identity.csv')\ntest_transaction = pd.read_csv('/kaggle/input/ieee-fraud-detection/test_transaction.csv')\nsample_submission = pd.read_csv('/kaggle/input/ieee-fraud-detection/sample_submission.csv')\ntrain_transaction = pd.read_csv('/kaggle/input/ieee-fraud-detection/train_transaction.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 컬럼과 PK 확인","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"check_dataframe(train_transaction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction['V335'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# unique()는 nan을 포함한다\nlen(train_transaction['V335'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# value_counts()는 nan을 포함 하지 않는다. \nlen(train_transaction['V335'].value_counts().index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def resumetable(df):\n    \n    ## Take This Function From https://www.kaggle.com/kabure/extensive-eda-and-modeling-xgb-hyperopt\n    print(f\"Dataset Shape: {df.shape}\")\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    \n    \n    ## Add new\n    for name in df.columns:\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print('{} Columns ---> {}\\n'.format('train_identity', len(test_identity.columns)))\n# print(train_identity.info())\n# train_identity.head()\n\nprint('{} Columns ---> {}\\n'.format('train_transaction', len(test_transaction.columns)))\n# print(train_transaction.info())\n# train_transaction.head()\n\nprint(\"<Compare TransactinId>\")\n# TransactionID로 매핑이 된다고 했지만.. identity 정보가 훨씬 적음\nprint('train_transaction table -> Unique Of transactionId {} '.format(train_transaction['TransactionID'].nunique()))\nprint('train_identity table -> Unique Of transactionId {}'.format(train_identity['TransactionID'].nunique()))\nprint(str(math.floor(train_identity['TransactionID'].nunique()/train_transaction['TransactionID'].nunique()*100))+'%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`train_identity Table`은 `train_transaction Table`에 비해 많은 **TransactionID**를 가지고 있지 않다.\n* chek1. `Unique Of TransactionId` 개수가 다른데 이들을 그냥 join 하는지에 대한 여부 확인","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### NaN값 확인","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# missing value count\ndef missing_values_count(df):\n    missing_values_count = df.isnull().sum()\n    total_cells = np.product(df.shape) # ((590540, 394) -> (590540 * 394))\n    total_missing = missing_values_count.sum()\n    return \"% of missing data = \",(total_missing/total_cells) * 100\n\nprint(missing_values_count(train_transaction))\nprint(missing_values_count(train_identity))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Imbalanced Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"summary = pd.DataFrame(train_transaction.dtypes, columns=['dtypes'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary = summary.reset_index()\nsummary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary['Name'] = summary['index']\nsummary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary = summary[['Name', 'dtypes']]\nsummary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary.isnull().sum().values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary = summary[['Name', 'dtypes']]\n\nsummary['Missing'] = train_transaction.isnull().sum().values\nsummary['Uniques'] = train_transaction.nunique().values\nsummary['First Value'] = train_transaction.loc[0].values\nsummary['Second Value'] = train_transaction.loc[1].values\nsummary['Third Value'] = train_transaction.loc[2].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for name in summary['Name'].value_counts().index:\n    print(name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# stats.entropy \n# scipy.stats.entropy(pk, qk=None, base=None, axis=0)[source]\n# Calculate the entropy of a distribution for given probability values.\n# About Entropy [entropy](https://datascienceschool.net/view-notebook/d3ecf5cc7027441c8509c0cae7fea088/)\n\nimport scipy.stats as stats\n\nfor name in summary['Name']:\n    print(name)\n#     summary.loc[summary['Name'] == name, 'Entropy']\n    print(round(stats.entropy(train_transaction[name].value_counts(normalize=True), base=2), 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from scipy.stats import entropy\nimport scipy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TransactionID로 두 테이블을 조인\ntrain = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\n\n# del train_identity, train_transaction, test_transaction, test_identity\n\ntest = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'There are {train.isnull().any().sum()} columns in train dataset with missing values')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Value check each columns\n\none_value_cols = [col for col in train.columns if train[col].nunique() <= 1]\none_value_cols_test = [col for col in test.columns if test[col].nunique() <= 1]\nprint(one_value_cols == one_value_cols_test)\nprint(one_value_cols)\nprint(one_value_cols_test)\n# ['a', 'b'] ==  ['b', 'a'] 도 False가 나오므로 다른 방법 비교 필요","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Exploration","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.merge(train_transaction, train_identity, on='TransactionID', how='inner')['TransactionID'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'{train_identity.shape}')\nprint(f'{train_transaction.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 각 값들의 unique한 값과 null값을 얼마나 포함하고 있는지 확인\n# 이들을 그래프로 그려보자\nprint(train_identity.columns)\nprint(train_transaction.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def check_dataframe(df):\n\n    dict_check_value = {}\n    list_check_value = []\n    for col in df.columns:\n    #     print(col)\n        null_value = df[col].isnull().sum()\n        unique_value_exam = df[col][~df[col].isnull()].unique()\n        unique_value = len(unique_value_exam)\n        unique_value_exam = unique_value_exam[:5]\n        value_type = df[col].dtype\n\n\n        list_check_value = [unique_value, null_value, unique_value_exam, value_type]\n\n\n        dict_check_value[col] = list_check_value\n    \n    new_df = pd.DataFrame.from_dict(dict_check_value, orient='index', columns=['unique_value', 'isNullcnt', 'value_exam', 'value_type'])\n    return new_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"check_identity = check_dataframe(train_identity)\ncheck_transaction = check_dataframe(train_transaction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"check_identity.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"check_transaction.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# identity 부터 살펴보면 \ncheck_identity['value_type'].value_counts().to_frame()\ncheck_identity['value_type'].value_counts().keys()\ncheck_identity['value_type'].value_counts().tolist() # https://stackoverflow.com/questions/35523635/extract-values-in-pandas-value-counts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 각 테이블은 어떠한 데이터타입의 데이터로 이루어져 있는지\ndef split_ValueCounts(a, b):\n    return [a, b]\n\ndef make_String(stat_df):\n    \"\"\"\n    Using Statistics DataFrame\n    \"\"\"\n    total = len(stat_df.index)\n    stat_list = stat_df['value_type'].value_counts().to_frame().reset_index().apply(lambda x: split_ValueCounts(x['index'], x['value_type']), axis=1)\n    stat_str = ','.join([(str(x[0])+\"는 \"+str(x[1])+'개('+str(math.floor(x[1]/total*100))+'%)') for x in stat_list])\n    return f'{total}개 컬럼 중 {stat_str}이다.'\n\nprint(make_String(check_identity))\nprint(make_String(check_transaction))\n# aa= check_identity['value_type'].value_counts().to_frame().reset_index().apply(lambda x: split_value(x['index'], x['value_type']), axis=1)\n\nprint(make_String(check_dataframe(train)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- indentity 테이블에는 Object형의 데이터도 17개 존재\n- transaction 테이블에는 대부분 Float형태의 데이터이면서 object의 형태의 데이터도 3%정도 존재한다. \n\n\n의미있는 데이터는 무엇일지, 어떤 데이터를 버려야할지를 찾을 수 있을까?\n- 각 테이블을 join해서 볼 경우, identity는 상대적으로 Null값의 비율을 비교할 수 없으므로 각자 진행해, 각 테이블에 유용해보이는 데이터와 아닌 경우를 뽑아보자","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\ndef check_DataEDA_withlineChart(df, columns): # figsize -> default (30, 20), 넣을 수 있도록.. \n    \"\"\"\n    df = Stats DataFrame\n    columns = df.columns.to_list(), type=[]\n    \"\"\"\n    \n    print(len(columns))\n    # Working With Columns that they are Selected\n    new_df = df[df.index.isin(columns)].copy(deep=True)\n#     new_df = df[columns].copy(deep=True)\n    \n    \n    # Check Ordering and Ordered Number .. \n    col_2_idx_dict = {}\n    for i, column in enumerate(columns):\n        col_2_idx_dict[column] = i\n    \n    # inverted_dict \n    idx_2_col_dict = {val: key for key, val in col_2_idx_dict.items()}\n    \n    cols_idx = sorted(idx_2_col_dict.keys())\n    \n#     print(cols_idx)\n#     print(len(cols_idx))\n    # Draw Line Chart\n    fig = plt.figure(figsize=(30, 20))\n    \n    \n    x1 = new_df['unique_value']\n    x2 = new_df['isNullcnt']\n    \n    \n    plt.plot(cols_idx, new_df['unique_value'][idx_2_col_dict], label='unique_value', linestyle='-', marker='x')\n    plt.plot(cols_idx, new_df['isNullcnt'][idx_2_col_dict], label='isNullcnt', linestyle='--', marker='o')\n    \n    plt.legend()\n    plt.xlabel(\"Number Of Columns\")\n    plt.ylabel(\"Values\")\n    plt.title(\"CheckData with LineChart that NullVal and UniqVal\")\n    \n    # Annotate Infomation\n    print(idx_2_col_dict)\n    for i in sorted(idx_2_col_dict.keys()):\n#         print(i)\n#         print(idx_2_col_dict[i])\n\n        # Annotate DataType\n        width = i\n        height = new_df['unique_value'][idx_2_col_dict[i]]\n        text = str(new_df['value_type'][idx_2_col_dict[i]])\n        plt.annotate(text, xy=(width, height), xytext=(width+0.05, height+0.3), rotation=70)\n        \n        # Annotate Uniq Values Number\n        text = str(height)+'(n)' # str(check_identity['value_type'][i])\n        plt.annotate(text, xy=(width, height), xytext=(1, +50), textcoords=\"offset points\", va=\"top\", color='b', rotation=0)\n        \n        # Annotate Null Values \n#         total_num = max([new_df['unique_value'].max(), new_df['isNullcnt'].max()])\n        \n        # Stat Dataframe에 사용된 기본 데이터프레임\n        total_num = len(train_identity)\n    \n        text = str(math.floor(new_df['isNullcnt'][idx_2_col_dict[i]]/total_num*100))+'%'\n        plt.annotate(text, xy=(width, height), xytext=(1, +70), textcoords=\"offset points\", va=\"top\", color='orange', rotation=0)\n        \n        \n    fx1, fx2, fy1, fy2 = plt.axis() # fig.axis()\n    plt.annotate(\"The Red Text: Num of Unique Values\", xy=(fx2, fy2), xytext=(1, 1), textcoords=\"offset points\", ha=\"right\", va=\"bottom\", color='b')\n    plt.annotate(\"The Black Text: type Of Data\", xy=(fx2, fy2), xytext=(1, 10), textcoords=\"offset points\", ha=\"right\", va=\"bottom\")\n    plt.annotate(\"The Orange Text: Percentage Of Null value\", xy=(fx2, fy2), xytext=(1, 20), textcoords=\"offset points\", ha=\"right\", va=\"bottom\", color='orange')\n    plt.xticks(cols_idx, list([val for key, val in sorted(idx_2_col_dict.items())]), rotation=90)\n    \n    return fig\n    \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = check_identity.index\ncheck_DataEDA_withlineChart(check_identity, columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n# NULL값을 많이 포함하는 경우\ncheck_identity['isNullper'] = check_identity['isNullcnt'].apply(lambda x: x/train_identity.shape[0])\n\n# 모든 컬럼 조회\n# select_col = check_identity.index\n\n# Null값이 많은 컬럼 체크\nselect_col = check_identity[check_identity['isNullper']>0.9].index\nprint(f'{len(select_col)}과 {select_col}')\n\n\nplt.hist(check_identity[check_identity.index.isin(select_col)]['isNullper'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"check_identity[check_identity.index.isin(select_col)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 사기거래 중 97%값은 비어있다. \n[train[train['isFraud']==1][col].isnull().sum()/train[train['isFraud']==1].shape[0] for col in select_col]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 사기거래가 아닌 것 중 99%가 비어있다. .. 이 차이는 유의미한가?\n[train[train['isFraud']==0][col].isnull().sum()/train[train['isFraud']==0].shape[0] for col in select_col]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"remove_cols_identity = ['id_07', 'id_08', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26','id_27']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Null value가 많은 값 제외\n# Null값이 많은 컬럼을 제외한 칼럼들을 저장\nselect_col = check_identity[~check_identity.index.isin(select_col)].index\nprint(f'{len(select_col)}과 {select_col}')\n\n\nplt.hist(check_identity[check_identity.index.isin(select_col)]['isNullper'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"remian_identity_col = ['TransactionID', 'id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06',\n       'id_09', 'id_10', 'id_11', 'id_12', 'id_13', 'id_14', 'id_15', 'id_16',\n       'id_17', 'id_18', 'id_19', 'id_20', 'id_28', 'id_29', 'id_30', 'id_31',\n       'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38',\n       'DeviceType', 'DeviceInfo']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = check_identity[check_identity.index.isin(select_col)].index\ncheck_DataEDA_withlineChart(check_identity, columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- TransactinID는 제외\n- Float는 스케일링 처리\n- object는 uniq한 값에 따라 2개, 3개, 65개, 130개, 260개, 4개, 1786개를 어떻게 다룰지 살펴보자","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"check_identity[(check_identity['value_type']=='object') & (check_identity.index.isin(select_col))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 너무 많은 값들을 가진 경우는 나중에 처리\n\ntooMany_col_identity = check_identity[(check_identity['value_type']=='object') & (check_identity.index.isin(select_col))&(check_identity['unique_value']>=5)].index\nprint(tooMany_col_identity)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tooMany_col_identity = tooMany_col_identity.to_list()\nremove_cols_identity.extend(tooMany_col_identity)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"remove_cols_identity = ['id_07', 'id_08', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_30', 'id_31', 'id_33', 'DeviceInfo']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del check_identity","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### transaction\n\n트랜잭션은 너무 많으므로 identy와 비슷한 프로세스로 임의로 진행","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# NULL값을 많이 포함하는 경우\ncheck_transaction['isNullper'] = check_transaction['isNullcnt'].apply(lambda x: x/train_transaction.shape[0])\n\n# 모든 컬럼 조회\n# select_col = check_identity.index\n\n# Null값이 많은 컬럼 체크\nselect_col = check_transaction[check_transaction['isNullper']>0.7].index\nprint(f'{len(select_col)}과 {select_col}')\n\nplt.hist(check_transaction['isNullper'])\nplt.hist(check_transaction[check_transaction.index.isin(select_col)]['isNullper'])\n\nprint(len(select_col))\nprint(select_col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 우선 Null 값이 많은 칼럼을 제거하고, object중 값이 높은 것들도 제외... \nremove_cols_transacton = select_col.copy()\n\ncheck_transaction[(~check_transaction.index.isin(remove_cols_transacton))&(check_transaction['value_type']=='object')]\n\n\nremove_cols_transacton = remove_cols_transacton.to_list() # .append(\"P_emaildomain\")\n\nremove_cols_transacton.append(\"P_emaildomain\")\n\nremove_cols_transacton\n\n# remove_cols_transacton = ['dist2','R_emaildomain','D6','D7','D8','D9','D12','D13','D14','V138','V139','V140','V141','V142','V143','V144','V145','V146','V147','V148','V149','V150','V151','V152','V153','V154','V155','V156','V157','V158','V159','V160','V161','V162','V163','V164','V165','V166','V167','V168','V169','V170','V171','V172','V173','V174','V175','V176','V177','V178','V179','V180','V181','V182','V183','V184','V185','V186','V187','V188','V189','V190','V191','V192','V193','V194','V195','V196','V197','V198','V199','V200','V201','V202','V203','V204','V205','V206','V207','V208','V209','V210','V211','V212','V213','V214','V215','V216','V217','V218','V219','V220','V221','V222','V223','V224','V225','V226','V227','V228','V229','V230','V231','V232','V233','V234','V235','V236','V237','V238','V239','V240','V241','V242','V243','V244','V245','V246','V247','V248','V249','V250','V251','V252','V253','V254','V255','V256','V257','V258','V259','V260','V261','V262','V263','V264','V265','V266','V267','V268','V269','V270','V271','V272','V273','V274','V275','V276','V277','V278','V322','V323','V324','V325','V326','V327','V328','V329','V330','V331','V332','V333','V334','V335','V336','V337','V338','V339','P_emaildomain']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"remain_cols = [col for col in train.columns if col not in remove_cols_identity+remove_cols_transacton ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(remain_cols)\n# remian_cols = ['TransactionID', 'isFraud', 'TransactionDT', 'TransactionAmt', 'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'dist1', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D10', 'D11', 'D15', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_09', 'id_10', 'id_11', 'id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_28', 'id_29', 'id_32', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Columns 지우기 전에 train 칼럼 보관\n# _train = train.copy(deep=True)\ntrain = train[remain_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape  # 칼럼 개수를 434개에서 252개로 .. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.columns = [val.replace('-', '_') if 'id' in val else val for val in list(test.columns)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# _test = test.copy(deep=True)\nremain_test_cols = [col for col in remain_cols if col not in ['isFraud']]\ntest = test[remain_test_cols]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 피쳐 엔지니어링은 Skip","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Prepare data for modeling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()\nnew_train = check_dataframe(train)\nnew_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(train.select_dtypes('object').columns))\nprint(len(new_train[new_train['value_type']=='object']))\ncat_cols = new_train[new_train['value_type']=='object'].index.to_list()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del new_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nfor col in cat_cols:\n    if col in train.columns:\n        le = preprocessing.LabelEncoder()\n        le.fit(list(train[col].astype(str).values) + list(test[col].astype(str).values))\n        train[col] = le.transform(list(train[col].astype(str).values))\n        test[col] = le.transform(list(test[col].astype(str).values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.sort_values('TransactionDT').drop(['isFraud', 'TransactionDT', 'TransactionID'], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train.sort_values('TransactionDT')['isFraud']\n\nX_test = test.drop(['TransactionDT', 'TransactionID'], axis=1)\n\ntest = test[['TransactionDT', 'TransactionID']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# null 처리\ndef clean_inf_nan(df):\n    return df.replace([np.inf, -np.inf], np.nan)\n\n# Cleaning infinite values to NaN\nX = clean_inf_nan(X)\nX_test = clean_inf_nan(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ngc.collect()\nprint(\"Garbage collection thresholds: {}\".format(gc.get_threshold()))\n\n# 출처: https://weicomes.tistory.com/277 [25%]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"위 그래프에서 identity의 각 특징마다의 null value의 정도, 그리고 얼마나 많은 unique value를 가지고 있는지 살펴보려고 하였다. \n- 칼럼갯수가 많은 만큼 하나씩 살펴보기 어려움\n- 먼저 살펴볼 수 있는 데이터를 정하고 싶었음. \n- object에서는 대략적인 어떤 값을 쓸 수 있는지 살펴보고 싶었음\n- Float라면 null value가 높은 값을 쓸 수 있을지 미지수 \n\n-> 위의 그래프는 값들이 어떻게 분포되어있는지에 대해서는 알 수 없음","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## ML","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, cross_val_predict, TimeSeriesSplit, KFold, cross_val_score\nn_fold = 5\nfolds = TimeSeriesSplit(n_splits=n_fold)\nfolds = KFold(n_splits=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_features_indices = np.where(X.dtypes != np.float)[0]\n\nclf = CatBoostClassifier(random_seed=rnd_state)\n\nclf.fit(X_train, y_train, cat_features=categorical_features_indices)\nclf.score(X_val, y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'num_leaves': 256,\n          'min_child_samples': 79,\n          'objective': 'binary',\n          'max_depth': 13,\n          'learning_rate': 0.03,\n          \"boosting_type\": \"gbdt\",\n          \"subsample_freq\": 3,\n          \"subsample\": 0.9,\n          \"bagging_seed\": 11,\n          \"metric\": 'auc',\n          \"verbosity\": -1,\n          'reg_alpha': 0.3,\n          'reg_lambda': 0.3,\n          'colsample_bytree': 0.9,\n          #'categorical_feature': cat_cols\n         }\nresult_dict_lgb = train_model_classification(X=X, X_test=X_test, y=y, params=params, folds=folds, model_type='lgb', eval_metric='auc', plot_feature_importance=True,\n                                                      verbose=500, early_stopping_rounds=200, n_estimators=5000, averaging='usual', n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['isFraud']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['isFraud'] = result_dict_lgb['prediction']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(result_dict_lgb['oof']).to_csv('lgb_oof.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def check_DataEDA_withlineChart(df, columns): # figsize -> default (30, 20), 넣을 수 있도록.. \n    \"\"\"\n    df = DataFrame\n    columns = df.columns.to_list(), type=[]\n    \"\"\"\n    \n    print(len(columns))\n    # Working With Columns that they are Selected\n    new_df = df[df.index.isin(columns)].copy(deep=True)\n#     new_df = df[columns].copy(deep=True)\n    \n    \n    # Check Ordering and Ordered Number .. \n    col_2_idx_dict = {}\n    for i, column in enumerate(columns):\n        col_2_idx_dict[column] = i\n    \n    # inverted_dict \n    idx_2_col_dict = {val: key for key, val in col_2_idx_dict.items()}\n    \n    cols_idx = sorted(idx_2_col_dict.keys())\n    \n    print(cols_idx)\n    print(len(cols_idx))\n    # Draw Line Chart\n    fig = plt.figure(figsize=(30, 20))\n    \n    \n    x1 = new_df['unique_value']\n    x2 = new_df['isNullcnt']\n    \n    \n    plt.plot(cols_idx, new_df['unique_value'][idx_2_col_dict], label='unique_value', linestyle='-', marker='x')\n    plt.plot(cols_idx, new_df['isNullcnt'][idx_2_col_dict], label='isNullcnt', linestyle='--', marker='o')\n    \n    plt.legend()\n    plt.xlabel(\"Number Of Columns\")\n    plt.ylabel(\"Values\")\n    plt.title(\"CheckData with LineChart that NullVal and UniqVal\")\n    \n    # Annotate Infomation\n    print(idx_2_col_dict)\n    for i in sorted(idx_2_col_dict.keys()):\n        print(i)\n        print(idx_2_col_dict[i])\n\n        \n        # Annotate DataType\n        width = i\n        height = new_df['unique_value'][idx_2_col_dict[i]]\n        text = str(new_df['value_type'][idx_2_col_dict[i]])\n        plt.annotate(text, xy=(width, height), xytext=(width+0.05, height+0.3), rotation=70)\n        \n        # Annotate Uniq Values Number\n        text = str(height)+'(n)' # str(check_identity['value_type'][i])\n        plt.annotate(text, xy=(width, height), xytext=(1, +50), textcoords=\"offset points\", va=\"top\", color='b', rotation=0)\n        \n        # Annotate Null Values \n#         total_num = max([new_df['unique_value'].max(), new_df['isNullcnt'].max()])\n        total_num = len(train)\n        text = str(math.floor(new_df['isNullcnt'][idx_2_col_dict[i]]/total_num*100))+'%'\n        plt.annotate(text, xy=(width, height), xytext=(1, +70), textcoords=\"offset points\", va=\"top\", color='orange', rotation=0)\n        \n        \n    fx1, fx2, fy1, fy2 = plt.axis() # fig.axis()\n    plt.annotate(\"The Red Text: Num of Unique Values\", xy=(fx2, fy2), xytext=(1, 1), textcoords=\"offset points\", ha=\"right\", va=\"bottom\", color='b')\n    plt.annotate(\"The Black Text: type Of Data\", xy=(fx2, fy2), xytext=(1, 10), textcoords=\"offset points\", ha=\"right\", va=\"bottom\")\n    plt.annotate(\"The Orange Text: Percentage Of Null value\", xy=(fx2, fy2), xytext=(1, 20), textcoords=\"offset points\", ha=\"right\", va=\"bottom\", color='orange')\n    plt.xticks(cols_idx, list([val for key, val in sorted(idx_2_col_dict.items())]), rotation=90)\n    \n    return fig\n    \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"select_col = check_train[(check_train['value_type']!='object') & (check_train['isNullper']<0.6) & (check_train['isNullper']>0.4)].index\ncheck_DataEDA_withlineChart(check_train, select_col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"check_train[check_train.index.isin(['dist1', 'D2', 'D3'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"select_col = check_train[check_train['value_type']!='object'].index\nprint(len(select_col))\nplt.hist(check_train[check_train.index.isin(select_col)]['isNullper'])\nplt.title(\"Histogram Of Percetage Of Null Data\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"select_col = check_train.index\nprint(len(select_col))\nplt.hist(check_train[check_train.index.isin(select_col)]['isNullper'])\nplt.title(\"Histogram Of Percetage Of Null Data\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"check_train[(check_train['value_type']!='object') & (check_train['isNullper']<0.6) & (check_train['isNullper']>0.4)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['card1'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['card1'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(check_train[check_train['value_type']!='object'].index))\nprint(len(check_train[(check_train['value_type']!='object') & ((check_train['isNullper']<0.05))].index))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"check_train[check_train.index.isin(select_col)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.hist(check_train['isNullper'])\nplt.title(\"Histogram Of Percetage Of Null Data\")\ncheck_train[check_train['isNullper']<0.3].index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"check_train[check_train.index=='card3']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"select_col = list(check_train[(check_train['value_type']!='object')&(check_train['isNullper']<0.3)].index)\ncheck_DataEDA_withlineChart(check_train, select_col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx2col= {val: key for key, val in column_number.items()}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_idx = list(column_number.values())\n\nfor i in idx2col.keys():\n    print(i)\n    print(check_identity['unique_value'][idx2col[i]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30, 20)) # width, height\n# plt.plot(check_identity.index, check_identity['unique_value'], label='unique_value', linestyle='-', marker='x')\n# plt.plot(check_identity.index, check_identity['isNullcnt'], label='isNullcnt', linestyle='--', marker='o')\nplt.plot(list(column_number.values()), check_identity['unique_value'], label='unique_value', linestyle='-', marker='x')\nplt.plot(list(column_number.values()), check_identity['isNullcnt'], label='isNullcnt', linestyle='--', marker='o')\n# plt.annotate\nplt.legend()\nplt.xlabel(\"Columns\")\nplt.ylabel(\"Values Each Cols\")\nfor i in column_number.values():\n    width = list(column_number.values())[i]\n    height = check_identity['unique_value'][i]\n#     print(width, height)\n    plt.annotate(str(check_identity['value_type'][i]), xy=(width, height), xytext=(width+0.05, height+0.3), rotation=70)\n    \nfor i in column_number.values():\n    width = list(column_number.values())[i]\n    height = check_identity['unique_value'][i]\n#     print(width, height)\n    plt.annotate(str(check_identity['unique_value'][i]), xy=(width, height), xytext=(width+0.05, height+5000), color='b', rotation=0)\n# plt.annotate(check_identity['value_type'], xy=(list(column_number.values()), check_identity['isNullcnt']+2))\n\n# null value의 percentage\nfor i in column_number.values():\n    total_num = int(check_identity[check_identity.index=='TransactionID']['unique_value'])\n    width = list(column_number.values())[i]\n    height = check_identity['unique_value'][i]\n#     print(width, height)\n    plt.annotate(str(math.floor(check_identity['isNullcnt'][i]/total_num*100))+'%', xy=(width, height), xytext=(1, +70), textcoords=\"offset points\", va=\"top\", color='orange', rotation=0)\n\n\nx1, x2, y1, y2 = plt.axis()\nplt.annotate(\"The Red Text: Num of Unique Values\", xy=(x2, y2), xytext=(1, 1), textcoords=\"offset points\", ha=\"right\", va=\"bottom\", color='b')\nplt.annotate(\"The Black Text: type Of Data\", xy=(x2, y2), xytext=(1, 10), textcoords=\"offset points\", ha=\"right\", va=\"bottom\")\nplt.annotate(\"The Orange Text: Percentage Of Null value\", xy=(x2, y2), xytext=(1, 20), textcoords=\"offset points\", ha=\"right\", va=\"bottom\", color='orange')\nplt.xticks(list(column_number.values()), list(column_number.keys()), rotation=90)\n# plt.xticks(rotation=90, fontsize=20)\n# plt.xticks(list(column_number.keys()))\n\nplt.title(\"Check unique and null values\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"check_train[check_train['value_type'] == 'object']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"check_train['value_type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"check_identity","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = np.arange(5)\nb = np.arange(5, 10)\nplt.plot(a, b)\nbbox_args = dict(boxstyle=\"round\", fc=\"0.8\")\nfor i in range(len(a)):\n    plt.annotate('a', xy=(a[i], b[i]), xytext=(a[i]+0.1, b[i]+0))\n    plt.annotate('b', xy=(a[i], b[i]), xytext=(a[i], b[i]), color='b')\n\nx1, x2, y1, y2 = plt.axis()\nplt.annotate('test', xy=(x2, y2), textcoords=\"offset points\", ha=\"left\", va=\"bottom\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.axis()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"locs, labels = plt.xticks()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"locs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = ('Tom', 'Dick', 'Harry', 'Sally', 'Sue')\nplt.xticks(np.arange(5), labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot([1, 2, 3, 4], [0.1, 0.2, 0.3, 0.4], \n         [1, 2, 3, 4], [2, 2, 2, 2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot('unique_value', 'isNullcnt')\nplt.show()\n# plt.plot(y = 'isNullcnt',kind='line')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"check_identity.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(check_identity.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}