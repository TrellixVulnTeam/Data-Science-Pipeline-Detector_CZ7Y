{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this kernel I'm trying to fill some NaNs values using one intresting observation."},{"metadata":{},"cell_type":"markdown","source":"*just random pic idk*\n![](https://sun9-31.userapi.com/c857628/v857628861/4c59e/JiPqE9xmzjs.jpg)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport warnings\n\nwarnings.simplefilter('ignore')\n\ntrain = pd.read_csv('../input/ieee-fraud-detection/train_transaction.csv')\ntest = pd.read_csv('../input/ieee-fraud-detection/test_transaction.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I've noticed that there are cases in card columns that depends on other card columns. So using that approach we can fill some NaNs in data. Let's look at the data!"},{"metadata":{"trusted":true},"cell_type":"code","source":"card_features = ['card1', 'card2', 'card3', 'card4', 'card5', 'card6']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[card_features].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's count all NaNs in every card columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.concat([train[card_features].isna().sum(), test[card_features].isna().sum()], axis=1).rename(columns={0: 'train_NaNs', 1: 'test_NaNs'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that card2 is the most NaN card feature. What is more, card3, card4 and car6 in test have 2 times more NaNs values."},{"metadata":{},"cell_type":"markdown","source":"Let's look ratio of missing values to the total number of rows."},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.concat([train[card_features].isna().sum() / train.shape[0], test[card_features].isna().sum() / test.shape[0]], axis=1).rename(columns={0: 'train_NaNs_%', 1: 'test_NaNs_%'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not very high ratios though."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Some usefull functions\n\ndef count_uniques(train, test, pair):\n    unique_train = []\n    unique_test = []\n\n    for value in train[pair[0]].unique():\n        unique_train.append(train[pair[1]][train[pair[0]] == value].value_counts().shape[0])\n\n    for value in test[pair[0]].unique():\n        unique_test.append(test[pair[1]][test[pair[0]] == value].value_counts().shape[0])\n\n    pair_values_train = pd.Series(data=unique_train, index=train[pair[0]].unique())\n    pair_values_test = pd.Series(data=unique_test, index=test[pair[0]].unique())\n    \n    return pair_values_train, pair_values_test\n\ndef fill_card_nans(train, test, pair_values_train, pair_values_test, pair):\n    print(f'In train{[pair[1]]} there are {train[pair[1]].isna().sum()} NaNs' )\n    print(f'In test{[pair[1]]} there are {test[pair[1]].isna().sum()} NaNs' )\n\n    print('Filling train...')\n    \n    for value in pair_values_train[pair_values_train == 1].index:\n        train[pair[1]][train[pair[0]] == value] = train[pair[1]][train[pair[0]] == value].value_counts().index[0]\n        \n    print('Filling test...')\n\n    for value in pair_values_test[pair_values_test == 1].index:\n        test[pair[1]][test[pair[0]] == value] = test[pair[1]][test[pair[0]] == value].value_counts().index[0]\n        \n    print(f'In train{[pair[1]]} there are {train[pair[1]].isna().sum()} NaNs' )\n    print(f'In test{[pair[1]]} there are {test[pair[1]].isna().sum()} NaNs' )\n    \n    return train, test\n\ndef nans_distribution(train, test, unique_train, unique_test, pair):\n    train_nans_per_category = []\n    test_nans_per_category = []\n\n    for value in unique_train.unique():\n        train_nans_per_category.append(train[train[pair[0]].isin(list(unique_train[unique_train == value].index))][pair[1]].isna().sum())\n\n    for value in unique_test.unique():\n        test_nans_per_category.append(test[test[pair[0]].isin(list(unique_test[unique_test == value].index))][pair[1]].isna().sum())\n\n    pair_values_train = pd.Series(data=train_nans_per_category, index=unique_train.unique())\n    pair_values_test = pd.Series(data=test_nans_per_category, index=unique_test.unique())\n    \n    return pair_values_train, pair_values_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Card1 and Card2"},{"metadata":{},"cell_type":"markdown","source":"There is dependency between —Åard2 and card1 values.  "},{"metadata":{},"cell_type":"markdown","source":"In the dataset we can found a lot of cases like that. Where most of the values are the same, but there are some missing values. So we can assume that in NaN rows should be that only value which occurs in that card1 category. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['card1'] == 13926][['card1', 'card2']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's count unique values for each card1 category."},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_values_train, unique_values_test = count_uniques(train, test, ('card1', 'card2'))\npd.concat([unique_values_train.value_counts(), unique_values_test.value_counts()], axis=1).rename(columns={0: 'train', 1: 'test'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that most of the card1 category have only one unique value. "},{"metadata":{},"cell_type":"markdown","source":"Now let's count amount of the missing values for amount of the unique values."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_nan_dist, test_nan_dist = nans_distribution(train, test, unique_values_train, unique_values_test, ('card1', 'card2'))\npd.concat([train_nan_dist, test_nan_dist], axis=1).rename(columns={0: 'train', 1: 'test'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hm. There are a lot of missing values for categorys where there is no values(only NaNs) and where only one value.\nSo we can do that:\n\n* Fill NaNs in 1-amount category with most frequent value\n* Treat 0-amount category NaNs as only one category. We can just encode it somehow."},{"metadata":{},"cell_type":"markdown","source":"But right now we will focus only on 1-amount category and fill NaNs with most frequent value in card1 category."},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = fill_card_nans(train, test, unique_values_train, unique_values_test, ('card1', 'card2'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Card1 and Card3"},{"metadata":{},"cell_type":"markdown","source":"Let's do all the same but for card3 category."},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['card1'] == 13926][['card1', 'card3']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_values_train, unique_values_test = count_uniques(train, test, ('card1', 'card3'))\npd.concat([unique_values_train.value_counts(), unique_values_test.value_counts()], axis=1).rename(columns={0: 'train', 1: 'test'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_nan_dist, test_nan_dist = nans_distribution(train, test, unique_values_train, unique_values_test, ('card1', 'card3'))\npd.concat([train_nan_dist, test_nan_dist], axis=1).rename(columns={0: 'train', 1: 'test'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = fill_card_nans(train, test, unique_values_train, unique_values_test, ('card1', 'card3'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we filled almost all NaNs in card3."},{"metadata":{},"cell_type":"markdown","source":"# Card1 and Card4"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['card1'] == 13926][['card1', 'card4']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok, here is the same dependency."},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_values_train, unique_values_test = count_uniques(train, test, ('card1', 'card4'))\npd.concat([unique_values_train.value_counts(), unique_values_test.value_counts()], axis=1).rename(columns={0: 'train', 1: 'test'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_nan_dist, test_nan_dist = nans_distribution(train, test, unique_values_train, unique_values_test, ('card1', 'card4'))\npd.concat([train_nan_dist, test_nan_dist], axis=1).rename(columns={0: 'train', 1: 'test'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we have the same problem. And that approach can solve it too."},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = fill_card_nans(train, test, unique_values_train, unique_values_test, ('card1', 'card4'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Card1 and Card5"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['card1'] == 13926][['card1', 'card5']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_values_train, unique_values_test = count_uniques(train, test, ('card1', 'card5'))\npd.concat([unique_values_train.value_counts(), unique_values_test.value_counts()], axis=1).rename(columns={0: 'train', 1: 'test'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_nan_dist, test_nan_dist = nans_distribution(train, test, unique_values_train, unique_values_test, ('card1', 'card5'))\npd.concat([train_nan_dist, test_nan_dist], axis=1).rename(columns={0: 'train', 1: 'test'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = fill_card_nans(train, test, unique_values_train, unique_values_test, ('card1', 'card5'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Card1 and Card6"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['card1'] == 13926][['card1', 'card6']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_values_train, unique_values_test = count_uniques(train, test, ('card1', 'card6'))\npd.concat([unique_values_train.value_counts(), unique_values_test.value_counts()], axis=1).rename(columns={0: 'train', 1: 'test'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_nan_dist, test_nan_dist = nans_distribution(train, test, unique_values_train, unique_values_test, ('card1', 'card6'))\npd.concat([train_nan_dist, test_nan_dist], axis=1).rename(columns={0: 'train', 1: 'test'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = fill_card_nans(train, test, unique_values_train, unique_values_test, ('card1', 'card6'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ok. Let's look at number on NaNs now."},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.concat([train[card_features].isna().sum(), test[card_features].isna().sum()], axis=1).rename(columns={0: 'train_NaNs', 1: 'test_NaNs'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Still there are a lot of NaNs in the card2 and card5. Let's try some other fill combinations."},{"metadata":{"trusted":true},"cell_type":"code","source":"train[card_features].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's find another dependent feature for card2."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Card3 == 150: ', train[train['card3'] == 150]['card2'].nunique())\nprint('Card4 == mastercard: ', train[train['card4'] == 'mastercard']['card2'].nunique())\nprint('Card5 == 102: ', train[train['card5'] == 102]['card2'].nunique())\nprint('Card6 == credit: ', train[train['card6'] == 'credit']['card2'].nunique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are too many unique values to implement this approch to fill remaining NaNs in card2."},{"metadata":{},"cell_type":"markdown","source":"Let's find another dependent feature for card5."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Card2 == 327: ', train[train['card2'] == 327]['card5'].nunique())\nprint('Card3 == 150: ', train[train['card3'] == 150]['card5'].nunique())\nprint('Card4 == mastercard: ', train[train['card4'] == 'mastercard']['card5'].nunique())\nprint('Card6 == credit: ', train[train['card6'] == 'credit']['card5'].nunique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Same for card5."},{"metadata":{},"cell_type":"markdown","source":"Let's try some other features."},{"metadata":{},"cell_type":"markdown","source":"# Card1 and Addr2"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['card1'] == 13926][['card1', 'addr2']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_values_train, unique_values_test = count_uniques(train, test, ('card1', 'addr2'))\npd.concat([unique_values_train.value_counts(), unique_values_test.value_counts()], axis=1).rename(columns={0: 'train', 1: 'test'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_nan_dist, test_nan_dist = nans_distribution(train, test, unique_values_train, unique_values_test, ('card1', 'addr2'))\npd.concat([train_nan_dist, test_nan_dist], axis=1).rename(columns={0: 'train', 1: 'test'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are really a lot of missing values in addr2, especially for 1-amount category."},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = fill_card_nans(train, test, unique_values_train, unique_values_test, ('card1', 'addr2'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, we can fill to many values using this approach. But we cannot be 100% sure that missing values in 1-amount category is most frequent category."},{"metadata":{},"cell_type":"markdown","source":"# Let's find all the features that depends on card1"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['card1'] == 13926]['addr2'].value_counts().shape[0] == 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"depend_features = []\n\nfor col in train.columns:\n    if train[train['card1'] == 13926][col].value_counts().shape[0] == 1:\n        depend_features.append(col)\n\nprint(depend_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are a lot of columns that we can suspect in dependency. And some of them we can fill like above."},{"metadata":{},"cell_type":"markdown","source":"## If you want to apply my kernel you can use that function:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def fill_pairs(train, test, pairs):\n    for pair in pairs:\n\n        unique_train = []\n        unique_test = []\n\n        print(f'Pair: {pair}')\n        print(f'In train{[pair[1]]} there are {train[pair[1]].isna().sum()} NaNs' )\n        print(f'In test{[pair[1]]} there are {test[pair[1]].isna().sum()} NaNs' )\n\n        for value in train[pair[0]].unique():\n            unique_train.append(train[pair[1]][train[pair[0]] == value].value_counts().shape[0])\n\n        for value in test[pair[0]].unique():\n            unique_test.append(test[pair[1]][test[pair[0]] == value].value_counts().shape[0])\n\n        pair_values_train = pd.Series(data=unique_train, index=train[pair[0]].unique())\n        pair_values_test = pd.Series(data=unique_test, index=test[pair[0]].unique())\n        \n        print('Filling train...')\n\n        for value in pair_values_train[pair_values_train == 1].index:\n            train.loc[train[pair[0]] == value, pair[1]] = train.loc[train[pair[0]] == value, pair[1]].value_counts().index[0]\n\n        print('Filling test...')\n\n        for value in pair_values_test[pair_values_test == 1].index:\n            test.loc[test[pair[0]] == value, pair[1]] = test.loc[test[pair[0]] == value, pair[1]].value_counts().index[0]\n\n        print(f'In train{[pair[1]]} there are {train[pair[1]].isna().sum()} NaNs' )\n        print(f'In test{[pair[1]]} there are {test[pair[1]].isna().sum()} NaNs' )\n        \n    return train, test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pairs = [('card1', 'card2'), ('card1', 'card3')]\n\ntrain, test = fill_pairs(train, test, pairs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you find this kernel helpful please upvote!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}