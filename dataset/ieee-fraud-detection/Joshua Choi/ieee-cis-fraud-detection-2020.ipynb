{"cells":[{"metadata":{},"cell_type":"markdown","source":"## General information\n\n이 내용은 안드레이 룩야넨코의 노트북을 한글화한 것 입니다.\n이 커널에서는 IEEE Fraud Detection competition을 다룹니다.\n\nIEEE-CIS는 다양한 분야의 AI 및 머신러닝 부분에서 일하고 있는데요\t\n이는 deep neural networks, fuzzy systems, evolutionary computation 및 swarm intelligence를 포함합니다\n이제 세계 굴지의 지불 시스템 회사인 Vesta와 협력하여, 카드 사기 방지를 할 수 있는 최적의 솔루션을 찾아서 수 많은 데이터 사이언티스트에게 이에 도전하라고 했습니다\n\n이 챌린지는 binary classification problem으로 이러한 과제가 경험할 수 밖에 없는 data의 심한 imbalance를 보여 줍니다.\n우리는 여기서 데이터를 탐구하고 소중한 인사이트를 파악하여 많은 feature engineering을 통하여 성능이 높은 모델링을 하려고 합니다\n\n![](https://cis.ieee.org/images/files/slideshow/abstract01.jpg)\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Importing libraries","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# importing libraries\nimport numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom IPython.display import display\npd.options.display.precision = 15\npd.options.display.max_rows = 10000\npd.options.display.max_columns = 10000\npd.options.display.max_colwidth = 1000\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport datetime\nfrom numba import jit\nimport random\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nimport lightgbm as lgb\n\n%load_ext autoreload\n%autoreload 2\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n\nseed = 42\nseed_everything(seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data overview\n\n데이터를 우선 로딩하고 보도록 하겠습니다.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"folder_path = '../input/'\ntrain_identity = pd.read_csv(f'{folder_path}train_identity.csv')\ntrain_transaction = pd.read_csv(f'{folder_path}train_transaction.csv')\ntest_identity = pd.read_csv(f'{folder_path}test_identity.csv')\ntest_transaction = pd.read_csv(f'{folder_path}test_transaction.csv')\nsub = pd.read_csv(f'{folder_path}sample_submission.csv')\n# let's combine the data and work with the whole dataset\ntrain = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\ntest = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"        구글 콜랩을 사용하시면 데이터를 PC에 다운로드 하신 후 그 파일들을 아래 코드를 활용하여 다시 업로드 함으로 import 하세요 \n\n        from google.colab import files\n        uploaded = files.upload()\n\n        이 다음에 파일 선택하고 (미리 다운로드 받아 놓으셔야 해요) 아래 코드로 변환 시키면 됩니다.\n\n        그런데 콜래벵서 업로드 하면 시간이 엄청 걸립니다. (콜랩에서는 고용량 RAM 체크하고 하셔야 합니다)\n        \n        이어서 아래 실행\n\n        import io\n        train_identity = pd.read_csv(io.BytesIO(uploaded['train_identity.csv']))\n        train_transaction = pd.read_csv(io.BytesIO(uploaded['train_transaction.csv']))\n        test_identity = pd.read_csv(io.BytesIO(uploaded['test_identity.csv']))\n        test_transaction = pd.read_csv(io.BytesIO(uploaded['test_transaction.csv']))\n        sub = pd.read_csv(io.BytesIO(uploaded['sample_submission.csv']))","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_identity.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"우리는 열이 많은 꽤 큰 데이터셋 두 개를 볼 수 있습니다. Train과 test data는 비슷한 길이의 행을 가지고 있습니다.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_identity.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_identity.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_transaction.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"데이터는 두 개로 나누어져 있는 것 같습니다: identity와 transaction.\n\n아무래도 identity는 어떤 카드인가에 초점이 맞추어 지겠고 id 30또는 31을 보면 안드로이드나 크롬 같은 단어도 나옵니다?\n\nIdentity 40개 가량의 열이 있고 열의 내용을 눈치채게 할 이름은 없습니다.\n\n하지만 사용자의 버전을 볼 수 있죠: OS, browser version, resolution, type 및 기타 등 등 \n\nTransaction data는 거의 400 columns가 있습니다!\n\n주목 할만한 열들은 다음과 같습니다:\n\ncard information\n\ntransaction date & amount\n\ne-mail","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"이제 데이터를 합쳐 봅니다.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 어떤 이유에선지 테스트 컬럼에서 언더바 대신 하이픈으로 쓴 것이 있어서 좀 바꿉니다.\ntest_identity.columns = [col.replace('-', '_') for col in test_identity.columns]\n\n# merge method를 사용하여 트레인과 테스트의 트랜잭션(거래)와 아이덴티티(신분)을 파일을 합합니다. \n# 여기서 on은 그 것을 인덱스로 합치라는 것이고 left나 right는 왼쪽 것 기준으로 또는 오른쪽 것 기준으로 파일을 정열하는데...여기서 말하는 기준은 데이터 값이 있는 것입니다.\n# 다시 말해 left로 되어 있는데 왼쪽의 transaction의 해당 행이 빈 값이면 identity에 값이 있어도 그 행은 보이지 않게되고, 반대로 right라면 그 행은 보이게 됩니다.\n\ntrain = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\ntest = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"그리고 기존 데이터는 무거우니 지웁니다.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_transaction, train_identity, test_transaction, test_identity","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"트레인 데이터에는 C1부터 C15까지 빈 값이 없으나 테스트 데이터에서는 빈값이 있어서 이들의 빈 값을 0으로 채웁니다.\n\n아래에서 f는 f-string이라 불리는 것으로 이를 통해 다양한 표현식을 사용할 수 있다.\n\n예를 들어 python2에서 나오는 %나 python3에서 나오는 str.format과 비교하였을 때 훨씬 간단하게 표현할 수 있다 ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test[[f'C{i}' for i in range(1,15)]] = test[[f'C{i}' for i in range(1,15)]].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"이어서 얼마나 많은 램을 쓰는지 한 번 보기로 하자","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"end_mem = train.memory_usage().sum() / 1024 ** 2 + test.memory_usage().sum() / 1024 ** 2\nprint(f'Mem. usage {end_mem:5.2f} Mb')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"이는 상당히 많이 쓴 것으로 RAM사용량을 줄이는 메소드가 있으므로 사용해 보도록 합니다\n\n정보는 그대로 주고 데이터 타입을 줄이므로 이를 달성합니다.\n\n아시다시피 integer가 float보다 메모리를 덜 사용하고 integer8이 integer16보다 메모리를 덜 사용합니다.\n\n일단 낮출 수 잇는 만큼 낮추어 보겠습니다.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(memory_df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = memory_df.memory_usage().sum() / 1024 ** 2\n    for col in memory_df.columns:\n        col_type = memory_df[col].dtypes\n        if col_type in numerics:\n            c_min = memory_df[col].min()\n            c_max = memory_df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    memory_df[col] = memory_df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    memory_df[col] = memory_df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    memory_df[col] = memory_df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    memory_df[col] = memory_df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    memory_df[col] = memory_df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    memory_df[col] = memory_df[col].astype(np.float32)\n                else:\n                    memory_df[col] = memory_df[col].astype(np.float64)\n    end_mem = memory_df.memory_usage().sum() / 1024 ** 2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (\n            start_mem - end_mem) / start_mem))\n    return memory_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"메모리 사용도가 2/3가 줄어 1/3 수준으로 줄어들었습니다.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Data Exploration\n\n이제 identity information을 파악해 봅니다.\n\nid_01 - id_11는 continuous variables입니다.\n\nid_12 - id_38는 categorical variable이고 마지막 두 개의 열은 확실히 범주 항목입니다.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(train['id_01'], bins=77);\nplt.title('Distribution of id_01 variable');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`id_01` 은 아주 흥미로운 분포를 가지고 있습니다. 77개의 유니크한  양수가 아닌 수로 0에서 비대칭이 일어나는 분포입니다.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['id_03'].value_counts(dropna=False, normalize=True).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`id_03` 은 88%가 빈 값이며 98%가 빈값이거나 0입니다.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['id_11'].value_counts(dropna=False, normalize=True).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`id_11` 의 값 중 22%는 100이며 76%는 빈 값입니다. 좀 이상합니다.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(train['id_07']);\nplt.title('Distribution of id_07 variable');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"일부 항목은 normalized 된 것 같습니다. 모든 항목을 normalize하려면 이미 normalized 된 것은 분리하여야 할 것입니다. \n\n이제 transaction data를 살펴 보겠습니다.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.hist(train['TransactionDT'], label='train');\nplt.hist(test['TransactionDT'], label='test');\nplt.legend();\nplt.title('Distribution of transactiond dates');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"여기서 중요점이 발견 됩니다.\n\n트레인 및 테스트 트랜잭션 날짜가 겹치지 않는 것 같으므로 유효성 검사를 위해 시간 기반 분할을 사용하는 것이 현명할 것입니다.\n\n대회 설명에서도 다음과 같은 내용이 있었습니다. \"TransactionDT\" 항목은 주어진 참조 날짜 / 시간 (실제 타임 스탬프 아님)의 타임 델타입니다.\n\n시작일이 2017-11-30이라는 것을 깨달았습니다. 이 정보는 나중에 사용할 것입니다.\n\n* 참조 사항\n\n우리가 여기서 사기를 일으키는 카드를 찾는 것이 아니라 그런 사용자를 찾는 것이란 것을 명심하세요.\n\n위의 TransactionDT를 설명 드리면 거래를 하려는데 카드가 정지되거나 카드를 누가 불법으로 썼는데 카드 주인이 모르고 넘어 간다거나 등 등에서 모두 날짜가 중요하겠죠. \n\n이에 대한 reference 포인트가 될 항목입니다.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"start_date = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (start_date)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Filling missing values in `card` columns\n\nСard columns are important but missing values.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"for col in train.columns:\n    if 'card' in col:\n        print(f\"{col} has {train[col].isnull().sum()} missing values.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'card1'의 각 값에 대한 'card2'의 가장 일반적인 값을 살펴 보겠습니다.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"group_with_mode = train.groupby(['card1']).agg({'card2': ['nunique', pd.Series.mode]})\ngroup_with_mode.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'card1'의 유니크한 값이 얼마나 있는지, 1개를 초과하는 유니크한 값을 가진 'card1'의 유니크한 값이 얼마나 있는지 보겠습니다","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"non_one = group_with_mode[group_with_mode['card2']['nunique'] > 1].shape[0]\ncard1_nunique = train['card1'].nunique()\nprint(f'Number of unique values of card1: {card1_nunique}')\nprint(f'Number of unique values of card1 which have more than one unique value: {non_one}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"대부분의 경우 card1의 각 값에 대해 card2 열에는 고유 한 값이 하나만 있습니다. 이것을 참조하여 누락 된 값을 채워 봅시다","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for card in ['card2','card3','card4','card5','card6']:\n    group_with_mode = train.groupby(['card1']).agg({card: ['nunique', pd.Series.mode]})\n    to_merge = group_with_mode[group_with_mode[card]['nunique'] == 1][card]['mode']\n    merged = pd.merge(train['card1'], to_merge, on='card1', how='left')\n    merged['mode'] = merged['mode'].fillna(train[card])\n    train[card] = merged['mode']\n    \n    group_with_mode = test.groupby(['card1']).agg({card: ['nunique', pd.Series.mode]})\n    to_merge = group_with_mode[group_with_mode[card]['nunique'] == 1][card]['mode']\n    merged = pd.merge(test['card1'], to_merge, on='card1', how='left')\n    merged['mode'] = merged['mode'].fillna(test[card])\n    test[card] = merged['mode']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Target and Users\n\n라벨링 로직에 대한 정보는 https://www.kaggle.com/c/ieee-fraud-detection/discussion/101203#588953 에서 볼 수 있습니다.\n```\n라벨링의 논리는 카드에 보고된 지불 거절을 사기 거래 (isFraud = 1)로 정의하고 사용자 계정, 이메일 주소 또는 이러한 속성에 직접 연결된 청구 주소를 사기로 사후에 거래하는 것입니다. \n\n120 일이 지난 후에도 위에 나온 것들에서 보고된 것이 없는 경우 합법적인 거래로 정의합니다 (isFraud = 0).\n```\n\n결과적으로 사기의 경우 단일 거래가 사기로 간주될 뿐만 아니라 사용자의 다른 거래도 사기로 간주된다는 것을 알 수 있기 때문에 이것은 매우 중요합니다. (다른 카드 모두 정지될 테니까요)\n\n하지만 우리에게는 데이터에 사용자 ID가 있는 열이 없습니다. 익명 처리되었습니다. \n\n결과적으로 다른 열을 기반으로 사용자 ID 열을 만들어야 겠지요. \n\n예를 들어 주소와 카드 정보의 조합을 ID로 사용할 수 있습니다.\n\n우리는 'D1'이 사용자의 첫 거래 이후 일수를 의미한다는 것을 알고 있습니다.\n\n더 나아가기 전에 캐글의 컬럼 정보 부분을 다시 보겠습니다.\n\n* TransactionDT: 주어진 포인트로부터 타임델타 (실 타임 스템프는 아님)\n* TransactionAMT: 미국 달러로 결제한 거래 액수\n* ProductCD: 상품 코드, 거래한 상품\n* card1 — card6: 지불 수단 카드 정보, 카드 타입, 카드 분류, 은행, 국가 등\n* addr: 주소\n* dist: 거리\n* P_ and (R__) emaildomain: 구매자의 이메일 정보\n* C1-C14: 카운팅 얼마나 많은 주소가 연결되었느냐 등인데 실제 내용은 마스킹 되어 있어서 파악이 안 됨\n* D1-D15: 타임델타, 예를 들어 그 전 거래부터 얼마나 되었는지 등\n* M1-M9: 매치...카드와 이름 주소의 매치 등\n* Vxxx: Vesta 제작 항목 랭킹, 카운팅, 기타 관련\n* identity table의 항목은 네트워크 연결 정보 예를 들어 IP, ISP, Proxy, 등 또는 디지털 시그내쳐 UA/browser/os/version 등으로 거래 관련 정보를 나타낸다","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"DaysFromStart라는 항목을 만듭니다. \n\nTransactionDT에서 60초 60분 24시간을 나누어서 버림(floor)로 해서 1을 뺀 날짜를 구하고 \n\nD1-DaysFromStart 항목을 만들어 D1-DaysFromStart 날짜를 봅니다","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in [train,test]:\n    df['DaysFromStart'] = np.floor(df['TransactionDT']/(60*60*24)) - 1\n    df['D1-DaysFromStart'] = df['D1'] - df['DaysFromStart']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"이어서 연속으로 정보를 합쳐서 uid를 생성합니다","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in [train,test]:\n    df['uid'] = df['ProductCD'].astype(str) + '_' + df['card1'].astype(str) + '_' + df['card2'].astype(str)\n    df['uid'] = df['uid'] + '_' + df['card3'].astype(str) + '_' + df['card4'].astype(str)\n    df['uid'] = df['uid'] + '_' + df['card5'].astype(str) + '_' + df['card6'].astype(str)\n    df['uid'] = df['uid'] + '_' + df['addr1'].astype(str) + '_' + df['D1-DaysFromStart'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"uid의 크기와 유니크한 값을 보겠습니다","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"len(set(train['uid']).intersection(set(test['uid']))), train.uid.nunique(), test.uid.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['uid'].value_counts(dropna=False, normalize=True).head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"트레인 데이터와 테스트 데이터간에 일부 uid 만 겹치는 것을 볼 수 있습니다. 즉, 이 항목을 직접 적용하여 사용할 수 없으며 이를 기반으로 다양한 항목을 만들어 사용해야 할 것 같습니다.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### processing some device information and ids","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"DeviceInfo의 밸류 카운트를 보겠습니다","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['DeviceInfo'].value_counts(dropna=False, normalize=True).head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"보시면, 거의 80%가 빈 값이고. 윈도우가 8%가량, iOS가 3.34% 등 나오고 삼성도 좀 보입니다.\n\n유니크한 값이 몇개 인지 보겠습니다.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['DeviceInfo'].nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"아주 많은 디바이스가 있습니다. 그리고 80% 가량은 빈값이니 어느 장비인 줄 알 수가 없습니다.\n\n이를 좀 고쳐서 항목을 만들어 보려 하겠습니다","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in [train, test]:\n    df['DeviceInfo'] = df['DeviceInfo'].fillna('unknown_device').str.lower()\n    df['DeviceInfo_device'] = df['DeviceInfo'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))\n    df['DeviceInfo_version'] = df['DeviceInfo'].apply(lambda x: ''.join([i for i in x if i.isnumeric()]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"join은 데이터를 합쳐 주는데 dtype이 같아야만 합니다\n\n모델명 숫자 등을 지우고 종류를 정리해 보았습니다\n\n아래에서 얼마나 줄어들었는지 봅니다","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['DeviceInfo_device'].value_counts(dropna=False, normalize=True).head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['DeviceInfo_version'].value_counts(dropna=False, normalize=True).head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'id_30' 의 밸류 카운트를 비율로 봅니다","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['id_30'].value_counts(dropna=False, normalize=True).head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"유니크한 값이 몇개나 있는지 보겠습니다.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['id_30'].nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'id_31' 의 밸류 카운트를 비율로 봅니다","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['id_31'].value_counts(dropna=False, normalize=True).head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"이들을 위에 DeviceInfo에 했던 것 처럼 해봅니다.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in [train, test]:\n    df['id_30'] = df['id_30'].fillna('unknown_device').str.lower()\n    df['id_30_device'] = df['id_30'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))\n    df['id_30_version'] = df['id_30'].apply(lambda x: ''.join([i for i in x if i.isnumeric()]))\n\n    df['id_31'] = df['id_31'].fillna('unknown_device').str.lower()\n    df['id_31_device'] = df['id_31'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"빈 값에는 unknown device라 입력을 하고 합치고 줄여 봅니다","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['id_30_device'].value_counts(dropna=False, normalize=True).head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### process some D-columns","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"우리는 Card 열들은 카드 종류나 카드사 같은 정보라는 것을 위에서 보았으며\n\nid는 사용된 OS나 디바이스 등임을 알았습니다.\n\n이제 D항목들은 어떤 것인지 보겠습니다","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"일단 D1 에서 D15까지 어떤 값들이 있나 봅니다","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.D1.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.D2.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.D3.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.D4.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.D5.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.D6.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.D7.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.D8.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.D9.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.D10.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.D11.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.D12.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.D13.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.D14.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.D15.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"D8과 D9의 분포부터 보겠습니다","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(train['D8']);\nplt.title('Distribution of D8 variable');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(train['D9']);\nplt.title('Distribution of D9 variable');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in [train, test]:\n    # 클립 메소드를 사용하면 음수를 0으로 만들 수 있습니다\n    for col in ['D'+str(i) for i in range(1,16)]:\n        df[col] = df[col].clip(0)\n    \n    # D9을 빈 값이 아닌 것을 따로 떼어내어 D9_not_na 항목을 새로 만듭니다.\n    df['D9_not_na'] = np.where(df['D9'].isna(),0,1)\n    # D8을 1보다 크거나 같은 것만 떼어내어 D8_not_same_day 항목을 만듭니다\n    df['D8_not_same_day'] = np.where(df['D8']>=1,1,0)\n    # D8에서 소수점 이하를 뽑아서 D8_D9_decimal_dist로 정의한 후 여기서 D9을 뺍니다\n    df['D8_D9_decimal_dist'] = df['D8'].fillna(0)-df['D8'].fillna(0).astype(int)\n    df['D8_D9_decimal_dist'] = ((df['D8_D9_decimal_dist']-df['D9'])**2)**0.5\n    df['D8'] = df['D8'].fillna(-1).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.D9_not_na.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.D8_not_same_day.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.D8_D9_decimal_dist.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.D8.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Process `TransactionAmt`","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"이제는 거래 액수에 대해 보겠습니다 \n\n분포를 보겠습니다","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(train['TransactionAmt']);\nplt.title('Distribution of TransactionAmt variable');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"상위 1%(퍼센타일 99) 를 봅니다 ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"np.percentile(train['TransactionAmt'], 99)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 클립하여 0부터 5000사이의 값으로 봅니다\ntrain['TransactionAmt'] = train['TransactionAmt'].clip(0,5000)\ntest['TransactionAmt']  = test['TransactionAmt'].clip(0,5000)\n\n# 거래 액수가 일반적인지 아닌지를 봅니다\ntrain['TransactionAmt_check'] = np.where(train['TransactionAmt'].isin(test['TransactionAmt']), 1, 0)\ntest['TransactionAmt_check']  = np.where(test['TransactionAmt'].isin(train['TransactionAmt']), 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.TransactionAmt_check.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.TransactionAmt_check.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"트레인 거래액수에서 테스트 거래 액수에 없는 것이 16,920건 테스트 거래 액수에서 트레인 거래 액수에 없는 것이 2363건인 것을 볼 수 있습니다.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"다른 유용한 항목들을 만들어 봅니다.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in [train, test]:\n    df['ProductCD_card1'] = df['ProductCD'].astype(str) + '_' + df['card1'].astype(str)\n    df['card1_addr1'] = df['card1'].astype(str) + '_' + df['addr1'].astype(str)\n    df['TransactionAmt_dist2'] = df['TransactionAmt'].astype(str) + '_' + df['dist2'].astype(str)\n    df['card3_card5'] = df['card3'].astype(str) + '_' + df['card5'].astype(str)\n    df['ProductCD_TransactionAmt'] = df['ProductCD'].astype(str) + '_' + df['TransactionAmt'].astype(str)\n    df['cents'] = np.round(df['TransactionAmt'] - np.floor(df['TransactionAmt']), 3)\n    df['ProductCD_cents'] = df['ProductCD'].astype(str) + '_' + df['cents'].astype(str)\n    df['TransactionAmt'] = np.log1p(df['TransactionAmt'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.ProductCD_card1.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.card1_addr1.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.TransactionAmt_dist2.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.ProductCD_TransactionAmt.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.TransactionAmt.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['ProductCD_card1']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 아래 항목들은 다른 것을 만드는데만 유용하니 이제 없애도록 하겠습니다\ntrain = train.drop(['DaysFromStart','D1-DaysFromStart'], axis=1)\ntest = test.drop(['DaysFromStart','D1-DaysFromStart'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"이제 날짜 기반 항목들을 만들어 보겠습니다.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in [train, test]:\n    # 집계를위한 임시 항목들\n    df['DT'] = df['TransactionDT'].apply(lambda x: (start_date + datetime.timedelta(seconds=x)))\n    df['DT_M'] = ((df['DT'].dt.year - 2017) * 12 + df['DT'].dt.month).astype(np.int8)\n    df['DT_W'] = ((df['DT'].dt.year - 2017) * 52 + df['DT'].dt.weekofyear).astype(np.int8)\n    df['DT_D'] = ((df['DT'].dt.year - 2017) * 365 + df['DT'].dt.dayofyear).astype(np.int16)\n\n    df['DT_hour'] = df['DT'].dt.hour.astype(np.int8)\n    df['DT_day_week'] = df['DT'].dt.dayofweek.astype(np.int8)\n    df['DT_day_month'] = df['DT'].dt.day.astype(np.int8)\n\n    # 잠재적 솔로 항목\n    df['is_december'] = df['DT'].dt.month\n    df['is_december'] = (df['is_december'] == 12).astype(np.int8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Validation\n\n올바른 유효성 검사는 매우 중요합니다. \n\n로컬 점수가 리더 보드 점수와 관련이 있는 경우 모델을 로컬에서 학습 및 비교하고 때때로 제출할 수 있습니다. \n\n로컬 점수가 리더 보드 점수와 관련이 없는 경우 모델이 좋은지 여부를 실제로 추정 할 수 없습니다.\n\n데이터에 날짜가 있으므로 일종의 시계열 검증을 사용해야합니다. \n\n즉, 훈련 데이터가 항상 검증 데이터보다 시간 상으로 먼저 있어야합니다.\n\n이 커널에서는 학습을 위해 처음 80 % 데이터를, 유효성 검사를 위해 마지막 20%를 취하겠습니다. \n\n신뢰성을 보장하기 위해 다양한 랜덤 시드로 여러 모델을 훈련시키고 점수를 평균화합니다.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train['isFraud']\nX = train.drop(['isFraud'], axis=1)\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Processing the data separately\n\n검증이 올바른지 확인하기 위해 트레인 및 검증 데이터를 트레인 및 테스트 데이터를 다룰 때와 동일하게 처리해야합니다.\n\n이를 위해 두 개의 데이터 프레임을 처리하는 함수를 작성합니다.\n\n이 함수에서 모델링에 사용되지 않을 항목을 위해 remove_features 목록을 만들 것입니다.\n\n그들 중 일부는 일시적인 것이었고 일부는 쓸모 없거나 나쁜 것으로 판명되었습니다.\n\n먼저 프리퀀시 인코딩을 위한 함수를 작성해 보겠습니다.\n\n두 개의 데이터 프레임을 사용하고 이러한 데이터 프레임에서 하나의 열을 결합하고 그 안에있는 각 범주의 수를 계산합니다.\n\n카테고리 대신 이러한 값을 사용하여 새 열을 만드는 데 사용됩니다.\n\n이것은 널리 사용되는 kaggle 기술입니다.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def freq_encode_full(df1, df2, col, normalize=True):\n    \"\"\"\n    Encode\n\n    https://www.kaggle.com/cdeotte/high-scoring-lgbm-malware-0-702-0-775\n    \"\"\"\n    df = pd.concat([df1[col], df2[col]])\n    freq_dict = df.value_counts(dropna=False, normalize=normalize).to_dict()\n    col_name = col + '_freq_enc_full'\n    return col_name, freq_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_data(df_train: pd.DataFrame, df_test: pd.DataFrame):\n    # 제자리에서 변경되지 않도록 복사하십시오.\n    train_df = df_train.copy()\n    test_df = df_test.copy()\n    remove_features = ['TransactionID', 'TransactionDT', 'DT', 'DT_M', 'DT_W', 'DT_D', 'DT_hour', 'DT_day_week',\n                       'DT_day_month',\n                       'ProductCD_card1', 'card1_addr1', 'TransactionAmt_dist2', 'card3_card5', 'uid',\n                       'D5_DT_W_std_score',\n                       'ProductCD_TransactionAmt_DT_W', 'D4_DT_D_std_score', 'D15_DT_D_std_score', 'D3_DT_W_std_score',\n                       'D11_DT_W_std_score',\n                       'card3_card5_DT_W_week_day_dist', 'card5_DT_W_week_day_dist', 'D10_DT_D_std_score',\n                       'card3_card5_DT_D', 'ProductCD_cents_DT_D',\n                       'D4_DT_W_std_score', 'D15_DT_W_std_score', 'uid_DT_D', 'card3_DT_W_week_day_dist',\n                       'D10_DT_W_std_score', 'D8_DT_D_std_score',\n                       'card3_card5_DT_W', 'ProductCD_cents_DT_W', 'uid_DT_W', 'D8_DT_W_std_score',\n                       'ProductCD_TransactionAmt']\n\n    \"\"\"\n     다음 코드는 다음을 수행합니다.\n     * 학습 및 테스트 데이터에서 특정 열의 값을 결합하고 'value_counts'계산\n     *`valid_card`를 하나 이상의 값이 있는 카테고리로 정의\n     * 트레인 데이터의 열 범주도 테스트 데이터에 있으면 유지하고, 그렇지 않으면 None으로 바꿉니다.\n     * 트레인 데이터의 열 범주가`valid_card`에 있으면 유지하고, 그렇지 않으면 None으로 바꿉니다.\n     * 테스트 데이터에 대해 동일하게 수행\n    \"\"\"\n    for col in ['card1', 'ProductCD_card1', 'card1_addr1', 'TransactionAmt_dist2']:\n        valid_card = pd.concat([train_df[[col]], test_df[[col]]])\n        valid_card = valid_card[col].value_counts()\n\n        valid_card = valid_card[valid_card > 2]\n        valid_card = list(valid_card.index)\n\n        train_df[col] = np.where(train_df[col].isin(test_df[col]), train_df[col], np.nan)\n        train_df[col] = np.where(train_df[col].isin(valid_card), train_df[col], np.nan)\n\n        test_df[col] = np.where(test_df[col].isin(valid_card), test_df[col], np.nan)\n        test_df[col] = np.where(test_df[col].isin(train_df[col]), test_df[col], np.nan)\n\n    # 이전과 같이 값이 하나의 데이터 세트에만있는 경우 None으로 바꿉니다\n    for col in ['card2', 'card3', 'card4', 'card5', 'card6']:\n        train_df[col] = np.where(train_df[col].isin(test_df[col]), train_df[col], np.nan)\n        test_df[col] = np.where(test_df[col].isin(train_df[col]), test_df[col], np.nan)\n\n    ####### 투레인 데이터에서 지난달의 최대 값으로 C 열을 자릅니다.\n    i_cols = ['C' + str(i) for i in range(1, 15)]\n    for df in [train_df, test_df]:\n        for col in i_cols:\n            max_value = train_df[train_df['DT_M'] == train_df['DT_M'].max()][col].max()\n            df[col] = df[col].clip(None, max_value)\n\n    ####### V feature - NaN group agg\n    # null 값과 열 목록을 포함하는 dictionary가 됩니다.\n    nans_groups = {}\n    nans_df = pd.concat([train_df, test_df]).isna()\n\n    i_cols = ['V' + str(i) for i in range(1, 340)]\n    for col in i_cols:\n        # NaN 값을 세워 봅니다\n        cur_group = nans_df[col].sum()\n        if cur_group > 0:\n            try:\n                nans_groups[cur_group].append(col)\n            except:\n                nans_groups[cur_group] = [col]\n\n    for i, (n_group, n_cols) in enumerate(nans_groups.items()):\n        for df in [train_df, test_df]:\n            df[f'nan_group_{i}_sum'] = df[n_cols].sum(axis=1)\n            df[f'nan_group_{i}_mean'] = df[n_cols].mean(axis=1)\n            df[f'nan_group_{i}_std'] = df[n_cols].std(axis=1)\n\n    del nans_groups, nans_df\n    remove_features += i_cols\n    # 너무나 많은 공간을 차지합니다. 필요한 것만 남기고 drop 시킵니다.\n    i_cols = [i for i in i_cols if i not in ['V258', 'V306', 'V307', 'V308', 'V294']]\n    train_df = train_df.drop(i_cols, axis=1)\n    test_df = test_df.drop(i_cols, axis=1)\n\n    # frequency encoding. 인코딩한 항목과 오리지날 항목을 `remove_features`에 더합니다\n    i_cols = [\n        'ProductCD_TransactionAmt', 'ProductCD_cents', 'cents',\n        'DeviceInfo', 'DeviceInfo_device', 'DeviceInfo_version',\n        'id_30', 'id_30_device', 'id_30_version',\n        'id_31', 'id_31_device',\n        'id_33',\n    ]\n    for col in i_cols:\n        col_name, freq_dict = freq_encode_full(train_df, test_df, col)\n\n        train_df[col_name] = train_df[col].map(freq_dict).astype('float32')\n        test_df[col_name] = test_df[col].map(freq_dict).astype('float32')\n\n        remove_features.append(col)\n\n    # 원래 항목을 유지하면서 frequency encoding을 합니다. \n    i_cols = ['id_01', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07', 'id_08', 'id_09', 'id_10', 'id_11', 'id_13',\n              'id_14', 'id_17', 'id_18', 'id_19', 'id_20',\n              'id_21', 'id_22', 'id_24', 'id_25', 'id_26', 'card1', 'card2', 'card3', 'card5', 'ProductCD_card1',\n              'card1_addr1', 'TransactionAmt_dist2']\n    for col in i_cols:\n        col_name, freq_dict = freq_encode_full(train_df, test_df, col)\n\n        train_df[col_name] = train_df[col].map(freq_dict).astype('float32')\n        test_df[col_name] = test_df[col].map(freq_dict).astype('float32')\n\n    return train_df, test_df, remove_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nX_train, X_val, remove_features = process_data(X_train, X_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 참조; %%time은 CPU times와 wall time을 출력합니다 %%time은 셀 전체에 대해, %time은 첫 줄에 대한 것만을 출력합니다.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Feature engineering\n\n이미 피쳐엔지니어링을 좀 했지만, 기본적인 것 부터 시작하겠습니다.\n\n범주 항목에 label encoding부터 하겠습니다","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_engineering(df_train: pd.DataFrame, df_test: pd.DataFrame, remove_features: list):\n    train_df = df_train.copy()\n    test_df = df_test.copy()\n\n    # Label Encoding    \n    for f in train_df.columns:\n        if train_df[f].dtype == 'object' or test_df[f].dtype == 'object':\n            train_df[f] = train_df[f].fillna('unseen_before_label')\n            test_df[f] = test_df[f].fillna('unseen_before_label')\n            lbl = LabelEncoder()\n            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n            train_df[f] = lbl.transform(list(train_df[f].values))\n            test_df[f] = lbl.transform(list(test_df[f].values))\n            train_df[f] = train_df[f].astype('category')\n            test_df[f] = test_df[f].astype('category')\n\n    print('remove_features:', remove_features)\n\n    feature_columns = [col for col in list(train_df) if col not in remove_features]\n    categorical_features = [col for col in feature_columns if train_df[col].dtype.name == 'category']\n    categorical_features = [col for col in categorical_features if col not in remove_features]\n\n    print(f'train.shape : {train_df[feature_columns].shape}, test.shape : {test_df[feature_columns].shape}')\n\n    return train_df[feature_columns], test_df[feature_columns], categorical_features\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, categorical_features = feature_engineering(X_train, X_val, remove_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training the model\n\n모델을 트레인하고 우리의 접근법을 검증할 함수를 만들어 봅니다.\n\n요즘 테이블형 데이터는 그래디언트 부스팅 모델로 많이 하는데 우리도 여기서 LightGBM을 사용해 보겠습니다.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_params = {\n    'objective': 'binary',\n    'metric': 'None',\n    'learning_rate': 0.01,\n    'num_leaves': 2**8,\n    'max_bin': 255,\n    'max_depth': -1,\n    'bagging_freq': 5,\n    'bagging_fraction': 0.7,\n    'bagging_seed': seed,\n    'feature_fraction': 0.7,\n    'feature_fraction_seed': seed,\n    'first_metric_only': True,\n    'verbose': 100,\n    'n_jobs': -1,\n    'seed': seed,\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"여기서 competition metric은 roc_auc으로 표현됩니다. \n\n기본적인 LightGBM으로는 너무 느릴 수가 잇어서 여기서 우리 나름의 맞춤 평가 함수를 만들어 봅니다.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"@jit\ndef fast_auc(y_true, y_prob):\n    \"\"\"\n    fast roc_auc computation: https://www.kaggle.com/c/microsoft-malware-prediction/discussion/76013\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_true = y_true[np.argsort(y_prob)]\n    nfalse = 0\n    auc = 0\n    n = len(y_true)\n    for i in range(n):\n        y_i = y_true[i]\n        nfalse += (1 - y_i)\n        auc += y_i * nfalse\n    auc /= (nfalse * (n - nfalse))\n    return auc\n\n\ndef eval_auc(y_pred, y_true):\n    \"\"\"\n    Fast auc eval function for lgb.\n    \"\"\"\n    return 'auc', fast_auc(y_true.get_label(), y_pred), True\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_val_prediction(X_train, y_train, X_val, y_val, seed=0, seed_range=3, lgb_params=None,\n                        category_cols=None):\n    train_data = lgb.Dataset(X_train, label=y_train)\n    val_data = lgb.Dataset(X_val, label=y_val)\n\n    auc_scores = []\n    best_iterations = []\n    val_preds = np.zeros((X_val.shape[0], 3))\n\n    feature_importance_df = pd.DataFrame()\n    feature_importance_df['feature'] = X_train.columns.tolist()\n    feature_importance_df['gain_importance'] = 0\n\n    for i, s in enumerate(range(seed, seed + seed_range)):\n        seed_everything(s)\n        params = lgb_params.copy()\n        params['seed'] = s\n        params['bagging_seed'] = s\n        params['feature_fraction_seed'] = s\n\n        clf = lgb.train(params, train_data, 10000, valid_sets=[train_data, val_data],\n                        categorical_feature=categorical_features,\n                        early_stopping_rounds=500, feval=eval_auc, verbose_eval=200)\n\n        best_iteration = clf.best_iteration\n        best_iterations.append(best_iteration)\n        val_pred = clf.predict(X_val, best_iteration)\n        val_preds[:, i] = val_pred\n\n        auc = fast_auc(y_val, val_pred)\n        auc_scores.append(auc)\n        print('seed:', s, ', auc:', auc, ', best_iteration:', best_iteration)\n\n        feature_importance_df['gain_importance'] += clf.feature_importance('gain') / seed_range\n\n    auc_scores = np.array(auc_scores)\n    best_iterations = np.array(best_iterations)\n    best_iteration = int(np.mean(best_iterations))\n\n    avg_pred_auc = fast_auc(y_val, np.mean(val_preds, axis=1))\n    print(\n        f'avg pred auc: {avg_pred_auc:.5f}, avg auc: {np.mean(auc_scores):.5f}+/-{np.std(auc_scores):.5f}, avg best iteration: {best_iteration}')\n\n    feature_importance_df = feature_importance_df.sort_values(by='gain_importance', ascending=False).reset_index(\n        drop=True)\n    plt.figure(figsize=(16, 12));\n    sns.barplot(x=\"gain_importance\", y=\"feature\", data=feature_importance_df[:50])\n    plt.title('LGB Features (avg over folds)');\n\n    return feature_importance_df, best_iteration, val_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importance_df, best_iteration, val_preds = make_val_prediction(X_train, y_train, X_val, y_val, category_cols=categorical_features,\n                                                       lgb_params=lgb_params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Making prediction\n\n이제 우리는 예측을 만들어 냅니다.\n\nStratifiedFold에 대한 전체 데이터에 대한 모델을 학습 할 것입니다.\n모델에서 유효성 검사를 사용하지 않지만 과적합을 피하기 위해 고정된 수의 반복을 설정합니다.\n훈련은 폴드에서 수행됩니다.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def prediction(X, y, X_test, best_iteration, seed=seed, category_cols=None, n_folds=5):\n    print('best iteration:', best_iteration)\n    preds = np.zeros((X_test.shape[0], n_folds))\n\n    print(X.shape, X_test.shape)\n\n    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n\n    for i, (trn_idx, _) in enumerate(skf.split(X, y)):\n        fold = i + 1\n\n        tr_x, tr_y = X.iloc[trn_idx, :], y.iloc[trn_idx]\n\n        tr_data = lgb.Dataset(tr_x, label=tr_y)\n\n        clf = lgb.train(lgb_params, tr_data, best_iteration, categorical_feature=category_cols)\n        preds[:, i] = clf.predict(X_test)\n\n    return preds\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nX_full, X_test, remove_features = process_data(X, test.copy())\nX_full, X_test, categorical_features = feature_engineering(X_full, X_test, remove_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = prediction(X_full, y, X_test, best_iteration, category_cols=categorical_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['isFraud'] = np.mean(preds, axis=1)\nsub.to_csv('sub_1.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## More feature engineering\n\n이제는 D열을 위주로 작업을 해보겠습니다","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"일정 기간 (주 또는 일)이 포함된 함수를 정의하고 이 기간을 기준으로 값을 그룹화한 다음 정규화된 값을 최소 / 최대 및 평균 / 표준으로 계산하는 것으로 만들어 봅니다.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def values_normalization(dt_df, periods, columns):\n    for period in periods:\n        for col in columns:\n            new_col = col + '_' + period\n\n            dt_df[col] = dt_df[col].astype(float)\n\n            temp_min = dt_df.groupby([period])[col].agg(['min']).reset_index()\n            temp_min.index = temp_min[period].values\n            temp_min = temp_min['min'].to_dict()\n\n            temp_max = dt_df.groupby([period])[col].agg(['max']).reset_index()\n            temp_max.index = temp_max[period].values\n            temp_max = temp_max['max'].to_dict()\n\n            temp_mean = dt_df.groupby([period])[col].agg(['mean']).reset_index()\n            temp_mean.index = temp_mean[period].values\n            temp_mean = temp_mean['mean'].to_dict()\n\n            temp_std = dt_df.groupby([period])[col].agg(['std']).reset_index()\n            temp_std.index = temp_std[period].values\n            temp_std = temp_std['std'].to_dict()\n\n            dt_df['temp_min'] = dt_df[period].map(temp_min)\n            dt_df['temp_max'] = dt_df[period].map(temp_max)\n            dt_df['temp_mean'] = dt_df[period].map(temp_mean)\n            dt_df['temp_std'] = dt_df[period].map(temp_std)\n\n            dt_df[new_col + '_min_max'] = (dt_df[col] - dt_df['temp_min']) / (dt_df['temp_max'] - dt_df['temp_min'])\n            dt_df[new_col + '_std_score'] = (dt_df[col] - dt_df['temp_mean']) / (dt_df['temp_std'])\n            del dt_df['temp_min'], dt_df['temp_max'], dt_df['temp_mean'], dt_df['temp_std']\n    return dt_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_engineering(df_train: pd.DataFrame, df_test: pd.DataFrame, remove_features: list):\n    # 변경되지 않도록 복사합니다\n    train_df = df_train.copy()\n    test_df = df_test.copy()\n\n    i_cols = ['D' + str(i) for i in range(1, 16)]\n\n    ####### Values Normalization\n    i_cols.remove('D1')\n    i_cols.remove('D2')\n    i_cols.remove('D9')\n    periods = ['DT_D', 'DT_W']\n    for df in [train_df, test_df]:\n        df = values_normalization(df, periods, i_cols)\n\n    # TransactionAmt Normalization\n    periods = ['DT_D', 'DT_W']\n    for df in [train_df, test_df]:\n        df = values_normalization(df, periods, ['TransactionAmt'])\n\n    # normalize by max train value\n    for col in ['D1', 'D2']:\n        for df in [train_df, test_df]:\n            df[col + '_scaled'] = df[col] / train_df[col].max()\n\n    # frequency encoding\n    i_cols = ['D' + str(i) for i in range(1, 16)] + ['DT_D', 'DT_W', 'ProductCD_TransactionAmt', 'ProductCD_cents']\n    for col in i_cols:\n        col_name, freq_dict = freq_encode_full(train_df, test_df, col)\n\n        train_df[col_name] = train_df[col].map(freq_dict).astype('float32')\n        test_df[col_name] = test_df[col].map(freq_dict).astype('float32')\n\n        remove_features.append(col)\n\n    # Label Encoding    \n    for f in train_df.columns:\n        if train_df[f].dtype == 'object' or test_df[f].dtype == 'object':\n            train_df[f] = train_df[f].fillna('unseen_before_label')\n            test_df[f] = test_df[f].fillna('unseen_before_label')\n            lbl = LabelEncoder()\n            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n            train_df[f] = lbl.transform(list(train_df[f].values))\n            test_df[f] = lbl.transform(list(test_df[f].values))\n            train_df[f] = train_df[f].astype('category')\n            test_df[f] = test_df[f].astype('category')\n\n    print('remove_features:', remove_features)\n    print(f'train.shape : {train_df.shape}, test.shape : {test_df.shape}')\n\n    ########################### Final features list\n    feature_columns = [col for col in list(train_df) if col not in remove_features]\n    print('feature_columns:', len(feature_columns))\n    categorical_features = [col for col in feature_columns if train_df[col].dtype.name == 'category']\n    categorical_features = [col for col in categorical_features if col not in remove_features]\n\n    return train_df[feature_columns], test_df[feature_columns], categorical_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=False)\nX_train, X_val, remove_features = process_data(X_train, X_val)\nX_train, X_val, categorical_features = feature_engineering(X_train, X_val, remove_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importance_df, best_iteration, val_preds = make_val_prediction(X_train, y_train, X_val, y_val, category_cols=categorical_features,\n                                                       lgb_params=lgb_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nX_full, X_test, remove_features = process_data(X, test.copy())\nX_full, X_test, categorical_features = feature_engineering(X_full, X_test, remove_features)\npreds = prediction(X_full, y, X_test, best_iteration, category_cols=categorical_features)\nsub['isFraud'] = np.mean(preds, axis=1)\nsub.to_csv('sub_2.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## More feature engineering 2\n\n이제는 C-columns에 대한 피쳐 엔지니어링을 좀 해 보겠습니다","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_engineering(df_train: pd.DataFrame, df_test: pd.DataFrame, remove_features: list):\n    # 원본이 변경되지 않도록 복사해 사용합니다\n    train_df = df_train.copy()\n    test_df = df_test.copy()\n\n    i_cols = ['D' + str(i) for i in range(1, 16)]\n\n    ####### Values Normalization\n    i_cols.remove('D1')\n    i_cols.remove('D2')\n    i_cols.remove('D9')\n    periods = ['DT_D', 'DT_W']\n    for df in [train_df, test_df]:\n        df = values_normalization(df, periods, i_cols)\n\n    # TransactionAmt Normalization\n    periods = ['DT_D', 'DT_W']\n    for df in [train_df, test_df]:\n        df = values_normalization(df, periods, ['TransactionAmt'])\n\n    # normalize by max train value\n    for col in ['D1', 'D2']:\n        for df in [train_df, test_df]:\n            df[col + '_scaled'] = df[col] / train_df[col].max()\n\n    # frequency encoding\n    i_cols = ['D' + str(i) for i in range(1, 16)] + ['DT_D', 'DT_W', 'ProductCD_TransactionAmt', 'ProductCD_cents']\n    for col in i_cols:\n        col_name, freq_dict = freq_encode_full(train_df, test_df, col)\n\n        train_df[col_name] = train_df[col].map(freq_dict).astype('float32')\n        test_df[col_name] = test_df[col].map(freq_dict).astype('float32')\n\n        remove_features.append(col)\n\n    i_cols = ['C' + str(i) for i in range(1, 15)]\n    # 여기서는 C column의 0들을 카운트합니다\n    for df in [train_df, test_df]:\n        df['c_cols_0_bin'] = ''\n        for c in i_cols:\n            df['c_cols_0_bin'] += (df[c] == 0).astype(int).astype(str)\n    col_name, freq_dict = freq_encode_full(train_df, test_df, 'c_cols_0_bin')\n\n    train_df[col_name] = train_df['c_cols_0_bin'].map(freq_dict).astype('float32')\n    test_df[col_name] = test_df['c_cols_0_bin'].map(freq_dict).astype('float32')\n\n    for col in i_cols:\n        col_name, freq_dict = freq_encode_full(train_df, test_df, col)\n\n        train_df[col_name] = train_df[col].map(freq_dict).astype('float32')\n        test_df[col_name] = test_df[col].map(freq_dict).astype('float32')\n\n    # Label Encoding    \n    for f in train_df.columns:\n        if train_df[f].dtype == 'object' or test_df[f].dtype == 'object':\n            train_df[f] = train_df[f].fillna('unseen_before_label')\n            test_df[f] = test_df[f].fillna('unseen_before_label')\n            lbl = LabelEncoder()\n            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n            train_df[f] = lbl.transform(list(train_df[f].values))\n            test_df[f] = lbl.transform(list(test_df[f].values))\n            train_df[f] = train_df[f].astype('category')\n            test_df[f] = test_df[f].astype('category')\n\n    print('remove_features:', remove_features)\n    print(f'train.shape : {train_df.shape}, test.shape : {test_df.shape}')\n\n    ########################### Final features list\n    feature_columns = [col for col in list(train_df) if col not in remove_features]\n    print('feature_columns:', len(feature_columns))\n    categorical_features = [col for col in feature_columns if train_df[col].dtype.name == 'category']\n    categorical_features = [col for col in categorical_features if col not in remove_features]\n\n    return train_df[feature_columns], test_df[feature_columns], categorical_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=False)\nX_train, X_val, remove_features = process_data(X_train, X_val)\nX_train, X_val, categorical_features = feature_engineering(X_train, X_val, remove_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importance_df, best_iteration, val_preds = make_val_prediction(X_train, y_train, X_val, y_val, category_cols=categorical_features,\n                                                       lgb_params=lgb_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nX_full, X_test, remove_features = process_data(X, test.copy())\nX_full, X_test, categorical_features = feature_engineering(X_full, X_test, remove_features)\npreds = prediction(X_full, y, X_test, best_iteration, category_cols=categorical_features)\nsub['isFraud'] = np.mean(preds, axis=1)\nsub.to_csv('sub_3.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## More feature engineering - aggregations\n\n데이터에 대한 다양한 집계를 계산합니다.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def uid_aggregation(train_df, test_df, main_columns, uids, aggregations):\n    for main_column in main_columns:\n        for col in uids:\n            for agg_type in aggregations:\n                new_col_name = col + '_' + main_column + '_' + agg_type\n                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col, main_column]]])\n                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n                    columns={agg_type: new_col_name})\n\n                temp_df.index = list(temp_df[col])\n                temp_df = temp_df[new_col_name].to_dict()\n\n                train_df[new_col_name] = train_df[col].map(temp_df)\n                test_df[new_col_name] = test_df[col].map(temp_df)\n    return train_df, test_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_engineering(df_train: pd.DataFrame, df_test: pd.DataFrame, remove_features: list):\n    # 복사해 둡니다\n    train_df = df_train.copy()\n    test_df = df_test.copy()\n\n    i_cols = ['D' + str(i) for i in range(1, 16)]\n\n    ####### Values Normalization\n    i_cols.remove('D1')\n    i_cols.remove('D2')\n    i_cols.remove('D9')\n    periods = ['DT_D', 'DT_W']\n    for df in [train_df, test_df]:\n        df = values_normalization(df, periods, i_cols)\n\n    # TransactionAmt Normalization\n    periods = ['DT_D', 'DT_W']\n    for df in [train_df, test_df]:\n        df = values_normalization(df, periods, ['TransactionAmt'])\n\n    for col in ['D1', 'D2']:\n        for df in [train_df, test_df]:\n            df[col + '_scaled'] = df[col] / train_df[col].max()\n\n    i_cols = ['D' + str(i) for i in range(1, 16)] + ['DT_D', 'DT_W', 'ProductCD_TransactionAmt', 'ProductCD_cents']\n    for col in i_cols:\n        col_name, freq_dict = freq_encode_full(train_df, test_df, col)\n\n        train_df[col_name] = train_df[col].map(freq_dict).astype('float32')\n        test_df[col_name] = test_df[col].map(freq_dict).astype('float32')\n\n        remove_features.append(col)\n\n    i_cols = ['C' + str(i) for i in range(1, 15)]\n\n    for df in [train_df, test_df]:\n        df['c_cols_0_bin'] = ''\n        for c in i_cols:\n            df['c_cols_0_bin'] += (df[c] == 0).astype(int).astype(str)\n    col_name, freq_dict = freq_encode_full(train_df, test_df, 'c_cols_0_bin')\n\n    train_df[col_name] = train_df['c_cols_0_bin'].map(freq_dict).astype('float32')\n    test_df[col_name] = test_df['c_cols_0_bin'].map(freq_dict).astype('float32')\n\n    for col in i_cols:\n        col_name, freq_dict = freq_encode_full(train_df, test_df, col)\n\n        train_df[col_name] = train_df[col].map(freq_dict).astype('float32')\n        test_df[col_name] = test_df[col].map(freq_dict).astype('float32')\n\n    i_cols = ['TransactionAmt', 'id_01', 'id_02', 'id_05', 'id_06', 'id_09', 'id_14', 'dist1'] + ['C' + str(i) for i in\n                                                                                                  range(1, 15)]\n    uids = ['card1', 'card2', 'card3', 'card5', 'uid', 'card3_card5']\n    aggregations = ['mean', 'std']\n\n    # uIDs aggregations\n    train_df, test_df = uid_aggregation(train_df, test_df, i_cols, uids, aggregations)\n\n    i_cols = [\n                 'V258',\n                 'V306', 'V307', 'V308', 'V294'\n             ] + ['D' + str(i) for i in range(1, 16)]\n    uids = ['uid', 'card3_card5']\n    aggregations = ['mean', 'std']\n    train_df, test_df = uid_aggregation(train_df, test_df, i_cols, uids, aggregations)\n\n    # Label Encoding    \n    for f in train_df.columns:\n        if train_df[f].dtype == 'object' or test_df[f].dtype == 'object':\n            train_df[f] = train_df[f].fillna('unseen_before_label')\n            test_df[f] = test_df[f].fillna('unseen_before_label')\n            lbl = LabelEncoder()\n            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n            train_df[f] = lbl.transform(list(train_df[f].values))\n            test_df[f] = lbl.transform(list(test_df[f].values))\n            train_df[f] = train_df[f].astype('category')\n            test_df[f] = test_df[f].astype('category')\n\n    print('remove_features:', remove_features)\n    print(f'train.shape : {train_df.shape}, test.shape : {test_df.shape}')\n\n    ########################### Final features list\n    feature_columns = [col for col in list(train_df) if col not in remove_features]\n    print('feature_columns:', len(feature_columns))\n    categorical_features = [col for col in feature_columns if train_df[col].dtype.name == 'category']\n    categorical_features = [col for col in categorical_features if col not in remove_features]\n\n    return train_df[feature_columns], test_df[feature_columns], categorical_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=False)\nX_train, X_val, remove_features = process_data(X_train, X_val)\nX_train, X_val, categorical_features = feature_engineering(X_train, X_val, remove_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importance_df, best_iteration, val_preds = make_val_prediction(X_train, y_train, X_val, y_val, category_cols=categorical_features,\n                                                       lgb_params=lgb_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nX_full, X_test, remove_features = process_data(X, test.copy())\nX_full, X_test, categorical_features = feature_engineering(X_full, X_test, remove_features)\npreds = prediction(X_full, y, X_test, best_iteration, category_cols=categorical_features)\nsub['isFraud'] = np.mean(preds, axis=1)\nsub.to_csv('sub_4.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## More feature engineering - time-block frequency encoding\n\n이것은 일반적인 frequency 인코딩과 유사하지만 해당 기간 내에 값을 인코딩합니다.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def timeblock_frequency_encoding(train_df, test_df, periods, columns):\n    for period in periods:\n        for col in columns:\n            new_col = col + '_' + period\n            print('timeblock frequency encoding:', new_col)\n            train_df[new_col] = train_df[col].astype(str) + '_' + train_df[period].astype(str)\n            test_df[new_col] = test_df[col].astype(str) + '_' + test_df[period].astype(str)\n\n            temp_df = pd.concat([train_df[[new_col]], test_df[[new_col]]])\n            fq_encode = temp_df[new_col].value_counts(normalize=True).to_dict()\n\n            train_df[new_col] = train_df[new_col].map(fq_encode)\n            test_df[new_col] = test_df[new_col].map(fq_encode)\n\n            train_df[new_col] = train_df[new_col] / train_df[period + '_freq_enc_full']\n            test_df[new_col] = test_df[new_col] / test_df[period + '_freq_enc_full']\n\n    return train_df, test_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_engineering(df_train: pd.DataFrame, df_test: pd.DataFrame, remove_features: list):\n    # 복사해 둡니다\n    train_df = df_train.copy()\n    test_df = df_test.copy()\n\n    i_cols = ['D' + str(i) for i in range(1, 16)]\n\n    ####### Values Normalization\n    i_cols.remove('D1')\n    i_cols.remove('D2')\n    i_cols.remove('D9')\n    periods = ['DT_D', 'DT_W']\n    for df in [train_df, test_df]:\n        df = values_normalization(df, periods, i_cols)\n\n    # TransactionAmt Normalization\n    periods = ['DT_D', 'DT_W']\n    for df in [train_df, test_df]:\n        df = values_normalization(df, periods, ['TransactionAmt'])\n\n    for col in ['D1', 'D2']:\n        for df in [train_df, test_df]:\n            df[col + '_scaled'] = df[col] / train_df[col].max()\n\n    i_cols = ['D' + str(i) for i in range(1, 16)] + ['DT_D', 'DT_W', 'ProductCD_TransactionAmt', 'ProductCD_cents']\n    for col in i_cols:\n        col_name, freq_dict = freq_encode_full(train_df, test_df, col)\n\n        train_df[col_name] = train_df[col].map(freq_dict).astype('float32')\n        test_df[col_name] = test_df[col].map(freq_dict).astype('float32')\n\n        remove_features.append(col)\n\n    i_cols = ['C' + str(i) for i in range(1, 15)]\n\n    for df in [train_df, test_df]:\n        df['c_cols_0_bin'] = ''\n        for c in i_cols:\n            df['c_cols_0_bin'] += (df[c] == 0).astype(int).astype(str)\n    col_name, freq_dict = freq_encode_full(train_df, test_df, 'c_cols_0_bin')\n\n    train_df[col_name] = train_df['c_cols_0_bin'].map(freq_dict).astype('float32')\n    test_df[col_name] = test_df['c_cols_0_bin'].map(freq_dict).astype('float32')\n\n    for col in i_cols:\n        col_name, freq_dict = freq_encode_full(train_df, test_df, col)\n\n        train_df[col_name] = train_df[col].map(freq_dict).astype('float32')\n        test_df[col_name] = test_df[col].map(freq_dict).astype('float32')\n\n    i_cols = ['TransactionAmt', 'id_01', 'id_02', 'id_05', 'id_06', 'id_09', 'id_14', 'dist1'] + ['C' + str(i) for i in\n                                                                                                  range(1, 15)]\n    uids = ['card1', 'card2', 'card3', 'card5', 'uid', 'card3_card5']\n    aggregations = ['mean', 'std']\n\n    # uIDs aggregations\n    train_df, test_df = uid_aggregation(train_df, test_df, i_cols, uids, aggregations)\n\n    i_cols = [\n                 'V258',\n                 'V306', 'V307', 'V308', 'V294'\n             ] + ['D' + str(i) for i in range(1, 16)]\n    uids = ['uid', 'card3_card5']\n    aggregations = ['mean', 'std']\n    train_df, test_df = uid_aggregation(train_df, test_df, i_cols, uids, aggregations)\n\n    i_cols = ['ProductCD_TransactionAmt', 'ProductCD_cents', 'ProductCD_cents', 'uid', 'card3_card5']\n    periods = ['DT_D', 'DT_W']\n    train_df, test_df = timeblock_frequency_encoding(train_df, test_df, periods, i_cols)\n\n    # Label Encoding    \n    for f in train_df.columns:\n        if train_df[f].dtype == 'object' or test_df[f].dtype == 'object':\n            train_df[f] = train_df[f].fillna('unseen_before_label')\n            test_df[f] = test_df[f].fillna('unseen_before_label')\n            lbl = LabelEncoder()\n            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n            train_df[f] = lbl.transform(list(train_df[f].values))\n            test_df[f] = lbl.transform(list(test_df[f].values))\n            train_df[f] = train_df[f].astype('category')\n            test_df[f] = test_df[f].astype('category')\n\n    print('remove_features:', remove_features)\n    print(f'train.shape : {train_df.shape}, test.shape : {test_df.shape}')\n\n    ########################### Final features list\n    feature_columns = [col for col in list(train_df) if col not in remove_features]\n    print('feature_columns:', len(feature_columns))\n    categorical_features = [col for col in feature_columns if train_df[col].dtype.name == 'category']\n    categorical_features = [col for col in categorical_features if col not in remove_features]\n\n    return train_df[feature_columns], test_df[feature_columns], categorical_features\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=False)\nX_train, X_val, remove_features = process_data(X_train, X_val)\nX_train, X_val, categorical_features = feature_engineering(X_train, X_val, remove_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importance_df, best_iteration, val_preds = make_val_prediction(X_train, y_train, X_val, y_val, category_cols=categorical_features,\n                                                       lgb_params=lgb_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nX_full, X_test, remove_features = process_data(X, test.copy())\nX_full, X_test, categorical_features = feature_engineering(X_full, X_test, remove_features)\npreds = prediction(X_full, y, X_test, best_iteration, category_cols=categorical_features)\nsub['isFraud'] = np.mean(preds, axis=1)\nsub.to_csv('sub_5.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}