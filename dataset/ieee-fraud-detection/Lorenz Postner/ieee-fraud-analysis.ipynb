{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Description of the IEEE Fraud project:"},{"metadata":{},"cell_type":"markdown","source":"The idea of this project is to analyse the transaction history taken from the dataset of researchers from the IEEE Computational Intelligence Society (IEEE-CIS).\n\nBased on that data, the goal is then to predict wether a transaction is likely to be fraudulent or not."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nfrom statsmodels.distributions.empirical_distribution import ECDF\n\n# time logic:\nimport time\nimport datetime\nfrom datetime import timedelta\nfrom dateutil.relativedelta import relativedelta\n\n#sklearn\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\nfrom sklearn.metrics import precision_recall_curve\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import preprocessing\n\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\nimport pickle\n\n# Input data files are available in the \"../input/\" directory.\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Standard plotly imports\nimport plotly.express as px","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Joining all the separate CSV files into one dataframe:"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_transaction = pd.read_csv('../input/train_transaction.csv', index_col='TransactionID')\ntest_transaction = pd.read_csv('../input/test_transaction.csv', index_col='TransactionID')\n\ntrain_identity = pd.read_csv('../input/train_identity.csv', index_col='TransactionID')\ntest_identity = pd.read_csv('../input/test_identity.csv', index_col='TransactionID')\n\nsample_submission = pd.read_csv('../input/sample_submission.csv', index_col='TransactionID')\n\n#merge the two separate CSV files:\ntrain = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n\nprint(train.shape)\nprint(test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check how many transaction there are for fraud vs non-fraud\nfraud_non_fraud_df = train.groupby(by='isFraud').TransactionDT.count()\nfraud_non_fraud_df = pd.DataFrame(fraud_non_fraud_df)\nnum_transactions = fraud_non_fraud_df.sum()\nfraud_non_fraud_df['perc'] = fraud_non_fraud_df/num_transactions*100\n\nprint(fraud_non_fraud_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-> We have round 20k fraud transactions and around 570k non-fraud transactions (class imbalance.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\nplt.subplot(121)\nplt.pie(fraud_non_fraud_df.perc, colors=['g','r'],autopct='%.2f')\nplt.title('Number of fraud/ non-fraud transactions')\nplt.xlabel('Fraud/ non-fraud transaction')\nplt.ylabel('Number of transactions')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How many transactions are made on mobile vs desktop?"},{"metadata":{"trusted":true},"cell_type":"code","source":"device_type_df = train.groupby(by='DeviceType').TransactionDT.count()\ndevice_type_df.plot(kind='pie',colors=['r','g'],autopct='%1.1f%%', shadow=True, startangle=140)\nplt.title('Number of transactions by device type')\nprint(device_type_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_device_type_pivot = train.pivot_table(columns='DeviceType',index='isFraud',values='TransactionDT',aggfunc='count',margins=True)\npd.options.display.float_format = '{:.2f}'.format\ndf_device_type_pivot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#as percent of total number of transactions:\ndf_device_type_pivot_percent = df_device_type_pivot / 140810*100\n\ncm_green = sns.light_palette(\"green\", as_cmap=True)\ndf_device_type_pivot_percent.style.background_gradient(cmap=cm_green, subset=['desktop','mobile'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Proportionally, there seem to be **more fraud transactions on mobile than desktop,** considering around 39.5% of transaction \nare on mobile but the percentage of fraud transactions are both around 4% for mobile and desktop."},{"metadata":{"trusted":true},"cell_type":"code","source":"perc_desktop_fraud = round(df_device_type_pivot['desktop'].loc[1] / df_device_type_pivot['desktop'].loc[0],5)\nperc_mobile_fraud  = round(df_device_type_pivot['mobile'].loc[1] / df_device_type_pivot['mobile'].loc[0],5)\n\nprint('% of fraud for desktop: ', perc_desktop_fraud)\nprint('% of fraud for mobile: ', perc_mobile_fraud)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems like mobile transactions are more prone to fraud than desktop. The common lack of anti-virus software on mobile devices would support this claim..."},{"metadata":{},"cell_type":"markdown","source":"**Which type of cards are mostly affected by fraud?**\nThe first approach is to only look at the number of transactions, the second approach is to look at the Transaction amount of the transactions."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_card_type = train[['isFraud','card4']].reset_index()\ndf_card_type.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_card_type_pivot = pd.pivot_table(data=df_card_type,index='card4', columns='isFraud',aggfunc='count')\ndf_card_type_pivot['perc'] = df_card_type_pivot.TransactionID[1]/df_card_type_pivot.TransactionID[0]*100\ndf_card_type_pivot.sort_values(by='perc',inplace=True)\ndf_card_type_pivot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\ndf_card_type_pivot.perc.plot(kind='bar',title='Percentage of fraudulent transactions by card type')\nplt.xticks(rotation=40)\nplt.xlabel('Card type')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_card_type_amount = train[['isFraud','card4','TransactionAmt']].reset_index(drop=True)\nprint(df_card_type_amount.head(5))\ndf_card_type_amount_pivot = pd.pivot_table(data=df_card_type_amount,index='card4', columns='isFraud',aggfunc='mean')\ndf_card_type_amount_pivot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_card_type_amount_pivot.plot(kind='bar',color=['g','r'],title='Avg.transaction amount for fraud/non-fraud transactions')\nplt.ylabel('Avg transaction amount')\nplt.xticks(rotation=40)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems like 'Discover' cards have both higher average transaction amount for all transactions, but also fraudulent transactions seem to be for higher amounts."},{"metadata":{},"cell_type":"markdown","source":"### Transaction amount distribution:"},{"metadata":{"trusted":true},"cell_type":"code","source":"transaction_amts = train['TransactionAmt']\ntransaction_amts.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(transaction_amts.quantile(q=0.90))\nprint(transaction_amts.quantile(q=0.95))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"90% of transactions are under 275 USD, to make distribution more visible, we can filter for that transaction amount."},{"metadata":{"trusted":true},"cell_type":"code","source":"transaction_amts_short = transaction_amts[transaction_amts< 275]\ntransaction_amts_short.hist(bins=50)\nplt.title('Histogram for transaction amount')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nplt.hist(transaction_amts_short, normed=True, cumulative=True, label='CDF',\n         histtype='bar', alpha=0.8, color='g',bins=50)\nplt.title('ECDF for transaction amount')\nplt.xlabel('Transaction amount')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use Plotly to make some graphs:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#use plotly to make fancy graphs:\ntransaction_amts_short_df = pd.DataFrame(transaction_amts_short).reset_index(drop=True)\nprint(transaction_amts_short_df.head(5))\nfig = px.histogram(transaction_amts_short_df,x='TransactionAmt',nbins=50)\nfig.update_layout(\n    autosize=False,\n    width=500,\n    height=500,\n    paper_bgcolor=\"LightSteelBlue\",\n)\nfig.show()\nprint('Done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objects as go\nprint(transaction_amts_short_df.head(10))\nfig = go.Figure(data=[go.Histogram(x=transaction_amts_short_df.TransactionAmt, \n                                   cumulative_enabled=True,\n                                   histnorm='percent',\n                                  nbinsx=200)])\nfig.update_layout(\n    autosize=False,\n    width=500,\n    height=500,\n    title_text='ECDF for transaction amounts under 275 USD', # title of plot\n    xaxis_title_text='Transaction amount', # xaxis label\n    yaxis_title_text='Count', # yaxis label\n    bargap=0.2, # gap between bars of adjacent location coordinates\n    bargroupgap=0.1 # gap between bars \n)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Machine Learning part:"},{"metadata":{},"cell_type":"markdown","source":"The problem can be classified as a binary classification problem with class imbalance. Suitable models could be Random Forest/ XGBoost. We have categorical columns which will be 'spread out' to dummy columns and numeric columns, which can be scaled."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.head(5))\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_raw = train.drop(columns=['isFraud']).reset_index(drop=True)\ntarget_raw = train[['isFraud']].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#list all columns to get an idea what we could work with\nall_cols = data_raw.columns.values\nprint(all_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_raw.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check the column \ndata_raw[[ 'V219', 'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226',\n       'V227', 'V228', 'V229', 'V230', 'V231', 'V232', 'V233', 'V234',\n       'V235', 'V236', 'V237', 'V238', 'V239', 'V240', 'V241', 'V242',\n       'V243', 'V244', 'V245', 'V246', 'V247', 'V248', 'V249', 'V250',\n       'V251', 'V252', 'V253', 'V254', 'V255', 'V256', 'V257', 'V258',\n       'V259', 'V260', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266',\n       'V267', 'V268', 'V269', 'V270', 'V271', 'V272', 'V273', 'V274',\n       'V275', 'V276', 'V277', 'V278', 'V279', 'V280', 'V281', 'V282',\n       'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290',\n       'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298',\n       'V299', 'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306',\n       'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314',\n       'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V322',\n       'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', 'V330',\n       'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338',\n       'V339', 'id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06',\n       'id_07', 'id_08', 'id_09', 'id_10', 'id_11', 'id_12', 'id_13',\n       'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20',\n       'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27',\n       'id_28', 'id_29', 'id_30', 'id_31', 'id_32', 'id_33', 'id_34',]].describe().transpose()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Version 1**** of Feature Engineering: Only Labelencoding and variance selection."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_v1 = data_raw\ndata_v1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\ncat_cols = ['id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29',\n            'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo', 'ProductCD', 'card4', 'card6', 'M4','P_emaildomain',\n            'R_emaildomain', 'card1', 'card2', 'card3',  'card5', 'addr1', 'addr2', 'M1', 'M2', 'M3', 'M5', 'M6', 'M7', 'M8', 'M9',\n            'P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3', 'R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3']\nfor col in cat_cols:\n    if col in data_v1.columns:\n        le = LabelEncoder()\n        le.fit(list(data_v1[col].astype(str).values) + list(test[col].astype(str).values))\n        data_v1[col] = le.transform(list(data_v1[col].astype(str).values))\n        test[col] = le.transform(list(test[col].astype(str).values)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_v1.drop(columns=['TransactionDT'],axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_v1_desc = data_v1.describe()\ndata_v1_desc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculate coefficient of variation (std / mean):"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_v1_coeffVars = data_v1_desc.loc['std'] / data_v1_desc.loc['mean']\ndata_v1_coeffVars = data_v1_coeffVars.sort_values(ascending=False)\nprint(data_v1_coeffVars.head(5))\n\nplt.figure(figsize=(40,15))\ndata_v1_coeffVars.plot(kind='barh')\nplt.axvline(x=0.1, color='k', linestyle='--',label='Potential cutoff variance threshold')\nplt.legend()\nplt.title","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A coefficient of variation of ~5 (meaning columns with at least standard deviations of at least 5 times the mean):"},{"metadata":{"trusted":true},"cell_type":"code","source":"coeff_var_threshold = 0.5\ndata_v1_coeffVars_more5 = data_v1_coeffVars[data_v1_coeffVars>coeff_var_threshold]\nlen(data_v1_coeffVars_more5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_v1a = data_v1[data_v1_coeffVars_more5.index]\ndata_v1a.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check the variance of data for each column:\ndata_v1_vars = data_v1.describe().loc['std'].sort_values(ascending=False)\nplt.figure(figsize=(20,15))\ndata_v1_vars.plot(kind='barh')\nplt.axvline(x=0.1, color='k', linestyle='--',label='Potential cutoff variance threshold')\nprint(data_v1_vars.head(20))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(data_v1a, target_raw, test_size=0.15, random_state=42)\nprint('X_train shape: ', X_train.shape)\nprint('X_test shape: ', X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#new model:\n#import sys\nprint(datetime.datetime.now())\nxgboost_model_v1a = XGBClassifier()\nxgboost_model_v1a.fit(X_test, y_test)\nprint(datetime.datetime.now())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#set up functions to plot confusion matrix and precision recall curve\ndef print_confusion_matrix(\n    confusion_matrix, class_names, figsize=(3, 2.5), fontsize=14\n):\n    \"\"\"Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n    \"\"\"\n    df_cm = pd.DataFrame(confusion_matrix, index=class_names, columns=class_names)\n    fig = plt.figure(figsize=figsize)\n    try:\n        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n    except ValueError:\n        raise ValueError(\"Confusion matrix values must be integers.\")\n    heatmap.yaxis.set_ticklabels(\n        heatmap.yaxis.get_ticklabels(), rotation=0, ha=\"right\", fontsize=fontsize\n    )\n    heatmap.xaxis.set_ticklabels(\n        heatmap.xaxis.get_ticklabels(), rotation=45, ha=\"right\", fontsize=fontsize\n    )\n    plt.ylabel(\"True label\")\n    plt.xlabel(\"Predicted label\")\n    plt.title(\"Confusion matrix for classifier:\")\n    # fig.colorbar(shrink=0.8)\n    #return fig\n    \ndef plot_precision_recall_curve(\n    y_test, y_pred_proba_df, title=\"Precision/Recall Curve\"\n):\n    \"\"\"\n    feed two arrays and output precision recall Curve\n    \"\"\"\n    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba_df)\n    # In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\n    #step_kwargs = ({\"step\": \"post\"} if \"step\" in signature(plt.fill_between).parameters else {})\n    plt.step(recall, precision, color=\"b\", alpha=0.5, where=\"post\")\n    #plt.fill_between(recall, precision, alpha=0.2, color=\"b\", **step_kwargs)\n\n    plt.xlabel(\"Recall\")\n    plt.ylabel(\"Precision\")\n    plt.title(title)\n    plt.show()\n    return plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test the xgboost model:\ny_pred_xgboost_v1 = pd.DataFrame(xgboost_model_v1a.predict_proba(X_test))\ny_pred_xgboost_v1 = y_pred_xgboost_v1[1]\nplot_precision_recall_curve(y_test=y_test,y_pred_proba_df=y_pred_xgboost_v1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import VarianceThreshold\n#Remove all the features with low variance:\ndef variance_threshold_selector(data, threshold=0.01):\n    \"\"\"\n    applies a variance threshold to a dataframe and returns the dataframe instead of array\n    \"\"\"\n    selector = VarianceThreshold(threshold)\n    selector.fit(data)\n    return data[data.columns[selector.get_support(indices=True)]]\n\ndef apply_std_threshold(data,std_threshold = 0.1):\n    '''removes all the columns with standard deviation under specific value'''\n    data_dropped_low_variance = variance_threshold_selector(data,threshold=std_threshold)\n    print(data_dropped_low_variance.shape)\n    #print(type(data_dropped_low_variance))\n    return data_dropped_low_variance\n\n\ndef plot_roc_curve(fpr, tpr):\n    plt.plot(fpr, tpr, color='orange', label='ROC')\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"auc = roc_auc_score(y_test, y_pred_xgboost_v1)\nprint('AUC: %.5f' % auc)\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_xgboost_v1)\nplot_roc_curve(fpr, tpr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Version 2**: Start with a few columns that are easy to interpret first and check if there is any benefit to using them already:"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_include = ['TransactionAmt', 'ProductCD', 'card1', 'card2',\n       'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'dist1',\n       'dist2', 'P_emaildomain', 'R_emaildomain','DeviceType']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_filtered_1 = data_raw[cols_to_include]\ndata_filtered_1.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#define which columns are to be treated as numberic and which ones as categorical\nlist_categorical_cols = ['ProductCD','card4','card6','P_emaildomain','R_emaildomain','DeviceType']\nlist_numerical_cols   = [column for column in cols_to_include if column not in list_categorical_cols]\nprint('Categorical cols: ', list_categorical_cols)\nprint('Numberic cols: ', list_numerical_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO get this working:\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit_transform(data_filtered_1[list_numerical_cols])\ndata_filtered_1.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data_filtered_1[list_numerical_cols].head(5))\nscaled_data = scaler.fit_transform(data_filtered_1[list_numerical_cols])\nscaled_data_df = pd.DataFrame(scaled_data)\nprint(scaled_data_df.head(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_filtered_2 = data_filtered_1.merge(scaled_data_df,left_on=data_filtered_1.index, right_on = scaled_data_df.index)\ndata_filtered_2.drop(columns=list_numerical_cols,inplace=True)\ndata_filtered_2.drop(columns='key_0',inplace=True)\ndata_filtered_2.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#make dummy columns with categorical cols:\ndata_filtered_catCols = pd.get_dummies(data_filtered_2, columns = list_categorical_cols)\ndata_filtered_catCols.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get rid of the NaN in the data:\ndata_filtered_catCols_woNa = data_filtered_catCols.fillna(data_filtered_catCols.mean())\ndata_filtered_catCols_woNa.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check if any value in the dataframe is NaN:\nif (data_filtered_catCols_woNa.isnull().values.any()) == False:\n    print('No NaN values found in dataframe, all good.')\nelse:\n    print('You have NaN values in the dataframe that need to be filled first.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check the variance of data for each column:\ndata_vars = data_filtered_catCols_woNa.describe().loc['std'].sort_values(ascending=False)\nplt.figure(figsize=(20,15))\ndata_vars.plot(kind='barh')\nplt.axvline(x=0.1, color='k', linestyle='--',label='Potential cutoff variance threshold')\nprint(data_vars.head(20))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"actual_var_threshold = 0.02\ndata_low_var = apply_std_threshold(data_filtered_catCols_woNa,std_threshold = actual_var_threshold)\nprint(data_low_var.columns.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check the data to be used in training:\ndata_filtered_catCols_woNa.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now to applying the actual model.\nTry with Random Forest Classifier:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(data_filtered_catCols_woNa, target_raw, test_size=0.15, random_state=42)\nprint('X_train shape: ', X_train.shape)\nprint('X_test shape: ', X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of datapoints: ', data_filtered_catCols_woNa.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\nclf.fit(data_filtered_catCols_woNa, target_raw)  \n#print(clf.feature_importances_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test the model performance:\ny_pred = clf.predict_proba(X_test)\nprint(y_pred[0:5])\nprint(y_pred.min())\nprint(y_pred.max())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#make a dataframe out of the results and select the first column\ny_pred_df = pd.DataFrame(y_pred)\ny_pred_series = y_pred_df[1]\nprint(y_pred_series[0:10])\nprint(y_pred_series.min())\nprint(y_pred_series.max())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def round_up_or_down(x, threshold=0.08):\n    if x > threshold:\n        x = 1\n    else:\n        x =0\n    return x\ny_pred_custom = y_pred_series.apply(round_up_or_down)\ny_pred_custom[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nconf_matrix_1 = confusion_matrix(y_test, y_pred_custom)\nprint_confusion_matrix(conf_matrix_1,class_names = ['Non-Fraud','Fraud'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nauc = roc_auc_score(y_test, y_pred_series)\nprint('AUC: %.5f' % auc)\nplot_precision_recall_curve(y_test=y_test,y_pred_proba_df=y_pred_series)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The first iteration is obviously not very good with standard parameters, so GridsearchCV will probably get better results with better hyperparameters:"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid_test = { \n    'n_estimators': [5],\n    'max_features': ['auto'],\n    'max_depth' : [2],\n    'criterion' :['gini']\n}\nparam_grid_1 = { \n    'n_estimators': [100,200],\n    'max_features': ['auto'],\n    'max_depth' : [4,5,6],\n    'criterion' :['gini']\n}\n\nparam_grid_2 = { \n    'n_estimators': [10,20,50,100,200,300,400,500],\n    'max_features': ['auto'],\n    'max_depth' : np.arange(2,20,2),\n    'criterion' :['gini']\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndef run_model_RandomForest(data, target, param_grid):\n    \"\"\"set the structure of the model and transform the target column\"\"\"\n    rfc = RandomForestClassifier(random_state=42)\n    CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv=5)\n    CV_rfc.fit(data, target.values.ravel())\n    return CV_rfc\n\nprint(datetime.datetime.now())\nCV_rfc = run_model_RandomForest(data_filtered_catCols_woNa, target_raw,param_grid_test)\nprint(datetime.datetime.now())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test the model:\ny_pred_randForest = pd.DataFrame(CV_rfc.predict_proba(X_test))\ny_pred_randForest = y_pred_randForest[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_precision_recall_curve(y_test=y_test,y_pred_proba_df=y_pred_randForest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Try with **XGBoost model:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#check if pickled model is available\nos.listdir()\n#load the model again:\ntry:\n    model_filename = 'xgboost_v1.joblib.dat'\n    loaded_model = pickle.load(open(model_filename, \"rb\"))\n    print('Loaded model...')\nexcept Exception as e:\n    print(e)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#new model:\n#import sys\nprint(datetime.datetime.now())\nxgboost_model = XGBClassifier()\nxgboost_model.fit(data_filtered_catCols_woNa, target_raw)\nprint(datetime.datetime.now())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test the xgboost model:\ny_pred_xgboost = pd.DataFrame(xgboost_model.predict_proba(X_test))\ny_pred_xgboost = y_pred_xgboost[1]\nplot_precision_recall_curve(y_test=y_test,y_pred_proba_df=y_pred_xgboost)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#save the model:\n\nmodel_filename = 'xgboost_v1.joblib.dat'\npickle.dump(xgboost_model, open(model_filename, \"wb\"))\nos.listdir()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"auc = roc_auc_score(y_test, y_pred_xgboost)\nprint('AUC: %.5f' % auc)\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_xgboost)\nplot_roc_curve(fpr, tpr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use some more feature engineering for training data:"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}