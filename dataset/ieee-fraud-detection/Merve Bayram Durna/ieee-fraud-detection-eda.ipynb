{"cells":[{"metadata":{},"cell_type":"markdown","source":"# IEEE Fraud Detection - EDA ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Introduction","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this project, I'll deal with the problem of IEEE Fraud Detection which is a Kaggle competition. My goal is to estimate whether transactions are fraudulent with as high success as possible using the data of real-world e-commerce transactions provided by the Vesta company.\n\nThis is a binary classification problem. The target variable (isFraud) has an unbalanced distribution as with all fraud and spam detection problem.\n\nFirst, I will be interested in understanding features using visualization and analysis techniques. Next, in a different kernel, I will process the data, create new features, and build a  model.\n\nTo reach the competition and the details: https://www.kaggle.com/c/ieee-fraud-\n\nAnd also ; \nThe following kernels helped me to create this kernel. Thanks for sharing, I learned a lot from your kernels.\n\n   Leonardo Ferreira : https://www.kaggle.com/kabure/extensive-eda-and-modeling-xgb-hyperopt \n   \n   Rob Mulla : https://www.kaggle.com/robikscube/ieee-fraud-detection-first-look-and-eda\n   \n   Chris Deotte : https://www.kaggle.com/cdeotte/xgb-fraud-with-magic-0-9600\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Table Of Contents : \n* Libraries\n* Helper Functions\n* Overwiev And Loading Data\n* Exploratory Data Analysis\n    * Exploring Target Feature - isFraud\n    * Exploring Continuous Features \n    * Exploring Categorical Features\n    * Exploring Group Features (card, C, D, M, V, id )","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Libraries","execution_count":null},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport gc\nimport time\nimport datetime\nfrom contextlib import contextmanager\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn import metrics, preprocessing\n\nfrom sklearn.model_selection import KFold # StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.decomposition import PCA # KernelPCA\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\n\nfrom sklearn.preprocessing import LabelEncoder\n\n\npd.set_option('display.max_rows', 999)\npd.set_option('display.max_columns',700)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \nwarnings.filterwarnings(\"ignore\", category=FutureWarning) \nwarnings.filterwarnings(\"ignore\", category=UserWarning) \n\n#plt.style.use('ggplot')\n#color_pal = [x['color'] for x in plt.rcParams['axes.prop_cycle']]\n# pd.show_versions (as_json = False)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Helper Functions ","execution_count":null},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"trusted":false},"cell_type":"code","source":"def getCatFeatureDetail(df,cat_cols):\n    cat_detail_dict = {} \n    for col in cat_cols:\n        cat_detail_dict[col] = df[col].nunique()\n    cat_detail_df = pd.DataFrame.from_dict(cat_detail_dict, orient='index', columns=['nunique'])\n    print('There are ' + str(len(cat_cols)) + ' categorical columns.')\n    print(cat_detail_df)\n    \n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n \n    \n\n                        \ndef ploting_cnt_amt(df, col, lim=2000):\n    tmp = pd.crosstab(df[col], df['isFraud'], normalize='index') * 100\n    tmp = tmp.reset_index()\n    tmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n    total = len(df)\n    \n    plt.figure(figsize=(16,14))    \n    plt.suptitle(f'{col} Distributions ', fontsize=24)\n    \n    plt.subplot(211)\n    g = sns.countplot( x=col,  data=df, order=list(tmp[col].values))\n    gt = g.twinx()\n    gt = sns.pointplot(x=col, y='Fraud', data=tmp, order=list(tmp[col].values),\n                       color='black', legend=False, )\n    gt.set_ylim(0,tmp['Fraud'].max()*1.1)\n    gt.set_ylabel(\"%Fraud Transactions\", fontsize=16)\n    g.set_title(f\"Most Frequent {col} values and % Fraud Transactions\", fontsize=20)\n    g.set_xlabel(f\"{col} Category Names\", fontsize=16)\n    g.set_ylabel(\"Count\", fontsize=17)\n    g.set_xticklabels(g.get_xticklabels(),rotation=45)\n    sizes = []\n    for p in g.patches:\n        height = p.get_height()\n        sizes.append(height)\n        g.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.2f}%'.format(height/total*100),\n                ha=\"center\",fontsize=12) \n        \n    g.set_ylim(0,max(sizes)*1.15)\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Overview And Loading Data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"There are two different datasets for both train and test datasets: transaction and identity. The identity datasets hold information about the identity of the customer and the transaction datasets hold information about then transactions.\n\nNote: Not all transactions have corresponding identity information.","execution_count":null},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"# Load data\n\n\n\ntrain_transaction = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/train_transaction.csv\")\ntrain_identity = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/train_identity.csv\")\n\ntest_transaction = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/test_transaction.csv\")\ntest_identity = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/test_identity.csv\")\n\n# Fix column name \nfix_col_name = {testIdCol:trainIdCol for testIdCol, trainIdCol in zip(test_identity.columns, train_identity.columns)}\ntest_identity.rename(columns=fix_col_name, inplace=True)\n    \n# Reduce memory\ntrain_transaction = reduce_mem_usage(train_transaction)\ntrain_identity = reduce_mem_usage(train_identity)\n\ntest_transaction = reduce_mem_usage(test_transaction)\ntest_identity = reduce_mem_usage(test_identity)\n    \n# Merge (transaction - identity)\ntrain = train_transaction.merge(train_identity, on='TransactionID', how='left')\ntest = test_transaction.merge(test_identity, on='TransactionID', how='left')\n\n# Merge (X_train - X_test)\ntrain_test = pd.concat([train, test], ignore_index=True)\n\nprint(f'train dataset has {train.shape[0]} rows and {train.shape[1]} columns.')\nprint(f'test dataset has {test.shape[0]} rows and {test.shape[1]} columns.')\n\ndel train_transaction, train_identity, test_transaction, test_identity; x = gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_test = train_test.copy()\ntrain = train.copy()\ntest = test.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.isnull().sum().sort_values(ascending =False)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"train.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploring  Target Features - isFraud","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* In the target feature, there is an unbalanced data problem that is frequently encountered in spam and fraud detection problems.\n* 3.5% of the transactions are fraudulent.\n* I divided the train dataset into two as fraud and non-fraud in order to see the effect of fraud transactions on other features. ","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"train_fraud = train.loc[train['isFraud'] == 1]\ntrain_non_fraud = train.loc[train['isFraud'] == 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train['isFraud'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('  {:.2f}% of Transactions that are fraud in train_transaction '.format(train['isFraud'].mean() * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.countplot(x=\"isFraud\", data=train).set_title('Distribution of Target')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploring Continuous Features","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### TransactionDT","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* **TransactionDT :** is a timedelta from a given reference datetime (not an actual timestamp).\n* TransactionDT is one of the features that can cause problems. \n* It seems as if there is a time difference between testing and train operations.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.distplot(train[\"TransactionDT\"])\nsns.distplot(test[\"TransactionDT\"])\nplt.title('train vs test TransactionDT distribution')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.distplot(train_fraud[\"TransactionDT\"], color='b', label='Fraud')\nsns.distplot(train_non_fraud[\"TransactionDT\"], color='r', label ='non-Fraud')\nplt.title('Fraud vs non-Fraud TransactionDT distribution')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"START_DATE = '2015-04-22'\nstartdate = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\")\n\ntrain_test['New_Date'] = train_test['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\ntrain_test['New_Date_YMD'] = train_test['New_Date'].dt.year.astype(str) + '-' + train_test['New_Date'].dt.month.astype(str) + '-' + train_test['New_Date'].dt.day.astype(str)\ntrain_test['New_Date_YearMonth'] = train_test['New_Date'].dt.year.astype(str) + '-' + train_test['New_Date'].dt.month.astype(str)\ntrain_test['New_Date_Weekday'] = train_test['New_Date'].dt.dayofweek\ntrain_test['New_Date_Hour'] = train_test['New_Date'].dt.hour\ntrain_test['New_Date_Day'] = train_test['New_Date'].dt.day\n\n\nfig,ax = plt.subplots(4, 1, figsize=(16,15))\n\ntrain_test.groupby('New_Date_Weekday')['isFraud'].mean().to_frame().plot.bar(ax=ax[0])\ntrain_test.groupby('New_Date_Hour')['isFraud'].mean().to_frame().plot.bar(ax=ax[1])\ntrain_test.groupby('New_Date_Day')['isFraud'].mean().to_frame().plot.bar(ax=ax[2])\ntrain_test.groupby('New_Date_YearMonth')['isFraud'].mean().to_frame().plot.bar(ax=ax[3])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### TransactionAmt","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* **TransactionAmt :** The ammount of transaction.\n* I apply log transform in order to better show the distribution of data. Otherwise very large transactions skew the distribution.\n* The mean of the fraud transaction amount is larger than the mean of non - fraud transaction amount.\n* And also , the lowest and highest transaction amounts seem to be more likely to be fraudulent transactions.","execution_count":null},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"print(pd.concat([train['TransactionAmt'].quantile([.01, .1, .25, .5, .75, .9, .99]).reset_index(),\n                 train_fraud['TransactionAmt'].quantile([.01, .1, .25, .5, .75, .9, .99]).reset_index(), \n                 train_non_fraud['TransactionAmt'].quantile([.01, .1, .25, .5, .75, .9, .99]).reset_index()],\n                   axis=1, keys=['Total','Fraud', \"No Fraud\"]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(' Fraud TransactionAmt mean      :  '+str(train_fraud['TransactionAmt'].mean()))\nprint(' Non - Fraud TransactionAmt mean:  '+str(train_non_fraud['TransactionAmt'].mean()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.distplot(train_test[\"TransactionAmt\"].apply(np.log))\nplt.title('Train - Test TransactionAmt distribution')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.distplot(train_fraud[\"TransactionAmt\"].apply(np.log), label = 'Fraud | isFraud = 1')\nsns.distplot(train_non_fraud[\"TransactionAmt\"].apply(np.log), label = 'non-Fraud | isFraud = 0')\nplt.title('Fraud vs non-Fraud TransactionAmt distribution')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train['New_TransactionAmt_Bin'] = pd.qcut(train['TransactionAmt'],15)\ntrain.groupby('New_TransactionAmt_Bin')[['isFraud']].mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### dist1 & dist2","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Perhaps this could be the distance of the transaction vs. the card owner's home/work address.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n\ntrain['dist1'].plot(kind='hist',bins=5000,ax=ax1,title='dist1 distribution',logx=True)\ntrain['dist2'].plot(kind='hist',bins=5000,ax=ax2,title='dist2 distribution',logx=True)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploring  Categorical Features","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this section, I will examine the effect of categorical variables on fraud.\n\nAlthough some of the group features are categorical(like M, card, id_), I will examine them in a diffrent section.\n\n**Categorical Features**\n* ProductCD\n* addr1 & addr2\n* P_emaildomain & R_emaildomain\n* DeviceType\n* DeviceInfo","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"cat_features = ['isFraud','ProductCD','addr1', 'addr2', 'P_emaildomain','R_emaildomain','DeviceType','DeviceInfo']\nall_cat_features = cat_features+ [f'card{i}' for i in range(1,7)]+ [f'M{i}' for i in range(1,10)] + [f'id_{i}' for i in range(12,39)]\n\ngetCatFeatureDetail(train_test, cat_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ProductCD","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* W, C and R are the most frequent values.\n* 75.45% of observations belong to product W.\n* 1.97% of observations belong to product S.\n* Approximately 12% of transactions with product C are fraudulent.\n* Approximately 2% of transactions with product W are fraudulent.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"ploting_cnt_amt(train, 'ProductCD')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### addr1 - addr2\n\n* The host of the competition stated that these features are categorical even if they look numerical.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"train['addr1'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"collapsed":true,"trusted":false},"cell_type":"code","source":"train['addr2'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.loc[train['addr1'].isin(train['addr1'].value_counts()[train['addr1'].value_counts() <= 5000 ].index), 'addr1'] = \"Others\"\ntrain.loc[train['addr2'].isin(train['addr2'].value_counts()[train['addr2'].value_counts() <= 50 ].index), 'addr2'] = \"Others\"\n\ntest.loc[test['addr1'].isin(test.addr1.value_counts()[test['addr1'].value_counts() <= 5000 ].index), 'addr1'] = \"Others\"\ntest.loc[test['addr2'].isin(test.addr2.value_counts()[test['addr2'].value_counts() <= 50 ].index), 'addr2'] = \"Others\"\n\ntrain['addr1'].fillna(\"NoInf\", inplace=True)\ntest['addr1'].fillna(\"NoInf\", inplace=True)\n\ntrain['addr2'].fillna(\"NoInf\", inplace=True)\ntest['addr2'].fillna(\"NoInf\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ploting_cnt_amt(train, 'addr1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ploting_cnt_amt(train, 'addr2')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### P-emaildomain & R-emaildomain","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* We can see a very similar distribution in both email domain features.\n* We have high values in google and icloud frauds.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**P-emaildomain** \n* I will group all e-mail domains by the respective enterprises.\n* Also, I will set as \"Others\" all values with less than 500 entries.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"train['P_emaildomain'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.loc[train['P_emaildomain'].isin(['gmail.com', 'gmail']),'P_emaildomain'] = 'Google'\n\ntrain.loc[train['P_emaildomain'].isin(['yahoo.com', 'yahoo.com.mx',  'yahoo.co.uk',\n                                         'yahoo.co.jp', 'yahoo.de', 'yahoo.fr',\n                                         'yahoo.es']), 'P_emaildomain'] = 'Yahoo Mail'\ntrain.loc[train['P_emaildomain'].isin(['hotmail.com','outlook.com','msn.com', 'live.com.mx', \n                                         'hotmail.es','hotmail.co.uk', 'hotmail.de',\n                                         'outlook.es', 'live.com', 'live.fr',\n                                         'hotmail.fr']), 'P_emaildomain'] = 'Microsoft'\ntrain.loc[train['P_emaildomain'].isin(train['P_emaildomain']\\\n                                         .value_counts()[train.P_emaildomain.value_counts() <= 500 ]\\\n                                         .index), 'P_emaildomain'] = \"Others\"\ntrain['P_emaildomain'].fillna(\"NoInf\", inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"ploting_cnt_amt(train, 'P_emaildomain')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**R-emaildomain**\n* I will group all e-mail domains by the respective enterprises.\n* I will set as \"Others\" all values with less than 300 entries.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"train['R_emaildomain'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.loc[train['R_emaildomain'].isin(['gmail.com', 'gmail']),'R_emaildomain'] = 'Google'\n\ntrain.loc[train['R_emaildomain'].isin(['yahoo.com', 'yahoo.com.mx',  'yahoo.co.uk',\n                                             'yahoo.co.jp', 'yahoo.de', 'yahoo.fr',\n                                             'yahoo.es']), 'R_emaildomain'] = 'Yahoo Mail'\ntrain.loc[train['R_emaildomain'].isin(['hotmail.com','outlook.com','msn.com', 'live.com.mx', \n                                             'hotmail.es','hotmail.co.uk', 'hotmail.de',\n                                             'outlook.es', 'live.com', 'live.fr',\n                                             'hotmail.fr']), 'R_emaildomain'] = 'Microsoft'\ntrain.loc[train['R_emaildomain'].isin(train.R_emaildomain\\\n                                         .value_counts()[train['R_emaildomain'].value_counts() <= 300 ]\\\n                                         .index), 'R_emaildomain'] = \"Others\"\ntrain['R_emaildomain'].fillna(\"NoInf\", inplace=True)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ploting_cnt_amt(train, 'R_emaildomain')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### DeviceType","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Most of the fraudulent transactions were done by the mobile device.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"train['DeviceType'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ploting_cnt_amt(train, 'DeviceType')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### DeviceInfo","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"train['DeviceInfo'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train['DeviceInfo'].value_counts()[train['DeviceInfo'].value_counts() > 1000 ], 'DeviceInfo'] = \"Others\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train['DeviceInfo'].value_counts().head(20).plot(kind='barh', figsize=(15, 5), title='Top 20 Devices in Train')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_test['DeviceInfo'] = train_test['DeviceInfo'].fillna('unknown_device').str.lower()\ntrain_test['DeviceName'] = train_test['DeviceInfo'].str.split('/', expand=True)[0]\n\ntrain_test.loc[train_test['DeviceName'].str.contains('SM', na=False), 'DeviceName'] = 'Samsung'\ntrain_test.loc[train_test['DeviceName'].str.contains('SAMSUNG', na=False), 'DeviceName'] = 'Samsung'\ntrain_test.loc[train_test['DeviceName'].str.contains('GT-', na=False), 'DeviceName'] = 'Samsung'\ntrain_test.loc[train_test['DeviceName'].str.contains('Moto G', na=False), 'DeviceName'] = 'Motorola'\ntrain_test.loc[train_test['DeviceName'].str.contains('Moto', na=False), 'DeviceName'] = 'Motorola'\ntrain_test.loc[train_test['DeviceName'].str.contains('moto', na=False), 'DeviceName'] = 'Motorola'\ntrain_test.loc[train_test['DeviceName'].str.contains('LG-', na=False), 'DeviceName'] = 'LG'\ntrain_test.loc[train_test['DeviceName'].str.contains('rv:', na=False), 'DeviceName'] = 'RV'\ntrain_test.loc[train_test['DeviceName'].str.contains('HUAWEI', na=False), 'DeviceName'] = 'Huawei'\ntrain_test.loc[train_test['DeviceName'].str.contains('ALE-', na=False), 'DeviceName'] = 'Huawei'\ntrain_test.loc[train_test['DeviceName'].str.contains('-L', na=False), 'DeviceName'] = 'Huawei'\ntrain_test.loc[train_test['DeviceName'].str.contains('Blade', na=False), 'DeviceName'] = 'ZTE'\ntrain_test.loc[train_test['DeviceName'].str.contains('BLADE', na=False), 'DeviceName'] = 'ZTE'\ntrain_test.loc[train_test['DeviceName'].str.contains('Linux', na=False), 'DeviceName'] = 'Linux'\ntrain_test.loc[train_test['DeviceName'].str.contains('XT', na=False), 'DeviceName'] = 'Sony'\ntrain_test.loc[train_test['DeviceName'].str.contains('HTC', na=False), 'DeviceName'] = 'HTC'\ntrain_test.loc[train_test['DeviceName'].str.contains('ASUS', na=False), 'DeviceName'] = 'Asus'\n\ntrain_test.loc[train_test['DeviceName'].isin(train_test['DeviceName'].value_counts()[train_test['DeviceName'].value_counts() < 1000].index), 'DeviceName'] = \"Others\"\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ploting_cnt_amt(train_test, 'DeviceName')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploring Group Features (card, C, D, M, V, id )","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### card1-card6","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* The host of the competition stated that some of the features are categorical even if they look numerical like card features.\n* card1 - card6: payment card information, such as card type, card category, issue bank, country, etc.\n* card4 and card6 have 4 unique values, and the others more than 100 \n* Except card1, card features have nan values ​​so I will group them according to card1 and fill with the most common value.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"card_cols = [c for c in train.columns if 'card' in c]\ntrain[card_cols].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_test[card_cols].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"for col in card_cols:\n    print(col+'  :' + str(train[col].nunique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#f = lambda x: np.nan if x.isnull().all() else x.value_counts(dropna=False).index[0]\nfor col in ['card2','card3','card4','card5','card6']:\n    train_test[col] = train_test.groupby(['card1'])[col].transform(lambda x: x.mode(dropna=False).iat[0])\n    train_test[col].fillna(train_test[col].mode()[0], inplace=True)\n    print(col+' has : '+str(train_test[col].isnull().sum())+' missing values')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ploting_cnt_amt(train, 'card4')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ploting_cnt_amt(train, 'card6')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### C1-C14","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is 0masked.\n* All of the C features are continuous. ","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"c_cols = [c for c in train if c[0] == 'C']\ntrain[c_cols].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train[c_cols].describe()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"train[c_cols].quantile([.01, .1, .25, .5, .75, .9, .99])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#train[train['C6']>118.000]['isFraud'].mean()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"for col in c_cols:\n    print('\\n Fraud '+col+' mean    :  '+str(train_fraud[train_fraud[col]<=37.00][col].mean()))\n    print(' Non - Fraud '+col+' mean:  '+str(train_non_fraud[train_non_fraud[col]<=37.00][col].mean()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### D1-D15","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* The D Columns are \"time deltas\" from some point in the past. ","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"d_cols = ['D1','D2','D3','D4','D5','D6','D7','D8','D9','D10','D11','D12','D13','D14']\ntrain[d_cols].head()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"train[d_cols].describe()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"for col in d_cols:\n    plt.figure(figsize=(15,5))\n    plt.scatter(train['TransactionDT'] ,train[col])\n    plt.title(col + ' Vs TransactionDT')\n    plt.xlabel('Time')\n    plt.ylabel(col)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"msno.matrix(train[d_cols])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### M1-M9","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* M1-M9 :  match, such as names on card and address, etc.\n* All of the M features are categorical.\n* Values are T F or NaN except M4.\n* M4 feature appears to be different from others.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"m_cols = [c for c in train if c[0] == 'M']\nfor col in m_cols:\n    ploting_cnt_amt(train, col, lim=2500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"msno.matrix(train[m_cols]) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### V1-V339","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.\n* I will group the v features that have a similar number of nan observations.","execution_count":null},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"v_cols = [c for c in train if c[0] == 'V']\ntrain[v_cols].head()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"train[v_cols].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"v_cols = [c for c in train_test if c[0] == 'V']\nv_nan_df = train_test[v_cols].isna()\nnan_groups={}\n\nfor col in v_cols:\n    cur_group = v_nan_df[col].sum()\n    try:\n        nan_groups[cur_group].append(col)\n    except:\n        nan_groups[cur_group]=[col]\ndel v_nan_df; x=gc.collect()\n\n'''\nfor nan_cnt, v_group in nan_groups.items():\n    train_test['New_v_group_'+str(nan_cnt)+'_nulls'] = nan_cnt\n    sc = preprocessing.MinMaxScaler()\n    pca = PCA(n_components=2)\n    v_group_pca = pca.fit_transform(sc.fit_transform(train_test[v_group].fillna(-1)))\n    train_test['New_v_group_'+str(nan_cnt)+'_pca0'] = v_group_pca[:,0]\n    train_test['New_v_group_'+str(nan_cnt)+'_pca1'] = v_group_pca[:,1]\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def plot_corr(v_cols):\n    cols = v_cols + ['TransactionDT']\n    plt.figure(figsize=(15,15))\n    sns.heatmap(train[cols].corr(),cmap='RdBu_r', annot=True, center=0.0)\n    plt.title(v_cols[0]+' - '+v_cols[-1],fontsize=14)\n    plt.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"for k,v in nan_groups.items():\n    plot_corr(v)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### id1-id38","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* id1-id11 are numeric features\n* id12-id38 are categorical features.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"id_cols = [c for c in train_test if c[:2] == 'id']\n\nid_num_cols=id_cols[:11]\nid_cat_cols=id_cols[11:]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"train[id_num_cols].describe()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"for col in id_num_cols:\n    print('\\n'+col)\n    print(' Fraud mean    :  ' + str(train_fraud[col].mean()))\n    print(' Non - Fraud mean:  ' + str(train_non_fraud[col].mean()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"getCatFeatureDetail(train,id_cat_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"for col in  ['id_12', 'id_15', 'id_16', 'id_23', 'id_27', 'id_28', 'id_29']:\n    ploting_cnt_amt(train, col, lim=2500)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}