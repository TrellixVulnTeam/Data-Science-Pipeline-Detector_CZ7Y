{"cells":[{"metadata":{},"cell_type":"markdown","source":"## General information\nIn this competition you are predicting the probability that an online transaction is fraudulent, as denoted by the binary target isFraud.\n\nSubmissions are evaluated on area under the ROC curve between the predicted probability and the observed target.\n\nIn this kernel I work with IEEE Fraud Detection competition.\n\n目标就是对测试集的isFraud作预测，并使用AUC值作为得分的评价\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('ggplot')\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import StandardScaler,LabelEncoder,OneHotEncoder\nfrom sklearn.decomposition import PCA,KernelPCA\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import SimpleImputer,IterativeImputer\nfrom sklearn.svm import NuSVR, SVR\nfrom sklearn.metrics import mean_absolute_error\npd.options.display.precision = 15\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nimport xgboost as xgb\nimport time\nimport datetime\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit\nfrom sklearn import metrics\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport json\nimport gc\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef reduce_mem_usage(df, verbose=True):\n    #reduce memory of data uesd\n    start_mem = df.memory_usage(deep=True).sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n    if verbose: \n        print(f'Memory usage of dataframe is {start_mem} MB --> {end_mem} MB (Decreased by {100 * (start_mem - end_mem) / start_mem})')\n    return df\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data loading and overview\n\nData is separated into two datasets: information about the identity of the customer and transaction information. Not all transactions belong to identities, which are available. Maybe it would be possible to use additional transactions to generate new features.\n\n这里只是简单对两部分数据按照transactionid作merge，how=left。"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfolder_path = '../input/'\ntrain_identity = pd.read_csv(f'{folder_path}train_identity.csv')\ntrain_transaction = pd.read_csv(f'{folder_path}train_transaction.csv')\ntest_identity = pd.read_csv(f'{folder_path}test_identity.csv')\ntest_transaction = pd.read_csv(f'{folder_path}test_transaction.csv')\nsub = pd.read_csv(f'{folder_path}sample_submission.csv')\n# let's combine the data and work with the whole dataset\ntrain = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\ntest = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')\nidentity_col=list(train_identity.columns)\ntrasac_col=list(train_transaction.columns)\n\ndel train_identity, train_transaction, test_identity, test_transaction\ntrain=reduce_mem_usage(train)\ntest=reduce_mem_usage(test)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 以R开头的列名有：['R_emaildomain']\n* 以T开头的列名有：['TransactionID', 'TransactionDT', 'TransactionAmt']\n* 以a开头的列名有：['addr1', 'addr2']\n* 以D开头的列名有：['D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'DeviceType', 'DeviceInfo']\n* 以d开头的列名有：['dist1', 'dist2']\n* 以c开头的列名有：['card1', 'card2', 'card3', 'card4', 'card5', 'card6']\n* 以C开头的列名有：['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14']\n* 以V开头的列名有：['V1'--'V339']\n* 以M开头的列名有：['M1'-- 'M9']\n* 以i开头的列名有：['id_01'-- 'id_38']\n* 以P开头的列名有：['ProductCD', 'P_emaildomain']"},{"metadata":{},"cell_type":"markdown","source":"## Data Exploration\nCategorical Features - Transaction\n* ProductCD\n* card1 - card6\n* addr1, addr2\n* P_emaildomain\n* R_emaildomain\n* M1 - M9\n\nCategorical Features - Identity\n* DeviceType\n* DeviceInfo\n* id_12 - id_38\n根据data文档，上述特征非continuous。\n\nLet's start with identity information.\nid_01 - id_11 are continuous variables, id_12 - id_38 are categorical and the last two columns are obviously also categorical."},{"metadata":{},"cell_type":"markdown","source":"先从简单的地方开始Transaction_DT,Transaction_Amt,ProductCD,DeviceType,DeviceInfo,P_emaildomain,R_emaildomain\n\nProductCD没有缺失值。\n\nProductCD: product code, the product for each transaction"},{"metadata":{},"cell_type":"markdown","source":"# Feature engenering"},{"metadata":{"trusted":true},"cell_type":"code","source":"#3.transactionDT\n\ndef datetime_trans(train,start_date='2017-11-30'):\n    startdate=datetime.datetime.strptime(start_date,\"%Y-%m-%d\")\n    train['TransactionDT']=train['TransactionDT'].fillna(train['TransactionDT'].mean())\n    train['date']=train['TransactionDT'].apply(lambda x : datetime.timedelta(seconds=x)+startdate)\n    train['weekday']=train['date'].apply(lambda x :x.weekday())#不适合单独使用\n    train['month']=(train['date'].dt.year-2017)*12+train['date'].dt.month\n    train['hour']=train['date'].apply(lambda x :x.hour)#可以使用\n    train['day']=(train['date'].dt.year-2017)*365+train['date'].dt.dayofyear\n    train['year_weekday']=train['date'].apply(lambda x : str(x.year)+'_'+str(x.weekday()))#有一定的偏度，但较为平坦\n    train['weekday_hour']=train['date'].apply(lambda x :str(x.weekday())+'_'+str(x.hour))#波动性质较好\ndate_col=['weekday','month','day','hour','year_weekday','weekday_hour']\ndatetime_trans(train)\ndatetime_trans(test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#it's most important trick ,card1-card3 and card5 seems like some serial number\ndef addNewFeatures(data): \n    data['uid'] = data['card1'].astype(str)+'_'+data['card2'].astype(str)\n\n    data['uid2'] = data['uid'].astype(str)+'_'+data['card3'].astype(str)+'_'+data['card5'].astype(str)\n\n    data['uid3'] = data['uid2'].astype(str)+'_'+data['addr1'].astype(str)+'_'+data['addr2'].astype(str)\n    data['uid4'] = data['addr1'].astype(str)+'_'+data['addr2'].astype(str)\n    data['D9'] = np.where(data['D9'].isna(),0,1)\n    \n    return data\n\ntrain = addNewFeatures(train)\ntest = addNewFeatures(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"agg_cols = ['card1','card2','card3','card5','uid','uid2','uid3','uid4']\ndef add_agg_col(col_prefix,agg_col,col_suffix='TransactionAmt'):\n    if isinstance(agg_col,list):\n        temp_df=pd.concat([train[[col_prefix,col_suffix]],test[[col_prefix,col_suffix]]])\n        temp_df=temp_df.groupby(col_prefix)[col_suffix].agg(agg_col)\n        for c in agg_col:\n            new_col=col_prefix+'_'+c+'_'+col_suffix\n            train[new_col]=train[col_prefix].map(temp_df[c])#problem is here temp_df.columns\n            test[new_col]=test[col_prefix].map(temp_df[c])\n    else:\n        raise TypeError('agg_col must be List')\n\nfor i in agg_cols:\n    add_agg_col(i,['mean','std'])\n    print(f'{i} for [\\'mean\\',\\'std\\'] aggregate is done!')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain['id_02_to_mean_card1'] = train['id_02'] / train.groupby(['card1'])['id_02'].transform('mean')\ntrain['id_02_to_mean_card4'] = train['id_02'] / train.groupby(['card4'])['id_02'].transform('mean')\ntrain['id_02_to_std_card1'] = train['id_02'] / train.groupby(['card1'])['id_02'].transform('std')\ntrain['id_02_to_std_card4'] = train['id_02'] / train.groupby(['card4'])['id_02'].transform('std')\n\ntest['id_02_to_mean_card1'] = test['id_02'] / test.groupby(['card1'])['id_02'].transform('mean')\ntest['id_02_to_mean_card4'] = test['id_02'] / test.groupby(['card4'])['id_02'].transform('mean')\ntest['id_02_to_std_card1'] = test['id_02'] / test.groupby(['card1'])['id_02'].transform('std')\ntest['id_02_to_std_card4'] = test['id_02'] / test.groupby(['card4'])['id_02'].transform('std')\n\ntrain['D15_to_mean_card1'] = train['D15'] / train.groupby(['card1'])['D15'].transform('mean')\ntrain['D15_to_mean_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('mean')\ntrain['D15_to_std_card1'] = train['D15'] / train.groupby(['card1'])['D15'].transform('std')\ntrain['D15_to_std_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('std')\n\ntest['D15_to_mean_card1'] = test['D15'] / test.groupby(['card1'])['D15'].transform('mean')\ntest['D15_to_mean_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('mean')\ntest['D15_to_std_card1'] = test['D15'] / test.groupby(['card1'])['D15'].transform('std')\ntest['D15_to_std_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('std')\n\ntrain['D15_to_mean_addr1'] = train['D15'] / train.groupby(['addr1'])['D15'].transform('mean')\ntrain['D15_to_mean_addr2'] = train['D15'] / train.groupby(['addr2'])['D15'].transform('mean')\ntrain['D15_to_std_addr1'] = train['D15'] / train.groupby(['addr1'])['D15'].transform('std')\ntrain['D15_to_std_addr2'] = train['D15'] / train.groupby(['addr2'])['D15'].transform('std')\n\ntest['D15_to_mean_addr1'] = test['D15'] / test.groupby(['addr1'])['D15'].transform('mean')\ntest['D15_to_mean_addr2'] = test['D15'] / test.groupby(['addr2'])['D15'].transform('mean')\ntest['D15_to_std_addr1'] = test['D15'] / test.groupby(['addr1'])['D15'].transform('std')\ntest['D15_to_std_addr2'] = test['D15'] / test.groupby(['addr2'])['D15'].transform('std')\n\ntrain=train.replace([np.inf,-np.inf],np.nan)\ntest=test.replace([np.inf,-np.inf],np.nan)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#P and R emaildomain's np.nan is float type\ndef p_r_domain(train,test,col):\n    train[col]=train[col].fillna('Nan').apply(lambda x : x.split('.')[0])\n    test[col]=test[col].fillna('Nan').apply(lambda x : x.split('.')[0])\n    \nfor c in ['P_emaildomain','R_emaildomain']:\n    p_r_domain(train,test,c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#add 'others' mark threshold=0.95 exclude 'na's\ndef add_others_mark(train,test,categ_col):\n    temp_df=pd.concat([train[[categ_col]],test[[categ_col]]])\n    series=temp_df[categ_col].value_counts(normalize=True).cumsum()\n    others_index=list(series[series>0.95].index)\n    if len(others_index)!=0:\n        train[categ_col]=train[categ_col].apply(lambda x : 'others' if x in others_index else x)\n        test[categ_col]=test[categ_col].apply(lambda x : 'others' if x in others_index else x)\n        print(f'{categ_col}:{len(others_index)} of {len(series)} feature values has been replaced to \\'others\\'')\nmail_col=['P_emaildomain', 'R_emaildomain',\n          'DeviceInfo',\n          'id_30','id_33']       \n\nfor c in mail_col:\n    add_others_mark(train,test,c)\nadd_others_mark(train,test,mail_col[3])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#set frequency\nfreq_cols = ['card1','card2','card3','card5',\n          'C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14',\n          'D1','D2','D3','D4','D5','D6','D7','D8',\n          'addr1','addr2',\n          'dist1','dist2',\n          'P_emaildomain', 'R_emaildomain',\n          'DeviceInfo','DeviceType',\n          'id_30','id_33',\n          'uid','uid2','uid3','uid4'\n         ]+date_col\n\ndef set_freq_col(train,test,col):\n    prefix='_fq'\n    temp_df=pd.concat([train[[col]],test[[col]]])\n    fq=temp_df[col].value_counts(dropna=False)\n    train[col+prefix]=train[col].map(fq)\n    test[col+prefix]=test[col].map(fq)\n    \nfor c in freq_cols:\n    set_freq_col(train,test,c)\n    \n\nperiods = ['month','year_weekday','weekday_hour']\nuids = ['uid','uid2','uid3','uid4']\ndef set_uid_period(train,test,periods,uids):\n    for period in periods:\n        for col in uids:\n            new_column = col + '_' + period\n\n            temp_df = pd.concat([train[[col,period]], test[[col,period]]])\n            temp_df[new_column] = temp_df[col].astype(str) + '_' + (temp_df[period]).astype(str)\n            fq_encode = temp_df[new_column].value_counts()\n\n            train[new_column] = (train[col].astype(str) + '_' + train[period].astype(str)).map(fq_encode)\n            test[new_column]  = (test[col].astype(str) + '_' + test[period].astype(str)).map(fq_encode)\n\n            train[new_column] /= train[period+'_fq']\n            test[new_column]  /= test[period+'_fq']\n            \nset_uid_period(train,test,periods,uids)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preprossesing"},{"metadata":{"trusted":true},"cell_type":"code","source":"#去掉na较多，单值占比较大的列。threshold=0.85\ntr_na_count=train.isnull().sum()/len(train)\ntr_drop_cols=[c for c in train.columns if tr_na_count[c]>0.85]\ntr_big_cols=[c for c in train.columns if train[c].value_counts(normalize=True,dropna=False).values[0]>0.85]\ndrop_cols=list(set(tr_drop_cols+tr_big_cols))\ndrop_cols.remove('isFraud')\ny_=train['isFraud']\ntrain.drop(columns=drop_cols+['isFraud'],inplace=True)\ntest.drop(columns=drop_cols,inplace=True)\n#15去掉多余的列。\nexcess_col=['date','TransactionDT','TransactionID']\n\ntrain.drop(columns=excess_col,inplace=True)\ntest.drop(columns=excess_col,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#use sklearn.imputer object instead of pd.fillna()\n\ndef imputing_na(train,col):\n    #use sklearn.imputer object instead of pd.fillna()\n    if train[col].dtypes == object:\n        imp=SimpleImputer(strategy='constant',fill_value='Nan').fit_transform(train[col].values.reshape(-1,1))\n        train[col]=pd.Series(imp[:,0])\n    else:\n        imp=SimpleImputer(strategy='constant',fill_value=-999).fit_transform(train[col].values.reshape(-1,1))\n        train[col]=pd.Series(imp[:,0])\nfor c in train.columns:\n    imputing_na(train,c)\n    imputing_na(test,c)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#label encoder categorical columns\nnumerical_cols = train.select_dtypes(exclude = 'object').columns\ncategorical_cols = train.select_dtypes(include = 'object').columns\n\ndef labelencoder(train,test,col):\n    cod=list(train[col].values)+list(test[col].values)\n    le=LabelEncoder().fit(cod)\n    train[col]=le.transform(train[col])\n    test[col]=le.transform(test[col])\n    \nfor c in categorical_cols:\n    labelencoder(train,test,c)\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"\"\"\"\n#many columns\n\"\"\"amount_col=['TransactionAmt']\nid_cols=[c for c in train.columns if c[0]=='i']\nid_numerical_col=[c for c in id_cols if train[c].dtypes != object]\nid_category_col=[c for c in id_cols if c not in id_numerical_col]\ndevice=['DeviceType','DeviceInfo']\ndist_col=['dist1']\naddr_col=['addr1','addr2']\nv_col=[c for c in train.columns if c[0]=='V']\nm_col=[c for c in train.columns if c[0]=='M']\nd_col=[c for c in train.columns if c[0]=='D' ]\nd_col=list(set(d_col)^set(device))\nc_col=[c for c in train.columns if c[0]=='C']\ncard_cols=['card1', 'card2', 'card3', 'card4', 'card5', 'card6']\ncard_numerical_col=[c for c in card_cols if train[c].dtypes != object]\ncard_category_col=[c for c in card_cols if c not in card_numerical_col]\n\"\"\"\nss_col=list(set(numerical_cols)^set(v_col))\ndef standarscaler_(train,test,cols):\n    sca=StandardScaler()\n    sca.fit(pd.concat([train[cols],test[cols]]))\n    train[cols]=sca.transform(train[cols])\n    test[cols]=sca.transform(test[cols])\n\nstandarscaler_(train,test,ss_col)\ngc.collect()\n\"\"\""},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape,test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training With LGBM"},{"metadata":{},"cell_type":"markdown","source":"从此前的结果看，在numerical features中主要利用到的还是amount和其衍生出来的mean,std,freq等信息。像V系列基本没用到，D系列还能深度挖掘，C系列也可以。\nLGBM的参数调整一头雾水。貌似早停在1200-1600之间就停了，这个意味着什么，是不是好事，不得而知。\n\n这两点就是后面的主要问题。"},{"metadata":{"trusted":true},"cell_type":"code","source":"#training LGBM\nparams = {'num_leaves': int((2**10)*0.72),\n          'min_child_weight': 0.17,\n          'feature_fraction': 0.72,\n          'bagging_fraction': 0.72,\n          'min_data_in_leaf': 179,\n          'objective': 'binary',\n          'max_depth': -1,\n          'learning_rate': 0.006,\n          \"boosting_type\": \"gbdt\",\n          \"bagging_seed\": 13,\n          \"metric\": 'auc',\n          \"verbosity\": -1,\n          'reg_alpha': 0.3299927210061127,\n          'reg_lambda': 0.3885237330340494,\n          'random_state': 4,\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nNFOLDS = 7\nfolds = KFold(n_splits=NFOLDS)\n\ncolumns = train.columns\nsplits = folds.split(train, y_)\ny_preds = np.zeros(test.shape[0])\ny_oof = np.zeros(train.shape[0])\nscore = 0\n\nfeature_importances = pd.DataFrame()\nfeature_importances['feature'] = columns\n  \nfor fold_n, (train_index, valid_index) in enumerate(splits):\n    X_train, X_valid = train[columns].iloc[train_index], train[columns].iloc[valid_index]\n    y_train, y_valid = y_.iloc[train_index], y_.iloc[valid_index]\n    \n    dtrain = lgb.Dataset(X_train, label=y_train)\n    dvalid = lgb.Dataset(X_valid, label=y_valid)\n\n    clf = lgb.train(params, dtrain, 10000, valid_sets = [dtrain, dvalid], verbose_eval=200, early_stopping_rounds=300)\n    \n    feature_importances[f'fold_{fold_n + 1}'] = clf.feature_importance()\n    \n    y_pred_valid = clf.predict(X_valid)\n    y_oof[valid_index] = y_pred_valid\n    print(f\"Fold {fold_n + 1} | AUC: {roc_auc_score(y_valid, y_pred_valid)}\")\n    \n    score += roc_auc_score(y_valid, y_pred_valid) / NFOLDS\n    y_preds += clf.predict(test) / NFOLDS\n    \n    del X_train, X_valid, y_train, y_valid\n    gc.collect()\n    \nprint(f\"\\nMean AUC = {score}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsub['isFraud']= y_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('submission.csv', index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances['average'] = feature_importances[[f'fold_{fold_n + 1}' for fold_n in range(folds.n_splits)]].mean(axis=1)\n\nplt.figure(figsize=(16, 16))\nsns.barplot(data=feature_importances.sort_values(by='average', ascending=False).head(50), x='average', y='feature');\nplt.title('50 TOP feature importance over {} folds average'.format(NFOLDS));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb.create_tree_digraph(clf)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}