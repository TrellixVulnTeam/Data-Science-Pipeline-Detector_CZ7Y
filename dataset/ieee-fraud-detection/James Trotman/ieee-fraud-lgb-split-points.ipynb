{"cells":[{"cell_type":"markdown","metadata":{},"source":"# IEEE-CIS Fraud Detection &mdash; LightGBM Split Points\n\nThis notebook shows some techniques to snoop on the gradient boosting process used by LightGBM - using its own APIs.\n\nBy counting the split points used in the decision trees, we can see the ways the algorithm divides the input space up. This may lead to new insights about what indicates fraud, and may help in smoothing or binning the data to reduce splits that model only noise.\n\nFor more info on LightGBM see [pdf by Microsoft][3] or the [LightGBM github][4].\n\nFor another example of gradient boosting model analysis with XGBoost see the great [xgbfi][2] tool by [Faron][1].\n\n___\n\nWe start by building a model...\n\n [1]: https://www.kaggle.com/mmueller\n [2]: https://github.com/Far0n/xgbfi\n [3]: https://www.microsoft.com/en-us/research/wp-content/uploads/2017/11/lightgbm.pdf\n [4]: https://github.com/Microsoft/LightGBM\n"},{"cell_type":"code","execution_count":194,"metadata":{"_kg_hide-input":true},"outputs":[],"source":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport gc, os, sys, re, time\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\nfrom collections import Counter, defaultdict\nimport matplotlib.pyplot as plt\nfrom IPython.display import display, HTML"},{"cell_type":"code","execution_count":4,"metadata":{"_kg_hide-input":true},"outputs":[],"source":"DTYPE = {\n    'TransactionID': 'int32',\n    'isFraud': 'int8',\n    'TransactionDT': 'int32',\n    'TransactionAmt': 'float32',\n    'ProductCD': 'category',\n    'card1': 'int16',\n    'card2': 'float32',\n    'card3': 'float32',\n    'card4': 'category',\n    'card5': 'float32',\n    'card6': 'category',\n    'addr1': 'float32',\n    'addr2': 'float32',\n    'dist1': 'float32',\n    'dist2': 'float32',\n    'P_emaildomain': 'category',\n    'R_emaildomain': 'category',\n}\n\nIDX = 'TransactionID'\nTGT = 'isFraud'\n\nCCOLS = [f'C{i}' for i in range(1, 15)]\nDCOLS = [f'D{i}' for i in range(1, 16)]\nMCOLS = [f'M{i}' for i in range(1, 10)]\nVCOLS = [f'V{i}' for i in range(1, 340)]\n\nDTYPE.update((c, 'float32') for c in CCOLS)\nDTYPE.update((c, 'float32') for c in DCOLS)\nDTYPE.update((c, 'float32') for c in VCOLS)\nDTYPE.update((c, 'category') for c in MCOLS)\n\nIN_DIR = '../input'\n\nNR = None\n\ntran = pd.read_csv(f'{IN_DIR}/train_transaction.csv', index_col=IDX, nrows=NR, dtype=DTYPE)\ntran.shape"},{"cell_type":"code","execution_count":201,"metadata":{"ExecuteTime":{"end_time":"2019-09-18T20:13:33.910882Z","start_time":"2019-09-18T20:13:33.899969Z"}},"outputs":[],"source":"# utility: encode binary 0/1 columns as bits in a single integer\ndef encode_bits(binary_df):\n    ncols = binary_df.shape[1]\n    assert ncols < 64\n    return binary_df @ (1 << np.arange(ncols))"},{"cell_type":"markdown","metadata":{},"source":"Add count features..."},{"cell_type":"code","execution_count":5,"metadata":{"ExecuteTime":{"end_time":"2019-09-17T22:48:02.486244Z","start_time":"2019-09-17T22:48:02.477906Z"}},"outputs":[],"source":"to_count = tran.columns[2:].tolist()\n\nfor c in to_count:\n    s = tran[c]\n    if hasattr(s, 'cat'):\n        s = s.cat.codes\n    vc = s.value_counts(dropna=False)\n    tran[f'{c}_count'] = s.map(vc).astype(np.int32)"},{"cell_type":"markdown","metadata":{},"source":"Add some simple extra features."},{"cell_type":"code","execution_count":195,"metadata":{"ExecuteTime":{"end_time":"2019-09-18T19:14:05.462358Z","start_time":"2019-09-18T19:14:05.37056Z"}},"outputs":[],"source":"tran['TimeInDay'] = tran.TransactionDT % 86400\ntran['Cents'] = tran.TransactionAmt % 1\ntran['C_bin'] = encode_bits(tran[CCOLS]>0)\ntran['D_bin'] = encode_bits(tran[DCOLS].isnull())\ntran['M_bin'] = encode_bits(tran[MCOLS].isnull())\ntran['addr_bin'] = encode_bits(tran[['addr1','addr2','dist1','dist2']].isnull())\ntran['email_bin'] = encode_bits(tran[['R_emaildomain','P_emaildomain']].isnull())"},{"cell_type":"markdown","metadata":{},"source":"Simple time based validation split, first 75% is training data, rest is validation."},{"cell_type":"code","execution_count":198,"metadata":{"ExecuteTime":{"end_time":"2019-09-18T19:24:57.969692Z","start_time":"2019-09-18T19:24:57.347032Z"}},"outputs":[],"source":"split = tran.TransactionDT.quantile(0.75)\nistrain = tran.TransactionDT < split\ntrain_df = tran.loc[istrain]\nvalid_df = tran.loc[~istrain]\nprint(train_df.shape, valid_df.shape)"},{"cell_type":"code","execution_count":180,"metadata":{"ExecuteTime":{"end_time":"2019-09-18T15:32:46.369217Z","start_time":"2019-09-18T15:32:46.351826Z"}},"outputs":[],"source":"params = {\n    'num_leaves': 64,\n    'objective': 'binary',\n    'min_data_in_leaf': 12,\n    'learning_rate': 0.01,\n    'feature_fraction': 0.6,\n    'bagging_fraction': 0.9,\n    'bagging_freq': 1,\n    'max_cat_to_onehot': 128,\n    'metric': 'auc',\n    'num_threads': 8,\n    'seed': 42,\n}"},{"cell_type":"markdown","metadata":{},"source":"## Note &mdash; TransactionDT\n\nI will use all columns as features, even `TransactionDT` which is terrible as a feature - none of the test set values overlap with the training set values. By leaving it in here though we get to see if there are any hotspots of `TransactionDT` that get frequently used as a split point, showing us a potential *regime change* or shift in fraud behaviour."},{"cell_type":"code","execution_count":10,"metadata":{"ExecuteTime":{"end_time":"2019-09-17T22:48:59.697498Z","start_time":"2019-09-17T22:48:07.418859Z"}},"outputs":[],"source":"y_tr = train_df[TGT].values\ny_va = valid_df[TGT].values\n\nuse = [c for c in train_df.columns if c != TGT]\ntrain = train_df[use]\nvalid = valid_df[use]\n\ndtrain = lgb.Dataset(train, y_tr, params=params)\ndvalid = lgb.Dataset(valid, y_va, params=params, reference=dtrain)\n\nclf = lgb.train(params,\n                dtrain,\n                num_boost_round=3000,\n                valid_sets=(dvalid,),\n                early_stopping_rounds=100,\n                verbose_eval=100)"},{"cell_type":"code","execution_count":11,"metadata":{"ExecuteTime":{"end_time":"2019-09-17T22:49:01.069349Z","start_time":"2019-09-17T22:48:59.707348Z"}},"outputs":[],"source":"roc_auc_score(y_va, clf.predict(valid))"},{"cell_type":"markdown","metadata":{},"source":"Save the model - it saves the trees in an easy to parse text format. (The file won't be used here but it is useful in general to save.)"},{"cell_type":"code","execution_count":217,"metadata":{"ExecuteTime":{"end_time":"2019-09-18T20:51:52.693686Z","start_time":"2019-09-18T20:51:52.61372Z"}},"outputs":[],"source":"_ = clf.save_model('ieee_fraud_lgb_model.txt')"},{"cell_type":"markdown","metadata":{},"source":"# Booster.dump_model()\n\nThe returned LightGBM model format is hierarchical, trees are nested `dict` objects containing `left_child` and `right_child` subtrees. Walking over the trees and summarizing the splits can be done with a short recursive function...\n\n    tree_info  - list of dicts\n    (each contains):\n        tree_structure\n            left_child\n            right_child\n\nThe `dump_model()` information records 'gain' at each split, and we simply re-use that."},{"cell_type":"code","execution_count":156,"metadata":{"ExecuteTime":{"end_time":"2019-09-18T14:47:22.781256Z","start_time":"2019-09-18T14:47:22.777528Z"}},"outputs":[],"source":"# uncomment to see model structure\n# clf.dump_model(num_iteration=2)['tree_info']"},{"cell_type":"code","execution_count":28,"metadata":{"ExecuteTime":{"end_time":"2019-09-17T22:53:53.279455Z","start_time":"2019-09-17T22:53:53.266548Z"}},"outputs":[],"source":"# NOTE: lightgbm.Booster has a new get_split_value_histogram API which counts split points used.\n# This code pre-dates that, and sums gain instead of counting appearances.\ndef get_split_point_stats(clf):\n    split_points = defaultdict(Counter)\n    names = clf.feature_name()\n\n    def visit_node(d):\n        if 'tree_info' in d:\n            for tree in d['tree_info']: # a list of trees\n                visit_node(tree)\n        for k in ['tree_structure', 'left_child', 'right_child' ]:\n            if k in d:\n                visit_node(d[k])\n        if 'split_feature' in d:\n            split_points[names[d['split_feature']]] [d['threshold']] += d['split_gain']\n\n    visit_node(clf.dump_model())\n    return split_points"},{"cell_type":"code","execution_count":14,"metadata":{"ExecuteTime":{"end_time":"2019-09-17T22:49:01.587902Z","start_time":"2019-09-17T22:49:01.144093Z"}},"outputs":[],"source":"split_points = get_split_point_stats(clf)"},{"cell_type":"markdown","metadata":{},"source":"Each feature indexes a Counter object in the `split_points` dict. In each Counter, the keys are feature values, and the values are sum of gain, for example, 3.5 is the most used value in feature `C1`:"},{"cell_type":"code","execution_count":94,"metadata":{"ExecuteTime":{"end_time":"2019-09-18T00:11:52.314395Z","start_time":"2019-09-18T00:11:52.305038Z"}},"outputs":[],"source":"split_points['C1'].most_common(5)"},{"cell_type":"markdown","metadata":{},"source":"Dump all the split point data to an xlsx file (can be opened with open-source *Open Office* or *[Libre Office][1]*)\n\n [1]: https://www.libreoffice.org/download/download/"},{"cell_type":"code","execution_count":17,"metadata":{"ExecuteTime":{"end_time":"2019-09-17T22:49:04.193923Z","start_time":"2019-09-17T22:49:01.631973Z"}},"outputs":[],"source":"with pd.ExcelWriter('ieee_fraud_split_points.xlsx') as writer:\n    for feat in use:\n        counter = split_points[feat]\n        df = pd.Series(counter, name=feat).sort_index().to_frame('GainSum')\n        df.to_excel(writer, feat, index_label=feat)\n\n    for sheet in writer.sheets.values():\n        sheet.set_column(0, 0, 30)"},{"cell_type":"markdown","metadata":{},"source":"# Plotting Code\n\nWarning: this only shows the 50 split points with the most gain, so the x-axis will be a bit nonlinear, some values won't appear. See the xlsx file for all the values."},{"cell_type":"code","execution_count":20,"metadata":{"ExecuteTime":{"end_time":"2019-09-17T22:49:04.23432Z","start_time":"2019-09-17T22:49:04.227174Z"}},"outputs":[],"source":"MAX_SHOW = 50"},{"cell_type":"code","execution_count":1,"metadata":{"ExecuteTime":{"end_time":"2019-09-19T22:39:25.235905Z","start_time":"2019-09-19T22:39:24.877176Z"},"_kg_hide-input":true},"outputs":[],"source":"ADJS = 'abundant:common:ubiquitous:omnipresent:rampant:rife:permeant:widespread:legendary:popular:fashionable:frequent:usual:useful:predominant:recurrent:repetitive:repetitious:marked:prevalent:prevalent:prevalent'.split(':')\nCOLORS = [\n    'tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple',\n    'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan'\n]\nGAINS = pd.Series(index=clf.feature_name(), data=clf.feature_importance('gain'))\nCOUNTS = pd.Series(index=clf.feature_name(), data=clf.feature_importance())\nnp.random.seed(42)\n\ndef plot_it(col):\n    counts = split_points[col]\n    ser = pd.Series(dict(counts)).sort_values(ascending=False)\n    if hasattr(tran[col], 'cat'):\n        # remap categories from int -> cat value\n        try:\n            ser.index = tran[col].cat.categories[ser.index.astype(int)]\n        except:\n            # e.g. TypeError: Cannot cast Index to dtype <class 'int'>\n            # a categorical with many categories and '1||4||7' etc type splits\n            # leave it as it is\n            pass\n    adj = np.random.choice(ADJS)\n    display(\n        HTML(\n            f'<h1 id=\"plot_{col}\">{col}</h1>'\n            f'<p>Used {COUNTS[col]} times, total gain is {GAINS[col]}.'\n            f'<p>{len(ser)} split point values used. '\n            f'Most {adj} is {ser.index[0]} with gain of {ser.values[0]}.'\n        )\n    )\n    ser = ser.head(MAX_SHOW).sort_index()\n    ax = ser.plot.bar(title=f'{col} — Split points by gain',\n                      rot=90, fontsize=12, figsize=(15,5),\n                      width=0.7, color=COLORS)\n    plt.show()"},{"cell_type":"markdown","metadata":{},"source":"# Plots For IEEE Features\n\nAll the features with 4 or more unique values are shown (to avoid \"Too many output files (max 500)\" error).\n\n## Notes\n\nMost of the split points have long decimal values like `379.00000000000006` - the LightGBM algorithm only sees binned data, so it sets split thresholds as values [halfway between neighbouring bin lower/upper edges][6], but bumped upwards a tiny fraction using `std::nextafter` in the [C++ standard library][5], resulting in strangely precise [floating point format][1] values :)\n\nZero is checked for using a [kZeroThreshold = 1e-35f][7] variable - this comes out of the model as a split point of 1.0000000180025095e-35 &mdash; a tiny number. When you see that, think *zero*.\n\nSplit points for categorical dtypes depends on the `max_cat_to_onehot` which I have set to 128 - so categoricals in this data set are treated with a one-vs-all split. This means `feature==value` in the node split test, instead of the usual `feature<=value`. `max_cat_to_onehot` is by default set to 4, meaning categories with more values than this use splits based on target statistics, and the resulting split points have values like `1||3||5||7||8||9` which indicate which category codes go down the *left* branch. (But this is hard to show in bar charts... hence I used *one-vs-all splits*.)\n\nLightGBM keeps a separate bin for NaN values and at all node tests, records whether that bin goes left/right separately - this is not shown here (yet! Upvote to make me attempt something!)\n\n## What to Look For\n\nIn some ways what we **don't** see is more interesting than what we **do**. As with normal feature importances: if we see a feature is not used at all it is clearly redudant and should be removed. So seeing low gain (0) is very reliable but seeing high gain can *can* be misleading - it may be fitting noise.\n\nIf there is **one prominent peak** it means the feature acts a bit like a boolean, and perhaps would be better fed into the model that way (e.g. seeing a split value of 100, for feature 'foo', you could instead change column 'foo' to `foo<=100`.)  \n\nSimilarly if there are **several prominent peaks** it could imply the feature should be discretized/binned in a pre-processing step, as the less-often used split points may just be picking up on noise. See this [interesting old discussion thread on this subject of feature discretization][2].\n\nFor the TimeOfDay feature you may like to check out [my time series heatmaps notebook][4] that shows the density of transactions over time, and clearly indicates night time.\n\n [1]: https://en.wikipedia.org/wiki/Double-precision_floating-point_format\n [2]: https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/43886\n [3]: https://www.kaggle.com/tilii7\n [4]: https://www.kaggle.com/jtrotman/ieee-fraud-time-series-heatmaps\n [5]: https://en.cppreference.com/w/cpp/numeric/math/nextafter\n [6]: https://github.com/microsoft/LightGBM/blob/master/src/io/bin.cpp\n [7]: https://github.com/microsoft/LightGBM/blob/master/include/LightGBM/meta.h "},{"cell_type":"code","execution_count":216,"metadata":{"ExecuteTime":{"end_time":"2019-09-18T20:27:38.218468Z","start_time":"2019-09-18T20:27:30.843737Z"},"scrolled":false},"outputs":[],"source":"for col in use:\n    counts = split_points[col]\n    if len(counts) >= 4:\n        plot_it(col)"},{"cell_type":"markdown","metadata":{},"source":"# Gain Over Time\n\nThis part is more for illustration/teaching about gradient boosting.\n\nAs well as counting split points, we can look at how feature gain evolves as trees are added to the model.\n\nIn the gradient boosting learning process, each tree adds something to the training set predictions that moves the overall predictions closer to the target. It takes small steps towards lowering the loss function. Early trees are more like standard decision trees, fitting the big patterns. Later trees are more specialised, correcting small deviations, fine-grained wrinkles in the loss function: often little patterns, sometimes noise.\n\nAs features are incorporated into the model in early trees, their predictive power can run out, which is most notable for boolean features; at some point the existing predictions have accounted for all of the variance of the feature and they are no longer used in new trees.\n\nWe can see this by looking at gain statistics over time by passing the `iteration` parameter to the `feature_importance()` method.\n"},{"cell_type":"code","execution_count":190,"metadata":{"ExecuteTime":{"end_time":"2019-09-18T19:01:00.607167Z","start_time":"2019-09-18T19:01:00.594365Z"}},"outputs":[],"source":"def make_importances(clf, divisions):\n    max_n = clf.num_trees()\n    thres = np.arange(1, divisions+1) / divisions\n\n    idx = pd.Index(clf.feature_name(), name='Feature')\n    importances = pd.DataFrame(index=idx)\n\n    for t in thres:\n        n = int(max_n * t)\n        c = f'count_{t*100:.0f}'\n        dat = clf.feature_importance(iteration=n)\n        importances[c] = pd.Series(dat, index=idx).astype(np.int32)\n\n    for t in thres:\n        n = int(max_n * t)\n        c = f'gain_{t*100:.0f}'\n        dat = clf.feature_importance('gain', iteration=n)\n        importances[c] = pd.Series(dat, index=idx).astype(np.float32)\n\n    return importances"},{"cell_type":"code","execution_count":191,"metadata":{"ExecuteTime":{"end_time":"2019-09-18T19:01:01.265386Z","start_time":"2019-09-18T19:01:01.238761Z"}},"outputs":[],"source":"importances = make_importances(clf, 5)"},{"cell_type":"markdown","metadata":{},"source":"`V258` and `V258_count` reach high gain by 20% of the way through and are not used much after that. `card1` and `card2` are still being used with high gain throughout."},{"cell_type":"code","execution_count":192,"metadata":{"ExecuteTime":{"end_time":"2019-09-18T19:01:01.960114Z","start_time":"2019-09-18T19:01:01.931426Z"}},"outputs":[],"source":"importances.sort_values('gain_100', ascending=False).head(10)"},{"cell_type":"code","execution_count":196,"metadata":{"ExecuteTime":{"end_time":"2019-09-18T19:21:54.40033Z","start_time":"2019-09-18T19:21:54.368642Z"}},"outputs":[],"source":"importances.to_csv('ieee_fraud_lgb_importances.csv')"},{"cell_type":"markdown","metadata":{},"source":"The blue bar indicates gain at 40% of the way through the learning process, and red marks the gain at the end."},{"cell_type":"code","execution_count":193,"metadata":{"ExecuteTime":{"end_time":"2019-09-18T19:01:11.074608Z","start_time":"2019-09-18T19:01:05.874689Z"}},"outputs":[],"source":"toplot = importances.sort_values('gain_100').tail(80)\ntoplot['gain_100'].plot.barh(figsize=(12,20), legend=True, color='red', title='Feature Gain at 40% and 100%')\ntoplot['gain_40'].plot.barh(figsize=(12,20), legend=True, color='royalblue')"},{"cell_type":"markdown","metadata":{},"source":"Here we see `V258` and `V258_count` have *run out of steam* early, whilst other features are still gaining in importance..."},{"cell_type":"code","execution_count":178,"metadata":{"ExecuteTime":{"end_time":"2019-09-18T15:30:13.663407Z","start_time":"2019-09-18T15:30:13.142279Z"}},"outputs":[],"source":"gaincols = importances.columns[importances.columns.str.startswith('gain')].tolist()\ntoplot = importances.sort_values('gain_100', ascending=False).head(10).copy()\ntoplot['gain_0'] = 0\ntoplot[['gain_0'] + gaincols].T.plot(figsize=(14,6), title='Cumulative Gain')"},{"cell_type":"markdown","metadata":{},"source":"A different plot, show the most used features at the 20% point of training, and how their gain evolves after that..."},{"cell_type":"code","execution_count":179,"metadata":{"ExecuteTime":{"end_time":"2019-09-18T15:30:21.977868Z","start_time":"2019-09-18T15:30:21.443297Z"}},"outputs":[],"source":"gaincols = importances.columns[importances.columns.str.startswith('gain')].tolist()\ntoplot = importances.sort_values('count_20', ascending=False).head(10).copy()\ntoplot['gain_0'] = 0\ntoplot[['gain_0'] + gaincols].T.plot(figsize=(14,6), title='Cumulative Gain')"},{"cell_type":"markdown","metadata":{},"source":"# Conclusions\n\nNow we can inspect trained models to see **which points** in the feature space matter for fraud detection... You can build this in to your pipeline to help with reducing the resolution of the data in later modelling iterations, and aid further feature engineering.\n\nIf this kernel gets enough votes I will apply it to adversarial validation too to see **where** in the feature space the train and test set differ the most. Perhaps this could even be integrated into an *auto-relaxing* function that buckets the data for us in a way that makes the  train and test sets more similar, without any tedious manual inspection of plots :)\n\n<font color=red>Update</font>: [adversarial version here][2].\n\n___\n\nA note to any n00bs reading: the original features used here are only a starting point, used just to demonstrate. If (say) `DeviceInfo` of `hi6210sft Build/MRA58K` comes along in the training set and makes a fast burst of transactions (all marked fraud), then appears in the test set but spread out and on many separate days, it does not make sense to predict a high fraud likelihood, simply because of that one feature. Features that capture *event* timing & behaviour are needed :)\n\nFor inspiration you should check out [an **extensive** index of **winning** and high ranking Kaggle **solutions** here][1] (and upvote if this helps you find something useful &mdash; I guarantee there are useful links there ;)\n\n [1]: https://www.kaggle.com/jtrotman/high-ranking-solution-posts\n [2]: https://www.kaggle.com/jtrotman/ieee-fraud-adversarial-lgb-split-points\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":2}