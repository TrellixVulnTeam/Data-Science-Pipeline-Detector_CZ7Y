{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport gc\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import roc_auc_score\nimport random\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n# Any results you write to the current directory are saved as output.\n# Helper function for column value details\ndef column_value_freq(sel_col,cum_per):\n    dfpercount = pd.DataFrame(columns=['col_name','num_values_'+str(round(cum_per,2))])\n    for col in sel_col:\n        col_value = train_transaction[col].value_counts(normalize=True)\n        colpercount = pd.DataFrame({'value' : col_value.index,'per_count' : col_value.values})\n        colpercount['cum_per_count'] = colpercount['per_count'].cumsum()\n        if len(colpercount.loc[colpercount['cum_per_count'] < cum_per,] ) < 2:\n            num_col_99 = len(colpercount.loc[colpercount['per_count'] > (1- cum_per),])\n        else:\n            num_col_99 = len(colpercount.loc[colpercount['cum_per_count']< cum_per,] )\n        dfpercount=dfpercount.append({'col_name': col,'num_values_'+str(round(cum_per,2)): num_col_99},ignore_index = True)\n    dfpercount['unique_values'] = train_transaction[sel_col].nunique().values\n    dfpercount['unique_value_to_num_values'+str(round(cum_per,2))+'_ratio'] = 100 * (dfpercount['num_values_'+str(round(cum_per,2))]/dfpercount.unique_values)\n    dfpercount['percent_missing'] = percent_na(train_transaction[sel_col])['percent_missing'].round(3).values\n    return dfpercount\n\ndef column_value_details(sel_col,cum_per):\n    dfpercount = pd.DataFrame(columns=['col_name','values_'+str(round(cum_per,2)),'values_'+str(round(1-cum_per,2))])\n    for col in sel_col:\n        col_value = train_transaction[col].value_counts(normalize=True)\n        colpercount = pd.DataFrame({'value' : col_value.index,'per_count' : col_value.values})\n        colpercount['cum_per_count'] = colpercount['per_count'].cumsum()\n        if len(colpercount.loc[colpercount['cum_per_count'] < cum_per,] ) < 2:\n            values_freq = colpercount.loc[colpercount['per_count'] > (1- cum_per),'value'].tolist()\n        else:\n            values_freq = colpercount.loc[colpercount['cum_per_count']< cum_per,'value'].tolist() \n        values_less_freq =  [item for item in colpercount['value'] if item not in values_freq]\n        dfpercount=dfpercount.append({'col_name': col,'values_'+str(round(cum_per,2)) : values_freq ,'values_'+str(round(1-cum_per,2)): values_less_freq},ignore_index = True)\n    num_values_per =[]\n    for i in range(len(dfpercount)):\n        num_values_per.append(len(dfpercount['values_'+str(round(cum_per,2))][i]))\n    dfpercount['num_values_per'] = num_values_per\n    return dfpercount\n\n# Helper functions\n# 1. For calculating % na values in  columns\ndef percent_na(df):\n    percent_missing = df.isnull().sum() * 100 / len(df)\n    missing_value_df = pd.DataFrame({'column_groups': percent_missing.index,\n                                 'percent_missing': percent_missing.values})\n    return missing_value_df\n# 2. For plotting grouped histograms \ndef sephist(col):\n    yes = train_transaction[train_transaction['isFraud'] == 1][col]\n    no = train_transaction[train_transaction['isFraud'] == 0][col]\n    return yes, no\n\ndef plot_feature_distribution(df,features,label1='1',label2='0',target_field = 'isFraud'):\n    df1 = df.loc[df[target_field] == 1].copy();\n    df2 = df.loc[df[target_field] == 0].copy();\n    i = 0;\n    sns.set_style('whitegrid');\n    #plt.figure(figsize=((10,8)));\n    \n    fig,ax = plt.subplots(1,3,figsize=[17,20]);\n    \n    for feature in features:\n        i += 1\n        plt.subplot(3,3,i);\n        sns.distplot(df1[feature],label=label1);\n        sns.distplot(df2[feature],label=label2);\n        plt.xlabel(feature,fontsize=9);\n        locs, labels = plt.xticks();\n        plt.tick_params(axis='x',which='major',labelsize=8,pad=1);\n        plt.tick_params(axis='y',which='major',labelsize=8);\n    plt.show();\n    \ndef get_score(df_temp,KEYS,feature,flip=True):\n    \n    #df_temp[feature+'mu'] = df_temp[KEYS].min(axis=1)\n    df_temp[feature] = df_temp[KEYS].skew(axis=1)\n    #df_temp.loc[df_temp[feature+'mu'] == 0,feature] = np.nan\n    dff = df_temp[[feature,'isFraud']].copy().dropna()\n    \n    the_score = roc_auc_score(dff['isFraud'],dff[feature])\n    if the_score < .5:\n        df_temp[feature] = -df_temp[feature] + df_temp[feature].min()\n        dff = df_temp[[feature,'isFraud']].copy().dropna()\n        return roc_auc_score(dff['isFraud'],dff[feature])\n    \n    return the_score\n\n\ndef check_score(score,CHANGED,KEY_LIST,new_score,KEY_LIST_TEMP,what='remove'):\n    if new_score > score - OFFSET:\n        BREAK   = False\n        CHANGED = True\n        if (random.random()  < .5) or (score < new_score):\n            print(FEATURE,' ',what,' new high score: ',new_score)# ,KEY_LIST)\n            score    = new_score\n            KEY_LIST = KEY_LIST_TEMP\n            BREAK   = True\n    return score,CHANGED,KEY_LIST,BREAK","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### V Feature Engineering\nHere we look at Features derived from the V fields"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# setup data\ntrain_transaction = pd.read_csv('../input/ieee-fraud-detection/train_transaction.csv')\ntest_transaction  = pd.read_csv('../input/ieee-fraud-detection/test_transaction.csv')\ntransaction       = pd.concat([train_transaction,test_transaction],sort=False)\n\n# find patterns\npd.options.display.max_colwidth = 300\nVcols=train_transaction.columns[train_transaction.columns.str.startswith('V')]\ntrain_transaction_vcol_na = percent_na(train_transaction[Vcols])\ntrain_transaction_vcol_na_group= train_transaction_vcol_na.groupby('percent_missing')['column_groups'].unique().reset_index()\nnum_values_per =[]\nfor i in range(len(train_transaction_vcol_na_group)):\n    num_values_per.append(len(train_transaction_vcol_na_group['column_groups'][i]))\ntrain_transaction_vcol_na_group['num_columns_group'] = num_values_per\n\n# delete data\ndel train_transaction, test_transaction\ngc.collect()\n\n# print out data\ntrain_transaction_vcol_na_group","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def get_plot(df_temp,KEYS,feature,flip=True):\n    \n    df_temp[feature] = df_temp[KEYS].mean(axis=1)\n    for key_in in KEYS:\n        df_temp.loc[transaction[feature] == 0,key_in] = np.nan\n        \n    df_temp[feature] = df_temp[KEYS].kurt(axis=1)\n    dff = df_temp[[feature,'isFraud']].copy().dropna()\n    \n    if flip:\n        dff[feature] = -dff[feature] + dff[feature].max()\n    plt.figure(figsize = (8, 8))\n    sns.distplot(dff.loc[dff['isFraud'] == 0, feature], label = 'target == 0',kde=False,norm_hist = True)\n    sns.distplot(dff.loc[dff['isFraud'] == 1, feature] , label = 'target == 1',kde=False,norm_hist = True)\n    plt.show()\n    \n    \nADDED_SET = []\nKEEP_FEATURES = ['TransactionID']\nTOTAL_KEY_LIST = []\nby   = ['card1', 'card2', 'card3', 'card4', 'card5', 'card6']\nfor n_key in np.linspace(0,14,15):\n    \n    # get feature list\n    v_list = list(train_transaction_vcol_na_group.column_groups[n_key])\n\n    # setup feature names\n    feature1 = 'super_combo_sum_'+ str(int(n_key))\n    feature2 = 'super_combo_zscore_'+ str(int(n_key))\n    feature3 = 'super_member_zscore_'+ str(int(n_key))\n    \n    # add features to list\n    KEEP_FEATURES += [feature1,feature2,feature3]\n    \n    # \n    transaction[feature1] = 0\n    transaction[feature2] = 0\n    good_list = []\n    FEATURE_2 = []\n    \n    for key in v_list:\n\n        if (transaction[key].min() != 0) | (transaction[key].nunique() < 6):\n            continue\n\n        # append to total key list\n        TOTAL_KEY_LIST.append(key)\n        \n        # append key to list\n        good_list.append(key)\n        \n        # generate log transform\n        XNORM = np.log(transaction[key] - transaction[key].min() + 1)\n        \n        # add log transform to feature 1\n        transaction[feature1] += XNORM\n        \n        # z-score normalize\n        transaction[key] = (XNORM - XNORM.mean())/XNORM.std()\n        \n        # save data under feature two\n        transaction[feature2] += transaction[key]       \n        \n        # append feature 2\n        FEATURE_2.append(feature2)\n        \n    # take secondary log of data\n    transaction[feature1] = np.log(transaction[feature1] + 1) \n    \n    # transform features\n    transaction[feature3] = (transaction[feature1] - transaction.groupby(by)[feature1].transform('mean'))/(transaction.groupby(by)[feature1].transform('std')+1)\n        \n    dff = transaction[[feature1,feature2,feature3,'isFraud']].copy().dropna()    \n    # plot data\n    plot_feature_distribution(dff,[feature1,feature2,feature3])\n       ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"#KEEP_FEATURES += ['V_std_feature']\ntransaction[KEEP_FEATURES].to_pickle('transaction.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"\n# reset transactions\n'''\ntransaction = get_transactions(transaction)\n\nfor the_key in TOTAL_KEY_LIST:\n    transaction[the_key] = (transaction[the_key] - transaction[the_key].min())/(transaction[the_key].max() - transaction[the_key].min())\n'''\n\n'''\ngc.collect()\n\n\n# define list of new features\nfeature  = 'adv_group_1_mu'\nKEY_LIST =  ['V303', 'V317', 'V318', 'V133', 'V281', 'V75', 'V79', 'V81', 'V87', 'V38', 'V40', 'V201', 'V242', 'V243', 'V244', 'V247', 'V248', 'V249', 'V252', 'V274', 'V146', 'V147', 'V156', 'V158', 'V163', 'V320', 'V68', 'V27', 'V312', 'V28', 'V302', 'V304', 'V17', 'V19', 'V13', 'V18', 'V33', 'V21', 'V74', 'V309', 'V34', 'V59', 'V15', 'V54', 'V67', 'V89', 'V44', 'V308', 'V16', 'V45', 'V227', 'V259', 'V184', 'V211', 'V332', 'V294', 'V36', 'V52', 'V210', 'V213']\nKEY_LIST = TOTAL_KEY_LIST\nscore = get_score(transaction,KEY_LIST,feature)\nprint('original score: ',score)\n\nCHANGED = True\nEXIT    = True\nOFFSET  = .0000\n\nwhile EXIT:\n    print('')\n    print('new loop')\n    OFFSET = OFFSET*.9\n    \n    for FEATURE in TOTAL_KEY_LIST:\n        if FEATURE in KEY_LIST:\n            continue\n\n        KEY_LIST_TEMP = KEY_LIST + [FEATURE]\n        new_score =  get_score(transaction,KEY_LIST_TEMP,feature)\n        \n        # check to update score \n        if new_score > score - OFFSET:\n            score,CHANGED,KEY_LIST,BREAK = check_score(score,CHANGED,KEY_LIST,new_score,KEY_LIST_TEMP,what='add')\n            if BREAK & (random.random() < .2):\n                break\n     \n    print('')\n    print('Stage II')\n    gc.collect()\n    \n    # if changed is false do not exit\n    if CHANGED == False:  EXIT = False\n        \n    # reset changed\n    CHANGED      = False\n    \n    # reset key list save\n    KEY_LIST_SAVE = KEY_LIST\n    \n    # get features in key list\n    for FEATURE in KEY_LIST_SAVE:\n\n        KEY_LIST_TEMP = [col for col in KEY_LIST if col not in  [FEATURE]]\n        new_score =  get_score(transaction,KEY_LIST_TEMP,feature)\n\n        # check to update score \n        if new_score > score:\n            score,CHANGED,KEY_LIST,BREAK = check_score(score,CHANGED,KEY_LIST,new_score,KEY_LIST_TEMP)\n            if BREAK & (random.random() < .05):\n                break\n    gc.collect()\n    if CHANGED == False:  EXIT = False\n'''\n\n\n'''\nSTD_KEYS = ['V303', 'V317', 'V318', 'V133', 'V281', 'V75', 'V79', 'V81', 'V87', 'V38', 'V40', 'V201', 'V242', 'V243', 'V244', 'V247', 'V248', 'V249', 'V252', 'V274', 'V146', 'V147', 'V156', 'V158', 'V163', 'V320', 'V68', 'V27', 'V312', 'V28', 'V302', 'V304', 'V17', 'V19', 'V13', 'V18', 'V33', 'V21', 'V74', 'V309', 'V34', 'V59', 'V15', 'V54', 'V67', 'V89', 'V44', 'V308', 'V16', 'V45', 'V227', 'V259', 'V184', 'V211', 'V332', 'V294', 'V36', 'V52', 'V210', 'V213']\n\ntransaction['V_std_feature'] = np.log(1+transaction[STD_KEYS].mean(axis=1))\ntransaction['V_std_feature'] = (transaction['V_std_feature'] - transaction.groupby(by)['V_std_feature'].transform('mean'))/(transaction.groupby(by)['V_std_feature'].transform('std')+1)\n        \nfor feature in ['V_std_feature']:\n    \n    dff = transaction[[feature,'isFraud']].copy().dropna()\n    print('score: ',roc_auc_score(dff['isFraud'],dff[feature]))\n    dff = transaction[[feature,'isFraud']].copy().dropna()\n    \n    plt.figure(figsize = (8, 8))\n    sns.distplot(dff.loc[dff['isFraud'] == 0, feature], label = 'target == 0',kde=False,norm_hist = True)\n    sns.distplot(dff.loc[dff['isFraud'] == 1, feature] , label = 'target == 1',kde=False,norm_hist = True)\n    plt.show()\n\n'''\n\n\n\n'''# get feature list\nv_list = list(train_transaction_vcol_na_group.column_groups[0])\n\nfor key in v_list:\n    \n    tr = transaction.loc[transaction.isFraud.isna() &transaction[key].notna(),key]\n    te = transaction.loc[transaction.isFraud.notna() &transaction[key].notna(),key]\n    tr = np.log(tr + 1)\n    te = np.log(te + 1)\n    \n    plt.figure(figsize = (8, 8))\n    sns.distplot(tr, label = 'train',kde=False,norm_hist = True)\n    sns.distplot(te, label = 'test',kde=False,norm_hist = True)\n    plt.show()\n# get feature list\nv_list = list(train_transaction_vcol_na_group.column_groups[0])\n\nby   = ['card1', 'card2', 'card3', 'card4', 'card5', 'card6']\n\ntransaction['DBOX'] = (transaction['super_combo_sum_0'] - transaction.groupby(by)['super_combo_sum_0'].transform('mean'))#/(transaction.groupby(by)['super_combo_sum_0'].transform('std')+1)\n\nFEAT = 'super_combo_sum_0'\n\n# plot data\nfor feature in ['DBOX','super_combo_sum_0']:\n    transaction['DBOX'] = transaction.groupby(by)[FEAT].transform('nunique')\n    dff = transaction[[feature,'isFraud']].copy().dropna()\n    plt.figure(figsize = (8, 8))\n    sns.distplot(dff.loc[dff['isFraud'] == 0, feature], label = 'target == 0',kde=False,norm_hist = True)\n    sns.distplot(dff.loc[dff['isFraud'] == 1, feature] , label = 'target == 1',kde=False,norm_hist = True)\n    plt.show()\n    \n'''","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}