{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Transaction Fraud Detection\n\n## Content\n\n### 1. [Introduction](#Introduction)\n\n### 2. [Data](#data)\n \n  2.1 [Load Data](#load)\n  \n  2.2 [Data Management](#data_management)\n   \n### 3. [Models](#models) \n\n  3.1 [Logistic Regression](#logit)\n  \n 3.2 [LightGBM](#LightGBM)\n \n 3.3 [Xgboost](#xgboost)\n \n 3.4 [Random Forest](#rf)\n \n  3.5 [Stacking Models](#stacking)\n\n  \n### 4. [Predictions](#predictions)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Introduction\"></a>\n# 1. Introduction\n\nCompetition Description:  \n\nImagine standing at the check-out counter at the grocery store with a long line behind you and the cashier not-so-quietly announces that your card has been declined. In this moment, you probably aren’t thinking about the data science that determined your fate.  \n    \nEmbarrassed, and certain you have the funds to cover everything needed for an epic nacho party for 50 of your closest friends, you try your card again. Same result. As you step aside and allow the cashier to tend to the next customer, you receive a text message from your bank. “Press 1 if you really tried to spend $500 on cheddar cheese.”  \n\nWhile perhaps cumbersome (and often embarrassing) in the moment, this fraud prevention system is actually saving consumers millions of dollars per year. Researchers from the IEEE Computational Intelligence Society (IEEE-CIS) want to improve this figure, while also improving the customer experience. With higher accuracy fraud detection, you can get on with your chips without the hassle.  \n\nIn this competition, the teams will benchmark machine learning models on a challenging large-scale dataset. The data comes from Vesta's real-world e-commerce transactions and contains a wide range of features from device type to product features. New features are created  to improve the results.\n\nEvaluation:  \nSubmissions are evaluated on area under the ROC curve between the predicted probability and the observed target.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"data\"></a>\n# 2. Data\n<a id=\"load\"></a>\n## 2.1 Load Data\nKaggle provide the script to pull data from given path.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Load data\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#get training data and testing data\ntrain_identity= pd.read_csv(\"/kaggle/input/ieee-fraud-detection/train_identity.csv\", index_col='TransactionID')\ntrain_transaction= pd.read_csv(\"/kaggle/input/ieee-fraud-detection/train_transaction.csv\", index_col='TransactionID')\ntest_identity= pd.read_csv(\"/kaggle/input/ieee-fraud-detection/test_identity.csv\", index_col='TransactionID')\ntest_transaction= pd.read_csv(\"/kaggle/input/ieee-fraud-detection/test_transaction.csv\", index_col='TransactionID')\nsubmission= pd.read_csv(\"/kaggle/input/ieee-fraud-detection/sample_submission.csv\", index_col='TransactionID')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Reduce Memory Usage","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#reduce memory usage by setting the proper data structure\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('object')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n#    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\n\n#apply the function on the datasets\ntrain_transaction=reduce_mem_usage(train_transaction)\ntrain_identity=reduce_mem_usage(train_identity)\ntest_transaction=reduce_mem_usage(test_transaction)\ntest_identity=reduce_mem_usage(test_identity)\nsubmission=reduce_mem_usage(submission)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"data_management\"></a>\n## 2.2 Data Management","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Merge the identity table with transaction table","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# join the identity table and transaction table together\ntrain = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\nx_test = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')\n\n#make the column name of train and test consistent(modify column name)\ntrain = train.rename(columns=lambda x: x.replace('_','').replace(\"-\",\"\"))\nx_test = x_test.rename(columns=lambda x: x.replace('_','').replace(\"-\",\"\"))\n\ny_train=train['isFraud']\nx_train=train.copy()\nx_train.drop('isFraud', axis=1, inplace=True)\n\nfor col in x_train.columns:\n    x_test[col]=x_test[col].astype(x_train[col].dtype)\n\n#delete the unnecessary datasets\ndel train_identity,train_transaction,test_identity,test_transaction, train\n\n#show data size\nprint(\"\\nThe x_train data size  is : {} \".format(x_train.shape)) \nprint(\"The test data size  is : {} \".format(x_test.shape))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create new features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\n\ndef id_split(dataframe):\n    dataframe['device_name'] = dataframe['DeviceInfo'].str.split('/', expand=True)[0]\n    dataframe['device_version'] = dataframe['DeviceInfo'].str.split('/', expand=True)[1].astype('object')\n\n    dataframe['OS_id_30'] = dataframe['id30'].str.split(' ', expand=True)[0]\n    dataframe['version_id_30'] = dataframe['id30'].str.split(' ', expand=True)[1].astype('object')\n\n    dataframe['browser_id_31'] = dataframe['id31'].str.split(' ', expand=True)[0]\n    dataframe['version_id_31'] = dataframe['id31'].str.split(' ', expand=True)[1].astype('object')\n\n    dataframe['screen_width'] = dataframe['id33'].str.split('x', expand=True)[0]\n    dataframe['screen_height'] = dataframe['id33'].str.split('x', expand=True)[1]\n\n    dataframe['id34'] = dataframe['id34'].str.split(':', expand=True)[1]\n    dataframe['id23'] = dataframe['id23'].str.split(':', expand=True)[1]\n    \n    \n    dataframe.loc[dataframe['device_name'].str.contains('SM', na=False), 'device_name'] = 'Samsung'\n    dataframe.loc[dataframe['device_name'].str.contains('SAMSUNG', na=False), 'device_name'] = 'Samsung'\n    dataframe.loc[dataframe['device_name'].str.contains('GT-', na=False), 'device_name'] = 'Samsung'\n    dataframe.loc[dataframe['device_name'].str.contains('Moto G', na=False), 'device_name'] = 'Motorola'\n    dataframe.loc[dataframe['device_name'].str.contains('Moto', na=False), 'device_name'] = 'Motorola'\n    dataframe.loc[dataframe['device_name'].str.contains('moto', na=False), 'device_name'] = 'Motorola'\n    dataframe.loc[dataframe['device_name'].str.contains('LG-', na=False), 'device_name'] = 'LG'\n    dataframe.loc[dataframe['device_name'].str.contains('rv:', na=False), 'device_name'] = 'RV'\n    dataframe.loc[dataframe['device_name'].str.contains('HUAWEI', na=False), 'device_name'] = 'Huawei'\n    dataframe.loc[dataframe['device_name'].str.contains('ALE-', na=False), 'device_name'] = 'Huawei'\n    dataframe.loc[dataframe['device_name'].str.contains('-L', na=False), 'device_name'] = 'Huawei'\n    dataframe.loc[dataframe['device_name'].str.contains('Blade', na=False), 'device_name'] = 'ZTE'\n    dataframe.loc[dataframe['device_name'].str.contains('BLADE', na=False), 'device_name'] = 'ZTE'\n    dataframe.loc[dataframe['device_name'].str.contains('Linux', na=False), 'device_name'] = 'Linux'\n    dataframe.loc[dataframe['device_name'].str.contains('XT', na=False), 'device_name'] = 'Sony'\n    dataframe.loc[dataframe['device_name'].str.contains('HTC', na=False), 'device_name'] = 'HTC'\n    dataframe.loc[dataframe['device_name'].str.contains('ASUS', na=False), 'device_name'] = 'Asus'\n    dataframe.loc[dataframe.device_name.isin(dataframe.device_name.value_counts()[dataframe.device_name.value_counts() < 200].index), 'device_name'] = \"Others\"\n    dataframe['had_id'] = 1\n    \n    gc.collect()#run a Garbage Collector to release memory\n    \n    return dataframe\n\n#apply the function on the datasets\nx_train = id_split(x_train)\nx_test = id_split(x_test)\n\n#show data size\nprint(\"\\nThe x_train data size  is : {} \".format(x_train.shape)) \nprint(\"The test data size  is : {} \".format(x_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Missing Values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#show missing values\nmiss_number=x_train.isnull().sum()\nmiss_ratio=x_train.isnull().sum()/len(x_train)\nmiss_info=pd.DataFrame({'Number of miss':miss_number,'Proportion of miss':miss_ratio},)\nmiss_info=miss_info.loc[miss_info['Number of miss']>0]\nmiss_info=miss_info.sort_values(by='Number of miss',ascending=0)\nprint(miss_info[miss_info['Proportion of miss']>0.99])\n\n#drop the feature with overwhelm missing values\nx_train.drop(list(miss_info[miss_info['Proportion of miss']>0.99].index), axis=1, inplace=True)\nx_test.drop(list(miss_info[miss_info['Proportion of miss']>0.99].index), axis=1, inplace=True)\n\n#fill the rest missing value with mode\nfor col in x_train.columns:\n    x_train[col] = x_train[col].fillna(x_train[col].mode()[0])\n    x_test[col] = x_test[col].fillna(x_train[col].mode()[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Label Encoding the Categorical Variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Label Encoding the Categorical Variables\nfrom sklearn.preprocessing import LabelEncoder\nfor col in x_train.columns:\n    if x_train[col].dtype=='object' or x_test[col].dtype=='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(x_train[col].values.astype('str')) + list(x_test[col].values.astype('str')))\n        x_train[col] = lbl.transform(list(x_train[col].values.astype('str')))\n        x_test[col] = lbl.transform(list(x_test[col].values.astype('str')))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"models\"></a>\n# 3. Models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load packages\nimport pandas as pd\nfrom pandas.plotting import scatter_matrix\nimport matplotlib.pyplot as plt #using for plot\nfrom sklearn import model_selection\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV#GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"logit\"></a>\n## 3.1 Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#logistic regression\nfrom sklearn.linear_model import LogisticRegression\nmodel_lr = LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"LightGBM\"></a>\n## 3.2 LightGBM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#lightGBM\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nimport datetime\n\nprint('####################################################\\n{}\\start_time'.format(datetime.datetime.now().strftime('%H:%M')))\n\nparams = {'num_leaves': [491],\n          'min_child_weight': [0.03454472573214212],\n          'feature_fraction': [0.3797454081646243],\n          'bagging_fraction': [0.4181193142567742],\n          'min_data_in_leaf': [106],\n          'objective': ['binary'],\n          'max_depth': [-1],\n          'learning_rate': [0.006883242363721497],\n          \"boosting_type\": [\"gbdt\"],\n          \"bagging_seed\": [11],\n          \"metric\": ['auc'],\n          \"verbosity\": [-1],\n          'reg_alpha': [0.3899927210061127],\n          'reg_lambda': [0.6485237330340494],\n          'random_state': [47],\n          'device_type':['gpu']\n         }\n\nlgb_temp = lgb.LGBMClassifier()\nmodel_lgb_tuned = GridSearchCV(lgb_temp, params,scoring='roc_auc')\nmodel_lgb_tuned.fit(x_train,y_train)\nmodel_lgb = lgb.LGBMClassifier(**model_lgb_tuned.best_params_)\n\nprint(model_lgb)\nprint('{}\\tEnd_time\\n####################################################'.format(datetime.datetime.now().strftime('%H:%M')))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"xgboost\"></a>\n## 3.3 Xgboost","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#xgboost\nimport xgboost as xgb\n\nprint('####################################################\\n{}\\start_time'.format(datetime.datetime.now().strftime('%H:%M')))\n\nparams={\n    'n_estimators':[500],\n    'max_depth':[3],\n    'learning_rate':[0.05],\n    'subsample':[0.5],\n    'tree_method':['gpu_hist']  # THE MAGICAL PARAMETER, use gpu\n        }\n\n\nxgb_temp = xgb.XGBClassifier()\nmodel_xgb_tuned = GridSearchCV(xgb_temp, params,scoring='roc_auc')\nmodel_xgb_tuned.fit(x_train,y_train)\nmodel_xgb = xgb.XGBClassifier(**model_xgb_tuned.best_params_)\n\nprint(model_xgb)\nprint('{}\\tEnd_time\\n####################################################'.format(datetime.datetime.now().strftime('%H:%M')))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"rf\"></a>\n## 3.4 Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\n#random forest (random forest is time consuming and space consuming. Can not run it with other models in one Kaggle notebook)\n'''\nfrom sklearn.ensemble import RandomForestClassifier\nprint('####################################################\\n{}\\start_time'.format(datetime.datetime.now().strftime('%H:%M')))\n\nparams={\n    'n_estimators':[200],\n    'max_features':[0.3],\n    'min_samples_leaf':[20],\n    #'verbose':[1],#show steps\n    'n_jobs':[-1]\n}\n\nrf_temp = RandomForestClassifier();\nmodel_rf_tuned = GridSearchCV(rf_temp, params,scoring='roc_auc');\nmodel_rf_tuned.fit(x_train,y_train);\nmodel_rf = RandomForestClassifier(**model_rf_tuned.best_params_);\n\nmodel_rf.fit(x_train, y_train)\nsub_rf = pd.DataFrame()\nsub_rf['TransactionID'] = x_test.index.tolist()\nsub_rf['isFraud'] = model_rf.predict_proba(x_test)[:,1]\nsub_rf.to_csv('submission_rf.csv',index=False)\n\nprint(model_rf)\nprint('{}\\tEnd_time\\n####################################################'.format(datetime.datetime.now().strftime('%H:%M')))\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"stacking\"></a>\n## 3.5 Use Cross Validation to Compare the Performance and Stacking the Models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#cross validation to Compare the Performance and Stacking the Models\nfrom sklearn.model_selection import KFold,cross_val_score\nfrom mlxtend.classifier import StackingClassifier\n\n#Validation function\nn_folds = 3\n\nmodels = {\n    'Logistic':model_lr,\n    'Lightgbm':model_lgb,\n    'XGBoost':model_xgb,\n    #'Random forest':model_rf  #random forest is time consuming\n    }\n\n\n# kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(x_train)\nkf=n_folds\n\nfor model_ind, model_fn in models.items():\n    print('Fitting:\\t{}'.format(model_ind))\n    #model_fn.fit(x_train, y_train)\n    \n    #cross validation\n    auc= cross_val_score(model_fn, x_train, y_train, scoring='roc_auc', cv = kf)    \n    \n    print('Done! Error:\\t{}\\n'.format(auc.mean()))\n\n\n#combine the model together(stacking)\nlr = LogisticRegression()\naveraged_models=StackingClassifier(classifiers=[model_lgb, model_xgb], \n                                   use_probas=True,average_probas=True,meta_classifier=lr)\n\n# kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(x_train)\nauc= cross_val_score(averaged_models, x_train, y_train, scoring='roc_auc', cv = kf)\nscore =auc.mean()\nprint(\" Averaged base models score: \\t{}\\n\".format(score))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"predictions\"></a>\n## 4. Predictions and Submit the Results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#We use the stacked model for our final predictions.\naveraged_models.fit(x_train, y_train)\nsub = pd.DataFrame()\nsub['TransactionID'] = x_test.index.tolist()\nsub['isFraud'] = averaged_models.predict_proba(x_test)[:,1]\nsub.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}