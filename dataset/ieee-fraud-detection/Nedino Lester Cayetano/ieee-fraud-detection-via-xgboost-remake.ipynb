{"cells":[{"metadata":{},"cell_type":"markdown","source":"# About this kernel\n\nThis is most common (to me) use case of machine learning, I just want to make a simple version for myself \nto server as a reference. Everything on here is basic, nothing special, really."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Import needed libraries\n%time\nimport os\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nprint(\"XGBoost version:\", xgb.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing\n\nWe will use label encoding (instead of dropping the column or using one-hote encode) for balanced\nefficient cleansing and data size management"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check process time\n# We only have limited memory (13GB)\n# Need to properly manage or the kernel will die\n%time\n\n# Read the data\ntrain_transaction = pd.read_csv('../input/train_transaction.csv', index_col='TransactionID')\ntest_transaction  = pd.read_csv('../input/test_transaction.csv', index_col='TransactionID')\ntrain_identity    = pd.read_csv('../input/train_identity.csv', index_col='TransactionID')\ntest_identity     = pd.read_csv('../input/test_identity.csv', index_col='TransactionID')\nsample_submission = pd.read_csv('../input/sample_submission.csv', index_col='TransactionID')\n\n# Merge\ntrain = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest  = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n\n# Manage memory\ndel train_transaction, train_identity, test_transaction, test_identity\nprint(train.shape)\nprint(test.shape)\n\n# Identify targets and features, also fill the NaNs\nX_test  = test.copy()\nX_test  = X_test.fillna(-999)\ny = train['isFraud'].copy()\nX = train.drop('isFraud', axis=1)\nX = X.fillna(-999)\n\n# Manage memory\ndel train, test\n\n# Preprocess categorical features using label encoding for both test and train features\nfor f in X.columns:\n    if X[f].dtype=='object' or X_test[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(X[f].values) + list(X_test[f].values))\n        X[f] = lbl.transform(list(X[f].values))\n        X_test[f] = lbl.transform(list(X_test[f].values))\n        \n# Do the legendary split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.20, random_state = 0)\n\n# Manage Memory\ndel X, y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training and Scoring\n\nFor GPU usage, simply use `tree_method='gpu_hist'` (took me an hour to figure out, I wish XGBoost documentation was clearer about that)."},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = xgb.XGBClassifier(\n    n_estimators=500,\n    max_depth=9,\n    learning_rate=0.1,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    missing=-999,\n    random_state=2019,\n    tree_method='gpu_hist'\n)\n%time clf.fit(X_train, y_train)\npreds = clf.predict(X_val)\nscore = round(accuracy_score(preds, y_val) * 100, 2)\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some of you must be wondering how we were able to decrease the fitting time by that much. The reason for that is not only we are running on gpu, but we are also computing an approximation of the real underlying algorithm (which is a greedy algorithm). This hurts your score slightly, but as a result is much faster.\n\nSo why am I not using CPU with `tree_method='hist'`? If you try it out yourself, you'll realize it'll take ~ 7 min, which is still far from the GPU fitting time. Similarly, `tree_method='gpu_exact'` will take ~ 4 min, but likely yields better accuracy than `gpu_hist` or `hist`.\n\nThe [docs on parameters](https://xgboost.readthedocs.io/en/latest/parameter.html) has a section on `tree_method`, and it goes over the details of each option."},{"metadata":{},"cell_type":"markdown","source":"# Predicting\n\nTime to make some predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['isFraud'] = clf.predict_proba(X_test)[:,1]\nsample_submission.to_csv('simple_xgboost.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}