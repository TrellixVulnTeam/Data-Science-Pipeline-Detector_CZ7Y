{"cells":[{"metadata":{},"cell_type":"markdown","source":"# About this Kernel\nThis is my first attempt to publish a kernel in Kaggle. I will try TPOT AutoML  on this IEEE Fraud transaction dataset. Primary goal for me is to work on a dataset that has combination of features (numbers, categorical, datetime) and the features are not explanatory, so i could experiment with pre-processing & Feature engineering concepts\n\n\n## Credits\nThanks to kernels submitted by xhulu & makalesta2. I have used some of their concepts in this Kernel\nhttps://www.kaggle.com/xhlulu/ieee-fraud-efficient-grid-search-with-xgboost\nhttps://www.kaggle.com/makalesta2/fraud-detection-randomforest"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import os\nimport gc\nimport itertools\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nfrom sklearn.metrics import roc_auc_score\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Basic Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_transaction = pd.read_csv('../input/train_transaction.csv', index_col='TransactionID')\ntest_transaction = pd.read_csv('../input/test_transaction.csv', index_col='TransactionID')\n\ntrain_identity = pd.read_csv('../input/train_identity.csv', index_col='TransactionID')\ntest_identity = pd.read_csv('../input/test_identity.csv', index_col='TransactionID')\n\nsample_submission = pd.read_csv('../input/sample_submission.csv', index_col='TransactionID')\n\ntrain = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n\nprint(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's treat the TransactionDT feature, per the dataset description given\nReferences: https://www.kaggle.com/wajihullahbaig/up-sampling-on-every-hour"},{"metadata":{"trusted":true},"cell_type":"code","source":"def derive_hour_feature(df,tname):\n    \"\"\"\n    Creates an hour of the day feature, encoded as 0-23. \n    Parameters: \n        df : pd.DataFrame\n            df to manipulate.\n        tname : str\n            Name of the time column in df.\n    \"\"\"\n    hours = df[tname] / (3600)        \n    encoded_hours = np.floor(hours) % 24\n    return encoded_hours\n\ntrain['hours'] = derive_hour_feature(train,'TransactionDT')\ntest['hours'] = derive_hour_feature(test,'TransactionDT')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_transaction, train_identity, test_transaction, test_identity","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Drop TransactionDT"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train.drop(['TransactionDT'], axis=1)\nX_test = test.drop(['TransactionDT'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train, test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lets do Data exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"total = X_train.isnull().sum().sort_values(ascending=False)\npercent = (X_train.isnull().sum()/X_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# notuseful_features = missing_data[missing_data['Percent']>0.80]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_fs = X_train.dtypes[X_train.dtypes != \"object\"].index\nprint(\"Number of Numerical features: \", len(num_fs))\n\ncat_fs = X_train.dtypes[X_train.dtypes == \"object\"].index\nprint(\"Number of Categorical features: \", len(cat_fs))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# n = X_train.select_dtypes(include=object)\n# for col in n.columns:\n#     print(col, ':  ', X_train[col].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Let's see the distribuition of the categories: \n# for cat in list(cat_fs):\n#     print('Distribuition of feature:', cat)\n#     print(X_train[cat].value_counts(normalize=True))\n#     print('#'*50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will still explore EDA in the next version of this notebook. For now, proceeding with other steps."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Seaborn visualization library\n# import seaborn as sns\n# Create the default pairplot\n# sns.pairplot(X_train, hue = 'isFraud')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lets separate the target variable from the training dataset "},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = X_train['isFraud']\nX_train.drop(['isFraud'], axis=1, inplace = True)\ny_pred = sample_submission","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Treat NaNs\nThere are better ways to do. but for now going with filling as below; intend to revisit this later"},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train = X_train.fillna(-999)\n# X_test = X_test.fillna(-999)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.columns, X_test.columns, y_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Explore Feature selection"},{"metadata":{},"cell_type":"markdown","source":"1. Check the absolute value of the Pearsonâ€™s correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_feats=len(cat_fs)+len(num_fs)-1\nprint(num_feats)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cor_selector(X, y,num_feats):\n    cor_list = []\n    feature_name = X.columns.tolist()\n    # calculate the correlation with y for each feature\n    for i in X.columns.tolist():\n        cor = np.corrcoef(X[i], y)[0, 1]\n        cor_list.append(cor)\n    # replace NaN with 0\n    cor_list = [0 if np.isnan(i) else i for i in cor_list]\n    # feature name\n    cor_feature = X.iloc[:,np.argsort(np.abs(cor_list))[-num_feats:]].columns.tolist()\n    # feature selection? 0 for not select, 1 for select\n    cor_support = [True if i in cor_feature else False for i in feature_name]\n    return cor_support, cor_feature\n#cor_support, cor_feature = cor_selector(X_train, y_train,num_feats)\n#print(str(len(cor_feature)), 'selected features')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. Chi-squared"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.preprocessing import MinMaxScaler\nX_norm = MinMaxScaler().fit_transform(X_train)\nchi_selector = SelectKBest(chi2, k=num_feats)\nchi_selector.fit(X_norm, y_train)\nchi_support = chi_selector.get_support()\nchi_feature = X_train.loc[:,chi_support].columns.tolist()\nprint(str(len(chi_feature)), 'selected features')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. Recursive Feature elimination"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nrfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=num_feats, step=10, verbose=5)\nrfe_selector.fit(X_norm, y_train)\nrfe_support = rfe_selector.get_support()\nrfe_feature = X.loc[:,rfe_support].columns.tolist()\nprint(str(len(rfe_feature)), 'selected features')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4. Lasso: SelectFromModel"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LogisticRegression\n\nembeded_lr_selector = SelectFromModel(LogisticRegression(penalty=\"l1\"), max_features=num_feats)\nembeded_lr_selector.fit(X_norm, y_train)\n\nembeded_lr_support = embeded_lr_selector.get_support()\nembeded_lr_feature = X.loc[:,embeded_lr_support].columns.tolist()\nprint(str(len(embeded_lr_feature)), 'selected features')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"5. Tree based model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier\n\nembeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=100), max_features=num_feats)\nembeded_rf_selector.fit(X_train, y_train)\n\nembeded_rf_support = embeded_rf_selector.get_support()\nembeded_rf_feature = X.loc[:,embeded_rf_support].columns.tolist()\nprint(str(len(embeded_rf_feature)), 'selected features')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comparison view of feature selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"# put all selection together\nfeature_selection_df = pd.DataFrame({'Feature':feature_name, 'Chi-2':chi_support, 'RFE':rfe_support, 'Logistics':embeded_lr_support,\n                                    'Random Forest':embeded_rf_support})\n# count the selected times for each feature\nfeature_selection_df['Total'] = np.sum(feature_selection_df, axis=1)\n# display the top 100\nfeature_selection_df = feature_selection_df.sort_values(['Total','Feature'] , ascending=False)\nfeature_selection_df.index = range(1, len(feature_selection_df)+1)\nfeature_selection_df.head(num_feats)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Convert categorical variables \nIdeally, we should apply label encoding & then one hot encoding OR use getdummies from pandas. My worry is, there are already too many unexplainable features in this dataset. Applying one hot encoding will add more features by cardinality and not sure if that is a good sign. Also, I have read in the discussion forum of this competition that one hot encoding was not that useful. So going with the flow of just applying label encoding. Remember i told you, this is my first Kaggle Kernel submission and so excited to have couple of good commits:P"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Label Encoding\nfor f in X_train.columns:\n    if X_train[f].dtype=='object' or X_test[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(X_train[f].values) + list(X_test[f].values))\n        X_train[f] = lbl.transform(list(X_train[f].values))\n        X_test[f] = lbl.transform(list(X_test[f].values))   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RAM Optimization\nThis is adapted from the kaggle kernel https://www.kaggle.com/xhlulu/ieee-fraud-efficient-grid-search-with-xgboost.\nI just use a normal PC that has 8GB RAM. so any memory saving step is a boost for people like me. Also believe me, i truly understand the below code snippet :P"},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%%time\n#X_train = reduce_mem_usage(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_test = reduce_mem_usage(X_test)#","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's try the AutoML frameworks"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, X_test.shape, y_train.shape, y_pred.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.model_selection import train_test_split\nX_tr, X_te, y_tr, y_te = train_test_split(X_train, y_train,train_size=0.90, test_size=0.10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom tpot import TPOTClassifier\ntpot = TPOTClassifier(generations=5, population_size=5, verbosity=2,cv=5, scoring='roc_auc', warm_start=True, early_stop=5 )\ntpot.fit(X_tr, y_tr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nprint(\"ROC_AUC is {}%\".format(tpot.score(X_te, y_te)*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now lets do prediction on the sample_submission dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\npreds = tpot.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\npreds_probab = tpot.predict_proba(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['isFraud'] = '0'\nsample_submission['isFraud'] = preds\nsample_submission.to_csv('TPOT_automl_submission_pred_3.csv', index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using 1 - probab, since the output class predictor is predicting non-Fraud %"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['isFraud'] = '0'\nsample_submission['isFraud'] = 1.000000 - preds_probab\nsample_submission.to_csv('TPOT_automl_submission_probab_4.csv', index=True)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}