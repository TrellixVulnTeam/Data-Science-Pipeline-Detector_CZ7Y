{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-11T12:08:26.980053Z","iopub.execute_input":"2021-11-11T12:08:26.980925Z","iopub.status.idle":"2021-11-11T12:08:27.011065Z","shell.execute_reply.started":"2021-11-11T12:08:26.980793Z","shell.execute_reply":"2021-11-11T12:08:27.010376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"# Plotting libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport plotly.express as px\n\n# scikit-learn is a rich ML library\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# library of XGBoost algorithm. You can find one in scikit-learn, too.\nimport xgboost as xgb\n\n# library for regex\nimport re\n\n# library for garbage collection\nimport gc\ngc.enable()\n\n# Let's ignore the warnings\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-11-11T12:08:27.012803Z","iopub.execute_input":"2021-11-11T12:08:27.013197Z","iopub.status.idle":"2021-11-11T12:08:30.163651Z","shell.execute_reply.started":"2021-11-11T12:08:27.01316Z","shell.execute_reply":"2021-11-11T12:08:30.162721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"%%time\n# TransactionID is the key column. We define it as the index column for ease of use.\ntr_identity = pd.read_csv('/kaggle/input/ieee-fraud-detection/train_identity.csv', index_col='TransactionID')\nts_identity = pd.read_csv('/kaggle/input/ieee-fraud-detection/test_identity.csv', index_col='TransactionID')\ntr_transaction = pd.read_csv('/kaggle/input/ieee-fraud-detection/train_transaction.csv', index_col='TransactionID')\nts_transaction = pd.read_csv('/kaggle/input/ieee-fraud-detection/test_transaction.csv', index_col='TransactionID')\nprint(\"Data Loaded!\")","metadata":{"execution":{"iopub.status.busy":"2021-11-11T12:08:30.167327Z","iopub.execute_input":"2021-11-11T12:08:30.167644Z","iopub.status.idle":"2021-11-11T12:09:21.633531Z","shell.execute_reply.started":"2021-11-11T12:08:30.167613Z","shell.execute_reply":"2021-11-11T12:09:21.632763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's see the size of the tables\nprint(\"train transaction shape:\", tr_transaction.shape)\nprint(\"test transaction shape:\", ts_transaction.shape)\nprint(\"train identity shape:\", tr_identity.shape)\nprint(\"test identity shape:\", ts_identity.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T12:09:21.63482Z","iopub.execute_input":"2021-11-11T12:09:21.635072Z","iopub.status.idle":"2021-11-11T12:09:21.642102Z","shell.execute_reply.started":"2021-11-11T12:09:21.635037Z","shell.execute_reply":"2021-11-11T12:09:21.641298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Explore Data","metadata":{}},{"cell_type":"code","source":"# How is the data distributed among the classes?\nsns.countplot(tr_transaction['isFraud'], palette='Pastel1')","metadata":{"execution":{"iopub.status.busy":"2021-11-11T12:09:21.644456Z","iopub.execute_input":"2021-11-11T12:09:21.6449Z","iopub.status.idle":"2021-11-11T12:09:21.951992Z","shell.execute_reply.started":"2021-11-11T12:09:21.644864Z","shell.execute_reply":"2021-11-11T12:09:21.951036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# What portion of data in transaction table is missing?\nmissing_values_count = tr_transaction.isnull().sum()\nprint (missing_values_count)\ntotal_cells = np.product(tr_transaction.shape)\ntotal_missing = missing_values_count.sum()\nprint (f'{round((total_missing/total_cells) * 100, 2)}% of transaction data is missing!')","metadata":{"execution":{"iopub.status.busy":"2021-11-11T12:09:21.95707Z","iopub.execute_input":"2021-11-11T12:09:21.959353Z","iopub.status.idle":"2021-11-11T12:09:23.156565Z","shell.execute_reply.started":"2021-11-11T12:09:21.959312Z","shell.execute_reply":"2021-11-11T12:09:23.155795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# What portion of data in identity table is missing?\nmissing_values_count = tr_identity.isnull().sum()\nprint (missing_values_count)\ntotal_cells = np.product(tr_identity.shape)\ntotal_missing = missing_values_count.sum()\nprint (f'{round((total_missing/total_cells) * 100, 2)}% of identity data is missing!')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-11-11T12:09:23.157638Z","iopub.execute_input":"2021-11-11T12:09:23.158074Z","iopub.status.idle":"2021-11-11T12:09:23.382779Z","shell.execute_reply.started":"2021-11-11T12:09:23.158036Z","shell.execute_reply":"2021-11-11T12:09:23.381968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# What portion of transactions have an identity record?\nprint(f'{round(np.sum(tr_transaction.index.isin(tr_identity.index.unique())) / len(tr_transaction) *100, 2)}% of transactions have identity.')","metadata":{"execution":{"iopub.status.busy":"2021-11-11T12:09:23.384062Z","iopub.execute_input":"2021-11-11T12:09:23.384454Z","iopub.status.idle":"2021-11-11T12:09:23.409707Z","shell.execute_reply.started":"2021-11-11T12:09:23.384419Z","shell.execute_reply":"2021-11-11T12:09:23.409027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# What is the distribution of transaction's date-time?\nfig = px.histogram(tr_transaction, x='TransactionDT', color='isFraud', marginal='box')\n# Overlay both histograms\nfig.update_layout(barmode='overlay')\n# Reduce opacity to see both histograms\nfig.update_traces(opacity=0.6)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-11T12:09:23.410778Z","iopub.execute_input":"2021-11-11T12:09:23.41147Z","iopub.status.idle":"2021-11-11T12:09:26.796933Z","shell.execute_reply.started":"2021-11-11T12:09:23.411415Z","shell.execute_reply":"2021-11-11T12:09:26.796064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's plot it in logarithm scale for a better insight\nfig, ax = plt.subplots(1, 2, figsize=(18,4))\n\ntime_val = tr_transaction.loc[tr_transaction['isFraud'] == 1]['TransactionDT'].values\n\nsns.distplot(np.log(time_val), ax=ax[0], color='r')\nax[0].set_title('Distribution of LOG TransactionDT, isFraud=1', fontsize=14)\nax[1].set_xlim([min(np.log(time_val)), max(np.log(time_val))])\n\ntime_val = tr_transaction.loc[tr_transaction['isFraud'] == 0]['TransactionDT'].values\n\nsns.distplot(np.log(time_val), ax=ax[1], color='b')\nax[1].set_title('Distribution of LOG TransactionDT, isFraud=0', fontsize=14)\nax[1].set_xlim([min(np.log(time_val)), max(np.log(time_val))])\n\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-11T12:09:26.798216Z","iopub.execute_input":"2021-11-11T12:09:26.798545Z","iopub.status.idle":"2021-11-11T12:09:31.169974Z","shell.execute_reply.started":"2021-11-11T12:09:26.7985Z","shell.execute_reply":"2021-11-11T12:09:31.169327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Do the date-times of transactions in train and test overlap?\nplt.hist(tr_transaction['TransactionDT'], label='train')\nplt.hist(ts_transaction['TransactionDT'], label='test')\nplt.legend()\nplt.title(\"Histogram of transaction datetime\")","metadata":{"execution":{"iopub.status.busy":"2021-11-11T12:09:31.171307Z","iopub.execute_input":"2021-11-11T12:09:31.171724Z","iopub.status.idle":"2021-11-11T12:09:31.500095Z","shell.execute_reply.started":"2021-11-11T12:09:31.171686Z","shell.execute_reply":"2021-11-11T12:09:31.499314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# What is the distribution of transactions' amount?\nfig = px.histogram(tr_transaction, x='TransactionAmt', color='isFraud', marginal='box')\nfig.update_layout(barmode='overlay')\nfig.update_traces(opacity=0.6)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-11T12:09:31.502639Z","iopub.execute_input":"2021-11-11T12:09:31.502872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's plot in logarithm scale for a better insight\nfig, ax = plt.subplots(1, 2, figsize=(18,4))\n\ntime_val = tr_transaction.loc[tr_transaction['isFraud'] == 1]['TransactionAmt'].values\n\nsns.distplot(np.log(time_val), ax=ax[0], color='r')\nax[0].set_title('Distribution of LOG TransactionAmt, isFraud=1', fontsize=14)\nax[1].set_xlim([min(np.log(time_val)), max(np.log(time_val))])\n\ntime_val = tr_transaction.loc[tr_transaction['isFraud'] == 0]['TransactionAmt'].values\n\nsns.distplot(np.log(time_val), ax=ax[1], color='b')\nax[1].set_title('Distribution of LOG TransactionAmt, isFraud=0', fontsize=14)\nax[1].set_xlim([min(np.log(time_val)), max(np.log(time_val))])\n\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# M is a categorical feature. Let's explore it.\nfig, axes = plt.subplots(3, 3, figsize=(20, 20))\nfig.suptitle(\"Value Counts in M features\")\nfor i in range(3):\n    for j in range(3):\n        sns.countplot(data=tr_transaction, x=f'M{3*i+j+1}', hue='isFraud', ax=axes[i,j])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ProductCD is a categorical feature\nsns.countplot(data=tr_transaction, x=\"ProductCD\", hue='isFraud')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# How many unique values are there in each card feature in the train set?\nplt.figure(figsize=(35, 8))\nfeatures = [f'card{i}' for i in range(1, 7)]\nuniques = [len(tr_transaction[col].unique()) for col in features]\nsns.set(font_scale=1.2)\nax = sns.barplot(features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='unique count', title='Number of unique values per feature TRAIN')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height + 10,\n            uniq,\n            ha=\"center\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# How many unique values are there in each card feature in the test set?\nplt.figure(figsize=(35, 8))\nfeatures = [f'card{i}' for i in range(1, 7)]\nuniques = [len(ts_transaction[col].unique()) for col in features]\nsns.set(font_scale=1.2)\nax = sns.barplot(features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='unique count', title='Number of unique values per feature TEST')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height + 10,\n            uniq,\n            ha=\"center\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizing card4 categories\nsns.countplot(data=tr_transaction, x='card4', hue='isFraud')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizing card6 categories\nsns.countplot(data=tr_transaction, x='card6', hue='isFraud')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# How many unique values are there in id features in the train set?\nplt.figure(figsize=(35, 8))\nfeatures = list(tr_identity.columns[0:38])\nuniques = [len(tr_identity[col].unique()) for col in features]\nsns.set(font_scale=1.2)\nax = sns.barplot(features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='unique count', title='Number of unique values per feature TRAIN')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height + 10,\n            uniq,\n            ha=\"center\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# How many unique values are there in id features in the test set?\nplt.figure(figsize=(35, 8))\nfeatures = list(ts_identity.columns[0:38])\nuniques = [len(ts_identity[col].unique()) for col in features]\nsns.set(font_scale=1.2)\nax = sns.barplot(features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='unique count', title='Number of unique values per feature TEST')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height + 10,\n            uniq,\n            ha=\"center\") ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr_identity.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Are there categories that exist in the test set but not in the train?\nfor ft in features[11:]:\n    print(\"Feature:\", ft)\n    print(set(ts_identity[ft].unique()).difference(set(tr_identity[ft.replace('-', '_')].unique())))\n    print(\"*\"*40)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at unique values in some other categorical features","metadata":{}},{"cell_type":"code","source":"tr_transaction['P_emaildomain'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr_transaction['R_emaildomain'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr_transaction['addr1'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr_transaction['addr2'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizing DeviceType feature\nsns.countplot(data=tr_identity, x='DeviceType')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr_identity['DeviceInfo'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"code","source":"# id features are not named unifyingly in the train and test and should be fixed.\nts_identity.rename(columns={x: x.replace('-', '_') for x in ts_identity.columns[:38]}, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# From kernel https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n# WARNING! THIS CAN DAMAGE THE DATA \ndef reduce_mem_usage_numeric(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\n\n\ndef reduce_mem_usage_cat(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type == object and col not in ['DeviceInfo', 'id_30', 'id_31']:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to drop some columns\ndef columns2drop(df):\n    drop_list = ['id_33', 'P_emaildomain', 'R_emaildomain', 'TransactionDT']\n    for col in df.columns:\n        if df[col].isnull().sum() / df.shape[0] > 0.9:\n            drop_list.append(col)\n    return drop_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to split the email domains into separate columns\ndef split_email_domains(df):\n    df[['P_emaildomain1', 'P_emaildomain2', 'P_emaildomain3']] = df['P_emaildomain'].str.split('.', expand=True)\n    df[['R_emaildomain1', 'R_emaildomain2', 'R_emaildomain3']] = df['R_emaildomain'].str.split('.', expand=True)\n    for x in ['R', 'P']:\n        for i in range(1, 4):\n            df[f'{x}_emaildomain{i}'].fillna('', inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to generate a numerical column from id_33 which apparently contains string of dimensions\ndef split_id33(df):\n    name = \"id-33\"\n    if not 'id-33' in df.columns:\n        name = \"id_33\"\n    df[[\"height\", \"width\"]] = df[name].str.split('x', expand=True)\n    df['height'].fillna(0, inplace=True)\n    df['width'].fillna(-1, inplace=True)\n    df['aspect_ratio'] = df['height'].astype('uint16') / df['width'].astype('uint16')\n    df.drop(['height', 'width'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to apply the preprocessing\ndef preprocessing(df, drop_list):\n    split_id33(df)\n    split_email_domains(df)\n    \n    df['Transaction_day_of_week'] = np.floor((df['TransactionDT'] / (3600 * 24) - 1) % 7)\n    df['Transaction_hour'] = np.floor(df['TransactionDT'] / 3600) % 24\n    \n    df.drop(drop_list, axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Functions to handle tfidf vectorization\ndef tokenizer(x):\n    return re.split(' ._-/', x)\n\n\ndef tfidf_vectorizer(train_df, test_df, col):\n    train_df[col].fillna('Unknown', inplace=True)\n    test_df[col].fillna('Unknown', inplace=True)\n\n    tfidf = TfidfVectorizer(decode_error='replace', lowercase=True, strip_accents='ascii', analyzer='char_wb', tokenizer=tokenizer)\n    v = tfidf.fit_transform(train_df[col])\n    w = tfidf.transform(test_df[col])\n    \n    tr_tfidf = pd.DataFrame.sparse.from_spmatrix(v, index=train_df.index, columns=[f'{col}_{i}' for i in tfidf.vocabulary_])\n    ts_tfidf = pd.DataFrame.sparse.from_spmatrix(w, index=test_df.index, columns=[f'{col}_{i}' for i in tfidf.vocabulary_])\n    \n    for col in tr_tfidf.columns:\n        tr_tfidf[col] = tr_tfidf[col].values.to_dense().astype(np.float16)\n        ts_tfidf[col] = ts_tfidf[col].values.to_dense().astype(np.float16)\n\n    tr = pd.concat([train_df, tr_tfidf], axis=1)\n    del tr_tfidf, train_df\n    ts = pd.concat([test_df, ts_tfidf], axis=1)\n    del ts_tfidf, test_df\n    \n    gc.collect()\n    \n    return tr, ts","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Join the two tables\ntrain = tr_transaction.merge(tr_identity, how='left', left_index=True, right_index=True)\ny_train = train['isFraud'].astype('uint8').copy()\ndel tr_transaction, tr_identity\n\ntest = ts_transaction.merge(ts_identity, how='left', left_index=True, right_index=True)\ndel ts_transaction, ts_identity\n\nprint(f\"Shape of train data: {train.shape}, Shape of test data: {test.shape}\")\n\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = train.drop('isFraud', axis=1)\ndel train\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"drop_list = columns2drop(X_train)\npreprocessing(X_train, drop_list)","metadata":{"execution":{"iopub.status.idle":"2021-11-11T12:10:16.711571Z","shell.execute_reply.started":"2021-11-11T12:10:01.324045Z","shell.execute_reply":"2021-11-11T12:10:16.710523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocessing(test, drop_list)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T12:10:16.712803Z","iopub.execute_input":"2021-11-11T12:10:16.713074Z","iopub.status.idle":"2021-11-11T12:10:26.355129Z","shell.execute_reply.started":"2021-11-11T12:10:16.713037Z","shell.execute_reply":"2021-11-11T12:10:26.354352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = reduce_mem_usage_numeric(X_train)\ntest = reduce_mem_usage_numeric(test)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T12:10:26.356512Z","iopub.execute_input":"2021-11-11T12:10:26.356854Z","iopub.status.idle":"2021-11-11T12:13:50.303984Z","shell.execute_reply.started":"2021-11-11T12:10:26.356816Z","shell.execute_reply":"2021-11-11T12:13:50.303238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encoding labels in categorical features\nfor f in X_train.columns:\n    if f not in ['DeviceInfo', 'id_30', 'id_31'] and (X_train[f].dtype=='object' or test[f].dtype=='object'):\n        lbl = LabelEncoder()\n        lbl.fit(list(X_train[f].values) + list(test[f].values))\n        X_train[f] = lbl.transform(list(X_train[f].values))\n        test[f] = lbl.transform(list(test[f].values))","metadata":{"execution":{"iopub.status.busy":"2021-11-11T12:13:50.305141Z","iopub.execute_input":"2021-11-11T12:13:50.305818Z","iopub.status.idle":"2021-11-11T12:14:43.358082Z","shell.execute_reply.started":"2021-11-11T12:13:50.305778Z","shell.execute_reply":"2021-11-11T12:14:43.35726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, test = tfidf_vectorizer(X_train, test, 'DeviceInfo')\nX_train, test = tfidf_vectorizer(X_train, test, 'id_30')\nX_train, test = tfidf_vectorizer(X_train, test, 'id_31')","metadata":{"execution":{"iopub.status.busy":"2021-11-11T12:14:43.359676Z","iopub.execute_input":"2021-11-11T12:14:43.359939Z","iopub.status.idle":"2021-11-11T12:15:30.701873Z","shell.execute_reply.started":"2021-11-11T12:14:43.359903Z","shell.execute_reply":"2021-11-11T12:15:30.701128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.drop(['DeviceInfo', 'id_30', 'id_31'], axis=1, inplace=True)\ntest.drop(['DeviceInfo', 'id_30', 'id_31'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T12:15:30.703132Z","iopub.execute_input":"2021-11-11T12:15:30.703374Z","iopub.status.idle":"2021-11-11T12:15:32.55243Z","shell.execute_reply.started":"2021-11-11T12:15:30.703342Z","shell.execute_reply":"2021-11-11T12:15:32.551657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now all the features are numerical. Let's fill the missings with -1.\nX_train.fillna(-1, inplace=True)\ntest.fillna(-1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T12:15:32.553654Z","iopub.execute_input":"2021-11-11T12:15:32.553896Z","iopub.status.idle":"2021-11-11T12:15:35.238072Z","shell.execute_reply.started":"2021-11-11T12:15:32.553863Z","shell.execute_reply":"2021-11-11T12:15:35.237307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Just to ensure no other category is left and if so, reduce its memory\nX_train = reduce_mem_usage_cat(X_train)\ntest = reduce_mem_usage_cat(test)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T12:15:35.239454Z","iopub.execute_input":"2021-11-11T12:15:35.23989Z","iopub.status.idle":"2021-11-11T12:15:35.311417Z","shell.execute_reply.started":"2021-11-11T12:15:35.239854Z","shell.execute_reply":"2021-11-11T12:15:35.310674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build Model and Evaluate","metadata":{}},{"cell_type":"code","source":"NFOLDS = 5\nkf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=1400)\n\ny_preds = np.zeros(test.shape[0])\ny_oof = np.zeros(X_train.shape[0])\nscore = 0\n  \nfor fold, (tr_idx, val_idx) in enumerate(kf.split(X_train, y_train)):\n    clf = xgb.XGBClassifier(  # For more info about the parameters, visit https://xgboost.readthedocs.io/en/latest/parameter.html\n        n_estimators=500,\n        max_depth=12,\n        learning_rate=0.05,\n        subsample=0.9,\n        colsample_bytree=0.9,\n        gamma = 0.2,\n        alpha = 5,\n        missing=-1,\n        tree_method='gpu_hist'\n    )\n    \n    X_tr, X_vl = X_train.iloc[tr_idx, :], X_train.iloc[val_idx, :]\n    y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n    clf.fit(X_tr, y_tr)\n    y_pred_train = clf.predict_proba(X_vl)[:,1]\n    y_oof[val_idx] = y_pred_train\n    print(\"FOLD: \",fold,' AUC {}'.format(roc_auc_score(y_vl, y_pred_train)))\n    score += roc_auc_score(y_vl, y_pred_train) / NFOLDS\n    y_preds += clf.predict_proba(test)[:,1] / NFOLDS\n    \n    # delete the excess memory\n    del X_tr, X_vl, y_tr, y_vl\n    gc.collect()\n    \n    \nprint(\"\\nMEAN AUC = {}\".format(score))\nprint(\"OOF AUC = {}\".format(roc_auc_score(y_train, y_oof)))  # OOF stands for out-of-fold","metadata":{"execution":{"iopub.status.busy":"2021-11-11T12:15:35.312584Z","iopub.execute_input":"2021-11-11T12:15:35.312842Z","iopub.status.idle":"2021-11-11T12:25:15.94836Z","shell.execute_reply.started":"2021-11-11T12:15:35.312796Z","shell.execute_reply":"2021-11-11T12:25:15.946765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get xgBoost importances\nimportance_dict = {}\nfor import_type in ['weight', 'gain', 'cover']:\n    importance_dict['xgBoost-'+import_type] = clf.get_booster().get_score(importance_type=import_type)\n    \n# MinMax scale all importances\nimportance_df = pd.DataFrame(importance_dict).fillna(0)\nimportance_df = pd.DataFrame(\n    MinMaxScaler().fit_transform(importance_df),\n    columns=importance_df.columns,\n    index=importance_df.index\n)\n\n# Create mean column\nimportance_df['mean'] = importance_df.mean(axis=1)\n\n# Plot the feature importances\nimportance_df.sort_values('mean', ascending=False).head(25).plot(kind='bar', figsize=(30, 7))","metadata":{"execution":{"iopub.status.busy":"2021-11-11T12:25:15.949884Z","iopub.execute_input":"2021-11-11T12:25:15.950164Z","iopub.status.idle":"2021-11-11T12:25:16.706007Z","shell.execute_reply.started":"2021-11-11T12:25:15.950129Z","shell.execute_reply":"2021-11-11T12:25:16.705309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# delete the excess memory\ndel clf, importance_df\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-11-11T12:25:16.707424Z","iopub.execute_input":"2021-11-11T12:25:16.70791Z","iopub.status.idle":"2021-11-11T12:25:16.897067Z","shell.execute_reply.started":"2021-11-11T12:25:16.707862Z","shell.execute_reply":"2021-11-11T12:25:16.89609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepare for submission\nsub = pd.read_csv('/kaggle/input/ieee-fraud-detection/sample_submission.csv', index_col='TransactionID')\nsub['isFraud'] = y_preds\nsub.to_csv('submission.csv')\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-11T12:25:16.898644Z","iopub.execute_input":"2021-11-11T12:25:16.898959Z","iopub.status.idle":"2021-11-11T12:25:19.684064Z","shell.execute_reply.started":"2021-11-11T12:25:16.898918Z","shell.execute_reply":"2021-11-11T12:25:19.68332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Resources\n\n[Fraud complete EDA](https://www.kaggle.com/jesucristo/fraud-complete-eda/notebook)\n\n[EDA for CIS Fraud Detection](https://www.kaggle.com/nroman/eda-for-cis-fraud-detection)\n\n[~Almost~ complete Feature Engineering IEEE data](https://www.kaggle.com/kabure/almost-complete-feature-engineering-ieee-data)\n\n[Extensive EDA and Modeling XGB Hyperopt](https://www.kaggle.com/kabure/extensive-eda-and-modeling-xgb-hyperopt)","metadata":{}}]}