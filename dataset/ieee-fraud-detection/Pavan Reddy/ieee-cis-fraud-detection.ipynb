{"cells":[{"metadata":{},"cell_type":"markdown","source":"### 'IEEE_CIS' Dataset is of high memory and we will explore how to handle high memory datasets in this kernel"},{"metadata":{},"cell_type":"markdown","source":" # 'IEEE-CIS' Fraud Detection Data\n \n \n \n #### The data is broken into two files identity and transaction, which are joined by TransactionID.\n \n #### Categorical Features - Transaction\n\n1. ProductCD\n2. emaildomain\n3. card1 - card6\n4. addr1, addr2\n5. P_emaildomain\n6. R_emaildomain\n7. M1 - M9\n\n#### Categorical Features - Identity\n\n1. DeviceType\n2. DeviceInfo\n3. id_12 - id_38\n \n \n \n \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport gc\nimport os","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Kaggle input path\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Read train(transaction and identity) and test(transaction and identity) data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Read train data\ntrain_trans = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/train_transaction.csv\")\ntrain_identity = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/train_identity.csv\")\n\n# Read test data\ntest_trans = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/test_transaction.csv\")\ntest_identity = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/test_identity.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Combine 'transaction' and 'identity' data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train data (Combine 'train_identity' and 'train_trans')\ndf_train = train_trans.merge(train_identity, how='left', left_index=True, right_index=True, on='TransactionID')\n\n# Test data (Combine 'test_identity' and 'test_trans')\ndf_test =  test_trans.merge(test_identity, how='left', left_index=True, right_index=True, on='TransactionID')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### In this kernel, we will delete temporary storage to provide space to RAM or else session crashes due to shortage of RAM"},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_trans, train_identity, test_trans, test_identity; x = gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 'train' and 'test' dataset Memory"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('train data memory in MB:', df_train.memory_usage().sum() / 1024**2) \nprint('test data memory in MB:', df_test.memory_usage().sum() / 1024**2) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### As the memory of train and test dataset is high and our RAM space is low, we will try to reduce the dataset memory"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Most often used function to reduce dataset memory\n\ndef reduce_mem_usage(props):\n    start_mem_usg = props.memory_usage().sum() / 1024**2 \n    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n    NAlist = [] # Keeps track of columns that have missing values filled in. \n    for col in props.columns:\n        if props[col].dtype != object:  # Exclude strings\n            \n            # Print current column type\n            #print(\"******************************\")\n            #print(\"Column: \",col)\n            #print(\"dtype before: \",props[col].dtype)\n            \n            # make variables for Int, max and min\n            IsInt = False\n            mx = props[col].max()\n            mn = props[col].min()\n            \n            # Integer does not support NA, therefore, NA needs to be filled\n            if not np.isfinite(props[col]).all(): \n                NAlist.append(col)\n                props[col].fillna(mn-1,inplace=True)  \n                   \n            # test if column can be converted to an integer\n            asint = props[col].fillna(0).astype(np.int64)\n            result = (props[col] - asint)\n            result = result.sum()\n            if result > -0.01 and result < 0.01:\n                IsInt = True\n\n            \n            # Make Integer/unsigned Integer datatypes\n            if IsInt:\n                if mn >= 0:\n                    if mx < 255:\n                        props[col] = props[col].astype(np.uint8)\n                    elif mx < 65535:\n                      props[col] = props[col].astype(np.uint16)\n                    elif mx < 4294967295:\n                        props[col] = props[col].astype(np.uint32)\n                    else:\n                        props[col] = props[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        props[col] = props[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        props[col] = props[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        props[col] = props[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        props[col] = props[col].astype(np.int64) \n            # Make float datatypes 32 bit\n            else:\n                props[col] = props[col].astype(np.float32)\n            \n            # Print new column type\n            #print(\"dtype after: \",props[col].dtype)\n            #print(\"******************************\")\n    \n    # Print final result\n    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n    mem_usg = props.memory_usage().sum() / 1024**2 \n    print(\"Memory usage is: \",mem_usg,\" MB\")\n    print(\"This is \",100*mem_usg/start_mem_usg,\"% of the initial size\")\n    return props","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduce_mem_usage(df_train)\nreduce_mem_usage(df_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusion: \n1. Train dataset memory reduced from '1955' MB to '546' MB\n2. Test  dataset memory reduced from '1673' MB to '459' MB"},{"metadata":{},"cell_type":"markdown","source":"## Combine train and test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['isFraud'] = 'test'\ndf = pd.concat([df_train, df_test], axis = 0, sort=False)\ndf = df.reset_index()\ndf.drop('index', axis=1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del df_train, df_test; x = gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Understanding the columns\n1. transaction related columns\n2. card related columns\n3. addr,dist,domain related columns\n4. C columns\n5. D columns\n6. M columns\n7. V columns\n8. others ('identity' columns along with 'device' information)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transaction columns\ndf.columns[0:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Card related columns\ndf.columns[5:11]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  addr, dist, emaildomain related columns\ndf.columns[11:17]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# C columns\ndf.columns[17:31]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# D columns\ndf.columns[31:46]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# M columns\ndf.columns[46:55]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# V columns\ndf.columns[55:394]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Identity columns\ndf.columns[394:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary\n**It's been observed that, V columns are in large number (around 340). So we can either ignore all V columns or apply PCA for all V columns in order to reduce the columns/memory.**\n\n**In this kernel, we will apply PCA for V columns in order not to lose any information.**"},{"metadata":{},"cell_type":"markdown","source":"#### Email Mappings"},{"metadata":{"trusted":true},"cell_type":"code","source":"emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', \n          'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft',\n          'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',\n          'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', \n          'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink',\n          'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other',\n          'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', \n          'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', \n          'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo',\n          'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n          'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft',\n          'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', \n          'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', \n           'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', \n          'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', \n          'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', \n          'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', \n          'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other',\n          'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = ['P_emaildomain', 'R_emaildomain']\n\nfor x in col:\n    df[x + '_bin'] = df[x].map(emails)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Label Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in df.drop('isFraud', axis = 1).columns:    \n    if df[col].dtype == 'object':\n        le = preprocessing.LabelEncoder()\n        le.fit(list(df[col].values))\n        df[col] = le.transform(list(df[col].values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.memory_usage().sum() / 1024**2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduce_mem_usage(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### I have tried to apply PCA for V columns using the entire dataset 'df' at once, but session crashed due to low RAM.\n\n### Kaggle has RAM usage of 13GB. So i need to split the data and perform PCA"},{"metadata":{},"cell_type":"markdown","source":"### Split the data back into train and test "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train, df_test = df[df['isFraud'] != 'test'], df[df['isFraud'] == 'test'].drop('isFraud', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del df; x = gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Applying PCA for V columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import minmax_scale","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"v_columns = df_train.columns[55:394]\nv_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# fill NaN values and scale the data using scalar function\n\n# for train data\nfor col in v_columns:\n    df_train[col] = df_train[col].fillna((df_train[col].min() - 2))\n    df_train[col] = (minmax_scale(df_train[col], feature_range=(0,1)))\n\n# for test data\nfor col in v_columns:\n    df_test[col] = df_test[col].fillna((df_test[col].min() - 2))\n    df_test[col] = (minmax_scale(df_test[col], feature_range=(0,1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def func_pca(df, v_columns, prefix):\n    \n    pca = PCA(n_components = 30, random_state = 1)\n    pca = pca.fit_transform(df[v_columns])\n    pca_df = pd.DataFrame(pca)\n    df.drop(v_columns, axis=1, inplace=True)\n    pca_df.rename(columns=lambda x: str(prefix)+str(x), inplace=True)\n    df = pd.concat([df, pca_df], axis=1)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"train = func_pca(df_train, v_columns = v_columns, prefix = 'PCA_V_')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del df_train; x= gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = func_pca(df_test, v_columns = v_columns, prefix = 'PCA_V_')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del df_test; x= gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary\n\n**Still test dataset is having 1.2 GB memory. Function that we have used to reduce the memory is not effective.**\n\n**There is a simple approach to reduce dataset memory. Just convert float64 into float32 and int64 into int32**"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in test.columns:\n    if test[col].dtype=='float64': test[col] = test[col].astype('float32')    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in train.columns:\n    if train[col].dtype=='float64': train[col] = train[col].astype('float32')\n    if train[col].dtype=='int64': train[col] = train[col].astype('int32')      ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusion: Almost 50% of test dataset memory is reduced with simple approach"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Build the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# spilt the train data for 'training' and 'validation'.\n\n# train index\nidxT = train.index[:3*len(train)//4]\n\n# Validation index\nidxV = train.index[3*len(train)//4:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# only X columns\ncols = train.columns.difference(['isFraud'])\ncols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model\nimport xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# xgb.XGBClassifier?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = xgb.XGBClassifier(n_estimators = 300, eval_metric = 'auc')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train.loc[idxT, cols]\ny_train = train['isFraud'][idxT]\n\nX_val = train.loc[idxV, cols]\ny_val = train['isFraud'][idxV]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" clf.fit(X_train, y_train,eval_set=[(X_val, y_val)],verbose=50, early_stopping_rounds=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion: \n\n#### Without using any feature engineering or optimizing the model, we have acheived validation accuracy around 90% (which is not bad). \n\n#### Now the model is ready and we can predict the 'Y' for test data."},{"metadata":{},"cell_type":"markdown","source":"### References for feature engineering, EDA and some advance techniques.\n\nhttps://www.kaggle.com/cdeotte/xgb-fraud-with-magic-0-9600\n\nhttps://www.kaggle.com/alijs1/ieee-transaction-columns-reference\n\nhttps://www.kaggle.com/kabure/extensive-eda-and-modeling-xgb-hyperopt/comments\n\n#### Thank you for reading the kernel, hope you find it useful:)\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}