{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Welcome to my fraud detection Kernel. \n\n"},{"metadata":{},"cell_type":"markdown","source":"\n![](http://technosavvy.co.ke/wp-content/uploads/2015/12/fraud.jpg)"},{"metadata":{},"cell_type":"markdown","source":"# Competition Objective is to detect fraud in transactions; \n\n## Data\n\n\nIn this competition you are predicting the probability that an online transaction is fraudulent, as denoted by the binary target ```isFraud```.\n\nThe data is broken into two files **identity** and **transaction**, which are joined by ```TransactionID```. \n\n> Note: Not all transactions have corresponding identity information.\n\n**Categorical Features - Transaction**\n\n- ProductCD\n- emaildomain\n- card1 - card6\n- addr1, addr2\n- P_emaildomain\n- R_emaildomain\n- M1 - M9\n\n**Categorical Features - Identity**\n\n- DeviceType\n- DeviceInfo\n- id_12 - id_38\n\n**The TransactionDT feature is a timedelta from a given reference datetime (not an actual timestamp).**\n\n## Questions\nI will start exploring based on Categorical Features and Transaction Amounts.\nThe aim is answer some questions like:\n- What type of data we have on our data?\n- How many cols, rows, missing values we have?\n- Whats the target distribution?\n- What's the Transactions values distribution of fraud and no fraud transactions?\n- We have predominant fraudulent products? \n- What features or target shows some interesting patterns? \n- And a lot of more questions that will raise trought the exploration. \n\n\nI hope you enjoy my kernel and if it be useful for you, <b>upvote</b> the kernel"},{"metadata":{},"cell_type":"markdown","source":"## <font color=\"red\">Please if this kernel were useful for you, please <b>UPVOTE</b> the kernel and give me your feedback =)</font>\n"},{"metadata":{},"cell_type":"markdown","source":"## Importing Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Standard plotly imports\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom plotly.offline import iplot, init_notebook_mode\nimport cufflinks\nimport cufflinks as cf\nimport plotly.figure_factory as ff\n\n# Using plotly + cufflinks in offline mode\ninit_notebook_mode(connected=True)\ncufflinks.go_offline(connected=True)\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reading dataset"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df_train_id = pd.read_csv(\"../input/train_identity.csv\", nrows=50000)\ndf_train_trans = pd.read_csv(\"../input/train_transaction.csv\", nrows=50000)\n#df_test_id = pd.read_csv(\"../input/test_identity.csv\")\n#df_test_trans = pd.read_csv(\"../input/test_transaction.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Functions to epxlore the data"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def resumetable(df):\n    print(f\"Dataset Shape: {df.shape}\")\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['First Value'] = df.loc[0].values\n    summary['Second Value'] = df.loc[1].values\n    summary['Third Value'] = df.loc[2].values\n\n    for name in summary['Name'].value_counts().index:\n        summary.loc[summary['Name'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True), base=2),2) \n\n    return summary\n\ndef plot_distribution(df, var_select=None, title=None, bins=1.0): \n    # Calculate the correlation coefficient between the new variable and the target\n    tmp_fraud = df[df['isFraud'] == 1]\n    tmp_no_fraud = df[df['isFraud'] == 0]    \n    corr = df['isFraud'].corr(df[var_select])\n    corr = np.round(corr,3)\n    tmp1 = tmp_fraud[var_select].dropna()\n    tmp2 = tmp_no_fraud[var_select].dropna()\n    hist_data = [tmp1, tmp2]\n    \n    group_labels = ['Fraud', 'No Fraud']\n    colors = ['seagreen','indianred', ]\n\n    fig = ff.create_distplot(hist_data,\n                             group_labels,\n                             colors = colors, \n                             show_hist = True,\n                             curve_type='kde', \n                             bin_size = bins\n                            )\n    \n    fig['layout'].update(title = title+' '+'(corr target ='+ str(corr)+')')\n\n    iplot(fig, filename = 'Density plot')\n    \ndef plot_dist_churn(df, col, binary=None):\n    tmp_churn = df[df[binary] == 1]\n    tmp_no_churn = df[df[binary] == 0]\n    tmp_attr = round(tmp_churn[col].value_counts().sort_index() / df[col].value_counts().sort_index(),2)*100\n    print(f'Distribution of {col}: ')\n    trace1 = go.Bar(\n        x=tmp_churn[col].value_counts().sort_index().index,\n        y=tmp_churn[col].value_counts().sort_index().values, \n        name='Fraud',opacity = 0.8, marker=dict(\n            color='seagreen',\n            line=dict(color='#000000',width=1)))\n\n    trace2 = go.Bar(\n        x=tmp_no_churn[col].value_counts().sort_index().index,\n        y=tmp_no_churn[col].value_counts().sort_index().values,\n        name='No Fraud', opacity = 0.8, \n        marker=dict(\n            color='indianred',\n            line=dict(color='#000000',\n                      width=1)\n        )\n    )\n\n    trace3 =  go.Scatter(   \n        x=tmp_attr.sort_index().index,\n        y=tmp_attr.sort_index().values,\n        yaxis = 'y2', \n        name='% Fraud', opacity = 0.6, \n        marker=dict(\n            color='black',\n            line=dict(color='#000000',\n                      width=2 )\n        )\n    )\n    \n    layout = dict(title =  f'Distribution of {str(col)} feature by %Fraud',\n              xaxis=dict(type='category'), \n              yaxis=dict(title= 'Count'), \n              yaxis2=dict(range= [0, 15], \n                          overlaying= 'y', \n                          anchor= 'x', \n                          side= 'right',\n                          zeroline=False,\n                          showgrid= False, \n                          title= 'Percentual Fraud Transactions'\n                         ))\n\n    fig = go.Figure(data=[trace1, trace2, trace3], layout=layout)\n    iplot(fig)\n    \n## Function to reduce the DF size\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n\n## REducing memory\ndf_train_trans = reduce_mem_usage(df_train_trans)\ndf_train_id = reduce_mem_usage(df_train_id)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## As we have a high dimensional data, I will reduce the memory usage"},{"metadata":{"trusted":true},"cell_type":"code","source":"## REducing memory\ndf_train_trans = reduce_mem_usage(df_train_trans)\ndf_train_id = reduce_mem_usage(df_train_id)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Knowning the Identity dataset\n- What type of data we have on our data?"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"resumetable(df_train_id)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have shape 144.2 rows by 41 columns. <br>\nAlso, we can see that almost all features has missing values. We will need to work with that. <br>\nLet's see the transactions table and see the details "},{"metadata":{},"cell_type":"markdown","source":"## Knowing the transactions\n- What type of data we have on our data?"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"resumetable(df_train_trans)[:25]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow, We have a bizarre high dimension. The shape of Transactions is: 506691, 393<br>\nI will need some time to explore it further. The first aim is start simple. "},{"metadata":{},"cell_type":"markdown","source":"## Understanding the Target Distribution"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(\"Transactions % Fraud:\")\nprint(round(df_train_trans[['isFraud', 'TransactionID']]['isFraud'].value_counts(normalize=True) * 100,2))\n# df_train.groupby('Churn')['customerID'].count().iplot(kind='bar', title='Churn (Target) Distribution', \n#                                                      xTitle='Customer Churn?', yTitle='Count')\n\ntrace0 = go.Bar(\n    x=df_train_trans[['isFraud', 'TransactionID']].groupby('isFraud')['TransactionID'].count().index,\n    y=df_train_trans[['isFraud', 'TransactionID']].groupby('isFraud')['TransactionID'].count().values,\n    marker=dict(\n        color=['indianred', 'seagreen']),\n)\n\ndata = [trace0] \nlayout = go.Layout(\n    title='Fraud (Target) Distribution <br>## 0: No Fraud | 1: Is Fraud ##', \n    xaxis=dict(\n        title='Transaction is fraud', \n        type='category'),\n    yaxis=dict(\n        title='Count')\n)\n\nfig = go.Figure(data=data, layout=layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nice. <br>\nWe have only 3.5% of positive values in our target. It's an unbalanced data and we will keep investiganting the data to find some insight."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_trans(tmp, num_col='TransactionAmt'):\n    print(f\"The mininum value in Transaction Amount is {tmp[num_col].min()}, median is {round(tmp[num_col].median(),2)}, and the maximum is {df_train_trans[num_col].max()}\")\n    print(f\"The mean Transaction Amount of Fraudulent Transactions is {round(tmp[tmp['isFraud'] == 1][num_col].median(),2)}\\\n          \\nThe mean Transaction Amount of No-Fraudulent Transactions is {round(tmp[tmp['isFraud'] == 0][num_col].median(),2)}\")\n    \nprint_trans(df_train_trans[['isFraud', 'TransactionAmt']], 'TransactionAmt')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that fraudulent transactions has a higher mean than No-Fraudulent Transactions"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Transaction Amount Quantiles: \")\nprint(df_train_trans['TransactionAmt'].quantile([0.01, .025, .1, .25, .5, .75, .975, .99]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To avoid us of outliers and a better view of distribution, I will filter the data and get only values equal or lower than 800"},{"metadata":{},"cell_type":"markdown","source":"## Ploting and Knowing Transaction Amount distribution \n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# df_train_trans['TransactionAmt_log'] = df_train_trans['TransactionAmt'].apply(np.log)\ntmp = df_train_trans[['TransactionAmt', 'isFraud']]\ntmp['TransactionAmt_log'] = tmp['TransactionAmt'].apply(np.log)\n## Calling the function\nplot_distribution(tmp[(tmp['TransactionAmt'] <= 800)], 'TransactionAmt', 'Transaction Amount Distribution', bins=10.0,)\nplot_distribution(tmp[(tmp['TransactionAmt'] <= 800)], 'TransactionAmt_log', 'Transaction Amount Log Distribution', bins=0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We don't have a high correlation between Transaction Amount and Fraud Transactions. <br>\nAlso, we can see many cases of fraud transactions with values between 5 to 14 and other peak in 75 -  85.\n\nLet's keep investigating this data"},{"metadata":{},"cell_type":"markdown","source":"## Knowing the Product feature\n- We have predominant fraudulent products? "},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_dist_churn(df_train_trans[['ProductCD', 'isFraud']], 'ProductCD', 'isFraud')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool!!! I think that this chart is very insightful. <br>\nAltought the W is the most frequent Product we can see higher values in C, R and S products altought we have many lowest values in these categories. "},{"metadata":{"_kg_hide-input":true},"cell_type":"markdown","source":"## Exploring Card Features \nWe have 6 columns that are about the Card of the transaction.<br>\nI will start by the categoricals and after it, I will explore the continuous "},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ['card4', 'card6']:\n    df_train_trans[col] = df_train_trans[col].fillna('NoInf')\n    plot_dist_churn(df_train_trans, col, 'isFraud')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool!! Again, we can clearly see that card4 we can't see different patterns, but in Card6 we can note that Credit has higher incidence of fraud than Debit payment"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Card Features Quantiles: \")\nprint(df_train_trans[['card1', 'card2', 'card3', 'card5']].quantile([0.01, .025, .1, .25, .5, .75, .975, .99]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will transform Card1 and Card2 to Logarithm scale to we better understand the distribution "},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ['card1', 'card2', 'card3', 'card5']:\n    df_train_trans[str(col)+'_log'] = np.log(df_train_trans[col])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Card1 feature by Target"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Calling the function\nplot_distribution(df_train_trans[['isFraud','card1_log']], 'card1_log', 'Card 1 Feature Log Distribution by Target', bins=0.05,)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Card2 feature by Target"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Calling the function\nplot_distribution(df_train_trans[['isFraud','card2_log']], 'card2_log', 'Card 2 Feature Log Distribution by Target', bins=0.05)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Card3 feature by Target\n- As we have many values with low frequency, I will set all values with frequency lower than 10 as -99"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_train_trans.loc[df_train_trans.card3.isin(df_train_trans['card3'].value_counts()[df_train_trans['card3'].value_counts() < 10].index), 'card3'] = -99","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plot_dist_churn(df_train_trans[['card3', 'isFraud']], 'card3', 'isFraud')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Card5 feature by Target\n- Again, as we have many values with low frequency I will set all values with frequency lower than 20 as -99"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_train_trans.loc[df_train_trans.card5.isin(df_train_trans['card5']\\\n                                             .value_counts()\\\n                                             [df_train_trans['card5']\\\n                                              .value_counts() < 20]\\\n                                             .index), 'card5'] = -99\n\nplot_dist_churn(df_train_trans[['card5', 'isFraud']], 'card5', 'isFraud')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploring the M2-M9 features\n- Seen"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"tmp = df_train_trans[['M1','M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'isFraud']]\nfor col in ['M1','M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']:\n    tmp[col] = tmp[col].fillna('NoInf')\n    plot_dist_churn(tmp, col, 'isFraud')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"## ScatterPollar of Binary Features"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"markdown","source":"from sklearn.preprocessing import LabelEncoder\ntmp = df_train_trans[]\n#Label encoding Binary columns\nle = LabelEncoder()\n\ntmp_churn = df_train_trans[df_train_trans['isFraud'] == 1]\ntmp_no_churn = df_train_trans[df_train_trans['isFraud'] == 0]\n\nbi_cs = df_train_trans.nunique()[df_train_trans.nunique() == 2].keys()\ndat_rad = df_train_trans[bi_cs]\n\nfor cols in bi_cs :\n    tmp_churn[cols] = le.fit_transform(tmp_churn[cols])\n    \ndata_frame_x = tmp_churn[bi_cs].sum().reset_index()\ndata_frame_x.columns  = [\"feature\",\"yes\"]\ndata_frame_x[\"no\"]    = tmp_churn.shape[0]  - data_frame_x[\"yes\"]\ndata_frame_x  = data_frame_x[data_frame_x[\"feature\"] != \"Churn\"]\n\n#count of 1's(yes)\ntrace1 = go.Scatterpolar(r = data_frame_x[\"yes\"].values.tolist(), \n                         theta = data_frame_x[\"feature\"].tolist(),\n                         fill  = \"toself\",name = \"Fraud 1's\",\n                         mode = \"markers+lines\", visible=True,\n                         marker = dict(size = 5)\n                        )\n\n#count of 0's(No)\ntrace2 = go.Scatterpolar(r = data_frame_x[\"no\"].values.tolist(),\n                         theta = data_frame_x[\"feature\"].tolist(),\n                         fill  = \"toself\",name = \"Fraud 0's\",\n                         mode = \"markers+lines\", visible=True,\n                         marker = dict(size = 5)\n                        ) \nfor cols in bi_cs :\n    tmp_no_churn[cols] = le.fit_transform(tmp_no_churn[cols])\n    \ndata_frame_x = tmp_no_churn[bi_cs].sum().reset_index()\ndata_frame_x.columns  = [\"feature\",\"yes\"]\ndata_frame_x[\"no\"]    = tmp_no_churn.shape[0]  - data_frame_x[\"yes\"]\ndata_frame_x  = data_frame_x[data_frame_x[\"feature\"] != \"Churn\"]\n\n#count of 1's(yes)\ntrace3 = go.Scatterpolar(r = data_frame_x[\"yes\"].values.tolist(),\n                         theta = data_frame_x[\"feature\"].tolist(),\n                         fill  = \"toself\",name = \"NoFraud 1's\",\n                         mode = \"markers+lines\", visible=False,\n                         marker = dict(size = 5)\n                        )\n\n#count of 0's(No)\ntrace4 = go.Scatterpolar(r = data_frame_x[\"no\"].values.tolist(),\n                         theta = data_frame_x[\"feature\"].tolist(),\n                         fill  = \"toself\",name = \"NoFraud 0's\",\n                         mode = \"markers+lines\", visible=False,\n                         marker = dict(size = 5)\n                        ) \n\ndata = [trace1, trace2, trace3, trace4]\n\nupdatemenus = list([\n    dict(active=0,\n         x=-0.15,\n         buttons=list([  \n            dict(\n                label = 'Fraud Dist',\n                 method = 'update',\n                 args = [{'visible': [True, True, False, False]}, \n                     {'title': 'Transaction Fraud Binary Counting Distribution'}]),\n             \n             dict(\n                  label = 'No-Fraud Dist',\n                 method = 'update',\n                 args = [{'visible': [False, False, True, True]},\n                     {'title': 'Transaction No-Fraud Binary Counting Distribution'}]),\n\n        ]),\n    )\n])\n\nlayout = dict(title='ScatterPolar Distribution of Fraud and No-Fraud Transactions (Select from Dropdown)', \n              showlegend=False,\n              updatemenus=updatemenus)\n\nfig = dict(data=data, layout=layout)\n\niplot(fig)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# NOTE: THIS KERNEL IS NOT FINISHED. I WILL KEEP EXPLORING IT. "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}