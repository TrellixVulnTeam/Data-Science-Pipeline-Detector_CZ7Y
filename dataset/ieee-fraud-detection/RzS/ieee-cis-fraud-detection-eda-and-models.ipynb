{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Credits for this notebook:\n\nhttps://www.kaggle.com/kabure/extensive-eda-and-modeling-xgb-hyperopt\n\nhttps://www.kaggle.com/artgor/eda-and-models"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\nfrom scipy import stats\nfrom cycler import cycler\nimport math\nimport matplotlib\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, precision_score, recall_score, f1_score\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn import svm, tree, linear_model, neighbors, ensemble\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.neighbors import KNeighborsClassifier\nimport matplotlib\nimport xgboost as xgb\nimport gc\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fantastic helper functions borrowed from:\n\nhttps://www.kaggle.com/kabure/extensive-eda-and-modeling-xgb-hyperopt"},{"metadata":{"trusted":true},"cell_type":"code","source":"def tablestats(df):\n    print(f\"Dataset Shape: {df.shape}\")\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['Mode (with NaNs)'] = df.mode(dropna = False).iloc[0].values\n    summary['Mode (without NaNs)'] = df.mode().iloc[0].values\n\n    for name in summary['Name'].value_counts().index:\n        summary.loc[summary['Name'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True), base=2),2) \n\n    return summary\n## Function to reduce the DF size\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n\ndef calcoutliers(df_num): \n\n    #the mem reduction routine changes the transaction amount into float16. Computations like sum, mean, std result in overflow.\n    vals = df_num.astype('float64') \n    \n    # calculating mean and std of the array\n    data_mean = np.mean(vals)\n    data_std = np.std(vals)\n    \n    # calculating min and max of the array\n    data_min = np.min(vals)\n    data_max = np.max(vals)\n\n    # seting the cut line to both higher and lower values\n    # You can change this value\n    cut = data_std * 3\n\n    #Calculating the higher and lower cut values\n    lower = data_mean - cut \n    upper = data_mean + cut\n\n    # creating an array of lower, higher and total outlier values \n    outliers_lower = [x for x in df_num if x < lower]\n    outliers_higher = [x for x in df_num if x > upper]\n    outliers_total = [x for x in df_num if x < lower or x > upper]\n\n    # array without outlier values\n    outliers_removed = [x for x in df_num if x > lower and x < upper]\n    \n    print('Mean: ', data_mean)\n    print('StD: ', data_std)\n    print('Min. value: ', data_min)\n    print('Max. value: ', data_max)\n    print('Identified lower outliers: %d' % len(outliers_lower)) # printing total number of values in lower cut of outliers\n    print('Identified upper outliers: %d' % len(outliers_higher)) # printing total number of values in higher cut of outliers\n    print('Total outlier observations: %d' % len(outliers_total)) # printing total number of values outliers of both sides\n    print('Non-outlier observations: %d' % len(outliers_removed)) # printing total number of non outlier values\n    print(\"% outliers: \", round((len(outliers_total) / len(outliers_removed) )*100, 4)) # % outliers in points\n    \n    return","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def autolabel(rects, xpos='center'):\n    \"\"\"\n    Attach a text label above each bar in *rects*, displaying its height.\n\n    *xpos* indicates which side to place the text w.r.t. the center of\n    the bar. It can be one of the following {'center', 'right', 'left'}.\n    \"\"\"\n\n    xpos = xpos.lower()  # normalize the case of the parameter\n    ha = {'center': 'center', 'right': 'left', 'left': 'right'}\n    offset = {'center': 0.5, 'right': 0.57, 'left': 0.43}  # x_txt = x + w*off\n\n    for rect in rects:\n        height = rect.get_height()\n        ax.text(rect.get_x() + rect.get_width()*offset[xpos], 1.01*height,\n                '{}'.format(height), ha=ha[xpos], va='bottom')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reading Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_identity = pd.read_csv('/kaggle/input/ieee-fraud-detection/train_identity.csv')\ntest_identity = pd.read_csv('/kaggle/input/ieee-fraud-detection/test_identity.csv')\ntrain_transaction = pd.read_csv('/kaggle/input/ieee-fraud-detection/train_transaction.csv')\ntest_transaction = pd.read_csv('/kaggle/input/ieee-fraud-detection/test_transaction.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train_identity: ', train_identity.shape)\nprint('Train_transaction: ', train_transaction.shape)\nprint('Test_identity: ', test_identity.shape)\nprint('Test_transaction: ', test_transaction.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Transaction Table\n\n* TransactionDT: timedelta from a given reference datetime (not an actual timestamp)\n* TransactionAMT: transaction payment amount in USD\n* ProductCD: product code, the product for each transaction\n* card1 - card6: payment card information, such as card type, card category, issue bank, country, etc.\n* addr: address\n* dist: distance\n* P_ and (R__) emaildomain: purchaser and recipient email domain\n* C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.\n* D1-D15: timedelta, such as days between previous transaction, etc.\n* M1-M9: match, such as names on card and address, etc.\n* Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.\n\nhttps://www.kaggle.com/c/ieee-fraud-detection/discussion/101203\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Categorical Features:\n\n* ProductCD\n* card1 - card6\n* addr1, addr2\n* P_emaildomain\n* R_emaildomain\n* M1 - M9\n\nhttps://www.kaggle.com/c/ieee-fraud-detection/discussion/101203"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_identity.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\ntest = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train: ', train.shape)\nprint('Test: ', test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_transaction\ndel train_identity\ndel test_transaction\ndel test_identity\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Exploration\n\nSee this link for data description:\n\nhttps://www.kaggle.com/c/ieee-fraud-detection/discussion/101203"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Columns with at least one null value (train):', train.isnull().any().sum())\nprint('Columns with at least one null value (test):', test.isnull().any().sum())\nprint('Columns with all null values (train):', train.isnull().all().sum())\nprint('Columns with all null values (test):', test.isnull().all().sum())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unique Values in Columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Columns with at least one null value (train):', [col for col in train.columns if train[col].nunique() == 1])\nprint('Columns with at least one null value (test):', [col for col in test.columns if test[col].nunique() == 1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Top Null Columns By Percentage"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_stats = pd.DataFrame(train.isnull().sum()).reset_index()\ntrain_stats.columns = ['cols', 'nulls']\ntrain_stats['percent_nulls'] = 100.0 * train_stats['nulls']/train.shape[0]\ntrain_stats = train_stats.sort_values(by = 'nulls', ascending = False).reset_index(drop = True)\ntrain_stats.head(n = 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tablestats(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Target Label: isFraud"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(1, 1, figsize=(6,4))\nplt.rcParams.update({'font.size': 10})\n\ncol = 'isFraud'\nprint(train[col].value_counts(dropna=False, normalize = True))\n\ntrain[col].value_counts(dropna=False).plot(kind='bar', ax = ax)\nax.set_title(col)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ID Columns\n\n“id01 to id11 are numerical features for identity, which is collected by Vesta and security partners such as device rating, ip_domain rating, proxy rating, etc. Also it recorded behavioral fingerprint like account login times/failed to login times, how long an account stayed on the page, etc. All of these are not able to elaborate due to security partner T&C.”"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_id_cols = []\nfor col in train.columns:\n    if train[col].dtype != 'O' and col[0:2] == 'id':\n        num_id_cols.append(col)\n        \nprint('Numerical ID colums: ', len(num_id_cols))\nprint(num_id_cols)\nprint('='*60)\nprint('id_yy columns with at least one null =', train[num_id_cols].isnull().any().sum())\n\ncols = 4\nrows = math.ceil(len(num_id_cols)/cols)\n#print(rows, cols)\n\nfont = {'weight' : 'normal',\n        'size' : 18}\n\nmatplotlib.rc('font', **font)\n\nf, ax = plt.subplots(rows, cols, figsize = (40,40))\nplt.rcParams.update({'font.size': 16})\n\nfor i, col in enumerate(num_id_cols):\n    sns.distplot(a = train[col], ax = ax[int(i/cols)][i%cols], kde = False)\n    #ax[int(i/cols)][i%cols].set_title('Distribution of ', col)\n    #ax[int(i/cols)][i%cols].set_xlabel(size = 20)\n\nax[-1][-1].axis('off')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some colums that seem like mostly single valued in above plots"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['id_03', 'id_04', 'id_10']\nfor col in cols:\n    print('Column:', col)\n    print(train[col].value_counts(dropna=False, normalize=True).head(n = 10))\n    print('='*60)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Categorical ID Columns with greater than 5 unique values"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_id_cols = []\nfor col in train.columns:\n    if train[col].dtype == 'O' and col[0:2] == 'id':\n        if train[col].nunique() <= 5:\n            cat_id_cols.append(col)\n        else:\n            print('Column:', col)\n            print(train[col].value_counts(dropna=False, normalize=True).head(n=10))\n            print('='*60)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Categorical ID Columns with <= 5 unique values"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Categorical ID colums: ', len(cat_id_cols))\nprint(cat_id_cols)\n\ncols = 3\nrows = math.ceil(len(cat_id_cols)/cols)\n\nf, ax = plt.subplots(rows, cols, figsize = (30,15))\n#plt.rcParams.update({'font.size': 22})\n\nfor i, col in enumerate(cat_id_cols):\n    train[col].value_counts(dropna=False).plot(kind='barh', ax = ax[int(i/cols)][i%cols], fontsize = 18)\n    ax[int(i/cols)][i%cols].set_title('Counts of ' + col, fontsize = 18)\n\nf.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Selecting crowded x-axis\ncols = ['id_30', 'id_31', 'id_33']\nfor col in cols:\n    print('Column:', col)\n    print(train[col].value_counts(dropna=False, normalize=True).head(n=10))\n    print('='*60)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Card Columns\n\ncard1 - card6: payment card information, such as card type, card category, issue bank, country, etc.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sel_cols = []\nfor col in train.columns:\n    if train[col].dtype != 'O' and col.startswith('card'):\n        sel_cols.append(col)\n        \nprint('Numerical ID colums: ', len(sel_cols))\nprint(sel_cols)\nprint('='*60)\n\ncols = 2\nrows = math.ceil(len(sel_cols)/cols)\n#print(rows, cols)\n\nfont = {'weight' : 'normal',\n        'size' : 14}\n\nmatplotlib.rc('font', **font)\n\nf, ax = plt.subplots(rows, cols, figsize = (15,10))\nplt.rcParams.update({'font.size': 14})\n\nfor i, col in enumerate(sel_cols):\n    sns.distplot(a = train[col], ax = ax[int(i/cols)][i%cols], kde = False)\n    #ax[int(i/cols)][i%cols].set_title('Distribution of ', col)\n    #ax[int(i/cols)][i%cols].set_xlabel(size = 20)\n\nf.tight_layout()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tablestats(train[sel_cols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[sel_cols].quantile([.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sel_cols = ['card4', 'card6']\n\nf, ax = plt.subplots(1, 2, figsize = (20,4))\n\nfor i, col in enumerate(sel_cols):\n    #train[col].value_counts(dropna=False).plot(kind='barh', ax = ax[int(i/cols)][i%cols], fontsize = 18)\n    ct = pd.crosstab(train[col].fillna(\"NA\"), train['isFraud'], normalize='index', dropna=False) * 100\n    ct = ct.reset_index()\n    ct = ct.rename(columns={0:'NoFraud', 1:'Fraud'})\n\n    sns.countplot(x=train[col], hue=train['isFraud'], ax = ax[i], order = ct[col])\n    \n    ax[i].set_title('Counts of ' + col, fontsize = 14)\n    \n    bx = ax[i].twinx()\n    sns.pointplot(x=col, y='Fraud', data=ct, color='r', linestyle = '--', markers = 'x', ax = bx, order = ct[col])\n                  #order=ax[int(i/cols)][i%cols].get_xticks(), legend=False)\n\nf.tight_layout()\nplt.legend(loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Addr1 and Addr2\n\nAddr1: Zip Code\n\nAddr2: Country"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[['addr1', 'addr2']].quantile([.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tablestats(train[['addr1', 'addr2']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### M columns\n\n> M1-M9: match, such as names on card and address, etc. Mx is attribute of matching check, e.g. is phone areacode matched with billing zipcode, purchaser and recipient first/or last name match, etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"sel_cols = ['M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']\n\ncols = 3\nrows = math.ceil(len(sel_cols)/cols)\n\nf, ax = plt.subplots(rows, cols, figsize = (30,25))\n#plt.rcParams.update({'font.size': 22})\n\nfor i, col in enumerate(sel_cols):\n    #train[col].value_counts(dropna=False).plot(kind='barh', ax = ax[int(i/cols)][i%cols], fontsize = 18)\n    train[col] = train[col].fillna(\"Missing\")\n    test[col] = test[col].fillna(\"Missing\")\n    ct = pd.crosstab(train[col].fillna(\"NA\"), train['isFraud'], normalize='index', dropna=False) * 100\n    ct = ct.reset_index()\n    ct = ct.rename(columns={0:'NoFraud', 1:'Fraud'})\n\n    sns.countplot(x=train[col], hue=train['isFraud'], ax = ax[int(i/cols)][i%cols], order = ct[col])\n    plt.legend(loc='upper right')\n    ax[int(i/cols)][i%cols].set_title('Counts of ' + col, fontsize = 18)\n    \n    bx = ax[int(i/cols)][i%cols].twinx()\n    sns.pointplot(x=col, y='Fraud', data=ct, color='r', linestyle = '--', markers = 'x', ax = bx, order = ct[col])\n                  #order=ax[int(i/cols)][i%cols].get_xticks(), legend=False)\n\n        \nf.tight_layout()\n#ax[-1][-1].axis('off')\n#ax[-1][-2].axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Transaction Amount"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(3, 2, figsize = (15,10))\n\nsns.distplot(train['TransactionAmt'], kde = False, ax = ax[0][0])#, hist_kws = {'log':True})\nax[0][0].set_title('Transaction Amount (Train)') \n\nsns.distplot(test['TransactionAmt'], kde = False, ax = ax[0][1])#, hist_kws = {'log':True})\nax[0][1].set_title('Transaction Amount (Test)') \nax[0][1].legend()\n\nsns.distplot(np.log(train['TransactionAmt']), kde = False, ax = ax[1][0])#, hist_kws = {'log':True})\nax[1][0].set_title('Log Transaction Amount (Train)') \nax[1][0].legend()\n\nsns.distplot(np.log(test['TransactionAmt']), kde = False, ax = ax[1][1])#, hist_kws = {'log':True})\nax[1][1].set_title('Log Transaction Amount (Test)') \nax[1][1].legend()\n\nax[2][0].plot(np.sort(train[train['isFraud'] == 0]['TransactionAmt'].values), 'bo', label='Normal', alpha=0.3)\nax[2][0].plot(np.sort(train[train['isFraud'] == 1]['TransactionAmt'].values), 'ro', label='Fraud', alpha=0.3)\nax[2][0].set_title('Distribution of Normal/Fraud Transaction Amounts (Train)')\nax[2][0].legend()\n\nax[2][1].plot(np.sort(train['TransactionAmt'].values), 'bo', label='Train', alpha=0.3)\nax[2][1].plot(np.sort(test['TransactionAmt'].values), 'ro', label='Test', alpha=0.3)\nax[2][1].set_title('Distribution of Transaction Amounts (Train v. Test)') \nax[2][1].legend()\n\n\n\nf.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> The log transformation can be used to make highly skewed distributions less skewed. This can be valuable both for making patterns in the data more interpretable.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_bins = train[train['isFraud'] == 0]['TransactionAmt'].quantile([.01, .1, .25, .5, .75, .9, .99]).reset_index()\ntest_bins = train[train['isFraud'] == 1]['TransactionAmt'].quantile([.01, .1, .25, .5, .75, .9, .99]).reset_index()\n\nprint(pd.concat([train_bins, test_bins], axis = 1, keys = ['No Fraud', 'Fraud']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calcoutliers(train['TransactionAmt'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ProductCD\n\nProductCD: product code, the product for each transaction\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"pcd = pd.crosstab(train['ProductCD'], train['isFraud'], normalize='index') * 100\npcd = pcd.reset_index()\npcd = pcd.rename(columns={0:'NoFraud', 1:'Fraud'})\npcd\n# tmp = tmp.reset_index()\n# tmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fontsize = 14\ncol = 'ProductCD'\nf, ax = plt.subplots(1, 2, figsize=(14,5))\n\ng1 = train[col].value_counts(dropna=False).plot(kind='bar', fontsize = 14, ax = ax[0], rot = 0)\nfor p in g1.patches:\n    g1.annotate(str(np.round(100.0 * p.get_height()/train.shape[0],decimals=2)) + '%', (p.get_x()+p.get_width()/2., p.get_height()),\n                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\ng1.set_xlabel(col, fontsize = fontsize)\ng1.set_ylabel(\"Count\", fontsize = fontsize)\ng1.set_title(col + \" Distribution\", fontsize=fontsize+2)\ng1.set_ylim(0,500000)\n\nsns.countplot(x=col, hue='isFraud', data=train, ax = ax[1], order=['W', 'C',\"R\", \"H\", \"S\"])\nplt.legend(title='Fraud', loc='best', labels=['No', 'Yes'])\nax[1].set_xlabel(col, fontsize = fontsize)\nax[1].set_ylabel(\"Count\", fontsize = fontsize)\nax[1].set_title(\"Transaction by \" + col + \" and Fraud\", fontsize=fontsize+2)\nax[1].set_ylim(0,500000)\ng2 = ax[1].twinx()\nsns.pointplot(x=col, y='Fraud', data=pcd, color='r', linestyle = '--', markers = 'x', order=['W', 'C',\"R\", \"H\", \"S\"], legend=False, ax = g2)\ng2.set_ylabel(\"% of Fraud Transactions\", fontsize=fontsize)\n\nf.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(2, 1, figsize=(15,10))\n\nsns.boxplot(x='ProductCD', y='TransactionAmt', hue='isFraud', \n              data=train[train['TransactionAmt'] <= 2000], ax=ax[0])\nax[0].set_title(\"Transaction Amount Distribuition by ProductCD and Target\", fontsize=14)\nax[0].set_xlabel(\"ProductCD Name\", fontsize=17)\nax[0].set_ylabel(\"Transaction Amt Values\", fontsize=14)\n\nsns.boxenplot(x='ProductCD', y='TransactionAmt', hue='isFraud', \n              data=train[train['TransactionAmt'] <= 2000], ax=ax[1])\nax[1].set_title(\"Transaction Amount Distribuition by ProductCD and Target\", fontsize=14)\nax[1].set_xlabel(\"ProductCD Name\", fontsize=17)\nax[1].set_ylabel(\"Transaction Amt Values\", fontsize=14)\n\n#plt.subplots_adjust(hspace = 0.6, top = 0.85)\nf.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[Boxenplots](https://vita.had.co.nz/papers/letter-value-plot.pdf)\n\nSource: [Interpreting Boxplots](https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51)\n\n![Comparison of a boxplot of a nearly normal distribution and a probability density function (pdf) for a normal distribution](https://miro.medium.com/max/700/1*NRlqiZGQdsIyAu0KzP7LaQ.png)\n"},{"metadata":{},"cell_type":"markdown","source":"### Some more columns\n\nP_ and (R__) emaildomain: purchaser and recipient email domain.\n\nCertain transactions don't need recipient, so R_emaildomain is null."},{"metadata":{"trusted":true},"cell_type":"code","source":"sel_cols = ['id_30', 'id_31', 'id_33', 'DeviceType', 'DeviceInfo', 'P_emaildomain', 'R_emaildomain']\n\ncols = 2\nrows = math.ceil(len(sel_cols)/cols)\n\nf, ax = plt.subplots(rows, cols, figsize = (30,30))\n#plt.rcParams.update({'font.size': 22})\n\nfor i, col in enumerate(sel_cols):\n    train[col].value_counts(dropna=False).head(n=15).plot(kind='barh', ax = ax[int(i/cols)][i%cols], fontsize = 18)\n    ax[int(i/cols)][i%cols].set_title('Counts of ' + col, fontsize = 18)\n\nax[-1][-1].axis('off')\nf.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Transaction Time"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(train['TransactionDT'], label='train');\nplt.hist(test['TransactionDT'], label='test');\nplt.legend();\nplt.title('Distribution of transaction dates');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Emails"},{"metadata":{"trusted":true},"cell_type":"code","source":"sel_cols = ['P_emaildomain', 'R_emaildomain']\n\nf, ax = plt.subplots(2, 1, figsize = (15,15))\n\nfor i, col in enumerate(sel_cols):\n    #train[col].value_counts(dropna=False).plot(kind='barh', ax = ax[int(i/cols)][i%cols], fontsize = 18)\n    ct = pd.crosstab(train[col].fillna(\"NA\"), train['isFraud'], normalize='index', dropna=False) * 100\n    ct = ct.reset_index()\n    ct = ct.rename(columns={0:'NoFraud', 1:'Fraud'})\n    ct = ct.sort_values(by = 'Fraud', ascending = False).head(35)\n\n    sns.countplot(x=train[col], hue=train['isFraud'], ax = ax[i], order = ct[col])\n    \n    ax[i].set_title('Counts of ' + col, fontsize = 14)\n    plt.setp(ax[i].get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n\n    \n    bx = ax[i].twinx()\n    sns.pointplot(x=col, y='Fraud', data=ct, color='r', linestyle = '--', markers = 'x', ax = bx, order = ct[col])\n                  #order=ax[int(i/cols)][i%cols].get_xticks(), legend=False)\n\nf.tight_layout()\nplt.legend(loc='best')\nplt.xticks(rotation = 0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[['P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3']] = train['P_emaildomain'].str.split('.', expand=True)\ntrain[['R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3']] = train['R_emaildomain'].str.split('.', expand=True)\ntest[['P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3']] = test['P_emaildomain'].str.split('.', expand=True)\ntest[['R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3']] = test['R_emaildomain'].str.split('.', expand=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Splitting emails into 3 tokens: e.g. abc.com.au"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[['P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3']] = train['P_emaildomain'].str.split('.', expand=True)\ntrain[['R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3']] = train['R_emaildomain'].str.split('.', expand=True)\ntest[['P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3']] = test['P_emaildomain'].str.split('.', expand=True)\ntest[['R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3']] = test['R_emaildomain'].str.split('.', expand=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preparing Data for Modeling\n\nAs in this [kernel](https://www.kaggle.com/artgor/eda-and-models), we will drop values that have huge number of null values (>90%)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns = [x.replace('-','_') for x in train.columns]\ntest.columns = [x.replace('-','_') for x in test.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_null_cols = [col for col in train.columns if train[col].isnull().sum() / train.shape[0] > 0.9]\ntest_null_cols = [col for col in test.columns if test[col].isnull().sum() / test.shape[0] > 0.9]\nprint('Train columns with 90% or more null values')\nprint(train_null_cols)\nprint('='*60)\nprint('Test columns with 90% or more null values')\nprint(test_null_cols)\nprint('='*60)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"dist1/2: distances between (not limited) billing address, mailing address, zip code, IP address, phone area, etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_top_value_cols = [col for col in train.columns if train[col].value_counts(dropna=False, normalize=True).values[0] > 0.9 and col != 'isFraud']\ntest_top_value_cols = [col for col in test.columns if test[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train columns with 90% or more of same values')\nprint(train_top_value_cols)\nprint('='*60)\nprint('Test columns with 90% or more of same values')\nprint(test_top_value_cols)\nprint('='*60)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_one_value_cols = [col for col in train.columns if train[col].nunique() <= 1]\ntest_one_value_cols = [col for col in test.columns if test[col].nunique() <= 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train single value columns')\nprint(train_one_value_cols)\nprint('='*60)\nprint('Test single value columns')\nprint(test_one_value_cols)\nprint('='*60)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"remove_cols = np.sort(np.unique(train_null_cols + test_null_cols + train_top_value_cols + test_top_value_cols + train_one_value_cols + test_one_value_cols))\nprint(remove_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(columns = remove_cols)\ntest = test.drop(columns = remove_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From [Discussion](https://www.kaggle.com/c/ieee-fraud-detection/discussion/101203)\n\n**Categorical Features (Transaction Table):**\n* ProductCD\n* card1 - card6\n* addr1, addr2\n* P_emaildomain\n* R_emaildomain\n* M1 - M9\n\n**Categorical Features (Identity Table):**\n* DeviceType\n* DeviceInfo\n* id_12 - id_38\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols = ['id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', \n            'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', \n            'DeviceType', 'DeviceInfo', 'ProductCD', 'card1', 'card2', 'card3',  'card4', 'card5', 'card6',\n            'P_emaildomain', 'R_emaildomain', 'addr1', 'addr2', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9',\n            'P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3', 'R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3']\n\ncat_cols = [x for x in cat_cols if x in train.columns]\nprint(cat_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in cat_cols:\n    #print(col)\n    lbl = LabelEncoder()\n    train_vals = train[col].astype(str).values\n    test_vals  = test[col].astype(str).values\n    lbl.fit(list(train_vals) + list(test_vals))\n    train[col] = lbl.transform(list(train_vals))\n    test[col] = lbl.transform(list(test_vals))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.sort_values('TransactionDT').drop(columns = ['isFraud', 'TransactionDT', 'TransactionID'])\nX = X.replace([np.inf, -np.inf], np.nan)\n\ny = train.sort_values('TransactionDT')['isFraud']\n#X_test = test.sort_values('TransactionDT').drop(['TransactionDT', 'TransactionID'], axis=1)\n\nX_test = test.drop(['TransactionDT', 'TransactionID'], axis=1)\nX_test.replace([np.inf, -np.inf], np.nan)\n\n#del train\ntest = test[[\"TransactionDT\", 'TransactionID']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cols_with_nulls = X.columns[X.isnull().any()].tolist()\ntest_cols_with_nulls = X_test.columns[X_test.isnull().any()].tolist()\ncols_with_nulls = np.unique(train_cols_with_nulls + test_cols_with_nulls)\nlen(cols_with_nulls)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some good discussion on metric for imbalanced datasets:\n\nhttps://stats.stackexchange.com/questions/222558/classification-evaluation-metrics-for-highly-imbalanced-data\n"},{"metadata":{},"cell_type":"markdown","source":"### Untuned Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = DecisionTreeClassifier(max_depth = 20)\nclf.fit(X.drop(columns = cols_with_nulls), y)\ny_preds = clf.predict_proba(X_test.drop(columns = cols_with_nulls))[:,1]\nsub = pd.read_csv('/kaggle/input/ieee-fraud-detection/sample_submission.csv')\nsub['isFraud'] = y_preds\nsub.to_csv('DT_model.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"featuresRank = pd.DataFrame()\nfeaturesRank['features'] = X.drop(columns = cols_with_nulls).columns\nfeaturesRank['scores'] = clf.feature_importances_\nfeaturesRank = featuresRank.sort_values(by=['scores'], ascending=False).reset_index()\nfeaturesRank.plot(x = 'features', y = 'scores', kind=\"bar\", fontsize=12, figsize=(25,5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"# clf_params = {\n#     #'criterion': [\"gini\", \"entropy\"], \n#     #'max_depth': range(2,20,2), \n#     #'min_samples_split': range(2, 50, 4), \n#     #'min_samples_leaf' : range(2, 50, 4), \n#     'n_estimators': [100,200], \n#     #'max_features': ['auto', 'log2', None]\n# }\n\n# cv_split = TimeSeriesSplit(n_splits=2)\n\n# clf = RandomForestClassifier()\n\n# clf_grid = GridSearchCV(clf, clf_params, scoring = 'roc_auc', verbose = True, n_jobs = -1, cv = cv_split)\n\n# X_non_null = X.drop(columns = cols_with_nulls)\n# X_test_non_null = X_test.drop(columns = cols_with_nulls)\n\n# clf_grid.fit(X_non_null, y)\n\n# # print(clf_grid.score(X_val, y_val))\n\n# print('Best params: ', clf_grid.best_params_)\n# print('Mean cross-validated score of the best_estimator', clf_grid.best_score_)\n\n\n# #A lot of useful information\n# #print(clf_grid.cv_results_)\n\n# #Create submission file\n# y_preds = clf_grid.predict_proba(X_test_non_null)[:,1]\n# sub['isFraud'] = y_preds\n# sub.to_csv('RF_model.csv')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!nvidia-smi","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost\n\nLearning point for me:\n\nXGBoost deals with missing values by moving examples in the direction of assignment that minimizes the loss. [-Source](https://datascience.stackexchange.com/questions/15305/how-does-xgboost-learn-what-are-the-inputs-for-missing-values) \n\nAdded this to my reading list: [XGBoost: A Scalable Tree Boosting System](https://arxiv.org/pdf/1603.02754v3.pdf)"},{"metadata":{"trusted":true},"cell_type":"code","source":"tscv = TimeSeriesSplit(n_splits=5)\n\nclf = xgb.XGBClassifier(random_state = 42, tree_method = 'gpu_hist', verbosity = 2,\n                        objective='binary:logistic')#, eval_metric = 'auc')\nclf_params = {'max_depth' : np.arange(2,16,2), \n              'subsample' : np.arange(0.5,1.0,0.1),\n              'n_estimators' : [100,200,500],\n              'eta' : [0.1, 0.3, 0.5],\n              'min_child_weight': [1,2,4,8,16,32]\n             }\n\nclf_grid = GridSearchCV(clf, clf_params, scoring = 'roc_auc', verbose = True, n_jobs = -1, cv = tscv)\n\nclf_grid.fit(X, y)\n\nprint('Best params: ', clf_grid.best_params_)\nprint('Mean cross-validated score of the best_estimator', clf_grid.best_score_)\n\ny_preds = clf_grid.predict_proba(X_test)[:,1]\nsub['isFraud'] = y_preds\nsub.to_csv('XGB_model.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importance = clf_grid.best_estimator_.get_booster().get_score(importance_type=\"weight\")\nkeys = list(feature_importance.keys())\nvalues = list(feature_importance.values())\n\ndata = pd.DataFrame(data=values, index=keys, columns=[\"score\"]).sort_values(by = \"score\", ascending=False)\n\n# Top 10 features\ndata.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}