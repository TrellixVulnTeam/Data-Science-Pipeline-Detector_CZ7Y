{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import os, gc, sys, warnings, datetime\nwarnings.filterwarnings('ignore')\nfrom tqdm import tqdm\nfrom IPython.display import FileLink\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import GroupKFold\nimport xgboost as xgb\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.style.use('seaborn')\nimport seaborn as sns\nimport random\nseed = 10\nnp.random.seed(seed)\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\nimport eli5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_transaction = pd.read_csv('../input/ieee-fraud-detection/train_transaction.csv', index_col='TransactionID')\ntest_transaction = pd.read_csv('../input/ieee-fraud-detection/test_transaction.csv', index_col='TransactionID')\n\ntrain_identity = pd.read_csv('../input/ieee-fraud-detection/train_identity.csv', index_col='TransactionID')\ntest_identity = pd.read_csv('../input/ieee-fraud-detection/test_identity.csv', index_col='TransactionID')\n\nsample_submission = pd.read_csv('../input/ieee-fraud-detection/sample_submission.csv', index_col='TransactionID')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n\ndel train_transaction, test_transaction, train_identity, test_identity\n\nprint ('data length after merging')\nprint(train.shape)\nprint(test.shape)\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Initial setup"},{"metadata":{"trusted":true},"cell_type":"code","source":"col_target = 'isFraud'\n\nplot = True\n\ndrop_cols = list()\ncount_encode_list = list()\none_hot_encode_list = list()\ntarget_encode_list = list()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Helper function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\n\ndef Num_EDA(data, column):\n    plt.figure(figsize=(12, 5))\n    plt.subplot(121)\n    sns.distplot(data[(~data[column].isna())&(train[col_target] == 0)][column], label='negative')\n    sns.distplot(data[(~data[column].isna())&(train[col_target] == 1)][column], label='positive')\n    plt.title(column + ' distplot')\n    plt.legend()\n    \n    plt.subplot(122)\n    data.groupby(col_target)[column].mean().plot('bar')\n    plt.title(column + ' average')\n    plt.axhline(data[column].mean(), color='r', linestyle='-.')\n\n    plt.tight_layout()\n    plt.show()\n\n# choose sort value 30\ndef Cat_EDA(data, column, n_show=30):\n    plt.figure(figsize=(12, 5))\n    plt.subplot(121)\n    temp_data = data[column].value_counts().sort_values(ascending=False).iloc[:n_show]\n    temp_data.sort_index().plot('bar')\n    plt.title('Number of %s in data'%column)\n    \n    plt.subplot(122)\n    temp_data = data.groupby(column)[col_target].mean().sort_values(ascending=False).iloc[:n_show]\n    temp_data.sort_index().plot(kind='bar')\n    plt.axhline(data[col_target].mean(), color='r', linestyle='-.')\n    plt.title('Average fraud percentage per %s'%column)\n\n    plt.tight_layout()\n    plt.show()\n\ndef distribution(column, method='original'):\n    \n    train_col = train[~train[column].isna()][column]\n    test_col = test[~test[column].isna()][column]\n    \n    plt.figure(figsize=(12, 4))\n    if method == 'original':\n        sns.distplot(train_col, label='train')\n        sns.distplot(test_col, label='test')\n        \n    elif method == 'log1p':\n        sns.distplot(np.log1p(train_col), label='train')\n        sns.distplot(np.log1p(test_col), label='test')\n        \n    else:\n        print ('Wrong method')\n        pass\n        \n    plt.legend()\n    plt.title('%s distribution in train and test'%column)\n    plt.show()\n\n    plt.figure(figsize=(12, 4))\n    if method == 'original':\n        sns.distplot(train_col[train[col_target] == 1], label='positive')\n        sns.distplot(train_col[train[col_target] == 0], label='negative')\n        \n    elif method == 'log1p':\n        sns.distplot(np.log1p(train_col[train[col_target] == 1]), label='positive')\n        sns.distplot(np.log1p(train_col[train[col_target] == 0]), label='negative')\n        \n    else:\n        print ('Wrong method')\n        pass\n        \n    plt.legend()\n    plt.title('%s distribution in positive and negative'%column)\n    plt.show()\n\n\ndef distribution(column, method='original'):\n    \n    train_col = train[~train[column].isna()][column]\n    test_col = test[~test[column].isna()][column]\n    \n    plt.figure(figsize=(12, 4))\n    if method == 'original':\n        sns.distplot(train_col, label='train')\n        sns.distplot(test_col, label='test')\n        \n    elif method == 'log1p':\n        sns.distplot(np.log1p(train_col), label='train')\n        sns.distplot(np.log1p(test_col), label='test')\n        \n    else:\n        print ('Wrong method')\n        pass\n        \n    plt.legend()\n    plt.title('%s distribution in train and test'%column)\n    plt.show()\n\n    plt.figure(figsize=(12, 4))\n    if method == 'original':\n        sns.distplot(train_col[train[col_target] == 1], label='positive')\n        sns.distplot(train_col[train[col_target] == 0], label='negative')\n        \n    elif method == 'log1p':\n        sns.distplot(np.log1p(train_col[train[col_target] == 1]), label='positive')\n        sns.distplot(np.log1p(train_col[train[col_target] == 0]), label='negative')\n        \n    else:\n        print ('Wrong method')\n        pass\n        \n    plt.legend()\n    plt.title('%s distribution in positive and negative'%column)\n    plt.show()\n\n\ndef make_count_full(col, dropna_encode=False):\n    temp_data = pd.concat([train[col], test[col]], ignore_index=True)\n    \n    train['%s_count_full'%col] = train[col].map(temp_data.value_counts(dropna=dropna_encode))\n    test['%s_count_full'%col] = test[col].map(temp_data.value_counts(dropna=dropna_encode))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## datetime : TransactionDT"},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_datetime_features(df):\n    from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n    start_date = pd.to_datetime('2017-11-30')\n    date_range = pd.date_range('2017-01-01', '2019-01-01')\n    us_holidays = calendar().holidays(start=date_range.min(), end=date_range.max())\n\n    # add some time variables\n    df['DT'] = df['TransactionDT'].apply(lambda x:(start_date + datetime.timedelta(seconds=x)))\n    \n    df['DT_M'] = ((df['DT'].dt.year-2017)*12 + df['DT'].dt.month).astype(np.int8)\n    df['DT_W'] = ((df['DT'].dt.year-2017)*52 + df['DT'].dt.weekofyear).astype(np.int8)\n    df['DT_D'] = ((df['DT'].dt.year-2017)*365 + df['DT'].dt.dayofyear).astype(np.int16)\n    \n    df['DT_day_week'] = df['DT'].dt.dayofweek.astype(np.int8)\n    df['DT_day_month'] = df['DT'].dt.day.astype(np.int8)\n    df['DT_Hour'] = df['DT'].dt.hour.astype(np.int8)\n    \n    df['Month'] = df['DT'].dt.month.astype(np.int8)\n    df['early_morning'] = np.array([(df['DT_Hour'] >=0)&(df['DT_Hour'] <=5)]).astype(int)[0]\n    df['morning'] = np.array([(df['DT_Hour'] >=6)&(df['DT_Hour'] <=11)]).astype(int)[0]\n    df['afternoon'] = np.array([(df['DT_Hour'] >=12)&(df['DT_Hour'] <=17)]).astype(int)[0]\n    df['evening'] = np.array([(df['DT_Hour'] >=18)&(df['DT_Hour'] <=23)]).astype(int)[0]\n    \n    df['is_holiday'] = df['DT'].dt.date.astype('datetime64').isin(us_holidays).astype(np.int8)\n    return df\n\nfor df in [train, test]:\n    df = make_datetime_features(df)\n    \n# Total transactions per timeblock\nfor col in ['DT_M','DT_W','DT_D']:\n    temp_df = pd.concat([train[[col]], test[[col]]])\n    fq_encode = temp_df[col].value_counts().to_dict()\n            \n    train[col+'_total'] = train[col].map(fq_encode)\n    test[col+'_total']  = test[col].map(fq_encode)\n\ndrop_cols.append('DT')\ndel temp_df\n\nif plot:\n    train.groupby('DT_M')[col_target].mean().plot('bar')\n    plt.title('Fraud proportion through months')\n    plt.show()\n\n    train.groupby(['DT_Hour'])[col_target].mean().plot(figsize=(15, 6))\n    for i in [5, 11, 17]:\n        plt.axvline(i, color='r')\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# average fraud rate per hour per month\n\nif plot:\n    t = train.groupby(['DT_M', 'DT_Hour'])[col_target].mean()\n    t.plot(figsize=(15, 6))\n    # plt.figure(figsize=(15, 6))\n    # plt.plot(range(len(t)), t.values)\n    plt.title('Average fraud rate per hour per month')\n\n    for i in range(1, int(len(t)/24)):\n        plt.axvline(i*24, color='r', linestyle='--')\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if plot:\n    cols = ['TransactionAmt']\n    periods = ['DT_M', 'DT_D']\n\n    temp_df = pd.concat([train[cols+periods], test[cols+periods]], axis=0)\n\n    for period in periods:\n        for col in cols:\n            plt.figure(figsize=(12, 4))\n\n            if period == 'DT_D':\n                temp_df[temp_df[col]<5000].set_index(period)[col].plot(style='.', title=col)\n                temp_df[temp_df[col]>5000].set_index(period)[col].plot(style='.', title=col, color='r')\n                plt.axhline(5000, ls='--', c='r')\n\n            else:\n                temp_df.groupby(period)[col].mean().plot(kind='bar', title=col)\n\n            plt.show()\n\n    print ('number of train data which exceed 5000:', len(train[train['TransactionAmt'].sort_values(ascending=False)>5000]))\n    print ('number of test data which exceed 5000:', len(test[test['TransactionAmt'].sort_values(ascending=False)>5000]))\n\n    del temp_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if plot:\n    # TransactionAmt distribution\n    distribution('TransactionAmt', method='log1p')\n\n    # TransactionAmt value compares the closest number (abs(1.7 - 2)).\n    plt.figure(figsize=(12, 4))\n    sns.distplot(abs(train['TransactionAmt'] - np.round(train['TransactionAmt'])), label='train')\n    sns.distplot(abs(test['TransactionAmt'] - np.round(test['TransactionAmt'])), label='test')\n    plt.legend()\n    plt.show()\n\n    plt.figure(figsize=(12, 4))\n    sns.distplot(abs(train[train[col_target] == 1]\\\n        ['TransactionAmt'] - np.round(train[train[col_target] == 1]['TransactionAmt'])), label='positive')\n    sns.distplot(abs(train[train[col_target] == 0]\\\n        ['TransactionAmt'] - np.round(train[train[col_target] == 0]['TransactionAmt'])), label='negative')\n    plt.legend()\n    plt.show()\n\n    # decimal part\n    plt.figure(figsize=(12, 4))\n    sns.distplot(train['TransactionAmt'] - train['TransactionAmt'].astype(int), label='train')\n    sns.distplot(test['TransactionAmt'] - test['TransactionAmt'].astype(int), label='test')\n    plt.legend()\n    plt.title('TransactionAmt decimal distribution in train and test')\n    plt.show()\n\n    plt.figure(figsize=(12, 4))\n    sns.distplot(1000*abs(train[train[col_target] == 1]['TransactionAmt'] - train[train[col_target] == 1]\\\n                                                                                     ['TransactionAmt'].astype(int)), label='positive')\n    sns.distplot(1000*abs(train[train[col_target] == 0]['TransactionAmt'] - train[train[col_target] == 0]\\\n                                                                                     ['TransactionAmt'].astype(int)), label='negative')\n    plt.legend()\n    plt.title('TransactionAmt decimal distribution in positive and negative data')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## money : TransactionAmt"},{"metadata":{"trusted":true},"cell_type":"code","source":"if plot:\n    cols = ['TransactionAmt']\n    periods = ['DT_M', 'DT_D', 'DT_Hour']\n\n    temp_df = pd.concat([train[cols+periods], test[cols+periods]], axis=0)\n\n    for period in periods:\n        for col in cols:\n            if period == 'DT_D':\n                plt.figure(figsize=(12, 4))\n                temp_df[temp_df[col]<5000].set_index(period)[col].plot(style='.', title=col)\n                temp_df[temp_df[col]>5000].set_index(period)[col].plot(style='.', title=col, color='r')\n                plt.axhline(5000, ls='--', c='r')\n                plt.title('%s Scatter plot and outlier'%period)\n                plt.show()\n\n            else:\n                temp_1 = train.groupby(period)[col].mean()\n                temp_2 = train.groupby(period)[col_target].mean()\n\n                plt.figure(figsize=(12, 4))\n                temp_1.plot(kind='bar')\n                (1000*temp_2).plot()\n                plt.title('%s Mean and fraud rate, coef %s'%(period, np.corrcoef(temp_1, temp_2)[0, 1]))\n\n                plt.show()\n\n                temp_1 = train.groupby(period)[col].std()\n                temp_2 = train.groupby(period)[col_target].mean()\n\n                plt.figure(figsize=(12, 4))\n                temp_1.plot(kind='bar')\n                (1000*temp_2).plot()\n                plt.title('%s Std and fraud rate, coef %s'%(period, np.corrcoef(temp_1, temp_2)[0, 1]))\n\n                plt.show()\n\n\n            plt.show()\n\n    print ('number of train data which exceed 5000:', len(train[train['TransactionAmt'].sort_values(ascending=False)>5000]))\n    print ('number of test data which exceed 5000:', len(test[test['TransactionAmt'].sort_values(ascending=False)>5000]))\n\n    del temp_df, temp_1, temp_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # remove ouliar\n# train[cols] = train[cols].clip(0, 5000)\n# test[cols] = test[cols].clip(0, 5000)\n\nfor df in [train, test]:\n    df['TransactionAmt_charge'] = abs(df['TransactionAmt'] - np.round(df['TransactionAmt']))\n    df['TransactionAmt_decimal'] = 1000*(df['TransactionAmt'] - df['TransactionAmt'].astype(int))\n\n    # foreign currency transforming makes the two decimal places\n    df['TransactionAmt_decimal'] = df['TransactionAmt'].apply(lambda x:str('%.3f'%x)[-1])\n\n# TransactionAmt per time\n\nfor t in ['DT_M', 'DT_Hour', 'card1','card2','card3', 'card5','addr1']:\n    temp_df = pd.concat([train[[t, 'TransactionAmt']], test[[t, 'TransactionAmt']]], axis=0)\n    for df in [train, test]:\n        df['VAL_MEAN_TransactionAmt_BY_%s'%t] = df[t].map(temp_df.groupby(t)['TransactionAmt'].mean())\n        df['VAL_STD_TransactionAmt_BY_%s'%t] = df[t].map(temp_df.groupby(t)['TransactionAmt'].std())\n    del temp_df\n    \ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## product code : ProductCD"},{"metadata":{"trusted":true},"cell_type":"code","source":"if plot:\n    Cat_EDA(train, 'ProductCD')\n\ntrain['is_code_C'] = sum([train['ProductCD'] == 'C'])\ntest['is_code_C'] = sum([test['ProductCD'] == 'C'])\n\nfor product_col in ['ProductCD']:\n    make_count_full(product_col)\n    if plot:\n        distribution('%s_count_full'%product_col)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## card : card1~6"},{"metadata":{"trusted":true},"cell_type":"code","source":"if plot:\n    # few unique EDA\n    for col in ['card4', 'card6']:\n        Cat_EDA(train, col)\n\ntrain['is_card4_discover'] = sum([train['card4'] == 'discover'])\ntest['is_card4_discover'] = sum([test['card4'] == 'discover'])\n\ntrain['is_card6_discover'] = sum([train['card6'] == 'credit'])\ntest['is_card6_discover'] = sum([test['card6'] == 'credit'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if plot:\n    for i, m in zip(['card1', 'card2', 'card3', 'card5'], ['original', 'original', 'log1p', 'original']):\n        distribution(i, method=m)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count encoding for some card feature. \n# Explained in this kernel: https://www.kaggle.com/nroman/eda-for-cis-fraud-detection\n\n# card3 doesn't show any clue in eda\nfor card_col in ['card1', 'card2', 'card5']:\n    make_count_full(card_col)\n    if plot:\n        distribution('%s_count_full'%card_col)\n\ncount_encode_list.extend(['card1', 'card2', 'card3', 'card5'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## addr : addr1~2"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ['addr1', 'addr2']: \n    print ('nunique in train', col, train[col].nunique())\n    print ('nunique in test', col, test[col].nunique())\n    \n    print('No intersection in Train', col, len(train[~train[col].isin(test[col])]))\n    print('Intersection in Train', col, len(train[train[col].isin(test[col])]))\n    \n    if plot:\n        Cat_EDA(train, col, 30)\n        distribution(col)\n    \n    print('#'*20)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# na的mapping可以再考慮要不要讓他自動補值\n# Count encoding for addr feature. \n# Explained in this kernel: https://www.kaggle.com/nroman/eda-for-cis-fraud-detection\n\nfor addr_col in ['addr1', 'addr2']:\n    make_count_full(addr_col)\n    if plot:\n        distribution('%s_count_full'%addr_col)\n    \ncount_encode_list.extend(['addr1', 'addr2'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_uid(data):\n    # use card1, card2, card5, addr1, P_emaildomain\n    former_cols = data.columns\n    \n    # 2*3-3-1 = 4\n    data['uid_card1_2'] = data['card1'].astype(str) + data['card2'].astype(str)\n    data['uid_card1_5'] = data['card1'].astype(str) + data['card5'].astype(str)\n    data['uid_card1_2_5'] = data['uid_card1_2'] + data['card5'].astype(str)\n    \n    data['uid_card2_5'] = data['card2'].astype(str) + data['card5'].astype(str)\n    \n    # 2*4-4-1-(4) = 7\n    data['uid_card1_addr1'] = data['card1'].astype(str) + data['addr1'].astype(str)\n    data['uid_card2_addr1'] = data['card2'].astype(str) + data['addr1'].astype(str)\n    data['uid_card5_addr1'] = data['card5'].astype(str) + data['addr1'].astype(str)\n    \n    data['uid_card1_2_addr1'] = data['uid_card1_2'] + data['addr1'].astype(str)\n    data['uid_card1_5_addr1'] = data['uid_card1_5'] + data['addr1'].astype(str)\n    data['uid_card2_5_addr1'] = data['uid_card2_5'] + data['addr1'].astype(str)\n    \n    data['uid_card1_2_5_addr1'] = data['uid_card1_2_5'] + data['addr1'].astype(str)\n    \n    # 2*5-5-1-(4)-(7) = 15(X)\n    data['uid_card1_P'] = data['card1'].astype(str) + data['P_emaildomain'].astype(str) # week after count encode\n    data['uid_card2_P'] = data['card2'].astype(str) + data['P_emaildomain'].astype(str)\n    data['uid_card1_2_P'] = data['uid_card1_2'].astype(str) + data['P_emaildomain'].astype(str) # week after count encode\n    \n    after_cols = data.columns\n    \n    new_cols = after_cols.difference(former_cols)\n    return data, new_cols\n\ntrain, new_cols = make_uid(train)\ntest, _ = make_uid(test)\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# na的mapping可以再考慮要不要讓他自動補值\n# Count encoding for uid feature. \n# Explained in this kernel: https://www.kaggle.com/nroman/eda-for-cis-fraud-detection\n\n# 'uid_card1_P', 'uid_card1_2_P'需要被確認\n\nuid_cols = ['uid_card1_2', 'uid_card1_5', 'uid_card1_2_5', 'uid_card2_5', 'uid_card1_addr1', 'uid_card2_addr1', 'uid_card5_addr1', 'uid_card1_2_addr1',\\\n          'uid_card1_5_addr1', 'uid_card2_5_addr1', 'uid_card1_2_5_addr1', 'uid_card2_P', 'uid_card1_P', 'uid_card1_2_P']\n\nfor uid_col in uid_cols:\n    make_count_full(uid_col)\n    if plot:\n        distribution('%s_count_full'%uid_col)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndef make_card_features(data, target_cols, groupby_cols):\n    \n    for groupby_col in groupby_cols:\n        for target_col in target_cols:\n            data['VAL_MEAN_%s_BY_%s'%(target_col, groupby_col)] = data[target_col] / data.groupby([groupby_col])[target_col].transform('mean')\n            data['VAL_STD_%s_BY_%s'%(target_col, groupby_col)] = data[target_col] / data.groupby([groupby_col])[target_col].transform('std')\n            data['VAL_STD_%s_BY_%s'%(target_col, groupby_col)].loc[np.isinf(data['VAL_STD_%s_BY_%s'%(target_col, groupby_col)])] = np.nan\n            \n# TransactionAmt has bean done\ngroupby_cols = ['card1', 'card2', 'card3', 'card4', 'card5']\ntarget_cols = ['D15', 'id_02']\n\nfor df in [train, test]:\n    make_card_features(df, target_cols, groupby_cols)\n    \ndel df, groupby_cols, target_cols\ngc.collect()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## dist : dist1~2"},{"metadata":{"trusted":true},"cell_type":"code","source":"if plot:\n    for col in ['dist1', 'dist2']:\n        distribution(col, method='log1p')\n        Num_EDA(train, col)\n\n    #     print (col, '-'*30)\n    #     print (train[col].value_counts(dropna=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## emaildomain : P_emaildomain, R_emaildomain"},{"metadata":{"trusted":true},"cell_type":"code","source":"# email information features\n\ndef email_domain(df):\n    df[\"P_emaildomain\"] = df[\"P_emaildomain\"].fillna('Unknown')\n    df[\"P_country_emaildomain\"] = df[\"P_emaildomain\"].str.extract('(\\.de)|(\\.ds)|(\\.mx)|(\\.es)|(\\.fr)|(\\.uk)|(\\.jp)|(Unknown)').stack().droplevel(1).apply(lambda x:x.replace('.', ''))\n    df['P_type_emaildomain'] = df[\"P_emaildomain\"].str.split('.').apply(lambda x:np.nan if x == 'Unknown' else x[0])\n    df[\"P_domain_emaildomain\"] = df[\"P_emaildomain\"].str.extract('(\\.com)|(\\.net)|(\\.co)|(Unknown)').stack().droplevel(1).apply(lambda x:x.replace('.', ''))\n    df[\"P_len_emaildomain\"] = df[\"P_emaildomain\"].str.split(\".\").apply(lambda x: 0 if x == 'Unknown' else len(x))\n    \n    df[\"R_emaildomain\"] = df[\"R_emaildomain\"].fillna('Unknown')\n    df[\"R_country_emaildomain\"] = df[\"R_emaildomain\"].str.extract('(\\.de)|(\\.ds)|(\\.mx)|(\\.es)|(\\.fr)|(\\.uk)|(\\.jp)|(Unknown)').stack().droplevel(1).apply(lambda x:x.replace('.', ''))\n    df['R_type_emaildomain'] = df[\"R_emaildomain\"].str.split('.').apply(lambda x:np.nan if x == 'Unknown' else x[0])\n    df[\"R_domain_emaildomain\"] = df[\"R_emaildomain\"].str.extract('(\\.com)|(\\.net)|(\\.co)|(\\.Unknown)').stack().droplevel(1).apply(lambda x:x.replace('.', ''))\n    df[\"R_len_emaildomain\"] = df[\"R_emaildomain\"].str.split(\".\").apply(lambda x: 0 if x == 'Unknown' else len(x))\n    \n    df[\"same_eamildomain\"] = (df[\"P_emaildomain\"] == df[\"R_emaildomain\"]).astype(int)\n    df['is_es'] = (df['P_country_emaildomain'] == 'es').astype(int)\n        \nfor df in [train, test]:\n    df = email_domain(df)\n\nif plot:\n    for col in ['P_country_emaildomain', 'P_type_emaildomain', 'P_domain_emaildomain', 'P_len_emaildomain', 'same_eamildomain']:\n        Cat_EDA(train, col)\n    \none_hot_encode_list.extend(['P_country_emaildomain', 'P_domain_emaildomain'])\ntarget_encode_list.extend(['P_type_emaildomain'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if plot:\n    # check train and test data distribution\n    plt.figure(figsize=(12, 5))\n    v = train['P_type_emaildomain'].value_counts(normalize=True)\n    v.plot('bar')\n    test['P_type_emaildomain'].value_counts(normalize=True).loc[v.index].plot(color='r')\n    plt.xticks(rotation=90)\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## C : C1~14"},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in [train, test]:\n    df['C_mean'] = df[['C%s'%i for i in range(1, 15)]].mean(axis=1)\n    df['C_std'] = df[['C%s'%i for i in range(1, 15)]].std(axis=1)\n    \nif plot:\n    for col in ['C%s'%i for i in range(1, 15)] + ['C_mean', 'C_std']:\n        distribution(col, method='log1p')\n        \nfor c_col in ['C%s'%i for i in range(1, 15)]:\n    make_count_full(c_col)\n    if plot:\n        distribution('%s_count_full'%c_col)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\n\nn_pc = 4\nC_pca_encoder = Pipeline([('SimpleImputer',SimpleImputer()),\n                         ('normalize',StandardScaler()),\n                         ('pca',PCA(n_components=n_pc))]).fit(train[['C'+str(i+1) for i in range(14)]])\n\ndef C_pca(df):\n    temp = pd.DataFrame(C_pca_encoder.transform(df[['C'+str(i+1) for i in range(14)]]), \\\n                        index=df.index,\\\n                        columns=['C_pca'+str(i) for i in range(n_pc)])\n    df = pd.concat([df, temp], axis=1)\n    return df\n    \ntrain = C_pca(train)\ntest = C_pca(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ncol1 = ['card1','card2','card5','addr1']\ncol2 = ['C%s'%i for i in range(1, 15)]\n\nfor col_1 in col1:\n    for col_2 in col2:\n        temp_df = pd.concat([train[[col_1, col_2]], test[[col_1, col_2]]], ignore_index=True)\n        col_count0 = temp_df[temp_df[col_2] == 0].groupby(col_1)[col_2].count()\n        col_count1 = temp_df[temp_df[col_2] != 0].groupby(col_1)[col_2].count()\n        \n        for df in [train, test]:\n            df['VAL_RATIO_zero_nonzero_%s_BY_%s'%(col_2, col_1)] = df[col_1].map(col_count1) / (df[col_1].map(col_count0) + 0.01)\n\n        del temp_df\n        gc.collect()\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## D : D1~15"},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in [train, test]:\n    df['D_isna_num'] = df[['D%s'%i for i in range(1, 16)]].isna().sum(axis=1)\n    df['D_mean'] = df[['D%s'%i for i in range(1, 16)]].mean(axis=1)\n    \nif plot:\n    for col in ['D%s'%i for i in range(1, 16)]+['D_isna_num', 'D_mean']:\n        distribution(col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## M : M1~9"},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in [train, test]:\n    df['M_isna_num'] = df[['M%s'%i for i in range(1, 10)]].isna().sum(axis=1)\n    df[['M%s'%i for i in range(1, 10)]] = df[['M%s'%i for i in range(1, 10)]].fillna('Unknown')\n    df['is_M2'] = (df['M4'] == 'M2').astype(int)\n\nif plot:\n    for col in ['M%s'%i for i in range(1, 10)]:\n        Cat_EDA(train, col)\n        \n    distribution('M_isna_num')\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## V : V1~339"},{"metadata":{"trusted":true},"cell_type":"code","source":"### V : 1~11, 12~34, 35~52, 53~94, 95~106, 107~125, 126~130, 131~134, 135~137, 138~278, 279~305, 306~315, \\\n#    316~318, 319~321, 322~339\n\ndef v_variable(df):\n    df['v_isna_num'] = df[['V%s'%i for i in range(1, 339)]].isna().sum(axis=1)\n    \n    for i, j in zip([1, 12, 35, 53, 95, 107, 126, 131, 135, 138, 279, 306, 316, 319, 322], \\\n                    [11, 34, 52, 94, 106, 125, 130, 134, 137, 278, 305, 315, 318, 321, 339]):\n        df['v_%s_%s_isna_num'%(i, j)] = df[['V%s'%i for i in range(i, j+1)]].isna().sum(axis=1)\n    \n    df['V257_258_sum'] = df['V257'] + df['V258']\n    df['V257_258_equal'] = sum([train['V257'].fillna(-999) != train['V258'].fillna(-999)])\n    \nfor df in [train, test]:\n    df = v_variable(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## id : id_01~38"},{"metadata":{"trusted":true},"cell_type":"code","source":"def id_variable(df):\n    df['id_isna_num'] = df[['id_%s'%i if len(str(i))==2 else 'id_0%s'%i for i in range(1, 39)]].isna().sum(axis=1)\n    df['OS'] = df[\"id_30\"].str.split(' ', expand = True)[[0]]\n    df['good_browser'] = df['id_31'].astype(str).str.extract('(edge)|(google)|(ie)|(safari)|(nan)').any(axis = 'columns').astype(int)\n\nfor df in [train, test]:\n    df = id_variable(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Device : DeviceType, DeviceType"},{"metadata":{"trusted":true},"cell_type":"code","source":"popular_device_list = (pd.concat([train['DeviceInfo'],test['DeviceInfo']]).value_counts().iloc[:15]).index.values\ndef device(df):\n    df['popular_deviceinfo'] = df[\"DeviceInfo\"].isin(popular_device_list).astype(int)\n    df['good_deviceinfo'] = df[\"DeviceInfo\"].astype(str).str.extract('(Trident)|(iOS)|(MacOS)|(Windows)|(rv:[^(5\\d)(6\\d)])|(nan)').any(axis = 'columns').astype(int)\n\nfor df in [train, test]:\n    df[['DeviceInfo', 'DeviceType']] = df[['DeviceInfo', 'DeviceType']].fillna('Unknown')\n    df = device(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## others"},{"metadata":{"trusted":true},"cell_type":"code","source":"def same_tx_in_radius(df,radius = 5):\n    temp = df[['TransactionAmt']].copy()\n    for i in range(-radius,radius+1):\n        temp['shift_'+str(i)] = temp['TransactionAmt'] == temp['TransactionAmt'].shift(i)\n    df['same_TransactionAmt_in_radius'] = temp[['shift_'+str(i) for i in range(-radius,radius+1)]].sum(axis = 'columns')\n    \nfor df in [train, test]:\n    df = same_tx_in_radius(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# null number\ntrain['nulls'] = train.isnull().sum(axis=1)\ntest['nulls'] = test.isnull().sum(axis=1)\n\nif plot:\n    distribution('nulls')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ncols = ['card1', 'card2', 'card5', 'addr1', 'addr2', 'uid_card1_2', 'uid_card1_5', 'uid_card1_2_5', 'uid_card2_5',\\\n        'uid_card1_addr1', 'uid_card2_addr1', 'uid_card5_addr1', 'uid_card1_2_addr1', 'uid_card1_5_addr1',\\\n        'uid_card2_5_addr1', 'uid_card1_2_5_addr1', 'uid_card2_P', 'uid_card1_P', 'uid_card1_2_P']\n\ndef make_transactionAmt_groupby(columns):\n    remain_cols = columns + ['ProductCD', 'TransactionAmt']\n    all_transaction = pd.concat([train[remain_cols], test[remain_cols]], axis=0).reset_index()\n    all_transaction[columns] = all_transaction[columns].astype(str)\n    \n    for col in columns:\n        print (col, ' processing...')\n        two_groupby = all_transaction.groupby(['ProductCD', col])['TransactionAmt'].mean().reset_index()\n        two_groupby.rename({'TransactionAmt':'ProductCD_%s_TransactionAmt'%col}, axis=1, inplace=True)\n\n        all_transaction = all_transaction.merge(two_groupby, on=['ProductCD', col])\n        all_transaction['ProductCD_%s_TransactionAmt_MAPE'%col] = abs(all_transaction['TransactionAmt'] - \\\n            all_transaction['ProductCD_%s_TransactionAmt'%col])/all_transaction['ProductCD_%s_TransactionAmt'%col]\n    \n    return pd.concat([train, all_transaction.set_index('TransactionID').loc[train.index].drop(cols + ['ProductCD', 'TransactionAmt'], axis=1)], axis=1), \\\n        pd.concat([test, all_transaction.set_index('TransactionID').loc[test.index].drop(cols + ['ProductCD', 'TransactionAmt'], axis=1)], axis=1)\n    \n\ntrain, test = make_transactionAmt_groupby(cols)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# col_del = []\n# for i in range(339):\n#     col = \"V\" + str(i+1)\n#     s = train[col].fillna(0).map(lambda x:0 if x%1 == 0 else 1).sum()\n#     if s > 100:\n#         print(col,s)\n#         col_del.append(col)\n# #         del test_transaction[col],train_transaction[col]\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Make train & test data and save memory"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop target\ny_train = train['isFraud']\nX_train = train.drop(drop_cols + ['isFraud'], axis=1)\n\nX_test = test.drop(drop_cols, axis=1).copy()\n\ndel train\ndel test\n\n\n# From kernel https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n# WARNING! THIS CAN DAMAGE THE DATA \n\nX_train = reduce_mem_usage(X_train)\nX_test = reduce_mem_usage(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"# One Hot Encoding\nprint ('one-hot encoding...')\nfor df in [X_train, X_test]:\n    temp = pd.DataFrame({})\n    for O in one_hot_encode_list:\n        temp = pd.concat([temp, pd.get_dummies(df[O], prefix=O)], axis=1)\n    \n    df = pd.concat([df, temp], axis=1)\n    df.drop(one_hot_encode_list, axis=1, inplace=True)\n\n# Count Encoding\nprint ('count encoding...')\nfor C in count_encode_list:\n    temp = X_train[C].value_counts()\n    X_train[C] = X_train[C].map(temp)\n    X_test[C] = X_test[C].map(temp)\n\n# Target Encoding\nprint ('target encoding...')\nfor T in target_encode_list:\n    temp = pd.concat([X_train, y_train], axis=1)\n    temp = temp.groupby(T)[col_target].mean()\n    \n    X_train[T] = X_train[T].map(temp)\n    X_test[T] = X_test[T].map(temp)\n\n# Label Encoding\nprint ('label encoding...')\nfor L in set(list(X_train.select_dtypes('category').columns)+list(X_test.select_dtypes('category').columns)):\n    lbl = preprocessing.LabelEncoder()\n    lbl.fit(np.array(list(X_train[L].values) + list(X_test[L].values)).astype(str))\n    X_train[L] = lbl.transform(list(X_train[L].values.astype(str)))\n    X_test[L] = lbl.transform(list(X_test[L].values.astype(str)))\n        \ndel temp, C, T, L, lbl\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### drop columns from feature importance selection of LGBM in local test"},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop_cols_FI = []\n\ndrop_cols_FI = ['V122', 'V89', 'id_27', 'V325', 'V196', 'v_95_106_isna_num', 'V252', 'id_22', 'V297', 'V299', \\\n                'V191', 'V334', 'v_126_130_isna_num', 'V113', 'M1', 'V240', 'V138', 'V88', 'id_35', 'V104', \\\n                'V117', 'V302', 'V84', 'id_28', 'V119', 'V241', 'V8', 'V322', 'V330', 'V167', 'id_24', 'V50', \\\n                'V181', 'V305', 'V14', 'V1', 'V118', 'v_1_11_isna_num', 'v_53_94_isna_num', 'V247', \\\n                'V257_258_equal', 'v_131_134_isna_num', 'V27', 'V106', 'id_29', 'v_135_137_isna_num', 'V142', \\\n                'V116', 'V153', 'V9', \\\n#                 'V21', 'id_12', 'V110', 'V114', 'v_138_278_isna_num', 'V269', 'addr2_count_full', 'V121', \\\n#                 'is_es','addr2', 'V28', 'v_322_339_isna_num', 'V120', 'V107', 'v_306_315_isna_num', \\\n#                 'v_279_305_isna_num', 'V68', 'v_35_52_isna_num', 'v_12_34_isna_num', 'V41', 'is_code_C', \\\n#                 'v_316_318_isna_num', 'V65', 'v_319_321_isna_num', 'v_107_125_isna_num'\n               ]\n\nX_train.drop(drop_cols_FI, axis=1, inplace=True)\nX_test.drop(drop_cols_FI, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = reduce_mem_usage(X_train)\nX_test = reduce_mem_usage(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Set debug = True to see validation metric**"},{"metadata":{"trusted":true},"cell_type":"code","source":"debug = False\nif debug:\n    split_pos = X_train.shape[0]*4//5\n    y_test = y_train.iloc[split_pos:]\n    y_train = y_train.iloc[:split_pos]\n    X_test = X_train.iloc[split_pos:,:]\n    X_train = X_train.iloc[:split_pos,:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Xgboost"},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# from sklearn.metrics import roc_auc_score\n# folds = 6\n# gkf = GroupKFold(n_splits = folds)\n# split_groups = X_train['DT_M']\n\n# xgb_y_preds = np.zeros(X_test.shape[0])\n# count = 0\n# for tr_idx, val_idx in gkf.split(X_train, y_train, groups=split_groups):\n#     count+=1\n#     if count != 1:\n#         print ('fold %s ------------------------------------'%count)\n        \n#         clf = xgb.XGBClassifier(\n#             n_estimators=10000,\n#             max_depth=9,\n#             learning_rate=0.01,\n#             subsample=0.9,\n#             colsample_bytree=0.9,\n#             tree_method='gpu_hist'\n#         )\n\n#         X_tr = X_train.iloc[tr_idx, :]\n#         y_tr = y_train.iloc[tr_idx]\n        \n#         X_val = X_train.iloc[val_idx, :]\n#         y_val = y_train.iloc[val_idx]\n        \n#         clf.fit(X_tr, y_tr, eval_set=[(X_tr, y_tr), (X_val, y_val)], early_stopping_rounds=50)\n#         del X_tr, X_val\n#         xgb_y_preds+= clf.predict_proba(X_test)[:,1] / folds\n#         if debug:    \n#             print(\"debug:\",roc_auc_score(y_test, clf.predict_proba(X_test)[:,1] / folds)) \n#         del clf\n#         gc.collect()\n    \n\n# if debug:    \n#     print(\"debug:\",roc_auc_score(y_test, xgb_y_preds))  \n\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# if not debug:   \n#     sample_submission['isFraud'] = xgb_y_preds\n#     sample_submission.to_csv('xgb_y_preds.csv')\n\n#     FileLink('xgb_y_preds.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Catboost"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import catboost as cb\n# from catboost import CatBoostClassifier,Pool\n\n# cate = one_hot_encode_list+count_encode_list+target_encode_list+list(set(list(X_train.select_dtypes('category').columns)+list(X_test.select_dtypes('category').columns)))\n\n# print(cate)\n# verbose_eval = 500\n# num_rounds = 10000\n\n# folds = 6\n# gkf = GroupKFold(n_splits = folds)\n# split_groups = X_train['DT_M']\n\n# feature_importance_df = pd.DataFrame()\n\n# catb_y_preds = np.zeros(X_test.shape[0])\n# count = 0\n# for tr_idx, val_idx in gkf.split(X_train, y_train, groups=split_groups):\n#     count+=1\n#     if count != 1:\n#         print ('fold %s ------------------------------------'%count)\n        \n#         X_tr = X_train.iloc[tr_idx, :].fillna(-1)\n#         y_tr = y_train.iloc[tr_idx]\n\n#         model=cb.CatBoostClassifier(iterations=num_rounds,depth=14,learning_rate=0.04,loss_function='Logloss',eval_metric='AUC'\n#                                     )#,task_type = \"GPU\"\n#         if debug:\n#             model.fit(X_tr,y_tr,cat_features=cate,verbose_eval = verbose_eval)\n#         else:\n#             model.fit(X_tr,y_tr,cat_features=cate,verbose_eval = verbose_eval)\n\n\n#         del X_tr\n#         catb_y_preds+= model.predict_proba(X_test.fillna(-1))[:,1] / folds\n\n\n#         if debug:    \n#             print(\"debug:\",roc_auc_score(y_test, model.predict_proba(X_test.fillna(-1))[:,1] / folds))  \n        \n#         gc.collect()\n            \n# if debug:    \n#     print(\"debug:\",roc_auc_score(y_test, catb_y_preds))  \n\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# if debug:    \n#     print(\"debug:\",roc_auc_score(y_test, y_preds))\n#     print(\"debug:\",roc_auc_score(y_test, y_preds2))\n#     print(\"debug:\",roc_auc_score(y_test, y_preds3))  \n#     print(\"debug:\",roc_auc_score(y_test, (y_preds + y_preds3)*0.5))  \n#     print(\"debug:\",roc_auc_score(y_test, (y_preds + y_preds2 + y_preds3*0.5)*0.33))\n#     print(\"debug:\",roc_auc_score(y_test, (y_preds11*0.5 + y_preds*0.5 + y_preds2 + y_preds3*0.5)*0.33))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# if not debug:   \n#     sample_submission['isFraud'] = catb_y_preds\n#     sample_submission.to_csv('catb_y_preds.csv')\n\n#     FileLink('catb_y_preds.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}