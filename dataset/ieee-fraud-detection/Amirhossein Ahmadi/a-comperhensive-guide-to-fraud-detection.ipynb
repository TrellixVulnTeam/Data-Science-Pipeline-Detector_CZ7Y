{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"text-align:center; color:DarkBlue\">Fraud Detection</h1>\n\n<img src=\"https://upload.wikimedia.org/wikipedia/en/thumb/2/21/IEEE_logo.svg/500px-IEEE_logo.svg.png\" style=\"display:block;margin-left:25%;margin-right:auto;width:50%;\"/>\n\n<div style=\"margin-left: 10px\">\n<a style=\"cursor:pointer\">1. Intorduction</a><br>\n<a style=\"cursor:pointer\">2. IEEE Fraud Detection</a><br>\n<a style=\"cursor:pointer\">3. Import Libraries</a><br>\n<a style=\"cursor:pointer\">4. Loading Files</a><br>\n<a style=\"cursor:pointer\">5. Reduce Memory Size</a><br>\n<a style=\"cursor:pointer\">6. Exploring Data</a><br>\n    <a style=\"cursor:pointer\">&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; <span>&#8226;</span>&emsp;Visualization</a><br>\n    <a style=\"cursor:pointer\">&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<span>&#8226;</span>&emsp;Delve into TransactionDT</a><br>\n    <a style=\"cursor:pointer\">&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<span>&#8226;</span>&emsp;Check Device Info</a><br>\n    <a style=\"cursor:pointer\">&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<span>&#8226;</span>&emsp;Check Email Address</a><br>\n<a style=\"cursor:pointer\">7. Feature Engineering</a><br>\n<a style=\"cursor:pointer\">8. Prepare Data for Modeling</a><br>\n    <a style=\"cursor:pointer\">&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<span>&#8226;</span>&emsp;Fill Nan Values</a><br>\n    <a style=\"cursor:pointer\">&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<span>&#8226;</span>&emsp;Label Encoding</a><br>\n<a style=\"cursor:pointer\">9. Create Test Set and Train Set</a><br>\n<a style=\"cursor:pointer\">10. Finding Best Model</a>\n</div>\n\n<br>\n\nThanks to this kernels (Refrences) : \n* [feature-engineering-lightgbm-corrected](https://www.kaggle.com/davidcairuz/feature-engineering-lightgbm-corrected\")\n* [eda-and-models](https://www.kaggle.com/artgor/eda-and-models)\n* [feature-engineering-lightgbm-corrected](https://www.kaggle.com/davidcairuz/feature-engineering-lightgbm-corrected)\n* [reducing-memory-size-for-ieee](https://www.kaggle.com/mjbahmani/reducing-memory-size-for-ieee)\n* [day-and-time-powerful-predictive-feature](https://www.kaggle.com/fchmiel/day-and-time-powerful-predictive-feature)\n\n<br>\n\n## Intorduction\nIn law, `fraud` is intentional deception to secure unfair or unlawful gain, or to deprive a victim of a legal right. Fraud can violate civil law (i.e., a fraud victim may sue the fraud perpetrator to avoid the fraud or recover monetary compensation), a criminal law (i.e., a fraud perpetrator may be prosecuted and imprisoned by governmental authorities), or it may cause no loss of money, property or legal right but still be an element of another civil or criminal wrong.The purpose of fraud may be monetary gain or other benefits, for example by obtaining a passport, travel document, or driver's license, or mortgage fraud, where the perpetrator may attempt to qualify for a mortgage by way of false statements. [Fraud](https://en.wikipedia.org/wiki/Fraud)\n<br>\n\n`Fraud detection` is a set of activities undertaken to prevent money or property from being obtained through false pretenses. Fraud detection is applied to many industries such as banking or insurance. In banking, fraud may include forging checks or using stolen credit cards. Other forms of fraud may involve exaggerating losses or causing an accident with the sole intent for the payout.<br> [Data analysis techniques for fraud detection](https://en.wikipedia.org/wiki/Data_analysistechniques_for_fraud_detection)"},{"metadata":{},"cell_type":"markdown","source":"## IEEE Fraud Detection\n<div style=\"text-align: justify\">In this competition, we will benchmark `machine learning models` on a challenging large-scale dataset. The data comes from Vesta's real-world e-commerce transactions and contains a wide range of features from device type to product features. We also have the opportunity to create new features to improve your results. [Vesta Coopration](https://trustvesta.com/)</div>\n<br>\n\n<img src=\"http://news.mit.edu/sites/mit.edu.newsoffice/files/styles/news_article_image_top_slideshow/public/images/2018/MIT-Fraud-Detection-PRESS_0.jpg?itok=laiU-5nR\" style=\"display:block;margin-left:25%;margin-right:auto;width:50%;\"/>"},{"metadata":{},"cell_type":"markdown","source":"We have to use classification method on this dataset and find out which of these instances are seem to be fraud; actually, we should find the probability of being fraud for each of these instances.\n<br><br>\nAs you can see the dataset, it is too big and working on that may take a lot of time, therefore we should reduce the size of data either by using PCA(Principle Component Analysis) or by downcasting integer and float columns.\n<br>\n<br>\nThe submission is measured with calculating  `AUC - ROC Curve`\n<br><br>\n<b>What is AUC - ROC Curve?</b><br>\nAUC - ROC curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s. By analogy, Higher the AUC, better the model is at distinguishing between patients with disease and no disease.\nThe ROC curve is plotted with TPR against the FPR where TPR is on y-axis and FPR is on the x-axis.<br>[Understanding AUC - ROC Curve](https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5)\n<br>\n\n<img src=\"https://miro.medium.com/max/722/1*pk05QGzoWhCgRiiFbz-oKQ.png\" style=\"display:block;margin-left:25%;margin-right:auto;width:40%;\"/>"},{"metadata":{},"cell_type":"markdown","source":"## Import Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport gc\nfrom itertools import cycle, islice\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading Files\n\nFirst we should check what files we have in input directory.\n\nWe have four files two of which are train and the other two are test files."},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then we should load csv datasets in pandas' dataframes.\n\n`TransactionID` is the index of all files, so we use index_col to use this column when loading files."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_transaction = pd.read_csv('/kaggle/input/ieee-fraud-detection/train_transaction.csv', index_col='TransactionID')\ntest_transaction = pd.read_csv('/kaggle/input/ieee-fraud-detection/test_transaction.csv', index_col='TransactionID')\ntrain_identity = pd.read_csv('/kaggle/input/ieee-fraud-detection/train_identity.csv', index_col='TransactionID')\ntest_identity = pd.read_csv('/kaggle/input/ieee-fraud-detection/test_identity.csv', index_col='TransactionID')\nsample_submission = pd.read_csv('/kaggle/input/ieee-fraud-detection/sample_submission.csv', index_col='TransactionID')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we should `join` train and test dataset.\n\nWe use merge method."},{"metadata":{"trusted":false},"cell_type":"code","source":"train = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reduce Memory Size\n\nAs mentioned above, this dataset is feeding on memory, so we should use anything to reduce memory usage.\n\nOne we to dimnish memory usage is calling `garbage collector` just after we don't need a dataframe.\n\nTo dimnish `memory usage`, we delete previous dataframes and call garbage collector to collet this unrefrence data."},{"metadata":{"trusted":false},"cell_type":"code","source":"del train_transaction, train_identity\ndel test_transaction, test_identity\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another way which is told earlier is to `downcast` float and integer columns.\n\nThis is a function that downcast the float columns"},{"metadata":{"trusted":false},"cell_type":"code","source":"def downcast_df_float_columns(df):\n    list_of_columns = list(df.select_dtypes(include=[\"float64\"]).columns)\n        \n    if len(list_of_columns)>=1:\n        max_string_length = max([len(col) for col in list_of_columns])\n        print(\"downcasting float for:\", list_of_columns, \"\\n\")\n        \n        for col in list_of_columns:\n            df[col] = pd.to_numeric(df[col], downcast=\"float\")\n    else:\n        print(\"no columns to downcast\")\n    gc.collect()\n    print(\"done\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is a function that downcast the integer columns"},{"metadata":{"trusted":false},"cell_type":"code","source":"def downcast_df_int_columns(df):\n    list_of_columns = list(df.select_dtypes(include=[\"int32\", \"int64\"]).columns)\n        \n    if len(list_of_columns)>=1:\n        max_string_length = max([len(col) for col in list_of_columns])\n        print(\"downcasting integers for:\", list_of_columns, \"\\n\")\n        \n        for col in list_of_columns:\n            df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n    else:\n        print(\"no columns to downcast\")\n    gc.collect()\n    print(\"done\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploring Data"},{"metadata":{"trusted":false},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train dataset has 399 float, 3 int and 31 object(category) columns."},{"metadata":{"trusted":false},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.bar(['train', 'test'], [len(train), len(test)], width=0.2, color='b')\nplt.title(\"Number of train set and test set instances \")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are more columns with at least one null value in train set than in test set"},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.bar(['train', 'test'], [train.isnull().any().sum(), test.isnull().any().sum()], width=0.2, color='g')\nplt.title(\"Number of column with null values\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Find missing values in train and test set\n\nThese are columns with the most `null instances` in both train set and test set"},{"metadata":{"trusted":false},"cell_type":"code","source":"train_missing_values = train.isnull().sum().sort_values(ascending=False) / len(train)\ntest_missing_values = test.isnull().sum().sort_values(ascending=False) / len(test)\n\nfig, axes = plt.subplots(2, 1, figsize=(12, 8))\nsns.barplot(list(train_missing_values.keys()[:10]), train_missing_values[:10], ax=axes[0])\nsns.barplot(list(test_missing_values.keys()[:10]), test_missing_values[:10], ax=axes[1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Show the percentage of fraud and not-fraud instances\n\nWe can see up to 97% of instances are not fruad and only 3% of data are labeled fraud; as a result, we ought to take care of our model not being `overfitted`"},{"metadata":{"trusted":false},"cell_type":"code","source":"def show_values_on_bars(axs):\n    def _show_on_single_plot(ax):        \n        for p in ax.patches:\n            _x = p.get_x() + p.get_width() / 2\n            _y = p.get_y() + p.get_height()\n            value = '{:.2f}'.format(p.get_height())\n            ax.text(_x, _y, value, ha=\"center\") \n\n    if isinstance(axs, np.ndarray):\n        for idx, ax in np.ndenumerate(axs):\n            _show_on_single_plot(ax)\n    else:\n        _show_on_single_plot(axs)\n\nplt.figure(figsize=(5, 4))\nax = sns.barplot([\"fraud\", \"not fraud\"],\n            [len(train[train.isFraud == 1])/len(train),\n             len(train[train.isFraud == 0])/len(train)])\nshow_values_on_bars(ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Delve into TransactionDT\n\nAs we can see in **[this](https://www.kaggle.com/c/ieee-fraud-detection/discussion/100071#latest-577632)** discussion, probably TransactionDT column is in seconds.\n\nWe can add day's hour to out dataset. **[Day and Time - powerful predictive feature?)](https://www.kaggle.com/fchmiel/day-and-time-powerful-predictive-feature)**"},{"metadata":{"trusted":false},"cell_type":"code","source":"train[\"hour\"] = np.floor(train[\"TransactionDT\"] / 3600) % 24\ntest[\"hour\"] = np.floor(train[\"TransactionDT\"] / 3600) % 24","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can `visualize` to see if there is any relationship between day's hour and fraud\n\nIt's showing that in which hours, the rate of fraud is `greater`"},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.plot(train.groupby('hour').mean()['isFraud'], color='r')\nax = plt.gca()\nax2 = ax.twinx()\n_ = ax2.hist(train['hour'], alpha=0.3, bins=24)\nax.set_xlabel('Encoded hour')\nax.set_ylabel('Fraction of fraudulent transactions')\n\nax2.set_ylabel('Number of transactions')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check Device Info"},{"metadata":{},"cell_type":"markdown","source":"Check which type of devices has been used"},{"metadata":{"trusted":false},"cell_type":"code","source":"train[\"DeviceType\"].value_counts(dropna=False).plot.bar()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see Windows, iOS and MacOS are the most popular operating systems"},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(8, 8))\nsns.barplot(train[\"DeviceInfo\"].value_counts(dropna=False)[:15], \n            train[\"DeviceInfo\"].value_counts(dropna=False).keys()[:15])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check Email Address"},{"metadata":{"trusted":false},"cell_type":"code","source":"my_colors = list(islice(cycle(['b', 'r', 'g', 'y', 'k']), None, len(train.P_emaildomain.value_counts())))\ntrain.P_emaildomain.value_counts().plot.bar(figsize=(20, 10), color=my_colors)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that more than 40% of instnces which have `P_emaildomain` equal to `protonmail.com` are Fraud and this shows that we should check if P_emaildomain or R_emaildomain is equal to protonmail.com or not"},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(6, 6))\nplt.pie([np.sum(train[(train['P_emaildomain'] == 'protonmail.com')].isFraud.values),\n                                 len(train[(train['P_emaildomain'] == 'protonmail.com')].isFraud.values) - \n                                 np.sum(train[(train['P_emaildomain'] == 'protonmail.com')].isFraud.values)],\n        labels=['isFraud', 'notFraud'], autopct='%1.1f%%')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train['is_proton_mail'] = (train['P_emaildomain'] == 'protonmail.com') | (train['R_emaildomain']  == 'protonmail.com')\ntest['is_proton_mail'] = (test['P_emaildomain'] == 'protonmail.com') | (test['R_emaildomain']  == 'protonmail.com')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Changing label of emails with their address from P_emaildomain and R_emaildomain columns"},{"metadata":{"trusted":false},"cell_type":"code","source":"emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', 'scranton.edu': 'other', 'optonline.net': 'other',\n          'hotmail.co.uk': 'microsoft', 'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',\n          'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', 'aim.com': 'aol', 'hotmail.de': 'microsoft',\n          'centurylink.net': 'centurylink', 'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', \n          'gmx.de': 'other', 'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', 'protonmail.com': 'other',\n          'hotmail.fr': 'microsoft', 'windstream.net': 'other', 'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo',\n          'yahoo.de': 'yahoo', 'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n          'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft', 'verizon.net': 'yahoo',\n          'msn.com': 'microsoft', 'q.com': 'centurylink', 'prodigy.net.mx': 'att', 'frontier.com': 'yahoo',\n          'anonymous.com': 'other', 'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo',\n          'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', 'bellsouth.net': 'other',\n          'embarqmail.com': 'centurylink', 'cableone.net': 'other', 'hotmail.es': 'microsoft', 'mac.com': 'apple',\n          'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', 'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other',\n          'cox.net': 'other', 'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\nus_emails = ['gmail', 'net', 'edu']\nfor c in ['P_emaildomain', 'R_emaildomain']:\n    train[c + '_bin'] = train[c].map(emails)\n    test[c + '_bin'] = test[c].map(emails)\n    \n    train[c + '_suffix'] = train[c].map(lambda x: str(x).split('.')[-1])\n    test[c + '_suffix'] = test[c].map(lambda x: str(x).split('.')[-1])\n    \n    train[c + '_suffix'] = train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n    test[c + '_suffix'] = test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n\nprint('done')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Change Browser Label\n\nWe can check if browser is the lastest version of browser or not"},{"metadata":{"trusted":false},"cell_type":"code","source":"a = np.zeros(train.shape[0])\ntrain[\"lastest_browser\"] = a\na = np.zeros(test.shape[0])\ntest[\"lastest_browser\"] = a\ndef setbrowser(df):\n    df.loc[df[\"id_31\"]==\"samsung browser 7.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"opera 53.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"mobile safari 10.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"google search application 49.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"firefox 60.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"edge 17.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 69.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 67.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 63.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 63.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0 for ios\",'lastest_browser']=1\n    return df\ntrain=setbrowser(train)\ntest=setbrowser(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see 10.7% of transactions which are done by users with lastest browser are fraud"},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(6, 6))\nplt.pie([np.sum(train[(train['lastest_browser'] == True)].isFraud.values),\n                                 len(train[(train['lastest_browser'] == True)].isFraud.values) - \n                                 np.sum(train[(train['lastest_browser'] == True)].isFraud.values)],\n        labels=['isFraud', 'notFraud'], autopct='%1.1f%%', colors=['y', 'g'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering\n\nFirst we are going to drop columns with more than 80 precent of null values in both train and test set"},{"metadata":{"trusted":false},"cell_type":"code","source":"train_missing_values = [str(x) for x in train_missing_values[train_missing_values > 0.80].keys()]\ntest_missing_values = [str(x) for x in test_missing_values[test_missing_values > 0.80].keys()]\n\ndropped_columns = train_missing_values + test_missing_values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then we should drop columns that have more than 90 precent of a same value.\n\nWe must notice that the label column (isFraud) also has more than 90% of the same value, therefore we have to remove it from dropped columns."},{"metadata":{"trusted":false},"cell_type":"code","source":"dropped_columns = dropped_columns + [col for col in train.columns if train[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\ndropped_columns = dropped_columns + [col for col in test.columns if test[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\ndropped_columns.remove('isFraud')\n\ntrain.drop(dropped_columns, axis=1, inplace=True)\ntest.drop(dropped_columns, axis=1, inplace=True)\n\nlen(dropped_columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Adding some feature to make prediction better"},{"metadata":{"trusted":false},"cell_type":"code","source":"train['TransactionAmt_to_mean_card1'] = train['TransactionAmt'] / train.groupby(['card1'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_mean_card4'] = train['TransactionAmt'] / train.groupby(['card4'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_std_card1'] = train['TransactionAmt'] / train.groupby(['card1'])['TransactionAmt'].transform('std')\ntrain['TransactionAmt_to_std_card4'] = train['TransactionAmt'] / train.groupby(['card4'])['TransactionAmt'].transform('std')\n\ntest['TransactionAmt_to_mean_card1'] = test['TransactionAmt'] / test.groupby(['card1'])['TransactionAmt'].transform('mean')\ntest['TransactionAmt_to_mean_card4'] = test['TransactionAmt'] / test.groupby(['card4'])['TransactionAmt'].transform('mean')\ntest['TransactionAmt_to_std_card1'] = test['TransactionAmt'] / test.groupby(['card1'])['TransactionAmt'].transform('std')\ntest['TransactionAmt_to_std_card4'] = test['TransactionAmt'] / test.groupby(['card4'])['TransactionAmt'].transform('std')\n\ntrain['id_02_to_mean_card1'] = train['id_02'] / train.groupby(['card1'])['id_02'].transform('mean')\ntrain['id_02_to_mean_card4'] = train['id_02'] / train.groupby(['card4'])['id_02'].transform('mean')\ntrain['id_02_to_std_card1'] = train['id_02'] / train.groupby(['card1'])['id_02'].transform('std')\ntrain['id_02_to_std_card4'] = train['id_02'] / train.groupby(['card4'])['id_02'].transform('std')\n\ntest['id_02_to_mean_card1'] = test['id_02'] / test.groupby(['card1'])['id_02'].transform('mean')\ntest['id_02_to_mean_card4'] = test['id_02'] / test.groupby(['card4'])['id_02'].transform('mean')\ntest['id_02_to_std_card1'] = test['id_02'] / test.groupby(['card1'])['id_02'].transform('std')\ntest['id_02_to_std_card4'] = test['id_02'] / test.groupby(['card4'])['id_02'].transform('std')\n\ntrain['D15_to_mean_card1'] = train['D15'] / train.groupby(['card1'])['D15'].transform('mean')\ntrain['D15_to_mean_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('mean')\ntrain['D15_to_std_card1'] = train['D15'] / train.groupby(['card1'])['D15'].transform('std')\ntrain['D15_to_std_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('std')\n\ntest['D15_to_mean_card1'] = test['D15'] / test.groupby(['card1'])['D15'].transform('mean')\ntest['D15_to_mean_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('mean')\ntest['D15_to_std_card1'] = test['D15'] / test.groupby(['card1'])['D15'].transform('std')\ntest['D15_to_std_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('std')\n\ntrain['D15_to_mean_addr1'] = train['D15'] / train.groupby(['addr1'])['D15'].transform('mean')\ntrain['D15_to_mean_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('mean')\ntrain['D15_to_std_addr1'] = train['D15'] / train.groupby(['addr1'])['D15'].transform('std')\ntrain['D15_to_std_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('std')\n\ntest['D15_to_mean_addr1'] = test['D15'] / test.groupby(['addr1'])['D15'].transform('mean')\ntest['D15_to_mean_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('mean')\ntest['D15_to_std_addr1'] = test['D15'] / test.groupby(['addr1'])['D15'].transform('std')\ntest['D15_to_std_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('std')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train['uid'] = train['card1'].astype(str)+'_'+train['card2'].astype(str)\ntest['uid'] = test['card1'].astype(str)+'_'+test['card2'].astype(str)\n\ntrain['uid2'] = train['uid'].astype(str)+'_'+train['card3'].astype(str)+'_'+train['card5'].astype(str)\ntest['uid2'] = test['uid'].astype(str)+'_'+test['card3'].astype(str)+'_'+test['card5'].astype(str)\n\ntrain['uid3'] = train['uid2'].astype(str)+'_'+train['addr1'].astype(str)+'_'+train['addr2'].astype(str)\ntest['uid3'] = test['uid2'].astype(str)+'_'+test['addr1'].astype(str)+'_'+test['addr2'].astype(str)\n\ntrain['TransactionAmt_check'] = np.where(train['TransactionAmt'].isin(test['TransactionAmt']), 1, 0)\ntest['TransactionAmt_check']  = np.where(test['TransactionAmt'].isin(train['TransactionAmt']), 1, 0)\n\ntrain['TransactionAmt'] = np.log1p(train['TransactionAmt'])\ntest['TransactionAmt'] = np.log1p(test['TransactionAmt'])    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"for feature in ['id_36']:\n    train[feature + '_count_full'] = train[feature].map(pd.concat([train[feature], test[feature]], ignore_index=True).value_counts(dropna=False))\n    test[feature + '_count_full'] = test[feature].map(pd.concat([train[feature], test[feature]], ignore_index=True).value_counts(dropna=False))\n        \nfor feature in ['id_01', 'id_31', 'id_35', 'id_36']:\n    train[feature + '_count_dist'] = train[feature].map(train[feature].value_counts(dropna=False))\n    test[feature + '_count_dist'] = test[feature].map(test[feature].value_counts(dropna=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"for col in ['card1']: \n    valid_card = pd.concat([train[[col]], test[[col]]])\n    valid_card = valid_card[col].value_counts()\n    valid_card = valid_card[valid_card>2]\n    valid_card = list(valid_card.index)\n\n    train[col] = np.where(train[col].isin(test[col]), train[col], np.nan)\n    test[col]  = np.where(test[col].isin(train[col]), test[col], np.nan)\n\n    train[col] = np.where(train[col].isin(valid_card), train[col], np.nan)\n    test[col]  = np.where(test[col].isin(valid_card), test[col], np.nan)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prepare Data for Modeling\n\n### Fill Nan Values\n\nFirst we should fill null values for both categorical and numerical values"},{"metadata":{"trusted":false},"cell_type":"code","source":"numerical_columns = list(test.select_dtypes(exclude=['object']).columns)\n\ntrain[numerical_columns] = train[numerical_columns].fillna(train[numerical_columns].median())\ntest[numerical_columns] = test[numerical_columns].fillna(train[numerical_columns].median())\nprint(\"filling numerical columns null values done\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we find out categorical columns"},{"metadata":{"trusted":false},"cell_type":"code","source":"categorical_columns = list(filter(lambda x: x not in numerical_columns, list(test.columns)))\ncategorical_columns[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"then, fill missing values in categorical columns"},{"metadata":{"trusted":false},"cell_type":"code","source":"train[categorical_columns] = train[categorical_columns].fillna(train[categorical_columns].mode())\ntest[categorical_columns] = test[categorical_columns].fillna(train[categorical_columns].mode())\nprint(\"filling numerical columns null values done\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Label Encoding"},{"metadata":{},"cell_type":"markdown","source":"Encode categorical columns"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nfor col in categorical_columns:\n    le = LabelEncoder()\n    le.fit(list(train[col].astype(str).values) + list(test[col].astype(str).values))\n    train[col] = le.transform(list(train[col].astype(str).values))\n    test[col] = le.transform(list(test[col].astype(str).values))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create Test Set and Train Set"},{"metadata":{},"cell_type":"markdown","source":"Because of using cross validation, we use this step later when we want to find best model.\n\nNow, we have to remove isFraud column from daraframe"},{"metadata":{"trusted":false},"cell_type":"code","source":"labels = train[\"isFraud\"]\ntrain.drop([\"isFraud\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train, y_train = train, labels\ndel train, labels\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Finding Best Model\n\nuse sample_submission file for final submission"},{"metadata":{"trusted":false},"cell_type":"code","source":"lgb_submission=sample_submission.copy()\nlgb_submission['isFraud'] = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We use cross validation with 5 folds"},{"metadata":{"trusted":false},"cell_type":"code","source":"n_fold = 5\nfolds = KFold(n_fold)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using LGBM for finding best model"},{"metadata":{"trusted":false},"cell_type":"code","source":"for fold_n, (train_index, valid_index) in enumerate(folds.split(X_train)):\n    print(fold_n)\n    \n    X_train_, X_valid = X_train.iloc[train_index], X_train.iloc[valid_index]\n    y_train_, y_valid = y_train.iloc[train_index], y_train.iloc[valid_index]\n    dtrain = lgb.Dataset(X_train, label=y_train)\n    dvalid = lgb.Dataset(X_valid, label=y_valid)\n    \n    lgbclf = lgb.LGBMClassifier(\n            num_leaves= 512,\n            n_estimators=512,\n            max_depth=9,\n            learning_rate=0.064,\n            subsample=0.85,\n            colsample_bytree=0.85,\n            boosting_type= \"gbdt\",\n            reg_alpha=0.3,\n            reg_lamdba=0.243\n    )\n    \n    X_train_, X_valid = X_train.iloc[train_index], X_train.iloc[valid_index]\n    y_train_, y_valid = y_train.iloc[train_index], y_train.iloc[valid_index]\n    lgbclf.fit(X_train_,y_train_)\n    \n    del X_train_,y_train_\n    print('finish train')\n    pred=lgbclf.predict_proba(test)[:,1]\n    val=lgbclf.predict_proba(X_valid)[:,1]\n    print('finish pred')\n    del lgbclf, X_valid\n    print('ROC accuracy: {}'.format(roc_auc_score(y_valid, val)))\n    del val,y_valid\n    lgb_submission['isFraud'] = lgb_submission['isFraud']+ pred/n_fold\n    del pred\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create submission file with name `prediction.csv`"},{"metadata":{"trusted":false},"cell_type":"code","source":"lgb_submission.insert(0, \"TransactionID\", np.arange(3663549, 3663549 + 506691))\nlgb_submission.to_csv('prediction.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}