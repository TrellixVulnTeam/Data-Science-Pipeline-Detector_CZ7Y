{"cells":[{"metadata":{},"cell_type":"markdown","source":"Hello, This Kernel is the first kernel I write in Kaggle.\n\nThis competetion is the unbalance binary classification competition, which is common in Kaggle.\n\nWhat is unique is that Train / Test Data is provided in two separated files, transaction and identify, both are linked by 'TransactionID' value."},{"metadata":{},"cell_type":"markdown","source":"### OK. Let's start the kernel by loading the basic packages."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\n\nos.listdir(\"../input\")\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### I will check if the necessary data set files are located correctly.\n#### 2 train data sets, 2 test data sets and lastly data for submission.\n#### All data sets are OK."},{"metadata":{},"cell_type":"markdown","source":"* As mentioned earlier, this competition is divided into train and test data set into transaction and identity data sets.\n* Therefore, we need to combine these two data sets into one.\n* The 'merge_train_data' is a function that loads two files of Train Data Set and merges identity based on transaction data set."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"def merge_train_data():    \n    print( \"Loading Train Data...\")\n    train_transaction = pd.read_csv('../input/train_transaction.csv')\n    train_identity = pd.read_csv('../input/train_identity.csv')\n    \n    print( \"Shape of train_transaction\" , train_transaction.shape )\n    train_transaction.head()\n    \n    print(\"Shape of train_identity\" , train_identity.shape )\n    train_identity.head()\n    \n    print( \"Merging...\" )\n    train_merged = pd.merge( train_transaction , train_identity , on='TransactionID' , how='left' )\n    \n    del train_transaction \n    del train_identity\n    \n    return train_merged","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_merged = merge_train_data()\ntrain_merged.shape\ntrain_merged.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#### Transaction data has about 590,000 data, 394 features, while identity data has about 140,000 data, and 41 features.\n\n#### As we compare the shape of these two data sets, when we merge identity Data based on transaction data, merged data will include many NaN data."},{"metadata":{"trusted":true},"cell_type":"code","source":"chk_NaN = train_merged.isnull().sum()\nchk_NaN","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#### Similar to train data, let's load test data and merge as same ways of train data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def merge_test_data():\n    print( \"Loading Test Data...\")\n    test_transaction = pd.read_csv('../input/test_transaction.csv')\n    test_identity = pd.read_csv('../input/test_identity.csv')\n    \n    print( \"Shape of test_transaction\" , test_transaction.shape )\n    test_transaction.head()\n    \n    print(\"Shape of test_identity\" , test_identity.shape )\n    test_identity.head()\n    \n    print( \"Merging...\" )\n    test_merged = pd.merge( test_transaction , test_identity , on='TransactionID' , how='left' )\n\n    del test_transaction\n    del test_identity\n    return test_merged","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_merged = merge_test_data()\ntest_merged.shape\ntest_merged.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"chk_NaN = test_merged.isnull().sum()\nchk_NaN","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.countplot( x = 'isFraud', data = train_merged )\nplt.xticks(rotation=45)\nplt.title( 'Target' )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The distribution of target is very unbalanced."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#### In the train dats set, the target column is removed from the features."},{"metadata":{"trusted":true},"cell_type":"code","source":"target = train_merged['isFraud']\ntrain_merged = train_merged.drop('isFraud' , axis='columns')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_merged.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Currently, the total number of features is 433 and I think it seems there are too many features.\n#### So, I decided to remove some features that has little information"},{"metadata":{},"cell_type":"markdown","source":"#### First, let's remove the feature with a large number of NaN."},{"metadata":{"trusted":true},"cell_type":"code","source":"rows_count = train_merged.shape[0]\n\nmissing_value = pd.DataFrame(columns=['Missing Rate'])\nmissing_value['Missing Rate'] = train_merged.isnull().sum() / rows_count\n\nhigh_miss_rate = missing_value[ missing_value['Missing Rate'] >= 0.85 ].index.values.tolist()\n\nprint(\"High Missing Rate(85%) Feature Count : \",len(high_miss_rate))\n\ntrain_merged = train_merged.drop(high_miss_rate , axis='columns')\ntrain_merged.shape\ndel high_miss_rate","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Total 74 features are consisted of NaN over 85% and will be removed from train data set.\n#### However, since we can do feature engineering later using the ratio of NaN or the meaning of NaN, let's try to decide whether to use it"},{"metadata":{},"cell_type":"markdown","source":"#### Let's take a look at each of the remaining features by their data type.\n#### Let's break it down into Int, Float, and Object"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_int = []\ncols_float = []\ncols_object = []\n\nall_cols_list = train_merged.columns.values.tolist()\n\nfor col in all_cols_list:\n    if train_merged[col].dtypes == 'int64':\n        cols_int.append(col)\n    elif train_merged[col].dtypes == 'float64':\n        cols_float.append(col)\n    elif train_merged[col].dtypes == 'object':\n        cols_object.append(col)\n    else:\n        print('Exception')\n\nprint( 'Total train_merged Feature Numbers : ', len(all_cols_list))\nprint( 'int64 Feature Numbers : ', len(cols_int))\nprint( 'float64 Feature Numbers : ', len(cols_float))\nprint( 'object Feature Numbers : ', len(cols_object))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's measure the feature importance of the remaining features with XGBoost."},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nfrom xgboost import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit model no training data\nmodel = XGBClassifier( nthread = 4 )\nmodel.fit( train_merged[cols_int + cols_float] , target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_fea_imp = pd.DataFrame(list(model.get_booster().get_fscore().items()),columns=['feature','importance']).sort_values('importance', ascending=False)\nxgb_fea_imp.shape\nxgb_fea_imp.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### XGBoost has selected 132 features with Importance.\n#### Now let's train using these features."},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\n\ntrain_ds = lgb.Dataset( train_merged[ xgb_fea_imp['feature'].tolist() ] , label = target )\n\nparameters = {\n    'application': 'binary',\n    'objective': 'binary',\n    'metric': 'auc',\n    'is_unbalance': 'true',\n    'boosting': 'gbdt',\n    'num_threads' : 4,\n    'num_leaves': 31,\n    'feature_fraction': 0.5,\n    'bagging_fraction': 0.5,\n    'bagging_freq': 20,\n    'learning_rate': 0.05,\n    'verbose': 1\n}\n\nmodel = lgb.train(parameters, train_ds ,num_boost_round=5000)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prob = model.predict( test_merged[ xgb_fea_imp['feature'].tolist() ] )\nprob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['TransactionID'] = test_merged['TransactionID']\nsubmission['isFraud'] = prob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Not a high score, but it seems appropriate for use as a Baseline Model."},{"metadata":{},"cell_type":"markdown","source":"#### The features I used are all features without special feature engineering\n#### We only used numerical features, and we did not use any other categorical features at all."},{"metadata":{},"cell_type":"markdown","source":"### In addition, the following applies to the following kernel.\n- Using categorical features, additional Feature Engineering\n- Application of Cross Validation\n- Stacking"},{"metadata":{},"cell_type":"markdown","source":"#### I will share with you the next kernel on the methods I will use.\n#### Please let me know if I made a mistake or have a better suggestion.\n\n### Thank you for reading."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}