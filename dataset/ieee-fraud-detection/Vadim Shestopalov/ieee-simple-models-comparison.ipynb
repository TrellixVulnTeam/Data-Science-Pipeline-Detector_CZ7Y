{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Motivation\nThis kernel is dedicated to show simple models comparison, inspired by Pavel Pleskov [pipeline shared during Kaggle Days at Dubai](https://gitlab.com/ppleskov/kaggle-days-dubai). \n\nIn this kenel I am comparing standard algos used for classification task in tabular data. \n\nEvaluation metric is [ROC-AUC](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc). \n\n<left><img src='https://images.martechadvisor.com/images/uploads/content_images/frauddetectio_5b60873e86283.jpg'>`"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cb\nfrom catboost import CatBoostClassifier, Pool\nimport random \n\nimport os\nfrom os import listdir\nfrom tqdm import tqdm\nfrom os.path import isfile\n\nimport sklearn\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler \nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn import preprocessing\nfrom sklearn.externals import joblib\nfrom sklearn.decomposition import TruncatedSVD\n\nfrom bayes_opt import BayesianOptimization\nfrom bayes_opt.event import Events\nfrom bayes_opt.util import load_logs\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.set()\n\nimport time\nimport datetime\n\n#import shap\n# load JS visualization code to notebook\n#shap.initjs()\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nprint(os.listdir(\"../input\"))\nprint()\n\nprint(\"pandas:\", pd.__version__)\nprint(\"numpy:\", np.__version__)\nprint(\"sklearn:\", sklearn.__version__)\nprint()\nprint(\"lightgbm:\", lgb.__version__)\nprint(\"xgboost:\", xgb.__version__)\nprint(\"catboost:\", cb.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_transaction = pd.read_csv('../input/train_transaction.csv', index_col='TransactionID')\ntest_transaction = pd.read_csv('../input/test_transaction.csv', index_col='TransactionID')\n\ntrain_identity = pd.read_csv('../input/train_identity.csv', index_col='TransactionID')\ntest_identity = pd.read_csv('../input/test_identity.csv', index_col='TransactionID')\n\nsample_submission = pd.read_csv('../input/sample_submission.csv', index_col='TransactionID')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's merge data as in [starter kernel](https://www.kaggle.com/inversion/ieee-simple-xgboost):"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n\nprint(f'Shape of train set: {train.shape}')\nprint(f'Shape of test set: {test.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on 590k entries in train we should predict 506k for test set.\n\nLet's take a look on target distribution:"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train['isFraud']) #Imbalanced Dataset\nplt.title('Target distribution');","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(f'Number of fraud samples in train: {len(np.where(train[\"isFraud\"]==1)[0])}')\nprint(f'Percent of fraud samples in train: {round(100.0*len(np.where(train[\"isFraud\"]==1)[0])/len(train[\"isFraud\"]),2)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now, let's take a 10 % sample of the dataset to speed up all calculations"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.sample(frac=0.1, random_state=42) # comment if you want to run on entire set (takes longer time)\ntrain.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train.isFraud.values\n\ntrain = train.drop('isFraud', axis=1)\ntest = test.copy()\ntrain = train.fillna(-1) #nan substitution could be done in a better way\ntest = test.fillna(-1) \ndel train_transaction, train_identity, test_transaction, test_identity","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Label Encoding\nfor f in train.columns:\n    if train[f].dtype=='object' or test[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(train[f].values) + list(test[f].values))\n        train[f] = lbl.transform(list(train[f].values))\n        test[f] = lbl.transform(list(test[f].values))  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = list(train.columns)\nlen(cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Standard scaler preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler() #MinMaxScaler StandardScaler RobustScaler\n\ntrain[cols] = scaler.fit_transform(train[cols])\ntest[cols] = scaler.transform(test[cols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N = 50\n\nsvd = TruncatedSVD(n_components=N, random_state=42)\nX = svd.fit_transform(train[cols], y)  \nsvd.explained_variance_ratio_.sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame()\ndf[\"target\"] = y\n\nfor i in range(50):\n    df[i] = X[:,i]\n    \ndf.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\narch = \"reg\"\n\ntrain[arch] = 0\n\nfor i, (train_index, valid_index) in enumerate(skf.split(X, y)):\n    \n    X_train = X[train_index]\n    X_valid = X[valid_index]\n\n    y_train = y[train_index]\n    y_valid = y[valid_index]\n    \n    reg = LogisticRegression(C=1,\n                             solver=\"newton-cg\", \n                             penalty=\"l2\", \n                             n_jobs=-1, \n                             max_iter=100).fit(X_train, y_train) \n    \n    y_pred = reg.predict_proba(X_valid)[:,1]\n    train.loc[valid_index, arch] = y_pred\n    print(i, \"ROC AUC:\", round(roc_auc_score(y_valid, y_pred), 5))\n\nprint()\nprint(\"OOF ROC AUC:\", round(roc_auc_score(y, train[arch]), 5))\nprint()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\narch = \"rfc\"\n\ntrain[arch] = 0\ntest[arch] = 0\n\nfor i, (train_index, valid_index) in enumerate(skf.split(X, y)):\n    \n    X_train = X[train_index]\n    X_valid = X[valid_index]\n\n    y_train = y[train_index]\n    y_valid = y[valid_index]\n    \n    rfc = RandomForestClassifier(n_estimators=100,\n                                 criterion='gini',\n                                 n_jobs=-1).fit(X_train, y_train) \n    \n    y_pred = rfc.predict_proba(X_valid)[:,1]\n    train.loc[valid_index, arch] = y_pred\n    print(i, \"ROC AUC:\", round(roc_auc_score(y_valid, y_pred), 5))\n\nprint()\nprint(\"OOF ROC AUC:\", round(roc_auc_score(y, train[arch]), 5))\nprint()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\narch = \"lgb\"\n\ntrain[arch] = 0\n\nrounds = 10000\nearly_stop_rounds = 300\n\nparams = {'objective': 'binary',\n          'boosting_type': 'gbrt',\n          'metric': 'auc',\n          'seed': 42,\n          'max_depth': -1,\n          'verbose': -1,\n          'n_jobs': -1}\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor i, (train_index, valid_index) in enumerate(skf.split(X, y)):\n    \n    X_train = X[train_index]\n    X_valid = X[valid_index]\n\n    y_train = y[train_index]\n    y_valid = y[valid_index]\n\n    d_train = lgb.Dataset(X_train, y_train)\n    d_valid = lgb.Dataset(X_valid, y_valid)    \n\n    model = lgb.train(params,\n                      d_train,\n                      num_boost_round=rounds,\n                      valid_sets=[d_train, d_valid],\n                      valid_names=['train','valid'],\n                      early_stopping_rounds=early_stop_rounds,\n                      verbose_eval=0) \n\n\n    y_pred = model.predict(X_valid)\n    train.loc[valid_index, arch] = y_pred\n    auc = roc_auc_score(y_valid, y_pred)\n    print(i, \"ROC AUC:\", round(auc, 5))\n\nprint()\nprint(\"OOF ROC AUC:\", round(roc_auc_score(y, train[arch]), 5))\nprint()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Catboost"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\narch = \"cat\"\n\ntrain[arch] = 0\n\nrounds = 10000\nearly_stop_rounds = 100\n\nparams = {'task_type': 'CPU', #GPU\n          'iterations': rounds,\n          'loss_function': 'Logloss',\n          'eval_metric':'AUC',\n          'random_seed': 42,\n          'learning_rate': 0.5,\n          'depth': 2}\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor i, (train_index, valid_index) in enumerate(skf.split(X, y)):\n    \n    X_train = X[train_index]\n    X_valid = X[valid_index]\n\n    y_train = y[train_index]\n    y_valid = y[valid_index]\n    \n    trn_data = Pool(X_train, y_train)\n    val_data = Pool(X_valid, y_valid)\n    \n    clf = CatBoostClassifier(**params)\n    clf.fit(trn_data,\n            eval_set=val_data,\n            use_best_model=True,\n            early_stopping_rounds=early_stop_rounds,\n            verbose=0)\n    \n    y_pred = clf.predict_proba(X_valid)[:, 1]\n    train.loc[valid_index, arch] = y_pred\n    auc = roc_auc_score(y_valid, y_pred)\n    print(i, \"ROC AUC:\", round(auc, 5))\n\nprint()\nprint(\"OOF ROC AUC:\", round(roc_auc_score(y, train[arch]), 5))\nprint()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## NN \n\nI also checked FastAI implementation described in Pavel's pipeline, but it gives much less score than previous models."},{"metadata":{},"cell_type":"markdown","source":"## Correlation of the models"},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [\"cat\", \"lgb\", \"rfc\", \"reg\"] #\"nn\"\n\nfor model in models:\n    train[model] = train[model].rank()/len(train)\n\ntrain[models].corr(method=\"spearman\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for arch in models:\n    print(arch, round(roc_auc_score(y, train[arch]), 5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Maybe now you know which model to tune. "},{"metadata":{},"cell_type":"markdown","source":"### Blending"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"avg\"] = train[models].mean(axis=1)\nprint(\"avg\", round(roc_auc_score(y, train[\"avg\"]), 5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats.mstats import gmean\n\ndef power_mean(x, p=1):\n    if p==0:\n        return gmean(x, axis=1)\n    return np.power(np.mean(np.power(x,p), axis=1), 1/p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for power in [0,1,2,4,8]:\n    train[\"avg\"] = power_mean(train[models].values, power)\n    print(power, round(roc_auc_score(y, train[\"avg\"]), 5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stacking\n\nLet's stack predictions of previos model and learn LR on them. "},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\narch = \"stack\"\n\ntrain[arch] = 0\n\nfor i, (train_index, valid_index) in enumerate(skf.split(X, y)):\n    \n    X_train = train.loc[train_index, models]\n    X_valid = train.loc[valid_index, models]\n\n    y_train = y[train_index]\n    y_valid = y[valid_index]\n    \n    reg = LogisticRegression(C=1,\n                             solver=\"newton-cg\", \n                             penalty=\"l2\", \n                             n_jobs=-1, \n                             max_iter=100).fit(X_train, y_train) \n    \n    y_pred = reg.predict_proba(X_valid)[:,1]\n    train.loc[valid_index, arch] = y_pred\n    print(i, \"ROC AUC:\", round(roc_auc_score(y_valid, y_pred), 5))\n\nprint()\nprint(\"OOF ROC AUC:\", round(roc_auc_score(y, train[arch]), 5))\nprint()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## What is not covered (yet)\n\n- SVM\n- KNN\n- FastAI (because of low score)\n- H2O (that approach is already shown in Bojan's kernel: https://www.kaggle.com/tunguz/ieee-with-h2o-automl)"},{"metadata":{},"cell_type":"markdown","source":"### Reference: \n\nMost of the code is taken from https://gitlab.com/ppleskov/kaggle-days-dubai as I wrote in the beginning of the kernel.\n\nBut it's applies it for Fraud detection task and gives a hint about models which could be used in that competition."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}