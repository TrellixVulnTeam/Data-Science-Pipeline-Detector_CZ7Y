{"cells":[{"metadata":{"_uuid":"9ce100c5-2427-4406-9965-4e429664bff1","_cell_guid":"4d92be29-cc17-4f40-9d65-4a8d9e2c71d7","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Proyecto de Detección de Fraude En Transacciones Bancarias\nAntonio Montilla\n\nNeoland\n\nMadrid, 15 de Marzo de 2020"},{"metadata":{},"cell_type":"markdown","source":"## Introducción\nLa forma en que los consumidores realizamos nuestras compras habituales de bienes y servicios ha sufrido una gran trasnformación en las últimas décadas, caracterizada por el mayor uso de dinero electrónico como medio de pago. La bancarización en los países desarrollados ha alcanzado nuevos niveles de penetración, que, en conjunto con la masificación en el uso del internet, ha propulsado el comercio en linea, o e-commerce.\n\nCon el desarrollo tecnológico, las transacciones que utilizan tarjetas de crédito/débito o, incluso, medios electrónicos de pago a través de dispositivos móviles (como Apple pay o Paypal), son cada vez más frecuentes. En España, por ejemplo, las transacciones de este tipo, es decir aquellas que se efectuan sin efectivo, ya superan el 10% del total de las compras para el año 2018, de acuerdo a estimaciones del Banco Central Europeo. En EEUU, la proporsión es de 75%, de acuerdo a a la Reserva Federal, mientras que en países como Suecia estas transacciones alcanzan niveles superiores al 80% del total, según estimaciones del Riksbank.\n\nSin embargo, este desarrollo del mercado bancario y del e-commerce ha traido consigo la proliferación de fraudes, ya sea a través del robo de los medios electrónicos de pago o simplemente mediante la usurpación de la identidad. Como referencia, el Banco Central Europeo estima que el fraude significa costes al sector financiero en torno a 1.800 millones de euros en la zona monetaria europea; en España la cifra se ubica en torna a 90 millones de euros. Desarrollar herramientas que permitan minimizar dichos costes resulta, definitivamente, crucial para la sostenibilidad y el desarrollo de estos avances tecnológicos en el consumo privado.\n\nEn este contexto, este proyecto intenta desarrollar modelos que permitirían predecir si una transacción es de índole fraudulante o no, a través del análisis y explotación de una base de datos que contiene información de 590.540 transacciones efectuadas en linea a través de la plataforma de Vesta. El proyecto evalúa las variables más relevantes para predecir la verocidad de una transacción, construye modelos y selecciona el que mejor se adapta a los datos.\n\nEl uso de este tipo de metodología, si aplicada masivamente en el mundo real, resultaría en una disminución significativa de las pérdidas ocasianadas por el fraude, tanto a instituciones financieras como al usuario final.\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Objetivos\n* Explorar la database provista por Vesta, seleccionar las variables más relevantes para la predicción de la columna que comprueba si la transacción es fradulenta o no.\n* Diseñar y construir diferentes modelos usando la base de dato, que permita dicha predicción.\n* Evaluar la validez de cada modelo y eligir el que mejor se aplica para este ejercicio.\n* Ilustrar el aporte al sector bancario de la implementación de este tipo de algoritmos.\n"},{"metadata":{},"cell_type":"markdown","source":"## Proceso de captura de datos\nEste proyecto se fundamenta en el análisis y explotación de una base de datos provista por Vesta Corporation, disponible en kaggle (https://www.kaggle.com/c/ieee-fraud-detection). Vesta corporation proveé una plataforma para la realización de operaciones financieras en linea. En este challenge, la compañia instruye a los participantes a proponer modelos de detección de transacciones fradulentas en línea.\n\nLa base de dato incluye información de 590.540 transacciones efectuadas en linea a travês de la plataforma de Vesta. En concreto, la base de dato incluye 414 columnas por cada observación, detallando información respecto a la transacción misma (e.g. código de producto, fecha, medio de pago) y la identidad del comprador (e.g. hábitos de consumo, ubicación, dispositivo utilizado). Por tema de seguridad de datos, las columnas referentes a la identidiad son ocultadas.\n\nLa variable objetivo es _is Fraud_, que toma valores binarios (0, 1). La base de dato ha sido dividida en un fichero para entrenar el modelo (train) y un fichero para testear (test)."},{"metadata":{},"cell_type":"markdown","source":"## Cargando las librerias necesarias"},{"metadata":{"trusted":true},"cell_type":"code","source":"# analisis de datos\nimport pandas as pd\nimport numpy as np\nimport random as rnd\nfrom pandas import read_csv\n\n# visualización\nimport seaborn as sns\nfrom scipy.stats import norm, skew\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neural_network import MLPClassifier\n\n## scikit modeling libraries\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier,\n                             GradientBoostingClassifier, ExtraTreesClassifier,\n                             VotingClassifier)\n\nfrom sklearn.model_selection import (GridSearchCV, cross_val_score, cross_val_predict,\n                                     StratifiedKFold, learning_curve)\n\n## Predictive modeling\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.feature_selection import RFE\n\n#Principal components & otros\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.naive_bayes import GaussianNB\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importando base de datos"},{"metadata":{"trusted":true},"cell_type":"code","source":"original = '../input/'\nurl1 = \"/kaggle/input/ieee-fraud-detection/train_identity.csv\"\nurl2 = \"/kaggle/input/ieee-fraud-detection/train_transaction.csv\"\n\ntrain_identity = read_csv(url1)\ntrain_transaction = read_csv(url2)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploración Preliminar de la Base de Datos"},{"metadata":{"trusted":true},"cell_type":"code","source":"#exploración inicial del fichero transaction\nprint('El tamaño del fichero train_transaction es: ', train_transaction.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"El fichero train transaction contiene 394 columnas, incluyendo transaction ID como identificador de transacción, y 590.540 observaciones."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction.columns.values # Nombre de las variables","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Debido al gran número de observaciones, la memoria resulta insuficiente para explorar todos los datos en su conjunto.\nExploraré el fichero identity para poder tomar una decisión del manejo completo de la base"},{"metadata":{"trusted":true},"cell_type":"code","source":"#exploración inicial del fichero identity\nprint('El tamaño del fichero train_identity es: ', train_identity.shape)\ntrain_identity.columns.values # Nombre de las variables","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"El fichero train identity contiene 41 columnas, incluyendo transaction ID como identificador de transacción, y 144.233 observaciones.\n\nDebido a que esta base de dato contiene menos observaciones, procederé a unir los dos dataframes (usando la columna _transaction ID_) y posteriormente eliminaré las observaciones con datos nulos."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Por ahorrar memoria, ahora proceso a eliminar las base de datos _train transaction_ y _train identity_\ndel train_identity, train_transaction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#exploración inicial del fichero con todos los datos _train_\nprint('El tamaño del fichero train es: ', train.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"En su conjunto, la base de dato contiene 590.540 observaciones y 434 columnas."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns.values # Nombre de las variables","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Las variables se encuentran agrupadas en función al siguiente criterio:\n* _TransactionID_, que representa el identificador de cada transacción.\n* _isFraud_, que es la variable objetivo. Toma valores 1 y 0, dependiendo si la transacción es fraude o no.\n* _TransactionDT_, indica tiempo desde transacción (en segundos).\n* _TransactionAmt_, indica al monto de la transacción (es USD).\n* _ProductCD_, el código de producto de la transacción. Variable categórica.\n* _card_ (01-06), información sobre la tarjeta de pago, como tipo, categoría, banco, país, entre otros. Variables categóricas.\n* _addr_ (01-02), dirección de cliente y de vendedor. Variables categóricas.\n* _dist_ (01-02), se refiere a la distancia entre la ubicación del cliente y el vendedor.\n* _Pemaildomain_, el dominio del email del comprador. Variable categórica.\n* _Remaildomain_, el dominio del email del vendedor. Variable categórica.\n* _C_ (01-14), columnas ocultas e incriptadas. Se refieren a variables de conteo, como el número de direcciones asociadas a la tarjeta, números de teléfono, direcciones de email, entre otros, tanto para el comprador como para el vendedor.\n* _D_ (01-15), variables del tiempo, como tiempo transacurrido desde última transacción, entre otros.\n* _M_ (01-09), variables que indican si hay un match entre la información de compra. Variables categóricas.\n* _V_ (01-339), variables provistas por Vesta referente a la transacción, como clasificación, conteo, entre otros.\n* _id_ (01-138), variables ocultas e incriptadas. Se refieren a datos de identidad, que por razones de protección no pueden ser reveladas. Datos personales del vendedor y comprador, datos de la conección o equipo (IP, ISP, proxy). \n* _devicetype_, tipo de dispositivo usado por el comprador. Variable categórica.\n* _deviceinfo_, información del dispositivo usado por el comprador. Variable categórica."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Explorando los datos vacíos para posible eliminación\ntrain.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total = train.isnull().sum().sort_values(ascending = False) #sumando valores nulos por columna\nporcentaje = (train.isnull().sum()/train.isnull().count()*100).sort_values(ascending = False) #y % del total\nmissing_train_data  = pd.concat([total, porcentaje], axis=1, keys=['Total', 'Porcentaje']) #df para explorar\nmissing_train_data.head(50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_train_data.head(235)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* La exploración inicial sugiere que ciertas columnas contienen una gran cantidad de datos vacíos, que pueden llegar a representar casi la totalidad de los datos, es decir 99% del total para ciertas variables de identidad.\n\n* En total, 232 columnas contienen más del 40% de las observaciones vacías; las 202 columnas restantes contienen más del 70% de los datos cargados, es decir como no nulos.\n\n* Como primer paso, procedo a eliminar las 232 columnas con más del 40% de valores nulos."},{"metadata":{"trusted":true},"cell_type":"code","source":"#creo nuevo df con las columnas con más del 70% de las observaciones\nTrain_new = train.drop(train.loc[:,list((100*(train.isnull().sum()/len(train.index))>30))].columns, 1)\n#compruebo presencia de NAs\ntotal1 = Train_new.isnull().sum().sort_values(ascending = False) #sumando valores nulos por columna\nporcentaje1 = (Train_new.isnull().sum()/Train_new.isnull().count()*100).sort_values(ascending = False) #y % del total\nmissing_train_data1  = pd.concat([total1, porcentaje1], axis=1, keys=['Total', 'Porcentaje']) #df para explorar\nmissing_train_data1.head(50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('El tamaño del fichero train es: ', Train_new.shape)\nTrain_new.columns.values # Nombre de las variables","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Como siguiente caso procedo a eliminar las observaciones nulas, incluyendo tanto valores vacios como NAs, lo que me permitirá reducir el tamaño de la dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"#creo una nueva version del dataset sin valores nulos\nTrain_new2 = Train_new.dropna()\nprint('El tamaño del fichero train es: ', Train_new2.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#compruebo presencia de valores missing y nulos\ntotal1 = Train_new2.isna().sum().sort_values(ascending = False) #sumando valores nulos por columna\nporcentaje1 = (Train_new2.isna().sum()/Train_new2.isna().count()*100).sort_values(ascending = False) #y % del total\nmissing_train_data1  = pd.concat([total1, porcentaje1], axis=1, keys=['Total', 'Porcentaje']) #df para explorar\nmissing_train_data1.head(50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Se confirma que no hay observaciones nulos o vacías.\n\nDe esta manera, el dataframe se ha reducido a 202 columnas y 328.198 observaciones. Como siguiente paso, evaluaré la presencia de valores vacíos con posibles caracteres especiales, como \"?\" o \"missing\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"interrogante = Train_new2.apply(lambda x: True if \"?\" in list(x) else False, axis=1)\nnumOfRows = len(interrogante[interrogante == True].index)\n \nprint('El número de observaciones con caracter ? es ', numOfRows)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing = Train_new2.apply(lambda x: True if \"missing\" in list(x) else False, axis=1)\nnumOfRows = len(interrogante[interrogante == True].index)\n \nprint('El número de observaciones con caracter missing es ', numOfRows)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Se concluye que la dataframe _Train new2_ no contiene valores vacíos, nulos, o con ciertos caracteres especiales.\nAhora procederé a la realización del Exploratory Data Analysis (EDA)."},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis\n\nEn esta sección se explorará con más detalle la base de datos, en particular el comportamiento de la variable objetivo _Is Fraud_ y su relación con las demás columnas.\n\nDebido al tamaño de la base de dato, procederé a extraer una muestra del 15% (50.000 datos) del total para facilitar la exploración de los datos."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extrayendo Muestra\nTrain_sample = Train_new2.sample(frac =.1524, random_state = 2)\nprint('El tamaño de la muestra de train es: ', Train_sample.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_sample.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ahora se procederá a la exploración de la variable objetivo _isFraud_"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"La variable objetivo _Is Fraud_ tiene {0} obervaciones y {1} son valores únicos.\".format(Train_sample['isFraud'].count(),Train_sample['isFraud'].nunique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Train_sample['isFraud'].describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_sample.groupby('isFraud') \\\n    .count()['TransactionID'] \\\n    .plot(kind='barh',\n          title='Distribución de IsFraud',\n          figsize=(15, 3))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Train_sample['isFraud'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* La variable objetivo toma valores 0 (no fraude) y 1 (fraude).\n* Sólo 991 de las observaciones son fraude, es decir 2% del total. En principio, la poca proporsión de observaciones con fraude relativo a los no fraude pudiera incidir en la calidez de los estimadores en el modelado, en sentido que se puede presentar sesgos de estimación.\n\nAhora se evaluará la relación con cada subset de variables explicativas. "},{"metadata":{},"cell_type":"markdown","source":"### TransactionDT\n* Columna numérica que indica tiempo desde transacción (en segundos)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#TransactionDT\nprint(Train_sample['TransactionDT'].describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Histograma de columna en log, para suavizar escala\nTrain_sample['TransactionDT'] \\\n    .apply(np.log) \\\n    .plot(kind='hist',\n          bins=100,\n          figsize=(15, 5),\n          title='Distribución del Log de TransactionDT')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Variable numérica que mide el tiempo desde transacción.\n* Se distribuye con pendiente positiva: los datos se concentran en los valores más altos, es decir, que la moda es que el tiempo transcurrido desde la transacción sea lo más largo."},{"metadata":{"trusted":true},"cell_type":"code","source":"#relación con columna objetivo\nTrain_sample[['TransactionDT', 'isFraud']].groupby(['isFraud'], as_index=False).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('La media de TransactionDT con IsFraud igual a 1 es: {:.4f}'.format(Train_sample.loc[Train_sample['isFraud'] == 1]['TransactionDT'].mean()))\nprint('La media de TransactionDT con IsFraud igual a 0 es: {:.4f}'.format(Train_sample.loc[Train_sample['isFraud'] == 0]['TransactionDT'].mean()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* En general se puede decir que las trasacciones con fraude se tendieron a producir con menos tiempo desde la recogida de los datos. \n* Sin embargo, dado que la unidad de tiempo es segundos, no se observa una gran diferencia en la media de tiempo transcurrido entre transacciones con fraude o no.\n* A priori, la observación sugiere que esta columna no tendrá un nivel explicativo elevado en _IsFraud_."},{"metadata":{},"cell_type":"markdown","source":"### TransactionAmt \n\n* Variable numérica que indica al monto de la transacción, en USD."},{"metadata":{"trusted":true},"cell_type":"code","source":"#TransactionAmt\nprint(Train_sample['TransactionAmt'].describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Histograma de columna en log, para suavizar escala\nTrain_sample['TransactionAmt'] \\\n    .apply(np.log) \\\n    .plot(kind='hist',\n          bins=100,\n          figsize=(15, 5),\n          title='Distribución del Log de TransactionAmt')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* En promedio, el monto de una transacción gira en torno a 160 USD; el monto mínimo es de 3,5 USD mientras el máximo es de 4.843 USD\n* El log de la serie muestra una distribución que se asemeja a la normal"},{"metadata":{"trusted":true},"cell_type":"code","source":"#relación con columna objetivo\nTrain_sample[['TransactionAmt', 'isFraud']].groupby(['isFraud'], as_index=False).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('La media de TransactionAmt con IsFraud igual a 1 es: {:.4f}'.format(Train_sample.loc[Train_sample['isFraud'] == 1]['TransactionAmt'].mean()))\nprint('La media de TransactionAmt con IsFraud igual a 0 es: {:.4f}'.format(Train_sample.loc[Train_sample['isFraud'] == 0]['TransactionAmt'].mean()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Histogramas de TransactionAmt vs. IsFraud\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 6))\nTrain_sample.loc[Train_sample['isFraud'] == 1] \\\n    ['TransactionAmt'].apply(np.log) \\\n    .plot(kind='hist',\n          bins=100,\n          title='Log TransactionAmt vs IsFraud = 1',\n          xlim=(-3, 10),\n         ax= ax1)\nTrain_sample.loc[Train_sample['isFraud'] == 0] \\\n    ['TransactionAmt'].apply(np.log) \\\n    .plot(kind='hist',\n          bins=100,\n          title='Log TransactionAmt vs IsFraud = 0',\n          xlim=(-3, 10),\n         ax=ax2)\nTrain_sample.loc[Train_sample['isFraud'] == 1] \\\n    ['TransactionAmt'] \\\n    .plot(kind='hist',\n          bins=100,\n          title='TransactionAmt vs IsFraud = 1',\n         ax= ax3)\nTrain_sample.loc[Train_sample['isFraud'] == 0] \\\n    ['TransactionAmt'] \\\n    .plot(kind='hist',\n          bins=100,\n          title='TransactionAmt vs IsFraud = 0',\n         ax=ax4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Las transacciones fradulentas, en promedio, tienden a ser por montos superiores (en torno al 50%) que las transacciones no fradulentas, como en principio se esperaría a priori.\n* Esta columna será relevante en predecir si una transacción es fraudulenta."},{"metadata":{},"cell_type":"markdown","source":"### ProductCD \n\n* Variable categórica que indica el código de producto de la transacción."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Train_sample['ProductCD'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_sample.groupby('ProductCD') \\\n    .count()['TransactionID'] \\\n    .plot(kind='barh',\n          title='Distribución de ProductCD',\n          figsize=(15, 3))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* La exploración muestra que la variable _ProductCD_ toma un único valor (W), lo que resulta inusual.\n* Se debe contrastar con la distribución de la columna en la dataframe poblacional para verificar que no sea un problema de muestreo."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Train_new2['ProductCD'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* En la base completa, la columna también toma un valor único.\n* Debido a la no variabilidad, procedo a eliminar la columna de mi dataframe, tanto el de muestra como el poblacional."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Eliminando la columna _ProductCD_ de las base de datos\nTrain_sample = Train_sample.drop(['ProductCD'], axis=1)\nTrain_new2 = Train_new2.drop(['ProductCD'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Card1 - Card6\n* Conjunto de columnas categóricas que denotan información sobre la tarjeta de pago, como tipo, categoría, banco, país, entre otros."},{"metadata":{"trusted":true},"cell_type":"code","source":"#creando vector con nombre de las columnas para facilitar exploración\ncard_cols = [\"card1\", \"card2\", \"card3\", \"card4\", \"card5\", \"card6\"]\nTrain_sample[card_cols].head(50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_sample[card_cols].tail(50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* _Card4_ es categórica e indica el tipo de la tarjeta usada en la operación (visa, mastercard). \n* _Card6_ es categórica e indica la categoría de la tarjeta usada (débito, crédito).\n* _Card1, Card2, Card3 y Card5, son numéricas indicando categoría de clasificación de la tarjeta, como código país, banco, antiguedad. El significado no es revelado."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Distribución de _Card1_\nTrain_sample['card1'] \\\n    .plot(kind='hist',\n          bins=100,\n          figsize=(7.5, 2.5),\n          title='Distribución de card1')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Distribución de _Card1_ relativo a IsFraud\nTrain_sample[['card1', 'isFraud']].groupby(['isFraud'], as_index=False).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* No hay clara distinción de esta clasificación de la tarjeta relativo a la variable objetivo.\n* Este comportamiento aplica también a _Card1, Card2 y Card5_ (abajo los gráficos y tablas)."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Distribución de _Card2_\nTrain_sample['card2'] \\\n    .plot(kind='hist',\n          bins=100,\n          figsize=(7.5, 2.5),\n          title='Distribución de card2')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Distribución de _Card2_ relativo a IsFraud\nTrain_sample[['card2', 'isFraud']].groupby(['isFraud'], as_index=False).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Distribución de _Card5_\nTrain_sample['card5'] \\\n    .plot(kind='hist',\n          bins=100,\n          figsize=(7.5, 2.5),\n          title='Distribución de card5')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Distribución de _Card5_ relativo a IsFraud\nTrain_sample[['card5', 'isFraud']].groupby(['isFraud'], as_index=False).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Distribución de _Card3_\nTrain_sample['card3'] \\\n    .plot(kind='hist',\n          bins=100,\n          figsize=(7.5, 2.5),\n          title='Distribución de card3')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* La varible toma un único valor (150), que probablemente se refiera al código país de la tarjeta (EEUU).\n* Evaluo la distribución en la data poblacional para contrastar. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Valores únicos de _Card3_ en la base poblacional \nprint(Train_new2['card3'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Aunque si muestra multiples valores únicos, estos son insignificantes en términos del tamaño de la muestra: la segunda categoría más repetida incluye menos del 0,1% de todos los datos.\n* Debido a esto, procedo a eliminar la variable de las bases de datos, tanto muestral como poblacional."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Eliminando la columna _card3_ de las base de datos\nTrain_sample = Train_sample.drop(['card3'], axis=1)\nTrain_new2 = Train_new2.drop(['card3'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Distribución de variable categórica _Card4_ y _Card6_ referente a tipo de tarjeta\n#_Card4_\nprint(Train_sample['card4'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_sample.groupby('card4') \\\n    .count()['TransactionID'] \\\n    .plot(kind='barh',\n          title='Distribución de ProductCD',\n          figsize=(15, 3))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#_Card6_\nprint(Train_sample['card6'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_sample.groupby('card6') \\\n    .count()['TransactionID'] \\\n    .plot(kind='barh',\n          title='Distribución de ProductCD',\n          figsize=(15, 3))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Las transacciones son predominadamente efectuadas a través de tarjetas de débito visa.\n* Mastercard es la segunda opción de tipo tarjeta\n* Crédito es la segunda opción en términos de categoria"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Ahora evaluamos la relación de estas con la variable objetivo _IsFraud_\nTrain_sample_fr1 = Train_sample.loc[Train_sample['isFraud'] == 1]\nTrain_sample_fr0 = Train_sample.loc[Train_sample['isFraud'] == 0]\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 8))\nTrain_sample_fr1.groupby('card4')['card4'].count().plot(kind='barh', ax=ax1, title='card4 con IsFraud = 1')\nTrain_sample_fr0.groupby('card4')['card4'].count().plot(kind='barh', ax=ax2, title='card4 con IsFraud = 0')\nTrain_sample_fr1.groupby('card6')['card6'].count().plot(kind='barh', ax=ax3, title='card6 con IsFraud = 1')\nTrain_sample_fr0.groupby('card6')['card6'].count().plot(kind='barh', ax=ax4, title='card6 con IsFraud = 0')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* En términos de tipo de tarjeta, no se observa una gran distición entre operaciones de fraude y no fraude.\n* Esto también es cierto para la categoría, si bien, al margen, las tarjetas de crédito tienden a tener un mayor peso relativo en las transacciones de fraude.\n* Ahora se realizará la factorización de las columnas, tanto en la muestra como en la población."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Card_4\nTrain_sample['visa'] = np.where(Train_sample['card4'] == 'visa', 1, 0)\nprint(Train_sample['visa'].value_counts())\nTrain_sample['mastercard'] = np.where(Train_sample['card4'] == 'mastercard', 1, 0)\nprint(Train_sample['mastercard'].value_counts())\nTrain_new2['visa'] = np.where(Train_new2['card4'] == 'visa', 1, 0)\nTrain_new2['mastercard'] = np.where(Train_new2['card4'] == 'mastercard', 1, 0)\n\n#Card_6\nTrain_sample['debit'] = np.where(Train_sample['card6'] == 'debit', 1, 0)\nprint(Train_sample['debit'].value_counts())\nTrain_sample['credit'] = np.where(Train_sample['card6'] == 'credit', 1, 0)\nprint(Train_sample['credit'].value_counts())\nTrain_new2['debit'] = np.where(Train_new2['card6'] == 'debit', 1, 0)\nTrain_new2['credit'] = np.where(Train_new2['card6'] == 'credit', 1, 0)\n\n#Elimando ambas columnas de las dataframes muestral y poblacional\nTrain_sample = Train_sample.drop(['card4'], axis=1)\nTrain_new2 = Train_new2.drop(['card4'], axis=1)\nTrain_sample = Train_sample.drop(['card6'], axis=1)\nTrain_new2 = Train_new2.drop(['card6'], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Addr 01-02 \n* Variables que indican dirección de comprador y de vendedor. No se revela información adicional."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Distribución de _Addr1_\nTrain_sample['addr1'] \\\n    .plot(kind='hist',\n          bins=100,\n          figsize=(7.5, 2.5),\n          title='Distribución de addr1')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Distribución de _Addr1_ relativo a IsFraud\nTrain_sample[['addr1', 'isFraud']].groupby(['isFraud'], as_index=False).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* La columna Addr1 se distribuye de manera relativamente uniforme en los datos.\n* No se observa, a priori, distinción significativa en esta columna respecto a si hay fraude o no."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Distribución de _Addr2_\nTrain_sample['addr2'] \\\n    .plot(kind='hist',\n          bins=100,\n          figsize=(7.5, 2.5),\n          title='Distribución de addr2')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* La columna _addr2_ toma un valor unico en la muestra.\n* Evaluaré su distribución en la data poblacional."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Valores únicos de _addr2_ en la base poblacional \nprint(Train_new2['addr2'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Debido a la no variabilidad en la columna, procedo a eliminar de las bases de datos"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Eliminando la columna _addr2_ de las base de datos\nTrain_sample = Train_sample.drop(['addr2'], axis=1)\nTrain_new2 = Train_new2.drop(['addr2'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### _Pemaildomain_\n* el dominio del email del comprador. Variable categórica."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Train_sample['P_emaildomain'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_sample.groupby('P_emaildomain') \\\n    .count()['TransactionID'] \\\n    .plot(kind='barh',\n          title='Distribución de P_emaildomain',\n          figsize=(15, 15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Ahora evaluamos la relación de estas con la variable objetivo _IsFraud_\n#fraude\nTrain_sample_fr1 = Train_sample.loc[Train_sample['isFraud'] == 1]\nprint(Train_sample_fr1['P_emaildomain'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#No fraude\nTrain_sample_fr0 = Train_sample.loc[Train_sample['isFraud'] == 0]\nprint(Train_sample_fr0['P_emaildomain'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Para todas las transacciones, gmail.com es el dominio más común del email del comprador, seguido de yahoo.com y aol.com\n* No se observa distinción entre el tipo de transacción, por lo tanto eliminaré la columna tanto de la muestra como de la población"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Eliminando la columna _P_emaildomain_ de las base de datos\nTrain_sample = Train_sample.drop(['P_emaildomain'], axis=1)\nTrain_new2 = Train_new2.drop(['P_emaildomain'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### _C 01-14_ \n* Columnas ocultas e incriptadas. \n* Se refieren a variables de conteo, como el número de direcciones asociadas a la tarjeta, números de teléfono, direcciones de email, entre otros.\n* Datos tanto para el comprador como para el vendedor."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creando una dataframes con el conjunto de columnas _C_ para su exploración\nc_cols = [c for c in Train_sample if c[0] == 'C']\nc_df = Train_sample[c_cols]\nc_df.head(50)\nc_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Debido a la poca información disponible de las 14 columnas de conteo C, procedo a realizar un análisis de componentes principales y extraer nuevas columnas que sintetizen la información.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#PCA en columnas _C 01-14_\n\n#1) Normalizar ó Estandalizar los datos\nscaler=StandardScaler()#instantiate\nscaler.fit(c_df) # calcula la media y estandar para cada dimension\nX_scaled=scaler.transform(c_df)# transforma los datos a su nueva escala\n\n#2) Aplicando PCA\npca=PCA(n_components=14)\npca.fit(X_scaled) # buscar los componentes principales\nX_pca=pca.transform(X_scaled) \n#revisemos la forma del array\nprint(\"shape of X_pca\", X_pca.shape)\n\n#3) Comprobando variabilidad del PCA\nexpl = pca.explained_variance_ratio_\nprint(expl)\nprint('suma:',sum(expl[0:2]))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* El análisis de componente principales permite explicar el 97% de la variabilidad de los datos con tan solo 2 nuevas columnas (i.e. componentes).\n* Procedo a incluir estas dos componentes en la base de datos _Train sample_ y eliminar las columnas _C 01-14_.\n* Posteriormente replicaré este proceso en la base poblacional."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Incluyendo las dos nuevas componentes en columnas _C PCA1_ y _C PCA2_\nTrain_sample['C_PCA1'] = X_pca[:,0]\nTrain_sample['C_PCA2'] = X_pca[:,1]\n\n#Elimando las columnas C\nTrain_sample = Train_sample.drop([c for c in c_cols], axis=1)\n\nTrain_sample.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Replicando pasos anteriores en la base poblacional\n\n#PCA en columnas _C 01-14_ en df _Train_new2_\n\nc_df = Train_new2[c_cols]\n\n#1) Normalizar ó Estandalizar los datos\nscalerp=StandardScaler()#instantiate\nscalerp.fit(c_df) # calcula la media y estandar para cada dimension\nX_scaled=scalerp.transform(c_df)# transforma los datos a su nueva escala\n\n#2) Aplicando PCA\npca=PCA(n_components=14)\npca.fit(X_scaled) # buscar los componentes principales\nX_pca=pca.transform(X_scaled) \n#revisemos la forma del array\nprint(\"shape of X_pca\", X_pca.shape)\n\n#3) Comprobando variabilidad del PCA\nexpl = pca.explained_variance_ratio_\nprint(expl)\nprint('suma:',sum(expl[0:2]))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Incluyendo las dos nuevas componentes en columnas _C PCA1_ y _C PCA2_ en df _Train_new2_\nTrain_new2['C_PCA1'] = X_pca[:,0]\nTrain_new2['C_PCA2'] = X_pca[:,1]\n\n#Elimando las columnas C\nTrain_new2 = Train_new2.drop([c for c in c_cols], axis=1)\n\nTrain_new2.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### _V 01-339_\n* Conjunto de 169 variables provistas por Vesta referente a la transacción, como clasificación, conteo, entre otros.\n* Son numéricas, sin embargo su significando ha sido oculto.\n* Procedo a replicar los pasos de análisis de componente principales, así como para las variables _C_"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creando una dataframes con el conjunto de columnas _V_ para su exploración\nv_cols = [v for v in Train_sample if v[0] == 'V']\nv_df = Train_sample[v_cols]\nv_df.head(50)\nv_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#PCA en columnas _V 01-339_\n\n#1) Normalizar ó Estandalizar los datos\nscaler=StandardScaler()#instantiate\nscaler.fit(v_df) # calcula la media y estandar para cada dimension\nX_scaled=scaler.transform(v_df)# transforma los datos a su nueva escala\n\n#2) Aplicando PCA\npca=PCA(n_components=20)\npca.fit(X_scaled) # buscar los componentes principales\nX_pca=pca.transform(X_scaled) \n#revisemos la forma del array\nprint(\"shape of X_pca\", X_pca.shape)\n\n#3) Comprobando variabilidad del PCA\nexpl = pca.explained_variance_ratio_\nprint(expl)\nprint('suma:',sum(expl[0:20]))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* El análisis de componente principales permite explicar el 74% de la variabilidad de los datos con 20 nuevas columnas (i.e. componentes).\n* Procedo a incluir estas veinte componentes en la base de datos _Train sample_ y eliminar las columnas _V 01-33_.\n* Posteriormente replicaré este proceso en la base poblacional."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Incluyendo las 20 nuevas componentes en columnas _V PCA1_ al _V PCA20_ en df _Train_new2_\nTrain_sample['V_PCA1'] = X_pca[:,0]\nTrain_sample['V_PCA2'] = X_pca[:,1]\nTrain_sample['V_PCA3'] = X_pca[:,2]\nTrain_sample['V_PCA4'] = X_pca[:,3]\nTrain_sample['V_PCA5'] = X_pca[:,4]\nTrain_sample['V_PCA6'] = X_pca[:,5]\nTrain_sample['V_PCA7'] = X_pca[:,6]\nTrain_sample['V_PCA8'] = X_pca[:,7]\nTrain_sample['V_PCA9'] = X_pca[:,8]\nTrain_sample['V_PCA10'] = X_pca[:,9]\nTrain_sample['V_PCA11'] = X_pca[:,10]\nTrain_sample['V_PCA12'] = X_pca[:,11]\nTrain_sample['V_PCA13'] = X_pca[:,12]\nTrain_sample['V_PCA14'] = X_pca[:,13]\nTrain_sample['V_PCA15'] = X_pca[:,14]\nTrain_sample['V_PCA16'] = X_pca[:,15]\nTrain_sample['V_PCA17'] = X_pca[:,16]\nTrain_sample['V_PCA18'] = X_pca[:,17]\nTrain_sample['V_PCA19'] = X_pca[:,18]\nTrain_sample['V_PCA20'] = X_pca[:,19]\n\n#Elimando las columnas V\nTrain_sample = Train_sample.drop([v for v in v_cols], axis=1)\n\nTrain_sample.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Replicando pasos anteriores en la base poblacional\n\n#PCA en columnas _V 01-_339 en df _Train_new2_\n\nv_df = Train_new2[v_cols]\n\n#1) Normalizar ó Estandalizar los datos\nscalerp=StandardScaler()#instantiate\nscalerp.fit(v_df) # calcula la media y estandar para cada dimension\nX_scaled=scalerp.transform(v_df)# transforma los datos a su nueva escala\n\n#2) Aplicando PCA\npca=PCA(n_components=20)\npca.fit(X_scaled) # buscar los componentes principales\nX_pca=pca.transform(X_scaled) \n#revisemos la forma del array\nprint(\"shape of X_pca\", X_pca.shape)\n\n#3) Comprobando variabilidad del PCA\nexpl = pca.explained_variance_ratio_\nprint(expl)\nprint('suma:',sum(expl[0:20]))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Incluyendo las 20 nuevas componentes en columnas _V PCA1_ al _V PCA20_ en df _Train_new2_\nTrain_new2['V_PCA1'] = X_pca[:,0]\nTrain_new2['V_PCA2'] = X_pca[:,1]\nTrain_new2['V_PCA3'] = X_pca[:,2]\nTrain_new2['V_PCA4'] = X_pca[:,3]\nTrain_new2['V_PCA5'] = X_pca[:,4]\nTrain_new2['V_PCA6'] = X_pca[:,5]\nTrain_new2['V_PCA7'] = X_pca[:,6]\nTrain_new2['V_PCA8'] = X_pca[:,7]\nTrain_new2['V_PCA9'] = X_pca[:,8]\nTrain_new2['V_PCA10'] = X_pca[:,9]\nTrain_new2['V_PCA11'] = X_pca[:,10]\nTrain_new2['V_PCA12'] = X_pca[:,11]\nTrain_new2['V_PCA13'] = X_pca[:,12]\nTrain_new2['V_PCA14'] = X_pca[:,13]\nTrain_new2['V_PCA15'] = X_pca[:,14]\nTrain_new2['V_PCA16'] = X_pca[:,15]\nTrain_new2['V_PCA17'] = X_pca[:,16]\nTrain_new2['V_PCA18'] = X_pca[:,17]\nTrain_new2['V_PCA19'] = X_pca[:,18]\nTrain_new2['V_PCA20'] = X_pca[:,19]\n\n#Elimando las columnas V\nTrain_new2 = Train_new2.drop([v for v in v_cols], axis=1)\n\nTrain_new2.shape\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### _D1_ , _D4_ , _D10_ , _D15_\n* Variables numéricas que denotan tiempo, como tiempo transacurrido desde última transacción, entre otros.\n* Su significado real es oculto.\n* Exploraré indivualmente su comportamiento así como su relación con la variable objetivo _IsFraud_"},{"metadata":{"trusted":true},"cell_type":"code","source":"#D1\n#TransactionDT\nprint(Train_sample['D1'].describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Histograma de columna D1\nTrain_sample['D1'] \\\n    .plot(kind='hist',\n          bins=100,\n          figsize=(15, 5),\n          title='Distribución de D1 TransactionDT')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#relación con columna objetivo\nTrain_sample[['D1', 'isFraud']].groupby(['isFraud'], as_index=False).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('La media de D1 con IsFraud igual a 1 es: {:.4f}'.format(Train_sample.loc[Train_sample['isFraud'] == 1]['D1'].mean()))\nprint('La media de D1 con IsFraud igual a 0 es: {:.4f}'.format(Train_sample.loc[Train_sample['isFraud'] == 0]['D1'].mean()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Se observa una diferencia relevante en esta columnas entre transacciones fradulentas y no: para observaciones con fraude, D1 tiende a ser casi la mitad que cuando se trata de no fraude."},{"metadata":{"trusted":true},"cell_type":"code","source":"#D4\n#TransactionDT\nprint(Train_sample['D4'].describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Histograma de columna D4\nTrain_sample['D4'] \\\n    .plot(kind='hist',\n          bins=100,\n          figsize=(15, 5),\n          title='Distribución de D4')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#relación con columna objetivo\nTrain_sample[['D4', 'isFraud']].groupby(['isFraud'], as_index=False).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('La media de D4 con IsFraud igual a 1 es: {:.4f}'.format(Train_sample.loc[Train_sample['isFraud'] == 1]['D4'].mean()))\nprint('La media de D4 con IsFraud igual a 0 es: {:.4f}'.format(Train_sample.loc[Train_sample['isFraud'] == 0]['D4'].mean()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Al igual que con D1, esta columna muestra un comportamiento diferenciado respecto a los valores de la variable objetivo: tiende a ser 50% superior en transacciones no fradulentas."},{"metadata":{"trusted":true},"cell_type":"code","source":"#D10\n#TransactionDT\nprint(Train_sample['D10'].describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Histograma de columna D10\nTrain_sample['D10'] \\\n    .plot(kind='hist',\n          bins=100,\n          figsize=(15, 5),\n          title='Distribución de D10')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#relación con columna objetivo\nTrain_sample[['D10', 'isFraud']].groupby(['isFraud'], as_index=False).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('La media de D10 con IsFraud igual a 1 es: {:.4f}'.format(Train_sample.loc[Train_sample['isFraud'] == 1]['D10'].mean()))\nprint('La media de D10 con IsFraud igual a 0 es: {:.4f}'.format(Train_sample.loc[Train_sample['isFraud'] == 0]['D10'].mean()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Al igual que con D1 y D4, esta columna muestra un comportamiento diferenciado respecto a los valores de la variable objetivo: tiende a ser 50% superior en transacciones no fradulentas."},{"metadata":{"trusted":true},"cell_type":"code","source":"#D15\n#TransactionDT\nprint(Train_sample['D15'].describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Histograma de columna D15\nTrain_sample['D15'] \\\n    .plot(kind='hist',\n          bins=100,\n          figsize=(15, 5),\n          title='Distribución de D15')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#relación con columna objetivo\nTrain_sample[['D15', 'isFraud']].groupby(['isFraud'], as_index=False).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('La media de D15 con IsFraud igual a 1 es: {:.4f}'.format(Train_sample.loc[Train_sample['isFraud'] == 1]['D15'].mean()))\nprint('La media de D15 con IsFraud igual a 0 es: {:.4f}'.format(Train_sample.loc[Train_sample['isFraud'] == 0]['D15'].mean()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Distribución y relación con variable objetivo similar a las otras columnas D"},{"metadata":{},"cell_type":"markdown","source":"### _M6_\n* Variable categórica que denota si existe un match entre la información de compra."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Train_sample['M6'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* _M6_ toma valores 'F' (False) si no hay un match y 'T' (True) si hay un match\n* Procederé a sustituir las observaciones con 'T' por 1 y 'F' por 0, de modo de poder ver distribución y relación con columna objetivo"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Convirtiendo columna _M6_ en binaria, T == 1\nTrain_sample['M6'] = np.where(Train_sample['M6'] == 'T', 1, 0)\nprint(Train_sample['M6'].value_counts())\n\n#Replicando para la df poblacional\nTrain_new2['M6'] = np.where(Train_new2['M6'] == 'T', 1, 0)\nprint(Train_new2['M6'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_sample.groupby('M6') \\\n    .count()['TransactionID'] \\\n    .plot(kind='barh',\n          title='Distribución de M6',\n          figsize=(15, 3))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#relación con columna objetivo\n#fraude\nTrain_sample_fr1 = Train_sample.loc[Train_sample['isFraud'] == 1]\nTrain_sample_fr1.groupby('M6') \\\n    .count()['TransactionID'] \\\n    .plot(kind='barh',\n          title='Distribución de M6 con isFraud = 1',\n          figsize=(15, 3))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#No fraude\nTrain_sample_fr0 = Train_sample.loc[Train_sample['isFraud'] == 0]\nTrain_sample_fr0.groupby('M6') \\\n    .count()['TransactionID'] \\\n    .plot(kind='barh',\n          title='Distribución de M6 con isFraud = 0',\n          figsize=(15, 3))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* En línea con lo esperado, en las transacciones fradulentas la proporsión de observaciones en las que no hay un match en la información (respecto al total) tiende a ser notablemente superior que en aquellas transacciones no fradulentas.\n* Esta columna debería tener un caracter explicativo en el modelado de _IsFraud_"},{"metadata":{},"cell_type":"markdown","source":"# EDA Summary\n* El análisis exploratorio permitió en balance reducir la base de datos desde 202 columnas a tan solo 39, incluyendo las columnas categóricas factorizadas, lo que ayudará en el proceso de modelado y evaluación.\n* Se practicaron eliminación de datos vacios, nulos, transformaciones de las variables, análisis de componentes principales y factorización.\n* Se identificó también el riesgo de que la base de dato sea no balanceada: la variable objetivo incluye solo el 2% de las observaciones como fraude. Esto pudiera suponer problemas en el proceso de modelado.\n* Como paso adicional, procederé a realizar análisis de feature engeniring antes del modelado."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creo un backup de las bases de datos por resguardo antes de eliminar nuevas columnas.\nTrain_sample_bk = Train_sample\nTrain_new2_bk = Train_new2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_new2_bk.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Matriz de correlaciones"},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation_matrix = Train_sample.corr()\ncorrelation_matrix\n\nplt.figure(figsize=(23.0,23.0))\nplt.title('Matriz de Correlación de Pearson')\nsns.heatmap(correlation_matrix, annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* El análisis inicial sugiere que la variable objetivo se correlaciona más con el conjunto de columnas _D_, tipo de tarjeta (débito o crédito), y algunas componentes principales de las columnas _C_ y _V_\n* De igual manera, se puede observar que hay ciertas columnas que no parecen estar relacionadas con la variable objetivo y podría ser util extraerlas del database.\n* Se observará la matriz de correlación solo para el top 15 de las columnas más correlacionadas. "},{"metadata":{"trusted":true},"cell_type":"code","source":"corrmat = Train_sample.corr()\ncols_corrmat = corrmat['isFraud'].abs()\ncols_corrmat = cols_corrmat.sort_values(ascending=False)\ncols_corrmat.head(39)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* A partir de la columna 13, el coeficiente de correlación con _Is Fraud_ cae por debajo del 3%, en sentido directo o inversamente.\n* Se tomarán estas primeras 13 columnas para el proceso de modelado."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Guardando columnas finales en array para proceso de modelado\ncols = cols_corrmat.index\ncols = cols[0:14]\ncols\ndf_model = Train_new2[cols]\ndf_model.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ahora guardaré la base de dato original,muestral y la que se utilizará en el proceso de modelado en CSV indiduales."},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_new2.to_csv('Train_new2.csv',index=False)\nTrain_sample.to_csv('Train_sample.csv',index=False)\ndf_model.to_csv('df_model.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Modelos"},{"metadata":{},"cell_type":"markdown","source":"Continua en un notebook separado"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}