{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gc\nimport os\nimport random\nfrom typing import List, Tuple, Optional, Union\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\n\n\nfrom sklearn.model_selection import KFold\nfrom joblib import Parallel, delayed\nfrom sklearn.decomposition import PCA\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-05T09:40:19.378151Z","iopub.execute_input":"2022-03-05T09:40:19.378574Z","iopub.status.idle":"2022-03-05T09:40:21.675217Z","shell.execute_reply.started":"2022-03-05T09:40:19.378486Z","shell.execute_reply":"2022-03-05T09:40:21.674205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = pd.read_csv('../input/xgb-fraud-with-magic-0-9600/X_train.csv')\nX_test = pd.read_csv('../input/xgb-fraud-with-magic-0-9600/X_test.csv')\ny_train = pd.read_csv('../input/xgb-fraud-with-magic-0-9600/y_train.csv',header=None)[1]","metadata":{"execution":{"iopub.status.busy":"2022-03-05T09:46:23.419319Z","iopub.execute_input":"2022-03-05T09:46:23.419805Z","iopub.status.idle":"2022-03-05T09:46:34.196299Z","shell.execute_reply.started":"2022-03-05T09:46:23.419775Z","shell.execute_reply":"2022-03-05T09:46:34.193506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emb_dim: int = 50\nbatch_size: int = 1024\nmodel_type: str = 'mlp'\nmlp_dropout: float = 0.0\nmlp_hidden: int = 64\nmlp_bn: bool = False\ncnn_hidden: int = 128\ncnn_channel1: int = 32\ncnn_channel2: int = 32\ncnn_channel3: int = 32\ncnn_kernel1: int = 5\ncnn_celu: bool = False\ncnn_weight_norm: bool = False\ndropout_emb: bool = 0.0\nlr: float = 1e-3\nweight_decay: float = 0.0\nmodel_path: str = 'fold_{}.pth'\nscaler_type: str = 'standard'\noutput_dir: str = 'artifacts'\nscheduler_type: str = 'onecycle'\noptimizer_type: str = 'adam'\nmax_lr: float = 0.01\nepochs: int = 30\nseed: int = 42\nn_pca: int = -1\nbatch_double_freq: int = 50\ncnn_dropout: float = 0.1\nna_cols: bool = True\ncnn_leaky_relu: bool = False\npatience: int = 8\nfactor: float = 0.5","metadata":{"execution":{"iopub.status.busy":"2022-03-05T09:46:34.197658Z","iopub.status.idle":"2022-03-05T09:46:34.198139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NN_VALID_TH = 0.185\nNN_MODEL_TOP_N = 3\nTAB_MODEL_TOP_N = 3\nENSEMBLE_METHOD = 'mean'\nNN_NUM_MODELS = 10\nTABNET_NUM_MODELS = 5","metadata":{"execution":{"iopub.status.busy":"2022-03-05T09:46:34.199303Z","iopub.status.idle":"2022-03-05T09:46:34.199909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","metadata":{"execution":{"iopub.status.busy":"2022-03-05T09:46:34.200992Z","iopub.status.idle":"2022-03-05T09:46:34.201538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AverageMeter:\n    \"\"\"Computes and stores the average and current value\"\"\"\n\n    def __init__(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\nclass TabularDataset(Dataset):\n    def __init__(self, x_num: np.ndarray, x_cat: np.ndarray, y: Optional[np.ndarray]):\n        super().__init__()\n        self.x_num = x_num\n        self.x_cat = x_cat\n        self.y = y\n\n    def __len__(self):\n        return len(self.x_num)\n\n    def __getitem__(self, idx):\n        if self.y is None:\n            return self.x_num[idx], torch.LongTensor(self.x_cat[idx])\n        else:\n            return self.x_num[idx], torch.LongTensor(self.x_cat[idx]), self.y[idx]\n\n\nclass MLP(nn.Module):\n    def __init__(self,\n                 src_num_dim: int,\n                 n_categories: List[int],\n                 dropout: float = 0.0,\n                 hidden: int = 50,\n                 emb_dim: int = 10,\n                 dropout_cat: float = 0.2,\n                 bn: bool = False):\n        super().__init__()\n\n        self.embs = nn.ModuleList([\n            nn.Embedding(x, emb_dim) for x in n_categories])\n        self.cat_dim = emb_dim * len(n_categories)\n        self.dropout_cat = nn.Dropout(dropout_cat)\n\n        if bn:\n            self.sequence = nn.Sequential(\n                nn.Linear(src_num_dim + self.cat_dim, hidden),\n                nn.Dropout(dropout),\n                nn.BatchNorm1d(hidden),\n                nn.ReLU(),\n                nn.Linear(hidden, hidden),\n                nn.Dropout(dropout),\n                nn.BatchNorm1d(hidden),\n                nn.ReLU(),\n                nn.Linear(hidden, 1)\n            )\n        else:\n            self.sequence = nn.Sequential(\n                nn.Linear(src_num_dim + self.cat_dim, hidden),\n                nn.Dropout(dropout),\n                nn.ReLU(),\n                nn.Linear(hidden, hidden),\n                nn.Dropout(dropout),\n                nn.ReLU(),\n                nn.Linear(hidden, 1)\n            )\n\n    def forward(self, x_num, x_cat):\n        embs = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embs)]\n        x_cat_emb = self.dropout_cat(torch.cat(embs, 1))\n        x_all = torch.cat([x_num, x_cat_emb], 1)\n        x = self.sequence(x_all)\n        return torch.squeeze(x)\n\n\nclass CNN(nn.Module):\n    def __init__(self,\n                 num_features: int,\n                 hidden_size: int,\n                 n_categories: List[int],\n                 emb_dim: int = 10,\n                 dropout_cat: float = 0.2,\n                 channel_1: int = 256,\n                 channel_2: int = 512,\n                 channel_3: int = 512,\n                 dropout_top: float = 0.1,\n                 dropout_mid: float = 0.3,\n                 dropout_bottom: float = 0.2,\n                 weight_norm: bool = True,\n                 two_stage: bool = True,\n                 celu: bool = True,\n                 kernel1: int = 5,\n                 leaky_relu: bool = False):\n        super().__init__()\n\n        num_targets = 1\n\n        cha_1_reshape = int(hidden_size / channel_1)\n        cha_po_1 = int(hidden_size / channel_1 / 2)\n        cha_po_2 = int(hidden_size / channel_1 / 2 / 2) * channel_3\n\n        self.cat_dim = emb_dim * len(n_categories)\n        self.cha_1 = channel_1\n        self.cha_2 = channel_2\n        self.cha_3 = channel_3\n        self.cha_1_reshape = cha_1_reshape\n        self.cha_po_1 = cha_po_1\n        self.cha_po_2 = cha_po_2\n        self.two_stage = two_stage\n\n        self.expand = nn.Sequential(\n            nn.BatchNorm1d(num_features + self.cat_dim),\n            nn.Dropout(dropout_top),\n            nn.utils.weight_norm(nn.Linear(num_features + self.cat_dim, hidden_size), dim=None),\n            nn.CELU(0.06) if celu else nn.ReLU()\n        )\n\n        def _norm(layer, dim=None):\n            return nn.utils.weight_norm(layer, dim=dim) if weight_norm else layer\n\n        self.conv1 = nn.Sequential(\n            nn.BatchNorm1d(channel_1),\n            nn.Dropout(dropout_top),\n            _norm(nn.Conv1d(channel_1, channel_2, kernel_size=kernel1, stride=1, padding=kernel1 // 2, bias=False)),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool1d(output_size=cha_po_1),\n            nn.BatchNorm1d(channel_2),\n            nn.Dropout(dropout_top),\n            _norm(nn.Conv1d(channel_2, channel_2, kernel_size=3, stride=1, padding=1, bias=True)),\n            nn.ReLU()\n        )\n\n        if self.two_stage:\n            self.conv2 = nn.Sequential(\n                nn.BatchNorm1d(channel_2),\n                nn.Dropout(dropout_mid),\n                _norm(nn.Conv1d(channel_2, channel_2, kernel_size=3, stride=1, padding=1, bias=True)),\n                nn.ReLU(),\n                nn.BatchNorm1d(channel_2),\n                nn.Dropout(dropout_bottom),\n                _norm(nn.Conv1d(channel_2, channel_3, kernel_size=5, stride=1, padding=2, bias=True)),\n                nn.ReLU()\n            )\n\n        self.max_po_c2 = nn.MaxPool1d(kernel_size=4, stride=2, padding=1)\n\n        self.flt = nn.Flatten()\n\n        if leaky_relu:\n            self.dense = nn.Sequential(\n                nn.BatchNorm1d(cha_po_2),\n                nn.Dropout(dropout_bottom),\n                _norm(nn.Linear(cha_po_2, num_targets), dim=0),\n                nn.LeakyReLU()\n            )\n        else:\n            self.dense = nn.Sequential(\n                nn.BatchNorm1d(cha_po_2),\n                nn.Dropout(dropout_bottom),\n                _norm(nn.Linear(cha_po_2, num_targets), dim=0),\n#                 _norm(nn.Linear(cha_po_2, num_targets), dim=0)\n            )\n\n        self.embs = nn.ModuleList([nn.Embedding(x, emb_dim) for x in n_categories])\n        self.cat_dim = emb_dim * len(n_categories)\n        self.dropout_cat = nn.Dropout(dropout_cat)\n\n    def forward(self, x_num, x_cat):\n        embs = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embs)]\n        x_cat_emb = self.dropout_cat(torch.cat(embs, 1))\n        x = torch.cat([x_num, x_cat_emb], 1)\n\n        x = self.expand(x)\n\n\n        x = x.reshape(x.shape[0], self.cha_1, self.cha_1_reshape)\n\n        x = self.conv1(x)\n        if self.two_stage:\n            x = self.conv2(x) * x\n        x = self.max_po_c2(x)\n        x = self.flt(x)\n        x = self.dense(x)\n\n        return torch.squeeze(x)\n\n\n\ndef train_epoch(data_loader: DataLoader,\n                model: nn.Module,\n                optimizer,\n                scheduler,\n                device,\n                clip_grad: float = 1.5):\n    model.train()\n    losses = AverageMeter()\n    step = 0\n\n    for x_num, x_cat, y in tqdm(data_loader, position=0, leave=True, desc='Training'):\n        batch_size = x_num.size(0)\n        x_num = x_num.to(device, dtype=torch.float)\n        x_cat = x_cat.to(device)\n        y = y.to(device, dtype=torch.float)\n\n\n        target = model(x_num, x_cat).float()\n#         y = y.long()\n        loss = nn.BCEWithLogitsLoss()(target, y)\n        losses.update(loss.detach().cpu().numpy(), batch_size)\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n        optimizer.step()\n        optimizer.zero_grad()\n\n        if scheduler is not None:\n            scheduler.step()\n\n        step += 1\n\n    return losses.avg\n\n\ndef evaluate(data_loader: DataLoader, model, device):\n    model.eval()\n\n    losses = AverageMeter()\n\n    final_targets = []\n    final_outputs = []\n\n    with torch.no_grad():\n        for x_num, x_cat, y in data_loader:\n            batch_size = x_num.size(0)\n            x_num = x_num.to(device, dtype=torch.float)\n            x_cat = x_cat.to(device)\n            y = y.to(device, dtype=torch.float)\n\n            with torch.no_grad():\n                output = model(x_num, x_cat)\n            target = output.float()\n#             y = y.long()\n            loss = nn.BCEWithLogitsLoss()(target, y)\n            # record loss\n            losses.update(loss.detach().cpu().numpy(), batch_size)\n\n            targets = y.detach().cpu().numpy()\n            output = nn.Softmax(dim=0)(output).detach().cpu().numpy()\n\n            final_targets.append(targets)\n            final_outputs.append(output)\n\n    final_targets = np.concatenate(final_targets)\n    final_outputs = np.concatenate(final_outputs)\n\n\n    return final_outputs, final_targets, losses.avg\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-05T09:46:34.202591Z","iopub.status.idle":"2022-03-05T09:46:34.203116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fast_auc(y_true, y_prob):\n    y_true = np.asarray(y_true)\n    y_true = y_true[np.argsort(y_prob)]\n    nfalse = 0\n    auc = 0\n    n = len(y_true)\n    for i in range(n):\n        y_i = y_true[i]\n        nfalse += (1 - y_i)\n        auc += y_i * nfalse\n    auc /= (nfalse * (n - nfalse))\n    return auc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train['dummy_emb'] = np.random.randint(1)\nX_num = X_train[X_train.columns[~X_train.columns.isin(['dummy_emb'])]]\nscaler = StandardScaler()\nscaled_features = scaler.fit_transform(X_num.values)\nX_num = pd.DataFrame(scaled_features, index=X_num.index, columns=X_num.columns)","metadata":{"execution":{"iopub.status.busy":"2022-03-05T09:46:34.204225Z","iopub.status.idle":"2022-03-05T09:46:34.204672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\n\n\ngc.collect()\n\n\nfold_score = []\n\nkf = KFold(n_splits=10)\n\nfor fold, (train_idx, test_idx) in enumerate(kf.split(X_train)):\n    \n    X_tr = np.array(X_num.loc[train_idx])\n    X_tr_cat = np.array(X_train[['dummy_emb']].loc[train_idx])\n    y_tr = np.array(y_train.iloc[train_idx]).flatten()\n \n    X_va = np.array(X_num.loc[test_idx])\n    X_va_cat = np.array(X_train[['dummy_emb']].loc[test_idx])\n    y_va = np.array(y_train.iloc[test_idx]).flatten()\n\n    train_dataset = TabularDataset(X_tr, X_tr_cat, y_tr)\n    valid_dataset = TabularDataset(X_va, X_va_cat, y_va)\n    \n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n                                               num_workers=0)\n    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False,\n                                               num_workers=0)\n\n\n    model = CNN(X_tr.shape[1],\n                hidden_size=cnn_hidden,\n                n_categories=[200],\n                emb_dim=emb_dim,\n                dropout_cat=dropout_emb,\n                channel_1=cnn_channel1,\n                channel_2=cnn_channel2,\n                channel_3=cnn_channel3,\n                two_stage=False,\n                kernel1=cnn_kernel1,\n                celu=cnn_celu,\n                dropout_top=cnn_dropout,\n                dropout_mid=cnn_dropout,\n                dropout_bottom=cnn_dropout,\n                weight_norm=cnn_weight_norm,\n                leaky_relu=cnn_leaky_relu)    \n    \n    \n    model = model.to(device)\n\n    if optimizer_type == 'adamw':\n        opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    elif optimizer_type == 'adam':\n        opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n    else:\n        raise NotImplementedError()\n\n    scheduler = epoch_scheduler = None\n    if scheduler_type == 'onecycle':\n        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=opt, pct_start=0.1, div_factor=1e3,\n                                                        max_lr=max_lr, epochs=epochs,\n                                                        steps_per_epoch=len(train_loader))\n    elif scheduler_type == 'reduce':\n        epoch_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=opt,\n                                                                     mode='min',\n                                                                     min_lr=1e-7,\n                                                                     patience=patience,\n                                                                     verbose=True,\n                                                                     factor=factor)\n\n    for epoch in range(epochs):\n        if epoch > 0 and epoch % batch_double_freq == 0:\n            batch_size = batch_size * 2\n            print(f'batch: {cur_batch}')\n        train_loader = torch.utils.data.DataLoader(train_dataset,\n                                                   batch_size=batch_size,\n                                                   shuffle=True,\n                                                   num_workers=0)\n        train_loss = train_epoch(train_loader, model, opt, scheduler, device)\n        predictions, valid_targets, valid_loss = evaluate(valid_loader, model, device=device)\n        print(f\"epoch {epoch}, train loss: {train_loss:.3f}, valid loss: {valid_loss:.3f}\")\n    fold_score.append(fast_auc(valid_targets, predictions))\n    torch.save(model.state_dict(), 'model_fold' + str(fold) + '.pth')","metadata":{"execution":{"iopub.status.busy":"2022-03-05T09:46:34.206093Z","iopub.status.idle":"2022-03-05T09:46:34.206534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Cross Validation score = %1.6f' % np.mean(fold_score))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test['dummy_emb'] = np.random.randint(1)\nX_num = X_test[X_test.columns[~X_test.columns.isin(['dummy_emb'])]]\nscaled_features = scaler.transform(X_num.values)\nX_num = pd.DataFrame(scaled_features, index=X_num.index, columns=X_num.columns)\nX_num = np.array(X_num)\nX_cat = np.array(X_train[['dummy_emb']])\n\nvalid_dataset = TabularDataset(X_num, X_cat, None)\nvalid_loader = torch.utils.data.DataLoader(valid_dataset,\n                                           batch_size=512,\n                                           shuffle=False,\n                                           num_workers=4)","metadata":{"execution":{"iopub.status.busy":"2022-03-05T09:46:34.207549Z","iopub.status.idle":"2022-03-05T09:46:34.208073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nfinal_outputs = []\nfor fold in range(10):\n\n    \n    model = CNN(num_features=X_num.shape[1],\n                hidden_size=cnn_hidden,\n                n_categories=[200],\n                emb_dim=emb_dim,\n                dropout_cat=dropout_emb,\n                channel_1=cnn_channel1,\n                channel_2=cnn_channel2,\n                channel_3=cnn_channel3,\n                two_stage=False,\n                kernel1=cnn_kernel1,\n                celu=cnn_celu,\n                dropout_top=cnn_dropout,\n                dropout_mid=cnn_dropout,\n                dropout_bottom=cnn_dropout,\n                weight_norm=cnn_weight_norm,\n                leaky_relu=cnn_leaky_relu)\n    model.load_state_dict(torch.load('model_fold' + str(fold) + '.pth'))\n    model.eval()\n    \n    temp_outputs = []\n    with torch.no_grad():\n        for x_num, x_cat in tqdm(valid_loader, position=0, leave=True, desc='Evaluating'):\n            x_num = x_num.to(device, dtype=torch.float)\n            x_cat = x_cat.to(device)\n\n            outputs = []\n            with torch.no_grad():\n\n                output = model(x_num, x_cat)\n                outputs.append(nn.Softmax(dim=0)(output).detach().cpu().numpy())\n\n\n            pred = np.array(outputs).mean(axis=0)\n            temp_outputs.append(pred)\n    final_outputs.append(np.concatenate(temp_outputs))\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-05T09:46:34.209091Z","iopub.status.idle":"2022-03-05T09:46:34.209529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.read_csv('../input/ieee-fraud-detection/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-03-05T09:46:34.210577Z","iopub.status.idle":"2022-03-05T09:46:34.211072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df['isFraud'] = np.mean(final_outputs,axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-03-05T09:46:34.211994Z","iopub.status.idle":"2022-03-05T09:46:34.212484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-05T09:46:34.213561Z","iopub.status.idle":"2022-03-05T09:46:34.214072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# result = []\n\n# for fold in range(10):\n\n#     clf = xgb.XGBClassifier()\n#     clf.load_model('model_fold_' +str(fold)+ '.json')\n#     result.append(clf.predict_proba(X_test)[:,1])","metadata":{"execution":{"iopub.status.busy":"2021-10-16T11:30:53.71451Z","iopub.execute_input":"2021-10-16T11:30:53.715051Z","iopub.status.idle":"2021-10-16T11:30:59.682203Z","shell.execute_reply.started":"2021-10-16T11:30:53.714983Z","shell.execute_reply":"2021-10-16T11:30:59.681303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission_df = pd.read_csv('../input/ieee-fraud-detection/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-10-16T11:31:01.568869Z","iopub.execute_input":"2021-10-16T11:31:01.569244Z","iopub.status.idle":"2021-10-16T11:31:01.795472Z","shell.execute_reply.started":"2021-10-16T11:31:01.569211Z","shell.execute_reply":"2021-10-16T11:31:01.794336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission_df['isFraud'] = np.mean(result,axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-10-16T11:31:03.007887Z","iopub.execute_input":"2021-10-16T11:31:03.008285Z","iopub.status.idle":"2021-10-16T11:31:03.019048Z","shell.execute_reply.started":"2021-10-16T11:31:03.008249Z","shell.execute_reply":"2021-10-16T11:31:03.018155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission_df.to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}