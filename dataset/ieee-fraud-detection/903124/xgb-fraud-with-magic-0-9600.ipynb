{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Load Data\nWe will load all the data except 219 V columns that were determined redundant by correlation analysis [here][1]\n\n[1]: https://www.kaggle.com/cdeotte/eda-for-columns-v-and-id","metadata":{}},{"cell_type":"code","source":"BUILD95 = True\nBUILD96 = True\n\nimport numpy as np, pandas as pd, os, gc\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# COLUMNS WITH STRINGS\nstr_type = ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain','M1', 'M2', 'M3', 'M4','M5',\n            'M6', 'M7', 'M8', 'M9', 'id_12', 'id_15', 'id_16', 'id_23', 'id_27', 'id_28', 'id_29', 'id_30', \n            'id_31', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo']\nstr_type += ['id-12', 'id-15', 'id-16', 'id-23', 'id-27', 'id-28', 'id-29', 'id-30', \n            'id-31', 'id-33', 'id-34', 'id-35', 'id-36', 'id-37', 'id-38']\n\n# FIRST 53 COLUMNS\ncols = ['TransactionID', 'TransactionDT', 'TransactionAmt',\n       'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6',\n       'addr1', 'addr2', 'dist1', 'dist2', 'P_emaildomain', 'R_emaildomain',\n       'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11',\n       'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8',\n       'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'M1', 'M2', 'M3', 'M4',\n       'M5', 'M6', 'M7', 'M8', 'M9']\n\n# V COLUMNS TO LOAD DECIDED BY CORRELATION EDA\n# https://www.kaggle.com/cdeotte/eda-for-columns-v-and-id\nv =  [1, 3, 4, 6, 8, 11]\nv += [13, 14, 17, 20, 23, 26, 27, 30]\nv += [36, 37, 40, 41, 44, 47, 48]\nv += [54, 56, 59, 62, 65, 67, 68, 70]\nv += [76, 78, 80, 82, 86, 88, 89, 91]\n\n#v += [96, 98, 99, 104] #relates to groups, no NAN \nv += [107, 108, 111, 115, 117, 120, 121, 123] # maybe group, no NAN\nv += [124, 127, 129, 130, 136] # relates to groups, no NAN\n\n# LOTS OF NAN BELOW\nv += [138, 139, 142, 147, 156, 162] #b1\nv += [165, 160, 166] #b1\nv += [178, 176, 173, 182] #b2\nv += [187, 203, 205, 207, 215] #b2\nv += [169, 171, 175, 180, 185, 188, 198, 210, 209] #b2\nv += [218, 223, 224, 226, 228, 229, 235] #b3\nv += [240, 258, 257, 253, 252, 260, 261] #b3\nv += [264, 266, 267, 274, 277] #b3\nv += [220, 221, 234, 238, 250, 271] #b3\n\nv += [294, 284, 285, 286, 291, 297] # relates to grous, no NAN\nv += [303, 305, 307, 309, 310, 320] # relates to groups, no NAN\nv += [281, 283, 289, 296, 301, 314] # relates to groups, no NAN\n#v += [332, 325, 335, 338] # b4 lots NAN\n\ncols += ['V'+str(x) for x in v]\ndtypes = {}\nfor c in cols+['id_0'+str(x) for x in range(1,10)]+['id_'+str(x) for x in range(10,34)]+\\\n    ['id-0'+str(x) for x in range(1,10)]+['id-'+str(x) for x in range(10,34)]:\n        dtypes[c] = 'float32'\nfor c in str_type: dtypes[c] = 'category'","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-04T12:20:47.297346Z","iopub.execute_input":"2021-09-04T12:20:47.297771Z","iopub.status.idle":"2021-09-04T12:20:48.82699Z","shell.execute_reply.started":"2021-09-04T12:20:47.297729Z","shell.execute_reply":"2021-09-04T12:20:48.825934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# LOAD TRAIN\nX_train = pd.read_csv('../input/ieee-fraud-detection/train_transaction.csv',index_col='TransactionID', dtype=dtypes, usecols=cols+['isFraud'])\ntrain_id = pd.read_csv('../input/ieee-fraud-detection/train_identity.csv',index_col='TransactionID', dtype=dtypes)\nX_train = X_train.merge(train_id, how='left', left_index=True, right_index=True)\n# LOAD TEST\nX_test = pd.read_csv('../input/ieee-fraud-detection/test_transaction.csv',index_col='TransactionID', dtype=dtypes, usecols=cols)\ntest_id = pd.read_csv('../input/ieee-fraud-detection/test_identity.csv',index_col='TransactionID', dtype=dtypes)\nfix = {o:n for o, n in zip(test_id.columns, train_id.columns)}\ntest_id.rename(columns=fix, inplace=True)\nX_test = X_test.merge(test_id, how='left', left_index=True, right_index=True)\n# TARGET\ny_train = X_train['isFraud'].copy()\ndel train_id, test_id, X_train['isFraud']; x = gc.collect()\n# PRINT STATUS\nprint('Train shape',X_train.shape,'test shape',X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T12:20:48.828296Z","iopub.execute_input":"2021-09-04T12:20:48.828552Z","iopub.status.idle":"2021-09-04T12:21:30.589083Z","shell.execute_reply.started":"2021-09-04T12:20:48.828507Z","shell.execute_reply":"2021-09-04T12:21:30.588366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# NORMALIZE D COLUMNS\nfor i in range(1,16):\n    if i in [1,2,3,5,9]: continue\n    X_train['D'+str(i)] =  X_train['D'+str(i)] - X_train.TransactionDT/np.float32(24*60*60)\n    X_test['D'+str(i)] = X_test['D'+str(i)] - X_test.TransactionDT/np.float32(24*60*60) ","metadata":{"execution":{"iopub.status.busy":"2021-09-04T12:21:30.590069Z","iopub.execute_input":"2021-09-04T12:21:30.590415Z","iopub.status.idle":"2021-09-04T12:21:30.67355Z","shell.execute_reply.started":"2021-09-04T12:21:30.590378Z","shell.execute_reply":"2021-09-04T12:21:30.6726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# LABEL ENCODE AND MEMORY REDUCE\nfor i,f in enumerate(X_train.columns):\n    # FACTORIZE CATEGORICAL VARIABLES\n    if (np.str(X_train[f].dtype)=='category')|(X_train[f].dtype=='object'): \n        df_comb = pd.concat([X_train[f],X_test[f]],axis=0)\n        df_comb,_ = df_comb.factorize(sort=True)\n        if df_comb.max()>32000: print(f,'needs int32')\n        X_train[f] = df_comb[:len(X_train)].astype('int16')\n        X_test[f] = df_comb[len(X_train):].astype('int16')\n    # SHIFT ALL NUMERICS POSITIVE. SET NAN to -1\n    elif f not in ['TransactionAmt','TransactionDT']:\n        mn = np.min((X_train[f].min(),X_test[f].min()))\n        X_train[f] -= np.float32(mn)\n        X_test[f] -= np.float32(mn)\n        X_train[f].fillna(-1,inplace=True)\n        X_test[f].fillna(-1,inplace=True)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-04T12:21:30.675164Z","iopub.execute_input":"2021-09-04T12:21:30.67556Z","iopub.status.idle":"2021-09-04T12:21:33.993842Z","shell.execute_reply.started":"2021-09-04T12:21:30.675485Z","shell.execute_reply":"2021-09-04T12:21:33.992858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# FREQUENCY ENCODE TOGETHER\ndef encode_FE(df1, df2, cols):\n    for col in cols:\n        df = pd.concat([df1[col],df2[col]])\n        vc = df.value_counts(dropna=True, normalize=True).to_dict()\n        vc[-1] = -1\n        nm = col+'_FE'\n        df1[nm] = df1[col].map(vc)\n        df1[nm] = df1[nm].astype('float32')\n        df2[nm] = df2[col].map(vc)\n        df2[nm] = df2[nm].astype('float32')\n        print(nm,', ',end='')\n        \n# LABEL ENCODE\ndef encode_LE(col,train=X_train,test=X_test,verbose=True):\n    df_comb = pd.concat([train[col],test[col]],axis=0)\n    df_comb,_ = df_comb.factorize(sort=True)\n    nm = col\n    if df_comb.max()>32000: \n        train[nm] = df_comb[:len(train)].astype('int32')\n        test[nm] = df_comb[len(train):].astype('int32')\n    else:\n        train[nm] = df_comb[:len(train)].astype('int16')\n        test[nm] = df_comb[len(train):].astype('int16')\n    del df_comb; x=gc.collect()\n    if verbose: print(nm,', ',end='')\n        \n# GROUP AGGREGATION MEAN AND STD\n# https://www.kaggle.com/kyakovlev/ieee-fe-with-some-eda\ndef encode_AG(main_columns, uids, aggregations=['mean'], train_df=X_train, test_df=X_test, \n              fillna=True, usena=False):\n    # AGGREGATION OF MAIN WITH UID FOR GIVEN STATISTICS\n    for main_column in main_columns:  \n        for col in uids:\n            for agg_type in aggregations:\n                new_col_name = main_column+'_'+col+'_'+agg_type\n                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n                if usena: temp_df.loc[temp_df[main_column]==-1,main_column] = np.nan\n                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n                                                        columns={agg_type: new_col_name})\n\n                temp_df.index = list(temp_df[col])\n                temp_df = temp_df[new_col_name].to_dict()   \n\n                train_df[new_col_name] = train_df[col].map(temp_df).astype('float32')\n                test_df[new_col_name]  = test_df[col].map(temp_df).astype('float32')\n                \n                if fillna:\n                    train_df[new_col_name].fillna(-1,inplace=True)\n                    test_df[new_col_name].fillna(-1,inplace=True)\n                \n                print(\"'\"+new_col_name+\"'\",', ',end='')\n                \n# COMBINE FEATURES\ndef encode_CB(col1,col2,df1=X_train,df2=X_test):\n    nm = col1+'_'+col2\n    df1[nm] = df1[col1].astype(str)+'_'+df1[col2].astype(str)\n    df2[nm] = df2[col1].astype(str)+'_'+df2[col2].astype(str) \n    encode_LE(nm,verbose=False)\n    print(nm,', ',end='')\n    \n# GROUP AGGREGATION NUNIQUE\ndef encode_AG2(main_columns, uids, train_df=X_train, test_df=X_test):\n    for main_column in main_columns:  \n        for col in uids:\n            comb = pd.concat([train_df[[col]+[main_column]],test_df[[col]+[main_column]]],axis=0)\n            mp = comb.groupby(col)[main_column].agg(['nunique'])['nunique'].to_dict()\n            train_df[col+'_'+main_column+'_ct'] = train_df[col].map(mp).astype('float32')\n            test_df[col+'_'+main_column+'_ct'] = test_df[col].map(mp).astype('float32')\n            print(col+'_'+main_column+'_ct, ',end='')","metadata":{"execution":{"iopub.status.busy":"2021-09-04T12:21:33.997629Z","iopub.execute_input":"2021-09-04T12:21:33.998063Z","iopub.status.idle":"2021-09-04T12:21:34.021062Z","shell.execute_reply.started":"2021-09-04T12:21:33.997997Z","shell.execute_reply":"2021-09-04T12:21:34.019793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# TRANSACTION AMT CENTS\nX_train['cents'] = (X_train['TransactionAmt'] - np.floor(X_train['TransactionAmt'])).astype('float32')\nX_test['cents'] = (X_test['TransactionAmt'] - np.floor(X_test['TransactionAmt'])).astype('float32')\nprint('cents, ', end='')\n# FREQUENCY ENCODE: ADDR1, CARD1, CARD2, CARD3, P_EMAILDOMAIN\nencode_FE(X_train,X_test,['addr1','card1','card2','card3','P_emaildomain'])\n# COMBINE COLUMNS CARD1+ADDR1, CARD1+ADDR1+P_EMAILDOMAIN\nencode_CB('card1','addr1')\nencode_CB('card1_addr1','P_emaildomain')\n# FREQUENCY ENOCDE\nencode_FE(X_train,X_test,['card1_addr1','card1_addr1_P_emaildomain'])\n# GROUP AGGREGATE\nencode_AG(['TransactionAmt','D9','D11'],['card1','card1_addr1','card1_addr1_P_emaildomain'],['mean','std'],usena=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T12:21:34.022463Z","iopub.execute_input":"2021-09-04T12:21:34.022701Z","iopub.status.idle":"2021-09-04T12:22:01.132853Z","shell.execute_reply.started":"2021-09-04T12:21:34.022663Z","shell.execute_reply":"2021-09-04T12:22:01.131059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Selection - Time Consistency\nWe added 28 new feature above. We have already removed 219 V Columns from correlation analysis done [here][1]. So we currently have 242 features now. We will now check each of our 242 for \"time consistency\". We will build 242 models. Each model will be trained on the first month of the training data and will only use one feature. We will then predict the last month of the training data. We want both training AUC and validation AUC to be above `AUC = 0.5`. It turns out that 19 features fail this test so we will remove them. Additionally we will remove 7 D columns that are mostly NAN. More techniques for feature selection are listed [here][2]\n\n[1]: https://www.kaggle.com/cdeotte/eda-for-columns-v-and-id\n[2]: https://www.kaggle.com/c/ieee-fraud-detection/discussion/111308","metadata":{}},{"cell_type":"code","source":"cols = list( X_train.columns )\ncols.remove('TransactionDT')\nfor c in ['D6','D7','D8','D9','D12','D13','D14']:\n    cols.remove(c)\n    \n# FAILED TIME CONSISTENCY TEST\nfor c in ['C3','M5','id_08','id_33']:\n    cols.remove(c)\nfor c in ['card4','id_07','id_14','id_21','id_30','id_32','id_34']:\n    cols.remove(c)\nfor c in ['id_'+str(x) for x in range(22,28)]:\n    cols.remove(c)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T12:22:01.134392Z","iopub.execute_input":"2021-09-04T12:22:01.134685Z","iopub.status.idle":"2021-09-04T12:22:01.141726Z","shell.execute_reply.started":"2021-09-04T12:22:01.134623Z","shell.execute_reply":"2021-09-04T12:22:01.14079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('NOW USING THE FOLLOWING',len(cols),'FEATURES.')\nnp.array(cols)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T12:22:01.143818Z","iopub.execute_input":"2021-09-04T12:22:01.144288Z","iopub.status.idle":"2021-09-04T12:22:01.158255Z","shell.execute_reply.started":"2021-09-04T12:22:01.144222Z","shell.execute_reply":"2021-09-04T12:22:01.157452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_train.to_csv('X_train.csv')\n# y_train.to_csv('y_train.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-30T06:50:01.632543Z","iopub.execute_input":"2021-08-30T06:50:01.632968Z","iopub.status.idle":"2021-08-30T06:50:01.643722Z","shell.execute_reply.started":"2021-08-30T06:50:01.632911Z","shell.execute_reply":"2021-08-30T06:50:01.642619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import xgboost as xgb\n# print(\"XGBoost version:\", xgb.__version__)\n\n# if BUILD95:\n#     clf = xgb.XGBClassifier( \n#         n_estimators=2000,\n#         max_depth=12, \n#         learning_rate=0.02, \n#         subsample=0.8,\n#         colsample_bytree=0.4, \n#         missing=-1, \n#         eval_metric='auc',\n#         # USE CPU\n#         #nthread=4,\n#         #tree_method='hist' \n#         # USE GPU\n#         tree_method='gpu_hist' \n#     )\n#     h = clf.fit(X_train.loc[idxT,cols], y_train[idxT], \n#         eval_set=[(X_train.loc[idxV,cols],y_train[idxV])],\n#         verbose=50, early_stopping_rounds=100)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T06:50:01.645204Z","iopub.execute_input":"2021-08-30T06:50:01.645577Z","iopub.status.idle":"2021-08-30T06:50:01.656266Z","shell.execute_reply.started":"2021-08-30T06:50:01.64552Z","shell.execute_reply":"2021-08-30T06:50:01.655211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if BUILD95:\n\n#     feature_imp = pd.DataFrame(sorted(zip(clf.feature_importances_,cols)), columns=['Value','Feature'])\n#     plt.figure(figsize=(20, 10))\n#     sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).iloc[:50])\n#     plt.title('XGB95 Most Important Features')\n#     plt.tight_layout()\n#     plt.show()\n#     del clf, h; x=gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-30T06:50:01.658274Z","iopub.execute_input":"2021-08-30T06:50:01.658711Z","iopub.status.idle":"2021-08-30T06:50:01.66855Z","shell.execute_reply.started":"2021-08-30T06:50:01.658635Z","shell.execute_reply":"2021-08-30T06:50:01.667566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict test.csv\nWe will predict `test.csv` using GroupKFold with months as groups. The training data are the months December 2017, January 2018, February 2018, March 2018, April 2018, and May 2018. We refer to these months as 12, 13, 14, 15, 16, 17. Fold one in GroupKFold will train on months 13 thru 17 and predict month 12. Note that the only purpose of month 12 is to tell XGB when to `early_stop` we don't actual care about the backwards time predictions. The model trained on months 13 thru 17 will also predict `test.csv` which is forward in time.\n  \nNote that we use local validation to determine features but GroupKFold to predict `test.csv`. Many other prediction schemes were tried but GroupKFold performed best.","metadata":{}},{"cell_type":"code","source":"import datetime\nSTART_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')\nX_train['DT_M'] = X_train['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\nX_train['DT_M'] = (X_train['DT_M'].dt.year-2017)*12 + X_train['DT_M'].dt.month \n\nX_test['DT_M'] = X_test['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\nX_test['DT_M'] = (X_test['DT_M'].dt.year-2017)*12 + X_test['DT_M'].dt.month ","metadata":{"execution":{"iopub.status.busy":"2021-09-04T12:22:01.159583Z","iopub.execute_input":"2021-09-04T12:22:01.159857Z","iopub.status.idle":"2021-09-04T12:22:02.676877Z","shell.execute_reply.started":"2021-09-04T12:22:01.159806Z","shell.execute_reply":"2021-09-04T12:22:02.676058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if BUILD95:\n#     oof = np.zeros(len(X_train))\n#     preds = np.zeros(len(X_test))\n\n#     skf = GroupKFold(n_splits=6)\n#     for i, (idxT, idxV) in enumerate( skf.split(X_train, y_train, groups=X_train['DT_M']) ):\n#         month = X_train.iloc[idxV]['DT_M'].iloc[0]\n#         print('Fold',i,'withholding month',month)\n#         print(' rows of train =',len(idxT),'rows of holdout =',len(idxV))\n#         clf = xgb.XGBClassifier(\n#             n_estimators=5000,\n#             max_depth=12,\n#             learning_rate=0.02,\n#             subsample=0.8,\n#             colsample_bytree=0.4,\n#             missing=-1,\n#             eval_metric='auc',\n#             # USE CPU\n#             #nthread=4,\n#             #tree_method='hist'\n#             # USE GPU\n#             tree_method='gpu_hist' \n#         )        \n#         h = clf.fit(X_train[cols].iloc[idxT], y_train.iloc[idxT], \n#                 eval_set=[(X_train[cols].iloc[idxV],y_train.iloc[idxV])],\n#                 verbose=100, early_stopping_rounds=200)\n    \n#         oof[idxV] += clf.predict_proba(X_train[cols].iloc[idxV])[:,1]\n#         preds += clf.predict_proba(X_test[cols])[:,1]/skf.n_splits\n#         del h, clf\n#         x=gc.collect()\n#     print('#'*20)\n#     print ('XGB95 OOF CV=',roc_auc_score(y_train,oof))","metadata":{"execution":{"iopub.status.busy":"2021-08-30T06:50:04.159504Z","iopub.execute_input":"2021-08-30T06:50:04.159884Z","iopub.status.idle":"2021-08-30T06:50:04.164826Z","shell.execute_reply.started":"2021-08-30T06:50:04.159785Z","shell.execute_reply":"2021-08-30T06:50:04.163761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if BUILD95:\n#     plt.hist(oof,bins=100)\n#     plt.ylim((0,5000))\n#     plt.title('XGB OOF')\n#     plt.show()\n\n#     X_train['oof'] = oof\n#     X_train.reset_index(inplace=True)\n#     X_train[['TransactionID','oof']].to_csv('oof_xgb_95.csv')\n#     X_train.set_index('TransactionID',drop=True,inplace=True)\n    \n# else: X_train['oof'] = 0","metadata":{"execution":{"iopub.status.busy":"2021-08-30T06:50:04.166658Z","iopub.execute_input":"2021-08-30T06:50:04.166949Z","iopub.status.idle":"2021-08-30T06:50:04.177271Z","shell.execute_reply.started":"2021-08-30T06:50:04.166901Z","shell.execute_reply":"2021-08-30T06:50:04.176022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train['oof'] = 0","metadata":{"execution":{"iopub.status.busy":"2021-09-04T12:22:02.678517Z","iopub.execute_input":"2021-09-04T12:22:02.678842Z","iopub.status.idle":"2021-09-04T12:22:02.685672Z","shell.execute_reply.started":"2021-09-04T12:22:02.678788Z","shell.execute_reply":"2021-09-04T12:22:02.684264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The Magic Feature - UID\nWe will now create and use the MAGIC FEATURES. First we create a UID which will help our model find clients (credit cards). This UID isn't perfect. Many UID values contain 2 or more clients inside. However our model will detect this and by adding more splits with its trees, it will split these UIDs and find the single clients (credit cards).","metadata":{}},{"cell_type":"code","source":"X_train['day'] = X_train.TransactionDT / (24*60*60)\nX_train['uid'] = X_train.card1_addr1.astype(str)+'_'+np.floor(X_train.day-X_train.D1).astype(str)\n\nX_test['day'] = X_test.TransactionDT / (24*60*60)\nX_test['uid'] = X_test.card1_addr1.astype(str)+'_'+np.floor(X_test.day-X_test.D1).astype(str)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T12:22:02.687051Z","iopub.execute_input":"2021-09-04T12:22:02.687338Z","iopub.status.idle":"2021-09-04T12:22:04.21041Z","shell.execute_reply.started":"2021-09-04T12:22:02.687288Z","shell.execute_reply":"2021-09-04T12:22:04.209503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Group Aggregation Features\nFor our model to use the new UID, we need to make lots of aggregated group features. We will add 47 new features! The pictures in the introduction to this notebook explain why this works. Note that after aggregation, we remove UID from our model. We don't use UID directly.","metadata":{}},{"cell_type":"code","source":"%%time\n# FREQUENCY ENCODE UID\nencode_FE(X_train,X_test,['uid'])\n# AGGREGATE \nencode_AG(['TransactionAmt','D4','D9','D10','D15'],['uid'],['mean','std'],fillna=True,usena=True)\n# AGGREGATE\nencode_AG(['C'+str(x) for x in range(1,15) if x!=3],['uid'],['mean'],X_train,X_test,fillna=True,usena=True)\n# AGGREGATE\nencode_AG(['M'+str(x) for x in range(1,10)],['uid'],['mean'],fillna=True,usena=True)\n# AGGREGATE\nencode_AG2(['P_emaildomain','dist1','DT_M','id_02','cents'], ['uid'], train_df=X_train, test_df=X_test)\n# AGGREGATE\nencode_AG(['C14'],['uid'],['std'],X_train,X_test,fillna=True,usena=True)\n# AGGREGATE \nencode_AG2(['C13','V314'], ['uid'], train_df=X_train, test_df=X_test)\n# AGGREATE \nencode_AG2(['V127','V136','V309','V307','V320'], ['uid'], train_df=X_train, test_df=X_test)\n# NEW FEATURE\nX_train['outsider15'] = (np.abs(X_train.D1-X_train.D15)>3).astype('int8')\nX_test['outsider15'] = (np.abs(X_test.D1-X_test.D15)>3).astype('int8')\nprint('outsider15')","metadata":{"execution":{"iopub.status.busy":"2021-09-04T12:22:04.211744Z","iopub.execute_input":"2021-09-04T12:22:04.212013Z","iopub.status.idle":"2021-09-04T12:25:37.879659Z","shell.execute_reply.started":"2021-09-04T12:22:04.211969Z","shell.execute_reply":"2021-09-04T12:25:37.878686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = list( X_train.columns )\ncols.remove('TransactionDT')\nfor c in ['D6','D7','D8','D9','D12','D13','D14']:\n    cols.remove(c)\nfor c in ['oof','DT_M','day','uid']:\n    cols.remove(c)\n    \n# FAILED TIME CONSISTENCY TEST\nfor c in ['C3','M5','id_08','id_33']:\n    cols.remove(c)\nfor c in ['card4','id_07','id_14','id_21','id_30','id_32','id_34']:\n    cols.remove(c)\nfor c in ['id_'+str(x) for x in range(22,28)]:\n    cols.remove(c)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-04T12:25:37.881239Z","iopub.execute_input":"2021-09-04T12:25:37.881601Z","iopub.status.idle":"2021-09-04T12:25:37.889111Z","shell.execute_reply.started":"2021-09-04T12:25:37.881533Z","shell.execute_reply":"2021-09-04T12:25:37.888301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('NOW USING THE FOLLOWING',len(cols),'FEATURES.')\nnp.array(cols)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T12:25:37.890798Z","iopub.execute_input":"2021-09-04T12:25:37.89117Z","iopub.status.idle":"2021-09-04T12:25:37.907164Z","shell.execute_reply.started":"2021-09-04T12:25:37.891095Z","shell.execute_reply":"2021-09-04T12:25:37.906418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = X_train[cols]\nX_test = X_test[cols]","metadata":{"execution":{"iopub.status.busy":"2021-09-04T12:27:13.679675Z","iopub.execute_input":"2021-09-04T12:27:13.680042Z","iopub.status.idle":"2021-09-04T12:27:14.460025Z","shell.execute_reply.started":"2021-09-04T12:27:13.679984Z","shell.execute_reply":"2021-09-04T12:27:14.458836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.to_csv('X_train.csv')\nX_test.to_csv('X_test.csv')\ny_train.to_csv('y_train.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-30T06:45:35.73145Z","iopub.status.idle":"2021-08-30T06:45:35.732079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if BUILD96:\n#     clf = xgb.XGBClassifier( \n#         n_estimators=2000,\n#         max_depth=12, \n#         learning_rate=0.02, \n#         subsample=0.8,\n#         colsample_bytree=0.4, \n#         missing=-1, \n#         eval_metric='auc',\n#         #nthread=4,\n#         #tree_method='hist' \n#         tree_method='gpu_hist' \n#     )\n#     h = clf.fit(X_train.loc[idxT,cols], y_train[idxT], \n#         eval_set=[(X_train.loc[idxV,cols],y_train[idxV])],\n#         verbose=50, early_stopping_rounds=100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if BUILD96:\n\n#     feature_imp = pd.DataFrame(sorted(zip(clf.feature_importances_,cols)), columns=['Value','Feature'])\n\n#     plt.figure(figsize=(20, 10))\n#     sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).iloc[:50])\n#     plt.title('XGB96 Most Important')\n#     plt.tight_layout()\n#     plt.show()\n        \n#     del clf, h; x=gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if BUILD96:\n#     oof = np.zeros(len(X_train))\n#     preds = np.zeros(len(X_test))\n\n#     skf = GroupKFold(n_splits=6)\n#     for i, (idxT, idxV) in enumerate( skf.split(X_train, y_train, groups=X_train['DT_M']) ):\n#         month = X_train.iloc[idxV]['DT_M'].iloc[0]\n#         print('Fold',i,'withholding month',month)\n#         print(' rows of train =',len(idxT),'rows of holdout =',len(idxV))\n#         clf = xgb.XGBClassifier(\n#             n_estimators=5000,\n#             max_depth=12,\n#             learning_rate=0.02,\n#             subsample=0.8,\n#             colsample_bytree=0.4,\n#             missing=-1,\n#             eval_metric='auc',\n#             # USE CPU\n#             #nthread=4,\n#             #tree_method='hist'\n#             # USE GPU\n#             tree_method='gpu_hist' \n#         )        \n#         h = clf.fit(X_train[cols].iloc[idxT], y_train.iloc[idxT], \n#                 eval_set=[(X_train[cols].iloc[idxV],y_train.iloc[idxV])],\n#                 verbose=100, early_stopping_rounds=200)\n    \n#         oof[idxV] += clf.predict_proba(X_train[cols].iloc[idxV])[:,1]\n#         preds += clf.predict_proba(X_test[cols])[:,1]/skf.n_splits\n#         del h, clf\n#         x=gc.collect()\n#     print('#'*20)\n#     print ('XGB96 OOF CV=',roc_auc_score(y_train,oof))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if BUILD96:\n#     plt.hist(oof,bins=100)\n#     plt.ylim((0,5000))\n#     plt.title('XGB OOF')\n#     plt.show()\n\n#     X_train['oof'] = oof\n#     X_train.reset_index(inplace=True)\n#     X_train[['TransactionID','oof']].to_csv('oof_xgb_96.csv')\n#     X_train.set_index('TransactionID',drop=True,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}