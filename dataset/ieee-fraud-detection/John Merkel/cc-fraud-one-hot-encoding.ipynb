{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to [Competition Discussion](http://https://www.kaggle.com/c/ieee-fraud-detection/discussion/101203#latest-643955) the Categorical Features are:   \n* ProductCD   \n* card1 - card6 \n* addr1, addr2\n* Pemaildomain \n* Remaildomain  \n* M1 - M9     \n* DeviceType\n* DeviceInfo  \n* id12 - id38    \nHowever, some of these look like they have been preprocessed into a numerical values. \nThese columns are:  \n* card1 - card3\n* addr1, addr2\n* id32   "},{"metadata":{},"cell_type":"markdown","source":"## Load Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Path to data files\npath = '../input/ieee-fraud-detection/'\n\n# Load transaction data\ntrain_transaction = pd.read_csv(path + 'train_transaction.csv', index_col='TransactionID')\ntest_transaction = pd.read_csv(path + 'test_transaction.csv', index_col='TransactionID')\n\n# Load identity data\ntrain_identity = pd.read_csv(path + 'train_identity.csv', index_col='TransactionID')\ntest_identity = pd.read_csv(path + 'test_identity.csv', index_col='TransactionID')\n\n# These merges will keep the data in the left dataframe. Data from the right \n# dataframe will only be kept if the index appears in the left dataframe.\ntrn = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntst = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n\n# Delete dataframes no longer needed to free up RAM\ndel train_transaction, train_identity, test_transaction, test_identity","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Process Data"},{"metadata":{},"cell_type":"markdown","source":"Drop columns that are missing \"large\" amounts of data.   \nThis seems to help predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parameters\npct_thresh = 0.2\n\nnumRows = trn.shape[0]\ncol_list = trn.columns.values.tolist()\n\n# Initialize list to hold cols to drop\ndrop_cols = []\nfor col in col_list:\n    missing_ratio = trn[trn[col].isnull()].shape[0] / numRows\n    if missing_ratio > pct_thresh:\n        drop_cols.append(col)\n        \n# Drop columns\nfor col in drop_cols:\n    col_list.remove(col)\n    \nprint(len(drop_cols))\n#print(drop_cols)\n\n# Update train and test dataframes.\ntrn = trn[col_list]\ntst = tst[col_list[1:]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Process categorical variables"},{"metadata":{},"cell_type":"markdown","source":"'ProductCD' has an 'S' category in the testing data that does not appear in the training data, so let's skip it for now.   \n'P_emaildomain' is complicated. Could probably create a new category 'other' then throw the odds and ends in there, but let's skip it for now."},{"metadata":{"trusted":true},"cell_type":"code","source":"# 'ProductCD' and 'P_emaildomain' will take a little more massaging to process. \n# For now let's drop them. We can come back to them later.\ntrn = trn.drop(['ProductCD', 'P_emaildomain'], axis=1)\ntst = tst.drop(['ProductCD', 'P_emaildomain'], axis=1)\n\n# Replace missing entries (NaN) with string 'missing'\ntrn.loc[trn['card4'].isnull(), ['card4']] = 'missing4'\ntrn.loc[trn['card6'].isnull(), ['card6']] = 'missing6'\ntst.loc[tst['card4'].isnull(), ['card4']] = 'missing4'\ntst.loc[tst['card6'].isnull(), ['card6']] = 'missing6'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# trn['card6'] contains the category 'debit or credit'. \n# As these are the only two possiblities, this info is not helpful. \n# Change 'debit or credit' to 'missing'\n# tst['card6'] does not have this category\ntrn.loc[trn['card6'] == 'debit or credit', ['card6']] = 'missing6'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'card4' has the same categories in both the testing and training sets.   \n'card6' has a few 'charge card' entries, but that's just a credit card, so let's change that to the laeger existing category 'credit'. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's change 'charge card' entry in the 'card6' field to 'credit'\ntrn.loc[trn['card6'] == 'charge card', ['card6']] = 'credit'\ntst.loc[tst['card6'] == 'charge card', ['card6']] = 'credit'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#trn['card6'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Apply One-hot encoding to categorical vars <br>\n "},{"metadata":{},"cell_type":"markdown","source":"Here I'll try pd.get_dummies. It has the advantage that I don't need to first apply label encoding"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Get list of categorical variables\ncat_cols = (trn.dtypes == 'object')\ncat_cols = list(cat_cols[cat_cols].index)\n\nfor col in cat_cols:\n    # Process training data\n    dummy = pd.get_dummies(trn[col])\n    \n    # I'm doing this so I can concatinate. Maybe not necessary?\n    dummy.index = trn.index\n    trn = pd.concat([trn, dummy], axis = 1)\n    \n    # Now process test data\n    dummy = pd.get_dummies(tst[col])\n    dummy.index = tst.index\n    tst = pd.concat([tst, dummy], axis = 1)\n\n# Drop categorical columns\ntrn = trn.drop(cat_cols, axis=1)\ntst = tst.drop(cat_cols, axis=1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#trn.head()\n#trn.loc[trn['debit or credit'] == 1]\n#ts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One source suggested it is necessary to perform label encoding prior to one-hot encoding. Apparently one-hot encoding can't convert categorical data to numbers. It only converts numbers to binary. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\n# Get list of categorical variables\ncat_cols = (trn.dtypes == 'object')\ncat_cols = list(cat_cols[cat_cols].index)\nprint(cat_cols)\n\n# Apply one-hot encoder to each column with categorical data\n# handle_unknows = 'ignore' will result in a row of 0s when fitting and a \n# previously unseen category is encountered.\n# sparse = 'False' places output in a matrix (I think)\nhot1_encoder = OneHotEncoder(handle_unknown='ignore', sparse='False')\nhot1_cols_trn = pd.DataFrame(hot1_encoder.fit_transform(trn[cat_cols]).toarray())\nhot1_cols_tst = pd.DataFrame(hot1_encoder.transform(tst[cat_cols]).toarray())\n\n# Add index\nhot1_cols_trn.index = trn.index\nhot1_cols_tst.index = tst.index\n\n# Drop categorical cols, will replace w/ one-hot cols\ntrn = trn.drop(cat_cols, axis=1)\ntst = tst.drop(cat_cols, axis=1)\n\n# Add one-hot cols\ntrn = pd.concat([trn, hot1_cols_trn], axis=1)\ntst = pd.concat([tst, hot1_cols_tst], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#trn[cat_cols].head()\n#hot1_cols_trn.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Imputation  \nUse **imputation** on missing data. I.e. fill in missing data with *some* value.<br>\nCan use column mean, median, etc. for missing data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pull out the target\ny = trn['isFraud']\ntrn = trn.drop(columns = ['isFraud'])\n\n# Get all cols in a list\ncols = trn.columns.values.tolist()\n\n# Perform imputation. This will run through the one-hot cols as well. It's probably best to skip those,\n# but shouldn't cause a problem if you don't, as they should not be missing data.\n\nfor col in cols:\n    #print(col)\n    mean = trn[col].mean()\n    trn[col] = trn[col].fillna(mean)\n    tst[col] = tst[col].fillna(mean)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make sure columns are in the same order for train and test\ncols = trn.columns.values.tolist()\ntst = tst[cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's try XGBoost\nimport xgboost as xgb\n\nclf = xgb.XGBClassifier(\n    n_estimators=300,\n    max_depth=15,\n    learning_rate=0.02,\n    subsample=0.5,\n    colsample_bytree=0.9,\n    missing=-999,\n    random_state=1,\n    tree_method='exact'  # THE MAGICAL PARAMETER\n)\n\nclf.fit(trn, y)\n\n# Make predictions\npredictions = clf.predict(tst)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"#print(trn.columns.to_list())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Specify Model\n#from sklearn.ensemble import RandomForestRegressor\n#seed = 1\n#rf_model = RandomForestRegressor(n_estimators=200, max_depth=20, random_state = seed)\n\n# Fit Model (Can take hours to run)\n#rf_model.fit(trn, y)\n\n# Make predictions\n#predictions = rf_model.predict(tst)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Submission - write predictions to .csv file"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame(index = tst.index)\nsubmission['isFraud'] = predictions\nsubmission.to_csv('nov10.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#trn['card2'].loc[trn['card2'].isnull()]#.sum()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}