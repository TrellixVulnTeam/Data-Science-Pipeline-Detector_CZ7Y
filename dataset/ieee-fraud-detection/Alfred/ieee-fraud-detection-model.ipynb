{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport pyarrow.parquet as pq\nimport gc\nimport time\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom matplotlib import pyplot\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import plot_tree\nfrom sklearn import metrics   \nfrom sklearn.metrics import roc_curve,auc\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import f1_score\nimport xgboost as xgb\n\n\nfrom sklearn.metrics import roc_auc_score\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reduce memory for 13GB limited memory in Kaggle\n# Code Reference: https://www.kaggle.com/sbunzini/reduce-memory-usage-by-75\ndef reduce_memory_usage(df):\n    \n    start_memory = df.memory_usage().sum() / 1024**2\n    print(f\"Memory usage of dataframe is {start_memory} MB\")\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != 'object':\n            c_min = df[col].min()\n            c_max = df[col].max()\n            \n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    pass\n        else:\n            df[col] = df[col].astype('category')\n    \n    end_memory = df.memory_usage().sum() / 1024**2\n    print(f\"Memory usage of dataframe after reduction {end_memory} MB\")\n    print(f\"Reduced by {100 * (start_memory - end_memory) / start_memory} % \")\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading input data\ntrain_transaction = pd.read_csv('/kaggle/input/ieee-fraud-detection/train_transaction.csv')\ntest_transaction = pd.read_csv('/kaggle/input/ieee-fraud-detection/test_transaction.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_identity = pd.read_csv('/kaggle/input/ieee-fraud-detection/train_identity.csv')\ntest_identity = pd.read_csv('/kaggle/input/ieee-fraud-detection/test_identity.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# align the same column name in train identity [id_01,id_02,...] and test identity [id-01,id-02,...]\nid_cols = train_identity.columns\ntest_identity.columns = id_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_transaction = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\ntest_transaction = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_transaction = train_transaction.drop('P_emaildomain',1)\n#train_transaction = train_transaction.drop('R_emaildomain',1)\ntrain_transaction = train_transaction.drop('TransactionDT',1)\ntrain_transaction = train_transaction.drop('TransactionID',1)\n\n#test_transaction = test_transaction.drop('P_emaildomain',1)\n#test_transaction = test_transaction.drop('R_emaildomain',1)\ntest_transaction = test_transaction.drop('TransactionDT',1)\ntest_id = test_transaction['TransactionID']\n\ntest_transaction = test_transaction.drop('TransactionID',1)\n\ntrain_label = train_transaction.isFraud\ntrain_transaction = train_transaction.drop('isFraud',1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 1: Data Processing for training set\n## ProductCD, card1 - card6, addr1 - addr2, M1 - M9, D1 - D15, C1 - C14, TransactionAmt, dist1, dist2 ","metadata":{}},{"cell_type":"code","source":"category_cols =['ProductCD','card1','card2','card3','card4','card5','card6','addr1','addr2',\n                'M1','M2','M3','M4','M5','M6','M7','M8','M9']\ntime_delta_cols = ['D1','D2','D3','D4','D5','D6','D7','D8','D9','D10','D11','D12','D13','D14','D15']\ncounter_cols = ['C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14']\nnumeric_cols = ['TransactionAmt','dist1','dist2']\nlabel_col = ['isFraud']\n#text_col= ['P_emaildomain','R_emaildomain']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in time_delta_cols:\n    train_transaction[col].fillna(0,inplace=True)\n    \nfor col in counter_cols:\n    train_transaction[col].fillna(0,inplace=True)\n    \nfor col in numeric_cols:\n    train_transaction[col].fillna(train_transaction[col].mean(),inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_transaction['Dsum'] = train_transaction['D1'] + train_transaction['D2'] + train_transaction['D3'] + train_transaction['D4'] + train_transaction['D5']+ train_transaction['D6'] + train_transaction['D7'] + train_transaction['D8']+ train_transaction['D9'] + train_transaction['D10'] + train_transaction['D11'] + train_transaction['D12']+ train_transaction['D13'] + train_transaction['D14'] + train_transaction['D15']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_transaction['amt_cat'] = pd.cut(train_transaction.TransactionAmt,bins=[0,50,100,150,200,100000],labels=['<=50','>50&<=100','>100&<=150','>150&<=200','>200'])\nlabelencoder = LabelEncoder()\ntrain_transaction['amt_cat'] = labelencoder.fit_transform(train_transaction['amt_cat'])\n#test_set['amt_cat'] = pd.cut(test_set.TransactionAmt,bins=[0,50,100,150,200,100000],labels=['<=50','>50&<=100','>100&<=150','>150&<=200','>200'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_transaction['Csum'] = train_transaction['C1'] + train_transaction['C2'] + train_transaction['C3'] + train_transaction['C4'] + train_transaction['C5'] + train_transaction['C6'] + train_transaction['C7'] + train_transaction['C8'] + train_transaction['C9'] + train_transaction['C10'] + train_transaction['C11'] + train_transaction['C12'] + train_transaction['C13'] + train_transaction['C14']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_transaction[category_cols] = train_transaction[category_cols].astype('str')\n#test_set[category_cols] = test_set[category_cols].astype('str')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_transaction['P_emaildomain'] = train_transaction['P_emaildomain'].astype('str')\ntrain_transaction['R_emaildomain'] = train_transaction['R_emaildomain'].astype('str')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_transaction['P_neq_R'] = train_transaction['P_emaildomain'] != train_transaction['R_emaildomain']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_transaction['P_neq_R'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_transaction = train_transaction.drop('P_emaildomain',1)\ntrain_transaction = train_transaction.drop('R_emaildomain',1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labelencoder = LabelEncoder()\nfor col in category_cols:\n    train_transaction[col] = labelencoder.fit_transform(train_transaction[col])\n    \ntrain_transaction['P_neq_R'] = labelencoder.fit_transform(train_transaction['P_neq_R'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_transaction['M1'] = train_transaction['M1'].replace('T','1').replace('F','0').replace('None','0').fillna('0').astype(int)\ntrain_transaction['M2'] = train_transaction['M2'].replace('T','1').replace('F','0').replace('None','0').fillna('0').astype(int)\ntrain_transaction['M3'] = train_transaction['M3'].replace('T','1').replace('F','0').replace('None','0').fillna('0').astype(int)\ntrain_transaction['M5'] = train_transaction['M5'].replace('T','1').replace('F','0').replace('None','0').fillna('0').astype(int)\ntrain_transaction['M6'] = train_transaction['M6'].replace('T','1').replace('F','0').replace('None','0').fillna('0').astype(int)\ntrain_transaction['M7'] = train_transaction['M7'].replace('T','1').replace('F','0').replace('None','0').fillna('0').astype(int)\ntrain_transaction['M8'] = train_transaction['M8'].replace('T','1').replace('F','0').replace('None','0').fillna('0').astype(int)\ntrain_transaction['M9'] = train_transaction['M9'].replace('T','1').replace('F','0').replace('None','0').fillna('0').astype(int)\ntrain_transaction['Msum'] = train_transaction['M1'] + train_transaction['M2'] + train_transaction['M3'] + train_transaction['M5'] + train_transaction['M6'] + train_transaction['M7'] + train_transaction['M8'] + train_transaction['M9']\n\ntrain_transaction['distSum'] = train_transaction['dist1'] + train_transaction['dist2']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"minmax_scaler = MinMaxScaler()\ntrain_transaction[time_delta_cols] = minmax_scaler.fit_transform(train_transaction[time_delta_cols])\ntrain_transaction[counter_cols] = minmax_scaler.fit_transform(train_transaction[counter_cols])\ntrain_transaction[['Csum']] = minmax_scaler.fit_transform(train_transaction[['Csum']])\ntrain_transaction[['Dsum']] = minmax_scaler.fit_transform(train_transaction[['Dsum']])\ntrain_transaction[['Msum']] = minmax_scaler.fit_transform(train_transaction[['Msum']])\ntrain_transaction[['TransactionAmt']] = minmax_scaler.fit_transform(train_transaction[['TransactionAmt']])\ntrain_transaction[['dist1']] = minmax_scaler.fit_transform(train_transaction[['dist1']])\ntrain_transaction[['dist2']] = minmax_scaler.fit_transform(train_transaction[['dist2']])\ntrain_transaction[['distSum']] = minmax_scaler.fit_transform(train_transaction[['distSum']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_cols = time_delta_cols + counter_cols + numeric_cols + category_cols + ['distSum'] + ['Msum'] + ['Dsum'] + ['Csum']+ ['amt_cat'] + ['P_neq_R']\n#features_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig_width = 35\nfig_height = 8\nfont_size = 15","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 2 Data Processing for training set\n## V columns (V1 - V339) data processing","metadata":{}},{"cell_type":"code","source":"vcol = [v for v in train_transaction if v[0] == 'V']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Found V-columns with <= 50 class categories \nCATEGORY_COUNT = 50\n\ndef find_categorical_v(df):\n    num_v = []\n    cat_v = []\n    for v in vcol:\n        cnt = train_transaction[v].value_counts(dropna=False).count()\n        if (cnt <= CATEGORY_COUNT):\n            cat_v.append([v,cnt])\n        else:\n            num_v.append([v,cnt])\n    return cat_v, num_v","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(cat_v, num_v) = find_categorical_v(train_transaction)\narr_cat_v = np.array(cat_v)\narr_num_v = np.array(num_v)\n\ncatv_df = pd.DataFrame({'vcolname': arr_cat_v[:,0], 'catcnt': arr_cat_v[:,1]})  \nnumv_df = pd.DataFrame({'vcolname': arr_num_v[:,0], 'catcnt': arr_num_v[:,1]})\nnum_v_cols = numv_df.vcolname.tolist()\ncat_v_cols = catv_df.vcolname.tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in num_v_cols:\n    train_transaction[col].fillna(train_transaction[col].mean(),inplace=True)\n\nfor col in cat_v_cols:\n    train_transaction[col].fillna(0,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 3 Data Processing\n## Identity Columns: id_01 - id_38, DeviceType, DeviceInfo","metadata":{}},{"cell_type":"code","source":"train_transaction['DeviceType'].value_counts(dropna = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_transaction['DeviceInfo'].value_counts(dropna = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"id_categorial_cols = ['id_12','id_13','id_14','id_15','id_16','id_17','id_18','id_19','id_20','id_21',\n                      'id_22','id_23','id_24','id_25','id_26','id_27','id_28','id_29','id_30','id_31',\n                      'id_32','id_33','id_34','id_35','id_36','id_37','id_38','DeviceType','DeviceInfo']\n\nid_numeric_cols = ['id_01','id_02','id_03','id_04','id_05','id_06','id_07','id_08','id_09','id_10','id_11']\n\n\nfor col in id_numeric_cols:\n    train_transaction[col].fillna(train_transaction[col].mean(), inplace=True)\n    \nfor col in id_categorial_cols:\n    train_transaction[col].fillna('na', inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_transaction.loc[train_transaction['id_31'].str.contains('samsung', case=False), 'id_31'] = 'samsung'\ntrain_transaction.loc[train_transaction['id_31'].str.contains('chrome', case=False) & train_transaction['id_31'].str.contains('android', case=False), 'id_31'] = 'chrome'\ntrain_transaction.loc[train_transaction['id_31'].str.contains('chrome', case=False), 'id_31'] = 'chrome'\ntrain_transaction.loc[train_transaction['id_31'].str.contains('ie', case=False), 'id_31'] = 'ie'\ntrain_transaction.loc[train_transaction['id_31'].str.contains('edge', case=False), 'id_31'] = 'edge'\ntrain_transaction.loc[train_transaction['id_31'].str.contains('firefox', case=False), 'id_31'] = 'firefox'\ntrain_transaction.loc[train_transaction['id_31'].str.contains('opera', case=False), 'id_31'] = 'opera'\ntrain_transaction.loc[train_transaction['id_31'].str.contains('safari', case=False), 'id_31'] = 'safari'\n#for other\ntrain_transaction.loc[~train_transaction['id_31'].isin(['samsung','chrome','ie','edge','firefox','opera','safari']), 'id_31'] = 'other'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_transaction.loc[train_transaction['DeviceInfo'].str.contains('iOS', case=False), 'DeviceInfo'] = 'iOS'\ntrain_transaction.loc[train_transaction['DeviceInfo'].str.contains('Trident', case=False), 'DeviceInfo'] = 'Trident'\ntrain_transaction.loc[train_transaction['DeviceInfo'].str.contains('Windows', case=False), 'DeviceInfo'] = 'Windows'\ntrain_transaction.loc[train_transaction['DeviceInfo'].str.contains('MacOS', case=False), 'DeviceInfo'] = 'MacOS'\n#for other\ntrain_transaction.loc[~train_transaction['DeviceInfo'].isin(['iOS','Trident','Windows','MacOS']), 'DeviceInfo'] = 'other'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_transaction['DeviceInfo'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labelencoder = LabelEncoder()\nfor col in id_categorial_cols:\n    train_transaction[col] = labelencoder.fit_transform(train_transaction[col].astype(str))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"minmax_scaler = MinMaxScaler()\ntrain_transaction[id_numeric_cols] = minmax_scaler.fit_transform(train_transaction[id_numeric_cols])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"id_cols = id_categorial_cols + id_numeric_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_transaction[features_cols+vcol+id_cols].columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Features Selection","metadata":{}},{"cell_type":"code","source":"all_cols = features_cols+vcol+id_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(all_cols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lg_modelfit(dtrain, target):\n    model = LogisticRegression(class_weight=\"balanced\")\n    model.fit(dtrain, target) \n    return model\n\ndef xgb_modelfit(dtrain, target):    \n    model = xgb.XGBClassifier(\n         learning_rate =0.01,\n         n_estimators=1000,\n         max_depth=8,\n         min_child_weight=1,\n         gamma=0,\n         subsample=0.8,\n         colsample_bytree=0.8,\n         objective= 'binary:logistic',\n         nthread=4,\n         scale_pos_weight=1,\n         seed=27,\n         verbosity=0,\n         tree_method='gpu_hist'\n    )\n    # fit the model\n    model.fit(dtrain, target)\n    return model\n\ndef adaboost_modelfit(dtrain, target):\n    model = AdaBoostClassifier(\n        learning_rate=0.1,  \n        n_estimators=1000\n    )\n    model.fit(dtrain, target) \n    return model\n\ndef get_Importances(importances, cols):\n    feature_score = {}\n    for i,v in enumerate(importances):\n        feature_score[cols[i]] = v\n    return feature_score ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import csv\ndef write_csv_kaggle_sub(fname, y_pred):\n    sub = pd.DataFrame()\n    sub['TransactionID'] = test_id\n    sub['isFraud'] = y_pred[:, 1]\n    sub.to_csv(fname, index=False)\n\ndef write_feature_importances(fname, feature_score):\n    with open(fname, 'w') as f:\n        writer = csv.writer(f)\n        for key, value in dict(sorted(feature_score.items(), key=lambda item: item[1], reverse=True)).items():\n            writer.writerow([key, value])\n\ndef select_Top_features(feature_score, top_num):\n    top_features =[]\n    for key, value in dict(sorted(feature_score.items(), key=lambda item: item[1], reverse=True)).items():\n        top_features.append(key)\n    return top_features[:top_num]\n\ndef select_features_range(feature_score, ftop_num, ttop_num):\n    top_features =[]\n    top_scores =[]\n    for key, value in dict(sorted(feature_score.items(), key=lambda item: item[1], reverse=True)).items():\n        top_features.append(key)\n        top_scores.append(value)\n        \n    return top_features[ftop_num:ttop_num], top_scores[ftop_num:ttop_num]\n\ndef select_features_byCol(feature_score, colList, ftop_num, ttop_num):\n    top_features =[]\n    top_scores =[]\n    for key, value in dict(sorted(feature_score.items(), key=lambda item: item[1], reverse=True)).items():\n        if key in colList:\n            top_features.append(key)\n            top_scores.append(value)\n        \n    return top_features[ftop_num:ttop_num], top_scores[ftop_num:ttop_num]\n\ndef features_plot(fscore, frange, trange, titlename):\n    col, val = select_features_range(fscore, frange, trange)\n    fig = plt.figure(figsize = (fig_width, fig_height))\n    plt.bar(range(len(col)), val, align='center')\n    plt.title(titlename)\n    plt.xticks(range(len(col)),col)\n    plt.show()\n\ndef features_plot_bycol(fscore, col_list, frange, trange, titlename):\n    col, val = select_features_byCol(fscore, col_list,frange, trange)\n    fig = plt.figure(figsize = (fig_width, fig_height))\n    plt.bar(range(len(col)), val, align='center')\n    plt.title(titlename)\n    plt.xticks(range(len(col)),col)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_transaction = reduce_memory_usage(train_transaction)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train_identity\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Find the weights of the following features by Logistic Regression\n#### ProductCD, card1 - card6, addr1 - addr2, M1 - M9, D1 - D15, C1 - C14, TransactionAmt, dist1, dist2 \n#### V1 - V339\n#### id_01 - id_38, DeviceType, DeviceInfo","metadata":{}},{"cell_type":"code","source":"LR_all_feature_score = {}\nlg_model = lg_modelfit(train_transaction[all_cols], train_label)\nf_scores = get_Importances(lg_model.coef_[0],all_cols)\nLR_all_feature_score.update(f_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"write_feature_importances(\"LR_FeatureSelection.csv\",LR_all_feature_score)\ndel lg_model\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_plot_bycol(LR_all_feature_score, features_cols, 0, 50,\"Top 50 selected transaction cols in LR\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_plot_bycol(LR_all_feature_score, vcol, 0, 50,\"Top 50 selected V's cols in LR\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_plot_bycol(LR_all_feature_score, id_cols, 0, 50 ,\"Top 50 selected ids' in LR\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_plot(LR_all_feature_score, 0, 50 ,\"Rank 0 - 50 selected features in LR\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Find the importances of the following features by XGBClassifier\n#### ProductCD, card1 - card6, addr1 - addr2, M1 - M9, D1 - D15, C1 - C14, TransactionAmt, dist1, dist2 \n#### V1 - V339\n#### id_01 - id_38, DeviceType, DeviceInfo\n","metadata":{}},{"cell_type":"code","source":"XGB_all_feature_score = {}\nxgb_model = xgb_modelfit(train_transaction[all_cols], train_label)\nf_scores = get_Importances(xgb_model.feature_importances_,all_cols)\nXGB_all_feature_score.update(f_scores)\n#XGB_feature_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"write_feature_importances(\"XGB_FeatureSelection.csv\",XGB_all_feature_score)\n\ndel xgb_model\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_plot_bycol(XGB_all_feature_score, features_cols, 0, 50,\"Top 50 selected transaction cols in XGB\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_plot_bycol(XGB_all_feature_score, vcol, 0, 50,\"Top 50 selected V's cols in XGB\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_plot_bycol(XGB_all_feature_score, id_cols, 0, 50,\"Top 50 selected ids' in XGB\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_plot(XGB_all_feature_score, 0, 50 ,\"Rank 0 - 50 selected features in XGB\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Find the importances of the following features by AdaBoostClassifier\n#### ProductCD, card1 - card6, addr1 - addr2, M1 - M9, D1 - D15, C1 - C14, TransactionAmt, dist1, dist2 \n#### V1 - V339\n#### id_01 - id_38, DeviceType, DeviceInfo","metadata":{}},{"cell_type":"code","source":"AdaBoost_all_feature_score = {}\nada_model = adaboost_modelfit(train_transaction[all_cols], train_label)\nf_scores = get_Importances(ada_model.feature_importances_,all_cols)\nAdaBoost_all_feature_score.update(f_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"write_feature_importances(\"AdaBoost_FeatureSelection.csv\",AdaBoost_all_feature_score)\n\ndel ada_model\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_plot_bycol(AdaBoost_all_feature_score, features_cols, 0, 50,\"Top 50 selected transaction cols in AdaBoost\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_plot_bycol(AdaBoost_all_feature_score, vcol, 0, 50,\"Top 50 selected V's cols in AdaBoost\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_plot_bycol(AdaBoost_all_feature_score, id_cols, 0, 50,\"Top 50 selected ids' in AdaBoost\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_plot(AdaBoost_all_feature_score, 0, 50 ,\"Rank 0 - 50 selected features in AdaBoost\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Buidling","metadata":{}},{"cell_type":"markdown","source":"### Test Data Processing","metadata":{}},{"cell_type":"code","source":"for col in time_delta_cols:\n    test_transaction[col].fillna(0,inplace=True)\n    \nfor col in counter_cols:\n    test_transaction[col].fillna(0,inplace=True)\n    \nfor col in numeric_cols:\n    test_transaction[col].fillna(test_transaction[col].mean(),inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_transaction['Dsum'] = test_transaction['D1'] + test_transaction['D2'] + test_transaction['D3'] + test_transaction['D4'] + test_transaction['D5']+ test_transaction['D6'] + test_transaction['D7'] + test_transaction['D8']+ test_transaction['D9'] + test_transaction['D10'] + test_transaction['D11'] + test_transaction['D12']+ test_transaction['D13'] + test_transaction['D14'] + test_transaction['D15']\n\ntest_transaction['amt_cat'] = pd.cut(test_transaction.TransactionAmt,bins=[0,50,100,150,200,100000],labels=['<=50','>50&<=100','>100&<=150','>150&<=200','>200'])\ntest_transaction['amt_cat'] = labelencoder.fit_transform(test_transaction['amt_cat'])\n\ntest_transaction['Csum'] = test_transaction['C1'] + test_transaction['C2'] + test_transaction['C3'] + test_transaction['C4'] + test_transaction['C5'] + test_transaction['C6'] + test_transaction['C7'] + test_transaction['C8'] + test_transaction['C9'] + test_transaction['C10'] + test_transaction['C11'] + test_transaction['C12'] + test_transaction['C13'] + test_transaction['C14']\n\ntest_transaction[category_cols] = test_transaction[category_cols].astype('str')\n#test_set[category_cols] = test_set[category_cols].astype('str')\n\ntest_transaction['P_neq_R'] = test_transaction['P_emaildomain'] != test_transaction['R_emaildomain']\n\nlabelencoder = LabelEncoder()\nfor col in category_cols:\n    test_transaction[col] = labelencoder.fit_transform(test_transaction[col])\n    \ntest_transaction['P_neq_R'] = labelencoder.fit_transform(test_transaction['P_neq_R'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_transaction['M1'] = test_transaction['M1'].replace('T','1').replace('F','0').replace('None','0').fillna('0').astype(int)\ntest_transaction['M2'] = test_transaction['M2'].replace('T','1').replace('F','0').replace('None','0').fillna('0').astype(int)\ntest_transaction['M3'] = test_transaction['M3'].replace('T','1').replace('F','0').replace('None','0').fillna('0').astype(int)\ntest_transaction['M5'] = test_transaction['M5'].replace('T','1').replace('F','0').replace('None','0').fillna('0').astype(int)\ntest_transaction['M6'] = test_transaction['M6'].replace('T','1').replace('F','0').replace('None','0').fillna('0').astype(int)\ntest_transaction['M7'] = test_transaction['M7'].replace('T','1').replace('F','0').replace('None','0').fillna('0').astype(int)\ntest_transaction['M8'] = test_transaction['M8'].replace('T','1').replace('F','0').replace('None','0').fillna('0').astype(int)\ntest_transaction['M9'] = test_transaction['M9'].replace('T','1').replace('F','0').replace('None','0').fillna('0').astype(int)\ntest_transaction['Msum'] = test_transaction['M1'] + test_transaction['M2'] + test_transaction['M3'] + test_transaction['M5'] + test_transaction['M6'] + test_transaction['M7'] + test_transaction['M8'] + test_transaction['M9']\n\ntest_transaction['distSum'] = test_transaction['dist1'] + test_transaction['dist2']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"minmax_scaler = MinMaxScaler()\ntest_transaction[time_delta_cols] = minmax_scaler.fit_transform(test_transaction[time_delta_cols])\ntest_transaction[counter_cols] = minmax_scaler.fit_transform(test_transaction[counter_cols])\ntest_transaction[['Csum']] = minmax_scaler.fit_transform(test_transaction[['Csum']])\ntest_transaction[['Dsum']] = minmax_scaler.fit_transform(test_transaction[['Dsum']])\ntest_transaction[['Msum']] = minmax_scaler.fit_transform(test_transaction[['Msum']])\ntest_transaction[['TransactionAmt']] = minmax_scaler.fit_transform(test_transaction[['TransactionAmt']])\ntest_transaction[['dist1']] = minmax_scaler.fit_transform(test_transaction[['dist1']])\ntest_transaction[['dist2']] = minmax_scaler.fit_transform(test_transaction[['dist2']])\ntest_transaction[['distSum']] = minmax_scaler.fit_transform(test_transaction[['distSum']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in num_v_cols:\n    test_transaction[col].fillna(test_transaction[col].mean(),inplace=True)\n\nfor col in cat_v_cols:\n    test_transaction[col].fillna(0,inplace=True)\n    \nfor col in id_numeric_cols:\n    test_transaction[col].fillna(test_transaction[col].mean(), inplace=True)\n    \nfor col in id_categorial_cols:\n    test_transaction[col].fillna('na', inplace=True)\n    \ntest_transaction.loc[test_transaction['id_31'].str.contains('samsung', case=False), 'id_31'] = 'samsung'\ntest_transaction.loc[test_transaction['id_31'].str.contains('chrome', case=False) & test_transaction['id_31'].str.contains('android', case=False), 'id_31'] = 'chrome'\ntest_transaction.loc[test_transaction['id_31'].str.contains('chrome', case=False), 'id_31'] = 'chrome'\ntest_transaction.loc[test_transaction['id_31'].str.contains('ie', case=False), 'id_31'] = 'ie'\ntest_transaction.loc[test_transaction['id_31'].str.contains('edge', case=False), 'id_31'] = 'edge'\ntest_transaction.loc[test_transaction['id_31'].str.contains('firefox', case=False), 'id_31'] = 'firefox'\ntest_transaction.loc[test_transaction['id_31'].str.contains('opera', case=False), 'id_31'] = 'opera'\ntest_transaction.loc[test_transaction['id_31'].str.contains('safari', case=False), 'id_31'] = 'safari'\n#for other\ntest_transaction.loc[~test_transaction['id_31'].isin(['samsung','chrome','ie','edge','firefox','opera','safari']), 'id_31'] = 'other'\ntest_transaction.loc[test_transaction['DeviceInfo'].str.contains('iOS', case=False), 'DeviceInfo'] = 'iOS'\ntest_transaction.loc[test_transaction['DeviceInfo'].str.contains('Trident', case=False), 'DeviceInfo'] = 'Trident'\ntest_transaction.loc[test_transaction['DeviceInfo'].str.contains('Windows', case=False), 'DeviceInfo'] = 'Windows'\ntest_transaction.loc[test_transaction['DeviceInfo'].str.contains('MacOS', case=False), 'DeviceInfo'] = 'MacOS'\n#for other\ntest_transaction.loc[~test_transaction['DeviceInfo'].isin(['iOS','Trident','Windows','MacOS']), 'DeviceInfo'] = 'other'\n\nlabelencoder = LabelEncoder()\nfor col in id_categorial_cols:\n    test_transaction[col] = labelencoder.fit_transform(test_transaction[col].astype(str))\n\nminmax_scaler = MinMaxScaler()\ntest_transaction[id_numeric_cols] = minmax_scaler.fit_transform(test_transaction[id_numeric_cols])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Select Top x Features for Logistic Regression, XGB, AdaBoost\n\n### x = 50, 100, 150, 200","metadata":{}},{"cell_type":"code","source":"num_features_selected_List = [50,100,150, 200]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for num_features_selected in num_features_selected_List:\n    print(\"Number of features: \" + str(num_features_selected))\n    print(\"Running LR\")\n    \n    start_time = time.time()\n    LR_train_set = train_transaction[select_Top_features(LR_all_feature_score, num_features_selected)]\n    LR_test_set = test_transaction[select_Top_features(LR_all_feature_score, num_features_selected)]\n    \n    lg_model = lg_modelfit(LR_train_set, train_label)\n    \n    \n    Y_train_pred = lg_model.predict_proba(LR_train_set)\n    fpr, tpr, thresholds = roc_curve(train_label, Y_train_pred[:, 1])\n    roc_auc = auc(fpr, tpr)\n    \n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n    fig.tight_layout() # Or equivalently,  \"plt.tight_layout()\"\n\n    plt.subplot(1, 2, 1)\n    plt.plot(fpr, tpr, lw=2, alpha=0.3)\n    plt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'black')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Area under ROC = %0.2f (Train set)' % (roc_auc))\n\n    \n    lg_precision, lg_recall, _ = precision_recall_curve(train_label, Y_train_pred[:, 1])\n    prc_auc = auc(lg_recall, lg_precision)\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(lg_recall, lg_precision, marker='.')\n    # axis labels\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Area under PRC = %0.2f (Train set)' % (prc_auc))\n \n    plt.show()\n    \n    \n    y_pred = lg_model.predict_proba(LR_test_set)\n    write_csv_kaggle_sub(\"LR_Submission_Selected2_\" + str(num_features_selected) +\".csv\",y_pred)\n    \n    print(\"--- %s seconds ---\" % (time.time() - start_time))\n    del LR_train_set\n    del LR_test_set\n    del lg_model\n    del y_pred\n    gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for num_features_selected in num_features_selected_List:\n    print(\"Number of features: \" + str(num_features_selected))\n    print(\"Running AdaBoost\")\n    start_time = time.time()\n    AdaBoost_train_set = train_transaction[select_Top_features(AdaBoost_all_feature_score, num_features_selected)]\n    AdaBoost_test_set = test_transaction[select_Top_features(AdaBoost_all_feature_score, num_features_selected)]\n    \n    ada_model = adaboost_modelfit(AdaBoost_train_set, train_label)\n    \n    \n    Y_train_pred = ada_model.predict_proba(AdaBoost_train_set)\n    fpr, tpr, thresholds = roc_curve(train_label, Y_train_pred[:, 1])\n    roc_auc = auc(fpr, tpr)\n    \n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n    fig.tight_layout() # Or equivalently,  \"plt.tight_layout()\"\n\n    plt.subplot(1, 2, 1)\n    plt.plot(fpr, tpr, lw=2, alpha=0.3)\n    plt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'black')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Area under ROC = %0.2f (Train set)' % (roc_auc))\n\n    \n    ada_precision, ada_recall, _ = precision_recall_curve(train_label, Y_train_pred[:, 1])\n    prc_auc = auc(ada_recall, ada_precision)\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(ada_recall, ada_precision, marker='.')\n    # axis labels\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Area under PRC = %0.2f (Train set)' % (prc_auc))\n \n    plt.show()\n    \n    \n    y_pred = ada_model.predict_proba(AdaBoost_test_set)\n    write_csv_kaggle_sub(\"AdaBoost_Submission_Selected2_\" + str(num_features_selected) +\".csv\",y_pred)\n    \n    print(\"--- %s seconds ---\" % (time.time() - start_time))\n    del AdaBoost_train_set\n    del AdaBoost_test_set\n    del ada_model\n    del y_pred\n    gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### XGB Modeling","metadata":{}},{"cell_type":"code","source":"for num_features_selected in num_features_selected_List:\n    print(\"Number of features: \" + str(num_features_selected))\n    print(\"Running XGB\")\n    start_time = time.time()\n    XGB_train_set = train_transaction[select_Top_features(XGB_all_feature_score, num_features_selected)]\n    XGB_test_set = test_transaction[select_Top_features(XGB_all_feature_score, num_features_selected)]\n    xgb_model = xgb_modelfit(XGB_train_set, train_label)\n    \n    Y_train_pred = xgb_model.predict_proba(XGB_train_set)\n    fpr, tpr, thresholds = roc_curve(train_label, Y_train_pred[:, 1])\n    roc_auc = auc(fpr, tpr)\n    \n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n    fig.tight_layout() # Or equivalently,  \"plt.tight_layout()\"\n\n    plt.subplot(1, 2, 1)\n    plt.plot(fpr, tpr, lw=2, alpha=0.3)\n    plt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'black')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Area under ROC = %0.2f (Train set)' % (roc_auc))\n\n    \n    xgb_precision, xgb_recall, _ = precision_recall_curve(train_label, Y_train_pred[:, 1])\n    prc_auc = auc(xgb_recall, xgb_precision)\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(xgb_recall, xgb_precision, marker='.')\n    # axis labels\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Area under PRC = %0.2f (Train set)' % (prc_auc))\n \n    plt.show()\n\n    y_pred = xgb_model.predict_proba(XGB_test_set)\n    write_csv_kaggle_sub(\"XGB_Submission_Selected2_\" + str(num_features_selected) +\".csv\",y_pred)\n    print(\"--- %s seconds ---\" % (time.time() - start_time))\n    del XGB_train_set\n    del XGB_test_set\n    del xgb_model\n    del y_pred\n    gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### XGB Modeling for common importances in XGB and AdaBoost","metadata":{}},{"cell_type":"code","source":"num_features_selected_List = [100, 200]\nfor num_features_selected in num_features_selected_List:\n    #print(\"Number of features: \" + str(num_features_selected))\n    #print(\"Running XGB\")\n    start_time = time.time()\n    xgb_cols = select_Top_features(XGB_all_feature_score, num_features_selected)\n    ada_cols = select_Top_features(AdaBoost_all_feature_score, num_features_selected)\n    \n    common_cols = list(set(xgb_cols).intersection(ada_cols))\n    print(\"Number of features: \" + str(len(common_cols)))\n    print(common_cols)\n    \n    XGB_train_set = train_transaction[common_cols]\n    XGB_test_set = test_transaction[common_cols]\n    xgb_model = xgb_modelfit(XGB_train_set, train_label)\n    \n    Y_train_pred = xgb_model.predict_proba(XGB_train_set)\n    fpr, tpr, thresholds = roc_curve(train_label, Y_train_pred[:, 1])\n    roc_auc = auc(fpr, tpr)\n    \n    fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10,5))\n    fig.tight_layout()\n\n    plt.subplot(1, 2, 1)\n    plt.plot(fpr, tpr, lw=2, alpha=0.3)\n    plt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'black')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Area under ROC = %0.2f (Train set)' % (roc_auc))\n\n    \n    xgb_precision, xgb_recall, _ = precision_recall_curve(train_label, Y_train_pred[:, 1])\n    prc_auc = auc(xgb_recall, xgb_precision)\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(xgb_recall, xgb_precision, marker='.')\n    # axis labels\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Area under PRC = %0.2f (Train set)' % (prc_auc))\n \n    plt.show()\n    # show the legend\n\n    y_pred = xgb_model.predict_proba(XGB_test_set)\n    \n    write_csv_kaggle_sub(\"XGB_Submission_CommonSelected2_\" + str(len(common_cols)) +\".csv\",y_pred)\n    print(\"--- %s seconds ---\" % (time.time() - start_time))\n\n    del XGB_train_set\n    del XGB_test_set\n    del xgb_model\n    del y_pred\n    gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stratified 80-Fold training","metadata":{}},{"cell_type":"code","source":"kfold = 80\nskf = StratifiedKFold(n_splits=kfold, shuffle=True, random_state=42)\nskf_pd = pd.DataFrame()\nskf_pd['TransactionID'] = test_id\nskf_pd['isFraud'] = np.zeros_like(test_id)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_cols = select_Top_features(XGB_all_feature_score, 100)\nada_cols = select_Top_features(AdaBoost_all_feature_score, 100)\n    \ncommon_cols = list(set(xgb_cols).intersection(ada_cols))\nprint(\"Number of features: \" + str(len(common_cols)))\nprint(common_cols)\n    \nXGB_train_set = train_transaction[common_cols]\nXGB_test_set = test_transaction[common_cols]\n\ntotal_roc_auc = 0\ntotal_prc_auc = 0\nfig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10,5))\nfig.tight_layout()\nfor i, (train_index, test_index) in enumerate(skf.split(XGB_train_set, train_label)):\n    X_train = XGB_train_set.values\n    Y_label = train_label.values\n    \n    X_train, X_valid = X_train[train_index], X_train[test_index]\n    Y_train, Y_valid = Y_label[train_index], Y_label[test_index]\n    \n    xgb_model = xgb_modelfit(X_train, Y_train)\n    Y_valid_pred = xgb_model.predict_proba(X_valid)\n    \n    fpr, tpr, thresholds = roc_curve(Y_valid, Y_valid_pred[:, 1])\n    roc_auc = auc(fpr, tpr)\n    total_roc_auc = total_roc_auc + roc_auc\n    \n    plt.subplot(1, 2, 1)\n    plt.plot(fpr, tpr, lw=2, alpha=0.3)\n\n    xgb_precision, xgb_recall, _ = precision_recall_curve(Y_valid, Y_valid_pred[:, 1])\n    prc_auc = auc(xgb_recall, xgb_precision)\n    total_prc_auc = total_prc_auc + prc_auc\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(xgb_recall, xgb_precision, lw=2, alpha=0.3)\n    \n    y_pred = xgb_model.predict_proba(XGB_test_set)\n    skf_pd['isFraud'] += y_pred[:, 1]/kfold\n\nplt.subplot(1, 2, 1)\nplt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'black')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Mean area under ROC = %0.2f (80 fold)' % (total_roc_auc/kfold))\n\nplt.subplot(1, 2, 2)\n# axis labels\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Mean area under PRC = %0.2f (80 fold)' % (total_prc_auc/kfold))\nplt.show()\n    \ndel XGB_train_set\ndel XGB_test_set\ndel xgb_model\ndel y_pred\ngc.collect()\n\nskf_pd.to_csv(\"XGB_StratifiedKFold\" + str(len(common_cols)) + \"_.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skf_pd = pd.DataFrame()\nskf_pd['TransactionID'] = test_id\nskf_pd['isFraud'] = np.zeros_like(test_id)\n\nxgb_cols = select_Top_features(XGB_all_feature_score,200)\nada_cols = select_Top_features(AdaBoost_all_feature_score, 200)\n    \ncommon_cols = list(set(xgb_cols).intersection(ada_cols))\nprint(\"Number of features: \" + str(len(common_cols)))\nprint(common_cols)\n    \nXGB_train_set = train_transaction[common_cols]\nXGB_test_set = test_transaction[common_cols]\n\ntotal_roc_auc = 0\ntotal_prc_auc = 0\nfig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10,5))\nfig.tight_layout()\n\nfor i, (train_index, test_index) in enumerate(skf.split(XGB_train_set, train_label)):\n    X_train = XGB_train_set.values\n    Y_label = train_label.values\n    \n    X_train, X_valid = X_train[train_index], X_train[test_index]\n    Y_train, Y_valid = Y_label[train_index], Y_label[test_index]\n    \n    xgb_model = xgb_modelfit(X_train, Y_train)\n    Y_valid_pred = xgb_model.predict_proba(X_valid)\n    \n    fpr, tpr, thresholds = roc_curve(Y_valid, Y_valid_pred[:, 1])\n    roc_auc = auc(fpr, tpr)\n    total_roc_auc = total_roc_auc + roc_auc\n    \n    plt.subplot(1, 2, 1)\n    plt.plot(fpr, tpr, lw=2, alpha=0.3)\n\n    xgb_precision, xgb_recall, _ = precision_recall_curve(Y_valid, Y_valid_pred[:, 1])\n    prc_auc = auc(xgb_recall, xgb_precision)\n    total_prc_auc = total_prc_auc + prc_auc\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(xgb_recall, xgb_precision, lw=2, alpha=0.3)\n    \n    y_pred = xgb_model.predict_proba(XGB_test_set)\n    skf_pd['isFraud'] += y_pred[:, 1]/kfold\n\nplt.subplot(1, 2, 1)\nplt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'black')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Mean area under ROC = %0.2f (80 fold)' % (total_roc_auc/kfold))\n\nplt.subplot(1, 2, 2)\n# axis labels\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Mean area under PRC = %0.2f (80 fold)' % (total_prc_auc/kfold))\nplt.show()\n    \ndel XGB_train_set\ndel XGB_test_set\ndel xgb_model\ndel y_pred\ngc.collect()\n\nskf_pd.to_csv(\"XGB_StratifiedKFold\" + str(len(common_cols)) + \"_.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}