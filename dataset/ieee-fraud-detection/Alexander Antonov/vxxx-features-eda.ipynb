{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this kernel I want to make an univariate analysis of Vxxx features. looking ahead, I think that these features are very important for fraud detection, because they give us a lot of information which transaction is fraudent and which is not, and using these features we can, theoretically, reduce our false negatives and false positives rate. Also they give us a lot of insights about feeature selection.\n\nWe do not know, what these features means, only information we have is:\n\nVxxx: Vesta engineered rich features, including ranking, counting, and other entity relations."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Import libraries\nimport pandas as pd\nimport numpy as np\nimport os\nimport gc\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom IPython.display import Image, display\n\n%matplotlib inline\n\n# Plots look better and clearer in svg format\n%config InlineBackend.figure_format = 'svg' \n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\n# Some aesthetic settings\nplt.style.use('bmh')\nsns.set(style = 'white', font_scale = 0.6, rc={\"grid.linewidth\": 0.5, \"lines.linewidth\": 1})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Loading dataset\ntrain = pd.read_csv('../input/ieee-fraud-detection/train_transaction.csv', index_col = 'TransactionID')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select only Vxx features\nv_cols = [f'V{i}' for i in range(1, 340)]\n# v_cols.append('TransactionAmt')\n\ntrain_df = train[v_cols]\n\ntrain_df['isFraud'] = train['isFraud']\n\ndel train\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def null_table(dataset):\n    \n    '''Create table with ammount of null values for dataset'''\n    \n    return pd.DataFrame({'Null values': dataset.isnull().sum(), \n                         '% of nulls': round((dataset.isnull().sum() / dataset.shape[0]) * 100, 2)}).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Null values in train dataset\nnull_table(train_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vast ammount of features in datasets have a lot of null values, up to 86%."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.describe(include = 'all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see something interesting here - our features have very small numbers in first, second and third quartiles, but maximum value on contrary is very high.\n\nAlso it seems like some features are binary (V1 for example), some looks like ordinal (V2) and some looks like numeric (V126), let's try to divide features by groups."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logic is simple:\n# if feature have only 2 values - it's binary\n# if sum of feature values minus integer sum of feature values equal to zero - it's ordinal\n# else it's numeric\n\nbinary = []\nordinal = []\nnumeric = []\n\nfor col in v_cols:\n    if train_df[col].value_counts().shape[0] == 2:\n        binary.append(col)\n    elif train_df[col].sum() - train_df[col].sum().astype('int') == 0:\n        ordinal.append(col)\n    else:\n        numeric.append(col)\n        \nprint(f'Binary features {len(binary)}: {binary}\\n')\nprint(f'Ordinal features {len(ordinal)}: {ordinal}\\n')\nprint(f'Numeric features {len(numeric)}: {numeric}\\n')        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can start plotting.\n\nFirst - binary features (bars in plots ordered by fraud rate), I'll create function to plot categorical features with fraud rate."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_cat(col, rot = 0, n = False, fillna = np.nan, annot = False, show_rate = True):\n    \n    '''       \n       col - column name       \n       rot - rotation of xticks\n       n - plot only n top values\n       fillna - fill nulls with specified values\n       annot - whether to plot % of transactions\n       show_rate - whether to show fraud rate\n    '''\n    \n    # Fraud rate calculation\n    rate = (train_df[train_df['isFraud'] == 1][col].fillna(fillna).value_counts() /\n            train_df[col].fillna(fillna).value_counts()).sort_values(ascending = False)\n    \n    # Values ordered by fraud rate\n    if n:\n        order = rate.iloc[:n].index\n    else:\n        order = rate.index    \n    \n    g1 = sns.countplot(train_df[col].fillna(fillna), hue = train_df['isFraud'], order = order)\n    g1.set_ylabel('')\n    \n    # Annotations show\n    if annot:\n        for p in g1.patches:\n            g1.annotate('{:.2f}%'.format((p.get_height() / train_df.shape[0]) * 100, 2), \n                       (p.get_x() + 0.05, p.get_height()+5000))\n            \n    plt.xticks(rotation = rot)\n    \n    # Fraud rate show\n    if show_rate:\n        g2 = g1.twinx()\n        g2 = sns.pointplot(x = rate.index.values, y = rate.fillna(0).values, order = order, color = 'black')\n        plt.xticks(rotation = rot)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking ahead, I want to plot fraud rates for different values of features."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_rate(cols, legend = True, figsize = (13, 4), alpha = 1, df = False, fillna = 'Null'):\n    \n    '''\n        Plot fraud rates for selected features\n        cols - list of features\n        legend - whether to show legend\n        figsize - size of plot\n        alpha - alpha\n        df - returns only dataframe if True\n    '''\n    \n    cat = []\n    val = []\n    clmn = []\n\n\n    for col in cols:\n        rate = (train_df[train_df['isFraud'] == 1][col].fillna(fillna).value_counts() /\n                    train_df[col].fillna(fillna).value_counts()).sort_values(ascending = False)\n\n        cat += list(rate.index.values)\n        val += list(rate.values)\n        clmn += [col] * rate.shape[0]\n\n    kur = pd.DataFrame({'cat': cat, 'val': val, 'clmn': clmn})\n    \n    if df:\n        return kur\n\n    fig = plt.figure(figsize = figsize)\n    g = sns.pointplot(x = 'cat', y = 'val', hue = 'clmn', data = kur, plot_kws = dict(alpha = alpha))\n    plt.setp(g.collections, alpha = alpha) #for the markers\n    plt.setp(g.lines, alpha = alpha)       #for the lines\n    plt.legend().set_visible(legend)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_rate(binary, alpha = 0.6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that values for our binary features have very similar fraud rates.\n\nLet's return to plotting"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (11, 15))\nfor i, col in enumerate(binary):\n    plt.subplot(f'42{i}')\n    plot_cat(col, annot = True, fillna = 'Null')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at counts:"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in binary:\n    print(train_df[col].value_counts(), '\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"0 values (2 for V305 feature) of binary features have very small ammount of transactions - less than 0.1% and fraud rate of these transacions is less than 0.1% or equal to 0."},{"metadata":{},"cell_type":"markdown","source":"Next step - Ordinal features.\n\nWe have 257 ordinal features in dataset, i'll plot them by small groups and make some preparations for aesthetic purposes.\n\nFirst, I want to divide them by number of values, if feature have more than 20 unique values - it goes to long_ordinal list, else - to short_ordinal list."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Divide features by number of values\nshort_ordinal = [] # less or equal to 20 values\nlong_ordinal = [] # more than 20 values\n\nfor col in ordinal:\n    if train_df[col].value_counts().shape[0] > 20:\n        long_ordinal.append(col)\n    else:\n        short_ordinal.append(col)\n\nprint(f'Short: {len(short_ordinal)}', short_ordinal, '\\n')\nprint(f'Long: {len(long_ordinal)}', long_ordinal)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at fraud rates."},{"metadata":{"trusted":true},"cell_type":"code","source":"# short_ordinal\nplot_rate(short_ordinal, legend = False, figsize = (13, 7), alpha = 0.4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"6, 3, 4, 2, 5, 7, 8, 9 values don't give us much information, but we can see that a lot of features have fraud rate close to 0 at Null values, as 1 and 0 values.\n\nOn contrary: 15, 24, 23, 17, 16, 18, 19 values have features with fraud rate equal to 1.\n\nIt's a pity that we can't make such plot for long_ordinal, it's too computational expensive and even if we will have unlimited computational resourses, all we got is mess.\n\nSo, I want to use different approach for these features."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Return dataframe for long_ordinal\nlong_df = plot_rate(long_ordinal, df = True)\nlong_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"long_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Group long_df by values and calculate mean and std\nlong_df = long_df['val'].groupby(long_df['cat']).agg(['mean', 'std'])\nlong_df = long_df.dropna()\nprint(long_df.shape)\nlong_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"long_df['cat'] = long_df.index.values\nlong_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Points - mean\n# Bars - std\nfig = plt.figure(figsize = (13, 20))\nsns.barplot(y = 'cat', x = 'std', data = long_df)\nsns.pointplot(y = 'cat', x = 'mean', data = long_df, color = 'black')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's return to the routine and make plots for ordinal features.\n\nWhen I worked on this part, I faced with a problem, when such number of plots just crashed my kernel, so I decided to save plots in .png fomat and show them here as pictures.\n\nAlso, I'm including code for plots in comments."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function for plotting in grid\n'''\ndef plot_grid(data, rows, cols, start, end, rot = 90, n = 50, figsize = (11, 8)):    \n    fig = plt.figure(figsize = figsize)\n    for i, col in enumerate(data[start:end]):\n        plt.subplot(f'{rows}{cols}{i}')\n        plot_cat(col, annot = False, n = n, fillna = 'Null', rot = rot)\n    plt.tight_layout()\n'''\n\n# plot_grid(data = short_ordinal, rows = 3, cols = 3, start = 0, end = 9)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# short_ordinal\nfor i in range(1, 16):\n    display(Image(f'../input/fraud-detection-plots/{i}.png'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see something interesting here. Almost every feature in short_ordinal have a fraud peaks on some value for example V17 and V18 have 6 values with 100% fraud rate. Also every feature have values which fraud rate close or equal to zero."},{"metadata":{"trusted":true},"cell_type":"code","source":"# long_ordinal, includes only 50 values sorted by fraud rate\nfor i in range(16, 32):\n    display(Image(f'../input/fraud-detection-plots/{i}.png'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similar situation here, we can easily see which walues give us 100% or 0% fraud rates.\n\nNext - numeric features. I'll use log1p transformed distribution plots to see fraud and non fraud peaks."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(numeric)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_hist(data, start, end, figsize = (11, 7)):\n    fig = plt.figure(figsize = figsize)\n    for i, col in enumerate(data[start:end]):\n        plt.subplot(f'33{i}')\n        sns.distplot(train_df[col].apply(np.log1p), hist = False, label = 'Train', color = 'black')\n        sns.distplot(train_df[train_df['isFraud'] == 1][col].apply(np.log1p), hist = False, label = 'Fraud', color = 'red')\n        sns.distplot(train_df[train_df['isFraud'] == 0][col].apply(np.log1p), hist = False, label = 'NonFraud', color = 'green')\n        plt.legend()\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(9):\n    plot_hist(numeric, i * 9,  i * 9 + 9)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}