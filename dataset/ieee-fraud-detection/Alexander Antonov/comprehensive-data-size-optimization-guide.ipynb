{"cells":[{"metadata":{},"cell_type":"markdown","source":"The goal of this kernel is to show very simple way to reduce data size without writing kilometers of code with if/else constructions, also I want to give some explanations for beginners - why it works.\n\nSo, let's get our hands dirty."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Import libraries\nimport pandas as pd\nimport numpy as np\nimport gc\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading data\ntrain_tr = pd.read_csv('../input/ieee-fraud-detection/train_transaction.csv', index_col = 'TransactionID')\ntrain_id = pd.read_csv('../input/ieee-fraud-detection/train_identity.csv', index_col = 'TransactionID')\n\ntest_tr = pd.read_csv('../input/ieee-fraud-detection/test_transaction.csv', index_col = 'TransactionID')\ntest_id = pd.read_csv('../input/ieee-fraud-detection/test_identity.csv', index_col = 'TransactionID')\n\n# Join train and test datasets\ntrain_df = train_tr.join(train_id)\ntest_df = test_tr.join(test_id)\n\n# Removing datasets that we don't need anymore\ndel train_id\ndel train_tr\ndel test_id\ndel test_tr\n\ngc.collect()\n\nprint(train_df.shape)\nprint(test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After loading our data and joining it in separate dataseets we can start working on size of our data.\n\nFirst let's look what we have now using info() method."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check memory usage of different features\n# train_df.memory_usage()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that weight of our train and test datasets are about 2GB each. Also we can see that in both datasets we have only 3 datatypes - float64, int64 and object and each columns have size of 4724320 bytes (even isFraud feature). This is most interesting part, because the size of our data depends on datatypes of our features.\n\nHere is short description of dtypes:\n* bool type - consumes 1 byte of memory, range True or False\n\n**int types**:\n* int8 - consumes 1 byte of memory, range from -128 to 127\n* int16 - consumes 2 bytes of memory, range from -32 768 to 32 767\n* int32 - consumes 4 bytes of memory, range from -2 147 483 648 to 2 147 483 648\n* int64 - consumes 8 bytes of memory, range from -9 223 372 036 854 775 808 to 9 223 372 036 854 775 808\n\n**uint types:**\n* uint8 - consumes 1 byte of memory, range from 0 to 255\n* uint16 - consumes 2 bytes of memory, range from 0 to 65 535\n* uint32 - consumes 4 bytes of memory, range from 0 to 4 294 967 295\n* uint64 - consumes 8 bytes of memory, range from 0 to 18 446 744 073 709 551 615\n\n**float types:**\n* float16 - consumes 2 bytes of memory, range from -6.55040e+04 to 6.55040e+04, resolution 0.001\n* float32 - consumes 4 bytes of memory, range from -3.4028235e+38 to 3.4028235e+38, resolution 1e-06\n* float64 - consumes 8 bytes of memory, range from -1.7976931348623157e+308 to 1.7976931348623157e+308, resolution 1e-15"},{"metadata":{"trusted":true},"cell_type":"code","source":"# You can use these commands to see datatypes description\nprint(np.iinfo('int16'))\nprint(np.finfo('float64'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First I will select only numeric columns\nnum_cols = [col for col in train_df.columns.values if str(train_df[col].dtype) != 'object']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To fullfill my curiocity, I'll create small dataframe with minimum and maximum values\ntypes_df = pd.DataFrame({'Col': num_cols, \n              'min': [train_df[col].min() for col in num_cols],\n              'max': [train_df[col].max() for col in num_cols],\n              'dtype': [str(train_df[col].dtype) for col in num_cols]})\n\ntypes_df['dtype_min'] = types_df['dtype'].map({'int64': np.iinfo('int64').min, 'float64': np.finfo('float64').min})\ntypes_df['dtype_max'] = types_df['dtype'].map({'int64': np.iinfo('int64').max, 'float64': np.finfo('float64').max})\ntypes_df.sample(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that int64 or float64 consumes 8 bits of memory and our dataset does not have such big values to store, so we can easily reduce dataset size by downcasting dtypes of our features.\n\nAlso pandas stores strings as 'object' type. If amount of unique values in selected column is less than 50% of the count of these values, than we can convert it to 'category' datatype to reduce memory usage.\n\nSo, let's start data size reduction. First I want to divide numeric features by two groups - to_integer and to_float."},{"metadata":{},"cell_type":"markdown","source":"To change datatype of each feature I'll use pd.to_numeric function with 'downcast' parameter ([link to documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_numeric.html))\n\n**Downcast**: If not None, and if the data has been successfully cast to a numerical dtype (or if the data was numeric to begin with), downcast that resulting data to the smallest numerical dtype possible according to the following rules:\n\n*     ‘integer’ or ‘signed’: smallest signed int dtype (min.: np.int8)\n*     ‘unsigned’: smallest unsigned int dtype (min.: np.uint8)\n*     ‘float’: smallest float dtype (min.: np.float32)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_size(dataset):\n    for col in dataset.columns.values:\n        if str(dataset[col].dtype) == 'object':\n            # Change object to category if needed\n#             dataset[col] = dataset[col].astype('category')\n            continue\n        elif str(dataset[col].dtype)[:3] == 'int':                    \n            dataset[col] = pd.to_numeric(dataset[col], downcast = 'integer')\n        else:   \n            dataset[col] = pd.to_numeric(dataset[col], downcast = 'float')\n        \n    return dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = reduce_size(train_df)\ntest_df = reduce_size(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, using couple strings of code we managed to decrease size of our datasets by 2GB. But It's not all what we can do here, we can see that we have a lot of float32 types in datasets and after some feature engineering and filling of NaN values we can convert them to int or uint types decreasing size of our data even more."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}