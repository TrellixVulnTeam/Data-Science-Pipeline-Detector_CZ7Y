{"cells":[{"metadata":{},"cell_type":"markdown","source":"- To reduce CPU load, we are using kernel output from this kernel : https://www.kaggle.com/mjbahmani/reducing-memory-size-for-ieee\n- We are using some fastai v0.7 functions for preprocessing etc, hence I've added the structured.py file as a utility script : https://www.kaggle.com/priteshshrivastava/fastai-structured\n- This kernel is focussed on model interpretation using Permutation Feature Importance, Partial Dependence Plots and SHAP values."},{"metadata":{"_kg_hide-output":true,"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nfrom IPython.display import display\nfrom pandas_summary import DataFrameSummary\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nimport os\nfrom pandas_summary import DataFrameSummary\nfrom matplotlib import pyplot as plt\nimport math\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nimport graphviz\nimport re\n\nimport shap\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom pdpbox import pdp, get_dataset, info_plots\n\nimport IPython\nfrom IPython.display import display\nprint(os.listdir(\"../input/\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/reducing-memory-size-for-ieee/train.csv\")\ntest_df = pd.read_csv(\"../input/reducing-memory-size-for-ieee/test.csv\")\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll just use a Random Forest Classifier. For that, we need to convert all columns to numeric type. But there are some categorical variables too."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import fastai_structured   ## Adding structured.py from fastai v0.7 as a utility script to the kernel\nfastai_structured.train_cats(train_df)\nfastai_structured.apply_cats(test_df, train_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll replace categories with their numeric codes, handle missing continuous values, and split the dependent variable into a separate variable. Fastai to the rescue again !!"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"nas = {}\ndf_trn, y_trn, nas = fastai_structured.proc_df(train_df, 'isFraud', na_dict=nas)   ## Avoid creating NA columns as total cols may not match later\ndf_test, _, _ = fastai_structured.proc_df(test_df, na_dict=nas)\ndf_trn.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_df, test_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## To handle imbalanced datasets, we'll use [resampling](https://www.kaggle.com/shahules/tackling-class-imbalance)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.under_sampling import RandomUnderSampler\n\nran=RandomUnderSampler(return_indices=True) ##intialize to return indices of dropped rows\ndf_trn_sm,y_trn_sm,dropped = ran.fit_sample(df_trn,y_trn)\n\n#print(\"The number of removed indices are \",len(dropped))\n#plot_2d_space(X_rs,y_rs,X,y,'Random under sampling')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Split the data into training and validation sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X, val_X, train_y, val_y = train_test_split(df_trn_sm, y_trn_sm, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Defining function to calculate the evaluation metric"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\ndef print_score(m):\n    res = [roc_auc_score(m.predict(train_X), train_y), roc_auc_score(m.predict(val_X), val_y)]\n    print(res)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can now pass this processed data frame to Random Forest Classifier."},{"metadata":{"trusted":true},"cell_type":"code","source":"##To reduce CPU load, and for faster iteration\nfastai_structured.set_rf_samples(200000)\ndel df_trn, y_trn, df_trn_sm, y_trn_sm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Initially, let's just fit a single decision tree to visualize it properly"},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\nm = RandomForestClassifier(n_estimators=1, min_samples_leaf=5, max_depth = 3) ## Use all CPUs available\nm.fit(train_X, train_y)\n\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"def draw_tree(t, df, size=10, ratio=0.6, precision=0):\n    \"\"\" Draws a representation of a random forest in IPython.\n    Parameters:\n    -----------\n    t: The tree you wish to draw\n    df: The data used to train the tree. This is used to get the names of the features.\n    \"\"\"\n    s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True,\n                      special_characters=True, rotate=True, precision=precision)\n    IPython.display.display(graphviz.Source(re.sub('Tree {',\n       f'Tree {{ size={size}; ratio={ratio}', s)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#draw_tree(m.estimators_[0], train_X, precision=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A single decision tree did not perform so badly. You can read more about the gini impurity metric [here](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity).\n\nNow, let's bag a collection of trees to create a random forest."},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\nm = RandomForestClassifier(n_estimators=30, min_samples_leaf=20, max_features=0.7, \n                                n_jobs=-1, oob_score=True) ## Use all CPUs available\nm.fit(train_X, train_y)\n\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submitting Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"## pred = m.predict(df_test)          ## Gets an AUC of ~0.8\npred = m.predict_proba(df_test)[:,1]  ## Gets an AUC of ~0.9\nsubmission = pd.read_csv('../input/ieee-fraud-detection/sample_submission.csv')\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['isFraud'] = pred   \nsubmission.to_csv('rf_submission_vf.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}