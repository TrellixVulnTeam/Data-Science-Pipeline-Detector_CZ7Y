{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\n\ndef import_data(file):\n    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n    df = pd.read_csv(file, parse_dates=True, keep_date_col=True, index_col='TransactionID')\n    df = reduce_mem_usage(df)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nprint('Loading data...')\n\ntrain_id = import_data(\"../input/ieee-fraud-detection/train_identity.csv\")\nprint('\\tSuccessfully loaded train_identity!')\n\nX_train = import_data('../input/ieee-fraud-detection/train_transaction.csv')\nprint('\\tSuccessfully loaded train_transaction!')\nX_train = X_train.merge(train_id, how='left', left_index=True, right_index=True) # Train setini kendi içinde merge etmiş\n\n\ntest_id = import_data('../input/ieee-fraud-detection/test_identity.csv')\nprint('\\tSuccessfully loaded test_identity!')\n\nX_test = import_data('../input/ieee-fraud-detection/test_transaction.csv')\nprint('\\tSuccessfully loaded test_transaction!')\n\ntest_id.columns = train_id.columns\nX_test = X_test.merge(test_id, how='left', left_index=True, right_index=True)  # Test setini kendi içinde merge etmiş\n\npd.set_option('max_columns', None)\n\n# TARGET\ny_train = X_train['isFraud'].copy()  # Train deki bağımlı değişkeni y_train setine atamış.\n\n\nprint('Data was successfully loaded!\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**PREPROCESSING**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# CARD 1-6 \n\nvalid_card = pd.concat([X_train[['card1']], X_test[['card1']]])\nvalid_card = valid_card['card1'].value_counts()\nvalid_card_std = valid_card.values.std()\n\ninvalid_cards = valid_card[valid_card<=2]\n\nvalid_card = valid_card[valid_card>2]\nvalid_card = list(valid_card.index)\n\nX_train['card1'] = np.where(X_train['card1'].isin(X_test['card1']), X_train['card1'], np.nan)\nX_test['card1']  = np.where(X_test['card1'].isin(X_train['card1']), X_test['card1'], np.nan)\n\nX_train['card1'] = np.where(X_train['card1'].isin(valid_card), X_train['card1'], np.nan)\nX_test['card1']   = np.where(X_test['card1'].isin(valid_card), X_test['card1'], np.nan)\n\n# burda frekans sayısı 2 den az olan kartlara invalid çok olanlara valid kart demiş\n# sonra train ve testin ikisinde de bulunanları almış eğer birinde bulunmuyorsa nan yapmış\n# sonra invalid olanları nan yapmış\n\n\nfor col in ['card2','card3','card4','card5','card6']: \n    X_train[col] = np.where(X_train[col].isin(X_test[col]), X_train[col], np.nan)\n    X_test[col]  = np.where(X_test[col].isin(X_train[col]), X_test[col], np.nan)\n\n# train ve testin ikisinde de bulunanları almış eğer birinde bulunmuyorsa nan yapmış","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# id_30 - ID_31 \n\nX_train['OS_id_30'] = X_train['id_30'].str.split(' ', expand=True)[0]\nX_train['version_id_30'] = X_train['id_30'].str.split(' ', expand=True)[1]\n\nX_test['OS_id_30'] = X_test['id_30'].str.split(' ', expand=True)[0]\nX_test['version_id_30'] = X_test['id_30'].str.split(' ', expand=True)[1]\n\n\nX_train['browser_id_31'] = X_train['id_31'].str.split(' ', expand=True)[0]\nX_train['version_id_31'] = X_train['id_31'].str.split(' ', expand=True)[1]\n\nX_train['browser_id_31'] = X_train['id_31'].str.split(' ', expand=True)[0]\nX_test['version_id_31'] = X_test['id_31'].str.split(' ', expand=True)[1]\n\n# cihaz ve browser tespitinin onemli oldugu varsayimiyla yapildi...","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TransactionAmt\n\nX_train['TransactionAmt'] = X_train['TransactionAmt'].astype('float32')\n\nX_train['Trans_min_std'] = (X_train['TransactionAmt'] - X_train['TransactionAmt'].mean()) / X_train['TransactionAmt'].std()\n\nX_test['Trans_min_std'] = (X_test['TransactionAmt'] - X_test['TransactionAmt'].mean()) / X_test['TransactionAmt'].std()\n\n# amt ilk halinde float16, bu sekilde std ve mean NAN oluyor, float32 yapmamiz lazim","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**FEATURE ENGINEERING**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# lastest_browser (SON VERSIYON KONTROLU)\n\nX_train[\"lastest_browser\"] = np.zeros(X_train.shape[0])\nX_test[\"lastest_browser\"] = np.zeros(X_test.shape[0])\n\ndef setBrowser(df):\n    df.loc[df[\"id_31\"]==\"samsung browser 7.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"opera 53.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"mobile safari 10.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"google search application 49.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"firefox 60.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"edge 17.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 69.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 67.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 63.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 63.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0 for ios\",'lastest_browser']=1\n    return df\n\nX_train=setBrowser(X_train)\nX_test=setBrowser(X_test)\n\n# son versiyon kontrolu, son versiyon olanlar 1 seklinde belirtilmis","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CARD1 - CARD5 KULLANIM MIKTARLARI\n\nfor feature in ['card1','card5']:\n    X_train[feature + '_count_full'] = X_train[feature].map(pd.concat([X_train[feature], X_test[feature]], ignore_index=True).value_counts(dropna=False))\n    X_test[feature + '_count_full'] = X_test[feature].map(pd.concat([X_train[feature], X_test[feature]], ignore_index=True).value_counts(dropna=False))\n    \n# card larin kullanim yogunlugunun onemli olabilecegini varsayarak bu islem yapildi, sahtekarlik yapanlar yogun kullanilan kartlari... card 1 ve 5 in ozgun oldugu goruldu","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# MAIL ADRESLERININ SON KISIMLARINDAN ULKE TESPITINE YONELIK URETILEN FEATURE (com, us, mx, es, de, fr, uk, jp)\n\nus_emails = ['gmail', 'net', 'edu']\n\nfor c in ['P_emaildomain', 'R_emaildomain']:\n    \n    X_train[c + '_suffix'] = X_train[c].map(lambda x: str(x).split('.')[-1])\n    X_test[c + '_suffix'] = X_test[c].map(lambda x: str(x).split('.')[-1])\n    \n    X_train[c + '_suffix'] = X_train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n    X_test[c + '_suffix'] = X_test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p = 'P_emaildomain'\nr = 'R_emaildomain'\nunknown = 'email_not_provided'\n\ndef setDomain(df):\n    df[p] = df[p].astype('str')\n    df[r] = df[r].astype('str')\n    \n    df[p] = df[p].fillna(unknown)\n    df[r] = df[r].fillna(unknown)\n    \n    df['email_check'] = np.where((df[p]==df[r])&(df[p]!=unknown),1,0)\n\n    df[p+'_prefix'] = df[p].apply(lambda x: x.split('.')[0])\n    df[r+'_prefix'] = df[r].apply(lambda x: x.split('.')[0])\n    \n    return df\n    \nX_train=setDomain(X_train)\nX_test=setDomain(X_test)\n\n# Check if P_emaildomain matches R_emaildomain \n# extracts prefix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TransactionDT degerlerinden icin yeni degiskenler uretilmis.\n\nimport datetime\nfrom pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n\n# start='2017-10-01', end='2019-01-01 arasindaki tarihler listelenmis.\ndates_range = pd.date_range(start='2017-10-01', end='2019-01-01')\n\n# start='2017-10-01', end='2019-01-01 ABD ulusal tatil gunleri listelenmis. \nus_holidays = calendar().holidays(start=dates_range.min(), end=dates_range.max())\n\nSTART_DATE = '2017-12-01'\nstartdate = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\")\n\n\n# islemlerin yapildigi hour of day, day of week ve day of month ve month of year degiskeni olusturulmus.\n\nX_train[\"Date\"] = X_train['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\nX_train['_Weekdays'] = X_train['Date'].dt.dayofweek\nX_train['_Dayhours'] = X_train['Date'].dt.hour\nX_train['_Monthdays'] = X_train['Date'].dt.day\nX_train['_Yearmonths'] = (X_train['Date'].dt.month).astype(np.int8) \n\nX_test[\"Date\"] = X_test['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\nX_test['_Weekdays'] = X_test['Date'].dt.dayofweek\nX_test['_Dayhours'] = X_test['Date'].dt.hour\nX_test['_Monthdays'] = X_test['Date'].dt.day\nX_test['_Yearmonths'] = (X_test['Date'].dt.month).astype(np.int8) \n\n\n# yapilan islem tatil gunumu mu?\n\nX_train['is_holiday'] = (X_train['Date'].dt.date.astype('datetime64').isin(us_holidays)).astype(np.int8)\nX_test['is_holiday'] = (X_test['Date'].dt.date.astype('datetime64').isin(us_holidays)).astype(np.int8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Timestamp tipinde olduğu için algoritma tanımlayamıyor.\nX_train.drop(\"Date\", axis=1,inplace=True)\nX_test.drop(\"Date\", axis=1,inplace=True)\n\n#X_train.drop(\"DT_1\", axis=1,inplace=True)\n#X_test.drop(\"DT_1\", axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ProductCD value_count = (W,C,R,H,S) \n# M4 value_count = (M0,M1,M2)\n\n# kategorik degisken olan ProductCD ve M4, 'fraud' ortalamalarina gore gruplandiriliyor\n\nfor col in ['ProductCD','M4']:\n    temp_dict = X_train.groupby([col])['isFraud'].agg(['mean']).reset_index().rename(columns={'mean': col+'_target_mean'})\n    temp_dict.index = temp_dict[col].values\n    temp_dict = temp_dict[col+'_target_mean'].to_dict()\n    \n    if col=='ProductCD':\n        X_train['ProductCD_1'] = X_train[col].map(temp_dict)\n        X_test['ProductCD_1']  = X_test[col].map(temp_dict)\n    else:\n        X_train['M4_1'] = X_train[col].map(temp_dict)\n        X_test['M4_1']  = X_test[col].map(temp_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train['TransactionAmt_decimal_lenght'] = X_train['TransactionAmt'].astype(str).str.split('.', expand=True)[1].str.len()\nX_test['TransactionAmt_decimal_lenght'] = X_test['TransactionAmt'].astype(str).str.split('.', expand=True)[1].str.len()\n\n#dolar kuruna gore ulke tahmini","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.drop(\"isFraud\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Modelde D ve D' nin normalize edilmiş  kolonlarının çıkarılmış halini de deneyeceğiz.\n# The D Columns are \"time deltas\" from some point in the past. We will transform the D Columns into their point in the past.\n# NORMALIZE D COLUMNS\nfor i in range(1,16):\n    if i in [1,2,3,5,9]:\n        continue\n    X_train['D'+str(i)] =  X_train['D'+str(i)] - X_train.TransactionDT/np.float32(24*60*60)\n    X_test['D'+str(i)] = X_test['D'+str(i)] - X_test.TransactionDT/np.float32(24*60*60)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# BURASI KENDİMİZCE YAZILACAK VE FONKSİYON OLARAK TANIMLANACAK.\n\n\"\"\"\n\n#### FEATURE ENGINEERING ISLEMLERINDEN USERID BELIRLENDIKTEN SONRA USERID'YE GORE YAPILACAK AGGREGATION ISLEMLERININ SON HALINE BURADA KARAR VERILECEK\n\nX_train['mean_last'] = X_train['TransactionAmt'] - X_train.groupby('userid')['TransactionAmt'].transform(lambda x: x.rolling(10, 1).mean())\nX_train['min_last'] = X_train.groupby('userid')['TransactionAmt'].transform(lambda x: x.rolling(10, 1).min())\nX_train['max_last'] = X_train.groupby('userid')['TransactionAmt'].transform(lambda x: x.rolling(10, 1).max())\nX_train['std_last'] = X_train['mean_last'] / X_train.groupby('userid')['TransactionAmt'].transform(lambda x: x.rolling(10, 1).std())\n\nX_train['mean_last'].fillna(0, inplace=True, )\nX_train['std_last'].fillna(0, inplace=True)\n\n\n\nX_test['mean_last'] = X_test['TransactionAmt'] - X_test.groupby('userid')['TransactionAmt'].transform(lambda x: x.rolling(10, 1).mean())\nX_test['min_last'] = X_test.groupby('userid')['TransactionAmt'].transform(lambda x: x.rolling(10, 1).min())\nX_test['max_last'] = X_test.groupby('userid')['TransactionAmt'].transform(lambda x: x.rolling(10, 1).max())\nX_test['std_last'] = X_test['mean_last'] / X_test.groupby('userid')['TransactionAmt'].transform(lambda x: x.rolling(10, 1).std())\n\nX_test['mean_last'].fillna(0, inplace=True, )\nX_test['std_last'].fillna(0, inplace=True)\n\n\n\n\nX_train['TransactionAmt_to_mean_card_id'] = X_train['TransactionAmt'] - X_train.groupby(['userid'])['TransactionAmt'].transform('mean')\nX_train['TransactionAmt_to_std_card_id'] = X_train['TransactionAmt_to_mean_card_id'] / X_train.groupby(['userid'])['TransactionAmt'].transform('std')\nX_test['TransactionAmt_to_mean_card_id'] = X_test['TransactionAmt'] - X_test.groupby(['userid'])['TransactionAmt'].transform('mean')\nX_test['TransactionAmt_to_std_card_id'] = X_test['TransactionAmt_to_mean_card_id'] / X_test.groupby(['userid'])['TransactionAmt'].transform('std')\n\n\n\nX_train = X_train.replace(np.inf,999)# sonsuz değerleri 999 ile değiştiriyor\nX_test = X_test.replace(np.inf,999)\n\n\"\"\"\n\n\n# GROUP AGGREGATION MEAN AND STD\n# https://www.kaggle.com/kyakovlev/ieee-fe-with-some-eda\n# AMAÇ: seçilen değişkenlerin uids lere göre gruplanıp ortalamaların alıp yeni değişkene atamak.\ndef encode_AG(main_columns, uids, aggregations=['mean'], train_df=X_train, test_df=X_test, \n              fillna=True, usena=False):\n    # AGGREGATION OF MAIN WITH UID FOR GIVEN STATISTICS\n    for main_column in main_columns:  \n        for col in uids:\n            for agg_type in aggregations:\n                new_col_name = main_column+'_'+col+'_'+agg_type # sectigi kolon isimlerini ve aggregation degerini birleştirip yeni değişken ismi oluşturmuş\n                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]]) # Train ve Test setten seçtiği değişkenleri alt alta birleştirmiş.\n                if usena: \n                    temp_df.loc[temp_df[main_column]==-1,main_column] = np.nan # main_colums dan gelen değişkende -1 olan değerleri nan yapar.\n                # col değişkenine göre groupby atmış ve main_column değişkenine agg_type türüne göre işlem yapmış ve bunu new_col_name isimli yeni değişkene atamış.İNDEX İ DE SIFIRLADI.\n                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n                                                        columns={agg_type: new_col_name})\n                temp_df.index = list(temp_df[col]) # temp_df nin sıfırlanan indexleri col değişkeninin index leriyle değiştirildi.\n                temp_df = temp_df[new_col_name].to_dict()  # Yni oluşturulan değişken sözlük türüne çevirildi.\n                 \n                # Bu yeni değerler Train ve Test setlerinde ilgili kolona(col değişkeni) karşılık gelecek şekilde  \"new_col_name\" ismiyle Train ve Test sete eklendi.\n                train_df[new_col_name] = train_df[col].map(temp_df).astype('float32')\n                test_df[new_col_name]  = test_df[col].map(temp_df).astype('float32')\n                \n                # Yeni olusturulan degiskenlerdeki nan değerler yerine -1 yazdırır.\n                if fillna:\n                    train_df[new_col_name].fillna(-1,inplace=True)\n                    test_df[new_col_name].fillna(-1,inplace=True)\n                \n                print(\"'\"+new_col_name+\"'\",', ',end='')\n\n      \n    \n    \n # GROUP AGGREGATION NUNIQUE           \n            \n# AMAÇ: uids lere göre main_colums daki değişkenler gruplanır ve ve bu main_column daki farklı değerler sayılır ve \n# bu toplam sayı Test ve Trainde ilgili \"TransactionID\" nin karşısına atanır.\n\ndef encode_AG2(main_columns, uids, train_df=X_train, test_df=X_test):\n    for main_column in main_columns:  \n        for col in uids:\n            comb = pd.concat([train_df[[col]+[main_column]],test_df[[col]+[main_column]]],axis=0)\n            #Burada mp aynı col değişkenine denk gelen main_column değişkenindeki farklı değerlerin toplam sayısıdır.\n            mp = comb.groupby(col)[main_column].agg(['nunique'])['nunique'].to_dict()\n            \n            # \"uid_P_emaildomain_ct\" şeklinde değişken oluşturulur ve Train ile test setinde col değişkenindeki değerlere \"TransactionID\" baz alınarak mp değerleri eşlenir.\n            train_df[col+'_'+main_column+'_ct'] = train_df[col].map(mp).astype('float32')\n            test_df[col+'_'+main_column+'_ct'] = test_df[col].map(mp).astype('float32')\n            \n            print(col+'_'+main_column+'_ct, ',end='')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### NOT: BURADAKİ LABEL ENCODERLARDAN 1 İNİ SEÇ VE ALTTAKİ ELİF KISMINI AYRI BİR FOR DÖNGÜSÜ NDE NUMERİKLERE İŞLEM YAPACAK ŞEKİLDE TANIMLA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# SHIFT ALL NUMERICS POSITIVE. SET NAN to -1 \ndef num_positiv(X_train,X_test):\n    for f in X_train.columns:  \n        # Bütün nümerik değerleri pozitif yap ve NAN değerleri -1 yap. ['TransactionAmt','TransactionDT'] kolonları hariç.\n        if f not in ['TransactionAmt','TransactionDT',\"isFraud\"]: \n            mn = np.min((X_train[f].min(),X_test[f].min()))  # X_train ve X_test deki f kolonunun minimum degerlerini kıyasla ve en küçük olanını \"mn\" ye ata.\n\n            # Buradaki amaç bütün değerlerden en küçük değeri çıkartarak onları pozitif yaparken aralarındaki değer farkınıda korumaktır.\n            X_train[f] -= np.float32(mn)   # X_train deki f kolonundaki değerlerden mn yi yani en küçük değeri çıkarır.\n            X_test[f] -= np.float32(mn)    # X_test deki f kolonundaki değerlerden mn yi yani en küçük değeri çıkarır.\n            X_train[f].fillna(-1,inplace=True)  # X_train deki NaN değerleri -1 ile doldurur.\n            X_test[f].fillna(-1,inplace=True)   # X_test deki NaN değerleri -1 ile doldurur.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# FREQUENCY ENCODE TOGETHER\n# AMAÇ: encode_FE fonksiyonu girilen data setlerindeki belirtilen kolonları normalize edip türlerini \"float32\" ye çevirip _FE uzantılı yeni bir değişken olarak data setlerine ekler.\ndef encode_FE(df1, df2, cols):\n    for col in cols:\n        df = pd.concat([df1[col],df2[col]])\n        vc = df.value_counts(dropna=True).to_dict() # col. kolonundaki unique değerleri alıp bunları normalize ediyor ve listeye çevirip vc değişkeninde saklıyor.\n        vc[-1] = -1       # vc.  sözlüğüne -1 key adı ile -1 değerini ekliyor.\n        nm = col+'_FE'    # kolon isimlerine uyguladığı FE encode uzantısını ekliyor.\n        df1[nm] = df1[col].map(vc) #vc deki keys değerleri ile df1[col] daki index değerlerini eşleyip karşılığına vc deki values değerlerini atayıp bunu df1'e yeni değişken olarak atar.\n        df1[nm] = df1[nm].astype('float32') # yeni değişkenin türünü \"float32\" yapar.\n        df2[nm] = df2[col].map(vc) #vc deki keys değerleri ile df2[col] daki index değerlerini eşleyip karşılığına vc deki values değerlerini atayıp bunu df2'ye yeni değişken olarak atar.\n        df2[nm] = df2[nm].astype('float32')\n        print(nm,', ',end='')\n        \n\n# LABEL ENCODE\n# AMAÇ: BURADA her bir kolondaki unique değişkene bir factorize ile bir kod atamış ve bunu \"df_comb\" de saklamış. \n# df_comb de her bir uniqe değişkene 0 dan başlayıp 1 artırarak değer veriyor. Çoklayan değerlere aynı unique değeri atıyor.\n# \"_\" de ise col dan gelen gerçek değerleri saklıyor. \n# Kolonlardaki max değeri 32000 den küçükse bu kolonun türünü \"int16\" yapmış aksi halde \"int32\" yapmış.\ndef encode_LE(col,train=X_train,test=X_test,verbose=True, val=2):\n    \n    if val ==2:\n        df_comb = pd.concat([train[col],test[col]],axis=0)\n        df_comb,_ = df_comb.factorize(sort=True)\n        nm = col\n        if df_comb.max()>32000: \n            train[nm] = df_comb[:len(train)].astype('int32')\n            test[nm] = df_comb[len(train):].astype('int32')\n        else:\n            train[nm] = df_comb[:len(train)].astype('int16')\n            test[nm] = df_comb[len(train):].astype('int16')\n        del df_comb; x=gc.collect()\n        if verbose: \n            print(nm,', ',end='')\n            \n    else:\n        # LABEL ENCODE AND MEMORY REDUCE\n        for f in  col:  \n    # FACTORIZE CATEGORICAL VARIABLES\n            if (np.str(X_train[f].dtype)=='category')|(X_train[f].dtype=='object'): # seçilen f kolonu object veya kategorik ise\n                df_comb = pd.concat([X_train[f],X_test[f]],axis=0)     # X_train ve X_test in f kolon değerlerini alt alta birleştir.\n                df_comb,_ = df_comb.factorize(sort=True)  # Burada factorize ile f kolonundaki her bir unique değere bir unique ıd atıyor. df_comb ise kolondaki değerleri alır.(label encode)\n                if df_comb.max()>32000:  # Eğer f kolonunun max değeri 32000 den büyükse f in int32 ye cevrilmesi lazım yazdır. \n                    train[f] = df_comb[:len(train)].astype('int32')\n                    test[f] = df_comb[len(train):].astype('int32')\n                else:        \n                    X_train[f] = df_comb[:len(X_train)].astype('int16')   # X_train kategorik kolonunun dtype ını int16 ya cevir.\n                    X_test[f] = df_comb[len(X_train):].astype('int16')    # X_test kategorik kolonunun dtype ını int16 ya cevir.\n\n        \n\n                \n                \n                \n# COMBINE FEATURES\ndef encode_CB(col1,col2,df1=X_train,df2=X_test):\n    nm = col1+'_'+col2\n    df1[nm] = df1[col1].astype(str)+'_'+df1[col2].astype(str)   # 12926.0_215.0, 3663.0_230.0  şeklinde çıktı üretir.\n    df2[nm] = df2[col1].astype(str)+'_'+df2[col2].astype(str) \n    \n    encode_LE(nm,verbose=False) # yukarıdaki değerleri 44945, 60885 şeklinde gibi pozitif çıktıya dönüştürür.Ayrıntılı bilgi için encode_LE ye bakın.\n    print(nm,', ',end='')\n    \n    \n    \n\n\n        \n        \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# YUKARIDAKİ FONKSİYONLARIN KULLANILDIĞI ALAN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.drop(\"browser_id_31\", axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Burada 'TransactionAmt' değerlerinin küsüratlı kısımlarını alıp \"cents\" adlı değişkene atadı.\n# TRANSACTION AMT CENTS\nX_train['cents'] = (X_train['TransactionAmt'] - np.floor(X_train['TransactionAmt'])).astype('float32')\nX_test['cents'] = (X_test['TransactionAmt'] - np.floor(X_test['TransactionAmt'])).astype('float32')\nprint('cents, ', end='')\n\n\n# FREQUENCY ENCODE: ADDR1, CARD1, CARD2, CARD3, P_EMAILDOMAIN\n# AMAÇ: encode_FE fonksiyonu girilen data setlerindeki belirtilen kolonları normalize edip türlerini \"float32\" ye çevirip _FE uzantılı yeni bir değişken olarak data setlerine ekler.\nencode_FE(X_train,X_test,['addr1','card1','card2','card3','P_emaildomain'])\n\n# COMBINE COLUMNS CARD1+ADDR1, CARD1+ADDR1+P_EMAILDOMAIN\nencode_CB('card1','addr1')\nencode_CB('card1_addr1','P_emaildomain')\n\n# FREQUENCY ENOCDE\nencode_FE(X_train,X_test,['card1_addr1','card1_addr1_P_emaildomain'])\n\n# GROUP AGGREGATE\nencode_AG(['TransactionAmt','D9','D11'],['card1','card1_addr1','card1_addr1_P_emaildomain'],['mean','std'],usena=True)\n\nprint(1)\n\nimport datetime\nSTART_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')\nX_train['DT_M'] = X_train['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\nX_train['DT_M'] = (X_train['DT_M'].dt.year-2017)*12 + X_train['DT_M'].dt.month \n\nX_test['DT_M'] = X_test['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\nX_test['DT_M'] = (X_test['DT_M'].dt.year-2017)*12 + X_test['DT_M'].dt.month \n\nprint(2)\n\n# X_train['uid'] ler 10230_-13.0, 24904_-111.0 şeklinde değerler alacak.\nX_train['day'] = X_train.TransactionDT / (24*60*60)\nX_train['uid'] = X_train.card1_addr1.astype(str)+'_'+np.floor(X_train.day-X_train.D1).astype(str)\n\n\nX_test['day'] = X_test.TransactionDT / (24*60*60)\nX_test['uid'] = X_test.card1_addr1.astype(str)+'_'+np.floor(X_test.day-X_test.D1).astype(str)\n\nprint(3)\n# LABEL ENCODE AND MEMORY REDUCE\nencode_LE(X_train.columns,train=X_train,test=X_test,verbose=True, val=1)\nprint(4)\n# SHIFT ALL NUMERICS POSITIVE. SET NAN to -1 \nnum_positiv(X_train,X_test)\nprint(5)\n\n# FREQUENCY ENCODE UID\nencode_FE(X_train,X_test,['uid'])\nprint(6)\n# AGGREGATE \nencode_AG(['TransactionAmt','D4','D9','D10','D15'],['uid'],['mean','std'],fillna=True,usena=True)\nprint(7)\n\n# AGGREGATE\n# Burada C değişkenlerini (c3 hariç) uid e göre gruplayıp mean değerlerini almış.\nencode_AG(['C'+str(x) for x in range(1,15) if x!=3],['uid'],['mean'],X_train,X_test,fillna=True,usena=True)\nprint(8)\n# AGGREGATE\n# Burada M değişkenlerini uid e göre gruplayıp mean değerlerini almış.\nencode_AG(['M'+str(x) for x in range(1,10)],['uid'],['mean'],fillna=True,usena=True)\nprint(9)\n# AGGREGATE\n# Bu değişkenleri uid e göre gruplamış ve içlerindeki her bir unique değişkene bir sıfırdan başlayarak bir değer vermiş. \n# sonra bu değerleri train ve test setteki yerlerine yayeni değişken olarak atamış.\nencode_AG2(['P_emaildomain','dist1','DT_M','id_02','cents'], ['uid'], train_df=X_train, test_df=X_test)\nprint(10)\n# AGGREGATE\nencode_AG(['C14'],['uid'],['std'],X_train,X_test,fillna=True,usena=True)\nprint(11)\n# AGGREGATE \nencode_AG2(['C13','V314'], ['uid'], train_df=X_train, test_df=X_test)\nprint(12)\n# AGGREATE \nencode_AG2(['V127','V136','V309','V307','V320'], ['uid'], train_df=X_train, test_df=X_test)\nprint(13)\n# NEW FEATURE\n# Burada D1 den D15 i çıkartarak mutlak değerini almış ve değeri 3 ten büyük olanları int8 formatına çevirip train ve test setlerine yeni değişken olarak atamış.\nX_train['outsider15'] = (np.abs(X_train.D1-X_train.D15)>3).astype('int8')\nX_test['outsider15'] = (np.abs(X_test.D1-X_test.D15)>3).astype('int8')\nprint('outsider15')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n####### Clip max values\nfor df in [X_train, X_test]:\n    for col in i_cols:\n        max_value = df[df['DT_M']==df['DT_M'].max()][col].max()\n        df[col] = df[col].clip(None,max_value)\n\n\"\"\"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Burada ilk modelde yapmış olduğu testler sonucu tespit etmiş olduğu featureları çıkartmış.\n\ncols = list( X_train.columns )\ncols.remove('TransactionDT')\nfor c in ['D6','D7','D8','D9','D12','D13','D14']:\n    if c in cols:\n        cols.remove(c)\n    \n# Bu değişkenleri gruplamalar için kullandıktan sonra çıkartıyor. \nfor c in [\"DT_M\",'day','uid','Card_ID','uid1','uid2','uid3','uid4','uid5']:\n    if c in cols:\n        cols.remove(c)\n    \n# FAILED TIME CONSISTENCY TEST (19 değişkeni çıkartır)\nfor c in ['C3','M5','id_08','id_33']:\n    if c in cols:\n        cols.remove(c)\nfor c in ['card4','id_07','id_14','id_21','id_30','id_32','id_34']:\n    if c in cols:\n        cols.remove(c)\nfor c in ['id_'+str(x) for x in range(22,28)]:\n    if c in cols:\n        cols.remove(c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NAN DEĞERLER VARSA BU DEĞERLERİN MODELDEN ÖNCE BİR ŞEYLE DEĞİŞTİRİLMESİ LAZIM. YUKARIDA -999 İLE DEĞİŞTİRMİŞTİK.\nX_train = X_train.fillna(-1)\nX_test = X_test.fillna(-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = reduce_mem_usage(X_train)\nX_test = reduce_mem_usage(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MODEL KISMI","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X_train, y_train, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hyperparameters Tuning and Best fFeature Selection","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV\n\ndef auc(m, train, test,y_train1,y_test1): \n    return (metrics.roc_auc_score(y_train1,m.predict_proba(train)[:,1]),\n                            metrics.roc_auc_score(y_test1,m.predict_proba(test)[:,1]))\n\n# Parameter Tuning\nmodel = xgb.XGBClassifier()\nparam_dist = {\"max_depth\": [8,12],\n              \"min_child_weight\" : [1],\n              \"n_estimators\": [200],\n              \"learning_rate\": [0.14,0.2, 0.25],\n              \"nthread\":[4],\n              \"tree_method\":[\"gpu_hist\"],\n              \"random_state\": [2]}\n                             \ngrid_search = GridSearchCV(model, param_grid=param_dist, cv = 3, \n                                   verbose=10,n_jobs =-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.fit(X_train1, y_train1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_depth_best = grid_search.best_estimator_.max_depth\nn_estimators_best = grid_search.best_estimator_.n_estimators\nlearning_rate_best = grid_search.best_estimator_.learning_rate\nmin_child_weight_best = grid_search.best_estimator_.min_child_weight","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nprint(\"XGBoost version:\", xgb.__version__)\n\nif BUILD96:\n    clf = xgb.XGBClassifier( \n        n_estimators=n_estimators_best,\n        max_depth=max_depth_best, \n        learning_rate=learning_rate_best, \n        subsample=0.8,\n        colsample_bytree=0.4, \n        missing=-1, \n        eval_metric='auc',\n        min_child_weight = min_child_weight_best,\n        #nthread=4,\n        #tree_method='hist' \n        tree_method='gpu_hist' \n    )\n    h = clf.fit(X_train1, y_train1, \n        eval_set=[(X_test1,y_test1)],\n        verbose=50, early_stopping_rounds=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if BUILD96:\n\n    feature_imp = pd.DataFrame(sorted(zip(clf.feature_importances_,X_train.columns)), columns=['Value','Feature'])\n\n    plt.figure(figsize=(20, 10))\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).iloc[:50])\n    plt.title('XGB96 Most Important')\n    plt.tight_layout()\n    plt.show()\n        \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat = feature_imp.sort_values(by='Value', ascending=False)\nfeat_best = feat[\"Feature\"][:350]\nfeat_best_list =feat_best.to_list()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_best = X_train[feat_best_list]\nX_test_best = X_test[feat_best_list]\nX_train_best.shape, X_test_best.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del clf, h; x=gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MODEL AND PREDICTION","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"groups  =X_train['DT_M']\ngroup_kfold = GroupKFold(n_splits=6)\ngroup_kfold.get_n_splits(X_train_best, y_train, groups)\n\nprint(group_kfold)\n\nfor train_index, test_index in group_kfold.split(X_train_best, y_train, groups):\n    print(\"\\nTRAIN:\", train_index, \"TEST:\", test_index)\n    X_train_df, X_test_df = X_train_best.iloc[train_index], X_train_best.iloc[test_index]\n    y_train_df, y_test_df = y_train.iloc[train_index], y_train.iloc[test_index]\n    \nclf = xgb.XGBClassifier(\n            n_estimators=n_estimators_best,\n            max_depth=max_depth_best,\n            learning_rate=learning_rate_best,\n            subsample=0.8,\n            colsample_bytree=0.4,\n            missing=-1,\n            eval_metric='auc',\n            min_child_weight = min_child_weight_best,\n            # USE CPU\n            #nthread=4,\n            #tree_method='hist'\n            # USE GPU\n            tree_method='gpu_hist' \n        )        \nh = clf.fit(X_train_df, y_train_df, eval_set=[(X_test_df,y_test_df)],verbose=100, early_stopping_rounds=200)\n    \noof = clf.predict_proba(X_train_df)[:,1]\npreds = clf.predict_proba(X_test_best)[:,1]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n\n\npred1 = clf.predict(X_test_df)\nfpr, tpr, thresholds = metrics.roc_curve(y_test_df, pred1, pos_label=2)\nmetrics.auc(fpr, tpr)\n\nprint(metrics.confusion_matrix(y_test_df, pred1))\nprint(metrics.classification_report(y_test_df, pred1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(oof,bins=100)\nplt.ylim((0,5000))\nplt.title('XGB OOF')\nplt.show()\n\nX_train['oof'] = oof\nX_train.reset_index(inplace=True)\nX_train[['TransactionID','oof']].to_csv('oof_xgb_96.csv')\nX_train.set_index('TransactionID',drop=True,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv('../input/ieee-fraud-detection/sample_submission.csv')\nsample_submission.isFraud = preds\nsample_submission.to_csv('sub_xgb_99.csv',index=False)\n\nplt.hist(sample_submission.isFraud,bins=100)\nplt.ylim((0,5000))\nplt.title('XGB96 Submission')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}