{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data overview\n\n## Data description\n\nOriginally from [this](https://www.kaggle.com/c/ieee-fraud-detection/discussion/101203) discussion, but I've updated it to be clearer:\n\n### Transaction Table:\n\n-   TransactionDT: timedelta from a given reference datetime (not an actual timestamp).\n-   TransactionAMT: transaction payment amount in USD.\n-   ProductCD: product code, the product for each transaction.\n-   card1 - card6: payment card information, such as card type, card category, issue bank, country, etc.\n-   addr1: billing region.\n-   addr2: billing country.\n-   dist: distances between (not limited) billing address, mailing address, zip code, IP address, phone area, etc.\n-   P\\_ and (R\\_\\_) emaildomain: purchaser and recipient email domain, certain transactions don't need recipient, so R_emaildomain is null.\n-   C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.\n-   D1-D15: timedelta, such as days between previous transaction, etc.\n-   M1-M9: match, such as names on card and address, etc.\n-   Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.\n\n#### Categorical Features in Transaction Table:\n\n-   ProductCD\n-   card1 - card6\n-   addr1, addr2\n-   P_emaildomain\n-   R_emaildomain\n-   M1 - M9\n\n### Identity Table:\n\nVariables in this table are identity information – network connection information (IP, ISP, Proxy, etc) and digital signature (UA/browser/os/version, etc) associated with transactions.\nThey're collected by Vesta’s fraud protection system and digital security partners.\n(The field names are masked and pairwise dictionary will not be provided for privacy protection and contract agreement)\n\n#### Categorical Features in Identity Table:\n\n-   DeviceType\n-   DeviceInfo\n-   id_12 - id_38\n","metadata":{}},{"cell_type":"markdown","source":"# Data cleaning\n\nIn this section, we are going to remove some columns and correct a small number of values.","metadata":{}},{"cell_type":"code","source":"# Import libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport platform\n\nDATA_ROOT = \"../input/ieee-fraud-detection\"\nOUTPUT_ROOT = \".\"\n\n!dir \"../input/ieee-fraud-detection\"","metadata":{"execution":{"iopub.status.busy":"2022-01-08T05:13:10.916295Z","iopub.execute_input":"2022-01-08T05:13:10.916809Z","iopub.status.idle":"2022-01-08T05:13:13.00843Z","shell.execute_reply.started":"2022-01-08T05:13:10.916644Z","shell.execute_reply":"2022-01-08T05:13:13.007368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check environment\nif platform.system() == \"Windows\":\n    local = True\nelse:\n    local = False\n\n# Load data\nif local:\n    OUTPUT_ROOT = DATA_ROOT\n    train_transaction = pd.read_csv(f'{DATA_ROOT}/train_transaction.csv', nrows=10000)\n    test_transaction = pd.read_csv(f'{DATA_ROOT}/test_transaction.csv', nrows=10000)\n    # train_identity = pd.read_csv(f'{DATA_ROOT}/train_identity.csv', nrows=10000)\n    # test_identity = pd.read_csv(f'{DATA_ROOT}/test_identity.csv', nrows=10000)\nelse:\n    train_transaction = pd.read_csv(f'{DATA_ROOT}/train_transaction.csv')\n    test_transaction = pd.read_csv(f'{DATA_ROOT}/test_transaction.csv')\n    # train_identity = pd.read_csv(f'{DATA_ROOT}/train_identity.csv')\n    # test_identity = pd.read_csv(f'{DATA_ROOT}/test_identity.csv')","metadata":{"execution":{"iopub.status.busy":"2022-01-08T05:13:13.010659Z","iopub.execute_input":"2022-01-08T05:13:13.011685Z","iopub.status.idle":"2022-01-08T05:14:34.012508Z","shell.execute_reply.started":"2022-01-08T05:13:13.011636Z","shell.execute_reply":"2022-01-08T05:14:34.011223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compare_data_card(train_data, test_data, plot=True):\n    # Shape\n    print(f'Train data shape: {train_data.shape}')\n    print(f'Test data shape: {test_data.shape}')\n\n    # Count unique values of card and address columns\n    print('\\nUnique values of card and address columns in train data:')\n    for i in range(6):\n        j = i+1\n        print(f'card{j}: {train_data[\"card{}\".format(j)].nunique()}', end=', ')\n    for i in range(2):\n        j = i+1\n        print(f'addr{j}: {train_data[\"addr{}\".format(j)].nunique()}', end=', ')\n\n    print('\\nUnique values of card and address columns in test data:')\n    for i in range(6):\n        j = i+1\n        print(f'card{j}: {test_data[\"card{}\".format(j)].nunique()}', end=', ')\n    for i in range(2):\n        j = i+1\n        print(f'addr{j}: {test_data[\"addr{}\".format(j)].nunique()}', end=', ')\n\n    # Unique values in card6 column\n    print('\\nUnique values in card6 column in train data:')\n    print(train_data['card6'].value_counts())\n\n    print('Unique values in card6 column in test data:')\n    print(test_data['card6'].value_counts())\n\n    # Unique values in card4 column\n    print('Unique values in card4 column in train data:')\n    print(train_data['card4'].value_counts())\n\n    print('Unique values in card4 column in test data:')\n    print(test_data['card4'].value_counts())\n\n    if plot:\n        # Plot histogram of card1, card2, card3, card5\n        plt.figure(figsize=(20, 10))\n        plt.subplot(4, 2, 1)\n        sns.histplot(train_data['card1'])\n        plt.title('Train')\n        plt.subplot(4, 2, 2)\n        sns.histplot(test_data['card1'])\n        plt.title('Test')\n        plt.subplot(4, 2, 3)\n        sns.histplot(train_data['card2'])\n        plt.title('Train')\n        plt.subplot(4, 2, 4)\n        sns.histplot(test_data['card2'])\n        plt.title('Test')\n        plt.subplot(4, 2, 5)\n        sns.histplot(train_data['card3'])\n        plt.title('Train')\n        plt.subplot(4, 2, 6)\n        sns.histplot(test_data['card3'])\n        plt.title('Test')\n        plt.subplot(4, 2, 7)\n        sns.histplot(train_data['card5'])\n        plt.title('Train')\n        plt.subplot(4, 2, 8)\n        sns.histplot(test_data['card5'])\n        plt.title('Test')\n        plt.tight_layout()\n        plt.show()\n\n        # Plot histogram of addr1, addr2\n        plt.figure(figsize=(20, 10))\n        plt.subplot(2, 2, 1)\n        sns.histplot(train_data['addr1'])\n        plt.title('Train')\n        plt.subplot(2, 2, 2)\n        sns.histplot(test_data['addr1'])\n        plt.title('Test')\n        plt.subplot(2, 2, 3)\n        sns.histplot(train_data['addr2'])\n        plt.title('Train')\n        plt.subplot(2, 2, 4)\n        sns.histplot(test_data['addr2'])\n        plt.title('Test')\n        plt.tight_layout()\n        plt.show()\n\n\ncompare_data_card(train_transaction, test_transaction)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-08T05:14:34.014522Z","iopub.execute_input":"2022-01-08T05:14:34.014884Z","iopub.status.idle":"2022-01-08T05:14:44.058889Z","shell.execute_reply.started":"2022-01-08T05:14:34.014834Z","shell.execute_reply":"2022-01-08T05:14:44.058266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At first glance, we can see that the test data doesn't have value \"debit or credit\" in *card6* column and the number of rows contains value \"debit or credit\" or \"charge\" in *card6* column is very small. Thus, I decided to remove the rows with value \"debit or credit\" in *card6* and change the \"charge\" value into \"credit\" value since charge card is a type of credit card.\n\nThere seems to be not many differences in *card1*, *card2*, *card3*, *card5*, *addr1* and *addr2* column between train data and test data.","metadata":{}},{"cell_type":"code","source":"def correct_card6_column(df):\n    result_df = df.copy()\n    \n    # Remove rows where card6 is \"debit or credit\"\n    result_df = result_df[result_df.card6 != 'debit or credit']\n\n    # Change \"charge\" values in card6 column into \"credit\"\n    result_df.loc[result_df.card6 == 'charge', 'card6'] = 'credit'\n\n    return result_df\n\ntrain_transaction = correct_card6_column(train_transaction)\ntest_transaction = correct_card6_column(test_transaction)\n\ncompare_data_card(train_transaction, test_transaction, False)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T05:14:44.060175Z","iopub.execute_input":"2022-01-08T05:14:44.060524Z","iopub.status.idle":"2022-01-08T05:14:48.804022Z","shell.execute_reply.started":"2022-01-08T05:14:44.060494Z","shell.execute_reply":"2022-01-08T05:14:48.803141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compare_product_code(train_transaction, test_transaction):\n    # Unique values in ProductCD column in train data\n    print('\\nUnique values in ProductCD column in train data:', end=' ')\n    print(train_transaction['ProductCD'].unique())\n\n    # Unique values in ProductCD column in test data\n    print('Unique values in ProductCD column in test data:', end=' ')\n    print(test_transaction['ProductCD'].unique())\n\ncompare_product_code(train_transaction, test_transaction)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T05:14:48.807213Z","iopub.execute_input":"2022-01-08T05:14:48.807586Z","iopub.status.idle":"2022-01-08T05:14:48.896433Z","shell.execute_reply.started":"2022-01-08T05:14:48.807539Z","shell.execute_reply":"2022-01-08T05:14:48.895349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There also seems to be no mismatch in ProductCD between train and test data.","metadata":{}},{"cell_type":"code","source":"def compare_email(train_transaction, test_transaction):\n    # Count unique values of P_emaildomain colum in train data\n    print('Unique values of P_emaildomain column in train data:', end=' ')\n    print(train_transaction['P_emaildomain'].nunique())\n\n    # Count unique values of P_emaildomain colum in test data\n    print('Unique values of P_emaildomain column in test data:', end=' ')\n    print(test_transaction['P_emaildomain'].nunique())\n\n    # Count unique values of R_emaildomain colum in train data\n    print('Unique values of R_emaildomain column in train data:', end=' ')\n    print(train_transaction['R_emaildomain'].nunique())\n\n    # Count unique values of R_emaildomain colum in test data\n    print('Unique values of R_emaildomain column in test data:', end=' ')\n    print(test_transaction['R_emaildomain'].nunique())\n\n    # Different values in P_emaildomain column of train and test data\n    print('Values appeared in P_emaildomain column of test data but not in train data:', end=' ')\n    print(set(test_transaction['P_emaildomain'].unique()) -\n          set(train_transaction['P_emaildomain'].unique()))\n\n    # Different values in R_emaildomain column of train and test data\n    print('Values appeared in R_emaildomain column of test data but not in train data:', end=' ')\n    print(set(test_transaction['R_emaildomain'].unique()) -\n          set(train_transaction['R_emaildomain'].unique()))\n\n\ncompare_email(train_transaction, test_transaction)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T05:14:48.897897Z","iopub.execute_input":"2022-01-08T05:14:48.898238Z","iopub.status.idle":"2022-01-08T05:14:49.37315Z","shell.execute_reply.started":"2022-01-08T05:14:48.898193Z","shell.execute_reply":"2022-01-08T05:14:49.372128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is only 1 P_emaildomain appeared in test data but not in train data which is 'scranton.edu'.\nLets see how many rows are there where P_emaildomain is 'scranton.edu' in test data.","metadata":{}},{"cell_type":"code","source":"# Print rows where P_emaildomain is scranton.edu in test data\nprint('\\nRows where P_emaildomain is scranton.edu in test data:')\nprint(len(test_transaction[test_transaction['P_emaildomain'] == 'scranton.edu']))","metadata":{"execution":{"iopub.status.busy":"2022-01-08T05:14:49.37458Z","iopub.execute_input":"2022-01-08T05:14:49.37482Z","iopub.status.idle":"2022-01-08T05:14:49.448749Z","shell.execute_reply.started":"2022-01-08T05:14:49.374791Z","shell.execute_reply":"2022-01-08T05:14:49.447464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So there are only 2 rows in test data where P_emaildomain is 'scranton.edu'. Hence, we don't have to do anything special about this email domain, we just need to add 'scranton.edu' into the one-hot encoder later.\n\nThe next columns to be analyzed are M1-M9 columns.","metadata":{}},{"cell_type":"code","source":"def inspect_M_columns(df):\n    # Value count of M1-M9 columns in data\n    print('\\nValue count of M1-M9 columns in data:')\n    for i in range(1, 10):\n        print('nan:', end=' ')\n        print(df['M' + str(i)].isnull().sum())\n        print(df['M' + str(i)].value_counts())","metadata":{"execution":{"iopub.status.busy":"2022-01-08T05:14:49.450181Z","iopub.execute_input":"2022-01-08T05:14:49.450435Z","iopub.status.idle":"2022-01-08T05:14:49.460725Z","shell.execute_reply.started":"2022-01-08T05:14:49.450407Z","shell.execute_reply":"2022-01-08T05:14:49.45998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inspect_M_columns(train_transaction)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T05:14:49.461751Z","iopub.execute_input":"2022-01-08T05:14:49.462465Z","iopub.status.idle":"2022-01-08T05:14:50.359442Z","shell.execute_reply.started":"2022-01-08T05:14:49.462421Z","shell.execute_reply":"2022-01-08T05:14:50.358317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inspect_M_columns(test_transaction)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T05:14:50.360959Z","iopub.execute_input":"2022-01-08T05:14:50.361297Z","iopub.status.idle":"2022-01-08T05:14:51.170255Z","shell.execute_reply.started":"2022-01-08T05:14:50.361254Z","shell.execute_reply":"2022-01-08T05:14:51.169534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that M1, M2, M3 nan value count are exactly identical in both train and test data, which means they are correlated somehow. After seeing this, maybe we can group columns in the data by their nan value count.","metadata":{}},{"cell_type":"code","source":"def group_column_by_nan(df, log=False):\n    group = {}\n    # Count nan values in each column\n    for col in df.columns:\n        s = df[col].isna().sum()\n        if s not in group:\n            group[s] = [col]\n        else:\n            group[s].append(col)\n\n    if log:\n        # Print number of groups\n        print('Number of groups:', len(group))\n\n        # Print groups\n        for k, v in group.items():\n            print(k, v)\n\n    return group\n\n\ngroup_column_by_nan(train_transaction, True)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-08T05:14:51.171277Z","iopub.execute_input":"2022-01-08T05:14:51.171514Z","iopub.status.idle":"2022-01-08T05:14:52.496748Z","shell.execute_reply.started":"2022-01-08T05:14:51.171484Z","shell.execute_reply":"2022-01-08T05:14:52.495762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, lets check out the correlations between columns in each group. We will only inspect group with V and D columns since there will be many redundant columns in the data.","metadata":{}},{"cell_type":"code","source":"def check_group(group):\n    for col in group:\n        if 'addr' in col or 'card' in col or 'M' in col:\n            return True\n    return False\n\ndef correlation_V_D_col(df, plot=False):\n    group = group_column_by_nan(df)\n    \n    # Remove group with specified columns\n    group_pop = []\n    for k in group.keys():\n        if check_group(group[k]):\n            group_pop.append(k)\n    \n    for k in group_pop:\n        group.pop(k)\n    \n    # Plot correlation between columns in a group\n    count = 0\n    correlation_matrices = []\n    for k, v in group.items():\n        if len(v) > 1 and k > 0:\n            count += 1\n            corr = df[v].corr()\n            correlation_matrices.append(corr)\n            if plot:\n                print(v)\n                plt.figure(figsize=(15, 10))\n                sns.heatmap(corr, annot=True)\n                plt.show()\n\n    return correlation_matrices\n\nprint(len(correlation_V_D_col(train_transaction, True)))","metadata":{"execution":{"iopub.status.busy":"2022-01-08T05:14:52.498538Z","iopub.execute_input":"2022-01-08T05:14:52.498851Z","iopub.status.idle":"2022-01-08T05:15:52.899574Z","shell.execute_reply.started":"2022-01-08T05:14:52.498807Z","shell.execute_reply":"2022-01-08T05:15:52.898671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_correlated_group(matrix):\n    # Get columns with correlation higher than 0.7\n    groups = []\n    matrix = matrix.abs()\n    matrix = matrix.unstack()\n    matrix = matrix.sort_values(kind=\"quicksort\", ascending=False)\n    for i in range(len(matrix)):\n        if matrix.iloc[i] > 0.7:\n            groups.append(set(matrix.index[i]))\n\n    # Join correlated columns(A little inefficient)\n    check = True\n    while check is True:\n        check = False\n        for i in range(len(groups)):\n            for j in range(i+1, len(groups)):\n                if groups[i].intersection(groups[j]) != set():\n                    groups[i] = groups[i].union(groups[j])\n                    groups.pop(j)\n                    check = True\n                    break\n            if check is True:\n                break\n\n    # Remove single element groups\n    group_pop = []\n    for i in range(len(groups)):\n        if len(groups[i]) < 2:\n            group_pop.append(i)\n\n    # Sort group pop in descending order then pop\n    group_pop.sort(reverse=True)\n    for i in group_pop:\n        groups.pop(i)\n\n    return groups\n","metadata":{"execution":{"iopub.status.busy":"2022-01-08T05:15:52.900808Z","iopub.execute_input":"2022-01-08T05:15:52.901044Z","iopub.status.idle":"2022-01-08T05:15:52.912301Z","shell.execute_reply.started":"2022-01-08T05:15:52.901015Z","shell.execute_reply":"2022-01-08T05:15:52.911329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_highly_correlated_columns(df):\n    # Get correlation matrix\n    matrices = correlation_V_D_col(df)\n\n    # Get correlated groups\n    groups = []\n    for matrix in matrices:\n        groups.extend(get_correlated_group(matrix))\n\n    # Remove correlated columns\n    columns_to_remove = []\n    for group in groups:\n        columns_to_remove.extend(list(group)[1:])\n\n    # Remove columns\n    columns = list(df.columns)\n    for col in columns_to_remove:\n        columns.remove(col)\n\n    return columns\n\n\nlen(remove_highly_correlated_columns(train_transaction))\n","metadata":{"execution":{"iopub.status.busy":"2022-01-08T05:15:52.915275Z","iopub.execute_input":"2022-01-08T05:15:52.915857Z","iopub.status.idle":"2022-01-08T05:16:10.202998Z","shell.execute_reply.started":"2022-01-08T05:15:52.915813Z","shell.execute_reply":"2022-01-08T05:16:10.202002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next we need to remove columns where there are too many nan values. We will first visualize the data to choose the threshold and then remove later.","metadata":{}},{"cell_type":"code","source":"def visualize_nan_values(df):\n    # Calculate nan values percentage in each column\n    percentages = []\n    for col in df.columns:\n        percentage = df[col].isna().sum() / len(df)\n        percentages.append(percentage)\n    \n    # Sort nan values percentage in ascending order\n    percentages.sort()\n\n    # Plot percentages\n    plt.figure(figsize=(15, 10))\n    plt.plot(percentages)\n    plt.show()\n\nvisualize_nan_values(train_transaction)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T05:16:10.204544Z","iopub.execute_input":"2022-01-08T05:16:10.204793Z","iopub.status.idle":"2022-01-08T05:16:11.707324Z","shell.execute_reply.started":"2022-01-08T05:16:10.204764Z","shell.execute_reply":"2022-01-08T05:16:11.706455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_columns_with_many_nan(df):\n    # Calculate nan values percentage in each column, if it's more than 0.75 then remove the column\n    columns_to_remove = []\n    for col in df.columns:\n        percentage = df[col].isna().sum() / len(df)\n        if percentage > 0.75:\n            columns_to_remove.append(col)\n\n    # Remove columns\n    columns = list(df.columns)\n    for col in columns_to_remove:\n        columns.remove(col)\n\n    return columns\n\n\nlen(remove_columns_with_many_nan(train_transaction))\n","metadata":{"execution":{"iopub.status.busy":"2022-01-08T05:16:11.708553Z","iopub.execute_input":"2022-01-08T05:16:11.70882Z","iopub.status.idle":"2022-01-08T05:16:12.962205Z","shell.execute_reply.started":"2022-01-08T05:16:11.708788Z","shell.execute_reply":"2022-01-08T05:16:12.961231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def filter_columns(df):\n    # Filter columns in data using above functions\n    columns = set(remove_highly_correlated_columns(df))\n    columns = columns.intersection(set(remove_columns_with_many_nan(df)))\n    return columns\n\nlen(filter_columns(train_transaction))","metadata":{"execution":{"iopub.status.busy":"2022-01-08T05:16:12.96387Z","iopub.execute_input":"2022-01-08T05:16:12.964365Z","iopub.status.idle":"2022-01-08T05:16:31.516959Z","shell.execute_reply.started":"2022-01-08T05:16:12.964326Z","shell.execute_reply":"2022-01-08T05:16:31.516086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets quickly summarize what we have done to the data so far:\n- Correct card column values.\n- Discover a P_emaildomain value appeared in test data but not in train data: \"*scranton.edu*\".\n- Visualize the data.\n- Replace highly correlated columns with a single column.\n- Remove columns with many nan values.\n\nFinally, we have a list of columns that we will use later on in our model.\n\nHowever, we still need to do some more data preprocessing before we can use them.","metadata":{}},{"cell_type":"markdown","source":"# Data processing\n\nIn this section, we will replace some values and one-hot encoding non-numerical columns.","metadata":{}},{"cell_type":"markdown","source":"Lets start by finding out which columns don't contain numerical values.","metadata":{}},{"cell_type":"code","source":"def get_non_numeric_columns(df):\n    # Get non-numeric columns\n    columns = df.select_dtypes(include=np.number).columns\n    columns = list(set(df.columns) - set(columns))\n    return columns\n\nget_non_numeric_columns(train_transaction[filter_columns(train_transaction)])","metadata":{"execution":{"iopub.status.busy":"2022-01-08T05:16:31.518348Z","iopub.execute_input":"2022-01-08T05:16:31.519132Z","iopub.status.idle":"2022-01-08T05:16:50.248436Z","shell.execute_reply.started":"2022-01-08T05:16:31.519087Z","shell.execute_reply":"2022-01-08T05:16:50.247583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that all M columns are not numerical, they only contains \"T\" and \"F\" values, except for M4 which contains \"M0\", \"M1\" or \"M2\". Lets replace \"T\" and \"F\" values with 1 and 9 respectively.","metadata":{}},{"cell_type":"code","source":"def replace_boolean_M_col(df):\n    result_df = df.copy()\n    for i in range(9):\n        result_df[f\"M{i+1}\"] = result_df[f\"M{i+1}\"].replace(\"T\", 1)\n        result_df[f\"M{i+1}\"] = result_df[f\"M{i+1}\"].replace(\"F\", 0)\n    return result_df\n\n\nfixed_train_transaction = replace_boolean_M_col(train_transaction[filter_columns(train_transaction)])\nprint(fixed_train_transaction.info())\n","metadata":{"execution":{"iopub.status.busy":"2022-01-08T05:16:50.249986Z","iopub.execute_input":"2022-01-08T05:16:50.250407Z","iopub.status.idle":"2022-01-08T05:17:11.370593Z","shell.execute_reply.started":"2022-01-08T05:16:50.250362Z","shell.execute_reply":"2022-01-08T05:17:11.369637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After changing M columns values, lets run the filter again to see if they are still non numerical.","metadata":{}},{"cell_type":"code","source":"get_non_numeric_columns(fixed_train_transaction)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T05:17:11.372012Z","iopub.execute_input":"2022-01-08T05:17:11.372787Z","iopub.status.idle":"2022-01-08T05:17:11.915693Z","shell.execute_reply.started":"2022-01-08T05:17:11.372736Z","shell.execute_reply":"2022-01-08T05:17:11.914784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The next step is to one hot encode the non-numerical columns.","metadata":{}},{"cell_type":"code","source":"def one_hot_encode_train(df, columns):\n    # One hot encode columns\n    result_df = df.copy()\n    # Save uniques values to encode test data later\n    uniques = {}\n    for column in columns:\n        # Get unique values in sorted order\n        unique = list(result_df[column].value_counts().index)\n        uniques[column] = unique\n        # Replace them\n        for i in range(len(unique)):\n            result_df[column] = result_df[column].replace(unique[i], i)\n    return result_df, uniques\n    \nfinal_train_transaction, uniques_val = one_hot_encode_train(fixed_train_transaction, get_non_numeric_columns(fixed_train_transaction))\nprint(final_train_transaction.info())\n# Note that the index is off since I deleted some rows","metadata":{"execution":{"iopub.status.busy":"2022-01-08T05:17:11.91701Z","iopub.execute_input":"2022-01-08T05:17:11.917252Z","iopub.status.idle":"2022-01-08T05:17:16.669696Z","shell.execute_reply.started":"2022-01-08T05:17:11.917222Z","shell.execute_reply":"2022-01-08T05:17:16.668656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def one_hot_encode_test(df, uniques, columns, label):\n    # Remove label column\n    columns.remove(label)\n\n    # One hot encode columns\n    result_df = df.copy()\n    result_df = result_df[columns]\n\n    for column in uniques.keys():\n        unique = uniques[column]\n        # Remember the scranton.edu email domain\n        if column == \"P_emaildomain\":\n            unique.append(\"scranton.edu\")\n        # Replace them\n        for i in range(len(unique)):\n            result_df[column] = result_df[column].replace(unique[i], i)\n\n    return result_df\n\n\nfinal_test_transaction = one_hot_encode_test(\n    test_transaction, uniques_val, list(final_train_transaction.columns), \"isFraud\")\nfinal_test_transaction = replace_boolean_M_col(final_test_transaction)\nprint(final_test_transaction.info())\n","metadata":{"execution":{"iopub.status.busy":"2022-01-08T05:17:16.671281Z","iopub.execute_input":"2022-01-08T05:17:16.671867Z","iopub.status.idle":"2022-01-08T05:17:23.767327Z","shell.execute_reply.started":"2022-01-08T05:17:16.671818Z","shell.execute_reply":"2022-01-08T05:17:23.766443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That's all for data preprocessing, now lets export processed data to csv file. Note that I don't use identity data here since it has much lower records than transaction data so I think merging them is going to hurt our model performance.","metadata":{}},{"cell_type":"code","source":"# Check if there are any non-numeric columns left\nprint(get_non_numeric_columns(final_train_transaction))\nprint(get_non_numeric_columns(final_test_transaction))","metadata":{"execution":{"iopub.status.busy":"2022-01-08T05:17:23.768524Z","iopub.execute_input":"2022-01-08T05:17:23.768861Z","iopub.status.idle":"2022-01-08T05:17:24.801309Z","shell.execute_reply.started":"2022-01-08T05:17:23.768816Z","shell.execute_reply":"2022-01-08T05:17:24.800343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Export final_train_transaction to csv\nfinal_train_transaction.to_csv(f\"{OUTPUT_ROOT}/final_train_transaction.csv\", index=False)\nfinal_test_transaction.to_csv(f\"{OUTPUT_ROOT}/final_test_transaction.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T05:17:24.802499Z","iopub.execute_input":"2022-01-08T05:17:24.802716Z","iopub.status.idle":"2022-01-08T05:19:02.934371Z","shell.execute_reply.started":"2022-01-08T05:17:24.802689Z","shell.execute_reply":"2022-01-08T05:19:02.933349Z"},"trusted":true},"execution_count":null,"outputs":[]}]}