{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Minimalist LightGBM\n\nThis kernal was created as a baseline for gradual improvements in data transformation, feature engineering, model creation, and ensembling. Hence, with a stick in the ground, I can develop the kernal incrementally.\n\nI noticed that LightGBM is a model used in many of the kernals, so I studied up on it and found that one of the main features of the model is how it handles categorical variables \"in column\" rather than relying upon one-hot coding, which also seems to be difficult for decision-tree-based models anyway. So my process to some degree is developed around LightGBM and will hopefully serve as a foundation for comparing across other gradient boosting models.\n\nTo learn more about LightGBM, you can go [here](https://lightgbm.readthedocs.io/en/latest/Features.html)"},{"metadata":{},"cell_type":"markdown","source":"# Notes up front\n\n## Comments from Vesta\nThere are a few data points worth keeping here for clarifications:\n\n- addr1 as billing zipcode\n- addr2 as billing country\n- dist = \"distances between (not limited) billing address, mailing address, zip code, IP address, phone area, etc.\"\n\n#### Transaction Table\n- TransactionDT: timedelta from a given reference datetime (not an actual timestamp)\n- TransactionAMT: transaction payment amount in USD\n- ProductCD: product code, the product for each transaction\n- card1 - card6: payment card information, such as card type, card category, issue bank, country, etc.\n- addr: address\n- dist: distance\n- P_ and (R__) emaildomain: purchaser and recipient email domain\n- C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.\n- D1-D15: timedelta, such as days between previous transaction, etc.\n- M1-M9: match, such as names on card and address, etc.\n- Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.\n\nCategorical Features:\n- ProductCD\n- card1 - card6\n- addr1, addr2\n- Pemaildomain Remaildomain\n- M1 - M9\n\n#### Identity Table\n\nVariables in this table are identity information – network connection information (IP, ISP, Proxy, etc) and digital signature (UA/browser/os/version, etc) associated with transactions. \nThey're collected by Vesta’s fraud protection system and digital security partners.\n(The field names are masked and pairwise dictionary will not be provided for privacy protection and contract agreement)\n\nCategorical Features:\n- DeviceType\n- DeviceInfo\n- id12 - id38"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport gc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_identity = pd.read_csv('../input/train_identity.csv')\ntrain_transaction = pd.read_csv('../input/train_transaction.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction['isFraud'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train_transaction.merge(train_identity, on='TransactionID', how='left')\ndel train_identity, train_transaction\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_identity = pd.read_csv('../input/test_identity.csv')\ntest_transaction = pd.read_csv('../input/test_transaction.csv')\ntest = test_transaction.merge(test_identity, on='TransactionID', how='left')\ndel test_identity, test_transaction\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Transformations\n\nAgain, at this point I will not do much feature engineering, and especially will not remove any features, as I want to first establish a baseline.\n\nThat said, new data types include:\n- Rounded-off transaction amount - fraudulent transactions may have a certain pattern\n- Timedelta to relative day of week and time of day - fraudulent transactions may occur at atypical times in the week/day\n\nAdjustments:\n- Object types to category types\n\nAt this point I don't want to mess with adding missing values, since in my mind, a missing value may itself be useful to detecting a fraudulent transaction. I will experiment with this in a subsequent kernal."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a new value for the transaction decimal\n\ntrain['TransactionDecimal'] = ((train['TransactionAmt'] - train['TransactionAmt'].astype(int)) * 1000).astype(int)\ntest['TransactionDecimal'] = ((test['TransactionAmt'] - test['TransactionAmt'].astype(int)) * 1000).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a new values from continuous variable 'id_02'\n\ntrain['id_02_bins'] = train['id_02']\ntrain['id_02_bins'] = pd.cut(train['id_02_bins'], 10, labels=False)\ntest['id_02_bins'] = test['id_02']\ntest['id_02_bins'] = pd.cut(test['id_02_bins'], 10, labels=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a new values from continuous variable 'id_11'\n\ntrain['id_11_flag'] = np.where(train['id_11'].isnull(),'F','T')\ntest['id_11_flag'] = np.where(test['id_11'].isnull(),'F','T')\ntrain['id_11_flag'] = train['id_11_flag'].astype('category')\ntest['id_11_flag'] = test['id_11_flag'].astype('category')\n\ntrain['id_11_residual'] = train['id_11']\ntrain['id_11_residual'] = np.where(train['id_11'] == 100.0,'100',np.where(train['id_11_residual'].isnull(),'None','Residual'))\ntrain['id_11_residual'] = train['id_11_residual'].astype('category')\ntest['id_11_residual'] = test['id_11']\ntest['id_11_residual'] = np.where(test['id_11'] == 100.0,'100',np.where(test['id_11_residual'].isnull(),'None','Residual'))\ntest['id_11_residual'] = test['id_11_residual'].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a new value for the Datetime\n# https://www.kaggle.com/fchmiel/day-and-time-powerful-predictive-feature\ntrain['Transaction_DOW'] = np.floor((train['TransactionDT'] / (3600 * 24) - 1) % 7)\ntest['Transaction_DOW'] = np.floor((test['TransactionDT'] / (3600 * 24) - 1) % 7)\ntrain['Transaction_H'] = np.floor(train['TransactionDT'] / 3600) % 24\ntest['Transaction_H'] = np.floor(test['TransactionDT'] / 3600) % 24","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building the LightGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom bayes_opt import BayesianOptimization\nfrom sklearn.metrics import roc_auc_score, roc_curve, auc\nimport lightgbm as lgb\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert categorical data types to object for Bayesian Optimization\n\ncat_types = ['ProductCD','card1','card2','card3','card4','card5','card6','addr1','addr2','P_emaildomain','R_emaildomain',\n             'M1','M2','M3','M4','M5','M6','M7','M8','M9','DeviceType','DeviceInfo','id_12','id_13','id_14','id_15','id_16',\n             'id_17','id_18','id_19','id_20','id_21','id_22','id_23','id_24','id_25','id_26','id_27','id_28','id_29','id_30',\n             'id_31','id_32','id_33','id_34','id_35','id_36','id_37','id_38','id_02_bins','id_11_flag','id_11_residual']\n\ntrain[cat_types] = train[cat_types].astype('object')\ntest[cat_types] = test[cat_types].astype('object')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.fillna(-999)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nfor f in train.columns:\n    if  train[f].dtype=='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(train[f].values) + list(test[f].values))\n        train[f] = lbl.transform(list(train[f].values))\n        test[f] = lbl.transform(list(test[f].values))  \ntrain = train.reset_index()\ntest = test.reset_index()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = list(train)\nfeatures.remove('isFraud')\ntarget = 'isFraud'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/vincentlugat/ieee-lgb-bayesian-opt\n\nbayesian_tr_idx, bayesian_val_idx = train_test_split(train, test_size = 0.3, random_state = 42, stratify = train[target])\nbayesian_tr_idx = bayesian_tr_idx.index\nbayesian_val_idx = bayesian_val_idx.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def LGB_bayesian(\n    #learning_rate,\n    num_leaves, \n    bagging_fraction,\n    feature_fraction,\n    min_child_weight, \n    min_data_in_leaf,\n    max_depth,\n    reg_alpha,\n    reg_lambda\n     ):\n    \n    # LightGBM expects next three parameters need to be integer. \n    num_leaves = int(num_leaves)\n    min_data_in_leaf = int(min_data_in_leaf)\n    max_depth = int(max_depth)\n\n    assert type(num_leaves) == int\n    assert type(min_data_in_leaf) == int\n    assert type(max_depth) == int\n    \n\n    param = {\n              'num_leaves': num_leaves, \n              'min_data_in_leaf': min_data_in_leaf,\n              'min_child_weight': min_child_weight,\n              'bagging_fraction' : bagging_fraction,\n              'feature_fraction' : feature_fraction,\n              #'learning_rate' : learning_rate,\n              'max_depth': max_depth,\n              'reg_alpha': reg_alpha,\n              'reg_lambda': reg_lambda,\n              'objective': 'binary',\n              'save_binary': True,\n              'seed': 11,\n              'feature_fraction_seed': 11,\n              'bagging_seed': 11,\n              'drop_seed': 11,\n              'data_random_seed': 11,\n              'boosting_type': 'gbdt',\n              'verbose': 1,\n              'is_unbalance': False,\n              'boost_from_average': True,\n              'metric':'auc'}    \n    \n    oof = np.zeros(len(train))\n    trn_data= lgb.Dataset(train.iloc[bayesian_tr_idx][features].values, label=train.iloc[bayesian_tr_idx][target].values)\n    val_data= lgb.Dataset(train.iloc[bayesian_val_idx][features].values, label=train.iloc[bayesian_val_idx][target].values)\n\n    clf = lgb.train(param, trn_data,  num_boost_round=50, valid_sets = [trn_data, val_data], verbose_eval=0, early_stopping_rounds = 50)\n    \n    oof[bayesian_val_idx]  = clf.predict(train.iloc[bayesian_val_idx][features].values, num_iteration=clf.best_iteration)  \n    \n    score = roc_auc_score(train.iloc[bayesian_val_idx][target].values, oof[bayesian_val_idx])\n\n    return score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using Bayesian Optimization\n\n# https://www.kaggle.com/vincentlugat/ieee-lgb-bayesian-opt\n\nbounds_LGB = {\n    'num_leaves': (31, 500), \n    'min_data_in_leaf': (20, 200),\n    'bagging_fraction' : (0.1, 0.9),\n    'feature_fraction' : (0.1, 0.9),\n    #'learning_rate': (0.01, 0.3),\n    'min_child_weight': (0.00001, 0.01),   \n    'reg_alpha': (1, 2), \n    'reg_lambda': (1, 2),\n    'max_depth':(-1,50),\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/vincentlugat/ieee-lgb-bayesian-opt\n\nLGB_BO = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/vincentlugat/ieee-lgb-bayesian-opt\n\ninit_points = 10\nn_iter = 15","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/vincentlugat/ieee-lgb-bayesian-opt\n\nwith warnings.catch_warnings():\n    warnings.filterwarnings('ignore')\n    LGB_BO.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LGB_BO.max['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LGB_BO.max['params']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rebuild dataset\n\ntrain_identity = pd.read_csv('../input/train_identity.csv')\ntrain_transaction = pd.read_csv('../input/train_transaction.csv')\ntrain = train_transaction.merge(train_identity, on='TransactionID', how='left')\ndel train_identity, train_transaction\ngc.collect()\n\ntest_identity = pd.read_csv('../input/test_identity.csv')\ntest_transaction = pd.read_csv('../input/test_transaction.csv')\ntest = test_transaction.merge(test_identity, on='TransactionID', how='left')\ndel test_identity, test_transaction\ngc.collect()\n\n# Create a new value for the transaction decimal\n\ntrain['TransactionDecimal'] = ((train['TransactionAmt'] - train['TransactionAmt'].astype(int)) * 1000).astype(int)\ntest['TransactionDecimal'] = ((test['TransactionAmt'] - test['TransactionAmt'].astype(int)) * 1000).astype(int)\n\n# Create a new values from continuous variable 'id_02'\n\ntrain['id_02_bins'] = train['id_02']\ntrain['id_02_bins'] = pd.cut(train['id_02_bins'], 10, labels=False)\ntest['id_02_bins'] = test['id_02']\ntest['id_02_bins'] = pd.cut(test['id_02_bins'], 10, labels=False)\n\n# Create a new values from continuous variable 'id_11'\n\ntrain['id_11_flag'] = np.where(train['id_11'].isnull(),'F','T')\ntest['id_11_flag'] = np.where(test['id_11'].isnull(),'F','T')\ntrain['id_11_flag'] = train['id_11_flag'].astype('category')\ntest['id_11_flag'] = test['id_11_flag'].astype('category')\n\ntrain['id_11_residual'] = train['id_11']\ntrain['id_11_residual'] = np.where(train['id_11'] == 100.0,'100',np.where(train['id_11_residual'].isnull(),'None','Residual'))\ntrain['id_11_residual'] = train['id_11_residual'].astype('category')\ntest['id_11_residual'] = test['id_11']\ntest['id_11_residual'] = np.where(test['id_11'] == 100.0,'100',np.where(test['id_11_residual'].isnull(),'None','Residual'))\ntest['id_11_residual'] = test['id_11_residual'].astype('category')\n\n# Create a new value for the Datetime\n# https://www.kaggle.com/fchmiel/day-and-time-powerful-predictive-feature\ntrain['Transaction_DOW'] = np.floor((train['TransactionDT'] / (3600 * 24) - 1) % 7)\ntest['Transaction_DOW'] = np.floor((test['TransactionDT'] / (3600 * 24) - 1) % 7)\ntrain['Transaction_H'] = np.floor(train['TransactionDT'] / 3600) % 24\ntest['Transaction_H'] = np.floor(test['TransactionDT'] / 3600) % 24","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert categorical data types to category; from Vesta's comments\n\ncat_types = ['ProductCD','card1','card2','card3','card4','card5','card6','addr1','addr2','P_emaildomain','R_emaildomain',\n             'M1','M2','M3','M4','M5','M6','M7','M8','M9','DeviceType','DeviceInfo','id_12','id_13','id_14','id_15','id_16',\n             'id_17','id_18','id_19','id_20','id_21','id_22','id_23','id_24','id_25','id_26','id_27','id_28','id_29','id_30',\n             'id_31','id_32','id_33','id_34','id_35','id_36','id_37','id_38']\n\ntrain[cat_types] = train[cat_types].astype('category')\ntest[cat_types] = test[cat_types].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare for model building\ntrain_ID = train['TransactionID']\ntest_ID = test['TransactionID']\ny = train['isFraud']\nX = train.drop(['TransactionID','TransactionDT','isFraud'], axis=1)\nX_test = test.drop(['TransactionID','TransactionDT'], axis=1)\n\ndel train, test\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Quickly ensure comparable structures\nX.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\n\n# With some help from https://www.kaggle.com/davidcairuz/feature-engineering-lightgbm-w-gpu\n\nparams = {'num_leaves': int(LGB_BO.max['params']['num_leaves']),\n          'min_child_weight': LGB_BO.max['params']['min_child_weight'],\n          'feature_fraction': LGB_BO.max['params']['feature_fraction'],\n          'bagging_fraction': LGB_BO.max['params']['bagging_fraction'],\n          'min_data_in_leaf': int(LGB_BO.max['params']['min_data_in_leaf']),\n          'objective': 'binary',\n          'max_depth': int(LGB_BO.max['params']['max_depth']),\n          'learning_rate': 0.006883242363721497,\n          \"boosting_type\": \"gbdt\",\n          \"bagging_seed\": 11,\n          'seed': 11,\n          'drop_seed': 11,\n          'feature_fraction_seed': 11,\n          'data_random_seed': 11,\n          'metric': 'auc',\n          'verbosity': 1,\n          'reg_alpha': LGB_BO.max['params']['reg_alpha'],\n          'reg_lambda': LGB_BO.max['params']['reg_lambda'],\n          'random_state': 47\n         }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/davidcairuz/feature-engineering-lightgbm-w-gpu\n\nNFOLDS = 5\nfolds = KFold(n_splits=NFOLDS)\n\ncolumns = X.columns\nsplits = folds.split(X, y)\ny_preds = np.zeros(X_test.shape[0])\ny_oof = np.zeros(X.shape[0])\nscore = 0\n\n# feature_importances = pd.DataFrame()\n# feature_importances['feature'] = columns\n  \nfor fold_n, (train_index, valid_index) in enumerate(splits):\n    X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n    \n    dtrain = lgb.Dataset(X_train, label=y_train)\n    dvalid = lgb.Dataset(X_valid, label=y_valid)\n\n    clf = lgb.train(params, dtrain, 10000, valid_sets = [dtrain, dvalid], verbose_eval=200, early_stopping_rounds=500)\n    \n#     feature_importances[f'fold_{fold_n + 1}'] = clf.feature_importance()\n    \n    y_pred_valid = clf.predict(X_valid)\n    y_oof[valid_index] = y_pred_valid\n    print(f\"Fold {fold_n + 1} | AUC: {roc_auc_score(y_valid, y_pred_valid)}\")\n    \n    score += roc_auc_score(y_valid, y_pred_valid) / NFOLDS\n    y_preds += clf.predict(X_test) / NFOLDS\n    \n    del X_train, X_valid, y_train, y_valid\n    gc.collect()\n    \nprint(f\"\\nMean AUC = {score}\")\nprint(f\"Out of folds AUC = {roc_auc_score(y, y_oof)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission\n\nThat's it! Now just for the submission itself."},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('../input/sample_submission.csv')\nsub['isFraud'] = y_preds\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Please feel free to give any feedback!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}