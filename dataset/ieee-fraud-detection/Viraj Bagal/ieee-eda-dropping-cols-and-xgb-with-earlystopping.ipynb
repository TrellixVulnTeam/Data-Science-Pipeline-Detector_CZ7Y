{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1><center><font size=\"6\">IEEE FRAUD DETECTION</font></center></h1>\n\n\n\n\n<img src=\"https://gdprinformer.com/wp-content/uploads/2017/09/fraud-laptop.jpg\" width=\"800\"></img>\n\n\n\n<br>"},{"metadata":{},"cell_type":"markdown","source":"# <a id='0'>Content</a>\n\n- <a href='#1'>Importing Libraries and Modules</a>  \n- <a href='#2'>Distribution of features across Train and Test</a>\n - <a href='#21'>Card Features</a>   \n  - <a href='#22'>Addr,Dist and EmailDomain</a>  \n  - <a href='#23'>C Features</a> \n  - <a href='#24'>D Features</a> \n  - <a href='#25'>M features</a> \n  - <a href='#26'>V Features</a>   \n  - <a href='#27'>ID features</a> \n- <a href='#3'>Dropping Columns</a>   \n- <a href='#4'>Reducing Memory Size</a>    \n- <a href='#5'>Feature Engineering</a>     \n- <a href='#6'>Model Development</a>\n- <a href='#7'>Feature Importance</a>\n\n\n**Please upvote if you like the kernel . Happy Kaggling and all the best for the competition**"},{"metadata":{},"cell_type":"markdown","source":"# <a id='1'>Importing Libraries and Modules</a> "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import StratifiedKFold\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\nfrom hyperopt import tpe,hp,Trials\nfrom hyperopt.fmin import fmin\nimport gc\nimport os\nprint(os.listdir(\"../input\"))\nseed=5\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_identity=pd.read_csv('../input/train_identity.csv')\ntest_identity=pd.read_csv('../input/test_identity.csv')\ntrain_transaction=pd.read_csv('../input/train_transaction.csv')\ntest_transaction=pd.read_csv('../input/test_transaction.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.merge(train_transaction,train_identity,how='left',on='TransactionID')\ntest=pd.merge(test_transaction,test_identity,how='left',on='TransactionID')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_identity,test_identity,train_transaction,test_transaction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# most_null_values=[col for col in train.columns if (train[col].isna().sum()/train.shape[0])>0.9]\n# len(most_null_values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dominant_unique_values=[col for col in train.columns if (train[col].value_counts().values[0]/train.shape[0])>0.9]\n# len(dominant_unique_values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dominant_unique_values.remove('isFraud')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cols_to_drop=list(set(most_null_values+dominant_unique_values+['TransactionID','TransactionDT']))\n# train=train.drop(cols_to_drop,axis=1)\n# test=test.drop(cols_to_drop,axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='2'>Distribution of features across Train and Test</a> "},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns[:50]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train['isFraud'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax=plt.subplots(1,2,figsize=(15,5))\nsns.distplot(train[train['isFraud']==0]['TransactionAmt'],ax=ax[0],hist=False,label='NonFraud')\nsns.distplot(train[train['isFraud']==1]['TransactionAmt'],ax=ax[0],hist=False,label='Fraud')\nax[0].set_title('Fraud and NonFraud TransactionAmt Distribution')\n\nsns.distplot(np.log(test['TransactionAmt']),ax=ax[1],hist=False,label='Test')\nsns.distplot(np.log(train['TransactionAmt']),ax=ax[1],hist=False,label='Train')\nax[1].set_title('Test and Train TransationAmt Distribution')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remember to take log of Transaction Amount. "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax=plt.subplots(1,2,figsize=(10,5))\nsns.distplot(test['TransactionAmt'],hist=False,label='Test',ax=ax[0],color='orange')\nax[0].set_title('Distribution of TransactionAmt in Test')\nsns.distplot(train['TransactionAmt'],hist=False,label='Train',ax=ax[1])\nax[1].set_title('Distribution of TransactionAmt in Train')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remove the Transaction amout outliers or take log."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax=plt.subplots(1,2,figsize=(10,5))\nsns.countplot(train['ProductCD'],ax=ax[0])\nax[0].set_title('Train ProductCD Distribution')\n\nsns.countplot(test['ProductCD'],ax=ax[1],label='Test')\nax[1].set_title('Test ProductCD Distribution')\n\nplt.tight_layout()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='21'>Distribution of  Card features across Train and Test</a> "},{"metadata":{"trusted":true},"cell_type":"code","source":"card_cols=['card1','card2','card3','card4','card5','card6']\nfor c in card_cols:\n    print(f'Number of unique variables in {c}: ',train[c].nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax=plt.subplots(1,2,figsize=(15,5))\nsns.distplot(train[train['isFraud']==0]['card1'],ax=ax[0],hist=False,label='NonFraud')\nsns.distplot(train[train['isFraud']==1]['card1'],ax=ax[0],hist=False,label='Fraud')\nax[0].set_title('Fraud and NonFraud Card1 Distribution')\n\nsns.distplot(test['card1'],ax=ax[1],hist=False,label='Test')\nsns.distplot(train['card1'],ax=ax[1],hist=False,label='Train')\nax[1].set_title('Test and Train Card1 Distribution')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax=plt.subplots(1,2,figsize=(15,5))\nsns.distplot(train[train['isFraud']==0]['card2'],ax=ax[0],hist=False,label='NonFraud')\nsns.distplot(train[train['isFraud']==1]['card2'],ax=ax[0],hist=False,label='Fraud')\nax[0].set_title('Fraud and NonFraud Card1 Distribution')\n\nsns.distplot(test['card2'],ax=ax[1],hist=False,label='Test')\nsns.distplot(train['card2'],ax=ax[1],hist=False,label='Train')\nax[1].set_title('Test and Train Card1 Distribution')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax=plt.subplots(2,2,figsize=(15,8))\nsns.distplot(train[train['isFraud']==0]['card3'],ax=ax[0,0],hist=False,label='NonFraud')\nsns.distplot(train[train['isFraud']==1]['card3'],ax=ax[0,0],hist=False,label='Fraud')\nax[0,0].set_title('Fraud and NonFraud Card3 Distribution')\n\nsns.distplot(test['card3'],ax=ax[0,1],hist=False,label='Test')\nsns.distplot(train['card3'],ax=ax[0,1],hist=False,label='Train')\nax[0,1].set_title('Test and Train Card3 Distribution')\n\nsns.distplot(train[train['isFraud']==0]['card5'],ax=ax[1,0],hist=False,label='NonFraud')\nsns.distplot(train[train['isFraud']==1]['card5'],ax=ax[1,0],hist=False,label='Fraud')\nax[1,0].set_title('Fraud and NonFraud Card5 Distribution')\n\nsns.distplot(test['card5'],ax=ax[1,1],hist=False,label='Test')\nsns.distplot(train['card5'],ax=ax[1,1],hist=False,label='Train')\nax[1,1].set_title('Test and Train Card5 Distribution')\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax=plt.subplots(2,2,figsize=(10,8))\nsns.countplot(train['card4'],ax=ax[0,0])\nax[0,0].set_title('Train Card4 Distribution')\n\nsns.countplot(test['card4'],ax=ax[0,1])\nax[0,1].set_title('Test Card4 Distribution')\n\nsns.countplot(train['card6'],ax=ax[1,0])\nax[1,0].set_title('Train Card6 Distribution')\n\nsns.countplot(test['card6'],ax=ax[1,1])\nax[1,1].set_title('Test Card6 Distribution')\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='22'>Distribution of Addr,Dist and Emaildomain across Train and Test</a> "},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns[:50]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"addr=['addr1','addr2','dist1','dist2','P_emaildomain','R_emaildomain']\nfor c in addr:\n    print(f'Number of unique variables in {c}: ',train[c].nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax=plt.subplots(2,2,figsize=(15,8))\nsns.distplot(train[train['isFraud']==0]['addr1'],ax=ax[0,0],hist=False,label='NonFraud')\nsns.distplot(train[train['isFraud']==1]['addr1'],ax=ax[0,0],hist=False,label='Fraud')\nax[0,0].set_title('Fraud and NonFraud addr1 Distribution')\n\nsns.distplot(test['addr1'],ax=ax[0,1],hist=False,label='Test')\nsns.distplot(train['addr1'],ax=ax[0,1],hist=False,label='Train')\nax[0,1].set_title('Test and Train addr1 Distribution')\n\nsns.distplot(train[train['isFraud']==0]['addr2'],ax=ax[1,0],hist=False,label='NonFraud')\nsns.distplot(train[train['isFraud']==1]['addr2'],ax=ax[1,0],hist=False,label='Fraud')\nax[1,0].set_title('Fraud and NonFraud addr2 Distribution')\n\nsns.distplot(test['addr2'],ax=ax[1,1],hist=False,label='Test')\nsns.distplot(train['addr2'],ax=ax[1,1],hist=False,label='Train')\nax[1,1].set_title('Test and Train addr2 Distribution')\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"addr2 of test perfectly overlaps the addr2 of train and so it is not seen."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax=plt.subplots(2,2,figsize=(15,8))\nsns.distplot(train[train['isFraud']==0]['dist1'],ax=ax[0,0],hist=False,label='NonFraud')\nsns.distplot(train[train['isFraud']==1]['dist1'],ax=ax[0,0],hist=False,label='Fraud')\nax[0,0].set_title('Fraud and NonFraud dist1 Distribution')\n\nsns.distplot(test['dist1'],ax=ax[0,1],hist=False,label='Test')\nsns.distplot(train['dist1'],ax=ax[0,1],hist=False,label='Train')\nax[0,1].set_title('Test and Train dist1 Distribution')\n\nsns.distplot(train[train['isFraud']==0]['dist2'],ax=ax[1,0],hist=False,label='NonFraud')\nsns.distplot(train[train['isFraud']==1]['dist2'],ax=ax[1,0],hist=False,label='Fraud')\nax[1,0].set_title('Fraud and NonFraud dist2 Distribution')\n\nsns.distplot(test['dist2'],ax=ax[1,1],hist=False,label='Test')\nsns.distplot(train['dist2'],ax=ax[1,1],hist=False,label='Train')\nax[1,1].set_title('Test and Train dist2 Distribution')\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax=plt.subplots(1,2,figsize=(15,5))\nsns.distplot(test['dist2'],hist=False,label='Test',ax=ax[0],color='orange')\nax[0].set_title('Distribution of dist2 in Test')\nsns.distplot(train['dist2'],hist=False,label='Train',ax=ax[1])\nax[1].set_title('Distribution of dist2 in Train')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly the distribution is a lot different. I shall eliminate dist2."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax=plt.subplots(2,2,figsize=(15,8))\ntrain['P_emaildomain'].value_counts()[:10].plot.bar(ax=ax[0,0])\nax[0,0].set_title('Train P_emaildomain Distribution')\n\ntest['P_emaildomain'].value_counts()[:10].plot.bar(ax=ax[0,1])\nax[0,1].set_title('Test P_emaildomain Distribution')\n\ntrain['R_emaildomain'].value_counts()[:10].plot.bar(ax=ax[1,0])\nax[1,0].set_title('Train R_emaildomain Distribution')\n\ntest['R_emaildomain'].value_counts()[:10].plot.bar(ax=ax[1,1])\nax[1,1].set_title('Test R_emaildomain Distribution')\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='23'>Distribution of C features across Train and Test</a> "},{"metadata":{"trusted":true},"cell_type":"code","source":"C_columns=[col for col in train.columns if 'C'==col[0]]\nfor c in C_columns:\n    print(f'Number of unique entries in {c}:',train[c].nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_subplots(feature1,feature2):\n    fig,ax=plt.subplots(2,2,figsize=(15,8))\n    sns.distplot(train[train['isFraud']==0][feature1],ax=ax[0,0],hist=False,label='NonFraud')\n    sns.distplot(train[train['isFraud']==1][feature1],ax=ax[0,0],hist=False,label='Fraud')\n    ax[0,0].set_title(f'Fraud and NonFraud {feature1} Distribution')\n\n    sns.distplot(test[feature1],ax=ax[0,1],hist=False,label='Test')\n    sns.distplot(train[feature1],ax=ax[0,1],hist=False,label='Train')\n    ax[0,1].set_title(f'Test and Train {feature1} Distribution')\n\n    sns.distplot(train[train['isFraud']==0][feature2],ax=ax[1,0],hist=False,label='NonFraud')\n    sns.distplot(train[train['isFraud']==1][feature2],ax=ax[1,0],hist=False,label='Fraud')\n    ax[1,0].set_title(f'Fraud and NonFraud {feature2} Distribution')\n\n    sns.distplot(test[feature2],ax=ax[1,1],hist=False,label='Test')\n    sns.distplot(train[feature2],ax=ax[1,1],hist=False,label='Train')\n    ax[1,1].set_title(f'Test and Train {feature2} Distribution')\n\n    plt.tight_layout()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_subplots('C1','C2')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_subplots('C3','C4')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"C4 seems very much useful for nonfraud and fraud distinction because its distribution is very different for those two categories."},{"metadata":{"trusted":true},"cell_type":"code","source":"get_subplots('C5','C6')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_subplots('C7','C8')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"C8 and C7 have very different distribution in case of Fraud and NonFraud categories. So they might be useful. But again, C8 distribution in test and train is also very different. We will try dropping and not dropping it. "},{"metadata":{"trusted":true},"cell_type":"code","source":"get_subplots('C9','C10')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"C10 can be useful for distinction. But its distribution in test and train is very different as well. Same in case of C9. But I think it can be definitely dropped because its distribution in case of fraud and nonfraud is almost same."},{"metadata":{"trusted":true},"cell_type":"code","source":"get_subplots('C11','C12')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_subplots('C13','C14')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='24'>Distribution of D features across Train and Test</a> "},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns[:50]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"D_cols=[col for col in train.columns if col[0]=='D']\nfor d in D_cols:\n    print(f'Number of unique entries in {d}:',train[d].nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_subplots('D1','D2')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_subplots('D3','D4')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_subplots('D5','D6')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def detailed_subplot(feature):\n    fig,ax=plt.subplots(1,2,figsize=(15,5))\n    sns.distplot(test[feature],hist=False,label='Test',ax=ax[0],color='orange')\n    ax[0].set_title(f'Distribution of {feature} in Test')\n    sns.distplot(train[feature],hist=False,label='Train',ax=ax[1])\n    ax[1].set_title(f'Distribution of {feature} in Train')\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"detailed_subplot('D6')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I think we can safely drop this column"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_subplots('D7','D8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train['D9'].value_counts())\nprint('*'*100)\nprint(test['D9'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_subplots('D10','D11')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" Drop some columns : From https://www.kaggle.com/jazivxt/safe-box/notebook"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_subplots('D12','D13')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"detailed_subplot('D12')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can try dropping this column."},{"metadata":{"trusted":true},"cell_type":"code","source":"get_subplots('D14','D15')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"detailed_subplot('D14')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can try dropping this."},{"metadata":{},"cell_type":"markdown","source":"## <a id='25'>Distribution of M features across Train and Test</a> "},{"metadata":{"trusted":true},"cell_type":"code","source":"M_cols=[col for col in train.columns if col[0]=='M']\nfor m in M_cols:\n    print(f'Number of unique entries in {m}:',train[m].nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_bar_subplots(feature1,feature2,top=10,incre=0):\n    fig,ax=plt.subplots(2,2,figsize=(15,8))\n    train[feature1].value_counts()[:top].plot.bar(ax=ax[0,0])\n    ax[0,0].set_title(f'Train {feature1} Distribution')\n    \n    test[feature1].value_counts()[:top].plot.bar(ax=ax[0,1])\n    ax[0,1].set_title(f'Test {feature1} Distribution')\n\n    train[feature2].value_counts()[:top+incre].plot.bar(ax=ax[1,0])\n    ax[1,0].set_title(f'Train {feature2} Distribution')\n\n    test[feature2].value_counts()[:top+incre].plot.bar(ax=ax[1,1])\n    ax[1,1].set_title(f'Test {feature2} Distribution')\n\n    plt.tight_layout()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_bar_subplots('M1','M2',2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_bar_subplots('M3','M4',2,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_bar_subplots('M5','M6',2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_bar_subplots('M7','M8',2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax=plt.subplots(1,2,figsize=(15,8))\ntrain['M9'].value_counts().plot.bar(ax=ax[0])\nax[0].set_title('Train M9 Distribution')\n    \ntest['M9'].value_counts().plot.bar(ax=ax[1])\nax[1].set_title('Test M9 Distribution')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns[50:100]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='26'>Distribution of V features across Train and Test</a> "},{"metadata":{"trusted":true},"cell_type":"code","source":"V_cols=[col for col in train.columns if col[0]=='V']\nV_type=[train[col].dtype for col in V_cols]\nprint(set(V_type))\nprint(len(V_cols))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique=[(col,train[col].nunique()) for col in V_cols]\nsorted_unique=sorted(unique,key=lambda x: x[1])\nsorted_unique[:50]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_bar_subplots(feature1,feature2):\n    fig,ax=plt.subplots(2,2,figsize=(15,8))\n    train[feature1].value_counts().plot.bar(ax=ax[0,0])\n    ax[0,0].set_title(f'Train {feature1} Distribution')\n    \n    test[feature1].value_counts().plot.bar(ax=ax[0,1])\n    ax[0,1].set_title(f'Test {feature1} Distribution')\n\n    train[feature2].value_counts().plot.bar(ax=ax[1,0])\n    ax[1,0].set_title(f'Train {feature2} Distribution')\n\n    test[feature2].value_counts().plot.bar(ax=ax[1,1])\n    ax[1,1].set_title(f'Test {feature2} Distribution')\n\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I skimmed through the distribution of every V_column and removed the columns which satisfied following criteria:\n1. **extreme outliers in case of test or train** or \n2. **the Density of the major peak of test and train had very significant mismatch** or \n3. **test or train had lot of extra unique values compared to each other. **"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_subplots('V339','V338')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"detailed_subplot('V328')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='27'>Distribution of ID features across Train and Test</a> "},{"metadata":{},"cell_type":"markdown","source":"Same as done with other features. All the distributions are similar in test and train. So no ID column is dropped."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns[350:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i_cols=[col for col in train.columns if col[0]=='i']\nunique_val=[(col,train[col].nunique()) for col in i_cols]\nunique_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_subplots('id_01','id_02')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_subplots('id_03','id_04')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_subplots('id_05','id_06')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_subplots('id_07','id_08')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_subplots('id_09','id_10')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_subplots('id_11','id_13')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_subplots('id_14','id_17')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_subplots('id_18','id_19')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_subplots('id_20','id_21')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_subplots('id_22','id_24')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_subplots('id_25','id_26')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_bar_subplots('id_38','id_37')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_bar_subplots('id_35','id_36')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_bar_subplots('id_32','id_34')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_bar_subplots('id_28','id_29')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_bar_subplots('id_23','id_27')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_bar_subplots('id_15','id_16')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_bar_subplots('id_12','id_15')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='3'>Dropping Columns</a> "},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_cols=['dist2','C8','C9','C10','D6','D12','V27','V28','V45','V70','V71','V77','V78','V86','V87','V89','V91','V92','V95','V96','V97',\n          'V101','V102','V103','V107','V126','V127','V128','V130','V131','V132','V133','V134','V135','V136','V137','V143','V145','V150',\n          'V159','V160','V164','V165','V166','V167','V168','V171','V176','V177','V178','V179','V181','V182','V183','V190','V199','V202',\n          'V203','V204','V207','V211','V212','V213','V214','V215','V216','V221','V226','V227','V228','V230','V234','V240','V241','V245',\n          'V246','V255','V256','V257','V258','V259','V261','V263','V264','V265','V270','V271','V272','V273','V274','V275','V276','V277',\n          'V278','V279','V280','V291','V292','V293','V294','V295','V297','V306','V307','V308','V310','V312','V316','V317','V318','V319',\n          'V320','V321','V322','V323','V324','V331','V332','V333','V338','V339','TransactionDT','TransactionID']\n\ntrain=train.drop(drop_cols,1)\ntest=test.drop(drop_cols,1)\ntrain['TransactionAmt']=np.log(train['TransactionAmt'])\ntest['TransactionAmt']=np.log(test['TransactionAmt'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# drop_col = ['TransactionDT','TransactionID','V300', 'V309', 'V111', 'C3', 'V124', 'V106', 'V125', 'V315', 'V134', 'V102', 'V123', \n#             'V316', 'V113', 'V136', 'V305', 'V110', 'V299', 'V289', 'V286', 'V318', 'V103', 'V304', 'V116', 'V298', \n#             'V284', 'V293', 'V137', 'V295', 'V301', 'V104', 'V311', 'V115', 'V109', 'V119', 'V321', 'V114', 'V133', \n#             'V122', 'V319', 'V105', 'V112', 'V118', 'V117', 'V121', 'V108', 'V135', 'V320', 'V303', 'V297', 'V120']\n\n# drop_col=['TransactionDT','TransactionID']\n# train=train.drop(drop_col,1)\n# test=test.drop(drop_col,1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='4'>Reducing memory size</a> "},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# From kernel https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n# WARNING! THIS CAN DAMAGE THE DATA \ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=reduce_mem_usage(train)\ntest=reduce_mem_usage(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# obj_cols=[col for col in train.columns if train[col].dtype=='object']\n# if 'isFraud' in obj_cols:\n#     obj_cols.remove('isFraud')\n    \n# unique_values=sorted([(col,train[col].nunique()+test[col].nunique()) for col in obj_cols],key=lambda x: x[1],reverse=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dummy_cols=[col[0] for col in unique_values[:18]]\n# target=train['isFraud']\n# train=train.drop('isFraud',1)\n# ntrain=train.shape[0]\n# print(train.shape)\n# merged_data=pd.concat([train,test],axis=0,ignore_index=True)\n# X=pd.get_dummies(merged_data,columns=dummy_cols)\n# merged_data.drop(dummy_cols,axis=1,inplace=True)\n# merged_data=pd.concat([merged_data,X],axis=1)\n# del X\n# train=merged_data[:ntrain]\n# test=merged_data[ntrain:]\n# print(train.shape)\n# del merged_data\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='5'>Feature Engineering</a> "},{"metadata":{"trusted":true},"cell_type":"code","source":"num_cols=[col for col in train.columns if (('float' in str(train[col].dtype)) or ('int' in str(train[col].dtype)))]\nnum_cols.remove('isFraud')\ntrain['mean']=train[num_cols].mean(axis=1)\ntest['mean']=test[num_cols].mean(axis=1)\ntrain['std']=train[num_cols].std(axis=1)\ntest['std']=test[num_cols].std(axis=1)\ntrain['max']=train[num_cols].max(axis=1)\ntest['max']=test[num_cols].max(axis=1)\ntrain['min']=train[num_cols].min(axis=1)\ntest['min']=test[num_cols].min(axis=1)\ntrain['median']=train[num_cols].median(axis=1)\ntest['median']=test[num_cols].median(axis=1)\ntrain['skew']=train[num_cols].skew(axis=1)\ntest['skew']=test[num_cols].skew(axis=1)\ntrain['kurt']=train[num_cols].kurt(axis=1)\ntest['kurt']=test[num_cols].kurt(axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Randomly generating some features. Very illogical I know :P"},{"metadata":{"trusted":true},"cell_type":"code","source":"# columns=['TransactionAmt','card1','card2','addr2','dist1','C1','C2','D1','D2','V1','V2',\n#         'id_01','id_02']\n# obj_cols=['DeviceInfo','card4','card6','ProductCD','DeviceType']\n\n# for col in columns:\n#     for feat in obj_cols:\n#         train[f'{col}_mean_group_{feat}']=train[col]/train.groupby(feat)[col].transform('mean')\n#         test[f'{col}_mean_group_{feat}']=test[col]/test.groupby(feat)[col].transform('mean')\n#         train[f'{col}_max_group_{feat}']=train[col]/train.groupby(feat)[col].transform('max')\n#         test[f'{col}_max_group_{feat}']=test[col]/test.groupby(feat)[col].transform('max')\n#         train[f'{col}_min_group_{feat}']=train[col]/train.groupby(feat)[col].transform('min')\n#         test[f'{col}_min_group_{feat}']=test[col]/test.groupby(feat)[col].transform('min')\n#         train[f'{col}_skew_group_{feat}']=train[col]/train.groupby(feat)[col].transform('skew')\n#         test[f'{col}_skew_group_{feat}']=test[col]/test.groupby(feat)[col].transform('skew')\n#         train[f'{col}_skew_group_{feat}']=train[col]/train.groupby(feat)[col].transform('count')\n#         test[f'{col}_skew_group_{feat}']=test[col]/test.groupby(feat)[col].transform('count')\n   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def fill_missing(df):\n#     num_cols=[col for col in df.columns if df[col].dtype=='float64' or df[col].dtype=='int64']\n#     for col in num_cols:\n#         df[col]=df[col].fillna(df[col].mean())\n#     obj_cols=[col for col in df.columns if df[col].dtype=='object']\n#     for col in obj_cols:\n#         df[col]=df[col].fillna(df[col].mode()[0])\n        \n#     return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_df=fill_missing(train_df)\n# test_df=fill_missing(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nobject_cols=[col for col in train.columns if (('category' in str(train[col].dtype)) or ('object' in str(train[col].dtype)))]\nle=LabelEncoder()\nfor col in object_cols:\n    le.fit(list(train[col].values)+list(test[col].values))\n    train[col]=le.transform(list(train[col].values))\n    test[col]=le.transform(list(test[col].values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape , test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target=train['isFraud']\ntrain=train.drop('isFraud',1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.model_selection import train_test_split\n# train_X,val_X,train_y,val_y=train_test_split(train,target,test_size=0.2,random_state=seed,stratify=target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from xgboost import XGBClassifier\n# from sklearn.metrics import roc_auc_score\n\n\n# def objective(params):\n#     params=dict(max_depth=int(params['max_depth']),\n#                subsample=np.round(params['subsample'],3),\n#                colsample_bytree=np.round(params['colsample_bytree'],3),\n#                learning_rate=np.round(params['learning_rate'],3),\n#                verbosity=0)\n    \n#     clf=XGBClassifier(n_estimators=1000,random_state=seed,**params,tree_method='gpu_hist')\n#     clf.fit(train_X,train_y,eval_set=[(val_X,val_y)],eval_metric='auc',early_stopping_rounds=10)\n#     val_pred=clf.predict(val_X)\n#     score=roc_auc_score(val_y,val_pred)\n#     return score\n\n# space={'max_depth':hp.quniform('max_depth',2,10,2),\n#       'subsample':hp.uniform('subsample',0.1,1),\n#       'colsample_bytree':hp.uniform('colsample_bytree',0.1,1),\n#       'learning_rate':hp.uniform('learning_rate',0.01,0.1)}\n\n# trial=Trials()\n# best=fmin(fn=objective,algo=tpe.suggest,space=space,max_evals=100,trials=trial,rstate=np.random.RandomState(seed))\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# best['max_depth']=int(best['max_depth'])\n# print('Best parameters:',best)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# del train_X,val_X,train_y,val_y\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TID=[t['tid'] for t in trial.trials]\n# Loss=[t['result']['loss'] for t in trial.trials]\n# maxd=[t['misc']['vals']['max_depth'][0] for t in trial.trials]\n# lr=[t['misc']['vals']['learning_rate'][0] for t in trial.trials]\n# sub=[t['misc']['vals']['subsample'][0] for t in trial.trials]\n# col_samp=[t['misc']['vals']['colsample_bytree'][0] for t in trial.trials]\n\n\n# hyperopt_xgb=pd.DataFrame({'tid':TID,'loss':Loss,\n#                           'max_depth':maxd,'learning_rate':lr,\n#                           'subsample':sub, 'colsample_bytree':col_samp})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.subplots(3,2,figsize=(10,10))\n# plt.subplot(3,2,1)\n# sns.scatterplot(x='tid',y='max_depth',data=hyperopt_xgb)\n# plt.subplot(3,2,2)\n# sns.scatterplot(x='tid',y='loss',data=hyperopt_xgb)\n# plt.subplot(3,2,3)\n# sns.scatterplot(x='tid',y='learning_rate',data=hyperopt_xgb)\n# plt.subplot(3,2,4)\n# sns.scatterplot(x='tid',y='subsample',data=hyperopt_xgb)\n# plt.subplot(3,2,5)\n# sns.scatterplot(x='tid',y='colsample_bytree',data=hyperopt_xgb)\n# plt.subplot(3,2,6)\n# sns.scatterplot(x='tid',y='loss',data=hyperopt_xgb)\n\n# plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='6'>Model Development and Feature Importance</a> "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nimport gc\n\nnfolds=10\n\n\nxgb_params=dict(n_estimators=1000,\n                verbosity=0,\n                tree_method='gpu_hist',\n                random_state=seed,\n               colsample_bytree=0.6,\n               subsample=0.6,\n               learning_rate=0.05,\n               max_depth=9)\n\nlgb_params=dict(objective='binary',\n               num_leaves=62,\n               seed=seed,\n               max_depth=9,\n               pos_bagging_fraction=0.5,\n               neg_bagging_fraction=1.0,\n               bagging_freq=5,\n               feature_fraction=0.9,\n                metric='auc',\n               learning_rate=0.05,\n               verbosity=-1,\n               device='gpu')\n\n\nskfold=StratifiedKFold(nfolds,random_state=seed)\n\n\n\ndef build_model(params,model='xgb',plot_feature_importance=True):\n    oof=np.zeros(train.shape[0])\n    pred=np.zeros(test.shape[0])\n    scores=[]\n    feature_importance=pd.DataFrame()\n    for i,(train_index,val_index) in enumerate(skfold.split(train,target)):\n        print('Fold :',i+1)\n\n        \n        if model=='xgb':\n            train_X,val_X=train.iloc[train_index,:],train.iloc[val_index,:]\n            train_y,val_y=target[train_index],target[val_index]\n            clf=XGBClassifier(**params)\n            clf.fit(train_X,train_y,eval_metric='auc',eval_set=[(val_X,val_y)],early_stopping_rounds=10,verbose=20)\n            val_pred=clf.predict_proba(val_X)[:,1]\n        \n        \n        if model=='lgb':\n        \n            train_d=lgb.Dataset(train.iloc[train_index,:].values,label=target[train_index].values)\n            val_d=lgb.Dataset(train.iloc[val_index,:].values,label=target[val_index].values)\n            clf=lgb.train(params,train_d,num_boost_round=1000,valid_sets=[val_d],verbose_eval=20,early_stopping_rounds=10)\n            val_pred=clf.predict(train.iloc[val_index,:].values)\n        \n    \n        oof[val_index]=val_pred\n        val_score=roc_auc_score(target[val_index],val_pred)\n        scores.append(val_score)\n        print(f'Validation score using {model} for fold {i} :'+ str(val_score))\n        print('-'*100)\n        \n        if model=='xgb':\n            pred+=clf.predict_proba(test)[:,1]/nfolds\n        if model=='lgb':\n            pred+=clf.predict(test.values)/nfolds\n            \n        if model=='xgb':\n            del train_X,val_X,train_y,val_y\n        if model=='lgb':\n            del train_d,val_d\n       \n        gc.collect()\n        \n        \n        fold_importance=pd.DataFrame()\n        fold_importance['feature']=train.columns\n        if model=='xgb':\n            fold_importance['importance']=clf.feature_importances_\n        if model=='lgb':\n            fold_importance['importance']=clf.feature_importance()\n        fold_importance['fold']=i+1\n        feature_importance=pd.concat([feature_importance,fold_importance],axis=0)\n            \n            \n    print('Mean validation score :',np.mean(scores)) \n    \n    if plot_feature_importance:\n        df=feature_importance[['feature','importance']].groupby('feature').mean().sort_values(by='importance',ascending=False).reset_index()\n        plt.figure(figsize=(10,10))\n        sns.barplot(x='importance',y='feature',data=df.iloc[:25,:])\n        plt.title('Feature Importances')\n        \n    return pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred=build_model(xgb_params,model='xgb')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub=pd.read_csv('../input/sample_submission.csv')\nsub['isFraud']=pred\nsub.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}