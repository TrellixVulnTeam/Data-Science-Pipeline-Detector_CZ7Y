{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Here we will use simple one-hot encoding with bayesian hyperparameter tuning for\n#XGB regressor to predict fraud probability over attached dataset\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn import metrics\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom xgboost import XGBRegressor\nfrom fancyimpute import KNN\nimport gc\nfrom bayes_opt import BayesianOptimization\nimport warnings\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n#id_test = pd.read_csv('../input/test_identity.csv')\ns_s = pd.read_csv('../input/sample_submission.csv')\nid_train = pd.read_csv('../input/train_identity.csv')\ntrans_train = pd.read_csv('../input/train_transaction.csv')\n#trans_test = pd.read_csv('../input/test_transaction.csv')\n# Any results you write to the current directory are saved as output.\ntrans_train.set_index('TransactionID').join(id_train.set_index('TransactionID'))\ngc.collect()\ntrain = trans_train\n#trans_test.set_index('TransactionID').join(id_test.set_index('TransactionID'))\n#del [id_test, trans_test]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#let's drop cols that are mostly nulls or 1 value\n#There's over 50 of them!\none_value_cols = [col for col in train.columns if train[col].nunique() <= 1]\nmany_null_cols = [col for col in train.columns if train[col].isnull().sum() / train.shape[0] > 0.9]\nbig_top_value_cols = [col for col in train.columns if train[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\n\ncols_to_drop = list(set(many_null_cols + big_top_value_cols + one_value_cols))\ncols_to_drop.remove('isFraud')\nlen(cols_to_drop)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(cols_to_drop, axis=1)\ntrain = train.head(10000)\n#train.to_csv('train_cols_trimmed.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nt_train = train.copy()\ny = train.isFraud\nX = t_train.drop(['isFraud'], axis = 1, inplace=True)\nX_train_full, X_valid_full, y_train_full, y_valid_full = train_test_split(t_train, y, test_size = 0.2, random_state = 0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#I've commited the sin of verbosity here, but it's just plain label encoding\n#I've dropped categorical columns with too much categories\n#Could have limit memory usage by changing types\n\nfrom sklearn.preprocessing import LabelEncoder\n\ncategorical_cols = [cname for cname in X_train_full.columns if\n                    X_train_full[cname].nunique() < 10 and \n                    X_train_full[cname].dtype == \"object\"]\nnum_cols = [cname for cname in X_train_full.columns if\n           X_train_full[cname].dtype in ['int64','float64']]\nmy_cols = categorical_cols + num_cols\n\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nmy_cols2 = [col for col in X_train.columns if col in X_valid]\nfor i in my_cols2:\n    if i in X_train: X_train.drop(i, axis=1)\n    if i in X_valid: X_valid.drop(i, axis=1)\n\nnumerical_transformer = SimpleImputer(strategy='mean')\ncat_imp = SimpleImputer(strategy='most_frequent')\n\nX_train_n = pd.DataFrame(numerical_transformer.fit_transform(X_train[num_cols]))\nX_train_c = pd.DataFrame(cat_imp.fit_transform(X_train[categorical_cols]))\nX_train_n.columns = num_cols\nX_train_c.columns = categorical_cols\nX_train2 = X_train_n.join(X_train_c,how = 'left')\n\nX_vn = pd.DataFrame(numerical_transformer.fit_transform(X_valid[num_cols]))\nX_vc = pd.DataFrame(cat_imp.fit_transform(X_valid[categorical_cols]))\nX_vn.columns = num_cols\nX_vc.columns = categorical_cols\nX_valid2 = X_vn.join(X_vc,how = 'left')\n\nfor f in X_train2.columns:\n    if  X_train2[f].dtype=='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(X_train2[f].values) + list(X_valid2[f].values))\n        X_train2[f] = lbl.transform(list(X_train2[f].values))\n        X_valid2[f] = lbl.transform(list(X_valid2[f].values))  \nX_train2 = X_train2.reset_index()\nX_valid2 = X_valid2.reset_index()\n\n\n'''\ncategorical_transformer = Pipeline(steps=[\n    ('onehot', OneHotEncoder(handle_unknown='ignore')),\n    ('imputer', SimpleImputer(strategy='most_frequent'))\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, num_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now I'm doing same thing for our test set\n\ntrans_test = pd.read_csv('../input/test_transaction.csv').head(10000)\nid_test = pd.read_csv('../input/test_identity.csv').head(10000)\ntrans_test.set_index('TransactionID').join(id_test.set_index('TransactionID'))\ntest0 = trans_test\ntest0 = test0.drop(cols_to_drop, axis=1)\nfor i in my_cols2:\n    if i in test0: test0.drop(i, axis=1)\ndel trans_test, id_test\ngc.collect()\ntest0_n = pd.DataFrame(numerical_transformer.fit_transform(test0[num_cols]))\ntest0_c = pd.DataFrame(cat_imp.fit_transform(test0[categorical_cols]))\ntest0_n.columns = num_cols\ntest0_c.columns = categorical_cols\ntest00 = test0_n.join(test0_c,how = 'left')\ndel test0_c, test0_n, test0\n\nfor f in test00.columns:\n    if  test00[f].dtype=='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(test00[f].values))\n        test00[f] = lbl.transform(list(test00[f].values))\ntest00 = test00.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#This is by far the coolest part, so let me explain what is going to happen.\n#We will use XGB regressor as our model (tried also random forest, but it a bit\n#worse), but we don't know what our hyperparameters should be. But we won't use\n#grid, we will minimize the number of searches by using bayesian optimization!\n\nimport warnings\nmodel = XGBRegressor()\n#model = RandomForestRegressor(n_estimators=400, random_state=0)\nX_train1 = X_train2\ny_train1 = y_train_full\ny_valid = y_valid_full.head(3000)\nX_valid = X_valid2.head(3000)\n\ndef BB_fun(max_depth, n_estimators, learning_rate):# reg_alpha, reg_lambda):\n    param = {\n        'max_depth':int(max_depth),\n        'n_estimators':int(n_estimators),\n        'objective':'binary:logistic',\n        'learning_rate':learning_rate,\n        #'reg_alpha':reg_alpha,\n        #'reg_lambda':reg_lambda,\n        'random_state':0\n    }\n    model = XGBRegressor(**param)#max_depth=int(max_depth), n_estimators=int(n_estimators),objective='binary:logistic', learning_rate=learning_rate, reg_alpha=reg_alpha, reg_lambda=reg_lambda)\n    clf = model.fit(X_train1,y_train1)\n    YY = clf.predict(X_valid)\n    score = roc_auc_score(y_valid,YY)\n    return score\n\n\n#clf = Pipeline(steps=[('preprocessor', preprocessor),\n#                      ('model', model)\n#                     ])\n\npbounds = {'max_depth':(2,5),\n           'n_estimators':(50,400),\n           'learning_rate':(0.04,0.2),\n           #'reg_alpha':(0,0.5),\n           #'reg_lambda':(0.5,1.5),\n          }\noptimizer = BayesianOptimization(\n        f = BB_fun,\n        pbounds = pbounds,\n        verbose = 2,\n        random_state=0\n)\nwith warnings.catch_warnings():\n    warnings.filterwarnings('ignore')\n    optimizer.maximize(init_points=10, n_iter=10, acq='ucb', xi=0.0, alpha=1e-6)\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = XGBRegressor(learning_rate=0.1278,max_depth=4,n_estimators=261)\nmodel.fit(X_train2,y_train_full)\nprint(\"gotowe\")\ndel X_valid_full, X_train_full, y_train_full, y_valid_full\ngc.collect()\nYY = model.predict(test00)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = pd.DataFrame({'Id': s_s.index,\n                       'isFraud': YY})\noutput.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}