{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"# Input data\ntrain_id = pd.read_csv('/kaggle/input/train_identity.csv')\ntest_id = pd.read_csv('/kaggle/input/test_identity.csv')\ntest_tx = pd.read_csv('/kaggle/input/test_transaction.csv')\ntrain_tx = pd.read_csv('/kaggle/input/train_transaction.csv')\n# sample = pd.read_csv('/kaggle/input/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# train_id.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# train_tx.loc[train_tx.isFraud==1].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# train_tx.loc[train_tx.isFraud==0].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"device = ['Windows', 'iOS Device', 'MacOS']\ntrain_id.loc[~train_id.DeviceInfo.isin(device), 'DeviceInfo'] = 'other'\ntest_id.loc[~test_id.DeviceInfo.isin(device), 'DeviceInfo'] = 'other'","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#https://stackoverflow.com/questions/43311555/how-to-drop-column-according-to-nan-percentage-for-dataframe\ntrain_tx = train_tx.loc[:, train_tx.isnull().sum() < 0.85*train_tx.shape[0]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_id = train_id.loc[:, train_id.isnull().sum() < 0.85*train_id.shape[0]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_tx.shape, train_id.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df = pd.merge(left=train_id,right=train_tx, how = 'right', left_on='TransactionID', right_on='TransactionID')\ndel train_id\ndel train_tx","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df['isFraud'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_n = train_df[train_df['isFraud'] ==0].sample(30000,random_state = 2)\ndf_y = train_df[train_df['isFraud'] == 1]\ntrain_df = pd.concat([df_n,df_y])\ntrain_df['isFraud'].value_counts()\ndel df_n\ndel df_y","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# train_id.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df['DeviceType'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# device = ['Windows', 'iOS Device', 'MacOS']\n# train_df.loc[~train_df.DeviceInfo.isin(device), 'DeviceInfo'] = 'other'","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# train_df['DeviceInfo'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# train_tx.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# train_df['P_emaildomain'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# train_df['R_emaildomain'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"categorical = ['ProductCD','card1','card2','card3','card4','card5','card6','addr1','addr2', 'DeviceType', 'DeviceInfo',\n               'P_emaildomain','R_emaildomain','M1','M2','M3','M4','M5','M6','M7','M8','M9']\nnumerical = list(set(list(train_df.columns))-set(['ProductCD','card1','card2','card3','card4','card5','card6','addr1','addr2', 'DeviceType', 'DeviceInfo',\n                                                  'P_emaildomain','R_emaildomain','M1','M2','M3','M4','M5','M6','M7','M8','M9']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"target = ['isFraud']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X = train_df[numerical].copy()\n\nX.fillna(0,inplace = True)\n\nY = train_df[categorical].copy()\n\nY.fillna('NA',inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df = pd.concat([X,Y],axis = 1)\ndel X\ndel Y","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df = train_df.loc[:,~train_df.columns.duplicated()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"corr_matrix = train_df[numerical].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"var_list = corr_matrix['isFraud'].sort_values()[:85]\ndel corr_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# var_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"num_var_list = list(var_list.index)\ncat_var_list_prev = ['card4','card6','M4','ProductCD','DeviceType', 'DeviceInfo', 'P_emaildomain','R_emaildomain']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import tensorflow as tf\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import preprocessing","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"##Splitting data into train, test & validation\nfrom sklearn.model_selection import train_test_split\nX_train, X_val = train_test_split(train_df, test_size=0.15, random_state=0)\nX_val, X_test = train_test_split(X_val, test_size=0.5, random_state=0)\ndel train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train.shape, X_val.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Deep features\n# Defining tensor type for numerical attributes\nnumerical_features = [tf.feature_column.numeric_column(key = k) for k in num_var_list]\n\n# Defining tensor type for categorical attributes\nfrom tensorflow.contrib import layers\n\ncat_features1 = [tf.feature_column.embedding_column(tf.feature_column.categorical_column_with_hash_bucket(key = k, hash_bucket_size=100), dimension=4) for k in cat_var_list_prev]\n\nfeatures = num_var_list + cat_var_list_prev\nDEEP_FEATURES= numerical_features + cat_features1\nLABEL_NAME = target","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Cross columns\n_HASH_BUCKET_SIZE=1000\ncrossed_columns = [\n     tf.feature_column.crossed_column(\n         [\"DeviceType\",\"DeviceInfo\"], hash_bucket_size=_HASH_BUCKET_SIZE),\n     tf.feature_column.crossed_column(\n         [\"P_emaildomain\", \"R_emaildomain\"],\n         hash_bucket_size=_HASH_BUCKET_SIZE)\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Wide features\nfrom tensorflow.contrib import layers\n\ncat_var_wide = list(set(cat_var_list_prev) - set(['DeviceType', 'DeviceInfo','P_emaildomain','R_emaildomain']))\ncat_features2 = [tf.feature_column.categorical_column_with_hash_bucket(key = k, hash_bucket_size=100) for k in cat_var_wide]\n\nWIDE_FEATURES = numerical_features + cat_features2  + crossed_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# initializer function for train data \ndef train_input_fn(df, batch_size = 256):\n    #1. Convert dataframe into correct (features,label) format for Estimator API\n    dataset = tf.data.Dataset.from_tensor_slices(tensors = (dict(df[features]), df[LABEL_NAME]))\n    \n    # Note:\n    # If we returned now, the Dataset would iterate over the data once  \n    # in a fixed order, and only produce a single element at a time.\n    \n    #2. Shuffle, repeat, and batch the examples.\n    dataset = dataset.shuffle(buffer_size = 1000).repeat(count = None).batch(batch_size = batch_size)\n   \n    return dataset\n\n# initializer function for validate data \ndef eval_input_fn(df, batch_size = 256):\n    #1. Convert dataframe into correct (features,label) format for Estimator API\n    dataset = tf.data.Dataset.from_tensor_slices(tensors = (dict(df[features]), df[LABEL_NAME]))\n    \n    #2.Batch the examples.\n    dataset = dataset.batch(batch_size = batch_size)\n   \n    return dataset\n\n# initializer function for test data \ndef predict_input_fn(df, batch_size = 256):\n    #1. Convert dataframe into correct (features) format for Estimator API\n    dataset = tf.data.Dataset.from_tensor_slices(tensors = dict(df[features])) # no label\n\n    #2.Batch the examples.\n    dataset = dataset.batch(batch_size = batch_size)\n   \n    return dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# DNN Classifier object\nprint(\"No. of Layers 3\")\nprint(\"No. of Units [256,128, 64]\")\nlayer1 = 128\nlayer2 = 64\nlayer3 = 32\n#layer4 = 64\nmodel = tf.estimator.DNNLinearCombinedClassifier(\n    linear_feature_columns=WIDE_FEATURES,\n    linear_optimizer=tf.train.FtrlOptimizer(learning_rate=0.01),\n    dnn_feature_columns=DEEP_FEATURES,\n    dnn_hidden_units=[layer1, layer2],\n    dnn_activation_fn=tf.nn.relu, \n#     dnn_dropout=0.7,\n    dnn_optimizer=tf.train.ProximalAdagradOptimizer(\n      learning_rate=0.008),\n#     l1_regularization_strength=0.001,\n#     l2_regularization_strength=0.001),\n    n_classes=2,\n    config = tf.estimator.RunConfig(tf_random_seed = 1), # for reproducibility\n    batch_norm=True   #loss_reduction=tf.losses.Reduction.SUM\n#    ,model_dir = 'mar/'\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Executing the DNN model on train data\ntf.reset_default_graph()\nimport logging\ntf.logging.set_verbosity(tf.logging.INFO) # so loss is printed during training\nmodel.train(input_fn = lambda: train_input_fn(df = X_train), steps = 3500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(model.evaluate(input_fn = lambda: eval_input_fn(df = X_train)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"eval = model.evaluate(input_fn = lambda: eval_input_fn(df = X_val))\nprint(eval)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(model.evaluate(input_fn = lambda: eval_input_fn(df = X_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"##### Function to calculate various evaluation metrics\ndef acc_matrix(model, data, features, LABEL_NAME):\n\n\ttest = model.predict(input_fn = lambda: predict_input_fn(df = data[features]))\n\n\tl=[]\n\tfor i in test:\n\t   l.append(int(i[\"classes\"][0])) \n\n\tmodel_predict=np.array([i for i in l]) \n\n\tmodel_actual=np.array(data[LABEL_NAME]) \n\n\tprint(confusion_matrix(model_actual,model_predict) )\n\n\n# Accuracy matrix for train data \nprint(\"For Train\")\nacc_matrix(model, X_train, features, LABEL_NAME)\n\n# Accuracy matrix for validate data \nprint(\"For Validate\")\nacc_matrix(model, X_val, features, LABEL_NAME)\n\n# Accuracy matrix for test data \nprint(\"For Test\")\nacc_matrix(model, X_test, features, LABEL_NAME)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Predict dataset\ntest_df = pd.merge(left=test_id,right=test_tx,how = 'right',left_on='TransactionID', right_on='TransactionID')\n\n# test_df.loc[~test_df.DeviceInfo.isin(device), 'DeviceInfo'] = 'other'\n\nX = test_df[num_var_list].copy()\n\nX.fillna(0,inplace = True)\n\nY = test_df[cat_var_list_prev].copy()\n\nY.fillna('NA',inplace = True)\n\ntest_df = pd.concat([X,Y],axis = 1)\ntest_df = test_df.loc[:,~test_df.columns.duplicated()]\ndel X\ndel Y","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Prediction metrics\nresults = model.predict(input_fn = lambda: predict_input_fn(df = test_df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"l1=[]\nfor i in results:\n    l1.append((i[\"probabilities\"][1]))\n#     prob=np.array([i for i in l1])\n\n# l2=[]\n# for probability_of_1 in l1:\n#   if probability_of_1 > 0.5:\n#     l2.append(int(1))\n#   else:\n#     l2.append(int(0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# l2","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"transaction_id = pd.merge(left=test_id,how = 'right',right=test_tx, left_on='TransactionID', right_on='TransactionID')['TransactionID']\ndel test_id\ndel test_tx\ndel test_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"len(transaction_id)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"len(l1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"v1 = pd.DataFrame({'TransactionID':list(transaction_id),'isFraud':l1})\nv1.to_csv('submission2.csv',index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"v1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}