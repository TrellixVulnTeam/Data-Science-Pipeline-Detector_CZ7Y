{"cells":[{"metadata":{"_uuid":"fb854c39da7d8fc6e098c22086fcdb9602779616"},"cell_type":"markdown","source":"For more details :\nhttp://fastml.com/adversarial-validation-part-one/\nhttps://www.kaggle.com/konradb/adversarial-validation-and-other-scary-terms\nhttps://www.kaggle.com/ogrellier/adversarial-validation-and-lb-shakeup\n\nBasic data processing from:\nhttps://www.kaggle.com/artkulak/ieee-fraud-simple-baseline-0-9383-lb\n\n**Some code snippet below is from other past competition kernels , but now I don't remember the owner, if you are the one please mention in comment and I will add your credit here :)**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Load libraries\nimport numpy as np\nimport pandas as pd\nimport gc\nimport datetime\n\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nfrom sklearn import preprocessing\n# Params\nNFOLD = 5\nDATA_PATH = '../input/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_transaction = pd.read_csv('../input/train_transaction.csv', index_col='TransactionID')\ntest_transaction = pd.read_csv('../input/test_transaction.csv', index_col='TransactionID')\n\ntrain_identity = pd.read_csv('../input/train_identity.csv', index_col='TransactionID')\ntest_identity = pd.read_csv('../input/test_identity.csv', index_col='TransactionID')\n\nsample_submission = pd.read_csv('../input/sample_submission.csv', index_col='TransactionID')\n\ntrain = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n\ndel train_transaction, train_identity, test_transaction, test_identity\n\n\n# Label Encoding\nfor f in test.columns:\n    if train[f].dtype=='object' or test[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(train[f].values) + list(test[f].values))\n        train[f] = lbl.transform(list(train[f].values))\n        test[f] = lbl.transform(list(test[f].values))\n        \nprint(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# From kernel https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n# WARNING! THIS CAN DAMAGE THE DATA \ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\ntrain = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train.drop('isFraud', axis=1,inplace=True)\n# Mark train as 1, test as 0\ntrain['target'] = 1\ntest['target'] = 0\n\n# Concat dataframes\nn_train = train.shape[0]\ndf = pd.concat([train, test], axis = 0)\ndel train, test\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"996b4a6f6ffde9ff68a505473b46692886d42268"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c1db7ede9d1a51d76cc5e3d142f41cc2df01f394"},"cell_type":"code","source":"# Remove columns with only one value in our training set\npredictors = list(df.columns.difference(['target']))\ndf_train = df.iloc[:n_train].copy()\ncols_to_remove = [c for c in predictors if df_train[c].nunique() == 1]\ndf.drop(cols_to_remove, axis=1, inplace=True)\n\n# Update column names\npredictors = list(df.columns.difference(['target']))\n\n# Get some basic meta features\ndf['cols_mean'] = df[predictors].replace(0, np.NaN).mean(axis=1)\ndf['cols_count'] = df[predictors].replace(0, np.NaN).count(axis=1)\ndf['cols_sum'] = df[predictors].replace(0, np.NaN).sum(axis=1)\ndf['cols_std'] = df[predictors].replace(0, np.NaN).std(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c17503acbf649cc5b5df9e09b300b4557f677d6"},"cell_type":"code","source":"# Prepare for training\n\n# Shuffle dataset\ndf = df.iloc[np.random.permutation(len(df))]\ndf.reset_index(drop = True, inplace = True)\n\n# Get target column name\ntarget = 'target'\n\n# lgb params\nlgb_params = {\n        'boosting': 'gbdt',\n        'application': 'binary',\n        'metric': 'auc', \n        'learning_rate': 0.1,\n        'num_leaves': 32,\n        'max_depth': 8,\n        'bagging_fraction': 0.7,\n        'bagging_freq': 5,\n        'feature_fraction': 0.7,\n}\n\n# Get folds for k-fold CV\nfolds = KFold(n_splits = NFOLD, shuffle = True, random_state = 0)\nfold = folds.split(df)\n    \neval_score = 0\nn_estimators = 0\neval_preds = np.zeros(df.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"6099aa185a9d5313f9f4541438d8b4387508a552"},"cell_type":"code","source":"# Run LightGBM for each fold\nfor i, (train_index, test_index) in enumerate(fold):\n    print( \"\\n[{}] Fold {} of {}\".format(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), i+1, NFOLD))\n    train_X, valid_X = df[predictors].values[train_index], df[predictors].values[test_index]\n    train_y, valid_y = df[target].values[train_index], df[target].values[test_index]\n\n    dtrain = lgb.Dataset(train_X, label = train_y,\n                          feature_name = list(predictors)\n                          )\n    dvalid = lgb.Dataset(valid_X, label = valid_y,\n                          feature_name = list(predictors)\n                          )\n        \n    eval_results = {}\n    \n    bst = lgb.train(lgb_params, \n                         dtrain, \n                         valid_sets = [dtrain, dvalid], \n                         valid_names = ['train', 'valid'], \n                         evals_result = eval_results, \n                         num_boost_round = 5000,\n                         early_stopping_rounds = 100,\n                         verbose_eval = 100)\n    \n    print(\"\\nRounds:\", bst.best_iteration)\n    print(\"AUC: \", eval_results['valid']['auc'][bst.best_iteration-1])\n\n    n_estimators += bst.best_iteration\n    eval_score += eval_results['valid']['auc'][bst.best_iteration-1]\n   \n    eval_preds[test_index] += bst.predict(valid_X, num_iteration = bst.best_iteration)\n    \nn_estimators = int(round(n_estimators/NFOLD,0))\neval_score = round(eval_score/NFOLD,6)\n\nprint(\"\\nModel Report\")\nprint(\"Rounds: \", n_estimators)\nprint(\"AUC: \", eval_score)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9be534b35a88366312600f4d74808452b983cd5d"},"cell_type":"code","source":"# Feature importance\nlgb.plot_importance(bst, max_num_features = 20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1d5823908008e583509eee00842644bc8dd21900"},"cell_type":"markdown","source":"As we can see, the separation is almost perfect - which strongly suggests that the train / test rows are very easy to distinguish. **Meaning the distribution of Train and Test is not the same**"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}