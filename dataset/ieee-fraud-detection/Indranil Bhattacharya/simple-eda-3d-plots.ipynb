{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"%%time\ntrain_transaction = pd.read_csv('../input/train_transaction.csv')\ntest_transaction = pd.read_csv('../input/test_transaction.csv')\n\ntrain_identity = pd.read_csv('../input/train_identity.csv')\ntest_identity = pd.read_csv('../input/test_identity.csv')\n\nsample_submission = pd.read_csv('../input/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_transaction.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_identity.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_identity.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction.shape , test_transaction.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_identity.shape , test_identity.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l1 = train_transaction.columns\nl2= train_identity.columns\nlist(set(l1) & set(l2)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train_transaction.merge(train_identity , how = 'left' , on = 'TransactionID')\ntest = test_transaction.merge(test_identity , how = 'left' , on = 'TransactionID')\nprint(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\n\ndel train_transaction, train_identity, test_transaction, test_identity\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see missing values in the train data variable wise "},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have lots of variables with missing values, now let's see the data types of each column"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What are the categorical columns do we have in this data?, let's see"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols = [c for c in train.columns if train[c].dtype == object ]\ncat_cols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see the number of different entries for those categorical columns "},{"metadata":{"trusted":true},"cell_type":"code","source":"for c in cat_cols:\n    print('number of unique entries for column' , c , '=' , train[c].nunique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's quickly check the distribution of the target variable here"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isFraud.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isFraud.value_counts().plot('bar')\nprint('target ratio is', round(20663/len(train)*100,2) , 'percent')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that we have highly imbalanced binary target distribution.\nNow, from [Bojan's public kernel](https://www.kaggle.com/tunguz/adversarial-ieee) we know that TransactionDT is the variable which has different distribution in both train and test data, let's re-confirm that quickly. "},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.display.float_format = '{:.2f}'.format\ntrain.TransactionDT.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.TransactionDT.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.TransactionDT.max() < test.TransactionDT.min()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, we see that the data was splot by this variable and we can not use this variable readily, probably creating other variables from this variable like - time or day or weekend or not kind of variable can be useful from this one (If possible). Also, this variable can help us in creating different effective validation strategy.\n\nAnyway, let's move on and see the distribution of transaction amount."},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.hist(train['TransactionAmt'] , bins = 100)\nplt.title('transaction amount for train set')\nplt.show()\n\nplt.hist(test['TransactionAmt'] , bins = 100)\nplt.title('transaction amount for test set')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.TransactionAmt.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.TransactionAmt.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the transaction amount in the train data is ranging till 32K where as the same ranges till 10K in test, let's see their distribution in the log scale."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(np.log(train['TransactionAmt']) , bins = 100)\nplt.title('Log scale transaction amount for train set')\nplt.show()\n\nplt.hist(np.log(test['TransactionAmt']), bins = 100)\nplt.title('Log scale transaction amount for test set')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What about the numeric columns? Let's look at them quickly."},{"metadata":{"trusted":true},"cell_type":"code","source":"## Let's subset the numerical columns in train data ##\n\nnum_cols = [c for c in train.columns if train[c].dtype != object ]\ntrain_num = train[num_cols]\n#print(train_num.shape)\ntrain_num.head()\nmissing_cols = [c for c in train_num.columns if train_num[c].isnull().sum()/len(train_num) >0.80 ]\nlen(missing_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow, as we can see that, there are 69 columns in the train data which have more than 80% missing entries.\nBut, are all of them really numeric?"},{"metadata":{"trusted":true},"cell_type":"code","source":"for c in train_num.columns:\n    print('number of unique entries for column' , c , '=' , train_num[c].nunique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are lot's of columns which have very less number of unique values altogether, probably treating them as categorical can help.\nLet's subset our train data with columns which have more unique numbers, so probably the numerical columns. Also, let's impute the missing values by their column means"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_cols = [c for c in train_num.columns if train_num[c].nunique()>5000 ]\nlen(num_cols) ## 40 columns\n#num_cols\ntrain_num = train_num[num_cols]\ntrain_num = train_num.fillna(train_num.mean())\ntrain_num['target'] = train['isFraud']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's for the sake of simplicity and time we randomly select the 10% of the data and run a PCA on that, after that, we will take 3 PCA components to plot."},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\n\ndata1 = train_num.sample(frac= 0.1 , random_state=10)\ndata1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1.target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check if we preserve the target ratio in our sample data or not"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('target ratio in the sample data is' , round(2060/len(data1)*100,2) , 'which seems okay')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = data1['target']\ndel data1['target'], data1['TransactionDT'], data1['TransactionID']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Let's try PCA on this dataset ##\n\nfrom sklearn.preprocessing import StandardScaler\ndata_pca = StandardScaler().fit_transform(data1)\n\n#data_pca = pd.DataFrame(data_pca)\n#data_pca.head()\n#data_pca.describe()\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=3)\ncomps = pca.fit_transform(data_pca)\n\nfinal_pca_data = pd.DataFrame(data = comps , columns=['pc1' , 'pc2' , 'pc3'])\n\nfinal_pca_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now a 3D plot using these PCA data. I learned this 3D plotting from [this kernel](https://www.kaggle.com/chechir/molecular-eda-3d-plots)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly\nimport plotly.graph_objs as go\nfrom plotly.graph_objs import FigureWidget\n\ntraces = go.Scatter3d(\n    x=final_pca_data['pc1'],\n    y=final_pca_data['pc2'],\n    z=final_pca_data['pc3'],\n    mode='markers',\n    marker=dict(\n        size=4,\n        opacity=0.2,\n        color=target,\n        colorscale='Viridis',\n     )\n)\n\nlayout = go.Layout(\n    autosize=True,\n    showlegend=True,\n    width=800,\n    height=1000,\n)\n\nFigureWidget(data=[traces], layout=layout)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above plot it seems that the yellow points (which are target = 1) are quite mixed up with the purple (target = 0) ones, which may indicate not to do oversampling of target = 1s blindly. So, we may need to be careful incase we try oversampling. A better perspective is possible if we plot t-sne components, which I have commented below. However, please note that the results are based on a very small subset so results colud be very much inconclusive. "},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n## same with T-SNE ##\n\nfrom sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=3 , random_state=0)\ndata_tsne = tsne.fit_transform(data1)\n\ndata_tsne\n\ndata_tsne = pd.DataFrame(data_tsne , columns=['tsne1' , 'tsne2' , 'tsne3'])\ndata_tsne.head()\n\n## 3D plot with TSNE components ##\n\ntraces = go.Scatter3d(\n    x=data_tsne['tsne1'],\n    y=data_tsne['tsne2'],\n    z=data_tsne['tsne3'],\n    mode='markers',\n    marker=dict(\n        size=4,\n        opacity=0.2,\n        color=target,\n        colorscale='Viridis',\n     )\n)\n\nlayout = go.Layout(\n    autosize=True,\n    showlegend=True,\n    width=800,\n    height=1000,\n)\n\nFigureWidget(data=[traces], layout=layout)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}