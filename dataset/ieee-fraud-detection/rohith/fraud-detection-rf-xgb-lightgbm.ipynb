{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\n\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport multiprocessing\nimport gc\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix,roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\nimport lightgbm as lgb\nimport xgboost as xgb\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\nsample_submission = pd.read_csv(\"../input/ieee-fraud-detection/sample_submission.csv\")\ntest_identity = pd.read_csv(\"../input/ieee-fraud-detection/test_identity.csv\")\ntest_transaction = pd.read_csv(\"../input/ieee-fraud-detection/test_transaction.csv\")\ntrain_identity = pd.read_csv(\"../input/ieee-fraud-detection/train_identity.csv\")\ntrain_transaction = pd.read_csv(\"../input/ieee-fraud-detection/train_transaction.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of data points in train data of identity data :\", train_identity.shape)\nprint('-'*50)\n\nprint(\"Number of data points in train data of transaction data:\", train_transaction.shape)\nprint('-'*50)\n\nprint(\"Number of data points in test data of transaction data:\", test_transaction.shape)\nprint('-'*50)\n\nprint(\"Number of data points in test data of identity data:\", test_identity.shape)\nprint('-'*50)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The attributes of identity data :\", train_identity.columns.values)\nprint('-'*100)\nprint(\"The attributes of transaction data :\", train_transaction.columns.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Combining both transaction and identity data\ntrain_data = pd.merge(train_transaction,train_identity, on='TransactionID', how='left',left_index=True,right_index=True)\ntest_data = pd.merge(test_transaction,test_identity, on='TransactionID', how='left',left_index=True,right_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of data points in total train data :\", train_data.shape)\nprint(\"Number of data points in total test data :\", test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(1,10):\n    i = str(i)\n    test_data.rename(columns={'id-0'+i: 'id_0'+i}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(10,39):\n    i = str(i)\n    test_data.rename(columns={'id-'+i: 'id_'+i}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fill in inf values with some constant\ntrain_data = train_data.replace([np.inf,-np.inf],999)\ntest_data = test_data.replace([np.inf,-np.inf],999)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## REducing memory\ntrain_data = reduce_mem_usage(train_data)\ntest_data = reduce_mem_usage(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_identity,test_identity,train_transaction,test_transaction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def addNewFeatures(data): \n    data['uid'] = data['card1'].astype(str)+'_'+data['card2'].astype(str)\n\n    data['uid2'] = data['uid'].astype(str)+'_'+data['card3'].astype(str)+'_'+data['card5'].astype(str)\n\n    data['uid3'] = data['uid2'].astype(str)+'_'+data['addr1'].astype(str)+'_'+data['addr2'].astype(str)\n\n    data['D9'] = np.where(data['D9'].isna(),0,1)\n    \n    return data\n\ntrain_data = addNewFeatures(train_data)\ntest_data = addNewFeatures(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i_cols = ['card1','card2','card3','card5','uid','uid2','uid3']\n\nfor col in i_cols:\n    for agg_type in ['mean','std']:\n        new_col_name = col+'_TransactionAmt_'+agg_type\n        temp_df = pd.concat([train_data[[col, 'TransactionAmt']], test_data[[col,'TransactionAmt']]])\n        #temp_df['TransactionAmt'] = temp_df['TransactionAmt'].astype(int)\n        temp_df = temp_df.groupby([col])['TransactionAmt'].agg([agg_type]).reset_index().rename(\n                                                columns={agg_type: new_col_name})\n\n        temp_df.index = list(temp_df[col])\n        temp_df = temp_df[new_col_name].to_dict()   \n\n        train_data[new_col_name] = train_data[col].map(temp_df)\n        test_data[new_col_name]  = test_data[col].map(temp_df)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['card1_count_full'] = train_data['card1'].map(pd.concat([train_data['card1'], test_data['card1']], ignore_index=True).value_counts(dropna=False))\ntest_data['card1_count_full'] = test_data['card1'].map(pd.concat([train_data['card1'], test_data['card1']], ignore_index=True).value_counts(dropna=False))\n\ntrain_data['card2_count_full'] = train_data['card2'].map(pd.concat([train_data['card2'], test_data['card2']], ignore_index=True).value_counts(dropna=False))\ntest_data['card2_count_full'] = test_data['card2'].map(pd.concat([train_data['card2'], test_data['card2']], ignore_index=True).value_counts(dropna=False))\n\ntrain_data['card3_count_full'] = train_data['card3'].map(pd.concat([train_data['card3'], test_data['card3']], ignore_index=True).value_counts(dropna=False))\ntest_data['card3_count_full'] = test_data['card3'].map(pd.concat([train_data['card3'], test_data['card3']], ignore_index=True).value_counts(dropna=False))\n\ntrain_data['card4_count_full'] = train_data['card4'].map(pd.concat([train_data['card4'], test_data['card4']], ignore_index=True).value_counts(dropna=False))\ntest_data['card4_count_full'] = test_data['card4'].map(pd.concat([train_data['card4'], test_data['card4']], ignore_index=True).value_counts(dropna=False))\n\ntrain_data['card5_count_full'] = train_data['card5'].map(pd.concat([train_data['card5'], test_data['card5']], ignore_index=True).value_counts(dropna=False))\ntest_data['card5_count_full'] = test_data['card5'].map(pd.concat([train_data['card5'], test_data['card5']], ignore_index=True).value_counts(dropna=False))\n\ntrain_data['card6_count_full'] = train_data['card6'].map(pd.concat([train_data['card6'], test_data['card6']], ignore_index=True).value_counts(dropna=False))\ntest_data['card6_count_full'] = test_data['card6'].map(pd.concat([train_data['card6'], test_data['card6']], ignore_index=True).value_counts(dropna=False))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['addr1_count_full'] = train_data['addr1'].map(pd.concat([train_data['addr1'], test_data['addr1']], ignore_index=True).value_counts(dropna=False))\ntest_data['addr1_count_full'] = test_data['addr1'].map(pd.concat([train_data['addr1'], test_data['addr1']], ignore_index=True).value_counts(dropna=False))\n\ntrain_data['addr2_count_full'] = train_data['addr2'].map(pd.concat([train_data['addr2'], test_data['addr2']], ignore_index=True).value_counts(dropna=False))\ntest_data['addr2_count_full'] = test_data['addr2'].map(pd.concat([train_data['addr2'], test_data['addr2']], ignore_index=True).value_counts(dropna=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime\nSTART_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')\n\ndef setTime(df):\n    df['TransactionDT'] = df['TransactionDT'].fillna(df['TransactionDT'].median())\n    # Temporary\n    df['DT'] = df['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n    df['DT_M'] = (df['DT'].dt.year-2017)*12 + df['DT'].dt.month\n    df['DT_W'] = (df['DT'].dt.year-2017)*52 + df['DT'].dt.weekofyear\n    df['DT_D'] = (df['DT'].dt.year-2017)*365 + df['DT'].dt.dayofyear\n    \n    df['DT_hour'] = df['DT'].dt.hour\n    df['DT_day_week'] = df['DT'].dt.dayofweek\n    df['DT_day'] = df['DT'].dt.day\n    \n    return df\n    \ntrain_data=setTime(train_data)\ntest_data=setTime(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = train_data.drop('DT',axis=1)\ntest_data = test_data.drop('DT',axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/c/ieee-fraud-detection/discussion/100499\nemails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', 'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft',\n          'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo', 'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft',\n          'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink', 'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other',\n          'gmx.de': 'other', 'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', 'protonmail.com': 'other', 'hotmail.fr': 'microsoft',\n          'windstream.net': 'other', 'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo', 'servicios-ta.com': 'other',\n          'netzero.net': 'other', 'suddenlink.net': 'other', 'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft', 'verizon.net': 'yahoo',\n          'msn.com': 'microsoft', 'q.com': 'centurylink', 'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', 'rocketmail.com': 'yahoo',\n          'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', 'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', 'bellsouth.net': 'other',\n          'embarqmail.com': 'centurylink', 'cableone.net': 'other', 'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other',\n          'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other', 'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\nus_emails = ['gmail', 'net', 'edu']\n\nfor c in ['P_emaildomain', 'R_emaildomain']:\n    train_data[c + '_bin'] = train_data[c].map(emails)\n    test_data[c + '_bin'] = test_data[c].map(emails)\n\n    \n    train_data[c + '_suffix'] = train_data[c].map(lambda x: str(x).split('.')[-1])\n    test_data[c + '_suffix'] = test_data[c].map(lambda x: str(x).split('.')[-1])\n\n    \n    train_data[c + '_suffix'] = train_data[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n    test_data[c + '_suffix'] = test_data[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p = 'P_emaildomain'\nr = 'R_emaildomain'\nuknown = 'email_not_provided'\n\ndef setDomain(df):\n    df[p] = df[p].fillna(uknown)\n    df[r] = df[r].fillna(uknown)\n    \n    # Check if P_emaildomain matches R_emaildomain\n    df['email_check'] = np.where((df[p]==df[r])&(df[p]!=uknown),1,0)\n\n    df[p+'_prefix'] = df[p].apply(lambda x: x.split('.')[0])\n    df[r+'_prefix'] = df[r].apply(lambda x: x.split('.')[0])\n    \n    return df\n    \ntrain_data=setDomain(train_data)\ntest_data=setDomain(test_data)\n#x_cv= setDomain(x_cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['P_isproton']=(train_data['P_emaildomain']=='protonmail.com')\ntrain_data['R_isproton']=(train_data['R_emaildomain']=='protonmail.com')\ntest_data['P_isproton']=(test_data['P_emaildomain']=='protonmail.com')\ntest_data['R_isproton']=(test_data['R_emaildomain']=='protonmail.com')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[\"lastest_browser\"] = np.zeros(train_data.shape[0])\ntest_data[\"lastest_browser\"] = np.zeros(test_data.shape[0])\n#x_cv[\"lastest_browser\"] = np.zeros(x_cv.shape[0])\n\ndef setBrowser(df):\n    df.loc[df[\"id_31\"]==\"samsung browser 7.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"opera 53.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"mobile safari 10.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"google search application 49.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"firefox 60.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"edge 17.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 69.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 67.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 63.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 63.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0 for ios\",'lastest_browser']=1\n    return df\n\ntrain_data=setBrowser(train_data)\ntest_data=setBrowser(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def id_split(dataframe):\n  dataframe['DeviceInfo'] = dataframe['DeviceInfo'].fillna('unknown_device').str.lower()\n  dataframe['device_name'] = dataframe['DeviceInfo'].str.split('/', expand=True)[0]\n\n  dataframe.loc[dataframe['device_name'].str.contains('SM', na=False), 'device_name'] = 'Samsung'\n  dataframe.loc[dataframe['device_name'].str.contains('SAMSUNG', na=False), 'device_name'] = 'Samsung'\n  dataframe.loc[dataframe['device_name'].str.contains('GT-', na=False), 'device_name'] = 'Samsung'\n  dataframe.loc[dataframe['device_name'].str.contains('Moto G', na=False), 'device_name'] = 'Motorola'\n  dataframe.loc[dataframe['device_name'].str.contains('Moto', na=False), 'device_name'] = 'Motorola'\n  dataframe.loc[dataframe['device_name'].str.contains('moto', na=False), 'device_name'] = 'Motorola'\n  dataframe.loc[dataframe['device_name'].str.contains('LG-', na=False), 'device_name'] = 'LG'\n  dataframe.loc[dataframe['device_name'].str.contains('rv:', na=False), 'device_name'] = 'RV'\n  dataframe.loc[dataframe['device_name'].str.contains('HUAWEI', na=False), 'device_name'] = 'Huawei'\n  dataframe.loc[dataframe['device_name'].str.contains('ALE-', na=False), 'device_name'] = 'Huawei'\n  dataframe.loc[dataframe['device_name'].str.contains('-L', na=False), 'device_name'] = 'Huawei'\n  dataframe.loc[dataframe['device_name'].str.contains('Blade', na=False), 'device_name'] = 'ZTE'\n  dataframe.loc[dataframe['device_name'].str.contains('BLADE', na=False), 'device_name'] = 'ZTE'\n  dataframe.loc[dataframe['device_name'].str.contains('Linux', na=False), 'device_name'] = 'Linux'\n  dataframe.loc[dataframe['device_name'].str.contains('XT', na=False), 'device_name'] = 'Sony'\n  dataframe.loc[dataframe['device_name'].str.contains('HTC', na=False), 'device_name'] = 'HTC'\n  dataframe.loc[dataframe['device_name'].str.contains('ASUS', na=False), 'device_name'] = 'Asus'\n\n  dataframe.loc[dataframe.device_name.isin(dataframe.device_name.value_counts()[dataframe.device_name.value_counts() < 200].index), 'device_name'] = \"Others\"\n  dataframe['had_id'] = 1\n  gc.collect()\n  return dataframe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = id_split(train_data)\ntest_data = id_split(test_data)\n#x_cv = id_split(x_cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# New feature - log of transaction amount.\n\ntrain_data['TransactionAmt_Log'] = np.log(train_data['TransactionAmt'])\ntest_data['TransactionAmt_Log'] = np.log(test_data['TransactionAmt'])\n#x_cv['TransactionAmt_Log'] = np.log(x_cv['TransactionAmt'])\n\n# New feature - decimal part of the transaction amount.\n\ntrain_data['TransactionAmt_decimal'] = ((train_data['TransactionAmt'] - train_data['TransactionAmt'].astype(int)) * 1000).astype(int)\ntest_data['TransactionAmt_decimal'] = ((test_data['TransactionAmt'] - test_data['TransactionAmt'].astype(int)) * 1000).astype(int)\n#x_cv['TransactionAmt_decimal'] = ((x_cv['TransactionAmt'] - x_cv['TransactionAmt'].astype(int)) * 1000).astype(int)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['nulls'] = train_data.isna().sum(axis=1)\ntest_data['nulls'] = test_data.isna().sum(axis=1)\n#x_cv['nulls'] = x_cv.isna().sum(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['id_02_to_mean_card1'] = train_data['id_02'] / train_data.groupby(['card1'])['id_02'].transform('mean')\ntrain_data['id_02_to_mean_card4'] = train_data['id_02'] / train_data.groupby(['card4'])['id_02'].transform('mean')\ntrain_data['id_02_to_std_card1'] = train_data['id_02'] / train_data.groupby(['card1'])['id_02'].transform('std')\ntrain_data['id_02_to_std_card4'] = train_data['id_02'] / train_data.groupby(['card4'])['id_02'].transform('std')\n\ntest_data['id_02_to_mean_card1'] = test_data['id_02'] / test_data.groupby(['card1'])['id_02'].transform('mean')\ntest_data['id_02_to_mean_card4'] = test_data['id_02'] / test_data.groupby(['card4'])['id_02'].transform('mean')\ntest_data['id_02_to_std_card1'] = test_data['id_02'] / test_data.groupby(['card1'])['id_02'].transform('std')\ntest_data['id_02_to_std_card4'] = test_data['id_02'] / test_data.groupby(['card4'])['id_02'].transform('std')\n\ntrain_data['D15_to_mean_card1'] = train_data['D15'] / train_data.groupby(['card1'])['D15'].transform('mean')\ntrain_data['D15_to_mean_card4'] = train_data['D15'] / train_data.groupby(['card4'])['D15'].transform('mean')\ntrain_data['D15_to_std_card1'] = train_data['D15'] / train_data.groupby(['card1'])['D15'].transform('std')\ntrain_data['D15_to_std_card4'] = train_data['D15'] / train_data.groupby(['card4'])['D15'].transform('std')\n\ntest_data['D15_to_mean_card1'] = test_data['D15'] / test_data.groupby(['card1'])['D15'].transform('mean')\ntest_data['D15_to_mean_card4'] = test_data['D15'] / test_data.groupby(['card4'])['D15'].transform('mean')\ntest_data['D15_to_std_card1'] = test_data['D15'] / test_data.groupby(['card1'])['D15'].transform('std')\ntest_data['D15_to_std_card4'] = test_data['D15'] / test_data.groupby(['card4'])['D15'].transform('std')\n\ntrain_data['D15_to_mean_addr1'] = train_data['D15'] / train_data.groupby(['addr1'])['D15'].transform('mean')\ntrain_data['D15_to_mean_card4'] = train_data['D15'] / train_data.groupby(['card4'])['D15'].transform('mean')\ntrain_data['D15_to_std_addr1'] = train_data['D15'] / train_data.groupby(['addr1'])['D15'].transform('std')\ntrain_data['D15_to_std_card4'] = train_data['D15'] / train_data.groupby(['card4'])['D15'].transform('std')\n\ntest_data['D15_to_mean_addr1'] = test_data['D15'] / test_data.groupby(['addr1'])['D15'].transform('mean')\ntest_data['D15_to_mean_card4'] = test_data['D15'] / test_data.groupby(['card4'])['D15'].transform('mean')\ntest_data['D15_to_std_addr1'] = test_data['D15'] / test_data.groupby(['addr1'])['D15'].transform('std')\ntest_data['D15_to_std_card4'] = test_data['D15'] / test_data.groupby(['card4'])['D15'].transform('std')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_too_many_null_attr(data):\n    many_null_cols = [col for col in data.columns if data[col].isnull().sum() / data.shape[0] > 0.9]\n    return many_null_cols\n\ndef get_too_many_repeated_val(data):\n    big_top_value_cols = [col for col in data.columns if data[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\n    return big_top_value_cols\n\ndef get_useless_columns(data):\n    too_many_null = get_too_many_null_attr(data)\n    print(\"More than 90% null: \" + str(len(too_many_null)))\n    too_many_repeated = get_too_many_repeated_val(data)\n    print(\"More than 90% repeated value: \" + str(len(too_many_repeated)))\n    cols_to_drop = list(set(too_many_null + too_many_repeated))\n    \n    return cols_to_drop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_drop = get_useless_columns(train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_drop.remove('isFraud')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = train_data.drop(cols_to_drop, axis=1)\ntest_data = test_data.drop(cols_to_drop, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assigning the target variable.\nY = train_data['isFraud'].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop target and transactionID and transactionDT\n\ntrain_data = train_data.drop('isFraud', axis=1)\n#x_cv = x_cv.drop('isFraud', axis=1)\n\ntrain_data = train_data.drop('TransactionID', axis=1)\ntest_data = test_data.drop('TransactionID', axis=1)\n#x_cv = x_cv.drop('TransactionID', axis=1)\n\ntrain_data = train_data.drop('TransactionDT', axis=1)\ntest_data = test_data.drop('TransactionDT', axis=1)\n#x_cv = x_cv.drop('TransactionDT', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data.shape)\nprint(test_data.shape)\nprint(Y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for c in train_data.columns:\n    if train_data[c].dtype=='float16' or  train_data[c].dtype=='float32' or  train_data[c].dtype=='float64':\n        train_data[c].fillna(train_data[c].mean())\n        test_data[c].fillna(test_data[c].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Label Encoding\nfor f in train_data.columns:\n    if train_data[f].dtype=='object' or test_data[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(train_data[f].values))\n        train_data[f] = lbl.transform(list(train_data[f].values))\n        test_data[f] = test_data[f].map(lambda s: '<unknown>' if s not in lbl.classes_ else s)\n        lbl.classes_ = np.append(lbl.classes_, '<unknown>')\n        test_data[f] = lbl.transform(list(test_data[f].values))\n\nprint('Labelling done.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"raw","source":"train_data = train_data.fillna(-1)\ntest_data = test_data.fillna(-1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyperparameter grid\nparam_grid = {\n    'boosting_type': ['gbdt'],\n    'num_leaves': [300,400,500,600,700],\n    'learning_rate': list(np.linspace(0.005, 0.05,num=10)),\n    'reg_alpha': list(np.linspace(0.3, 0.7,num=10)),\n    'reg_lambda': list(np.linspace(0.5, 0.9,num=10)),\n    'colsample_bytree': list(np.linspace(0.6, 1, num=10)),\n    'metric':'auc',\n    'scale_pos_weight': [30],\n    'max_bin': [255]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#modelling\nclf = lgb.LGBMClassifier(objective='binary',random_state=42,n_jobs=-1,max_depth=-1)\ngrid = RandomizedSearchCV(clf,param_grid,verbose=1,cv=3,n_iter=10)\ngrid.fit(train_data,Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_preds = grid.predict_proba(test_data)[:,1] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['isFraud']=y_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.to_csv('Lightgbm.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}