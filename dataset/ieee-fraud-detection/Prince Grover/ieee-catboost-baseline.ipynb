{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom scipy import interp\nimport gc\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve, precision_recall_curve, auc\nfrom catboost import CatBoostClassifier, Pool, cv\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nprint(os.listdir(\".\"))\n\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note** - The score is low from kernel submission because only 50% of training data was used for each fold. I did an median ensemble of 4 such models with random 50% of data. The score will improve if you use full dataset with shuffled k-fold (>0.935). I had to do that because my kernel was dying when I was trying to create *Dataset Pool* for Catboost. The creation of Pool eats up a lot of RAM. Another option for me was to reduce memory of dataframe. But I didn't try that. \n\n- Catboost with last 5% as validation set\n- Only null treatment required in pre-processing\n- Categorical variables handled by passing as index\n\nOnly 50% of data has been used in each fold. The score is better if use stratified fold so that it covers all the data. \n\nReduce data memory size by using this kernel - https://www.kaggle.com/mjbahmani/reducing-memory-size-for-ieee/output\nFound this kernel to be useful for experiments and took some code from here - https://www.kaggle.com/vincentlugat/ieee-lgb-bayesian-opt    [](http://)"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"train_trans = pd.read_csv('../input/train_transaction.csv')\ntest_trans = pd.read_csv('../input/test_transaction.csv')\ntrain_iden = pd.read_csv('../input/train_identity.csv') \ntest_iden = pd.read_csv('../input/test_identity.csv')\n\ntrain_trans.shape, test_trans.shape, train_iden.shape, test_iden.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = train_trans.merge(train_iden, on=\"TransactionID\", how=\"left\")\ndf_test = test_trans.merge(test_iden, on=\"TransactionID\", how=\"left\")\ndel train_iden, train_trans, test_iden, test_trans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_train = pd.read_csv('train_reduced.csv')\n# df_test = pd.read_csv('test_reduced.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# filling NAs\ndf_train.fillna(-999, inplace=True)\ndf_test.fillna(-999, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_cols = [\"TransactionID\", \"TransactionDT\"]\nlabel_col = \"isFraud\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = df_train.loc[:,label_col]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.drop([label_col]+drop_cols, axis=1, inplace=True)\ndf_test.drop([\"TransactionDT\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = list(df_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Defining categorical features. Defining Card, Email, Device, Product, M, id12-id38, addr as categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols = [\"card{}\".format(i) for i in range(1,7)] +\\\n            ['ProductCD', 'P_emaildomain', 'R_emaildomain',  'DeviceType', 'DeviceInfo'] +\\\n            [\"M{}\".format(i) for i in range(1,10)] +\\\n            [\"id_{}\".format(i) for i in range(12, 39)] +\\\n            [\"addr{}\".format(i) for i in range(1,3)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_idx = [df_train.columns.get_loc(c) for c in cat_cols]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We create random subsamples from the training data. The reason is that running Pool function exceeds the kernel memory limit and kernel dies. So we create 5 models with 66% bootstrap samples and average predictions from these 5 models for test set. "},{"metadata":{"trusted":true},"cell_type":"code","source":"nfold = 5\n# skf = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=42)\n# tscv = TimeSeriesSplit(n_splits=5)\npredictions_df = pd.DataFrame()\n\nseeds = [1,12,123,21,423]\n\nmean_fpr = np.linspace(0,1,100)\nroc_aucs = []\ntprs = []\ncms= []\naucs = []\npredictions = np.zeros(len(df_test))\nfeature_importance_df = pd.DataFrame()\n\ni = 1\n# for train_idx, valid_idx in skf.split(df_train_con, df_train[label_col].values):\nfor i in range(nfold):\n    np.random.seed(seeds[i])  # to reporoduce\n    size_95_per = int(0.95*df_train.shape[0])\n    train_idx = np.random.choice(range(size_95_per),\n                                 int(0.5*size_95_per), replace=False)  # 70% sampling random\n    valid_idx = range(size_95_per, df_train.shape[0]) # last 5% of data\n    print(\"\\nfold {}\".format(i))\n    print(\"train pool\")\n    trn_data = Pool(df_train.iloc[train_idx].values,\n                     y_train.iloc[train_idx].values,\n                     cat_features=cat_idx)\n    gc.collect()\n    print(\"valid pool\")\n    val_data = Pool(df_train.iloc[valid_idx].values,\n                    y_train.iloc[valid_idx].values,\n                    cat_features=cat_idx) \n    gc.collect()\n    clf = CatBoostClassifier(iterations=1500,\n                           random_state=10,\n                           learning_rate=0.08,\n                           task_type = \"GPU\",\n                           eval_metric= 'AUC', \n                           scale_pos_weight = sum(y_train.iloc[train_idx]==0)/sum(y_train.iloc[train_idx]==1)/5.,\n#                            one_hot_max_size = 4,\n#                            has_time=True,\n#                            min_data_in_leaf=5,\n                           early_stopping_rounds = 50,\n                          )\n    print(\"model\")\n    clf.fit(trn_data,\n          use_best_model=True,\n          eval_set=val_data,\n          verbose=False,\n          plot=True)\n    gc.collect()\n    print(\"predict valid\")\n    oof = clf.predict_proba(df_train.iloc[valid_idx].values)[:,1] \n    print(\"predict test\")\n    this_fold_preds = clf.predict_proba(df_test.drop(\"TransactionID\", axis=1))[:,1]\n    predictions += this_fold_preds/np.float(nfold)\n    predictions_df[str(i)] = this_fold_preds\n    \n    # Scores \n    roc_aucs.append(roc_auc_score(y_train.iloc[valid_idx].values, oof))\n   \n    # Roc curve by fold\n    fpr, tpr, t = roc_curve(y_train.iloc[valid_idx].values, oof)\n    tprs.append(interp(mean_fpr, fpr, tpr))\n    roc_auc = auc(fpr, tpr)\n    aucs.append(roc_auc)\n    plt.plot(fpr, tpr, lw=2, alpha=0.3, label='ROC fold %d (AUC = %0.4f)' % (i,roc_auc))\n    # don't need them now\n    del trn_data, val_data\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del df_train\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"h=plt.hist(predictions)  # mean of folds ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Submitting median of predictions. You can also submit meanm"},{"metadata":{"trusted":true},"cell_type":"code","source":"h = plt.hist(predictions_df.median(1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Test submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_submission = pd.DataFrame()\ntest_submission['TransactionID'] = df_test.iloc[0:506691,0]\ntest_submission['isFraud'] = predictions_df.median(1)\ntest_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_submission.to_csv('submission_catboost_baseline.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## End"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}