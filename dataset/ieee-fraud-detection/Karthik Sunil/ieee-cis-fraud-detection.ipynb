{"cells":[{"metadata":{},"cell_type":"markdown","source":"# IEEE-CIS Fraud Detection \nIn this dataset contains about 590K records of online transactions. Purpose is to detect if the transaction is fraudulent or not. This would be the binary classification problem\n\nData: https://www.kaggle.com/c/ieee-fraud-detection\n\nReference Kernel: https://www.kaggle.com/plasticgrammer/ieee-cis-fraud-detection-playground/data\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom sklearn.feature_selection import SelectKBest, chi2,f_classif\nfrom sklearn.preprocessing import MinMaxScaler\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We use the following code to reduce the memory use while importing the file data into Pandas DF"},{"metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\n\n\ndef import_data(file):\n    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n    df = pd.read_csv(file, parse_dates=True, keep_date_col=True)\n    df = reduce_mem_usage(df)\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us see what files are there in the kernel data"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"! ls ../input/ieee-fraud-detection/","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Importing the data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#!ls ../input/ieee-fraud-detection\ndf_transacation = import_data('../input/ieee-fraud-detection/train_transaction.csv')\ndf_identity = import_data('../input/ieee-fraud-detection/train_identity.csv')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are 2 training datasets, one is related to transaction and other is related to identity. Shape of those DFs are given below"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(df_transacation.shape, df_identity.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can learn that we don't have identity data for all the transactions. Let us try to  understand more about it"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_transacation.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are 393 features and 1 target vaiable which is ```isFraud```"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_identity.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And there are 40 features in identity dataset"},{"metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_identity.set_index(['TransactionID'], inplace=True)\ndef get_id(x):\n#     return df_identity[df_identity['TransactionID'] == x].shape[0]\n    try :\n        df_identity.loc[x].shape\n        return 1\n    except :\n        return 0\n    \n# get_id(2987004)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_transacation['id_exists'] = df_transacation['TransactionID'].apply(get_id)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"'Identity records exists in {:.2f}% of transactions'.format(df_transacation[df_transacation['id_exists'] == 1].shape[0] / df_transacation.shape[0]*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# High level Data Analysis\nLet us try to understand how many fradulent transactions are there"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print('Fradulent transactions are {:.2f}%'.format(df_transacation[df_transacation['isFraud'] == 1].shape[0] / df_transacation.shape[0]*100))\nprint('with {} Fradulent transactions'.format(df_transacation[df_transacation['isFraud'] == 1].shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We understand that the dataset is highly imbalanced. "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"\n# gives some infos on columns types and number of null values\ntab_info=pd.DataFrame(df_transacation.dtypes).T.rename(index={0:'column type'})\ntab_info=tab_info.append(pd.DataFrame(df_transacation.isnull().sum()).T.rename(index={0:'null values (nb)'}))\ntab_info=tab_info.append(pd.DataFrame(df_transacation.isnull().sum()/df_transacation.shape[0]*100)\n                         .T.rename(index={0:'null values (%)'}))\ntab_info = tab_info.transpose()\ntab_info","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see many of the features have more than 86% null value. We can remove them. Let see how many such clumns are there with more than 30% null values."},{"metadata":{"trusted":true},"cell_type":"code","source":"'There are {} columns which have  more than {}% null values'.format(tab_info[tab_info['null values (%)'] > 30].shape[0], 30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us remove them from our dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_remove = tab_info[tab_info['null values (%)'] > 30].index.values\ncols_to_remove\ndf_transacation = df_transacation.drop(cols_to_remove,axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we need to handle the null values\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df  = df_transacation.dropna()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature encoding\n\nLet us first find out categorical features to be transformed"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df['isFraud']\nX = df.drop(['isFraud'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_feature_mask = X.dtypes=='category'\ncategorical_cols = X.columns[categorical_feature_mask].tolist()\ncategorical_cols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above are the categorical features which will be considered for one hot encoding."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_cat = X[categorical_cols]\nX_cat = pd.get_dummies(X_cat)\nX = X.drop(categorical_cols,axis=1)\nX = pd.concat([X,X_cat], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()\nX_cols = X.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature analysis\n\n## Correlation Analysis"},{"metadata":{},"cell_type":"markdown","source":"Before performing any feature analysis, let us transform the features into values between 0 and 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = MinMaxScaler()\nX = scaler.fit_transform(X)\nX = pd.DataFrame(X)\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bestfeatures = SelectKBest(chi2, k=20)\nfit = bestfeatures.fit(X,y)\n# dfcolumns = pd.DataFrame(X_new.columns)\n# dfcolumns\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X_cols)\nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nfeatureScores = featureScores.sort_values(by='Score', ascending=False)\nfeatureScores.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bestfeatures = SelectKBest(f_classif, k=20)\nfit = bestfeatures.fit(X,y)\n# dfcolumns = pd.DataFrame(X_new.columns)\n# dfcolumns\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X_cols)\nfeatureScores1 = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores1.columns = ['Specs','Score']  #naming the dataframe columns\nfeatureScores1 = featureScores1.sort_values(by='Score', ascending=False)\nfeatureScores1.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Common features in both chi2 and ANOVA\n```\n155\tV281\t6528.623034\n182\tV308\t5564.151940\n154\tV280\t5471.553775\n153\tV279\t5185.097715\n169\tV295\t4804.288722\n167\tV293\t4173.462409\n191\tV317\t3029.784155\n```\n\nThese features will be considered for further model building"},{"metadata":{},"cell_type":"markdown","source":"# Model Building\nAs this is the data is credit card Fraud, this is the typical example where, Recall metric should be used. F Score with High beta is used for checking the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df['isFraud']\nX = df.drop(['isFraud'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us take only features which are having impact on target "},{"metadata":{"trusted":true},"cell_type":"code","source":"X1 = X[['V281', 'V308','V280','V279','V295','V293','V317']]\n\ny = pd.DataFrame(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X1, y['isFraud'], test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nd_train = lgb.Dataset(X_train, label=y_train)\n# d_test = lgbm.Dataset(X_test, y_test)\nparams = {}\nparams['learning_rate'] = 0.02\nparams['boosting_type'] = 'gbdt'\n# params['boosting_type'] = 'dart'\nparams['objective'] = 'binary'\nparams['metric'] = 'binary'\nparams['sub_feature'] = 0.99\nparams['num_leaves'] = 500\nparams['min_data'] = 100\nparams['max_depth'] = 10000\nparams['is_unbalance'] = True\n# y_train=y_train.ravel()\nclf = lgb.train(params, d_train, 1000)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score, accuracy_score, fbeta_score\nresults=clf.predict(X_test)\nscore1 = fbeta_score(y_test,results.round(), beta=100 )\nprint('F1 Score ',score1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost\nLet us use XGBoost tree based algorithm to see if it performs any better"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost\nxgb = xgboost.XGBClassifier(n_estimators=800, learning_rate=0.02, gamma=0, subsample=0.2,\n                           colsample_bytree=1, max_depth=100)\nxgb.fit(X_train,y_train)\nresults=xgb.predict(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nscore1 = fbeta_score(y_test,results.round(), beta=100 )\nprint('F Score - Recall:',score1)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}