{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this notebook, I will try to build some Autoencoder models for anomaly detection. \nMy autoencoder models are mainly based on this article \nhttps://towardsdatascience.com/build-the-right-autoencoder-tune-and-optimize-using-pca-principles-part-ii-24b9cca69bd6\n\nAlso, every training data is pre-processed. (I refered following notebook and pick up top-150 features)\nhttps://www.kaggle.com/artgor/eda-and-models\n\nI built 5 models and the last one is a conbination of others."},{"metadata":{"id":"OfN7lohOyWO9","colab_type":"code","outputId":"f1191038-36d8-4399-f0d7-52bfd0be7d2c","executionInfo":{"status":"ok","timestamp":1566171620283,"user_tz":-540,"elapsed":18220,"user":{"displayName":"hideaki takahashi","photoUrl":"","userId":"16154026581542772178"}},"colab":{"base_uri":"https://localhost:8080/","height":52},"trusted":true},"cell_type":"code","source":"#from google.colab import drive\n#drive.mount('/content/gdrive')\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom pylab import rcParams\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom keras.models import Model, load_model\nfrom keras.layers import Input, Dense\n#from keras.callbacks import ModelCheckpoint, EarlyStoping\nfrom keras import regularizers\n\nfolder_pathin = '../input/fraud-detection-processed-dara/'\nX_train = pd.read_csv(f'{folder_pathin}X_ae_150_train.csv')\nX_valid = pd.read_csv(f'{folder_pathin}X_ae_150_test.csv')\ny_valid = pd.read_csv(f'{folder_pathin}y_ae_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"id":"Cri_wd3Ayp_l","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(X_train)\nscaler.transform(X_train)\nX_train = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns)\nX_valid = pd.DataFrame(scaler.transform(X_valid), columns=X_valid.columns)","execution_count":null,"outputs":[]},{"metadata":{"id":"CF0B6pOeyzPB","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"train_data, valid_data = train_test_split(X_train, test_size=0.2, random_state=42)\n\ninput_dim = train_data.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"id":"yV7AA58ayzRk","colab_type":"code","outputId":"bd3d7cde-a3a1-4fe1-9fdf-5c68a32f6efb","executionInfo":{"status":"ok","timestamp":1566171624984,"user_tz":-540,"elapsed":22837,"user":{"displayName":"hideaki takahashi","photoUrl":"","userId":"16154026581542772178"}},"colab":{"base_uri":"https://localhost:8080/","height":34},"trusted":true},"cell_type":"code","source":"train_data.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"m-SbVHPFzU7e","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"nb_epoch = 200\nbatch_size = 2048\n\n#encoding_dim = int(input_dim/2)\nencoding_dim = input_dim\nlearning_rate = 1e-3","execution_count":null,"outputs":[]},{"metadata":{"id":"iKTUedWJy4mx","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"from keras.layers import Dense, BatchNormalization, Activation\nimport tensorflow as tf\nfrom keras.models import Model, load_model\nfrom keras.layers import Input, Dense, Layer, InputSpec\nfrom keras.callbacks import ModelCheckpoint, TensorBoard\nfrom keras import regularizers, activations, initializers, constraints, Sequential\nfrom keras import backend as K\nfrom keras.constraints import UnitNorm, Constraint","execution_count":null,"outputs":[]},{"metadata":{"id":"SV9oq-1czqaL","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint, EarlyStopping\n\nDO_TRANING = True\n \n#autoencoder.compile(optimizer='adam', \n#                    loss='mean_squared_error', \n#                    metrics=['accuracy'])\n \ncheckpointer = ModelCheckpoint(filepath=\"model.h5\",\n                               verbose=0,\n                               save_best_only=True)\n\nearlystoping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto')","execution_count":null,"outputs":[]},{"metadata":{"id":"54_XgXcJzwWV","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def train_AE(autoencoder):\n  if DO_TRANING:\n     history = autoencoder.fit(train_data, train_data,\n                               epochs=nb_epoch,\n                               batch_size=batch_size,\n                               shuffle=True,\n                               validation_data=(valid_data, valid_data),\n                               verbose=1,\n                               callbacks=[checkpointer, earlystoping]).history\n\n     # Model loss\n     plt.plot(history['loss'])\n     plt.plot(history['val_loss'])\n     plt.title('model loss')\n     plt.ylabel('acc')\n     plt.xlabel('epoch')\n     plt.legend(['train', 'val'], loc='upper right');\n     plt.show()\n     \n  return(autoencoder)","execution_count":null,"outputs":[]},{"metadata":{"id":"i7Lh51j0zLlX","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,\n                             roc_curve, recall_score, classification_report, f1_score,\n                             precision_recall_fscore_support)\n\ndef result_visualization(autoencoder):\n  predictions = autoencoder.predict(X_valid)\n\n  # X_test データと再現データの mse(平均２乗誤差)を計算する\n  mse = np.mean(np.power(X_valid - predictions, 2), axis=1)\n\n  error_df = pd.DataFrame(mse).join(y_valid)\n  error_df = error_df.rename(columns = {0:\"reconstruction_error\", \"isfraut\":\"true_class\"})\n  #error_df = error_df.rename(columns = {\"reconstruction_error\":0, \"true_class\":\"isfraut\"})\n  error_df = error_df.rename(columns = {\"isfraud\":\"true_class\"})\n  \n  print(\"true_class = 1.0\")\n  print(error_df[error_df.true_class == 1.0].describe())\n  print(\" \")\n  print(\"true_class = 0.0\")\n  print(error_df[error_df.true_class == 0.0].describe())\n  print(\" \")\n  \n  fpr, tpr, thresholds = roc_curve(error_df.true_class, error_df.reconstruction_error)\n  roc_auc = auc(fpr, tpr)\n\n  plt.title('ROC curve')\n  plt.plot(fpr, tpr, label='AUC = %0.4f'% roc_auc)\n  plt.legend(loc='lower right')\n  plt.plot([0,1],[0,1],'r--')\n  plt.xlim([-0.001, 1])\n  plt.ylim([0, 1.001])\n  plt.ylabel('True Positive Rate')\n  plt.xlabel('False Positive Rate')\n  plt.show();\n\n\n  # get data(precision, recall, th)\n  precision, recall, th = precision_recall_curve(error_df.true_class, error_df.reconstruction_error)\n\n  # Precision & Recall  vs  Reconstruction error\n  plt.plot(th, precision[1:], 'b', label='Precision curve', color='r')\n  plt.plot(th, recall[1:], 'b', label='Recall curve', color='b')\n  plt.title('Precision and Recall  vs  mse')\n  plt.xlabel('mse')\n  plt.ylabel('Precision and Recall')\n  #plt.xlim(0, 250)\n  plt.xlim([0, 1.0]) #I'm sorry that I should have put this.\n  plt.legend(loc=1)\n  plt.show()\n  \n  print(\" \")\n  # Reconstruction error for different classes\n  thresholds = [0.1,0.15, 0.2,0.25, 0.3]\n  \n  for threshold in thresholds:\n      \n    groups = error_df.groupby('true_class')\n    fig, ax = plt.subplots()\n\n    for name, group in groups:\n        ax.plot(group.index, group.reconstruction_error, marker='o', ms=0.1, linestyle='',\n                label= \"Fraud\" if name == 1 else \"Normal\")\n    ax.hlines(threshold, ax.get_xlim()[0], ax.get_xlim()[1], colors=\"g\", zorder=100, label='Threshold')\n    ax.legend()\n    ax.set_ylim(0, 1.0)  \n    #ax2.set_ylim(0, .12)  \n    plt.title(\"mse for different classes\")\n    plt.ylabel(\"mse\")\n    plt.xlabel(\"Data point index\")\n    plt.show();\n\n    # Confusion matrix\n    y_pred = [1 if e > threshold else 0 for e in error_df.reconstruction_error.values]\n    conf_matrix = confusion_matrix(error_df.true_class, y_pred)\n\n    plt.figure(figsize=(5, 5))\n    LABELS = [\"Normal\", \"Fraud\"]\n    sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\", square = True);\n    plt.title(\"Confusion matrix\")\n    plt.ylabel('True class')\n    plt.xlabel('Predicted class')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"L3G9lQfCy4pn","colab_type":"code","outputId":"0548c0db-8b89-4372-ef98-4d790c891e02","executionInfo":{"status":"ok","timestamp":1566171953874,"user_tz":-540,"elapsed":351690,"user":{"displayName":"hideaki takahashi","photoUrl":"","userId":"16154026581542772178"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"trusted":true},"cell_type":"code","source":"# baseline_model\n\nencoder = Dense(encoding_dim, activation=\"relu\", input_shape=(input_dim,), use_bias = True) \ndecoder = Dense(input_dim, activation=\"relu\", use_bias = True)\n\nautoencoder_0 = Sequential()\nautoencoder_0.add(encoder)\nautoencoder_0.add(decoder)\n\nautoencoder_0.compile(metrics=['accuracy'],\n                    loss='mean_squared_error',\n                    optimizer='adam')\nautoencoder_0.summary()\n\nautoencoder_trained_0 = train_AE(autoencoder_0)\n\nresult_visualization(autoencoder_trained_0)","execution_count":null,"outputs":[]},{"metadata":{"id":"nWjYOfg0y4sO","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"#Autoencoder Optimization\nclass DenseTied(Layer):\n    def __init__(self, units,\n                 activation=None,\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 bias_constraint=None,\n                 tied_to=None,\n                 **kwargs):\n        self.tied_to = tied_to\n        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n        super().__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.use_bias = use_bias\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n        self.input_spec = InputSpec(min_ndim=2)\n        self.supports_masking = True\n                \n    def build(self, input_shape):\n        assert len(input_shape) >= 2\n        input_dim = input_shape[-1]\n\n        if self.tied_to is not None:\n            self.kernel = K.transpose(self.tied_to.kernel)\n            self._non_trainable_weights.append(self.kernel)\n        else:\n            self.kernel = self.add_weight(shape=(input_dim, self.units),\n                                          initializer=self.kernel_initializer,\n                                          name='kernel',\n                                          regularizer=self.kernel_regularizer,\n                                          constraint=self.kernel_constraint)\n        if self.use_bias:\n            self.bias = self.add_weight(shape=(self.units,),\n                                        initializer=self.bias_initializer,\n                                        name='bias',\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n        self.input_spec = InputSpec(min_ndim=2, axes={-1: input_dim})\n        self.built = True\n\n    def compute_output_shape(self, input_shape):\n        assert input_shape and len(input_shape) >= 2\n        output_shape = list(input_shape)\n        output_shape[-1] = self.units\n        return tuple(output_shape)\n\n    def call(self, inputs):\n        output = K.dot(inputs, self.kernel)\n        if self.use_bias:\n            output = K.bias_add(output, self.bias, data_format='channels_last')\n        if self.activation is not None:\n            output = self.activation(output)\n        return output","execution_count":null,"outputs":[]},{"metadata":{"id":"nX3HsVa90rmf","colab_type":"code","outputId":"dc51fc3d-d3e0-4a2e-cc9a-3d44b0fdda3c","executionInfo":{"status":"ok","timestamp":1566172449721,"user_tz":-540,"elapsed":847516,"user":{"displayName":"hideaki takahashi","photoUrl":"","userId":"16154026581542772178"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"trusted":true},"cell_type":"code","source":"encoder = Dense(encoding_dim, activation=\"relu\", input_shape=(input_dim,), use_bias = True) \ndecoder = DenseTied(input_dim, activation=\"relu\", tied_to=encoder, use_bias = True)\nautoencoder_1 = Sequential()\nautoencoder_1.add(encoder)\nautoencoder_1.add(decoder)\nautoencoder_1.compile(metrics=['accuracy'],\n                    loss='mean_squared_error',\n                    optimizer='adam')\nautoencoder_1.summary()\n\nautoencoder_trained_1 = train_AE(autoencoder_1)\n\nresult_visualization(autoencoder_trained_1)","execution_count":null,"outputs":[]},{"metadata":{"id":"ANBbfh6U1Gmo","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"class WeightsOrthogonalityConstraint (Constraint):\n    def __init__(self, encoding_dim, weightage = 1.0, axis = 0):\n        self.encoding_dim = encoding_dim\n        self.weightage = weightage\n        self.axis = axis\n        \n    def weights_orthogonality(self, w):\n        if(self.axis==1):\n            w = K.transpose(w)\n        if(self.encoding_dim > 1):\n            m = K.dot(K.transpose(w), w) - K.eye(self.encoding_dim)\n            return self.weightage * K.sqrt(K.sum(K.square(m)))\n        else:\n            m = K.sum(w ** 2) - 1.\n            return m\n\n    def __call__(self, w):\n        return self.weights_orthogonality(w)","execution_count":null,"outputs":[]},{"metadata":{"id":"qHFBQAs11Gss","colab_type":"code","outputId":"3baa4d9c-eed6-4300-f8f4-dd1ceccf1806","executionInfo":{"status":"ok","timestamp":1566172731343,"user_tz":-540,"elapsed":1129122,"user":{"displayName":"hideaki takahashi","photoUrl":"","userId":"16154026581542772178"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"trusted":true},"cell_type":"code","source":"encoder = Dense(encoding_dim, activation=\"relu\", input_shape=(input_dim,), use_bias=True, kernel_regularizer=WeightsOrthogonalityConstraint(encoding_dim, weightage=1., axis=0)) \ndecoder = Dense(input_dim, activation=\"relu\", use_bias = True, kernel_regularizer=WeightsOrthogonalityConstraint(encoding_dim, weightage=1., axis=1))\n\nautoencoder_2 = Sequential()\nautoencoder_2.add(encoder)\nautoencoder_2.add(decoder)\n\nautoencoder_2.compile(metrics=['accuracy'],\n                    loss='mean_squared_error',\n                    optimizer='adam')\nautoencoder_2.summary()\n\nautoencoder_trained_2 = train_AE(autoencoder_2)\n\nresult_visualization(autoencoder_trained_2)","execution_count":null,"outputs":[]},{"metadata":{"id":"PvZg-BhV1Gzt","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"class UncorrelatedFeaturesConstraint (Constraint):\n    \n    def __init__(self, encoding_dim, weightage = 1.0):\n        self.encoding_dim = encoding_dim\n        self.weightage = weightage\n    \n    def get_covariance(self, x):\n        x_centered_list = []\n\n        for i in range(self.encoding_dim):\n            x_centered_list.append(x[:, i] - K.mean(x[:, i]))\n        \n        x_centered = tf.stack(x_centered_list)\n        covariance = K.dot(x_centered, K.transpose(x_centered)) / tf.cast(x_centered.get_shape()[0], tf.float32)\n        \n        return covariance\n            \n    # Constraint penalty\n    def uncorrelated_feature(self, x):\n        if(self.encoding_dim <= 1):\n            return 0.0\n        else:\n            output = K.sum(K.square(self.covariance - K.dot(self.covariance, K.eye(self.encoding_dim))))\n            return output\n\n    def __call__(self, x):\n        self.covariance = self.get_covariance(x)\n        return self.weightage * self.uncorrelated_feature(x)","execution_count":null,"outputs":[]},{"metadata":{"id":"ybH-tXuw1j1v","colab_type":"code","outputId":"132bd7db-f811-4ea0-8e64-a13e67f5c04c","executionInfo":{"status":"ok","timestamp":1566173235862,"user_tz":-540,"elapsed":1633625,"user":{"displayName":"hideaki takahashi","photoUrl":"","userId":"16154026581542772178"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"trusted":true},"cell_type":"code","source":"encoder = Dense(encoding_dim, activation=\"relu\", input_shape=(input_dim,), use_bias = True, activity_regularizer=UncorrelatedFeaturesConstraint(encoding_dim, weightage = 1.)) \ndecoder = Dense(input_dim, activation=\"relu\", use_bias = True)\n\nautoencoder_3 = Sequential()\nautoencoder_3.add(encoder)\nautoencoder_3.add(decoder)\n\nautoencoder_3.compile(metrics=['accuracy'],\n                    loss='mean_squared_error',\n                    optimizer='adam')\nautoencoder_3.summary()\n\nautoencoder_trained_3 = train_AE(autoencoder_3)\n\nresult_visualization(autoencoder_trained_3)","execution_count":null,"outputs":[]},{"metadata":{"id":"UkW_97052PRf","colab_type":"code","outputId":"d28f624e-40b2-472f-9da4-d805841b2c7b","executionInfo":{"status":"ok","timestamp":1566173468121,"user_tz":-540,"elapsed":1865873,"user":{"displayName":"hideaki takahashi","photoUrl":"","userId":"16154026581542772178"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"trusted":true},"cell_type":"code","source":"encoder = Dense(encoding_dim, activation=\"relu\", input_shape=(input_dim,), use_bias = True, kernel_constraint=UnitNorm(axis=0)) \ndecoder = Dense(input_dim, activation=\"relu\", use_bias = True, kernel_constraint=UnitNorm(axis=1))\nautoencoder_4 = Sequential()\nautoencoder_4.add(encoder)\nautoencoder_4.add(decoder)\nautoencoder_4.compile(metrics=['accuracy'],\n                    loss='mean_squared_error',\n                    optimizer='adam')\nautoencoder_4.summary()\n\nautoencoder_trained_4 = train_AE(autoencoder_4)\n\nresult_visualization(autoencoder_trained_4)","execution_count":null,"outputs":[]},{"metadata":{"id":"G4Uo8jnw2XId","colab_type":"code","outputId":"964e2295-acdc-4959-e4f3-9b87ecc0dd3d","executionInfo":{"status":"ok","timestamp":1566173740798,"user_tz":-540,"elapsed":2138542,"user":{"displayName":"hideaki takahashi","photoUrl":"","userId":"16154026581542772178"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"trusted":true},"cell_type":"code","source":"encoder = Dense(encoding_dim, activation=\"relu\", input_shape=(input_dim,), use_bias = True, kernel_regularizer=WeightsOrthogonalityConstraint(encoding_dim, weightage=1., axis=0), kernel_constraint=UnitNorm(axis=0)) \ndecoder = DenseTied(input_dim, activation=\"relu\", tied_to=encoder, use_bias = False)\nautoencoder_5 = Sequential()\nautoencoder_5.add(encoder)\nautoencoder_5.add(decoder)\nautoencoder_5.compile(metrics=['accuracy'],\n                    loss='mean_squared_error',\n                      optimizer='adam')\nautoencoder_5.summary()\n\nautoencoder_trained_5 = train_AE(autoencoder_5)\n\nresult_visualization(autoencoder_trained_5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, I feel that it is really difficult to set proper threshold.\nThe Auc and ROC carve are not so bad, but I can't take advantage of them. So, plz give me advice! "},{"metadata":{"id":"KxXt-PyIapOY","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"autoencoder_trained_0.save(\"model_0.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"autoencoder_trained_4.save(\"model_4.h5\")","execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"AE_proper_revised150.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":1}