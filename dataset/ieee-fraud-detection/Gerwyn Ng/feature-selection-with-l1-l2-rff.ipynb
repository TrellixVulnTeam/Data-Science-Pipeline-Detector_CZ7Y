{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1 align=\"center\"> Feature Selection via Regularised Random Forest </h1> <br>\n\nThis notebook describe a way of feature selection using regularised trees. \n\n> The key idea is to penalize selecting a new feature by its information gain in the splitting process"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport lightgbm as lgbm\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nimport gc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def reduce_mem_usage(df):\n    for col in df.columns:\n        if df[col].dtype=='float64': df[col] = df[col].astype('float32')\n        if df[col].dtype=='int64': df[col] = df[col].astype('int32')\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# import data\ntrain_transaction = reduce_mem_usage(pd.read_csv('../input/ieee-fraud-detection/train_transaction.csv', index_col='TransactionID'))\ntrain_identity = reduce_mem_usage(pd.read_csv('../input/ieee-fraud-detection/train_identity.csv', index_col='TransactionID'))\n\n# merge\ntrain_df = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\n\n# Prepare data\nX = train_df.drop(['TransactionDT','isFraud'],axis=1)\ny = train_df['isFraud']\n\nfor col in X.columns:\n    if X[col].dtype == 'object':\n        le = LabelEncoder()\n        X[col] = le.fit_transform(list(X[col].astype(str).values))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 align=\"center\"> L1 Regularisation </h1> <br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nlgbm_params = {'num_leaves':165,\n               'min_child_weight': 10,\n               'min_child_samples': 10,\n               'min_split_gain': 0,\n               'subsample': .632,\n               'subsample_freq':1,\n               'objective': 'binary',\n               'max_depth': -1,\n               'learning_rate': 0.1, \n               \"boosting_type\": \"rf\",\n               \"bagging_seed\": 11,\n               \"metric\": 'auc',\n               'random_state': 47,\n               'num_rounds': 400,\n               'reg_alpha':10 # Tweak Me\n              }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### train/test/split\ntrain_idx = int(len(X)*0.6)\nx_trn = X[:train_idx]\ny_trn = y[:train_idx]\n\n### hold out valid\nidx = int(0.8*len(X))\n\ny_ho = y[idx:]\nx_ho = X[idx:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CV\nNFOLDS = 5\nfolds = KFold(n_splits=NFOLDS, shuffle=True, random_state=47)\n\ncolumns = x_trn.columns\nsplits = folds.split(x_trn, y_trn)\ny_preds = np.zeros(x_ho.shape[0])\n\nl1_feature_importances = pd.DataFrame()\nl1_feature_importances['feature'] = columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"for fold_n, (train_index, valid_index) in enumerate(splits):\n    X_train, X_valid = x_trn[columns].iloc[train_index], x_trn[columns].iloc[valid_index]\n    y_train, y_valid = y_trn.iloc[train_index], y_trn.iloc[valid_index]\n    \n    dtrain = lgbm.Dataset(X_train, label=y_train)\n    dvalid = lgbm.Dataset(x_ho, label=y_ho)\n\n    clf = lgbm.train(lgbm_params, dtrain, valid_sets = [dtrain, dvalid], \n                    verbose_eval=200)\n    \n    l1_feature_importances[f'fold_{fold_n + 1}'] = clf.feature_importance()\n    \n    y_pred_valid = clf.predict(x_ho)\n\n    del X_train, X_valid, y_train, y_valid\n    gc.collect()\n    \ndel clf, x_trn, y_trn, x_ho, y_ho","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l1_feature_importances['average'] = l1_feature_importances[[f'fold_{fold_n + 1}' for fold_n in range(folds.n_splits)]].mean(axis=1)\nl1_feature_importances.to_csv('l1_feature_importances.csv')\n\nplt.figure(figsize=(16, 16))\nsns.barplot(data=l1_feature_importances.sort_values(by='average', ascending=False).head(30), x='average', y='feature');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('L1 Zero Importance Features:')\nprint(l1_feature_importances[l1_feature_importances['average'] == 0]['feature'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 align=\"center\"> L2 Regularisation </h1> <br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_params = {'num_leaves':165,\n               'min_child_weight': 10,\n               'min_child_samples': 10,\n               'min_split_gain': 0,\n               'subsample': .632,\n               'subsample_freq':1,\n               'objective': 'binary',\n               'max_depth': -1,\n               'learning_rate': 0.1, \n               \"boosting_type\": \"rf\",\n               \"bagging_seed\": 11,\n               \"metric\": 'auc',\n               'random_state': 47,\n               'num_rounds': 400,\n               'reg_lambda':5 # Tweak Me\n              }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":true},"cell_type":"code","source":"### train/test/split\ntrain_idx = int(len(X)*0.6)\nx_trn = X[:train_idx]\ny_trn = y[:train_idx]\n\n### hold out valid\nidx = int(0.8*len(X))\n\ny_ho = y[idx:]\nx_ho = X[idx:]\n\n# CV\nNFOLDS = 5\nfolds = KFold(n_splits=NFOLDS, shuffle=True, random_state=47)\n\ncolumns = x_trn.columns\nsplits = folds.split(x_trn, y_trn)\ny_preds = np.zeros(x_ho.shape[0])\n\nl2_feature_importances = pd.DataFrame()\nl2_feature_importances['feature'] = columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":true},"cell_type":"code","source":"for fold_n, (train_index, valid_index) in enumerate(splits):\n    X_train, X_valid = x_trn[columns].iloc[train_index], x_trn[columns].iloc[valid_index]\n    y_train, y_valid = y_trn.iloc[train_index], y_trn.iloc[valid_index]\n    \n    dtrain = lgbm.Dataset(X_train, label=y_train)\n    dvalid = lgbm.Dataset(x_ho, label=y_ho)\n\n    clf = lgbm.train(lgbm_params, dtrain, valid_sets = [dtrain, dvalid], \n                    verbose_eval=200)\n    \n    l2_feature_importances[f'fold_{fold_n + 1}'] = clf.feature_importance()\n    \n    y_pred_valid = clf.predict(x_ho)\n\n    del X_train, X_valid, y_train, y_valid\n    gc.collect()\n    \ndel clf, x_trn, y_trn, x_ho, y_ho","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l2_feature_importances['average'] = l2_feature_importances[[f'fold_{fold_n + 1}' for fold_n in range(folds.n_splits)]].mean(axis=1)\nl2_feature_importances.to_csv('l2_feature_importances.csv')\n\nplt.figure(figsize=(16, 16))\nsns.barplot(data=l2_feature_importances.sort_values(by='average', ascending=False).head(30), x='average', y='feature');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('L2 Zero Importance Features:')\nprint(l2_feature_importances[l2_feature_importances['average'] == 0]['feature'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Reference\n# https://www.kaggle.com/ogrellier/lgbm-regularized-random-forest","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}