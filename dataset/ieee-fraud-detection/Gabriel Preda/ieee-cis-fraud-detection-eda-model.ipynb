{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1>IEEE-CIS Fraud Detection EDA & Model</h1>\n\n**Note**: this kernel uses for the feature engineering and model the following kernel: https://www.kaggle.com/plasticgrammer/ieee-cis-fraud-detection-eda"},{"metadata":{},"cell_type":"markdown","source":"# Load packages"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nfrom sklearn import metrics, preprocessing\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.decomposition import PCA\nimport lightgbm as lgb\n\npd.set_option('display.max_columns', 400)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_identity_df = pd.read_csv(os.path.join('../input/ieee-fraud-detection/', 'train_identity.csv'))\ntrain_transaction_df = pd.read_csv(os.path.join('../input/ieee-fraud-detection/', 'train_transaction.csv'))\ntest_identity_df = pd.read_csv(os.path.join('../input/ieee-fraud-detection/', 'test_identity.csv'))\ntest_transaction_df = pd.read_csv(os.path.join('../input/ieee-fraud-detection/', 'test_transaction.csv'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"train identity: {train_identity_df.shape}\")\nprint(f\"train transaction: {train_transaction_df.shape}\")\nprint(f\"test identity: {test_identity_df.shape}\")\nprint(f\"test transaction: {test_transaction_df.shape}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Glimpse the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_identity_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_identity_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_transaction_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Missing data and data types"},{"metadata":{"trusted":true},"cell_type":"code","source":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(np.transpose(tt))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_data(train_identity_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_data(test_identity_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_data(train_transaction_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_data(test_transaction_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data values distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame()\nfor column in list(train_identity_df.columns.values):\n    field_type = train_identity_df[column].dtype\n    df=df.append(pd.DataFrame({'column':column,'train':train_identity_df[column].nunique(), 'test':test_identity_df[column].nunique(),\\\n                               'type':field_type},index=[0]))\ndf['delta'] = df.train - df.test\ndf['flag'] = (df['delta'] < 0).astype(int)\ntest_dom_categories = df.loc[(df.flag == 1) & (df.type == 'object'), 'column']\ndf = df.transpose()\n\nprint('Unique column values in identity datasets')\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Columns of type `object` and with more categories in test than in train: {list(test_dom_categories)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame()\nfor column in list(test_transaction_df.columns.values):\n    field_type = test_transaction_df[column].dtype\n    try:\n        df=df.append(pd.DataFrame({'column':column,'train':train_transaction_df[column].nunique(), \\\n            'test':test_transaction_df[column].nunique(), 'type':field_type},index=[0]))\n    except:\n        \"Error trying to add target from test\"\ndf['delta'] = df.train - df.test\ndf['flag'] = (df['delta'] < 0).astype(int)\ntest_dom_categories = df.loc[(df.flag == 1) & (df.type == 'object'), 'column']\ndf = df.transpose()\n\nprint('Unique column values in transaction datasets')\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Columns of type `object` and with more categories in test than in train: {list(test_dom_categories)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Categorical fields distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_count(feature, title, df, size=1):\n    f, ax = plt.subplots(1,1, figsize=(4*size,4))\n    total = float(len(df))\n    g = sns.countplot(df[feature], order = df[feature].value_counts().index[:30], palette='Set3')\n    g.set_title(\"Number and percentage of {}\".format(title))\n    if(size > 2):\n        plt.xticks(rotation=90, size=8)\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.2f}%'.format(100*height/total),\n                ha=\"center\") \n    plt.show()   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_count('id_30', 'train: id_30', df=train_identity_df, size=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_count('id_30', 'test: id_30', df=test_identity_df, size=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_count('id_31', 'train: id_31', df=train_identity_df, size=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_count('id_31', 'test: id_31', df=test_identity_df, size=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_count('id_33', 'train: id_33', df=train_identity_df, size=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_count('id_33', 'test: id_33', df=test_identity_df, size=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_count('DeviceInfo', 'train: DeviceInfo', df=train_identity_df, size=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_count('DeviceInfo', 'test: DeviceInfo', df=test_identity_df, size=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_count('P_emaildomain', 'train: P_emaildomain', df=train_transaction_df, size=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_count('P_emaildomain', 'test: P_emaildomain', df=test_transaction_df, size=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.\n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        #else:\n            #df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB --> {:.2f} MB (Decreased by {:.1f}%)'.format(\n        start_mem, end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"identity_cols = list(train_identity_df.columns.values)\ntransaction_cols = list(train_transaction_df.drop('isFraud', axis=1).columns.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = pd.merge(train_transaction_df[transaction_cols + ['isFraud']], train_identity_df[identity_cols], how='left')\nX_train = reduce_mem_usage(X_train)\nX_test = pd.merge(test_transaction_df[transaction_cols], train_identity_df[identity_cols], how='left')\nX_test = reduce_mem_usage(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_id = X_train.pop('TransactionID')\nX_test_id = X_test.pop('TransactionID')\ndel train_identity_df,train_transaction_df, test_identity_df, test_transaction_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = X_train.append(X_test, sort=False).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vcols = [f'V{i}' for i in range(1,340)]\n\nsc = preprocessing.MinMaxScaler()\n\npca = PCA(n_components=2) #0.99\nvcol_pca = pca.fit_transform(sc.fit_transform(all_data[vcols].fillna(-1)))\n\nall_data['_vcol_pca0'] = vcol_pca[:,0]\nall_data['_vcol_pca1'] = vcol_pca[:,1]\nall_data['_vcol_nulls'] = all_data[vcols].isnull().sum(axis=1)\n\nall_data.drop(vcols, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data['_P_emaildomain__addr1'] = all_data['P_emaildomain'] + '__' + all_data['addr1'].astype(str)\nall_data['_card1__card2'] = all_data['card1'].astype(str) + '__' + all_data['card2'].astype(str)\nall_data['_card1__addr1'] = all_data['card1'].astype(str) + '__' + all_data['addr1'].astype(str)\nall_data['_card2__addr1'] = all_data['card2'].astype(str) + '__' + all_data['addr1'].astype(str)\nall_data['_card12__addr1'] = all_data['_card1__card2'] + '__' + all_data['addr1'].astype(str)\nall_data['_card_all__addr1'] = all_data['_card1__card2'] + '__' + all_data['addr1'].astype(str)\nall_data['_amount_decimal'] = ((all_data['TransactionAmt'] - all_data['TransactionAmt'].astype(int)) * 1000).astype(int)\nall_data['_amount_decimal_len'] = all_data['TransactionAmt'].apply(lambda x: len(re.sub('0+$', '', str(x)).split('.')[1]))\nall_data['_amount_fraction'] = all_data['TransactionAmt'].apply(lambda x: float('0.'+re.sub('^[0-9]|\\.|0+$', '', str(x))))\ncols = ['ProductCD','card1','card2','card5','card6','P_emaildomain','_card_all__addr1']\n\nfor f in cols:\n    all_data[f'_amount_mean_{f}'] = all_data['TransactionAmt'] / all_data.groupby([f])['TransactionAmt'].transform('mean')\n    all_data[f'_amount_std_{f}'] = all_data['TransactionAmt'] / all_data.groupby([f])['TransactionAmt'].transform('std')\n    all_data[f'_amount_pct_{f}'] = (all_data['TransactionAmt'] - all_data[f'_amount_mean_{f}']) / all_data[f'_amount_std_{f}']\n\nfor f in cols:\n    vc = all_data[f].value_counts(dropna=False)\n    all_data[f'_count_{f}'] = all_data[f].map(vc)\n    \ncat_cols = [f'id_{i}' for i in range(12,39)]\nfor i in cat_cols:\n    if i in all_data.columns:\n        all_data[i] = all_data[i].astype(str)\n        all_data[i].fillna('unknown', inplace=True)\n\nenc_cols = []\nfor i, t in all_data.loc[:, all_data.columns != 'isFraud'].dtypes.iteritems():\n    if t == object:\n        enc_cols.append(i)\n        all_data[i] = pd.factorize(all_data[i])[0]  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = all_data[all_data['isFraud'].notnull()]\nX_test = all_data[all_data['isFraud'].isnull()].drop('isFraud', axis=1)\nY_train = X_train.pop('isFraud')\ndel all_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params={'learning_rate': 0.0097,\n        'objective': 'binary',\n        'metric': 'auc',\n        'num_threads': -1,\n        'num_leaves': 256,\n        'verbose': 1,\n        'random_state': 314,\n        'bagging_fraction': 1,\n        'feature_fraction': 0.82\n       }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_preds = np.zeros(X_train.shape[0])\nsub_preds = np.zeros(X_test.shape[0])\n\nclf = lgb.LGBMClassifier(**params, n_estimators=4000)\nclf.fit(X_train, Y_train)\noof_preds = clf.predict_proba(X_train, num_iteration=clf.best_iteration_)[:,1]\nsub_preds = clf.predict_proba(X_test, num_iteration=clf.best_iteration_)[:,1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['TransactionID'] = X_test_id\nsubmission['isFraud'] = sub_preds\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}