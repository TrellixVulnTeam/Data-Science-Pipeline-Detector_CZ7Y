{"cells":[{"metadata":{"papermill":{"duration":0.051334,"end_time":"2020-10-05T19:12:04.469794","exception":false,"start_time":"2020-10-05T19:12:04.41846","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## About the Project"},{"metadata":{"papermill":{"duration":0.044068,"end_time":"2020-10-05T19:12:04.559106","exception":false,"start_time":"2020-10-05T19:12:04.515038","status":"completed"},"tags":[]},"cell_type":"markdown","source":"* Vesta Corporation provided the dataset for this competition. Vesta Corporation is the forerunner in guaranteed e-commerce payment solutions. Founded in 1995.\n \n* In this competition, the aim is to benchmark machine learning models on a challenging large-scale dataset. \n* The data comes from Vesta's real-world e-commerce transactions and contains a wide range of features from device type to product features. \n* The machine learning model will alert the fraudulent transaction for millions of people around the world, helping hundreds of thousands of businesses reduce their fraud loss and increase their revenue. \n* The training dataset consists of more than 400 features and 5.9 Million samples. This is supervised binary classification problem and goal is to predict if a credit card transaction is Fraud based on input features mentioned below\n\n**Evaluation**\n* The model is evaluated on AUC ROC score. The notebook will produce an output csv file with TransactionID and predicted probabilties on test set,  which will be automatically evaluted by Kaggle.\n\n### Transaction Table \n* TransactionDT: timedelta from a given reference datetime (not an actual timestamp)\n* TransactionAMT: transaction payment amount in USD\n* ProductCD: product code, the product for each transaction\n* card1 - card6: payment card information, such as card type, card category, issue bank, country, etc.\n* addr: address\n* dist: distance\n* P_ and (R__) emaildomain: purchaser and recipient email domain\n* C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.\n* D1-D15: timedelta, such as days between previous transaction, etc.\n* M1-M9: match, such as names on card and address, etc.\n* Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.\n\n<br>  **Categorical Features:**\n* ProductCD\n* card1 - card6\n* addr1, addr2\n* P_emaildomain\n* R_emaildomain\n* M1 - M9\n\n### Identity Table \n* Variables in this table are identity information – network connection information (IP, ISP, Proxy, etc) and digital signature (UA/browser/os/version, etc) associated with transactions.\n* They're collected by Vesta’s fraud protection system and digital security partners.\n* (The field names are masked and pairwise dictionary will not be provided for privacy protection and contract agreement)\n\n<br> **Categorical Features:**\n* DeviceType\n* DeviceInfo\n* id_12 - id_38\n\n"},{"metadata":{"papermill":{"duration":0.044264,"end_time":"2020-10-05T19:12:04.648196","exception":false,"start_time":"2020-10-05T19:12:04.603932","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Import Libraries"},{"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2020-10-05T19:12:04.744507Z","iopub.status.busy":"2020-10-05T19:12:04.743569Z","iopub.status.idle":"2020-10-05T19:12:16.95798Z","shell.execute_reply":"2020-10-05T19:12:16.959639Z"},"papermill":{"duration":12.267069,"end_time":"2020-10-05T19:12:16.959927","exception":false,"start_time":"2020-10-05T19:12:04.692858","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"#Install latest version of the package as  the defualt version is not working fine\n!pip install seaborn==0.11.0","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2020-10-05T19:12:17.081173Z","iopub.status.busy":"2020-10-05T19:12:17.079944Z","iopub.status.idle":"2020-10-05T19:12:18.033973Z","shell.execute_reply":"2020-10-05T19:12:18.034683Z"},"papermill":{"duration":1.022778,"end_time":"2020-10-05T19:12:18.034863","exception":false,"start_time":"2020-10-05T19:12:17.012085","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os, gc\nprint(os.listdir(\"../input\"))\n\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n\nimport matplotlib.pyplot as plt\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 100)\n\n#setting for plot fonts \nSMALL_SIZE = 14\nMEDIUM_SIZE = 16\nBIGGER_SIZE = 18\n\nplt.rc('font', size=SMALL_SIZE)          # controls default text sizes\nplt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\nplt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\nplt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\nplt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\nplt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\nplt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"papermill":{"duration":0.049946,"end_time":"2020-10-05T19:12:18.137197","exception":false,"start_time":"2020-10-05T19:12:18.087251","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Constants"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-05T19:12:18.247596Z","iopub.status.busy":"2020-10-05T19:12:18.246469Z","iopub.status.idle":"2020-10-05T19:12:18.250038Z","shell.execute_reply":"2020-10-05T19:12:18.249415Z"},"papermill":{"duration":0.059896,"end_time":"2020-10-05T19:12:18.250173","exception":false,"start_time":"2020-10-05T19:12:18.190277","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"RANDOM_STATE = 42\nDEBUG_MODE = False  # Load fewer samples to save time for quick testing\nTARGET = 'isFraud'\n","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.051098,"end_time":"2020-10-05T19:12:18.352201","exception":false,"start_time":"2020-10-05T19:12:18.301103","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Read Data\nhttps://www.kaggle.com/c/ieee-fraud-detection/discussion/101203#latest-586800\n\n"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-05T19:12:18.466019Z","iopub.status.busy":"2020-10-05T19:12:18.465183Z","iopub.status.idle":"2020-10-05T19:13:21.474171Z","shell.execute_reply":"2020-10-05T19:13:21.474925Z"},"papermill":{"duration":63.072004,"end_time":"2020-10-05T19:13:21.475125","exception":false,"start_time":"2020-10-05T19:12:18.403121","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"%%time\n\n# Load fewer samples to save time for quick testing\nif DEBUG_MODE:\n    nrows = 50000\nelse:\n    nrows = None\n        \ndata_path = '/kaggle/input/ieee-fraud-detection/'\ntrain_identity = pd.read_csv(os.path.join(data_path, 'train_identity.csv'))\ntrain_transaction = pd.read_csv(os.path.join(data_path, 'train_transaction.csv'), nrows = nrows)\ntest_identity = pd.read_csv(os.path.join(data_path, 'test_identity.csv'))\ntest_transaction =pd.read_csv(os.path.join(data_path, 'test_transaction.csv'), nrows = nrows)\nprint('Train Identity Data - rows:', train_identity.shape[0], \n      'columns:', train_identity.shape[1])\nprint('Train Transaction Data - rows:', train_transaction.shape[0], \n      'columns:', train_transaction.shape[1])\nprint('Test Identity Data - rows:', test_identity.shape[0], \n      'columns:', test_identity.shape[1])\nprint('Test Transaction Data - rows:', test_transaction.shape[0], \n      'columns:', test_transaction.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.05123,"end_time":"2020-10-05T19:13:21.578022","exception":false,"start_time":"2020-10-05T19:13:21.526792","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Transaction Data\n\n"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-05T19:13:21.734942Z","iopub.status.busy":"2020-10-05T19:13:21.725795Z","iopub.status.idle":"2020-10-05T19:13:21.81861Z","shell.execute_reply":"2020-10-05T19:13:21.817866Z"},"papermill":{"duration":0.186331,"end_time":"2020-10-05T19:13:21.81877","exception":false,"start_time":"2020-10-05T19:13:21.632439","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train_transaction.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2020-10-05T19:13:21.940295Z","iopub.status.busy":"2020-10-05T19:13:21.939152Z","iopub.status.idle":"2020-10-05T19:13:21.943491Z","shell.execute_reply":"2020-10-05T19:13:21.942858Z"},"papermill":{"duration":0.07122,"end_time":"2020-10-05T19:13:21.943655","exception":false,"start_time":"2020-10-05T19:13:21.872435","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"\ndef column_properties(df):\n    columns_prop = pd.DataFrame()\n    columns_prop['column'] = df.columns.tolist()\n    columns_prop['count_non_null'] = df.count().values\n    columns_prop['count_null'] = df.isnull().sum().values\n    columns_prop['perc_null'] = columns_prop['count_null'] * 100 / df.shape[0]\n\n    #using df.nunique() is memory intensive and slow resulting in kernal death\n    unique_list = []\n    for col in df.columns.tolist():\n        unique_list.append(df[col].value_counts().shape[0])\n    columns_prop['count_unique'] =  unique_list\n    \n    columns_prop['dtype'] = df.dtypes.values\n    columns_prop.set_index('column', inplace = True)\n    return columns_prop\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-05T19:13:22.075728Z","iopub.status.busy":"2020-10-05T19:13:22.074813Z","iopub.status.idle":"2020-10-05T19:13:30.362796Z","shell.execute_reply":"2020-10-05T19:13:30.362069Z"},"papermill":{"duration":8.359528,"end_time":"2020-10-05T19:13:30.362961","exception":false,"start_time":"2020-10-05T19:13:22.003433","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"column_properties(train_transaction).T","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.055213,"end_time":"2020-10-05T19:13:30.474618","exception":false,"start_time":"2020-10-05T19:13:30.419405","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Identity Data\n* Variables in this table are identity information – network connection information (IP, ISP, Proxy, etc) and digital signature (UA/browser/os/version, etc) associated with transactions.\n* As we can see that the columns name for training set and test are not same,we will correct columns names of test set using traning column name"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-05T19:13:30.947624Z","iopub.status.busy":"2020-10-05T19:13:30.946695Z","iopub.status.idle":"2020-10-05T19:13:30.953101Z","shell.execute_reply":"2020-10-05T19:13:30.952387Z"},"papermill":{"duration":0.072429,"end_time":"2020-10-05T19:13:30.953274","exception":false,"start_time":"2020-10-05T19:13:30.880845","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# the columns name for training set and test are not same,we will correct columns names of test set using traning column name\nidentity_col_names =  train_identity.columns.tolist()\ntest_identity.columns = identity_col_names\nprint(test_identity.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-05T19:13:31.10579Z","iopub.status.busy":"2020-10-05T19:13:31.09727Z","iopub.status.idle":"2020-10-05T19:13:31.131818Z","shell.execute_reply":"2020-10-05T19:13:31.130958Z"},"papermill":{"duration":0.120492,"end_time":"2020-10-05T19:13:31.131964","exception":false,"start_time":"2020-10-05T19:13:31.011472","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"test_identity.head()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-05T19:13:31.263083Z","iopub.status.busy":"2020-10-05T19:13:31.262193Z","iopub.status.idle":"2020-10-05T19:13:32.514512Z","shell.execute_reply":"2020-10-05T19:13:32.513568Z"},"papermill":{"duration":1.322096,"end_time":"2020-10-05T19:13:32.514689","exception":false,"start_time":"2020-10-05T19:13:31.192593","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"column_properties(train_identity).T","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.063131,"end_time":"2020-10-05T19:13:32.645169","exception":false,"start_time":"2020-10-05T19:13:32.582038","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Merge Data\nMake a join between transaction data and identity data which are connected by key 'TransactionID"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-05T19:13:32.776871Z","iopub.status.busy":"2020-10-05T19:13:32.775979Z","iopub.status.idle":"2020-10-05T19:13:52.562341Z","shell.execute_reply":"2020-10-05T19:13:52.561547Z"},"papermill":{"duration":19.854296,"end_time":"2020-10-05T19:13:52.562505","exception":false,"start_time":"2020-10-05T19:13:32.708209","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"%%time\ntrain = pd.merge(train_transaction, train_identity, on= 'TransactionID', how = 'left')\ntest = pd.merge(test_transaction, test_identity, on= 'TransactionID', how = 'left')\ndel train_transaction, train_identity, test_transaction, test_identity\ngc.collect()\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.067369,"end_time":"2020-10-05T19:14:21.165915","exception":false,"start_time":"2020-10-05T19:14:21.098546","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Categorical Columns\nCreate list of categorical columns based on decsription below\n<br>https://www.kaggle.com/c/ieee-fraud-detection/discussion/101203#latest-586800"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-05T19:14:22.899807Z","iopub.status.busy":"2020-10-05T19:14:21.541449Z","iopub.status.idle":"2020-10-05T19:14:27.813925Z","shell.execute_reply":"2020-10-05T19:14:27.813098Z"},"papermill":{"duration":6.577503,"end_time":"2020-10-05T19:14:27.814066","exception":false,"start_time":"2020-10-05T19:14:21.236563","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"\n\n\n\ncat_cols = ['DeviceType', 'DeviceInfo', 'ProductCD', 'addr1', 'addr2', 'P_emaildomain', 'R_emaildomain']\ncat_cols +=  ['M' + str(i) for i in range(1,10)]\ncat_cols += ['card' + str(i) for i in range(1,7)]\ncat_cols += ['id_' + str(i) for i in range(12,39)]\ncolumn_properties(train[cat_cols]).T\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-05T19:14:28.187316Z","iopub.status.busy":"2020-10-05T19:14:28.080207Z","iopub.status.idle":"2020-10-05T19:14:28.270817Z","shell.execute_reply":"2020-10-05T19:14:28.270023Z"},"papermill":{"duration":0.384236,"end_time":"2020-10-05T19:14:28.270957","exception":false,"start_time":"2020-10-05T19:14:27.886721","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train[cat_cols].head()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.070079,"end_time":"2020-10-05T19:14:28.410862","exception":false,"start_time":"2020-10-05T19:14:28.340783","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Numeric Columns\n* From list of all columns remove categorical columns, Target Value, and ID, this will give us numerical columns\n* Display the statistical properties of numeric columns"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-05T19:14:30.484063Z","iopub.status.busy":"2020-10-05T19:14:29.492376Z","iopub.status.idle":"2020-10-05T19:14:38.968525Z","shell.execute_reply":"2020-10-05T19:14:38.969118Z"},"papermill":{"duration":10.488567,"end_time":"2020-10-05T19:14:38.969319","exception":false,"start_time":"2020-10-05T19:14:28.480752","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"%%time\n\n\nall_cols = train.columns.tolist()\nnum_cols = [x for x in all_cols if x not in cat_cols]\n\nnum_cols.remove('TransactionID')\nnum_cols.remove(TARGET)\ntrain[num_cols].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering\nCreate new feature and do Analysis for them. \n"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def plot_numeric_data(df, col, target_col, remove_outliers = False):\n   \n    df = df[[col, target_col]].copy()\n    df.dropna(subset=[col], inplace =True)\n    \n    #Remove Outliers: keep only the ones that are within +3 to -3 standard deviations in the column \n    if remove_outliers:       \n       \n        df = df[np.abs(df[col]-df[col].mean()) <= (3*df[col].std())]\n       \n\n\n    fig, (ax1, ax2,ax3)  =  plt.subplots(ncols = 3, figsize = (24,4))\n    fig.suptitle('Plots for {}'.format(col))\n    \n    #Display Density Plot\n    sns.distplot(df[col], color = 'b',  kde = True ,  ax = ax1 )\n    plt.ylabel('Density')\n\n\n    # Display Box Plot for feature\n    sns.boxplot(x = col , data = df,ax = ax2)\n   \n    #Display Density Plot for Fraud Vs NotFraud\n    sns.distplot(df[df[target_col] == 0][col], color = 'b', label = 'NotFraud',ax = ax3)\n    sns.distplot(df[df[target_col] == 1][col], color = 'r', label = 'Fraud',ax = ax3)\n    plt.legend(loc = 'best')\n    plt.ylabel('Density NotFraud vs Fraud')\n\n    plt.show()\n    \n\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def plot_categorical_data(col, data, top_n = 10, display_data = False ):\n    \n    # Prpare a dataframe for count and postive classs percent givel colums\n    df_data = data[[col, TARGET]].copy()    \n    df = df_data.groupby(col).agg({col:['count'], TARGET:['sum']})\n    df.columns = ['count', 'fraud_count']\n\n    df['fraud_perc'] = df['fraud_count'] * 100 / df['count']\n    df['fraude_perc'] = df['fraud_perc'].round(2)\n    \n#    % missing values in the columns to be displayed in title\n    null_perc = (df_data.shape[0]- df['count'].sum())  / df_data.shape[0]\n\n    width = 18\n    height = 6\n\n#   select only top n categories\n    df_disp = df.sort_values(by ='count', ascending= False).head(top_n )\n\n    fig, (ax1, ax2)  =  plt.subplots(ncols = 2, figsize = (width,height))\n    fig.suptitle('Plots for {} (Missing Values: {:.2%})'.format(col, null_perc))\n    \n#   Display Sort order should be by descending value of count\n    plot_order = df_disp.sort_values(by='count', ascending=False).index.values\n\n#   Display Bar chart for frequency count of top_n categories\n    s = sns.barplot(ax = ax1,  y = df_disp.index, x = df_disp['count'], order=plot_order, orient = 'h'  )\n    s.set_title('Count for {}'.format(col))\n    \n#   Display Bar chart for perecnt of positive class for top categories\n    s = sns.barplot(ax = ax2,  y = df_disp.index, x = df_disp['fraud_perc'], order=plot_order , orient = 'h'    )\n    s.set(xlabel='Fraud Percent')\n    s.set_title('% Fraud {}'.format(col))\n    plt.show()\n    if display_data:\n        return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### New Feature: Number of Nulls\n* Count the number of null values in a row. As we can see this is an important feature .\n* This is because as evident from from joint distribution plot if a trasactions have fewer data points availible(more nulls), the chances of fraud are low"},{"metadata":{"trusted":true},"cell_type":"code","source":"display_features = ['TransactionID']\n\n\ncol = 'nulls'\ntrain[col] = train.isnull().sum(axis=1)\ntest[col] = test.isnull().sum(axis=1)\n\ndisplay_features.append(col)\nplot_numeric_data(train, col, TARGET, remove_outliers = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### New Feature:Transaction Amount Decimal part\n* Get the decimal part of Transaction Amount and Multiply it by 1000\n* This is probably due to fraud transaction happening in foreign currency hence the credit card is charged with decimal amount.\n* We can also see that if the decimal part of transaction amount is zero chances of fraud are less\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncol = 'TAmt_decimal'\ntrain[col] = ((train['TransactionAmt'] - train['TransactionAmt'].astype(int)) * 1000).astype(int)\ntest[col] = ((test['TransactionAmt'] - test['TransactionAmt'].astype(int)) * 1000).astype(int)\n\ndisplay_features.append('TransactionAmt')\ndisplay_features.append(col)\nplot_numeric_data(train, col, TARGET, remove_outliers = True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### New Feature: Frequency Counts\n* Count the frequency of important categorical variables related to card, address, emaildoman and product code.\n* As we can see credit card which are used frequently have lesser chance of fraud\n* Frequency counts of categorical variables in general is good way to convert a categorical column into a numeric column which ML model can understand better\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"freq_cols = ['card1', 'addr1', 'addr2', 'card2', 'card3', 'card4', 'card5',\n             'card6', 'P_emaildomain', 'ProductCD', 'R_emaildomain']\nfor col in freq_cols:\n    display_features.append(col)\n    \n    train[col + '_count'] = train[col].map(train[col].value_counts(dropna=False) )\n    test[col + '_count'] =  test[col].map(train[col].value_counts(dropna=False) )\n    display_features.append(col + '_count')\n    plot_numeric_data(train, col + '_count',  TARGET, remove_outliers = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### New Feature: Hour of the day\n* From TransactionDT extract the hour of day of the transaction time, encoded as 0-23\n* TransactionDT field indicates the timestamp of a transaction and we can extract time related data from it\n* We can see that more frauds are committed between 1AM and 11 AM. This is probably because fraud originated in different time zone\n"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def make_hour_feature(df, tname='TransactionDT'):\n    #Creates an hour of the day feature, encoded as 0-23.  \n   \n    hours = df[tname] / (3600)        \n    encoded_hours = np.floor(hours) % 24\n    return encoded_hours","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display_features.append('TransactionDT')\n\ncol = 'hour'\ntrain[col] = make_hour_feature(train)\ntest[col] = make_hour_feature(test)\ndisplay_features.append(col)\n\nplot_numeric_data(train, col,  TARGET, remove_outliers = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Display Fraud Transactions for New Features \nDisplay data with new features and source features for which transactions were fraud\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"display_features.append(TARGET)\ntrain[train[TARGET]==1][display_features].head(10)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.260827,"end_time":"2020-10-05T19:24:12.500974","exception":false,"start_time":"2020-10-05T19:24:12.240147","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Data Pre-Processing\n* Concatenate the training and test dataset by appending. This is done so that we can apply feature engineering and pre-processing steps to combined set.\n* Convert the categorical features from string to int using ordinal encoding. For example, convert ['A', 'B'. 'C'] to [1,2,3]\n* Create a data frame sub for submission of test scores, we will later fill it with predictions on test set\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n\n# Concatenate the tranining and test dataset by appending\ndata_all = train.append(test, ignore_index = True, sort=False)\n\n#Create submission pandas dataframe \nsub = pd.DataFrame()\nsub['TransactionID'] = test.TransactionID\n\n\n\n# free the memory which is not required as it can exceed the physical ram \ndel train, test\ngc.collect()\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-05T19:24:13.049249Z","iopub.status.busy":"2020-10-05T19:24:13.048084Z","iopub.status.idle":"2020-10-05T19:24:31.875595Z","shell.execute_reply":"2020-10-05T19:24:31.874853Z"},"papermill":{"duration":19.091259,"end_time":"2020-10-05T19:24:31.87574","exception":false,"start_time":"2020-10-05T19:24:12.784481","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"\n\n\n# Do ordinal encoding for categorical features\nfor col in cat_cols:\n    data_all[col], uniques = pd.factorize(data_all[col])\n    #the factorize sets null values to -1, so convert them back to null, as we want LGB to handle null values\n    data_all[col] = data_all[col].replace(-1, np.nan)\n    \n\n    \n","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.253147,"end_time":"2020-10-05T19:24:32.395975","exception":false,"start_time":"2020-10-05T19:24:32.142828","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Create Test, Train and Validation sets\n* From combined dataset split the training and test datasets and separate the target and features\n* Split the training set into training and validation set. We will use first 80% of data as training set and last 20% as validation set.\n* Since the data is sorted in time according to transaction timestamp, we should not use random split.\n"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-05T19:24:32.893502Z","iopub.status.busy":"2020-10-05T19:24:32.892157Z","iopub.status.idle":"2020-10-05T19:24:42.701897Z","shell.execute_reply":"2020-10-05T19:24:42.701062Z"},"papermill":{"duration":10.061838,"end_time":"2020-10-05T19:24:42.702037","exception":false,"start_time":"2020-10-05T19:24:32.640199","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n#For test set target value will be null\nX_train =  data_all[data_all[TARGET].notnull()]\nX_test  =  data_all[data_all[TARGET].isnull()]\ndel data_all\ngc.collect()\n\n#get the labels for traning set\ny_train = X_train[TARGET]\n\n# Remove ID and TARGET column from train and test set\nX_train = X_train.drop(['TransactionID', TARGET], axis = 1)\nX_test = X_test.drop(['TransactionID',   TARGET], axis = 1)\n\n# Split the training set into training and validation set. \n# We will use first 80% of data as traningg set and last 20% as validation set.\n# Since the data is sorted in time according to transaction timestamp, we should not use random split.\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.2, shuffle=False, \n                                                      random_state = RANDOM_STATE)\n\nprint('Train shape{} Valid Shape{}, Test Shape {}'.format(X_train.shape, X_valid.shape, X_test.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ## Feature Selection: Select best features\n* Train the model on all features and get the feature importance for all features from trained model.\n* Select Top N features (TOP_N_FEATURES) based on feature importance\n* Final Model can be then trained on Top N features \n* Experiment with Variable TOP_N_FEATURES to select optimal number of feature. The Value 200 used here is based on results of multiple rounds of experiments\n* Set TOP_N_FEATURES to None if you want to train Final Model on all features \n* Goal is to get best possible AUC score on validation set by using minimum number of features by eliminating less useful features for prediction\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, roc_auc_score,f1_score, precision_score, recall_score,confusion_matrix\n\ndef validation_results(y_valid, y_prob, verbose = True):   \n    scores = {}                      \n    y_pred_class =  [0  if x < 0.5 else 1 for x in y_prob]\n    scores['val_accuracy']  = accuracy_score(y_valid, y_pred_class)\n    scores['val_auc']       = roc_auc_score(y_valid, y_prob)\n    scores['val_f1']        =   f1_score(y_valid, y_pred_class, average = 'binary')\n    scores['val_precision'] = precision_score(y_valid, y_pred_class)\n    scores['val_recall']    = recall_score(y_valid, y_pred_class)\n    \n    cm = confusion_matrix(y_valid, y_pred_class)\n    cm_df = pd.DataFrame(cm, columns=np.unique(y_valid), index = np.unique(y_valid))\n    if verbose:\n        print('\\nValidation Accuracy      {:0.5f}'.format( scores['val_accuracy'] ))\n        print('Validation   AUC         {:0.5f}'.format( scores['val_auc']   ))\n        print('Validation Precision     {:0.5f}'.format(scores['val_precision']))\n        print('Validation Recall        {:0.5f}'.format(scores['val_recall']))\n        print('Validation  F1           %0.5f' %scores['val_f1'] )\n    return scores , cm_df\n\n\ndef train_and_evalaute(params ,X_train, y_train,X_valid, y_valid, feature_imp  ):\n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_valid  = lgb.Dataset(X_valid, y_valid)\n    early_stopping_rounds = 200\n    lgb_results = {}\n\n    model = lgb.train(params,\n                      lgb_train,\n                      num_boost_round = 10000,\n                      valid_sets =  [lgb_train,lgb_valid],\n                      early_stopping_rounds = early_stopping_rounds,                    \n    #                   categorical_feature = cat_cols,\n                      evals_result = lgb_results,\n                      verbose_eval = 100\n                       )\n\n\n    y_prob = model.predict(X_valid)\n    \n    auc = roc_auc_score(y_valid, y_prob)\n    \n    \n    feature_imp['feature'] = model.feature_name()\n    feature_imp['importance']  = model.feature_importance()\n    feature_imp = feature_imp.sort_values(['importance'], ascending = False)\n    \n    return model, auc, feature_imp, lgb_results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Train model on full featureset and get the AUC and Feature Importance\nVariable TOP_N_FEATURES can be set to select number of features. Set it to None if you want to train Final Model on all features\n"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-05T19:24:43.739633Z","iopub.status.busy":"2020-10-05T19:24:43.738644Z","iopub.status.idle":"2020-10-05T19:29:52.441756Z","shell.execute_reply":"2020-10-05T19:29:52.443083Z"},"papermill":{"duration":308.973746,"end_time":"2020-10-05T19:29:52.44352","exception":false,"start_time":"2020-10-05T19:24:43.469774","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"%%time\nimport lightgbm as lgb\nTOP_N_FEATURES = 200\nfeature_imp = pd.DataFrame()\n\nparams = {}\n\nparams['learning_rate'] = 0.06\nparams['boosting_type'] = 'gbdt'\nparams['objective'] = 'binary'\nparams['seed'] =  RANDOM_STATE\nparams['metric'] =    'auc'\nparams['bagging_fraction'] = 0.8\nparams['bagging_freq'] = 1\nparams['feature_fraction'] = 0.8\nparams['max_bin'] = 127\nparams['scale_pos_weight'] = 4\n\nif TOP_N_FEATURES is not None:\n    print('Training model on {} Features'.format(X_train.shape[1]))\n    print('Train shape{} Valid Shape{}, Test Shape {}\\n'.format(X_train.shape, X_valid.shape, X_test.shape))\n    \n    \n    _, auc, feature_imp, _ = train_and_evalaute(params ,X_train, y_train,X_valid, y_valid, feature_imp   )\n\n    print('\\nValidation AUC Score on ALL Features is {:.6f}\\n'.format(auc))\n    \nfeature_imp.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Select the top N features based on feature importance\nWe can see that below engineered features are among top 20 features\n* card1_count\n* card2_count\n* addr1_count\n* TAmt_decimal"},{"metadata":{"trusted":true},"cell_type":"code","source":"if TOP_N_FEATURES is not None:\n\n    feature_imp = feature_imp.head(TOP_N_FEATURES)\n    top_n_features = feature_imp['feature'].tolist()\n    print('\\nTop {} Features\\n'.format(TOP_N_FEATURES))\n    print(top_n_features )\n    \n    X_train = X_train[top_n_features]\n    X_valid  = X_valid[top_n_features]\n    X_test  =  X_test[top_n_features]","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.250444,"end_time":"2020-10-05T19:24:43.208216","exception":false,"start_time":"2020-10-05T19:24:42.957772","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Train LightGBM Model With Validation\n* LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient https://lightgbm.readthedocs.io/en/latest/Features.html\n* Train on first 80% of dataset and evaluate on next 20 % as data is sorted in time\n* parameter 'SCALE_POS_WEIGHT' is to handle the unbalanced nature of dataset. This parameter gives more weight to minority class, which improves precision, recall and F1 scores\n* No imputation of missing values is necessary as LightGBM can use optimized strategies automatically\n* Feature selection method discussed in above section is used to determine the optimal number of feature.\n* New Features created in above section will be also be used if they are in Top N features list\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nprint('Train shape{} Valid Shape{}, Test Shape {}\\n'.format(X_train.shape, X_valid.shape, X_test.shape))\n\nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_valid  = lgb.Dataset(X_valid, y_valid)\nearly_stopping_rounds = 200\nlgb_results = {}\n\nmodel = lgb.train(params,\n                      lgb_train,\n                      num_boost_round = 10000,\n                      valid_sets =  [lgb_train,lgb_valid],\n                      early_stopping_rounds = early_stopping_rounds,                    \n    #                   categorical_feature = cat_cols,\n                      evals_result = lgb_results,\n                      verbose_eval = 100\n                       )\n\nprint('\\nPrinting Model Parameters\\n')\nprint(params)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.252384,"end_time":"2020-10-05T19:29:52.95704","exception":false,"start_time":"2020-10-05T19:29:52.704656","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Display Results"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-05T19:29:54.180283Z","iopub.status.busy":"2020-10-05T19:29:54.178378Z","iopub.status.idle":"2020-10-05T19:29:58.749035Z","shell.execute_reply":"2020-10-05T19:29:58.747741Z"},"papermill":{"duration":4.865712,"end_time":"2020-10-05T19:29:58.749311","exception":false,"start_time":"2020-10-05T19:29:53.883599","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"y_prob = model.predict(X_valid)\nresults, cm_df  = validation_results(y_valid, y_prob, verbose = True)\n","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.28554,"end_time":"2020-10-05T19:29:59.319967","exception":false,"start_time":"2020-10-05T19:29:59.034427","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Display Confusion Matrix"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-05T19:29:59.8854Z","iopub.status.busy":"2020-10-05T19:29:59.869359Z","iopub.status.idle":"2020-10-05T19:30:00.242803Z","shell.execute_reply":"2020-10-05T19:30:00.241872Z"},"papermill":{"duration":0.649343,"end_time":"2020-10-05T19:30:00.242969","exception":false,"start_time":"2020-10-05T19:29:59.593626","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"\ncm_df.index.name = 'Actual'\ncm_df.columns.name = 'Predicted'\nplt.figure(figsize = (10,7))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(cm_df, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16}, fmt='g')# font size\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.30716,"end_time":"2020-10-05T19:30:00.879751","exception":false,"start_time":"2020-10-05T19:30:00.572591","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Plot Training vs Validation scores"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-05T19:30:01.477369Z","iopub.status.busy":"2020-10-05T19:30:01.476055Z","iopub.status.idle":"2020-10-05T19:30:01.481591Z","shell.execute_reply":"2020-10-05T19:30:01.480638Z"},"papermill":{"duration":0.310908,"end_time":"2020-10-05T19:30:01.48177","exception":false,"start_time":"2020-10-05T19:30:01.170862","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"\ndef plot_lgb_scores(lgb_results):\n    train_res = lgb_results['training']['auc']\n    valid_res = lgb_results['valid_1']['auc']\n    ntrees = range(1, len(train_res) + 1)\n\n    plt.figure(figsize = (12, 6))\n    plt.plot(ntrees, train_res , 'b', label = 'Training')\n    plt.plot(ntrees, valid_res, 'r', label = 'Validation')\n    plt.xlabel('Number of Trees', fontsize = 14)\n    plt.ylabel('AUC Score', fontsize = 14)\n    plt.legend(fontsize = 14)\n    plt.show()\n    \n\n    \n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-05T19:30:02.049802Z","iopub.status.busy":"2020-10-05T19:30:02.044858Z","iopub.status.idle":"2020-10-05T19:30:02.524039Z","shell.execute_reply":"2020-10-05T19:30:02.523332Z"},"papermill":{"duration":0.77464,"end_time":"2020-10-05T19:30:02.524301","exception":false,"start_time":"2020-10-05T19:30:01.749661","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"plot_lgb_scores(lgb_results)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plot ROC Curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n\nimport sklearn.metrics as metrics\nfpr, tpr, threshold = metrics.roc_curve(y_valid, y_prob)\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.figure(figsize = (12, 6))\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.260748,"end_time":"2020-10-05T19:30:03.046368","exception":false,"start_time":"2020-10-05T19:30:02.78562","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Plot Feature Importance\nDisplay top 20 features "},{"metadata":{"execution":{"iopub.execute_input":"2020-10-05T19:30:03.564835Z","iopub.status.busy":"2020-10-05T19:30:03.563798Z","iopub.status.idle":"2020-10-05T19:30:03.567285Z","shell.execute_reply":"2020-10-05T19:30:03.56787Z"},"papermill":{"duration":0.268993,"end_time":"2020-10-05T19:30:03.568069","exception":false,"start_time":"2020-10-05T19:30:03.299076","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def plot_feature_imp(model, top_n = 30):\n    feature_imp = pd.DataFrame()\n    feature_imp['feature'] = model.feature_name()\n    feature_imp['importance']  = model.feature_importance()\n    feature_imp = feature_imp.sort_values(['importance'], ascending = False)\n    feature_imp_disp = feature_imp.head(top_n)\n    plt.figure(figsize=(10, 12))\n    sns.barplot(x=\"importance\", y=\"feature\", data=feature_imp_disp)\n    plt.title('LightGBM Features')\n    plt.show() \n#     return feature_imp","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-05T19:30:04.088134Z","iopub.status.busy":"2020-10-05T19:30:04.086419Z","iopub.status.idle":"2020-10-05T19:30:04.626195Z","shell.execute_reply":"2020-10-05T19:30:04.625523Z"},"papermill":{"duration":0.810723,"end_time":"2020-10-05T19:30:04.62635","exception":false,"start_time":"2020-10-05T19:30:03.815627","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"plot_feature_imp(model, top_n = 20)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.275271,"end_time":"2020-10-05T19:30:05.184185","exception":false,"start_time":"2020-10-05T19:30:04.908914","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Predict on test set\nAlso write the results as csv file"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-05T19:30:05.719621Z","iopub.status.busy":"2020-10-05T19:30:05.717326Z","iopub.status.idle":"2020-10-05T19:30:22.030565Z","shell.execute_reply":"2020-10-05T19:30:22.029742Z"},"papermill":{"duration":16.577317,"end_time":"2020-10-05T19:30:22.030708","exception":false,"start_time":"2020-10-05T19:30:05.453391","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"y_prob_test = model.predict(X_test)\nsub['isFraud'] = y_prob_test\nsub.to_csv('lgb_sub.csv', index=False)\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.251153,"end_time":"2020-10-05T19:30:22.547622","exception":false,"start_time":"2020-10-05T19:30:22.296469","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Summary\n\n* Do basic data preprocessing steps to convert categorical variables to integers using ordinal encoding.\n* Train on a LightGBM model with 80% data and 20% validation data,\n* Use metric AUC to evaluate the performance of model.\n* Handle Imbalanced Dataset by using inbuilt model parameter SCALE_POS_WEIGHT= 4. Using this we were able to improve F1 Score from 0.38244(Baseline Model) to  0.56081 on current model\n* Using new engineered features only AUC score increased from 0.0.90693(Baseline Model) to  0.92259 on validation set. The AUC of public test dataset increased from 0.906929(baseline) to 0.924521\n* We used Feature selection to select best 200 feature out of 400 plus features without significant loss of any performance. Infract on validation set score increased slightly\n     *    AUC Validation  with 446 Features:  0.922263\n     *    AUC Validation  with 200 Features:  0.923574\n     *    AUC Public test with 446 Features:  0.924521\n     *    AUC Public test with 200 Features  0.923118\n"},{"metadata":{"papermill":{"duration":0.254611,"end_time":"2020-10-05T19:30:23.051793","exception":false,"start_time":"2020-10-05T19:30:22.797182","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}