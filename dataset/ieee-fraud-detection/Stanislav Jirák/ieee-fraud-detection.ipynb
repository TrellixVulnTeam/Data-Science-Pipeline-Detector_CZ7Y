{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Steps to solve the problem:**\n\n1. Categorical encoding (label encoding, OHE, target encoding, ...)\n1. Dimensionality reduction (PCA, t-SNE, UMAP, ...)\n1. Under/Over-sampling (NearMiss, SMOTE)\n1. Logistic model vs. NN","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Basic\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n\n# Tools\nfrom sklearn.utils import resample\nfrom sklearn.manifold import TSNE\n\n# System\nimport time\n\n# Defaults\nimport warnings\nwarnings.filterwarnings('ignore') #Supress warnings\n%matplotlib inline\n\n# Dataset\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_id = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/train_identity.csv\")\ntrain_id.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_trans = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/train_transaction.csv\")\ntrain_trans.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_id = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/test_identity.csv\")\ntest_id.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_trans = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/test_transaction.csv\")\ntest_trans.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.merge(train_id, train_trans, on='TransactionID', how='inner')\ntrain_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.merge(test_id, test_trans, on='TransactionID', how='inner')\ntest_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge both datasets for convinient feature engineering such as categorical encoding\ndf = pd.concat([train_df, test_df])\n\nsplit = len(train_df) #Get a split marker to part the train and test df for a model\ntarget = train_df['isFraud'] #Save target values for a model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Function to reduce the DF size\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reduce memory usage\ndf = reduce_mem_usage(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Release memory (unused dataframes)\ndel train_id, train_trans, test_id, test_trans\n#del train_df, test_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for null values\ndf.isnull().sum().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Class imbalance\nprint(\"Fraud rate: \", round(target.to_frame().isFraud.sum() / target.to_frame().isFraud.count() * 100, 2), \" %\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data pre-processing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encode categorical features\ndf_ohe = pd.get_dummies(df, dummy_na=True, drop_first=True)\ndf_ohe.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill default NAN value\ndf_ohe = df_ohe.fillna(-1) #dev","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#TODO: Scaling","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Undersample","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Downsize the training dataset 50:50\nnot_fraud=train_df[train_df.isFraud==0]\nfraud=train_df[train_df.isFraud==1]\n\nnot_fraud_downsampled = resample(not_fraud,\n                                replace = False, # sample without replacement\n                                n_samples = len(fraud), # match minority n\n                                random_state = 27) # reproducible results\n\n# combine minority and downsampled majority\ndownsampled = pd.concat([not_fraud_downsampled, fraud])\n\n# checking counts\ndownsampled.isFraud.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# t-SNE","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# New_df is from the random undersample data (fewer instances)\nX = downsampled.drop('isFraud', axis=1)\ny = downsampled['isFraud']\n\n# T-SNE Implementation\nt0 = time.time()\nX_reduced_tsne = TSNE(n_components=2, random_state=42).fit_transform(X.values)\nt1 = time.time()\nprint(\"T-SNE took {:.2} s\".format(t1 - t0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot t-SNE\nplt.suptitle('Clusters using Dimensionality Reduction', fontsize=14)\n\nblue_patch = mpatches.Patch(color='#0A0AFF', label='No Fraud')\nred_patch = mpatches.Patch(color='#AF0000', label='Fraud')\n\n# t-SNE scatter plot\nplt.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\nplt.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\n\nplt.grid(True)\n\nplt.legend(handles=[blue_patch, red_patch])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"downsampled = downsampled.fillna(-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# New_df is from the random undersample data (fewer instances)\nX = downsampled.drop('isFraud', axis=1)\ny = downsampled['isFraud']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Our data is already scaled we should split our training and test sets\nfrom sklearn.model_selection import train_test_split\n\n# This is explicitly used for undersampling.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get categorical features\ncat_features_indices = np.where(X.dtypes != float)[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import catboost\n\nmodel = catboost.CatBoostClassifier(\n    one_hot_max_size=7,\n    iterations=100,\n    random_seed=42,\n    verbose=False,\n    #loss_function = 'RMSE'\n    #eval_metric='Accuracy'\n)\n\npool = catboost.Pool(X_train, y_train, cat_features_indices)\ncv_scores = catboost.cv(pool, model.get_params(), fold_count=5, plot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('CV score: {:.5f}'.format(cv_scores['test-RMSE-mean'].values[-1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Turn the values into an array for feeding the classification algorithms.\nX_train = X_train.values\nX_test = X_test.values\ny_train = y_train.values\ny_test = y_test.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's implement simple classifiers\nclassifiers = {\n    \"LogisiticRegression\": LogisticRegression(),\n    \"KNearest\": KNeighborsClassifier(),\n    \"Support Vector Classifier\": SVC(),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Wow our scores are getting even high scores even when applying cross validation.\nfrom sklearn.model_selection import cross_val_score\n\nfor key, classifier in classifiers.items():\n    classifier.fit(X_train, y_train)\n    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use GridSearchCV to find the best parameters.\nfrom sklearn.model_selection import GridSearchCV\n\n\n# Logistic Regression \nlog_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\n\n\ngrid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\ngrid_log_reg.fit(X_train, y_train)\n# We automatically get the logistic regression with the best parameters.\nlog_reg = grid_log_reg.best_estimator_\n\nknears_params = {\"n_neighbors\": list(range(2,5,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n\ngrid_knears = GridSearchCV(KNeighborsClassifier(), knears_params)\ngrid_knears.fit(X_train, y_train)\n# KNears best estimator\nknears_neighbors = grid_knears.best_estimator_\n\n# Support Vector Classifier\nsvc_params = {'C': [0.5, 0.7, 0.9, 1], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\ngrid_svc = GridSearchCV(SVC(), svc_params)\ngrid_svc.fit(X_train, y_train)\n\n# SVC best estimator\nsvc = grid_svc.best_estimator_\n\n# DecisionTree Classifier\ntree_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)), \n              \"min_samples_leaf\": list(range(5,7,1))}\ngrid_tree = GridSearchCV(DecisionTreeClassifier(), tree_params)\ngrid_tree.fit(X_train, y_train)\n\n# tree best estimator\ntree_clf = grid_tree.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Overfitting Case\nlog_reg_score = cross_val_score(log_reg, X_train, y_train, cv=5)\nprint('Logistic Regression Cross Validation Score: ', round(log_reg_score.mean() * 100, 2).astype(str) + '%')\n\n\nknears_score = cross_val_score(knears_neighbors, X_train, y_train, cv=5)\nprint('Knears Neighbors Cross Validation Score', round(knears_score.mean() * 100, 2).astype(str) + '%')\n\nsvc_score = cross_val_score(svc, X_train, y_train, cv=5)\nprint('Support Vector Classifier Cross Validation Score', round(svc_score.mean() * 100, 2).astype(str) + '%')\n\ntree_score = cross_val_score(tree_clf, X_train, y_train, cv=5)\nprint('DecisionTree Classifier Cross Validation Score', round(tree_score.mean() * 100, 2).astype(str) + '%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will undersample during cross validating\nundersample_X = df.drop('Class', axis=1)\nundersample_y = df['Class']\n\nfor train_index, test_index in sss.split(undersample_X, undersample_y):\n    print(\"Train:\", train_index, \"Test:\", test_index)\n    undersample_Xtrain, undersample_Xtest = undersample_X.iloc[train_index], undersample_X.iloc[test_index]\n    undersample_ytrain, undersample_ytest = undersample_y.iloc[train_index], undersample_y.iloc[test_index]\n    \nundersample_Xtrain = undersample_Xtrain.values\nundersample_Xtest = undersample_Xtest.values\nundersample_ytrain = undersample_ytrain.values\nundersample_ytest = undersample_ytest.values \n\nundersample_accuracy = []\nundersample_precision = []\nundersample_recall = []\nundersample_f1 = []\nundersample_auc = []\n\n# Implementing NearMiss Technique \n# Distribution of NearMiss (Just to see how it distributes the labels we won't use these variables)\nX_nearmiss, y_nearmiss = NearMiss().fit_sample(undersample_X.values, undersample_y.values)\nprint('NearMiss Label Distribution: {}'.format(Counter(y_nearmiss)))\n# Cross Validating the right way\n\nfor train, test in sss.split(undersample_Xtrain, undersample_ytrain):\n    undersample_pipeline = imbalanced_make_pipeline(NearMiss(sampling_strategy='majority'), log_reg) # SMOTE happens during Cross Validation not before..\n    undersample_model = undersample_pipeline.fit(undersample_Xtrain[train], undersample_ytrain[train])\n    undersample_prediction = undersample_model.predict(undersample_Xtrain[test])\n    \n    undersample_accuracy.append(undersample_pipeline.score(original_Xtrain[test], original_ytrain[test]))\n    undersample_precision.append(precision_score(original_ytrain[test], undersample_prediction))\n    undersample_recall.append(recall_score(original_ytrain[test], undersample_prediction))\n    undersample_f1.append(f1_score(original_ytrain[test], undersample_prediction))\n    undersample_auc.append(roc_auc_score(original_ytrain[test], undersample_prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's Plot LogisticRegression Learning Curve\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import learning_curve\n\ndef plot_learning_curve(estimator1, estimator2, estimator3, estimator4, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(20,14), sharey=True)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    # First Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator1, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax1.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax1.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax1.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax1.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax1.set_title(\"Logistic Regression Learning Curve\", fontsize=14)\n    ax1.set_xlabel('Training size (m)')\n    ax1.set_ylabel('Score')\n    ax1.grid(True)\n    ax1.legend(loc=\"best\")\n    \n    # Second Estimator \n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator2, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax2.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax2.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax2.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax2.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax2.set_title(\"Knears Neighbors Learning Curve\", fontsize=14)\n    ax2.set_xlabel('Training size (m)')\n    ax2.set_ylabel('Score')\n    ax2.grid(True)\n    ax2.legend(loc=\"best\")\n    \n    # Third Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator3, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax3.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax3.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax3.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax3.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax3.set_title(\"Support Vector Classifier \\n Learning Curve\", fontsize=14)\n    ax3.set_xlabel('Training size (m)')\n    ax3.set_ylabel('Score')\n    ax3.grid(True)\n    ax3.legend(loc=\"best\")\n    \n    # Fourth Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator4, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax4.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax4.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax4.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax4.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax4.set_title(\"Decision Tree Classifier \\n Learning Curve\", fontsize=14)\n    ax4.set_xlabel('Training size (m)')\n    ax4.set_ylabel('Score')\n    ax4.grid(True)\n    ax4.legend(loc=\"best\")\n    return plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\nplot_learning_curve(log_reg, knears_neighbors, svc, tree_clf, X_train, y_train, (0.87, 1.01), cv=cv, n_jobs=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluate","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\nprint('Logistic Regression: ', roc_auc_score(y_train, log_reg_pred))\nprint('KNears Neighbors: ', roc_auc_score(y_train, knears_pred))\nprint('Support Vector Classifier: ', roc_auc_score(y_train, svc_pred))\nprint('Decision Tree Classifier: ', roc_auc_score(y_train, tree_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_fpr, log_tpr, log_thresold = roc_curve(y_train, log_reg_pred)\nknear_fpr, knear_tpr, knear_threshold = roc_curve(y_train, knears_pred)\nsvc_fpr, svc_tpr, svc_threshold = roc_curve(y_train, svc_pred)\ntree_fpr, tree_tpr, tree_threshold = roc_curve(y_train, tree_pred)\n\n\ndef graph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr):\n    plt.figure(figsize=(16,8))\n    plt.title('ROC Curve \\n Top 4 Classifiers', fontsize=18)\n    plt.plot(log_fpr, log_tpr, label='Logistic Regression Classifier Score: {:.4f}'.format(roc_auc_score(y_train, log_reg_pred)))\n    plt.plot(knear_fpr, knear_tpr, label='KNears Neighbors Classifier Score: {:.4f}'.format(roc_auc_score(y_train, knears_pred)))\n    plt.plot(svc_fpr, svc_tpr, label='Support Vector Classifier Score: {:.4f}'.format(roc_auc_score(y_train, svc_pred)))\n    plt.plot(tree_fpr, tree_tpr, label='Decision Tree Classifier Score: {:.4f}'.format(roc_auc_score(y_train, tree_pred)))\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.01, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n                )\n    plt.legend()\n    \ngraph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def logistic_roc_curve(log_fpr, log_tpr):\n    plt.figure(figsize=(12,8))\n    plt.title('Logistic Regression ROC Curve', fontsize=16)\n    plt.plot(log_fpr, log_tpr, 'b-', linewidth=2)\n    plt.plot([0, 1], [0, 1], 'r--')\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.axis([-0.01,1,0,1])\n    \n    \nlogistic_roc_curve(log_fpr, log_tpr)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\n\nprecision, recall, threshold = precision_recall_curve(y_train, log_reg_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\n\ny_pred = log_reg.predict(X_train)\n\n# Overfitting Case\nprint('---' * 45)\nprint('Overfitting: \\n')\nprint('Recall Score: {:.2f}'.format(recall_score(y_train, y_pred)))\nprint('Precision Score: {:.2f}'.format(precision_score(y_train, y_pred)))\nprint('F1 Score: {:.2f}'.format(f1_score(y_train, y_pred)))\nprint('Accuracy Score: {:.2f}'.format(accuracy_score(y_train, y_pred)))\nprint('---' * 45)\n\n# How it should look like\nprint('---' * 45)\nprint('How it should be:\\n')\nprint(\"Accuracy Score: {:.2f}\".format(np.mean(undersample_accuracy)))\nprint(\"Precision Score: {:.2f}\".format(np.mean(undersample_precision)))\nprint(\"Recall Score: {:.2f}\".format(np.mean(undersample_recall)))\nprint(\"F1 Score: {:.2f}\".format(np.mean(undersample_f1)))\nprint('---' * 45)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"undersample_y_score = log_reg.decision_function(original_Xtest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import average_precision_score\n\nundersample_average_precision = average_precision_score(original_ytest, undersample_y_score)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      undersample_average_precision))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(12,6))\n\nprecision, recall, _ = precision_recall_curve(original_ytest, undersample_y_score)\n\nplt.step(recall, precision, color='#004a93', alpha=0.2,\n         where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.2,\n                 color='#48a6ff')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('UnderSampling Precision-Recall curve: \\n Average Precision-Recall Score ={0:0.2f}'.format(\n          undersample_average_precision), fontsize=16)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}