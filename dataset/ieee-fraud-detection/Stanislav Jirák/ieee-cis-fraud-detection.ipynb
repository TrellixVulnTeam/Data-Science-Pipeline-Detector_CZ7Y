{"cells":[{"metadata":{},"cell_type":"markdown","source":"# The Problem"},{"metadata":{},"cell_type":"markdown","source":"Imagine standing at the check-out counter at the grocery store with a long line behind you and the cashier not-so-quietly announces that your card has been declined. In this moment, you probably aren’t thinking about the data science that determined your fate.\n\nEmbarrassed, and certain you have the funds to cover everything needed for an epic nacho party for 50 of your closest friends, you try your card again. Same result. As you step aside and allow the cashier to tend to the next customer, you receive a text message from your bank. “Press 1 if you really tried to spend $500 on cheddar cheese.”\n\nWhile perhaps cumbersome (and often embarrassing) in the moment, this fraud prevention system is actually saving consumers millions of dollars per year. Researchers from the IEEE Computational Intelligence Society (IEEE-CIS) want to improve this figure, while also improving the customer experience. With higher accuracy fraud detection, you can get on with your chips without the hassle.\n\nIEEE-CIS works across a variety of AI and machine learning areas, including deep neural networks, fuzzy systems, evolutionary computation, and swarm intelligence. Today they’re partnering with the world’s leading payment service company, Vesta Corporation, seeking the best solutions for fraud prevention industry, and now you are invited to join the challenge.\n\nIn this competition, you’ll benchmark machine learning models on a challenging large-scale dataset. The data comes from Vesta's real-world e-commerce transactions and contains a wide range of features from device type to product features. You also have the opportunity to create new features to improve your results.\n\nIf successful, you’ll improve the efficacy of fraudulent transaction alerts for millions of people around the world, helping hundreds of thousands of businesses reduce their fraud loss and increase their revenue. And of course, you will save party people just like you the hassle of false positives."},{"metadata":{},"cell_type":"markdown","source":"# Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install datatable==0.11.0","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport datatable as dt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_t = pd.read_csv('/kaggle/input/ieee-fraud-detection/train_transaction.csv', index_col='TransactionID', nrows=10**5)\ntrain_t.shape\n\n'''\n#dt.fread(\"../input/riiid-test-answer-prediction/train.csv\").to_jay(\"train.jay\")\ntrain_t = dt.fread(\"/kaggle/input/ieee-fraud-detection/train_transaction.csv\").to_pandas()\ntrain_t.set_index('TransactionID')\ntrain_t.shape\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select frauds\ntrain_t_f = train_t.loc[train_t['isFraud'] == 1]\ntrain_t_f.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select not frauds\ntrain_t_nf = train_t.loc[train_t['isFraud'] == 0]\ntrain_t_nf = train_t_nf.sample(frac=0.1).iloc[0:2560]\ntrain_t_nf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create balanced dataset\ntrain_t = pd.concat([train_t_f, train_t_nf], axis=0)\ntrain_t.shape\n\ndel train_t_f, train_t_nf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_id = pd.read_csv('/kaggle/input/ieee-fraud-detection/train_identity.csv', index_col='TransactionID', nrows=10**5)\ntrain_id.shape\n'''\ntrain_id = dt.fread(\"/kaggle/input/ieee-fraud-detection/train_identity.csv\").to_pandas()\ntrain_id.set_index('TransactionID')\ntrain_id.shape\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = train_t.merge(train_id, on='TransactionID', how='left')\ndf_train.shape\n\n# Release memory\ndel train_t\ndel train_id","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\ntest_t = pd.read_csv('/kaggle/input/ieee-fraud-detection/test_transaction.csv', index_col='TransactionID')\ntest_t.shape\n'''\n\ndt.fread(\"/kaggle/input/ieee-fraud-detection/test_transaction.csv\").to_jay(\"test_transaction.jay\")\ntest_t = dt.fread(\"test_transaction.jay\").to_pandas()\ntest_t.set_index('TransactionID')\ntest_t.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\ntest_id = pd.read_csv('/kaggle/input/ieee-fraud-detection/test_identity.csv', index_col='TransactionID')\ntest_id.shape\n'''\n\ndt.fread(\"/kaggle/input/ieee-fraud-detection/test_identity.csv\").to_jay(\"test_identity.jay\")\ntest_id = dt.fread(\"test_identity.jay\").to_pandas()\ntest_id.set_index('TransactionID')\ntest_id.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = test_t.merge(test_id, on='TransactionID', how='left')\ndf_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Release memory\ndel test_t\ndel test_id","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('/kaggle/input/ieee-fraud-detection/sample_submission.csv')\nsub.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Target distribution\nimport seaborn as sns\n\ng = sns.countplot(x='isFraud', data=df_train, )\ng.set_title(\"Fraud Transactions Distribution\", fontsize=18)\ng.set_xlabel(\"Is fraud?\", fontsize=14)\ng.set_ylabel('Count', fontsize=14)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Wrangling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Concat dataset for better manipulation\nsplit = len(df_train)\ntarget = df_train.isFraud\ndf = pd.concat([df_train, df_test], axis=0).drop('isFraud', axis=1)\n\ndel df_train, df_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/kabure/extensive-eda-and-modeling-xgb-hyperopt\nemails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', \n          'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft',\n          'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',\n          'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', \n          'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink',\n          'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other',\n          'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', \n          'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', \n          'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo',\n          'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n          'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft',\n          'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', \n          'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', \n          'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', \n          'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', \n          'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', \n          'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', \n          'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other',\n          'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n\nus_emails = ['gmail', 'net', 'edu']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for c in ['P_emaildomain', 'R_emaildomain']:\n    df[c + '_bin'] = df[c].map(emails)    \n    df[c + '_suffix'] = df[c].map(lambda x: str(x).split('.')[-1])    \n    df[c + '_suffix'] = df[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoding\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\nfor f in df.columns:\n    if df[f].dtype=='object':\n        df[f] = le.fit_transform(list(df[f].values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Impute nulls\ndf = df.fillna(-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#TODO: Feature engineering","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#TODO: Box-Cox?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#TODO: Scalling\nfrom sklearn.preprocessing import StandardScaler\n\nss = StandardScaler()\ndf_ss = ss.fit_transform(df.iloc[:,3:470].values)\ndf_ss = pd.DataFrame(df_ss, index=df.index, columns=df.iloc[:,3:470].columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dimensionality reduction\nfrom sklearn.decomposition import PCA\n\npca = PCA(.999) # retain 95% of the variance #PCA(n_components=10)\ndf_pca = pca.fit_transform(df.iloc[:,3:470])\ndf_pca = pd.DataFrame(df_pca, index=df.index) # Convert to df\n#print(pca.explained_variance_ratio_.sum())\nprint(pca.n_components_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO: Use df, df_c is for dev purpose\ndf_c = pd.concat([df.iloc[:,:3], df_pca, df.iloc[:,470:]], axis=1)\ndf_c.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_c[:split]\ndf_test = df_c[split:]\n\n#del df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get train and validation sub-datasets\nfrom sklearn.model_selection import train_test_split\n\nX = df_train\ny = target\n\n#Do train data splitting\nX_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.75, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\n\nmodel = xgb.XGBClassifier(\n    n_estimators=1000,\n    tree_method='gpu_hist',\n    eval_metric='auc'\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train, \n          eval_set=[(X_test, y_test)], \n          verbose=50, \n          early_stopping_rounds=300)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation"},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use whole training data\nmodel.fit(df_train, target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict_proba(df_test)[:,1] \ny_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create submission\nsub['isFraud'] = y_pred\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since this competition is over, this is just a simplified notebook to solve the problem - which guides you through all the necessary steps and leaves a room for your own innovation and improment to try.\nIf you like it, please **upvote**!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}