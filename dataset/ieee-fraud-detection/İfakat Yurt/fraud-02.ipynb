{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-27T18:47:06.9248Z","iopub.execute_input":"2021-08-27T18:47:06.925465Z","iopub.status.idle":"2021-08-27T18:47:06.939519Z","shell.execute_reply.started":"2021-08-27T18:47:06.925361Z","shell.execute_reply":"2021-08-27T18:47:06.938485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import NuSVR, SVR\nfrom sklearn.metrics import mean_absolute_error\npd.options.display.precision = 15\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport time\nimport datetime\nfrom catboost import CatBoostRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit\nfrom sklearn import metrics\nfrom sklearn import linear_model\nimport gc\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport eli5\nimport shap\nfrom IPython.display import HTML\nimport json\nimport altair as alt\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:47:06.958135Z","iopub.execute_input":"2021-08-27T18:47:06.958524Z","iopub.status.idle":"2021-08-27T18:47:18.998033Z","shell.execute_reply.started":"2021-08-27T18:47:06.95849Z","shell.execute_reply":"2021-08-27T18:47:18.997005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Zaman degiskenleri icin gerekli kutuphaneleri yukledik\nimport os\nimport time\nimport datetime\nimport json\nimport gc\nfrom numba import jit\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm_notebook\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nfrom sklearn import metrics\n\nfrom itertools import product\n\nimport altair as alt\nfrom altair.vega import v5\nfrom IPython.display import HTML\n\n# using ideas from this kernel: https://www.kaggle.com/notslush/altair-visualization-2018-stackoverflow-survey\ndef prepare_altair():\n    \"\"\"\n    Helper function to prepare altair for working.\n    \"\"\"\n\n    vega_url = 'https://cdn.jsdelivr.net/npm/vega@' + v5.SCHEMA_VERSION\n    vega_lib_url = 'https://cdn.jsdelivr.net/npm/vega-lib'\n    vega_lite_url = 'https://cdn.jsdelivr.net/npm/vega-lite@' + alt.SCHEMA_VERSION\n    vega_embed_url = 'https://cdn.jsdelivr.net/npm/vega-embed@3'\n    noext = \"?noext\"\n    \n    paths = {\n        'vega': vega_url + noext,\n        'vega-lib': vega_lib_url + noext,\n        'vega-lite': vega_lite_url + noext,\n        'vega-embed': vega_embed_url + noext\n    }\n    \n    workaround = f\"\"\"    requirejs.config({{\n        baseUrl: 'https://cdn.jsdelivr.net/npm/',\n        paths: {paths}\n    }});\n    \"\"\"\n    \n    return workaround\n    \n\ndef add_autoincrement(render_func):\n    # Keep track of unique <div/> IDs\n    cache = {}\n    def wrapped(chart, id=\"vega-chart\", autoincrement=True):\n        if autoincrement:\n            if id in cache:\n                counter = 1 + cache[id]\n                cache[id] = counter\n            else:\n                cache[id] = 0\n            actual_id = id if cache[id] == 0 else id + '-' + str(cache[id])\n        else:\n            if id not in cache:\n                cache[id] = 0\n            actual_id = id\n        return render_func(chart, id=actual_id)\n    # Cache will stay outside and \n    return wrapped\n           \n\n@add_autoincrement\ndef render(chart, id=\"vega-chart\"):\n    \"\"\"\n    Helper function to plot altair visualizations.\n    \"\"\"\n    chart_str = \"\"\"\n    <div id=\"{id}\"></div><script>\n    require([\"vega-embed\"], function(vg_embed) {{\n        const spec = {chart};     \n        vg_embed(\"#{id}\", spec, {{defaultStyle: true}}).catch(console.warn);\n        console.log(\"anything?\");\n    }});\n    console.log(\"really...anything?\");\n    </script>\n    \"\"\"\n    return HTML(\n        chart_str.format(\n            id=id,\n            chart=json.dumps(chart) if isinstance(chart, dict) else chart.to_json(indent=None)\n        )\n    )\n    \n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage(deep=True).sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                c_prec = df[col].apply(lambda x: np.finfo(x).precision).max()\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max and c_prec == np.finfo(np.float32).precision:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n    \n\n@jit\ndef fast_auc(y_true, y_prob):\n    \"\"\"\n    fast roc_auc computation: https://www.kaggle.com/c/microsoft-malware-prediction/discussion/76013\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_true = y_true[np.argsort(y_prob)]\n    nfalse = 0\n    auc = 0\n    n = len(y_true)\n    for i in range(n):\n        y_i = y_true[i]\n        nfalse += (1 - y_i)\n        auc += y_i * nfalse\n    auc /= (nfalse * (n - nfalse))\n    return auc\n\n\ndef eval_auc(y_true, y_pred):\n    \"\"\"\n    Fast auc eval function for lgb.\n    \"\"\"\n    return 'auc', fast_auc(y_true, y_pred), True\n\n\ndef group_mean_log_mae(y_true, y_pred, types, floor=1e-9):\n    \"\"\"\n    Fast metric computation for this competition: https://www.kaggle.com/c/champs-scalar-coupling\n    Code is from this kernel: https://www.kaggle.com/uberkinder/efficient-metric\n    \"\"\"\n    maes = (y_true-y_pred).abs().groupby(types).mean()\n    return np.log(maes.map(lambda x: max(x, floor))).mean()\n    \n\ndef train_model_regression(X, X_test, y, params, folds=None, model_type='lgb', eval_metric='mae', columns=None, plot_feature_importance=False, model=None,\n                               verbose=10000, early_stopping_rounds=200, n_estimators=50000, splits=None, n_folds=3):\n    \"\"\"\n    A function to train a variety of regression models.\n    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n    \n    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: y - target\n    :params: folds - folds to split data\n    :params: model_type - type of model to use\n    :params: eval_metric - metric to use\n    :params: columns - columns to use. If None - use all columns\n    :params: plot_feature_importance - whether to plot feature importance of LGB\n    :params: model - sklearn model, works only for \"sklearn\" model type\n    \n    \"\"\"\n    columns = X.columns if columns is None else columns\n    X_test = X_test[columns]\n    splits = folds.split(X) if splits is None else splits\n    n_splits = folds.n_splits if splits is None else n_folds\n    \n    # to set up scoring parameters\n    metrics_dict = {'mae': {'lgb_metric_name': 'mae',\n                        'catboost_metric_name': 'MAE',\n                        'sklearn_scoring_function': metrics.mean_absolute_error},\n                    'group_mae': {'lgb_metric_name': 'mae',\n                        'catboost_metric_name': 'MAE',\n                        'scoring_function': group_mean_log_mae},\n                    'mse': {'lgb_metric_name': 'mse',\n                        'catboost_metric_name': 'MSE',\n                        'sklearn_scoring_function': metrics.mean_squared_error}\n                    }\n\n    \n    result_dict = {}\n    \n    # out-of-fold predictions on train data\n    oof = np.zeros(len(X))\n    \n    # averaged predictions on train data\n    prediction = np.zeros(len(X_test))\n    \n    # list of scores on folds\n    scores = []\n    feature_importance = pd.DataFrame()\n    \n    # split and train on folds\n    for fold_n, (train_index, valid_index) in enumerate(splits):\n        if verbose:\n            print(f'Fold {fold_n + 1} started at {time.ctime()}')\n        if type(X) == np.ndarray:\n            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n            y_train, y_valid = y[train_index], y[valid_index]\n        else:\n            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n            \n        if model_type == 'lgb':\n            model = lgb.LGBMRegressor(**params, n_estimators = n_estimators, n_jobs = -1)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n            \n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=verbose, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n        \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            \n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n            print('')\n            \n            y_pred = model.predict(X_test).reshape(-1,)\n        \n        if model_type == 'cat':\n            model = CatBoostRegressor(iterations=20000,  eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n                                      loss_function=metrics_dict[eval_metric]['catboost_metric_name'])\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test)\n        \n        oof[valid_index] = y_pred_valid.reshape(-1,)\n        if eval_metric != 'group_mae':\n            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n        else:\n            scores.append(metrics_dict[eval_metric]['scoring_function'](y_valid, y_pred_valid, X_valid['type']))\n\n        prediction += y_pred    \n        \n        if model_type == 'lgb' and plot_feature_importance:\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction /= n_splits\n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    result_dict['oof'] = oof\n    result_dict['prediction'] = prediction\n    result_dict['scores'] = scores\n    \n    if model_type == 'lgb':\n        if plot_feature_importance:\n            feature_importance[\"importance\"] /= n_splits\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n            \n            result_dict['feature_importance'] = feature_importance\n        \n    return result_dict\n    \n\n\ndef train_model_classification(X, X_test, y, params, folds, model_type='lgb', eval_metric='auc', columns=None, plot_feature_importance=False, model=None,\n                               verbose=10000, early_stopping_rounds=200, n_estimators=50000, splits=None, n_folds=3, averaging='usual', n_jobs=-1):\n    \"\"\"\n    A function to train a variety of classification models.\n    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n    \n    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: y - target\n    :params: folds - folds to split data\n    :params: model_type - type of model to use\n    :params: eval_metric - metric to use\n    :params: columns - columns to use. If None - use all columns\n    :params: plot_feature_importance - whether to plot feature importance of LGB\n    :params: model - sklearn model, works only for \"sklearn\" model type\n    \n    \"\"\"\n    columns = X.columns if columns is None else columns\n    n_splits = folds.n_splits if splits is None else n_folds\n    X_test = X_test[columns]\n    \n    # to set up scoring parameters\n    metrics_dict = {'auc': {'lgb_metric_name': eval_auc,\n                        'catboost_metric_name': 'AUC',\n                        'sklearn_scoring_function': metrics.roc_auc_score},\n                    }\n    \n    result_dict = {}\n    if averaging == 'usual':\n        # out-of-fold predictions on train data\n        oof = np.zeros((len(X), 1))\n\n        # averaged predictions on train data\n        prediction = np.zeros((len(X_test), 1))\n        \n    elif averaging == 'rank':\n        # out-of-fold predictions on train data\n        oof = np.zeros((len(X), 1))\n\n        # averaged predictions on train data\n        prediction = np.zeros((len(X_test), 1))\n\n    \n    # list of scores on folds\n    scores = []\n    feature_importance = pd.DataFrame()\n    \n    # split and train on folds\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n        print(f'Fold {fold_n + 1} started at {time.ctime()}')\n        if type(X) == np.ndarray:\n            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n            y_train, y_valid = y[train_index], y[valid_index]\n        else:\n            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n            \n        if model_type == 'lgb':\n            model = lgb.LGBMClassifier(**params, n_estimators=n_estimators, n_jobs = n_jobs)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n            \n            y_pred_valid = model.predict_proba(X_valid)[:, 1]\n            y_pred = model.predict_proba(X_test, num_iteration=model.best_iteration_)[:, 1]\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=n_estimators, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=verbose, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n        \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            \n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n            print('')\n            \n            y_pred = model.predict_proba(X_test)\n        \n        if model_type == 'cat':\n            model = CatBoostClassifier(iterations=n_estimators, eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n                                      loss_function=Logloss)\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test)\n        \n        if averaging == 'usual':\n            \n            oof[valid_index] = y_pred_valid.reshape(-1, 1)\n            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n            \n            prediction += y_pred.reshape(-1, 1)\n\n        elif averaging == 'rank':\n                                  \n            oof[valid_index] = y_pred_valid.reshape(-1, 1)\n            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n                                  \n            prediction += pd.Series(y_pred).rank().values.reshape(-1, 1)        \n        \n        if model_type == 'lgb' and plot_feature_importance:\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction /= n_splits\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    result_dict['oof'] = oof\n    result_dict['prediction'] = prediction\n    result_dict['scores'] = scores\n    \n    if model_type == 'lgb':\n        if plot_feature_importance:\n            feature_importance[\"importance\"] /= n_splits\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n            \n            result_dict['feature_importance'] = feature_importance\n            result_dict['top_columns'] = cols\n        \n    return result_dict\n\n# setting up altair\nworkaround = prepare_altair()\nHTML(\"\".join((\n    \"<script>\",\n    workaround,\n    \"</script>\",\n)))\n","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:47:19.000109Z","iopub.execute_input":"2021-08-27T18:47:19.000524Z","iopub.status.idle":"2021-08-27T18:47:19.134071Z","shell.execute_reply.started":"2021-08-27T18:47:19.000484Z","shell.execute_reply":"2021-08-27T18:47:19.133423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# General imports\nimport numpy as np\nimport pandas as pd\nimport os, warnings, datetime, math\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\npd.set_option('display.max_columns', None)\npd.set_option('display.float_format', lambda x: '%.5f' % x)\npd.set_option('display.width', 200)\npd.set_option('display.max_rows', 500)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:47:19.135365Z","iopub.execute_input":"2021-08-27T18:47:19.135782Z","iopub.status.idle":"2021-08-27T18:47:19.141038Z","shell.execute_reply.started":"2021-08-27T18:47:19.13575Z","shell.execute_reply":"2021-08-27T18:47:19.140167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# datalarin yuklenmesi\ntrain_transaction = pd.read_csv(\"../input/ieee-fraud-detection/train_transaction.csv\")\ntest_transaction  = pd.read_csv(\"../input/ieee-fraud-detection/test_transaction.csv\")\n\ntest_identity = pd.read_csv(\"../input/ieee-fraud-detection/test_identity.csv\")\ntrain_identity = pd.read_csv(\"../input/ieee-fraud-detection/train_identity.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:47:19.142904Z","iopub.execute_input":"2021-08-27T18:47:19.143465Z","iopub.status.idle":"2021-08-27T18:48:20.629777Z","shell.execute_reply.started":"2021-08-27T18:47:19.143424Z","shell.execute_reply":"2021-08-27T18:48:20.628297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#kullanilan memory azaltmak icin fonksiyon tanimladik\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:48:20.632271Z","iopub.execute_input":"2021-08-27T18:48:20.632659Z","iopub.status.idle":"2021-08-27T18:48:20.647759Z","shell.execute_reply.started":"2021-08-27T18:48:20.632624Z","shell.execute_reply":"2021-08-27T18:48:20.646701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# kolon isimlerini esitliyoruz, alt ve orta cizgi farki var\ntest_identity.columns=train_identity.columns","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:48:20.649084Z","iopub.execute_input":"2021-08-27T18:48:20.649549Z","iopub.status.idle":"2021-08-27T18:48:20.665965Z","shell.execute_reply.started":"2021-08-27T18:48:20.649517Z","shell.execute_reply":"2021-08-27T18:48:20.664835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# trainleri birleştirdik:\ntrain = pd.merge(train_transaction,train_identity, on=\"TransactionID\", how=\"left\")\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:48:20.667376Z","iopub.execute_input":"2021-08-27T18:48:20.667959Z","iopub.status.idle":"2021-08-27T18:48:28.143933Z","shell.execute_reply.started":"2021-08-27T18:48:20.667914Z","shell.execute_reply":"2021-08-27T18:48:28.142869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# testleri birleştirdik:\ntest = pd.merge(test_transaction,test_identity, on=\"TransactionID\", how=\"left\")\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:48:28.146594Z","iopub.execute_input":"2021-08-27T18:48:28.146906Z","iopub.status.idle":"2021-08-27T18:48:34.423034Z","shell.execute_reply.started":"2021-08-27T18:48:28.146877Z","shell.execute_reply":"2021-08-27T18:48:34.421875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test ve train datasetinin memory kullanimini azalttik\ntrain = reduce_mem_usage(train)\ntrain.head()\ntest = reduce_mem_usage(test)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:48:34.425778Z","iopub.execute_input":"2021-08-27T18:48:34.426259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#kullanmayacagimizi siliyoruz\ndel train_identity, train_transaction, test_identity, test_transaction","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Feature selection icin daha sonra kullanacagiz\ntest_columns=test.columns\ntrain_columns=train.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"one_value_cols = [col for col in train.columns if train[col].nunique() <= 1]\none_value_cols_test = [col for col in test.columns if test[col].nunique() <= 1]\none_value_cols == one_value_cols_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SUNUMDA GÖSTER!!!!\n\n# New feature: number of NaN's\n#We have plenty of NaN's in this dataset and they can have a significant effect so' \\\n#' why don't we use them? I am adding a new column to the dateset, which will contain\n#\"a number of NaN for each row. So if a row\n#(a single training example) contain, say, 10 NaNs, a new feature's value for this row will be 10.\n\n# rowlardaki na'ları sayıyor ve fe olarak tutuyor.\ntrain['nulls'] = train.isnull().sum(axis=1)\ntest['nulls'] = test.isnull().sum(axis=1)\ntrain['nulls'][0:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## GÖSTER!!\n\n#############card1#########################\n#card1 in frekanslarini encode edip yeni bir feature urettik\n\ntemp = pd.concat([train['card1'], test['card1']], ignore_index=True).value_counts(dropna=False)\ntrain['card1_count'] = train['card1'].map(temp)\ntest['card1_count'] = test['card1'].map(temp)\ndel temp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## GÖSTER!!\n\n#############card2#########################\n#card2 in frekanslarini encode edip yeni bir feature urettik\n\ntemp = pd.concat([train['card2'], test['card2']], ignore_index=True).value_counts(dropna=False)\ntrain['card2_count'] = train['card2'].map(temp)\ntest['card2_count'] = test['card2'].map(temp)\ndel temp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## GÖSTER!!!!\n\n#####################TransactionAmount############\n# Transaction amount'un virgulden sonraki kismiyla yeni bir feature urettik, fraudlarda virgulden sonrasi daha fazla\n#Decimal part of transaction amount.\n#Lenght of the decimal part of transaction amount. What does it mean?\n# Well, if lenght is 1 or 2 signs it is totaly understandable - it might be cents.\n# But what is wrong with a decimal part's lenght being 3 and more sings? Maybe it is due to a currency convertion?\n\ntrain['TransactionAmt_decimal'] = ((train['TransactionAmt'] - train['TransactionAmt'].astype(int)) * 1000).astype(int)\ntest['TransactionAmt_decimal'] = ((test['TransactionAmt'] - test['TransactionAmt'].astype(int)) * 1000).astype(int)\n# plot_numerical('TransactionAmt_decimal')\n#train['TransactionAmt_decimal'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#virgulden sonraki uzunluk bir feature olarak yaratildi\ntrain['TransactionAmt_decimal_lenght'] = train['TransactionAmt'].astype(str).str.split('.', expand=True)[1].str.len()\ntest['TransactionAmt_decimal_lenght'] = test['TransactionAmt'].astype(str).str.split('.', expand=True)[1].str.len()\ntrain['TransactionAmt_decimal'][0:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['TransactionAmt_decimal_lenght'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.groupby('TransactionAmt_decimal_lenght').mean()[\"isFraud\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##card1 >> card1 could be the Issuer Identification Number (IIN), encoded as a categorical variable with LabelEncoder.\n## card4 >> mastercard visa\n\n# >> card1 ve  card4 e gore 'TransactionAmt' un mean standart sapma alip, yeni feature urettik, train icin\n\ntrain['TransactionAmt_to_mean_card1'] = train['TransactionAmt'] / train.groupby(['card1'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_mean_card4'] = train['TransactionAmt'] / train.groupby(['card4'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_std_card1'] = train['TransactionAmt'] / train.groupby(['card1'])['TransactionAmt'].transform('std')\ntrain['TransactionAmt_to_std_card4'] = train['TransactionAmt'] / train.groupby(['card4'])['TransactionAmt'].transform('std')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##  ayni islem test icin\ntest['TransactionAmt_to_mean_card1'] = test['TransactionAmt'] / test.groupby(['card1'])['TransactionAmt'].transform('mean')\ntest['TransactionAmt_to_mean_card4'] = test['TransactionAmt'] / test.groupby(['card4'])['TransactionAmt'].transform('mean')\ntest['TransactionAmt_to_std_card1'] = test['TransactionAmt'] / test.groupby(['card1'])['TransactionAmt'].transform('std')\ntest['TransactionAmt_to_std_card4'] = test['TransactionAmt'] / test.groupby(['card4'])['TransactionAmt'].transform('std')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## card1 ve  card4 e gore gore id_02 ye gore group by alip, mean standart sapmasini aldik\n#id_02 sanirim balance bilgisi\ntrain['id_02_to_mean_card1'] = train['id_02'] / train.groupby(['card1'])['id_02'].transform('mean')\ntrain['id_02_to_mean_card4'] = train['id_02'] / train.groupby(['card4'])['id_02'].transform('mean')\ntrain['id_02_to_std_card1'] = train['id_02'] / train.groupby(['card1'])['id_02'].transform('std')\ntrain['id_02_to_std_card4'] = train['id_02'] / train.groupby(['card4'])['id_02'].transform('std')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ayni islemi test datasina da gerceklestirdik\ntest['id_02_to_mean_card1'] = test['id_02'] / test.groupby(['card1'])['id_02'].transform('mean')\ntest['id_02_to_mean_card4'] = test['id_02'] / test.groupby(['card4'])['id_02'].transform('mean')\ntest['id_02_to_std_card1'] = test['id_02'] / test.groupby(['card1'])['id_02'].transform('std')\ntest['id_02_to_std_card4'] = test['id_02'] / test.groupby(['card4'])['id_02'].transform('std')","metadata":{"execution":{"iopub.status.idle":"2021-08-27T18:51:41.800542Z","shell.execute_reply.started":"2021-08-27T18:51:41.568047Z","shell.execute_reply":"2021-08-27T18:51:41.799323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## D li featurelar zamana ait featurelar, zaman odaginda, group by card1 ve card4 alinarak\n## mean ve std ler feature haline getirildi\n\ntrain['D15_to_mean_card1'] = train['D15'] / train.groupby(['card1'])['D15'].transform('mean')\ntrain['D15_to_mean_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('mean')\ntrain['D15_to_std_card1'] = train['D15'] / train.groupby(['card1'])['D15'].transform('std')\ntrain['D15_to_std_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('std')\n\ntest['D15_to_mean_card1'] = test['D15'] / test.groupby(['card1'])['D15'].transform('mean')\ntest['D15_to_mean_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('mean')\ntest['D15_to_std_card1'] = test['D15'] / test.groupby(['card1'])['D15'].transform('std')\ntest['D15_to_std_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('std')\n\ntrain['D15_to_mean_addr1'] = train['D15'] / train.groupby(['addr1'])['D15'].transform('mean')\ntrain['D15_to_mean_addr2'] = train['D15'] / train.groupby(['addr2'])['D15'].transform('mean')\ntrain['D15_to_std_addr1'] = train['D15'] / train.groupby(['addr1'])['D15'].transform('std')\ntrain['D15_to_std_addr2'] = train['D15'] / train.groupby(['addr2'])['D15'].transform('std')\n\ntest['D15_to_mean_addr1'] = test['D15'] / test.groupby(['addr1'])['D15'].transform('mean')\ntest['D15_to_mean_addr2'] = test['D15'] / test.groupby(['addr2'])['D15'].transform('mean')\ntest['D15_to_std_addr1'] = test['D15'] / test.groupby(['addr1'])['D15'].transform('std')\ntest['D15_to_std_addr2'] = test['D15'] / test.groupby(['addr2'])['D15'].transform('std')","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:51:41.803544Z","iopub.execute_input":"2021-08-27T18:51:41.803936Z","iopub.status.idle":"2021-08-27T18:51:42.633276Z","shell.execute_reply.started":"2021-08-27T18:51:41.8039Z","shell.execute_reply":"2021-08-27T18:51:42.632299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# email,noktaya gore 3 parcaya bolundu, test ve train icin\ntrain[['P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3']] = train['P_emaildomain'].str.split('.', expand=True)\ntrain[['R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3']] = train['R_emaildomain'].str.split('.', expand=True)\ntest[['P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3']] = test['P_emaildomain'].str.split('.', expand=True)\ntest[['R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3']] = test['R_emaildomain'].str.split('.', expand=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:51:42.63473Z","iopub.execute_input":"2021-08-27T18:51:42.635023Z","iopub.status.idle":"2021-08-27T18:51:50.644069Z","shell.execute_reply.started":"2021-08-27T18:51:42.634997Z","shell.execute_reply":"2021-08-27T18:51:50.643114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Tarayici guncelleyenleri yakalayabilecegimiz bir feature urettik\n\n#https://www.kaggle.com/yasagure/fraud-makers-may-be-earnest-people-about-browser?scriptVersionId=21010669\n## This result suggests that people who update browser are more possibly fraud makers than people who are lazy about browser.\n## Fraud makers may be earnest people.\n\n## #https://en.wikipedia.org/wiki/List_of_Microsoft_Windows_versions(2019/8/10)\n## #windows 10 July 29, 2015\n## #windows 8.1 October 17, 2013\n## #Windows 8 October 26, 2012\n## #Windows 7 October 22, 2009\n## #windows vista November 8, 2006\n## #windows XP October 25, 2001\n\n\n## iOS\n## https://en.wikipedia.org/wiki/IOS_version_history\n##\n## 12.4 July 22, 2019\n## 10.3.4 July 22, 2019\n## 10.3.3 July 19, 2017\n## 9.3.6 July 22, 2019\n## 9.3.5 August 25, 2016\n## 7.1.2 June 30, 2014\n## 6.1.6 February 21, 2014\n## 5.1.1 May 7, 2012\n## 4.2.1 November 22, 2010\n## 3.1.3 February 2, 2010","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:51:50.64549Z","iopub.execute_input":"2021-08-27T18:51:50.645812Z","iopub.status.idle":"2021-08-27T18:51:50.650092Z","shell.execute_reply.started":"2021-08-27T18:51:50.645783Z","shell.execute_reply":"2021-08-27T18:51:50.648902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = np.zeros(train.shape[0])\na = np.nan\ntrain[\"lastest_browser\"] = a\n\ntrain.lastest_browser[train[\"id_31\"]==\"samsung browser 7.0\"]=1\ntrain.lastest_browser[train[\"id_31\"]==\"opera 53.0\"]=1\ntrain.lastest_browser[train[\"id_31\"]==\"mobile safari 10.0\"]=1\ntrain.lastest_browser[train[\"id_31\"]==\"google search application 49.0\"]=1\ntrain.lastest_browser[train[\"id_31\"]==\"firefox 60.0\"]=1\ntrain.lastest_browser[train[\"id_31\"]==\"edge 17.0\"]=1\ntrain.lastest_browser[train[\"id_31\"]==\"chrome 69.0\"]=1\ntrain.lastest_browser[train[\"id_31\"]==\"chrome 67.0 for android\"]=1\ntrain.lastest_browser[train[\"id_31\"]==\"chrome 63.0\"]=1\ntrain.lastest_browser[train[\"id_31\"]==\"chrome 63.0 for android\"]=1\ntrain.lastest_browser[train[\"id_31\"]==\"chrome 63.0 for ios\"]=1\ntrain.lastest_browser[train[\"id_31\"]==\"chrome 64.0\"]=1\ntrain.lastest_browser[train[\"id_31\"]==\"chrome 64.0 for android\"]=1\ntrain.lastest_browser[train[\"id_31\"]==\"chrome 64.0 for ios\"]=1\ntrain.lastest_browser[train[\"id_31\"]==\"chrome 65.0\"]=1\ntrain.lastest_browser[train[\"id_31\"]==\"chrome 65.0 for android\"]=1\ntrain.lastest_browser[train[\"id_31\"]==\"chrome 65.0 for ios\"]=1\ntrain.lastest_browser[train[\"id_31\"]==\"chrome 66.0\"]=1\ntrain.lastest_browser[train[\"id_31\"]==\"chrome 66.0 for android\"]=1\ntrain.lastest_browser[train[\"id_31\"]==\"chrome 66.0 for ios\"]=1","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:51:50.651764Z","iopub.execute_input":"2021-08-27T18:51:50.652207Z","iopub.status.idle":"2021-08-27T18:51:51.447132Z","shell.execute_reply.started":"2021-08-27T18:51:50.652172Z","shell.execute_reply":"2021-08-27T18:51:51.446066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del a","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:51:51.448589Z","iopub.execute_input":"2021-08-27T18:51:51.448888Z","iopub.status.idle":"2021-08-27T18:51:51.45415Z","shell.execute_reply.started":"2021-08-27T18:51:51.448861Z","shell.execute_reply":"2021-08-27T18:51:51.45316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.groupby(\"lastest_browser\").mean()[\"isFraud\"]","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:51:51.45566Z","iopub.execute_input":"2021-08-27T18:51:51.456013Z","iopub.status.idle":"2021-08-27T18:51:55.160839Z","shell.execute_reply.started":"2021-08-27T18:51:51.455982Z","shell.execute_reply":"2021-08-27T18:51:55.159578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#aynisini test icin yaparsak:\n\nb = np.zeros(test.shape[0])\nb = np.nan\ntest[\"lastest_browser\"] = b\n\ntest.lastest_browser[test[\"id_31\"]==\"samsung browser 7.0\"]=1\ntest.lastest_browser[test[\"id_31\"]==\"opera 53.0\"]=1\ntest.lastest_browser[test[\"id_31\"]==\"mobile safari 10.0\"]=1\ntest.lastest_browser[test[\"id_31\"]==\"google search application 49.0\"]=1\ntest.lastest_browser[test[\"id_31\"]==\"firefox 60.0\"]=1\ntest.lastest_browser[test[\"id_31\"]==\"edge 17.0\"]=1\ntest.lastest_browser[test[\"id_31\"]==\"chrome 69.0\"]=1\ntest.lastest_browser[test[\"id_31\"]==\"chrome 67.0 for android\"]=1\ntest.lastest_browser[test[\"id_31\"]==\"chrome 63.0\"]=1\ntest.lastest_browser[test[\"id_31\"]==\"chrome 63.0 for android\"]=1\ntest.lastest_browser[test[\"id_31\"]==\"chrome 63.0 for ios\"]=1\ntest.lastest_browser[test[\"id_31\"]==\"chrome 64.0\"]=1\ntest.lastest_browser[test[\"id_31\"]==\"chrome 64.0 for android\"]=1\ntest.lastest_browser[test[\"id_31\"]==\"chrome 64.0 for ios\"]=1\ntest.lastest_browser[test[\"id_31\"]==\"chrome 65.0\"]=1\ntest.lastest_browser[test[\"id_31\"]==\"chrome 65.0 for android\"]=1\ntest.lastest_browser[test[\"id_31\"]==\"chrome 65.0 for ios\"]=1\ntest.lastest_browser[test[\"id_31\"]==\"chrome 66.0\"]=1\ntest.lastest_browser[test[\"id_31\"]==\"chrome 66.0 for android\"]=1\ntest.lastest_browser[test[\"id_31\"]==\"chrome 66.0 for ios\"]=1","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:51:55.16222Z","iopub.execute_input":"2021-08-27T18:51:55.16252Z","iopub.status.idle":"2021-08-27T18:51:55.88807Z","shell.execute_reply.started":"2021-08-27T18:51:55.162491Z","shell.execute_reply":"2021-08-27T18:51:55.887007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.lastest_browser.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:51:55.889346Z","iopub.execute_input":"2021-08-27T18:51:55.889649Z","iopub.status.idle":"2021-08-27T18:51:55.900233Z","shell.execute_reply.started":"2021-08-27T18:51:55.889621Z","shell.execute_reply":"2021-08-27T18:51:55.8992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del b","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:51:55.901693Z","iopub.execute_input":"2021-08-27T18:51:55.901991Z","iopub.status.idle":"2021-08-27T18:51:55.907176Z","shell.execute_reply.started":"2021-08-27T18:51:55.901964Z","shell.execute_reply":"2021-08-27T18:51:55.906273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"############Tarayici#####################\n# tarayicilari siniflandirdik\n#There are a lot of categories in id_31.\n##here, I change bool data to numeric data by multipling 1( * 1)\n\ntrain[\"chrome\"]=train['id_31'].str.contains('chrome')*1\ntrain[\"samsung_browser\"] = train['id_31'].str.contains('samsung')*1\ntrain[\"opera\"] = train['id_31'].str.contains('opera')*1\ntrain[\"ie\"] = train['id_31'].str.contains('ie')*1\ntrain[\"google_browser\"] = train['id_31'].str.contains('google')*1\ntrain[\"firefox\"] = train['id_31'].str.contains('firefox')*1\ntrain[\"edge\"] = train['id_31'].str.contains('edge')*1\n\n# when it comes to Android browser, I will use different way because chrome has \"chrome for android\"\n\ntrain[\"android_browser\"] = train['id_31'].str.contains('android browser')*1\ntrain[\"android_browser\"] = train['id_31'].str.contains('android webview')*1\ntrain[\"android_browser\"] = train['id_31'].str.contains('Generic/Android')*1\ntrain[\"android_browser\"] = train['id_31'].str.contains('Generic/Android 7.0')*1","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:51:55.909237Z","iopub.execute_input":"2021-08-27T18:51:55.909608Z","iopub.status.idle":"2021-08-27T18:51:58.990644Z","shell.execute_reply.started":"2021-08-27T18:51:55.909577Z","shell.execute_reply":"2021-08-27T18:51:58.989571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# chrome browser user is more possibly fraud maker than people who do not use it.\n# This may be because chrome has a lot of extension\ntrain.groupby(\"chrome\").mean().isFraud","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:51:58.993031Z","iopub.execute_input":"2021-08-27T18:51:58.993336Z","iopub.status.idle":"2021-08-27T18:52:02.916241Z","shell.execute_reply.started":"2021-08-27T18:51:58.993307Z","shell.execute_reply":"2021-08-27T18:52:02.915435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#aynisini test icin yaptik\n\n\ntest[\"chrome\"]=test['id_31'].str.contains('chrome')*1\ntest[\"samsung_browser\"] = test['id_31'].str.contains('samsung')*1\ntest[\"opera\"] = test['id_31'].str.contains('opera')*1\ntest[\"ie\"] = test['id_31'].str.contains('ie')*1\ntest[\"google_browser\"] = test['id_31'].str.contains('google')*1\ntest[\"firefox\"] = test['id_31'].str.contains('firefox')*1\ntest[\"edge\"] = test['id_31'].str.contains('edge')*1\n\n# when it comes to Android browser, I will use different way because chrome has \"chrome for android\"\n\ntest[\"android_browser\"] = train['id_31'].str.contains('android browser')*1\ntest[\"android_browser\"] = train['id_31'].str.contains('android webview')*1\ntest[\"android_browser\"] = train['id_31'].str.contains('Generic/Android')*1\ntest[\"android_browser\"] = train['id_31'].str.contains('Generic/Android 7.0')*1","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:52:02.91734Z","iopub.execute_input":"2021-08-27T18:52:02.917759Z","iopub.status.idle":"2021-08-27T18:52:05.870125Z","shell.execute_reply.started":"2021-08-27T18:52:02.917729Z","shell.execute_reply":"2021-08-27T18:52:05.869397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#id_01 i null olanlardan feature urettik\ntrain[\"null_id_01\"]=train.id_01.isnull()\ntrain.groupby(\"null_id_01\").mean()[\"isFraud\"]","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:52:05.871187Z","iopub.execute_input":"2021-08-27T18:52:05.871704Z","iopub.status.idle":"2021-08-27T18:52:09.789236Z","shell.execute_reply.started":"2021-08-27T18:52:05.871665Z","shell.execute_reply":"2021-08-27T18:52:09.788474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#aynisini test icin yaptik\ntest[\"null_id_01\"]=test.id_01.isnull()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:52:09.790491Z","iopub.execute_input":"2021-08-27T18:52:09.791071Z","iopub.status.idle":"2021-08-27T18:52:09.799694Z","shell.execute_reply.started":"2021-08-27T18:52:09.791038Z","shell.execute_reply":"2021-08-27T18:52:09.798824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########################################\n\n#Huawei has a lot of series of device. Blade is also one of them.\n\ndef change(some):\n    if some != some:\n        return np.nan\n    elif \"Huawei\" in some:\n        return 1\n    elif \"Blade\" in some:\n        return 1\n\n    else:\n        return 0\n\ntrain[\"Huawei\"] = train[\"DeviceInfo\"].map(change)\ntrain[\"Huawei\"][0:5]","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:52:09.800999Z","iopub.execute_input":"2021-08-27T18:52:09.801478Z","iopub.status.idle":"2021-08-27T18:52:10.097513Z","shell.execute_reply.started":"2021-08-27T18:52:09.801431Z","shell.execute_reply":"2021-08-27T18:52:10.096273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.groupby(\"Huawei\").mean().isFraud","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:52:10.099204Z","iopub.execute_input":"2021-08-27T18:52:10.099651Z","iopub.status.idle":"2021-08-27T18:52:13.089286Z","shell.execute_reply.started":"2021-08-27T18:52:10.09961Z","shell.execute_reply":"2021-08-27T18:52:13.088474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[\"ZTE\"] = test['DeviceInfo'].str.contains('ZTE')*1\ntrain[\"ZTE\"] = train['DeviceInfo'].str.contains('ZTE')*1\n\ntrain.groupby(\"ZTE\").mean().isFraud\n# ZTE\n# False   0.07248\n# True    0.18750","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:52:13.090905Z","iopub.execute_input":"2021-08-27T18:52:13.091371Z","iopub.status.idle":"2021-08-27T18:52:17.561357Z","shell.execute_reply.started":"2021-08-27T18:52:13.091324Z","shell.execute_reply":"2021-08-27T18:52:17.560357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"iOS Device\"] = train['DeviceInfo'].str.contains('iOS Device')*1\ntrain[\"Lanix\"] = train['DeviceInfo'].str.contains('Ilium')*1\ntrain[\"ASUS\"] = train['DeviceInfo'].str.contains('ASUS')*1\ntrain[\"Hisense\"] = train['DeviceInfo'].str.contains('Hisense')*1\ntrain[\"Desire\"] = train['DeviceInfo'].str.contains('Desire')*1\ntrain[\"LG\"] = train['DeviceInfo'].str.contains('LG')*1\ntrain[\"Alcatel\"] = train['DeviceInfo'].str.contains('Alcatel')*1","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:52:17.562652Z","iopub.execute_input":"2021-08-27T18:52:17.56295Z","iopub.status.idle":"2021-08-27T18:52:19.510873Z","shell.execute_reply.started":"2021-08-27T18:52:17.56292Z","shell.execute_reply":"2021-08-27T18:52:19.509959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[\"Huawei\"] = test[\"DeviceInfo\"].map(change)\ntest[\"iOS Device\"] = test['DeviceInfo'].str.contains('iOS Device')*1\ntest[\"Lanix\"] = test['DeviceInfo'].str.contains('Ilium')*1\ntest[\"ASUS\"] = test['DeviceInfo'].str.contains('ASUS')*1\ntest[\"Hisense\"] = test['DeviceInfo'].str.contains('Hisense')*1\ntest[\"Desire\"] = test['DeviceInfo'].str.contains('Desire')*1\ntest[\"LG\"] = test['DeviceInfo'].str.contains('LG')*1\ntest[\"Alcatel\"] = test['DeviceInfo'].str.contains('Alcatel')*1","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:52:19.51224Z","iopub.execute_input":"2021-08-27T18:52:19.512556Z","iopub.status.idle":"2021-08-27T18:52:21.374955Z","shell.execute_reply.started":"2021-08-27T18:52:19.512528Z","shell.execute_reply":"2021-08-27T18:52:21.373861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"HTC\"] = train['DeviceInfo'].str.contains('HTC')*1\ntest[\"HTC\"] = test['DeviceInfo'].str.contains('HTC')*1\ntrain[\"Sony\"] = train['DeviceInfo'].str.contains('XT')*1\ntest[\"Sony\"] = test['DeviceInfo'].str.contains('XT')*1\ntrain['Linux'] = train['DeviceInfo'].str.contains('Linux')*1\ntest['Linux'] = test['DeviceInfo'].str.contains('Linux')*1\n                                               ","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:52:21.376465Z","iopub.execute_input":"2021-08-27T18:52:21.376788Z","iopub.status.idle":"2021-08-27T18:52:22.856768Z","shell.execute_reply.started":"2021-08-27T18:52:21.376761Z","shell.execute_reply":"2021-08-27T18:52:22.855822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['TransactionAmt'] = np.log1p(train['TransactionAmt'])\ntest['TransactionAmt'] = np.log1p(test['TransactionAmt'])","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:52:22.858047Z","iopub.execute_input":"2021-08-27T18:52:22.858312Z","iopub.status.idle":"2021-08-27T18:52:22.899194Z","shell.execute_reply.started":"2021-08-27T18:52:22.858286Z","shell.execute_reply":"2021-08-27T18:52:22.898209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', 'scranton.edu': 'other', 'optonline.net': 'other',\n          'hotmail.co.uk': 'microsoft', 'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',\n          'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', 'aim.com': 'aol', 'hotmail.de': 'microsoft',\n          'centurylink.net': 'centurylink', 'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', \n          'gmx.de': 'other', 'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', 'protonmail.com': 'other',\n          'hotmail.fr': 'microsoft', 'windstream.net': 'other', 'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo',\n          'yahoo.de': 'yahoo', 'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n          'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft', 'verizon.net': 'yahoo',\n          'msn.com': 'microsoft', 'q.com': 'centurylink', 'prodigy.net.mx': 'att', 'frontier.com': 'yahoo',\n          'anonymous.com': 'other', 'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo',\n          'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', 'bellsouth.net': 'other',\n          'embarqmail.com': 'centurylink', 'cableone.net': 'other', 'hotmail.es': 'microsoft', 'mac.com': 'apple',\n          'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', 'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other',\n          'cox.net': 'other', 'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\nus_emails = ['gmail', 'net', 'edu']\n\nfor c in ['P_emaildomain', 'R_emaildomain']:\n    train[c + '_bin'] = train[c].map(emails)\n    test[c + '_bin'] = test[c].map(emails)\n    \n    train[c + '_suffix'] = train[c].map(lambda x: str(x).split('.')[-1])\n    test[c + '_suffix'] = test[c].map(lambda x: str(x).split('.')[-1])\n    \n    train[c + '_suffix'] = train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n    test[c + '_suffix'] = test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:52:22.900526Z","iopub.execute_input":"2021-08-27T18:52:22.900823Z","iopub.status.idle":"2021-08-27T18:52:25.889088Z","shell.execute_reply.started":"2021-08-27T18:52:22.900794Z","shell.execute_reply":"2021-08-27T18:52:25.888288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[['P_emaildomain_bin','P_emaildomain_suffix' ,\"P_emaildomain_1\",\"P_emaildomain_2\",\"P_emaildomain_3\"]].head()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:52:25.890142Z","iopub.execute_input":"2021-08-27T18:52:25.890557Z","iopub.status.idle":"2021-08-27T18:52:27.315013Z","shell.execute_reply.started":"2021-08-27T18:52:25.890528Z","shell.execute_reply":"2021-08-27T18:52:27.313976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.drop(['P_emaildomain_1','P_emaildomain_suffix','R_emaildomain_1','R_emaildomain_suffix'],axis=1,inplace=True)\ntest.drop(['P_emaildomain_1','P_emaildomain_suffix','R_emaildomain_1','R_emaildomain_suffix'],axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:52:27.316999Z","iopub.execute_input":"2021-08-27T18:52:27.317274Z","iopub.status.idle":"2021-08-27T18:52:30.241723Z","shell.execute_reply.started":"2021-08-27T18:52:27.317248Z","shell.execute_reply":"2021-08-27T18:52:30.240748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p = 'P_emaildomain'\nr = 'R_emaildomain'\nuknown = 'email_not_provided'\n\ndef setDomain(df):\n    df[p] = df[p].fillna(uknown)\n    df[r] = df[r].fillna(uknown)\n    \n    # Check if P_emaildomain matches R_emaildomain\n    df['email_check'] = np.where((df[p]==df[r])&(df[p]!=uknown),1,0)\n\n    df[p+'_prefix'] = df[p].apply(lambda x: x.split('.')[0])\n    df[r+'_prefix'] = df[r].apply(lambda x: x.split('.')[0])\n    \n    return df\n    \ntrain=setDomain(train)\ntest=setDomain(test)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:52:30.242989Z","iopub.execute_input":"2021-08-27T18:52:30.24327Z","iopub.status.idle":"2021-08-27T18:52:31.692426Z","shell.execute_reply.started":"2021-08-27T18:52:30.243244Z","shell.execute_reply":"2021-08-27T18:52:31.691331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:52:31.693663Z","iopub.execute_input":"2021-08-27T18:52:31.693952Z","iopub.status.idle":"2021-08-27T18:52:31.974466Z","shell.execute_reply.started":"2021-08-27T18:52:31.693926Z","shell.execute_reply":"2021-08-27T18:52:31.973661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## ve ['TransactionDT'] bu bilgi tutulacak sortlama icin\nstartdate = datetime.datetime.strptime('2017-11-01', '%Y-%m-%d')\ntrain['TransactionDT_transformed'] = train['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\ntest['TransactionDT_transformed'] = test['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\ntrain['dow'] = train['TransactionDT_transformed'].dt.dayofweek\ntrain['hour'] = train['TransactionDT_transformed'].dt.hour\ntest['dow'] = test['TransactionDT_transformed'].dt.dayofweek\ntest['hour'] = test['TransactionDT_transformed'].dt.hour","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:52:31.975556Z","iopub.execute_input":"2021-08-27T18:52:31.97597Z","iopub.status.idle":"2021-08-27T18:52:34.00448Z","shell.execute_reply.started":"2021-08-27T18:52:31.975935Z","shell.execute_reply":"2021-08-27T18:52:34.003513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:52:34.005874Z","iopub.execute_input":"2021-08-27T18:52:34.006165Z","iopub.status.idle":"2021-08-27T18:52:34.011241Z","shell.execute_reply.started":"2021-08-27T18:52:34.006139Z","shell.execute_reply":"2021-08-27T18:52:34.010575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:52:34.012578Z","iopub.execute_input":"2021-08-27T18:52:34.012882Z","iopub.status.idle":"2021-08-27T18:52:34.026301Z","shell.execute_reply.started":"2021-08-27T18:52:34.012854Z","shell.execute_reply":"2021-08-27T18:52:34.025304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in ['card4', 'card6', 'ProductCD']:\n    print('Encoding', col)\n    temp_df = pd.concat([train[[col]], test[[col]]])\n    col_encoded = temp_df[col].value_counts().to_dict()   \n    train[col] = train[col].map(col_encoded)\n    test[col]  = test[col].map(col_encoded)\n    print(col_encoded)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:52:34.027858Z","iopub.execute_input":"2021-08-27T18:52:34.028285Z","iopub.status.idle":"2021-08-27T18:52:40.040319Z","shell.execute_reply.started":"2021-08-27T18:52:34.028242Z","shell.execute_reply":"2021-08-27T18:52:40.039247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########################### M columns\n#################################################################################\n# Converting Strings to ints(or floats if nan in column)\n\nfor col in ['M1','M2','M3','M5','M6','M7','M8','M9']:\n    train[col] = train[col].map({'T':1, 'F':0})\n    test[col]  = test[col].map({'T':1, 'F':0})\n\nfor col in ['M4']:\n    print('Encoding', col)\n    temp_df = pd.concat([train[[col]], test[[col]]])\n    col_encoded = temp_df[col].value_counts().to_dict()   \n    train[col] = train[col].map(col_encoded)\n    test[col]  = test[col].map(col_encoded)\n    print(col_encoded)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:52:40.04156Z","iopub.execute_input":"2021-08-27T18:52:40.04186Z","iopub.status.idle":"2021-08-27T18:52:49.430199Z","shell.execute_reply.started":"2021-08-27T18:52:40.041832Z","shell.execute_reply":"2021-08-27T18:52:49.429266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########################### Identity columns\n#################################################################################\n\ndef minify_identity_df(df):\n\n    df['id_12'] = df['id_12'].map({'Found':1, 'NotFound':0})\n    df['id_15'] = df['id_15'].map({'New':2, 'Found':1, 'Unknown':0})\n    df['id_16'] = df['id_16'].map({'Found':1, 'NotFound':0})\n\n    df['id_23'] = df['id_23'].map({'TRANSPARENT':4, 'IP_PROXY':3, 'IP_PROXY:ANONYMOUS':2, 'IP_PROXY:HIDDEN':1})\n\n    df['id_27'] = df['id_27'].map({'Found':1, 'NotFound':0})\n    df['id_28'] = df['id_28'].map({'New':2, 'Found':1})\n\n    df['id_29'] = df['id_29'].map({'Found':1, 'NotFound':0})\n\n    df['id_35'] = df['id_35'].map({'T':1, 'F':0})\n    df['id_36'] = df['id_36'].map({'T':1, 'F':0})\n    df['id_37'] = df['id_37'].map({'T':1, 'F':0})\n    df['id_38'] = df['id_38'].map({'T':1, 'F':0})\n\n    df['id_34'] = df['id_34'].fillna(':0')\n    df['id_34'] = df['id_34'].apply(lambda x: x.split(':')[1]).astype(np.int8)\n    df['id_34'] = np.where(df['id_34']==0, np.nan, df['id_34'])\n    \n    df['id_33'] = df['id_33'].fillna('0x0')\n    df['id_33_0'] = df['id_33'].apply(lambda x: x.split('x')[0]).astype(int)\n    df['id_33_1'] = df['id_33'].apply(lambda x: x.split('x')[1]).astype(int)\n    df['id_33'] = np.where(df['id_33']=='0x0', np.nan, df['id_33'])\n\n    df['DeviceType'].map({'desktop':1, 'mobile':0})\n    return df\n\ntrain = minify_identity_df(train)\ntest = minify_identity_df(test)\n\nfor col in ['id_33']:\n    train[col] = train[col].fillna('unseen_before_label')\n    test[col]  = test[col].fillna('unseen_before_label')\n    \n    le = LabelEncoder()\n    le.fit(list(train[col])+list(test[col]))\n    train[col] = le.transform(train[col])\n    test[col]  = le.transform(test[col])","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:52:49.431499Z","iopub.execute_input":"2021-08-27T18:52:49.431782Z","iopub.status.idle":"2021-08-27T18:53:03.85799Z","shell.execute_reply.started":"2021-08-27T18:52:49.431754Z","shell.execute_reply":"2021-08-27T18:53:03.856826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_train_cols=train.columns\nnew_test_cols=train.columns","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:53:03.859348Z","iopub.execute_input":"2021-08-27T18:53:03.85973Z","iopub.status.idle":"2021-08-27T18:53:03.863875Z","shell.execute_reply.started":"2021-08-27T18:53:03.859697Z","shell.execute_reply":"2021-08-27T18:53:03.862779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#yeni urettigimiz featurelarin listesi tutalim\nnew_cols_= [col for col in train.columns if col not in train_columns]\nnew_cols_\n","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:53:03.86521Z","iopub.execute_input":"2021-08-27T18:53:03.865524Z","iopub.status.idle":"2021-08-27T18:53:03.882439Z","shell.execute_reply.started":"2021-08-27T18:53:03.865497Z","shell.execute_reply":"2021-08-27T18:53:03.881422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(new_cols_)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:53:03.883497Z","iopub.execute_input":"2021-08-27T18:53:03.883864Z","iopub.status.idle":"2021-08-27T18:53:03.896152Z","shell.execute_reply.started":"2021-08-27T18:53:03.883836Z","shell.execute_reply":"2021-08-27T18:53:03.895338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# yeni uretilen featurelar icerine denk geliyorsa cikarilacak\n\n#cols = [col for col in train.columns if col not in useless_cols]\nmany_null_cols = [col for col in train_columns if train[col].isnull().sum() / train.shape[0] > 0.9]\nmany_null_cols_test = [col for col in test_columns if test[col].isnull().sum() / test.shape[0] > 0.9]","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:53:03.897648Z","iopub.execute_input":"2021-08-27T18:53:03.898241Z","iopub.status.idle":"2021-08-27T18:53:07.175444Z","shell.execute_reply.started":"2021-08-27T18:53:03.898194Z","shell.execute_reply":"2021-08-27T18:53:07.174573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(train[train_columns].isnull().sum() /train[train_columns].shape[0] * 100).sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:53:07.176641Z","iopub.execute_input":"2021-08-27T18:53:07.17714Z","iopub.status.idle":"2021-08-27T18:53:09.972298Z","shell.execute_reply.started":"2021-08-27T18:53:07.177091Z","shell.execute_reply":"2021-08-27T18:53:09.971523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"many_null_cols_test","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:53:09.973468Z","iopub.execute_input":"2021-08-27T18:53:09.973921Z","iopub.status.idle":"2021-08-27T18:53:09.978902Z","shell.execute_reply.started":"2021-08-27T18:53:09.973871Z","shell.execute_reply":"2021-08-27T18:53:09.978199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# yeni uretilen featurelar icerine denk geliyorsa cikarilacak\nbig_top_value_cols = [col for col in train_columns if train[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\nbig_top_value_cols_test = [col for col in test_columns if test[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:53:09.979907Z","iopub.execute_input":"2021-08-27T18:53:09.980289Z","iopub.status.idle":"2021-08-27T18:53:21.69367Z","shell.execute_reply.started":"2021-08-27T18:53:09.980261Z","shell.execute_reply":"2021-08-27T18:53:21.692927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#big_top_value_cols","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:53:21.694759Z","iopub.execute_input":"2021-08-27T18:53:21.695173Z","iopub.status.idle":"2021-08-27T18:53:21.697986Z","shell.execute_reply.started":"2021-08-27T18:53:21.695144Z","shell.execute_reply":"2021-08-27T18:53:21.697308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#big_top_value_cols_test","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:53:21.698957Z","iopub.execute_input":"2021-08-27T18:53:21.699422Z","iopub.status.idle":"2021-08-27T18:53:21.710682Z","shell.execute_reply.started":"2021-08-27T18:53:21.69936Z","shell.execute_reply":"2021-08-27T18:53:21.709922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols_to_drop = list(set(many_null_cols + many_null_cols_test + big_top_value_cols + big_top_value_cols_test + one_value_cols+ one_value_cols_test))","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:53:21.711874Z","iopub.execute_input":"2021-08-27T18:53:21.712301Z","iopub.status.idle":"2021-08-27T18:53:21.721904Z","shell.execute_reply.started":"2021-08-27T18:53:21.712273Z","shell.execute_reply":"2021-08-27T18:53:21.721153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols_to_drop.remove('isFraud')\nlen(cols_to_drop)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:53:21.722951Z","iopub.execute_input":"2021-08-27T18:53:21.723344Z","iopub.status.idle":"2021-08-27T18:53:21.736067Z","shell.execute_reply.started":"2021-08-27T18:53:21.723316Z","shell.execute_reply":"2021-08-27T18:53:21.735313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.drop(cols_to_drop, axis=1)\ntest = test.drop(cols_to_drop, axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:53:21.737354Z","iopub.execute_input":"2021-08-27T18:53:21.737864Z","iopub.status.idle":"2021-08-27T18:53:23.147279Z","shell.execute_reply.started":"2021-08-27T18:53:21.737832Z","shell.execute_reply":"2021-08-27T18:53:23.146477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test ve train datasetinin memory kullanimini azalttik\ntrain = reduce_mem_usage(train)\ntrain.head()\ntest = reduce_mem_usage(test)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:53:23.152935Z","iopub.execute_input":"2021-08-27T18:53:23.153458Z","iopub.status.idle":"2021-08-27T18:53:35.206301Z","shell.execute_reply.started":"2021-08-27T18:53:23.15339Z","shell.execute_reply":"2021-08-27T18:53:35.205382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Hangileri eklenecek???\n\ncat_cols = ['id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29',\n            'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo','P_emaildomain',\n            'R_emaildomain', 'card1', 'card2', 'card3',  'card5', 'addr1', 'addr2', 'M1', 'M2', 'M3', 'M5', 'M6', 'M7', 'M8', 'M9',\n            'P_emaildomain_2', 'P_emaildomain_3','R_emaildomain_2', 'R_emaildomain_3','P_emaildomain_bin','R_emaildomain_bin','P_emaildomain_prefix','R_emaildomain_prefix']\nfor col in cat_cols:\n    if col in train.columns:\n        le = LabelEncoder()\n        le.fit(list(train[col].astype(str).values) + list(test[col].astype(str).values))\n        train[col] = le.transform(list(train[col].astype(str).values))\n        test[col] = le.transform(list(test[col].astype(str).values)) \n        \n        \n#P_emaildomain_bin\tR_emaildomain_bin\temail_check\tP_emaildomain_prefix\tR_emaildomain_prefix\n#train.drop(['P_emaildomain_1','P_emaildomain_suffix','R_emaildomain_1','R_emaildomain_suffix'],axis=1,inplace=True)\n#P_emaildomain_bin\tR_emaildomain_bin\temail_check\tP_emaildomain_prefix\tR_emaildomain_prefix","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:53:35.207866Z","iopub.execute_input":"2021-08-27T18:53:35.208151Z","iopub.status.idle":"2021-08-27T18:55:30.405324Z","shell.execute_reply.started":"2021-08-27T18:53:35.208125Z","shell.execute_reply":"2021-08-27T18:55:30.404215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"P_emaildomain_prefix\"].dtypes","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:55:30.406973Z","iopub.execute_input":"2021-08-27T18:55:30.407421Z","iopub.status.idle":"2021-08-27T18:55:30.414609Z","shell.execute_reply.started":"2021-08-27T18:55:30.407363Z","shell.execute_reply":"2021-08-27T18:55:30.413517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nX = train.sort_values('TransactionDT').drop(['isFraud', 'TransactionDT', 'TransactionID','TransactionDT_transformed'], axis=1)\ny = train.sort_values('TransactionDT')['isFraud']\n#X_test = test.sort_values('TransactionDT').drop(['TransactionDT', 'TransactionID'], axis=1)\nX_test = test.drop(['TransactionDT', 'TransactionID','TransactionDT_transformed'], axis=1)\ndel train\ntest = test[[\"TransactionDT\", 'TransactionID']]","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:55:30.416131Z","iopub.execute_input":"2021-08-27T18:55:30.416581Z","iopub.status.idle":"2021-08-27T18:55:35.439944Z","shell.execute_reply.started":"2021-08-27T18:55:30.416549Z","shell.execute_reply":"2021-08-27T18:55:35.438947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=X.loc[0:118108,:]","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:55:35.441491Z","iopub.execute_input":"2021-08-27T18:55:35.441939Z","iopub.status.idle":"2021-08-27T18:55:35.459315Z","shell.execute_reply.started":"2021-08-27T18:55:35.441899Z","shell.execute_reply":"2021-08-27T18:55:35.458182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y=y.loc[0:118108]","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:55:35.460739Z","iopub.execute_input":"2021-08-27T18:55:35.461038Z","iopub.status.idle":"2021-08-27T18:55:35.479304Z","shell.execute_reply.started":"2021-08-27T18:55:35.461011Z","shell.execute_reply":"2021-08-27T18:55:35.478353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X[\"ZTE\"] = X[\"ZTE\"].map({'T':1, 'F':0})","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:55:35.480854Z","iopub.execute_input":"2021-08-27T18:55:35.481279Z","iopub.status.idle":"2021-08-27T18:55:35.583403Z","shell.execute_reply.started":"2021-08-27T18:55:35.481235Z","shell.execute_reply":"2021-08-27T18:55:35.582631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test[\"ZTE\"] = X_test[\"ZTE\"].map({'T':1, 'F':0})","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:55:35.584625Z","iopub.execute_input":"2021-08-27T18:55:35.585218Z","iopub.status.idle":"2021-08-27T18:55:35.935344Z","shell.execute_reply.started":"2021-08-27T18:55:35.585173Z","shell.execute_reply":"2021-08-27T18:55:35.934593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# by https://www.kaggle.com/dimartinot\ndef clean_inf_nan(df):\n    return df.replace([np.inf, -np.inf], np.nan)   \n\n# Cleaning infinite values to NaN\nX = clean_inf_nan(X)\nX_test = clean_inf_nan(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:55:35.93662Z","iopub.execute_input":"2021-08-27T18:55:35.937369Z","iopub.status.idle":"2021-08-27T18:55:43.909354Z","shell.execute_reply.started":"2021-08-27T18:55:35.93732Z","shell.execute_reply":"2021-08-27T18:55:43.908593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nimport gc, datetime, random\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import roc_auc_score\n\npd.options.display.max_rows = 4000\n\ndef seed_everything(seed=0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n\nSEED = 42\nseed_everything(SEED)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:55:43.910645Z","iopub.execute_input":"2021-08-27T18:55:43.911268Z","iopub.status.idle":"2021-08-27T18:55:43.918716Z","shell.execute_reply.started":"2021-08-27T18:55:43.911226Z","shell.execute_reply":"2021-08-27T18:55:43.917937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {'num_leaves': 546,\n          'min_child_weight': 0.03454472573214212,\n          'feature_fraction': 0.1797454081646243,\n          'bagging_fraction': 0.2181193142567742,\n          'min_data_in_leaf': 106,\n          'objective': 'binary',\n          'max_depth': -1,\n          'learning_rate': 0.005883242363721497,\n          \"boosting_type\": \"gbdt\",\n          \"bagging_seed\": 11,\n          \"metric\": 'auc',\n          \"verbosity\": -1,\n          'reg_alpha': 0.3299927210061127,\n          'reg_lambda': 0.3885237330340494,\n          'random_state': 42,\n}","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:55:43.91986Z","iopub.execute_input":"2021-08-27T18:55:43.920309Z","iopub.status.idle":"2021-08-27T18:55:43.93116Z","shell.execute_reply.started":"2021-08-27T18:55:43.92027Z","shell.execute_reply":"2021-08-27T18:55:43.930423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nNFOLDS = 5\nfolds = KFold(n_splits=NFOLDS)\n\ncolumns = X.columns\nsplits = folds.split(X, y)\ny_preds = np.zeros(X_test.shape[0])\ny_oof = np.zeros(X.shape[0])\nscore = 0\n\nfeature_importances = pd.DataFrame()\nfeature_importances['feature'] = columns\n  \nfor fold_n, (train_index, valid_index) in enumerate(splits):\n    X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n    \n    dtrain = lgb.Dataset(X_train, label=y_train)\n    dvalid = lgb.Dataset(X_valid, label=y_valid)\n\n    clf = lgb.train(params, dtrain, 2000, valid_sets = [dtrain, dvalid], verbose_eval=200, early_stopping_rounds=500)\n    \n    feature_importances[f'fold_{fold_n + 1}'] = clf.feature_importance()\n    \n    y_pred_valid = clf.predict(X_valid)\n    y_oof[valid_index] = y_pred_valid\n    print(f\"Fold {fold_n + 1} | AUC: {roc_auc_score(y_valid, y_pred_valid)}\")\n    \n    score += roc_auc_score(y_valid, y_pred_valid) / NFOLDS\n    y_preds += clf.predict(X_test) / NFOLDS\n    \n    del X_train, X_valid, y_train, y_valid\n    gc.collect()\n    \nprint(f\"\\nMean AUC = {score}\")\nprint(f\"Out of folds AUC = {roc_auc_score(y, y_oof)}\")","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:55:43.932313Z","iopub.execute_input":"2021-08-27T18:55:43.932615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission1 = pd.read_csv('/kaggle/input/ieee-fraud-detection/sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission1['isFraud'] = y_preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_importances['average'] = feature_importances[[f'fold_{fold_n + 1}' for fold_n in range(folds.n_splits)]].mean(axis=1)\nfeature_importances.to_csv('feature_importances1.csv')\n\nplt.figure(figsize=(16, 16))\nsns.barplot(data=feature_importances.sort_values(by='average', ascending=False).head(50), x='average', y='feature');\nplt.title('50 TOP feature importance over {} folds average'.format(folds.n_splits));","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}