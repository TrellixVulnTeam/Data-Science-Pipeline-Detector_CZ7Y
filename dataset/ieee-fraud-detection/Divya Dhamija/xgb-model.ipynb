{"cells":[{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"BUILD95 = True\nBUILD96 = True\n\nimport numpy as np, pandas as pd, os, gc\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# COLUMNS WITH STRINGS\nstr_type = ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain','M1', 'M2', 'M3', 'M4','M5',\n            'M6', 'M7', 'M8', 'M9', 'id_12', 'id_15', 'id_16', 'id_23', 'id_27', 'id_28', 'id_29', 'id_30', \n            'id_31', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo']\nstr_type += ['id-12', 'id-15', 'id-16', 'id-23', 'id-27', 'id-28', 'id-29', 'id-30', \n            'id-31', 'id-33', 'id-34', 'id-35', 'id-36', 'id-37', 'id-38']\n\n# FIRST 53 COLUMNS\ncols = ['TransactionID', 'TransactionDT', 'TransactionAmt',\n       'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6',\n       'addr1', 'addr2', 'dist1', 'dist2', 'P_emaildomain', 'R_emaildomain',\n       'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11',\n       'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8',\n       'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'M1', 'M2', 'M3', 'M4',\n       'M5', 'M6', 'M7', 'M8', 'M9']\n\n# V COLUMNS TO LOAD DECIDED BY CORRELATION EDA\n# https://www.kaggle.com/cdeotte/eda-for-columns-v-and-id\nv =  [1, 3, 4, 6, 8, 11]\nv += [13, 14, 17, 20, 23, 26, 27, 30]\nv += [36, 37, 40, 41, 44, 47, 48]\nv += [54, 56, 59, 62, 65, 67, 68, 70]\nv += [76, 78, 80, 82, 86, 88, 89, 91]\n\n#v += [96, 98, 99, 104] #relates to groups, no NAN \nv += [107, 108, 111, 115, 117, 120, 121, 123] # maybe group, no NAN\nv += [124, 127, 129, 130, 136] # relates to groups, no NAN\n\n# LOTS OF NAN BELOW\nv += [138, 139, 142, 147, 156, 162] #b1\nv += [165, 160, 166] #b1\nv += [178, 176, 173, 182] #b2\nv += [187, 203, 205, 207, 215] #b2\nv += [169, 171, 175, 180, 185, 188, 198, 210, 209] #b2\nv += [218, 223, 224, 226, 228, 229, 235] #b3\nv += [240, 258, 257, 253, 252, 260, 261] #b3\nv += [264, 266, 267, 274, 277] #b3\nv += [220, 221, 234, 238, 250, 271] #b3\n\nv += [294, 284, 285, 286, 291, 297] # relates to grous, no NAN\nv += [303, 305, 307, 309, 310, 320] # relates to groups, no NAN\nv += [281, 283, 289, 296, 301, 314] # relates to groups, no NAN\n#v += [332, 325, 335, 338] # b4 lots NAN\n\ncols += ['V'+str(x) for x in v]\ndtypes = {}\nfor c in cols+['id_0'+str(x) for x in range(1,10)]+['id_'+str(x) for x in range(10,34)]+\\\n    ['id-0'+str(x) for x in range(1,10)]+['id-'+str(x) for x in range(10,34)]:\n        dtypes[c] = 'float32'\nfor c in str_type: dtypes[c] = 'category'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# LOAD TRAIN\nX_train = pd.read_csv('../input/ieee-fraud-detection/train_transaction.csv',index_col='TransactionID', dtype=dtypes, usecols=cols+['isFraud'])\ntrain_id = pd.read_csv('../input/ieee-fraud-detection/train_identity.csv',index_col='TransactionID', dtype=dtypes)\nX_train = X_train.merge(train_id, how='left', left_index=True, right_index=True)\n# LOAD TEST\nX_test = pd.read_csv('../input/ieee-fraud-detection/test_transaction.csv',index_col='TransactionID', dtype=dtypes, usecols=cols)\ntest_id = pd.read_csv('../input/ieee-fraud-detection/test_identity.csv',index_col='TransactionID', dtype=dtypes)\nfix = {o:n for o, n in zip(test_id.columns, train_id.columns)}\ntest_id.rename(columns=fix, inplace=True)\nX_test = X_test.merge(test_id, how='left', left_index=True, right_index=True)\n# TARGET\ny_train = X_train['isFraud'].copy()\ndel train_id, test_id, X_train['isFraud']; x = gc.collect()\n# PRINT STATUS\nprint('Train shape',X_train.shape,'test shape',X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PLOT ORIGINAL D\nplt.figure(figsize=(15,5))\nplt.scatter(X_train.TransactionDT,X_train.D15)\nplt.title('Original D15')\nplt.xlabel('Time')\nplt.ylabel('D15')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NORMALIZE D COLUMNS\nfor i in range(1,16):\n    if i in [1,2,3,5,9]: continue\n    X_train['D'+str(i)] =  X_train['D'+str(i)] - X_train.TransactionDT/np.float32(24*60*60)\n    X_test['D'+str(i)] = X_test['D'+str(i)] - X_test.TransactionDT/np.float32(24*60*60) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PLOT TRANSFORMED D\nplt.figure(figsize=(15,5))\nplt.scatter(X_train.TransactionDT,X_train.D15)\nplt.title('Transformed D15')\nplt.xlabel('Time')\nplt.ylabel('D15n')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%time\n# LABEL ENCODE AND MEMORY REDUCE\nfor i,f in enumerate(X_train.columns):\n    # FACTORIZE CATEGORICAL VARIABLES\n    if (np.str(X_train[f].dtype)=='category')|(X_train[f].dtype=='object'): \n        df_comb = pd.concat([X_train[f],X_test[f]],axis=0)\n        df_comb,_ = df_comb.factorize(sort=True)\n        if df_comb.max()>32000: print(f,'needs int32')\n        X_train[f] = df_comb[:len(X_train)].astype('int16')\n        X_test[f] = df_comb[len(X_train):].astype('int16')\n    # SHIFT ALL NUMERICS POSITIVE. SET NAN to -1\n    elif f not in ['TransactionAmt','TransactionDT']:\n        mn = np.min((X_train[f].min(),X_test[f].min()))\n        X_train[f] -= np.float32(mn)\n        X_test[f] -= np.float32(mn)\n        X_train[f].fillna(-1,inplace=True)\n        X_test[f].fillna(-1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# FREQUENCY ENCODE TOGETHER\ndef encode_FE(df1, df2, cols):\n    for col in cols:\n        df = pd.concat([df1[col],df2[col]])\n        vc = df.value_counts(dropna=True, normalize=True).to_dict()\n        vc[-1] = -1\n        nm = col+'_FE'\n        df1[nm] = df1[col].map(vc)\n        df1[nm] = df1[nm].astype('float32')\n        df2[nm] = df2[col].map(vc)\n        df2[nm] = df2[nm].astype('float32')\n        print(nm,', ',end='')\n        \n# LABEL ENCODE\ndef encode_LE(col,train=X_train,test=X_test,verbose=True):\n    df_comb = pd.concat([train[col],test[col]],axis=0)\n    df_comb,_ = df_comb.factorize(sort=True)\n    nm = col\n    if df_comb.max()>32000: \n        train[nm] = df_comb[:len(train)].astype('int32')\n        test[nm] = df_comb[len(train):].astype('int32')\n    else:\n        train[nm] = df_comb[:len(train)].astype('int16')\n        test[nm] = df_comb[len(train):].astype('int16')\n    del df_comb; x=gc.collect()\n    if verbose: print(nm,', ',end='')\n        \n# GROUP AGGREGATION MEAN AND STD\n# https://www.kaggle.com/kyakovlev/ieee-fe-with-some-eda\ndef encode_AG(main_columns, uids, aggregations=['mean'], train_df=X_train, test_df=X_test, \n              fillna=True, usena=False):\n    # AGGREGATION OF MAIN WITH UID FOR GIVEN STATISTICS\n    for main_column in main_columns:  \n        for col in uids:\n            for agg_type in aggregations:\n                new_col_name = main_column+'_'+col+'_'+agg_type\n                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n                if usena: temp_df.loc[temp_df[main_column]==-1,main_column] = np.nan\n                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n                                                        columns={agg_type: new_col_name})\n\n                temp_df.index = list(temp_df[col])\n                temp_df = temp_df[new_col_name].to_dict()   \n\n                train_df[new_col_name] = train_df[col].map(temp_df).astype('float32')\n                test_df[new_col_name]  = test_df[col].map(temp_df).astype('float32')\n                \n                if fillna:\n                    train_df[new_col_name].fillna(-1,inplace=True)\n                    test_df[new_col_name].fillna(-1,inplace=True)\n                \n                print(\"'\"+new_col_name+\"'\",', ',end='')\n                \n# COMBINE FEATURES\ndef encode_CB(col1,col2,df1=X_train,df2=X_test):\n    nm = col1+'_'+col2\n    df1[nm] = df1[col1].astype(str)+'_'+df1[col2].astype(str)\n    df2[nm] = df2[col1].astype(str)+'_'+df2[col2].astype(str) \n    encode_LE(nm,verbose=False)\n    print(nm,', ',end='')\n    \n# GROUP AGGREGATION NUNIQUE\ndef encode_AG2(main_columns, uids, train_df=X_train, test_df=X_test):\n    for main_column in main_columns:  \n        for col in uids:\n            comb = pd.concat([train_df[[col]+[main_column]],test_df[[col]+[main_column]]],axis=0)\n            mp = comb.groupby(col)[main_column].agg(['nunique'])['nunique'].to_dict()\n            train_df[col+'_'+main_column+'_ct'] = train_df[col].map(mp).astype('float32')\n            test_df[col+'_'+main_column+'_ct'] = test_df[col].map(mp).astype('float32')\n            print(col+'_'+main_column+'_ct, ',end='')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# TRANSACTION AMT CENTS\nX_train['cents'] = (X_train['TransactionAmt'] - np.floor(X_train['TransactionAmt'])).astype('float32')\nX_test['cents'] = (X_test['TransactionAmt'] - np.floor(X_test['TransactionAmt'])).astype('float32')\nprint('cents, ', end='')\n# FREQUENCY ENCODE: ADDR1, CARD1, CARD2, CARD3, P_EMAILDOMAIN\nencode_FE(X_train,X_test,['addr1','card1','card2','card3','P_emaildomain'])\n# COMBINE COLUMNS CARD1+ADDR1, CARD1+ADDR1+P_EMAILDOMAIN\nencode_CB('card1','addr1')\nencode_CB('card1_addr1','P_emaildomain')\n# FREQUENCY ENOCDE\nencode_FE(X_train,X_test,['card1_addr1','card1_addr1_P_emaildomain'])\n# GROUP AGGREGATE\nencode_AG(['TransactionAmt','D9','D11'],['card1','card1_addr1','card1_addr1_P_emaildomain'],['mean','std'],usena=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = list( X_train.columns )\ncols.remove('TransactionDT')\nfor c in ['D6','D7','D8','D9','D12','D13','D14']:\n    cols.remove(c)\n    \n# FAILED TIME CONSISTENCY TEST\nfor c in ['C3','M5','id_08','id_33']:\n    cols.remove(c)\nfor c in ['card4','id_07','id_14','id_21','id_30','id_32','id_34']:\n    cols.remove(c)\nfor c in ['id_'+str(x) for x in range(22,28)]:\n    cols.remove(c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('NOW USING THE FOLLOWING',len(cols),'FEATURES.')\nnp.array(cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CHRIS - TRAIN 75% PREDICT 25%\nidxT = X_train.index[:3*len(X_train)//4]\nidxV = X_train.index[3*len(X_train)//4:]\n\n# KONSTANTIN - TRAIN 4 SKIP 1 PREDICT 1 MONTH\n#idxT = X_train.index[:417559]\n#idxV = X_train.index[-89326:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nprint(\"XGBoost version:\", xgb.__version__)\n\nif BUILD95:\n    clf = xgb.XGBClassifier( \n        n_estimators=2000,\n        max_depth=12, \n        learning_rate=0.02, \n        subsample=0.8,\n        colsample_bytree=0.4, \n        missing=-1, \n        eval_metric='auc',\n        # USE CPU\n        #nthread=4,\n        #tree_method='hist' \n        # USE GPU\n        tree_method='gpu_hist' \n    )\n    h = clf.fit(X_train.loc[idxT,cols], y_train[idxT], \n        eval_set=[(X_train.loc[idxV,cols],y_train[idxV])],\n        verbose=50, early_stopping_rounds=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if BUILD95:\n\n    feature_imp = pd.DataFrame(sorted(zip(clf.feature_importances_,cols)), columns=['Value','Feature'])\n    plt.figure(figsize=(20, 10))\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).iloc[:50])\n    plt.title('XGB95 Most Important Features')\n    plt.tight_layout()\n    plt.show()\n    del clf, h; x=gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime\nSTART_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')\nX_train['DT_M'] = X_train['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\nX_train['DT_M'] = (X_train['DT_M'].dt.year-2017)*12 + X_train['DT_M'].dt.month \n\nX_test['DT_M'] = X_test['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\nX_test['DT_M'] = (X_test['DT_M'].dt.year-2017)*12 + X_test['DT_M'].dt.month ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if BUILD95:\n    oof = np.zeros(len(X_train))\n    preds = np.zeros(len(X_test))\n\n    skf = GroupKFold(n_splits=6)\n    for i, (idxT, idxV) in enumerate( skf.split(X_train, y_train, groups=X_train['DT_M']) ):\n        month = X_train.iloc[idxV]['DT_M'].iloc[0]\n        print('Fold',i,'withholding month',month)\n        print(' rows of train =',len(idxT),'rows of holdout =',len(idxV))\n        clf = xgb.XGBClassifier(\n            n_estimators=5000,\n            max_depth=12,\n            learning_rate=0.02,\n            subsample=0.8,\n            colsample_bytree=0.4,\n            missing=-1,\n            eval_metric='auc',\n            # USE CPU\n            #nthread=4,\n            #tree_method='hist'\n            # USE GPU\n            tree_method='gpu_hist' \n        )        \n        h = clf.fit(X_train[cols].iloc[idxT], y_train.iloc[idxT], \n                eval_set=[(X_train[cols].iloc[idxV],y_train.iloc[idxV])],\n                verbose=100, early_stopping_rounds=200)\n    \n        oof[idxV] += clf.predict_proba(X_train[cols].iloc[idxV])[:,1]\n        preds += clf.predict_proba(X_test[cols])[:,1]/skf.n_splits\n        del h, clf\n        x=gc.collect()\n    print('#'*20)\n    print ('XGB95 OOF CV=',roc_auc_score(y_train,oof))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if BUILD95:\n    plt.hist(oof,bins=100)\n    plt.ylim((0,5000))\n    plt.title('XGB OOF')\n    plt.show()\n\n    X_train['oof'] = oof\n    X_train.reset_index(inplace=True)\n    X_train[['TransactionID','oof']].to_csv('oof_xgb_95.csv')\n    X_train.set_index('TransactionID',drop=True,inplace=True)\n    \nelse: X_train['oof'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if BUILD95:\n    sample_submission = pd.read_csv('../input/ieee-fraud-detection/sample_submission.csv')\n    sample_submission.isFraud = preds\n    sample_submission.to_csv('sub_xgb_95.csv',index=False)\n\n    plt.hist(sample_submission.isFraud,bins=100)\n    plt.ylim((0,5000))\n    plt.title('XGB95 Submission')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train['day'] = X_train.TransactionDT / (24*60*60)\nX_train['uid'] = X_train.card1_addr1.astype(str)+'_'+np.floor(X_train.day-X_train.D1).astype(str)\n\nX_test['day'] = X_test.TransactionDT / (24*60*60)\nX_test['uid'] = X_test.card1_addr1.astype(str)+'_'+np.floor(X_test.day-X_test.D1).astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# FREQUENCY ENCODE UID\nencode_FE(X_train,X_test,['uid'])\n# AGGREGATE \nencode_AG(['TransactionAmt','D4','D9','D10','D15'],['uid'],['mean','std'],fillna=True,usena=True)\n# AGGREGATE\nencode_AG(['C'+str(x) for x in range(1,15) if x!=3],['uid'],['mean'],X_train,X_test,fillna=True,usena=True)\n# AGGREGATE\nencode_AG(['M'+str(x) for x in range(1,10)],['uid'],['mean'],fillna=True,usena=True)\n# AGGREGATE\nencode_AG2(['P_emaildomain','dist1','DT_M','id_02','cents'], ['uid'], train_df=X_train, test_df=X_test)\n# AGGREGATE\nencode_AG(['C14'],['uid'],['std'],X_train,X_test,fillna=True,usena=True)\n# AGGREGATE \nencode_AG2(['C13','V314'], ['uid'], train_df=X_train, test_df=X_test)\n# AGGREATE \nencode_AG2(['V127','V136','V309','V307','V320'], ['uid'], train_df=X_train, test_df=X_test)\n# NEW FEATURE\nX_train['outsider15'] = (np.abs(X_train.D1-X_train.D15)>3).astype('int8')\nX_test['outsider15'] = (np.abs(X_test.D1-X_test.D15)>3).astype('int8')\nprint('outsider15')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"cols = list( X_train.columns )\ncols.remove('TransactionDT')\nfor c in ['D6','D7','D8','D9','D12','D13','D14']:\n    cols.remove(c)\nfor c in ['oof','DT_M','day','uid']:\n    cols.remove(c)\n    \n# FAILED TIME CONSISTENCY TEST\nfor c in ['C3','M5','id_08','id_33']:\n    cols.remove(c)\nfor c in ['card4','id_07','id_14','id_21','id_30','id_32','id_34']:\n    cols.remove(c)\nfor c in ['id_'+str(x) for x in range(22,28)]:\n    cols.remove(c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('NOW USING THE FOLLOWING',len(cols),'FEATURES.')\nnp.array(cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CHRIS - TRAIN 75% PREDICT 25%\nidxT = X_train.index[:2*len(X_train)//4]\nidxV = X_train.index[2*len(X_train)//4:]\n\n# KONSTANTIN - TRAIN 4 SKIP 1 PREDICT 1 MONTH\n#idxT = X_train.index[:417559]\n#idxV = X_train.index[-89326:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if BUILD96:\n    clf = xgb.XGBClassifier( \n        n_estimators=2000,\n        max_depth=12, \n        learning_rate=0.02, \n        subsample=0.8,\n        colsample_bytree=0.4, \n        missing=-1, \n        eval_metric='auc',\n        #nthread=4,\n        #tree_method='hist' \n        tree_method='gpu_hist' \n    )\n    h = clf.fit(X_train.loc[idxT,cols], y_train[idxT], \n        eval_set=[(X_train.loc[idxV,cols],y_train[idxV])],\n        verbose=50, early_stopping_rounds=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if BUILD96:\n\n    feature_imp = pd.DataFrame(sorted(zip(clf.feature_importances_,cols)), columns=['Value','Feature'])\n\n    plt.figure(figsize=(20, 10))\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).iloc[:50])\n    plt.title('XGB96 Most Important')\n    plt.tight_layout()\n    plt.show()\n        \n    del clf, h; x=gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if BUILD96:\n    oof = np.zeros(len(X_train))\n    preds = np.zeros(len(X_test))\n\n    skf = GroupKFold(n_splits=6)\n    for i, (idxT, idxV) in enumerate( skf.split(X_train, y_train, groups=X_train['DT_M']) ):\n        month = X_train.iloc[idxV]['DT_M'].iloc[0]\n        print('Fold',i,'withholding month',month)\n        print(' rows of train =',len(idxT),'rows of holdout =',len(idxV))\n        clf = xgb.XGBClassifier(\n            n_estimators=5000,\n            max_depth=12,\n            learning_rate=0.02,\n            subsample=0.8,\n            colsample_bytree=0.4,\n            missing=-1,\n            eval_metric='auc',\n            # USE CPU\n            #nthread=4,\n            #tree_method='hist'\n            # USE GPU\n            tree_method='gpu_hist' \n        )        \n        h = clf.fit(X_train[cols].iloc[idxT], y_train.iloc[idxT], \n                eval_set=[(X_train[cols].iloc[idxV],y_train.iloc[idxV])],\n                verbose=100, early_stopping_rounds=200)\n    \n        oof[idxV] += clf.predict_proba(X_train[cols].iloc[idxV])[:,1]\n        preds += clf.predict_proba(X_test[cols])[:,1]/skf.n_splits\n        del h, clf\n        x=gc.collect()\n    print('#'*20)\n    print ('XGB96 OOF CV=',roc_auc_score(y_train,oof))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if BUILD96:\n    plt.hist(oof,bins=100)\n    plt.ylim((0,5000))\n    plt.title('XGB OOF')\n    plt.show()\n\n    X_train['oof'] = oof\n    X_train.reset_index(inplace=True)\n    X_train[['TransactionID','oof']].to_csv('oof_xgb_96.csv')\n    X_train.set_index('TransactionID',drop=True,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if BUILD96:\n    sample_submission = pd.read_csv('../input/ieee-fraud-detection/sample_submission.csv')\n    sample_submission.isFraud = preds\n    sample_submission.to_csv('sub_xgb_96.csv',index=False)\n\n    plt.hist(sample_submission.isFraud,bins=100)\n    plt.ylim((0,5000))\n    plt.title('XGB96 Submission')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test['isFraud'] = sample_submission.isFraud.values\nX_train['isFraud'] = y_train.values\ncomb = pd.concat([X_train[['isFraud']],X_test[['isFraud']]],axis=0)\n\nuids = pd.read_csv('/kaggle/input/ieee-submissions-and-uids/uids_v4_no_multiuid_cleaning..csv',usecols=['TransactionID','uid']).rename({'uid':'uid2'},axis=1)\ncomb = comb.merge(uids,on='TransactionID',how='left')\nmp = comb.groupby('uid2').isFraud.agg(['mean'])\ncomb.loc[comb.uid2>0,'isFraud'] = comb.loc[comb.uid2>0].uid2.map(mp['mean'])\n\nuids = pd.read_csv('/kaggle/input/ieee-submissions-and-uids/uids_v1_no_multiuid_cleaning.csv',usecols=['TransactionID','uid']).rename({'uid':'uid3'},axis=1)\ncomb = comb.merge(uids,on='TransactionID',how='left')\nmp = comb.groupby('uid3').isFraud.agg(['mean'])\ncomb.loc[comb.uid3>0,'isFraud'] = comb.loc[comb.uid3>0].uid3.map(mp['mean'])\n\nsample_submission.isFraud = comb.iloc[len(X_train):].isFraud.values\nsample_submission.to_csv('submission.csv')\nsample_submission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}