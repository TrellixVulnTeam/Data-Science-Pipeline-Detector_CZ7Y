{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nimport gc\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load and prepare data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"%%time\n\ntrain_transaction = pd.read_csv('../input/ieee-fraud-detection/train_transaction.csv', index_col='TransactionID')\ntrain_identity = pd.read_csv('../input/ieee-fraud-detection/train_identity.csv', index_col='TransactionID')\ntrain = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ndel train_transaction, train_identity\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.fillna(-999)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"many_same_values_columns_train = [c for c in train.columns if train[c].value_counts(normalize=True).values[0] > 0.9]\ncolumns_to_drop = many_same_values_columns_train\ncolumns_to_drop.remove('isFraud')\ntrain = train.drop(columns_to_drop, axis=1)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# From https://www.kaggle.com/pavelvpster/ieee-fraud-eda-lightgbm-baseline\n\nfrom sklearn.preprocessing import LabelEncoder\n\ndef encode_categorial_features_fit(df, columns_to_encode):\n    encoders = {}\n    for c in columns_to_encode:\n        if c in df.columns:\n            encoder = LabelEncoder()\n            encoder.fit(df[c].astype(str).values)\n            encoders[c] = encoder\n    return encoders\n\ndef encode_categorial_features_transform(df, encoders):\n    out = pd.DataFrame(index=df.index)\n    for c in encoders.keys():\n        if c in df.columns:\n            out[c] = encoders[c].transform(df[c].astype(str).values)\n    return out\n\ncategorial_features_columns = [\n    'id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21',\n    'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31',\n    'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38',\n    'DeviceType', 'DeviceInfo', 'ProductCD', 'P_emaildomain', 'R_emaildomain',\n    'card1', 'card2', 'card3', 'card4', 'card5', 'card6',\n    'addr1', 'addr2',\n    'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9',\n    'P_emaildomain_vendor', 'P_emaildomain_suffix', 'P_emaildomain_us',\n    'R_emaildomain_vendor', 'R_emaildomain_suffix', 'R_emaildomain_us'\n]\n\ncategorial_features_encoders = encode_categorial_features_fit(train, categorial_features_columns)\ntemp = encode_categorial_features_transform(train, categorial_features_encoders)\ncolumns_to_drop = list(set(categorial_features_columns) & set(train.columns))\ntrain = train.drop(columns_to_drop, axis=1).merge(temp, how='left', left_index=True, right_index=True)\ndel temp\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# From https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n#        else:\n#            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\n\ntrain = reduce_mem_usage(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train['isFraud'].copy()\nx_train = train.drop('isFraud', axis=1)\n\ndel train\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFECV\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"percent = 25 # percent of dataset to use for feature selection\n\nx_train_train, x_train_valid, y_train_train, y_train_valid = train_test_split(x_train, y_train, test_size=1.0-percent/100.0, random_state=42)\n\ndel x_train\ndel y_train\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'objective': 'binary',\n    'metric': 'auc',\n    'is_unbalance': False,\n    'boost_from_average': True,\n    'num_threads': 4,\n    \n    'num_leaves': 500,\n    # 'min_data_in_leaf': 25,\n    'max_depth': -1,\n    'learning_rate': 0.01\n}\n\n# n_jobs=-1 causes out of memory\nfeature_selector = RFECV(lgb.LGBMClassifier(**params), step=10, scoring='roc_auc', cv=5, verbose=1)\nfeature_selector.fit(x_train_train, y_train_train)\nprint('Features selected:', feature_selector.n_features_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_features = [f for f in x_train_train.columns[feature_selector.ranking_ == 1]]\n\nselected_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Compare score (all features vs selected features)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"percent = 25 # percent of dataset to use for validation\n\n_, x_train_valid_part, _, y_train_valid_part = train_test_split(x_train_valid, y_train_valid, test_size=percent/100.0, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_model = lgb.LGBMClassifier(**params).fit(x_train_train, y_train_train)\ny_all_features = lgb_model.predict_proba(x_train_valid_part)[:,1]\nscore_all_features = roc_auc_score(y_train_valid_part, y_all_features)\nprint('Score / all features:', score_all_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_selected_features = feature_selector.estimator_.predict_proba(x_train_valid_part[selected_features])[:,1]\nscore_selected_features = roc_auc_score(y_train_valid_part, y_selected_features)\nprint('Score / selected features:', score_selected_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Score change:', score_selected_features-score_all_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Compare feature importance and feature selection results"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importance_df = pd.concat([\n    pd.Series(x_train_train.columns),\n    pd.Series(lgb_model.feature_importances_)], axis=1)\nfeature_importance_df.columns = ['featureName', 'importance']\n\nfeature_importance_df['selected'] = feature_importance_df['featureName'].map(lambda x: x in selected_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n    print(feature_importance_df.sort_values(by=['importance'], ascending=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Confusion matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"# From https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.utils.multiclass import unique_labels\n\n\ndef plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    classes = classes[unique_labels(y_true, y_pred)]\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = pd.Series([0,1])\n\nplot_confusion_matrix(y_train_valid_part, y_all_features.round(), classes=classes, normalize=True,\n                      title='Confusion matrix / all features')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(y_train_valid_part, y_selected_features.round(), classes=classes, normalize=True,\n                      title='Confusion matrix / selected features')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*) Lower values for 1-0 and 0-1 mean better performance"},{"metadata":{},"cell_type":"markdown","source":"## ROC curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"# From https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n#  and https://stackoverflow.com/questions/25009284/how-to-plot-roc-curve-in-python\n\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\nfpr_all_features, tpr_all_features, _ = roc_curve(y_train_valid_part, y_all_features.round())\nroc_auc_all_features = auc(fpr_all_features, tpr_all_features)\n\nfpr_selected_features, tpr_selected_features, _ = roc_curve(y_train_valid_part, y_selected_features.round())\nroc_auc_selected_features = auc(fpr_selected_features, tpr_selected_features)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr_all_features, tpr_all_features, 'g', label = 'AUC (all features) = %0.2f' % roc_auc_all_features)\nplt.plot(fpr_selected_features, tpr_selected_features, 'b', label = 'AUC (selected features) = %0.2f' % roc_auc_selected_features)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":1}