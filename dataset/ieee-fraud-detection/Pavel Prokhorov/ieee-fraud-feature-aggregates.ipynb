{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Feature aggregates\n\nThe purpose of this notebook is no test weighting by features aggregates.\n\nAlgorithm:\n1. Take one feature\n2. Group data by this feature\n3. Take second feature\n4. Devide seond feature values by mean, std values of second feature of group created by first feature value"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nimport gc\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load and prepare data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"%%time\n\ntrain_transaction = pd.read_csv('../input/ieee-fraud-detection/train_transaction.csv', index_col='TransactionID')\ntrain_identity = pd.read_csv('../input/ieee-fraud-detection/train_identity.csv', index_col='TransactionID')\ntrain = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ndel train_transaction, train_identity\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.fillna(-999)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"many_same_values_columns_train = [c for c in train.columns if train[c].value_counts(normalize=True).values[0] > 0.9]\ncolumns_to_drop = many_same_values_columns_train\ncolumns_to_drop.remove('isFraud')\ntrain = train.drop(columns_to_drop, axis=1)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# From https://www.kaggle.com/pavelvpster/ieee-fraud-eda-lightgbm-baseline\n\nfrom sklearn.preprocessing import LabelEncoder\n\ndef encode_categorial_features_fit(df, columns_to_encode):\n    encoders = {}\n    for c in columns_to_encode:\n        if c in df.columns:\n            encoder = LabelEncoder()\n            encoder.fit(df[c].astype(str).values)\n            encoders[c] = encoder\n    return encoders\n\ndef encode_categorial_features_transform(df, encoders):\n    out = pd.DataFrame(index=df.index)\n    for c in encoders.keys():\n        if c in df.columns:\n            out[c] = encoders[c].transform(df[c].astype(str).values)\n    return out\n\ncategorial_features_columns = [\n    'id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21',\n    'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31',\n    'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38',\n    'DeviceType', 'DeviceInfo', 'ProductCD', 'P_emaildomain', 'R_emaildomain',\n    'card1', 'card2', 'card3', 'card4', 'card5', 'card6',\n    'addr1', 'addr2',\n    'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9',\n    'P_emaildomain_vendor', 'P_emaildomain_suffix', 'P_emaildomain_us',\n    'R_emaildomain_vendor', 'R_emaildomain_suffix', 'R_emaildomain_us'\n]\n\ncategorial_features_encoders = encode_categorial_features_fit(train, categorial_features_columns)\ntemp = encode_categorial_features_transform(train, categorial_features_encoders)\ncolumns_to_drop = list(set(categorial_features_columns) & set(train.columns))\ntrain = train.drop(columns_to_drop, axis=1).merge(temp, how='left', left_index=True, right_index=True)\ndel temp\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# From https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n#        else:\n#            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\n\ntrain = reduce_mem_usage(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train['isFraud'].copy()\nx_train = train.drop('isFraud', axis=1)\n\ndel train\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Light GBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\n\ndef test(x_train, y_train):\n    \n    x_train_train, x_train_valid, y_train_train, y_train_valid = train_test_split(x_train, y_train, test_size=0.33, random_state=42)\n    \n    lgb_train = lgb.Dataset(data=x_train_train.astype('float32'), label=y_train_train.astype('float32'))\n    lgb_valid = lgb.Dataset(data=x_train_valid.astype('float32'), label=y_train_valid.astype('float32'))\n    \n    params = {\n        'objective': 'binary',\n        'metric': 'auc',\n        'is_unbalance': False,\n        'boost_from_average': True,\n        'num_threads': 4,\n        \n        'num_leaves': 500,\n        'min_data_in_leaf': 25,\n        'max_depth': 50\n    }\n    \n    lgb_model = lgb.train(params, lgb_train, valid_sets=lgb_valid, verbose_eval=1000)\n    \n    y = lgb_model.predict(x_train_valid.astype('float32'), num_iteration=lgb_model.best_iteration)\n    \n    score = roc_auc_score(y_train_valid.astype('float32'), y)\n    return score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test feature aggregates"},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_aggregates(df, feature_to_group_by, feature):\n    out = pd.DataFrame(index=df.index)\n    \n    # filter for -999 is needed because NaN values were filled by this constant\n    agg = df[df[feature] != -999].groupby([feature_to_group_by])[feature]\n    \n    new_feature = feature + '_' + feature_to_group_by\n    out[new_feature + '_mean'] = df[feature] / agg.transform('mean')\n    out[new_feature + '_std' ] = df[feature] / agg.transform('std')\n    \n    return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import itertools\n\nbaseline_score = test(x_train, y_train)\nprint('Baseline score =', baseline_score)\n\n# Modify these lists\nfeatures_to_group_by = ['card1', 'card4', 'addr1']\nfeatures = ['TransactionAmt', 'id_02', 'D15']\n\nnew_features = {}\n\nfor pair in itertools.product(features_to_group_by, features):\n    new_feature = pair[1] + '_' + pair[0]\n    print('Test feature:', new_feature, '/ aggregates of', pair[1], 'by', pair[0])\n    \n    temp = x_train.copy()\n    \n    agg = make_aggregates(temp, pair[0], pair[1])\n    print('Columns added:', agg.columns.values)\n    \n    temp = temp.merge(agg, how='left', left_index=True, right_index=True)\n\n    score = test(temp, y_train)\n    print('Score =', score)\n    \n    new_features[new_feature] = score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_features_df = pd.DataFrame.from_dict(new_features, orient='index', columns=['score']).sub(baseline_score)\nnew_features_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(12, 12))\np = sns.barplot(x=new_features_df.index, y='score', data=new_features_df, color='gray')\n\n# Rotate labels\nfor x in p.get_xticklabels():\n    x.set_rotation(90)\n\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}