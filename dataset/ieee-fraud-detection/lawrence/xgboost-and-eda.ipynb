{"cells":[{"metadata":{"_uuid":"4430e338-f7c9-463e-8496-79e514bc879c","_cell_guid":"0fb409f7-28d0-49a6-804e-fcf5960f3a98","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom time import time\nimport datetime \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, TimeSeriesSplit\nfrom sklearn.preprocessing import MinMaxScaler\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\nimport lightgbm as lgb\nimport sklearn\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.externals import joblib\nimport gc\nimport tqdm\nfrom sklearn.metrics import roc_auc_score\n\n# Preprocessing \nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler\n\n'''\nFraud Detection for Kaggle competition\n'''\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run on whole data\ndata_path = '../input/ieee-fraud-detection'\ntrain_id = pd.read_csv(f'{data_path}/train_identity.csv', index_col = None)\ntest_id = pd.read_csv(f'{data_path}/test_identity.csv')\ntrain_action = pd.read_csv(f'{data_path}/train_transaction.csv', index_col = None)\ntest_action = pd.read_csv(f'{data_path}/test_transaction.csv')\n\n# Now this is the version that we don't split data into 2 part\n\n#print(train_action.info())\n#print(train_id.info())\n\n# Combine transaction & respective ID features\ntrain = train_action.merge(train_id, how = 'left', on = 'TransactionID')\ntest = test_action.merge(test_id, how = 'left', on = 'TransactionID')\n# Use index: TransactionID to merge two dataframe, use transaction.csv as the base. Preserve both index\ndel train_action, train_id, test_action, test_id\n\nimport warnings\nwarnings.simplefilter('ignore')\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"72d05576-0cc2-4e24-87ef-a98bc5355e26","_cell_guid":"78e9d8fb-8776-4fc6-b0ef-ee59e6eb2468","trusted":true},"cell_type":"code","source":"\nfeatures = pd.read_csv('../input/rfe-feature/rfe_feature.csv')\nfeatures = list(features.iloc[:, 1].values)\n\n# Other than the essesntial features, there are some features such as prediction target to preserve, so do not apply these features directly\ncol_not_drop = ['isFraud', 'TransactionID', 'TransactionDT']\ntrain = train[features + col_not_drop]\ncol_not_drop.remove('isFraud')\ntest = test[features + col_not_drop]\nprint('Useful features selected by RFECV: ', len(features))\n\nprint('Current train shape:', train.values.shape)\nprint('Current test shape:', test.values.shape)\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.info())\nprint(train.head())\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# EDA: TransactionAMT decimal part as new feature\ntrain['TransactionAmtDec'] = ((train['TransactionAmt'] - train['TransactionAmt'].astype(int))*100).astype(int)\ntest['TransactionAmtDec'] = ((test['TransactionAmt'] - test['TransactionAmt'].astype(int))*100).astype(int)\n\n# Encode card1 features: using the frequency (value counts in whole dataset)\ntrain['card1_count'] = train['card1'].map(pd.concat([train['card1'], test['card1']], \n                                                  ignore_index = True).value_counts(dropna = False))\ntest['card1_count'] = test['card1'].map(pd.concat([train['card1'], test['card1']],\n                                                  ignore_index = True).value_counts(dropna = False))\n\n# Create new features: day of week, hours in a day   to detect Fraud action\ntrain['day_of_week'] = np.floor(train['TransactionDT']/ (3600*24)) % 7\ntest['day_of_week'] = np.floor(test['TransactionDT']/ (3600*24)) % 7\ntrain['hour_of_day'] = np.floor(train['TransactionDT']/ (3600)) % 24\ntest['hour_of_day'] = np.floor(test['TransactionDT']/ (3600)) % 24\n\n# Create feature: Arbitrary feature interaction ????\n\n# Use frequency to create new features for id_01, id_33, ...etc ?? Why??\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encode category features\n# For null values in obj_feature: View NaN as string, then encode in same way\nfor feat in train.columns:\n    le = LabelEncoder()\n    if train[feat].dtypes == 'object':\n        le.fit(list(train[feat].astype(str).values) + list(test[feat].astype(str).values))\n        train[feat] = le.transform(train[feat].astype(str).values)\n        test[feat] = le.transform(test[feat].astype(str).values)\n\nprint(train.head())\n        \ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"null = train.isnull().sum()\nprint('Null values for now: \\n')\nprint(null[null > 0].sort_values(ascending = False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LightGBM could deal with missing values better ? Why?\n# Why should I take time element into account? This is an classification problem. \nx = train.sort_values('TransactionDT').drop(['isFraud', 'TransactionID', 'TransactionDT'], axis = 1)\ny = train.sort_values('TransactionDT')['isFraud']\n#test = test.sort_values('TransactionDT').drop(['TransactionID', 'TransactionDT'], axis = 1)\n\ndel train\nprint('Training data shape: ', x.values.shape)\nprint('Testing data shape:', test.values.shape)\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Start training\nparams = {\n    'num_leaves':490,\n    'min_child_weight':0.035,\n    'feature_fraction':0.379,\n    'baggin_fraction':0.418,\n    'min_data_in_leaf':105,\n    'objective':'binary',\n    'max_depth':-1, \n    'learning_rate':0.0068,\n    'boosting':'gbdt',\n    'bagging_seed':11,\n    'metric':'auc',\n    'verbosity':-1,\n    'reg_alpha':0.389,\n    'reg_lambda':0.648,\n    'random_state':47  \n}\n\n'''\n# Split by time step: check if time step influence the performance later\nspliter = TimeSeriesSplit(n_splits = 5)\n\nauc = []\nfeature_importance = pd.DataFrame({'feature':x.columns})\n\n# Train the data by each fold\nfor fold, (train_idx, valid_idx) in enumerate(spliter.split(x)):\n    start_t = time()\n    print('Traning on fold: ', (fold + 1))\n    \n    train_data = lgb.Dataset(x.iloc[train_idx], label = y.iloc[train_idx])\n    valid_data = lgb.Dataset(x.iloc[valid_idx], label = y.iloc[valid_idx])\n    clf = lgb.train(params, train_data,\n                    num_boost_round= 10000,\n                    valid_sets = [train_data, valid_data],\n                    verbose_eval = 1000,\n                    early_stopping_rounds = 500\n                   )\n    \n    feature_importance['fold_{}'.format(fold + 1)] = clf.feature_importance()\n    auc.append(clf.best_score['valid_1']['auc'])\n    print('Fold {} finished in {}'.format((fold + 1), \n                                          str(datetime.timedelta(seconds = time() - start_t)) ))\n    \nprint('-'*30)\nprint('Training has finished')\nprint('Total training time: ', str(datetime.timedelta(seconds = time() - start_t)))\nprint('Mean auc: ', np.mean(auc))\nprint('-'*30)\n\n# Use lgb.train & timeseries split first: observe performance and validation accuracy??\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the speed of lightgbm vs. xgboost \n\nstart = time.time()\nlgbm = lgb.LGBMClassifier(**params, num_boost_round = 1300)\nxgbm = XGBClassifier(**params, n_estimators = 1300)\n\nx_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size = 0.2, random_state = 9)\n\nlgbm.fit(x_train, y_train)\nend = time.time()\nprint('light gbm time:', end - start)\n\nx_train = \nxgbm.fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nbest_iter = 1305\nprint('Best iteration for LGB:', best_iter)\nclf2 = lgb.LGBMClassifier(**params, num_boost_round= best_iter)\nclf2.fit(x_train, y_train)\nscore = clf2.score(x_valid, y_valid)\nprint('Validation accuracy of LGB:', score)\n'''\n\n# Fill NaN for other models\ndrop_null = x.isnull().sum().sort_values(ascending = False)\ndrop_null = drop_null[drop_null > len(x)*0.7]\nx.drop(drop_null.index, axis = 1, inplace = True)\ndrop_null = x.isnull().sum().sort_values(ascending = False)\nx.fillna(-999, inplace = True)\n\nx_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size = 0.2, random_state = 9)\n\nfrom keras.\nforest = RandomForestClassifier(100)\nforest.fit(x_train, y_train)\nscore = forest.score(x_valid, y_valid)\nprint('Random Forest score:', score)\n\nlogit = LogisticRegression()\nlogit.fit(x_train, y_train)\nscore = logit.score(x_valid, y_valid)\nprint('Logistic Regression score:', score)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.applications import vgg16\n\nvgg16 = vgg16(include_top = False)\n\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save the predicted result to submission.csv\n\nsubmission = pd.read_csv('../input/ieee-fraud-detection/sample_submission.csv')\nsubmission['isFraud'] = clf2.predict_proba(test)[:, 1]\nsubmission.to_csv('submission.csv')\n\n# import the modules we'll need\nfrom IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n# create a random sample dataframe\n\n# create a link to download the dataframe\ncreate_download_link(submission)","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}