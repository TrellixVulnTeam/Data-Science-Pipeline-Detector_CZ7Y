{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Ensembling With StackNet"},{"metadata":{},"cell_type":"markdown","source":"![](https://github.com/kaz-Anova/StackNet/raw/master/images/StackNet_Logo.png?raw=true)"},{"metadata":{},"cell_type":"markdown","source":"In this kernel we will take a look on how to use StackNet to stack multiple levels of models in order to efficiently blend models. StackNet is a powerful package that works really well for competitions! We are going to stack a random forest on top of 3 GBM models as an example. We will use data from the [IEEE Fraud Detection](https://www.kaggle.com/c/ieee-fraud-detection) competition to explain StackNet.\n\nStackNet was created by Kaggle Grandmaster Marios Michailidis ([kazanova](https://www.kaggle.com/kazanova)) as part of his PhD. Thanks to [Kiran Kunapuli](https://www.kaggle.com/kirankunapuli) for uploading the package as [a Kaggle dataset](https://www.kaggle.com/kirankunapuli/pystacknet) so it can conveniently be used with Kaggle kernels.\n\nLet's dive in!"},{"metadata":{},"cell_type":"markdown","source":"## Table Of Contents"},{"metadata":{},"cell_type":"markdown","source":"- [Dependencies](#1)\n- [Metric (AUC)](#2)\n- [Data Preparation](#3)\n- [Modeling](#4)\n- [Evaluation](#5)\n- [Submission](#6)"},{"metadata":{},"cell_type":"markdown","source":"## Dependencies <a id=\"1\"></a>"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Import PyStackNet Package\n# Source: https://www.kaggle.com/kirankunapuli/pystacknet\nimport os\nimport sys\nsys.path.append(\"../input/pystacknet/repository/h2oai-pystacknet-af571e0\")\nimport pystacknet","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Standard Dependencies\nimport gc\nimport sys\nimport time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, roc_curve\n\n# Machine Learning\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.linear_model import Ridge\n\n# Specify paths and key features\nKAGGLE_DIR = '../input/ieee-fraud-detection/'\nTARGET = 'isFraud'\n\n# Seed for reproducability\nseed = 1234\nnp.random.seed(seed)\n\n# For keeping time. Limit for Kaggle kernels is set to approx. 9 hours.\nt_start = time.time()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# File sizes and specifications\nprint('\\n# Files and file sizes:')\nfor file in os.listdir(KAGGLE_DIR):\n    print('{}| {} MB'.format(file.ljust(30), \n                             str(round(os.path.getsize(KAGGLE_DIR + file) / 1000000, 2))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Metric (AUC) <a id=\"2\"></a>"},{"metadata":{},"cell_type":"markdown","source":"The Metric used in this competition is \"[Area Under ROC Curve (AUC)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)\". We create this curve by plotting the true positive rate (TPR) against the false positive rate (FPR) at different threshold settings. \nThis is very convenient since with binary classification problems like fraud detection the accuracy score is not that informative. For example, if we predict only 0 (not fraud) on this dataset, then we will get an accuracy score of 0.965. The AUC score will be 0.5 (no better than random). All naive baselines will get an AUC score of approximately 0.5.\n\nTo calculate the AUC score we can use [sklearn's roc_auc_score function](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html) straight out of the box."},{"metadata":{},"cell_type":"markdown","source":"Image: An example of an ROC curve. AOC is a typo and should be AUC.\n\n![](https://miro.medium.com/max/722/1*pk05QGzoWhCgRiiFbz-oKQ.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def auc_score(y_true, y_pred):\n    \"\"\"\n    Calculates the Area Under ROC Curve (AUC)\n    \"\"\"\n    return roc_auc_score(y_true, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To plot the ROC curve we will use a function using sklearn and matplotlib. An example of this visualization is shown in the evaluation section of this kernel."},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"def plot_curve(y_true_train, y_pred_train, y_true_val, y_pred_val, model_name):\n    \"\"\"\n    Plots the ROC Curve given predictions and labels\n    \"\"\"\n    fpr_train, tpr_train, _ = roc_curve(y_true_train, y_pred_train, pos_label=1)\n    fpr_val, tpr_val, _ = roc_curve(y_true_val, y_pred_val, pos_label=1)\n    plt.figure(figsize=(8, 8))\n    plt.plot(fpr_train, tpr_train, color='black',\n             lw=2, label=f\"ROC train curve (AUC = {round(roc_auc_score(y_true_train, y_pred_train), 4)})\")\n    plt.plot(fpr_val, tpr_val, color='darkorange',\n             lw=2, label=f\"ROC validation curve (AUC = {round(roc_auc_score(y_true_val, y_pred_val), 4)})\")\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([-0.05, 1.05])\n    plt.ylim([-0.05, 1.05])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.xticks(fontsize=14)\n    plt.yticks(fontsize=14)\n    plt.title(f'ROC Plot for {model_name}', weight=\"bold\", fontsize=20)\n    plt.legend(loc=\"lower right\", fontsize=16)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preparation <a id=\"3\"></a>"},{"metadata":{},"cell_type":"markdown","source":"Since this kernel is meant to explain StackNet and establish a baseline we will not go into advanced feature engineering and EDA here. However, your performance will greatly benefit from feature engineering so I encourage you to explore it. A good kernel which does that for this competition can be found [here](https://www.kaggle.com/kyakovlev/ieee-gb-2-make-amount-useful-again?scriptVersionId=18874747)."},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Load in datasets\ntrain_transaction = pd.read_csv(f\"{KAGGLE_DIR}train_transaction.csv\", index_col='TransactionID')\ntest_transaction = pd.read_csv(f\"{KAGGLE_DIR}test_transaction.csv\", index_col='TransactionID')\ntrain_identity = pd.read_csv(f\"{KAGGLE_DIR}train_identity.csv\", index_col='TransactionID')\ntest_identity = pd.read_csv(f\"{KAGGLE_DIR}test_identity.csv\", index_col='TransactionID')\n\n# Merge datasets into full training and test dataframe\ntrain_df = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True).reset_index()\ntest_df = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True).reset_index()\ndel train_identity, train_transaction, test_identity, test_transaction\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select only first 52 features\n# The other columns are quite noisy\ntrain_df = train_df.iloc[:, :53]\ntest_df = test_df.iloc[:, :52]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"StackNet does not accept missing values (NaN's), Infinity values (inf) or values higher than 32 bytes (for example float64 or int64). Therefore, we have to fill in missing values and compress certain columns as the Pandas standard is 64 bytes. Big thanks to [Arjan Groen](https://www.kaggle.com/arjanso) for creating this convenient function. The function is taken from [this Kaggle kernel](https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\"\n    Reduces memory usage for all columns in a Pandas DataFrame\n    \"\"\"\n    start_mem_usg = df.memory_usage().sum() / 1024**2 \n    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n    NAlist = [] # Keeps track of columns that have missing values filled in. \n    for col in df.columns:\n        if df[col].dtype != object:  # Exclude strings                       \n            # make variables for Int, max and min\n            IsInt = False\n            mx = df[col].max()\n            mn = df[col].min()\n            # Integer does not support NA, therefore, NA needs to be filled\n            if not np.isfinite(df[col]).all(): \n                NAlist.append(col)\n                df[col].fillna(mn-1,inplace=True)  \n                   \n            # test if column can be converted to an integer\n            asint = df[col].fillna(0).astype(np.int32)\n            result = (df[col] - asint)\n            result = result.sum()\n            if result > -0.01 and result < 0.01:\n                IsInt = True            \n            # Make Integer/unsigned Integer datatypes\n            if IsInt:\n                if mn >= 0:\n                    if mx < 255:\n                        df[col] = df[col].astype(np.uint8)\n                    elif mx < 65535:\n                        df[col] = df[col].astype(np.uint16)\n                    else:\n                        df[col] = df[col].astype(np.uint32)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    else:\n                        df[col] = df[col].astype(np.int32)\n            # Make float datatypes 32 bit\n            else:\n                df[col] = df[col].astype(np.float32)\n\n    # Print final result\n    mem_usg = df.memory_usage().sum() / 1024**2 \n    print(\"Memory usage of properties dataframe is after reduction is:\",mem_usg,\" MB\")\n    return df, NAlist","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Reduce memory\ntrain_df, _ = reduce_mem_usage(train_df)\ntest_df, _ = reduce_mem_usage(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop nuisance columns and specify target variable\nX_train = train_df.drop([TARGET, 'TransactionID', 'TransactionDT'], axis=1)\nX_test = test_df.drop(['TransactionID', 'TransactionDT'], axis=1)\ntarget = train_df[TARGET]\n\n# Label Encoding\nlbl = LabelEncoder()\nfor f in X_train.columns:\n    if X_train[f].dtype == 'object': \n        lbl.fit(list(X_train[f].values) + list(X_test[f].values))\n        X_train[f] = lbl.transform(list(X_train[f].values))\n        X_test[f] = lbl.transform(list(X_test[f].values))   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split Train and Validation\nX_train, X_val, y_train, y_val = train_test_split(X_train,\n                                                  target,\n                                                  test_size=0.15, \n                                                  random_state=seed, \n                                                  stratify=target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling <a id=\"4\"></a>"},{"metadata":{},"cell_type":"markdown","source":"StackNet allows you to define all kinds of models. For example, Sklearn models, LightGBM, XGBoost, CatBoost and Keras models can all be used with StackNet.\n\nFor the individual models, you are responsible for not overfitting. Therefore, it is advisable to first experiment with individual models and make sure they are sound, before combining them into StackNet. For this example we will use [sklearn's Gradient Boosting Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html), a [LightGBM Regressor](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html#lightgbm.LGBMRegressor) and a [CatBoost Regressor](https://catboost.ai/docs/concepts/python-reference_catboostregressor.html) in the 1st level. Then we will train a [Random Forest Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) in level 2, which takes the predictions of the models in the 1st level as input. StackNet takes care of the stacking and cross validation."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Level 1 are the base models that take the training dataset as input\nl1_clf1 = GradientBoostingRegressor(n_estimators=400,\n                                    learning_rate=0.006,\n                                    min_samples_leaf=10,\n                                    max_depth=20, \n                                    max_features='sqrt', \n                                    subsample=0.85,\n                                    random_state=seed)\n\nl1_clf2 = LGBMRegressor(boosting_type='gbdt',\n                        objective=\"binary\",\n                        metric=\"AUC\",\n                        boost_from_average=\"false\",\n                        learning_rate=0.007,\n                        num_leaves=491,\n                        max_depth=25,\n                        min_child_weight=0.035,\n                        feature_fraction=0.38,\n                        bagging_fraction=0.42,\n                        min_data_in_leaf=100,\n                        max_bin=255,\n                        importance_type='split',\n                        reg_alpha=0.4,\n                        reg_lambda=0.65,\n                        bagging_seed=seed,\n                        random_state=seed,\n                        verbosity=-1,\n                        subsample=0.85,\n                        colsample_bytree=0.8,\n                        min_child_samples=79)\n\nl1_clf3 = CatBoostRegressor(learning_rate=0.2,\n                            bagging_temperature=0.1, \n                            l2_leaf_reg=30,\n                            depth=12, \n                            max_bin=255,\n                            iterations=100,\n                            loss_function='Logloss',\n                            objective='RMSE',\n                            eval_metric=\"AUC\",\n                            bootstrap_type='Bayesian',\n                            random_seed=seed,\n                            early_stopping_rounds=10)\n\n# Level 2 models will take predictions from level 1 models as input\n# Remember to keep level 2 models smaller\n# Basic models like Ridge Regression with large regularization or small random forests work well\nl2_clf1 = Ridge(alpha=0.001, normalize=True, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model tree that StackNet takes as input is a list of lists. The 1st list defines the 1st level, the 2nd one the 2nd level, etc. You can build a model tree of arbitrary depth and width."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Specify model tree for StackNet\nmodels = [[l1_clf1, l1_clf2, l1_clf3], # Level 1\n          [l2_clf1]] # Level 2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model is compiled and fitted through the a familiar sklearn-like API. The StackNetClassifier will perform cross-validation (CV) and will output the CV scores for each model. To make sure we can output a probability of fraud we specify \"use_proba=True\".\n\nThe \"folds\" argument in StackNetClassifier can also accept an iterable of train/test splits. Since the target distibution is imbalanced you can probably improve on the CV strategy by first yielding stratified train/test split with for example [sklearn's StratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html)."},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"from pystacknet.pystacknet import StackNetClassifier\n\n# Specify parameters for stacked model and begin training\nmodel = StackNetClassifier(models, \n                           metric=\"auc\", \n                           folds=3,\n                           restacking=False,\n                           use_retraining=True,\n                           use_proba=True, # To use predict_proba after training\n                           random_state=seed,\n                           n_jobs=-1, \n                           verbose=1)\n\n# Fit the entire model tree\nmodel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Get score on training set and validation set for our StackNetClassifier\ntrain_preds = model.predict_proba(X_train)[:, 1]\nval_preds = model.predict_proba(X_val)[:, 1]\ntrain_score = auc_score(y_train, train_preds)\nval_score = auc_score(y_val, val_preds)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(f\"StackNet AUC on training set: {round(train_score, 4)}\")\nprint(f\"StackNet AUC on validation set: {round(val_score, 4)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation <a id=\"5\"></a>"},{"metadata":{},"cell_type":"markdown","source":"The blue line signifies the baseline AUC which is 0.5. The final validation score is the area under the orange curve, which is mentioned in the plot."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Plot ROC curve\nplot_curve(y_train, train_preds, y_val, val_preds, \"StackNet Baseline\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission <a id=\"6\"></a>"},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":false},"cell_type":"code","source":"# Write predictions to csv\nsub = pd.read_csv(f\"{KAGGLE_DIR}sample_submission.csv\")\npreds = model.predict_proba(X_test)[:, 1]\nsub[TARGET] = preds\nsub.to_csv(f\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To check if the predictions are sound, we check the format of our submission and compare our prediction distribution with that of the target distribution in the training set."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Check Submission format\nprint(\"Final Submission Format:\")\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,4))\nplt.hist(train_df[TARGET], bins=100)\nplt.title(\"Distribution for train set\", weight='bold', fontsize=18)\nplt.xlabel(\"Predictions\", fontsize=15)\nplt.ylabel(\"Frequency\", fontsize=15)\nplt.xlim(0, 1);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(12,4))\nplt.hist(sub[TARGET], bins=100)\nplt.title(\"Prediction Distribution for test set\", weight='bold', fontsize=18)\nplt.xlabel(\"Predictions\", fontsize=15)\nplt.ylabel(\"Frequency\", fontsize=15)\nplt.xlim(0, 1);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Check kernels run-time. Limit for Kaggle Kernels is set to approx. 9 hours.\nt_finish = time.time()\ntotal_time = round((t_finish-t_start) / 3600, 4)\nprint('Kernel runtime = {} hours ({} minutes)'.format(total_time, \n                                                      int(total_time*60)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Try to experiment with [StackNet](https://github.com/h2oai/pystacknet) yourself. The possibilities are almost endless!\n\nIf you want to check out another solution using PyStackNet, check out [this Kaggle kernel on the Titanic dataset by Yann Berthelot](https://www.kaggle.com/yannberthelot/pystacknet-working-implementation).\n\n**If you like this Kaggle kernel, feel free to give an upvote and leave a comment! I will try to implement your suggestions in this kernel!**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}