{"cells":[{"metadata":{},"cell_type":"markdown","source":"### This kernel so far is almost copy from the below Kernels so of you want upvote those Kernels.\n### I am just trying to learn my first ML project \n#### Update 1: Today I have learnt a little bit about ensembling and stacking . Tried to apply those concepts in this data set . It didnt do anything better now , but helped me understand the concepts a bit with a working code .\n\n#### I referred to some existing solution for this , however I forgot which kernel it was , anyone has idea , they can point out and i will add due credit . \n\nhttps://www.kaggle.com/artkulak/ieee-fraud-simple-baseline-0-9383-lb\n\nNanashi : https://www.kaggle.com/jesucristo/fraud-complete-eda\n\nXhulu's kernel: https://www.kaggle.com/xhlulu/ieee-fraud-xgboost-with-gpu-fit-in-40s"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport gc\ngc.enable()\n\nimport logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.DEBUG)\nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nimport catboost\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import roc_auc_score, roc_curve\nimport matplotlib.gridspec as gridspec\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Going to use these 5 base models for the stacking\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn import metrics\n\nfrom sklearn.svm import SVC\nimport time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Some switches \nuseXGB = False\nuseLGBM = True\nuseEnsemble = False\n## Accidental Switch Setting\nif (useXGB) and(useLGBM):\n    useEnsemble = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_transaction = pd.read_csv('../input/train_transaction.csv', index_col='TransactionID')\ntest_transaction = pd.read_csv('../input/test_transaction.csv', index_col='TransactionID')\n\ntrain_identity = pd.read_csv('../input/train_identity.csv', index_col='TransactionID')\ntest_identity = pd.read_csv('../input/test_identity.csv', index_col='TransactionID')\n\nsample_submission = pd.read_csv('../input/sample_submission.csv', index_col='TransactionID')\n\ntrain = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n\nprint(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Quick Trick to Create Email Feature\nemails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', 'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft', 'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo', 'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', 'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink', 'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other', 'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', 'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', 'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo', 'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other', 'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft', 'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', 'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', 'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', 'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', 'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', 'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', 'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other', 'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\nus_emails = ['gmail', 'net', 'edu']\n\nfor c in ['P_emaildomain', 'R_emaildomain']:\n    train[c + '_bin'] = train[c].map(emails)\n    test[c + '_bin'] = test[c].map(emails)\n    \n    train[c + '_suffix'] = train[c].map(lambda x: str(x).split('.')[-1])\n    test[c + '_suffix'] = test[c].map(lambda x: str(x).split('.')[-1])\n    \n    train[c + '_suffix'] = train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n    test[c + '_suffix'] = test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Trick to create Hr Feature :\n## https://www.kaggle.com/fchmiel/day-and-time-powerful-predictive-feature\ndef make_day_feature(df, offset=0, tname='TransactionDT'):\n    \"\"\"\n    Creates a day of the week feature, encoded as 0-6. \n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        df to manipulate.\n    offset : float (default=0)\n        offset (in days) to shift the start/end of a day.\n    tname : str\n        Name of the time column in df.\n    \"\"\"\n    # found a good offset is 0.58\n    days = df[tname] / (3600*24)        \n    encoded_days = np.floor(days-1+offset) % 7\n    return encoded_days\n\ndef make_hour_feature(df, tname='TransactionDT'):\n    \"\"\"\n    Creates an hour of the day feature, encoded as 0-23. \n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        df to manipulate.\n    tname : str\n        Name of the time column in df.\n    \"\"\"\n    hours = df[tname] / (3600)        \n    encoded_hours = np.floor(hours) % 24\n    return encoded_hours\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Create Feature in Train and Test\ntrain['hours'] = make_hour_feature(train)\ntest['hours']= make_hour_feature(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Label Encoding\nfor f in train.columns:\n    if train[f].dtype=='object' or train[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(train[f].values) + list(test[f].values))\n        train[f] = lbl.transform(list(train[f].values))\n        test[f] = lbl.transform(list(test[f].values))   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train['isFraud'].copy()\ndel train_transaction, train_identity, test_transaction, test_identity\n\n# Drop target, fill in NaNs\nX_train = train.drop('isFraud', axis=1)\nX_test = test.copy()\n\ndel train, test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# From kernel https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n# WARNING! THIS CAN DAMAGE THE DATA \ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\nX_train = reduce_mem_usage(X_train)\nX_test = reduce_mem_usage(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Bucket the columns for easier use in future"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = []\nfor f in X_train.columns:\n    columns.append(f)\n    \ndtcol = [columns[0]]\ncardcol = columns[3:9]\nccol = columns[15:29]\ndcol = columns[29:44]\nmcol = columns[44:53]\nvcol = columns[53:392]\nidcol = columns[392:430]\ncustomcol = columns[432:437] \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The below code for value density comparison of train and test are taken from Chris Deottes kernel for Malware detection"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# COMPARE VALUE DENSITIES FROM TWO DIFFERENT DATAFRAMES\n#\n# PARAMETERS\n# df1: pandas.DataFrame containing variable\n# df2: pandas.DataFrame containing variable\n# col: column to compare between df1 and df2\n# override: set to False to prevent display when variables similar\n# verbose: display text summary\n# scale: zooms y-axis\n# title: plot title\n# lab1: legend label for df1\n# lab2: legend label for df2\n# prefix: pre text for verbose summary\n#\ndef comparePlot(df1, df2, col, factor=4, override=True, verbose=True, scale=0.5, title='',\n                lab1='', lab2='', prefix=''):\n    cv1 = pd.DataFrame(df1[col].value_counts(normalize=True).reset_index().rename({col:'train'},axis=1))\n    cv2 = pd.DataFrame(df2[col].value_counts(normalize=True).reset_index().rename({col:'test'},axis=1))\n    cv3 = pd.merge(cv1,cv2,on='index',how='outer')\n    cv3['train'].fillna(0,inplace=True)\n    cv3['test'].fillna(0,inplace=True)\n    cv3 = cv3.iloc[np.lexsort((cv3['test'], -cv3['train']))]\n    cv3['total'] = cv3['train']+cv3['test']\n    cv3['trainMX'] = cv3['train']*factor\n    cv3['trainMN'] = cv3['train']/factor\n    cv3 = cv3[cv3['total']>0.0001]\n    if (len(cv3)<5): return\n    cv3.reset_index(inplace=True)\n    MX = (cv3['test'] > cv3['trainMX'])\n    mxSum = round(100*cv3.loc[MX,'test'].sum(),1)\n    MN = (cv3['test'] < cv3['trainMN'])\n    mnSum = round(100*cv3.loc[MN,'test'].sum(),1)\n    #if override | (MX.sum()+MN.sum()>0):\n    if override | (mxSum + mnSum > 1):\n        plt.figure(figsize=(12,3))\n        if lab1=='': lab1='Train'\n        if lab2=='': lab2='Test'\n        plt.plot(cv3.index,cv3['train'],linewidth=3,alpha=0.7,color='b',label=lab1)\n        plt.plot(cv3.index,cv3['trainMX'],linewidth=2,alpha=1.0,linestyle=':',color='b',label=str())\n        plt.plot(cv3.index,cv3['trainMN'],linewidth=2,alpha=1.0,linestyle=':',color='b',label=str())\n        #plt.bar(cv3.index,cv3['test'],linewidth=3,alpha=0.7,color='g', label='Test.csv')\n        plt.plot(cv3.index,cv3['test'],linewidth=3,alpha=0.7,color='g',label=lab2)\n        plt.legend()\n        if title=='': plt.title(col)\n        else: plt.title(col+' - '+title)\n        plt.xlabel(col+' values (ordered by train frequency and relabeled)')\n        plt.ylabel('Frequency')\n        mx = max(cv3['train'].max(),cv3['test'].max())\n        #plt.ylim(0,mx*1.05)\n        plt.ylim(0,mx*scale)\n        plt.show()\n        tempMX = cv3.loc[MX.values,['index','test']].sort_values('test',ascending=False)['index']\n        tempMN = cv3.loc[MN.values,['index','test']].sort_values('test',ascending=False)['index']\n        if verbose:\n            if MX.sum()>0:    \n                print(prefix+'Test.csv',col,'has',MX.sum(),'values 4x MORE freq than Train.csv. (',mxSum,'% of data)')\n            if MX.sum()>10: print('  Top 10 by test freq:',list(tempMX)[:10])\n            elif MX.sum()>0: print(list(tempMX)[:10])\n            if MN.sum()>0:\n                print(prefix+'Test.csv',col,'has',MN.sum(),'values 4x LESS freq than Train.csv. (',mnSum,'% of data)')\n            if MN.sum()>10: print('  Top 10 by test freq:',list(tempMN)[:10])\n            elif MN.sum()>0: print(list(tempMN)[:10])\n    return","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for cols in cardcol: \n        comparePlot(X_train, X_test, cols, verbose=True, title='Test vs. Train')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Note : Drop card1 and card2 ? The frequency of data is slightly different "},{"metadata":{"trusted":true},"cell_type":"code","source":"for cols in ccol: \n        comparePlot(X_train, X_test, cols, verbose=True, title='Test vs. Train')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*#### Note: Nothing interesting here .*"},{"metadata":{"trusted":true},"cell_type":"code","source":"for cols in dcol: \n        comparePlot(X_train, X_test, cols, verbose=True, title='Test vs. Train')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Note : Except for D3,5,7,9,13 others have larges variance between train and test with respect to value counts . Can we drop those ?"},{"metadata":{"trusted":true},"cell_type":"code","source":"for cols in vcol[0:15]:\n        comparePlot(X_train, X_test, cols, verbose=True, title='Test vs. Train')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*#### Does not seem to have much problem for now . Since based on the latest clarification it was created as features by DS community of the organizers , we can probably keep them . Just few check on V201 and V258 , seen in the last version that they were important features by LGBM*"},{"metadata":{"trusted":true},"cell_type":"code","source":"for cols in vcol[200:202]: \n        comparePlot(X_train, X_test, cols, verbose=True, title='Test vs. Train')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for cols in vcol[250:260]: \n        comparePlot(X_train, X_test, cols, verbose=True, title='Test vs. Train')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### *Does not seem to be a big problem as of now .*"},{"metadata":{},"cell_type":"markdown","source":"*#### Why dont mcols give an output. Need to check .*"},{"metadata":{"trusted":true},"cell_type":"code","source":"for cols in idcol: \n        comparePlot(X_train, X_test, cols, verbose=True, title='Test vs. Train')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### id 7,13,18,19,21,24,26,31 seems to have problem in distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"for cols in customcol: \n        comparePlot(X_train, X_test, cols, verbose=True, title='Test vs. Train')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*#### Newly created features seem to be okay*"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndrop_col = dtcol +mcol[0:len(mcol)]+ idcol[5:39]  + dcol[0:len(dcol)] + ccol[4:len(ccol)]\nX_train.drop(drop_col,axis=1, inplace=True)\nX_test.drop(drop_col, axis=1, inplace=True)\n\n\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.fillna(-1,inplace=True)\nX_test.fillna(-1,inplace=True)\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_amt_feature(df, tname='TransactionAmt'):\n    \"\"\"\n    Creates an hour of the day feature, encoded as 0-23. \n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        df to manipulate.\n    tname : str\n        Name of the time column in df.\n    \"\"\"\n    logAmt = np.log(df[tname])      \n    \n    return logAmt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Create Feature in Train and Test\nX_train['LogAmt'] = make_amt_feature(X_train)\nX_test['LogAmt']= make_amt_feature(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nif useXGB :\n    NFOLDS = 6\n    kf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=123)\n\n    y_preds_xgb = np.zeros(X_test.shape[0])\n    y_oof_xgb = np.zeros(X_train.shape[0])\n    score = 0\n  \n    for fold, (tr_idx, val_idx) in enumerate(kf.split(X_train, y_train)):\n        clf = xgb.XGBClassifier(\n            n_estimators=500,\n            max_depth=9,\n            learning_rate=0.05,\n            subsample=0.9,\n            colsample_bytree=0.9,\n            gamma = 0.2,\n            alpha = 4,\n            missing = -1,\n            tree_method='gpu_hist'\n        )\n    \n        X_tr, X_vl = X_train.iloc[tr_idx, :], X_train.iloc[val_idx, :]\n        y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n        clf.fit(X_tr, y_tr)\n        y_pred_train = clf.predict_proba(X_vl)[:,1]\n        y_oof_xgb[val_idx] = y_pred_train\n        print(\"FOLD: \",fold,' AUC {}'.format(roc_auc_score(y_vl, y_pred_train)))\n        score += roc_auc_score(y_vl, y_pred_train) / NFOLDS\n        y_preds_xgb+= clf.predict_proba(X_test)[:,1] / NFOLDS\n    \n        del X_tr, X_vl, y_tr, y_vl\n        gc.collect()\n    \n    y_preds_xgb.reshape(-1,1)   \n    y_oof_xgb.reshape(-1,1)\n    print(\"\\nMEAN AUC = {}\".format(score))\n    print(\"OOF AUC = {}\".format(roc_auc_score(y_train, y_oof_xgb)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nif useXGB :\n    NFOLDS = 5\n    kf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=0)\n\n    y_preds_xgb1 = np.zeros(X_test.shape[0])\n    y_oof_xgb1 = np.zeros(X_train.shape[0])\n    score = 0\n  \n    for fold, (tr_idx, val_idx) in enumerate(kf.split(X_train, y_train)):\n        clf = xgb.XGBClassifier(\n            n_estimators=1500,\n            max_depth=10,\n            learning_rate=0.05,\n            subsample=0.9,\n            colsample_bytree=0.9,\n            gamma = 0.2,\n            alpha = 4,\n            missing = -1,\n            tree_method='gpu_hist'\n        )\n    \n        X_tr, X_vl = X_train.iloc[tr_idx, :], X_train.iloc[val_idx, :]\n        y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n        clf.fit(X_tr, y_tr)\n        y_pred_train = clf.predict_proba(X_vl)[:,1]\n        y_oof_xgb1[val_idx] = y_pred_train\n        print(\"FOLD: \",fold,' AUC {}'.format(roc_auc_score(y_vl, y_pred_train)))\n        score += roc_auc_score(y_vl, y_pred_train) / NFOLDS\n        y_preds_xgb1+= clf.predict_proba(X_test)[:,1] / NFOLDS\n    \n        del X_tr, X_vl, y_tr, y_vl\n        gc.collect()\n    \n    y_preds_xgb1.reshape(-1,1)   \n    y_oof_xgb1.reshape(-1,1)\n    print(\"\\nMEAN AUC = {}\".format(score))\n    print(\"OOF AUC = {}\".format(roc_auc_score(y_train, y_oof_xgb1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nif useXGB :\n    NFOLDS = 5\n    kf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=42)\n\n    y_preds_xgb2 = np.zeros(X_test.shape[0])\n    y_oof_xgb2 = np.zeros(X_train.shape[0])\n    score = 0\n  \n    for fold, (tr_idx, val_idx) in enumerate(kf.split(X_train, y_train)):\n        clf = xgb.XGBClassifier(\n            n_estimators=1500,\n            max_depth=10,\n            learning_rate=0.05,\n            subsample=0.9,\n            colsample_bytree=0.9,\n            gamma = 0.2,\n            alpha = 4,\n            missing = -1,\n            tree_method='gpu_hist'\n        )\n    \n        X_tr, X_vl = X_train.iloc[tr_idx, :], X_train.iloc[val_idx, :]\n        y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n        clf.fit(X_tr, y_tr)\n        y_pred_train = clf.predict_proba(X_vl)[:,1]\n        y_oof_xgb2[val_idx] = y_pred_train\n        print(\"FOLD: \",fold,' AUC {}'.format(roc_auc_score(y_vl, y_pred_train)))\n        score += roc_auc_score(y_vl, y_pred_train) / NFOLDS\n        y_preds_xgb2+= clf.predict_proba(X_test)[:,1] / NFOLDS\n    \n        del X_tr, X_vl, y_tr, y_vl\n        gc.collect()\n    \n    y_preds_xgb2.reshape(-1,1)   \n    y_oof_xgb2.reshape(-1,1)\n    print(\"\\nMEAN AUC = {}\".format(score))\n    print(\"OOF AUC = {}\".format(roc_auc_score(y_train, y_oof_xgb2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param = {'num_leaves': 30,\n         'min_data_in_leaf': 30, \n         'objective':'binary',\n         'max_depth': -1,\n         'learning_rate': 0.01,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.8,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.8 ,\n         \"bagging_seed\": 11,\n         \"metric\": 'auc',\n         \"lambda_l1\": 0.1,\n         \"random_state\": 133,\n         \"verbosity\": -1}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_iter = 5\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.model_selection import TimeSeriesSplit\n\nif useLGBM :\n    #folds = KFold(n_splits=5, shuffle=True, random_state=15)\n    folds = TimeSeriesSplit(n_splits =5)\n    oof = np.zeros(len(X_train))\n    #categorical_columns = [c for c in cat_cols ]\n    features = [c for c in X_train.columns]\n    predictions = np.zeros(len(X_test))\n    start = time.time()\n    feature_importance_df = pd.DataFrame()\n    start_time= time.time()\n    score = [0 for _ in range(folds.n_splits)]\n\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train, y_train)):\n        print(\"fold n°{}\".format(fold_))\n        trn_data = lgb.Dataset(X_train.iloc[trn_idx][features],\n                           label=y_train.iloc[trn_idx]#,\n                           #categorical_feature = categorical_columns\n                          )\n        val_data = lgb.Dataset(X_train.iloc[val_idx][features],\n                           label=y_train.iloc[val_idx]#,\n                           #categorical_feature = categorical_columns\n                          )\n\n        num_round = 500\n        clf = lgb.train(param,\n                    trn_data,\n                    num_round,\n                    valid_sets = [trn_data, val_data],\n                    verbose_eval=10,\n                    early_stopping_rounds = 100)\n    \n        oof[val_idx] = clf.predict(X_train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    \n        fold_importance_df = pd.DataFrame()\n        fold_importance_df[\"feature\"] = features\n        fold_importance_df[\"importance\"] = clf.feature_importance(importance_type='gain')\n        fold_importance_df[\"fold\"] = fold_ + 1\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n        # we perform predictions by chunks\n        initial_idx = 0\n        chunk_size = 100000\n        current_pred = np.zeros(len(X_test))\n        while initial_idx < X_test.shape[0]:\n            final_idx = min(initial_idx + chunk_size, X_test.shape[0])\n            idx = range(initial_idx, final_idx)\n            current_pred[idx] = clf.predict(X_test.iloc[idx][features], num_iteration=clf.best_iteration)\n            initial_idx = final_idx\n        predictions += current_pred / min(folds.n_splits, max_iter)\n   \n        print(\"time elapsed: {:<5.2}s\".format((time.time() - start_time) / 3600))\n        score[fold_] = metrics.roc_auc_score(y_train.iloc[val_idx], oof[val_idx])\n        if fold_ == max_iter - 1: break\n        \n    if (folds.n_splits == max_iter):\n        print(\"CV score: {:<8.5f}\".format(metrics.roc_auc_score(y_train, oof)))\n    else:\n         print(\"CV score: {:<8.5f}\".format(sum(score) / max_iter))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importance_df['fold'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_lose_it = feature_importance_df.loc[feature_importance_df['fold'] == 5].sort_values(by=\"importance\", ascending=True).head(100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_lose_it=df_lose_it.loc[df_lose_it['importance'] == 0.0000]\ndrop_add = []\nfor vals in df_lose_it['feature'] :\n    drop_add.append(vals)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Fold1 Feature Importance\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"importance\", y=\"feature\", data=feature_importance_df.loc[feature_importance_df['fold'] == 1].sort_values(by=\"importance\", ascending=False).head(20))\nplt.title('LightGBM Features fold 1')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-01.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Fold 2 Feature Importance\n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"importance\", y=\"feature\", data=feature_importance_df.loc[feature_importance_df['fold'] == 2].sort_values(by=\"importance\", ascending=False).head(20))\nplt.title('LightGBM Features fold 2')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-02.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Fold 3 Feature importance \n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"importance\", y=\"feature\", data=feature_importance_df.loc[feature_importance_df['fold'] == 3].sort_values(by=\"importance\", ascending=False).head(20))\nplt.title('LightGBM Features fold 3')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-03.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Fold 4 Feature importance \n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"importance\", y=\"feature\", data=feature_importance_df.loc[feature_importance_df['fold'] == 4].sort_values(by=\"importance\", ascending=False).head(20))\nplt.title('LightGBM Features fold 4')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-04.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Fold 5 Feature importance \n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"importance\", y=\"feature\", data=feature_importance_df.loc[feature_importance_df['fold'] == 5].sort_values(by=\"importance\", ascending=False).head(20))\nplt.title('LightGBM Features fold 5')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-05.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.drop(drop_add,axis=1, inplace=True)\nX_test.drop(drop_add, axis=1, inplace=True)\n\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.model_selection import TimeSeriesSplit\n\nif useLGBM :\n    #folds = KFold(n_splits=5, shuffle=True, random_state=15)\n    folds = TimeSeriesSplit(n_splits =5)\n    oof = np.zeros(len(X_train))\n    #categorical_columns = [c for c in cat_cols ]\n    features = [c for c in X_train.columns]\n    predictions = np.zeros(len(X_test))\n    start = time.time()\n    feature_importance_df = pd.DataFrame()\n    start_time= time.time()\n    score = [0 for _ in range(folds.n_splits)]\n\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train, y_train)):\n        print(\"fold n°{}\".format(fold_))\n        trn_data = lgb.Dataset(X_train.iloc[trn_idx][features],\n                           label=y_train.iloc[trn_idx]#,\n                           #categorical_feature = categorical_columns\n                          )\n        val_data = lgb.Dataset(X_train.iloc[val_idx][features],\n                           label=y_train.iloc[val_idx]#,\n                           #categorical_feature = categorical_columns\n                          )\n\n        num_round = 10000\n        clf = lgb.train(param,\n                    trn_data,\n                    num_round,\n                    valid_sets = [trn_data, val_data],\n                    verbose_eval=200,\n                    early_stopping_rounds = 200)\n    \n        oof[val_idx] = clf.predict(X_train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    \n        fold_importance_df = pd.DataFrame()\n        fold_importance_df[\"feature\"] = features\n        fold_importance_df[\"importance\"] = clf.feature_importance(importance_type='gain')\n        fold_importance_df[\"fold\"] = fold_ + 1\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n        # we perform predictions by chunks\n        initial_idx = 0\n        chunk_size = 100000\n        current_pred = np.zeros(len(X_test))\n        while initial_idx < X_test.shape[0]:\n            final_idx = min(initial_idx + chunk_size, X_test.shape[0])\n            idx = range(initial_idx, final_idx)\n            current_pred[idx] = clf.predict(X_test.iloc[idx][features], num_iteration=clf.best_iteration)\n            initial_idx = final_idx\n        predictions += current_pred / min(folds.n_splits, max_iter)\n   \n        print(\"time elapsed: {:<5.2}s\".format((time.time() - start_time) / 3600))\n        score[fold_] = metrics.roc_auc_score(y_train.iloc[val_idx], oof[val_idx])\n        if fold_ == max_iter - 1: break\n        \n    if (folds.n_splits == max_iter):\n        print(\"CV score: {:<8.5f}\".format(metrics.roc_auc_score(y_train, oof)))\n    else:\n         print(\"CV score: {:<8.5f}\".format(sum(score) / max_iter))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sample_submission['isFraud'] = 0.25 * y_preds_xgb +0.25 *y_preds_xgb1 +0.25 *y_preds_xgb2 +0.25*predictions\nsample_submission['isFraud'] = predictions\nsample_submission.to_csv('simple_ensemble.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}