{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom datetime import datetime,timedelta\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nfrom sklearn.model_selection import GroupKFold\nfrom scipy.stats import rankdata,ks_2samp\nfrom collections import Counter,OrderedDict\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# This notebook investigates following analysis\n\n1. Check the relationship of transaction table data and identity table data  \n2. Compare the distribution of each feature data when its target is 1(fraud) and 0(normal)  \nby using Kolmogorov-Smirov test  \n3. Analyze time consistency of each feature data\n4. Analyze the correlation of all features each other"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# ================================================================================\ntrain_transaction_all_columns=['TransactionID', 'isFraud', 'TransactionDT', 'TransactionAmt', 'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'dist1', 'dist2', 'P_emaildomain', 'R_emaildomain', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V138', 'V139', 'V140', 'V141', 'V142', 'V143', 'V144', 'V145', 'V146', 'V147', 'V148', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V155', 'V156', 'V157', 'V158', 'V159', 'V160', 'V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V168', 'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176', 'V177', 'V178', 'V179', 'V180', 'V181', 'V182', 'V183', 'V184', 'V185', 'V186', 'V187', 'V188', 'V189', 'V190', 'V191', 'V192', 'V193', 'V194', 'V195', 'V196', 'V197', 'V198', 'V199', 'V200', 'V201', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V218', 'V219', 'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V230', 'V231', 'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V238', 'V239', 'V240', 'V241', 'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V248', 'V249', 'V250', 'V251', 'V252', 'V253', 'V254', 'V255', 'V256', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V269', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', 'V277', 'V278', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V322', 'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339']\ntrain_identity_all_columns=['TransactionID', 'id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07', 'id_08', 'id_09', 'id_10', 'id_11', 'id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo']\n\n# ================================================================================\n# Categorical columns in transaction\n\ncategorical_columns_in_train_transaction=[]\ncategorical_columns_in_train_transaction.append(\"ProductCD\")\ncategorical_columns_in_train_transaction.extend(['card{}'.format(i) for i in range(1,7)])\ncategorical_columns_in_train_transaction.extend(['addr1','addr2'])\ncategorical_columns_in_train_transaction.extend(['P_emaildomain','R_emaildomain'])\ncategorical_columns_in_train_transaction.extend(['M{}'.format(i) for i in range(1,10)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ================================================================================\n# Numerical columns in transaction\n\nnumerical_columns_in_train_transaction=[one_column for one_column in train_transaction_all_columns if one_column not in categorical_columns_in_train_transaction]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ================================================================================\n# Categorical columns in identity\n\ncategorical_columns_in_train_identity=[]\ncategorical_columns_in_train_identity.extend(['DeviceType','DeviceInfo'])\ncategorical_columns_in_train_identity.extend(['id_{}'.format(i) for i in range(12,39)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ================================================================================\n# Numerical columns in identity\n\nnumerical_columns_in_train_identity=[one_column for one_column in train_identity_all_columns if one_column not in categorical_columns_in_train_identity]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ================================================================================\n# Setting datatype in loading dataframe\n\ndtypes = {}\n\nfor c in numerical_columns_in_train_transaction+numerical_columns_in_train_identity:\n    dtypes[c]='float32'\nfor c in categorical_columns_in_train_transaction+categorical_columns_in_train_identity:\n    dtypes[c]='category'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ================================================================================\n# Load train data\n\ntrain_transaction=pd.read_csv('/kaggle/input/ieee-fraud-detection/train_transaction.csv',dtype=dtypes)\ntrain_identity=pd.read_csv('/kaggle/input/ieee-fraud-detection/train_identity.csv',dtype=dtypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ================================================================================\n# 1. Check the relationship of transaction table data and identity table data  \n\n# At first, I thought these 2 tables have 1 (identity) : N (transaction) relationship but it is 1:1 relationship  \n# TransactionID in transaction is consecutive but TransactionID in identity is not consecutive  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Are TransactionID values of train transaction table data unique?\n# Yes, 590540 rows are unique in TransactionID column\nCounter(list(Counter(list(train_transaction[\"TransactionID\"])).values()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Are TransactionID values of train identity table data unique?\n# Yes, 144233 rows are unique in TransactionID column\nCounter(list(Counter(list(train_identity[\"TransactionID\"])).values()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ================================================================================\n# See the data by using Python's set form\n\ndef investigate_frequency(train_transaction,train_identity):\n  intersection_of_transaction_and_identity=set(list(train_transaction[\"TransactionID\"])).intersection(set(list(train_identity[\"TransactionID\"])))\n  transaction_minus_identity=set(list(train_transaction[\"TransactionID\"]))-set(list(train_identity[\"TransactionID\"]))\n  identity_minus_transaction=set(list(train_identity[\"TransactionID\"]))-set(list(train_transaction[\"TransactionID\"]))\n  return len(intersection_of_transaction_and_identity),len(transaction_minus_identity),len(identity_minus_transaction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"intersection_of_transaction_and_identity,transaction_minus_identity,identity_minus_transaction=investigate_frequency(train_transaction,train_identity)\nintersection_of_transaction_and_identity,transaction_minus_identity,identity_minus_transaction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def frequency_visualization(\n  frequency_distribution_of_transaction_TransactionID,\n  frequency_distribution_of_idendity_TransactionID,\n  number_of_transaction_rows,\n  number_of_identity_rows,\n  intersection_of_transaction_and_identity,\n  transaction_minus_identity,\n  identity_minus_transaction):\n\n  def my_fmt(x):\n    return '{:.4f}%'.format(x)\n  \n  plt.figure(figsize=(11,11))\n  ax1=plt.subplot2grid((2,2),(0,0),colspan=1)\n  ax2=plt.subplot2grid((2,2),(0,1),colspan=1)\n  ax3=plt.subplot2grid((2,2),(1,0),colspan=2)\n  ax1.pie(frequency_distribution_of_transaction_TransactionID.values(), labels=frequency_distribution_of_transaction_TransactionID.keys(),autopct=my_fmt)\n  ax1.set_title('TransactionID distribution from transaction')\n  ax2.pie(frequency_distribution_of_idendity_TransactionID.values(), labels=frequency_distribution_of_idendity_TransactionID.keys(),autopct=my_fmt)\n  ax2.set_title('TransactionID distribution from identity')\n  ax3.bar([\"transaction\",\"idendity\",\"Transaction \\cap identity\",\"Transaction-Identity\",\"Identity-Transaction\"],[number_of_transaction_rows,number_of_identity_rows,intersection_of_transaction_and_identity,transaction_minus_identity,identity_minus_transaction])\n  for index,data in enumerate([number_of_transaction_rows,number_of_identity_rows,intersection_of_transaction_and_identity,transaction_minus_identity,identity_minus_transaction]):\n    plt.text(x=index,y=data+1,s=f\"{data}\",fontdict=dict(fontsize=11))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfrequency_visualization(\n    Counter(list(Counter(list(train_transaction[\"TransactionID\"])).values())),\n    Counter(list(Counter(list(train_identity[\"TransactionID\"])).values())),\n    train_transaction.shape[0],\n    train_identity.shape[0],\n    intersection_of_transaction_and_identity,\n    transaction_minus_identity,\n    identity_minus_transaction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ================================================================================\n# Merge the tables and sort it\n\ncsv_train=pd.merge(train_transaction,train_identity,on=['TransactionID'],how='left')\ncsv_train=csv_train.sort_values(by=['TransactionID'],axis=0)\ncsv_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ================================================================================\n# 2. Compare the distribution of each feature data when its target is 1(fraud) and 0(normal)  \n# by using Kolmogorov-Smirov test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def investigate_difference_of_target0_and_target1_numerical_date_distribution(csv_train):\n  train_transaction_all_columns=['TransactionID', 'isFraud', 'TransactionDT', 'TransactionAmt', 'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'dist1', 'dist2', 'P_emaildomain', 'R_emaildomain', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V138', 'V139', 'V140', 'V141', 'V142', 'V143', 'V144', 'V145', 'V146', 'V147', 'V148', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V155', 'V156', 'V157', 'V158', 'V159', 'V160', 'V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V168', 'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176', 'V177', 'V178', 'V179', 'V180', 'V181', 'V182', 'V183', 'V184', 'V185', 'V186', 'V187', 'V188', 'V189', 'V190', 'V191', 'V192', 'V193', 'V194', 'V195', 'V196', 'V197', 'V198', 'V199', 'V200', 'V201', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V218', 'V219', 'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V230', 'V231', 'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V238', 'V239', 'V240', 'V241', 'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V248', 'V249', 'V250', 'V251', 'V252', 'V253', 'V254', 'V255', 'V256', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V269', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', 'V277', 'V278', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V322', 'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339']\n  train_identity_all_columns=['TransactionID', 'id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07', 'id_08', 'id_09', 'id_10', 'id_11', 'id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo']\n\n  # ================================================================================\n  # Categorical columns in transaction\n\n  categorical_columns_in_train_transaction=[]\n  categorical_columns_in_train_transaction.append(\"ProductCD\")\n  categorical_columns_in_train_transaction.extend(['card{}'.format(i) for i in range(1,7)])\n  categorical_columns_in_train_transaction.extend(['addr1','addr2'])\n  categorical_columns_in_train_transaction.extend(['P_emaildomain','R_emaildomain'])\n  categorical_columns_in_train_transaction.extend(['M{}'.format(i) for i in range(1,10)])\n\n  numerical_columns_in_train_transaction=[one_column for one_column in train_transaction_all_columns if one_column not in categorical_columns_in_train_transaction]\n\n  # ================================================================================\n  # Categorical columns in identity\n\n  categorical_columns_in_train_identity=[]\n  categorical_columns_in_train_identity.extend(['DeviceType','DeviceInfo'])\n  categorical_columns_in_train_identity.extend(['id_{}'.format(i) for i in range(12,39)])\n\n  numerical_columns_in_train_identity=[one_column for one_column in train_identity_all_columns if one_column not in categorical_columns_in_train_identity]\n\n  # ================================================================================\n  numerical_columns_all=numerical_columns_in_train_transaction+numerical_columns_in_train_identity\n  categorical_columns_all=categorical_columns_in_train_transaction+categorical_columns_in_train_identity\n\n  numerical_columns_all.remove('TransactionID')\n  numerical_columns_all.remove('isFraud')\n  numerical_columns_all.remove('TransactionDT')\n  numerical_columns_all.remove('TransactionID')\n\n  # ================================================================================\n  csv_train_class0=csv_train[csv_train['isFraud']==0]\n  csv_train_class1=csv_train[csv_train['isFraud']==1]\n\n  # ================================================================================\n  large_difference_columns=[]\n  for one_numerical_column in numerical_columns_all:\n    \n    # ================================================================================\n    # Remove outliers\n\n    lower_bound=csv_train[one_numerical_column].quantile(0.003)\n    upper_bound=csv_train[one_numerical_column].quantile(0.997)\n  \n    csv_train_class0_filtered=csv_train_class0[one_numerical_column][\n      (csv_train_class0[one_numerical_column]>=lower_bound)&\n      (csv_train_class0[one_numerical_column]<=upper_bound)\n    ]\n\n    csv_train_class1_filtered=csv_train_class1[one_numerical_column][\n      (csv_train_class1[one_numerical_column]>=lower_bound)&\n      (csv_train_class1[one_numerical_column]<=upper_bound)\n    ]\n\n    # ================================================================================\n    # Get histogram data\n\n    csv_train_class0_filtered_hist,csv_train_class0_filtered_bin_edges=np.histogram(csv_train_class0_filtered)\n    csv_train_class1_filtered_hist,csv_train_class1_filtered_bin_edges=np.histogram(csv_train_class1_filtered)\n    \n    # ================================================================================\n    # Normalize data to correctly compare\n\n    csv_train_class0_filtered_hist_normed=(np.array(csv_train_class0_filtered_hist)-np.array(csv_train_class0_filtered_hist).min())/(np.array(csv_train_class0_filtered_hist).max()-np.array(csv_train_class0_filtered_hist).min())\n    csv_train_class1_filtered_hist_normed=(np.array(csv_train_class1_filtered_hist)-np.array(csv_train_class1_filtered_hist).min())/(np.array(csv_train_class1_filtered_hist).max()-np.array(csv_train_class1_filtered_hist).min())\n\n    # ================================================================================\n    fig,ax=plt.subplots(1,1,figsize=(20,5))\n    ax.plot(csv_train_class0_filtered_bin_edges[:-1],csv_train_class0_filtered_hist_normed,color=\"blue\")\n    ax.plot(csv_train_class1_filtered_bin_edges[:-1],csv_train_class1_filtered_hist_normed,color=\"red\")\n  \n    # ================================================================================\n    from scipy.stats import rankdata,ks_2samp\n    result_from_komogorov_smirnov=ks_2samp(csv_train_class0_filtered_hist_normed,csv_train_class1_filtered_hist_normed)\n\n    if result_from_komogorov_smirnov.pvalue<0.05:\n      large_difference_columns.append([one_numerical_column,result_from_komogorov_smirnov.pvalue])\n  return large_difference_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"large_difference_columns=investigate_difference_of_target0_and_target1_numerical_date_distribution(csv_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"large_difference_columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Note  \n- I thought I could use aggregation like \"mean\", \"standard deviation\", \"min\", \"max\" on columns  \nwhich have large difference between data with target1(fraud) and data with target0(normal)\n- To compare distribution, I tried using Kolmogorov-Smirnov test and it says for exmple C7 has large different distribution  \n- But checking the distribution of C7 in above images, that plot shows similar distribution of red line and blue line  \nand I wonder the reason of it"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ================================================================================\n# Calculate ratio of NaN from all columns, and we will discard columns which have NaN over 50%\n\ndef check_nan(merged,NAN_CRITERION):\n  number_of_rows_from_data=merged.shape[0]\n  number_of_columns_from_data=merged.shape[1]\n\n  # ================================================================================\n  number_of_nan_in_column=merged.isnull().sum(axis=0)\n  number_of_nan_in_row=merged.isnull().sum(axis=1)\n\n  # ================================================================================\n  df=(number_of_nan_in_column/number_of_rows_from_data*100).to_frame().reset_index()\n\n  # ================================================================================\n  df=df.rename(columns={\"index\":'column_name',0:'nan_percent'})\n\n  # ================================================================================\n  columns_to_be_dropped=list((df[df['nan_percent']>NAN_CRITERION])['column_name'])\n\n  # ================================================================================\n  plt.figure(figsize=(21,11),dpi=500)\n  plt.bar(list(df[\"column_name\"]),list(df[\"nan_percent\"]))\n  plt.xticks(rotation=90,fontsize=3)\n  plt.axhline(y=50,color='r',linestyle='--')\n  return df,columns_to_be_dropped","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_nan_ratio_df,train_columns_to_be_dropped=check_nan(csv_train,NAN_CRITERION=50)\ntrain_nan_ratio_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def discard_nan_columns(merged,columns_to_be_dropped):\n  merged.drop(columns_to_be_dropped,axis=1,inplace=True)\n  return merged","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"csv_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"csv_train=discard_nan_columns(csv_train,train_columns_to_be_dropped)\ncsv_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ================================================================================\n# Add time related data for time consistency check test\n\ndef create_datetime_column(csv_train):\n\n  start_datetime=datetime(2017,7,1,0,0)\n\n  converted_datetime_series=csv_train['TransactionDT'].map(lambda x:start_datetime+timedelta(seconds=x))\n  \n  csv_train[\"TransactionDT_datetime\"]=converted_datetime_series\n\n  # ================================================================================\n  rankdata_year_month=rankdata(list(csv_train[\"TransactionDT_datetime\"].map(lambda x:str(x.year)+\"-\"+str(x.month).zfill(2))),method='dense')\n\n  csv_train[\"TransactionDT_year_month\"]=rankdata_year_month\n\n  return csv_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"csv_train=create_datetime_column(csv_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ================================================================================\n# 3. Analyze time consistency of each feature data\n\n# I got the idea from  \n# https://www.kaggle.com/cdeotte/xgb-fraud-with-magic-0-9600#Feature-Selection---Time-Consistency","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def check_time_consistency_of_each_feature(csv_train):\n\n  feature_to_be_checked=list(csv_train.columns)\n  feature_to_be_checked.remove(\"isFraud\")\n  feature_to_be_checked.remove(\"TransactionDT_datetime\")\n  feature_to_be_checked.remove(\"TransactionDT_year_month\")\n\n  csv_train=csv_train.set_index(\"TransactionID\")\n  csv_train_target_df=csv_train[[\"isFraud\",\"TransactionDT_year_month\"]]\n\n  feature_to_be_checked.remove(\"TransactionID\")\n  \n  all_features_name_for_visualizeation=[]\n  all_features_rocauc_for_visualizeation=[]\n  for one_feature_to_be_checked in feature_to_be_checked:\n    one_feature_df=csv_train[[one_feature_to_be_checked]].reset_index()\n    one_feature_df=pd.merge(one_feature_df,csv_train_target_df,on=['TransactionID'],how='left')\n    \n    # ================================================================================\n    normalized_train_X=one_feature_df.iloc[:,1].reset_index()\n    train_y=one_feature_df.iloc[:,2].reset_index()\n    del normalized_train_X[\"index\"]\n    del train_y[\"index\"]\n\n    # ================================================================================\n    group_kfold=GroupKFold(n_splits=4)\n    groups=list(one_feature_df['TransactionDT_year_month'])\n\n    roc_auc_score_init=0\n    for fold_n,(train,test) in enumerate(group_kfold.split(normalized_train_X,train_y,groups)):\n\n      X_train_,X_valid=normalized_train_X.iloc[train],normalized_train_X.iloc[test]\n      y_train_,y_valid=train_y.iloc[train],train_y.iloc[test]\n\n      # ================================================================================\n      params={\n        'n_estimators':100, \n        'learning_rate':0.1, \n        'num_leaves':31, \n        'max_depth':-1, \n        'boosting':'gbdt'\n      }\n\n      # ================================================================================\n      lgbclf=lgb.LGBMClassifier(**params)\n\n      # ================================================================================\n      # Train lgb model with train dataset\n\n      lgbclf.fit(X_train_,y_train_.values.ravel())\n      \n      # ================================================================================\n      # Delete used data\n\n      del X_train_,y_train_\n\n      # ================================================================================\n      # Make prediction on test dataset\n\n      val=lgbclf.predict_proba(X_valid)[:,1]\n\n      # ================================================================================\n      # Delete used data\n\n      del X_valid\n\n      # ================================================================================\n      roc_auc_score_init+=roc_auc_score(y_valid,val)/4\n\n    if roc_auc_score_init<0.5:\n      print('one_feature_to_be_checked',one_feature_to_be_checked)\n      print('roc_auc_score_init',roc_auc_score_init)\n\n    all_features_name_for_visualizeation.append(one_feature_to_be_checked)\n    all_features_rocauc_for_visualizeation.append(roc_auc_score_init)\n\n  fig,ax=plt.subplots(1,1,figsize=(20,5),dpi=500)\n  ax.bar(all_features_name_for_visualizeation,all_features_rocauc_for_visualizeation)\n  ax.set_title('Time consistency test on all features')\n  ax.set_xlabel('Feature names')\n  ax.set_ylabel('ROC AUC score (average from 4 groupkfolds)')\n  ax.set_xticklabels(all_features_name_for_visualizeation,rotation=90,fontsize=2)\n  ax.axhline(y=0.5,color='r',linestyle='--')\n  ax.axhline(y=0.52,color='r',linestyle='--')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"check_time_consistency_of_each_feature(csv_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Discussion  \n\nIf you set the threshold as 0.5 in roc_auc score, 2 columns (card4, V41) seems no time consistency  \nIf you set the threshold as slightly higher like 0.52 in roc_auc score,  \nmore columns will be determined as no time consistency columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ================================================================================\n# Split full column data into numerical data and categorical data\n\ndef separate_full_column_data_into_categorical_and_numerical(csv_train):\n\n  # ================================================================================\n  # Set index\n\n  csv_train=csv_train.set_index(\"TransactionID\")\n\n  # ================================================================================\n  numerical_data=[]\n  categorical_data=[]\n  for one_column_name in csv_train:\n    \n    if 'float' in str(csv_train[one_column_name].dtype) or 'int' in str(csv_train[one_column_name].dtype):\n      numerical_data.append(csv_train[[one_column_name]])\n    else:\n      categorical_data.append(csv_train[[one_column_name]])\n\n  numerical_train_df=pd.concat(numerical_data,axis=1)\n  categorical_train_df=pd.concat(categorical_data,axis=1)\n\n  return numerical_train_df,categorical_train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ================================================================================\n# Delete useless columns which had been used for above time consistency check test\n\ndel csv_train[\"TransactionDT_datetime\"]\ndel csv_train[\"TransactionDT_year_month\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_train_df,categorical_train_df=separate_full_column_data_into_categorical_and_numerical(csv_train)\nnumerical_train_df=numerical_train_df.astype(\"float32\")\nnumerical_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ================================================================================\n# Add label data based on 'year and month' combination\n\ndef convert_time_delte(df):\n  START_DATE=datetime.strptime('2017-11-30','%Y-%m-%d')\n  df['DT_M']=df['TransactionDT'].apply(lambda x:(START_DATE+timedelta(seconds=x)))\n  df['DT_M']=(df['DT_M'].dt.year-2017)*12+df['DT_M'].dt.month \n  return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_train_df=convert_time_delte(numerical_train_df)\nnumerical_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ================================================================================\n# Impute null of categorical data by \"nullstr\" string\n\ndef impute_categorical_data_by_mode(under40_nan_categorical_df):\n\n  temp_df1=[]\n  for one_column_name in under40_nan_categorical_df:\n    under40_nan_categorical_df[one_column_name]=under40_nan_categorical_df[one_column_name].cat.add_categories('nullstr')\n    bb=under40_nan_categorical_df[[one_column_name]].fillna(\"nullstr\")\n    temp_df1.append(bb)\n  temp_train_df2=pd.concat(temp_df1,axis=1)\n  return temp_train_df2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imputed_categorical_train_df=impute_categorical_data_by_mode(categorical_train_df)\nimputed_categorical_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ================================================================================\n# Integrate categorical values of all columns which rarely show into \"others\" category\n\ndef categorical_other(imputed_categorical_df):\n  \n  low_threshold={\n    \"card1\":100,\n    \"card2\":50,\n    \"card3\":20,\n    \"card5\":20,\n    \"addr1\":30,\n    \"id_31\":10,\n    \"DeviceInfo\":100}\n\n  # ================================================================================\n  column_collection=[]\n  for one_column_name in imputed_categorical_df:\n    one_column_df=imputed_categorical_df[[one_column_name]]\n    if one_column_name in [\"card1\",\"card2\",\"card3\",\"card5\",\"addr1\",\"id_31\",\"DeviceInfo\"]:\n      for one_cate in list(map(lambda x:x[0],list(one_column_df.value_counts()[one_column_df.value_counts()<low_threshold[one_column_name]].to_frame().T.columns))):\n        imputed_categorical_df[one_column_name]=imputed_categorical_df[one_column_name].replace(one_cate,'others')\n    else:\n      continue\n  return imputed_categorical_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imputed_categorical_train_df=categorical_other(imputed_categorical_train_df)\nimputed_categorical_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ================================================================================\n# Impute null of numerical data by mean value of each column\n\ndef impute_numerical_data_by_mean(under40_nan_numerical_df):\n  temp_df=[]\n  for one_column_name in under40_nan_numerical_df:\n    one_df=under40_nan_numerical_df[[one_column_name]]\n    one_df_mean=one_df.mean()\n    temp_df.append(one_df.fillna(one_df_mean))\n  temp_df2=pd.concat(temp_df,axis=1)\n\n  # ================================================================================\n  number_of_nan_in_entire_columns=temp_df2.isnull().sum(axis=0).sum()\n  assert number_of_nan_in_entire_columns==0,'number_of_nan_in_entire_columns!=0'\n  return temp_df2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imputed_numerical_train_df=impute_numerical_data_by_mean(numerical_train_df)\nimputed_numerical_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ================================================================================\n# 4. Analyze the correlation of all features each other","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def visualize_correlation_in_features(numerical_train_df):\n  corr=numerical_train_df.corr()\n  fig,ax=plt.subplots(figsize=(15,15),dpi=500)\n  aa=ax.matshow(corr,cmap=plt.get_cmap('Reds'))\n  fig.colorbar(aa,ax=ax)\n  plt.xticks(range(len(corr.columns)), corr.columns,rotation=90,fontsize=3)\n  plt.yticks(range(len(corr.columns)), corr.columns,fontsize=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visualize_correlation_in_features(imputed_numerical_train_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Discussion  \n\nSame groups like C1,C2,C3,,... have high correlation  \nSo, I think some of them which have high correlation can be removed,  \nfor example, if C1 and C2 have very high correlation, I deleted the one which has more NaNs from C1 and C2"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ================================================================================\n# Display correlation table after deleteing useless rows like (V10,V10,1) and selecting the one from rows like (V10,V11,0.9), (V11,V10,0.9)\n\ndef investigate_correlation_in_features(numerical_train_df):\n  corr=numerical_train_df.corr()\n\n  c1 = corr.abs().unstack()\n  res=c1.sort_values(ascending = False).reset_index()\n  res2=c1.sort_values(ascending = True).reset_index()\n\n  duplicated_list=[]\n  for i in range(res.shape[0]):\n    first_column_name=res.iloc[i,:][\"level_0\"]\n    second_column_name=res.iloc[i,:][\"level_1\"]\n    \n    # ================================================================================\n    # Sort for consistent order\n\n    consistent_order_list=[first_column_name,second_column_name]\n    consistent_order_list.sort()\n\n    # ================================================================================\n    filtered_row=res[(res['level_0']==consistent_order_list[0])&(res['level_1']==consistent_order_list[1])]\n\n    duplicated_list.append(filtered_row)\n\n  concat_duplicated_list=pd.concat(duplicated_list)\n\n  # ================================================================================\n  concat_duplicated_list=concat_duplicated_list.drop_duplicates(keep='first')\n  concat_duplicated_list=concat_duplicated_list[concat_duplicated_list['level_0']!=concat_duplicated_list['level_1']]\n\n  return concat_duplicated_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation_df=investigate_correlation_in_features(imputed_numerical_train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.float_format','{:.10f}'.format)\ncorrelation_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}