{"cells":[{"metadata":{"_uuid":"66e715de-93dc-4ca1-b5af-2d22ebfa2262","_cell_guid":"be05ed65-52c3-4531-8480-de4c8309512e","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn import metrics\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import model_selection  \nimport lightgbm as lgb\nimport time\nimport matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transaction CSVs\ntrain_identity= pd.read_csv(\"../input/ieee-fraud-detection/train_identity.csv\")\ntrain_transaction = pd.read_csv(\"../input/ieee-fraud-detection/train_transaction.csv\")\ntest_transaction = pd.read_csv(\"../input/ieee-fraud-detection/test_transaction.csv\")\ntest_identity= pd.read_csv(\"../input/ieee-fraud-detection/test_identity.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\n#Keep every row in the left dataframe.\ntest  = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Step is done to reduce memory by conversion\n# convert it to the low memory to fit the RAM\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            #Downsizing \n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    #iinfo-Machine limits for integer datatype\n                    df[col] = df[col].astype(np.int8)\n                    #Casting pandas object to a certain type-to int8,int16,int32 &int64 depending on size\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n# Using  above function to reduce memory usage for Train test df\ntrain_df=reduce_mem_usage(train)\ntest_df=reduce_mem_usage(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train shape: \", train.shape)\nprint(\"Test shape:\", test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Explore Categorical features\nprint('Training set:')\nfor col_name in train_df.columns:\n    if train_df[col_name].dtypes == 'object' :\n        unique_cat = len(train_df[col_name].unique())\n        print(\"Feature '{col_name}' has {unique_cat} categories\".format(col_name=col_name, unique_cat=unique_cat))\n\nprint(\"****************\")\n    \nprint('Test set:')\nfor col_name in test_df.columns:\n    if test_df[col_name].dtypes == 'object' :\n        unique_cat = len(test_df[col_name].unique())\n        print(\"Feature '{col_name}' has {unique_cat} categories\".format(col_name=col_name, unique_cat=unique_cat))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols = [col for col in train_df.columns if train_df[col].dtype in ['object']]\ncat_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"v_feat = train_df.columns[55:394]\nv_feat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"v_test_feat=test_df.columns[54:393]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"v_test_feat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import minmax_scale\nfrom sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Replacing NULL values\nfor col in v_feat:\n    train_df[col].fillna((train_df[col].min() - 2), inplace=True)\n    train_df[col] = (minmax_scale(train_df[col], feature_range=(0,1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=35,random_state=5)\nprincipalComponents = pca.fit_transform(train_df[v_feat])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"principalComponents","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"principalDf = pd.DataFrame(principalComponents)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"principalDf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.drop(v_feat,axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"principalDf.rename(columns=lambda x: str('V')+str(x), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"principalDf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" train_df = pd.concat([train_df, principalDf], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Replacing NULL values for test data\nfor col in v_test_feat:\n    test_df[col].fillna((test_df[col].min() - 2), inplace=True)\n    test_df[col] = (minmax_scale(test_df[col], feature_range=(0,1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=35,random_state=5)\npComponents = pca.fit_transform(test_df[v_test_feat])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_principalDf = pd.DataFrame(pComponents)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.drop(v_test_feat,axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_principalDf.rename(columns=lambda x: str('V')+str(x), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" test_df = pd.concat([test_df, test_principalDf], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['DeviceInfo'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Frequency Table-CrossTab\ndata_freq=pd.crosstab(index=train_df['DeviceInfo'], columns=\"count\")    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_freq=data_freq[data_freq['count']>350]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_freq.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_freq","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def change_value_Dev_Info(x) :\n    if x in ['ALE-L23 Build/HuaweiALE-L23', 'MacOS', 'Trident/7.0' ,'Windows','iOS Device','rv:11.0\t','rv:57.0','rv:59.0','SM-G531H Build/LMY48B','SM-G610M Build/MMB29K','SM-J700M Build/MMB29K'] :\n        return 0\n    else :\n        return 1\n    \ntrain_df.loc[:,'DeviceInfo'] = train_df['DeviceInfo'].apply(lambda x : change_value_Dev_Info(x))\ntest_df.loc[:,'DeviceInfo'] = test_df['DeviceInfo'].apply(lambda x : change_value_Dev_Info(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df.shape)\nprint(test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def change_value_P_emaildomain(x) :\n    if x in ['gmail.com', 'icloud.com', 'mail.com' , 'outlook.es', 'protonmail.com'] :\n        return x\n    else :\n        return 'etc'\n    \ntrain_df.loc[:,'P_emaildomain'] = train_df['P_emaildomain'].apply(lambda x : change_value_P_emaildomain(x))\ntest_df.loc[:,'P_emaildomain'] = test_df['P_emaildomain'].apply(lambda x : change_value_P_emaildomain(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def change_value_R_emaildomain(x) :\n    if x in ['gmail.com', 'icloud.com', 'mail.com' , 'outlook.es', 'protonmail.com'] :\n        return x\n    else :\n        return 'etc'\n    \ntrain_df.loc[:,'R_emaildomain'] = train_df['R_emaildomain'].apply(lambda x : change_value_P_emaildomain(x))\ntest_df.loc[:,'R_emaildomain'] = test_df['R_emaildomain'].apply(lambda x : change_value_P_emaildomain(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_news=pd.crosstab(index=train_df['DeviceType'], columns=\"count\")    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_news","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def change_value_Dev_Type(x) :\n    if x in ['desktop', 'mobile'] :\n        return 0\n    else :\n        return 1\n    \ntrain_df.loc[:,'DeviceType'] = train_df['DeviceType'].apply(lambda x : change_value_Dev_Type(x))\ntest_df.loc[:,'DeviceType'] = test_df['DeviceType'].apply(lambda x : change_value_Dev_Type(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Replacing NULL values in Device Info and Device Type with 0 i.e-Unrecognized Activity\ntrain_df[['DeviceType', 'DeviceInfo']] = train_df[['DeviceType','DeviceInfo']].fillna(value=0)\ntest_df[['DeviceType', 'DeviceInfo']] = test_df[['DeviceType','DeviceInfo']].fillna(value=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m_feat = ['M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']\n\nfor col in m_feat:\n    train_df[col].fillna('None', inplace=True)\n    test_df[col].fillna('None',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[m_feat].dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[m_feat].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d_feat =['D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11','D12', 'D13', 'D14', 'D15']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[d_feat].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[d_feat].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[d_feat].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in d_feat:\n    train_df[col] = (minmax_scale(train_df[col], feature_range=(0,1)))\n    train_df[col] = train_df[col].fillna(-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in d_feat:\n    test_df[col] = (minmax_scale(test_df[col], feature_range=(0,1)))\n    test_df[col] = test_df[col].fillna(-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df.shape)\nprint(test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c_feat = ['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7','C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14']\n\nfor col in c_feat:\n    train_df[col] = train_df[col].fillna((train_df[col].min() - 1))\n    train_df[col] = (minmax_scale(train_df[col], feature_range=(0,1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in c_feat:\n    test_df[col] = test_df[col].fillna((test_df[col].min() - 1))\n    test_df[col] = (minmax_scale(test_df[col], feature_range=(0,1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"id_cols=['id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07', 'id_08',\n       'id_09', 'id_10', 'id_11', 'id_12', 'id_13', 'id_14', 'id_15', 'id_16',\n       'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24',\n       'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', 'id_32',\n       'id_33', 'id_34','id_35','id_36','id_37','id_38']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in id_cols:\n     print(\"Feature '{col_name}' has {unique_cat} \".format(col_name=col, unique_cat=train_df[col].dtype))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_id_cols = [col for col in id_cols if train_df[col].dtype in ['object']]\n\ncat_id_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_cat_id=[col for col in id_cols if test_df[col].dtype in ['object']]\ntest_cat_id","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_cat=train_df[cat_id_cols]\ntr_cat.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"train_df.drop(cat_id_cols,axis=1,inplace=True)\ntest_df.drop(cat_id_cols,axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"id_cols=train_df.columns[55:78]\nid_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in id_cols:\n    train_df[col] = (minmax_scale(train_df[col], feature_range=(0,1)))\n    train_df[col].fillna(-1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_id_cols=test_df.columns[54:77]\ntest_id_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in test_id_cols:\n    test_df[col] = (minmax_scale(test_df[col], feature_range=(0,1)))\n    test_df[col].fillna(-1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['addr1'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['addr1'] = (minmax_scale(train_df['addr1'], feature_range=(0,1)))\ntrain_df['addr1'].fillna((train_df['addr1'].max() - 200), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['addr1'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['addr1'] = (minmax_scale(test_df['addr1'], feature_range=(0,1)))\ntest_df['addr1'].fillna((test_df['addr1'].max() - 200), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['addr2'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['addr2'].fillna(87,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['addr2'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['addr2'].fillna(87,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df.shape)\nprint(test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"card_feat=['card1', 'card2', 'card3', 'card4', 'card5', 'card6']\ncard_feat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in card_feat:\n     print(\"Feature '{col_name}' has {unique_cat} \".format(col_name=col, unique_cat=train_df[col].dtype))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cat_card=[col for col in card_feat if train_df[col].dtype in ['object']]\ntrain_cat_card","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_card=[col for col in card_feat if train_df[col].dtype not in ['object']]\nnum_card","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"card_types=pd.crosstab(index=train_df['card4'], columns=\"count\")    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"card_types","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_card_types=pd.crosstab(index=train_df['card6'], columns=\"count\")   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_card_types","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in train_cat_card:\n    train_df[col] = train_df[col].fillna('None')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in train_cat_card:\n    test_df[col] = test_df[col].fillna('None')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in num_card:\n    train_df[col] = train_df[col].fillna((train_df[col].min() - 1))\n    train_df[col] = (minmax_scale(train_df[col], feature_range=(0,1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in num_card:\n    test_df[col] = test_df[col].fillna((test_df[col].min() - 1))\n    test_df[col] = (minmax_scale(test_df[col], feature_range=(0,1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check missing data - Many Columns have more than 50% NA/Null records\ndef missing_data(df) :\n    count = df.isnull().sum()\n    percent = (df.isnull().sum()) / (df.isnull().count()) * 100\n    total = pd.concat([count, percent], axis=1, keys = ['Count', 'Percent'])\n    types = []\n    for col in df.columns :\n        dtypes = str(df[col].dtype)\n        types.append(dtypes)\n    total['dtypes'] = types\n    \n    return np.transpose(total)\n\ntotal=missing_data(train_df)\nprint(total)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## WE will Drop columns with more 50% Null value.\nnull_percent = train_df.isnull().sum()/train_df.shape[0]*100\n\ncols_to_drop = np.array(null_percent[null_percent > 50].index)\n\ncols_to_drop\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop Columns (cols_to_drop) from train and test\ntrain_df = train_df.drop(cols_to_drop, axis=1)\ntest_df = test_df.drop(cols_to_drop,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.fillna(-999)\ntest_df.fillna(-999)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_y = train_df['isFraud']\ntrain_X = train_df.drop('isFraud', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_X.shape)\nprint(test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"null_columns=train_df.columns[train_df.isnull().any()]\ntrain_df[null_columns].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"null_columns=test_df.columns[test_df.isnull().any()]\ntest_df[null_columns].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Label Encoding for categorical variables.\nfor f in train_X.columns:\n    if train_X[f].dtype=='object' or test_df[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(train_X[f].values) + list(test_df[f].values))\n        train_X[f] = lbl.transform(list(train_X[f].values))\n        test_df[f] = lbl.transform(list(test_df[f].values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Create Base Model - LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(train_X, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/ieee-fraud-detection/sample_submission.csv',index_col='TransactionID')\nsubmission['isFraud'] = logreg.predict_proba(test_df)[:,1]\nsubmission.to_csv('Logreg_submission.csv')\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=train_X \nX_test=test_df\ny=train_df['isFraud']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_fold = 5\nfolds = TimeSeriesSplit(n_splits=n_fold)\nfolds = model_selection.KFold(n_splits=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fast_auc(y_true, y_prob):\n    y_true = np.asarray(y_true)\n    y_true = y_true[np.argsort(y_prob)]\n    nfalse = 0\n    auc = 0\n    n = len(y_true)\n    for i in range(n):\n        y_i = y_true[i]\n        nfalse += (1 - y_i)\n        auc += y_i * nfalse\n    auc /= (nfalse * (n - nfalse))\n    return auc\n\n\ndef eval_auc(y_true, y_pred):\n    \"\"\"\n    Fast auc eval function for lgb.\n    \"\"\"\n    return 'auc', fast_auc(y_true, y_pred), True\n\n\ndef group_mean_log_mae(y_true, y_pred, types, floor=1e-9):\n    \"\"\"\n    Fast metric computation for this competition: https://www.kaggle.com/c/champs-scalar-coupling\n    Code is from this kernel: https://www.kaggle.com/uberkinder/efficient-metric\n    \"\"\"\n    maes = (y_true-y_pred).abs().groupby(types).mean()\n    return np.log(maes.map(lambda x: max(x, floor))).mean()\n    \n\ndef train_model_regression(X, X_test, y, params, folds=None, model_type='lgb', eval_metric='mae', columns=None, plot_feature_importance=False, model=None,\n                               verbose=10000, early_stopping_rounds=200, n_estimators=50000, splits=None, n_folds=3):\n    \"\"\"\n    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: y - target\n    :params: folds - folds to split data\n    :params: model_type - type of model to use\n    :params: eval_metric - metric to use\n    :params: columns - columns to use. If None - use all columns\n    :params: plot_feature_importance - whether to plot feature importance of LGB\n    :params: model - sklearn model, works only for \"sklearn\" model type\n    \n    \"\"\"\n    columns = X.columns if columns is None else columns\n    X_test = X_test[columns]\n    splits = folds.split(X) if splits is None else splits\n    n_splits = folds.n_splits if splits is None else n_folds\n    \n    # to set up scoring parameters\n    metrics_dict = {'mae': {'lgb_metric_name': 'mae',\n                        'catboost_metric_name': 'MAE',\n                        'sklearn_scoring_function': metrics.mean_absolute_error},\n                    'group_mae': {'lgb_metric_name': 'mae',\n                        'catboost_metric_name': 'MAE',\n                        'scoring_function': group_mean_log_mae},\n                    'mse': {'lgb_metric_name': 'mse',\n                        'catboost_metric_name': 'MSE',\n                        'sklearn_scoring_function': metrics.mean_squared_error}\n                    }\n\n    \n    result_dict = {}\n    \n    # out-of-fold predictions on train data\n    oof = np.zeros(len(X))\n    \n    # averaged predictions on train data\n    prediction = np.zeros(len(X_test))\n    \n    # list of scores on folds\n    scores = []\n    feature_importance = pd.DataFrame()\n    \n    # split and train on folds\n    for fold_n, (train_index, valid_index) in enumerate(splits):\n        if verbose:\n            print(f'Fold {fold_n + 1} started at {time.ctime()}')\n        if type(X) == np.ndarray:\n            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n            y_train, y_valid = y[train_index], y[valid_index]\n        else:\n            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n            \n        if model_type == 'lgb':\n            model = lgb.LGBMRegressor(**params, n_estimators = n_estimators, n_jobs = -1)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n            \n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n        \n        oof[valid_index] = y_pred_valid.reshape(-1,)\n        if eval_metric != 'group_mae':\n            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n        else:\n            scores.append(metrics_dict[eval_metric]['scoring_function'](y_valid, y_pred_valid, X_valid['type']))\n\n        prediction += y_pred    \n        \n        if model_type == 'lgb' and plot_feature_importance:\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction /= n_splits\n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    result_dict['oof'] = oof\n    result_dict['prediction'] = prediction\n    result_dict['scores'] = scores\n    \n    if model_type == 'lgb':\n        if plot_feature_importance:\n            feature_importance[\"importance\"] /= n_splits\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n            \n            result_dict['feature_importance'] = feature_importance\n        \n    return result_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model_classification(X, X_test, y, params, folds, model_type='lgb', eval_metric='auc', columns=None, plot_feature_importance=False, model=None,\n                               verbose=10000, early_stopping_rounds=200, n_estimators=50000, splits=None, n_folds=3, averaging='usual', n_jobs=-1):\n    \"\"\" \n    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: y - target\n    :params: folds - folds to split data\n    :params: model_type - type of model to use\n    :params: eval_metric - metric to use\n    :params: columns - columns to use. If None - use all columns\n    :params: plot_feature_importance - whether to plot feature importance of LGB\n    :params: model - sklearn model, works only for \"sklearn\" model type\n    \n    \"\"\"\n    columns = X.columns if columns is None else columns\n    n_splits = folds.n_splits if splits is None else n_folds\n    X_test = X_test[columns]\n    \n    # to set up scoring parameters\n    metrics_dict = {'auc': {'lgb_metric_name': eval_auc,\n                        'catboost_metric_name': 'AUC',\n                        'sklearn_scoring_function': metrics.roc_auc_score},\n                    }\n    \n    result_dict = {}\n    if averaging == 'usual':\n        # out-of-fold predictions on train data\n        oof = np.zeros((len(X), 1))\n\n        # averaged predictions on train data\n        prediction = np.zeros((len(X_test), 1))\n        \n    elif averaging == 'rank':\n        # out-of-fold predictions on train data\n        oof = np.zeros((len(X), 1))\n\n        # averaged predictions on train data\n        prediction = np.zeros((len(X_test), 1))\n\n    \n    # list of scores on folds\n    scores = []\n    feature_importance = pd.DataFrame()\n    \n    # split and train on folds\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n        print(f'Fold {fold_n + 1} started at {time.ctime()}')\n        if type(X) == np.ndarray:\n            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n            y_train, y_valid = y[train_index], y[valid_index]\n        else:\n            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n            \n        if model_type == 'lgb':\n            model = lgb.LGBMClassifier(**params, n_estimators=n_estimators, n_jobs = n_jobs)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n            \n            y_pred_valid = model.predict_proba(X_valid)[:, 1]\n            y_pred = model.predict_proba(X_test, num_iteration=model.best_iteration_)[:, 1]\n        \n        if averaging == 'usual':\n            \n            oof[valid_index] = y_pred_valid.reshape(-1, 1)\n            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n            \n            prediction += y_pred.reshape(-1, 1)\n\n        elif averaging == 'rank':\n                                  \n            oof[valid_index] = y_pred_valid.reshape(-1, 1)\n            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n                                  \n            prediction += pd.Series(y_pred).rank().values.reshape(-1, 1)        \n        \n        if model_type == 'lgb' and plot_feature_importance:\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction /= n_splits\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    result_dict['oof'] = oof\n    result_dict['prediction'] = prediction\n    result_dict['scores'] = scores\n    \n    if model_type == 'lgb':\n        if plot_feature_importance:\n            feature_importance[\"importance\"] /= n_splits\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n            \n            result_dict['feature_importance'] = feature_importance\n            result_dict['top_columns'] = cols\n        \n    return result_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'num_leaves': 256,\n          'min_child_samples': 79,\n          'objective': 'binary',\n          'max_depth': 13,\n          'learning_rate': 0.03,\n          \"boosting_type\": \"gbdt\",\n          \"subsample_freq\": 3,\n          \"subsample\": 0.9,\n          \"bagging_seed\": 11,\n          \"metric\": 'auc',\n          \"verbosity\": -1,\n          'reg_alpha': 0.3,\n          'reg_lambda': 0.3,\n          'colsample_bytree': 0.9,\n          #'categorical_feature': cat_cols\n         }\nresult_dict_lgb = train_model_classification(X=X, X_test=X_test, y=y, params=params, folds=folds, model_type='lgb', eval_metric='auc', plot_feature_importance=True,\n                                                      verbose=500, early_stopping_rounds=200, n_estimators=5000, averaging='usual', n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('/kaggle/input/ieee-fraud-detection/sample_submission.csv')\nsub['isFraud'] = result_dict_lgb['prediction']\nsub.to_csv('submission.csv', index=False)\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}