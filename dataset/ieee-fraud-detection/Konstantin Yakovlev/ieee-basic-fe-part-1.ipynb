{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# General imports\nimport numpy as np\nimport pandas as pd\nimport os, sys, gc, warnings, random, datetime, math\n\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split, KFold,GroupShuffleSplit\nfrom sklearn.preprocessing import LabelEncoder\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"########################### Helpers\n#################################################################################\n## Seeder\n# :seed to make all processes deterministic     # type: int\ndef seed_everything(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n    \n## Global frequency encoding    \ndef frequency_encoding(df, columns, self_encoding=False):\n    for col in columns:\n        fq_encode = df[col].value_counts(dropna=False).to_dict()\n        if self_encoding:\n            df[col] = df[col].map(fq_encode)\n        else:\n            df[col+'_fq_enc'] = df[col].map(fq_encode)\n    return df\n\n\ndef values_normalization(dt_df, periods, columns, enc_type='both'):\n    for period in periods:\n        for col in columns:\n            new_col = col +'_'+ period\n            dt_df[col] = dt_df[col].astype(float)  \n\n            temp_min = dt_df.groupby([period])[col].agg(['min']).reset_index()\n            temp_min.index = temp_min[period].values\n            temp_min = temp_min['min'].to_dict()\n\n            temp_max = dt_df.groupby([period])[col].agg(['max']).reset_index()\n            temp_max.index = temp_max[period].values\n            temp_max = temp_max['max'].to_dict()\n\n            temp_mean = dt_df.groupby([period])[col].agg(['mean']).reset_index()\n            temp_mean.index = temp_mean[period].values\n            temp_mean = temp_mean['mean'].to_dict()\n\n            temp_std = dt_df.groupby([period])[col].agg(['std']).reset_index()\n            temp_std.index = temp_std[period].values\n            temp_std = temp_std['std'].to_dict()\n\n            dt_df['temp_min'] = dt_df[period].map(temp_min)\n            dt_df['temp_max'] = dt_df[period].map(temp_max)\n            dt_df['temp_mean'] = dt_df[period].map(temp_mean)\n            dt_df['temp_std'] = dt_df[period].map(temp_std)\n            \n            if enc_type=='both':\n                dt_df[new_col+'_min_max'] = (dt_df[col]-dt_df['temp_min'])/(dt_df['temp_max']-dt_df['temp_min'])\n                dt_df[new_col+'_std_score'] = (dt_df[col]-dt_df['temp_mean'])/(dt_df['temp_std'])\n            elif enc_type=='norm':\n                 dt_df[new_col+'_std_score'] = (dt_df[col]-dt_df['temp_mean'])/(dt_df['temp_std'])\n            elif enc_type=='min_max':\n                dt_df[new_col+'_min_max'] = (dt_df[col]-dt_df['temp_min'])/(dt_df['temp_max']-dt_df['temp_min'])\n\n            del dt_df['temp_min'],dt_df['temp_max'],dt_df['temp_mean'],dt_df['temp_std']\n    return dt_df\n\ndef get_new_columns(temp_list):\n    temp_list = [col for col in list(full_df) if col not in temp_list]\n    temp_list.sort()\n\n    temp_list2 = [col if col not in remove_features else '-' for col in temp_list ]\n    temp_list2.sort()\n\n    temp_list = {'New columns (including dummy)': temp_list,\n                 'New Features': temp_list2}\n    temp_list = pd.DataFrame.from_dict(temp_list)\n    return temp_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Vars\n#################################################################################\nSEED = 42\nseed_everything(SEED)\nLOCAL_TEST = True\nMAKE_TESTS = True\nTARGET = 'isFraud'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Model params\nlgb_params = {\n                    'objective':'binary',\n                    'boosting_type':'gbdt',\n                    'metric':'auc',\n                    'n_jobs':-1,\n                    'learning_rate':0.01,\n                    'num_leaves': 2**8,\n                    'max_depth':-1,\n                    'tree_learner':'serial',\n                    'colsample_bytree': 0.7,\n                    'subsample_freq':1,\n                    'subsample':0.7,\n                    'n_estimators':80000,\n                    'max_bin':255,\n                    'verbose':-1,\n                    'seed': SEED,\n                    'early_stopping_rounds':100, \n                } ","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"########################### Model\nimport lightgbm as lgb\n\ndef make_test(old_score=0, output=False):\n\n    features_columns = [col for col in list(full_df) if col not in remove_features]\n    train_mask = full_df['TransactionID'].isin(local_train_id['TransactionID'])\n    test_mask = full_df['TransactionID'].isin(local_test_id['TransactionID'])\n    \n    X,y = full_df[train_mask][features_columns], full_df[train_mask][TARGET]    \n    P,P_y = full_df[test_mask][features_columns], full_df[test_mask][TARGET]  \n\n    for col in list(X):\n        if X[col].dtype=='O':\n            X[col] = X[col].fillna('unseen_before_label')\n            P[col] = P[col].fillna('unseen_before_label')\n\n            X[col] = X[col].astype(str)\n            P[col] = P[col].astype(str)\n\n            le = LabelEncoder()\n            le.fit(list(X[col])+list(P[col]))\n            X[col] = le.transform(X[col])\n            P[col]  = le.transform(P[col])\n\n            X[col] = X[col].astype('category')\n            P[col] = P[col].astype('category')\n        \n    tt_df = full_df[test_mask][['TransactionID','DT_W',TARGET]]        \n    tt_df['prediction'] = 0\n    \n    tr_data = lgb.Dataset(X, label=y)\n    vl_data = lgb.Dataset(P, label=P_y) \n    estimator = lgb.train(\n            lgb_params,\n            tr_data,\n            valid_sets = [tr_data, vl_data],\n            verbose_eval = 200,\n        )   \n        \n    tt_df['prediction'] = estimator.predict(P)\n    feature_imp = pd.DataFrame(sorted(zip(estimator.feature_importance(),X.columns)), columns=['Value','Feature'])\n    \n    if output:\n        tt_df[['TransactionID','prediction']].to_csv('oof.csv',index=False)\n        print('---Wrote OOF to file---')\n    \n    m_results = []\n    print('#'*20)\n    g_auc = metrics.roc_auc_score(tt_df[TARGET], tt_df['prediction'])\n    score_diff = g_auc - old_score\n    print('Global AUC', g_auc)\n    m_results.append(g_auc)\n    \n    for i in range(full_df[test_mask]['DT_W'].min(), full_df[test_mask]['DT_W'].max()+1):\n        mask = tt_df['DT_W']==i\n        w_auc = metrics.roc_auc_score(tt_df[mask][TARGET], tt_df[mask]['prediction'])\n        print('Week', i, w_auc, len(tt_df[mask]))\n        m_results.append(w_auc)\n        \n    print('#'*20)\n    print('Features Preformance:', g_auc)\n    print('Diff with previous__:', score_diff)\n    \n    return tt_df, feature_imp, m_results, estimator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### DATA LOAD\n#################################################################################\nprint('Load Data')\ntrain_df = pd.read_pickle('../input/ieee-data-minification-private/train_transaction.pkl')\ntest_df = pd.read_pickle('../input/ieee-data-minification-private/test_transaction.pkl')\n\n# Full Data set (careful with target encoding)\nfull_df = pd.concat([train_df, test_df]).reset_index(drop=True)\n\n# Local test IDs with one month gap\nlocal_test_id  = train_df[train_df['DT_M']==train_df['DT_M'].max()].reset_index(drop=True)\nlocal_train_id = train_df[train_df['DT_M']<(train_df['DT_M'].max()-1)].reset_index(drop=True)\nlocal_train_id = local_train_id[['TransactionID']]\nlocal_test_id  = local_test_id[['TransactionID']]\ndel train_df, test_df\n\n# Identity Data set\ntrain_identity = pd.read_pickle('../input/ieee-data-minification-private/train_identity.pkl')\ntest_identity = pd.read_pickle('../input/ieee-data-minification-private/test_identity.pkl')\nidentity_df = pd.concat([train_identity, test_identity]).reset_index(drop=True)\ndel train_identity, test_identity\n\nprint('Shape control (for local test):', local_train_id.shape, local_test_id.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### All features columns\n#################################################################################\n# Add list of feature that we will remove for sure\nremove_features = [\n    'TransactionID','TransactionDT', \n    TARGET,\n    'DT','DT_M','DT_W','DT_D','DTT',\n    'DT_hour','DT_day_week','DT_day_month',\n    'DT_M_total','DT_W_total','DT_D_total',\n    'is_december','is_holiday','temp','weight',\n    ]\n\n# Make sure that TransactionAmt is float64\n# To not lose values during aggregations\nfull_df['TransactionAmt'] = full_df['TransactionAmt'].astype(float)\n\n# Base lists for features to do frequency encoding\n# and saved initial state\nfq_encode = []\nbase_columns = list(full_df)\n\n# We don't need V columns in the initial phase \n# removing them to make predictions faster\nremove_features += ['V'+str(i) for i in range(1,340)]\n\n# Removing transformed D columns\nremove_features += ['uid_td_D'+str(i) for i in range(1,16) if i!=9]\n\n# Make sure we have m_results variable\nm_results = [0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### This is start baseline\nif MAKE_TESTS:\n    tt_df, feature_imp, m_results, model = make_test()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Fix card columns and encode\nprint('Fix card4 and card6 values')\nsaved_state = list(full_df)\n####\n\n####\n# card4 and card5 have strong connection\n# with card1 - we can unify values\n# to guarantee that it will be same combinations\n# for all data.\n\n# I've tried to fill others NaNs\n# But seems that there are no more bad values.\n# All rest NaNs are meaningful.\n####\n\nfull_df['card6'] = np.where(full_df['card6']==30, np.nan, full_df['card6'])\nfull_df['card6'] = np.where(full_df['card6']==16, np.nan, full_df['card6'])\n\ni_cols = ['card4','card6']\n\nfor col in i_cols:\n    temp_df = full_df.groupby(['card1',col])[col].agg(['count']).reset_index()\n    temp_df = temp_df.sort_values(by=['card1','count'], ascending=False)\n    del temp_df['count']\n    temp_df = temp_df.drop_duplicates(subset=['card1'], keep='first').reset_index(drop=True)\n    temp_df.index = temp_df['card1'].values\n    temp_df = temp_df[col].to_dict()\n    full_df[col] = full_df['card1'].map(temp_df)\n    \n# Add cards features for later encoding\ni_cols = ['card1','card2','card3','card4','card5','card6']\nfq_encode += i_cols\n\n####\nif MAKE_TESTS:\n    print(get_new_columns(saved_state))\n    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n####","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Client Virtual ID\nprint('Create client identification ID')\nsaved_state = list(full_df)\n####\n\n####\n# Client subgroups:\n\n# bank_type -> looking on card3 and card5 distributions\n# I would say it is bank branch and country\n# full_addr -> Client registration address in bank\n# uid1 -> client identification by bank and card type\n# uid2 -> client identification with additional geo information\n####\n\n# Bank type\nfull_df['bank_type'] = full_df['card3'].astype(str)+'_'+full_df['card5'].astype(str)\n\n# Full address\nfull_df['full_addr'] = full_df['addr1'].astype(str)+'_'+full_df['addr2'].astype(str)\n\n# Virtual client uid\ni_cols = ['card1','card2','card3','card4','card5','card6']\nfull_df['uid1'] = ''\nfor col in i_cols:\n    full_df['uid1'] += full_df[col].astype(str)+'_'\n\n# Virtual client uid + full_addr\nfull_df['uid2'] = full_df['uid1']+'_'+full_df['full_addr'].astype(str)\n\n\n# Add uids features for later encoding\ni_cols = ['full_addr','bank_type','uid1','uid2']\nfq_encode += i_cols\n\n# We can't use this features directly because\n# test data will have many unknow values\nremove_features += i_cols\n\n# We've created just \"ghost\" features -> no need to run test\nif False: \n    print(get_new_columns(saved_state))\n    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n####","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Client identification using deltas\nprint('Create client identification ID using deltas')\nsaved_state = list(full_df)\n####\n\n# Temporary list\nclient_cols = []\n\n# Convert all delta columns to some date\n# D8 and D9 are not days deltas -\n# we can try convert D8 to int and \n# probably it will give us date\n# but I'm very very unsure about it.\n\n# We will do all D columns transformation\n# (but save original values) as we will\n# use it later for other features.\n\nfor col in ['D'+str(i) for i in range(1,16) if i!=9]: \n    new_col = 'uid_td_'+str(col)\n    \n    new_col = 'uid_td_'+str(col)\n    full_df[new_col] = full_df['TransactionDT'] / (24*60*60)\n    full_df[new_col] = np.floor(full_df[new_col] - full_df[col])    \n    remove_features.append(new_col)\n    \n    # Date is useless itself -> add to dummy features\n    #remove_features.append(new_col)\n\n\n# The most possible deltas to identify account or client\n# initial activity are 'D1','D10','D15'\n# We can try to find certain client using uid and date\n# If client is the same uid+date combination will be\n# unique per client and all his transactions\nfor col in ['D1','D10','D15']:\n    new_col = 'uid_td_'+str(col)\n\n    # card1 + full_addr + date\n    full_df[new_col+'_cUID_1'] = full_df['card1'].astype(str)+'_'+full_df['full_addr'].astype(str)+'_'+full_df[new_col].astype(str)\n    \n    # uid1 + full_addr + date\n    full_df[new_col+'_cUID_2'] = full_df['uid2'].astype(str)+'_'+full_df[new_col].astype(str)\n\n    # columns 'D1','D2' are clipped we can't trust maximum values\n    if col in ['D1','D2']:\n        full_df[new_col+'_cUID_1'] = np.where(full_df[col]>=640, 'very_old_client', full_df[new_col+'_cUID_1'])\n        full_df[new_col+'_cUID_2'] = np.where(full_df[col]>=640, 'very_old_client', full_df[new_col+'_cUID_2'])\n\n    full_df[new_col+'_cUID_1'] = np.where(full_df[col].isna(), np.nan, full_df[new_col+'_cUID_1'])\n    full_df[new_col+'_cUID_2'] = np.where(full_df[col].isna(), np.nan, full_df[new_col+'_cUID_2'])\n\n    # reset cUID_1 if both address are nan (very unstable prediction)\n    full_df[new_col+'_cUID_1'] = np.where(full_df['addr1'].isna()&full_df['addr2'].isna(), np.nan, full_df[new_col+'_cUID_1'])\n\n    # cUID is useless itself -> add to dummy features\n    remove_features += [new_col+'_cUID_1',new_col+'_cUID_2']\n    \n    # Add to temporary list (to join with encoding list later)\n    client_cols += [new_col+'_cUID_1',new_col+'_cUID_2']\n    \n## Best candidate for client complete identification\n## uid_td_D1_cUID_1\n        \n# Add cUIDs features for later encoding\nfq_encode += client_cols\n\n# We will save this list and even append \n# few more columns for later use\nclient_cols += ['card1','card2','card3','card4','card5',\n                'uid1','uid2']\n\n####\n# We've created just \"ghost\" features -> no need to run test\nif False: \n    print(get_new_columns(saved_state))\n    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n####","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Mark card columns \"outliers\"\nprint('Outliers mark')\nsaved_state = list(full_df)\n####\n\n####\n# We are checking card and uid activity -\n# weither activity is constant during the year\n# or we have just single card/account use cases.\n\n# These features are categorical ones and\n# Catboost benefits the most from them.\n\n# Strange things:\n# - \"Time window\" should be big enough \n# - Doesn't work for DT_W and DT_D\n# even when local test showing score boost.\n\n# Seems to me that catboost start to combine \n# them with themselfs and loosing \"magic\".\n####\n\ni_cols = client_cols.copy()\nperiods = ['DT_M'] \n\nfor period in periods:\n    for col in i_cols:\n        full_df[col+'_catboost_check_'+period] = full_df.groupby([col])[period].transform('nunique')\n        full_df[col+'_catboost_check_'+period] = np.where(full_df[col+'_catboost_check_'+period]==1,1,0)\n        \n####\nif MAKE_TESTS:\n    print(get_new_columns(saved_state))\n    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n####","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### V columns compact and assign groups\nprint('V columns / Nan groups')\nsaved_state = list(full_df)\n####\n\n####\n# Nangroups identification are categorical features\n# and Catboost benefits the most from them.\n\n# Mean/std just occasion transformation.\n####\n\nnans_groups = {}\nnans_df = full_df.isna()\n\ni_cols = ['V'+str(i) for i in range(1,340)]\nfor col in i_cols:\n    cur_group = nans_df[col].sum()\n    try:\n        nans_groups[cur_group].append(col)\n    except:\n        nans_groups[cur_group]=[col]\n\nfor col in nans_groups:\n    # Very doubtful features -> Seems it works in tandem with other feature\n    # But I'm not sure\n    full_df['nan_group_sum_'+str(col)] = full_df[nans_groups[col]].to_numpy().sum(axis=1)\n    full_df['nan_group_mean_'+str(col)] = full_df[nans_groups[col]].to_numpy().mean(axis=1)\n        \n    # lgbm doesn't benefit from such feature -> \n    # let's transform and add it to dummy features list\n    full_df['nan_group_catboost_'+str(col)]  = np.where(nans_df[nans_groups[col]].sum(axis=1)>0,1,0).astype(np.int8)\n    remove_features.append('nan_group_catboost_'+str(col))\n        \n####\nif MAKE_TESTS:\n    print(get_new_columns(saved_state))\n    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n####","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Mean encoding using M columns\nprint('Mean encoding, using M columns')\nsaved_state = list(full_df)\n####\n\nmain_cols = {\n             'uid_td_D1_cUID_1':   ['M'+str(i) for i in [2,3,5,7,8,9]],\n             'uid_td_D1_cUID_2':   ['M'+str(i) for i in [2,3,5,6,9]],\n             'uid_td_D10_cUID_1':  ['M'+str(i) for i in [5,7,8,9]],\n             'uid_td_D10_cUID_2':  ['M'+str(i) for i in [3,6,7,8]],\n             'uid_td_D15_cUID_1':  ['M'+str(i) for i in [2,3,5,6,8,]],\n             'uid_td_D15_cUID_2':  ['M'+str(i) for i in [2,3,5,6,7,8]],\n             'card1':  ['M'+str(i) for i in [2,3,5,6,7,8,9]],\n             'card2':  ['M'+str(i) for i in [1,2,3,7,9]],\n             'card4':  ['M'+str(i) for i in [3,7,8]],\n             'card5':  ['M'+str(i) for i in [5,6,8]],\n             'uid1':   ['M'+str(i) for i in [3,5,6,7,8,9]],\n             'uid2':   ['M'+str(i) for i in [2,3,5,6,7,8,9]],\n            }\n\nfor main_col,i_cols in main_cols.items():\n    for agg_type in ['mean']:\n        temp_df = full_df[[main_col]+i_cols]\n        temp_df = temp_df.groupby([main_col])[i_cols].transform(agg_type)\n        temp_df.columns = [main_col+'_'+col+'_'+agg_type for col in list(temp_df)]\n        full_df = pd.concat([full_df,temp_df], axis=1)\n        \n####\nif MAKE_TESTS:\n    print(get_new_columns(saved_state))\n    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n####","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### D Columns Mean/Std\nprint('D columns Mean/Std')\nsaved_state = list(full_df)\n####\n\ni_cols = ['D'+str(i) for i in range(1,16)]\nmain_cols = {\n             'uid_td_D1_cUID_1': ['D'+str(i) for i in [1,2,3,10,11,14,15]],\n            }\n\nfor main_col,i_cols in main_cols.items():\n    print(main_col)\n    for agg_type in ['mean','std']:\n        temp_df = full_df.groupby([main_col])[i_cols].transform(agg_type)\n        temp_df.columns = [main_col+'_'+col+'_'+agg_type for col in list(temp_df)]\n        full_df = pd.concat([full_df,temp_df], axis=1)\n        \n####\nif MAKE_TESTS:\n    print(get_new_columns(saved_state))\n    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n####","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### TransactionAmt\nprint('TransactionAmt normalization')\nsaved_state = list(full_df)\n####\n\n# Decimal part\nfull_df['TransactionAmt_cents'] = np.round(100.*(full_df['TransactionAmt'] - np.floor(full_df['TransactionAmt'])),0)\nfull_df['TransactionAmt_cents'] = full_df['TransactionAmt_cents'].astype(np.int8)\n\n# Clip top values\nfull_df['TransactionAmt'] = full_df['TransactionAmt'].clip(0,5000)\n\n# Normalization by product\nmain_cols = [\n             'uid_td_D1_cUID_1','uid_td_D1_cUID_2',\n             'uid_td_D10_cUID_1','uid_td_D10_cUID_2',\n             'uid_td_D15_cUID_1','uid_td_D15_cUID_2',\n             'card1','card3',\n            ]\n\nfor col in main_cols:\n    for agg_type in ['mean','std']:\n        full_df[col+'_TransactionAmt_Product_' + agg_type] =\\\n                full_df.groupby([col,'ProductCD'])['TransactionAmt'].transform(agg_type)\n\n    f_std = col+'_TransactionAmt_Product_std'\n    f_mean = col+'_TransactionAmt_Product_mean'\n    full_df[col+'_Product_norm'] = (full_df['TransactionAmt']-full_df[f_mean])/full_df[f_std]\n    del full_df[f_mean], full_df[f_std]\n    \n\n####\nif MAKE_TESTS:\n    print(get_new_columns(saved_state))\n    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n####","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### TransactionAmt clients columns encoding\nprint('TransactionAmt encoding clients columns')\nsaved_state = list(full_df)\n####\n\ni_cols = ['TransactionAmt']\nmain_cols = client_cols.copy()\n\nfor main_col in main_cols:\n    print(main_col)\n    for agg_type in ['mean','std']:\n        temp_df = full_df.groupby([main_col])[i_cols].transform(agg_type)\n        temp_df.columns = [main_col+'_'+col+'_'+agg_type for col in list(temp_df)]\n        full_df = pd.concat([full_df,temp_df], axis=1)\n\n####\nif MAKE_TESTS:\n    print(get_new_columns(saved_state))\n    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n####","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Mark card columns \"outliers\"\nprint('Categorical outliers')\n## \nsaved_state = list(full_df)\n####\n\ni_cols = ['TransactionAmt','ProductCD','P_emaildomain','R_emaildomain',]\nperiods = ['DT_M']\n\nfor period in periods:\n    for col in i_cols:\n        full_df[col+'_catboost_check_'+period] = full_df.groupby([col])[period].transform('nunique')\n        full_df[col+'_catboost_check_'+period] = np.where(full_df[col+'_catboost_check_'+period]==1,1,0).astype(np.int8)\n\n        \n####\nif MAKE_TESTS:\n    print(get_new_columns(saved_state))\n    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n####","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### D Columns Normalize and remove original columns\nprint('D columns transformations')\n## \nsaved_state = list(full_df)\n####\n\n# Remove original features\n# test data will have many unknow values\ni_cols = ['D'+str(i) for i in range(1,16)]\nremove_features += i_cols\n\n####### Values Normalization\ni_cols.remove('D1')\ni_cols.remove('D2')\ni_cols.remove('D9')\nperiods = ['DT_D']\nfor col in i_cols:\n    full_df[col] = full_df[col].clip(0)\nfull_df = values_normalization(full_df, periods, i_cols, enc_type='norm')\n\ni_cols = ['D1','D2','D9']\nfor col in i_cols:\n    full_df[col+'_scaled'] = full_df[col]/full_df[col].max()\n\n\n####\nif MAKE_TESTS:\n    print(get_new_columns(saved_state))\n    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n####","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Dist\nprint('Distance normalization')\n## \nsaved_state = list(full_df)\n####\n\ni_cols = ['dist1','dist2']\nmain_cols = [\n             'uid_td_D1_cUID_1',\n             'card1',\n            ]\n\n\nfor main_col in main_cols:\n    print(main_col)\n    for agg_type in ['mean','std']:\n        temp_df = full_df.groupby([main_col])[i_cols].transform(agg_type)\n        temp_df.columns = [main_col+'_'+col+'_'+agg_type for col in list(temp_df)]\n        full_df = pd.concat([full_df,temp_df], axis=1)\n    \n    for col in i_cols:\n        f_std = main_col+'_'+col+'_std'\n        f_mean = main_col+'_'+col+'_mean'\n        full_df[main_col+'_'+col+'_norm'] = (full_df[col]-full_df[f_mean])/full_df[f_std]\n        del full_df[f_mean], full_df[f_std]\n\n\n####\nif MAKE_TESTS:\n    print(get_new_columns(saved_state))\n    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n####","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Count similar transactions per period\nprint('Similar transactions per period')\n## \nsaved_state = list(full_df)\n####\n\nperiods = ['DT_W','DT_D'] \n\nfor period in periods:\n    full_df['TransactionAmt_Product_counts_' + period] =\\\n        full_df.groupby([period,'ProductCD','TransactionAmt'])['TransactionAmt'].transform('count')\n    full_df['TransactionAmt_Product_counts_' + period] /= full_df[period+'_total']\n\n####\nif MAKE_TESTS:\n    print(get_new_columns(saved_state))\n    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n####","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Find nunique dates per client\nprint('Nunique dates per client')\n## \nsaved_state = list(full_df)\n####\n\nmain_cols = {\n            'uid_td_D1_cUID_1': ['uid_td_D'+str(i) for i in range(2,16) if i!=9] + ['D8','D9'],\n            }\n\nfor main_col,i_cols in main_cols.items():\n    for col in i_cols:\n        full_df[col+'_catboost_check_'+main_col] = full_df.groupby([main_col])[col].transform('nunique')\n\n####\nif MAKE_TESTS:\n    print(get_new_columns(saved_state))\n    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n####","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Email transformation\nprint('Email split')\nsaved_state = list(full_df)\n####\n\np = 'P_emaildomain'\nr = 'R_emaildomain'\n\nfull_df['full_email'] = full_df[p].astype(str) +'_'+ full_df[r].astype(str)\nfull_df['email_p_extension'] = full_df[p].apply(lambda x: str(x).split('.')[-1])\nfull_df['email_r_extension'] = full_df[r].apply(lambda x: str(x).split('.')[-1])\nfull_df['email_p_domain'] = full_df[p].apply(lambda x: str(x).split('.')[0])\nfull_df['email_r_domain'] = full_df[r].apply(lambda x: str(x).split('.')[0])\n\ni_cols = ['P_emaildomain','R_emaildomain',\n          'full_email',\n          'email_p_extension','email_r_extension',\n          'email_p_domain','email_r_domain']\n\nfull_df = frequency_encoding(full_df, i_cols, self_encoding=True)\n\n####\nif MAKE_TESTS:\n    print(get_new_columns(saved_state))\n    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n####","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Device info and identity\nprint('Identity sets')\nsaved_state = list(full_df)\n####\n\n########################### Device info\nidentity_df['DeviceInfo'] = identity_df['DeviceInfo'].fillna('unknown_device').str.lower()\nidentity_df['DeviceInfo_device'] = identity_df['DeviceInfo'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))\nidentity_df['DeviceInfo_version'] = identity_df['DeviceInfo'].apply(lambda x: ''.join([i for i in x if i.isnumeric()]))\n    \n########################### Device info 2\nidentity_df['id_30'] = identity_df['id_30'].fillna('unknown_device').str.lower()\nidentity_df['id_30_device'] = identity_df['id_30'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))\nidentity_df['id_30_version'] = identity_df['id_30'].apply(lambda x: ''.join([i for i in x if i.isnumeric()]))\n    \n########################### Browser\nidentity_df['id_31'] = identity_df['id_31'].fillna('unknown_device').str.lower()\nidentity_df['id_31_device'] = identity_df['id_31'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))\n    \n########################### Merge Identity columns\ntemp_df = full_df[['TransactionID']]\ntemp_df = temp_df.merge(identity_df, on=['TransactionID'], how='left')\ndel temp_df['TransactionID']\nfull_df = pd.concat([full_df,temp_df], axis=1)\n  \ni_cols = [\n          'DeviceInfo','DeviceInfo_device','DeviceInfo_version',\n          'id_30','id_30_device','id_30_version',\n          'id_31','id_31_device',\n          'id_33','DeviceType'\n         ]\n\n####### Global Self frequency encoding\nfull_df = frequency_encoding(full_df, i_cols, self_encoding=True)\n\n####\nif MAKE_TESTS:\n    print(get_new_columns(saved_state))\n    tt_df, feature_imp, m_results, model = make_test(m_results[0])\n####","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Export\nfull_df.to_pickle('baseline_full_df.pkl')\n\nremove_features_df = pd.DataFrame(remove_features, columns=['features_to_remove'])\nremove_features_df.to_pickle('baseline_remove_features.pkl')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}