{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Summary\n\nThe basic intent of this notebook is to look at how the `scale_pos_weight` parameter affects the prediction distribution and prediction performance metrics.\n\nThe LGBM parameters are mostly arbitrary. They are selected based on some notebooks I've seen for this competition. Either way, I'm not really looking to optimize my score, more just to show the effect of the `scale_pos_weight` parameter."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_parquet('../input/feature-engineering/train_eng.parquet')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parquet file format doesn't handle float16 so we will need to only re-type these columns\nnumerics = ['float32', 'float64']\n\ndef reduce_mem_usage(df, float_cols, verbose=True):\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in float_cols:\n        col_type = df[col].dtypes\n        if col_type == 'float32':\n            df[col] = df[col].astype(np.float16)\n            c_min = df[col].min()\n            c_max = df[col].max()\n        elif col_type == 'float64':\n            if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                df[col] = df[col].astype(np.float16)\n            elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                df[col] = df[col].astype(np.float32)\n            else:\n                pass  \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"float_cols = [col for col, dtype in train.dtypes.items() if dtype in numerics]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain = reduce_mem_usage(train, float_cols=float_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.sort_values(by=['TransactionDT'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train['isFraud'].copy()\nX = train.drop(columns=['TransactionDT', 'index', 'isFraud', 'TransactionID'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n          'num_leaves': 175,\n          'feature_fraction': 0.5,\n          'bagging_freq': 50,\n          'bagging_fraction': 0.75,\n          'min_data_in_leaf': 40,\n          'objective': 'binary',\n          'max_bin': 255,\n          'max_depth': -1,\n          'learning_rate': 0.02,\n          'scale_pos_weight': 25,\n          \"boosting_type\": \"gbdt\",\n          \"bagging_seed\": 11,\n          \"metric\": 'auc',\n          \"verbosity\": -1,\n          'random_state': 47\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_all = {}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training\nWill only train on the 2nd time series fold. And will use the 3rd time series fold to validate.\n\nNot really looking to optimize the hyperparameters, mainly trying to see how `scale_pos_weight` affects the prediction metrics."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.model_selection import GridSearchCV, train_test_split, TimeSeriesSplit\n\nfolds = TimeSeriesSplit(n_splits=5)\nfor fold, (trn_idx, test_idx) in enumerate(folds.split(X, y)):\n    if fold == 1:\n        for weight in [1, 3, 15, 45, 85]:\n            test_idx_3 = test_idx\n            X_test = X.iloc[test_idx]\n            y_test = y.iloc[test_idx]\n            params['scale_pos_weight'] = weight\n            print('Training on {:,} records'.format(len(trn_idx)))\n            trn_data = lgb.Dataset(X.iloc[trn_idx], label=y.iloc[trn_idx])\n            val_data = lgb.Dataset(X.iloc[test_idx], label=y.iloc[test_idx])\n            clf = lgb.train(params, trn_data, 400, \n                            valid_sets = [trn_data, val_data], \n                            verbose_eval=100)\n            predictions = clf.predict(data=X_test)\n            predictions_all[str(weight)] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score, precision_score, recall_score, precision_recall_curve, f1_score, roc_curve, average_precision_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,12))\nplt.subplots_adjust(left=0.25, right=1.25, top=1.5)\ncurrent = 1\nfor i, weight in enumerate([1, 3, 15, 45, 85]):\n    preds = pd.DataFrame(data={'y': y.iloc[test_idx_3], 'preds': predictions_all[str(weight)]})\n    f1 = f1_score(y_true=y.iloc[test_idx_3], y_pred=np.round(predictions_all[str(weight)]))\n    auc = roc_auc_score(y_true=y.iloc[test_idx_3], y_score=predictions_all[str(weight)])\n    recall = recall_score(y_true=y.iloc[test_idx_3], y_pred=np.round(predictions_all[str(weight)]))\n    precision = precision_score(y_true=y.iloc[test_idx_3], y_pred=np.round(predictions_all[str(weight)]))\n    avg_precision = average_precision_score(y_true=y.iloc[test_idx_3], y_score=predictions_all[str(weight)])\n    \n    plt.subplot(3, 2, current)\n    current += 1\n    plt.title(f'Scale: {weight}, AUC: {np.round(auc, 3)}, F1: {np.round(f1, 3)}, ' +\n              f'\\n Recall: {np.round(recall, 3)} Precision: {np.round(precision, 3)} Avg Precision: {np.round(avg_precision, 3)}')\n    sns.distplot(preds[preds['y']==1]['preds'], label='1')\n    sns.distplot(preds[preds['y']==0]['preds'], label='0')\n    \nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Results\n\n\nAt first glance, a small level of positive weight scaling results in more divergent looking distributions (we don't have most records predicted at around < 0.1 probability of fraud. \n\n**However, AUC on the validation set decreased each time `scale_pos_weight` was increased. **\nBecause AUC is simply concerned with relative ranking of predictions, this is not surprising.\n\n**The only metric that actually improved was recall**, which makes perfect sense considering recall is TP / (TP + FN) and the number of false negatives would decrease as we add weight to the positive records. \n\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}