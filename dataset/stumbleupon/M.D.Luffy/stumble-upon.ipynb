{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"!pip install pytorch_model_summary","metadata":{"execution":{"iopub.status.busy":"2021-11-18T12:59:52.601865Z","iopub.execute_input":"2021-11-18T12:59:52.602587Z","iopub.status.idle":"2021-11-18T12:59:59.791801Z","shell.execute_reply.started":"2021-11-18T12:59:52.602549Z","shell.execute_reply":"2021-11-18T12:59:59.79074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tqdm\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport seaborn as sns\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport sklearn.metrics as skm\n\nfrom pytorch_model_summary import summary\nfrom sklearn.model_selection import train_test_split\nfrom transformers import BertModel, BertTokenizer, AutoConfig, pipeline\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"   # \"last_expr\"\nrandom_seed=42","metadata":{"execution":{"iopub.status.busy":"2021-11-18T12:59:59.79428Z","iopub.execute_input":"2021-11-18T12:59:59.794606Z","iopub.status.idle":"2021-11-18T12:59:59.801726Z","shell.execute_reply.started":"2021-11-18T12:59:59.794562Z","shell.execute_reply":"2021-11-18T12:59:59.800983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load DataSet","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('../input/stumbleupon/train.tsv', delimiter='\\t')\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-18T12:59:59.803116Z","iopub.execute_input":"2021-11-18T12:59:59.803471Z","iopub.status.idle":"2021-11-18T13:00:00.320455Z","shell.execute_reply.started":"2021-11-18T12:59:59.803432Z","shell.execute_reply":"2021-11-18T13:00:00.319707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_df.shape) # Shape of train dataset","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:00:00.322595Z","iopub.execute_input":"2021-11-18T13:00:00.323026Z","iopub.status.idle":"2021-11-18T13:00:00.32967Z","shell.execute_reply.started":"2021-11-18T13:00:00.322985Z","shell.execute_reply":"2021-11-18T13:00:00.328872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Replace `?` with nan as per dataset `?` is null","metadata":{}},{"cell_type":"code","source":"train_df.replace(to_replace='?', value=np.nan, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:00:00.331289Z","iopub.execute_input":"2021-11-18T13:00:00.331974Z","iopub.status.idle":"2021-11-18T13:00:00.341232Z","shell.execute_reply.started":"2021-11-18T13:00:00.331905Z","shell.execute_reply":"2021-11-18T13:00:00.340498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data info","metadata":{}},{"cell_type":"code","source":"train_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:00:00.342737Z","iopub.execute_input":"2021-11-18T13:00:00.343159Z","iopub.status.idle":"2021-11-18T13:00:00.373712Z","shell.execute_reply.started":"2021-11-18T13:00:00.343123Z","shell.execute_reply":"2021-11-18T13:00:00.372915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['alchemy_category_score'] = train_df['alchemy_category_score'].astype(dtype='float')\ntrain_df['news_front_page'] = train_df['news_front_page'].astype(dtype='float')\ntrain_df['is_news'] = train_df['is_news'].astype(dtype='float')","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:00:00.375164Z","iopub.execute_input":"2021-11-18T13:00:00.375438Z","iopub.status.idle":"2021-11-18T13:00:00.386935Z","shell.execute_reply.started":"2021-11-18T13:00:00.375406Z","shell.execute_reply":"2021-11-18T13:00:00.38615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Desribe Data","metadata":{}},{"cell_type":"code","source":"train_df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:00:00.38836Z","iopub.execute_input":"2021-11-18T13:00:00.388803Z","iopub.status.idle":"2021-11-18T13:00:00.469534Z","shell.execute_reply.started":"2021-11-18T13:00:00.388758Z","shell.execute_reply":"2021-11-18T13:00:00.468517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"* We will be only using the text column for classifing the data\n* We can use python `eval` function to convert to `dict` object and access as keys\n* There are null text in `boilerplate` column replace null with empty string so that we don't have any error while using `eval` function","metadata":{}},{"cell_type":"markdown","source":"## Clean the `boilerplate` text column","metadata":{}},{"cell_type":"code","source":"train_df['boilerplate'] = train_df['boilerplate'].replace(to_replace=':null', value=':\"\"', regex=True) # replace null with empty strings","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:00:00.470773Z","iopub.execute_input":"2021-11-18T13:00:00.471108Z","iopub.status.idle":"2021-11-18T13:00:00.498929Z","shell.execute_reply.started":"2021-11-18T13:00:00.47107Z","shell.execute_reply":"2021-11-18T13:00:00.498222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check Label Count Distrubtion","metadata":{}},{"cell_type":"code","source":"train_df['label'].value_counts()\ntrain_df['label'].value_counts().plot(kind='barh', color='g')","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:00:00.502284Z","iopub.execute_input":"2021-11-18T13:00:00.502483Z","iopub.status.idle":"2021-11-18T13:00:00.729005Z","shell.execute_reply.started":"2021-11-18T13:00:00.50246Z","shell.execute_reply":"2021-11-18T13:00:00.728149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Concatenate all the boiler plate text","metadata":{}},{"cell_type":"code","source":"def concat_text(boilerplate_dict):\n    text = ''\n    for key in boilerplate_dict:\n        text += f\" {boilerplate_dict[key]}\"\n    return text.strip()","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:00:00.730383Z","iopub.execute_input":"2021-11-18T13:00:00.730921Z","iopub.status.idle":"2021-11-18T13:00:00.736221Z","shell.execute_reply.started":"2021-11-18T13:00:00.730877Z","shell.execute_reply":"2021-11-18T13:00:00.735143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['total_text'] = train_df['boilerplate'].apply(lambda x: concat_text(eval(x)))","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:00:00.737509Z","iopub.execute_input":"2021-11-18T13:00:00.738444Z","iopub.status.idle":"2021-11-18T13:00:01.094555Z","shell.execute_reply.started":"2021-11-18T13:00:00.738406Z","shell.execute_reply":"2021-11-18T13:00:01.093692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stats of text length in words","metadata":{}},{"cell_type":"code","source":"train_df['total_text_length (words)'] = train_df['total_text'].apply(lambda x: len(x.split()))","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:00:01.097706Z","iopub.execute_input":"2021-11-18T13:00:01.09844Z","iopub.status.idle":"2021-11-18T13:00:01.306018Z","shell.execute_reply.started":"2021-11-18T13:00:01.0984Z","shell.execute_reply":"2021-11-18T13:00:01.305316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['total_text_length (words)'].describe()\ntrain_df['total_text_length (words)'].plot(kind='hist', color='b')","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:00:01.307266Z","iopub.execute_input":"2021-11-18T13:00:01.307608Z","iopub.status.idle":"2021-11-18T13:00:01.535686Z","shell.execute_reply.started":"2021-11-18T13:00:01.30757Z","shell.execute_reply":"2021-11-18T13:00:01.53497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* We will take bert default 512 sequence length for tokenizer because 75% of data is near 632 length this would be better fit\n* While using tokenizer we will truncate sequence bigger than this.","metadata":{}},{"cell_type":"markdown","source":"# Data preprocessing and DataLoader ","metadata":{}},{"cell_type":"markdown","source":"## Target Label Encoding","metadata":{}},{"cell_type":"code","source":"cat2idx = {label: i for i, label in enumerate(sorted(train_df['label'].unique()))}\nidx2cat = {i: label for i, label in enumerate(sorted(train_df['label'].unique()))}","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:00:01.536976Z","iopub.execute_input":"2021-11-18T13:00:01.537555Z","iopub.status.idle":"2021-11-18T13:00:01.544005Z","shell.execute_reply.started":"2021-11-18T13:00:01.537514Z","shell.execute_reply":"2021-11-18T13:00:01.543191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining Text Tokenizer using `BertTokenizer`","metadata":{}},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:00:01.545388Z","iopub.execute_input":"2021-11-18T13:00:01.545893Z","iopub.status.idle":"2021-11-18T13:00:06.129775Z","shell.execute_reply.started":"2021-11-18T13:00:01.545852Z","shell.execute_reply":"2021-11-18T13:00:06.129086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset iterator","metadata":{}},{"cell_type":"code","source":"class Dataset(torch.utils.data.Dataset):\n    def __init__(self, df, source_column, target_column, max_length, transform=None):\n        self.df = df\n        self.transform = transform\n        \n        # get source and target texts\n        self.source_texts = [tokenizer(text, padding='max_length', max_length = max_length, truncation=True,\n                                return_tensors=\"pt\") for text in self.df[source_column]]\n        self.targets = self.df[target_column].map(cat2idx)\n    \n    def classes(self):\n        return self.targets\n    \n    def __len__(self):\n        return len(self.targets)\n    \n    def get_batch_labels(self, idx):\n        # Fetch a batch of labels\n        return np.array(self.targets[idx])\n    \n    def get_batch_texts(self, idx):\n        # Fetch a batch of inputs\n        return self.source_texts[idx]\n    \n    def __getitem__(self, idx):\n\n        batch_texts = self.get_batch_texts(idx)\n        batch_y = self.get_batch_labels(idx)\n\n        return batch_texts, batch_y","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:00:06.130959Z","iopub.execute_input":"2021-11-18T13:00:06.131312Z","iopub.status.idle":"2021-11-18T13:00:06.139997Z","shell.execute_reply.started":"2021-11-18T13:00:06.131276Z","shell.execute_reply":"2021-11-18T13:00:06.139215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train and Validation Split","metadata":{}},{"cell_type":"code","source":"y_train = train_df['label'] \nX_train = train_df.drop(['label'], axis=1)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, stratify=y_train,random_state=random_seed)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:00:06.141454Z","iopub.execute_input":"2021-11-18T13:00:06.141894Z","iopub.status.idle":"2021-11-18T13:00:06.168824Z","shell.execute_reply.started":"2021-11-18T13:00:06.141857Z","shell.execute_reply":"2021-11-18T13:00:06.168145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.reset_index(drop=True, inplace=True)\ny_train.reset_index(drop=True, inplace=True)\nX_val.reset_index(drop=True, inplace=True)\ny_val.reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:00:06.170025Z","iopub.execute_input":"2021-11-18T13:00:06.170291Z","iopub.status.idle":"2021-11-18T13:00:06.175808Z","shell.execute_reply.started":"2021-11-18T13:00:06.170257Z","shell.execute_reply":"2021-11-18T13:00:06.174954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape\ny_train.shape\nX_val.shape\ny_val.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:00:06.177212Z","iopub.execute_input":"2021-11-18T13:00:06.177666Z","iopub.status.idle":"2021-11-18T13:00:06.192104Z","shell.execute_reply.started":"2021-11-18T13:00:06.177626Z","shell.execute_reply":"2021-11-18T13:00:06.19141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.value_counts(normalize=True)*100\n\ny_train.value_counts().plot(kind='barh',color='green')\n\ny_val.value_counts(normalize=True)*100\n\ny_val.value_counts().plot(kind='barh',color='orange')","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:00:06.193448Z","iopub.execute_input":"2021-11-18T13:00:06.193881Z","iopub.status.idle":"2021-11-18T13:00:06.40056Z","shell.execute_reply.started":"2021-11-18T13:00:06.193846Z","shell.execute_reply":"2021-11-18T13:00:06.39981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train['label'] = y_train\nX_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:00:06.401852Z","iopub.execute_input":"2021-11-18T13:00:06.402096Z","iopub.status.idle":"2021-11-18T13:00:06.427953Z","shell.execute_reply.started":"2021-11-18T13:00:06.402063Z","shell.execute_reply":"2021-11-18T13:00:06.427231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_val['label'] = y_val\nX_val.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:00:06.429522Z","iopub.execute_input":"2021-11-18T13:00:06.429987Z","iopub.status.idle":"2021-11-18T13:00:06.455893Z","shell.execute_reply.started":"2021-11-18T13:00:06.429948Z","shell.execute_reply":"2021-11-18T13:00:06.455228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Data Iterator","metadata":{}},{"cell_type":"code","source":"train_iter = Dataset(X_train, 'total_text', 'label', 512)\nX_train['total_text_length (words)'][0]\ntrain_iter[0]","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:00:06.456937Z","iopub.execute_input":"2021-11-18T13:00:06.457141Z","iopub.status.idle":"2021-11-18T13:01:42.727877Z","shell.execute_reply.started":"2021-11-18T13:00:06.457116Z","shell.execute_reply":"2021-11-18T13:01:42.727053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Validation Data Iterator","metadata":{}},{"cell_type":"code","source":"val_iter = Dataset(X_val, 'total_text', 'label', 512)\nX_val['total_text_length (words)'][0]\nval_iter[0]","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:01:42.72908Z","iopub.execute_input":"2021-11-18T13:01:42.729379Z","iopub.status.idle":"2021-11-18T13:01:59.189701Z","shell.execute_reply.started":"2021-11-18T13:01:42.729341Z","shell.execute_reply":"2021-11-18T13:01:59.189015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DataLoader","metadata":{}},{"cell_type":"code","source":"train_dataloader = torch.utils.data.DataLoader(train_iter, batch_size=32, shuffle=True)\nval_dataloader = torch.utils.data.DataLoader(val_iter, batch_size=32)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:01:59.19111Z","iopub.execute_input":"2021-11-18T13:01:59.191409Z","iopub.status.idle":"2021-11-18T13:01:59.19667Z","shell.execute_reply.started":"2021-11-18T13:01:59.191374Z","shell.execute_reply":"2021-11-18T13:01:59.195849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Defintion","metadata":{}},{"cell_type":"markdown","source":"## Metrics","metadata":{}},{"cell_type":"code","source":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n\ndef f1_score(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(skm.f1_score(preds.cpu(), labels.cpu(), average='weighted'))\n","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:24:14.792005Z","iopub.execute_input":"2021-11-18T13:24:14.792275Z","iopub.status.idle":"2021-11-18T13:24:14.797919Z","shell.execute_reply.started":"2021-11-18T13:24:14.792244Z","shell.execute_reply":"2021-11-18T13:24:14.797135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Use GPU or not ","metadata":{}},{"cell_type":"code","source":"use_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:01:59.215873Z","iopub.execute_input":"2021-11-18T13:01:59.216388Z","iopub.status.idle":"2021-11-18T13:01:59.261201Z","shell.execute_reply.started":"2021-11-18T13:01:59.216351Z","shell.execute_reply":"2021-11-18T13:01:59.260428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Input Size to model and number of classes","metadata":{}},{"cell_type":"code","source":"input_size = 768\nnum_classes = len(cat2idx)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:01:59.262306Z","iopub.execute_input":"2021-11-18T13:01:59.26295Z","iopub.status.idle":"2021-11-18T13:01:59.270721Z","shell.execute_reply.started":"2021-11-18T13:01:59.262912Z","shell.execute_reply":"2021-11-18T13:01:59.269996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Architecture","metadata":{}},{"cell_type":"code","source":"class LogisticRegression(nn.Module):\n    def __init__(self):\n        super(LogisticRegression, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased', )\n        self.linear = nn.Linear(input_size, num_classes)\n    \n    def forward(self, input_id, mask):\n        # pooled outout is 768 dimension vector , _ is rest of vectors of bert model\n        _, pooled_output = self.bert(input_ids=input_id, attention_mask=mask,return_dict=False) \n        out = self.linear(pooled_output)\n        return out \n    \n    def training_step(self, batch):\n        bert_dict, labels = batch \n        out = self(bert_dict['input_ids'].squeeze(1).to(device), \n                                bert_dict['attention_mask'].to(device)) # Generate predictions\n        labels = labels.to(device)\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        bert_dict, labels = batch \n        out = self(bert_dict['input_ids'].squeeze(1).to(device), \n                                bert_dict['attention_mask'].to(device)) # Generate predictions\n        labels = labels.to(device)\n        loss = F.cross_entropy(out, labels)                             # Calculate loss\n        acc = accuracy(out, labels)                                     # Calculate accuracy\n        f1_sc = f1_score(out, labels)\n        return {'val_loss': loss, 'val_acc': acc, 'f1_score': f1_sc}\n    \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()                    # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()                       # Combine accuracies\n        batch_f1 = [x['f1_score'] for x in outputs]\n        epoch_f1 = torch.stack(batch_f1).mean()                          # Combine f1 scores\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item(), 'f1_score': epoch_f1.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(f\"Epoch [{epoch}], val_loss: {result['val_loss']:.4f}, val_acc: {result['val_acc']:.4f} val_f1_score: {result['f1_score']:.4f}\")\n    ","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:33:50.859992Z","iopub.execute_input":"2021-11-18T13:33:50.860573Z","iopub.status.idle":"2021-11-18T13:33:50.874522Z","shell.execute_reply.started":"2021-11-18T13:33:50.860533Z","shell.execute_reply":"2021-11-18T13:33:50.873441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate Model","metadata":{}},{"cell_type":"code","source":"def evaluate(model, val_loader):\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:33:54.116043Z","iopub.execute_input":"2021-11-18T13:33:54.116741Z","iopub.status.idle":"2021-11-18T13:33:54.123033Z","shell.execute_reply.started":"2021-11-18T13:33:54.116704Z","shell.execute_reply":"2021-11-18T13:33:54.122365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Model","metadata":{}},{"cell_type":"markdown","source":"## Intialize  Model","metadata":{}},{"cell_type":"code","source":"model = LogisticRegression()","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:33:57.196158Z","iopub.execute_input":"2021-11-18T13:33:57.196747Z","iopub.status.idle":"2021-11-18T13:34:00.249511Z","shell.execute_reply.started":"2021-11-18T13:33:57.196708Z","shell.execute_reply":"2021-11-18T13:34:00.248616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Disable training of bert embedding layer","metadata":{}},{"cell_type":"code","source":"for param in model.bert.parameters():\n    param.requires_grad = False\n    #print(param.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:34:05.753012Z","iopub.execute_input":"2021-11-18T13:34:05.753279Z","iopub.status.idle":"2021-11-18T13:34:05.757919Z","shell.execute_reply.started":"2021-11-18T13:34:05.753249Z","shell.execute_reply":"2021-11-18T13:34:05.757227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Summary","metadata":{}},{"cell_type":"code","source":"if torch.cuda.is_available():\n    model.cuda()","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:34:11.347907Z","iopub.execute_input":"2021-11-18T13:34:11.348549Z","iopub.status.idle":"2021-11-18T13:34:11.476421Z","shell.execute_reply.started":"2021-11-18T13:34:11.348509Z","shell.execute_reply":"2021-11-18T13:34:11.475683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch, labels  = next(iter(train_dataloader))\nmask = batch['attention_mask'].to(device)\ninput_id = batch['input_ids'].squeeze(1).to(device)\nprint(summary(model, input_id, mask))","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:34:18.157847Z","iopub.execute_input":"2021-11-18T13:34:18.158112Z","iopub.status.idle":"2021-11-18T13:34:18.184129Z","shell.execute_reply.started":"2021-11-18T13:34:18.158084Z","shell.execute_reply":"2021-11-18T13:34:18.183417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Intial Model Loss and accuracy using model evaulation","metadata":{}},{"cell_type":"code","source":"evaluate(model, val_dataloader)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:34:21.606044Z","iopub.execute_input":"2021-11-18T13:34:21.606601Z","iopub.status.idle":"2021-11-18T13:34:41.701374Z","shell.execute_reply.started":"2021-11-18T13:34:21.606562Z","shell.execute_reply":"2021-11-18T13:34:41.70053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train Function","metadata":{}},{"cell_type":"code","source":"def fit(epochs, learning_rate, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n    optimizer=opt_func(model.parameters(), learning_rate)\n    history = []\n    for epoch in range(epochs):\n        \n        # Training Phase\n        for batch in train_loader:\n            loss = model.training_step(batch)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            \n        # Validation Phase\n        result = evaluate(model, val_loader)\n        model.epoch_end(epoch, result)\n        history.append(result)\n        \n    return history","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:35:01.917064Z","iopub.execute_input":"2021-11-18T13:35:01.917743Z","iopub.status.idle":"2021-11-18T13:35:01.923633Z","shell.execute_reply.started":"2021-11-18T13:35:01.917706Z","shell.execute_reply":"2021-11-18T13:35:01.922683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history1 = fit(5, 0.001, model, train_dataloader, val_dataloader)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:35:08.074007Z","iopub.execute_input":"2021-11-18T13:35:08.074623Z","iopub.status.idle":"2021-11-18T13:46:16.235597Z","shell.execute_reply.started":"2021-11-18T13:35:08.074573Z","shell.execute_reply":"2021-11-18T13:46:16.234777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history2 = fit(5, 0.001, model, train_dataloader, val_dataloader)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:46:38.069634Z","iopub.execute_input":"2021-11-18T13:46:38.069907Z","iopub.status.idle":"2021-11-18T13:57:46.132928Z","shell.execute_reply.started":"2021-11-18T13:46:38.069878Z","shell.execute_reply":"2021-11-18T13:57:46.13124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history3 = fit(5, 0.001, model, train_dataloader, val_dataloader)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:58:51.543942Z","iopub.execute_input":"2021-11-18T13:58:51.544523Z","iopub.status.idle":"2021-11-18T14:09:59.55951Z","shell.execute_reply.started":"2021-11-18T13:58:51.544479Z","shell.execute_reply":"2021-11-18T14:09:59.558777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history4 = fit(5, 0.001, model, train_dataloader, val_dataloader)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T14:10:12.34371Z","iopub.execute_input":"2021-11-18T14:10:12.343966Z","iopub.status.idle":"2021-11-18T14:21:20.420885Z","shell.execute_reply.started":"2021-11-18T14:10:12.343938Z","shell.execute_reply":"2021-11-18T14:21:20.419937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test Model and Submission","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv('../input/stumbleupon/test.tsv', delimiter='\\t')\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-18T14:24:49.097013Z","iopub.execute_input":"2021-11-18T14:24:49.09783Z","iopub.status.idle":"2021-11-18T14:24:49.334021Z","shell.execute_reply.started":"2021-11-18T14:24:49.097792Z","shell.execute_reply":"2021-11-18T14:24:49.333257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['boilerplate'] = test_df['boilerplate'].replace(to_replace=':null', value=':\"\"', regex=True) # replace null with empty strings","metadata":{"execution":{"iopub.status.busy":"2021-11-18T14:30:47.528681Z","iopub.execute_input":"2021-11-18T14:30:47.528978Z","iopub.status.idle":"2021-11-18T14:30:47.541868Z","shell.execute_reply.started":"2021-11-18T14:30:47.528946Z","shell.execute_reply":"2021-11-18T14:30:47.541186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['total_text'] = test_df['boilerplate'].apply(lambda x: concat_text(eval(x)))\ntest_df['total_text_length (words)'] = test_df['total_text'].apply(lambda x: len(x.split()))","metadata":{"execution":{"iopub.status.busy":"2021-11-18T14:32:07.017886Z","iopub.execute_input":"2021-11-18T14:32:07.018496Z","iopub.status.idle":"2021-11-18T14:32:07.243897Z","shell.execute_reply.started":"2021-11-18T14:32:07.018459Z","shell.execute_reply":"2021-11-18T14:32:07.243208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = []\n\nfor text in test_df['total_text']:\n    bert_dict = tokenizer(text, padding='max_length', max_length = 512, truncation=True,\n                                return_tensors=\"pt\")\n    mask = bert_dict['attention_mask'].to(device)\n    input_id = bert_dict['input_ids'].squeeze(1).to(device)\n    output = model(input_id, mask)\n    pred = output.argmax(dim=1)\n    predictions.append(pred.cpu().numpy()[0])\n","metadata":{"execution":{"iopub.status.busy":"2021-11-18T14:41:11.19303Z","iopub.execute_input":"2021-11-18T14:41:11.193457Z","iopub.status.idle":"2021-11-18T14:43:09.087832Z","shell.execute_reply.started":"2021-11-18T14:41:11.193418Z","shell.execute_reply":"2021-11-18T14:43:09.086992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['label'] = predictions\ntest_df.to_csv('submission.csv',columns=['urlid','label'],index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T14:56:41.059322Z","iopub.execute_input":"2021-11-18T14:56:41.059717Z","iopub.status.idle":"2021-11-18T14:56:41.080432Z","shell.execute_reply.started":"2021-11-18T14:56:41.059672Z","shell.execute_reply":"2021-11-18T14:56:41.079657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}