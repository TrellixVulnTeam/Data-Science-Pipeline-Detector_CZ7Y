{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Importing all necessary libraries","metadata":{"execution":{"iopub.status.busy":"2021-12-09T17:49:23.774852Z","iopub.execute_input":"2021-12-09T17:49:23.775208Z","iopub.status.idle":"2021-12-09T17:49:24.768361Z","shell.execute_reply.started":"2021-12-09T17:49:23.775109Z","shell.execute_reply":"2021-12-09T17:49:24.767483Z"}}},{"cell_type":"code","source":"%config Completer.use_jedi = False\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport json\nimport sklearn\n\ntrain_path = '../input/stumbleupon/train.tsv'\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2021-12-10T17:07:08.285262Z","iopub.execute_input":"2021-12-10T17:07:08.285909Z","iopub.status.idle":"2021-12-10T17:07:09.399001Z","shell.execute_reply.started":"2021-12-10T17:07:08.285795Z","shell.execute_reply":"2021-12-10T17:07:09.39799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading Data","metadata":{}},{"cell_type":"code","source":"data=pd.read_csv(train_path,sep='\\t')\nnew_data = data[['url','boilerplate','label']]","metadata":{"execution":{"iopub.status.busy":"2021-12-10T17:07:09.400934Z","iopub.execute_input":"2021-12-10T17:07:09.401164Z","iopub.status.idle":"2021-12-10T17:07:09.934821Z","shell.execute_reply.started":"2021-12-10T17:07:09.401136Z","shell.execute_reply":"2021-12-10T17:07:09.934119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing Methods","metadata":{}},{"cell_type":"code","source":"def processing(text):\n    text = json.loads(text)\n    return text\n\ndef title_fn(dic):\n    text = dic.get('title')\n    if text != None:\n        return text\n    else:\n        return \"unknown_title\"\n        \ndef body_fn(dic):\n    text = dic.get('body')\n    if text != None:\n        return text\n    else:\n        return \"unknown_body\"\n    \ndef url_fn(dic):\n    text = dic.get('url', 'unknown_url')\n    if text != None:\n        return text\n    else:\n        return \"unknown_url\"\n    ","metadata":{"execution":{"iopub.status.busy":"2021-12-10T17:07:09.936068Z","iopub.execute_input":"2021-12-10T17:07:09.936399Z","iopub.status.idle":"2021-12-10T17:07:09.943367Z","shell.execute_reply.started":"2021-12-10T17:07:09.93637Z","shell.execute_reply":"2021-12-10T17:07:09.942507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fetching and concatenating URL, Title, Body ","metadata":{}},{"cell_type":"code","source":"def transform_data(new_data):\n    new_data.boilerplate = new_data.boilerplate.apply(lambda text: processing(text))\n    new_data['title'] = new_data.boilerplate.apply(title_fn)\n    new_data['Body'] = new_data.boilerplate.apply(body_fn)\n    new_data['b_url'] = new_data.boilerplate.apply(url_fn)\n    \n    return new_data\n\nnew_data = transform_data(data)\nnew_data['full'] = new_data['title'] + new_data['Body'] + new_data['b_url']","metadata":{"execution":{"iopub.status.busy":"2021-12-10T17:07:09.94541Z","iopub.execute_input":"2021-12-10T17:07:09.94566Z","iopub.status.idle":"2021-12-10T17:07:10.113079Z","shell.execute_reply.started":"2021-12-10T17:07:09.945629Z","shell.execute_reply":"2021-12-10T17:07:10.112153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install contractions","metadata":{"execution":{"iopub.status.busy":"2021-12-10T17:07:10.114544Z","iopub.execute_input":"2021-12-10T17:07:10.115569Z","iopub.status.idle":"2021-12-10T17:07:26.023996Z","shell.execute_reply.started":"2021-12-10T17:07:10.115486Z","shell.execute_reply":"2021-12-10T17:07:26.022509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport contractions\nimport unicodedata\nfrom bs4 import BeautifulSoup\nimport string\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\n \nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nstopword = stopwords.words('english')\n  \nps = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\n\n\ndef text_preprocess(text):\n    \n    try:\n        contractions.fix(text)\n    except:\n        text = text\n    else:\n        text = contractions.fix(text)\n    finally:\n        text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')## Removing/normalising accented characters.\n        text = re.sub(r' @[^\\s]*',\"\",text)#Remove @elements\n        #text = re.sub(r'RT[^A-Za-z]+',\"\",text)#Remove RT RETWEET tag\n        text = re.sub(r'(([A-Za-z0-9._-]+)@([A-Za-z0-9._-]+)(\\.)([A-Za-z]{2,8}))',\"\",text) #email\n        text = re.sub(r'([A-Za-z0-9]+)(\\*)+([A-Za-z0-9]+)','starword',text)# replacing ***words with \"star_word\"\n        text = re.sub(r'((https|http|ftp)?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})',\" \",text) #urls\n        text = BeautifulSoup(text, 'lxml').get_text(\" \")#tag removal\n        text = text.lower() #Lowering the characters\n        #text = re.sub(['!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'],'',text)\n        text =  re.sub(r'[^\\w\\s]', '', text)\n        text =  re.sub(r'[0-9]', '', text)\n        tokens = word_tokenize(text)\n        #text = [ps.stem(i) for i in tokens if i not in stopword]\n        text = [lemmatizer.lemmatize(i) for i in tokens if i not in stopword]\n        text = \" \".join(text)\n        \n    return text","metadata":{"execution":{"iopub.status.busy":"2021-12-10T17:07:26.026408Z","iopub.execute_input":"2021-12-10T17:07:26.027462Z","iopub.status.idle":"2021-12-10T17:07:26.973992Z","shell.execute_reply.started":"2021-12-10T17:07:26.027406Z","shell.execute_reply":"2021-12-10T17:07:26.973018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_data['full'] = new_data['full'].apply(lambda x: text_preprocess(x))","metadata":{"execution":{"iopub.status.busy":"2021-12-10T17:07:26.976501Z","iopub.execute_input":"2021-12-10T17:07:26.976818Z","iopub.status.idle":"2021-12-10T17:08:42.226067Z","shell.execute_reply.started":"2021-12-10T17:07:26.976768Z","shell.execute_reply":"2021-12-10T17:08:42.225146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Implementing Count Vectorizer / TFidf vectorizer","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\ncv2 = CountVectorizer( min_df=2)\n\n#transformed_data = cv2.fit_transform(new_data.full)\n\ncv_tf = TfidfVectorizer()\ntransformed_data = cv_tf.fit_transform(new_data.full)\nX = transformed_data\ny = new_data.label.values","metadata":{"execution":{"iopub.status.busy":"2021-12-10T17:08:42.227212Z","iopub.execute_input":"2021-12-10T17:08:42.227427Z","iopub.status.idle":"2021-12-10T17:08:45.033123Z","shell.execute_reply.started":"2021-12-10T17:08:42.227401Z","shell.execute_reply":"2021-12-10T17:08:45.032159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn import metrics\nfrom sklearn.base import clone","metadata":{"execution":{"iopub.status.busy":"2021-12-10T17:08:45.035057Z","iopub.execute_input":"2021-12-10T17:08:45.035387Z","iopub.status.idle":"2021-12-10T17:08:45.041961Z","shell.execute_reply.started":"2021-12-10T17:08:45.035343Z","shell.execute_reply":"2021-12-10T17:08:45.040926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Implementing StratifiedKfold validation and training logistic regression","metadata":{}},{"cell_type":"markdown","source":"## Simple Logistic\nskf = StratifiedKFold(n_splits=5, shuffle=True)\nlog = LogisticRegression(random_state=0, solver = 'liblinear')\n\nfor fold_no, (train_index, val_index) in enumerate(skf.split(X, y)):\n    #print(\"TRAIN:\", train_index, \"VAL:\", val_index)\n    X_train, X_val = X[train_index], X[val_index]\n    y_train, y_val = y[train_index], y[val_index]\n    \n    clf = clone(log)\n    clf.fit(X_train, y_train)\n    pred = clf.predict(X_val)\n    fpr, tpr, thresholds = metrics.roc_curve(y_val, pred, pos_label=1)\n    auc = metrics.auc(fpr,tpr)\n    print(\"this is our fold no - {} and roc_auc_score is{} and auc is {}\".format(fold_no,roc_auc_score(y_val, pred), auc))","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import make_scorer, roc_auc_score\nscore = make_scorer(roc_auc_score, greater_is_better=True)\nparameter_grid = {\n    'solver': ['saga'],\n    'penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n    'C' : np.logspace(-4, 4, 10),\n    'max_iter': [100,500,1000,2000]\n}\n\nlog_clf = LogisticRegression(random_state=0)\nclf = RandomizedSearchCV(log_clf, parameter_grid,cv = 5,n_iter = 30,verbose = 2, n_jobs = -1, scoring = score)\nhistory = clf.fit(X,y)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T17:08:45.043499Z","iopub.execute_input":"2021-12-10T17:08:45.043862Z","iopub.status.idle":"2021-12-10T17:15:18.39748Z","shell.execute_reply.started":"2021-12-10T17:08:45.043784Z","shell.execute_reply":"2021-12-10T17:15:18.396149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimised_logistic = clf.best_estimator_.fit(X,y)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T17:21:04.667857Z","iopub.execute_input":"2021-12-10T17:21:04.668383Z","iopub.status.idle":"2021-12-10T17:21:05.272755Z","shell.execute_reply.started":"2021-12-10T17:21:04.668311Z","shell.execute_reply":"2021-12-10T17:21:05.271659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion\n1. Considering this is our initial approach without any hyperparameter optimisation, we achieved good ROC only with the help of simple logistic regression and count vectorizer.\n\n2. Basically cleaning the text did not workout.\n\n3. let's go for vectorizer change to tfidf vectorizer so after applying it i noticed that score improved from 75 to 81 and it was fast too that's great.\n\n## Creating submission file","metadata":{}},{"cell_type":"code","source":"test_data = pd.read_csv('../input/stumbleupon/test.tsv', sep ='\\t')","metadata":{"execution":{"iopub.status.busy":"2021-12-10T17:27:37.279891Z","iopub.execute_input":"2021-12-10T17:27:37.280673Z","iopub.status.idle":"2021-12-10T17:27:37.444283Z","shell.execute_reply.started":"2021-12-10T17:27:37.28062Z","shell.execute_reply":"2021-12-10T17:27:37.44336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = transform_data(test_data)\ntest_data['full'] = test_data['title'] + test_data['Body'] + test_data['b_url']\ntest_data = test_data[['urlid','full']]","metadata":{"execution":{"iopub.status.busy":"2021-12-10T17:27:38.726207Z","iopub.execute_input":"2021-12-10T17:27:38.727198Z","iopub.status.idle":"2021-12-10T17:27:38.796936Z","shell.execute_reply.started":"2021-12-10T17:27:38.727143Z","shell.execute_reply":"2021-12-10T17:27:38.796064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data['full'] = test_data['full'].apply(lambda x: text_preprocess(x))\ntest_transformed_data = cv_tf.transform(test_data.full)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T17:27:53.466777Z","iopub.execute_input":"2021-12-10T17:27:53.46732Z","iopub.status.idle":"2021-12-10T17:28:25.529433Z","shell.execute_reply.started":"2021-12-10T17:27:53.467266Z","shell.execute_reply":"2021-12-10T17:28:25.528466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = pd.DataFrame(optimised_logistic.predict(test_transformed_data), columns=['label'])\nsubmission_dataframe = pd.concat([test_data,pred],axis=1).drop(['full'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T17:28:33.691759Z","iopub.execute_input":"2021-12-10T17:28:33.692573Z","iopub.status.idle":"2021-12-10T17:28:33.703093Z","shell.execute_reply.started":"2021-12-10T17:28:33.692527Z","shell.execute_reply":"2021-12-10T17:28:33.702249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_dataframe.to_csv('submission1_file.csv',index = False)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T17:28:57.726105Z","iopub.execute_input":"2021-12-10T17:28:57.726676Z","iopub.status.idle":"2021-12-10T17:28:57.740504Z","shell.execute_reply.started":"2021-12-10T17:28:57.726615Z","shell.execute_reply":"2021-12-10T17:28:57.739428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}