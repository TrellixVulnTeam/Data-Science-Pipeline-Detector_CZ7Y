{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nThis is my solution for the [StumbleUpon Evergreen Classification Challenge](https://www.kaggle.com/c/stumbleupon) challenge on Kaggle. In this challange, the task is to predict whether or not a given site is going to be relevant in future. In other words, we are required to predict whether or not a site will be \"evergreen\". To do this, we are provided with the text in the given url and various other meta-data features. Thus, It is a text classification problem and in this notebook, I have approached it with Feedforward Neural Networks having multiple inputs to handle meta-data and text features seperately.","metadata":{}},{"cell_type":"markdown","source":"# Part 1: Exploratory data analysis and data cleaning","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\npd.set_option(\"display.max_columns\", None)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-07T16:24:34.446083Z","iopub.execute_input":"2022-03-07T16:24:34.446566Z","iopub.status.idle":"2022-03-07T16:24:34.466442Z","shell.execute_reply.started":"2022-03-07T16:24:34.446436Z","shell.execute_reply":"2022-03-07T16:24:34.464663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"../input/stumbleupon/train.tsv\", delimiter=\"\\t\")\ntest_df = pd.read_csv(\"../input/stumbleupon/test.tsv\", delimiter=\"\\t\")\nsub_df = pd.read_csv(\"../input/stumbleupon/sampleSubmission.csv\")\n\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:24:34.468521Z","iopub.execute_input":"2022-03-07T16:24:34.469048Z","iopub.status.idle":"2022-03-07T16:24:35.093847Z","shell.execute_reply.started":"2022-03-07T16:24:34.46901Z","shell.execute_reply":"2022-03-07T16:24:35.091023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Seperating **label** from train set.","metadata":{}},{"cell_type":"code","source":"target = train_df[\"label\"]\ntrain_df.drop([\"label\"], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:24:35.097025Z","iopub.execute_input":"2022-03-07T16:24:35.097331Z","iopub.status.idle":"2022-03-07T16:24:35.115034Z","shell.execute_reply.started":"2022-03-07T16:24:35.097287Z","shell.execute_reply":"2022-03-07T16:24:35.11355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**urlid** is for giving ID to various columns so dropping from both train and test sets.","metadata":{}},{"cell_type":"code","source":"train_df.drop([\"urlid\"], axis=1, inplace=True)\ntest_df.drop([\"urlid\"], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:24:35.118255Z","iopub.execute_input":"2022-03-07T16:24:35.118552Z","iopub.status.idle":"2022-03-07T16:24:35.155301Z","shell.execute_reply.started":"2022-03-07T16:24:35.118521Z","shell.execute_reply":"2022-03-07T16:24:35.154202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Extracting the site name from the site URL and changing the column name from **url** to **site_name**.","metadata":{}},{"cell_type":"code","source":"def get_domain(url):\n    temp = url.split(\"/\")\n    if temp[0][:4] == \"http\":\n        temp = temp[2]\n    else:\n        temp = temp[0]\n\n    temp = temp.split(\".\")\n    if temp[0][:3] == \"www\":\n        temp = temp[1]\n    else:\n        temp = temp[0]\n\n    return temp\n\n\ntrain_df[\"url\"] = train_df[\"url\"].apply(get_domain)\ntest_df[\"url\"] = test_df[\"url\"].apply(get_domain)\n\ntrain_df.rename(columns={\"url\": \"site_name\"}, inplace=True)\ntest_df.rename(columns={\"url\": \"site_name\"}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:24:35.159076Z","iopub.execute_input":"2022-03-07T16:24:35.159762Z","iopub.status.idle":"2022-03-07T16:24:35.223591Z","shell.execute_reply.started":"2022-03-07T16:24:35.159707Z","shell.execute_reply":"2022-03-07T16:24:35.221708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are many sites which occur less than 5 time in the whole dataset. Grouping these site into the \"other\" category.","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\n\nsite = train_df[\"site_name\"].value_counts()\nsite_map = pd.Series(site.index, index=site.index)\n\nsite_map[site < 5] = \"other\"\ndel site\n\ntrain_df[\"site_name\"] = train_df[\"site_name\"].map(site_map)\ntest_df[\"site_name\"] = test_df[\"site_name\"].map(site_map)\ndel site_map\n\ntest_df[\"site_name\"] = SimpleImputer(strategy=\"constant\", fill_value=\"other\").fit_transform(test_df[[\"site_name\"]])","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:24:35.231372Z","iopub.execute_input":"2022-03-07T16:24:35.231823Z","iopub.status.idle":"2022-03-07T16:24:36.095364Z","shell.execute_reply.started":"2022-03-07T16:24:35.231772Z","shell.execute_reply":"2022-03-07T16:24:36.093862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Imputing missing values in **alchemy_category_score** using SimpleImputer with median strategy.","metadata":{}},{"cell_type":"code","source":"imputer = SimpleImputer(strategy=\"median\")\n\ntrain_df[\"alchemy_category_score\"].replace({\"?\": np.nan}, inplace=True)\ntest_df[\"alchemy_category_score\"].replace({\"?\": np.nan}, inplace=True)\n\ntrain_df[\"alchemy_category_score\"] = train_df[\"alchemy_category_score\"].astype(float)\ntest_df[\"alchemy_category_score\"] = test_df[\"alchemy_category_score\"].astype(float)\n\ntrain_df[\"alchemy_category_score\"] = imputer.fit_transform(train_df[[\"alchemy_category_score\"]])\ntest_df[\"alchemy_category_score\"] = imputer.fit_transform(test_df[[\"alchemy_category_score\"]])","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:24:36.097135Z","iopub.execute_input":"2022-03-07T16:24:36.09745Z","iopub.status.idle":"2022-03-07T16:24:36.134031Z","shell.execute_reply.started":"2022-03-07T16:24:36.097395Z","shell.execute_reply":"2022-03-07T16:24:36.132222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Analyzing the missing value count and dtype of various columns in train and test sets.","metadata":{}},{"cell_type":"code","source":"train_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:24:36.135148Z","iopub.execute_input":"2022-03-07T16:24:36.135401Z","iopub.status.idle":"2022-03-07T16:24:36.174338Z","shell.execute_reply.started":"2022-03-07T16:24:36.135372Z","shell.execute_reply":"2022-03-07T16:24:36.172434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:24:36.176658Z","iopub.execute_input":"2022-03-07T16:24:36.177787Z","iopub.status.idle":"2022-03-07T16:24:36.205478Z","shell.execute_reply.started":"2022-03-07T16:24:36.17773Z","shell.execute_reply":"2022-03-07T16:24:36.203309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Calculating the number of unique values of various columns in train and test sets.","metadata":{}},{"cell_type":"code","source":"pd.DataFrame({\n    \"Train\": train_df[test_df.columns].nunique(),\n    \"Test\": test_df.nunique()\n})","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:24:36.207978Z","iopub.execute_input":"2022-03-07T16:24:36.208313Z","iopub.status.idle":"2022-03-07T16:24:36.32744Z","shell.execute_reply.started":"2022-03-07T16:24:36.208274Z","shell.execute_reply":"2022-03-07T16:24:36.326057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**framebased** has 0 variance so dropping it from both train and test sets and also typecasting the dtype of various categorical columns to \"object\".","metadata":{}},{"cell_type":"code","source":"train_df.drop([\"framebased\"], axis=1, inplace=True)\ntest_df.drop([\"framebased\"], axis=1, inplace=True)\n\ncat_cols = [\n    \"site_name\",\n    \"alchemy_category\",\n    \"hasDomainLink\",\n    \"is_news\",\n    \"lengthyLinkDomain\",\n    \"news_front_page\",\n    \"numwords_in_url\"\n]\ncont_cols = list(set(test_df.columns) - set(cat_cols + [\"boilerplate\"]))\n\ncat_cols = np.array(cat_cols)\ncont_cols = np.array(cont_cols)\n\ntrain_df[cat_cols] = train_df[cat_cols].astype(str)\ntest_df[cat_cols] = test_df[cat_cols].astype(str)\n\ntrain_df[cont_cols] = train_df[cont_cols].astype(float)\ntest_df[cont_cols] = test_df[cont_cols].astype(float)","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:24:36.330139Z","iopub.execute_input":"2022-03-07T16:24:36.330654Z","iopub.status.idle":"2022-03-07T16:24:36.407975Z","shell.execute_reply.started":"2022-03-07T16:24:36.330605Z","shell.execute_reply":"2022-03-07T16:24:36.406701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some basic statistical parameters for numerical columns in train and test sets.","metadata":{}},{"cell_type":"code","source":"train_df[cont_cols].describe()","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:24:36.409926Z","iopub.execute_input":"2022-03-07T16:24:36.410562Z","iopub.status.idle":"2022-03-07T16:24:36.486068Z","shell.execute_reply.started":"2022-03-07T16:24:36.410513Z","shell.execute_reply":"2022-03-07T16:24:36.484504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[cont_cols].describe()","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:24:36.487954Z","iopub.execute_input":"2022-03-07T16:24:36.488336Z","iopub.status.idle":"2022-03-07T16:24:36.567839Z","shell.execute_reply.started":"2022-03-07T16:24:36.488272Z","shell.execute_reply":"2022-03-07T16:24:36.566661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plotting histogram for all the numerical columns in the train set.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=6, ncols=3, figsize=(20, 20))\nax = np.array(ax).ravel()\n\nfor i, col in enumerate(cont_cols):\n    ax[i].hist(train_df[col], bins=50)\n    ax[i].set_title(col)\n    ax[i].grid(True)\n\nfig.show()\nfig.savefig(\"feature_histogram.jpeg\")","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:24:36.569923Z","iopub.execute_input":"2022-03-07T16:24:36.57034Z","iopub.status.idle":"2022-03-07T16:24:41.904514Z","shell.execute_reply.started":"2022-03-07T16:24:36.570284Z","shell.execute_reply":"2022-03-07T16:24:41.902521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 2: Feature engineering categorical and text features","metadata":{}},{"cell_type":"markdown","source":"**site_name** has very high cardinality so it using the encoding using binary encoding scheme. For all other categorical columns, performing one-hot encoding.","metadata":{}},{"cell_type":"code","source":"from category_encoders.one_hot import OneHotEncoder\nfrom category_encoders.binary import BinaryEncoder\n\n\ntrain_df[[\"hasDomainLink\", \"lengthyLinkDomain\"]] = train_df[[\"hasDomainLink\", \"lengthyLinkDomain\"]].astype(int)\ntest_df[[\"hasDomainLink\", \"lengthyLinkDomain\"]] = test_df[[\"hasDomainLink\", \"lengthyLinkDomain\"]].astype(int)\n\ntrain_df[\"is_news\"].replace({\"1\": 1, \"?\": 0}, inplace=True)\ntest_df[\"is_news\"].replace({\"1\": 1, \"?\": 0}, inplace=True)\n\nonehot_enc = OneHotEncoder(cols=[\"alchemy_category\", \"news_front_page\", \"numwords_in_url\"])\ntrain_df = onehot_enc.fit_transform(train_df)\ntest_df = onehot_enc.transform(test_df)\n\nbinary_enc = BinaryEncoder(cols=[\"site_name\"])\ntrain_df = binary_enc.fit_transform(train_df)\ntest_df = binary_enc.fit_transform(test_df)\n\ntest_df.insert(0, \"site_name_-1\", 0)\n\nnew_name = {\"site_name_\"+str(i):\"site_name_\"+str(i+1) for i in range(-1, 8)}\ntest_df.rename(columns=new_name, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:24:41.90726Z","iopub.execute_input":"2022-03-07T16:24:41.907584Z","iopub.status.idle":"2022-03-07T16:24:42.280179Z","shell.execute_reply.started":"2022-03-07T16:24:41.907552Z","shell.execute_reply":"2022-03-07T16:24:42.278427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cleaning the boilerplate data. Every row is in JSON/Python-dictionary format with keys - *title*, *body* and *url*. Extracting the text from all the keys.","metadata":{}},{"cell_type":"code","source":"from json import loads\n\n\ndef get_text(bp):\n    text_dict = loads(bp)\n\n    text = \"\"\n    for value in text_dict.values():\n        if value != None:\n            text = text + \" \" + value\n\n    return text\n\n\ntrain_df[\"boilerplate\"] = train_df[\"boilerplate\"].apply(get_text)\ntest_df[\"boilerplate\"] = test_df[\"boilerplate\"].apply(get_text)\n\ntrain_df.rename(columns={\"boilerplate\": \"text\"}, inplace=True)\ntest_df.rename(columns={\"boilerplate\": \"text\"}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:24:42.282328Z","iopub.execute_input":"2022-03-07T16:24:42.284336Z","iopub.status.idle":"2022-03-07T16:24:42.50619Z","shell.execute_reply.started":"2022-03-07T16:24:42.284277Z","shell.execute_reply":"2022-03-07T16:24:42.505156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Removing stopwords, number, punctuations and performing lemmatization using the SpaCy library.","metadata":{}},{"cell_type":"code","source":"import spacy\n\n\ndef preprocess(df):\n    nlp = spacy.load(\"en_core_web_sm\")\n    count = 0\n\n    for text in nlp.pipe(df[\"text\"], n_process=4, batch_size=250, disable=[\"ner\", \"parser\"]):\n        df.loc[count, \"text\"] = \" \".join([token.lemma_ for token in text if token.is_alpha and not token.is_stop])\n        count += 1\n\n    return df\n\n\ntrain_df = preprocess(train_df)\ntest_df = preprocess(test_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:24:42.507774Z","iopub.execute_input":"2022-03-07T16:24:42.508376Z","iopub.status.idle":"2022-03-07T16:29:44.056489Z","shell.execute_reply.started":"2022-03-07T16:24:42.508324Z","shell.execute_reply":"2022-03-07T16:29:44.04995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:29:44.06024Z","iopub.execute_input":"2022-03-07T16:29:44.064946Z","iopub.status.idle":"2022-03-07T16:29:44.131691Z","shell.execute_reply.started":"2022-03-07T16:29:44.064867Z","shell.execute_reply":"2022-03-07T16:29:44.130472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Splitting the train dataframe into training and validation sets and then scaling the meta-data features with sklearn RobustScaler.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import RobustScaler\n\n\ntrain_df, val_df, train_target, val_target = train_test_split(\n    train_df,\n    target,\n    test_size=0.2,\n    stratify=target,\n    shuffle=True,\n    random_state=42\n)\n\ntrain_meta, train_text = train_df.drop([\"text\"], axis=1), train_df[\"text\"]\ndel train_df\n\nval_meta, val_text = val_df.drop([\"text\"], axis=1), val_df[\"text\"]\ndel val_df\n\ntest_meta, test_text = test_df.drop([\"text\"], axis=1), test_df[\"text\"]\ndel test_df\n\nscaler = RobustScaler()\ntrain_meta = scaler.fit_transform(train_meta)\nval_meta = scaler.transform(val_meta)\ntest_meta = scaler.transform(test_meta)","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:29:44.136143Z","iopub.execute_input":"2022-03-07T16:29:44.136474Z","iopub.status.idle":"2022-03-07T16:29:44.205808Z","shell.execute_reply.started":"2022-03-07T16:29:44.136439Z","shell.execute_reply":"2022-03-07T16:29:44.204948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 3: Building LSTM Model","metadata":{}},{"cell_type":"markdown","source":"Analyzing the corpus to find the total number of unique bi-gram tokens maximum sentence length.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import TextVectorization\n\n\nlayer = TextVectorization(ngrams=2)\nlayer.adapt(train_text)\n\nprint(\"Total number of tokens:\", layer.vocabulary_size())\nprint(\"Maximum sequence length:\", layer(train_text).shape[1])","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:29:44.207608Z","iopub.execute_input":"2022-03-07T16:29:44.207867Z","iopub.status.idle":"2022-03-07T16:29:51.672879Z","shell.execute_reply.started":"2022-03-07T16:29:44.20783Z","shell.execute_reply":"2022-03-07T16:29:51.671446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Performing TF-IDF vectorization of the corpus and defining a multi-input neural network model to handle text and meta-data features seperately.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Dense, Concatenate\nfrom tensorflow.keras.metrics import AUC\nfrom tensorflow.keras import Model\nfrom tensorflow import string\n\n\nMAX_TOKENS = 900000\n\ntext2vec = TextVectorization(\n    max_tokens=MAX_TOKENS,\n    ngrams=2,\n    pad_to_max_tokens=True,\n    output_mode=\"tf_idf\"\n)\ntext2vec.adapt(train_text)\n\ntext_input = Input(shape=(1,), dtype=string, name=\"text_input\")\nx = text2vec(text_input)\n\nmeta_input = Input(shape=train_meta.shape[1:], name=\"meta_input\")\ny = Concatenate()([x, meta_input])\ny = Dense(units=1, activation=\"sigmoid\", name=\"sigmoid_output\")(y)\n\nmodel = Model(inputs=[text_input, meta_input], outputs=y, name=\"NLP_Model\")\n\nmodel.compile(\n    optimizer=\"adam\",\n    loss=\"binary_crossentropy\",\n    metrics=[\"accuracy\", AUC(name=\"auc\")]\n)\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:29:51.674605Z","iopub.execute_input":"2022-03-07T16:29:51.674895Z","iopub.status.idle":"2022-03-07T16:29:59.101531Z","shell.execute_reply.started":"2022-03-07T16:29:51.67486Z","shell.execute_reply":"2022-03-07T16:29:59.100349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import plot_model\n\n\nplot_model(\n    model=model,\n    to_file=\"fnn_model.jpeg\",\n    show_shapes=True,\n    dpi=75\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:29:59.102972Z","iopub.execute_input":"2022-03-07T16:29:59.103232Z","iopub.status.idle":"2022-03-07T16:29:59.306381Z","shell.execute_reply.started":"2022-03-07T16:29:59.103199Z","shell.execute_reply":"2022-03-07T16:29:59.304558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Defining some callbacks for learning rate optimization and early stopping.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n\n\nreduce_lr = ReduceLROnPlateau(\n    monitor=\"val_auc\",\n    factor=0.2,\n    patience=5,\n    verbose=True,\n    mode=\"max\"\n)\n\nearly_stop = EarlyStopping(\n    monitor=\"val_auc\",\n    patience=30,\n    verbose=True,\n    mode=\"max\",\n    restore_best_weights=True\n)\n\ncallbacks = [reduce_lr, early_stop]","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:29:59.309224Z","iopub.execute_input":"2022-03-07T16:29:59.30967Z","iopub.status.idle":"2022-03-07T16:29:59.319317Z","shell.execute_reply.started":"2022-03-07T16:29:59.309629Z","shell.execute_reply":"2022-03-07T16:29:59.318489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(\n    x=[train_text, train_meta],\n    y=train_target,\n    batch_size=512,\n    epochs=300,\n    verbose=1,\n    callbacks=callbacks,\n    validation_data=([val_text, val_meta], val_target),\n    shuffle=True\n)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-07T16:29:59.320907Z","iopub.execute_input":"2022-03-07T16:29:59.321917Z","iopub.status.idle":"2022-03-07T16:36:04.405528Z","shell.execute_reply.started":"2022-03-07T16:29:59.321873Z","shell.execute_reply":"2022-03-07T16:36:04.404045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate([val_text, val_meta], val_target)","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:36:04.407661Z","iopub.execute_input":"2022-03-07T16:36:04.407938Z","iopub.status.idle":"2022-03-07T16:36:06.739699Z","shell.execute_reply.started":"2022-03-07T16:36:04.407907Z","shell.execute_reply":"2022-03-07T16:36:06.738555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df[\"label\"] = model.predict([test_text, test_meta])\nsub_df.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:36:06.741581Z","iopub.execute_input":"2022-03-07T16:36:06.742255Z","iopub.status.idle":"2022-03-07T16:36:11.812626Z","shell.execute_reply.started":"2022-03-07T16:36:06.74221Z","shell.execute_reply":"2022-03-07T16:36:11.811583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save(\"fnn_model\", save_format=\"tf\")","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:36:11.814351Z","iopub.execute_input":"2022-03-07T16:36:11.815191Z","iopub.status.idle":"2022-03-07T16:36:17.80464Z","shell.execute_reply.started":"2022-03-07T16:36:11.81515Z","shell.execute_reply":"2022-03-07T16:36:17.803448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plotting training history of various metrics.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 5))\nax = np.array(ax).ravel()\n\nfor i, metric in enumerate([\"loss\", \"accuracy\", \"auc\"]):\n    ax[i].plot(history.history[metric])\n    ax[i].plot(history.history[\"val_\"+metric])\n    ax[i].legend([\"train\", \"val\"])\n    ax[i].set_xlabel(\"epochs\")\n    ax[i].set_ylabel(metric)\n    ax[i].set_title(metric + \" vs epochs\")\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:36:17.80864Z","iopub.execute_input":"2022-03-07T16:36:17.809019Z","iopub.status.idle":"2022-03-07T16:36:18.631502Z","shell.execute_reply.started":"2022-03-07T16:36:17.808901Z","shell.execute_reply":"2022-03-07T16:36:18.629873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig.savefig(\"fnn_training_history.jpeg\")","metadata":{"execution":{"iopub.status.busy":"2022-03-07T16:36:18.633369Z","iopub.execute_input":"2022-03-07T16:36:18.633716Z","iopub.status.idle":"2022-03-07T16:36:18.844507Z","shell.execute_reply.started":"2022-03-07T16:36:18.633678Z","shell.execute_reply":"2022-03-07T16:36:18.843555Z"},"trusted":true},"execution_count":null,"outputs":[]}]}