{"cells":[{"metadata":{},"cell_type":"markdown","source":"plz upvote the sourve notebook\nsource notebook of Stumble Upon Challenge AUC Private LB 0.85 from Sumeet Sawant  "},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np \n\nimport matplotlib.pyplot as plt \nimport seaborn as sns \n%matplotlib inline \n\nimport tensorflow as tf ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load the data "},{"metadata":{"trusted":true},"cell_type":"code","source":"# taking all columns of training set only for data exploration\ndf_train=pd.read_csv('/kaggle/input/stumbleupon/train.tsv',sep='\\t')\n# taking boilerplate column as an input for the model beacuse only this column contain lot of high quality text data useful for our nlp task\ndf_test=pd.read_csv('/kaggle/input/stumbleupon/test.tsv',sep='\\t',usecols=['urlid','boilerplate'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Explorations \n\nThe dataset containes 27 columns and the end goal is predicting if the article is evergreen or non-evergreen. <br>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['alchemy_category'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.countplot(x=df_train['alchemy_category'],hue=df_train['label']);\nplt.xlabel('Category');\nplt.xticks(rotation=90);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Alchemy catergory does have a role in determining the label for the article \n\nWe see that business, Recreation and health are more likley to be evergreen <br>\n\nWhere as sports computer_internet and arts and entertainment are more like to be non-evergreen. <br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=df_train['label'])\n# This is a balanced dataset ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cleaning the boilerplate text\n\nLets remove the title and url word from each description . We will also lower case the words as we are planing to used a uncased version of Transformer model"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['boilerplate'].replace(to_replace=r'\"title\":', value=\"\",inplace=True,regex=True)\ndf_train['boilerplate'].replace(to_replace=r'\"url\":',value=\"\",inplace=True,regex=True)\n\ndf_train['boilerplate'].replace(to_replace=r'{|}',value=\"\",inplace=True,regex=True)\ndf_train['boilerplate']=df_train['boilerplate'].str.lower()\n\n\n#Cleaning the test dataframe \n\ndf_test['boilerplate'].replace(to_replace=r'\"title\":', value=\"\",inplace=True,regex=True)\ndf_test['boilerplate'].replace(to_replace=r'\"url\":',value=\"\",inplace=True,regex=True)\n\ndf_test['boilerplate'].replace(to_replace=r'{|}',value=\"\",inplace=True,regex=True)\ndf_test['boilerplate']=df_test['boilerplate'].str.lower()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Download from Hugging Face "},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import AutoTokenizer\n\n\n#Downloading the tokenizer and the Albert model for fine tuning\n\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ADD all the variable for the Transformer model \n# because bert base uncased Model can only handle upto 512 tokens at a time\nSEQ_length=512\n\n#Lets create the X and Y matrix from the Df train set \n\nXids=np.zeros((df_train.shape[0],SEQ_length))\nXmask=np.zeros((df_train.shape[0],SEQ_length))\ny=np.zeros((df_train.shape[0],1))\n\n#Preparing the test dataframe\n\nXids_test=np.zeros((df_test.shape[0],SEQ_length))\nXmask_test=np.zeros((df_test.shape[0],SEQ_length))\nXids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i,sequence in enumerate(df_train['boilerplate']):\n    tokens=tokenizer.encode_plus(sequence,max_length=SEQ_length,padding='max_length',add_special_tokens=True,\n                           truncation=True,return_token_type_ids=False,return_attention_mask=True,\n                           return_tensors='tf')\n    \n    Xids[i,:],Xmask[i,:],y[i,0]=tokens['input_ids'],tokens['attention_mask'],df_train.loc[i,'label']\n    \n\nfor i,sequence in enumerate(df_test['boilerplate']):\n    tokens=tokenizer.encode_plus(sequence,max_length=SEQ_length,padding='max_length',add_special_tokens=True,\n                           truncation=True,return_token_type_ids=False,return_attention_mask=True,\n                           return_tensors='tf')\n    \n    Xids_test[i,:],Xmask_test[i,:]=tokens['input_ids'],tokens['attention_mask']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xids.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check if the GPU is avalaible\ntf.config.get_visible_devices()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset=tf.data.Dataset.from_tensor_slices((Xids,Xmask,y))\n\ndef map_func(input_ids,mask,labels):\n    return {'input_ids':input_ids,'attention_mask':mask},labels\n\ndataset=dataset.map(map_func)\ndataset=dataset.shuffle(100000).batch(32).prefetch(1000)\n\nDS_size=len(list(dataset))\n\ntrain=dataset.take(round(DS_size*0.85))\nval=dataset.skip(round(DS_size*0.85))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Preparing the test dataset\n\ndataset_test=tf.data.Dataset.from_tensor_slices((Xids_test,Xmask_test))\n\ndef map_func(input_ids,mask):\n    return {'input_ids':input_ids,'attention_mask':mask}\n\ndataset_test=dataset_test.map(map_func)\ndataset_test=dataset_test.batch(32).prefetch(1000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Decode the test data and see if urlid and text matches "},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Build the model "},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import TFDistilBertModel, DistilBertConfig\ndistil_bert = 'distilbert-base-uncased'\n\nconfig = DistilBertConfig(dropout=0.2, attention_dropout=0.2)\nconfig.output_hidden_states = False\ntransformer_model = TFDistilBertModel.from_pretrained(distil_bert, config = config)\n\ninput_ids_in = tf.keras.layers.Input(shape=(SEQ_length,), name='input_ids', dtype='int32')\ninput_masks_in = tf.keras.layers.Input(shape=(SEQ_length,), name='attention_mask', dtype='int32') \n\nembedding_layer = transformer_model(input_ids_in, attention_mask=input_masks_in)[0]\nX = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(embedding_layer)\nX = tf.keras.layers.GlobalMaxPool1D()(X)\nX = tf.keras.layers.Dense(50, activation='relu')(X)\nX = tf.keras.layers.Dropout(0.2)(X)\nX = tf.keras.layers.Dense(1, activation='sigmoid')(X)\nmodel = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)\n\nfor layer in model.layers[:3]:\n  layer.trainable = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n              optimizer='adam',metrics=[tf.keras.metrics.AUC(),tf.keras.metrics.Precision(),tf.keras.metrics.Recall()\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# below is the Precision and Recall with loss and AUc"},{"metadata":{"trusted":true},"cell_type":"code","source":"history=model.fit(train,validation_data=val,epochs=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction "},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions=model.predict(dataset_test)\ndf_test['label']=predictions\n\ndf_test.to_csv('submission.csv',columns=['urlid','label'],index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Precision and recall for each classes"},{"metadata":{"trusted":true},"cell_type":"code","source":"input_x=tf.data.Dataset.from_tensor_slices((Xids,Xmask,y))\n\ndef map_func(input_ids,mask,labels):\n    return {'input_ids':input_ids,'attention_mask':mask}\n\ninput_x=input_x.map(map_func)\ninput_x=input_x.shuffle(100000).batch(32).prefetch(1000)\n\ny_true = y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=model.predict(dataset)\ny_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = np.round(y_pred)\ny_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nprint(metrics.classification_report(y_true, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}