{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data Visualization\n\n## Compare the original data with 4 types of cropped data\n\n### [Dataset Details](https://www.kaggle.com/c/happy-whale-and-dolphin/discussion/310779)","metadata":{}},{"cell_type":"code","source":"import re\nimport os\nimport numpy as np\nimport pandas as pd\nimport random\nimport math\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport pickle\nimport json\n\nfrom kaggle_datasets import KaggleDatasets","metadata":{"execution":{"iopub.status.busy":"2022-03-03T04:31:46.806535Z","iopub.execute_input":"2022-03-03T04:31:46.806962Z","iopub.status.idle":"2022-03-03T04:31:52.378084Z","shell.execute_reply.started":"2022-03-03T04:31:46.806883Z","shell.execute_reply":"2022-03-03T04:31:52.377311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class config:\n    IMAGE_SIZE=256\n    BATCH_SIZE=40","metadata":{"execution":{"iopub.status.busy":"2022-03-03T04:31:52.379472Z","iopub.execute_input":"2022-03-03T04:31:52.379687Z","iopub.status.idle":"2022-03-03T04:31:52.385822Z","shell.execute_reply.started":"2022-03-03T04:31:52.379663Z","shell.execute_reply":"2022-03-03T04:31:52.385095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\n\ndef arcface_format(posting_id, image, label_group, matches):\n    return posting_id, {'inp1': image, 'inp2': label_group}, label_group, matches\n\ndef arcface_inference_format(posting_id, image, label_group, matches):\n    return image,posting_id\n\n# Function to decode our images\n# Updated to include crops.\ndef decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels = 3)\n    image = tf.image.resize(image, [config.IMAGE_SIZE,config.IMAGE_SIZE])\n    image = tf.cast(image, tf.float32) / 255.0\n    return image\n\n# This function parse our images and also get the target variable\ndef read_labeled_tfrecord(example):\n    LABELED_TFREC_FORMAT = {\n        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"target\": tf.io.FixedLenFeature([], tf.int64)\n    }\n\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    posting_id = example['image_name']\n    image = decode_image(example['image'])\n    label_group = tf.cast(example['target'], tf.int32)\n    matches = 1\n    return posting_id, image, label_group, matches\n\n# This function loads TF Records and parse them into tensors\ndef load_dataset(filenames, ordered = False):\n    \n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False \n        \n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO)\n    dataset = dataset.with_options(ignore_order)\n    dataset = dataset.map(read_labeled_tfrecord, num_parallel_calls = AUTO) \n    return dataset\n\n# This function is to get our training tensors\ndef get_training_dataset(filenames):\n    dataset = load_dataset(filenames, ordered = True)\n    dataset = dataset.map(arcface_format, num_parallel_calls = AUTO)\n    dataset = dataset.map(lambda posting_id, image, label_group, matches: (image, label_group))\n    dataset = dataset.batch(config.BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\n# This function is to get our training tensors\ndef get_test_dataset(filenames, get_names = True):\n    dataset = load_dataset(filenames, ordered = True)\n    dataset = dataset.map(arcface_inference_format, num_parallel_calls = AUTO)\n    if not get_names:\n        dataset = dataset.map(lambda image, posting_id: image)\n    dataset = dataset.batch(config.BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2022-03-03T04:55:36.01563Z","iopub.execute_input":"2022-03-03T04:55:36.015949Z","iopub.status.idle":"2022-03-03T04:55:36.034024Z","shell.execute_reply.started":"2022-03-03T04:55:36.015917Z","shell.execute_reply":"2022-03-03T04:55:36.033368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_files = ['happywhale-tfrecords-v1', \n               'happywhale-tfrecords-detic-512', \n               'happywhale-tfrecords-yolov5-512-005',\n               'happywhale-tfrecords-yolov5-512-010',\n               'happywhale-tfrecords-tokencut-384']\ntrain_ds = []\ntest_ds = []\nfor input_file in input_files:\n    GCS_DS_PATH = KaggleDatasets().get_gcs_path(input_file)\n    train_file = np.sort(np.array(tf.io.gfile.glob(GCS_DS_PATH + '/happywhale-2022-train*.tfrec')))\n    test_file  = np.sort(np.array(tf.io.gfile.glob(GCS_DS_PATH + '/happywhale-2022-test*.tfrec')))\n    train_ds.append(get_test_dataset(train_file))\n    test_ds.append(get_test_dataset(test_file))","metadata":{"execution":{"iopub.status.busy":"2022-03-03T04:59:09.149815Z","iopub.execute_input":"2022-03-03T04:59:09.150768Z","iopub.status.idle":"2022-03-03T04:59:13.32111Z","shell.execute_reply.started":"2022-03-03T04:59:09.150704Z","shell.execute_reply":"2022-03-03T04:59:13.320001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train Images\n\n#### From left to right: orginal, detic, yolov5(0.05), yolov5(0.10), tokencut","metadata":{}},{"cell_type":"code","source":"row = 30; col = 5;\nrow = min(row, config.BATCH_SIZE)\n\nplt.figure(figsize=(25,int(25*row/col)))\nfor k, ds in enumerate(train_ds):\n    for img, label in ds:\n        for j in range(row):\n            plt.title(label[j].numpy())\n            plt.subplot(row, col, j*col+k+1)\n            plt.axis('off')\n            plt.imshow(img[j,])\n        break\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-03T04:59:28.26074Z","iopub.execute_input":"2022-03-03T04:59:28.261131Z","iopub.status.idle":"2022-03-03T04:59:51.186638Z","shell.execute_reply.started":"2022-03-03T04:59:28.261095Z","shell.execute_reply":"2022-03-03T04:59:51.185574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test Images\n\n#### From left to right: orginal, detic, yolov5(0.05), yolov5(0.10), tokencut","metadata":{}},{"cell_type":"code","source":"row = 30; col = 5;\nrow = min(row, config.BATCH_SIZE)\n\nplt.figure(figsize=(25,int(25*row/col)))\nfor k, ds in enumerate(test_ds):\n    for img, label in ds:\n        for j in range(row):\n            plt.title(label[j].numpy())\n            plt.subplot(row, col, j*col+k+1)\n            plt.axis('off')\n            plt.imshow(img[j,])\n        break\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-03T05:00:30.787601Z","iopub.execute_input":"2022-03-03T05:00:30.788403Z","iopub.status.idle":"2022-03-03T05:00:54.952171Z","shell.execute_reply.started":"2022-03-03T05:00:30.788362Z","shell.execute_reply":"2022-03-03T05:00:54.950389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}