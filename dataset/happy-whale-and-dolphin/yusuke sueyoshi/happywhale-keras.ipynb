{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-21T00:51:01.323307Z","iopub.execute_input":"2022-02-21T00:51:01.323651Z","iopub.status.idle":"2022-02-21T00:51:01.352273Z","shell.execute_reply.started":"2022-02-21T00:51:01.32356Z","shell.execute_reply":"2022-02-21T00:51:01.35164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" I've written below code which can apply for Keras.\n FYR.\n If mistakes or improvement points are found, please feel free to post commnets!\n\n\n とりあえずKerasに放り込むところまで作りました。\n よければ参考にして下さい。間違いや改善点があればコメントで教えてほしいです。","metadata":{}},{"cell_type":"markdown","source":"Reading CSV and organizing a dataframe and PATHs.\n\n\nCSVの読み込みとデータフレームPATH周りの調整を行っています。","metadata":{}},{"cell_type":"code","source":"BASE_PATH = \"/kaggle/input/happy-whale-and-dolphin/train_images/\"\ndef addpath(x):\n    return BASE_PATH + '/' + x\ndf = pd.read_csv(\"/kaggle/input/happy-whale-and-dolphin/train.csv\")\ndf['image'] = df['image'].map(addpath)\n#テストためデータフレームを適当な大きさに\n#df_cut = df[0:100]\ndf_cut =　df","metadata":{"execution":{"iopub.status.busy":"2022-02-21T00:51:04.742635Z","iopub.execute_input":"2022-02-21T00:51:04.743082Z","iopub.status.idle":"2022-02-21T00:51:04.746309Z","shell.execute_reply.started":"2022-02-21T00:51:04.743057Z","shell.execute_reply":"2022-02-21T00:51:04.745631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Loading the images datas and turn to numpy array. And then, augment the images.\n\n\n画像の読み込みとnumpy行列化画像データの水増しを行っています。","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom PIL import Image\nimport glob\nIM_SIZE = 128\ndummy = []\nfor index, i in enumerate(df_cut['image']):\n    im = Image.open(i)\n    im = im.resize((IM_SIZE, IM_SIZE))\n    im = im.convert('L')\n    np_im = np.array(im)\n    df_cut['image'][index] = np_im\n    #画像反転\n    np_im_hr = np_im[:, ::-1]\n    dummy.append([np_im_hr,df_cut['species'][index],df_cut['individual_id'][index]])\n    np_im_ve = np_im[::-1,:]\n    dummy.append([np_im_ve,df_cut['species'][index],df_cut['individual_id'][index]])\n    #画像回転\n    angle = [30,60,90,120,150,210,240,270,300,330]\n    for ag in angle:\n      rot = im.rotate(ag)\n      rot = rot.resize((IM_SIZE,IM_SIZE))\n      np_rot = np.array(rot)\n      print(np_rot.shape)\n      #np_rot = imresize(np_rot, (IM_SIZE, IM_SIZE))\n      dummy.append([np_rot,df_cut['species'][index],df_cut['individual_id'][index]])\n    print(\"\\r\"+str(index),end=\"\")\n#水増し分を追加\ndf_mizu = pd.DataFrame(dummy,columns = ['image','species','individual_id'])\ndf_cut = pd.concat([df_cut,df_mizu])\ny = pd.get_dummies(df_cut['individual_id'])\n","metadata":{"execution":{"iopub.status.busy":"2022-02-21T02:01:59.235953Z","iopub.execute_input":"2022-02-21T02:01:59.236183Z","iopub.status.idle":"2022-02-21T02:02:08.520222Z","shell.execute_reply.started":"2022-02-21T02:01:59.236153Z","shell.execute_reply":"2022-02-21T02:02:08.519536Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A variable of df_mizu is a dataframe which includes numpy images which generated by flipping and rotating the original images.\n\n\ndf_mizuは水増ししたデータnumpy行列が格納されています。","metadata":{}},{"cell_type":"code","source":"df_mizu = pd.DataFrame(dummy,columns = ['image','species','individual_id'])\ndf_cut = pd.concat([df_cut,df_mizu])\ny = pd.get_dummies(df_cut['individual_id'])\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(df_cut['image'], y, test_size=0.1)\ndummy = []\nfor i in x_train:\n    flat = np.reshape(i,IM_SIZE,IM_SIZE)\n    dummy.append(flat)\nx_train0 = np.array(dummy)\ndummy = []\nfor i in x_test:\n    flat = np.reshape(i,IM_SIZE,IM_SIZE)\n    dummy.append(flat)\nx_test0 = np.array(dummy)\ny_train0 = y_train.to_numpy()\ny_test0 = y_test.to_numpy()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T02:02:29.711457Z","iopub.execute_input":"2022-02-21T02:02:29.711689Z","iopub.status.idle":"2022-02-21T02:02:31.224011Z","shell.execute_reply.started":"2022-02-21T02:02:29.711666Z","shell.execute_reply":"2022-02-21T02:02:31.223144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Learning by NN.\n\n\nkeras(NN)で学習。","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nx_train0, x_test0 = x_train0 / 255.0, x_test0 / 255.0\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Flatten(input_shape=(IM_SIZE,IM_SIZE)),\n  tf.keras.layers.Dense(128, activation='relu'),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(15587, activation='softmax')\n])\n\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.fit(x_train0, y_train0, epochs=5, validation_data=(x_test0,y_test0))","metadata":{"execution":{"iopub.status.busy":"2022-02-18T09:20:07.290214Z","iopub.execute_input":"2022-02-18T09:20:07.290934Z","iopub.status.idle":"2022-02-18T09:20:07.322623Z","shell.execute_reply.started":"2022-02-18T09:20:07.290894Z","shell.execute_reply":"2022-02-18T09:20:07.321675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-02-18T09:20:12.378255Z","iopub.execute_input":"2022-02-18T09:20:12.378548Z","iopub.status.idle":"2022-02-18T09:20:13.319258Z","shell.execute_reply.started":"2022-02-18T09:20:12.378518Z","shell.execute_reply":"2022-02-18T09:20:13.318627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-02-18T09:20:15.967581Z","iopub.execute_input":"2022-02-18T09:20:15.967843Z","iopub.status.idle":"2022-02-18T09:20:23.328036Z","shell.execute_reply.started":"2022-02-18T09:20:15.967816Z","shell.execute_reply":"2022-02-18T09:20:23.32722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-02-18T03:28:10.417628Z","iopub.execute_input":"2022-02-18T03:28:10.417936Z","iopub.status.idle":"2022-02-18T03:28:10.42879Z","shell.execute_reply.started":"2022-02-18T03:28:10.417893Z","shell.execute_reply":"2022-02-18T03:28:10.428096Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-02-18T03:30:46.969534Z","iopub.execute_input":"2022-02-18T03:30:46.969862Z","iopub.status.idle":"2022-02-18T03:30:47.000366Z","shell.execute_reply.started":"2022-02-18T03:30:46.969828Z","shell.execute_reply":"2022-02-18T03:30:46.999714Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]}]}