{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<br>\n<h2 style = \"font-size:60px; font-family:Garamond ; font-weight : normal; background-color: #f6f5f5 ; color : #fe346e; text-align: center; border-radius: 100px 100px;\">[Pytorch] ArcFace Starter</h2>\n<br>","metadata":{}},{"cell_type":"markdown","source":"<br>\n\nIt is based on the work of Vlad Vaduva https://www.kaggle.com/vladvdv/pytorch-train-notebook-arcface-gem-pooling, https://colab.research.google.com/github/pytorch/xla/blob/master/contrib/colab/mnist-training.ipynb, https://github.com/pytorch/xla/blob/master/test/test_train_mp_imagenet.py and https://github.com/pytorch/xla/blob/master/test/test_train_mp_imagenet_amp.py\n\n### If you found this helpful, please give it an upvote!\n\n* v1: quick save\n* v2: add code for tpu with huggingface accelerate\n* v3: trying to fix error ...\n* v4: quick save\n* v5: trying to fix error ...\n* v6: check 2 epoch train time\n* v7: trying to fix error ...\n* v8: add pad_across_processes and increase batch size\n* v9: pad_index=-100\n* v10 ~ v15: trying to fix error ...\n* v16 ~ v23: pytorch lightning trials\n* v24: change whole code with pure pytorch xla\n* v25 ~ v31: trying to fix error ...\n* v32: save model\n* v33 ~ v36: trying to fix error ...\n* v37: gradient clipping and torch_xla 1.8.1\n* v38 ~: trying to fix error ...","metadata":{}},{"cell_type":"markdown","source":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Install Required Libraries</h1></span>","metadata":{}},{"cell_type":"code","source":"!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version 1.8.1 --apt-packages libomp5 libopenblas-dev","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-01T13:29:46.398715Z","iopub.execute_input":"2022-03-01T13:29:46.399067Z","iopub.status.idle":"2022-03-01T13:30:51.259157Z","shell.execute_reply.started":"2022-03-01T13:29:46.398979Z","shell.execute_reply":"2022-03-01T13:30:51.257752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade wandb","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-01T13:30:51.261621Z","iopub.execute_input":"2022-03-01T13:30:51.26191Z","iopub.status.idle":"2022-03-01T13:31:02.831138Z","shell.execute_reply.started":"2022-03-01T13:30:51.261874Z","shell.execute_reply":"2022-03-01T13:31:02.830379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Import Required Libraries üìö</h1></span>","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport cv2\nimport math\nimport copy\nimport time\nimport random\n\n# For data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Pytorch Imports\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\n\n# Utils\nimport joblib\nfrom tqdm.auto import tqdm\nfrom collections import defaultdict\n\n# Sklearn Imports\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\n\n# For Image Models\nimport sys\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\nimport timm\n\n# Albumentations for augmentations\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# For colored terminal text\nfrom colorama import Fore, Back, Style\nb_ = Fore.BLUE\nsr_ = Style.RESET_ALL\n\n# For TPU Training\nimport torch_xla\nimport torch_xla.core.xla_model as xm\nimport torch_xla.debug.metrics as met\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.distributed.xla_multiprocessing as xmp\nimport torch_xla.utils.utils as xu\nfrom torch_xla.utils.cached_dataset import CachedDataset\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-03-01T13:31:02.833203Z","iopub.execute_input":"2022-03-01T13:31:02.833468Z","iopub.status.idle":"2022-03-01T13:31:08.331689Z","shell.execute_reply.started":"2022-03-01T13:31:02.833437Z","shell.execute_reply":"2022-03-01T13:31:08.330633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<img src=\"https://i.imgur.com/gb6B4ig.png\" width=\"400\" alt=\"Weights & Biases\" />\n\n<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\"> Weights & Biases (W&B) is a set of machine learning tools that helps you build better models faster. <strong>Kaggle competitions require fast-paced model development and evaluation</strong>. There are a lot of components: exploring the training data, training different models, combining trained models in different combinations (ensembling), and so on.</span>\n\n> <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">‚è≥ Lots of components = Lots of places to go wrong = Lots of time spent debugging</span>\n\n<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">W&B can be useful for Kaggle competition with it's lightweight and interoperable tools:</span>\n\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Quickly track experiments,<br></span>\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Version and iterate on datasets, <br></span>\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Evaluate model performance,<br></span>\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Reproduce models,<br></span>\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Visualize results and spot regressions,<br></span>\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Share findings with colleagues.</span>\n\n<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">To learn more about Weights and Biases check out this <strong><a href=\"https://www.kaggle.com/ayuraj/experiment-tracking-with-weights-and-biases\">kernel</a></strong>.</span>","metadata":{}},{"cell_type":"code","source":"import wandb\n\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    api_key = user_secrets.get_secret(\"wandb_api\")\n    wandb.login(key=api_key)\n    anony = None\nexcept:\n    anony = \"must\"\n    print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')","metadata":{"execution":{"iopub.status.busy":"2022-03-01T13:31:08.333583Z","iopub.execute_input":"2022-03-01T13:31:08.334269Z","iopub.status.idle":"2022-03-01T13:31:09.260891Z","shell.execute_reply.started":"2022-03-01T13:31:08.334218Z","shell.execute_reply":"2022-03-01T13:31:09.259997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Training Configuration ‚öôÔ∏è</h1></span>","metadata":{}},{"cell_type":"code","source":"CONFIG = {\"seed\": 2022,\n          \"epochs\": 24,\n          \"img_size\": 768,\n          \"model_name\": \"tf_efficientnet_b4\",\n          \"num_classes\": 15587,\n          \"train_batch_size\": 8,\n          \"valid_batch_size\": 8,\n          \"learning_rate\": 0.0001,\n          \"scheduler\": 'OneCycleLR',\n          \"min_lr\": 1e-6,\n          \"max_norm\": 1, # For Gradient Clipping\n          \"T_max\": 500,\n          \"weight_decay\": 1e-6,\n          \"n_fold\": 5,\n          \"fold_to_run\": 0,\n          \"n_accumulate\": 1,\n          \"test_mode\":True, # enable for testing pipeline, changes epochs to 2, uses just 1000 training samples, and change log_steps to 1\n          \"log_steps\": 20,\n          # ArcFace Hyperparameters\n          \"s\": 30.0, \n          \"m\": 0.30,\n          \"ls_eps\": 0.0,\n          \"easy_margin\": False\n          }","metadata":{"execution":{"iopub.status.busy":"2022-03-01T13:31:09.263804Z","iopub.execute_input":"2022-03-01T13:31:09.264121Z","iopub.status.idle":"2022-03-01T13:31:09.271467Z","shell.execute_reply.started":"2022-03-01T13:31:09.264079Z","shell.execute_reply":"2022-03-01T13:31:09.270665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Set Seed for Reproducibility</h1></span>","metadata":{}},{"cell_type":"code","source":"def set_seed(seed=42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\nset_seed(CONFIG['seed'])","metadata":{"execution":{"iopub.status.busy":"2022-03-01T13:31:09.272925Z","iopub.execute_input":"2022-03-01T13:31:09.273414Z","iopub.status.idle":"2022-03-01T13:31:09.286602Z","shell.execute_reply.started":"2022-03-01T13:31:09.273366Z","shell.execute_reply":"2022-03-01T13:31:09.285822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROOT_DIR = '../input/happy-whale-and-dolphin'\nTRAIN_DIR = '../input/happy-whale-and-dolphin/train_images'\nTEST_DIR = '../input/happy-whale-and-dolphin/test_images'","metadata":{"execution":{"iopub.status.busy":"2022-03-01T13:31:09.287707Z","iopub.execute_input":"2022-03-01T13:31:09.288441Z","iopub.status.idle":"2022-03-01T13:31:09.295552Z","shell.execute_reply.started":"2022-03-01T13:31:09.2884Z","shell.execute_reply":"2022-03-01T13:31:09.294901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_train_file_path(id):\n    return f\"{TRAIN_DIR}/{id}\"","metadata":{"execution":{"iopub.status.busy":"2022-03-01T13:31:09.296873Z","iopub.execute_input":"2022-03-01T13:31:09.297354Z","iopub.status.idle":"2022-03-01T13:31:09.304464Z","shell.execute_reply.started":"2022-03-01T13:31:09.297324Z","shell.execute_reply":"2022-03-01T13:31:09.303759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Read the Data üìñ</h1>","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(f\"{ROOT_DIR}/train.csv\")\ndf['file_path'] = df['image'].apply(get_train_file_path)\ndf.head()\n\nif CONFIG[\"test_mode\"]==True:\n    df=df[:1000]\n    CONFIG[\"epochs\"] = 2\n    CONFIG[\"n_fold\"] = 3\n    CONFIG['log_steps'] = 1\nencoder = LabelEncoder()\ndf['individual_id'] = encoder.fit_transform(df['individual_id'])\n\nwith open(\"le.pkl\", \"wb\") as fp:\n    joblib.dump(encoder, fp)","metadata":{"execution":{"iopub.status.busy":"2022-03-01T13:31:09.305697Z","iopub.execute_input":"2022-03-01T13:31:09.306009Z","iopub.status.idle":"2022-03-01T13:31:09.50238Z","shell.execute_reply.started":"2022-03-01T13:31:09.305972Z","shell.execute_reply":"2022-03-01T13:31:09.501673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Create Folds</h1></span>","metadata":{}},{"cell_type":"code","source":"skf = StratifiedKFold(n_splits=CONFIG['n_fold'])\n\nfor fold, ( _, val_) in enumerate(skf.split(X=df, y=df.individual_id)):\n      df.loc[val_ , \"kfold\"] = fold","metadata":{"execution":{"iopub.status.busy":"2022-03-01T13:31:09.50362Z","iopub.execute_input":"2022-03-01T13:31:09.50388Z","iopub.status.idle":"2022-03-01T13:31:09.522674Z","shell.execute_reply.started":"2022-03-01T13:31:09.503853Z","shell.execute_reply":"2022-03-01T13:31:09.521965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Dataset Class</h1></span>","metadata":{}},{"cell_type":"code","source":"class HappyWhaleDataset(Dataset):\n    def __init__(self, df, transforms=None):\n        self.df = df\n        self.file_names = df['file_path'].values\n        self.labels = df['individual_id'].values\n        self.transforms = transforms\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        img_path = self.file_names[index]\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        label = self.labels[index]\n        \n        if self.transforms:\n            img = self.transforms(image=img)[\"image\"]\n            \n        return {\n            'image': img,\n            'label': torch.tensor(label)\n        }","metadata":{"execution":{"iopub.status.busy":"2022-03-01T13:31:09.523932Z","iopub.execute_input":"2022-03-01T13:31:09.524322Z","iopub.status.idle":"2022-03-01T13:31:09.533457Z","shell.execute_reply.started":"2022-03-01T13:31:09.52428Z","shell.execute_reply":"2022-03-01T13:31:09.532558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Augmentations</h1></span>","metadata":{}},{"cell_type":"code","source":"data_transforms = {\n    \"train\": A.Compose([\n        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n        A.HorizontalFlip(p=0.5),\n        A.Normalize(\n                mean=[0.485, 0.456, 0.406], \n                std=[0.229, 0.224, 0.225], \n                max_pixel_value=255.0, \n                p=1.0\n            ),\n        ToTensorV2()], p=1.),\n    \n    \"valid\": A.Compose([\n        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n        A.Normalize(\n                mean=[0.485, 0.456, 0.406], \n                std=[0.229, 0.224, 0.225], \n                max_pixel_value=255.0, \n                p=1.0\n            ),\n        ToTensorV2()], p=1.)\n}","metadata":{"execution":{"iopub.status.busy":"2022-03-01T13:31:09.534889Z","iopub.execute_input":"2022-03-01T13:31:09.535119Z","iopub.status.idle":"2022-03-01T13:31:09.54495Z","shell.execute_reply.started":"2022-03-01T13:31:09.535092Z","shell.execute_reply":"2022-03-01T13:31:09.544216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">GeM Pooling</h1></span>\n\n<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.5em; font-weight: 300;\">Code taken from <a href=\"https://amaarora.github.io/2020/08/30/gempool.html\">GeM Pooling Explained</a></span>\n\n![](https://i.imgur.com/thTgYWG.jpg)","metadata":{}},{"cell_type":"code","source":"class GeM(nn.Module):\n    def __init__(self, p=3, eps=1e-6):\n        super(GeM, self).__init__()\n        self.p = nn.Parameter(torch.ones(1)*p)\n        self.eps = eps\n\n    def forward(self, x):\n        return self.gem(x, p=self.p, eps=self.eps)\n        \n    def gem(self, x, p=3, eps=1e-6):\n        return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n        \n    def __repr__(self):\n        return self.__class__.__name__ + \\\n                '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + \\\n                ', ' + 'eps=' + str(self.eps) + ')'","metadata":{"execution":{"iopub.status.busy":"2022-03-01T13:31:09.546151Z","iopub.execute_input":"2022-03-01T13:31:09.546397Z","iopub.status.idle":"2022-03-01T13:31:09.557055Z","shell.execute_reply.started":"2022-03-01T13:31:09.546369Z","shell.execute_reply":"2022-03-01T13:31:09.555625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">ArcFace</h1></span>\n\n<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.5em; font-weight: 300;\">Code taken from <a href=\"https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/blob/master/src/modeling/metric_learning.py\">Landmark2019-1st-and-3rd-Place-Solution</a></span>","metadata":{}},{"cell_type":"code","source":"class ArcMarginProduct(nn.Module):\n    r\"\"\"Implement of large margin arc distance: :\n        Args:\n            in_features: size of each input sample\n            out_features: size of each output sample\n            s: norm of input feature\n            m: margin\n            cos(theta + m)\n        \"\"\"\n    def __init__(self, in_features, out_features, s=30.0, \n                 m=0.50, easy_margin=False, ls_eps=0.0):\n        super(ArcMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.s = s\n        self.m = m\n        self.ls_eps = ls_eps  # label smoothing\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.register_buffer(\"cos_m\", torch.tensor(math.cos(m)))\n        self.register_buffer(\"sin_m\", torch.tensor(math.sin(m)))\n        self.register_buffer(\"th\", torch.tensor(math.cos(math.pi - m)))\n        self.register_buffer(\"mm\", torch.tensor(math.sin(math.pi - m) * m))\n        \n    def set_device(self,device):\n        self.device = device\n\n    def forward(self, input, label):\n        # --------------------------- cos(theta) & phi(theta) ---------------------\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n        # --------------------------- convert label to one-hot ---------------------\n        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n        one_hot = torch.zeros(cosine.size(), device=self.device)\n        one_hot.scatter_(1, label.view(-1, 1), 1)\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n        # -------------torch.where(out_i = {x_i if condition_i else y_i) ------------\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.s\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-03-01T13:31:09.560402Z","iopub.execute_input":"2022-03-01T13:31:09.561184Z","iopub.status.idle":"2022-03-01T13:31:09.576589Z","shell.execute_reply.started":"2022-03-01T13:31:09.561146Z","shell.execute_reply":"2022-03-01T13:31:09.575723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Create Model</h1></span>","metadata":{}},{"cell_type":"code","source":"class HappyWhaleModel(nn.Module):\n    def __init__(self, model_name, pretrained=True):\n        super(HappyWhaleModel, self).__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained)\n        in_features = self.model.classifier.in_features\n        self.model.classifier = nn.Identity()\n        self.model.global_pool = nn.Identity()\n        self.pooling = GeM()\n        self.drop = nn.Dropout(p=0.2, inplace=False)\n        self.fc = nn.Linear(in_features,512)\n        self.arc = ArcMarginProduct(512, \n                           CONFIG[\"num_classes\"],\n                           s=CONFIG[\"s\"], \n                           m=CONFIG[\"m\"], \n                           easy_margin=CONFIG[\"ls_eps\"], \n                           ls_eps=CONFIG[\"ls_eps\"])\n        \n    def set_device(self,device):\n        self.arc.set_device(device)\n        \n    def forward(self, images, labels):\n        features = self.model(images)\n        pooled_features = self.pooling(features).flatten(1)\n        pooled_drop = self.drop(pooled_features)\n        emb = self.fc(pooled_drop)\n        output = self.arc(emb,labels)\n        return output,emb","metadata":{"execution":{"iopub.status.busy":"2022-03-01T13:31:09.577784Z","iopub.execute_input":"2022-03-01T13:31:09.578027Z","iopub.status.idle":"2022-03-01T13:31:09.591003Z","shell.execute_reply.started":"2022-03-01T13:31:09.578001Z","shell.execute_reply":"2022-03-01T13:31:09.590174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">LR Scheduler</h1></span>","metadata":{}},{"cell_type":"code","source":"# copied from https://github.com/pytorch/pytorch/blob/v1.8.2/torch/optim/lr_scheduler.py and add lr logging\nclass OneCycleLR(lr_scheduler._LRScheduler):\n    r\"\"\"Sets the learning rate of each parameter group according to the\n    1cycle learning rate policy. The 1cycle policy anneals the learning\n    rate from an initial learning rate to some maximum learning rate and then\n    from that maximum learning rate to some minimum learning rate much lower\n    than the initial learning rate.\n    This policy was initially described in the paper `Super-Convergence:\n    Very Fast Training of Neural Networks Using Large Learning Rates`_.\n    The 1cycle learning rate policy changes the learning rate after every batch.\n    `step` should be called after a batch has been used for training.\n    This scheduler is not chainable.\n    Note also that the total number of steps in the cycle can be determined in one\n    of two ways (listed in order of precedence):\n    #. A value for total_steps is explicitly provided.\n    #. A number of epochs (epochs) and a number of steps per epoch\n       (steps_per_epoch) are provided.\n       In this case, the number of total steps is inferred by\n       total_steps = epochs * steps_per_epoch\n    You must either provide a value for total_steps or provide a value for both\n    epochs and steps_per_epoch.\n    The default behaviour of this scheduler follows the fastai implementation of 1cycle, which\n    claims that \"unpublished work has shown even better results by using only two phases\". To\n    mimic the behaviour of the original paper instead, set ``three_phase=True``.\n    Args:\n        optimizer (Optimizer): Wrapped optimizer.\n        max_lr (float or list): Upper learning rate boundaries in the cycle\n            for each parameter group.\n        total_steps (int): The total number of steps in the cycle. Note that\n            if a value is not provided here, then it must be inferred by providing\n            a value for epochs and steps_per_epoch.\n            Default: None\n        epochs (int): The number of epochs to train for. This is used along\n            with steps_per_epoch in order to infer the total number of steps in the cycle\n            if a value for total_steps is not provided.\n            Default: None\n        steps_per_epoch (int): The number of steps per epoch to train for. This is\n            used along with epochs in order to infer the total number of steps in the\n            cycle if a value for total_steps is not provided.\n            Default: None\n        pct_start (float): The percentage of the cycle (in number of steps) spent\n            increasing the learning rate.\n            Default: 0.3\n        anneal_strategy (str): {'cos', 'linear'}\n            Specifies the annealing strategy: \"cos\" for cosine annealing, \"linear\" for\n            linear annealing.\n            Default: 'cos'\n        cycle_momentum (bool): If ``True``, momentum is cycled inversely\n            to learning rate between 'base_momentum' and 'max_momentum'.\n            Default: True\n        base_momentum (float or list): Lower momentum boundaries in the cycle\n            for each parameter group. Note that momentum is cycled inversely\n            to learning rate; at the peak of a cycle, momentum is\n            'base_momentum' and learning rate is 'max_lr'.\n            Default: 0.85\n        max_momentum (float or list): Upper momentum boundaries in the cycle\n            for each parameter group. Functionally,\n            it defines the cycle amplitude (max_momentum - base_momentum).\n            Note that momentum is cycled inversely\n            to learning rate; at the start of a cycle, momentum is 'max_momentum'\n            and learning rate is 'base_lr'\n            Default: 0.95\n        div_factor (float): Determines the initial learning rate via\n            initial_lr = max_lr/div_factor\n            Default: 25\n        final_div_factor (float): Determines the minimum learning rate via\n            min_lr = initial_lr/final_div_factor\n            Default: 1e4\n        three_phase (bool): If ``True``, use a third phase of the schedule to annihilate the\n            learning rate according to 'final_div_factor' instead of modifying the second\n            phase (the first two phases will be symmetrical about the step indicated by\n            'pct_start').\n        last_epoch (int): The index of the last batch. This parameter is used when\n            resuming a training job. Since `step()` should be invoked after each\n            batch instead of after each epoch, this number represents the total\n            number of *batches* computed, not the total number of epochs computed.\n            When last_epoch=-1, the schedule is started from the beginning.\n            Default: -1\n        verbose (bool): If ``True``, prints a message to stdout for\n            each update. Default: ``False``.\n    Example:\n        >>> data_loader = torch.utils.data.DataLoader(...)\n        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n        >>> scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(data_loader), epochs=10)\n        >>> for epoch in range(10):\n        >>>     for batch in data_loader:\n        >>>         train_batch(...)\n        >>>         scheduler.step()\n    .. _Super-Convergence\\: Very Fast Training of Neural Networks Using Large Learning Rates:\n        https://arxiv.org/abs/1708.07120\n    \"\"\"\n    def __init__(self,\n                 optimizer,\n                 max_lr,\n                 total_steps=None,\n                 epochs=None,\n                 steps_per_epoch=None,\n                 pct_start=0.3,\n                 anneal_strategy='cos',\n                 cycle_momentum=True,\n                 base_momentum=0.85,\n                 max_momentum=0.95,\n                 div_factor=25.,\n                 final_div_factor=1e4,\n                 three_phase=False,\n                 last_epoch=-1,\n                 verbose=False,\n                 summary_writer=None,):\n\n        # Validate optimizer\n        if not isinstance(optimizer, optim.Optimizer):\n            raise TypeError('{} is not an Optimizer'.format(\n                type(optimizer).__name__))\n        self.optimizer = optimizer\n        self._summary_writer = summary_writer\n\n        # Validate total_steps\n        if total_steps is None and epochs is None and steps_per_epoch is None:\n            raise ValueError(\"You must define either total_steps OR (epochs AND steps_per_epoch)\")\n        elif total_steps is not None:\n            if total_steps <= 0 or not isinstance(total_steps, int):\n                raise ValueError(\"Expected positive integer total_steps, but got {}\".format(total_steps))\n            self.total_steps = total_steps\n        else:\n            if epochs <= 0 or not isinstance(epochs, int):\n                raise ValueError(\"Expected positive integer epochs, but got {}\".format(epochs))\n            if steps_per_epoch <= 0 or not isinstance(steps_per_epoch, int):\n                raise ValueError(\"Expected positive integer steps_per_epoch, but got {}\".format(steps_per_epoch))\n            self.total_steps = epochs * steps_per_epoch\n\n        if three_phase:\n            self._schedule_phases = [\n                {\n                    'end_step': float(pct_start * self.total_steps) - 1,\n                    'start_lr': 'initial_lr',\n                    'end_lr': 'max_lr',\n                    'start_momentum': 'max_momentum',\n                    'end_momentum': 'base_momentum',\n                },\n                {\n                    'end_step': float(2 * pct_start * self.total_steps) - 2,\n                    'start_lr': 'max_lr',\n                    'end_lr': 'initial_lr',\n                    'start_momentum': 'base_momentum',\n                    'end_momentum': 'max_momentum',\n                },\n                {\n                    'end_step': self.total_steps - 1,\n                    'start_lr': 'initial_lr',\n                    'end_lr': 'min_lr',\n                    'start_momentum': 'max_momentum',\n                    'end_momentum': 'max_momentum',\n                },\n            ]\n        else:\n            self._schedule_phases = [\n                {\n                    'end_step': float(pct_start * self.total_steps) - 1,\n                    'start_lr': 'initial_lr',\n                    'end_lr': 'max_lr',\n                    'start_momentum': 'max_momentum',\n                    'end_momentum': 'base_momentum',\n                },\n                {\n                    'end_step': self.total_steps - 1,\n                    'start_lr': 'max_lr',\n                    'end_lr': 'min_lr',\n                    'start_momentum': 'base_momentum',\n                    'end_momentum': 'max_momentum',\n                },\n            ]\n\n        # Validate pct_start\n        if pct_start < 0 or pct_start > 1 or not isinstance(pct_start, float):\n            raise ValueError(\"Expected float between 0 and 1 pct_start, but got {}\".format(pct_start))\n\n        # Validate anneal_strategy\n        if anneal_strategy not in ['cos', 'linear']:\n            raise ValueError(\"anneal_strategy must by one of 'cos' or 'linear', instead got {}\".format(anneal_strategy))\n        elif anneal_strategy == 'cos':\n            self.anneal_func = self._annealing_cos\n        elif anneal_strategy == 'linear':\n            self.anneal_func = self._annealing_linear\n\n        # Initialize learning rate variables\n        max_lrs = self._format_param('max_lr', self.optimizer, max_lr)\n        if last_epoch == -1:\n            for idx, group in enumerate(self.optimizer.param_groups):\n                group['initial_lr'] = max_lrs[idx] / div_factor\n                group['max_lr'] = max_lrs[idx]\n                group['min_lr'] = group['initial_lr'] / final_div_factor\n\n        # Initialize momentum variables\n        self.cycle_momentum = cycle_momentum\n        if self.cycle_momentum:\n            if 'momentum' not in self.optimizer.defaults and 'betas' not in self.optimizer.defaults:\n                raise ValueError('optimizer must support momentum with `cycle_momentum` option enabled')\n            self.use_beta1 = 'betas' in self.optimizer.defaults\n            max_momentums = self._format_param('max_momentum', optimizer, max_momentum)\n            base_momentums = self._format_param('base_momentum', optimizer, base_momentum)\n            if last_epoch == -1:\n                for m_momentum, b_momentum, group in zip(max_momentums, base_momentums, optimizer.param_groups):\n                    if self.use_beta1:\n                        _, beta2 = group['betas']\n                        group['betas'] = (m_momentum, beta2)\n                    else:\n                        group['momentum'] = m_momentum\n                    group['max_momentum'] = m_momentum\n                    group['base_momentum'] = b_momentum\n\n        super(OneCycleLR, self).__init__(optimizer, last_epoch, verbose)\n\n    def _format_param(self, name, optimizer, param):\n        \"\"\"Return correctly formatted lr/momentum for each param group.\"\"\"\n        if isinstance(param, (list, tuple)):\n            if len(param) != len(optimizer.param_groups):\n                raise ValueError(\"expected {} values for {}, got {}\".format(\n                    len(optimizer.param_groups), name, len(param)))\n            return param\n        else:\n            return [param] * len(optimizer.param_groups)\n\n    def _annealing_cos(self, start, end, pct):\n        \"Cosine anneal from `start` to `end` as pct goes from 0.0 to 1.0.\"\n        cos_out = math.cos(math.pi * pct) + 1\n        return end + (start - end) / 2.0 * cos_out\n\n    def _annealing_linear(self, start, end, pct):\n        \"Linearly anneal from `start` to `end` as pct goes from 0.0 to 1.0.\"\n        return (end - start) * pct + start\n\n    def get_lr(self):\n        if not self._get_lr_called_within_step:\n            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n                          \"please use `get_last_lr()`.\", UserWarning)\n\n        lrs = []\n        step_num = self.last_epoch\n\n        if step_num > self.total_steps:\n            raise ValueError(\"Tried to step {} times. The specified number of total steps is {}\"\n                             .format(step_num + 1, self.total_steps))\n\n        for group in self.optimizer.param_groups:\n            start_step = 0\n            for i, phase in enumerate(self._schedule_phases):\n                end_step = phase['end_step']\n                if step_num <= end_step or i == len(self._schedule_phases) - 1:\n                    pct = (step_num - start_step) / (end_step - start_step)\n                    computed_lr = self.anneal_func(group[phase['start_lr']], group[phase['end_lr']], pct)\n                    if self.cycle_momentum:\n                        computed_momentum = self.anneal_func(group[phase['start_momentum']], group[phase['end_momentum']], pct)\n                    break\n                start_step = phase['end_step']\n\n            lrs.append(computed_lr)\n            if self.cycle_momentum:\n                if self.use_beta1:\n                    _, beta2 = group['betas']\n                    group['betas'] = (computed_momentum, beta2)\n                else:\n                    group['momentum'] = computed_momentum\n\n        return lrs\n    \n    def step(self, epoch=None):\n        # Raise a warning if old pattern is detected\n        # https://github.com/pytorch/pytorch/issues/20124\n        if self._step_count == 1:\n            if not hasattr(self.optimizer.step, \"_with_counter\"):\n                warnings.warn(\"Seems like `optimizer.step()` has been overridden after learning rate scheduler \"\n                              \"initialization. Please, make sure to call `optimizer.step()` before \"\n                              \"`lr_scheduler.step()`. See more details at \"\n                              \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n\n            # Just check if there were two first lr_scheduler.step() calls before optimizer.step()\n            elif self.optimizer._step_count < 1:\n                warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n                              \"In PyTorch 1.1.0 and later, you should call them in the opposite order: \"\n                              \"`optimizer.step()` before `lr_scheduler.step()`.  Failure to do this \"\n                              \"will result in PyTorch skipping the first value of the learning rate schedule. \"\n                              \"See more details at \"\n                              \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n        self._step_count += 1\n\n        class _enable_get_lr_call:\n\n            def __init__(self, o):\n                self.o = o\n\n            def __enter__(self):\n                self.o._get_lr_called_within_step = True\n                return self\n\n            def __exit__(self, type, value, traceback):\n                self.o._get_lr_called_within_step = False\n\n        with _enable_get_lr_call(self):\n            if epoch is None:\n                self.last_epoch += 1\n                values = self.get_lr()\n            else:\n                warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n                self.last_epoch = epoch\n                if hasattr(self, \"_get_closed_form_lr\"):\n                    values = self._get_closed_form_lr()\n                else:\n                    values = self.get_lr()\n\n        for i, data in enumerate(zip(self.optimizer.param_groups, values)):\n            param_group, lr = data\n            param_group['lr'] = lr\n            self.print_lr(self.verbose, i, lr, epoch)\n\n        self._last_lr = [group['lr'] for group in self.optimizer.param_groups]\n        \n        if self._summary_writer:\n            if self._is_warmup_epoch() or (self._step_count % self._num_steps_per_epoch == 0):\n                test_utils.write_to_summary(\n                    self._summary_writer,\n                    self._step_count,\n                    dict_to_write={\n                        'lr': self.optimizer.param_groups[0]['lr']\n                    },\n                    write_xla_metrics=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-01T13:31:09.592377Z","iopub.execute_input":"2022-03-01T13:31:09.592794Z","iopub.status.idle":"2022-03-01T13:31:09.642675Z","shell.execute_reply.started":"2022-03-01T13:31:09.592765Z","shell.execute_reply":"2022-03-01T13:31:09.64187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Run Training</h1></span>","metadata":{}},{"cell_type":"code","source":"# Only instantiate model weights once in memory.\nWRAPPED_MODEL = xmp.MpModelWrapper(HappyWhaleModel(CONFIG['model_name']))\n\ndef train():\n    device = xm.xla_device()\n    xm.set_rng_state(CONFIG['seed'], device)\n    \n    df_train = df[df.kfold != CONFIG['fold_to_run']].reset_index(drop=True)\n    df_valid = df[df.kfold == CONFIG['fold_to_run']].reset_index(drop=True)\n\n    train_dataset = HappyWhaleDataset(df_train, transforms=data_transforms[\"train\"])\n    valid_dataset = HappyWhaleDataset(df_valid, transforms=data_transforms[\"valid\"])\n    \n    train_dataset = CachedDataset(train_dataset, './train_dataset_cache')\n    valid_dataset = CachedDataset(valid_dataset, './valid_dataset_cache')\n    \n    train_sampler, test_sampler = None, None\n    if xm.xrt_world_size() > 1:\n        train_sampler = torch.utils.data.distributed.DistributedSampler(\n            train_dataset,\n            num_replicas=xm.xrt_world_size(),\n            rank=xm.get_ordinal(),\n            shuffle=True)\n        valid_sampler = torch.utils.data.distributed.DistributedSampler(\n            valid_dataset,\n            num_replicas=xm.xrt_world_size(),\n            rank=xm.get_ordinal(),\n            shuffle=False)\n        \n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=CONFIG['train_batch_size'],\n        sampler=train_sampler,\n        drop_last=True,\n        shuffle=False if train_sampler else True,\n        num_workers=2)\n    \n    valid_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=CONFIG['valid_batch_size'],\n        sampler=valid_sampler,\n        drop_last=False,\n        shuffle=False,\n        num_workers=2)\n    \n    \n    train_loader = pl.MpDeviceLoader(train_loader, device) # puts the train data onto the current TPU core\n    valid_loader = pl.MpDeviceLoader(valid_loader, device) # puts the valid data onto the current TPU core\n    \n    # Scale learning rate to world size\n    lr = CONFIG['learning_rate'] * xm.xrt_world_size()\n\n    # Get loss function, optimizer, lr_scheduler and model\n    model = WRAPPED_MODEL.to(device)\n    model.set_device(device)\n    optimizer = optim.AdamW(model.parameters(), lr = lr, weight_decay=CONFIG['weight_decay'])\n    lr_scheduler = OneCycleLR(optimizer,max_lr = lr,total_steps = CONFIG['epochs'] * len(train_loader))\n    loss_fn = nn.CrossEntropyLoss()\n    \n    def train_loop_fn(loader, epoch):\n        total_samples, total_loss = 0, 0\n        model.train()\n        \n        python_loss = None\n        for step, data in enumerate(loader, start=1): \n            optimizer.zero_grad()\n            images = data['image']\n            labels = data['label']\n            \n            outputs, emb = model(images, labels)\n            \n            loss = loss_fn(outputs, labels)\n            \n            loss.backward()\n            xm.optimizer_step(optimizer)\n            \n            if lr_scheduler:\n                lr_scheduler.step()\n                \n            if step % CONFIG['log_steps'] == 0:\n                python_loss = loss.item()\n                print('[xla:{}] (Epoch {} Step {}) Training Loss={:.5f} Time={}'.format(\n                    xm.get_ordinal(), epoch, step, python_loss, time.asctime()), flush=True)\n            \n                python_loss += loss * CONFIG['train_batch_size']\n                total_samples += CONFIG['train_batch_size']\n                del python_loss\n            \n            del images, labels, outputs, emb, loss\n            gc.collect()\n            \n        train_epoch_loss = total_loss / total_samples\n        train_epoch_loss = xm.mesh_reduce('train_epoch_loss', train_epoch_loss, np.mean)\n        del total_loss, total_samples\n        gc.collect()\n        return train_epoch_loss\n    \n    @torch.no_grad()\n    def val_loop_fn(loader, epoch):\n        total_samples, total_loss = 0, 0\n        model.eval()\n        \n        python_loss = None\n        for step, data in enumerate(loader, start=1):     \n            images = data['image']\n            labels = data['label']\n\n            batch_size = images.size(0)\n\n            outputs, emb = model(images, labels)\n            \n            loss = loss_fn(outputs, labels)\n            \n            python_loss = loss.item()\n            total_loss += python_loss * batch_size\n            total_samples += batch_size\n            \n            if step % CONFIG['log_steps'] == 0:\n                print('[xla:{}] (Epoch {} Step {}) Validation Loss={:.5f} Time={}'.format(\n                    xm.get_ordinal(), epoch, step, python_loss, time.asctime()), flush=True) \n            \n            del images, labels, batch_size, outputs, emb, loss, python_loss\n            gc.collect()\n                \n        val_epoch_loss = total_loss / total_samples\n        val_epoch_loss = xm.mesh_reduce('val_epoch_loss', val_epoch_loss, np.mean)\n        del total_loss, total_samples\n        gc.collect()\n        return val_epoch_loss\n\n    # Train and eval loops\n    train_epoch_loss, val_epoch_loss = None, None\n    xm.master_print(\"Start Training\")\n    gc.collect()\n    for epoch in range(1,CONFIG['epochs'] + 1):\n        train_epoch_loss = train_loop_fn(train_loader, epoch)\n        xm.master_print(\"Finished training epoch {} Loss={}\".format(epoch, train_epoch_loss))\n        \n        val_epoch_loss = val_loop_fn(valid_loader, epoch)\n        xm.master_print(\"Finished validation epoch {} Loss={}\".format(epoch, val_epoch_loss))\n        \n        del train_epoch_loss, val_epoch_loss\n        gc.collect()\n        \n        xm.master_print(met.metrics_report())\n        \n    gc.collect()\n    xm.rendezvous(\"save_model\")\n    xm.save(model.state_dict(), \"Loss{:.4f}_epoch{:.0f}.bin\".format(val_epoch_loss, epoch))","metadata":{"execution":{"iopub.status.busy":"2022-03-01T13:38:51.598386Z","iopub.execute_input":"2022-03-01T13:38:51.598725Z","iopub.status.idle":"2022-03-01T13:38:52.388353Z","shell.execute_reply.started":"2022-03-01T13:38:51.598693Z","shell.execute_reply":"2022-03-01T13:38:52.387629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Start training processes\ndef _mp_fn(rank):\n    torch.set_default_tensor_type('torch.FloatTensor')\n    train()\n\nxmp.spawn(_mp_fn, args=(), nprocs=8, start_method='fork')","metadata":{"execution":{"iopub.status.busy":"2022-03-01T13:38:59.105047Z","iopub.execute_input":"2022-03-01T13:38:59.105482Z","iopub.status.idle":"2022-03-01T13:45:05.69308Z","shell.execute_reply.started":"2022-03-01T13:38:59.105444Z","shell.execute_reply":"2022-03-01T13:45:05.687861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![Upvote!](https://img.shields.io/badge/Upvote-If%20you%20like%20my%20work-07b3c8?style=for-the-badge&logo=kaggle)","metadata":{}}]}