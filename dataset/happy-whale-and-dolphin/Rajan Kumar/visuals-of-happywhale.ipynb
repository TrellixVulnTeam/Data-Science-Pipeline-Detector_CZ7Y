{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# EDA for HappyWhale Image Prediction","metadata":{"_uuid":"3ba03c4e-d7b1-4ddc-91f2-e2c9af79730c","_cell_guid":"26e0f626-7b69-4c00-8edb-f33d439e3678","execution":{"iopub.status.busy":"2022-02-15T13:19:34.789237Z","iopub.execute_input":"2022-02-15T13:19:34.789543Z","iopub.status.idle":"2022-02-15T13:19:34.794359Z","shell.execute_reply.started":"2022-02-15T13:19:34.789513Z","shell.execute_reply":"2022-02-15T13:19:34.793482Z"},"trusted":true}},{"cell_type":"markdown","source":"# Load Packages","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nfrom PIL import Image\nimport re\nfrom termcolor import colored\nsns.set_style(\"darkgrid\")","metadata":{"_uuid":"907d7eac-5a7b-420d-a086-cef715a60729","_cell_guid":"084a0e5c-7ad0-4120-9e9d-c8dbc6b73b7b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-01T05:50:18.725094Z","iopub.execute_input":"2022-04-01T05:50:18.725505Z","iopub.status.idle":"2022-04-01T05:50:18.731763Z","shell.execute_reply.started":"2022-04-01T05:50:18.725457Z","shell.execute_reply":"2022-04-01T05:50:18.731146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"TRAIN_IMAGE_DIR = '/kaggle/input/happy-whale-and-dolphin/train_images'\nTEST_IMAGE_DIR = '/kaggle/input/happy-whale-and-dolphin/test_images'\n\nn_train_images = len(os.listdir(TRAIN_IMAGE_DIR))\nn_test_images = len(os.listdir(TEST_IMAGE_DIR))\ntrain_df = pd.read_csv('/kaggle/input/happy-whale-and-dolphin/train.csv')\nsample_submission = pd.read_csv('/kaggle/input/happy-whale-and-dolphin/sample_submission.csv')","metadata":{"_uuid":"5cbe91c0-2a3b-48a6-ac77-44ad636e7466","_cell_guid":"b157a538-d9ba-49de-902f-92a15afbc371","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-01T05:50:18.733189Z","iopub.execute_input":"2022-04-01T05:50:18.733823Z","iopub.status.idle":"2022-04-01T05:50:18.901665Z","shell.execute_reply.started":"2022-04-01T05:50:18.733788Z","shell.execute_reply":"2022-04-01T05:50:18.900544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Basic Statistics & Name Corrections","metadata":{}},{"cell_type":"code","source":"print('# of training images = {}'.format(n_train_images))\nprint('# of testing images = {}'.format(n_test_images))\nprint('Shape of train_df ={}'.format(train_df.shape))\nprint('Shape of sample_submission = {}'.format(sample_submission.shape))\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-01T05:50:18.90315Z","iopub.execute_input":"2022-04-01T05:50:18.903607Z","iopub.status.idle":"2022-04-01T05:50:18.918226Z","shell.execute_reply.started":"2022-04-01T05:50:18.903553Z","shell.execute_reply":"2022-04-01T05:50:18.917182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_species = train_df.species.unique()\nn_unique_species = len(unique_species)\nn_unique_ids = len(train_df.individual_id.unique())\nprint('Number of unique species = {}'.format(n_unique_species))\nprint('Number of unique ids = {}'.format(n_unique_ids))\nprint('\\nList of species:\\n', unique_species)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T05:50:18.919657Z","iopub.execute_input":"2022-04-01T05:50:18.919883Z","iopub.status.idle":"2022-04-01T05:50:18.946439Z","shell.execute_reply.started":"2022-04-01T05:50:18.919855Z","shell.execute_reply":"2022-04-01T05:50:18.945507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, we can see, there are some species which are misspelt. For example, kiler and bottlenose_dolpin should be killer and bottlenose_dolphin. Also, beluga and globis are not categorized. Let's categorize them as whale. However, before making all these changes, let's see the distribution of these categories.","metadata":{}},{"cell_type":"code","source":"train_df['category'] = [re.split('_', s)[-1] for s in train_df.species]\ntrain_df['only_species'] = [re.sub('_dolphin|_whale', '', s) for s in train_df.species]\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-01T05:50:18.949102Z","iopub.execute_input":"2022-04-01T05:50:18.949792Z","iopub.status.idle":"2022-04-01T05:50:19.142657Z","shell.execute_reply.started":"2022-04-01T05:50:18.949741Z","shell.execute_reply":"2022-04-01T05:50:19.141791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_train_dolphins = sum(train_df.category == 'dolphin')\nn_train_whales = sum(train_df.category == 'whale')\nprint('Number of dolphins in train = {}'.format(n_train_dolphins))\nprint('Number of whales in train = {}'.format(n_train_whales))\npie_plot_data = train_df.groupby('category')['image'].count()\nlabels = pie_plot_data.keys()\nexplode = [0.015]*len(labels)\npie, ax = plt.subplots(figsize = [10,6])\nplt.pie(pie_plot_data, autopct = \"%.1f%%\", labels = labels, explode = explode, pctdistance = 0.5)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-01T05:50:19.144334Z","iopub.execute_input":"2022-04-01T05:50:19.14482Z","iopub.status.idle":"2022-04-01T05:50:19.323405Z","shell.execute_reply.started":"2022-04-01T05:50:19.144773Z","shell.execute_reply":"2022-04-01T05:50:19.322464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From pie chart, we can clearly see, apart from whale and dolphin, there are few species which do not have whale or dolphin as suffix. Also, few dolphin are misspelt as dolpins. So, we will correct the spellings of dolpin. Also, consider beluga and globus as whale.\n","metadata":{}},{"cell_type":"code","source":"name_mapping = {'kiler_whale': 'killer_whale',\n               'bottlenose_dolpin': 'bottlenose_dolphin',\n               'beluga': 'beluga_whale',\n               'globis': 'globis_whale'}\n\ntrain_df['species'] = train_df['species'].replace(name_mapping)\n\ntrain_df['category'] = [re.split('_', s)[-1] for s in train_df.species]\ntrain_df['only_species'] = [re.sub('_dolphin|_whale', '', s) for s in train_df.species]\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-01T05:50:19.325288Z","iopub.execute_input":"2022-04-01T05:50:19.326416Z","iopub.status.idle":"2022-04-01T05:50:19.566968Z","shell.execute_reply.started":"2022-04-01T05:50:19.326352Z","shell.execute_reply":"2022-04-01T05:50:19.566077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(colored('Before preprocessing names:', 'red'))\nprint('Number of unique species {}'.format(n_unique_species))\nprint('Number of unique ids {}'.format(n_unique_ids))\nprint('Number of dolphins in train = {}'.format(n_train_dolphins))\nprint('Number of whales in train = {}'.format(n_train_whales))\n\nprint(colored('\\nAfter preprocessing names:', 'red'))\nunique_species = train_df.species.unique()\nprint('Number of unique species {}'.format(len(unique_species)))\nprint('Number of unique ids {}'.format(len(train_df.individual_id.unique())))\nprint('Number of dolphins in train = {}'.format(sum(train_df.category == 'dolphin')))\nprint('Number of whales in train = {}'.format(sum(train_df.category == 'whale')))\nprint('Number of dolphin species = {}'.format(len(train_df[train_df.category == 'dolphin'].species.unique())))\nprint('Number of whale species = {}'.format(len(train_df[train_df.category == 'whale'].species.unique())))\npie_plot_data = train_df.groupby('category')['image'].count()\nlabels = pie_plot_data.keys()\nexplode = [0.015]*len(labels)\npie, ax = plt.subplots(figsize = [10,6])\nplt.pie(pie_plot_data, autopct = \"%.1f%%\", labels = labels, explode = explode, pctdistance = 0.5)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-01T05:50:19.568216Z","iopub.execute_input":"2022-04-01T05:50:19.568431Z","iopub.status.idle":"2022-04-01T05:50:19.765155Z","shell.execute_reply.started":"2022-04-01T05:50:19.568404Z","shell.execute_reply":"2022-04-01T05:50:19.762839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the pie-chart, we can infer that two-third are of the images are of whale. Now, let's see, which species are more common in whales and dolphins.","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize = (10,7), dpi = 120)\ngs = gridspec.GridSpec(2,2)\ncolors = sns.color_palette()\n\nax0 = fig.add_subplot(gs[0:2,0])\nspecies_count = pd.DataFrame(train_df.groupby('species').image.count())\nfifth_largest_value = species_count['image'].nlargest(5)[4]\ncolor_plt = [colors[0] if value < fifth_largest_value else colors[3] for value in species_count.image]\nbar = sns.barplot(data = species_count, y = species_count.index, x = 'image',  ax = ax0, palette = color_plt)\nplt.xlabel('# of images')\nplt.ylabel('')\nplt.title('All Species')\n\n\nax1 = fig.add_subplot(gs[0,1])\ndolphin_count = train_df[train_df.category == 'dolphin'].groupby('only_species').image.count()\ndolphin_count = pd.DataFrame(dolphin_count.sort_values(ascending = False))\nsns.barplot(data = dolphin_count, y = dolphin_count.index, x = 'image', ax = ax1, color = colors[0])\nplt.xlabel('')\nplt.ylabel('')\nplt.title('Dolphin')\n\nax2 = fig.add_subplot(gs[1,1])\nwhale_count = train_df[train_df.category == 'whale'].groupby('only_species').image.count()\nwhale_count = pd.DataFrame(whale_count.sort_values(ascending = False))\nsns.barplot(data = whale_count, y = whale_count.index, x = 'image', ax = ax2, color = colors[0])\nplt.xlabel('# of images')\nplt.ylabel('')\nplt.title('Whale')\nplt.subplots_adjust(left=0,\n                    bottom=0, \n                    right=1, \n                    top=1, \n                    wspace=0.35, \n                    hspace=0.2)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T05:50:19.766645Z","iopub.execute_input":"2022-04-01T05:50:19.76706Z","iopub.status.idle":"2022-04-01T05:50:21.135594Z","shell.execute_reply.started":"2022-04-01T05:50:19.767012Z","shell.execute_reply":"2022-04-01T05:50:21.134617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From bar plot of all species, we can see the top 5 species (in red bars) in terms of number of images. 4 of them are whales and only one is dolphin. In the right hand side, we have separate bar plots for dolphins and whales. We have more species of whales compared to dolphins. Among dolphins, bottlenose is most common, however, among whales, beluga and humpback are most common.","metadata":{}},{"cell_type":"markdown","source":"# Visualizing Images","metadata":{"_uuid":"9ec99267-aef9-4512-a97d-d46def20011e","_cell_guid":"0f53b938-7665-4848-a3af-2f16a18701d3","execution":{"iopub.status.busy":"2022-02-15T13:23:14.017816Z","iopub.execute_input":"2022-02-15T13:23:14.018144Z","iopub.status.idle":"2022-02-15T13:23:14.028802Z","shell.execute_reply.started":"2022-02-15T13:23:14.018109Z","shell.execute_reply":"2022-02-15T13:23:14.028045Z"},"trusted":true}},{"cell_type":"markdown","source":"## Prepare dataset and save it","metadata":{"_uuid":"e61d41f1-3af9-480d-8a20-8a398e09d532","_cell_guid":"63b3a0d5-a0c4-4773-bee2-6e079b98df61","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-15T18:34:31.461115Z","iopub.execute_input":"2022-02-15T18:34:31.461333Z","iopub.status.idle":"2022-02-15T18:34:31.490661Z","shell.execute_reply.started":"2022-02-15T18:34:31.461307Z","shell.execute_reply":"2022-02-15T18:34:31.49003Z"}}},{"cell_type":"markdown","source":"Commented the below code to ignore running the code again and again. Data saved could be resused for further running.\n","metadata":{}},{"cell_type":"code","source":"train_df.head()\nprint(colored('Image loading and information extraction started...', 'red'))\nfor i, train_image in enumerate(train_df.image):\n    img = Image.open(os.path.join(TRAIN_IMAGE_DIR, train_image))\n    train_df.loc[i, 'image_mode'] = img.mode\n    train_df.loc[i, 'format'] = img.format\n    train_df.loc[i, 'image_width'] = img.size[0]\n    train_df.loc[i, 'image_height'] = img.size[1]\n    train_df.loc[i, 'aspect_ratio'] = img.size[0]/img.size[1]\n    if (i+1)%1000 == 0:\n        print('{}/{} images processed'.format(i+1, len(train_df.image)))\nprint(colored('Image loading and information extraction completed...', 'red'))\n\ntrain_df.to_csv('extra_data_train.csv', index = False)\nprint(colored('\\nTrain data saved in working directory...', 'red'))","metadata":{"execution":{"iopub.status.busy":"2022-04-01T05:50:21.137039Z","iopub.execute_input":"2022-04-01T05:50:21.137339Z","iopub.status.idle":"2022-04-01T06:02:33.884045Z","shell.execute_reply.started":"2022-04-01T05:50:21.137297Z","shell.execute_reply":"2022-04-01T06:02:33.883172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_df = pd.read_csv('../input/visuals-of-happywhale/extra_data_train.csv')\n\n# print('Number of unique mode = {}'.format(len(train_df.image_mode.unique())))\n# print('Number of unique format = {}'.format(len(train_df.format.unique())))\n# print('Number of unique width = {}'.format(len(train_df.image_width.unique())))\n# print('Number of unique height = {}'.format(len(train_df.image_height.unique())))\n# train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-01T06:02:33.885698Z","iopub.execute_input":"2022-04-01T06:02:33.885998Z","iopub.status.idle":"2022-04-01T06:02:33.916755Z","shell.execute_reply.started":"2022-04-01T06:02:33.885955Z","shell.execute_reply":"2022-04-01T06:02:33.915431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From above stats, we can see that mode of images, width and height are not unique. Let's explore it more.","metadata":{}},{"cell_type":"markdown","source":"## Check Modes of Images","metadata":{}},{"cell_type":"code","source":"train_image_modes = train_df.groupby('image_mode')['image'].count()\ntrain_image_modes\nlabels = train_image_modes.keys()\nexplode = [0.02]*len(labels)\npie, ax = plt.subplots(figsize = [10,6])\nplt.pie(train_image_modes, autopct = \"%.1f%%\", labels = labels, explode = explode, pctdistance = 0.5)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-01T06:32:25.953128Z","iopub.execute_input":"2022-04-01T06:32:25.953757Z","iopub.status.idle":"2022-04-01T06:32:26.050039Z","shell.execute_reply.started":"2022-04-01T06:32:25.953664Z","shell.execute_reply":"2022-04-01T06:32:26.048525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Basic difference between L mode and RGB mode is that L has only one channel and RGB has 3 channels. Let's plot and see these different modes of images. ","metadata":{}},{"cell_type":"code","source":"n_images_plot = 40\nn_col_plot = 8\nfigsize = (20,10)\nfontsize = 20\n\ndef plot_images(images, cmap = None):\n    fig, axs = plt.subplots(int(n_images_plot/n_col_plot), n_col_plot, figsize = figsize)\n    for i, train_image in enumerate(images.image[0:n_images_plot]):\n        img = Image.open(os.path.join(TRAIN_IMAGE_DIR, train_image))\n        axs.ravel()[i].imshow(img, cmap = cmap)\n        axs.ravel()[i].set_axis_off()\n        axs.ravel()[i].set_title('#{}. {}'.format(i+1, images.species[i]))\n    return fig, axs\n\nimages = train_df[train_df.image_mode == 'L'].reset_index()\nfig, axs = plot_images(images, cmap = 'gray')\nplt.suptitle('Sample exaples of L mode images', fontsize = fontsize)\nplt.tight_layout()\nplt.show()\n\nprint('')\nimages = train_df[train_df.image_mode == 'RGB'].reset_index()\nfig, axs = plot_images(images)\nplt.suptitle('Sample exaples of RGB images', fontsize = fontsize)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-01T06:02:33.919691Z","iopub.status.idle":"2022-04-01T06:02:33.920013Z","shell.execute_reply.started":"2022-04-01T06:02:33.919842Z","shell.execute_reply":"2022-04-01T06:02:33.919864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can clearly see the difference between L mode and RGB mode of images.\n\nNow let's explore variation in sizes of images.","metadata":{"execution":{"iopub.status.busy":"2022-02-20T10:31:36.601297Z","iopub.execute_input":"2022-02-20T10:31:36.601639Z","iopub.status.idle":"2022-02-20T10:32:00.643477Z","shell.execute_reply.started":"2022-02-20T10:31:36.601609Z","shell.execute_reply":"2022-02-20T10:32:00.641606Z"}}},{"cell_type":"markdown","source":"# Explore Various Sizes of Images","metadata":{}},{"cell_type":"code","source":"print('Minimum width = {}, Minimum height = {}'.format(int(train_df.image_width.min()),\n                                                       int(train_df.image_height.min())))\nprint('Maximum width = {}, Maximum height = {}'.format(int(train_df.image_width.max()),\n                                                       int(train_df.image_height.max())))\n\n# fig, ax = plt.subplots(figsize = (10, 5))\njp = sns.jointplot(data = train_df, x = 'image_width', y = 'image_height', hue = 'category', height = 8, ratio = 5)\njp.set_axis_labels('Image Width (pixels)', 'Image Height (pixels)')\n# jp.set_legend(loc=\"upper left\", ncol = 2)\nplt.show()\n\nfig = plt.figure(figsize = (15,5), dpi = 120)\ngs = gridspec.GridSpec(2,2)\n\nax0 = fig.add_subplot(gs[0, 0:2])\nsns.histplot(data = train_df, x = 'aspect_ratio', ax = ax0)\nplt.title('All Images')\n\nind_1p5 = (train_df['aspect_ratio'] >= 1.49) & (train_df['aspect_ratio'] <= 1.51)\nax1 = fig.add_subplot(gs[1, 0])\nsns.histplot(data = train_df[ind_1p5], x = 'aspect_ratio', ax = ax1)\nplt.title('Aspect Ratio is 1.5')\n\n# ind_1p5 = (train_df['aspect_ratio'] >= 1.4) & (train_df['aspect_ratio'] <= 1.6)\nax1 = fig.add_subplot(gs[1, 1])\nsns.histplot(data = train_df[(~ind_1p5)], x = 'aspect_ratio', ax = ax1)\nplt.title('Aspect Ratio is not 1.5')\nplt.subplots_adjust(hspace=0.5)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-01T06:02:33.921393Z","iopub.status.idle":"2022-04-01T06:02:33.921752Z","shell.execute_reply.started":"2022-04-01T06:02:33.921575Z","shell.execute_reply":"2022-04-01T06:02:33.9216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the distribution plot of image widths and heights, we can see most of the images have width more than 3000 px, similary heights are also mostly more than 2000 px. Another observation we can make here is that many images (for both, whales and dolphins) have smaller height, i.e the aspect ratio of width to height is high.\n    \nLooking into aspect ratio distribution plot, we can see many images have aspect ratio close to 1.5. I have separately plotted for close to 1.5 and other than 1.5.\n\nNow, let's see images of whales and dolphins of various aspect ratio.","metadata":{}},{"cell_type":"code","source":"n_images_plot = 16\nn_col_plot = 8\nfigsize = (20,5)\nfontsize = 20\n\ndef plot_images(images):\n    fig, axs = plt.subplots(int(n_images_plot/n_col_plot), n_col_plot, figsize = figsize)\n    for i, train_image in enumerate(images.image[0:n_images_plot]):\n        img = Image.open(os.path.join(TRAIN_IMAGE_DIR, train_image))\n        axs.ravel()[i].imshow(img)\n        axs.ravel()[i].set_axis_off()\n        axs.ravel()[i].set_title('#{}. Aspect Ratio = {:.2f}'.format(i+1, images.aspect_ratio[i]))\n    return fig, axs\n\nimages = train_df[train_df['aspect_ratio'] < 0.75].reset_index()\nfig, axs = plot_images(images)\nplt.suptitle('Sample images with aspect ratio less than 0.75', fontsize = fontsize)\nplt.tight_layout()\nplt.show()\n\nprint('')\nimages = train_df[(train_df['aspect_ratio'] >= 0.75) & (train_df['aspect_ratio'] < 1)].reset_index()\nfig, axs = plot_images(images)\nplt.suptitle('Sample images with aspect ratio between 0.75 and 1', fontsize = fontsize)\nplt.tight_layout()\nplt.show()\n\nprint('')\nimages = train_df[(train_df['aspect_ratio'] == 1)].reset_index()\nfig, axs = plot_images(images)\nplt.suptitle('Sample images with aspect ratio equal to 1', fontsize = fontsize)\nplt.tight_layout()\nplt.show()\n\nprint('')\nimages = train_df[(train_df['aspect_ratio'] >= 1) & (train_df['aspect_ratio'] < 1.5)].reset_index()\nfig, axs = plot_images(images)\nplt.suptitle('Sample images with aspect ratio between 1 and 1.5', fontsize = fontsize)\nplt.tight_layout()\nplt.show()\n             \nprint('')\nimages = train_df[(train_df['aspect_ratio'] == 1.5)].reset_index()\nfig, axs = plot_images(images)\nplt.suptitle('Sample images with aspect ratio equal to 1.5', fontsize = fontsize)\nplt.tight_layout()\nplt.show()\n             \nprint('')\nimages = train_df[(train_df['aspect_ratio'] >= 1.5) & (train_df['aspect_ratio'] < 2)].reset_index()\nfig, axs = plot_images(images)\nplt.suptitle('Sample images with aspect ratio between 1.5 and 2', fontsize = fontsize)\nplt.tight_layout()\nplt.show()\n             \nprint('')\nimages = train_df[(train_df['aspect_ratio'] >= 2) & (train_df['aspect_ratio'] < 5)].reset_index()\nfig, axs = plot_images(images)\nplt.suptitle('Sample images with aspect ratio between 2 and 5', fontsize = fontsize)\nplt.tight_layout()\nplt.show()\n\nprint('')\nimages = train_df[(train_df['aspect_ratio'] >= 5) & (train_df['aspect_ratio'] < 10)].reset_index()\nfig, axs = plot_images(images)\nplt.suptitle('Sample images with aspect ratio between 5 and 10', fontsize = fontsize)\nplt.tight_layout()\nplt.show()\n\nprint('')\nimages = train_df[(train_df['aspect_ratio'] >= 10)].reset_index()\nfig, axs = plot_images(images)\nplt.suptitle('Sample images with aspect ratio greater than 10', fontsize = fontsize)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-01T06:02:33.923151Z","iopub.status.idle":"2022-04-01T06:02:33.923511Z","shell.execute_reply.started":"2022-04-01T06:02:33.923314Z","shell.execute_reply":"2022-04-01T06:02:33.923336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above plots, we can observe that images with aspect ratio less than 1 captured mostly fins while with larger aspect ratio, shows whole body.\n    \nFor further processing, we need to convert all these images to same size. We have done it [in this notebook](https://www.kaggle.com/code/rajankumar/resize-images-128x128-using-cv2-and-tensorflow).    ","metadata":{"execution":{"iopub.status.busy":"2022-03-08T05:19:58.699954Z","iopub.execute_input":"2022-03-08T05:19:58.700322Z","iopub.status.idle":"2022-03-08T05:19:58.706333Z","shell.execute_reply.started":"2022-03-08T05:19:58.700282Z","shell.execute_reply":"2022-03-08T05:19:58.7054Z"}}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}