{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# WORK IN PROGRESS ! ","metadata":{}},{"cell_type":"markdown","source":"## FOR DATASET https://www.kaggle.com/awsaf49/happywhale-cropped-dataset-yolov5 thanks @awsaf49\n","metadata":{}},{"cell_type":"code","source":"# #For TPU\n# !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n# !python pytorch-xla-env-setup.py --version 1.7 --apt-packages libomp5 libopenblas-dev\n# ! pip -q install pytorch-lightning==1.1.5","metadata":{"execution":{"iopub.status.busy":"2022-02-12T18:39:36.269115Z","iopub.execute_input":"2022-02-12T18:39:36.26948Z","iopub.status.idle":"2022-02-12T18:40:29.94066Z","shell.execute_reply.started":"2022-02-12T18:39:36.269426Z","shell.execute_reply":"2022-02-12T18:40:29.939591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# importing required stuff\n# !pip -q install --upgrade wandb\nimport pandas as pd\nimport albumentations\nimport torchvision \nfrom PIL import Image\nimport wandb\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns \nimport torch \nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nwandb.login(key = user_secrets.get_secret(\"wandb_key\"))\nimport cv2\nfrom sklearn.model_selection import train_test_split\nimport pytorch_lightning as pl \nimport torch \nimport numpy as np \nimport pandas as pd\nimport  os \nimport sys \nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import SubsetRandomSampler\nsys.path.append(\"../input/timm-pytorch-image-models/pytorch-image-models-master\")\nimport timm\nimport math\nimport matplotlib.pyplot as plt\nimport torchvision","metadata":{"execution":{"iopub.status.busy":"2022-02-12T18:40:29.943787Z","iopub.execute_input":"2022-02-12T18:40:29.944523Z","iopub.status.idle":"2022-02-12T18:40:30.304288Z","shell.execute_reply.started":"2022-02-12T18:40:29.944476Z","shell.execute_reply":"2022-02-12T18:40:30.303742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip -q ../input/happywhale-cropped-dataset-yolov5/train_images.zip\n!unzip -q  ../input/happywhale-cropped-dataset-yolov5/test_images.zip\ntrain = pd.read_csv(\"../input/happywhale-cropped-dataset-yolov5/train.csv\")\ntest = pd.read_csv(\"../input/happywhale-cropped-dataset-yolov5/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-02-12T18:40:30.305076Z","iopub.execute_input":"2022-02-12T18:40:30.305265Z","iopub.status.idle":"2022-02-12T18:41:13.30797Z","shell.execute_reply.started":"2022-02-12T18:40:30.305243Z","shell.execute_reply":"2022-02-12T18:41:13.306963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ntrain.species= le.fit_transform(train.species)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T18:41:13.310696Z","iopub.execute_input":"2022-02-12T18:41:13.311142Z","iopub.status.idle":"2022-02-12T18:41:13.356337Z","shell.execute_reply.started":"2022-02-12T18:41:13.311104Z","shell.execute_reply":"2022-02-12T18:41:13.355488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DIFF_SPECIES = le.classes_","metadata":{"execution":{"iopub.status.busy":"2022-02-12T18:41:13.358214Z","iopub.execute_input":"2022-02-12T18:41:13.359809Z","iopub.status.idle":"2022-02-12T18:41:13.36406Z","shell.execute_reply.started":"2022-02-12T18:41:13.359766Z","shell.execute_reply":"2022-02-12T18:41:13.363035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train.image_id = train.image_id.apply(lambda x: f\"./train_images/{x}\")","metadata":{"execution":{"iopub.status.busy":"2022-02-12T18:41:13.365249Z","iopub.execute_input":"2022-02-12T18:41:13.365472Z","iopub.status.idle":"2022-02-12T18:41:13.375192Z","shell.execute_reply.started":"2022-02-12T18:41:13.365446Z","shell.execute_reply":"2022-02-12T18:41:13.374317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# run = wandb.init(project = \"hyperwhaletpu\",name = \"pytorchlightning\")\nBATCH_SIZE = 8\nss = pd.read_csv(\"../input/happy-whale-and-dolphin/sample_submission.csv\")\ntrain['image_id'] = train[\"image_id\"].apply(lambda x: f\"./train_images/{x}\")\nss['image'] = ss['image'].apply(lambda x : f\"./test_images/{x}\")\n\nlandmark_id2idx = {landmark_id: idx for idx, landmark_id in enumerate(sorted(train['individual_id'].unique()))}\nidx2landmark_id = {idx: landmark_id for idx, landmark_id in enumerate(sorted(train['individual_id'].unique()))}\ntrain['individual_id'] = train['individual_id'].map(landmark_id2idx)\n\n# train = train.sample(frac = 0.001)\ntrain,val = train_test_split(train)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T18:41:13.376367Z","iopub.execute_input":"2022-02-12T18:41:13.376607Z","iopub.status.idle":"2022-02-12T18:41:13.573721Z","shell.execute_reply.started":"2022-02-12T18:41:13.376582Z","shell.execute_reply":"2022-02-12T18:41:13.572818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_CLASSES= len(landmark_id2idx)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T18:41:13.575156Z","iopub.execute_input":"2022-02-12T18:41:13.576109Z","iopub.status.idle":"2022-02-12T18:41:13.58063Z","shell.execute_reply.started":"2022-02-12T18:41:13.576065Z","shell.execute_reply":"2022-02-12T18:41:13.579832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# transforms_224= albumentations.Compose([\n#     albumentations.Resize(224,224),\n#     albumentations.Normalize()\n# ])\n\n# transforms_768 = albumentations.Compose([\n#     albumentations.Resize(768, 768),\n#     albumentations.Normalize()  \n# ])\ntransforms_512 = albumentations.Compose([\n    albumentations.Resize(512, 512),\n#     albumentations.Normalize()\n])","metadata":{"execution":{"iopub.status.busy":"2022-02-12T18:41:13.584112Z","iopub.execute_input":"2022-02-12T18:41:13.584404Z","iopub.status.idle":"2022-02-12T18:41:13.591506Z","shell.execute_reply.started":"2022-02-12T18:41:13.584356Z","shell.execute_reply":"2022-02-12T18:41:13.59096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating the pytroch model AlexModel \nclass MyDataset(torch.utils.data.Dataset):\n    def __init__(self, file_paths , labels , transforms, species= None, is_test = False):\n        # Not sure if I am gonnaa use species but let's see\n        self.species = species\n        self.file_paths = file_paths \n        self.labels = labels \n        self.transforms = transforms\n        self.is_test = is_test \n    \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self,idx):\n        lab = self.labels[idx]\n#         image = plt.imread(self.file_paths[idx])\n#         if len(image.shape) == 2:\n#             image = np.expand_dims(image, axis =2)\n#             image = np.concatenate((image, image, image), axis=2)\n        image = cv2.imread(self.file_paths[idx])\n        image = image[:, :, ::-1]\n        if self.transforms != None:\n            res = self.transforms(image=image)\n            image = res['image'].astype(np.float32)\n            \n        image = image.transpose(2, 0, 1) \n        image = torch.tensor(image)\n        if not self.is_test:\n            return {'image':image, \"label\" :lab, \"spe\" : self.species[idx]}\n        else:\n            return {\"image\":image, \"label\":lab}","metadata":{"execution":{"iopub.status.busy":"2022-02-12T18:41:13.593484Z","iopub.execute_input":"2022-02-12T18:41:13.593957Z","iopub.status.idle":"2022-02-12T18:41:13.603387Z","shell.execute_reply.started":"2022-02-12T18:41:13.593924Z","shell.execute_reply":"2022-02-12T18:41:13.602743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.nn import Parameter\nclass ArcMarginProduct(nn.Module):\n    r\"\"\"\n    credits = https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/blob/master/src/modeling/metric_learning.py\n    Implement of large margin arc distance: :\n        Args:\n            in_features: size of each input sample\n            out_features: size of each output sample\n            s: norm of input feature\n            m: margin\n            cos(theta + m)\n        \"\"\"\n    def __init__(self, in_features, out_features, s=30.0, m=0.50, easy_margin=False, ls_eps=0.0):\n        super(ArcMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.s = s\n        self.m = m\n        self.ls_eps = ls_eps  # label smoothing\n        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(m)\n        self.sin_m = math.sin(m)\n        self.th = math.cos(math.pi - m)\n        self.mm = math.sin(math.pi - m) * m\n\n    def forward(self, input, label):\n       \n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n        # --------------------------- convert label to one-hot ---------------------------\n        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n        one_hot = torch.zeros(cosine.size())\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.s\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-02-12T18:41:13.604746Z","iopub.execute_input":"2022-02-12T18:41:13.604964Z","iopub.status.idle":"2022-02-12T18:41:13.6222Z","shell.execute_reply.started":"2022-02-12T18:41:13.604938Z","shell.execute_reply":"2022-02-12T18:41:13.621263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# # train_ds = MyDataset(file_paths = train.image, labels = train.individual_id, transforms = transforms_224)\n# # train_dl = DataLoader(train_ds, batch_size=10, num_workers = 4 , pin_memory = True, drop_last = True)\n\n# # val_ds = MyDataset(file_paths = val.image, labels = val.individual_id, transforms = transforms_224)\n# # val_dl = DataLoader(val_ds,batch_size = 10, num_workers = 4 , pin_memory = True, drop_last = False)\n\n# class AlexModel(pl.LightningModule):\n#     def __init__(self):\n#         super(AlexModel,self).__init__()\n#         self.backbone = timm.create_model(\"efficientnet_b6\",pretrained = True) \n#         self.linear = nn.Linear(self.backbone.classifier.out_features, 512)\n#         self.softmax = nn.Softmax(dim = 0)\n#         self.batch_size = BATCH_SIZE# 128 ## if TPU\n#         self.arc = ArcMarginProduct( in_features = 512,\n#                               out_feature s =15587 , s = 30, m = 0.3  )\n    \n#     def forward(self, input_image):\n#         x = self.backbone(input_image)\n#         x = self.linear(x)\n#         return x\n    \n#     def training_step(self,batch,batch_idx):\n#         data = batch[\"image\"]\n#         target = batch[\"label\"]\n#         spe = batch['spe']\n#         logits_m = self(data)\n#         lo = self.arc(logits_m,torch.tensor(spe)) \n#         lo = self.softmax(lo)\n#         loss = nn.CrossEntropyLoss()(lo,torch.tensor(target, dtype = torch.long))\n#         return {\"loss\":loss}\n    \n#     def validation_step(self,batch,batch_idx):\n#         data = batch[\"image\"]\n#         target = batch[\"label\"]\n#         spe = batch['spe']\n#         logits_m = self(data)\n#         lo = self.arc(logits_m,torch.tensor(spe)) \n#         lo = self.softmax(lo)\n#         loss = nn.CrossEntropyLoss()(lo,torch.tensor(target, dtype = torch.long))\n#         self.log('val_loss', loss)\n#         return {\"val_loss\":loss, \"pred\":logits_m}\n    \n#     def validation_epoch_end(self, outputs):\n#         avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n#         return {'avg_val_loss': avg_loss}\n    \n#     def configure_optimizers(self):\n#         return torch.optim.Adam(self.parameters(), lr=0.02)\n\n#     def train_dataloader(self):\n#         das = MyDataset(file_paths= train.image_id.to_numpy(), labels = train.individual_id.to_numpy(), transforms = transforms_224, species = train.species.to_numpy(), is_test = False)\n#         dl = DataLoader(das, batch_size = self.batch_size, num_workers= 4 )\n#         return dl\n\n#     def val_dataloader(self):\n#         dataset = MyDataset(file_paths = val.image_id.to_numpy(), labels = val.individual_id.to_numpy(), transforms = transforms_224, species = val.species.to_numpy(), is_test = False)\n#         loader = DataLoader(dataset, batch_size=self.batch_size, num_workers= 4 )\n#         return loader\n\nclass ArcMarginProduct(nn.Module):\n    r\"\"\"\n    credits = https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/blob/master/src/modeling/metric_learning.py\n    Implement of large margin arc distance: :\n        Args:\n            in_features: size of each input sample\n            out_features: size of each output sample\n            s: norm of input feature\n            m: margin\n            cos(theta + m)\n        \"\"\"\n    def __init__(self, in_features, out_features, s=30.0, m=0.50, easy_margin=False, ls_eps=0.0):\n        super(ArcMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.s = s\n        self.m = m\n        self.ls_eps = ls_eps  # label smoothing\n        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(m)\n        self.sin_m = math.sin(m)\n        self.th = math.cos(math.pi - m)\n        self.mm = math.sin(math.pi - m) * m\n\n    def forward(self, input, label):\n       \n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n        # --------------------------- convert label to one-hot ---------------------------\n        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n        one_hot = torch.zeros(cosine.size())\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.s\n\n        return output\n\n\nclass Base_mod(torch.nn.Module):\n    def __init__(self):\n        super(Base_mod,self).__init__()\n        self.backbone = timm.create_model(\"t\",pretrained = True) \n        self.linear = nn.Linear(self.backbone.classifier.out_features, 512)\n        self.softmax = nn.Softmax(dim = 0)\n    \n    def forward(self, input_image):\n        x = self.backbone(input_image)\n        x = self.linear(x)\n        return x\n\nclass AlexMod(pl.LightningModule):\n    def __init__(self, base_mod, arc_margin):\n        super(AlexMod, self).__init__()\n        self.base = base_mod\n        self.arc_face =  arc_margin\n        self.softmax = nn.Softmax(dim = 0)\n        self.batch_size = BATCH_SIZE\n    def forward(self, input_image, spe):\n        x = self.base(input_image)\n        x = self.arc_face(x, spe)\n        return self.softmax(x)\n\n    \n    def training_step(self,batch,batch_idx):\n        data = batch[\"image\"]\n        target = batch[\"label\"]\n        spe = batch['spe']\n        logits_m = self(data, spe)\n        loss = nn.CrossEntropyLoss()(logits_m,torch.tensor(target, dtype = torch.long))\n        return {\"loss\":loss}\n    \n    def validation_step(self,batch,batch_idx):\n        data = batch[\"image\"]\n        target = batch[\"label\"]\n        spe = batch['spe']\n        logits_m = self(data, spe )\n        loss = nn.CrossEntropyLoss()(logits_m,torch.tensor(target, dtype = torch.long))\n        return {\"loss\":loss}\n        self.log('val_loss', loss)\n        return {\"val_loss\":loss, \"pred\":logits_m}\n    \n    def validation_epoch_end(self, outputs):\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n        return {'avg_val_loss': avg_loss}\n    \n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=0.02)\n\n    def train_dataloader(self):\n        das = MyDataset(file_paths= train.image_id.to_numpy(), labels = train.individual_id.to_numpy(), transforms = transforms_512, species = train.species.to_numpy(), is_test = False)\n        dl = DataLoader(das, batch_size = self.batch_size, num_workers= 4 )\n        return dl\n\n    def val_dataloader(self):\n        dataset = MyDataset(file_paths = val.image_id.to_numpy(), labels = val.individual_id.to_numpy(), transforms = transforms_512, species = val.species.to_numpy(), is_test = False)\n        loader = DataLoader(dataset, batch_size=self.batch_size, num_workers= 4 )\n        return loader\n\nembed_model = Base_mod()\narc_face = ArcMarginProduct( in_features = 512,\n                              out_features =15587 , s = 30, m = 0.3  )\nmodel = AlexMod(embed_model , arc_face)       \n    ","metadata":{"execution":{"iopub.status.busy":"2022-02-12T18:41:13.623874Z","iopub.execute_input":"2022-02-12T18:41:13.624432Z","iopub.status.idle":"2022-02-12T18:41:14.563284Z","shell.execute_reply.started":"2022-02-12T18:41:13.624372Z","shell.execute_reply":"2022-02-12T18:41:14.562571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pytorch_lightning.loggers import WandbLogger  # newline 1\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\nfrom pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\nes = EarlyStopping(monitor=\"val_loss\")\ncheckpointing = ModelCheckpoint(monitor=\"val_loss\")\ntrainer = pl.Trainer(callbacks=[es,checkpointing],progress_bar_refresh_rate=20 ,gpus = 1  ,  max_epochs = 10)#, distributed_backend='ddp')\nif __name__ == '__main__':\n    trainer.fit(model)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T18:41:14.564762Z","iopub.execute_input":"2022-02-12T18:41:14.565015Z","iopub.status.idle":"2022-02-12T18:42:15.13117Z","shell.execute_reply.started":"2022-02-12T18:41:14.564978Z","shell.execute_reply":"2022-02-12T18:42:15.127907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(embed_model.state_dict(), \"./embed_model.pt\")","metadata":{"execution":{"iopub.status.busy":"2022-02-12T18:32:36.677723Z","iopub.status.idle":"2022-02-12T18:32:36.678264Z","shell.execute_reply.started":"2022-02-12T18:32:36.677967Z","shell.execute_reply":"2022-02-12T18:32:36.677991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(arc_face.state_dict(), \"./arc_face.pt\")","metadata":{"execution":{"iopub.status.busy":"2022-02-12T18:32:36.679375Z","iopub.status.idle":"2022-02-12T18:32:36.679875Z","shell.execute_reply.started":"2022-02-12T18:32:36.679609Z","shell.execute_reply":"2022-02-12T18:32:36.679633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(),\"./AlexModelHap.pt\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# train_targets = []\n# train_embeddings = []\n# for x in train_dl:\n#     embeddings = mod(x['image'])\n#     targets = x['label']\n#     train_embeddings.append(embeddings)\n#     train_targets.append(targets)\n# train_embeddings = np.concatenate(train_embeddigns)\n# train_targets = np.concatenate(train_targets )","metadata":{"execution":{"iopub.status.busy":"2022-02-11T16:52:32.342771Z","iopub.execute_input":"2022-02-11T16:52:32.343175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# torch.save(mod.state_dict(),\"./AlexModelHap.pt\")","metadata":{"execution":{"iopub.status.busy":"2022-02-11T14:17:23.43986Z","iopub.execute_input":"2022-02-11T14:17:23.440718Z","iopub.status.idle":"2022-02-11T14:17:23.445115Z","shell.execute_reply.started":"2022-02-11T14:17:23.440654Z","shell.execute_reply":"2022-02-11T14:17:23.443912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# get distance vector for train images ","metadata":{}},{"cell_type":"code","source":"# train_dl = mod.train_dataloader()","metadata":{"execution":{"iopub.status.busy":"2022-02-11T16:48:18.720368Z","iopub.execute_input":"2022-02-11T16:48:18.72087Z","iopub.status.idle":"2022-02-11T16:48:18.72527Z","shell.execute_reply.started":"2022-02-11T16:48:18.720819Z","shell.execute_reply":"2022-02-11T16:48:18.72462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for x in train_dl:\n#     print(x)\n#     break","metadata":{"execution":{"iopub.status.busy":"2022-02-11T16:48:29.171949Z","iopub.execute_input":"2022-02-11T16:48:29.172534Z","iopub.status.idle":"2022-02-11T16:48:29.742233Z","shell.execute_reply.started":"2022-02-11T16:48:29.172484Z","shell.execute_reply":"2022-02-11T16:48:29.740677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # creating the pytroch model AlexModel \n# class test_dataset(torch.utils.data.Dataset):\n#     def __init__(self, file_paths,transforms  ):\n#         # Not sure if I am gonnaa use species but let's see\n#         self.file_paths = file_paths \n#         self.transforms = transforms\n    \n#     def __len__(self):\n#         return len(self.file_paths)\n    \n#     def __getitem__(self,idx):\n#         image = plt.imread(self.file_paths[idx])\n#         if len(image.shape) == 2:\n#             image = np.expand_dims(image, axis =2)\n#             image = np.concatenate((image, image, image), axis=2)        \n#         if self.transforms != None:\n#             res = self.transforms(image=image)\n#             image = res['image'].astype(np.float32)\n#         image = torch.tensor(image).permute(2,0,1)\n#         return {\"image\":image}","metadata":{"execution":{"iopub.status.busy":"2022-02-11T16:16:50.435236Z","iopub.execute_input":"2022-02-11T16:16:50.435947Z","iopub.status.idle":"2022-02-11T16:16:50.448295Z","shell.execute_reply.started":"2022-02-11T16:16:50.435896Z","shell.execute_reply":"2022-02-11T16:16:50.447393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_ds = test_dataset(test.image_path, transforms = transforms_224)\n# test_dl = DataLoader(test_ds , batch_size = BATCH_SIZE,num_workers = 4  )","metadata":{"execution":{"iopub.status.busy":"2022-02-11T16:16:50.757799Z","iopub.execute_input":"2022-02-11T16:16:50.758117Z","iopub.status.idle":"2022-02-11T16:16:50.764427Z","shell.execute_reply.started":"2022-02-11T16:16:50.758082Z","shell.execute_reply":"2022-02-11T16:16:50.763409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# embeddings = mod(data['image'])","metadata":{"execution":{"iopub.status.busy":"2022-02-11T16:17:10.748509Z","iopub.execute_input":"2022-02-11T16:17:10.748951Z","iopub.status.idle":"2022-02-11T16:17:16.40858Z","shell.execute_reply.started":"2022-02-11T16:17:10.748921Z","shell.execute_reply":"2022-02-11T16:17:16.407735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# embeddings.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-11T16:17:17.816814Z","iopub.execute_input":"2022-02-11T16:17:17.817652Z","iopub.status.idle":"2022-02-11T16:17:17.823241Z","shell.execute_reply.started":"2022-02-11T16:17:17.817603Z","shell.execute_reply":"2022-02-11T16:17:17.822606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # from sklearn.neighbors import NearestNeighbors\n# neigh = NearestNeighbors(n_neighbors=5,metric='cosine')\n# neigh.fit(embeddings.detach().numpy())\n","metadata":{"execution":{"iopub.status.busy":"2022-02-11T16:31:48.264441Z","iopub.execute_input":"2022-02-11T16:31:48.264801Z","iopub.status.idle":"2022-02-11T16:31:48.273764Z","shell.execute_reply.started":"2022-02-11T16:31:48.264764Z","shell.execute_reply":"2022-02-11T16:31:48.272526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# neigh.kneighbors(np.array([embeddings.detach().numpy()[0]]), n_neighbors=5, return_distance=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T16:32:06.270771Z","iopub.execute_input":"2022-02-11T16:32:06.271734Z","iopub.status.idle":"2022-02-11T16:32:06.280905Z","shell.execute_reply.started":"2022-02-11T16:32:06.271692Z","shell.execute_reply":"2022-02-11T16:32:06.280242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_ids = []\n# test_nn_distances = []\n# test_nn_idxs = []\n# for filename in tqdm(test_files):\n#     embeddings = get_embeddings(filename)\n#     ids = get_ids(filename)\n#     distances,idxs = neigh.kneighbors(embeddings, config.KNN, return_distance=True)\n#     test_ids.append(ids)\n#     test_nn_idxs.append(idxs)\n#     test_nn_distances.append(distances)\n# test_nn_distances = np.concatenate(test_nn_distances)\n# test_nn_idxs = np.concatenate(test_nn_idxs)\n# test_ids = np.concatenate(test_ids)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T16:26:27.976158Z","iopub.execute_input":"2022-02-11T16:26:27.976489Z","iopub.status.idle":"2022-02-11T16:26:27.98227Z","shell.execute_reply.started":"2022-02-11T16:26:27.976451Z","shell.execute_reply":"2022-02-11T16:26:27.981639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample_submission = pd.read_csv('../input/happy-whale-and-dolphin/sample_submission.csv',index_col='image')\n# print(len(test_ids),len(sample_submission))\n# test_df = []\n# for i in tqdm(range(len(test_ids))):\n#     id_ = test_ids[i]\n#     targets = train_targets[test_nn_idxs[i]]\n#     distances = test_nn_distances[i]\n#     subset_preds = pd.DataFrame(np.stack([targets,distances],axis=1),columns=['target','distances'])\n#     subset_preds['image'] = id_\n#     test_df.append(subset_preds)\n# test_df = pd.concat(test_df).reset_index(drop=True)\n# test_df['confidence'] = 1-test_df['distances']\n# test_df = test_df.groupby(['image','target']).confidence.max().reset_index()\n# test_df = test_df.sort_values('confidence',ascending=False).reset_index(drop=True)\n# test_df['target'] = test_df['target'].map(target_encodings)\n# test_df.to_csv('test_neighbors.csv')\n# test_df.image.value_counts().value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predictions = {}\n# for i,row in tqdm(test_df.iterrows()):\n#     if row.image in predictions:\n#         if len(predictions[row.image])==5:\n#             continue\n#         predictions[row.image].append(row.target)\n#     elif row.confidence>best_threshold_adjusted:\n#         predictions[row.image] = [row.target,'new_individual']\n#     else:\n#         predictions[row.image] = ['new_individual',row.target]\n        \n# for x in tqdm(predictions):\n#     if len(predictions[x])<5:\n#         remaining = [y for y in sample_list if y not in predictions]\n#         predictions[x] = predictions[x]+remaining\n#         predictions[x] = predictions[x][:5]\n#     predictions[x] = ' '.join(predictions[x])\n    \n# predictions = pd.Series(predictions).reset_index()\n# predictions.columns = ['image','predictions']\n# predictions.to_csv('submission.csv',index=False)\n# predictions.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.spy(neigh.kneighbors_graph(embeddings.detach().numpy(),n_neighbors=5))\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-11T16:40:53.240735Z","iopub.execute_input":"2022-02-11T16:40:53.241996Z","iopub.status.idle":"2022-02-11T16:40:53.542873Z","shell.execute_reply.started":"2022-02-11T16:40:53.241933Z","shell.execute_reply":"2022-02-11T16:40:53.541697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-02-11T16:40:27.345943Z","iopub.execute_input":"2022-02-11T16:40:27.34628Z","iopub.status.idle":"2022-02-11T16:40:27.351509Z","shell.execute_reply.started":"2022-02-11T16:40:27.346249Z","shell.execute_reply":"2022-02-11T16:40:27.350818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    ","metadata":{},"execution_count":null,"outputs":[]}]}