{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<br>\n\n<h2 style=\"text-align: center; font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: underline; text-transform: none; letter-spacing: 2px; color: #3eb489; background-color: #ffffff;\">Happy Whale and Dolphin <span style=\"font-size: 16px;\">(üòäüê≥&üê¨)</span> - EDA & Baseline</h2>\n<h5 style=\"text-align: center; font-family: Verdana; font-size: 12px; font-style: normal; font-weight: bold; text-decoration: None; text-transform: none; letter-spacing: 1px; color: black; background-color: #ffffff;\">CREATED BY: DARIEN SCHETTLER</h5>\n\n<br>\n\n---\n\n<br>\n\n<center><div class=\"alert alert-block alert-danger\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 18px;\">üõë &nbsp; WARNING:</b><br><br><b>THIS IS A WORK IN PROGRESS</b><br>\n</div></center>\n\n\n<center><div class=\"alert alert-block alert-warning\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 18px;\">üëè &nbsp; IF YOU FORK THIS OR FIND THIS HELPFUL &nbsp; üëè</b><br><br><b style=\"font-size: 22px; color: darkorange\">PLEASE UPVOTE!</b><br><br>This was a lot of work for me and while it may seem silly, it makes me feel appreciated when others like my work. üòÖ\n</div></center>\n\n\n","metadata":{"papermill":{"duration":0.120561,"end_time":"2021-11-06T21:15:09.611563","exception":false,"start_time":"2021-11-06T21:15:09.491002","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<p id=\"toc\"></p>\n\n<br><br>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: #3eb489; background-color: #ffffff;\">TABLE OF CONTENTS</h1>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#imports\">0&nbsp;&nbsp;&nbsp;&nbsp;IMPORTS</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#background_information\">1&nbsp;&nbsp;&nbsp;&nbsp;BACKGROUND INFORMATION</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#setup\">2&nbsp;&nbsp;&nbsp;&nbsp;SETUP</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#helper_functions\">3&nbsp;&nbsp;&nbsp;&nbsp;HELPER FUNCTIONS</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#dataset_exploration\">4&nbsp;&nbsp;&nbsp;&nbsp;DATASET EXPLORATION</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#model_baseline\">5&nbsp;&nbsp;&nbsp;&nbsp;BASELINE</a></h3>\n\n---","metadata":{"papermill":{"duration":0.085591,"end_time":"2021-11-06T21:15:09.78303","exception":false,"start_time":"2021-11-06T21:15:09.697439","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<br>\n\n<a id=\"imports\"></a>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: #3eb489;\" id=\"imports\">0&nbsp;&nbsp;IMPORTS&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a></h1>","metadata":{"papermill":{"duration":0.050527,"end_time":"2021-11-06T21:15:09.894476","exception":false,"start_time":"2021-11-06T21:15:09.843949","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print(\"\\n... IMPORTS STARTING ...\\n\")\n\nprint(\"\\n... PIP/APT INSTALLS AND DOWNLOADS/ZIP STARTING ...\")\n!pip install -q kaleido\nprint(\"... PIP/APT INSTALLS COMPLETE ...\\n\")\n\nprint(\"\\n\\tVERSION INFORMATION\")\n# Machine Learning and Data Science Imports\nimport tensorflow as tf; print(f\"\\t\\t‚Äì TENSORFLOW VERSION: {tf.__version__}\");\nimport tensorflow_addons as tfa; print(f\"\\t\\t‚Äì TENSORFLOW ADDONS VERSION: {tfa.__version__}\");\nimport pandas as pd; pd.options.mode.chained_assignment = None;\nimport numpy as np; print(f\"\\t\\t‚Äì NUMPY VERSION: {np.__version__}\");\nimport sklearn; print(f\"\\t\\t‚Äì SKLEARN VERSION: {sklearn.__version__}\");\nfrom sklearn.preprocessing import RobustScaler, PolynomialFeatures\nfrom pandarallel import pandarallel; pandarallel.initialize();\nfrom sklearn.model_selection import GroupKFold, StratifiedKFold\n\n# RAPIDS\nimport cudf, cupy, cuml\nfrom cuml.neighbors import NearestNeighbors\nfrom cuml.manifold import TSNE, UMAP\n\n# Built In Imports\nfrom kaggle_datasets import KaggleDatasets\nfrom collections import Counter\nfrom datetime import datetime\nfrom glob import glob\nimport warnings\nimport requests\nimport hashlib\nimport imageio\nimport IPython\nimport sklearn\nimport urllib\nimport zipfile\nimport pickle\nimport random\nimport shutil\nimport string\nimport json\nimport math\nimport time\nimport gzip\nimport ast\nimport sys\nimport io\nimport os\nimport gc\nimport re\n\n# Visualization Imports\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.patches as patches\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm; tqdm.pandas();\nimport plotly.express as px\nimport seaborn as sns\nfrom PIL import Image, ImageEnhance\nimport matplotlib; print(f\"\\t\\t‚Äì MATPLOTLIB VERSION: {matplotlib.__version__}\");\nfrom matplotlib import animation, rc; rc('animation', html='jshtml')\nimport plotly\nimport PIL\nimport cv2\n\nimport plotly.io as pio\nprint(pio.renderers)\n\ndef seed_it_all(seed=7):\n    \"\"\" Attempt to be Reproducible \"\"\"\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\n    \nprint(\"\\n\\n... IMPORTS COMPLETE ...\\n\")","metadata":{"papermill":{"duration":162.144149,"end_time":"2021-11-06T21:17:52.087371","exception":false,"start_time":"2021-11-06T21:15:09.943222","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-23T13:34:49.57271Z","iopub.execute_input":"2022-02-23T13:34:49.573336Z","iopub.status.idle":"2022-02-23T13:35:20.304896Z","shell.execute_reply.started":"2022-02-23T13:34:49.573244Z","shell.execute_reply":"2022-02-23T13:35:20.303396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<a id=\"background_information\"></a>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: #3eb489; background-color: #ffffff;\" id=\"background_information\">1&nbsp;&nbsp;BACKGROUND INFORMATION&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a></h1>\n\n---\n","metadata":{"papermill":{"duration":0.05019,"end_time":"2021-11-06T21:17:52.231372","exception":false,"start_time":"2021-11-06T21:17:52.181182","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #3eb489; background-color: #ffffff;\">1.1 BASIC COMPETITION INFORMATION</h3>\n\n---\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">PRIMARY TASK DESCRIPTION</b>\n\nIn this competition, you‚Äôll develop a model to **match** individual whales and dolphins by **unique‚Äîbut often subtle‚Äîcharacteristics** of their natural markings. \n\nYou'll pay particular attention to **dorsal fins** and **lateral body views** in image sets from a **multi-species dataset** built by 28 research institutions.\n\n---\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">CONTEXT</b>\n\nWe use fingerprints and facial recognition to identify people, but can we use similar approaches with animals? In fact, researchers manually track marine life by the shape and markings on their tails, dorsal fins, heads and other body parts. Identification by natural markings via photographs‚Äîknown as photo-ID‚Äîis a powerful tool for marine mammal science. It allows individual animals to be tracked over time and enables assessments of population status and trends. With your help to automate whale and dolphin photo-ID, researchers can reduce image identification times by over 99%. More efficient identification could enable a scale of study previously unaffordable or impossible.\n\nCurrently, most research institutions rely on time-intensive‚Äîand sometimes inaccurate‚Äîmanual matching by the human eye. Thousands of hours go into manual matching, which involves staring at photos to compare one individual to another, finding matches, and identifying new individuals. While researchers enjoy looking at a whale photo or two, manual matching limits the scope and reach.\n\n\n---\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">MORE BACKGROUND INFORMATION</b>\n\n\nTBD","metadata":{"papermill":{"duration":0.053372,"end_time":"2021-11-06T21:17:52.337029","exception":false,"start_time":"2021-11-06T21:17:52.283657","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #3eb489; background-color: #ffffff;\">1.2 COMPETITION EVALUATION</h3>\n\n---\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">GENERAL EVALUATION INFORMATION</b>\n\n**Submissions are evaluated according to the Mean Average Precision @ 5 (MAP@5)**\n\nLATEX TO BE INSERTED\n\n*where ùëà is the number of images, ùëÉ(ùëò) is the precision at cutoff ùëò, ùëõ is the number predictions per image, and ùëüùëíùëô(ùëò) is an indicator function equaling 1 if the item at rank ùëò is a relevant (correct) label, zero otherwise.*\n\nOnce a correct label has been scored for an observation, that label is no longer considered relevant for that observation, and additional predictions of that label are skipped in the calculation. For example, if the correct label is A for an observation, the following predictions all score an average precision of 1.0.\n\n```\n[A, B, C, D, E]\n[A, A, A, A, A]\n[A, B, A, C, A]\n```\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">SUBMISSION FORMAT</b>\n\nFor each image in the test set, you may predict up to 5 **`individual_id`** labels. There are individuals in the test set that are not seen in the training data; these should be predicted as new_individual. The file should contain a header and have the following format:\n\n```\nimage,predictions \n000188a72f2562.jpg,37c7aba965a5 114207cab555 a6e325d8e924 19fbb960f07d new_individual \n000ba09273d6f3.jpg,37c7aba965a5 114207cab555 a6e325d8e924 19fbb960f07d new_individual \n...\n``` \n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase; color: red;\">IS THIS A CODE COMPETITION?</b>\n\n<font style=\"color:red; font-weight: bold; font-size: 20px;\">NO!</font>\n","metadata":{"papermill":{"duration":0.052259,"end_time":"2021-11-06T21:17:52.443218","exception":false,"start_time":"2021-11-06T21:17:52.390959","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #3eb489; background-color: #ffffff;\">1.3 DATASET OVERVIEW</h3>\n\n---\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">GENERAL INFORMATION</b>\n\nIn the [**previous HappyWhale competition**](https://www.kaggle.com/c/humpback-whale-identification), the task was to predict individual humpback whales from images of their flukes. Whales and dolphins in this dataset can be identified by shapes, features and markings (some natural, some acquired) of dorsal fins, backs, heads and flanks. Some species and some individuals have highly distinct features, others are very much less distinct. Further, individual features may change over time. \n\nThis competition expands that task significantly: \n* data in this competition contains images of over **15,000 unique individual marine mammals from 30 different species collected from 28 different research organizations**. \n* Individuals have been manually identified and given an **`individual_id`** by marine researches\n\nYour task is to correctly identify these individuals in the images. It's a challenging task that has the potential to drive significant advancements in understanding and protecting marine mammals across the globe.\n\nAn important note about data quality: \n* Bringing together this dataset from many different research organization posed a number of practical challenges. \n* Significant effort has been made to minimize data quality issues and as well as to minimize leakage as much as possible.\n* There are undoubtably issues. \n* We encourage the community to report these things so that future versions of the data can be improved, but unless there is a significant issue, we don't expect to make updates to the data during the competition.\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">DISCOVERED  INFORMATION [TENTATIVE]</b>\n\nTBD","metadata":{"papermill":{"duration":0.052579,"end_time":"2021-11-06T21:17:52.548406","exception":false,"start_time":"2021-11-06T21:17:52.495827","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<br>\n\n<a id=\"background_information\"></a>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: #3eb489; background-color: #ffffff;\" id=\"setup\">2&nbsp;&nbsp;SETUP&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a></h1>\n\n---\n","metadata":{"papermill":{"duration":0.052944,"end_time":"2021-11-06T21:17:52.65284","exception":false,"start_time":"2021-11-06T21:17:52.599896","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #3eb489; background-color: #ffffff;\">2.1 ACCELERATOR DETECTION</h3>\n\n---\n\nIn order to use **`TPU`**, we use **`TPUClusterResolver`** for the initialization which is necessary to connect to the remote cluster and initialize cloud TPUs. Let's go over two important points\n\n1. When using TPU on Kaggle, you don't need to specify arguments for **`TPUClusterResolver`**\n2. However, on **G**oogle **C**ompute **E**ngine (**GCE**), you will need to do the following:\n\n<br>\n\n```python\n# The name you gave to the TPU to use\nTPU_WORKER = 'my-tpu-name'\n\n# or you can also specify the grpc path directly\n# TPU_WORKER = 'grpc://xxx.xxx.xxx.xxx:8470'\n\n# The zone you chose when you created the TPU to use on GCP.\nZONE = 'us-east1-b'\n\n# The name of the GCP project where you created the TPU to use on GCP.\nPROJECT = 'my-tpu-project'\n\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_WORKER, zone=ZONE, project=PROJECT)\n```\n\n<div class=\"alert alert-block alert-danger\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 16px;\">üõë &nbsp; WARNING:</b><br><br>- Although the Tensorflow documentation says it is the <b>project name</b> that should be provided for the argument <b><code>`project`</code></b>, it is actually the <b>Project ID</b>, that you should provide. This can be found on the GCP project dashboard page.<br>\n</div>\n\n<div class=\"alert alert-block alert-info\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 16px;\">üìñ &nbsp; REFERENCES:</b><br><br>\n    - <a href=\"https://www.tensorflow.org/guide/tpu#tpu_initialization\"><b>Guide - Use TPUs</b></a><br>\n    - <a href=\"https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/TPUClusterResolver\"><b>Doc - TPUClusterResolver</b></a><br>\n\n</div>","metadata":{"papermill":{"duration":0.053821,"end_time":"2021-11-06T21:17:52.761303","exception":false,"start_time":"2021-11-06T21:17:52.707482","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print(f\"\\n... ACCELERATOR SETUP STARTING ...\\n\")\n\n# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()  \nexcept ValueError:\n    TPU = None\n\nif TPU:\n    print(f\"\\n... RUNNING ON TPU - {TPU.master()}...\")\n    tf.config.experimental_connect_to_cluster(TPU)\n    tf.tpu.experimental.initialize_tpu_system(TPU)\n    strategy = tf.distribute.experimental.TPUStrategy(TPU)\nelse:\n    print(f\"\\n... RUNNING ON CPU/GPU ...\")\n    # Yield the default distribution strategy in Tensorflow\n    #   --> Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy() \n\n# What Is a Replica?\n#    --> A single Cloud TPU device consists of FOUR chips, each of which has TWO TPU cores. \n#    --> Therefore, for efficient utilization of Cloud TPU, a program should make use of each of the EIGHT (4x2) cores. \n#    --> Each replica is essentially a copy of the training graph that is run on each core and \n#        trains a mini-batch containing 1/8th of the overall batch size\nN_REPLICAS = strategy.num_replicas_in_sync\n    \nprint(f\"... # OF REPLICAS: {N_REPLICAS} ...\\n\")\n\nprint(f\"\\n... ACCELERATOR SETUP COMPLTED ...\\n\")","metadata":{"papermill":{"duration":0.07574,"end_time":"2021-11-06T21:17:52.892074","exception":false,"start_time":"2021-11-06T21:17:52.816334","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-23T13:35:20.307691Z","iopub.execute_input":"2022-02-23T13:35:20.308744Z","iopub.status.idle":"2022-02-23T13:35:20.3305Z","shell.execute_reply.started":"2022-02-23T13:35:20.308692Z","shell.execute_reply":"2022-02-23T13:35:20.329513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #3eb489; background-color: #ffffff;\">2.2 COMPETITION DATA ACCESS</h3>\n\n---\n\nTPUs read data must be read directly from **G**oogle **C**loud **S**torage **(GCS)**. Kaggle provides a utility library ‚Äì¬†**`KaggleDatasets`** ‚Äì which has a utility function **`.get_gcs_path`** that will allow us to access the location of our input datasets within **GCS**.<br><br>\n\n<div class=\"alert alert-block alert-info\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 16px;\">üìå &nbsp; TIPS:</b><br><br>- If you have multiple datasets attached to the notebook, you should pass the name of a specific dataset to the <b><code>`get_gcs_path()`</code></b> function. <i>In our case, the name of the dataset is the name of the directory the dataset is mounted within.</i><br><br>\n</div>","metadata":{"papermill":{"duration":0.053551,"end_time":"2021-11-06T21:17:52.999073","exception":false,"start_time":"2021-11-06T21:17:52.945522","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print(\"\\n... DATA ACCESS SETUP STARTED ...\\n\")\n\nif TPU:\n    # Google Cloud Dataset path to training and validation images\n    DATA_DIR = KaggleDatasets().get_gcs_path('happy-whale-and-dolphin')\n    save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n    load_locally = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\nelse:\n    # Local path to training and validation images\n    DATA_DIR = \"/kaggle/input/happy-whale-and-dolphin\"\n    save_locally = None\n    load_locally = None\n\nEXTRA_DATA_DIR = \"/kaggle/input/extra-happywhale-metadata\"\nEMBED_384_DIR = \"/kaggle/input/happywhale-384x660-embeddings\"\nMODEL_384_DIR = \"../input/happywhale-384x660-models\"\nEMBED_512_DIR = \"/kaggle/input/happywhale-512x880-embeddings\"\nMODEL_512_DIR = \"../input/happywhale-512x880-models\"\n\nprint(f\"\\n... DATA DIRECTORY PATH IS:\\n\\t--> {DATA_DIR}\")\nprint(f\"\\n... EXTRA METADATA DIRECTORY PATH IS:\\n\\t--> {EXTRA_DATA_DIR}\")\n\nprint(f\"\\n... IMMEDIATE CONTENTS OF DATA DIRECTORY IS:\")\nfor file in tf.io.gfile.glob(os.path.join(DATA_DIR, \"*\")): print(f\"\\t--> {file}\")\n\nprint(f\"\\n... IMMEDIATE CONTENTS OF EXTRA METADATA DIRECTORY IS:\")\nfor file in tf.io.gfile.glob(os.path.join(EXTRA_DATA_DIR, \"*\")): print(f\"\\t--> {file}\")\n\nprint(\"\\n\\n... DATA ACCESS SETUP COMPLETED ...\\n\")","metadata":{"papermill":{"duration":0.0797,"end_time":"2021-11-06T21:17:53.133972","exception":false,"start_time":"2021-11-06T21:17:53.054272","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-23T13:35:20.332263Z","iopub.execute_input":"2022-02-23T13:35:20.332588Z","iopub.status.idle":"2022-02-23T13:35:20.359562Z","shell.execute_reply.started":"2022-02-23T13:35:20.332543Z","shell.execute_reply":"2022-02-23T13:35:20.358516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #3eb489; background-color: #ffffff;\">2.3 LEVERAGING XLA OPTIMIZATIONS</h3>\n\n---\n\n\n**XLA** (Accelerated Linear Algebra) is a domain-specific compiler for linear algebra that can accelerate TensorFlow models with potentially no source code changes. **The results are improvements in speed and memory usage**.\n\n<br>\n\nWhen a TensorFlow program is run, all of the operations are executed individually by the TensorFlow executor. Each TensorFlow operation has a precompiled GPU/TPU kernel implementation that the executor dispatches to.\n\nXLA provides us with an alternative mode of running models: it compiles the TensorFlow graph into a sequence of computation kernels generated specifically for the given model. Because these kernels are unique to the model, they can exploit model-specific information for optimization.<br><br>\n\n<div class=\"alert alert-block alert-danger\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 16px;\">üõë &nbsp; WARNING:</b><br><br>- XLA can not currently compile functions where dimensions are not inferrable: that is, if it's not possible to infer the dimensions of all tensors without running the entire computation<br>\n</div>\n\n<div class=\"alert alert-block alert-info\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 16px;\">üìå &nbsp; NOTE:</b><br><br>- XLA compilation is only applied to code that is compiled into a graph (in <b>TF2</b> that's only a code inside <b><code>tf.function</code></b>).<br>- The <b><code>jit_compile</code></b> API has must-compile semantics, i.e. either the entire function is compiled with XLA, or an <b><code>errors.InvalidArgumentError</code></b> exception is thrown)\n</div>\n\n<div class=\"alert alert-block alert-info\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 16px;\">üìñ &nbsp; REFERENCE:</b><br><br>    - <a href=\"https://www.tensorflow.org/xla\"><b>XLA: Optimizing Compiler for Machine Learning</b></a><br>\n</div>","metadata":{"papermill":{"duration":0.051494,"end_time":"2021-11-06T21:17:53.236017","exception":false,"start_time":"2021-11-06T21:17:53.184523","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print(f\"\\n... XLA OPTIMIZATIONS STARTING ...\\n\")\n\nprint(f\"\\n... CONFIGURE JIT (JUST IN TIME) COMPILATION ...\\n\")\n# enable XLA optmizations (10% speedup when using @tf.function calls)\ntf.config.optimizer.set_jit(True)\n\nprint(f\"\\n... XLA OPTIMIZATIONS COMPLETED ...\\n\")","metadata":{"papermill":{"duration":0.128128,"end_time":"2021-11-06T21:17:53.442803","exception":false,"start_time":"2021-11-06T21:17:53.314675","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-23T13:35:20.362667Z","iopub.execute_input":"2022-02-23T13:35:20.363032Z","iopub.status.idle":"2022-02-23T13:35:20.37182Z","shell.execute_reply.started":"2022-02-23T13:35:20.362968Z","shell.execute_reply":"2022-02-23T13:35:20.370339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #3eb489; background-color: #ffffff;\">2.4 BASIC DATA DEFINITIONS & INITIALIZATIONS</h3>\n\n---\n","metadata":{"papermill":{"duration":0.090507,"end_time":"2021-11-06T21:17:53.624612","exception":false,"start_time":"2021-11-06T21:17:53.534105","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print(\"\\n... BASIC DATA SETUP STARTING ...\\n\\n\")\n\n# Thanks @karthickp6 for noticing this\nFIX_NAME_MAPPING = {\"bottlenose_dolpin\":\"bottlenose_dolphin\", \n                    \"kiler_whale\":\"killer_whale\",\n                    \"pilot_whale\":\"short_finned_pilot_whale\",\n                    \"globis\":\"short_finned_pilot_whale\"}\n\nprint(\"\\n... TRAIN DATAFRAME ...\\n\")\nTRAIN_CSV = os.path.join(DATA_DIR, \"train.csv\")\nEX_META_TRAIN_CSV = os.path.join(EXTRA_DATA_DIR, \"train.csv\")\ntrain_df = pd.read_csv(TRAIN_CSV)\nex_train_df = pd.read_csv(EX_META_TRAIN_CSV)\ntrain_df[\"img_path\"] = os.path.join(DATA_DIR, \"train_images\")+\"/\"+train_df.image\ntrain_df[\"img_shape\"] = ex_train_df[\"img_shape\"]\ntrain_df[\"img_width\"] = ex_train_df[\"img_width\"]\ntrain_df[\"img_height\"] = ex_train_df[\"img_height\"]\ntrain_df[\"species\"] = train_df[\"species\"].apply(lambda x: x if x not in FIX_NAME_MAPPING.keys() else FIX_NAME_MAPPING[x])\ntrain_df[\"n_img_of_ind\"] = train_df.individual_id.map(train_df.individual_id.value_counts().to_dict())\n\nall_species = sorted(train_df.species.unique().tolist())\nspecies_int2str_lbl_map = {i:_s for i,_s in enumerate(all_species)}\nspecies_str2int_lbl_map = {v:k for k,v in species_int2str_lbl_map.items()}\nN_SPECIES = len(all_species)\n\nall_individuals = sorted(train_df.individual_id.unique().tolist())\nind_int2str_lbl_map = {i:_s for i,_s in enumerate(all_individuals)}\nind_str2int_lbl_map = {v:k for k,v in ind_int2str_lbl_map.items()}\nN_INDIVIDUALS = len(all_individuals)\n\ntrain_df[\"ind_sparse_lbl\"] = train_df[\"individual_id\"].map(ind_str2int_lbl_map)\ntrain_df[\"species_sparse_lbl\"] = train_df[\"species\"].map(species_str2int_lbl_map)\n\ndisplay(train_df)\n\nprint(\"\\n... TEST DATAFRAME ...\\n\")\nTEST_CSV = os.path.join(DATA_DIR, \"sample_submission.csv\")\nEX_META_TEST_CSV = os.path.join(EXTRA_DATA_DIR, \"test.csv\")\ntest_df = pd.read_csv(TEST_CSV)\nex_test_df = pd.read_csv(EX_META_TEST_CSV)\ntest_df[\"img_path\"] = os.path.join(DATA_DIR, \"test_images\")+\"/\"+test_df.image\ntest_df[\"img_shape\"] = ex_test_df[\"img_shape\"]\ntest_df[\"img_width\"] = ex_test_df[\"img_width\"]\ntest_df[\"img_height\"] = ex_test_df[\"img_height\"]\ntest_df = test_df.drop(columns=[\"predictions\"])\ndisplay(test_df)\n\nprint(\"\\n... SS DATAFRAME ..\\n\")\nSS_CSV = os.path.join(DATA_DIR, \"sample_submission.csv\")\nss_df = pd.read_csv(SS_CSV)\ndisplay(ss_df)\n\nK_FOLDS = N_FOLDS = 10\nfold_512_train_dfs = [pd.read_csv(f\"/kaggle/input/happywhaletfrecords512x880/train_df_fold_{i}.csv\", \n                                  usecols=[\n                                      'image', 'species', 'individual_id', \n                                      'img_path', 'img_shape', 'img_width', \n                                      'img_height', 'n_img_of_ind', 'ind_sparse_lbl', \n                                      'species_sparse_lbl']) for i in range(1, K_FOLDS+1)]\n\nprint(\"\\n\\n... BASIC DATA SETUP FINISHING ...\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-02-23T13:35:20.374076Z","iopub.execute_input":"2022-02-23T13:35:20.374695Z","iopub.status.idle":"2022-02-23T13:35:21.400628Z","shell.execute_reply.started":"2022-02-23T13:35:20.374648Z","shell.execute_reply":"2022-02-23T13:35:21.398584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n\n<a id=\"helper_functions\"></a>\n\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: #3eb489; background-color: #ffffff;\" id=\"helper_functions\">\n    3&nbsp;&nbsp;HELPER FUNCTION & CLASSES&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a>\n</h1>\n\n---","metadata":{"papermill":{"duration":0.054893,"end_time":"2021-11-06T21:17:54.695576","exception":false,"start_time":"2021-11-06T21:17:54.640683","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def flatten_l_o_l(nested_list):\n    \"\"\" Flatten a list of lists \"\"\"\n    nested_list = [x if type(x) is list else [x,] for x in nested_list]\n    return [item for sublist in nested_list for item in sublist]","metadata":{"papermill":{"duration":0.098071,"end_time":"2021-11-06T21:17:54.848036","exception":false,"start_time":"2021-11-06T21:17:54.749965","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-23T13:35:21.402507Z","iopub.execute_input":"2022-02-23T13:35:21.40292Z","iopub.status.idle":"2022-02-23T13:35:21.410383Z","shell.execute_reply.started":"2022-02-23T13:35:21.402853Z","shell.execute_reply":"2022-02-23T13:35:21.40928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n\n<a id=\"dataset_exploration\"></a>\n\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: #3eb489; background-color: #ffffff;\" id=\"dataset_exploration\">\n    4&nbsp;&nbsp;DATASET EXPLORATION&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a>\n</h1>\n\n---","metadata":{"papermill":{"duration":0.056443,"end_time":"2021-11-06T21:17:54.960518","exception":false,"start_time":"2021-11-06T21:17:54.904075","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #3eb489; background-color: #ffffff;\">4.1 TRAIN METADATA</h3>\n\n---\n","metadata":{"papermill":{"duration":0.053158,"end_time":"2021-11-06T21:17:55.070372","exception":false,"start_time":"2021-11-06T21:17:55.017214","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print(\"\\n... TRAIN METADATA INVESTIGATION STARTING ...\\n\\n\")\n\nprint(\"\\n... TRAIN DATAFRAME HEAD(5) ...\\n\\n\")\ndisplay(train_df.head(5))\n\nprint(\"\\n... TRAIN DATAFRAME TAIL(5) ...\\n\\n\")\ndisplay(train_df.tail(5))\n\nprint(\"\\n... TRAIN DATAFRAME SAMPLE(5) ...\\n\\n\")\ndisplay(train_df.sample(5))\n\nN_TRAIN = len(train_df)\nN_TEST = len(ss_df)\nN_SPECIES = train_df[\"species\"].nunique()\nN_INDIV = train_df[\"individual_id\"].nunique()\n# AVE_IMG_SHAPE = \n\nprint(f\"\\n... NUMBER OF UNIQUE TRAINING IMAGES: {N_TRAIN} ...\")\nprint(f\"... NUMBER OF UNIQUE SPECIES IN TRAINING DATASET: {N_SPECIES} ...\")\nprint(f\"... NUMBER OF UNIQUE INDIVIDUALS IN TRAINING DATASET: {N_INDIV} ...\")\n# print(f\"... AVERAGE IMAGE SHAPE: {AVE_IMG_SHAPE}\")\nprint(\"\\n... TRAIN DATAFRAME PANDAS DESCRIPTION ...\\n\\n\")\ndisplay(train_df.describe().T)\n\n# Plotly stuff\ncolor_discrete_sequence=px.colors.qualitative.Light24\ncategory_orders={\"species\": sorted(list(train_df.species.unique()))}\n\n# Species Plot\nfig = px.histogram(train_df, \"species\", color=\"species\", color_discrete_sequence=color_discrete_sequence, category_orders=category_orders, title=\"<b>UNIQUE SPECIES DISTRIBUTION</b>\")\nfig.show(renderer=\"png\")\n\n# Individual ID Plot\ntmp_df = train_df.groupby(\"individual_id\")[\"species\", \"individual_id\"].first()\ntmp_df[\"n_count\"] = train_df.groupby(\"individual_id\").size()\nprint(\"\\n... OUTLIER ...\\n\")\ndisplay(tmp_df[tmp_df[\"n_count\"]>300])\ntmp_df = tmp_df[tmp_df[\"n_count\"]<300] # There is one outlier\nfig = px.histogram(tmp_df, \"n_count\", log_y=True,\n                   color=\"species\", color_discrete_sequence=color_discrete_sequence, \n                   category_orders=category_orders, \n                   title=\"<b># OF EXAMPLES PER UNIQUE INDIVIDUAL DISTRIBUTION</b>\")\nfig.show(renderer=\"png\")\n\ntmp_df_raw = pd.DataFrame()\ntmp_df_round = pd.DataFrame()\nround_to=100\n\ntmp_df_raw[\"raw_img_shape\"] = train_df.groupby(\"img_shape\")[\"img_path\"].count().keys()\ntmp_df_raw[\"raw_img_width\"] = tmp_df_raw[\"raw_img_shape\"].apply(lambda x: ast.literal_eval(x)[1])\ntmp_df_raw[\"raw_img_height\"] = tmp_df_raw[\"raw_img_shape\"].apply(lambda x: ast.literal_eval(x)[0])\ntmp_df_raw[\"area\"] = tmp_df_raw[\"raw_img_width\"]*tmp_df_raw[\"raw_img_height\"]\ntmp_df_raw[\"raw_n_count\"] = tmp_df_raw.groupby(\"raw_img_shape\")[\"area\"].count().values\ntmp_df_raw[\"raw_n_count__2\"] = tmp_df_raw[\"raw_n_count\"]**2\n\ntmp_df_round[\"round_img_shape\"] = train_df.img_shape.apply(lambda x: str(tuple([int(round_to*round(x/round_to)) if x!=3 else x for x in ast.literal_eval(x)]))).value_counts().keys()\ntmp_df_round[\"round_img_width\"] = tmp_df_round[\"round_img_shape\"].apply(lambda x: int(round_to*round(ast.literal_eval(x)[1]/round_to)))\ntmp_df_round[\"round_img_height\"] = tmp_df_round[\"round_img_shape\"].apply(lambda x: int(round_to*round(ast.literal_eval(x)[0]/round_to)))\ntmp_df_round[\"area\"] = tmp_df_round[\"round_img_width\"]*tmp_df_round[\"round_img_height\"]\ntmp_df_round[\"round_n_count\"] = train_df.img_shape.apply(lambda x: str(tuple([int(round_to*round(x/round_to)) if x!=3 else x for x in ast.literal_eval(x)]))).value_counts().values\ntmp_df_round[\"round_n_count__2\"] = tmp_df_round[\"round_n_count\"]**2\n\n# Image Shape Plot\nfig = px.scatter(tmp_df_raw, x=\"raw_img_width\", y=\"raw_img_height\", \n                 color=\"area\", size=\"raw_n_count\", size_max=2, \n                 title=f\"<b>Image Shapes Within The Dataset (No Rounding - <i>Many Individual Sizes</i>)</b>\")\nfig.update_layout(yaxis_range=[0, tmp_df_round.round_img_height.max()+100], \n                  xaxis_range=[0, tmp_df_round.round_img_width.max()+100])\nfig.show(renderer=\"png\")\n\n# Image Shape Plot\nfig = px.scatter(tmp_df_round, x=\"round_img_width\", y=\"round_img_height\", \n                 color=\"area\", size=\"round_n_count\", size_max=100,\n                 title=f\"<b>Image Shapes Within The Dataset (Round To Nearest {round_to})</b>\")\nfig.update_layout(yaxis_range=[0, tmp_df_round.round_img_height.max()+100], \n                  xaxis_range=[0, tmp_df_round.round_img_width.max()+100])\nfig.show(renderer=\"png\")","metadata":{"papermill":{"duration":0.313817,"end_time":"2021-11-06T21:17:55.439001","exception":false,"start_time":"2021-11-06T21:17:55.125184","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-23T13:35:21.412678Z","iopub.execute_input":"2022-02-23T13:35:21.413475Z","iopub.status.idle":"2022-02-23T13:35:30.444107Z","shell.execute_reply.started":"2022-02-23T13:35:21.413367Z","shell.execute_reply":"2022-02-23T13:35:30.442982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #3eb489; background-color: #ffffff;\">4.2 VISUALIZE THE UNIQUE SPECIES PRESENT IN THE DATASET</h3>\n\n---\n\n","metadata":{"papermill":{"duration":0.05563,"end_time":"2021-11-06T21:17:55.55248","exception":false,"start_time":"2021-11-06T21:17:55.49685","status":"completed"},"tags":[]}},{"cell_type":"code","source":"n_to_plot=2\n\nfor _s in sorted(train_df.species.unique()):\n    ex_img_paths = train_df[train_df.species==_s].sample(n_to_plot).img_path.values\n    \n    plt.figure(figsize=(20,10))\n    for i, ex_img_path in enumerate(ex_img_paths):\n        plt.subplot(1,n_to_plot,i+1)\n        ex_img = cv2.imread(ex_img_path)[..., ::-1]\n        plt.title(f\"Example #{i+1} of species={_s}\\n(shape={ex_img.shape})\", fontweight=\"bold\")\n        plt.axis(False)\n        plt.imshow(ex_img)\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-23T13:35:30.446417Z","iopub.execute_input":"2022-02-23T13:35:30.446792Z","iopub.status.idle":"2022-02-23T13:36:23.194459Z","shell.execute_reply.started":"2022-02-23T13:35:30.446735Z","shell.execute_reply":"2022-02-23T13:36:23.192033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #3eb489; background-color: #ffffff;\">4.2 VISUALIZE EXAMPLES OF UNIQUE INDIVIDUALS IN THE DATASET</h3>\n\n---\n\n","metadata":{"papermill":{"duration":0.684105,"end_time":"2021-11-06T21:18:30.682923","exception":false,"start_time":"2021-11-06T21:18:29.998818","status":"completed"},"tags":[]}},{"cell_type":"code","source":"ids_to_plot = 3\nn_ex_per_id=3\n\nfor i, (img_id, img_id_df) in enumerate(train_df[train_df.n_img_of_ind==n_ex_per_id].groupby(\"individual_id\")):\n    if i==ids_to_plot:\n        break\n    ex_img_paths = img_id_df.img_path.values\n    \n    plt.figure(figsize=(20,4))\n    for i, ex_img_path in enumerate(ex_img_paths):\n        plt.subplot(1,n_ex_per_id,i+1)\n        ex_img = cv2.imread(ex_img_path)[..., ::-1]\n        plt.title(f\"Example #{i+1} of image_id={img_id}\\n(shape={ex_img.shape})\", fontweight=\"bold\")\n        plt.axis(False)\n        plt.imshow(ex_img)\n    plt.tight_layout()\n    plt.show()\n    print(\"\\n\\n\\n\")","metadata":{"papermill":{"duration":8.127387,"end_time":"2021-11-06T21:18:39.473456","exception":false,"start_time":"2021-11-06T21:18:31.346069","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-23T13:36:23.196104Z","iopub.execute_input":"2022-02-23T13:36:23.197076Z","iopub.status.idle":"2022-02-23T13:36:28.41269Z","shell.execute_reply.started":"2022-02-23T13:36:23.197025Z","shell.execute_reply":"2022-02-23T13:36:28.411674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ids_to_plot = 3\nn_ex_per_id = 15\n\nfor i, (img_id, img_id_df) in enumerate(train_df[train_df.n_img_of_ind==n_ex_per_id].groupby(\"individual_id\")):\n    if i==ids_to_plot:\n        break\n    ex_img_paths = img_id_df.img_path.values\n    \n    plt.figure(figsize=(20,4*int(np.ceil(n_ex_per_id/3))))\n    for i, ex_img_path in enumerate(ex_img_paths):\n        plt.subplot(int(np.ceil(n_ex_per_id/3)), 3, i+1)\n        ex_img = cv2.imread(ex_img_path)[..., ::-1]\n        plt.title(f\"Example #{i+1} of image_id={img_id}\\n(shape={ex_img.shape})\", fontweight=\"bold\")\n        plt.axis(False)\n        plt.imshow(ex_img)\n    plt.tight_layout()\n    plt.show()\n    print(\"\\n\\n\\n\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-02-23T13:36:28.416451Z","iopub.execute_input":"2022-02-23T13:36:28.417161Z","iopub.status.idle":"2022-02-23T13:37:05.832105Z","shell.execute_reply.started":"2022-02-23T13:36:28.417118Z","shell.execute_reply":"2022-02-23T13:37:05.831077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n\n<a id=\"model_baseline\"></a>\n\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: #3eb489; background-color: #ffffff;\" id=\"model_baseline\">\n    5&nbsp;&nbsp;BASELINE<a href=\"#toc\">&nbsp;&nbsp;&nbsp;&nbsp;&#10514;</a>\n</h1>\n\n---\n\nMy Baseline Consists Of the Following Steps:\n\n**COMPLETED IN OTHER NOTEBOOKS**\n- Train a model to predict species (arcface)**(done)**\n- Use this model to generate embeddings for all train and test data **(done)**\n- Train a model to predict individuals (arcface)**(done)**\n- Use this model to generate embeddings for all train and test data **(done)**\n\n**TO BE DONE BELOW**\n- Cluster/similarity between embeddings for both species and individuals\n- Use individual identification clusters to generate predictions\n- Blend various folds","metadata":{}},{"cell_type":"markdown","source":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #3eb489; background-color: #ffffff;\">5.0 HELPFUL FUNCTIONS</h3>\n\n---\n","metadata":{}},{"cell_type":"code","source":"def get_emb_arrs(emb_dir, style=\"individual_embeddings\", fold_num=None):\n    # Retreive all individual numpy array files\n    all_npy_files = glob(os.path.join(emb_dir, \"**/*.npy\"), recursive=True)\n    all_files = [_f for _f in all_npy_files if style in _f]\n    if fold_num is not None:\n        all_files = [_f for _f in all_npy_files if f\"fold_{fold_num}\" in _f]\n        name_split_index=4\n    else:\n        name_split_index=1\n    \n    # Sort files based on subset\n    train_files =  [_f for _f in all_files if \"train_\" in _f]\n    val_files =  [_f for _f in all_files if \"val_\" in _f]\n    test_files =  [_f for _f in all_files if \"test_\" in _f]\n    \n    emb_arr_map = {}\n    for _subset, files in zip([\"train\", \"val\", \"test\"], [train_files, val_files, test_files]):\n        if len(files)==0: continue;\n        emb_arr_map[_subset]={\n            f_path.rsplit(\"/\", 1)[-1].split(\"_\", name_split_index)[-1][:-4]:np.load(f_path, allow_pickle=True)\n            for f_path in files\n        }\n        if _subset==\"test\":\n             emb_arr_map[_subset][\"image_ids\"] = np.array([str(x.decode()) for x in emb_arr_map[_subset][\"image_ids\"].flatten()])\n        else:\n            emb_arr_map[_subset][\"image_ids\"] = np.array([str(x.decode()) for x in emb_arr_map[_subset][\"image_ids\"]])\n        \n    return emb_arr_map\n\ndef load_img_from_id(img_id, df=train_df, resize_to=(880,512)):\n    _img = cv2.imread(train_df.query(f\"image=='{img_id}.jpg'\").img_path.values[0])[..., ::-1]\n    if resize_to is not None:\n        return cv2.resize(_img, resize_to)\n    else:\n        return _img\n    \ndef plot_sixpack(o_img, o_lbl, o_ind, nn_imgs, nn_lbls, nn_inds, nn_distances=None, K=5):\n    \"\"\" TBD \"\"\"\n    plt.figure(figsize=(20, 24))\n    plt.subplot(3,2,1)\n    plt.imshow(o_img)\n    plt.title(f\"\\nORIGINAL IMAGE\\nLABEL=={o_lbl}\\nINDIVIDUAL=={o_ind}\", fontweight=\"bold\")\n    plt.axis(False)\n    \n    for k in range(K):\n        plt.subplot(3,2,k+2)\n        plt.imshow(nn_imgs[k])\n        if nn_distances is not None:\n            _title = f\"\\nNEIGHBOR {k+1} IMAGE\\nLABEL=={nn_lbls[k]}\\nINDIVIDUAL=={nn_inds[k]}\\nDISTANCE=={nn_distances[k]:.3f}\"\n        else:\n            _title = f\"\\nNEIGHBOR {k+1} IMAGE\\nLABEL=={nn_lbls[k]}\\nINDIVIDUAL=={nn_inds[k]}\"\n        plt.title(_title, fontweight=\"bold\")\n        plt.axis(False)\n    \n    plt.tight_layout()\n    plt.show()\n    \ndef get_knn_model(emb_train_on, k_neighbors, metric=\"cosine\"):\n    _model = NearestNeighbors(n_neighbors=k_neighbors, metric=metric)\n    _model.fit(emb_train_on)\n    return _model\n\ndef filter_duplicates(row, dwn_wt_distance_position_exp=False):\n    unique_indices = np.array(sorted([row[\"knn_ind_ids\"].index(x) for x in set(row[\"knn_ind_ids\"])]))\n    row[\"knn_ind_ids\"] = list(np.array(row[\"knn_ind_ids\"])[unique_indices])\n    row[\"knn_species\"] = list(np.array(row[\"knn_species\"])[unique_indices])\n    row[\"knn_img_ids\"] = list(np.array(row[\"knn_img_ids\"])[unique_indices])\n    row[\"knn_img_paths\"] = list(np.array(row[\"knn_img_paths\"])[unique_indices])\n    row[\"knn_distances\"] = list(np.array(row[\"knn_distances\"])[unique_indices])\n    return row\n\ndef get_match_acc(row):\n    row[\"match_acc\"] = 0.00\n    for i, knn_ind_id in enumerate(row.knn_ind_ids[:5]):\n        if knn_ind_id==row.gt_ind_ids:\n            row[\"match_acc\"] = 1.00-(0.2*i)\n    return row\n\ndef create_similarity_df(train_arr_map, query_arr_map, nn_idxs, nn_dists, is_test=False):\n    print(\"\\n... INSTANTIATING DATAFRAME ...\\n\")\n    _df = pd.DataFrame()\n    _df[\"root_img_ids\"] = query_arr_map[\"image_ids\"]\n    _ind2count = {\n        _id:np.count_nonzero(train_arr_map[\"individual_ids\"]==ind_str2int_lbl_map[_id]) \\\n        for _id in train_df.individual_id.unique()\n    }\n    \n    print(\"\\n... GETTING ROOT/GT INFORMATION ...\\n\")\n    if is_test:\n        _df[\"root_img_path\"] = _df[\"root_img_ids\"].apply(lambda x: os.path.join(DATA_DIR, \"test_images\", x+\".jpg\"))\n    else:\n        _df[\"root_img_path\"] = _df[\"root_img_ids\"].apply(lambda x: os.path.join(DATA_DIR, \"train_images\", x+\".jpg\"))\n        _df[\"gt_ind_ids\"] = [ind_int2str_lbl_map[x] for x in query_arr_map[\"individual_ids\"]]\n        _df[\"gt_species\"] = [species_int2str_lbl_map[x] for x in query_arr_map[\"image_labels\"]]\n        _df[\"gt_ind_count_in_train\"] = _df[\"gt_ind_ids\"].apply(lambda x: _ind2count[x])\n    \n    print(\"\\n... GETTING KNN INFORMATION ...\\n\")\n    _df[\"knn_ind_ids\"] = [[ind_int2str_lbl_map[_id] for _id in train_arr_map[\"individual_ids\"][_idx]] for _idx in nn_idxs]\n    _df[\"knn_species\"] = _df[\"knn_ind_ids\"].apply(lambda x: [ind2species[_x] for _x in x])\n    _df[\"knn_img_ids\"] = [list(train_arr_map[\"image_ids\"][_idx]) for _idx in nn_idxs]\n    _df[\"knn_img_paths\"] = _df[\"knn_img_ids\"].apply(lambda x: [os.path.join(DATA_DIR, \"train_images\", _x+\".jpg\") for _x in x])\n    _df[\"knn_distances\"] = [list(nn_dist_group) for nn_dist_group in nn_dists]\n    \n    print(\"\\n... FILTER OUT DUPLICATES ...\\n\")\n    _df = _df.progress_apply(filter_duplicates, axis=1)\n    \n    if not is_test:    \n        print(\"\\n... GET ACCURACY ...\\n\")\n        _df = _df.progress_apply(get_match_acc, axis=1)\n        print(f\"\\n\\nTOP K=1 ACCURACY = {_df.match_acc.apply(lambda x: 0.00 if x != 1.00 else 1.00).mean()}\")\n        print(f\"TOP K=5 ACCURACY = {_df.match_acc.mean()}\\n\\n\")\n        \n    return _df\n\ndef combine_master_df_preds(row, cols):\n    all_ids = flatten_l_o_l(row[[_c for _c in cols if \"_ind_ids\" in _c]].to_list()) \n    all_dists = flatten_l_o_l(row[[_c for _c in cols if \"_distances\" in _c]].to_list()) \n    _dists, _ind_ids = [list(x) for x in zip(*sorted(zip(all_dists, all_ids)))]\n    unique_ind_ids = set(_ind_ids)\n    unique_indices = np.array(sorted([_ind_ids.index(x) for x in unique_ind_ids]))\n    row[\"knn_ind_ids\"] = list(np.array(_ind_ids)[unique_indices])[:5]\n    row[\"knn_distances\"] = list(np.array(_dists)[unique_indices])[:5]\n    return row\n\ndef get_final_preds(row, new_species_thresh=0.4):\n    final_preds = []\n    for _ind_id, _dist in zip(row.knn_ind_ids, row.knn_distances):\n        if (_dist<new_species_thresh or \"new_species\" in final_preds):\n            final_preds.append(_ind_id)\n        else:\n            final_preds.append(\"new_species\")\n            final_preds.append(_ind_id)\n    if (len(final_preds)<5 and \"new_species\" not in final_preds):\n        final_preds.append(\"new_species\")\n    row[\"predictions\"] = \" \".join(final_preds)\n    row[\"image\"] = row[\"image\"]+\".jpg\"\n    return row","metadata":{"execution":{"iopub.status.busy":"2022-02-23T13:37:05.834021Z","iopub.execute_input":"2022-02-23T13:37:05.834604Z","iopub.status.idle":"2022-02-23T13:37:05.95413Z","shell.execute_reply.started":"2022-02-23T13:37:05.834557Z","shell.execute_reply":"2022-02-23T13:37:05.952549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #3eb489; background-color: #ffffff;\">5.1 LOAD EMBEDDINGS AND OTHER DATA</h3>\n\n---","metadata":{}},{"cell_type":"code","source":"# Helpful stuff\n_K = 12\nind2species = train_df.groupby(\"individual_id\")[\"species\"].first().to_dict()\n\n# Load Embeddings\n# fold_ind_embs = [get_emb_arrs(EMBED_512_DIR, style=\"individual_embeddings\", fold_num=i) for i in range(1,2)]\nfold_ind_embs = [get_emb_arrs(\"../input/baseline-solution-train-indiv-model\", style=\"individual_embeddings\", fold_num=i) for i in range(6,8)]\nspecies_embs = get_emb_arrs(EMBED_384_DIR, style=\"species_embeddings\")\n\n# Instantiate KNN Models\nfold_ind_models = [\n    get_knn_model(fold_emb[\"train\"][\"image_embeds\"], k_neighbors=_K)\\\n    for fold_emb in fold_ind_embs\n]\nspc_model = get_knn_model(species_embs[\"train\"][\"image_embeds\"], k_neighbors=_K)\n\n\n# Get Dataframe Representations of KNN Solutions\n#\n# ------------------------------\n#     FOR INDIVIDUAL MODELS\n# ------------------------------\nval_solution_dfs = []\nfor i in tqdm(range(len(fold_ind_models)), total=len(fold_ind_models)):\n    distances, idxs = fold_ind_models[i].kneighbors(fold_ind_embs[i][\"val\"][\"image_embeds\"])\n    val_solution_dfs.append(create_similarity_df(\n        train_arr_map=fold_ind_embs[i][\"train\"], \n        query_arr_map=fold_ind_embs[i][\"val\"], \n        nn_idxs=idxs, nn_dists=distances, \n        is_test=False))\n\ntest_solution_dfs = []\nfor i in tqdm(range(len(fold_ind_models)), total=len(fold_ind_models)):\n    distances, idxs = fold_ind_models[i].kneighbors(fold_ind_embs[i][\"test\"][\"image_embeds\"])\n    test_solution_dfs.append(create_similarity_df(\n        train_arr_map=fold_ind_embs[i][\"train\"], \n        query_arr_map=fold_ind_embs[i][\"test\"], \n        nn_idxs=idxs, nn_dists=distances, \n        is_test=True))\n# ------------------------------    \n#\n#\n# ------------------------------\n#     FOR SPECIES MODEL\n# ------------------------------\ndistances, idxs = spc_model.kneighbors(species_embs[\"val\"][\"image_embeds\"])\nval_species_df = create_similarity_df(\n    train_arr_map=species_embs[\"train\"], \n    query_arr_map=species_embs[\"val\"], \n    nn_idxs=idxs, nn_dists=distances, \n    is_test=False)\n\ndistances, idxs = spc_model.kneighbors(species_embs[\"test\"][\"image_embeds\"])\ntest_species_df = create_similarity_df(\n    train_arr_map=species_embs[\"train\"], \n    query_arr_map=species_embs[\"test\"], \n    nn_idxs=idxs, nn_dists=distances, \n    is_test=True)\n# ------------------------------","metadata":{"execution":{"iopub.status.busy":"2022-02-23T13:37:05.95618Z","iopub.execute_input":"2022-02-23T13:37:05.956731Z","iopub.status.idle":"2022-02-23T13:38:32.451024Z","shell.execute_reply.started":"2022-02-23T13:37:05.956686Z","shell.execute_reply":"2022-02-23T13:38:32.449933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For submission we take just the important stuff\nmaster_val_soln_df = val_solution_dfs[0][[\"root_img_ids\", \"knn_ind_ids\", \"knn_distances\"]]\nmaster_val_soln_df.columns = [\"image\", \"knn_ind_ids_1\", \"knn_distances_1\"]\nfor i in range(1,2):\n    master_val_soln_df[f\"knn_ind_ids_{i+1}\"] = val_solution_dfs[i][\"knn_ind_ids\"]\n    master_val_soln_df[f\"knn_distances_{i+1}\"] = val_solution_dfs[i][\"knn_distances\"]\nmaster_val_soln_df = master_val_soln_df.progress_apply(lambda x: combine_master_df_preds(x, master_val_soln_df.columns), axis=1)[[\"image\", \"knn_ind_ids\", \"knn_distances\"]]\nfinal_master_val_soln_df = master_val_soln_df.progress_apply(get_final_preds, axis=1)\ndisplay(final_master_val_soln_df)\nmaster_val_soln_df","metadata":{"execution":{"iopub.status.busy":"2022-02-23T14:14:53.470178Z","iopub.execute_input":"2022-02-23T14:14:53.470507Z","iopub.status.idle":"2022-02-23T14:14:53.496737Z","shell.execute_reply.started":"2022-02-23T14:14:53.470474Z","shell.execute_reply":"2022-02-23T14:14:53.495689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp_df = pd.DataFrame(val_solution_dfs[0].groupby(\"gt_species\").mean()[\"match_acc\"]).reset_index()\ntmp_df[\"freq_in_data\"] = tmp_df.gt_species.apply(lambda x: train_df.species.value_counts()[x]/train_df.species.value_counts().values.max())\nfig = px.bar(tmp_df, x=\"gt_species\", y=\"match_acc\", color=\"gt_species\", hover_data=[\"freq_in_data\",],\n             color_discrete_map={_c:_hex for _hex,_c in zip(px.colors.qualitative.Alphabet, all_species)},\n             labels={\"match_acc\":\"<b>Top 5 Accuracy</b>\", \n                     \"gt_species\":\"<b>Species</b>\", \n                     \"color\":\"<b>Species</b>\",\n                     \"freq_in_data\":\"<b>Frequency in Training Data</b>\"},\n             title=\"<b>Top 5 Accuracy By Species In Val Subset 1</b>\")\nfig.add_scatter(x=tmp_df.gt_species.tolist(), y=tmp_df.freq_in_data.tolist(), \n                name=\"<b>Frequency in Training Data</b>\", mode=\"markers+lines\", \n                line=dict(color=\"black\"),  opacity=0.5,)\nfig.show()\n\n# Mean Count Per Species\n# fig.add_hline(y=train_df.species.value_counts().values.mean()/train_df.species.value_counts().values.max())\n\n\ntmp_df = pd.DataFrame(val_solution_dfs[1].groupby(\"gt_species\").mean()[\"match_acc\"]).reset_index()\ntmp_df[\"freq_in_data\"] = tmp_df.gt_species.apply(lambda x: train_df.species.value_counts()[x]/train_df.species.value_counts().values.max())\nfig = px.bar(tmp_df, x=\"gt_species\", y=\"match_acc\", color=\"gt_species\", hover_data=[\"freq_in_data\",],\n             color_discrete_map={_c:_hex for _hex,_c in zip(px.colors.qualitative.Alphabet, all_species)},\n             labels={\"match_acc\":\"<b>Top 5 Accuracy</b>\", \n                     \"gt_species\":\"<b>Species</b>\", \n                     \"color\":\"<b>Species</b>\",\n                     \"freq_in_data\":\"<b>Frequency in Training Data</b>\"},\n             title=\"<b>Top 5 Accuracy By Species In Val Subset 2</b>\")\nfig.add_scatter(x=tmp_df.gt_species.tolist(), y=tmp_df.freq_in_data.tolist(),\n                name=\"<b>Frequency in Training Data</b>\", mode=\"markers+lines\", \n                line=dict(color=\"black\"),  opacity=0.5,)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-23T14:13:48.012885Z","iopub.execute_input":"2022-02-23T14:13:48.013442Z","iopub.status.idle":"2022-02-23T14:13:49.088833Z","shell.execute_reply.started":"2022-02-23T14:13:48.013403Z","shell.execute_reply":"2022-02-23T14:13:49.087637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-02-23T13:40:09.480616Z","iopub.execute_input":"2022-02-23T13:40:09.481056Z","iopub.status.idle":"2022-02-23T13:40:29.710421Z","shell.execute_reply.started":"2022-02-23T13:40:09.481011Z","shell.execute_reply":"2022-02-23T13:40:29.709451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For submission we take just the important stuff\nmaster_test_soln_df = test_solution_dfs[0][[\"root_img_ids\", \"knn_ind_ids\", \"knn_distances\"]]\nmaster_test_soln_df.columns = [\"image\", \"knn_ind_ids_1\", \"knn_distances_1\"]\nfor i in range(1, 2):\n    master_test_soln_df[f\"knn_ind_ids_{i+1}\"] = test_solution_dfs[i][\"knn_ind_ids\"]\n    master_test_soln_df[f\"knn_distances_{i+1}\"] = test_solution_dfs[i][\"knn_distances\"]\nmaster_test_soln_df = master_test_soln_df.progress_apply(lambda x: combine_master_df_preds(x, master_test_soln_df.columns), axis=1)[[\"image\", \"knn_ind_ids\", \"knn_distances\"]]\nfinal_master_test_soln_df = master_test_soln_df.progress_apply(get_final_preds, axis=1)\nss_df = ss_df[[\"image\",]].merge(final_master_test_soln_df[[\"image\", \"predictions\"]], on=\"image\", how=\"left\")\nss_df.to_csv(\"submission.csv\", index=False)\n\ndisplay(final_master_test_soln_df)","metadata":{"execution":{"iopub.status.busy":"2022-02-23T14:14:59.616755Z","iopub.execute_input":"2022-02-23T14:14:59.617184Z","iopub.status.idle":"2022-02-23T14:16:47.70588Z","shell.execute_reply.started":"2022-02-23T14:14:59.617148Z","shell.execute_reply":"2022-02-23T14:16:47.704912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #3eb489; background-color: #ffffff;\">5.2 VISUALIZE SPECIES MODEL PREDICTIONS</h3>\n\n---\n\nNote that these predictions and the visuals associated with them tell us a lot!\n1. From the TSNE/UMAP plots.\n  * We can see that the species are quite linearly seperable.\n  * We can see that the test distribution looks very similar to the train/val distribution\n  \n2. The 'closest' images (to some original image) are usually NOT the same individual. Usually the 'closest' image to some original image is an image that shares many similar features (not only features of the animal, but features of the background, image size, image colour, etc.)\n3. ","metadata":{}},{"cell_type":"code","source":"tsne = TSNE(n_components=2, n_iter=1500)\n\nprint(\"\\n\\n\\nTSNE Embedding for Train and Validation For Species\\n\\n\")\ntsne_embedding = tsne.fit_transform(np.concatenate([species_embs[\"train\"][\"image_embeds\"], species_embs[\"val\"][\"image_embeds\"], species_embs[\"test\"][\"image_embeds\"]], axis=0))\ntrain_tsne_embedding = tsne_embedding[:-N_TEST]\nfig = px.scatter(\n    train_tsne_embedding, x=0, y=1,\n    color=[species_int2str_lbl_map[x] for x in np.concatenate([species_embs[\"train\"][\"image_labels\"], species_embs[\"val\"][\"image_labels\"]], axis=0)],\n    color_discrete_map={_c:_hex for _hex,_c in zip(px.colors.qualitative.Alphabet, all_species)},\n    labels={'color': '<b>Species</b>', \n            \"0\":\"<b>X Dimension</b>\", \n            \"1\":\"<b>Y Dimension</b>\"}, \n    title=\"<b>T-SNE 2D Projection From Species Model</b> (Train+Val)\",\n    width=1400, height=800,\n    hover_data={\n        \"<b>Species</b>\":[\n            species_int2str_lbl_map[x] for x in \\\n            np.concatenate([species_embs[\"train\"][\"image_labels\"], \n                            species_embs[\"val\"][\"image_labels\"]], axis=0)\n        ],\n        \"<b>Individual ID</b>\":[\n            ind_int2str_lbl_map[x] for x in \\\n            np.concatenate([species_embs[\"train\"][\"individual_ids\"], \n                            species_embs[\"val\"][\"individual_ids\"]], axis=0)\n        ],\n        \"<b>Image ID</b>\":np.concatenate([species_embs[\"train\"][\"image_ids\"], \n                                          species_embs[\"val\"][\"individual_ids\"]], axis=0),\n    }\n)\nfig.show()\n\nprint(\"\\n\\n\\nTSNE Embedding for Train and Validation For Species\\n\\n\")\ntest_tsne_embedding = tsne_embedding[-N_TEST:]\nfig = px.scatter(\n    test_tsne_embedding, x=0, y=1,\n    color=test_species_df[\"knn_species\"].apply(lambda x: Counter(x).most_common()[0][0]),\n    color_discrete_map={_c:_hex for _hex,_c in zip(px.colors.qualitative.Alphabet, all_species)},\n    labels={'color': '<b>Predicted Species</b>', \n            \"0\":\"<b>X Dimension</b>\", \n            \"1\":\"<b>Y Dimension</b>\"}, \n    title=\"<b>T-SNE 2D Projection From Species Model</b> (Test)\",\n    width=1250, height=750,\n    hover_data={\n        \"<b>Image ID</b>\":species_embs[\"test\"][\"image_ids\"]\n    }\n)\nfig.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-02-23T14:16:47.708211Z","iopub.execute_input":"2022-02-23T14:16:47.708784Z","iopub.status.idle":"2022-02-23T14:16:56.727319Z","shell.execute_reply.started":"2022-02-23T14:16:47.708737Z","shell.execute_reply":"2022-02-23T14:16:56.726024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"umap = UMAP(n_components=2, n_epochs=500)\n\numap_embedding = umap.fit_transform(np.concatenate([species_embs[\"train\"][\"image_embeds\"], species_embs[\"val\"][\"image_embeds\"], species_embs[\"test\"][\"image_embeds\"]], axis=0))\ntrain_umap_embedding = umap_embedding[:-N_TEST]\nfig = px.scatter(\n    train_umap_embedding, x=0, y=1,\n    color=[species_int2str_lbl_map[x] for x in np.concatenate([species_embs[\"train\"][\"image_labels\"], species_embs[\"val\"][\"image_labels\"]], axis=0)],\n    color_discrete_map={_c:_hex for _hex,_c in zip(px.colors.qualitative.Alphabet, all_species)},\n    labels={'color': '<b>Species</b>', \n            \"0\":\"<b>X Dimension</b>\", \n            \"1\":\"<b>Y Dimension</b>\"}, \n    title=\"<b>UMAP 2D Projection From Species Model</b> (Train+Val)\",\n    width=1250, height=750,\n    hover_data={\n        \"<b>Species</b>\":[\n            species_int2str_lbl_map[x] for x in \\\n            np.concatenate([species_embs[\"train\"][\"image_labels\"], \n                            species_embs[\"val\"][\"image_labels\"]], axis=0)\n        ],\n        \"<b>Individual ID</b>\":[\n            ind_int2str_lbl_map[x] for x in \\\n            np.concatenate([species_embs[\"train\"][\"individual_ids\"], \n                            species_embs[\"val\"][\"individual_ids\"]], axis=0)\n        ],\n        \"<b>Image ID</b>\":np.concatenate([species_embs[\"train\"][\"image_ids\"], \n                                          species_embs[\"val\"][\"individual_ids\"]], axis=0),\n    }\n)\nfig.show()\n\ntest_umap_embedding = umap_embedding[-N_TEST:]\nfig = px.scatter(\n    test_umap_embedding, x=0, y=1,\n    color=test_species_df[\"knn_species\"].apply(lambda x: Counter(x).most_common()[0][0]),\n    color_discrete_map={_c:_hex for _hex,_c in zip(px.colors.qualitative.Alphabet, all_species)},\n    labels={'color': '<b>Predicted Species</b>', \n            \"0\":\"<b>X Dimension</b>\", \n            \"1\":\"<b>Y Dimension</b>\"}, \n    title=\"<b>UMAP 2D Projection From Species Model</b> (Test)\",\n    width=1400, height=800,\n    hover_data={\n        \"<b>Image ID</b>\":species_embs[\"test\"][\"image_ids\"]\n    }\n)\nfig.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-02-23T14:16:56.729126Z","iopub.execute_input":"2022-02-23T14:16:56.729573Z","iopub.status.idle":"2022-02-23T14:17:01.875323Z","shell.execute_reply.started":"2022-02-23T14:16:56.729529Z","shell.execute_reply":"2022-02-23T14:17:01.874024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"umap3d = UMAP(n_components=3)\n\numap_embedding_3d = umap3d.fit_transform(np.concatenate([species_embs[\"train\"][\"image_embeds\"], species_embs[\"val\"][\"image_embeds\"], species_embs[\"test\"][\"image_embeds\"]], axis=0))\ntrain_umap_embedding_3d = umap_embedding_3d[:-N_TEST]\nfig = px.scatter_3d(\n    train_umap_embedding_3d, x=0, y=1, z=2,\n    color=[species_int2str_lbl_map[x] for x in np.concatenate([species_embs[\"train\"][\"image_labels\"], species_embs[\"val\"][\"image_labels\"]], axis=0)],\n    color_discrete_map={_c:_hex for _hex,_c in zip(px.colors.qualitative.Alphabet, all_species)},\n    labels={'color': '<b>Species</b>', \n            \"0\":\"<b>X Dimension</b>\", \n            \"1\":\"<b>Y Dimension</b>\",\n            \"2\":\"<b>Z Dimension</b>\"}, \n    title=\"<b>UMAP 3D Projection From Species Model</b> (Train+Val)\",\n    width=1250, height=700,\n    hover_data={\n        \"<b>Species</b>\":[\n            species_int2str_lbl_map[x] for x in \\\n            np.concatenate([species_embs[\"train\"][\"image_labels\"], \n                            species_embs[\"val\"][\"image_labels\"]], axis=0)\n        ],\n        \"<b>Individual ID</b>\":[\n            ind_int2str_lbl_map[x] for x in \\\n            np.concatenate([species_embs[\"train\"][\"individual_ids\"], \n                            species_embs[\"val\"][\"individual_ids\"]], axis=0)\n        ],\n        \"<b>Image ID</b>\":np.concatenate([species_embs[\"train\"][\"image_ids\"], \n                                          species_embs[\"val\"][\"individual_ids\"]], axis=0),\n    }\n)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-23T14:17:01.878123Z","iopub.execute_input":"2022-02-23T14:17:01.878654Z","iopub.status.idle":"2022-02-23T14:17:05.07097Z","shell.execute_reply.started":"2022-02-23T14:17:01.8786Z","shell.execute_reply":"2022-02-23T14:17:05.069232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_umap_embedding_3d = umap_embedding_3d[-N_TEST:]\nfig = px.scatter_3d(\n    test_umap_embedding_3d, x=0, y=1, z=2,\n    color=test_species_df[\"knn_species\"].apply(lambda x: Counter(x).most_common()[0][0]),\n    color_discrete_map={_c:_hex for _hex,_c in zip(px.colors.qualitative.Alphabet, all_species)},\n    labels={'color': '<b>Predicted Species</b>', \n            \"0\":\"<b>X Dimension</b>\", \n            \"1\":\"<b>Y Dimension</b>\",\n            \"2\":\"<b>Z Dimension</b>\"}, \n    title=\"<b>UMAP 3D Projection From Species Model</b> (Test)\",\n    width=1400, height=800,\n    hover_data={\n        \"<b>Image ID</b>\":species_embs[\"test\"][\"image_ids\"]\n    }\n)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-23T14:17:05.072845Z","iopub.execute_input":"2022-02-23T14:17:05.073728Z","iopub.status.idle":"2022-02-23T14:17:05.962911Z","shell.execute_reply.started":"2022-02-23T14:17:05.073685Z","shell.execute_reply":"2022-02-23T14:17:05.96146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_rows = random.sample(range(len(val_species_df)), k=3)\nfor row_idx in random_rows:\n    print(f\"\\n\\n\\n\\nEXAMPLE OF SPECIES NEAREST NEIGHBORS USING ROW #{row_idx}\\n\")\n    display(val_species_df.iloc[row_idx:row_idx+1])\n    print(\"\\n\\n\")\n    plot_sixpack(\n        o_img=cv2.imread(val_species_df.iloc[row_idx][\"root_img_path\"])[..., ::-1], \n        o_lbl=val_species_df.iloc[row_idx][\"gt_species\"], \n        o_ind=val_species_df.iloc[row_idx][\"gt_ind_ids\"], \n        nn_imgs=[cv2.imread(x)[..., ::-1] for x in val_species_df.iloc[row_idx][\"knn_img_paths\"]], \n        nn_lbls=val_species_df.iloc[row_idx][\"knn_species\"], \n        nn_inds=val_species_df.iloc[row_idx][\"knn_ind_ids\"], \n        nn_distances=val_species_df.iloc[row_idx][\"knn_distances\"])","metadata":{"execution":{"iopub.status.busy":"2022-02-23T14:17:05.964746Z","iopub.execute_input":"2022-02-23T14:17:05.965188Z","iopub.status.idle":"2022-02-23T14:17:27.950526Z","shell.execute_reply.started":"2022-02-23T14:17:05.965108Z","shell.execute_reply":"2022-02-23T14:17:27.948131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #3eb489; background-color: #ffffff;\">5.3 VISUALIZE INDIVIDUAL MODEL PREDICTIONS</h3>\n\n---\n\nNote that these predictions and the visuals are for a given species.\n* i.e. We do not show TSNE/UMAP globally... rather we do it within a particular species... and sometimes a subset of that species (i.e. 100 individuals).\n1. TBD\n2. TBD","metadata":{}},{"cell_type":"code","source":"cluster_mean_thresh=3\nmin_num_samples = 500\n\ncluster_size_df = pd.DataFrame(train_df.groupby(\"species\")[\"n_img_of_ind\"].mean())\ncluster_size_df[\"above_thresh\"] = cluster_size_df.n_img_of_ind.apply(lambda x: x>=cluster_mean_thresh)\ncluster_size_df = cluster_size_df.reset_index()\ncluster_size_df[\"above_min\"] = cluster_size_df.species.apply(lambda x: train_df[\"species\"].value_counts().to_dict()[x]>min_num_samples)\ndisplay(cluster_size_df)\n\nspecies_to_probe = list(cluster_size_df[(cluster_size_df.above_thresh & cluster_size_df.above_min)][\"species\"])\nprint(species_to_probe)","metadata":{"execution":{"iopub.status.busy":"2022-02-23T14:17:27.952075Z","iopub.execute_input":"2022-02-23T14:17:27.953017Z","iopub.status.idle":"2022-02-23T14:17:28.156751Z","shell.execute_reply.started":"2022-02-23T14:17:27.952971Z","shell.execute_reply":"2022-02-23T14:17:28.155692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_species_specific_embeddings(fold_emb_map, species):\n    if type(species) is str: species=species_str2int_lbl_map[species]\n    species_indices = np.where(fold_emb_map[\"image_labels\"]==species)[0]\n    return_emb_map = {k:v[species_indices] for k,v in fold_emb_map.items()}\n    return return_emb_map\n\nfor species in species_to_probe:\n    _train_emb = get_species_specific_embeddings(fold_ind_embs[1][\"train\"], species)\n    _val_emb = get_species_specific_embeddings(fold_ind_embs[1][\"val\"], species)\n\n    tsne = TSNE(n_components=2)\n\n    print(f\"\\n\\n\\nTSNE Embedding for Validation For Individuals Of The {species.replace('_', ' ').title()} Species (AVE CLUSTER SIZE = {cluster_size_df[cluster_size_df['species']==species].n_img_of_ind.values[0]:.2f}\\n\\n\")\n    tsne_embedding = tsne.fit_transform(np.concatenate([_train_emb[\"image_embeds\"], _val_emb[\"image_embeds\"]], axis=0))\n    fig = px.scatter(\n        tsne_embedding, x=0, y=1,\n        color=np.concatenate([_train_emb[\"individual_ids\"], _val_emb[\"individual_ids\"]], axis=0),\n        color_discrete_map=px.colors.sequential.Plasma_r,\n        labels={\"0\":\"<b>X Dimension</b>\", \n                \"1\":\"<b>Y Dimension</b>\"}, \n        title=f\"<b>T-SNE 2D Projection From Individual Model Within {species.replace('_', ' ').title()} Species</b> (Train+Val)\",\n        width=1400, height=800,\n        hover_data={\n            \"<b>Species</b>\":[\n                species_int2str_lbl_map[x] for x in \\\n                np.concatenate([_train_emb[\"image_labels\"], \n                                _val_emb[\"image_labels\"]], axis=0)\n            ],\n            \"<b>Individual ID</b>\":[\n                ind_int2str_lbl_map[x] for x in \\\n                np.concatenate([_train_emb[\"individual_ids\"], \n                                _val_emb[\"individual_ids\"]], axis=0)\n            ],\n            \"<b>Image ID</b>\":np.concatenate([_train_emb[\"image_ids\"], \n                                              _val_emb[\"individual_ids\"]], axis=0),\n        }\n    )\n    fig.update_layout(showlegend=False)\n    fig.show(renderer=\"png\")","metadata":{"execution":{"iopub.status.busy":"2022-02-23T14:17:28.158891Z","iopub.execute_input":"2022-02-23T14:17:28.15924Z","iopub.status.idle":"2022-02-23T14:17:55.443505Z","shell.execute_reply.started":"2022-02-23T14:17:28.159193Z","shell.execute_reply":"2022-02-23T14:17:55.442283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for species in species_to_probe:\n    _train_emb = get_species_specific_embeddings(fold_ind_embs[1][\"train\"], species)\n    _val_emb = get_species_specific_embeddings(fold_ind_embs[1][\"val\"], species)\n\n    umap = UMAP(n_components=2)\n\n    print(f\"\\n\\n\\nUMAP Embedding for Validation For Individuals Of The {species.replace('_', ' ').title()} Species (AVE CLUSTER SIZE = {cluster_size_df[cluster_size_df['species']==species].n_img_of_ind.values[0]:.2f}\\n\\n\")\n    umap_embedding = umap.fit_transform(np.concatenate([_train_emb[\"image_embeds\"], _val_emb[\"image_embeds\"]], axis=0))\n    fig = px.scatter(\n        umap_embedding, x=0, y=1,\n        color=np.concatenate([_train_emb[\"individual_ids\"], _val_emb[\"individual_ids\"]], axis=0),\n        color_discrete_map=px.colors.sequential.Plasma_r,\n        labels={\"0\":\"<b>X Dimension</b>\", \n                \"1\":\"<b>Y Dimension</b>\"}, \n        title=f\"<b>UMAP 2D Projection From Individual Model Within {species.replace('_', ' ').title()} Species</b> (Train+Val)\",\n        width=800, height=500,\n        hover_data={\n            \"<b>Species</b>\":[\n                species_int2str_lbl_map[x] for x in \\\n                np.concatenate([_train_emb[\"image_labels\"], \n                                _val_emb[\"image_labels\"]], axis=0)\n            ],\n            \"<b>Individual ID</b>\":[\n                ind_int2str_lbl_map[x] for x in \\\n                np.concatenate([_train_emb[\"individual_ids\"], \n                                _val_emb[\"individual_ids\"]], axis=0)\n            ],\n            \"<b>Image ID</b>\":np.concatenate([_train_emb[\"image_ids\"], \n                                              _val_emb[\"individual_ids\"]], axis=0),\n        }\n    )\n    fig.update_layout(showlegend=False)\n    fig.show(renderer=\"png\")","metadata":{"execution":{"iopub.status.busy":"2022-02-23T14:17:55.445402Z","iopub.execute_input":"2022-02-23T14:17:55.446007Z","iopub.status.idle":"2022-02-23T14:18:08.506518Z","shell.execute_reply.started":"2022-02-23T14:17:55.445931Z","shell.execute_reply":"2022-02-23T14:18:08.505409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_rows = val_solution_dfs[0][val_solution_dfs[0][\"knn_ind_ids\"].apply(len)>=5].sample(5)\nfor _, row in val_rows.iterrows():\n    print(f\"\\n\\n\\n\\nEXAMPLE OF INDIVIDUAL ID NEAREST NEIGHBORS USING ROW #{row_idx}\\n\")\n    display(pd.DataFrame(row).T)\n    print(\"\\n\\n\")\n    plot_sixpack(\n        o_img=cv2.imread(row[\"root_img_path\"])[..., ::-1], \n        o_lbl=row[\"gt_species\"], \n        o_ind=row[\"gt_ind_ids\"], \n        nn_imgs=[cv2.imread(x)[..., ::-1] for x in row[\"knn_img_paths\"]], \n        nn_lbls=row[\"knn_species\"], \n        nn_inds=row[\"knn_ind_ids\"], \n        nn_distances=row[\"knn_distances\"])","metadata":{"execution":{"iopub.status.busy":"2022-02-23T14:19:20.799169Z","iopub.execute_input":"2022-02-23T14:19:20.799898Z","iopub.status.idle":"2022-02-23T14:19:39.8337Z","shell.execute_reply.started":"2022-02-23T14:19:20.799863Z","shell.execute_reply":"2022-02-23T14:19:39.832561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}