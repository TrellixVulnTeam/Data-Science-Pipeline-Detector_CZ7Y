{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.keras.utils import to_categorical\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom pathlib import Path","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datapath = Path('/kaggle/input/whale2-cropped-dataset')\n\ntrain_df = pd.read_csv(\n    datapath / 'train2.csv',\n    usecols=['image', 'species', 'individual_id'],\n    dtype={\n        'image': 'str',\n        'species': 'str',\n        'individual_id': 'str',\n    },\n)\n\ntrain_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(\n    datapath / 'test2.csv',\n    usecols=['image', 'predictions', 'box'],\n    dtype={\n        'image': 'str',\n        'predictions': 'str',\n        'box': 'str',\n    },\n)\ntest_df.drop(['box'], axis=1, inplace=True)\ntest_df['predictions'] = np.nan\ntest_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['species'].replace({\n    'bottlenose_dolpin': 'bottlenose_dolphin',\n    'kiler_whale': 'killer_whale',\n    'beluga': 'beluga_whale',\n    'globis': 'globis_whale',\n},inplace =True)\n\ntrain_df['class'] = train_df['species'].apply(lambda x: x.split('_')[-1])\n\ntrain_df.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = train_df['species']\nfilenames = train_df['image']\n\nlabels = np.array(labels)\n\nlabel_encoder = LabelEncoder()\nlabels_enc = label_encoder.fit_transform(labels)\n\n# One hot vector representation of labels\ny_labels_one_hot = to_categorical(labels_enc, dtype='int64')\n\ny_labels_one_hot.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from glob import glob\nimport os\nimport random\n\ndef serialize_example(image, filename, label):\n\n    feature = {\n        'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image])),\n        'filename': tf.train.Feature(bytes_list=tf.train.BytesList(value=[str.encode(filename)])),\n#        'label': tf.train.Feature(float_list=tf.train.FloatList(value=label))\n        'label': tf.train.Feature(int64_list = tf.train.Int64List(value=label))\n#        'label': tf.train.Feature(int64_list = tf.train.Int64List(value = tf.one_hot(label, depth=len(labels), on_value=1, off_value=0)))\n    }\n\n    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n    return example_proto.SerializeToString()\n\ndef make_tfrecords(path, files_list, labels, record_file='/kaggle/working/images.tfrecords'):\n    count = 0\n    with tf.io.TFRecordWriter(record_file) as writer:\n        tmp = list(zip(files_list, labels))\n        random.shuffle(tmp)\n        files_list, labels = zip(*tmp)    \n        for filename, label in zip(files_list, labels):\n            image_string = open(''.join([path, '/', filename]), 'rb').read()\n            image_decoded = tf.image.decode_jpeg(image_string)\n            image_resized = tf.image.resize(image_decoded, [IMG_SIZE, IMG_SIZE])\n            image_encoded = tf.image.encode_jpeg(tf.cast(image_resized, tf.uint8))\n            image_bytes = image_encoded.numpy()\n            tf_example = serialize_example(image_bytes, filename, label)\n            writer.write(tf_example)\n            if count % 5000 == 0:\n                print('step : {}'.format(count))\n            count = count + 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _parse_image_function(example):\n    image_feature_description = {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'filename': tf.io.FixedLenFeature([], tf.string),\n        'label': tf.io.FixedLenFeature([NUM_CLASSES], tf.int64),\n    }\n\n    features = tf.io.parse_single_example(example, image_feature_description)\n    image = tf.image.decode_jpeg(features['image'], channels=3)\n    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n    \n    filename = features['filename'] # ignore for training\n\n    label = tf.cast(features['label'], tf.int64)\n\n    return image, label\n\ndef read_dataset(filename, batch_size):\n    dataset = tf.data.TFRecordDataset(filename)\n    dataset = dataset.map(_parse_image_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n    dataset = dataset.shuffle(500)\n    dataset = dataset.batch(batch_size, drop_remainder=False)\n    dataset = dataset.repeat()\n    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n\n    return dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _parse_image_function_v1(example):\n    image_feature_description = {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'filename': tf.io.FixedLenFeature([], tf.string),\n        'label': tf.io.FixedLenFeature([NUM_CLASSES], tf.int64),\n    }\n\n    features = tf.io.parse_single_example(example, image_feature_description)\n    image = tf.image.decode_jpeg(features['image'], channels=3)\n    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n    \n    filename = features['filename']\n\n    label = tf.cast(features['label'], tf.int64)\n\n    return image, filename, label\n\ndef read_dataset_v1(filename, batch_size):\n    dataset = tf.data.TFRecordDataset(filename)\n    dataset = dataset.map(_parse_image_function_v1, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n    dataset = dataset.shuffle(500)\n    dataset = dataset.batch(batch_size, drop_remainder=False)\n    dataset = dataset.repeat()\n    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n\n    return dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\ndef build_model(num_classes, img_size=224):\n    input = tf.keras.layers.Input(shape=(img_size, img_size, 3))\n    model = tf.keras.applications.EfficientNetB3(include_top=False, input_tensor=input, weights=\"imagenet\")\n\n    # Freeze the pretrained weights\n    model.trainable = False\n\n    # Rebuild top\n    x = tf.keras.layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n    x = tf.keras.layers.BatchNormalization()(x)\n\n    top_dropout_rate = 0.2\n    x = tf.keras.layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n    output = tf.keras.layers.Dense(num_classes, activation=\"softmax\", name=\"pred\")(x)\n\n    # Compile\n    model = tf.keras.Model(input, output, name=\"EfficientNet\")\n    model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=[\"categorical_accuracy\"])\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\nfrom glob import glob\n\ndef use_tfrecords(path):\n    dataset = read_dataset('/kaggle/working/images.tfrecords', BATCH_SIZE)\n\n    model = build_model(NUM_CLASSES, IMG_SIZE)\n    model.fit(dataset, \n            batch_size=BATCH_SIZE, \n            epochs=20, \n            steps_per_epoch=math.ceil(NUM_TRAIN_IMAGES/BATCH_SIZE), \n            verbose=1,\n           )\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_path = '/kaggle/input/whale2-cropped-dataset/cropped_train_images/cropped_train_images/'\ntest_path = '/kaggle/input/whale2-cropped-dataset/cropped_test_images/cropped_test_images/'\n\nNUM_TRAIN_IMAGES, NUM_CLASSES = y_labels_one_hot.shape\nNUM_TEST_IMAGES = test_df.shape[0]\n\nNUM_FEATURES = 1536\n\nNUM_NEIGHBORS = 5\n\nIMG_SIZE = 80\n\nBATCH_SIZE = 64","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm /kaggle/working/train_images.tfrecords\n!rm /kaggle/working/test_images.tfrecords\n\nmake_tfrecords(train_path, \n               files_list = np.array(train_df['image']), \n               labels = y_labels_one_hot, \n               record_file = '/kaggle/working/train_images.tfrecords')\n\nmake_tfrecords(test_path, \n               files_list = np.array(test_df['image']), \n               labels = np.zeros([NUM_TEST_IMAGES, NUM_CLASSES], dtype='int64'), \n               record_file = '/kaggle/working/test_images.tfrecords')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = use_tfrecords(path)\n\nmodel.save('/kaggle/working/myModel_cropped.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.models.load_model('/kaggle/input/whale2-data/myModel_cropped.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"layer_name = 'avg_pool'\nintermediate_layer_model = tf.keras.Model(inputs=model.input, \n                                       outputs=model.get_layer(layer_name).output\n                                      )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = read_dataset_v1('/kaggle/input/whale2-data/train_images_cropped.tfrecords', BATCH_SIZE)\n\nj = 0\nembeddings_dict = {}\nfor image,filename,label in dataset.take(int(math.ceil((NUM_TRAIN_IMAGES / BATCH_SIZE)))):\n    embeddings = intermediate_layer_model(np.array(image))\n    embeddings_dict.update(zip(np.array(filename).astype(str),np.array(embeddings).tolist()))\n    \n    j = j + 1\n    if j%100 == 0:\n        print('Reads : {}'.format(j*BATCH_SIZE))\n        \nlen(embeddings_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n\nwith open('/kaggle/working/embeddings_dict.pkl', 'wb') as f:\n    pickle.dump(embeddings_dict, f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n\nwith open('/kaggle/input/whale2-data/embeddings_dict.pkl', 'rb') as f:\n    embeddings_dict = pickle.load(f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.neighbors import NearestNeighbors\n\nembeddings_keys = list(embeddings_dict.keys())\nembeddings_vals = list(embeddings_dict.values())\n\nknn = NearestNeighbors(n_neighbors=NUM_NEIGHBORS, metric=\"cosine\")\nknn.fit(embeddings_vals)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = read_dataset_v1('/kaggle/input/whale2-data/test_images_cropped.tfrecords', BATCH_SIZE)\n\n#keys = {tuple(np.array(test_df['image']))}\n#indices_dict = dict.fromkeys(keys)\nindices_dict = {}\nj = 0\nfor image,filename,label in dataset.take(int(math.ceil((NUM_TEST_IMAGES / BATCH_SIZE)))):\n    embeddings = intermediate_layer_model(np.array(image))\n    _, indices = knn.kneighbors(embeddings)\n    indices_list = indices.tolist()\n    indices_dict.update(zip(np.array(filename).astype(str),indices_list))\n    \n    j = j + BATCH_SIZE\n    if j%100 == 0:\n        print('Reads : {}'.format(j*BATCH_SIZE))\n    \nprint(len(indices_dict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n\nwith open('/kaggle/working/indices_dict.pkl', 'wb') as f:\n    pickle.dump(indices_dict, f)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n\nwith open('/kaggle/input/whale2-data/indices_dict.pkl', 'rb') as f:\n    indices_dict = pickle.load(f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#embeddings_list = np.array(list(embeddings_dict.keys()))\nindices_dict = {}\nfor k in indices_dict.keys():\n    _, indices = knn.kneighbors(np.array(embeddings_dict['00021adfb725ed.jpg']).reshape(1,-1))\n    indices_list = indices.tolist()\n\n#you embeddings_list[tuple(indices_list)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matches_dict = {}\nfor k in indices_dict.keys():\n    matches = train_df[train_df['image'].isin(filenames[indices_dict[k]].flatten())]['individual_id']\n    matches_dict.update({k:(' '.join(matches))})\nlen(matches_dict)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nimg=mpimg.imread('/kaggle/input/happy-whale-and-dolphin/test_images/0006287ec424cb.jpg')\n\nimg_resized = tf.image.resize(img, [IMG_SIZE,IMG_SIZE])\n\nimg_np = np.array(img_resized).reshape(\n      (1, img_resized.shape[0], img_resized.shape[1], 3)).astype(np.uint8)\n\nembedding = np.array(intermediate_layer_model(img_np))\n\n_, indices = knn.kneighbors(embedding.reshape(1,-1))\nindices_list = indices.tolist()\nindices_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings_keys = np.array(list(embeddings_dict.keys()))\nembeddings_vals = np.array(list(embeddings_dict.values()))\n\nindices_keys = np.array(list(indices_dict.keys()))\nindices_vals = np.array(list(indices_dict.values()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matches_dict = {}\nfor k in indices_dict.keys():\n    matches = embeddings_keys[list(indices_dict[k])]\n    matches_ids = train_df[train_df['image'].isin(matches.flatten())]['individual_id']\n    matches_dict.update({k:(' '.join(matches_ids))})\nlen(matches_dict)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for k in matches_dict.keys():\n    test_df.loc[test_df['image'] == k,'predictions'] = matches_dict[k]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.to_csv('/kaggle/working/submission.csv', sep=',', encoding='utf-8', header=True, index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# HOW TO predict\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nimg=mpimg.imread('/kaggle/input/happy-whale-and-dolphin/test_images/0006287ec424cb.jpg')\n\nimg_resized = tf.image.resize(img, [IMG_SIZE,IMG_SIZE])\n\nimg_np = np.array(img_resized).reshape(\n      (1, img_resized.shape[0], img_resized.shape[1], 3)).astype(np.uint8)\n\ny_prob = model.predict(img_np)\n\ny_pred = np.argmax(y_prob, axis=None, out=None)\n\n#y_pred = (y_prob == y_prob.max(axis=1)).astype(int)\n\ntop_k_values, top_k_indices = tf.nn.top_k(y_prob, k=5)\n\nlabel_encoder.inverse_transform(np.array(top_k_indices).squeeze())\n\n#top_k_indices","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# HOW TO save\nnp.save('/kaggle/working/intermediate_output.npy', intermediate_output)\nintermediate_output = np.load('/kaggle/input/whale2-data/intermediate_output.npy')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# HOWTO stack\nintermediate_layer_model(np.stack([img_resized,img_resized], axis=0))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.compat.v1.enable_eager_execution()\n\ndef read_dataset_new(filename, target='delay'):\n    ds = tf.data.TFRecordDataset(filename)\n    ds = ds.map(lambda buf: parse(buf, target=target))\n    ds = ds.batch(1)\n    return ds\n\n# This should return your key values for each example.\n#for features, labels in read_dataset_new(self.tf_rcrds_fl_nm):\n#    features_keys = features.keys()\n\n# This should return your tensor values if they supposed to be numeric.\nfor features, labels in read_dataset_new('/kaggle/working/images.tfrecords'):\n    features_array = numpy.array(features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom skimage.io import imread\nfrom skimage.transform import resize\n\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential, Model, load_model\nfrom tensorflow.keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten, BatchNormalization","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Conv2D(filters = 64, kernel_size = (5,5), activation ='relu',input_shape=(80,80,3)))\nmodel.add(BatchNormalization(axis=3))\nmodel.add(Conv2D(filters = 64, kernel_size = (5,5), activation ='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(BatchNormalization(axis=3))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(filters = 128, kernel_size = (5,5), activation ='relu'))\nmodel.add(BatchNormalization(axis=3))\nmodel.add(Conv2D(filters = 128, kernel_size = (5,5), activation ='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(BatchNormalization(axis=3))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(filters = 256, kernel_size = (5,5), activation ='relu'))\nmodel.add(BatchNormalization(axis=3))\nmodel.add(Conv2D(filters = 256, kernel_size = (5,5), activation ='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(BatchNormalization(axis=3))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(256, activation = \"relu\")) #Fully connected layer\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(60, activation = \"relu\")) #Fully connected layer\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(30, activation = \"softmax\")) #Classification layer or output layer\n\nmodel.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n# instantiating the model in the strategy scope creates the model on the TPU\nwith tpu_strategy.scope():\n    model = Sequential()\n\n    model.add(Conv2D(filters = 64, kernel_size = (5,5), activation ='relu',input_shape=(80,80,3)))\n    model.add(BatchNormalization(axis=3))\n    model.add(Conv2D(filters = 64, kernel_size = (5,5), activation ='relu'))\n    model.add(MaxPooling2D(pool_size=(2,2)))\n    model.add(BatchNormalization(axis=3))\n    model.add(Dropout(0.25))\n\n    model.add(Conv2D(filters = 128, kernel_size = (5,5), activation ='relu'))\n    model.add(BatchNormalization(axis=3))\n    model.add(Conv2D(filters = 128, kernel_size = (5,5), activation ='relu'))\n    model.add(MaxPooling2D(pool_size=(2,2)))\n    model.add(BatchNormalization(axis=3))\n    model.add(Dropout(0.25))\n\n    model.add(Conv2D(filters = 256, kernel_size = (5,5), activation ='relu'))\n    model.add(BatchNormalization(axis=3))\n    model.add(Conv2D(filters = 256, kernel_size = (5,5), activation ='relu'))\n    model.add(MaxPooling2D(pool_size=(2,2)))\n    model.add(BatchNormalization(axis=3))\n    model.add(Dropout(0.25))\n\n    model.add(Flatten())\n\n    model.add(Dense(256, activation = \"relu\")) #Fully connected layer\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n\n    model.add(Dense(60, activation = \"relu\")) #Fully connected layer\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n\n    model.add(Dense(30, activation = \"softmax\")) #Classification layer or output layer\n\n    model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n\n    model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(my_training_batch_generator,\n                   steps_per_epoch = int(X_train_filenames.shape[0] // batch_size),\n                   epochs = 10,\n                   verbose = 1,\n                   validation_data = my_validation_batch_generator,\n                   validation_steps = int(X_val_filenames.shape[0] // batch_size))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_val_filenames.shape[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/ultralytics/yolov5  # clone\n%cd yolov5\n%pip install -qr requirements.txt  # install\n\nimport torch\nfrom yolov5 import utils\ndisplay = utils.notebook_init()  # checks","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -r ./runs\n!python detect.py --weights yolov5s.pt --img 640 --conf 0.25 --source /kaggle/input/happy-whale-and-dolphin/train_images/001001f099519f.jpg\ndisplay.Image(filename='./runs/detect/exp/001001f099519f.jpg', width=600)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\n# Model\nmodel = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # or yolov5m, yolov5l, yolov5x, etc.\n\nmodel.conf = 0.25\nmodel.iou = 0.45\nmodel.classes = None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from os import listdir\nfrom os.path import isfile, join\n\nmypath = '/kaggle/input/happy-whale-and-dolphin/train_images/'\n\ninput_images = [join(mypath, f) for f in listdir(mypath) if isfile(join(mypath, f))]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print([model(file, size=640) for file in input_images[:10]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_images = ['/kaggle/input/happy-whale-and-dolphin/train_images/00087baf5cef7a.jpg']\n\n#print([file for file in input_images[:10]])\n\n#results = model(input_images[:10], size=640)\nresults = model(imgCropped, size=640)\nlabels, cord_thres = results.xyxyn[0][:, -1].numpy(), results.xyxyn[0][:, :-1].numpy()\n\nlabels, cord_thres","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = cord_thres[0][0]\ny = cord_thres[0][1]\nw = cord_thres[0][2]\nh = cord_thres[0][3]\n_ = cord_thres[0][4] # label confidence","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!/usr/bin/env python3\n\nimport sys\nimport pyvips\nimport numpy as np\n\n# Process all input files so we only incur Python startup overhead once\nfor filename in sys.argv[1:]:\n   print(f'Processing: {filename}')\n\n   img = pyvips.Image.new_from_file(filename, access='sequential')\n   roi = img.crop(0, 150, 270, 900)\n   mem_img = roi.write_to_memory()\n\n   # Make a numpy array from that buffer object\n   nparr = np.ndarray(buffer=mem_img, dtype=np.uint8,\n                   shape=[roi.height, roi.width, roi.bands])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nimg=mpimg.imread('/kaggle/input/happy-whale-and-dolphin/test_images/dae6e56cf64588.jpg')\n\nH,W,_ = img.shape\n\nx,y,w,h = int(cord_thres[0][1]*W), int(cord_thres[0][0]*H), int(cord_thres[0][2]*H), int(cord_thres[0][3]*W)\n\nimgCrop = img[x:x+w, y:y+h]\n\n#plt.imshow(imgCrop)\n\nmpimg.imsave('/kaggle/working/tmp.jpg', imgCrop)\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.imshow(imgCrop)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport matplotlib.pyplot as plt\n\nlabel = cord_thres[0]\nimg = cv2.imread('/kaggle/input/happy-whale-and-dolphin/train_images/00087baf5cef7a.jpg')\n\nH, W, _ = img.shape\nobject = [int(label[0]*W/2), int(label[1]*H/2), int(label[2]*W), int(label[3]*H)]\n\nx,y,w,h = object\nplt.subplot(1,2,1)\nplt.imshow(img)\nplt.subplot(1,2,2)\nplt.imshow(img[y:y+h, x:x+w])\nplt.show()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results.xyxy[0]  # img1 predictions (tensor)\nresults.pandas().xyxy[0]  # img1 predictions (pandas)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x,y,w,h = int(cord_thres[0][0]*W), int(cord_thres[0][1]*H), int(cord_thres[0][2]*W), int(cord_thres[0][3]*H)\nprint('({},{},{},{})'.format(x,y,w,h))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\nimport cv2\nimport matplotlib.pyplot as plt\n\nimg = cv2.imread('/kaggle/input/happy-whale-and-dolphin/train_images/000562241d384d.jpg')\n\nprint(img.shape)\n\nplt.imshow(img)\n\nplt.show()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow_hub as hub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ALL_MODELS = {\n'CenterNet HourGlass104 512x512' : 'https://tfhub.dev/tensorflow/centernet/hourglass_512x512/1',\n'CenterNet HourGlass104 Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/hourglass_512x512_kpts/1',\n'CenterNet HourGlass104 1024x1024' : 'https://tfhub.dev/tensorflow/centernet/hourglass_1024x1024/1',\n'CenterNet HourGlass104 Keypoints 1024x1024' : 'https://tfhub.dev/tensorflow/centernet/hourglass_1024x1024_kpts/1',\n'CenterNet Resnet50 V1 FPN 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v1_fpn_512x512/1',\n'CenterNet Resnet50 V1 FPN Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v1_fpn_512x512_kpts/1',\n'CenterNet Resnet101 V1 FPN 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet101v1_fpn_512x512/1',\n'CenterNet Resnet50 V2 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v2_512x512/1',\n'CenterNet Resnet50 V2 Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v2_512x512_kpts/1',\n'EfficientDet D0 512x512' : 'https://tfhub.dev/tensorflow/efficientdet/d0/1',\n'EfficientDet D1 640x640' : 'https://tfhub.dev/tensorflow/efficientdet/d1/1',\n'EfficientDet D2 768x768' : 'https://tfhub.dev/tensorflow/efficientdet/d2/1',\n'EfficientDet D3 896x896' : 'https://tfhub.dev/tensorflow/efficientdet/d3/1',\n'EfficientDet D4 1024x1024' : 'https://tfhub.dev/tensorflow/efficientdet/d4/1',\n'EfficientDet D5 1280x1280' : 'https://tfhub.dev/tensorflow/efficientdet/d5/1',\n'EfficientDet D6 1280x1280' : 'https://tfhub.dev/tensorflow/efficientdet/d6/1',\n'EfficientDet D7 1536x1536' : 'https://tfhub.dev/tensorflow/efficientdet/d7/1',\n'SSD MobileNet v2 320x320' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2',\n'SSD MobileNet V1 FPN 640x640' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v1/fpn_640x640/1',\n'SSD MobileNet V2 FPNLite 320x320' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_320x320/1',\n'SSD MobileNet V2 FPNLite 640x640' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_640x640/1',\n'SSD ResNet50 V1 FPN 640x640 (RetinaNet50)' : 'https://tfhub.dev/tensorflow/retinanet/resnet50_v1_fpn_640x640/1',\n'SSD ResNet50 V1 FPN 1024x1024 (RetinaNet50)' : 'https://tfhub.dev/tensorflow/retinanet/resnet50_v1_fpn_1024x1024/1',\n'SSD ResNet101 V1 FPN 640x640 (RetinaNet101)' : 'https://tfhub.dev/tensorflow/retinanet/resnet101_v1_fpn_640x640/1',\n'SSD ResNet101 V1 FPN 1024x1024 (RetinaNet101)' : 'https://tfhub.dev/tensorflow/retinanet/resnet101_v1_fpn_1024x1024/1',\n'SSD ResNet152 V1 FPN 640x640 (RetinaNet152)' : 'https://tfhub.dev/tensorflow/retinanet/resnet152_v1_fpn_640x640/1',\n'SSD ResNet152 V1 FPN 1024x1024 (RetinaNet152)' : 'https://tfhub.dev/tensorflow/retinanet/resnet152_v1_fpn_1024x1024/1',\n'Faster R-CNN ResNet50 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_640x640/1',\n'Faster R-CNN ResNet50 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_1024x1024/1',\n'Faster R-CNN ResNet50 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_800x1333/1',\n'Faster R-CNN ResNet101 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_640x640/1',\n'Faster R-CNN ResNet101 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_1024x1024/1',\n'Faster R-CNN ResNet101 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_800x1333/1',\n'Faster R-CNN ResNet152 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_640x640/1',\n'Faster R-CNN ResNet152 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_1024x1024/1',\n'Faster R-CNN ResNet152 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_800x1333/1',\n'Faster R-CNN Inception ResNet V2 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/inception_resnet_v2_640x640/1',\n'Faster R-CNN Inception ResNet V2 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/inception_resnet_v2_1024x1024/1',\n'Mask R-CNN Inception ResNet V2 1024x1024' : 'https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1'\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Model Selection { display-mode: \"form\", run: \"auto\" }\nmodel_display_name = 'CenterNet HourGlass104 Keypoints 512x512' # @param ['CenterNet HourGlass104 512x512','CenterNet HourGlass104 Keypoints 512x512','CenterNet HourGlass104 1024x1024','CenterNet HourGlass104 Keypoints 1024x1024','CenterNet Resnet50 V1 FPN 512x512','CenterNet Resnet50 V1 FPN Keypoints 512x512','CenterNet Resnet101 V1 FPN 512x512','CenterNet Resnet50 V2 512x512','CenterNet Resnet50 V2 Keypoints 512x512','EfficientDet D0 512x512','EfficientDet D1 640x640','EfficientDet D2 768x768','EfficientDet D3 896x896','EfficientDet D4 1024x1024','EfficientDet D5 1280x1280','EfficientDet D6 1280x1280','EfficientDet D7 1536x1536','SSD MobileNet v2 320x320','SSD MobileNet V1 FPN 640x640','SSD MobileNet V2 FPNLite 320x320','SSD MobileNet V2 FPNLite 640x640','SSD ResNet50 V1 FPN 640x640 (RetinaNet50)','SSD ResNet50 V1 FPN 1024x1024 (RetinaNet50)','SSD ResNet101 V1 FPN 640x640 (RetinaNet101)','SSD ResNet101 V1 FPN 1024x1024 (RetinaNet101)','SSD ResNet152 V1 FPN 640x640 (RetinaNet152)','SSD ResNet152 V1 FPN 1024x1024 (RetinaNet152)','Faster R-CNN ResNet50 V1 640x640','Faster R-CNN ResNet50 V1 1024x1024','Faster R-CNN ResNet50 V1 800x1333','Faster R-CNN ResNet101 V1 640x640','Faster R-CNN ResNet101 V1 1024x1024','Faster R-CNN ResNet101 V1 800x1333','Faster R-CNN ResNet152 V1 640x640','Faster R-CNN ResNet152 V1 1024x1024','Faster R-CNN ResNet152 V1 800x1333','Faster R-CNN Inception ResNet V2 640x640','Faster R-CNN Inception ResNet V2 1024x1024','Mask R-CNN Inception ResNet V2 1024x1024']\nmodel_handle = ALL_MODELS[model_display_name]\n\nprint('Selected model:'+ model_display_name)\nprint('Model Handle at TensorFlow Hub: {}'.format(model_handle))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('loading model...')\nhub_model = hub.load(model_handle)\nprint('model loaded!')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\nimport cv2\nimport matplotlib.pyplot as plt\n\nimg = cv2.imread('/kaggle/input/happy-whale-and-dolphin/train_images/001001f099519f.jpg')\n\nimg_np = np.array(img).reshape(\n      (1, img.shape[0], img.shape[1], 3)).astype(np.uint8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# running inference\nresults = hub_model(img_np)\n\n# different object detection models have additional results\n# all of them are explained in the documentation\nresult = {key:value.numpy() for key,value in results.items()}\nprint(result.keys())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"import cv2\nimport matplotlib.pyplot as plt\n\nimg = cv2.imread('/kaggle/input/happy-whale-and-dolphin/train_images/001001f099519f.jpg')\n\nH, W, _ = img.shape\n\nplt.subplot(3,2,1)\nplt.imshow(img)\n\nlabel = result['detection_boxes'][0][0]\nobject = [int(label[0]*W/2), int(label[1]*H/2), int(label[2]*W), int(label[3]*H)]\nx,y,w,h = object\nplt.subplot(3,2,2)\nplt.imshow(img[y:y+h, x:x+w])\n\nlabel = result['detection_boxes'][0][1]\nobject = [int(label[0]*W/2), int(label[1]*H/2), int(label[2]*W), int(label[3]*H)]\nx,y,w,h = object\nplt.subplot(3,2,3)\nplt.imshow(img[y:y+h, x:x+w])\n\nlabel = result['detection_boxes'][0][2]\nobject = [int(label[0]*W/2), int(label[1]*H/2), int(label[2]*W), int(label[3]*H)]\nx,y,w,h = object\nplt.subplot(3,2,4)\nplt.imshow(img[y:y+h, x:x+w])\n\nlabel = result['detection_boxes'][0][3]\nobject = [int(label[0]*W/2), int(label[1]*H/2), int(label[2]*W), int(label[3]*H)]\nx,y,w,h = object\nplt.subplot(3,2,5)\nplt.imshow(img[y:y+h, x:x+w])\n\nlabel = result['detection_boxes'][0][4]\nobject = [int(label[0]*W/2), int(label[1]*H/2), int(label[2]*W), int(label[3]*H)]\nx,y,w,h = object\nplt.subplot(3,2,6)\nplt.imshow(img[y:y+h, x:x+w])\n\nplt.show()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T09:13:28.981774Z","iopub.execute_input":"2022-04-12T09:13:28.98211Z","iopub.status.idle":"2022-04-12T09:13:31.142867Z","shell.execute_reply.started":"2022-04-12T09:13:28.982079Z","shell.execute_reply":"2022-04-12T09:13:31.141811Z"}}},{"cell_type":"code","source":"result['detection_scores']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\nimg = load_img('/kaggle/input/happy-whale-and-dolphin/train_images/00087baf5cef7a.jpg')\n\nprint(\"Found %d objects.\" % len(result[\"detection_scores\"]))\n\nimage_with_boxes = draw_boxes(\n    img.numpy(), result[\"detection_boxes\"],\n    None, result[\"detection_scores\"])\n\ndisplay_image(image_with_boxes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_img(path):\n  img = tf.io.read_file(path)\n  img = tf.image.decode_jpeg(img, channels=3)\n  return img\n\ndef display_image(image):\n  fig = plt.figure(figsize=(20, 15))\n  plt.grid(False)\n  plt.imshow(image)\n\ndef draw_boxes(image, boxes, class_names, scores, max_boxes=10, min_score=0.1):\n  \"\"\"Overlay labeled boxes on an image with formatted scores and label names.\"\"\"\n  colors = None #list(ImageColor.colormap.values())\n\n  try:\n    font = None # ImageFont.truetype(\"/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Regular.ttf\", 25)\n  except IOError:\n    print(\"Font not found, using default font.\")\n    font = None # ImageFont.load_default()\n\n  for i in range(min(boxes.shape[0], max_boxes)):\n    if scores[i] >= min_score:\n      ymin, xmin, ymax, xmax = tuple(boxes[i])\n      display_str = \"{}: {}%\".format(class_names[i].decode(\"ascii\"),\n                                     int(100 * scores[i]))\n      color = colors[hash(class_names[i]) % len(colors)]\n      image_pil = Image.fromarray(np.uint8(image)).convert(\"RGB\")\n      draw_bounding_box_on_image(\n          image_pil,\n          ymin,\n          xmin,\n          ymax,\n          xmax,\n          color,\n          font,\n          display_str_list=[display_str])\n      np.copyto(image, np.array(image_pil))\n  return image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone --depth 1 https://github.com/tensorflow/models","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%bash\nsudo apt install -y protobuf-compiler\ncd models/research/\nprotoc object_detection/protos/*.proto --python_out=.\ncp object_detection/packages/tf2/setup.py .\npython -m pip install .","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from object_detection.utils import label_map_util\nfrom object_detection.utils import visualization_utils as viz_utils\nfrom object_detection.utils import ops as utils_ops\n\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH_TO_LABELS = './models/research/object_detection/data/mscoco_label_map.pbtxt'\ncategory_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\n\nCOCO17_HUMAN_POSE_KEYPOINTS = [(0, 1),\n (0, 2),\n (1, 3),\n (2, 4),\n (0, 5),\n (0, 6),\n (5, 7),\n (7, 9),\n (6, 8),\n (8, 10),\n (5, 6),\n (5, 11),\n (6, 12),\n (11, 12),\n (11, 13),\n (13, 15),\n (12, 14),\n (14, 16)]\n\n\n\nlabel_id_offset = 0\nimage_np_with_detections = img_np.copy()\n\n# Use keypoints if available in detections\nkeypoints, keypoint_scores = None, None\nif 'detection_keypoints' in result:\n  keypoints = result['detection_keypoints'][0]\n  keypoint_scores = result['detection_keypoint_scores'][0]\n\nviz_utils.visualize_boxes_and_labels_on_image_array(\n      image_np_with_detections[0],\n      result['detection_boxes'][0],\n      (result['detection_classes'][0] + label_id_offset).astype(int),\n      result['detection_scores'][0],\n      category_index,\n      use_normalized_coordinates=True,\n      max_boxes_to_draw=200,\n      min_score_thresh=.30,\n      agnostic_mode=False,\n      keypoints=keypoints,\n      keypoint_scores=keypoint_scores,\n      keypoint_edges=COCO17_HUMAN_POSE_KEYPOINTS)\n\nplt.figure(figsize=(24,32))\nplt.imshow(image_np_with_detections[0])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}