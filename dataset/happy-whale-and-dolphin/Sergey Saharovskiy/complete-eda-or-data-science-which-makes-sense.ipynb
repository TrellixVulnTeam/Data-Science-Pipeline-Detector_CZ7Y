{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## <p style=\"background-color:#D9EDF7; font-family:newtimeroman; color:#31708F; font-size:120%; text-align:center; border: 2px; border-style:solid; border-color:#31708F; border-radius: 24px 0;\">Import of Libraries</p>\n<a id=\"Title\"></a>","metadata":{}},{"cell_type":"code","source":"!pip install gapminder -q\n!pip install openpyxl -q","metadata":{"execution":{"iopub.status.busy":"2022-03-08T17:44:02.45893Z","iopub.execute_input":"2022-03-08T17:44:02.459233Z","iopub.status.idle":"2022-03-08T17:44:23.120629Z","shell.execute_reply.started":"2022-03-08T17:44:02.4592Z","shell.execute_reply":"2022-03-08T17:44:23.119824Z"},"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class color:\n    \"\"\"\n    Sets colors for printouts.\n    \"\"\"\n    GREEN = '\\033[92m'\n    YELLOW = '\\033[93m'\n    RED = '\\033[91m'\n    BOLD = '\\033[1m'\n    UNDERLINE = '\\033[4m'\n    END = '\\033[0m'\n\n\n# Printouts color scheme.\ng_, y_, r_ = color.GREEN, color.YELLOW, color.RED\nbd_, un_, end_ = color.BOLD, color.UNDERLINE, color.END\nyb_, gb_,  = bd_+y_, bd_+g_\ngbu_ = gb_+un_\ncmap_ = ['#007427', '#B27D12']","metadata":{"execution":{"iopub.status.busy":"2022-03-08T17:43:55.227299Z","iopub.execute_input":"2022-03-08T17:43:55.227611Z","iopub.status.idle":"2022-03-08T17:43:55.235149Z","shell.execute_reply.started":"2022-03-08T17:43:55.227577Z","shell.execute_reply":"2022-03-08T17:43:55.234267Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom glob import glob\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom gapminder import gapminder\nfrom datetime import date\nfrom tqdm.notebook import tqdm\nfrom IPython.core.display import HTML\nfrom os.path import getsize\nimport string\nfrom PIL import Image\nimport cv2\nfrom copy import deepcopy\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(f'{yb_}\\n[INFO] Libraries set up has been completed.{end_}')","metadata":{"execution":{"iopub.status.busy":"2022-03-08T17:44:23.123459Z","iopub.execute_input":"2022-03-08T17:44:23.123721Z","iopub.status.idle":"2022-03-08T17:44:23.506686Z","shell.execute_reply.started":"2022-03-08T17:44:23.123694Z","shell.execute_reply":"2022-03-08T17:44:23.505801Z"},"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <p style=\"background-color:#D9EDF7; font-family:newtimeroman; color:#31708F; font-size:120%; text-align:center; border: 2px; border-style:solid; border-color:#31708F; border-radius: 24px 0;\">Table of Contents</p>\n1. [Introduction](#Introduction)\n2. [Dataset overview](#Dataset)\n3. [Data visualization and initial preprocessing](#Visualization)\n4. [Exploratory Data Analysis](#Exploration)\n5. [Bonus Feature Engineering](#FeatureEngineering)\n6. [Appendix](#Appendix)","metadata":{}},{"cell_type":"markdown","source":"<a id='Introduction'></a>\n# <p style=\"background-color:#D9EDF7; font-family:newtimeroman; color:#31708F; font-size:120%; text-align:center; border: 2px; border-style:solid; border-color:#31708F; border-radius: 24px 0;\">1. Introduction</p>\n\nThe aim of this competition is to build a machine learning model to identify individual marine mammals (whales). For each image presented, the model should output the whale's individual_id or, if the image corresponds to a new whale (not present in the database), identify it as a new_individual.\n\nSubmissions are evaluated according to the Mean Average Precision @ 5 (MAP@5):\n\n$$MAP@5 = \\frac{1}{U}\\sum\\limits_{u=1}^U\\sum\\limits_{k=1}^{min(n,5)}{P(k)}*{rel(k)}$$\n\nwhere ***U*** is the number of images, **P(k)**  is the precision at cutoff ***k***, ***n*** is the number predictions per image, and ***rel(k)*** is an indicator function equaling 1 if the item at rank ***k*** is a relevant (correct) label, zero otherwise.\n\nOnce a correct label has been scored for an *observation*, that label is no longer considered relevant for that observation, and additional predictions of that label are skipped in the calculation. For example, if the correct label is `A` for an observation, the following predictions all score an average precision of `1.0`.","metadata":{}},{"cell_type":"markdown","source":"<a href=\"#Title\" role=\"button\" aria-pressed=\"true\" >Back to the beginning 🔙</a>","metadata":{}},{"cell_type":"markdown","source":"<a id='Dataset'></a>\n# <p style=\"background-color:#D9EDF7; font-family:newtimeroman; color:#31708F; font-size:120%; text-align:center; border: 2px; border-style:solid; border-color:#31708F; border-radius: 24px 0;\">2. Dataset overview</p>","metadata":{}},{"cell_type":"markdown","source":"The dataset provided with this challenge consists of the following files:\n- `train.csv` containing 3 columns:\n  - `image`: image identifier, mapping to `.jpg` image files located in the `train_images` subfolder.\n  - `species`: [this indicates the basic unit of classification and a taxonomic rank of an organism in biology](https://en.wikipedia.org/wiki/Species).\n  - `individual_id`: unique whale identifier (what your model will be trained to predict given the image).\n\n\n- `sample_submission.csv`: example submission file showing the 2 column format for submissions:\n  - The first column is an `image`.\n  - The second column is a list of 5 `predictions` entries, showing your top 5 predictions for the identity of the whale shown in the image.\n  - Submission format example: \"37c7aba965a5 114207cab555 a6e325d8e924 19fbb960f07d\".\n  \nTo get a sense of the data, here are a few examples from the training dataset:\n\n<p align=\"left\">\n  <img src=\"https://drive.google.com/uc?export=view&id=1byyYzli8ckaqTSylg9YIyGrakdTSuRZW\"/>\n</p>\n\nYou can maybe already see some of the potential challenges inherent in this dataset!\n- **Whale positioning**. Whale picture can be taken from different distance and angle, a fin position can vary.\n- **Image dimensions**. Image dimensions and aspect ratios can vary, e.g. the \"top\" image is in a wide format, whereas the other two are close to 4:3 format. \n- **Image quality**. The blurriness vs. sharpness of the images can vary.\n\nLooking through other images, you'll find other sources of diversity:\n- Differences in backgrounds and lighting conditions.\n- Extraneous objects in the image such as annotations, flora in the background.\n- Whales can have a varying portion of the body out of the water.\n\nHere is a collage of some raw, unreshaped image samples from the training dataset:\n\n<p align=\"center\">\n  <img src=\"https://drive.google.com/uc?export=view&id=1N7ooaeaMnyS98YQZ7IS_oYmRl6LwPIhN\" width=\"800\"/>\n</p>","metadata":{}},{"cell_type":"markdown","source":"## <p style=\"background-color:#D9EDF7; font-family:newtimeroman; color:#31708F; font-size:120%; text-align:center; border: 2px; border-style:solid; border-color:#31708F; border-radius: 24px 0;\">Data Loading</p>","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/happy-whale-and-dolphin/train.csv')\nsub = pd.read_csv('../input/happy-whale-and-dolphin/sample_submission.csv')\ntrain.head(3)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-03-08T17:44:23.507871Z","iopub.execute_input":"2022-03-08T17:44:23.508153Z","iopub.status.idle":"2022-03-08T17:44:23.668875Z","shell.execute_reply.started":"2022-03-08T17:44:23.508114Z","shell.execute_reply":"2022-03-08T17:44:23.668022Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <p style=\"background-color:#D9EDF7; font-family:newtimeroman; color:#31708F; font-size:120%; text-align:center; border: 2px; border-style:solid; border-color:#31708F; border-radius: 24px 0;\">Quick Sanity Check</p>","metadata":{}},{"cell_type":"code","source":"p_trn = r'../input/happy-whale-and-dolphin/train_images'\np_tst = r'../input/happy-whale-and-dolphin/test_images'\ntrain_ims = os.listdir(p_trn)\ntest_ims = os.listdir(p_tst)\n\nsplt_trn = [i.split('.')[1] for i in train_ims]\nsplt_tst = [i.split('.')[1] for i in test_ims]\n\nprint(f'\\n{yb_}[+] Train_images ext.: {gb_}{set(splt_trn)}{end_}.')\nprint(f'{yb_}[+] Test_images ext.: {gb_}{set(splt_tst)}{end_}.\\n')\n\nprint(f'{yb_}[+] Number of train images: {gbu_}{len(splt_trn)}{end_}.')\nprint(f'{yb_}[+] Number of test images: {gbu_}{len(splt_tst)}{end_}.\\n')\n\nmask1 = train.isin(train_ims)\nout_of_folder = len(splt_trn) - train[mask1].shape[0]\ninter = len(set(train_ims).intersection(test_ims))\n\nprint(f'{yb_}[+] Number of images not in train.csv: {gbu_}{out_of_folder}{end_}.')\nprint(f'{yb_}[+] Images in train/test intersection: {gbu_}{inter}{end_}.')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-03-08T17:44:23.670913Z","iopub.execute_input":"2022-03-08T17:44:23.67175Z","iopub.status.idle":"2022-03-08T17:44:24.721459Z","shell.execute_reply.started":"2022-03-08T17:44:23.671704Z","shell.execute_reply":"2022-03-08T17:44:24.720443Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"#Title\" role=\"button\" aria-pressed=\"true\" >Back to the beginning 🔙</a>","metadata":{}},{"cell_type":"markdown","source":"<a name=\"Visualization\"></a>\n\n# <p style=\"background-color:#D9EDF7; font-family:newtimeroman; color:#31708F; font-size:120%; text-align:center; border: 2px; border-style:solid; border-color:#31708F; border-radius: 24px 0;\">3. Data visualization and initial preprocessing</p>","metadata":{}},{"cell_type":"markdown","source":"## <p style=\"background-color:#D9EDF7; font-family:newtimeroman; color:#31708F; font-size:120%; text-align:center; border: 2px; border-style:solid; border-color:#31708F; border-radius: 24px 0;\">Data Cleaning</p>","metadata":{}},{"cell_type":"markdown","source":"**Fixing Duplicate Labels:**\n\n - `bottlenose_dolpin` => `bottlenose_dolphi`.\n - `kiler_whale` => `killer_whale`.\n - `beluga` => `beluga_whale`.\n\n**Changing Label due to extreme similarities:**\n    \n - `globis & pilot_whale` => `short_finned_pilot_whale`.\n \n**References:**\n    \n - [Discussion: \"Fix all known species column problems\"](https://www.kaggle.com/c/happy-whale-and-dolphin/discussion/305574) by @kwentar.\n\n - [Notebook: \"Happywhale -⚡️ EDA + Augmentation + CNN 🔥🔥\"](https://www.kaggle.com/sahamed/happywhale-eda-augmentation-cnn) by @ahamed.\n\n - [Discussion: \"'short_finned' vs 'long_finned' vs 'pilot_whale'\"](https://www.kaggle.com/c/happy-whale-and-dolphin/discussion/305909) by @andradaolteanu.","metadata":{}},{"cell_type":"code","source":"before = train.species.unique()\n\ntrain.species.replace({\n    \"globis\": \"short_finned_pilot_whale\",\n    \"pilot_whale\": \"short_finned_pilot_whale\",\n    \"kiler_whale\": \"killer_whale\",\n    \"bottlenose_dolpin\": \"bottlenose_dolphin\",\n    'beluga' : 'beluga_whale'\n}, inplace=True)\n\nafter = train.species.unique()\n\nprint(\n    f'\\n{yb_}[+] Before fixing duplicate labels,'\n    f' unique species: {gbu_}{before.shape[0]}{end_}.'\n)\nprint(\n    f'{yb_}[+] After fixing duplicate labels,'\n    f' unique species: {gbu_}{after.shape[0]}{end_}.'\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-08T17:44:24.7226Z","iopub.execute_input":"2022-03-08T17:44:24.722817Z","iopub.status.idle":"2022-03-08T17:44:24.765401Z","shell.execute_reply.started":"2022-03-08T17:44:24.722791Z","shell.execute_reply":"2022-03-08T17:44:24.764413Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In total we have had 30 different species in the dataset. Having followed the recommendation provided on the discussions and in the kernels, we have removed the extra species, bringing the total number to 26.","metadata":{}},{"cell_type":"markdown","source":"<a href=\"#Title\" role=\"button\" aria-pressed=\"true\" >Back to the beginning 🔙</a>","metadata":{}},{"cell_type":"markdown","source":"## <p style=\"background-color:#D9EDF7; font-family:newtimeroman; color:#31708F; font-size:120%; text-align:center; border: 2px; border-style:solid; border-color:#31708F; border-radius: 24px 0;\">Data Enriching: Adding Species Summary</p>\n\nThere are multiple notebooks where the authors focus on describing the species in the details or showing the species separately. What has not been done yet, is an introduction of detailed summary of the species. I have encompassed the most important information down below.\n\n**Few References:**\n    \n - [Notebook: \"🐋 and 🐬 Identification: EDA + Augmentation\"](https://www.kaggle.com/ruchi798/and-identification-eda-augmentation) by @ruchi798.\n\n - [Notebook: \"What about species?\"](https://www.kaggle.com/kwentar/what-about-species)  by @kwentar.","metadata":{}},{"cell_type":"code","source":"p = '../input/whales-summary/Whales Summary.xlsx'\nwhales_summary = pd.read_excel(p, engine='openpyxl')\nwhales_summary.head(5)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-03-08T17:44:28.566693Z","iopub.execute_input":"2022-03-08T17:44:28.567396Z","iopub.status.idle":"2022-03-08T17:44:28.83875Z","shell.execute_reply.started":"2022-03-08T17:44:28.567347Z","shell.execute_reply":"2022-03-08T17:44:28.837789Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"order = [\n    'species', 'species_latin', 'size_to_human_img',\n    'body_length', 'body_weight','conserv_status', \n    'concerv_dscr', 'lifespan', 'size_to_human',\n    'img', 'wiki_link'\n]\n\nwhales_summary = whales_summary[order]","metadata":{"execution":{"iopub.status.busy":"2022-03-08T17:46:52.864298Z","iopub.execute_input":"2022-03-08T17:46:52.865211Z","iopub.status.idle":"2022-03-08T17:46:52.875204Z","shell.execute_reply.started":"2022-03-08T17:46:52.865164Z","shell.execute_reply":"2022-03-08T17:46:52.874484Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I have rendered the species summary table down below. Please, click on the images in the table to expand it. You can find useful to read about the table rendering by using the reference below.\n\n**Reference:** \n - [Article: \"Rendering Images inside a Pandas DataFrame\"](https://towardsdatascience.com/rendering-images-inside-a-pandas-dataframe-3631a4883f60) by Tanu N Prabhu.","metadata":{}},{"cell_type":"code","source":"def path_to_image_html(path):\n    return '<img src=\"'+ path + '\" width=\"90\" >'\n\ndef make_clickable(link):\n    return '<a href=\"%s\"target=\"_blank\">%s</a>' % (link, link)\n\nfrmts = dict(\n    size_to_human_img=path_to_image_html, \n    img=path_to_image_html, \n    wiki_link=make_clickable\n)\n\nHTML(whales_summary.to_html(escape=False, formatters=frmts))","metadata":{"execution":{"iopub.status.busy":"2022-03-08T17:47:09.637436Z","iopub.execute_input":"2022-03-08T17:47:09.637741Z","iopub.status.idle":"2022-03-08T17:47:09.655812Z","shell.execute_reply.started":"2022-03-08T17:47:09.637702Z","shell.execute_reply":"2022-03-08T17:47:09.654865Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Fields and Unit of Measurement:**\n\n - `body_length` is given in Meters.\n - `body_weight` is given in Metric Tones.\n - `lifespan` is given in years.\n - `size_to_human` is given as ratio upper bound of `body_length` divided by the average human height of 1.7526 Meters.\n","metadata":{}},{"cell_type":"markdown","source":"<a href=\"#Title\" role=\"button\" aria-pressed=\"true\" >Back to the beginning 🔙</a>","metadata":{}},{"cell_type":"markdown","source":"## <p style=\"background-color:#D9EDF7; font-family:newtimeroman; color:#31708F; font-size:120%; text-align:center; border: 2px; border-style:solid; border-color:#31708F; border-radius: 24px 0;\">Data Enriching: Adding Image Size, Subset, Size Bins, Height, Width and Aspect Ratio</p>","metadata":{}},{"cell_type":"code","source":"def size_get(df, im_dir):\n    \"\"\"\n    Gets a size in MB per each image in a list.\n    Creates 'image_size' col in the source DataFrame.\n    :param df: pd.DataFrame with col 'image' (base names)\n    :param im_dir: str\n    :return: pd.DataFrame enriched\n    \"\"\"\n    \n    f = lambda x: os.path.join(im_dir, x)\n    im_pathes = df.image.apply(f)\n    im_sizes = [getsize(p)/1e6 for p in tqdm(im_pathes)]\n    df['image_size'] = im_sizes\n    \n    return df\n\n\ndef sizebin_get(df):\n    \"\"\"\n    Enriches DataFrame with size_bin col, e.g. '6>x=>5' MB.\n    :param df: pd.DataFrame with col 'image_size' (MB)\n    \"\"\"\n    \n    sz = df.image_size\n    \n    df.loc[(sz >= 5) & (sz < 6), ['size_bin']] = '6>x=>5'  \n    df.loc[(sz >= 4) & (sz < 5), ['size_bin']] = '5>x=>4'  \n    df.loc[(sz >= 3) & (sz < 4), ['size_bin']] = '4>x=>3'  \n    df.loc[(sz >= 2) & (sz < 3), ['size_bin']] = '3>x=>2'  \n    df.loc[(sz >= 1) & (sz < 2), ['size_bin']] = '2>x=>1'  \n    df.loc[sz < 1, ['size_bin']] = 'x<1' \n    \n    return df\n    \n\ndef h_w_get(df):\n    \"\"\"\n    Gets height and width for each image in a list.\n    Creates 'height', 'width' and 'subset' cols in the source DataFrame.\n    :param df: pd.DataFrame with col 'image' (base names)\n    :return: pd.DataFrame enriched\n    \"\"\"\n        \n    N = df.shape[0]\n    df['height'] = np.ones_like(N)\n    df['width'] = np.ones_like(N)\n    \n    for base in tqdm(df.image, total=N):\n        \n        mask = df.image == base\n        subset = df[mask]['subset'].values[0]\n        dir_ = f'data/{subset}_images'\n        path = os.path.join(dir_, base)\n        \n        im = Image.open(path)\n        width = im.size[0]\n        height = im.size[1]\n        \n        df.loc[mask, 'width'] = width\n        df.loc[mask, 'height'] = height\n        \n    return df\n\n\n# Constructs a dummy train and test DataFrame.\n# Demonstrates how the size_get function works.\ntest_dummy = pd.DataFrame({'image': test_ims})\ntrain_dummy = deepcopy(size_get(train, p_trn))\ntest_dummy = size_get(test_dummy, p_tst)\ntrain.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T00:42:08.947604Z","iopub.execute_input":"2022-02-25T00:42:08.948204Z","iopub.status.idle":"2022-02-25T00:44:09.284421Z","shell.execute_reply.started":"2022-02-25T00:42:08.948173Z","shell.execute_reply":"2022-02-25T00:44:09.283537Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the sake of time saving and the notebook performance, I have commented out the data enriching functions for obtaining the size bins, height and width of images. Instead, I am going to read in the already prepared table.","metadata":{}},{"cell_type":"code","source":"# Creates a dummy  table with cols 'image', 'image_size'.\ncols = ['image', 'image_size']\ndf_images = pd.concat([train_dummy[cols], test_dummy[cols]])\ndf_images['subset'] = np.ones_like(df_images.shape[0])\n\n# df_images = sizebin_get(df)\n# df_images = h_w_get(df_images)\n\ndf_images = pd.read_csv('../input/image-sizes/image_sizes.csv')\ndf_images['aspect_ratio'] = df_images['width']/df_images['height']\ndf_images.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T00:44:09.285856Z","iopub.execute_input":"2022-02-25T00:44:09.286117Z","iopub.status.idle":"2022-02-25T00:44:09.485663Z","shell.execute_reply.started":"2022-02-25T00:44:09.286088Z","shell.execute_reply":"2022-02-25T00:44:09.485104Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"#Title\" role=\"button\" aria-pressed=\"true\" >Back to the beginning 🔙</a>","metadata":{}},{"cell_type":"markdown","source":"## <p style=\"background-color:#D9EDF7; font-family:newtimeroman; color:#31708F; font-size:120%; text-align:center; border: 2px; border-style:solid; border-color:#31708F; border-radius: 24px 0;\">Data Enriching: Final Train Table Enriched</p>","metadata":{}},{"cell_type":"code","source":"train.drop(columns=['image_size'], inplace=True)\nmask_subset = (df_images.subset == 'train')\ndf_images_tr = df_images[mask_subset]\ntrain_enrch = train.merge(df_images_tr, how='left', on='image')\ntrain_enrch = train_enrch.merge(whales_summary, how='left', on='species')","metadata":{"execution":{"iopub.status.busy":"2022-02-25T00:44:09.486711Z","iopub.execute_input":"2022-02-25T00:44:09.487017Z","iopub.status.idle":"2022-02-25T00:44:09.613992Z","shell.execute_reply.started":"2022-02-25T00:44:09.48699Z","shell.execute_reply":"2022-02-25T00:44:09.613216Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_enrch","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is the final view of the dateset with the extra metadata. I am going to use it during the data exploration phase.","metadata":{}},{"cell_type":"markdown","source":"<a href=\"#Title\" role=\"button\" aria-pressed=\"true\" >Back to the beginning 🔙</a>","metadata":{}},{"cell_type":"markdown","source":"<a name=\"Exploration\"></a>\n\n# <p style=\"background-color:#D9EDF7; font-family:newtimeroman; color:#31708F; font-size:120%; text-align:center; border: 2px; border-style:solid; border-color:#31708F; border-radius: 24px 0;\">4. Exploratory Data Analysis</p>\n\nObservations from the visualization and preprocessing part:\n\n* The image dataset consists of two major subsets:\n - `train images`: **51033** image(s).\n - `test images`: **27956** image(s).\n\n\n* The train dataset is relatively big, with the imbalanced number of images per `species` and `invividual_id`.\n\n\n---\n* Let's question our data:\n\n 1. What is the `species` and `invividual_id` statistics?\n 2. What is the memory usage per images, species and major subsets?\n 3. Are there outliers based on memory usage?\n 4. Are there outliers based on aspect ratio?\n 5. Are there blurred or distorted images? ","metadata":{}},{"cell_type":"markdown","source":"## <p style=\"background-color:#D9EDF7; font-family:newtimeroman; color:#31708F; font-size:120%; text-align:center; border: 2px; border-style:solid; border-color:#31708F; border-radius: 24px 0;\">4.1 What is the species and invividual_id statistics?</p>","metadata":{}},{"cell_type":"code","source":"species_order = train['species'].value_counts().index\nspecies_count = train.species.value_counts()\n\nplt.figure(figsize=(10, 6))\ns = sns.countplot(\n    data=train, y='species', palette='crest',\n    order=species_order\n)\n\ns.spines['top'].set_visible(False)\ns.spines['right'].set_visible(False)\n\nfor i, v in enumerate(species_count):\n    s.text(v, i+0.2, str(v), color='black', \n           fontweight='bold', fontsize=12)\n    \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-25T00:44:09.615449Z","iopub.execute_input":"2022-02-25T00:44:09.615774Z","iopub.status.idle":"2022-02-25T00:44:10.17506Z","shell.execute_reply.started":"2022-02-25T00:44:09.615735Z","shell.execute_reply":"2022-02-25T00:44:10.174218Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Observation:\n - The species are extremely imbalanced.\n* Assumption:\n - Focal loss might be helpful.\n - Data oversampling or downsampling  might be helpful.","metadata":{}},{"cell_type":"markdown","source":"Let's take a look at `individual_id`. I have used a \"dolphin vs whale\" mapping introduced in the notebook below.\n\n**References:**\n    \n - [Notebook: \"🐋 and 🐬 Identification: EDA + Augmentation\"](https://www.kaggle.com/ruchi798/and-identification-eda-augmentation) by @ruchi798.","metadata":{}},{"cell_type":"code","source":"f_ = lambda x: 'dolphin' if 'dolphin' in x else 'whale'\ntrain_enrch[''] = train_enrch.species.map(f_)\nfig, axes  = plt.subplots(figsize=(8, 4))\nfig.suptitle('Whales vs Dolphins', size=20, weight='bold', font='Serif')\n\nexplode = (0.05, 0.05)\nlabels = list(train_enrch[''].value_counts().index)\nsizes = train_enrch[''].value_counts().values\ntext_props = {'fontsize': 12, 'weight': 'bold', 'font': 'Serif'}\n\naxes.pie(\n    sizes, explode=explode, startangle=60, \n    labels=labels, autopct='%1.0f%%', pctdistance=0.7, \n    colors=cmap_, textprops=text_props)\n\naxes.add_artist(plt.Circle((0,0),0.4,fc='white'))\n\nplt.show()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-02-25T00:44:10.176463Z","iopub.execute_input":"2022-02-25T00:44:10.176771Z","iopub.status.idle":"2022-02-25T00:44:10.358644Z","shell.execute_reply.started":"2022-02-25T00:44:10.17673Z","shell.execute_reply":"2022-02-25T00:44:10.357496Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask_dolphin = (train_enrch[''] == 'dolphin')\nmask_whale = (train_enrch[''] == 'whale')\ntrain_iid_d = train_enrch[mask_dolphin].individual_id.value_counts()[:100]\ntrain_iid_w = train_enrch[mask_whale].individual_id.value_counts()[:100]\n\ndf_stack = [train_iid_d, train_iid_w]\ndf_names = ['Train images per dolphin', 'Train images per whale']\n\ndef set_xlabel(df, df_name):\n    mean = round(np.mean(df), 2)\n    median = int(np.median(df))\n    f_str = f'\\n{df_name} mean: {mean}, median {median}.'\n    return f_str\n\nfig, axes = plt.subplots(ncols=2, figsize=(12, 5))\nfig.suptitle(\n    'Whales vs Dolphins KDE TOP(100)', size=20, \n    weight='bold', font='Serif'\n)\n\nfor i, (df, df_name) in enumerate(zip(df_stack, df_names)):\n\n    xlabel = set_xlabel(df, df_name)\n    sns.histplot(df, color='#007427', stat='density', ax=axes[i])\n    sns.kdeplot(df, color=\"r\", ls='--', lw=2, ax=axes[i])\n    axes[i].set_xlabel(xlabel)\n    axes[i].set_xlim([0, 200])\n    \n    \nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-25T00:44:10.364138Z","iopub.execute_input":"2022-02-25T00:44:10.364964Z","iopub.status.idle":"2022-02-25T00:44:11.154273Z","shell.execute_reply.started":"2022-02-25T00:44:10.364906Z","shell.execute_reply":"2022-02-25T00:44:11.153495Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(ncols=2, figsize=(20, 21))\nfig.suptitle(\n    'Whales vs Dolphins INDIVIDUAL_ID COUNT TOP(100)', size=20, \n    weight='bold', font='Serif'\n)\n\nfor i, (df, df_name) in enumerate(zip(df_stack, df_names)):\n\n    sns.barplot(x=df, y=df.index, ax=axes[i],\n              palette='crest', orient='horizontal')\n\n    title = set_xlabel(df, df_name)\n    axes[i].set_title(title)\n    for j, v in enumerate(df):\n        axes[i].text(v+1, j+0.3, str(v), \n                     color='black', fontweight='bold')\n        \nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-25T00:44:11.155401Z","iopub.execute_input":"2022-02-25T00:44:11.155609Z","iopub.status.idle":"2022-02-25T00:44:15.498673Z","shell.execute_reply.started":"2022-02-25T00:44:11.155583Z","shell.execute_reply":"2022-02-25T00:44:15.498034Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Observations and clarification:\n - The data is pulled for TOP(100) dolphins and whales.\n - `individual_ids` are imbalanced for both whales and dolphins species.\n - `individual_ids` for whales are distributed differently in comparison to dolphins.\n - `individual_ids` for whales are overrepresented in comparison to dolphins.\n \n* Assumption:\n - It might be helpful to drill the acquired metadata to find patterns and nature of these differences.","metadata":{}},{"cell_type":"markdown","source":"<a href=\"#Title\" role=\"button\" aria-pressed=\"true\" >Back to the beginning 🔙</a>","metadata":{}},{"cell_type":"markdown","source":"## <p style=\"background-color:#D9EDF7; font-family:newtimeroman; color:#31708F; font-size:120%; text-align:center; border: 2px; border-style:solid; border-color:#31708F; border-radius: 24px 0;\">4.2 What is the memory usage per images, species and major subsets?</p>","metadata":{}},{"cell_type":"code","source":"a = train_dummy.describe().T\nb = test_dummy.describe().T\n\nnew_index = {0: 'train', 1: 'test'}\nfloat_fmt = {'count': '{:.0f}'}\nsize_stats = pd.concat([a, b], ignore_index=True).rename(new_index)\nsize_stats.style.background_gradient(cmap='Greens_r').format(float_fmt)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T00:44:15.49972Z","iopub.execute_input":"2022-02-25T00:44:15.500278Z","iopub.status.idle":"2022-02-25T00:44:15.596954Z","shell.execute_reply.started":"2022-02-25T00:44:15.500244Z","shell.execute_reply":"2022-02-25T00:44:15.596355Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def size_scatter(df, c=None, verbose=True):\n    \"\"\"\n    Draws a sactter plot of image sizes.\n    Prints out image count for a specific size bin.\n    :param df: pd.DataFrame with col image_size.\n    :param c: str (plot color optional)\n    :param verbose: bool (printouts)\n    :return: None\n    \"\"\"\n    sns.scatterplot(x=df.index, y=df.image_size, color=c)\n    plt.title('Sizes of image files in MB.')\n    plt.show()\n    print('\\n')\n    \n    if verbose:\n        for size in range(6, -1, - 1):\n            mask1 = (df.image_size >= size)\n            mask2 = (df.image_size < size + 1)\n            total = df[mask1 & mask2].shape[0]\n            print(\n              f'{yb_}[+] Images {gb_}{size+1}{yb_}'\n              f' > size >= {gb_}{size} {yb_}MB: {gbu_}{total}{end_}.'\n            )\n    \n    \nsize_scatter(train_dummy, c=cmap_[0])\nsize_scatter(test_dummy, c=cmap_[1])","metadata":{"execution":{"iopub.status.busy":"2022-02-25T00:44:15.598258Z","iopub.execute_input":"2022-02-25T00:44:15.598474Z","iopub.status.idle":"2022-02-25T00:44:16.21605Z","shell.execute_reply.started":"2022-02-25T00:44:15.598448Z","shell.execute_reply":"2022-02-25T00:44:16.21521Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask3 = df_images.image.isin(train_dummy.image)\nmask4 = df_images.image.isin(test_dummy.image)\ndf_images.loc[mask3, ['subset']] = 'train'\ndf_images.loc[mask4, ['subset']] = 'test'\n\nplt.figure(figsize=(10, 6))\nsns.scatterplot(\n    data=df_images, x=df_images.index, y='image_size', \n    hue='subset', palette=cmap_\n)\nplt.title('Sizes of image files in MB by subset.')\nplt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\nplt.show()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-02-25T00:44:16.217497Z","iopub.execute_input":"2022-02-25T00:44:16.217715Z","iopub.status.idle":"2022-02-25T00:44:18.600096Z","shell.execute_reply.started":"2022-02-25T00:44:16.217689Z","shell.execute_reply":"2022-02-25T00:44:18.599176Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Observation:\n\n It looks that the `train` and `test` image sizes share the same distribution. In this aspect the images are homogenous. `train` and `test` shares almost identical mean, std and min values, slightly vary in max stats.","metadata":{}},{"cell_type":"markdown","source":"<a href=\"#Title\" role=\"button\" aria-pressed=\"true\" >Back to the beginning 🔙</a>","metadata":{}},{"cell_type":"markdown","source":"## <p style=\"background-color:#D9EDF7; font-family:newtimeroman; color:#31708F; font-size:120%; text-align:center; border: 2px; border-style:solid; border-color:#31708F; border-radius: 24px 0;\">4.2*Bonus image Width x Height plotting</p>","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(ncols=1, figsize=(12, 9))\nsns.scatterplot(data=train_enrch, x='width', y='height', hue='species')\naxes.set_title(f'Image Height x Width by species.')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-25T00:44:18.601513Z","iopub.execute_input":"2022-02-25T00:44:18.60175Z","iopub.status.idle":"2022-02-25T00:44:23.034127Z","shell.execute_reply.started":"2022-02-25T00:44:18.601721Z","shell.execute_reply":"2022-02-25T00:44:23.033238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(ncols=1, figsize=(12, 6))\nsns.scatterplot(data=train_enrch, x='width', y='height', hue='', palette=cmap_)\naxes.set_title(f'Image Height x Width Dolphin vs Whale.')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-25T00:44:23.035544Z","iopub.execute_input":"2022-02-25T00:44:23.035794Z","iopub.status.idle":"2022-02-25T00:44:25.252233Z","shell.execute_reply.started":"2022-02-25T00:44:23.035763Z","shell.execute_reply":"2022-02-25T00:44:25.251345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(ncols=2, figsize=(14, 6))\nfor i, col in enumerate(['subset', 'size_bin']):\n    sns.scatterplot(data=df_images, x='width', \n                  y='height', hue=col, ax=axes[i])\n    axes[i].set_title(f'Image Height x Width by {col}.')\n\nplt.tight_layout()\nplt.show()\n\n# Counstructs a list of subsets for\n# masking data by a subset and creates plots.\n_subsets = ['train', 'test']\n\nplt.figure(figsize=(14, 12))\nfor i, subset in enumerate(_subsets):\n    ax = plt.subplot(2, 2, i + 1)\n    mask6 = df_images.subset == subset\n    sns.scatterplot(data=df_images[mask6], x='width', \n                  y='height', hue='size_bin', ax=ax)\n\n    # Basic stats:\n    mean_h = df_images[mask6]['height'].mean()\n    mean_w = df_images[mask6]['width'].mean()\n    median_h = df_images[mask6]['height'].median()\n    median_w = df_images[mask6]['width'].median()\n\n    ax.set_title(\n      f'Image Height x Width by subset: {subset}.\\n'\n      f'H x W mean: ({mean_h:.2f}, {mean_w:.2f}).\\n'\n      f'H x W median: ({median_h:.2f}, {median_w:.2f}).'\n    )\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-25T00:44:25.253616Z","iopub.execute_input":"2022-02-25T00:44:25.25386Z","iopub.status.idle":"2022-02-25T00:44:34.552287Z","shell.execute_reply.started":"2022-02-25T00:44:25.253828Z","shell.execute_reply":"2022-02-25T00:44:34.551481Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Observations and clarification:\n - The charts are lean to the right toward bigger width over height, e.g. aspect ratio > 1.\n - Seems there is a correlation between wide format images and the whales.\n - The Dolphins images are less wider than whales ones.\n - Most of the images are less than 1 MB in size.\n* Assumption:\n - It might be helpful to look at the pictures of different bins and outliers based on the image size.  \n - It might be helpful to drill the acquired metadata to find patterns and nature of these differences.","metadata":{}},{"cell_type":"markdown","source":"<a href=\"#Title\" role=\"button\" aria-pressed=\"true\" >Back to the beginning 🔙</a>","metadata":{}},{"cell_type":"markdown","source":"## <p style=\"background-color:#D9EDF7; font-family:newtimeroman; color:#31708F; font-size:120%; text-align:center; border: 2px; border-style:solid; border-color:#31708F; border-radius: 24px 0;\">4.3 Are there outliers based on memory usage?</p>","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(14, 5))\nfor i in range(5, 0, -1):\n\n    mask5 = df_images.image_size > i\n    a = df_images[mask5].subset.value_counts()\n\n    plt.subplot(3, 2, 5-i+1)\n    p = sns.barplot(x=a, y=a.index, orient='horizontal', palette=cmap_)\n    for j, v in enumerate(a):\n        p.text(v+0.2, j+0.1, str(v), \n                color='black', fontweight='bold')\n\n    plt.title(f'Images by subsets: {i+1} > size >= {i} MB.')\n    p.spines['top'].set_visible(False)\n    p.spines['right'].set_visible(False)\n    \nplt.tight_layout()\nplt.show()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-02-25T00:44:34.553546Z","iopub.execute_input":"2022-02-25T00:44:34.554361Z","iopub.status.idle":"2022-02-25T00:44:35.270773Z","shell.execute_reply.started":"2022-02-25T00:44:34.554318Z","shell.execute_reply":"2022-02-25T00:44:35.269922Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Observations:\n - There are not many extreme size cases. \n - Most of the cases are in the train dataset.\n* Assumption:\n - It might be helpful to look at the outliers based on the image size. ","metadata":{}},{"cell_type":"code","source":"def plot_images(df_images, sample_size, field='image_size', field_threshold=None):\n    \"\"\"\n    Plots a grid of images based on a field.\n    :param df_images: pd.DataFrame (col1: image, col2: field, col3: subset)\n    :param sample_size: int\n    :param field_threshold: None > tuple(lower_bound, upper_bound)\n    :return: None\n    \"\"\"\n\n    if field_threshold:\n        mask_lower = df_images[field] > field_threshold[0]\n        mask_upper = df_images[field] < field_threshold[1]\n        outliers = df_images[mask_lower & mask_upper]\n        outliers = outliers.sort_values(by=field, ascending=False)\n        samples = outliers.sample(sample_size)\n    else: \n        samples = df_images.sample(sample_size)\n\n    plt_size = int(np.sqrt(sample_size))\n    if plt_size**2 < sample_size:\n        plt_size = plt_size + 1\n\n    figsize = (plt_size*6, plt_size*6)\n    fig = plt.figure(figsize=figsize)\n    fig.suptitle(\n        f'Images by {field}, threshold: {field_threshold}', \n        size=20, weight='bold', font='Serif'\n    )\n    \n    _zip = zip(\n    samples.image, \n    samples[field], \n    samples.subset\n    )\n    \n    dir_ = '../input/happy-whale-and-dolphin/'\n    for idx, (image_id, size, subset) in enumerate(_zip):\n        plt.subplot(plt_size, plt_size, idx + 1)\n        image = cv2.imread(os.path.join(f'{dir_}{subset}_images', image_id))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        plt.imshow(image)\n        \n        if field == 'image_size':\n            title = f'Image: {image_id}, {field}: {size:.2f} MB,\\n subest: {subset}.'\n        else:\n            title = f'Image: {image_id}, {field}: {size:.2f},\\n subest: {subset}.'\n        plt.title(title, fontdict={'fontweight': 'bold'})\n        plt.axis(\"off\")\n        \n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-25T00:44:35.272187Z","iopub.execute_input":"2022-02-25T00:44:35.272421Z","iopub.status.idle":"2022-02-25T00:44:35.284939Z","shell.execute_reply.started":"2022-02-25T00:44:35.272393Z","shell.execute_reply":"2022-02-25T00:44:35.284125Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_images(df_images, 9, field_threshold=(3, 6))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-02-25T00:44:35.286188Z","iopub.execute_input":"2022-02-25T00:44:35.286425Z","iopub.status.idle":"2022-02-25T00:44:46.037743Z","shell.execute_reply.started":"2022-02-25T00:44:35.286397Z","shell.execute_reply":"2022-02-25T00:44:46.036997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_images(df_images, 9, field_threshold=(1, 2))","metadata":{"execution":{"iopub.status.busy":"2022-02-25T00:44:46.038819Z","iopub.execute_input":"2022-02-25T00:44:46.039245Z","iopub.status.idle":"2022-02-25T00:44:56.132252Z","shell.execute_reply.started":"2022-02-25T00:44:46.039202Z","shell.execute_reply":"2022-02-25T00:44:56.131273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_images(df_images, 9, field_threshold=(0, 0.003))","metadata":{"execution":{"iopub.status.busy":"2022-02-25T00:44:56.13362Z","iopub.execute_input":"2022-02-25T00:44:56.134386Z","iopub.status.idle":"2022-02-25T00:44:57.072206Z","shell.execute_reply.started":"2022-02-25T00:44:56.134346Z","shell.execute_reply":"2022-02-25T00:44:57.071298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"#Title\" role=\"button\" aria-pressed=\"true\" >Back to the beginning 🔙</a>","metadata":{}},{"cell_type":"markdown","source":"## <p style=\"background-color:#D9EDF7; font-family:newtimeroman; color:#31708F; font-size:120%; text-align:center; border: 2px; border-style:solid; border-color:#31708F; border-radius: 24px 0;\">4.4 Are there outliers based on aspect ratio?</p>","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\n\n# Vertical line for the square images.\nplt.vlines(\n    x=1, ymin=0, ymax=df_images.shape[0], colors='red', \n    ls=':', lw=4, label='aspect_ratio: 1'\n)\n\n# Vertical line for extremely wide images.\nplt.vlines(\n    x=10, ymin=0, ymax=df_images.shape[0], colors='red', \n    ls='--', lw=3, label='aspect_ratio: 10')\n\nsns.scatterplot(\n    data=df_images, x=df_images['aspect_ratio'], y=df_images.index, \n    hue='subset', palette=cmap_\n)\n\nplt.legend(\n    bbox_to_anchor=(1.02, 1), \n    loc='upper left', \n    borderaxespad=0\n)\n\nplt.ylim(0, 77000)\nplt.xticks(list(range(15)))\nplt.title('Aspect ratio of image files by subset.')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-25T00:44:57.073786Z","iopub.execute_input":"2022-02-25T00:44:57.074713Z","iopub.status.idle":"2022-02-25T00:44:59.234539Z","shell.execute_reply.started":"2022-02-25T00:44:57.074646Z","shell.execute_reply":"2022-02-25T00:44:59.233333Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"c = df_images[df_images.subset=='train'].describe()\nd = df_images[df_images.subset=='test'].describe()\n\n# Rearranges and renames the header.\ncols = [\n    'image_size_x', 'image_size_y', \n    'width_x', 'width_y', \n    'height_x', 'height_y',\n    'aspect_ratio_x', 'aspect_ratio_y'\n]\n\nnew_cols = dict()\nfor i in cols:\n    if i[-1] == 'x':\n        value = f'{i[:-1]}train'\n        new_cols[i] = value\n    else:\n        value = f'{i[:-1]}test'\n        new_cols[i] = value\n\nmrgd = c.merge(d, how='left', left_index=True, right_index=True)[cols]\nmrgd.rename(columns=new_cols).style\\\n.background_gradient(cmap='YlGn', subset=['image_size_train','image_size_test'])\\\n.background_gradient(cmap='Greens_r', subset=['width_train','width_test'])\\\n.background_gradient(cmap='YlGn', subset=['height_train','height_test']).format('{:.3f}')\\\n.background_gradient(cmap='YlGn', subset=['aspect_ratio_train', 'aspect_ratio_test']).format('{:.3f}')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-02-25T00:44:59.236046Z","iopub.execute_input":"2022-02-25T00:44:59.23659Z","iopub.status.idle":"2022-02-25T00:44:59.3354Z","shell.execute_reply.started":"2022-02-25T00:44:59.236546Z","shell.execute_reply":"2022-02-25T00:44:59.334494Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Observations:\n - Basic stats for the `train` and `test` images is almost indistinguishable. \n - There are extreme cases of aspect ration in the dataset (aspect ratio > 10).\n - The entire dataset tends toward wide format images.\n* Assumption:\n - It might be helpful to look at the outliers based on the aspect ratio. ","metadata":{}},{"cell_type":"code","source":"plot_images(df_images, 6, field='aspect_ratio', field_threshold=(13, 15))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-02-25T00:44:59.33657Z","iopub.execute_input":"2022-02-25T00:44:59.336797Z","iopub.status.idle":"2022-02-25T00:45:00.399Z","shell.execute_reply.started":"2022-02-25T00:44:59.336769Z","shell.execute_reply":"2022-02-25T00:45:00.398131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_images(df_images, 9, field='aspect_ratio', field_threshold=(0.4, 0.6))","metadata":{"execution":{"iopub.status.busy":"2022-02-25T00:45:00.400247Z","iopub.execute_input":"2022-02-25T00:45:00.400482Z","iopub.status.idle":"2022-02-25T00:45:02.633007Z","shell.execute_reply.started":"2022-02-25T00:45:00.400453Z","shell.execute_reply":"2022-02-25T00:45:02.632139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Observations:\n - By looking at the extreme cases of aspect ratio, it is easy to notice that the wide pictures are common for the big whales. \n - Most of the dolphin pictures are taken on edge devices. It seems reasonable enough to think that the huge whale encounter is not common and the pictures are taken panoramically from the distance, most likely with the professional camera.\n* Assumption:\n - If we prove the correlation between size of the whale and the aspect ratio it might be used during the posprocessing. ","metadata":{}},{"cell_type":"markdown","source":"<a href=\"#Title\" role=\"button\" aria-pressed=\"true\" >Back to the beginning 🔙</a>","metadata":{}},{"cell_type":"markdown","source":"## <p style=\"background-color:#D9EDF7; font-family:newtimeroman; color:#31708F; font-size:120%; text-align:center; border: 2px; border-style:solid; border-color:#31708F; border-radius: 24px 0;\">4.4*Bonus Aspect Ratio and Whale Size correlation</p>","metadata":{}},{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"code","source":"train_enrch","metadata":{"execution":{"iopub.status.busy":"2022-02-25T00:45:02.634328Z","iopub.execute_input":"2022-02-25T00:45:02.634892Z","iopub.status.idle":"2022-02-25T00:45:02.665775Z","shell.execute_reply.started":"2022-02-25T00:45:02.634855Z","shell.execute_reply":"2022-02-25T00:45:02.665204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Builds two aggregated views of aspect ration and image size.\ngroup1 = ['conserv_status', 'species', 'size_to_human']\nagg_f = ['count', 'min','max', 'mean', 'std']\ndata1 = train_enrch.groupby(by=group1)['aspect_ratio']\\\n                .agg(agg_f).sort_values(by='mean', ascending=False)\n\ndata2 = train_enrch.groupby(by=group1)['image_size']\\\n                .agg(agg_f).sort_values(by='mean', ascending=False)\n\ndata2_styler = data2.style.background_gradient(cmap='Greens').format('{:.3f}')","metadata":{"execution":{"iopub.status.busy":"2022-02-25T00:45:02.666653Z","iopub.execute_input":"2022-02-25T00:45:02.667138Z","iopub.status.idle":"2022-02-25T00:45:02.719239Z","shell.execute_reply.started":"2022-02-25T00:45:02.667109Z","shell.execute_reply":"2022-02-25T00:45:02.718008Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_ = train_enrch.corr().abs()\n\nfig, axes = plt.subplots(figsize=(8, 5))\nmask1 = np.zeros_like(corr_)\nmask1[np.triu_indices_from(mask1)] = True\nsns.heatmap(\n    corr_, mask=mask1, \n    linewidths=.5, cmap='Greens'\n)\n\nplt.show()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-02-25T00:45:02.720589Z","iopub.execute_input":"2022-02-25T00:45:02.720831Z","iopub.status.idle":"2022-02-25T00:45:03.201746Z","shell.execute_reply.started":"2022-02-25T00:45:02.720802Z","shell.execute_reply":"2022-02-25T00:45:03.201065Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = data1.index.get_level_values(\"species\")\ny = data1['mean']\nhue = data1.index.get_level_values(\"conserv_status\")\nsize = data1.index.get_level_values(\"size_to_human\")\n\nfig = plt.figure(figsize=(8, 6))\nsns.scatterplot(\n    data=data1, x=y, y=x, size=size, \n    hue=hue, alpha=0.5, sizes=(20, 800),\n)\n\nplt.legend(\n    bbox_to_anchor=(1.02, 1), \n    loc='upper left', \n    borderaxespad=0\n)\n\nfig.suptitle(\n        f'Species aspect_ratio mean vs size_to_human', \n        size=14, weight='bold', font='Serif'\n    )\n\nplt.grid()\nplt.show()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-02-25T00:45:03.202712Z","iopub.execute_input":"2022-02-25T00:45:03.20337Z","iopub.status.idle":"2022-02-25T00:45:03.767648Z","shell.execute_reply.started":"2022-02-25T00:45:03.203335Z","shell.execute_reply":"2022-02-25T00:45:03.766832Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Observations:\n - Now we can definitely see that the blue whale `aspect ratio` of images stands out.\n - The `aspect ratio` increases with the `size_to_human` value increases.\n - Dolphins `aspect_ratio` tends toward 1.5 value.\n\n* P.s. In fact, it is truly actionable insight. We can use this information during the postprocessing stage.","metadata":{}},{"cell_type":"markdown","source":"<a href=\"#Title\" role=\"button\" aria-pressed=\"true\" >Back to the beginning 🔙</a>","metadata":{}},{"cell_type":"markdown","source":"## <p style=\"background-color:#D9EDF7; font-family:newtimeroman; color:#31708F; font-size:120%; text-align:center; border: 2px; border-style:solid; border-color:#31708F; border-radius: 24px 0;\">4.5 Are there blurred or distorted images?</p>\n\nOne of the popular technique to identify blurred images is to apply the Laplacian kernel to calculate the variance of Laplacian. Then, we need to figure out the variance threshold.\n\nLet's see how it works:","metadata":{}},{"cell_type":"code","source":"path_f = lambda x: os.path.join(p_trn, x)\nim_pathes = train.image.map(path_f).tolist()","metadata":{"execution":{"iopub.status.busy":"2022-02-25T00:45:03.768691Z","iopub.execute_input":"2022-02-25T00:45:03.768891Z","iopub.status.idle":"2022-02-25T00:45:03.878223Z","shell.execute_reply.started":"2022-02-25T00:45:03.768866Z","shell.execute_reply":"2022-02-25T00:45:03.877238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"im1 = im_pathes[2]\nim2 = im_pathes[3]\nim3 = im_pathes[4]\n\ndef variance_of_laplacian(image):\n    # compute the Laplacian of the image and then return the focus\n    # measure, which is simply the variance of the Laplacian\n    return cv2.Laplacian(image, cv2.CV_64F).var()\n\nfor image in [im1, im2, im3]:\n    plt.figure(figsize=(10,10))\n    image = cv2.imread(image)\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    laplacian = variance_of_laplacian(gray)\n    text = f'Variance of Laplacian'\n\n    im3 = cv2.putText(image, \"{}: {:.2f}\".format(text, laplacian), (25, 200),\n              cv2.FONT_HERSHEY_SIMPLEX, 4, (255, 0, 0), 20)\n\n    plt.axis(\"off\")\n    plt.imshow(image)\n    plt.show()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-02-25T00:45:03.879698Z","iopub.execute_input":"2022-02-25T00:45:03.880011Z","iopub.status.idle":"2022-02-25T00:45:08.061447Z","shell.execute_reply.started":"2022-02-25T00:45:03.879971Z","shell.execute_reply":"2022-02-25T00:45:08.06051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems the blurred images share the variance of Laplacian less than 20. So, it might seem a good idea to try set the threshold for blurred images less than 20.","metadata":{}},{"cell_type":"markdown","source":"Below is a snippet for generating the variance of Laplacian for each image in the dataset and save it as a dict.","metadata":{}},{"cell_type":"code","source":"# def variance_of_laplacian(image):\n#     # compute the Laplacian of the image and then return the focus\n#     # measure, which is simply the variance of the Laplacian\n#     return cv2.Laplacian(image, cv2.CV_64F).var()\n\n# def get_laplasians(img_pathes):\n#     var_lap = []\n#     for image in tqdm(img_pathes, total=len(img_pathes)):\n#         im = cv2.imread(image)\n#         gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n#         var = variance_of_laplacian(gray)\n#         var_lap.append(var)\n        \n#     df_lap = pd.DataFrame({\n#         'image_id': img_pathes, \n#         'var_lap': var_lap\n#     })\n    \n#     return df_lap\n\n\n# df_lap = get_laplasians(img_pathes)\n# df_lap.to_csv('variance_of_laplacian.csv')","metadata":{"execution":{"iopub.status.busy":"2022-02-25T00:45:08.062914Z","iopub.execute_input":"2022-02-25T00:45:08.063252Z","iopub.status.idle":"2022-02-25T00:45:08.068498Z","shell.execute_reply.started":"2022-02-25T00:45:08.063211Z","shell.execute_reply":"2022-02-25T00:45:08.067648Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name=\"Exploration\"></a>\n\n# <p style=\"background-color:#D9EDF7; font-family:newtimeroman; color:#31708F; font-size:120%; text-align:center; border: 2px; border-style:solid; border-color:#31708F; border-radius: 24px 0;\">5.**Bonus Feature Engineering</p>\n\nIt can be noticed that the names of the images are anonymized by using some patterns.\nIf we take a look at the digits and characters, we might notice that characters are more prevalent than the others and some characters never exist.","metadata":{}},{"cell_type":"code","source":"train['sum_digits'] = train.image.str.findall(r'([0-9])').apply(lambda x: len(x))\ntrain['sum_char'] = train.image.str.findall(r'([a-z])').apply(lambda x: len(x) - 3)\n\nfor i in range(10):\n    all_matches = train.image.str.findall(f'([{i}])')\n    train[f'sum_{i}'] = all_matches.apply(lambda x: len(x))\n\n    \nalphabet = string.ascii_lowercase    \nfor c in alphabet:\n    all_matches = train.image.str.findall(f'([{c}])')\n    train[f'sum_{c}'] = all_matches.apply(lambda x: len(x))\n\ntrain.head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-02-25T00:45:08.069792Z","iopub.execute_input":"2022-02-25T00:45:08.070333Z","iopub.status.idle":"2022-02-25T00:45:13.338721Z","shell.execute_reply.started":"2022-02-25T00:45:08.070295Z","shell.execute_reply":"2022-02-25T00:45:13.337901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.sum(axis=0).T.head(15)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T00:45:13.339888Z","iopub.execute_input":"2022-02-25T00:45:13.34011Z","iopub.status.idle":"2022-02-25T00:45:16.793312Z","shell.execute_reply.started":"2022-02-25T00:45:13.340083Z","shell.execute_reply":"2022-02-25T00:45:16.79267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Assumption:\n - We might decipher the names of the images in order to probe the results.\n - P.s. further analysis is to be conducted...","metadata":{}},{"cell_type":"markdown","source":"<a href=\"#Title\" role=\"button\" aria-pressed=\"true\" >Back to the beginning 🔙</a>","metadata":{}},{"cell_type":"markdown","source":"<a id=''></a>\n# <p style=\"background-color:#D9EDF7; font-family:newtimeroman; color:#31708F; font-size:120%; text-align:center; border: 2px; border-style:solid; border-color:#31708F; border-radius: 24px 0px;\">Appendix</p>\n\n`aspect_ratio` and `image_size` grouped stats.","metadata":{}},{"cell_type":"code","source":"data1.style.background_gradient(cmap='Greens').format('{:.3f}')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-02-25T00:45:16.794396Z","iopub.execute_input":"2022-02-25T00:45:16.795042Z","iopub.status.idle":"2022-02-25T00:45:16.827748Z","shell.execute_reply.started":"2022-02-25T00:45:16.79501Z","shell.execute_reply":"2022-02-25T00:45:16.82671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data2.style.background_gradient(cmap='Greens').format('{:.3f}')","metadata":{"execution":{"iopub.status.busy":"2022-02-25T00:45:16.833227Z","iopub.execute_input":"2022-02-25T00:45:16.833448Z","iopub.status.idle":"2022-02-25T00:45:16.868962Z","shell.execute_reply.started":"2022-02-25T00:45:16.83342Z","shell.execute_reply":"2022-02-25T00:45:16.868408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=''></a>\n# <p style=\"background-color:#D9EDF7; font-family:newtimeroman; color:#31708F; font-size:120%; text-align:center; border: 2px; border-style:solid; border-color:#31708F; border-radius: 24px 0;\">Any suggestions to improve this notebook will be greatly appreciated. P/s If I have forgotten to reference someone's work, please, do not hesitate to leave your comment. Any questions, suggestions or complaints are most welcome. Upvotes keep me motivated... Thank you.</p>\n","metadata":{"execution":{"iopub.status.busy":"2022-02-24T23:57:45.277619Z","iopub.execute_input":"2022-02-24T23:57:45.27801Z","iopub.status.idle":"2022-02-24T23:57:45.285157Z","shell.execute_reply.started":"2022-02-24T23:57:45.277968Z","shell.execute_reply":"2022-02-24T23:57:45.283659Z"}}}]}