{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1><center>Shopee In-Depth EDA</center></h1>\n<h2><center>One Stop for all your needs!</center></h2>\n                                                      \n<center><img src = \"https://klgadgetguy.com/wp-content/uploads/2018/10/6ce1f4f6d79353c5f24ee047a5132d77.jpg\" width = \"750\" height = \"500\"/></center>                                                                                               "},{"metadata":{},"cell_type":"markdown","source":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Contents</center></h2>"},{"metadata":{},"cell_type":"markdown","source":"1. [Competition Overview](#competition-overview)  \n2. [Libraries](#libraries)  \n3. [Load Datasets](#load-datasets)  \n4. [Tabular Exploration](#tabular-exploration)  \n5. [Image Title Exploration](#image-title-exploration) \n6. [Label Group Exploration](#label-group-exploration)   \n7. [Basic Image Exploration](#basic-image-exploration)  \n8. [Intermediate Image Exploration](#intermediate-image-exploration)   \n9. [Advanced Image Exploration](#advanced-image-exploration)  \n10. [References](#references)  "},{"metadata":{},"cell_type":"markdown","source":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:magents; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>If you find this notebook useful, do give me an upvote, it helps to keep up my motivation. This notebook will be updated frequently so keep checking for furthur developments.</center></h2>"},{"metadata":{},"cell_type":"markdown","source":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:magents; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>My Target is to make it the most detailed EDA notebook of the competition!</center></h2>"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"competition-overview\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Competition Overview</center></h2>"},{"metadata":{},"cell_type":"markdown","source":"### Description\n\nRetail companies use a variety of methods to assure customers that their products are the cheapest. Among them is product matching, which allows a company to offer products at rates that are competitive to the same product sold by another retailer. To perform these matches automatically requires a thorough machine learning approach, which is where your data science skills could help.\n\nTwo different images of similar wares may represent the same product or two completely different items. Retailers want to avoid misrepresentations and other issues that could come from conflating two dissimilar products. Currently, a combination of deep learning and traditional machine learning analyzes image and text information to compare similarity. But major differences in images, titles, and product descriptions prevent these methods from being entirely effective.\n\nIn this competition, youâ€™ll apply your machine learning skills to build a model that predicts which items are the same products."},{"metadata":{},"cell_type":"markdown","source":"### Evaluation Criteria\nSubmissions will be evaluated based on their mean [F1 score](https://en.wikipedia.org/wiki/F1_score). The mean is calculated in a sample-wise fashion, meaning that an F1 score is calculated for every predicted row, then averaged."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"libraries\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Libraries</center></h2>"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"import plotly.express as px","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false,"collapsed":true,"_kg_hide-output":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\n# import plotly.express as px\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nfrom wordcloud import WordCloud, STOPWORDS\n\n#Text Color\nfrom termcolor import colored\n\n#Data Preprocessing\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n\n#NLP\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n#WordCloud\nfrom wordcloud import WordCloud, STOPWORDS\n\n#Text Processing\nimport re\nimport nltk\nnltk.download('popular')\n\n#Language Detection\n!pip install langdetect\nimport langdetect\n\n#Sentiment\nfrom textblob import TextBlob\n\n#ner\nimport spacy\n\n#Vectorizer\nfrom sklearn import feature_extraction, manifold\n\n#Word Embedding\nimport gensim.downloader as gensim_api\n\n#Topic Modeling\nimport gensim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install hvplot\nimport hvplot.pandas  # custom install\n\nfrom glob import glob\n\nfrom bq_helper import BigQueryHelper\nfrom dask import bag, diagnostics \nfrom urllib import request\n\nimport missingno as msno","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"from PIL import Image","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"load-datasets\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Load Datasets</center></h2>"},{"metadata":{},"cell_type":"markdown","source":"### CSV Files"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/shopee-product-matching/train.csv')\ntest = pd.read_csv('../input/shopee-product-matching/test.csv')\nsample = pd.read_csv('../input/shopee-product-matching/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Image Files"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Image Folder Paths\ntrain_jpg_directory = '../input/shopee-product-matching/train_images'\ntest_jpg_directory = '../input/shopee-product-matching/test_images'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getImagePaths(path):\n    \"\"\"\n    Function to Combine Directory Path with individual Image Paths\n    \n    parameters: path(string) - Path of directory\n    returns: image_names(string) - Full Image Path\n    \"\"\"\n    image_names = []\n    for dirname, _, filenames in os.walk(path):\n        for filename in filenames:\n            fullpath = os.path.join(dirname, filename)\n            image_names.append(fullpath)\n    return image_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get complete image paths for train and test datasets\ntrain_images_path = getImagePaths(train_jpg_directory)\ntest_images_path = getImagePaths(test_jpg_directory)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"tabular-exploration\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Tabular Exploration</center></h2>"},{"metadata":{},"cell_type":"markdown","source":"### Dataset Head"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This shows us that there are no nan values in the training dataset. One less thing to worry about!"},{"metadata":{},"cell_type":"markdown","source":"### Dataset Size"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Training Dataset Shape: {colored(train.shape, 'yellow')}\")\nprint(f\"Test Dataset Shape: {colored(test.shape, 'yellow')}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Column-wise Unique Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in train.columns:\n    print(col + \":\" + colored(str(len(train[col].unique())), 'yellow'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thus, only the `posting_id` columns has unique values. Rest all the columns have duplicate values. "},{"metadata":{},"cell_type":"markdown","source":"### Number of Images in Each Directory"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Number of train images: {colored(len(train_images_path), 'yellow')}\")\nprint(f\"Number of test images:  {colored(len(test_images_path), 'yellow')}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_multiple_img(images_paths, rows, cols):\n    \"\"\"\n    Function to Display Images from Dataset.\n    \n    parameters: images_path(string) - Paths of Images to be displayed\n                rows(int) - No. of Rows in Output\n                cols(int) - No. of Columns in Output\n    \"\"\"\n    figure, ax = plt.subplots(nrows=rows,ncols=cols,figsize=(16,8) )\n    for ind,image_path in enumerate(images_paths):\n        image=cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n        try:\n            ax.ravel()[ind].imshow(image)\n            ax.ravel()[ind].set_axis_off()\n        except:\n            continue;\n    plt.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train Images"},{"metadata":{"trusted":true},"cell_type":"code","source":"display_multiple_img(train_images_path[100:150], 5, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Test Images"},{"metadata":{"trusted":true},"cell_type":"code","source":"display_multiple_img(test_images_path, 1, 3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"image-title-exploration\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Image Title Exploration</center></h2>"},{"metadata":{},"cell_type":"markdown","source":"### Wordcloud"},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords = set(STOPWORDS) \nwordcloud = WordCloud(width = 800, \n                      height = 800,\n                      background_color ='white',\n                      min_font_size = 10,\n                      stopwords = stopwords,).generate(' '.join(train['title'])) \n\n# plot the WordCloud image                        \nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Basic NLP "},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_text(text, flg_stemm=False, flg_lemm=True):\n\n    lst_stopwords = nltk.corpus.stopwords.words(\"english\")\n    \n    ## clean (convert to lowercase and remove punctuations and characters and then strip)\n    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n            \n    ## Tokenize (convert from string to list)\n    lst_text = text.split()\n    ## remove Stopwords\n    if lst_stopwords is not None:\n        lst_text = [word for word in lst_text if word not in \n                    lst_stopwords]\n                \n    ## Stemming (remove -ing, -ly, ...)\n    if flg_stemm == True:\n        ps = nltk.stem.porter.PorterStemmer()\n        lst_text = [ps.stem(word) for word in lst_text]\n                \n    ## Lemmatisation (convert the word into root word)\n    if flg_lemm == True:\n        lem = nltk.stem.wordnet.WordNetLemmatizer()    \n        lst_text = [lem.lemmatize(word) for word in lst_text]\n            \n    ## back to string from list\n    text = \" \".join(lst_text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Clean Address\ntrain[\"clean_title\"] = train[\"title\"].apply(lambda x: preprocess_text(x, flg_stemm=False, flg_lemm=True, ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Length of Title\ntrain['clean_title_len'] = train['clean_title'].apply(lambda x: len(x))\n\n#Word Count\ntrain['clean_title_word_count'] =train[\"clean_title\"].apply(lambda x: len(str(x).split(\" \")))\n\n#Character Count\ntrain['clean_title_char_count'] = train[\"clean_title\"].apply(lambda x: sum(len(word) for word in str(x).split(\" \")))\n\n#Average Word Length\ntrain['clean_title_avg_word_length'] = train['clean_title_char_count'] / train['clean_title_word_count']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Distribution Plots"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_distribution(x, title):\n\n    fig = px.histogram(\n    train, \n    x = x,\n    width = 800,\n    height = 500,\n    title = title\n    )\n    \n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_distribution(x = 'clean_title_len', title = 'Title Length Distribution')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_distribution(x = 'clean_title_word_count', title = 'Word Count Distribution')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_distribution(x = 'clean_title_char_count', title = 'Character Count Distribution')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_distribution(x = 'clean_title_avg_word_length', title = 'Average Word Length Distribution')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"label-group-exploration\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Label Group Exploration</center></h2>"},{"metadata":{},"cell_type":"markdown","source":"### Count of Unique Label Groups"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"No. of Unique Label Groups: {colored(train.label_group.nunique(), 'yellow')}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Image Label Groups by No. of Images"},{"metadata":{"trusted":true},"cell_type":"code","source":"top10_names = train['label_group'].value_counts().index.tolist()[:15]\ntop10_values = train['label_group'].value_counts().tolist()[:15]\n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=top10_names, y=top10_values)\nplt.xticks(rotation=45)\nplt.xlabel(\"Label Group\")\nplt.ylabel(\"Image Count\")\nplt.title(\"Top-15 Label Groups by Image Count\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"basic-image-exploration\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Basic Image Exploration</center></h2>"},{"metadata":{},"cell_type":"markdown","source":"### Dimensions and 2D Histograms"},{"metadata":{},"cell_type":"markdown","source":"Just look at image dimensions, confirm it's 3 band (RGB), byte scaled (0-255)."},{"metadata":{"trusted":true},"cell_type":"code","source":"first = cv2.imread(train_images_path[0])\ndims = np.shape(first)\nprint(dims)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.min(first), np.max(first)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nFor any image specific classification, clustering, etc. transforms we'll want to collapse spatial dimensions so that we have a matrix of pixels by color channels."},{"metadata":{"trusted":true},"cell_type":"code","source":"pixel_matrix = np.reshape(first, (dims[0] * dims[1], dims[2]))\nprint(np.shape(pixel_matrix))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Scatter plots are a go to to look for clusters and separatbility in the data, but these are busy and don't reveal density well, so we switch to using 2d histograms instead. The data between bands is really correlated, typical with visible imagery and why most satellite image analysts prefer to at least have near infrared values."},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = plt.hist2d(pixel_matrix[:,1], pixel_matrix[:,2], bins=(50,50))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fifth = cv2.imread(train_images_path[4])\ndims = np.shape(fifth)\npixel_matrix5 = np.reshape(fifth, (dims[0] * dims[1], dims[2]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = plt.hist2d(pixel_matrix5[:,1], pixel_matrix5[:,2], bins=(50,50))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(first)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(fifth)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"intermediate-image-exploration\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Intermediate Image Exploration</center></h2>"},{"metadata":{},"cell_type":"markdown","source":"### Image Dimension"},{"metadata":{},"cell_type":"markdown","source":"We shall check if the provided images have the same dimension or not. I'll use Dask to parallelize the operation and speed things up."},{"metadata":{"trusted":true},"cell_type":"code","source":"# get image dimensions\ndef get_dims(file):\n    img = cv2.imread(file)\n    h,w = img.shape[:2]\n    return h,w\n\n# parallelize\nfilelist = train_images_path\ndimsbag = bag.from_sequence(filelist).map(get_dims)\nwith diagnostics.ProgressBar():\n    dims = dimsbag.compute()\n    \ndim_df = pd.DataFrame(dims, columns=['height', 'width'])\nsizes = dim_df.groupby(['height', 'width']).size().reset_index().rename(columns={0:'count'})\nsizes.hvplot.scatter(x='height', y='width', size='count', xlim=(0,1200), ylim=(0,1200), grid=True, xticks=2, \n        yticks=2, height=500, width=600).options(scaling_factor=0.1, line_alpha=1, fill_alpha=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### K-means Clustering"},{"metadata":{"trusted":true},"cell_type":"code","source":"# simple k means clustering\nfrom sklearn import cluster\n\nkmeans = cluster.KMeans(5)\nclustered = kmeans.fit_predict(pixel_matrix)\n\ndims = np.shape(first)\nclustered_img = np.reshape(clustered, (dims[0], dims[1]))\nplt.imshow(clustered_img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ind0, ind1, ind2, ind3 = [np.where(clustered == x)[0] for x in [0, 1, 2, 3]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\nplot_vals = [('r', 'o', ind0),\n             ('b', '^', ind1),\n             ('g', '8', ind2),\n             ('m', '*', ind3)]\n\nfor c, m, ind in plot_vals:\n    xs = pixel_matrix[ind, 0]\n    ys = pixel_matrix[ind, 1]\n    zs = pixel_matrix[ind, 2]\n    ax.scatter(xs, ys, zs, c=c, marker=m)\n\nax.set_xlabel('Blue channel')\nax.set_ylabel('green channel')\nax.set_zlabel('Red channel')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# quick look at color value histograms for pixel matrix from first image\nimport seaborn as sns\nsns.distplot(pixel_matrix[:,0], bins=12)\nsns.distplot(pixel_matrix[:,1], bins=12)\nsns.distplot(pixel_matrix[:,2], bins=12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Matching Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"img79_1, img79_2, img79_3, img79_4, img79_5 = [plt.imread(train_images_path[n]) for n in range(78, 83)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_list = (img79_1, img79_2, img79_3, img79_4, img79_5)\n\nplt.figure(figsize=(8,10))\nplt.imshow(img_list[0])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tracking dimensions across image transforms is annoying, so we'll make a class to do that. Also I'm going to use this brightness normalization transform and visualize the image that way, good test scenario for class."},{"metadata":{"trusted":true},"cell_type":"code","source":"class MSImage():\n    \"\"\"Lightweight wrapper for handling image to matrix transforms. No setters,\n    main point of class is to remember image dimensions despite transforms.\"\"\"\n    \n    def __init__(self, img):\n        \"\"\"Assume color channel interleave that holds true for this set.\"\"\"\n        self.img = img\n        self.dims = np.shape(img)\n        self.mat = np.reshape(img, (self.dims[0] * self.dims[1], self.dims[2]))\n\n    @property\n    def matrix(self):\n        return self.mat\n        \n    @property\n    def image(self):\n        return self.img\n    \n    def to_flat_img(self, derived):\n        \"\"\"\"Use dims property to reshape a derived matrix back into image form when\n        derived image would only have one band.\"\"\"\n        return np.reshape(derived, (self.dims[0], self.dims[1]))\n    \n    def to_matched_img(self, derived):\n        \"\"\"\"Use dims property to reshape a derived matrix back into image form.\"\"\"\n        return np.reshape(derived, (self.dims[0], self.dims[1], self.dims[2]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"msi79_1 = MSImage(img79_1)\nprint(np.shape(msi79_1.matrix))\nprint(np.shape(msi79_1.img))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Brightness Normalization"},{"metadata":{},"cell_type":"markdown","source":"Brightness Normalization is preprocessing strategy you can apply prior to using strategies to identify materials in a scene, if you want your matching algorithm to be robust across variations in illumination. See [Wu's paper](https://pantherfile.uwm.edu/cswu/www/my%20publications/2004_RSE.pdf)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def bnormalize(mat):\n    \"\"\"much faster brightness normalization, since it's all vectorized\"\"\"\n    bnorm = np.zeros_like(mat, dtype=np.float32)\n    maxes = np.max(mat, axis=1)\n    bnorm = mat / np.vstack((maxes, maxes, maxes)).T\n    return bnorm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bnorm = bnormalize(msi79_1.matrix)\nbnorm_img = msi79_1.to_matched_img(bnorm)\nplt.figure(figsize=(8,10))\nplt.imshow(bnorm_img)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"msi79_2 = MSImage(img79_2)\nbnorm79_2 = bnormalize(msi79_2.matrix)\nbnorm79_2_img = msi79_2.to_matched_img(bnorm79_2)\nplt.figure(figsize=(8,10))\nplt.imshow(bnorm79_2_img)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(msinorm79_1.matrix[:,0], bins=12)\nsns.distplot(msinorm79_1.matrix[:,1], bins=12)\nsns.distplot(msinorm79_1.matrix[:,2], bins=12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using Thresholds with Brightness Normalization"},{"metadata":{},"cell_type":"markdown","source":"The brightness normalization step is helpful because thresholds that aren't anchored by a preprocessing step end up being arbitrary and can't generalize between scenes even in the same image set, whereas thresholds following brightness normalization tend to pull out materils that stand out from the background more reliably. See the following demonstration:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,15))\nplt.subplot(121)\nplt.imshow(img79_1[:,:,0] > 230)\nplt.subplot(122)\nplt.imshow(img79_1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,15))\nplt.subplot(121)\nplt.imshow(img79_2[:,:,0] > 230)\nplt.subplot(122)\nplt.imshow(img79_2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,15))\nplt.subplot(121)\nplt.imshow(bnorm79_2_img[:,:,0] > 0.98)\nplt.subplot(122)\nplt.imshow(img79_2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,15))\nplt.subplot(121)\nplt.imshow(bnorm_img[:,:,0] > 0.98)\nplt.subplot(122)\nplt.imshow(img79_1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplot(121)\nplt.imshow((bnorm79_2_img[:,:,0] > 0.9999) & \\\n           (bnorm79_2_img[:,:,1] < 0.9999) & \\\n           (bnorm79_2_img[:,:,2] < 0.9999))\nplt.subplot(122)\nplt.imshow(img79_2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,15))\nplt.subplot(121)\nplt.imshow(bnorm_img[:,:,0] > 0.995)\nplt.subplot(122)\nplt.imshow(img79_1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"advanced-image-exploration\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Advanced Image Exploration</center></h2>"},{"metadata":{},"cell_type":"markdown","source":"### Rudimentary Transforms, Edge Detection, Texture"},{"metadata":{"trusted":true},"cell_type":"code","source":"set144 = [plt.imread(train_images_path[n]) for n in (10000, 11000)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(set144[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import skimage\nfrom skimage.feature import greycomatrix, greycoprops\nfrom skimage.filters import sobel","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sobel Edge Detection\n\nA Sobel filter is one means of getting a basic edge magnitude/gradient image. Can be useful to threshold and find prominent linear features, etc. Several other similar filters in skimage.filters are also good edge detectors: `roberts`, `scharr`, etc. and you can control direction, i.e. use an anisotropic version."},{"metadata":{"trusted":true},"cell_type":"code","source":"# a sobel filter is a basic way to get an edge magnitude/gradient image\nfig = plt.figure(figsize=(8, 8))\nplt.imshow(sobel(set144[0][:750,:750,2]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from skimage.filters import sobel_h\n\n# can also apply sobel only across one direction.\nfig = plt.figure(figsize=(8, 8))\nplt.imshow(sobel_h(set144[0][:750,:750,2]), cmap='BuGn')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### GLCM Textures\nProcessing time can be pretty brutal so we subset the image. We'll create texture images so we can characterize each pixel by the texture of its neighborhood.\n\nGLCM is inherently anisotropic but can be averaged so as to be rotation invariant. For more on GLCM, see [the tutorial](http://www.fp.ucalgary.ca/mhallbey/tutorial.htm).\n\nA good article on use in remote sensing is [here](http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=4660321&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D4660321):\n\nPesaresi, M., Gerhardinger, A., & Kayitakire, F. (2008). A robust built-up area presence index by anisotropic rotation-invariant textural measure. Selected Topics in Applied Earth Observations and Remote Sensing, IEEE Journal of, 1(3), 180-192."},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = set144[0][:150,:150,2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def glcm_image(img, measure=\"dissimilarity\"):\n    \"\"\"TODO: allow different window sizes by parameterizing 3, 4. Also should\n    parameterize direction vector [1] [0]\"\"\"\n    texture = np.zeros_like(sub)\n\n    # quadratic looping in python w/o vectorized routine, yuck!\n    for i in range(img.shape[0] ):  \n        for j in range(sub.shape[1] ):  \n          \n            # don't calculate at edges\n            if (i < 3) or \\\n               (i > (img.shape[0])) or \\\n               (j < 3) or \\\n               (j > (img.shape[0] - 4)):          \n                continue  \n        \n            # calculate glcm matrix for 7 x 7 window, use dissimilarity (can swap in\n            # contrast, etc.)\n            glcm_window = img[i-3: i+4, j-3 : j+4]  \n            glcm = greycomatrix(glcm_window, [1], [0],  symmetric = True, normed = True )   \n            texture[i,j] = greycoprops(glcm, measure)  \n    return texture","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dissimilarity = glcm_image(sub, \"dissimilarity\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(8, 8))\nplt.subplot(1,2,1)\nplt.imshow(dissimilarity, cmap=\"bone\")\nplt.subplot(1,2,2)\nplt.imshow(sub, cmap=\"bone\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### HSV Transform\nHSV is useful for identifying shadows and illumination, as well as giving us a means to identify similar objects that are distinct by color between scenes (hue), though there's no guarantee the hue will be stable."},{"metadata":{"trusted":true},"cell_type":"code","source":"from skimage import color\n\nhsv = color.rgb2hsv(set144[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(8, 8))\nplt.subplot(2,2,1)\nplt.imshow(set144[0], cmap=\"bone\")\nplt.subplot(2,2,2)\nplt.imshow(hsv[:,:,0], cmap=\"bone\")\nplt.subplot(2,2,3)\nplt.imshow(hsv[:,:,1], cmap='bone')\nplt.subplot(2,2,4)\nplt.imshow(hsv[:,:,2], cmap='bone')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(8, 8))\nplt.subplot(2,2,1)\nplt.imshow(set144[0][:200,:200,:])\nplt.subplot(2,2,2)\nplt.imshow(hsv[:200,:200,0], cmap=\"PuBuGn\")\nplt.subplot(2,2,3)\nplt.imshow(hsv[:200,:200,1], cmap='bone')\nplt.subplot(2,2,4)\nplt.imshow(hsv[:200,:200,2], cmap='bone')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(8, 6))\nplt.imshow(hsv[200:500,200:500,0], cmap='bone')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hsvmsi = MSImage(hsv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Shadow Detection\nWe can apply a threshold to the V band now to find dark areas that are probably thresholds. Let's look at the distribution of all values then work interactively to find a good filter value."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(hsvmsi.matrix[:,0], bins=12)\nsns.distplot(hsvmsi.matrix[:,1], bins=12)\nsns.distplot(hsvmsi.matrix[:,2], bins=12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(hsvmsi.image[:,:,2] < 0.4, cmap=\"plasma\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(8, 8))\nplt.subplot(1,2,1)\nplt.imshow(set144[0][:250,:250,:])\nplt.subplot(1,2,2)\nplt.imshow(hsvmsi.image[:250,:250,2] < 0.4, cmap=\"plasma\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(8, 8))\nimg2 = plt.imshow(set144[0][:250,:250,:], interpolation='nearest')\nimg3 = plt.imshow(hsvmsi.image[:250,:250,2] < 0.4, cmap='binary_r', alpha=0.4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"references\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>References</center></h2>"},{"metadata":{},"cell_type":"markdown","source":"1. [Shopee - Data understanding and analysis](https://www.kaggle.com/isaienkov/shopee-data-understanding-and-analysis) \n2. [EDA of Shopee for starter](https://www.kaggle.com/chumajin/eda-of-shopee-for-starter)  \n3. [Shopee Product Matching: EDA+Baseline](https://www.kaggle.com/ruchi798/shopee-product-matching-eda-baseline/data)   \n4. [Shopee EDA + Understanding Competition!](https://www.kaggle.com/heyytanay/shopee-eda-understanding-competition)    \n5. [Exploratory Image Analysis](https://www.kaggle.com/bkamphaus/exploratory-image-analysis#Brightness-Normalization)"},{"metadata":{},"cell_type":"markdown","source":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:blue; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>More Awesome Plots Coming Soon!</center></h2>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}