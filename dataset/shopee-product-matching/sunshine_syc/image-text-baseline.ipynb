{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # from BM25Vectorizer import Bm25Vectorizerc\n# from sklearn.feature_extraction.text import CountVectorizer\n# from sklearn.utils.validation import check_is_fitted, check_array, FLOAT_DTYPES\n# import numpy as np\n# import warnings\n# from sklearn.base import BaseEstimator, TransformerMixin\n# import scipy.sparse as sp\n# from sklearn.feature_extraction.text import _document_frequency\n# from sklearn.preprocessing import normalize\n\n# class Bm25Vectorizer(CountVectorizer):\n#     def __init__(self, input='content', encoding='utf-8',\n#                  decode_error='strict', strip_accents=None, lowercase=True,\n#                  preprocessor=None, tokenizer=None, analyzer='word',\n#                  stop_words=None, token_pattern=r\"\\w+\",\n#                  ngram_range=(1, 1), max_df=1.0, min_df=1,\n#                  max_features=None, vocabulary=None, binary=False,\n#                  dtype=np.float64, norm='l2', use_idf=True, smooth_idf=True,\n#                  sublinear_tf=False):\n\n#         super(Bm25Vectorizer, self).__init__(\n#             input=input, encoding=encoding, decode_error=decode_error,\n#             strip_accents=strip_accents, lowercase=lowercase,\n#             preprocessor=preprocessor, tokenizer=tokenizer, analyzer=analyzer,\n#             stop_words=stop_words, token_pattern=token_pattern,\n#             ngram_range=ngram_range, max_df=max_df, min_df=min_df,\n#             max_features=max_features, vocabulary=vocabulary, binary=binary,\n#             dtype=dtype)\n\n#         self._tfidf = Bm25Transformer(norm=norm, use_idf=use_idf,\n#                                        smooth_idf=smooth_idf,\n#                                        sublinear_tf=sublinear_tf)\n\n#     # Broadcast the TF-IDF parameters to the underlying transformer instance\n#     # for easy grid search and repr\n\n#     @property\n#     def norm(self):\n#         return self._tfidf.norm\n\n#     @norm.setter\n#     def norm(self, value):\n#         self._tfidf.norm = value\n\n#     @property\n#     def use_idf(self):\n#         return self._tfidf.use_idf\n\n#     @use_idf.setter\n#     def use_idf(self, value):\n#         self._tfidf.use_idf = value\n\n#     @property\n#     def smooth_idf(self):\n#         return self._tfidf.smooth_idf\n\n#     @smooth_idf.setter\n#     def smooth_idf(self, value):\n#         self._tfidf.smooth_idf = value\n\n#     @property\n#     def sublinear_tf(self):\n#         return self._tfidf.sublinear_tf\n\n#     @sublinear_tf.setter\n#     def sublinear_tf(self, value):\n#         self._tfidf.sublinear_tf = value\n\n#     @property\n#     def idf_(self):\n#         return self._tfidf.idf_\n\n#     @idf_.setter\n#     def idf_(self, value):\n#         self._validate_vocabulary()\n#         if hasattr(self, 'vocabulary_'):\n#             if len(self.vocabulary_) != len(value):\n#                 raise ValueError(\"idf length = %d must be equal \"\n#                                  \"to vocabulary size = %d\" %\n#                                  (len(value), len(self.vocabulary)))\n#         self._tfidf.idf_ = value\n\n#     def _check_params(self):\n#         if self.dtype not in FLOAT_DTYPES:\n#             warnings.warn(\"Only {} 'dtype' should be used. {} 'dtype' will \"\n#                           \"be converted to np.float64.\"\n#                           .format(FLOAT_DTYPES, self.dtype),\n#                           UserWarning)\n\n#     def fit(self, raw_documents, y=None):\n#         \"\"\"Learn vocabulary and idf from training set.\n\n#         Parameters\n#         ----------\n#         raw_documents : iterable\n#             an iterable which yields either str, unicode or file objects\n\n#         Returns\n#         -------\n#         self : TfidfVectorizer\n#         \"\"\"\n#         self._check_params()\n#         X = super(Bm25Vectorizer, self).fit_transform(raw_documents)\n#         self._tfidf.fit(X)\n#         return self\n\n#     def fit_transform(self, raw_documents, y=None):\n#         \"\"\"Learn vocabulary and idf, return term-document matrix.\n\n#         This is equivalent to fit followed by transform, but more efficiently\n#         implemented.\n\n#         Parameters\n#         ----------\n#         raw_documents : iterable\n#             an iterable which yields either str, unicode or file objects\n\n#         Returns\n#         -------\n#         X : sparse matrix, [n_samples, n_features]\n#             Tf-idf-weighted document-term matrix.\n#         \"\"\"\n#         self._check_params()\n#         X = super(Bm25Vectorizer, self).fit_transform(raw_documents)\n#         self._tfidf.fit(X)\n#         # X is already a transformed view of raw_documents so\n#         # we set copy to False\n#         return self._tfidf.transform(X, copy=False)\n\n#     def transform(self, raw_documents, copy=True):\n#         \"\"\"Transform documents to document-term matrix.\n\n#         Uses the vocabulary and document frequencies (df) learned by fit (or\n#         fit_transform).\n\n#         Parameters\n#         ----------\n#         raw_documents : iterable\n#             an iterable which yields either str, unicode or file objects\n\n#         copy : boolean, default True\n#             Whether to copy X and operate on the copy or perform in-place\n#             operations.\n\n#         Returns\n#         -------\n#         X : sparse matrix, [n_samples, n_features]\n#             Tf-idf-weighted document-term matrix.\n#         \"\"\"\n#         check_is_fitted(self, '_tfidf', 'The tfidf vector is not fitted')\n\n#         X = super(Bm25Vectorizer, self).transform(raw_documents)\n#         return self._tfidf.transform(X, copy=False)\n\n\n# class Bm25Transformer(BaseEstimator, TransformerMixin):\n\n#     def __init__(self,k=1.2,b=0.75, norm=\"l2\", use_idf=True, smooth_idf=True,\n#                  sublinear_tf=False):\n#         self.k = k\n#         self.b = b\n#         ######### tfidf的代码###########\n#         self.norm = norm\n#         self.use_idf = use_idf\n#         self.smooth_idf = smooth_idf\n#         self.sublinear_tf = sublinear_tf\n\n#     def fit(self, X, y=None):\n#         \"\"\"Learn the idf vector (global term weights)\n\n#         Parameters\n#         ----------\n#         X : sparse matrix, [n_samples, n_features]\n#             a matrix of term/token counts\n#         \"\"\"\n#         X = X.toarray()\n#         self.avdl = X.sum()/X.shape[0] #句子的平均长度\n#         # print(\"原来的fit的数据：\\n\",X)\n\n#         #计算每个词语的tf的值\n#         self.tf = X.sum(0)/X.sum()  #[M] #M表示总词语的数量\n#         self.tf = self.tf.reshape([1,self.tf.shape[0]]) #[1,M]\n#         # print(\"tf\\n\",self.tf)\n#         ######       原来tfidf的代码  ######\n\n#         X = check_array(X, accept_sparse=('csr', 'csc'))\n#         if not sp.issparse(X):\n#             X = sp.csr_matrix(X)\n#         dtype = X.dtype if X.dtype in FLOAT_DTYPES else np.float64\n\n#         if self.use_idf:\n#             n_samples, n_features = X.shape\n#             df = _document_frequency(X).astype(dtype)\n\n#             # perform idf smoothing if required\n#             df += int(self.smooth_idf)\n#             n_samples += int(self.smooth_idf)\n\n#             # log+1 instead of log makes sure terms with zero idf don't get\n#             # suppressed entirely.\n#             idf = np.log(n_samples / df) + 1\n#             self._idf_diag = sp.diags(idf, offsets=0,\n#                                       shape=(n_features, n_features),\n#                                       format='csr',\n#                                       dtype=dtype)\n\n#         return self\n\n#     def transform(self, X, copy=True):\n#         \"\"\"Transform a count matrix to a tf or tf-idf representation\n\n#         Parameters\n#         ----------\n#         X : sparse matrix, [n_samples, n_features]\n#             a matrix of term/token counts\n\n#         copy : boolean, default True\n#             Whether to copy X and operate on the copy or perform in-place\n#             operations.\n\n#         Returns\n#         -------\n#         vectors : sparse matrix, [n_samples, n_features]\n#         \"\"\"\n#         ########### 计算中间项  ###############\n#         cur_tf = np.multiply(self.tf, X.toarray()) #[N,M] #N表示数据的条数，M表示总词语的数量\n#         norm_lenght = 1 - self.b + self.b*(X.toarray().sum(-1)/self.avdl) #[N] #N表示数据的条数\n#         norm_lenght = norm_lenght.reshape([norm_lenght.shape[0],1]) #[N,1]\n#         X = (self.k+1)*cur_tf /(cur_tf +self.k*norm_lenght)\n#         ############# 结算结束  ################\n\n#         X = check_array(X, accept_sparse='csr', dtype=FLOAT_DTYPES, copy=copy)\n#         if not sp.issparse(X):\n#             X = sp.csr_matrix(X, dtype=np.float64)\n\n#         n_samples, n_features = X.shape\n\n#         if self.sublinear_tf:\n#             np.log(X.data, X.data)\n#             X.data += 1\n\n#         if self.use_idf:\n#             check_is_fitted(self, '_idf_diag', 'idf vector is not fitted')\n\n#             expected_n_features = self._idf_diag.shape[0]\n#             if n_features != expected_n_features:\n#                 raise ValueError(\"Input has n_features=%d while the model\"\n#                                  \" has been trained with n_features=%d\" % (\n#                                      n_features, expected_n_features))\n#             # *= doesn't work\n\n#             X = X * self._idf_diag\n\n#         if self.norm:\n#             X = normalize(X, norm=self.norm, copy=False)\n#         return X\n\n#     @property\n#     def idf_(self):\n#         # if _idf_diag is not set, this will raise an attribute error,\n#         # which means hasattr(self, \"idf_\") is False\n#         return np.ravel(self._idf_diag.sum(axis=0))\n\n#     @idf_.setter\n#     def idf_(self, value):\n#         value = np.asarray(value, dtype=np.float64)\n#         n_features = value.shape[0]\n#         self._idf_diag = sp.spdiags(value, diags=0, m=n_features,\n#                                     n=n_features, format='csr')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DATA_PATH = '../input/'\nDATA_PATH = '../input/shopee-product-matching/'","metadata":{"ExecuteTime":{"end_time":"2021-03-18T09:59:13.247406Z","start_time":"2021-03-18T09:59:13.24369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport cv2, matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\n\nimport cudf, cuml, cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors\n\ndef getMetric(col):\n    def f1score(row):\n        n = len( np.intersect1d(row.target,row[col]) )\n        return 2*n / (len(row.target)+len(row[col]))\n    return f1score","metadata":{"ExecuteTime":{"end_time":"2021-03-18T09:59:14.869532Z","start_time":"2021-03-18T09:59:14.482759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"COMPUTE_CV = True\n\ntest = pd.read_csv(DATA_PATH + 'test.csv')\nif len(test)>3: COMPUTE_CV = False\nelse: print('this submission notebook will compute CV score, but commit notebook will not')\n\n# COMPUTE_CV = False\n\nif COMPUTE_CV:\n    train = pd.read_csv(DATA_PATH + 'train.csv')\n    train['image'] = DATA_PATH + 'train_images/' + train['image']\n    tmp = train.groupby('label_group').posting_id.agg('unique').to_dict()\n    train['target'] = train.label_group.map(tmp)\n    train_gf = cudf.read_csv(DATA_PATH + 'train.csv')\nelse:\n    train = pd.read_csv(DATA_PATH + 'test.csv')\n    train['image'] = DATA_PATH + 'test_images/' + train['image']\n    train_gf = cudf.read_csv(DATA_PATH + 'test.csv')\n    \nprint('train shape is', train.shape )\ntrain.head()","metadata":{"ExecuteTime":{"end_time":"2021-03-18T09:59:15.92512Z","start_time":"2021-03-18T09:59:15.308672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# image hash","metadata":{}},{"cell_type":"code","source":"tmp = train.groupby('image_phash').posting_id.agg('unique').to_dict()\ntrain['oof_hash'] = train.image_phash.map(tmp)","metadata":{"ExecuteTime":{"end_time":"2021-03-18T09:59:19.569052Z","start_time":"2021-03-18T09:59:18.284395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if COMPUTE_CV:\n    train['f1'] = train.apply(getMetric('oof_hash'),axis=1)\n    print('CV score for baseline =',train.f1.mean())","metadata":{"ExecuteTime":{"end_time":"2021-03-18T09:59:21.98207Z","start_time":"2021-03-18T09:59:20.62671Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# image CNN","metadata":{}},{"cell_type":"code","source":"from PIL import Image\n\nimport torch\ntorch.manual_seed(0)\ntorch.backends.cudnn.deterministic = False\ntorch.backends.cudnn.benchmark = True\n\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torch.utils.data.dataset import Dataset\n\nclass ShopeeImageDataset(Dataset):\n    def __init__(self, img_path, transform):\n        self.img_path = img_path\n        self.transform = transform\n        \n    def __getitem__(self, index):\n        img = Image.open(self.img_path[index]).convert('RGB')\n        img = self.transform(img)\n        return img\n    \n    def __len__(self):\n        return len(self.img_path)","metadata":{"ExecuteTime":{"end_time":"2021-03-18T09:59:24.147684Z","start_time":"2021-03-18T09:59:23.6933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imagedataset = ShopeeImageDataset(\n    train['image'].values,\n    transforms.Compose([\n        transforms.Resize((512, 512)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]))\n    \nimageloader = torch.utils.data.DataLoader(\n    imagedataset,\n    batch_size=10, shuffle=False, num_workers=2\n)","metadata":{"ExecuteTime":{"end_time":"2021-03-18T09:59:25.6502Z","start_time":"2021-03-18T09:59:25.64389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ShopeeImageEmbeddingNet(nn.Module):\n    def __init__(self):\n        super(ShopeeImageEmbeddingNet, self).__init__()\n              \n        model = models.resnet18(True)\n        model.avgpool = nn.AdaptiveMaxPool2d(output_size=(1, 1))\n        model = nn.Sequential(*list(model.children())[:-1])\n        model.eval()\n        self.model = model\n        \n    def forward(self, img):        \n        out = self.model(img)\n        return out","metadata":{"ExecuteTime":{"end_time":"2021-03-18T09:59:27.08827Z","start_time":"2021-03-18T09:59:27.083495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir -p /root/.cache/torch/hub/checkpoints/\n!cp ../input/pretrained-pytorch-models/resnet18-5c106cde.pth /root/.cache/torch/hub/checkpoints/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DEVICE = 'cuda'\n\nimgmodel = ShopeeImageEmbeddingNet()\nimgmodel = imgmodel.to(DEVICE)\n\nimagefeat = []\nwith torch.no_grad():\n    for data in tqdm_notebook(imageloader):\n        data = data.to(DEVICE)\n        feat = imgmodel(data)\n        feat = feat.reshape(feat.shape[0], feat.shape[1])\n        feat = feat.data.cpu().numpy()\n        \n        imagefeat.append(feat)","metadata":{"ExecuteTime":{"end_time":"2021-03-18T10:01:20.420477Z","start_time":"2021-03-18T09:59:28.809744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import normalize\n\n# l2 norm to kill all the sim in 0-1\nimagefeat = np.vstack(imagefeat)\nimagefeat = normalize(imagefeat)","metadata":{"ExecuteTime":{"end_time":"2021-03-18T10:01:43.818543Z","start_time":"2021-03-18T10:01:43.401624Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"KNN = 50\nif len(test)==3: KNN = 2\nmodel = NearestNeighbors(n_neighbors=KNN)\nmodel.fit(imagefeat)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = []\nCHUNK = 1024*4\n\nimagefeat = cupy.array(imagefeat)\n\nprint('Finding similar images...')\nCTS = len(imagefeat)//CHUNK\nif len(imagefeat)%CHUNK!=0: CTS += 1\nfor j in range( CTS ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b, len(imagefeat))\n    print('chunk',a,'to',b)\n    \n    distances = cupy.matmul(imagefeat, imagefeat[a:b].T).T\n    # distances = np.dot(imagefeat[a:b,], imagefeat.T)\n    \n    for k in range(b-a):\n        IDX = cupy.where(distances[k,]>0.95)[0]\n        # IDX = np.where(distances[k,]>0.95)[0][:]\n        o = train.iloc[cupy.asnumpy(IDX)].posting_id.values\n        preds.append(o)\n        \n# del imagefeat, imgmodel","metadata":{"ExecuteTime":{"end_time":"2021-03-18T10:01:54.771453Z","start_time":"2021-03-18T10:01:44.50243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['oof_cnn'] = preds\n\nif COMPUTE_CV:\n    train['f1'] = train.apply(getMetric('oof_cnn'),axis=1)\n    print('CV score for baseline =',train.f1.mean())","metadata":{"ExecuteTime":{"end_time":"2021-03-18T10:01:58.132852Z","start_time":"2021-03-18T10:01:56.678412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# title TFIDF","metadata":{}},{"cell_type":"code","source":"# !pip install ../input/rank-bm25/rank_bm25-0.2.1-py3-none-any.whl\n# # from gensim.summarization import bm25\n# from rank_bm25 import BM25Okapi\n# from nltk.tokenize import word_tokenize","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# train_title_token = train['title'].apply(lambda x: word_tokenize(x))\n# train_array = []\n# for i in range(0,34250):\n#     train_array.append(train_title_token.iloc[i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# bm25 = BM25Okapi(train_title_token)\n# bm25","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_title_token.iloc[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preds = []\n# for i in train_title_token:\n#     ids = bm25.get_scores(i)\n#     ids = cupy.asarray(ids)\n#     idx = cupy.where(ids>50)[0]\n    \n#     if not idx.any():\n#         idx = cupy.where(ids==np.max(ids))[0]\n        \n#     o = train.iloc[cupy.asnumpy(idx)].posting_id.values\n#     print(o)\n#     preds.append(o)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preds = []\n# for i in train_array:\n#     ids = bm25.get_scores(i)\n#     ids = np.asarray(ids)\n#     idx = np.where(ids>50)[0]\n    \n#     if not idx.any():\n#         idx = np.where(ids==np.max(ids))[0]\n        \n#     o = train.iloc[idx].posting_id.values\n#     print(o)\n#     preds.append(o)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.feature_extraction.text import TfidfVectorizer\n# model = TfidfVectorizer(stop_words=None, binary=True, max_features=25000)\n# text_embeddings = model.fit_transform(train_gf.title).toarray()\n# print('text embeddings shape',text_embeddings.shape)","metadata":{"ExecuteTime":{"end_time":"2021-03-18T10:02:00.631468Z","start_time":"2021-03-18T10:01:59.851964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_array = train_gf.title.to_array()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# bm_vec = Bm25Vectorizer(stop_words=None, binary=True, max_features=25000)\n# # bm_vec.fit(train_array)\n# text_embeddings = bm_vec.fit_transform(train_gf.title).toarray()\n# print('text embeddings shape',text_embeddings.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preds = []\n# CHUNK = 1024*4\n\n# print('Finding similar titles...')\n# CTS = len(train)//CHUNK\n# if len(train)%CHUNK!=0: CTS += 1\n# for j in range( CTS ):\n    \n#     a = j*CHUNK\n#     b = (j+1)*CHUNK\n#     b = min(b,len(train))\n#     print('chunk',a,'to',b)\n    \n#     # COSINE SIMILARITY DISTANCE\n#     # cts = np.dot( text_embeddings, text_embeddings[a:b].T).T\n#     cts = cupy.matmul(text_embeddings, text_embeddings[a:b].T).T\n    \n#     for k in range(b-a):\n#         # IDX = np.where(cts[k,]>0.7)[0]\n#         print(cts[k,])\n#         IDX = cupy.where(cts[k,]>0.7)[0]\n#         o = train.iloc[cupy.asnumpy(IDX)].posting_id.values\n#         preds.append(o)\n        \n# del model, text_embeddings","metadata":{"ExecuteTime":{"end_time":"2021-03-18T10:05:46.252393Z","start_time":"2021-03-18T10:02:01.803979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Word2evc","metadata":{}},{"cell_type":"code","source":"from gensim.test.utils import get_tmpfile\nfrom gensim.models import KeyedVectors\nfrom gensim.models import Word2Vec\nfrom nltk.tokenize import word_tokenize\n\ntrain_title_token = train['title'].apply(lambda x: word_tokenize(x))\nmodel = Word2Vec(sentences=train_title_token, vector_size=200, window=5, min_count=1, workers=4)\nmodel.wv.save_word2vec_format(\"word2evc.txt\",binary = \"Ture/False\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectors = KeyedVectors.load_word2vec_format(\"word2evc.txt\",binary = \"Ture/False\") # import the data file","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_embeddings = []\nfor title in tqdm_notebook(train_title_token[:]):\n    title_feat = []\n    for word in title:\n        if word in vectors:\n            title_feat.append(vectors[word])\n    \n    if len(title_feat) == 0:\n        title_feat = np.random.rand(200)\n    else:\n        # max-pooling\n        # mean-pooling\n        # IDF\n        # SIF\\\n        \n        title_feat = np.vstack(title_feat).max(0)\n        \n    text_embeddings.append(title_feat)\n    # break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import normalize\n\n# l2 norm to kill all the sim in 0-1\ntext_embeddings = np.vstack(text_embeddings)\ntext_embeddings = normalize(text_embeddings)\n\nimport torch\ntext_embeddings = torch.from_numpy(text_embeddings)\ntext_embeddings = text_embeddings.cuda()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cudf, cuml, cupy\ntext_embeddings = cupy.asarray(text_embeddings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = []\nCHUNK = 1024*4\n\n\nprint('Finding similar images...')\nCTS = len(text_embeddings)//CHUNK\nif len(text_embeddings)%CHUNK!=0: CTS += 1\nfor j in range( CTS ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(train))\n    print('chunk',a,'to',b)\n    \n    cts = cupy.matmul(text_embeddings, text_embeddings[a:b].T).T\n    for k in range(b-a):\n        IDX = cupy.where(cts[k,]>0.93)[0]\n        o = train.iloc[cupy.asnumpy(IDX)].posting_id.values\n        preds.append(o)\n        \n    del cts\n    torch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['oof_w2v'] = preds\n# train['oof_text'] = preds\n\nif COMPUTE_CV:\n    train['f1'] = train.apply(getMetric('oof_w2v'),axis=1)\n    print('CV score for baseline =',train.f1.mean())","metadata":{"ExecuteTime":{"end_time":"2021-03-18T10:06:03.146166Z","start_time":"2021-03-18T10:06:01.83687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def combine_for_sub(row):\n    x = np.concatenate([row.oof_w2v,row.oof_cnn, row.oof_hash])\n    return ' '.join( np.unique(x) )\n\ndef combine_for_cv(row):\n    x = np.concatenate([row.oof_w2v,row.oof_cnn, row.oof_hash])\n    return np.unique(x)","metadata":{"ExecuteTime":{"end_time":"2021-03-18T10:06:04.931476Z","start_time":"2021-03-18T10:06:04.925838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if COMPUTE_CV:\n    tmp = train.groupby('label_group').posting_id.agg('unique').to_dict()\n    train['target'] = train.label_group.map(tmp)\n    train['oof'] = train.apply(combine_for_cv,axis=1)\n    train['f1'] = train.apply(getMetric('oof'),axis=1)\n    print('CV Score =', train.f1.mean() )\n\ntrain['matches'] = train.apply(combine_for_sub,axis=1)","metadata":{"ExecuteTime":{"end_time":"2021-03-18T10:06:09.759812Z","start_time":"2021-03-18T10:06:05.955972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[['posting_id','matches']].to_csv('submission.csv',index=False)\nsub = pd.read_csv('submission.csv')\nsub.head()","metadata":{"ExecuteTime":{"end_time":"2021-03-18T10:06:12.385916Z","start_time":"2021-03-18T10:06:12.180234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}