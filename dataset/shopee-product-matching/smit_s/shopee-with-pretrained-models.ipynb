{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport cv2\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport random\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\nimport inspect\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport torch\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tqdm\nfrom tqdm import tqdm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.read_csv('/kaggle/input/shopee-product-matching/sample_submission.csv')\ndf=pd.read_csv('/kaggle/input/shopee-product-matching/train.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x=LabelEncoder()\ndf['label_group']=x.fit_transform(df['label_group'].to_numpy())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['label_group'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install sentence_transformers\nfrom sentence_transformers import SentenceTransformer, util","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedder = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['title'].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Comment after encoding and saving\ntitle_embeddings = embedder.encode(df['title'])\nfor each in range(len(title_embeddings)):\n    df.at[each,'title']=title_embeddings[each]\n#comment this after saving embeddings and use the text file  by loading \n#torch.save(title_embeddings,'embed.txt')\ndel title_embeddings\ndel embedder","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df['title']=torch.load('kaggle/output/kaggle/working/embed.txt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()\ndirectory='/kaggle/input/shopee-product-matching/train_images/'\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications import Xception\nfrom tensorflow.keras.models import Sequential,Model\nfrom tensorflow.keras.layers import BatchNormalization,Input,LSTM,Dense,Input,Dropout,Concatenate,Flatten,Conv2D,MaxPooling2D,Flatten,Reshape,UpSampling2D\nfrom keras.layers.merge import concatenate\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n_datagen = ImageDataGenerator(\n            rescale=1./255.,\n            featurewise_center=False,\n            featurewise_std_normalization=False,\n            rotation_range=5,\n            width_shift_range=0,\n            height_shift_range=0,\n            horizontal_flip=False,\n            vertical_flip=False\n            )\nec=Sequential()\nec.add(Input(shape=(160,160,1)))\nec.add(Conv2D(64,3,padding='same'))\nec.add(BatchNormalization())\nec.add(MaxPooling2D((2,2),strides=2,padding='same'))\nec.add(Conv2D(32,3,padding='same'))\nec.add(MaxPooling2D((2,2),strides=2,padding='same'))\nec.add(Conv2D(16,3,padding='same'))\nec.add(MaxPooling2D((2,2),strides=2,padding='same'))\nec.add(Conv2D(8,3,padding='same'))\nec.add(MaxPooling2D((2,2),strides=2,padding='same'))\nec.add(Flatten())\nec.add(Dense(100))\nec.add(BatchNormalization())\nec.add(Dense(800))\nec.add(BatchNormalization())\nec.add(Reshape((10,10,8)))\nec.add(Conv2D(8,3,padding='same'))\nec.add(UpSampling2D((2,2)))\nec.add(Conv2D(16,3,padding='same'))\nec.add(UpSampling2D((2,2)))\nec.add(Conv2D(32,3,padding='same'))\nec.add(UpSampling2D((2,2)))\nec.add(Conv2D(64,3,padding='same'))\nec.add(UpSampling2D((2,2)))\nec.add(Conv2D(1,3,padding='same'))\nec.summary()\nec.compile(loss='mse',optimizer='adam')\nec.fit_generator(_datagen.flow_from_dataframe(df, target_size=((160,160)),\n                                                      color_mode='grayscale',\n                                                      directory='/kaggle/input/shopee-product-matching/train_images/',\n                                                      x_col='image',\n                                                      class_mode='input',\n                                                      shuffle=True,\n                                                      batch_size=32,\n                                                      seed=7,\n                                                      subset='training'),epochs=2)\n         ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bb=np.reshape(cv2.resize(cv2.imread(directory+df.loc[0,'image'],0),(160,160)),(160,160))\nprint(bb.shape)\nprint(ec.inputs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tqdm\n#Doing this to add array values to cell of df\ndf['image_embedding']=1\ndf=df.astype('object')\nextractor = Model(inputs=ec.inputs,\n                        outputs= ec.layers[10].output)\n\nfor i in tqdm.tqdm(range(len(df['image'].to_list()))):\n    aux=np.array(extractor.predict(np.reshape(cv2.resize(cv2.imread(directory+df.loc[i,'image'],0),(160,160)),(1,160,160)))[0])\n    df.at[i,\"image_embedding\"]=aux\n    del aux\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matching_list=pd.merge(df,df,on='label_group')\nmatching_list=matching_list.drop(matching_list[matching_list.loc[:,'posting_id_x']==matching_list.loc[:,'posting_id_y']].index)\nmatching_list.reset_index()\nmatching_list=matching_list.drop('label_group',axis=1)\nmatching_list['similarity']=1\nmatching_list.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"non_matching_list=pd.DataFrame(columns=df.columns)\nseen=[]\nsample=3\nfor i in tqdm.tqdm(range(df.shape[0])):\n    if(df.loc[i,'label_group'] not in seen):\n        seen.append(df.loc[i,'label_group'])\n        new_df_group=df[df['label_group']==int(df.loc[i,'label_group'])]\n        new_df_nogroup=df.drop(df[df['label_group']==int(df.loc[i,'label_group'])].index)\n        new_df_group['kw']=1\n        new_df_nogroup['kw']=1\n        if(i==0):\n            non_matching_list=pd.merge(new_df_group.iloc[random.sample(range(0, new_df_group.shape[0]), min(sample,new_df_group.shape[0]))],new_df_nogroup.iloc[random.sample(range(0, new_df_nogroup.shape[0]), sample)],on='kw',suffixes=['_x','_y'])\n        else:\n            non_matching_list=non_matching_list.append(pd.merge(new_df_group.iloc[random.sample(range(0, new_df_group.shape[0]),  min(sample,new_df_group.shape[0]))],new_df_nogroup.iloc[random.sample(range(0, new_df_nogroup.shape[0]), sample)],on='kw',suffixes=['_x','_y']))\n        del new_df_group\n        del new_df_nogroup","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"non_matching_list=non_matching_list.drop(columns=['label_group_x','label_group_y','kw'])\nnon_matching_list['similarity']=0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"non_matching_list.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# non_matching_list['title_x']=1\n# non_matching_list['image_embedding_x']=1\n# non_matching_list['title_y']=1\n# non_matching_list['image_embedding_y']=1\n# non_matching_list=non_matching_list.astype('object')\n\n# for i in tqdm.tqdm(range(non_matching_list.shape[0])):\n#     ind=df['posting_id'][df['posting_id']==non_matching_list.iloc[i,non_matching_list.columns.get_loc(\"posting_id_x\")]].index[0]\n#     non_matching_list.at[i,'title_x']=[df.iloc[ind,df.columns.get_loc('title')]]\n#     non_matching_list.at[i,'image_embedding_x']=[df.iloc[ind,df.columns.get_loc('image_embedding')]]\n#     ind=df['posting_id'][df['posting_id']==non_matching_list.iloc[i,non_matching_list.columns.get_loc(\"posting_id_y\")]].index[0]\n#     non_matching_list.at[i,'title_y']=[df.iloc[ind,df.columns.get_loc('title')]]\n#     non_matching_list.at[i,'image_embedding_y']=[df.iloc[ind,df.columns.get_loc('image_embedding')]]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matching_list.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=matching_list.append(non_matching_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del matching_list\ndel non_matching_list\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['similarity'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data.shape)\ndata=data.reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=data.drop(columns=['index'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_tr1,X_tr2,X_tr3,X_tr4=(data[\"title_x\"].to_numpy(),data[\"title_y\"].to_numpy(),data[\"image_embedding_x\"].to_numpy(),data[\"image_embedding_y\"].to_numpy())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_tr1=np.array(list(map( lambda vl: list(vl),X_tr1)))\nX_tr2=np.array(list(map( lambda vl: list(vl),X_tr2)))\nX_tr3=np.array(list(map( lambda vl: list(vl),X_tr3)))\nX_tr4=np.array(list(map( lambda vl: list(vl),X_tr4)))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(X_tr3)\nprint(X_tr1.shape)\n\n# ddd=np.array([np.array([2,4,5,3]),np.array([4,2,1,2])])\n# print(ddd.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"similarity=np.array(data[\"similarity\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['similarity'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.argwhere(np.isnan(X_tr3))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential,Model\nfrom tensorflow.keras.layers import LSTM,Dense,Input,Dropout,Concatenate,Flatten,Conv2D\nfrom keras.layers.merge import concatenate\n\nmodel0=Sequential()\nmodel0.add(Dense(300,activation='relu',input_shape=(768,)))\nmodel0.add(Dense(100,activation='relu'))\nmodel0.add(Dense(30,activation='relu'))\nmodel0.summary()\n\nmodel1=Sequential()\nmodel1.add(Dense(300,activation='relu',input_shape=(768,)))\nmodel1.add(Dense(100,activation='relu'))\nmodel1.add(Dense(30,activation='relu'))\nmodel1.summary()\n\n\n\nmodel2=Sequential()\nmodel2.add(Dense(100,activation='relu',input_shape=(100,)))\nmodel2.add(Dense(50,activation='relu'))\nmodel2.summary()\n\n\nmodel3=Sequential()\nmodel3.add(Dense(100,activation='relu',input_shape=(100,)))\nmodel3.add(Dense(50,activation='relu'))\nmodel3.summary()\n\n\nmodelx=concatenate([model0.output,model1.output,model2.output,model3.output])\nmodelx=Flatten()(modelx)\nmodelx=Dense(50,activation='relu')(modelx)\nmodelx=Dense(1,activation='sigmoid')(modelx)\n\nmodel=Model(inputs=[model0.input,model1.input,model2.input,model3.input],outputs=modelx)\nmodel.summary()\nmodel.compile(loss='binary_crossentropy',optimizer='adam')\nmodel.fit([X_tr1,X_tr2,X_tr3,X_tr4],similarity,epochs=2,batch_size=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test=pd.read_csv('/kaggle/input/shopee-product-matching/test.csv')","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedder = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\ntitle_embeddings = embedder.encode(df_test['title'])\nfor each in range(len(title_embeddings)):\n    df_test.at[each,'title']=title_embeddings[each]\n#comment this after saving embeddings and use the text file  by loading \n#torch.save(title_embeddings,'embed.txt')\ndel title_embeddings\ndel embedder","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"directory='/kaggle/input/shopee-product-matching/test_images/'\ndf_test['image_embedding']=1\ndf_test=df_test.astype('object')\nfor i in tqdm.tqdm(range(len(df_test['image'].to_list()))):\n    aux=np.array(extractor.predict(np.reshape(cv2.resize(cv2.imread(directory+df_test.loc[i,'image'],0),(160,160)),(1,160,160)))[0])\n    df_test.at[i,\"image_embedding\"]=aux","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['title'][0][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_post=[]\nfor i,tes in tqdm.tqdm(df_test.iterrows()):\n    all_comp=[]\n    X_ts1,X_ts2=(tes[\"title\"],tes[\"image_embedding\"])\n    X_ts1,X_ts2= np.reshape(X_ts1,(1,X_ts1.shape[0])),np.reshape(X_ts2,(1,X_ts2.shape[0]))\n    for j,tr  in df_test.iterrows():\n        X_ts3,X_ts4=(tr[\"title\"],tr[\"image_embedding\"])\n        X_ts3,X_ts4=np.reshape(X_ts3,(1,X_ts3.shape[0])),np.reshape(X_ts4,(1,X_ts4.shape[0]))\n        compare_val=model.predict([X_ts1,X_ts3,X_ts2,X_ts4])\n        all_comp.append({\"posting_id\":tr['posting_id'], \"val\":compare_val})\n    all_post.append({\"posting_id\":tes['posting_id'],\"vals\":all_comp})\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(all_post)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final=[]\nfor each in all_post:\n    lst=each['vals']\n    topk=sorted(lst, key=lambda i:i['val'],reverse=True)[: len(lst) if 50>=len(lst) else 50 ]\n    acul=[]\n    for val in topk:\n        if(float(val['val'])>0.5):\n            acul.append(val['posting_id'])  \n    final.append(acul)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"string_list=[]\nfor each in final:\n    string_list.append(' '.join(each))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"string_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub=pd.DataFrame(columns=['posting_id','matches'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub['posting_id']=pd.Series(df_test['posting_id'])\nsub['matches']=pd.Series(string_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv('submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}