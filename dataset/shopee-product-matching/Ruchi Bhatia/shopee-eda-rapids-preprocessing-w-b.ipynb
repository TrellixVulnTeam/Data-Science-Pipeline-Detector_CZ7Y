{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div>\n    <center><img src=\"https://i.imgur.com/mqPVRT5.png\"></center>\n    </div>","metadata":{}},{"cell_type":"markdown","source":"<center><h1>Introduction üìù</h1></center>\n\n> üéØGoal: To build a model that predicts which items are the same products\n> \n> As a shopaholicüõçÔ∏è , I admit getting the best deals for products is a very rewarding experience. Scanning through multiple shopping websites to get the perfect deal and keeping an eye on upcoming sales is one manual way to go about.\n> \n> We often find retail companies offering recommendations in which they promote their products in such a way that customers tend to get swayed and pick a similar product that is priced lower. Product matching üìãüìã is one of these strategies wherein a company to offers products at rates that are competitive to the same product sold by another retailer. \n> \n> These matches can be performed automatically with the help of machine learning and that is the goal of this competition. We have been provided with data of **Shopee**, which is the leading e-commerce platform in Southeast Asia and Taiwan. ","metadata":{}},{"cell_type":"markdown","source":"<center><h1>Diving into the Data ü§ø </h1></center>\n\n> **train/test.csv** - Each row contains the data for a single posting. \n> \n> ‚ÑπÔ∏èMultiple postings might have the exact same image ID, but with different titles or vice versa.\n> \n> - posting_id : the ID code for the posting\n> - image : the image id/md5sum\n> - image_phash : a perceptual hash of the image\n> - title : the product description for the posting\n> - label_group : ID code for all postings that map to the same product. Not provided for the test set\n> - matches - **Space delimited** list of all posting IDs that match a particular posting. \n> \n> üìåPosts always self-match. \n> \n> üìå**Group sizes were capped at 50**, so we need not predict more than 50 matches for a posting.","metadata":{}},{"cell_type":"markdown","source":"<h1><center>Evaluation metric: <b>F1-score üß™</b> </center></h1>\n\n> The evaluation metric for this competition is F1-Score or F-Score.\n> \n> <center><img src=\"https://www.gstatic.com/education/formulas2/355397047/en/f1_score.svg\"></center>\n> \n>  It finds the balance between precision and recall.\n>  <center><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/d37e557b5bfc8de22afa8aad1c187a357ac81bdb\"></center>\n>  <center><img src=\"https://miro.medium.com/max/560/1*AEV3TE67ahMn3NVpU0ov4g.png\" height=10></center>\n>  \n>  where-\n>  - TP = True Positive\n>  - FP = False Positive\n>  - TN = True Negative\n>  - FN = False Negative","metadata":{}},{"cell_type":"code","source":"from IPython.core.display import display, HTML, Javascript\n\ndef nb():\n    styles = open(\"../input/intermediate-notebooks-data/custom-orange.css\", \"r\").read()\n    return HTML(\"<style>\"+styles+\"</style>\")\nnb()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<center><h1>Import Libraries üìö</h1></center>","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np \nimport pandas as pd \nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cuml, cudf, cupy\nimport nltk\nimport tensorflow as tf\nimport wandb\n\nfrom pandas import DataFrame\nfrom sklearn.feature_extraction.text import CountVectorizer as CV\nfrom nltk.corpus import stopwords\nfrom cuml.feature_extraction.text import CountVectorizer\nfrom cuml.neighbors import NearestNeighbors\nfrom colorama import Fore, Back, Style\nfrom wordcloud import WordCloud,STOPWORDS\nfrom tensorflow.keras.applications import ResNet101\nfrom PIL import Image\n\nnltk.download('stopwords')\n\n# colored output\ny_ = Fore.YELLOW\nr_ = Fore.RED\ng_ = Fore.GREEN\nb_ = Fore.BLUE\nm_ = Fore.MAGENTA","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<center><img src=\"https://camo.githubusercontent.com/dd842f7b0be57140e68b2ab9cb007992acd131c48284eaf6b1aca758bfea358b/68747470733a2f2f692e696d6775722e636f6d2f52557469567a482e706e67\"></center>\n\nI will be integrating ```W&B``` for ```visualizations``` and ```logging artifacts```!\n\n[Shopee Project on W&B Dashboard](https://wandb.ai/ruchi798/shopee?workspace=user-ruchi798) üèãÔ∏è‚Äç‚ôÄÔ∏è\n\n* To get the API key, an account is to be created on the website first.\n* Next, use secrets to use API Keys more securelyü§´","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\napi_key = user_secrets.get_secret(\"api_key\")\n\nos.environ[\"WANDB_SILENT\"] = \"true\"","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! wandb login $api_key","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<center><h1>Reading csv files üìñ</h1></center>","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"../input/shopee-product-matching/train.csv\")\ntest_df = pd.read_csv(\"../input/shopee-product-matching/test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<center><h1>Getting image paths from the directory üõ£Ô∏è</h1></center>","metadata":{}},{"cell_type":"code","source":"# specifying directory paths\n\ntrain_jpg_directory = '../input/shopee-product-matching/train_images/'\ntest_jpg_directory = '../input/shopee-product-matching/test_images/'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to get image paths from train and test directory\n\ndef getImagePaths(path):\n    image_names = []\n    for dirname, _, filenames in os.walk(path):\n        for filename in filenames:\n            fullpath = os.path.join(dirname, filename)\n            image_names.append(fullpath)\n    return image_names","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_images_path = getImagePaths(train_jpg_directory)\ntest_images_path = getImagePaths(test_jpg_directory)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Number of images in each directory","metadata":{}},{"cell_type":"code","source":"print(f\"{y_}Number of train images: {g_} {len(train_images_path)}\\n\")\nprint(f\"{y_}Number of test images: {g_} {len(test_images_path)}\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking if images in each directory have the same shape","metadata":{}},{"cell_type":"code","source":"def getShape(images_paths):\n    shape = cv2.imread(images_paths[0]).shape\n    for image_path in images_paths:\n        image_shape=cv2.imread(image_path).shape\n        if (image_shape!=shape):\n            return \"Different image shape\"\n        else:\n            return \"Same image shape \" + str(shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"getShape(train_images_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"getShape(test_images_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<center><h1>Displaying images üì∑ </h1></center>","metadata":{}},{"cell_type":"code","source":"# function to display multiple images\n\ndef display_multiple_img(images_paths, rows, cols,title):\n    \n    figure, ax = plt.subplots(nrows=rows,ncols=cols,figsize=(16,8))\n    plt.suptitle(title, fontsize=20)\n    for ind,image_path in enumerate(images_paths):\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n        try:\n            ax.ravel()[ind].imshow(image)\n            ax.ravel()[ind].set_axis_off()\n        except:\n            continue;\n    plt.tight_layout()\n    plt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_multiple_img(train_images_path[0:25], 5, 5,\"Train images\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_multiple_img(test_images_path, 1, 3,\"Test images\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<center><h1>Colour Histograms üé®</h1></center>","metadata":{}},{"cell_type":"code","source":"def styling():\n    for spine in plt.gca().spines.values():\n        spine.set_visible(False)\n        plt.xticks([])\n        plt.yticks([])","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def hist(image_path):\n    plt.figure(figsize=(16, 3))\n    \n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n    \n    plt.subplot(1, 5, 1)\n    plt.imshow(img)\n    styling()\n    \n    custom_colors = [\"#ef233c\", \"#76da71\", \"#2667ff\",\"#aea3b0\"]\n    labels = ['Red Channel', 'Green Channel', 'Blue Channel','Total']\n    \n    for i in range(1,4):\n        plt.subplot(1, 5, i+1)\n        plt.hist(img[:, :, i-1].reshape(-1),bins=64,color=custom_colors[i-1],alpha = 0.6)\n        plt.xlabel(labels[i-1],fontsize=10)\n        styling()\n        \n    plt.subplot(1, 5, 5)\n    plt.hist(img.reshape(-1),bins=128,color=custom_colors[3],alpha = 0.6)\n    plt.xlabel(labels[3],fontsize=10)\n    styling()\n    plt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_hist(images_paths):\n        for ind,image_path in enumerate(images_paths):\n            if (ind<6):\n                hist(image_path)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_hist(train_images_path[5:10])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_hist(test_images_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Visualizing and querying the dataset** with W&B üèãÔ∏è‚Äç‚ôÄÔ∏è\n\n[Documentation](https://docs.wandb.ai/datasets-and-predictions)","metadata":{}},{"cell_type":"code","source":"# initializing the run\nrun = wandb.init(project=\"shopee\",\n                 job_type=\"upload\",\n                 config={\n                     \"num_examples\" : 8\n                 })\n\n# creating an artifact \nartifact = wandb.Artifact(name=\"histograms\", type=\"raw_data\")\n\n# setting up a WandB Table object to hold the dataset\ncolumns=[\"id\", \"raw image\", \"red channel\",\"green channel\",\"blue channel\",\"label\"]\n\ntable = wandb.Table(\n    columns=columns\n)\n\n# filling up the table\nimages_train = [f for f in train_images_path[5:10]]\nimages_test = test_images_path\n\nall_images = images_train + images_test\nlabels = [\"train\",\"train\",\"train\",\"train\",\"train\",\"test\",\"test\",\"test\"]\n\nfor ndx in range(wandb.config.num_examples):\n    img_file = all_images[ndx]\n    train_id = img_file.split(\"/\")[4].split(\".\")[0]\n    \n    img = cv2.imread(img_file)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n\n  # raw image\n    raw_img = wandb.Image(img_file)\n    \n  # plotting histograms \n    wb_color = [\"#ef233c\",\"#76da71\",\"#2667ff\"]\n    def wb_hist(i):\n        plt.figure(figsize=(16, 10))\n        plt.hist(img[:, :, i-1].reshape(-1),bins=64,color=wb_color[i-1],alpha = 0.6)\n        return wandb.Image(plt)\n    red = wb_hist(1)\n    green = wb_hist(2)\n    blue = wb_hist(3)\n    \n    # adding an artifact file\n    artifact.add_file(img_file, os.path.join(\"images\", train_id + \"_train_id.png\"))\n\n  # adding a row to the table\n    row = [train_id, raw_img, red,green,blue,labels[ndx]]\n    table.add_data(*row)\n    \n# adding the table to the artifact\nartifact.add(table, \"raw_examples\")\n    \n# logging the artifact\nrun.log_artifact(artifact)\n\nrun.finish()","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is a snapshot of the table I just created and added to an artifact.\n\n![](https://i.imgur.com/oF7CloS.png)","metadata":{}},{"cell_type":"markdown","source":"**Different versions** of the artifacts can be stored in W&B.\n\n**Comparison of any two artifact versions** in the table is possible. \n\nHere I'm comparing the ```highlighted versions``` in the left sidebar, i.e., ```v1``` with ```v2``` in a split panel view ‚¨áÔ∏è\n\nWe can see values from both artifact versions in a single table. \n\n![](https://i.imgur.com/9AVDPjQ.png)","metadata":{}},{"cell_type":"markdown","source":"We can even specify filters on any column to **limit the visible rows down to only rows that match**. \n\nHere I've filtered the table to see only the ```test images```.\n\n![](https://i.imgur.com/8eTOBbX.png)","metadata":{}},{"cell_type":"markdown","source":"We have 11014 unique label groups for products.","metadata":{}},{"cell_type":"code","source":"train_df['label_group'].nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_labels_count = train_df['label_group'].value_counts()\n\n# getting count for most frequent and least frequent label groups\nmost_freq = train_labels_count[train_labels_count == train_labels_count.max()]\nless_freq = train_labels_count[train_labels_count == train_labels_count.min()]\n\n# getting most frequent and least frequent label groups\nm_label = np.unique(train_df['label_group'][train_df['label_group'].isin(most_freq.index)].values)\nl_label = np.unique(train_df['label_group'][train_df['label_group'].isin(less_freq.index)].values)\n\nprint(f\"{m_} Most frequent label group: \", m_label)\nprint(f\"{y_} Less frequent label group: \", l_label)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Logging a dictionary of custom objects** üèãÔ∏è‚Äç‚ôÄÔ∏è","metadata":{}},{"cell_type":"code","source":"run = wandb.init(project='shopee', name='count')\n\nmw = train_labels_count.max()\nlw = train_labels_count.min()\nuw = train_df['label_group'].nunique()\n\nwandb.log({'Unique label groups': uw, \n           'Most frequent label groups': mw, \n           'Least frequent label groups': lw})\n\nrun.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def path(group,m):\n    PATH = \"../input/shopee-product-matching/train_images/\"\n    \n    #label\n    if m=='l':\n        z = train_df['image'][train_df['label_group']==group].values\n    \n    #title\n    if m=='t':\n        z = train_df['image'][train_df['title']==group].values\n   \n    image_names = []\n    for filename in z:\n        fullpath = os.path.join(PATH, filename)\n        image_names.append(fullpath)\n    return image_names","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<center><h1>Most frequent label groups üìà</h1></center>","metadata":{}},{"cell_type":"code","source":"lg = 159351600\ndisplay_multiple_img(path(lg,'l'), 3, 3,lg)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lg = 994676122\ndisplay_multiple_img(path(lg,'l'), 3, 3,lg)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lg = 562358068\ndisplay_multiple_img(path(lg,'l'), 3, 3,lg)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<center><h1>Least frequent label groups üìâ</h1></center>","metadata":{}},{"cell_type":"code","source":"lg = 297977\ndisplay_multiple_img(path(lg,'l'), 1, 2,lg)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lg = 887886\ndisplay_multiple_img(path(lg,'l'), 1, 2,lg)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lg = 4293276364\ndisplay_multiple_img(path(lg,'l'), 1, 2,lg)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the shape of the training dataframe and number of unique titles differ, we can infer that we have images with the same title.","metadata":{}},{"cell_type":"code","source":"train_df['title'].nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t = train_df['title'].value_counts().sort_values(ascending=False).reset_index()\nt.columns = ['title','count']\nt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<center><h1>Images with the same title ü¶âü¶â</h1></center>","metadata":{}},{"cell_type":"code","source":"img_title = \"Koko syubbanul muslimin koko azzahir koko baju\"\ndisplay_multiple_img(path(img_title,'t'), 3, 3,img_title)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_title = \"Baju Koko Pria Gus Azmi Syubbanul Muslimin Kombinasi Hadroh Azzahir Hilw HO187 KEMEJA KOKO PRIA BAJU\"\ndisplay_multiple_img(path(img_title,'t'), 4, 2, img_title)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_title = \"Monde Boromon Cookies 1 tahun+ 120gr\"\ndisplay_multiple_img(path(img_title,'t'), 2, 3, img_title)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> **Observations from EDA**üìù:\n> \n> * Visually similar images in different label groups\n> * Same images with different titles\n> * Same titles have different images","metadata":{}},{"cell_type":"markdown","source":"<center><h1>Wordcloud of image titles ‚òÅÔ∏è</h1></center>","metadata":{}},{"cell_type":"markdown","source":"**Logging an image** of the wordcloud of image titlesüèãÔ∏è‚Äç‚ôÄÔ∏è","metadata":{}},{"cell_type":"code","source":"# color function for the wordcloud\ndef color_wc(word=None,font_size=None,position=None, orientation=None,font_path=None, random_state=None):\n    h = int(360.0 * 21.0 / 255.0)\n    s = int(100.0 * 255.0 / 255.0)\n    l = int(100.0 * float(random_state.randint(80, 120)) / 255.0)\n    return \"hsl({}, {}%, {}%)\".format(h, s, l)\n\n\nrun = wandb.init(project='shopee', job_type='image-visualization',name='wordCloud')\n\nfig = plt.gcf()\nfig.set_size_inches(16, 8)\n\nwc = WordCloud(stopwords=STOPWORDS,background_color=\"white\", contour_width=2, contour_color='orange',width=1500, height=750,color_func=color_wc,max_words=150, max_font_size=256,random_state=42)\nwc.generate(' '.join(train_df['title']))\nfig = plt.imshow(wc, interpolation=\"bilinear\")\nfig = plt.axis('off')\n\nwandb.log({\"wordcloud\": [wandb.Image(plt, caption=\"Wordcloud\")]})\nrun.finish()\n\nrun","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<center><h1>Unigrams, bigrams and trigrams üî¢ </h1></center>","metadata":{}},{"cell_type":"code","source":"def get_top_n_words(corpus, n=None):\n    vec = CV().fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ndef get_top_n_bigram(corpus, n=None):\n    vec = CV(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\n\ndef get_top_n_trigram(corpus, n=None):\n    vec = CV(ngram_range=(3, 3)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_bt(x,w,p):\n    common_words = x(train_df['title'], 20)\n    common_words_df = DataFrame (common_words,columns=['word','freq'])\n\n    plt.figure(figsize=(16, 10))\n    sns.barplot(x='freq', y='word', data=common_words_df,palette=p)\n    plt.title(\"Top 20 \"+ w , fontsize=16)\n    plt.xlabel(\"Frequency\", fontsize=14)\n    plt.yticks(fontsize=13)\n    plt.xticks(rotation=45, fontsize=13)\n    plt.ylabel(\"\");\n    return common_words_df","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"common_words = get_top_n_words(train_df['title'], 20)\ncommon_words_df1 = DataFrame(common_words,columns=['word','freq'])\nplt.figure(figsize=(16, 8))\nax = sns.barplot(x='freq', y='word', data=common_words_df1,palette='Oranges')\n\nplt.title(\"Top 20 unigrams\", fontsize=16)\nplt.xlabel(\"Frequency\", fontsize=14)\nplt.yticks(fontsize=13)\nplt.xticks(rotation=45, fontsize=13)\nplt.ylabel(\"\");\n\ncommon_words_df2 = plot_bt(get_top_n_bigram,\"bigrams\",'BuGn')\ncommon_words_df3 = plot_bt(get_top_n_trigram,\"trigrams\",'RdPu')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Logging custom bar charts** for unigrams, bigrams and trigramsüèãÔ∏è‚Äç‚ôÄÔ∏è","metadata":{}},{"cell_type":"code","source":"def plot_wb(df, name, title): \n    run = wandb.init(project='shopee', job_type='image-visualization',name=name)\n\n    labels = df.sort_values('freq', ascending=False).word\n    values = df.sort_values('freq', ascending= False).freq\n    dt = [[label, val] for (label, val) in zip(labels, values)]\n    table = wandb.Table(data=dt, columns = [\"Word\", \"Frequency\"])\n    wandb.log({name : wandb.plot.bar(table, \"Word\", \"Frequency\",title=title)})\n\n    run.finish()\n    \nplot_wb(common_words_df1, \"unigrams\",\"Top 20 unigrams\")\nplot_wb(common_words_df2, \"bigrams\",\"Top 20 bigrams\")\nplot_wb(common_words_df3, \"trigrams\",\"Top 20 trigrams\")","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<center><h1>Plugging in RAPIDS üèÉ‚Äç‚ôÄÔ∏è </h1></center>\n<center><img src=\"https://i.imgur.com/qWulN0F.jpg\" height=40></center>","metadata":{}},{"cell_type":"code","source":"train_df_c = cudf.from_pandas(train_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df_c","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<center><h3>Pre-processing title ‚úÇÔ∏è</h3></center>","metadata":{}},{"cell_type":"code","source":"STOPWORDS = nltk.corpus.stopwords.words('english')\n\npunctuation = [ '!', '\"', '#', '$', '%', '&', '(', ')', '*', '+', '-', '.', '/',  '\\\\', ':', ';', '<', '=', '>',\n           '?', '@', '[', ']', '^', '_', '`', '{', '|', '}', '\\t','\\n',\"'\",\",\",'~' , '‚Äî']\n\ndef text_preprocessing(input_text, filters=None, stopwords=STOPWORDS):\n    # filter punctuation \n    translation_table = {ord(char): ord(' ') for char in filters}\n    input_text = input_text.str.translate(translation_table)\n    \n    #convert to lower case\n    input_text = input_text.str.lower()\n        \n    # remove stopwords \n    stopwords_gpu = cudf.Series(stopwords)\n    input_text =  input_text.str.replace_tokens(stopwords_gpu, ' ')\n        \n    # normalize spaces\n    input_text = input_text.str.normalize_spaces( )\n    \n    # strip leading and trailing spaces\n    input_text = input_text.str.strip(' ')\n    \n    return input_text\n\ndef preprocess_df(df, col, **kwargs):\n    df[col] = text_preprocessing(df[col], **kwargs)\n    return  df\n\n%time \ndf = preprocess_df(train_df_c,'title', filters=punctuation)\n\ntrain_df_c.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df_c.to_csv(\"title_preprocessed_dataset.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Logging** the preprocessed title dataset as **an artifact**üèãÔ∏è‚Äç‚ôÄÔ∏è","metadata":{}},{"cell_type":"code","source":"# run = wandb.init(project='shopee', name='title_preprocessed')\n\n# artifact = wandb.Artifact('title_preprocessed_dataset', type='dataset')\n\n# add a file to the artifact's contents\n# artifact.add_file(\"title_preprocessed_dataset.csv\")\n\n# save the artifact version to W&B and mark it as the output of this run\n# run.log_artifact(artifact)\n\n# run.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<center><h3>CountVectorizer for Feature Extraction üìê</h3></center>","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://i.imgur.com/1bEOBR1.png\">","metadata":{}},{"cell_type":"markdown","source":"[Documentation](https://docs.rapids.ai/api/cuml/nightly/api.html#cuml.feature_extraction.text) üìñ","metadata":{}},{"cell_type":"code","source":"vec = CountVectorizer(stop_words='english', binary=True)\n%time X = vec.fit_transform(train_df_c.title).toarray()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<center><h3>Titles with similar text ü¶âü¶â</h3></center>","metadata":{}},{"cell_type":"code","source":"n = 50\nknn = NearestNeighbors(n_neighbors=n)\nknn.fit(X)\ndistances, indices = knn.kneighbors(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for k in range(5):\n    plt.figure(figsize=(20,3))\n    plt.plot(np.arange(50),cupy.asnumpy(distances[k,]),'o-',color='#f48c06')\n    plt.title('Text Distance From Train Row %i to Other Train Rows'%k,fontsize=15, fontweight='bold',horizontalalignment='center',fontfamily='serif')\n    plt.ylabel('Distance to Train Row %i'%k,fontsize=13, fontweight='bold',fontfamily='serif')\n    plt.xlabel('Index Sorted by Distance to Train Row %i'%k,fontsize=13, fontweight='bold',fontfamily='serif')\n    plt.show()\n    \n    print( train_df_c.loc[cupy.asnumpy(indices[k,:10]),['title','label_group']] )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<center><h3>Similar Imagesü¶âü¶â</h3></center>","metadata":{}},{"cell_type":"code","source":"class DataGenerator(tf.keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, df, img_size=256, batch_size=32, path=train_jpg_directory): \n        self.df = df\n        self.img_size = img_size\n        self.batch_size = batch_size\n        self.path = path\n        self.indexes = np.arange(len(self.df))\n        \n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        ct = len(self.df) // self.batch_size\n        ct += int(((len(self.df)) % self.batch_size)!=0)\n        return ct\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        X = self.__data_generation(indexes)\n        return X\n            \n    def __data_generation(self, indexes):\n        'Generates data containing batch_size samples' \n        X = np.zeros((len(indexes),self.img_size,self.img_size,3),dtype='float32')\n        df = self.df.iloc[indexes]\n        for i,(index,row) in enumerate(df.iterrows()):\n            img = cv2.imread(self.path+row.image)\n            X[i,] = cv2.resize(img,(self.img_size,self.img_size))\n        return X","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = ResNet101(weights='imagenet',include_top=False, pooling='avg', input_shape=None)\ntrain_gen = DataGenerator(train_df, batch_size=128)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ie = model.predict(train_gen,verbose=1)\n# np.save(\"image_embedding_val.npy\", ie)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Logging** the image embeddings as **an artifact**üèãÔ∏è‚Äç‚ôÄÔ∏è\n\nThis helps me to save on time since the model need not be trained over and over againü•≥","metadata":{}},{"cell_type":"code","source":"# run = wandb.init(project='shopee', name='image_embedding_val')\n\n# artifact = wandb.Artifact(name='image_embedding_val', type='dataset')\n\n# Add a file to the artifact's contents\n# artifact.add_file(\"image_embedding_val.npy\")\n\n# Save the artifact version to W&B and mark it as the output of this run\n# run.log_artifact(artifact)\n\n# run.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A snapshot of the newly created artifacts ‚¨áÔ∏è","metadata":{}},{"cell_type":"markdown","source":"![](https://i.imgur.com/sn9xTWx.png)","metadata":{}},{"cell_type":"markdown","source":"Since I have already logged the image embeddings artifact, I can directly use it in this manner ‚¨áÔ∏è","metadata":{}},{"cell_type":"code","source":"run = wandb.init()\n\n# query W&B for an artifact and mark it as input to this run\nartifact = run.use_artifact('ruchi798/shopee/image_embedding_val:v0', type='dataset')\n\n# download the artifact's contents\nartifact_dir = artifact.download()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = os.path.join(artifact_dir, \"image_embedding_val.npy\")\nimg_embeddings = np.load(path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n = 50\nknn = NearestNeighbors(n_neighbors=n)\nknn.fit(img_embeddings)\ndistances, indices = knn.kneighbors(img_embeddings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROWS=2\nCOLS=4\nfor c in range(75,85):\n    print(\"Cluster \",c)  \n    t = train_df.loc[cupy.asnumpy(indices[c,:8])]   \n    for k in range(ROWS):\n        plt.figure(figsize=(20,5))\n        for j in range(COLS):\n            row = COLS*k + j\n            name = t.iloc[row,1]\n            img = cv2.imread(train_jpg_directory+name)\n            \n            #converting from BGR to RGB\n            img = img[:, :, ::-1]\n            \n            plt.subplot(1,COLS,j+1)\n            plt.axis('off')\n            plt.imshow(img)\n        plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here's a snapshot of my [project](https://wandb.ai/ruchi798/shopee?workspace=user-ruchi798) ‚¨áÔ∏è\n\n![](https://i.imgur.com/PYEnRRo.png)","metadata":{}},{"cell_type":"markdown","source":"References üìú\n- [RAPIDS cuML TfidfVectorizer and KNN](https://www.kaggle.com/cdeotte/rapids-cuml-tfidfvectorizer-and-knn)\n- [A very detailed explanation for data generation](https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly)\n\nInspiration üí°\n- [Custom Jupyter Notebook Theme with plain CSS](https://medium.com/@formigone/my-first-custom-theme-for-jupyter-notebook-a9c1e69efdfe) üé®\n\nIllustrations tools ‚ö°\n- [Canva](https://www.canva.com/en_gb/) üñåÔ∏è","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://i.imgur.com/pl3FhXV.png\">","metadata":{}}]}