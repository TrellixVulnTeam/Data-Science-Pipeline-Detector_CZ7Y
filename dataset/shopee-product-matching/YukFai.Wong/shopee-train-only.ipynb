{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# library","metadata":{"id":"UeE16v566o2F"}},{"cell_type":"code","source":"import json\nimport random\nrandom.seed(27)\nfrom functools import partial\nfrom collections import defaultdict\nfrom multiprocessing import Pool","metadata":{"id":"8g6NTNAj6o2Q","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","id":"wwTUozyg6o2S","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport albumentations as A","metadata":{"id":"RsBxe7uB6o2U","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n# import tensorflow_addons as tfa\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","metadata":{"id":"zpxdl5ge6o2U","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BertTokenizer","metadata":{"id":"s4LW3iUxiP5v","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data","metadata":{"id":"_PdAtmdD6o2V"}},{"cell_type":"markdown","source":"## Load csv","metadata":{"id":"W5rokGas6o2W"}},{"cell_type":"code","source":"path = '/kaggle/input/shopee-product-matching/'","metadata":{"id":"P0EIzavk6o2W","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_csv = pd.read_csv(path+'train.csv')\ntrain_csv.head()","metadata":{"id":"bCpzNUkU6o2X","outputId":"c2acd3db-0eb1-4d80-e468-09ddb36ff491","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_img_name = train_csv['image']\ntrain_img_path = '/kaggle/input/shopee-product-matching/train_images/' + train_csv['image']\ntrain_title = train_csv['title']\ntrain_label = train_csv['label_group']\n","metadata":{"id":"m8JtLnDA6o2Z","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_img_path[0]","metadata":{"id":"oa24AqFp6o2a","outputId":"881ad367-e6fc-4e4d-8aa6-f26bfa5e1881","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Title property","metadata":{"id":"XB8tI1fPoIAT"}},{"cell_type":"code","source":"print(\"Max words: {}\".format(train_title.map(lambda x: len(x.split())).max()))\nprint(\"Min words: {}\".format(train_title.map(lambda x: len(x.split())).min()))\nprint(\"Mean words: {}\".format(train_title.map(lambda x: len(x.split())).mean()))","metadata":{"id":"owl3aJuZnpHH","outputId":"a88962d7-a0fe-468f-bf5c-2c8419c9f551","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since we have 61 words at max and the token take extra dims, it will tokenize as 300 dim for safe","metadata":{"id":"2IVyoQMZDGO2"}},{"cell_type":"code","source":"text_token_dims = 300\n\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\ntrain_title_token = np.array(tokenizer(train_title.tolist(),\n                                       padding='max_length',\n                                       truncation=True , \n                                       max_length=text_token_dims)['input_ids'],\n                             dtype = np.uint32)","metadata":{"id":"O07RFR7vDEW7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load image","metadata":{"id":"Bx3j0N8B6o2d"}},{"cell_type":"code","source":"#train_img = (np.array(list(map(lambda x: cv2.imread(path+'train_images/'+x), train_img_name))))","metadata":{"id":"wa8ZWDd_6o2e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"crush due to not enough memory, change the data loading method to lazy loading","metadata":{"id":"aM8lhDAE6o2e"}},{"cell_type":"markdown","source":"# Data Generator","metadata":{"id":"nPOEjrq-6o2f"}},{"cell_type":"markdown","source":"## Triplet Data Problems","metadata":{"id":"q03IakWS6o2f"}},{"cell_type":"code","source":"print('Number of training data = {}'.format(len(train_img_name)))\nprint('Number of unique lable = {}'.format(len(set(train_img_name))))","metadata":{"id":"vEH2Iexb6o2g","outputId":"fd1c854f-213c-408b-9b90-5dbd21a23973","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_label(train_label):\n    unique, counts = np.unique(train_label, return_counts=True)\n    unique, counts = np.unique(counts, return_counts=True)\n    print(dict(zip(unique, counts)))\n    plt.plot(counts)\n    plt.title(\"Distribution of the number of images in each labels class\")\n    plt.xlabel(\"the number of images in each labels class\")\n    plt.ylabel(\"Count\")\n    plt.show()\ncount_label(train_label)","metadata":{"id":"cjUbD9m36o2g","outputId":"fc6a3e7a-e18f-49d4-8530-4917b08e10b2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"One problems is that most of the image has only 2-3 pair, which mean the varience in positive image is not as much as negative image. One soluation is that we can use those images with storng argumentation as the positive image.","metadata":{"id":"-_LQbEfv6o2h"}},{"cell_type":"markdown","source":"## Triplet Data ","metadata":{"id":"DUnP9cO16o2h"}},{"cell_type":"code","source":"class TripletDataGenerator():\n    def __init__(self, imgs_path, title_token, labels, text_token_dims = text_token_dims, resize = None):\n        ##imgs_path: list of imgs path\n        ##labels shape:(num, )\n        self.imgs_path = imgs_path\n        self.labels = labels\n        self.resize = resize\n\n        self.title_token = title_token\n\n        self.Y2X = defaultdict(list)\n        for i in range(len(labels)):\n            self.Y2X[labels[i]].append([imgs_path[i], self.title_token[i]])\n        self.Y2X_keys = list(self.Y2X.keys())\n\n    def augment(self, img):\n        augmentation_option = A.Compose([\n            A.Rotate(limit = (-20,20), p=0.5),\n            A.RandomScale(scale_limit = (0.7, 0.9), p=0.5),\n            A.RandomCrop(width=img.shape[1]*3//4, height=img.shape[0]*3//4, p = 0.7),\n            A.JpegCompression(quality_lower=90, quality_upper=100, p=0.5),\n            A.GaussNoise(var_limit=(20.0, 50.0), p=0.5),\n            A.RandomBrightnessContrast(brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), p=0.5),\n            A.RandomGamma(gamma_limit=(171, 200), p=0.5)\n        ])\n        return augmentation_option(image=img)[\"image\"]\n    def augment_st(self, img):\n        augmentation_option = A.Compose([\n            A.Rotate(limit = (-20,20), p=0.8),\n            A.RandomScale(scale_limit = (0.7, 0.9), p=0.8),\n            A.RandomCrop(width=img.shape[1]*3//4, height=img.shape[0]*3//4, p = 0.8),\n            A.JpegCompression(quality_lower=90, quality_upper=100, p=0.8),\n            A.GaussNoise(var_limit=(20.0, 50.0), p=0.8),\n            A.RandomBrightnessContrast(brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), p=0.8),\n            A.RandomGamma(gamma_limit=(171, 200), p=0.8)\n        ])\n        return augmentation_option(image=img)[\"image\"]\n\n    def __call__(self, anchor_idx):\n        anchor_label = self.labels[anchor_idx]\n        anchor_path = self.imgs_path[anchor_idx]\n\n        PositiveCandidate = self.Y2X[anchor_label]\n        Positive_choice = random.choice(PositiveCandidate)\n        \n\n        Negative_label_Candidate = random.choice(self.Y2X_keys)\n        while (anchor_label == Negative_label_Candidate):\n            Negative_label_Candidate = random.choice(self.Y2X_keys)\n        Negative_Candidate = self.Y2X[Negative_label_Candidate]\n        Negative_choice = random.choice(Negative_Candidate)\n        \n        anchor_img = cv2.imread(anchor_path)[...,[2,1,0]]\n        if(anchor_path == Positive_choice[0]):\n            positive_img = self.augment_st(cv2.imread(Positive_choice[0])[...,[2,1,0]])\n        else:\n            positive_img = self.augment(cv2.imread(Positive_choice[0])[...,[2,1,0]])\n        negative_img = cv2.imread(Negative_choice[0])[...,[2,1,0]]\n\n        if (self.resize != None):\n            dim = (self.resize,self.resize)\n            anchor_img = cv2.resize(anchor_img, dim, interpolation = cv2.INTER_AREA)\n            positive_img = cv2.resize(positive_img, dim, interpolation = cv2.INTER_AREA)\n            negative_img = cv2.resize(negative_img, dim, interpolation = cv2.INTER_AREA)\n\n        anchor_title = self.title_token[anchor_idx]\n        Positive_title = Positive_choice[1]\n        Negative_title = Negative_choice[1]\n\n        return [anchor_img, positive_img, negative_img, anchor_title, Positive_title, Negative_title]\n\nexample_Generator = TripletDataGenerator(train_img_path, train_title_token ,train_label, resize = 224, text_token_dims = text_token_dims)","metadata":{"id":"jpttmTPD6o2h","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_triplet(x):\n    num = 5\n    plt.figure(figsize=(20,12))\n    for i in range(num):\n        anchor_img, positive_img, negative_img, anchor_title, Positive_title, Negative_title  = x(i)\n        plt.subplot(3,num,i+1)\n        plt.imshow(anchor_img)\n        plt.title('Ancher')\n        print(anchor_title)\n        plt.subplot(3,num,i+num+1)\n        plt.imshow(positive_img)\n        plt.title('Positive')\n        print(Positive_title)\n        plt.subplot(3,num,i+num+num+1)\n        plt.imshow(negative_img)\n        plt.title('Negative')\n        print(Negative_title)\n        print('-------------------')\n    plt.show()\nshow_triplet(example_Generator)","metadata":{"id":"2Nmb88UR6o2i","outputId":"36b92813-9199-4624-ab4d-64a37edcb168","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train test split","metadata":{"id":"3AscA89j6o2j"}},{"cell_type":"markdown","source":"the val size need to be small in order to fit in memory","metadata":{"id":"KX7D-JPP6o2j"}},{"cell_type":"code","source":"train_img_idx = np.arange(len(train_label))\ntrain_img_idx_split, train_label_split = train_img_idx, train_label","metadata":{"id":"IkJg8u026o2j","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## train set piple line","metadata":{"id":"qz0KDFOu6o2l"}},{"cell_type":"code","source":"train_idx_dataset = tf.data.Dataset.from_tensor_slices(train_img_idx_split)## IF Turning, add [:10]","metadata":{"id":"PmFiIrD-6o2l","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nBase model |resolution\n --- | ---\nEfficientNetB0 | 224\nEfficientNetB1 | 240\nEfficientNetB2 | 260\nEfficientNetB3 | 300\nEfficientNetB4 | 380\nEfficientNetB5 | 456\nEfficientNetB6 | 528\nEfficientNetB7 | 600\n\nref: https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/\n\nWe are going to use B1, so 240","metadata":{"id":"befgO3yU6o2l"}},{"cell_type":"code","source":"image_size = 224\nbatch_size = 40 ##Depends on GPU memory size\n\nTripletGenerate = TripletDataGenerator(train_img_path, train_title_token, train_label, resize = image_size, text_token_dims = text_token_dims)","metadata":{"id":"e72NFz626o2m","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def GetData(idx, size):\n    Triplet = tf.numpy_function(func=TripletGenerate, inp=[idx], Tout=[tf.uint8, tf.uint8, tf.uint8, tf.uint32, tf.uint32, tf.uint32])\n    for i in Triplet[:3]:\n        i.set_shape((size,size,3))\n    return {\"Anchor_img\": Triplet[0], \"Positive_img\": Triplet[1], \"Negative_img\": Triplet[2], \"Anchor_title\": Triplet[3], \"Positive_title\": Triplet[4], \"Negative_title\": Triplet[5]}\n","metadata":{"id":"e7dwoVuf6o2m","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TripletDataSet_train = train_idx_dataset.map(partial(GetData, size=image_size), num_parallel_calls=tf.data.AUTOTUNE).batch(batch_size).prefetch(tf.data.AUTOTUNE)","metadata":{"id":"VkiTpQZF6o2m","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_batch = next(iter(TripletDataSet_train))","metadata":{"id":"mcKSGXD56o2n","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_batch(batch):\n    num = 5\n    plt.figure(figsize=(20,12))\n    for i in range(num):\n        plt.subplot(3,num,i+1)\n        plt.imshow(batch['Anchor_img'][i])\n        plt.title('Ancher')\n        plt.subplot(3,num,i+num+1)\n        plt.imshow(batch['Positive_img'][i])\n        plt.title('Positive')\n        plt.subplot(3,num,i+num+num+1)\n        plt.imshow(batch['Negative_img'][i])\n        plt.title('Negative')\n    plt.show()\nshow_batch(test_batch)","metadata":{"id":"ralCSSf46o2o","outputId":"73602a8f-aab4-45d6-eb3d-a3bdebb4446e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SiameseNet","metadata":{"id":"w2gxqHLd6o2r"}},{"cell_type":"markdown","source":"## Triplet Loss","metadata":{"id":"Rxo8zoig6o2r"}},{"cell_type":"code","source":"class TripletLossLayer(layers.Layer):\n    def __init__(self, alpha, **kwargs):\n        self.alpha = alpha\n        super(TripletLossLayer, self).__init__(**kwargs)\n    \n    def triplet_loss(self, inputs):\n        anchor, positive, negative = inputs\n        \n        anchor = tf.math.l2_normalize(anchor, axis=1)\n        positive = tf.math.l2_normalize(positive, axis=1)\n        negative = tf.math.l2_normalize(negative, axis=1)\n\n        p_dist = tf.math.reduce_sum(tf.math.square(anchor-positive), axis=-1)\n        n_dist = tf.math.reduce_sum(tf.math.square(anchor-negative), axis=-1)\n        return tf.math.reduce_sum(tf.math.maximum(p_dist - n_dist + self.alpha, 0), axis=0)\n    \n    def call(self, inputs):\n        loss = self.triplet_loss(inputs)\n        self.add_loss(loss)\n        return loss","metadata":{"id":"1pErCsIX6o2r","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Transformer ","metadata":{"id":"9ILqcvEE8-W8"}},{"cell_type":"markdown","source":"embedding layer","metadata":{"id":"bfjf8VyTIZZ_"}},{"cell_type":"code","source":"class TokenAndPositionEmbedding(layers.Layer):\n    def __init__(self, maxlen, vocab_size, embed_dim):\n        super(TokenAndPositionEmbedding, self).__init__()\n        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        positions = self.pos_emb(positions)\n        x = self.token_emb(x)\n        return x + positions","metadata":{"id":"ogeCrLCmHx5L","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"token_layer = TokenAndPositionEmbedding(text_token_dims, tokenizer.vocab_size, 50)","metadata":{"id":"cxw4hL-4cbc7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_batch['Anchor_title'][0]","metadata":{"id":"h96YcVJBcp2R","outputId":"1d1f58e2-c505-462a-c448-ba96b5c528f1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"token_layer(test_batch['Anchor_title'][0])","metadata":{"id":"xv6NsQe0c4ki","outputId":"50166e5c-e7d3-47b4-814c-7f663aadad16","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Transformer block","metadata":{"id":"rMDi0tTfIbvg"}},{"cell_type":"code","source":"class TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super(TransformerBlock, self).__init__()\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = keras.Sequential(\n            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)","metadata":{"id":"7k3s8WRo9BOw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Network build","metadata":{"id":"7Mdio5IC6o2r"}},{"cell_type":"code","source":"def model_builder(resolution, text_token_dims = text_token_dims, vocab_size = tokenizer.vocab_size, embedding_size = 128):\n    ##IMAGE\n    BackBone_img = keras.applications.EfficientNetB0(\n        include_top=False, weights='imagenet',\n        pooling = 'avg',\n        input_tensor=layers.Input((resolution,resolution,3), name = 'image')\n    )\n    net_image_flatten = layers.Flatten()(BackBone_img.layers[-1].output)\n\n    ##TEXT\n    input_text_embed_dim = 70 # Embedding size for each token\n    num_heads = 3 # Number of attention heads\n    out_dim = 100 # Hidden layer size in feed forward network inside transformer\n\n    inputs = layers.Input(shape=(text_token_dims,), name = 'title')\n    embedding_layer = TokenAndPositionEmbedding(text_token_dims, vocab_size, input_text_embed_dim)\n    x = embedding_layer(inputs)\n    transformer_block = TransformerBlock(input_text_embed_dim, num_heads, out_dim)\n    x = transformer_block(x)\n    x = layers.GlobalAveragePooling1D()(x)\n\n    #Connect two network\n    net_concate = layers.Concatenate()([net_image_flatten, x])\n\n    output = layers.Dense(embedding_size)(net_concate)\n\n    BackBone = keras.Model(inputs=[BackBone_img.layers[0].output, inputs], outputs=output, name = 'EfficientNetB0_SimpleTransformer')\n\n\n    img_input_anchor = layers.Input((resolution,resolution,3), name='Anchor_img')\n    img_input_positive = layers.Input((resolution,resolution,3), name='Positive_img')\n    img_input_negative = layers.Input((resolution,resolution,3), name='Negative_img')\n\n    title_input_anchor = layers.Input((text_token_dims,), name='Anchor_title')\n    title_input_positive = layers.Input((text_token_dims,), name='Positive_title')\n    title_input_negative = layers.Input((text_token_dims,), name='Negative_title')\n\n\n    anchor_embedding= BackBone([img_input_anchor, title_input_anchor])\n    positive_embedding = BackBone([img_input_positive, title_input_positive])\n    negative_embedding = BackBone([img_input_negative, title_input_negative])\n\n    margin = 1\n    loss_layer = TripletLossLayer(alpha=margin, name='triplet_loss_layer')([anchor_embedding, positive_embedding, negative_embedding])\n    Triplet_Net = keras.Model(inputs=[img_input_anchor, title_input_anchor, img_input_positive, title_input_positive, img_input_negative, title_input_negative], outputs=loss_layer)\n    Triplet_Net.compile(optimizer=keras.optimizers.RMSprop())\n    return BackBone, Triplet_Net\n\nEmbedding_Net, Triplet_Net = model_builder(image_size)","metadata":{"id":"N7M7k59p6o2s","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Triplet_Net.summary()","metadata":{"id":"T1BmGOWE6o2t","outputId":"41d966db-52fe-466a-aa78-a5d18b3fc4b8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keras.utils.plot_model(Triplet_Net, show_shapes=True)","metadata":{"id":"99L7tqJU6o2s","outputId":"43c2241f-8885-4216-c102-3026f225f93e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Embedding_Net.summary()","metadata":{"scrolled":true,"id":"WTNFgUQB6o2t","outputId":"bce64185-27d5-4203-aa55-b94cd8160b71","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keras.utils.plot_model(Embedding_Net, show_shapes=True)","metadata":{"id":"46JV76kjkWGN","outputId":"8bf72e5d-2bb9-4257-c069-b2be9068546c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{"id":"MIlV9m1C6o2t"}},{"cell_type":"code","source":"!mkdir -p /kaggle/working/checkpoint\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath='/kaggle/working/checkpoint/{epoch:02d}-{loss:.6f}.hdf5',\n    save_weights_only=True,\n    monitor='loss',\n    save_best_only=False)","metadata":{"id":"Vm6y2Ix76o2t","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"History = Triplet_Net.fit(TripletDataSet_train, epochs=8, callbacks=[model_checkpoint_callback])","metadata":{"id":"gFRIDBqx6o2u","outputId":"72d07fe5-65e8-47c3-ced9-a56dc6b7b8da","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_loss(history):\n    plt.figure(figsize=(10,5))\n    plt.subplot(1, 2, 1)\n    plt.plot(range(1,len(history.history['loss'])+1), history.history['loss'], label='train loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('loss')\n    plt.xticks(range(1,len(history.history['loss'])+1))\n    plt.legend()\n    plt.grid(True)\nplot_loss(History)","metadata":{"id":"ElxI1Cqm6o2v","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"scrolled":true,"id":"xfWJsupR6o2v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"kkQShBPg6o2v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"2TVZhujG6o2v"},"execution_count":null,"outputs":[]}]}