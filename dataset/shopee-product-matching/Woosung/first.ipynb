{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nimport cv2\nimport re\nimport nltk\nnltk.download('popular')\n###############################\nimport pytesseract\n\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport os\n\nimport random\n\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\n\nimport skimage\nfrom skimage.feature import greycomatrix, greycoprops\nfrom skimage.filters import sobel","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#https://www.kaggle.com/cdeotte/part-2-rapids-tfidfvectorizer-cv-0-700","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Set","metadata":{}},{"cell_type":"code","source":"##https://www.kaggle.com/cdeotte/part-2-rapids-tfidfvectorizer-cv-0-700\ntrain = pd.read_csv('../input/shopee-product-matching/train.csv')\ntrain","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/shopee-product-matching/train.csv')\nanswer=train[['posting_id','label_group']].groupby('label_group').posting_id.agg('unique').to_dict()\ntrain['answer']=train.label_group.map(answer)\ntrain","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def img_show(row_num):\n\n    image_name=train['image'].loc[row_num]\n\n    img = os.path.join('../input/shopee-product-matching/train_images/', image_name)\n\n    image = cv2.imread(img)\n\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    plt.figure(figsize=(5, 5))\n\n    plt.imshow(image)\n\n    plt.title(train['title'].loc[row_num])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def group_show(row_num):\n\n    lb_group=train['label_group'].loc[row_num]\n    \n    index_group=train[train['label_group']==lb_group].index\n    \n    for ele in index_group:\n        \n        img_show(ele)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## data overview","metadata":{}},{"cell_type":"code","source":"train[train['label_group']==train.loc[10]['label_group']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"group_show(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SHOPEE data analysis \n### pictures and titles by group","metadata":{}},{"cell_type":"code","source":"group_show(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"group_show(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Some titles are with upper  case, others are mixed. Need to arrage it.\ngroup_show(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#if we extract the strings from the picture 'Nescafe' it will be helpful.\ngroup_show(4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def img_show_other(row_num):\n\n    image_name=train['image'].loc[row_num]\n\n    img = os.path.join('../input/shopee-product-matching/train_images/', image_name)\n\n    image = cv2.imread(img)\n\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    plt.figure(figsize=(5, 5))\n\n    plt.imshow(image)\n\n    plt.title(train['title'].loc[row_num])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\ndef filtered_img(row_num):\n    image_name=train['image'].loc[row_num]\n    img = os.path.join('../input/shopee-product-matching/train_images/', image_name)\n    img\n    image = cv2.imread(img)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    plt.imshow(sobel(image[:,:,2]))\nfiltered_img(1111)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_num=0\nfor idx,ele in enumerate(train['answer']):\n    if max_num<len(ele):\n        max_num = len(ele)\n        index_ = idx\nprint('Index number:',index_,' has',max_num,' same items (maximum number)')\n# group_show(50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SHOPEE data analysis \n### pictures","metadata":{}},{"cell_type":"code","source":"\nrow_num=110\nimage_name=train['image'].loc[row_num]\nimg = os.path.join('../input/shopee-product-matching/train_images/', image_name)\nimage = cv2.imread(img)\nprint(image.shape)\nplt.imshow(image[:,:])\nprint(sobel(image[:,:,2]).shape)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(sobel(image[:,:,2]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SHOPEE data analysis \n### the WordCloud with titles","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\nstopwords = nltk.corpus.stopwords.words(\"english\")\nwordcloud = WordCloud(width = 800, \n                      height = 800,\n                      background_color ='white',\n                      min_font_size = 10,\n                      stopwords = stopwords,).generate(' '.join(train['title'])) \n\n# plot the WordCloud image                        \nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n\nplt.show() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Processing","metadata":{}},{"cell_type":"markdown","source":"### Text ","metadata":{}},{"cell_type":"code","source":"#https://www.kaggle.com/ishandutta/v7-shopee-indepth-eda-one-stop-for-all-your-needs\ndef preprocess_text(text, flg_stemm=False, flg_lemm=True):\n\n    lst_stopwords = nltk.corpus.stopwords.words(\"english\")\n    \n    ## clean (convert to lowercase and remove punctuations and characters and then strip)\n    text = re.sub(r'[^\\w\\s]', ' ', str(text).lower().strip())\n            \n    ## Tokenize (convert from string to list)\n    lst_text = text.split()\n    ## remove Stopwords\n    if lst_stopwords is not None:\n        lst_text = [word for word in lst_text if word not in \n                    lst_stopwords]\n                \n    ## Stemming (remove -ing, -ly, ...)\n    if flg_stemm == True:\n        ps = nltk.stem.porter.PorterStemmer()\n        lst_text = [ps.stem(word) for word in lst_text]\n                \n    ## Lemmatisation (convert the word into root word)\n    if flg_lemm == True:\n        lem = nltk.stem.wordnet.WordNetLemmatizer()    \n        lst_text = [lem.lemmatize(word) for word in lst_text]\n            \n    ## back to string from list\n    text = \" \".join(lst_text)\n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"clean_title\"] = train[\"title\"].apply(lambda x: preprocess_text(x, flg_stemm=False, flg_lemm=True ))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[[\"clean_title\",\"title\"]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom nltk.probability import FreqDist\nclean_title=train[\"clean_title\"]\ntitle=train['title']\nfdist=FreqDist()\nfdist_title=FreqDist()\nfor ele in clean_title:\n    ele_tok=word_tokenize(ele)\n    for tok_word in ele_tok:\n        fdist[tok_word]+=1\nfor ele in title:\n    ele_tok=word_tokenize(ele)\n    for tok_word in ele_tok:\n        fdist_title[tok_word]+=1\nprint('top 10 tokens from clean_title:',fdist.most_common(10))\n\nprint('top 10 tokens from title:',fdist_title.most_common(10))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fdist.most_common(400)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def abuse_score(keyword): \n    arr=[]\n    for idx,ele in enumerate(clean_title):\n        ele_tok=word_tokenize(ele)\n        for tok_word in ele_tok:\n            if tok_word == keyword:\n                arr.append(idx)\n                break\n    return len(arr)/fdist[keyword]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"abuse_score('original')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"abuse_score('serum')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"abuse_score('puzzle')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# abuse_score calculator\n# it takes lots of time so I uploaeded the result of it \n\n\n# result = []\n# for ele in fdist.most_common(300):\n#     print(ele)\n#     result.append((ele[0],abuse_score(ele[0])))\n# result_pd = pd.DataFrame(result) \n\n#https://www.kaggle.com/ishandutta/v7-shopee-indepth-eda-one-stop-for-all-your-needs\n\nresult_pd=pd.read_csv('../input/shop-result/result.csv')\ndef preprocess2_text(text,crite):\n    lst_stopwords = result_pd[result_pd['1']>crite]['0']\n\n    ## Tokenize (convert from string to list)\n    lst_text = text.split()\n    ## remove Stopwords\n    if lst_stopwords is not None:\n        lst_text = [word for word in lst_text if word not in \n                    lst_stopwords]\n    text = \" \".join(lst_text)\n    return text\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_pd[result_pd['1']>0.98]['0']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Tesseract Library\ntrain[\"clean_title_remove_98\"] = train[\"clean_title\"].apply(lambda x: preprocess2_text(x,0.94))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[[\"clean_title\",\"title\",\"clean_title_remove_98\"]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TEXT EXTRACT FROM PICTURES","metadata":{}},{"cell_type":"code","source":"image_id = random.choice(train['image'].values)\nprint(image_id)\nimg_path = os.path.join('../input/shopee-product-matching/train_images/', image_id)\nimage = cv2.imread(img_path)\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\nplt.figure(figsize=(5, 5))\n\nplt.imshow(image)\nplt.title(image_id)\nprint(' OCR result : ' + pytesseract.image_to_string(Image.open(img_path), timeout=10))\nprint('======================')\nprint(' GT title column is : ' + train[train['image'] == image_id]['title'].values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import easyocr\ndef img2txt(image_id):\n    img_path = os.path.join('../input/shopee-product-matching/train_images/', image_id)\n    image = cv2.imread(img_path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    result = pytesseract.image_to_string(Image.open(img_path), timeout=10)\n    #print(' OCR result : ' + pytesseract.image_to_string(Image.open(img_path), timeout=10))\n    return result\n\ndef img2txt_gpu(image_id):\n    img_path = os.path.join('../input/shopee-product-matching/train_images/', image_id)\n    reader = easyocr.Reader(['en'])\n    #im = PIL.Image.open(img_path) #provide your image path\n    result = reader.readtext(img_path, detail=0)\n    print(result)\n    #print(' OCR result : ' + pytesseract.image_to_string(Image.open(img_path), timeout=10))\n    return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# it takes lots of time so I uploaeded the result of it \n\n# Doing OCR using GPU\n# save the images text to dict\nreader = easyocr.Reader(['en', 'en'])\nimages_text = {}\n# for idx,files in enumerate(files_list):\n#     print(idx)\n#     img_text = reader.readtext(img_path + '/' +  files)\n#     final_text = \"\"\n#     for _, text, __ in img_text:\n#         final_text += \" \"\n#         final_text += text\n#     images_text[files] = final_text\n# train[\"img_txt\"] = train[\"image\"].apply(lambda x: img2txt_gpu(x))\n\ntrain=pd.read_csv('../input/shop-train/train_final.csv')\ntrain[['txt_img']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TEXT EXTRACT FROM PICTURES\n### Clean the txt_img and merge it with refined title ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom nltk.probability import FreqDist\ntrain[\"clean_txt_img\"] = train[\"txt_img\"].apply(lambda x: preprocess_text(x, flg_stemm=False, flg_lemm=True, ))\n\nclean_txt_img=train[\"clean_txt_img\"]\nfdist_clean_txt_img=FreqDist()\n\nfor ele in clean_txt_img:\n    ele_tok=word_tokenize(ele)\n    for tok_word in ele_tok:\n        fdist_clean_txt_img[tok_word]+=1\n\ntrain[\"clean_txt_img_98\"] = train[\"clean_txt_img\"].apply(lambda x: preprocess2_text(x,0.98))\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['title_N_text_img']=''\n\ndef listToString(s): \n    str1 = \" \"\n    return (str1.join(s))\n        \nfor idx in range(len(train)):\n    if idx%10000 == 0:\n        print(idx)\n    tok_txt=train['clean_txt_img_98'].loc[idx].split()\n    #print(tok_txt)\n    tok_txt=[word for word in tok_txt if word in train['clean_txt_img_98'].loc[idx]]\n    train['title_N_text_img'].loc[idx]=train['clean_title_remove_98'].loc[idx]+' '+listToString(tok_txt)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[['clean_title_remove_98','txt_img','title_N_text_img']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrain","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import numpy as np, pandas as pd, gc\nimport cv2, matplotlib.pyplot as plt\nimport cudf, cuml, cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors\nimport tensorflow as tf\nfrom tensorflow.keras.applications import EfficientNetB0\nprint('RAPIDS',cuml.__version__)\nprint('TF',tf.__version__)\nprint('Computing text embeddings...')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train=train.drop(['Unnamed: 0'],axis=1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction results and Interpretation\n### TITLE","metadata":{}},{"cell_type":"code","source":"train[['title','clean_title_remove_98','title_N_text_img']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrain_gf=cudf.DataFrame(train)\n\n# model = TfidfVectorizer(stop_words='english', binary=True, max_features=25_000)\n# text_embeddings = model.fit_transform(train_gf.clean_title_remove_98).toarray()\n# print('text embeddings shape',text_embeddings.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\ntrain_gf=cudf.DataFrame(train)","metadata":{}},{"cell_type":"code","source":"def TextPredictionModel_cosSimilarity(col_name):\n    \n    model = TfidfVectorizer(stop_words='english', binary=True, max_features=25_000)\n    text_embeddings = model.fit_transform(train_gf[col_name]).toarray()\n    print('text embeddings shape',text_embeddings.shape)\n\n    preds = []\n    CHUNK = 1024*4\n\n    print('Finding similar titles...')\n    CTS = len(train)//CHUNK\n    if len(train)%CHUNK!=0: CTS += 1\n    for j in range( CTS ):\n\n        a = j*CHUNK\n        b = (j+1)*CHUNK\n        b = min(b,len(train))\n        print('chunk',a,'to',b)\n\n        # COSINE SIMILARITY DISTANCE\n        cts = cupy.matmul( text_embeddings, text_embeddings[a:b].T).T\n\n        for k in range(b-a):\n            IDX = cupy.where(cts[k,]>0.7)[0]\n            o = train.iloc[cupy.asnumpy(IDX)].posting_id.values\n            preds.append(o)\n\n    del model, text_embeddings\n    _ = gc.collect()\n    return preds\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train[['title','clean_title','clean_title_remove_98','title_N_text_img']]\n\npred_title=TextPredictionModel_cosSimilarity('title')\npred_98_refined_title=TextPredictionModel_cosSimilarity('clean_title_remove_98')\npred_title_N_text_img=TextPredictionModel_cosSimilarity('title_N_text_img')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = pred_title\n# pred_clean_title\n# pred_98_refined_title\n# pred_title_N_text_img\n\ntrain['preds'] = preds\ndef getMetric(col):\n    def f1score(row):\n        n = len( np.intersect1d(row.answer,row[col]) )\n        return 2*n / (len(row.answer)+len(row[col]))\n    return f1score\ndef combine_for_sub(row):\n    x = np.concatenate([row.preds])\n    return ' '.join( np.unique(x) )\n\ndef combine_for_cv(row):\n    x = np.concatenate([row.preds])\n    return np.unique(x)\ntmp = train.groupby('label_group').posting_id.agg('unique').to_dict()\ntrain['answer'] = train.label_group.map(tmp)\ntrain['oof'] = train.apply(combine_for_cv,axis=1)\ntrain['f1'] = train.apply(getMetric('oof'),axis=1)\nprint('CV Score =', train.f1.mean() )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = pred_title_N_text_img\n\ntrain['preds'] = preds\ndef getMetric(col):\n    def f1score(row):\n        n = len( np.intersect1d(row.answer,row[col]) )\n        return 2*n / (len(row.answer)+len(row[col]))\n    return f1score\ndef combine_for_sub(row):\n    x = np.concatenate([row.preds])\n    return ' '.join( np.unique(x) )\n\ndef combine_for_cv(row):\n    x = np.concatenate([row.preds])\n    return np.unique(x)\ntmp = train.groupby('label_group').posting_id.agg('unique').to_dict()\ntrain['answer'] = train.label_group.map(tmp)\ntrain['oof'] = train.apply(combine_for_cv,axis=1)\ntrain['f1'] = train.apply(getMetric('oof'),axis=1)\nprint('CV Score =', train.f1.mean() )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = pred_98_refined_title\n# pred_title_N_text_img\n\ntrain['preds'] = preds\ndef getMetric(col):\n    def f1score(row):\n        n = len( np.intersect1d(row.answer,row[col]) )\n        return 2*n / (len(row.answer)+len(row[col]))\n    return f1score\ndef combine_for_sub(row):\n    x = np.concatenate([row.preds])\n    return ' '.join( np.unique(x) )\n\ndef combine_for_cv(row):\n    x = np.concatenate([row.preds])\n    return np.unique(x)\ntmp = train.groupby('label_group').posting_id.agg('unique').to_dict()\ntrain['answer'] = train.label_group.map(tmp)\ntrain['oof'] = train.apply(combine_for_cv,axis=1)\ntrain['f1'] = train.apply(getMetric('oof'),axis=1)\nprint('CV Score =', train.f1.mean() )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install ../input/shopee-external-models/Keras_Applications-1.0.8-py3-none-any.whl\n# !pip install ../input/shopee-external-models/efficientnet-1.1.0-py3-none-any.whl\n# import numpy as np\n# import pandas as pd\n# import gc\n# import matplotlib.pyplot as plt\n# from sklearn.neighbors import NearestNeighbors\n# from sklearn.feature_extraction.text import TfidfVectorizer\n# import tensorflow as tf\n# import efficientnet.tfkeras as efn\n# from tqdm.notebook import tqdm\n# import math\n# GET_CV=False\n# def get_text_embeddings(df, max_features = 15500):\n#     model = TfidfVectorizer(stop_words = 'english', binary = True, max_features = max_features)\n#     text_embeddings = model.fit_transform(df['clean_title_remove_98'])\n#     print(f'Our title text embedding shape is {text_embeddings.shape}')\n#     del model\n#     return text_embeddings\n\n# # Function to get 50 nearest neighbors of each image and text and apply thresholds find in the training phase that optimize f1 cv score\n# def get_neighbors(df, text_embeddings, KNN = 50):\n#     # Get distances and indices from image and text embeddings\n#     neighbors_model = NearestNeighbors(n_neighbors = KNN, metric = 'cosine').fit(text_embeddings)\n#     text_distances, text_indices = neighbors_model.kneighbors(text_embeddings)\n  \n#     # Iterate through different thresholds to maximize cv, run this in interactive mode, then replace else clause with a solid threshold\n#     if GET_CV:\n#         predictions = []\n#         for k in range(df.shape[0]):\n#             # This are the original thresholds that gives 0.8035 cv (optimize with a for loop)\n#             idx_text = np.where(text_distances[k,] < 0.30)[0]\n#             ids_text = text_indices[k,idx_text]\n#             # Get the union of boths ids\n#             ids = list(ids_text)\n#             posting_ids = ' '.join(df['posting_id'].iloc[ids].values)\n#             predictions.append(posting_ids)\n    \n#     else:\n#         predictions = []\n#         for k in range(df.shape[0]):\n#             idx_text = np.where(text_distances[k,] < 0.30)[0]\n#             ids_text = text_indices[k,idx_text]\n#             # Get the union of boths ids\n#             ids = list(ids_text)\n#             posting_ids = ' '.join(df['posting_id'].iloc[ids].values)\n#             predictions.append(posting_ids)\n        \n#     del neighbors_model, text_distances, text_indices\n#     gc.collect()\n#     return df, predictions\n\n# if GET_CV:\n#     text_embeddings = get_text_embeddings(train, max_features = 15500)\n# else:\n#     text_embeddings = get_text_embeddings(train, max_features = 21500)\n\n# # Get neighbors\n# df, predictions = get_neighbors(train, text_embeddings, KNN = 50)\n# df['matches'] = predictions\n# train['matches_conv']=train['matches'].apply(lambda x: x.split())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np, pandas as pd, gc\nimport cv2, matplotlib.pyplot as plt\nimport cudf, cuml, cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors\nimport tensorflow as tf\nfrom tensorflow.keras.applications import EfficientNetB0\nprint('RAPIDS',cuml.__version__)\nprint('TF',tf.__version__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from skimage.filters import sobel\nclass DataGenerator(tf.keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, df, img_size=256, batch_size=32, path='',g_kind=0): \n        self.df = df\n        self.img_size = img_size\n        self.batch_size = batch_size\n        self.path = path\n        self.indexes = np.arange( len(self.df) )\n        self.g_kind = g_kind\n        \n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        ct = len(self.df) // self.batch_size\n        ct += int(( (len(self.df)) % self.batch_size)!=0)\n        return ct\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        X = self.__data_generation(indexes)\n        return X\n            \n    def __data_generation(self, indexes):\n        'Generates data containing batch_size samples' \n        X = np.zeros((len(indexes),self.img_size,self.img_size,3),dtype='float32')\n        df = self.df.iloc[indexes]\n        for i,(index,row) in enumerate(df.iterrows()):\n            img = cv2.imread(self.path+row.image)\n            if self.g_kind==1:\n                img = sobel(img[:,:])\n            X[i,] = cv2.resize(img,(self.img_size,self.img_size)) #/128.0 - 1.0\n        return X","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BASE = '../input/shopee-product-matching/train_images/'\n\nWGT = '../input/effnetb0/efficientnetb0_notop.h5'\nmodel = EfficientNetB0(weights=WGT,include_top=False, pooling='avg', input_shape=None)\n\nembeds = []\nCHUNK = 1024*4\n\nprint('Computing image embeddings...')\nCTS = len(train)//CHUNK\nif len(train)%CHUNK!=0: CTS += 1\nfor i,j in enumerate( range( CTS ) ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(train))\n    print('chunk',a,'to',b)\n    \n    train_gen = DataGenerator(train.iloc[a:b], batch_size=32, path=BASE)\n    image_embeddings = model.predict(train_gen,verbose=1,use_multiprocessing=True, workers=4)\n    embeds.append(image_embeddings)\n\n    #if i>=1: break\n    \ndel model\n_ = gc.collect()\nimage_embeddings = np.concatenate(embeds)\nprint('image embeddings shape',image_embeddings.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_gf=cudf.DataFrame(train)\nKNN = 50\nmodel = NearestNeighbors(n_neighbors=KNN)\nmodel.fit(image_embeddings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_img = []\nCHUNK = 1024*4\n\nprint('Finding similar images...')\nCTS = len(image_embeddings)//CHUNK\nif len(image_embeddings)%CHUNK!=0: CTS += 1\nfor j in range( CTS ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(image_embeddings))\n    print('chunk',a,'to',b)\n    distances, indices = model.kneighbors(image_embeddings[a:b,])\n    \n    for k in range(b-a):\n        IDX = np.where(distances[k,]<6.0)[0]\n        #IDX = np.where(distances[k,]<3.9)[0]\n        IDS = indices[k,IDX]\n        o = train.iloc[IDS].posting_id.values\n        preds_img.append(o)\n        \ndel model, distances, indices, image_embeddings, embeds\n_ = gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['preds_img'] = preds_img\n\n\ndef getMetric(col):\n    def f1score(row):\n        n = len( np.intersect1d(row.answer,row[col]) )\n        return 2*n / (len(row.answer)+len(row[col]))\n    return f1score\ndef combine_for_sub(row):\n    x = np.concatenate([row.preds_img])\n    return ' '.join( np.unique(x) )\n\ndef combine_for_cv(row):\n    x = np.concatenate([row.preds_img])\n    return np.unique(x)\ntmp = train.groupby('label_group').posting_id.agg('unique').to_dict()\ntrain['answer'] = train.label_group.map(tmp)\ntrain['oof'] = train.apply(combine_for_cv,axis=1)\ntrain['f1'] = train.apply(getMetric('oof'),axis=1)\nprint('CV Score =', train.f1.mean() )\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp = train.groupby('image_phash').posting_id.agg('unique').to_dict()\ntrain['preds_img_hash'] = train.image_phash.map(tmp)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n\ndef getMetric(col):\n    def f1score(row):\n        n = len( np.intersect1d(row.answer,row[col]) )\n        return 2*n / (len(row.answer)+len(row[col]))\n    return f1score\ndef combine_for_sub(row):\n    x = np.concatenate([row.preds_img_hash])\n    return ' '.join( np.unique(x) )\n\ndef combine_for_cv(row):\n    x = np.concatenate([row.preds_img_hash])\n    return np.unique(x)\ntmp = train.groupby('label_group').posting_id.agg('unique').to_dict()\ntrain['answer'] = train.label_group.map(tmp)\ntrain['oof'] = train.apply(combine_for_cv,axis=1)\ntrain['f1'] = train.apply(getMetric('oof'),axis=1)\nprint('CV Score =', train.f1.mean() )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[['answer','preds','preds_img','preds_img_hash']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final result ","metadata":{}},{"cell_type":"code","source":"def getMetric(col):\n    def f1score(row):\n        n = len( np.intersect1d(row.answer,row[col]) )\n        return 2*n / (len(row.answer)+len(row[col]))\n    return f1score\ndef combine_for_sub(row):\n    x = np.concatenate([row.preds,row.preds_img, row.preds_img_hash])\n    return ' '.join( np.unique(x) )\n\ndef combine_for_cv(row):\n    x = np.concatenate([row.preds,row.preds_img, row.preds_img_hash])\n    return np.unique(x)\n\ntmp = train.groupby('label_group').posting_id.agg('unique').to_dict()\ntrain['answer'] = train.label_group.map(tmp)\ntrain['oof'] = train.apply(combine_for_cv,axis=1)\ntrain['f1'] = train.apply(getMetric('oof'),axis=1)\nprint('CV Score =', train.f1.mean() )\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}