{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# eca_nfnet_l0'(from timm) + ArcMarginProduct Module\n\n\n### References:\n1. https://www.kaggle.com/parthdhameliya77/shopee-pytorch-eca-nfnet-l0-image-training\n2. https://www.kaggle.com/parthdhameliya77/pytorch-eca-nfnet-l0-image-tfidf-inference\n3. [Custom LR schedular](https://www.kaggle.com/tanulsingh077/pytorch-metric-learning-pipeline-only-images?scriptVersionId=58269290&cellId=22) <br>\n4. [Ranger Optimizer + Centralized Gradient](https://github.com/Yonghongwei/Gradient-Centralization) <br>\n5. [Mish Activation Function](https://github.com/tyunist/memory_efficient_mish_swish/blob/master/mish.py) <br>","metadata":{}},{"cell_type":"markdown","source":"## Goal\n1) To use [eca-nfnet-10] pretrained model as the backbone to make predictions for the SHOPEE image dataset for product matching by [IMAGE].\n\n2) To use [TF-DIF Vectorizer] sklearn model as the backone to train SHOPEE metadata for product matching by [Title].\n\n3) Combine predictions from Step[1] & Step[2].","metadata":{}},{"cell_type":"markdown","source":"## Importing required libraries","metadata":{}},{"cell_type":"code","source":"import sys \nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master') ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport os \nimport cv2 \nimport timm \n\nimport albumentations \nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torch \nimport torch.nn.functional as F \nfrom torch import nn \nfrom torch.optim.optimizer import Optimizer\nfrom torch.optim.lr_scheduler import _LRScheduler\n\nimport math\n\nfrom tqdm.notebook import tqdm \nfrom sklearn.preprocessing import LabelEncoder","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining the configuration for eca_nfnet 10","metadata":{}},{"cell_type":"code","source":"class Config:\n    \n    DATA_DIR = '../input/shopee-product-matching/train_images'\n    TRAIN_CSV = '../input/shopee-product-matching/train.csv'\n\n    IMG_SIZE = 512\n    MEAN = [0.485, 0.456, 0.406]\n    STD = [0.229, 0.224, 0.225]\n\n    EPOCHS = 7\n    BATCH_SIZE = 8\n\n    NUM_WORKERS = 4\n    DEVICE = 'cuda'\n\n    CLASSES = 11014 \n    SCALE = 30 \n    MARGIN = 0.5\n\n    MODEL_NAME = 'eca_nfnet_l0'\n    FC_DIM = 512\n    SCHEDULER_PARAMS = {\n            \"lr_start\": 1e-5,\n            \"lr_max\": 1e-5 * 32,\n            \"lr_min\": 1e-6,\n            \"lr_ramp_ep\": 5,\n            \"lr_sus_ep\": 0,\n            \"lr_decay\": 0.8,\n        }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ShopeeDataset(torch.utils.data.Dataset):\n\n    def __init__(self,df, transform = None):\n        self.df = df \n        self.root_dir = Config.DATA_DIR\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self,idx):\n\n        row = self.df.iloc[idx]\n\n        img_path = os.path.join(self.root_dir,row.image)\n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        label = row.label_group\n\n        if self.transform:\n            augmented = self.transform(image=image)\n            image = augmented['image']\n\n        return {\n            'image' : image,\n            'label' : torch.tensor(label).long()\n        }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_train_transforms():\n    return albumentations.Compose(\n        [   \n            albumentations.Resize(Config.IMG_SIZE,Config.IMG_SIZE,always_apply=True),\n            albumentations.HorizontalFlip(p=0.5),\n            albumentations.VerticalFlip(p=0.5),\n            albumentations.Rotate(limit=120, p=0.8),\n            albumentations.RandomBrightness(limit=(0.09, 0.6), p=0.5),\n            albumentations.Normalize(mean = Config.MEAN, std = Config.STD),\n            ToTensorV2(p=1.0),\n        ]\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining the LR Scheduler","metadata":{}},{"cell_type":"code","source":"#credit : https://www.kaggle.com/tanulsingh077/pytorch-metric-learning-pipeline-only-images?scriptVersionId=58269290&cellId=22\n\nclass ShopeeScheduler(_LRScheduler):\n    def __init__(self, optimizer, lr_start=5e-6, lr_max=1e-5,\n                 lr_min=1e-6, lr_ramp_ep=5, lr_sus_ep=0, lr_decay=0.8,\n                 last_epoch=-1):\n        self.lr_start = lr_start\n        self.lr_max = lr_max\n        self.lr_min = lr_min\n        self.lr_ramp_ep = lr_ramp_ep\n        self.lr_sus_ep = lr_sus_ep\n        self.lr_decay = lr_decay\n        super(ShopeeScheduler, self).__init__(optimizer, last_epoch)\n        \n    def get_lr(self):\n        if not self._get_lr_called_within_step:\n            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n                          \"please use `get_last_lr()`.\", UserWarning)\n        \n        if self.last_epoch == 0:\n            self.last_epoch += 1\n            return [self.lr_start for _ in self.optimizer.param_groups]\n        \n        lr = self._compute_lr_from_epoch()\n        self.last_epoch += 1\n        \n        return [lr for _ in self.optimizer.param_groups]\n    \n    def _get_closed_form_lr(self):\n        return self.base_lrs\n    \n    def _compute_lr_from_epoch(self):\n        if self.last_epoch < self.lr_ramp_ep:\n            lr = ((self.lr_max - self.lr_start) / \n                  self.lr_ramp_ep * self.last_epoch + \n                  self.lr_start)\n        \n        elif self.last_epoch < self.lr_ramp_ep + self.lr_sus_ep:\n            lr = self.lr_max\n            \n        else:\n            lr = ((self.lr_max - self.lr_min) * self.lr_decay**\n                  (self.last_epoch - self.lr_ramp_ep - self.lr_sus_ep) + \n                  self.lr_min)\n        return lr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining the Central Gradient,Range and Mish Function","metadata":{}},{"cell_type":"code","source":"#credit : https://github.com/Yonghongwei/Gradient-Centralization\n\ndef centralized_gradient(x, use_gc=True, gc_conv_only=False):\n    if use_gc:\n        if gc_conv_only:\n            if len(list(x.size())) > 3:\n                x.add_(-x.mean(dim=tuple(range(1, len(list(x.size())))), keepdim=True))\n        else:\n            if len(list(x.size())) > 1:\n                x.add_(-x.mean(dim=tuple(range(1, len(list(x.size())))), keepdim=True))\n    return x\n\n\nclass Ranger(Optimizer):\n\n    def __init__(self, params, lr=1e-3,                       # lr\n                 alpha=0.5, k=5, N_sma_threshhold=5,           # Ranger options\n                 betas=(.95, 0.999), eps=1e-5, weight_decay=0,  # Adam options\n                 # Gradient centralization on or off, applied to conv layers only or conv + fc layers\n                 use_gc=True, gc_conv_only=False, gc_loc=True\n                 ):\n\n        # parameter checks\n        if not 0.0 <= alpha <= 1.0:\n            raise ValueError(f'Invalid slow update rate: {alpha}')\n        if not 1 <= k:\n            raise ValueError(f'Invalid lookahead steps: {k}')\n        if not lr > 0:\n            raise ValueError(f'Invalid Learning Rate: {lr}')\n        if not eps > 0:\n            raise ValueError(f'Invalid eps: {eps}')\n\n        \n        # prep defaults and init torch.optim base\n        defaults = dict(lr=lr, alpha=alpha, k=k, step_counter=0, betas=betas,\n                        N_sma_threshhold=N_sma_threshhold, eps=eps, weight_decay=weight_decay)\n        super().__init__(params, defaults)\n\n        # adjustable threshold\n        self.N_sma_threshhold = N_sma_threshhold\n\n        # look ahead params\n\n        self.alpha = alpha\n        self.k = k\n\n        # radam buffer for state\n        self.radam_buffer = [[None, None, None] for ind in range(10)]\n\n        # gc on or off\n        self.gc_loc = gc_loc\n        self.use_gc = use_gc\n        self.gc_conv_only = gc_conv_only\n        # level of gradient centralization\n        #self.gc_gradient_threshold = 3 if gc_conv_only else 1\n\n        print(\n            f\"Ranger optimizer loaded. \\nGradient Centralization usage = {self.use_gc}\")\n        if (self.use_gc and self.gc_conv_only == False):\n            print(f\"GC applied to both conv and fc layers\")\n        elif (self.use_gc and self.gc_conv_only == True):\n            print(f\"GC applied to conv layers only\")\n\n    def __setstate__(self, state):\n        print(\"set state called\")\n        super(Ranger, self).__setstate__(state)\n\n    def step(self, closure=None):\n        loss = None\n        # Evaluate averages and grad, update param tensors\n        for group in self.param_groups:\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n\n                if grad.is_sparse:\n                    raise RuntimeError(\n                        'Ranger optimizer does not support sparse gradients')\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]  # get state dict for this param\n\n                if len(state) == 0:  # if first time to run...init dictionary with our desired entries\n                    # if self.first_run_check==0:\n                    # self.first_run_check=1\n                    #print(\"Initializing slow buffer...should not see this at load from saved model!\")\n                    state['step'] = 0\n                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n\n                    # look ahead weight storage now in state dict\n                    state['slow_buffer'] = torch.empty_like(p.data)\n                    state['slow_buffer'].copy_(p.data)\n\n                else:\n                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(\n                        p_data_fp32)\n\n                # begin computations\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                # GC operation for Conv layers and FC layers\n                # if grad.dim() > self.gc_gradient_threshold:\n                #    grad.add_(-grad.mean(dim=tuple(range(1, grad.dim())), keepdim=True))\n                if self.gc_loc:\n                    grad = centralized_gradient(grad, use_gc=self.use_gc, gc_conv_only=self.gc_conv_only)\n\n                state['step'] += 1\n\n                # compute variance mov avg\n                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n\n                # compute mean moving avg\n                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n\n                buffered = self.radam_buffer[int(state['step'] % 10)]\n\n                if state['step'] == buffered[0]:\n                    N_sma, step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state['step']\n                    beta2_t = beta2 ** state['step']\n                    N_sma_max = 2 / (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * \\\n                        state['step'] * beta2_t / (1 - beta2_t)\n                    buffered[1] = N_sma\n                    if N_sma > self.N_sma_threshhold:\n                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (\n                            N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n                    else:\n                        step_size = 1.0 / (1 - beta1 ** state['step'])\n                    buffered[2] = step_size\n\n                # apply lr\n                if N_sma > self.N_sma_threshhold:\n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n                    G_grad = exp_avg / denom\n                else:\n                    G_grad = exp_avg\n\n                if group['weight_decay'] != 0:\n                    G_grad.add_(p_data_fp32, alpha=group['weight_decay'])\n                # GC operation\n                if self.gc_loc == False:\n                    G_grad = centralized_gradient(G_grad, use_gc=self.use_gc, gc_conv_only=self.gc_conv_only)\n\n                p_data_fp32.add_(G_grad, alpha=-step_size * group['lr'])\n                p.data.copy_(p_data_fp32)\n                \n                if state['step'] % group['k'] == 0:\n                    # get access to slow param tensor\n                    slow_p = state['slow_buffer']\n                    # (fast weights - slow weights) * alpha\n                    slow_p.add_(p.data - slow_p, alpha=self.alpha)\n                    # copy interpolated weights to RAdam param tensor\n                    p.data.copy_(slow_p)\n\n        return loss","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#credit : https://github.com/tyunist/memory_efficient_mish_swish/blob/master/mish.py\n\nclass Mish_func(torch.autograd.Function):\n    \n    @staticmethod\n    def forward(ctx, i):\n        result = i * torch.tanh(F.softplus(i))\n        ctx.save_for_backward(i)\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        i = ctx.saved_tensors[0]\n  \n        v = 1. + i.exp()\n        h = v.log() \n        grad_gh = 1./h.cosh().pow_(2) \n        \n        grad_hx = i.sigmoid()\n\n        grad_gx = grad_gh *  grad_hx  \n        \n        grad_f =  torch.tanh(F.softplus(i)) + i * grad_gx \n        \n        return grad_output * grad_f \n\n\nclass Mish(nn.Module):\n    def __init__(self, **kwargs):\n        super().__init__()\n        print(\"Mish initialized\")\n        pass\n    def forward(self, input_tensor):\n        return Mish_func.apply(input_tensor)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def replace_activations(model, existing_layer, new_layer):\n    for name, module in reversed(model._modules.items()):\n        if len(list(module.children())) > 0:\n            model._modules[name] = replace_activations(module, existing_layer, new_layer)\n\n        if type(module) == existing_layer:\n            layer_old = module\n            layer_new = new_layer\n            model._modules[name] = layer_new\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining ArcMarginProduct, Shopee Model, Train Model,Evaluation function","metadata":{}},{"cell_type":"code","source":"class ArcMarginProduct(nn.Module):\n    def __init__(self, in_features, out_features, scale=30.0, margin=0.50, easy_margin=False, ls_eps=0.0):\n        super(ArcMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.scale = scale\n        self.margin = margin\n        self.ls_eps = ls_eps  # label smoothing\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(margin)\n        self.sin_m = math.sin(margin)\n        self.th = math.cos(math.pi - margin)\n        self.mm = math.sin(math.pi - margin) * margin\n\n    def forward(self, input, label):\n        # --------------------------- cos(theta) & phi(theta) ---------------------------\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n        # --------------------------- convert label to one-hot ---------------------------\n    \n        one_hot = torch.zeros(cosine.size(), device='cuda')\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.scale\n\n        return output, nn.CrossEntropyLoss()(output,label)\n\nclass ShopeeModel(nn.Module):\n\n    def __init__(\n        self,\n        n_classes = Config.CLASSES,\n        model_name = Config.MODEL_NAME,\n        fc_dim = Config.FC_DIM,\n        margin = Config.MARGIN,\n        scale = Config.SCALE,\n        use_fc = True,\n        pretrained = True):\n\n\n        super(ShopeeModel,self).__init__()\n        print('Building Model Backbone for {} model'.format(model_name))\n\n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n\n        if model_name == 'resnext50_32x4d':\n            final_in_features = self.backbone.fc.in_features\n            self.backbone.fc = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n\n        elif 'efficientnet' in model_name:\n            final_in_features = self.backbone.classifier.in_features\n            self.backbone.classifier = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n        \n        elif 'nfnet' in model_name:\n            final_in_features = self.backbone.head.fc.in_features\n            self.backbone.head.fc = nn.Identity()\n            self.backbone.head.global_pool = nn.Identity()\n\n        self.pooling =  nn.AdaptiveAvgPool2d(1)\n\n        self.use_fc = use_fc\n\n        if use_fc:\n            self.dropout = nn.Dropout(p=0.0)\n            self.fc = nn.Linear(final_in_features, fc_dim)\n            self.bn = nn.BatchNorm1d(fc_dim)\n            self._init_params()\n            final_in_features = fc_dim\n\n        self.final = ArcMarginProduct(\n            final_in_features,\n            n_classes,\n            scale = scale,\n            margin = margin,\n            easy_margin = False,\n            ls_eps = 0.0\n        )\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, image, label):\n        feature = self.extract_feat(image)\n        logits = self.final(feature,label)\n        return logits\n\n    def extract_feat(self, x):\n        batch_size = x.shape[0]\n        x = self.backbone(x)\n        x = self.pooling(x).view(batch_size, -1)\n\n        if self.use_fc:\n            x = self.dropout(x)\n            x = self.fc(x)\n            x = self.bn(x)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_fn(model, data_loader, optimizer, scheduler, i):\n    model.train()\n    fin_loss = 0.0\n    tk = tqdm(data_loader, desc = \"Epoch\" + \" [TRAIN] \" + str(i+1))\n\n    for t,data in enumerate(tk):\n        for k,v in data.items():\n            data[k] = v.to(Config.DEVICE)\n        optimizer.zero_grad()\n        _, loss = model(**data)\n        loss.backward()\n        optimizer.step() \n        fin_loss += loss.item() \n\n        tk.set_postfix({'loss' : '%.6f' %float(fin_loss/(t+1)), 'LR' : optimizer.param_groups[0]['lr']})\n\n    scheduler.step()\n\n    return fin_loss / len(data_loader)\n\ndef eval_fn(model, data_loader, i):\n    model.eval()\n    fin_loss = 0.0\n    tk = tqdm(data_loader, desc = \"Epoch\" + \" [VALID] \" + str(i+1))\n\n    with torch.no_grad():\n        for t,data in enumerate(tk):\n            for k,v in data.items():\n                data[k] = v.to(Config.DEVICE)\n            _, loss = model(**data)\n            fin_loss += loss.item() \n\n            tk.set_postfix({'loss' : '%.6f' %float(fin_loss/(t+1))})\n        return fin_loss / len(data_loader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_training():\n    \n    df = pd.read_csv(Config.TRAIN_CSV)\n\n    labelencoder= LabelEncoder()\n    df['label_group'] = labelencoder.fit_transform(df['label_group'])\n    \n    trainset = ShopeeDataset(df, transform = get_train_transforms())\n\n    trainloader = torch.utils.data.DataLoader(\n        trainset,\n        batch_size = Config.BATCH_SIZE,\n        pin_memory = True,\n        num_workers = Config.NUM_WORKERS,\n        shuffle = True,\n        drop_last = True\n    )\n\n    model = ShopeeModel()\n    model.to(Config.DEVICE)\n    \n    \n    existing_layer = torch.nn.SiLU\n    new_layer = Mish()\n    model = replace_activations(model, existing_layer, new_layer) # in eca_nfnet_l0 SiLU() is used, but it will be replace by Mish()\n    \n    optimizer = Ranger(model.parameters(), lr = Config.SCHEDULER_PARAMS['lr_start'])\n    scheduler = ShopeeScheduler(optimizer,**Config.SCHEDULER_PARAMS)\n\n    for i in range(Config.EPOCHS):\n\n        avg_loss_train = train_fn(model, trainloader, optimizer, scheduler, i)\n        #torch.save(model.state_dict(),'arcface_512x512_nfnet_l0(mish).pt')\n\nrun_training()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing the Model","metadata":{}},{"cell_type":"code","source":"import random \nimport albumentations as A \nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom torch.utils.data import Dataset \nimport gc\nimport cudf\nimport cuml\nimport cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining the configurations for Testing","metadata":{}},{"cell_type":"code","source":"class CFG:\n    \n    img_size = 512\n    batch_size = 12\n    seed = 2020\n    \n    device = 'cuda'\n    classes = 11014\n    \n    model_name = 'eca_nfnet_l0'\n    model_path = '../input/shopee-pytorch-models/arcface_512x512_nfnet_l0 (mish).pt'\n    \n    scale = 30 \n    margin = 0.5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_dataset():\n    df = pd.read_csv('../input/shopee-product-matching/test.csv')\n    df_cu = cudf.DataFrame(df)\n    image_paths = '../input/shopee-product-matching/test_images/' + df['image']\n    return df, df_cu, image_paths","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_torch(CFG.seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def combine_predictions(row):\n    x = np.concatenate([row['image_predictions'], row['text_predictions']])\n    return ' '.join( np.unique(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_image_predictions(df, embeddings,threshold = 0.0):\n    \n    if len(df) > 3:\n        KNN = 50\n    else : \n        KNN = 3\n    \n    model = NearestNeighbors(n_neighbors = KNN, metric = 'cosine')\n    model.fit(embeddings)\n    distances, indices = model.kneighbors(embeddings)\n    \n    predictions = []\n    for k in tqdm(range(embeddings.shape[0])):\n        idx = np.where(distances[k,] < threshold)[0]\n        ids = indices[k,idx]\n        posting_ids = df['posting_id'].iloc[ids].values\n        predictions.append(posting_ids)\n        \n    del model, distances, indices\n    gc.collect()\n    return predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_test_transforms():\n\n    return A.Compose(\n        [\n            A.Resize(CFG.img_size,CFG.img_size,always_apply=True),\n            A.Normalize(),\n        ToTensorV2(p=1.0)\n        ]\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ShopeeTestSet(Dataset):\n    def __init__(self, image_paths, transforms=None):\n\n        self.image_paths = image_paths\n        self.augmentations = transforms\n\n    def __len__(self):\n        return self.image_paths.shape[0]\n\n    def __getitem__(self, index):\n        image_path = self.image_paths[index]\n        \n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.augmentations:\n            augmented = self.augmentations(image=image)\n            image = augmented['image']       \n    \n        return image,torch.tensor(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_image_embeddings(image_paths, model_name = CFG.model_name):\n    embeds = []\n    \n    model = ShopeeModel(model_name = model_name)\n    model.eval()\n    \n    if model_name == 'eca_nfnet_l0':\n        model = replace_activations(model, torch.nn.SiLU, Mish())\n\n    model.load_state_dict(torch.load(CFG.model_path))\n    model = model.to(CFG.device)\n    \n\n    image_dataset = ShopeeTestSet(image_paths=image_paths,transforms=get_test_transforms())\n    image_loader = torch.utils.data.DataLoader(\n        image_dataset,\n        batch_size=CFG.batch_size,\n        pin_memory=True,\n        drop_last=False,\n        num_workers=4\n    )\n    \n    \n    with torch.no_grad():\n        for img,label in tqdm(image_loader): \n            img = img.cuda()\n            label = label.cuda()\n            feat = model(img,label)[0]\n            image_embeddings = feat.detach().cpu().numpy()\n            embeds.append(image_embeddings)\n    \n    \n    del model\n    image_embeddings = np.concatenate(embeds)\n    print(f'Our image embeddings shape is {image_embeddings.shape}')\n    del embeds\n    gc.collect()\n    return image_embeddings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predictions","metadata":{}},{"cell_type":"code","source":"def get_text_predictions(df, max_features = 25_000):\n    \n    model = TfidfVectorizer(stop_words = 'english', binary = True, max_features = max_features)\n    text_embeddings = model.fit_transform(df_cu['title']).toarray()\n    preds = []\n    CHUNK = 1024*4\n\n    print('Finding similar titles...')\n    CTS = len(df)//CHUNK\n    if len(df)%CHUNK!=0: CTS += 1\n    for j in range( CTS ):\n\n        a = j*CHUNK\n        b = (j+1)*CHUNK\n        b = min(b,len(df))\n        print('chunk',a,'to',b)\n\n        # COSINE SIMILARITY DISTANCE\n        cts = cupy.matmul( text_embeddings, text_embeddings[a:b].T).T\n\n        for k in range(b-a):\n            IDX = cupy.where(cts[k,]>0.75)[0]\n            o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n            preds.append(o)\n    \n    del model,text_embeddings\n    gc.collect()\n    return preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df,df_cu,image_paths = read_dataset()\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_embeddings = get_image_embeddings(image_paths.values)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_predictions = get_image_predictions(df, image_embeddings, threshold = 0.36)\ntext_predictions = get_text_predictions(df, max_features = 25_000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['image_predictions'] = image_predictions\ndf['text_predictions'] = text_predictions\ndf['matches'] = df.apply(combine_predictions, axis = 1)\nprint(df)\ndf[['posting_id', 'matches']].to_csv('submission.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Inferences\nCombined predictions using [eca-nfnet-10] along with TF-DIF Vectorizer provided better F1 score over EfficientNetB[3,5,6] / RESNET152 + TF-DIF predictions.\n\nStep-5\nEnsemble [EfficientNetB6] + [eca-nfnet-10] to see if the [F1]-Score improves.","metadata":{}}]}