{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Training pipline to finetune pre-trained models from [SentenceTransformers](https://www.sbert.net/docs/pretrained_models.html) with Contrastive Loss and Hard Negative Samples\n\n* reference: [CONTRASTIVE LEARNING WITH\nHARD NEGATIVE SAMPLES](https://arxiv.org/pdf/2010.04592.pdf)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install faiss-cpu sentence_transformers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import itertools\nimport os\nimport random as rn\nimport shutil\nfrom scipy.special import comb\n\nimport faiss\nimport numpy as np\nimport pandas as pd\nfrom sentence_transformers import (\n    SentencesDataset,\n    SentenceTransformer,\n    evaluation,\n    losses,\n)\nfrom sentence_transformers.readers import InputExample\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm, notebook","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL_NAME = \"stsb-roberta-base\"\nMODEL = SentenceTransformer(MODEL_NAME)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL_SAVE_PATH = f\"finetuned-model/{MODEL_NAME}\"\nBATCH_SIZE = 32\nCAP_SIZE = 50\nGROUP_CUT = 0.71  # Use option `RUN_ON_TRAIN` to find this number","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Model_Save_Path='../input/sbert-models-for-shopee/' + MODEL_SAVE_PATH","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls $Model_Save_Path","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL = SentenceTransformer(Model_Save_Path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_2 = SentenceTransformer(Model_Save_Path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(MODEL)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DATA_PATH = '../input/'\nDATA_PATH = '../input/shopee-product-matching/'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# f1 score metric\ndef getMetric(col):\n    def f1score(row):\n        n = len( np.intersect1d(row.target,row[col]) )\n        return 2*n / (len(row.target)+len(row[col]))\n    return f1score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(DATA_PATH + 'train.csv')\ntrain['image'] = DATA_PATH + 'train_images/' + train['image']\ntmp = train.groupby('label_group').posting_id.agg('unique').to_dict()\ntrain['target'] = train.label_group.map(tmp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.sort_values(by='label_group')\ntrain['title'] = train['title'].str.lower()\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_embeddings = model_2.encode(train.title)\nprint('text embeddings shape',text_embeddings.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_embeddings = MODEL.encode(train.title)\nprint('text embeddings shape',text_embeddings.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import normalize\n\n# l2 norm to kill all the sim in 0-1\ntext_embeddings = np.vstack(text_embeddings)\ntext_embeddings = normalize(text_embeddings)\n\nimport torch\ntext_embeddings = torch.from_numpy(text_embeddings)\ntext_embeddings = text_embeddings.cuda()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_embeddings.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = []\nCHUNK = 1024*2\n\nprint('Finding similar titles...')\nCTS = len(train)//CHUNK\nif len(train)%CHUNK!=0: CTS += 1\ntext_ids = None\n    \nfor j in range( CTS ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(train))\n    print('chunk',a,'to',b)\n    \n    cts = torch.matmul(text_embeddings, text_embeddings[a:b].T).T\n    cts = cts.data.cpu().numpy()\n    for k in range(b-a):\n        IDX = np.where(cts[k,]>0.93)[0]\n        o = train.iloc[IDX].posting_id.values\n        preds.append(o)\n        \n    del cts\n    torch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['oof_bert'] = preds\n\nCOMPUTE_CV = True\nif COMPUTE_CV:\n    train['f1'] = train.apply(getMetric('oof_bert'),axis=1)\n    print('CV score for baseline =',train.f1.mean())\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import faiss\n\ndef find_similarities_and_indexes(feature_embeddings,embed_dim, top_n=100, features_file=None):\n    if features_file is not None:\n        np.save(features_file, features)\n        \n    # Create index\n    index = faiss.IndexFlatIP( embed_dim )\n    index.add(feature_embeddings)\n    # Search index\n    return index.search(feature_embeddings, top_n)\n    \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_embedding_2 = text_embeddings.cpu().numpy()\ntype(text_embedding_2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embed_dim=768\nsimilarities, indexes = find_similarities_and_indexes(text_embedding_2, embed_dim,top_n=20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"similarities[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"indexes[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GROUP_CUT = 0.71  # Use option `RUN_ON_TRAIN` to find this number\n# Apply cutoff of similiarites\ntrain_are_same_groups = (similarities > GROUP_CUT)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_are_same_groups[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"indexes[0][train_are_same_groups[0]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.index[[0, 33161, 10925]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['label_group'][:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"found_groups = train['label_group'].values[[0, 33161]]\nfound_groups","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"similarity = np.dot(text_embedding_2[:10], text_embedding_2[:10].T)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"similarity","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for k in range(10):\n    IDX = np.where(similarity[k,]>0.93)[0]\n    print(IDX)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build submission\nresults = []\npreds = []\n\nfor i, (test_is_same_group, index_result) in enumerate(zip(train_are_same_groups, indexes)):\n    row_results = set(train.index[index_result[test_is_same_group]])\n    #print(index_result[test_is_same_group])\n    #print(row_results)\n    row_posting_id_results = train.loc[row_results].posting_id.values\n    preds.append(row_posting_id_results)\n    results.append({         'posting_id': train.index[i],  \n                            'matches': ' '.join(row_posting_id_results)    })","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(results[:10])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['oof_bert_faiss'] = preds\nCOMPUTE_CV = True\nif COMPUTE_CV:\n    train['f1'] = train.apply(getMetric('oof_bert_faiss'),axis=1)\n    print('CV score for baseline =',train.f1.mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_groups = train[\"label_group\"].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = train.loc[train.label_group.isin(label_groups[: int(0.8 * len(label_groups))])]\neval_df = train.loc[train.label_group.isin(label_groups[int(0.8 * len(label_groups)) :])]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_df.shape)\ntrain_examples = list()\ntrain_groups = [\n        train_df.loc[train_df[\"label_group\"] == lg][\"title\"].values.tolist()\n        for lg in train_df[\"label_group\"].unique()\n    ]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_groups[:8]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_titles = sum(train_groups, [])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_titles)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_titles[:20]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_embeddings = MODEL.encode(train_titles)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_embeddings /= np.linalg.norm(train_embeddings, 2, axis=1, keepdims=True)\n#train_index = faiss.IndexFlatIP(train_embeddings.shape[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrain_index = faiss.IndexFlatL2(train_embeddings.shape[1])\ntrain_index.add(train_embeddings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_similarities, train_indexes = train_index.search(train_embeddings, 20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CAP_SIZE = 10","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" comb(len(group_1), 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"group_1 = train_groups[0]\nnegative_pairs_no = int(max(CAP_SIZE - comb(len(group_1), 2), comb(len(group_1), 2)))\nprint(negative_pairs_no)\ngroup_embedding = np.ascontiguousarray(\n            np.mean(MODEL.encode(group_1), axis=0).reshape(1, -1), dtype=np.float32\n        )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"group_1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_, similar_idx = train_index.search(group_embedding, negative_pairs_no * 2)\nnegative_titles = [train_titles[idx] for idx in similar_idx[0]]\nlen(negative_titles)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(group_1)\nnegative_titles[4:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for title in group_1:\n    try:\n        negative_titles.remove(title)\n    except:\n        pass\nnegative_titles = negative_titles[:negative_pairs_no]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(negative_titles)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"positive_pairs = [\n            list(pair)\n            for pair in list(itertools.combinations(group_1, 2))\n            if (isinstance(pair[0], str) and isinstance(pair[1], str))\n        ]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for _, group in enumerate(train_groups):\n    negative_pairs_no = int(max(CAP_SIZE - comb(len(group), 2), comb(len(group), 2)))\n    group_embedding = np.ascontiguousarray(\n            np.mean(MODEL.encode(group), axis=0).reshape(1, -1), dtype=np.float32\n        )\n    \n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for _, group in enumerate(train_groups):\n        negative_pairs_no = int(max(CAP_SIZE - comb(len(group), 2), comb(len(group), 2)))\n\n        group_embedding = np.ascontiguousarray(\n            np.mean(MODEL.encode(group), axis=0).reshape(1, -1), dtype=np.float32\n        )\n        _, similar_idx = train_index.search(group_embedding, negative_pairs_no * 2)\n        negative_titles = [train_titles[idx] for idx in similar_idx[0]]\n        for title in group:\n            try:\n                negative_titles.remove(title)\n            except:\n                pass\n        negative_titles = negative_titles[:negative_pairs_no]\n\n        positive_pairs = [\n            list(pair)\n            for pair in list(itertools.combinations(group, 2))\n            if (isinstance(pair[0], str) and isinstance(pair[1], str))\n        ]\n        for pair in positive_pairs:\n            train_examples.append(InputExample(texts=pair, label=1))\n        negative_pairs = [\n            [rn.choice(rn.choice(positive_pairs)), negative_title]\n            for negative_title in negative_titles\n        ]\n        for pair in negative_pairs:\n            train_examples.append(InputExample(texts=pair, label=0))","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_examples)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CAP_SIZE = 50","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL = SentenceTransformer(Model_Save_Path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_prep():\n    df = pd.read_csv(\"../input/shopee-product-matching/train.csv\")\n\n    label_groups = df[\"label_group\"].unique()\n    # rn.shuffle(label_groups)\n    train_df = df.loc[df.label_group.isin(label_groups[: int(0.8 * len(label_groups))])]\n    eval_df = df.loc[df.label_group.isin(label_groups[int(0.8 * len(label_groups)) :])]\n\n    # Prepare train_data according to ContrastiveLoss\n    train_examples = list()\n    train_groups = [\n        train_df.loc[train_df[\"label_group\"] == lg][\"title\"].values.tolist()\n        for lg in train_df[\"label_group\"].unique()\n    ]\n\n    # Build FAISS index to query hard negative samples\n    train_titles = sum(train_groups, [])\n    train_embeddings = MODEL.encode(train_titles)\n    train_index = faiss.IndexFlatL2(train_embeddings.shape[1])\n    train_index.add(train_embeddings)\n    for _, group in enumerate(train_groups):\n        negative_pairs_no = int(max(CAP_SIZE - comb(len(group), 2), comb(len(group), 2)))\n\n        group_embedding = np.ascontiguousarray(\n            np.mean(MODEL.encode(group), axis=0).reshape(1, -1), dtype=np.float32\n        )\n        _, similar_idx = train_index.search(group_embedding, negative_pairs_no * 2)\n        negative_titles = [train_titles[idx] for idx in similar_idx[0]]\n        for title in group:\n            try:\n                negative_titles.remove(title)\n            except:\n                pass\n        #negative_titles = negative_titles[:negative_pairs_no]\n        if negative_pairs_no > 10:\n            negative_titles = negative_titles[ negative_pairs_no-10:]\n        else:\n            negative_titles = negative_titles[:negative_pairs_no]\n                 \n\n        positive_pairs = [\n            list(pair)\n            for pair in list(itertools.combinations(group, 2))\n            if (isinstance(pair[0], str) and isinstance(pair[1], str))\n        ]\n        for pair in positive_pairs:\n            train_examples.append(InputExample(texts=pair, label=1))\n        negative_pairs = [\n            [rn.choice(rn.choice(positive_pairs)), negative_title]\n            for negative_title in negative_titles\n        ]\n        for pair in negative_pairs:\n            train_examples.append(InputExample(texts=pair, label=0))\n\n    print(\"train_examples len:\",len(train_examples))        \n    train_dataset = SentencesDataset(train_examples, MODEL)\n    train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n\n    # Prepare eval_data according to BinaryClassificationEvaluator\n    eval_examples = list()\n    eval_groups = [\n        eval_df.loc[eval_df[\"label_group\"] == lg][\"title\"].values.tolist()\n        for lg in eval_df[\"label_group\"].unique()\n    ]\n    # Build FAISS index to query hard negative samples\n    eval_titles = sum(eval_groups, [])\n    eval_embeddings = MODEL.encode(eval_titles)\n    eval_index = faiss.IndexFlatL2(eval_embeddings.shape[1])\n    eval_index.add(eval_embeddings)\n    for _, group in enumerate(eval_groups):\n        negative_pairs_no = int(max(CAP_SIZE - comb(len(group), 2), comb(len(group), 2)))\n\n        group_embedding = np.ascontiguousarray(\n            np.mean(MODEL.encode(group), axis=0).reshape(1, -1), dtype=np.float32\n        )\n        _, similar_idx = eval_index.search(group_embedding, negative_pairs_no * 2)\n        negative_titles = [eval_titles[idx] for idx in similar_idx[0]]\n        for title in group:\n            try:\n                negative_titles.remove(title)\n            except:\n                pass\n #       negative_titles = negative_titles[:negative_pairs_no]\n        if negative_pairs_no > 10:\n            negative_titles = negative_titles[ negative_pairs_no-10:]\n        else:\n            negative_titles = negative_titles[:negative_pairs_no]\n\n\n        positive_pairs = [\n            list(pair) + [1]\n            for pair in list(itertools.combinations(group, 2))\n            if (isinstance(pair[0], str) and isinstance(pair[1], str))\n        ]\n        negative_pairs = [\n            [rn.choice(rn.choice(positive_pairs)[:2]), negative_title, 0]\n            for negative_title in negative_titles\n        ]\n        eval_examples.append(positive_pairs)\n        eval_examples.append(negative_pairs)\n\n    eval_examples = sum(eval_examples, [])\n    print(\"eval_examples len:\",len(eval_examples))\n\n    evaluator = evaluation.BinaryClassificationEvaluator(\n        sentences1=list(zip(*eval_examples))[0],\n        sentences2=list(zip(*eval_examples))[1],\n        labels=list(zip(*eval_examples))[2],\n        batch_size=BATCH_SIZE,\n    )\n\n    return train_dataloader, evaluator","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader, evaluator = data_prep()","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(MODEL)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ntrain_loss = losses.ContrastiveLoss(model=MODEL)\n\nif not os.path.exists(MODEL_SAVE_PATH):\n    os.makedirs(MODEL_SAVE_PATH)\nelse:\n    shutil.rmtree(MODEL_SAVE_PATH)\n    os.makedirs(MODEL_SAVE_PATH)\n\nMODEL.fit(\n    train_objectives=[(train_dataloader, train_loss)],\n    epochs=3,\n    warmup_steps=100,\n    evaluation_steps=500,\n    output_path=MODEL_SAVE_PATH,\n    evaluator=evaluator,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DATA_PATH = '../input/'\nDATA_PATH = '../input/shopee-product-matching/'\nprint(DATA_PATH)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# f1 score metric\ndef getMetric(col):\n    def f1score(row):\n        n = len( np.intersect1d(row.target,row[col]) )\n        return 2*n / (len(row.target)+len(row[col]))\n    return f1score","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(DATA_PATH + 'train.csv')\ntrain['image'] = DATA_PATH + 'train_images/' + train['image']\ntmp = train.groupby('label_group').posting_id.agg('unique').to_dict()\ntrain['target'] = train.label_group.map(tmp)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.sort_values(by='label_group')\ntrain['title'] = train['title'].str.lower()\ntrain.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_embeddings = MODEL.encode(train.title)\nprint('text embeddings shape',text_embeddings.shape)","metadata":{},"execution_count":null,"outputs":[]}]}