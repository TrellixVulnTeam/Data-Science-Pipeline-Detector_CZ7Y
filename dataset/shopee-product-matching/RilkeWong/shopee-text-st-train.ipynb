{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport os\nimport logging\nfrom datetime import datetime\nimport gzip\nimport csv\nimport math\n\nimport cv2, matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\nimport gc \n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install ../input/sentence-transformers/sentence-transformers-1.1.0/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DATA_PATH = '../input/'\nDATA_PATH = '../input/shopee-product-matching/'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# f1 score metric\ndef getMetric(col):\n    def f1score(row):\n        n = len( np.intersect1d(row.target,row[col]) )\n        return 2*n / (len(row.target)+len(row[col]))\n    return f1score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(DATA_PATH + 'train.csv')\ntrain['image'] = DATA_PATH + 'train_images/' + train['image']\ntmp = train.groupby('label_group').posting_id.agg('unique').to_dict()\ntrain['target'] = train.label_group.map(tmp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.sort_values(by='label_group')\ntrain['title'] = train['title'].str.lower()\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_triplets_titles = pd.read_csv('../input/shopee-generate-data-for-triplet-loss/' + 'train_triplets_titles.csv')\ntrain_triplets_titles.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_triplets_titles.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# tfidf","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ncorpus = [\n    'This is the first document.',\n    'This document is the second document.',\n    'And this is the third one.',\n    'Is this the first document?',\n]\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())\n\nprint(X.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.toarray()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx_x = np.where(X.toarray() > 0)\nprint(idx_x)\nnp.asarray(idx_x).shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = TfidfVectorizer(stop_words=None, binary=True, max_features=55000)\ntext_embeddings = model.fit_transform(train.title).toarray()\nprint('text embeddings shape',text_embeddings.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ntext_embeddings = torch.from_numpy(text_embeddings)\ntext_embeddings = text_embeddings.cuda()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = []\nCHUNK = 1024*2\n\nprint('Finding similar titles...')\nCTS = len(train)//CHUNK\nif len(train)%CHUNK!=0: CTS += 1\ntext_ids = None\n    \nfor j in range( CTS ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(train))\n    print('chunk',a,'to',b)\n    \n    cts = torch.matmul(text_embeddings, text_embeddings[a:b].T).T\n    cts = cts.data.cpu().numpy()\n    for k in range(b-a):\n        IDX = np.where(cts[k,]>0.6)[0]\n        o = train.iloc[IDX].posting_id.values\n        preds.append(o)\n        \n    del cts\n    torch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del text_embeddings\ntorch.cuda.empty_cache()\n\ntrain['oof_text'] = preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"COMPUTE_CV = True\n\nif COMPUTE_CV:\n    train['f1'] = train.apply(getMetric('oof_text'),axis=1)\n    print('CV score for baseline =',train.f1.mean())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Word2Vec","metadata":{}},{"cell_type":"code","source":"from gensim.test.utils import get_tmpfile\nfrom gensim.models import KeyedVectors\n\nvectors = KeyedVectors.load_word2vec_format(\"../input/glove2word2vec/glove_w2v.txt\") # import the data file","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize\ntrain_title_token = train['title'].apply(lambda x: word_tokenize(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_embeddings = []\nfor title in tqdm_notebook(train_title_token[:]):\n    title_feat = []\n    for word in title:\n        if word in vectors:\n            title_feat.append(vectors[word])\n    \n    if len(title_feat) == 0:\n        title_feat = np.random.rand(200)\n    else:\n        # max-pooling\n        # mean-pooling\n        # IDF\n        # SIF\n        title_feat = np.vstack(title_feat).max(0)\n    text_embeddings.append(title_feat)\n    # break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import normalize\n\n# l2 norm to kill all the sim in 0-1\ntext_embeddings = np.vstack(text_embeddings)\ntext_embeddings = normalize(text_embeddings)\n\nimport torch\ntext_embeddings = torch.from_numpy(text_embeddings)\ntext_embeddings = text_embeddings.cuda()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = []\nCHUNK = 1024*4\n\n\nprint('Finding similar images...')\nCTS = len(text_embeddings)//CHUNK\nif len(text_embeddings)%CHUNK!=0: CTS += 1\nfor j in range( CTS ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(train))\n    print('chunk',a,'to',b)\n    \n    cts = torch.matmul(text_embeddings, text_embeddings[a:b].T).T\n    cts = cts.data.cpu().numpy()\n    for k in range(b-a):\n        IDX = np.where(cts[k,]>0.93)[0]\n        o = train.iloc[IDX].posting_id.values\n        preds.append(o)\n        \n    del cts\n    torch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['oof_w2v'] = preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"COMPUTE_CV = True\n\nif COMPUTE_CV:\n    train['f1'] = train.apply(getMetric('oof_w2v'),axis=1)\n    print('CV score for baseline =',train.f1.mean())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sentence Transformers","metadata":{}},{"cell_type":"code","source":"def combine_for_oof(row):\n    x = np.concatenate([row.oof_text,row.oof_w2v])\n    return np.unique(x)\n\n# merge product proposal by tfidf and word2vec, we have positive and negative example.\n# if two product in same group they are positive label.\ntrain['oof'] = train.apply(combine_for_oof,axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_pair = train.set_index('posting_id')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_pair.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom sentence_transformers import models, losses, util\nfrom sentence_transformers import LoggingHandler, SentenceTransformer\nfrom sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\nfrom sentence_transformers.readers import *","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\nmodel_save_path = '/kaggle/working/models/training_shopee_title_embeddings-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir /kaggle/working/models","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle/working/models","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer, InputExample, losses\nfrom torch.utils.data import DataLoader\n\n#Define the model. Either from scratch of by loading a pre-trained model\n#model = SentenceTransformer('distiluse-base-multilingual-cased-v1')\nst_model = SentenceTransformer('stsb-roberta-base')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_examples = []\ntitle_pair = []\nfor row in tqdm_notebook(train_pair.iterrows()):\n    for pair in row[1].oof:\n        # not match self\n        if pair == row[0]:\n            continue\n    \n        if pair in row[1].target:\n            lbl = 1.0\n        else:\n            lbl = 0.0\n        title1 = row[1].title\n        title2 = train_pair.loc[pair]['title']\n            \n        inputExample = InputExample( texts=[ row[1].title, train_pair.loc[pair]['title'] ], label=lbl)\n        train_examples.append(inputExample)\n        title_pair.append(\n            [row[1].title, train_pair.loc[pair]['title'], lbl]\n        )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title_pair = pd.DataFrame(title_pair, columns=['s1', 's2', 'label'])\ntitle_pair = title_pair.sample(frac=1)\ntitle_pair.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title_pair.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_inputs, test_inputs = train_test_split(train_examples,test_size=0.1,random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(train_inputs), len(test_inputs))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(train_inputs, shuffle=True, batch_size=32)\ntrain_loss = losses.CosineSimilarityLoss(st_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sentence_transformers import evaluation\nfrom sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\nevaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_inputs, name='shopee-titles-test')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(st_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"st_model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=100, evaluator=evaluator, evaluation_steps=500, output_path=model_save_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls $model_save_path","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model2 = SentenceTransformer(model_path)\n#print(model2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(st_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"st_model.evaluate(evaluator)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_embeddings = model.encode(train.title)\nprint('text embeddings shape',text_embeddings.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import normalize\n\n# l2 norm to kill all the sim in 0-1\ntext_embeddings = np.vstack(text_embeddings)\ntext_embeddings = normalize(text_embeddings)\n\nimport torch\ntext_embeddings = torch.from_numpy(text_embeddings)\ntext_embeddings = text_embeddings.cuda()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = []\nCHUNK = 1024*2\n\nprint('Finding similar titles...')\nCTS = len(train)//CHUNK\nif len(train)%CHUNK!=0: CTS += 1\ntext_ids = None\n    \nfor j in range( CTS ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(train))\n    print('chunk',a,'to',b)\n    \n    cts = torch.matmul(text_embeddings, text_embeddings[a:b].T).T\n    cts = cts.data.cpu().numpy()\n    for k in range(b-a):\n        IDX = np.where(cts[k,]>0.93)[0]\n        o = train.iloc[IDX].posting_id.values\n        preds.append(o)\n        \n    del cts\n    torch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['oof_bert'] = preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if COMPUTE_CV:\n    train['f1'] = train.apply(getMetric('oof_bert'),axis=1)\n    print('CV score for baseline =',train.f1.mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_embeddings.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}