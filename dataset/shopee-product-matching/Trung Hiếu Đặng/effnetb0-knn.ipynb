{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Load libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np, pandas as pd, gc\nimport cv2, matplotlib.pyplot as plt\nimport cudf, cuml, cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors\nimport tensorflow as tf\nfrom tensorflow.keras.applications import EfficientNetB0","metadata":{"execution":{"iopub.status.busy":"2022-01-04T04:57:00.973207Z","iopub.execute_input":"2022-01-04T04:57:00.973589Z","iopub.status.idle":"2022-01-04T04:57:09.651922Z","shell.execute_reply.started":"2022-01-04T04:57:00.973511Z","shell.execute_reply":"2022-01-04T04:57:09.651137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RESTRICT TENSORFLOW TO 1GB OF GPU RAM\n# SO THAT WE HAVE 15GB RAM FOR RAPIDS\nLIMIT = 1\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n  try:\n    tf.config.experimental.set_virtual_device_configuration(\n        gpus[0],\n        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*LIMIT)])\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n    #print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n  except RuntimeError as e:\n    print(e)\nprint('We will restrict TensorFlow to max %iGB GPU RAM'%LIMIT)\nprint('then RAPIDS can use %iGB GPU RAM'%(16-LIMIT))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load traindata","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/shopee-product-matching/train.csv')\ntrain.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Display random traindata","metadata":{}},{"cell_type":"code","source":"def displayTrainData(train, random=False, COLS=6, ROWS=4, path='../input/shopee-product-matching/train_images/'):\n    for k in range(ROWS):\n        plt.figure(figsize=(20,5))\n        for j in range(COLS):\n            if random: row = np.random.randint(0,len(train))\n            else: row = COLS*k + j\n            name = train.iloc[row,1]\n            title = train.iloc[row,3]\n            title_with_return = \"\"\n            for i,ch in enumerate(title):\n                title_with_return += ch\n                if (i!=0)&(i%20==0): title_with_return += '\\n'\n            img = cv2.imread(path+name)\n            plt.subplot(1,COLS,j+1)\n            plt.title(title_with_return)\n            plt.axis('off')\n            plt.imshow(img)\n        plt.show()\n        \ndisplayTrainData(train,random=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Display Train data group by label_group\nShow first 3 groups","metadata":{}},{"cell_type":"code","source":"groups = train.label_group.value_counts()\n\ndef displayTrainDataBLG(groups, COLS=6, ROWS=4, groups_count=3):\n    for k in range(groups_count):\n        print('label_group: ',groups.index[k])\n        \n        top = train.loc[train.label_group==groups.index[k]]\n        displayTrainData(top, random=False, ROWS=ROWS, COLS=COLS)\n\ndisplayTrainDataBLG(groups)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Number of items in the group (descend)\nShow first 10 groups","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,5))\nplt.bar(groups.index.values[:10].astype('str'),groups.values[:10])\nplt.xticks(rotation = 45)\nplt.ylabel('Number of items',size=14)\nplt.xlabel('Label Group',size=14)\nplt.title('Top 10 groups',size=16)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load test\nSince public test data only has 3 posts, we use training data for testing.","metadata":{}},{"cell_type":"code","source":"USE_TRAIN_DATA = True\n\ntest = pd.read_csv('../input/shopee-product-matching/test.csv')\n# Nếu submit thì không dùng traindata\nif len(test)>3: USE_TRAIN_DATA = False","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/shopee-product-matching/train.csv')\ntmp = train.groupby('label_group').posting_id.agg('unique').to_dict()\n# EXPECT RESULT\ntrain['target'] = train.label_group.map(tmp)\ntrain.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if USE_TRAIN_DATA:\n    test = pd.read_csv('../input/shopee-product-matching/train.csv')\n    test_gf = cudf.DataFrame(test)\n    print('Dùng traindata để test khi chưa submit' )\nelse:\n    #     Real test data\n    test = pd.read_csv('../input/shopee-product-matching/test.csv')\n    test_gf = cudf.read_csv('../input/shopee-product-matching/test.csv')\n    print('SUBMITTED')\ntest_gf.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Use image_phash\nPosts always self-match. So use image_phash to add self-post.","metadata":{}},{"cell_type":"code","source":"tmp = test.groupby('image_phash').posting_id.agg('unique').to_dict()\ntest['use_image_phash'] = test.image_phash.map(tmp)\ntest.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Use title\nExtract Text Embeddings with TfidfVectorizer. Finding similar titles, use cosine similarity instead of KNN.","metadata":{}},{"cell_type":"code","source":"print('Computing text embeddings...')\nmodel = TfidfVectorizer(stop_words='english', binary=True, max_features=25_000)\ntext_embeddings = model.fit_transform(test_gf.title).toarray()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = []\nCHUNK = 1024*4\n\nprint('Finding similar titles...')\nCTS = len(test)//CHUNK\nif len(test)%CHUNK!=0: CTS += 1\nfor j in range( CTS ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(test))\n    \n    # COSINE SIMILARITY DISTANCE\n    cts = cupy.matmul( text_embeddings, text_embeddings[a:b].T).T\n    \n    for k in range(b-a):\n        IDX = cupy.where(cts[k,]>0.7)[0]\n        o = test.iloc[cupy.asnumpy(IDX)].posting_id.values\n        preds.append(o)\n        \ndel model, text_embeddings\n_ = gc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['use_title'] = preds\ntest.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Use image\nUse KNN to find similar images.","metadata":{}},{"cell_type":"code","source":"class DataGenerator(tf.keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, df, img_size=256, batch_size=32, path=''): \n        self.df = df\n        self.img_size = img_size\n        self.batch_size = batch_size\n        self.path = path\n        self.indexes = np.arange( len(self.df) )\n        \n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        ct = len(self.df) // self.batch_size\n        ct += int(( (len(self.df)) % self.batch_size)!=0)\n        return ct\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        X = self.__data_generation(indexes)\n        return X\n            \n    def __data_generation(self, indexes):\n        'Generates data containing batch_size samples' \n        X = np.zeros((len(indexes),self.img_size,self.img_size,3),dtype='float32')\n        df = self.df.iloc[indexes]\n        for i,(index,row) in enumerate(df.iterrows()):\n            img = cv2.imread(self.path+row.image)\n            X[i,] = cv2.resize(img,(self.img_size,self.img_size))\n        return X","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BASE = '../input/shopee-product-matching/test_images/'\nif USE_TRAIN_DATA: BASE = '../input/shopee-product-matching/train_images/'\n\nWGT = '../input/effnetb0/efficientnetb0_notop.h5'\nmodel = EfficientNetB0(weights=WGT,include_top=False, pooling='avg', input_shape=None)\n\nembeds = []\nCHUNK = 1024*4\n\nprint('Computing image embeddings...')\nCTS = len(test)//CHUNK\nif len(test)%CHUNK!=0: CTS += 1\nfor i,j in enumerate( range( CTS ) ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(test))\n    \n    test_gen = DataGenerator(test.iloc[a:b], batch_size=32, path=BASE)\n    image_embeddings = model.predict(test_gen,verbose=1,use_multiprocessing=True, workers=4)\n    embeds.append(image_embeddings)\n    \ndel model\n_ = gc.collect()\nimage_embeddings = np.concatenate(embeds)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"KNN = 60\nif len(test)==3: KNN = 2\nmodel = NearestNeighbors(n_neighbors=KNN)\nmodel.fit(image_embeddings)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = []\nCHUNK = 1024*4\n\nprint('Finding similar images...')\nCTS = len(image_embeddings)//CHUNK\nif len(image_embeddings)%CHUNK!=0: CTS += 1\nfor j in range( CTS ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(image_embeddings))\n    distances, indices = model.kneighbors(image_embeddings[a:b,])\n    \n    for k in range(b-a):\n        IDX = np.where(distances[k,]<6.0)[0]\n        IDS = indices[k,IDX]\n        o = test.iloc[IDS].posting_id.values\n        preds.append(o)\n        \ndel model, distances, indices, image_embeddings, embeds\n_ = gc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['use_image'] = preds\ntest.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission CSV\nCreate submission.csv","metadata":{}},{"cell_type":"code","source":"def combine_for_sub(row):\n    x = np.concatenate([row.use_image,row.use_title, row.use_image_phash])\n    return ' '.join( np.unique(x) )\n\ntest['matches'] = test.apply(combine_for_sub,axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[['posting_id','matches']].to_csv('submission.csv',index=False)\nsub = pd.read_csv('submission.csv')\nsub.head()","metadata":{},"execution_count":null,"outputs":[]}]}