{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport transformers\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score","metadata":{"execution":{"iopub.status.busy":"2021-06-08T10:09:52.817943Z","iopub.execute_input":"2021-06-08T10:09:52.818276Z","iopub.status.idle":"2021-06-08T10:09:52.822298Z","shell.execute_reply.started":"2021-06-08T10:09:52.818249Z","shell.execute_reply":"2021-06-08T10:09:52.821623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2021-06-08T10:09:52.828748Z","iopub.execute_input":"2021-06-08T10:09:52.82928Z","iopub.status.idle":"2021-06-08T10:09:52.83703Z","shell.execute_reply.started":"2021-06-08T10:09:52.829245Z","shell.execute_reply":"2021-06-08T10:09:52.836162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('../input/shopee-product-matching/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-08T10:09:52.844685Z","iopub.execute_input":"2021-06-08T10:09:52.845144Z","iopub.status.idle":"2021-06-08T10:09:52.858458Z","shell.execute_reply.started":"2021-06-08T10:09:52.845111Z","shell.execute_reply":"2021-06-08T10:09:52.857361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_orig = pd.read_csv(\"../input/shopee-product-matching/train.csv\") ","metadata":{"execution":{"iopub.status.busy":"2021-06-08T10:09:52.859615Z","iopub.execute_input":"2021-06-08T10:09:52.860159Z","iopub.status.idle":"2021-06-08T10:09:52.956846Z","shell.execute_reply.started":"2021-06-08T10:09:52.860129Z","shell.execute_reply":"2021-06-08T10:09:52.955971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/shopee-siamese-training/siamese_data.csv\") ","metadata":{"execution":{"iopub.status.busy":"2021-06-08T10:09:52.958579Z","iopub.execute_input":"2021-06-08T10:09:52.958824Z","iopub.status.idle":"2021-06-08T10:09:53.135526Z","shell.execute_reply.started":"2021-06-08T10:09:52.958799Z","shell.execute_reply":"2021-06-08T10:09:53.134485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-08T10:09:53.137371Z","iopub.execute_input":"2021-06-08T10:09:53.137762Z","iopub.status.idle":"2021-06-08T10:09:53.1529Z","shell.execute_reply.started":"2021-06-08T10:09:53.13772Z","shell.execute_reply":"2021-06-08T10:09:53.15172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n#print(model)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T10:09:53.154373Z","iopub.execute_input":"2021-06-08T10:09:53.154771Z","iopub.status.idle":"2021-06-08T10:09:53.165452Z","shell.execute_reply.started":"2021-06-08T10:09:53.154727Z","shell.execute_reply":"2021-06-08T10:09:53.164344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ls","metadata":{"execution":{"iopub.status.busy":"2021-06-08T10:09:53.167188Z","iopub.execute_input":"2021-06-08T10:09:53.167813Z","iopub.status.idle":"2021-06-08T10:09:53.177714Z","shell.execute_reply.started":"2021-06-08T10:09:53.167765Z","shell.execute_reply":"2021-06-08T10:09:53.176601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model.save_pretrained(\"./pre_bert\")","metadata":{"execution":{"iopub.status.busy":"2021-06-08T10:09:53.17905Z","iopub.execute_input":"2021-06-08T10:09:53.179401Z","iopub.status.idle":"2021-06-08T10:09:53.188645Z","shell.execute_reply.started":"2021-06-08T10:09:53.179369Z","shell.execute_reply":"2021-06-08T10:09:53.18745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model = transformers.BertTokenizer.from_pretrained(\"./pre_bert\")","metadata":{"execution":{"iopub.status.busy":"2021-06-08T10:09:53.190322Z","iopub.execute_input":"2021-06-08T10:09:53.190731Z","iopub.status.idle":"2021-06-08T10:09:53.199466Z","shell.execute_reply.started":"2021-06-08T10:09:53.190699Z","shell.execute_reply":"2021-06-08T10:09:53.198236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(model)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T10:09:53.202278Z","iopub.execute_input":"2021-06-08T10:09:53.20278Z","iopub.status.idle":"2021-06-08T10:09:53.211096Z","shell.execute_reply.started":"2021-06-08T10:09:53.202728Z","shell.execute_reply":"2021-06-08T10:09:53.210353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_length = 357    #maximum length of input sentence\nbatch_size = 32     #number of samples propagated through the network\nepochs = 2          #one epoch: entire dataset is passed forward and backward through the neural network\n\nlabels = [0, 1]     # 0:no similarity; 1:similar\n\ndata = pd.read_csv(\"../input/shopee-siamese-training/siamese_data.csv\", nrows=10000)\ntrain_df, test_df = train_test_split(data, test_size=0.2, shuffle=True)     #split dataset into train and test\ntrain_df, val_df = train_test_split(train_df, test_size=0.2, shuffle=True)  # split training dataset to also get an validation dataset\n\nprint(f\"Total train samples: {train_df.shape[0]}\")\nprint(f\"Total validation samples: {val_df.shape[0]}\")\nprint(f\"Total test samples: {test_df.shape[0]}\")\n\n# print(f\"Title1: {train_df.loc[1, 'title_1']}\")\n# print(f\"title2: {train_df.loc[1, 'title_2']}\")\n# print(f\"similarity: {train_df.loc[1, 'label']}\")\n\nprint(\"Train target distribution\")\nprint(train_df.label.value_counts())\n\nprint(\"Valid target distribution\")\nprint(val_df.label.value_counts())\n\nprint(\"Test target distribution\")\nprint(test_df.label.value_counts())\n\ny_train = tf.keras.utils.to_categorical(train_df.label, num_classes=2)\ny_test = tf.keras.utils.to_categorical(test_df.label, num_classes=2)\ny_val = tf.keras.utils.to_categorical(val_df.label, num_classes=2)\n\nclass BertSemanticDataGenerator(tf.keras.utils.Sequence):\n    \"\"\"Generates batches of data.\n\n    Args:\n        sentence_pairs: Array of title_1 and title_2.\n        labels: Array of labels.\n        batch_size: Integer batch size.\n        shuffle: boolean, whether to shuffle the data.\n        include_targets: boolean, whether to include the labels.\n\n    Returns:\n        Tuples `([input_ids, attention_mask, `token_type_ids], labels)`\n        (or just `[input_ids, attention_mask, `token_type_ids]`\n         if `include_targets=False`)\n    \"\"\"\n\n    def __init__(\n        self,\n        sentence_pairs,\n        labels,\n        batch_size=batch_size,\n        shuffle=True,\n        include_targets=True,\n    ):\n        self.sentence_pairs = sentence_pairs\n        self.labels = labels\n        self.shuffle = shuffle\n        self.batch_size = batch_size\n        self.include_targets = include_targets\n        # Load BERT Tokenizer to encode the text\n        # bert-base-uncased pretrained model: pretrained model on English language\n        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n            \"../input/pre-bert\"\n        )\n        self.indexes = np.arange(len(self.sentence_pairs))\n        self.on_epoch_end()     #logs a dictionary with metric results for one training epoch (and validation epoch); example: {'loss':0.2, 'accuracy':0.7}\n\n    def __len__(self):\n        # Denotes the number of batches per epoch.\n        return len(self.sentence_pairs) // self.batch_size  # //-floor division: abrunden\n\n    def __getitem__(self, idx):\n        # Retrieves the batch of index.\n        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n        sentence_pairs = self.sentence_pairs[indexes]\n        \n                # With BERT tokenizer's batch_encode_plus batch of both the sentences are\n        # encoded together and separated by [SEP] token.\n        encoded = self.tokenizer.batch_encode_plus(\n            sentence_pairs.tolist(),\n            add_special_tokens=True,    # example: SEP, CLS\n            max_length=max_length,\n            return_attention_mask=True, # 1 indicates a value that should be attended to; no attention to values with 0\n            return_token_type_ids=True, # first sequence(sentence) represented by 0; second sequence represented by 1\n            pad_to_max_length=True,     # sentences are padded to the maximum sentence length\n            truncation=True,\n            return_tensors=\"tf\",        # returns TensorFlow tf.constant object (PyTorch also possible)\n        )\n\n        # Convert batch of encoded features to numpy array.\n        input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")               #tokens converted into IDs\n        attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n        token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n\n        # Set to true if data generator is used for training/validation.\n        if self.include_targets:\n            labels = np.array(self.labels[indexes], dtype=\"int32\")\n            return [input_ids, attention_masks, token_type_ids], labels\n        else:\n            return [input_ids, attention_masks, token_type_ids]\n\n    def on_epoch_end(self):\n        # Shuffle indexes after each epoch if shuffle is set to True.\n        if self.shuffle:\n            np.random.RandomState(42).shuffle(self.indexes)\n            \n# Create the model under a distribution strategy scope.\nstrategy = tf.distribute.MirroredStrategy()     # for training with multiple GPUs\n\nwith strategy.scope():\n    # Encoded token ids from BERT tokenizer.\n    input_ids = tf.keras.layers.Input(                          # input is used to instantiate a Keras tensor\n        shape=(max_length,), dtype=tf.int32, name=\"input_ids\"   # shape: indicates that the expected input will be batches of max_length-dimensional vectors; name: optional unique name string\n    )\n    # Attention masks indicates to the model which tokens should be attended to.\n    attention_masks = tf.keras.layers.Input(\n        shape=(max_length,), dtype=tf.int32, name=\"attention_masks\"\n    )\n    # Token type ids are binary masks identifying different sequences in the model.\n    token_type_ids = tf.keras.layers.Input(\n        shape=(max_length,), dtype=tf.int32, name=\"token_type_ids\"\n    )\n    # Loading pretrained BERT model.\n    bert_model = transformers.TFBertModel.from_pretrained(\"../input/pre-bert\")\n    # Freeze the BERT model to reuse the pretrained features without modifying them.\n    bert_model.trainable = False\n\n#     sequence_output, pooled_output = bert_model(\n#         input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n#     )\n    bert_output = bert_model(\n        input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n    )\n    sequence_output = bert_output.last_hidden_state\n    pooled_output = bert_output.pooler_output\n    # Add trainable layers on top of frozen layers to adapt the pretrained features on the new data.\n    bi_lstm = tf.keras.layers.Bidirectional(\n        tf.keras.layers.LSTM(64, return_sequences=True) # 64: dimensionality of output space\n    )(sequence_output)\n    # Applying hybrid pooling approach to bi_lstm sequence output.\n    avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm)\n    max_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm)\n    concat = tf.keras.layers.concatenate([avg_pool, max_pool])\n    dropout = tf.keras.layers.Dropout(0.3)(concat)  # to prevent overfitting; sets randomly input units to 0 with a frequency of 0.3\n    output = tf.keras.layers.Dense(2, activation=\"softmax\")(dropout)    # dense implements output = activation(...)\n    model = tf.keras.models.Model(\n        inputs=[input_ids, attention_masks, token_type_ids], outputs=output\n    )\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(),       # optimization algorithm to update network weights iterative\n        loss=\"categorical_crossentropy\",\n        metrics=[\"acc\"],\n    )\n\nprint(f\"Strategy: {strategy}\")\nmodel.summary()\n\ntrain_data = BertSemanticDataGenerator(\n    train_df[[\"title_1\", \"title_2\"]].values.astype(\"str\"),\n    y_train,\n    batch_size=batch_size,\n    shuffle=True,\n)\nvalid_data = BertSemanticDataGenerator(\n    val_df[[\"title_1\", \"title_2\"]].values.astype(\"str\"),\n    y_val,\n    batch_size=batch_size,\n    shuffle=False,\n)\n\nhistory = model.fit(\n    train_data,\n    validation_data=valid_data,     # data on which to evaluate the loss and metrics at the end of each epoch\n    epochs=epochs,\n    use_multiprocessing=True,\n    workers=-1,\n)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-08T10:09:53.212462Z","iopub.execute_input":"2021-06-08T10:09:53.212829Z","iopub.status.idle":"2021-06-08T13:49:51.464517Z","shell.execute_reply.started":"2021-06-08T10:09:53.212793Z","shell.execute_reply":"2021-06-08T13:49:51.462912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-08T13:49:51.468198Z","iopub.execute_input":"2021-06-08T13:49:51.468549Z","iopub.status.idle":"2021-06-08T13:49:51.484959Z","shell.execute_reply.started":"2021-06-08T13:49:51.468473Z","shell.execute_reply":"2021-06-08T13:49:51.483879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('../input/test-100/test_100.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-08T13:49:51.486547Z","iopub.execute_input":"2021-06-08T13:49:51.487102Z","iopub.status.idle":"2021-06-08T13:49:51.509724Z","shell.execute_reply.started":"2021-06-08T13:49:51.487051Z","shell.execute_reply":"2021-06-08T13:49:51.509043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_comp = pd.DataFrame(columns=['posting_id1', 'posting_id2', 'title1', 'title2'])\nfor i in tqdm(range(len(test))):\n    posting1 = []\n    posting2 = []\n    title1 =[]\n    title2 = []\n    for j in range(len(test)):\n        posting1.append(test['posting_id'].iloc[i])\n        posting2.append(test['posting_id'].iloc[j])\n        title1.append(test['title'].iloc[i])\n        title2.append(test['title'].iloc[j])\n    tmp_df = pd.DataFrame()\n    tmp_df['posting_id1'] = posting1\n    tmp_df['posting_id2'] = posting2\n    tmp_df['title1'] = title1\n    tmp_df['title2'] = title2\n    test_comp = test_comp.append(tmp_df, ignore_index = True)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T13:49:51.511423Z","iopub.execute_input":"2021-06-08T13:49:51.511784Z","iopub.status.idle":"2021-06-08T13:49:52.142726Z","shell.execute_reply.started":"2021-06-08T13:49:51.511745Z","shell.execute_reply":"2021-06-08T13:49:52.141692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_similarity(title_1, title_2):\n    sentence_pairs = np.array([[str(title_1), str(title_2)]])\n    test_data = BertSemanticDataGenerator(\n        sentence_pairs, labels=None, batch_size=1, shuffle=False, include_targets=False,\n    )\n\n    proba = model.predict(test_data)[0]\n    idx = np.argmax(proba)\n    proba = f\"{proba[idx]: .2f}%\"\n    pred = labels[idx]\n    return pred, proba\n\nprediction = []\nprobability = []\nfor i in tqdm(range(len(test_comp))):\n    pred, proba = check_similarity(test_comp['title1'].iloc[i], test_comp['title2'].iloc[i])\n    prediction.append(pred)\n    probability.append(proba)\ntest_comp['prediction'] = prediction\ntest_comp['probability'] = probability","metadata":{"execution":{"iopub.status.busy":"2021-06-08T15:41:18.71095Z","iopub.execute_input":"2021-06-08T15:41:18.711588Z","iopub.status.idle":"2021-06-08T15:41:18.791745Z","shell.execute_reply.started":"2021-06-08T15:41:18.711496Z","shell.execute_reply":"2021-06-08T15:41:18.789869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_comp.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.DataFrame(columns=['posting_id', 'matches'])\nfor i in test_comp.posting_id1.unique():\n    matches = []\n    id_df = test_comp[(test_comp.posting_id1 == i) & (test_comp.prediction == 1)]\n    matches = id_df.posting_id2.values\n    matches = np.append(matches, i)\n    submission_df = submission_df.append({'posting_id':i, 'matches': matches}, ignore_index=True)\n    #submission_df = submission_df.append({'posting_id':i, 'matches': ' '.join(matches)}, ignore_index=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['prediction'] = submission_df.matches","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def F1_score(target_column, pred_column):\n    '''Returns the F1_score for each row in the data.\n    Remember: The final score is the mean F1 score.\n    target_column: the name of the column that contains the target\n    pred_column: the name of the column that contains the prediction\n    '''\n    \n    def get_f1(row):\n        # Find the common values in target and prediction arrays.\n        intersection = len( np.intersect1d(row[target_column], row[pred_column]) )\n        # Computes the score by following the formula\n        f1_score = 2 * (intersection / (len(row[target_column]) + len(row[pred_column])))\n        \n        return f1_score\n    \n    return get_f1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['F1'] = train.apply(F1_score(target_column=\"target\", pred_column=\"prediction\"), axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('CV score for baseline = {:.3f}'.format(train[\"F1\"].mean()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}