{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nSUBMIT = False\n\nif SUBMIT:\n    train = pd.read_csv('../input/shopee-product-matching/test.csv', usecols=[\"posting_id\", \"title\"])\nelse:\n    train = pd.read_csv('../input/shopee-product-matching/train.csv', usecols=[\"posting_id\", \"title\", \"label_group\"])\n    tmp = train.groupby('label_group').posting_id.agg('unique').to_dict()\n    train['target'] = train.label_group.map(tmp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm.notebook import tqdm\nfrom transformers import *\nimport torch\n\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n    return sum_embeddings / sum_mask\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# downloaded from https://huggingface.co/sentence-transformers/stsb-xlm-r-multilingual/tree/main\ntokenizer = AutoTokenizer.from_pretrained(\"../input/xlmxlm\")\nmodel = AutoModel.from_pretrained(\"../input/xlmxlm\").to(device)\n\ntext_tensor = torch.zeros((train.shape[0], 768)).to(device)\nchunk = 64\nfor i in tqdm(list(range(0, train.shape[0], chunk)) + [train.shape[0]-chunk]):\n    titles = []\n    for title in train.title[i : i + chunk].values:\n        title = title.encode('utf-8').decode(\"unicode_escape\")\n        title = title.encode('ascii', 'ignore').decode(\"unicode_escape\")\n        title = title.lower()\n        titles.append(title)\n    \n    encoded_input = tokenizer(titles, padding=True, truncation=True,\n                              max_length=128, return_tensors='pt').to(device)\n    with torch.no_grad():\n        model_output = model(**encoded_input)\n\n    text_tensor[i : i + chunk] = mean_pooling(model_output, encoded_input['attention_mask'])\n\ntext_tensor /= torch.norm(text_tensor, p=2, dim=-1, keepdim=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm.notebook import tqdm\nimport torch\n\nout_preds = []\nchunk = 32\nfor i in tqdm(list(range(0, train.shape[0], chunk)) + [train.shape[0]-chunk]):\n    arr = text_tensor[i : i + chunk] @ text_tensor.T\n\n    indices = torch.nonzero(arr > 0.86)\n\n    preds = dict()\n    for k in range(arr.shape[0]):\n        preds[k] = []\n    for ind in range(indices.size(0)):\n        preds[indices[ind, 0].item()].append(indices[ind, 1].item())\n\n    out_preds.extend([(train.iloc[k].posting_id, train.iloc[v].posting_id.tolist()) for k, v in preds.items()])\n\nout_preds = out_preds[:train.shape[0]]\ndf = pd.DataFrame(out_preds, columns=[\"index\",\"pred\"])\ndf.set_index(\"index\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if not SUBMIT:\n    df[\"true\"] = train[\"target\"]\n\n    f1 = []\n    for index, row in df[[\"true\", \"pred\"]].iterrows():\n        f1.append((2 * len(set(row[\"true\"]) & set(row[\"pred\"])))/(len(row[\"true\"]) + len(row[\"pred\"])))\n\n    print(f'F1: {np.mean(f1)}')\nelse:\n    df[\"posting_id\"] = train[\"posting_id\"]\n    df[\"matches\"] = df[\"pred\"].apply(lambda x : \" \".join(x))\n    df[['posting_id','matches']].to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}