{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\n\n!cp ../input/rapids/rapids.0.18.0 /opt/conda/envs/rapids.tar.gz\n!cd /opt/conda/envs/ && tar -xzvf rapids.tar.gz > /dev/null\nsys.path = [\"/opt/conda/envs/rapids/lib/python3.7/site-packages\"] + sys.path\nsys.path = [\"/opt/conda/envs/rapids/lib/python3.7\"] + sys.path\nsys.path = [\"/opt/conda/envs/rapids/lib\"] + sys.path \n!cp /opt/conda/envs/rapids/lib/libxgboost.so /opt/conda/lib/\n\n!pip install ../input/shopee-inference-setup/textdistance-4.2.1-py3-none-any.whl\nsys.path.append(\"../input/timm-pytorch-image-models/pytorch-image-models-master\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nimport re\nimport sys\nimport string\nimport operator\nimport itertools\nfrom math import sqrt\nimport multiprocessing \nfrom pathlib import Path\nfrom functools import reduce, reduce\nfrom collections import Counter\nfrom joblib import Parallel, delayed\nfrom string import digits, ascii_letters\nfrom more_itertools import chunked\n\nimport numpy as np\nimport pandas as pd \n\nimport torch\n\nimport timm, textdistance\nfrom textdistance import Jaccard\n\nimport lightgbm\n\npd.options.display.max_colwidth = 1000\npd.options.display.max_rows = 1000","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"USE_GPU = True\nINFERENCE = True\nDEBUG = False\nPCT_RANK = False\nPOSTING_CHUNK_IDX = 1024 * 12\nTHRESHOLD = 0.8\nELIMINATE_SCORE = 0.5\nRANK = 3\n\ntest = pd.read_csv('../input/shopee-product-matching/test.csv')\n\nif len(test) > 3 or INFERENCE: \n    INFERENCE = True\n    config_mode = \"test\"\n    \nelse:\n    config_mode = \"train\"\n\nconfiguration = {\"train\": {\"image_dir\": \"train_images\",\n                           \"filename\": \"train.csv\"},\n                 \"test\": {\"image_dir\": \"test_images\",\n                          \"filename\": \"test.csv\"}}\n\ntest = pd.read_csv(f'../input/shopee-product-matching/{configuration[config_mode][\"filename\"]}')\nimage_paths = f'../input/shopee-product-matching/{configuration[config_mode][\"image_dir\"]}/' + test['image']\n\n\npost_mappings = dict(enumerate(test[\"posting_id\"].values))\ninverse_post_mappings = {val: key for key, val in post_mappings.items()}\ntest[\"posting_id\"] = test[\"posting_id\"].map(inverse_post_mappings)\n\n\nif not INFERENCE:\n    if DEBUG:\n        test = pd.concat([test, test])\n    test['target'] = test.label_group.map(test.groupby('label_group').posting_id.agg('unique').to_dict())\n\n                   \n\nimage_model_path = \"../input/shopee-inference-setup/efficientnet_b2_ra-bcdf34b7.pth\"\nlgbm_model_path = \"../input/shopee-inference-setup/lgbm_interaction_model_v5.bin\"\nlgbm_model = lightgbm.Booster(model_file=lgbm_model_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import make_scorer\nfrom contextlib import contextmanager\n\n@contextmanager\ndef timer(verbose, logger=None, format_str='{:.3f}[s]', prefix=None, suffix=None):\n    if prefix: format_str = str(prefix) + format_str\n    if suffix: format_str = format_str + str(suffix)\n    start = time.time()\n    yield\n    d = time.time() - start\n    out_str = format_str.format(d)\n    if verbose:\n        if logger:\n            logger.info(out_str)\n        else:\n            print(out_str)\n            \ndef get_word_frequency(df, title_col=\"unit_cleaned_text\", sep=\" \"):\n    return pd.DataFrame(dict(Counter(itertools.chain(*df[title_col].str.split(sep).values.tolist()))).items(), columns=[\"word\", \"frequency\"])\n\ndef get_cv_metric(col):\n    \n    def f1score(row):\n        n = len( np.intersect1d(row.target,row[col]) )\n        return 2*n / (len(row.target)+len(row[col]))\n    \n    return f1score\n\ndef get_character_ngrams(w, n):\n    if n > 1:\n        w = [\"<w>\"] + list(w) + [\"</w>\"]\n    else:\n        w = list(w)\n    return [\"\".join(w[i: i+n]) for i in range(len(w)-n+1)]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tokenizers\nfrom functools import partial\n\n\ndef lowercase_title(title: str):\n    return title.lower()\n\ndef remove_string_group(title: str,\n                        group):\n    return title.translate(str.maketrans(\"\", \"\", group))\n\n\nremove_punctuation = partial(remove_string_group,\n        group=string.punctuation)\n\nremove_digits = partial(remove_string_group,\n        group=string.digits)\n\nremove_ascii_letters = partial(remove_string_group,\n        group=string.ascii_letters)\n\ndef remove_multiple_whitespace(title: str):\n    return re.sub('\\s+',' ',title)\n\ndef get_alphanumerical_code_matching(title: str):\n    #matches = \",\".join(set(re.findall(\"[a-z]+\\d+\", title)))\n    matches = set(re.findall(\"[a-z]+\\d+\", title))\n    matches = \" \".join([remove_digits(match) + \" \" + remove_ascii_letters(match) for match in matches])\n    \n    return matches\n\ndef extract_letter_plus_numeric_substr(title_col: pd.Series):\n    return title_col.replace(regex=\"[a-z]+\\d+\", value='')\n\ndef extract_numeric_plus_letter_substr(title_col: pd.Series):\n    return title_col.replace(regex=\"\\d+[a-z]+\", value='')\n\ndef extract_measurement_matching(col: pd.Series): #clean_text\n    \n    unit_word_regex = r\"\\d+\\s+\\b(?:{unit_words})\\b\"\n\n    unit_match = col.apply(lambda x: \",\".join(set(re.findall(\"\\d+\\s*[a-z]+\\s*\", x))))\n    unit_match_mask = unit_match[unit_match != \"\"]\n    \n    units = set(\",\".join(unit_match_mask.apply(lambda x: remove_multiple_whitespace(x)).drop_duplicates().values).split(\",\"))\n    \n    repl = {unit: remove_ascii_letters(unit) + \" \" + remove_digits(unit) + \" \" for unit in units}\n    units = pd.Series(list(units)).apply(remove_digits).apply(remove_multiple_whitespace).str.replace(\" \", \"\").value_counts()\n    unit_words = \"|\".join([word for word in units[(units >= 3)].index.tolist() if len(word) <= 6])\n    \n\n    \n    unit_match = col.apply(lambda x: \" \".join(set(re.findall(unit_word_regex.format(unit_words=unit_words), x))))\n    \n    return repl, unit_match, unit_words\n\ndef extract_measurement_idenitifer(unit_col: pd.Series):\n    return unit_col.apply(remove_digits)\n    \ndef extract_numbers(title: str):\n    return \" \".join(re.findall(f\"\\d+\", title))\n\ndef remove_unit_measurements(col: pd.Series, unit_words):\n    return  col.replace(unit_word_regex.format(unit_words=unit_words), \"\", regex=True)\n\ndef get_ngrams(title: str, n=3):\n    return \" \".join(itertools.chain(*[get_character_ngrams(word, n) for word in title.split(\" \")]))\n\n\ndef train_wordpice_tokenizer(title_col: pd.Series,\n                             model_dir = \"models\",\n                             model_out_path = \"shopee_title\",\n                             vocab_size = 1000,\n                             min_frequency=2\n                             ):\n    \n    Path(model_dir).mkdir(exist_ok=True)\n\n    with open(model_out_path, \"w\") as title:\n        title.write(\" \".join(title_col.tolist()))\n        \n    tokenizer = tokenizers.BertWordPieceTokenizer()\n    tokenizer.train(model_out_path, vocab_size=vocab_size, min_frequency=min_frequency)\n    tokenizer.save_model(model_dir, model_out_path)\n    \n    return tokenizer\n\ndef encode_title_with_tokenizer(title: str,\n                                tokenizer):\n    return \" \".join(tokenizer.encode(title).tokens)\n\n\n#df_[\"number_match\"] = df_[\"clean_text\"].apply(lambda x: \" \".join(re.findall(f\"\\d+\", x)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import collections\n\nimport timm\nfrom tqdm import tqdm \n\nUSE_GPU = True\n\nif USE_GPU:\n    import cudf, cuml, cupy\n    from cuml.feature_extraction.text import TfidfVectorizer\n\ndef ngrams(words, n):\n    d = collections.deque(maxlen=n)\n    d.extend(words[:n])\n    words = words[n:]\n    ngram_ = []\n    for window, word in zip(itertools.cycle((d,)), words):\n        ngram_.append('-'.join(window))\n\n        d.append(word)\n        \n    return ngram_\n\ndef align_predictions_with_test_set(df_test, match, threshold=0.7):\n\n    match = pd.merge(df_test.loc[:, [\"posting_id\", \"target\"]],\n                         match,\n                         on=[\"posting_id\"],\n                         how=\"outer\")#.set_index(\"posting_id\")\n\n    is_not_matched = match[\"pred\"].isna()\n\n    match.loc[~is_not_matched, \"pred\"] = (match.loc[~is_not_matched, \"pred\"].apply(lambda x: \",\".join(x)) \\\n                                          + \",\" + match.loc[~is_not_matched, \"posting_id\"]).str.split(\",\")\n    match.loc[is_not_matched, \"pred\"] = match.loc[is_not_matched, \"posting_id\"].apply(lambda x: [x])\n    \n    return match","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import multiprocess","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport albumentations as A \nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom torch.utils.data import Dataset\n\nimport matplotlib.pyplot as plt\n\nfrom torch import nn\nfrom torch.hub import load_state_dict_from_url\n\nclass ShopeeDataset(Dataset):\n    def __init__(self, image_paths, transforms=None):\n\n        self.image_paths = image_paths\n        self.augmentations = transforms\n\n    def __len__(self):\n        return self.image_paths.shape[0]\n\n    def __getitem__(self, index):\n        image_path = self.image_paths[index]\n        \n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.augmentations:\n            augmented = self.augmentations(image=image)\n            image = augmented['image']       \n    \n        return image#,torch.tensor(1)\n    \ndef get_transformations():\n\n    return A.Compose(\n        [\n            A.Resize(288, 288,always_apply=True),\n            A.Normalize(),\n        ToTensorV2(p=1.0)\n        ]\n    )\n\ndef image_viz(image_path):\n    \"\"\"\n    Function for visualization.\n    Takes path to image as input.\n    \"\"\"\n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)    \n    plt.imshow(img)\n    plt.axis('off')\n    \n    \ndef extract_image_embeddings(image_paths,\n                             model_name = \"efficientnet_b2a\",\n                             model_path = image_model_path,\n                             global_pool = \"avg\",\n                             batch_size = 256,\n                             num_workers = 4,\n                             transform = None):\n    \n    embeddings = []\n    \n    dataset = ShopeeDataset(image_paths, transform)\n    dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    model = timm.create_model(model_name, pretrained=False, global_pool=\"avg\")\n    \n    model.to(dev)#.cuda()\n    #state_dict = load_state_dict_from_url('https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_b2_ra-bcdf34b7.pth')\n    #torch.save(state_dict, \"efficientnet_b2_ra-bcdf34b7.pth\")\n    model.load_state_dict(torch.load(model_path))\n    model.classifier = nn.Identity()\n    model.eval()\n    #model = timm.create_model(model_name, num_classes=0, global_pool=global_pool, pretrained=False).cuda()\n    \n    import multiprocess\n    \n    image_loader = torch.utils.data.DataLoader(\n                                        dataset,\n                                        batch_size=batch_size,\n                                        pin_memory=True,\n                                        drop_last=False,\n                                        num_workers=multiprocess.cpu_count()\n                                    )\n    \n    selu = torch.nn.SELU()\n    \n    with torch.no_grad():\n        for image in tqdm(image_loader): \n            feat = selu(model(image.cuda()))\n            image_embeddings = feat.detach().cpu().numpy()\n            embeddings.append(image_embeddings)\n\n    image_embeddings = cupy.array(np.concatenate(embeddings))\n    image_embeddings = (image_embeddings / cupy.linalg.norm(image_embeddings, ord=2, axis=1, keepdims=False).reshape(-1, 1))\n    \n    del embeddings, model, dataset\n\n\n    _ = gc.collect()\n    \n    \n    return image_embeddings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_df(df, substring_n=None, vocab_sizes=None):\n    df_ = df.copy()\n    df_[\"title_\"] = df_[\"title\"].apply(lowercase_title).apply(remove_punctuation).apply(remove_multiple_whitespace)\n    df_[\"title_\"] = extract_letter_plus_numeric_substr(df_[\"title_\"])\n    \n    \n    repl, df_[\"unit_seperated_title\"], unit_words = extract_measurement_matching(df_[\"title_\"])\n    df_[\"title_\"] = df_[\"title_\"].replace(repl, regex=True).apply(remove_multiple_whitespace)\n    df_[\"unit_seperated_title\"] = df_[\"title_\"].apply(lambda x: \",\".join(set(re.findall(r\"\\d+\\s+\\b(?:{unit_words})\\b\".format(unit_words=unit_words), x))))\n    \n    \n    \n    df_[\"matched_units\"] = extract_measurement_idenitifer(df_[\"unit_seperated_title\"]).apply(remove_multiple_whitespace)\n    #df_[\"title_\"] = extract_numeric_plus_letter_substr(extract_numeric_plus_letter_substr(df_[\"title_\"]))\n    \n    #df_[\"title_\"] = df_[\"title_\"].apply(lambda x: \" \".join(set(x.split(\" \"))))\n    df_[\"numeric_cleaned_title\"] = df_[\"title_\"].apply(remove_digits)\n    df_[\"numeric_text\"] = df_[\"title_\"].apply(extract_numbers).apply(remove_multiple_whitespace)\n    \n    if substring_n:\n        for n in substring_n:\n            df_[f\"subword_{n}_text\"] = df_[\"numeric_cleaned_title\"].apply(get_ngrams, n)\n        \n    if vocab_sizes:\n        for vocab_size in vocab_sizes:\n            tokenizer = train_wordpice_tokenizer(df_.numeric_cleaned_title, vocab_size=vocab_size, model_out_path=f\"shopee_{vocab_size}\")\n            df_[f\"vocab_{vocab_size}_text\"] = df_[\"numeric_cleaned_title\"].apply(partial(encode_title_with_tokenizer, tokenizer=tokenizer))\n            \n    df_[\"matched_units\"] = df_[\"matched_units\"].replace({\"gr\": \"g\", \"gram\": \"g\"}, regex=True)\n    df_[\"is_unit_available\"] = (df_[\"unit_seperated_title\"] != '')# * 1\n    df_[\"is_number_available\"] = (df_[\"numeric_text\"] != '')# * 1\n\n    df_[\"matched_units_measurement\"] = df_[\"matched_units\"].replace(\"\\s\", \"\", regex=True).apply(lambda x: \" \".join(set(x.split(\",\"))))\n    df_[\"matched_units_number\"] = df_[\"numeric_text\"].apply(lambda x: \" \".join(set(x.split(\" \"))))\n\n    df_[\"matched_codes\"] = df_[\"title\"].apply(lambda x: \" \".join(re.findall(\"[a-z]+\\d+\", x)))\n    df_[\"numeric_cleaned_title\"] = df_[\"numeric_cleaned_title\"].apply(remove_multiple_whitespace)\n\n    return df_\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nCOMPUTE_WORD_FREQUENCY = False\n\nidx = test.index\n\nDEBUG = False\nchunk = test.shape[0]\n\nif DEBUG: chunk = 1000\nsubword_list = [3]\ntokenization_list = None\n \nprint(\"Computing preprocessing ...\\n\")\ndf = preprocess_df(test, subword_list, tokenization_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nimport torch\nfrom cuml.neighbors import NearestNeighbors\nfrom cuml.feature_extraction.text import TfidfVectorizer\nmodel = TfidfVectorizer(stop_words='english', **{\"binary\": False,\n                                                \"lowercase\": False,\n                                                \"sublinear_tf\": False\n                                                #\"norm\": \"l2\",\n                                                })\ntext_embeddings = model.fit_transform(cudf.from_pandas(df.numeric_cleaned_title))\nprint('text embeddings shape is',text_embeddings.shape)\n\nKNN = min(test.shape[0], 75)\nmodel = NearestNeighbors(n_neighbors=KNN, metric=\"cosine\")\nmodel.fit(text_embeddings)\n\n\ndistances, indices = model.kneighbors(text_embeddings)\n\ndel model, text_embeddings\n_ = gc.collect()\n\ndistances = 1 - distances\n\ntitle_matches2 = cudf.DataFrame(distances.T.reshape(-1)).melt().reset_index()\ntitle_matches2[\"index\"] = title_matches2[\"index\"] % test.shape[0]\ntitle_matches2[\"variable\"] = indices.T.reshape(-1)\ntitle_matches2.columns = [\"posting_id\", \"pred\", \"score__title__text\"]\ntitle_matches2 = title_matches2.astype({\"posting_id\": \"uint16\",\n                                      \"pred\": \"uint16\"})\n\ndel distances, indices\n_ = gc.collect()\n\ntitle_matches2 = title_matches2.to_pandas()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nwith timer(True):\n    embeddings = extract_image_embeddings(image_paths, \n                                          transform = get_transformations(), \n                                          global_pool=\"avg\",\n                                          batch_size = 512)\n    torch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfrom cuml.neighbors import NearestNeighbors\n\nKNN = min(test.shape[0], 75)\nmodel = NearestNeighbors(n_neighbors=KNN, metric=\"cosine\")\nmodel.fit(embeddings[:chunk])\ndistances, indices = model.kneighbors(embeddings[:chunk])\n\ndel model, embeddings\n_ = gc.collect()\n\ndistances = 1 - distances\n\nimage_matches = cudf.DataFrame(distances.T.reshape(-1)).melt().reset_index()\nimage_matches[\"index\"] = image_matches[\"index\"] % chunk\nimage_matches[\"variable\"] = indices.T.reshape(-1)\nimage_matches.columns = [\"posting_id\", \"pred\", \"score__image\"]\nimage_matches = image_matches.astype({\"posting_id\": \"uint16\",\n                                      \"pred\": \"uint16\"})\n\ndel distances, indices\n_ = gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfrom cuml.feature_extraction.text import TfidfVectorizer\nmodel = TfidfVectorizer(stop_words='english', **{\"binary\": False,\n                                                \"lowercase\": False,\n                                                \"sublinear_tf\": True\n                                                #\"norm\": \"l2\",\n                                                })\ntext_embeddings = model.fit_transform(cudf.from_pandas(df.subword_3_text))\nprint('text embeddings shape is',text_embeddings.shape)\n\nKNN = min(test.shape[0], 75)\nmodel = NearestNeighbors(n_neighbors=KNN, metric=\"cosine\")\nmodel.fit(text_embeddings)\n\n\ndistances, indices = model.kneighbors(text_embeddings)\n\ndel model, text_embeddings\n_ = gc.collect()\n\ndistances = 1 - distances\n\nmatches = cudf.DataFrame(distances.T.reshape(-1)).melt().reset_index()\nmatches[\"index\"] = matches[\"index\"] % test.shape[0]\nmatches[\"variable\"] = indices.T.reshape(-1)\nmatches.columns = [\"posting_id\", \"pred\", \"score__subword3__text\"]\nmatches = matches.astype({\"posting_id\": \"uint16\",\n              \"pred\": \"uint16\"})\n\ndel distances, indices\n_ = gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nmatches = matches.to_pandas()\nmatches = pd.merge(matches,\n                   title_matches2,\n                   on=[\"posting_id\", \"pred\"],\n                   how=\"outer\")\n\ndel title_matches2\n\nimage_matches = image_matches.to_pandas()\nmatches = matches.merge(image_matches, on=[\"posting_id\", \"pred\"], how=\"outer\")\n\ndel image_matches","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matches[\"score__title__text\"] = matches.loc[:, [\"score__subword3__text\", \"score__title__text\"]].max(axis=1)\ndel matches[\"score__subword3__text\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nfreq_df = get_word_frequency(df, \"numeric_cleaned_title\")\nmappings = dict(zip(freq_df[\"word\"].astype(\"category\").cat.categories, range(0, freq_df.shape[0])))\nfreq_df[\"freq_ratio\"] = freq_df[\"frequency\"] / freq_df[\"frequency\"].sum()\nratio_map = dict(freq_df.loc[:, [\"word\", \"freq_ratio\"]].values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk import ngrams\n\ndef get_n_gramlist(text,n=2):\n    try:\n        nngramlist=[]\n        for s in ngrams(text.split(),n=n):        \n            nngramlist.append(s)                \n        return set(nngramlist)\n    except:\n        return set()\n\ndef jacc(x, y):\n    x = set(x.split(\" \"))\n    y = set(y.split(\" \"))\n    \n    return len(x), len(x.intersection(y)) / len(x.union(y))\n\ndef extract_valuable_info(x, y):\n    x = set(x.split(\" \"))\n    y = set(y.split(\" \"))\n\n    res = [len(x), len(x.intersection(y)) / len(x.union(y))]\n    \n    word_intersection = x.intersection(y)\n    \n    if len(word_intersection) > 0:\n        word_intersection = np.array([ratio_map[word] for word in word_intersection])\n    \n        return res + [word_intersection.min(), word_intersection.mean(), word_intersection.sum()]\n       \n    else:\n        return res + [np.nan, np.nan, np.nan]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_features(df):\n    \n    df_ = df.copy()\n\n    for group_col in [\"posting_id\", \"pred\"]:\n        for title in [\"subword3\", \"title\", \"image\"]:\n            if title == \"title\":\n                feat_col = f\"score__{title}__text\"\n            if title == \"subword3\":\n                feat_col = f\"score__{title}__text\"\n            if title == \"image\":   \n                feat_col = f\"score__{title}\"\n\n            features = {f\"cos_dist__{group_col}_{feat_col}_mean\": (feat_col, \"mean\"),\n                        f\"cos_dist__{group_col}_{feat_col}_std\": (feat_col, \"std\"),\n                        f\"cos_dist__{group_col}_{feat_col}_skew\": (feat_col, \"skew\"),\n                       f\"cos_dist__{group_col}_{feat_col}_ske\": (feat_col, \"skew\")}\n\n            df_ = df_.merge(df_.groupby(group_col).aggregate(**features).reset_index(),\n                                            on=group_col)\n            \n            rank_feat_col = f\"cos_dist__{group_col}_{feat_col}_rank_{{feat}}\"\n            \n    \n    return df_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"non_tr_cols = ['label',\n 'cos_dist__posting_id_score__title__text_quantile_75',\n 'cos_dist__pred_score__image_skew',\n 'cos_dist__pred_score__image_rank_image',\n 'cos_dist__posting_id_score__title__text_mean',\n 'cos_dist__pred_score__title__text_skew',\n 'cos_dist__pred_score__title__text_quantile_75',\n 'cos_dist__pred_score__title__text_mean',\n 'cos_dist__posting_id_score__subword3__text_rank_text']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grouped_features = []\n\nfor group_col in [\"posting_id\", \"pred\"]:\n    for title in [\"title\", \"image\"]:    \n        if title == \"title\":\n            feat_col = f\"score__{title}__text\"\n        \n        if title == \"image\":   \n            feat_col = f\"score__{title}\"\n\n        features = {f\"cos_dist__{group_col}_{feat_col}_mean\": (feat_col, \"mean\"),\n                    f\"cos_dist__{group_col}_{feat_col}_std\": (feat_col, \"std\"),\n                    f\"cos_dist__{group_col}_{feat_col}_skew\": (feat_col, \"skew\"),\n                    f\"cos_dist__{group_col}_{feat_col}_quantile_75\": (feat_col, lambda x: x.quantile(0.75))}\n        \n        features = {key:val for key, val in features.items() if key not in non_tr_cols}\n        \n        grouped_features.append((group_col, features))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_ordered_features(df):\n    \n    df_ = df.copy()\n    \n    for group_col in [\"posting_id\", \"pred\"]:\n        for title in [\"subword3\", \"image\"]:\n            if title == \"subword3\":\n                feat_col = f\"score__{title}__text\"\n            if title == \"image\":   \n                feat_col = f\"score__{title}\"\n            \n            rank_feat_col = f\"cos_dist__{group_col}_{feat_col}_rank_{{feat}}\"\n            \n            if title == \"image\":\n                if group_col == \"posting_id\":\n                    df_[rank_feat_col.format(feat=\"image\")] = df_.groupby([group_col])[feat_col].transform(lambda x: x.rank(ascending=False, pct=True))\n                \n                else:\n                    df_[rank_feat_col.format(feat=\"image\")] = df_.groupby([group_col])[feat_col].transform(lambda x: x.rank(ascending=False, pct=True))\n                    df_[rank_feat_col.format(feat=\"image\") + \"_pct\"] = df_.sort_values(rank_feat_col.format(feat=\"image\"), ascending=False).groupby([group_col])[feat_col].transform(lambda x: x.pct_change())\n            \n            if title == \"subword3\":\n                if group_col == \"posting_id\":\n                    df_[rank_feat_col.format(feat=\"text\")] = df_.groupby([group_col])[feat_col].transform(lambda x: x.rank(ascending=False, pct=True))\n                \n                else:\n                    df_[rank_feat_col.format(feat=\"text\")] = df_.groupby([group_col])[feat_col].transform(lambda x: x.rank(ascending=False, pct=True))\n                    df_[rank_feat_col.format(feat=\"text\") + \"_pct\"] = df_.sort_values(rank_feat_col.format(feat=\"text\"), ascending=False).groupby([group_col])[feat_col].transform(lambda x: x.pct_change())\n            \n\n    return df_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_match_text_features(df, \n                                metadata, \n                                threshold=0.925,\n                                inference=True):\n    \n    df_ = df.copy()\n    \n    feat_cols = [\"posting_id\",\n                 \"numeric_cleaned_title\",\n                 \"is_unit_available\",\n                 \"is_number_available\",\n                 \"matched_units_number\",\n                 \"matched_units_measurement\",\n                 \"matched_codes\"]\n\n    df_ = df_.rename(columns={\"posting_id\": \"posting_id_1\",\n                              \"pred\": \"posting_id_2\"})\n\n    df_ = df_.merge(metadata.loc[:, feat_cols],\n                 left_on=[\"posting_id_1\"],\n                 right_on=[\"posting_id\"],\n                 suffixes=(\"\", \"_1\"),\n                 how=\"left\")\\\n              .merge(metadata.loc[:, feat_cols],\n                     left_on=[\"posting_id_2\"],\n                     right_on=[\"posting_id\"],\n                     suffixes=(\"\", \"_2\"),\n                    how=\"left\").rename(columns={col: col + \"_1\" for col in feat_cols if col != 'posting_id'}).drop(columns=[\"posting_id\"])\n    \n    get_similarity_condition = lambda col: f\"{col}_1 != '' and {col}_2 != '' and score__subword3__text > 0.05\"\n    match_idx = ((df_[f\"numeric_cleaned_title_1\"] != '') & (df_[f\"numeric_cleaned_title_2\"] != '') & (df_[\"score__subword3__text\"] > 0.05))\n    match_idx = match_idx[match_idx == True].index\n    \n    \n    sim_cols = [\"matched_units_number\", \"matched_units_measurement\"]\n\n    jaccard = Jaccard(qval=None)\n\n    for col in sim_cols:\n        df_.loc[match_idx, [f\"{col}_1\", f\"{col}_2\"]] = df_.loc[match_idx, [f\"{col}_1\", f\"{col}_2\"]].fillna(\"\")\n        df_.loc[match_idx, f\"{col}__jaccard_similarity\"] = [jaccard(*keys) for idx, keys in df_.loc[match_idx, [f\"{col}_1\", f\"{col}_2\"]].iterrows()]\n        #df_.loc[match_idx, f\"both__{col}\"] = (df_.loc[match_idx, [f\"{col}_1\", f\"{col}_2\"]].all(axis=1) * 1)\n\n    _ = gc.collect()\n    \n    df_ = df_.astype({wd: \"uint8\" for wd in df_.columns[df_.columns.str.contains(\"|\".join([\"lt\", \"gt\"]))]})\n\n    df_.loc[match_idx, [\"numeric_cleaned_title_1\", \"numeric_cleaned_title_2\"]] = df_.loc[match_idx, [\"numeric_cleaned_title_1\", \"numeric_cleaned_title_2\"]].fillna(\"\")\n        \n    df_.loc[match_idx, \"ngram2__intersection\"] = [len(get_n_gramlist(key).intersection(get_n_gramlist(val))) \\\n                                       for idx, (key, val) in df_.loc[match_idx, [\"numeric_cleaned_title_1\", \"numeric_cleaned_title_2\"]].iterrows()]\n\n    df_.loc[match_idx, [\"tit__int\", \"jacc__intsc\", \"word__match__min\", \"word__match__mean\", \"word__match__sum\"]] = [extract_valuable_info(titles[0], titles[1]) \\\n                                                                                         for idx, (titles) in df_.loc[match_idx, \n                                                                                           [\"numeric_cleaned_title_1\", \"numeric_cleaned_title_2\"]].iterrows()]\n    \n    preds = df_.loc[:, [\"posting_id_1\", \"posting_id_2\"]]\n    preds = preds.loc[:, ~preds.columns.duplicated()]\n    \n    non_training_cols =  pd.Index([\"label\", \n                                   \"both__matched_units_number\",\n                                  ])\\\n                         .union(df_.columns[df_.columns.isin([col + f\"_{num}\" for num in [1,2] for col in sim_cols])])\n\n    cols = df_.columns[df_.columns.str.contains(\"__\")].difference(non_training_cols).union([\"posting_id_1\", \"posting_id_2\"])\n    df_.drop(columns=df_.columns[~df_.columns.isin(cols)], inplace=True)\n    df_ = df_.loc[:, ~df_.columns.duplicated()]\n    \n    \"\"\"col_order = ['both__matched_units_measurement',\n                 'cos_dist__posting_id_score__image_mean',\n                 'cos_dist__posting_id_score__image_rank_image',\n                 'cos_dist__posting_id_score__image_rank_image_pct',\n                 'cos_dist__posting_id_score__image_skew',\n                 'cos_dist__posting_id_score__image_quantile_75',\n                 'cos_dist__posting_id_score__image_std',\n                 'cos_dist__posting_id_score__subword3__text_mean',\n                 'cos_dist__posting_id_score__subword3__text_quantile_75',\n                 'cos_dist__posting_id_score__subword3__text_skew',\n                 'cos_dist__posting_id_score__subword3__text_std',\n                 'cos_dist__posting_id_score__title__text_mean',\n                 'cos_dist__posting_id_score__title__text_ske',\n                 'cos_dist__posting_id_score__title__text_skew',\n                 'cos_dist__posting_id_score__title__text_std',\n                 'cos_dist__pred_score__image_mean',\n                 'cos_dist__pred_score__image_rank_image',\n                 'cos_dist__pred_score__image_rank_image_pct',\n                 'cos_dist__pred_score__image_ske',\n                 'cos_dist__pred_score__image_skew',\n                 'cos_dist__pred_score__image_std',\n                 'cos_dist__pred_score__subword3__text_mean',\n                 'cos_dist__pred_score__subword3__text_ske',\n                 'cos_dist__pred_score__subword3__text_skew',\n                 'cos_dist__pred_score__subword3__text_std',\n                 'cos_dist__pred_score__title__text_mean',\n                 'cos_dist__pred_score__title__text_ske',\n                 'cos_dist__pred_score__title__text_skew',\n                 'cos_dist__pred_score__title__text_std',\n                 'jacc__intsc',\n                 'matched_units_measurement__jaccard_similarity',\n                 'matched_units_number__jaccard_similarity',\n                 'ngram2__intersection',\n                 'score__image',\n                 'score__subword3__text',\n                 'score__title__text',\n                 'tit__int',\n                 'word__match__mean',\n                 'word__match__min',\n                 'word__match__sum']\n    df_ = df_.loc[:, ~df_.columns.duplicated()]\n    \n    preds[\"score\"] = lgbm_model.predict(df_.loc[:, col_order].values)\n    preds = preds.query(f\"score > {threshold}\").loc[:, [\"posting_id_1\", \"posting_id_2\"]].iloc[:, [0, 1]].reset_index(drop=True)\"\"\"\n    \n    if not inference:\n        return df_\n    \n    col_order = ['cos_dist__posting_id_score__image_mean',\n       'cos_dist__posting_id_score__image_quantile_75',\n       'cos_dist__posting_id_score__image_rank_image',\n       'cos_dist__posting_id_score__image_skew',\n       'cos_dist__posting_id_score__image_std',\n       'cos_dist__posting_id_score__title__text_skew',\n       'cos_dist__posting_id_score__title__text_std',\n       'cos_dist__pred_score__image_mean',\n       'cos_dist__pred_score__image_quantile_75',\n       'cos_dist__pred_score__image_rank_image_pct',\n       'cos_dist__pred_score__image_std',\n       'cos_dist__pred_score__subword3__text_rank_text',\n       'cos_dist__pred_score__subword3__text_rank_text_pct',\n       'cos_dist__pred_score__title__text_std', 'jacc__intsc',\n       'matched_units_measurement__jaccard_similarity',\n       'matched_units_number__jaccard_similarity', 'ngram2__intersection',\n       'score__image', 'score__subword3__text', 'tit__int',\n       'word__match__mean', 'word__match__min', 'word__match__sum']\n    \n    preds[\"score\"] = lgbm_model.predict(df_.loc[:, col_order].values)\n    #preds = preds.query(f\"score > {threshold}\").loc[:, [\"posting_id_1\", \"posting_id_2\"]].iloc[:, [0, 1]].reset_index(drop=True)\n\n    return preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"posts = matches.groupby(\"posting_id\").aggregate(**{**grouped_features[0][1], **grouped_features[1][1]})#,  **grouped_features[2][1]})\npost_preds = matches.groupby(\"pred\").aggregate(**{**grouped_features[2][1], **grouped_features[3][1]})#,  **grouped_features[5][1]})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = gc.collect()\n\nmatches = matches.rename(columns={\"score__title__text\": \"score__subword3__text\"})\nmatches[\"chunk_idx\"] = (matches[\"posting_id\"] // POSTING_CHUNK_IDX).astype(\"uint8\")\nmatches = matches[((matches.loc[:, matches.columns.str.contains(\"score\")] > ELIMINATE_SCORE).any(axis=1))].reset_index(drop=True)#.groupby(\"label\").size()\nmatches = extract_ordered_features(matches)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"match_predictions = pd.DataFrame()\n\nfor chunk_idx in matches[\"chunk_idx\"].unique():\n    with timer(True, prefix=f\"Part - {chunk_idx} computing ... \"):\n        match_df = extract_match_text_features(matches.query(f\"chunk_idx == {chunk_idx}\").merge(posts.reset_index(),\n                                                                                      on=\"posting_id\", how=\"left\")\\\n                                                                               .merge(post_preds.reset_index(),\n                                                                                      on=\"pred\", how=\"left\"), \n                                               df, threshold=THRESHOLD, inference=True)\n        #match_df.to_parquet(f\"final_feats{chunk_idx}-t1.parquet\")\n        match_df[\"posting_id_1\"], match_df[\"posting_id_2\"] = match_df.posting_id_1.map(post_mappings), match_df.posting_id_2.map(post_mappings)\n        match_predictions = pd.concat([match_predictions, match_df])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#match_predictions_ = match_predictions\n#match_predictions = match_predictions_.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"self_match_groups = pd.DataFrame([match_predictions.posting_id_1.unique(),\n              match_predictions.posting_id_1.unique()], index=[\"posting_id_1\", \"posting_id_2\"]).T\nself_match_groups[\"score\"] = 1\n\nmatch_predictions = match_predictions[~(match_predictions[\"posting_id_1\"] == match_predictions[\"posting_id_2\"])].reset_index(drop=True)\"\"\"\n\n#match_idx = match_predictions[\"posting_id_1\"] != match_predictions[\"posting_id_2\"]\n\n#match_predictions[\"score_rank\"] = match_predictions.groupby([\"posting_id_1\"])[\"score\"].transform(lambda x: x.rank(ascending=False))\n\n#pred_matches = match_predictions[(match_predictions[\"score\"] > THRESHOLD)].groupby([\"posting_id_1\"])[\"posting_id_2\"].nunique()\n#non_match_groups = pred_matches[pred_matches == 1].index\n\n#match_predictions.loc[(match_predictions[\"posting_id_1\"].isin(non_match_groups)) & (match_predictions[\"score_rank\"] < RANK), \"score\"] = 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"self_match_groups = pd.DataFrame([match_predictions.posting_id_1.unique(),\n              match_predictions.posting_id_1.unique()], index=[\"posting_id_1\", \"posting_id_2\"]).T\nself_match_groups[\"score\"] = 1\n\nmatch_predictions = match_predictions[~(match_predictions[\"posting_id_1\"] == match_predictions[\"posting_id_2\"])].reset_index(drop=True)\n\n#print(match_predictions.shape)\n\n#print((match_predictions[\"score\"] > THRESHOLD).sum())\n\nif PCT_RANK:\n    match_predictions[\"rank_value\"] = match_predictions.groupby([\"posting_id_1\"])[\"score\"].transform(lambda x: (x > 0.825).sum())\n    match_predictions[\"rank_pct\"] = match_predictions.groupby([\"posting_id_1\"])[\"score\"].transform(lambda x: x.rank(ascending=False, pct=True))\n    non_match_idx = match_predictions.query(\"rank_value == 0 and score > 0.5 and score < 0.825\").index\n    match_predictions.loc[non_match_idx, \"score\"] = 1\n    \n    del match_predictions[\"rank_value\"]\n    del match_predictions[\"rank_pct\"]\n\n    #print((match_predictions[\"score\"] > THRESHOLD).sum())\nmatch_predictions = pd.concat([match_predictions, self_match_groups]).reset_index(drop=True)\n\n#print(match_predictions.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"match_predictions = match_predictions.loc[match_predictions[\"score\"] > THRESHOLD, [\"posting_id_1\", \"posting_id_2\"]]\nmatch_predictions = pd.concat([match_predictions, match_predictions.iloc[:, [1, 0]]]).drop_duplicates()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[\"posting_id\"] = test[\"posting_id\"].map(post_mappings)\n\nmatch_predictions = pd.DataFrame(match_predictions.groupby([\"posting_id_1\"])[\"posting_id_2\"].unique().to_dict().items(), \n                                 columns=[\"posting_id\", \"pred\"])\n\nmatch_predictions[\"pred\"] = match_predictions[\"pred\"].apply(lambda x: \" \".join(x))\nmatch_predictions.columns = ['posting_id', 'matches']\n\nmatch_predictions = test.loc[:, [\"posting_id\"]].merge(match_predictions, on=[\"posting_id\"], how=\"left\")\nmatch_predictions.loc[match_predictions[\"matches\"].isna(), \"matches\"] = match_predictions.loc[match_predictions[\"matches\"].isna(), \"posting_id\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not INFERENCE:\n    \n    test[\"target\"] = test[\"target\"].apply(lambda x: [post_mappings[target] for target in x])\n    \n    match_predictions = match_predictions.merge(test.loc[:, [\"posting_id\", \"target\"]], on=[\"posting_id\"], how=\"left\")\n    match_predictions.matches = match_predictions.matches.apply(lambda x: np.array(x.split(\" \")))\n    match_predictions.loc[:, 'f1'] = match_predictions.apply(get_cv_metric('matches'),axis=1).values\n    print(\"CV SCORE : {}\".format(match_predictions[\"f1\"].mean()))\n    \n    match_predictions = match_predictions.loc[:, [\"posting_id\", \"matches\"]]\n    match_predictions[\"matches\"] = match_predictions[\"matches\"].apply(lambda x: \" \".join(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"match_predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"match_predictions.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}