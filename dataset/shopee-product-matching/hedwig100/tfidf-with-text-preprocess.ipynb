{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# About This Notebook","metadata":{}},{"cell_type":"markdown","source":"## Preprocess\n\nIn this notebook, I tried tfidf with text preprocess. <br> \n\n- remove number \n- remove stopword \n- remove word which length is lower than 3 \n- use Indonesian to english translator [in this discussion](https://www.kaggle.com/c/shopee-product-matching/discussion/228358) \n\n## How to split CV \n\nMy cv is from [this notebook](https://www.kaggle.com/tmhrkt/shopee-cv-splitting-way). \n\n## CV score \nPreprocess worked for improving cv score. \n\n|fold|f1score in cv|\n|:--:|:--:|\n|0|0.7426|\n|1|0.73746|\n|2|0.74517|\n|3|0.72959|\n|4|0.74731| \n\n## LB score \n|th|lb|\n|:--:|:--:|\n|0.60||\n|0.65|0.602|\n|0.70|0.596|\n|0.75|0.585|\n|0.80|0.568|","metadata":{}},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"code","source":"DEBUG = False \nINPUT_DIR = \"../input/shopee-product-matching/\" \nOUT_DIR = \"./\"\n\nNB = \"11\"\nVERSION = 2\nN_BATCH = 10  \nTYPE = \"GPU\"\nSUBMIT = True \n\nclass CFG:\n    max_features = 25000\n    th = 0.60      \n    \n    remove_stopword = True\n    translate = True \n    stemmer = False \n    fold = -1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Library","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pickle,gc\nimport time \nimport cv2, matplotlib.pyplot as plt\n\nimport nltk\n\nif TYPE == \"GPU\":\n    import cudf, cuml, cupy\n    from cuml.feature_extraction.text import TfidfVectorizer\nelse:\n    from sklearn.feature_extraction.text import TfidfVectorizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"class Timer():\n    # Requrements\n    # import time\n    def __init__(self):\n        self.cnt = 0\n    def start(self):\n        self.start_time = time.time()\n        print(f\"Time{self.cnt} START \")\n    def stop(self):\n        s = int(time.time() - self.start_time)  \n        h = s//(3600) \n        s -= h*3600 \n        m = s//60 \n        s -= m*60\n        print(f\"Time{self.cnt} : {h}h {m}m {s}s\")  \n        self.cnt += 1\n\nclass Logger():\n    # Requirements\n    # import pickle\n    def __init__(self):\n        pass \n    def dump(self,obj,dir_name):\n        f = open(dir_name,\"wb\")\n        pickle.dump(obj,f)\n        f.close","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"if SUBMIT:\n    df = pd.read_csv(\"../input/shopee-product-matching/test.csv\")\nelse:\n    train = pd.read_csv(\"../input/shopee-fold/train_folds.csv\")\n    if CFG.fold < 0:\n        df = train\n    else:\n        df = train[train[\"fold\"] == CFG.fold].reset_index(drop=True)\n    tmp = df.groupby('label_group').posting_id.agg('unique').to_dict()\n    df['target'] = df.label_group.map(tmp)\n    \nif DEBUG:\n    df = df.sample(n = 5).reset_index(drop=True)\n    \nprint('df shape is', df.shape )\ndf.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Metric","metadata":{}},{"cell_type":"code","source":"def row_wise_f1_score(labels,preds):\n    scores = [] \n    for label,pred in zip(labels,preds):\n        n = len(np.intersect1d(label,pred))\n        score = 2*n/(len(label) + len(pred))\n        scores.append(score)\n    return scores,np.mean(scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess","metadata":{}},{"cell_type":"markdown","source":"## Stopword","metadata":{}},{"cell_type":"code","source":"# from nltk.corpus import stopwords\n# stopwords = stopwords.words(\"english\")\n# + \n# [ '!', '\"', '#', '$', '%', '&', '(', ')', '*', '+', '-', '.', '/',  '\\\\', ':', ';', '<', '=', '>','?', '@', '[', ']', '^', '_', '`', '{', '|', '}', '\\t','\\n',\"'\",\",\",'~' , '—']\n# + \n# [\"she's\", 'at', 'further', \"should've\", 'can', 'it', 'theirs', 'me', 'not', 'that', 'any', 'itself', 'did', 'such', 'as', 'x8f', 'both', 'having', 'under', \"shan't\", 'should', \"mustn't\", 'with', 'x87', 'whom', 'couldn', 'be', \"weren't\", 'myself', 'd', \"didn't\", 've', 'have', 'up', 'same', 'above', 'all', 'after', 'so', \"aren't\", 'his', 'of', 'between', 'll', 'what', 'those', \"wouldn't\", 'your', 'when', 'haven', \"won't\", 'below', 'her', 'she', 'until', 'why', 'its', 'down', 'the', 'here', 'where', 'has', 'own', 'wouldn', 'aren', \"don't\", 'doing', 'xb8', 'themselves', 'this', 'there', 'how', 's', 'don', 'ain', 'x9f', 'to', 'xc3', 'now', 't', 'again', 'is', \"you've\", \"doesn't\", \"wasn't\", 'during', 'x9d', 'xa2', \"isn't\", 'very', 'x90', 'through', 'from', 'ourselves', 'in', 'out', 'on', 'are', \"couldn't\", 'didn', 'an', 'ma', 'do', 'been', 'they', \"it's\", 'mightn', 'y', \"you're\", \"haven't\", 'each', 'because', \"hadn't\", 'other', 'their', 'my', 'off', \"needn't\", 'was', 'hers', 'some', 'weren', 'xa4', 'were', 'or', 'shan', 'hasn', 'a', 'he', 'no', 'over', 'xe2', 'xef', 'before', 'by', 'will', 'i', 'about', 'am', 'our', 'shouldn', 'just', 'xf0', 're', 'had', 'who', 'm', 'hadn', 'you', \"shouldn't\", 'won', 'while', 'yourselves', 'but', 'needn', 'against', 'too', \"you'll\", 'yours', 'being', 'does', 'xad', 'x80', \"that'll\", 'few', 'we', 'herself', 'him', 'then', 'himself', 'isn', 'than', \"mightn't\", 'for', 'once', 'them', 'these', 'more', 'and', 'doesn', 'mustn', 'most', \"hasn't\", 'xbf', 'wasn', \"you'd\", 'into', 'which', 'nor', 'if', 'ours', 'o', 'yourself', 'only', 'x89']\n# + \n# unit \n# + \n# single character \n\na = set(['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"])\nb = set([ '!', '\"', '$', '%', '&', '(', ')', '*', '+', '-', '.', '/',  '\\\\', ':', ';', '<', '=', '>','?', '@', '[', ']', '^', '_', '`', '{', '|', '}', '\\t','\\n',\"'\",\",\",'~' , '—'])\nc = set([\"she's\", 'at', 'further', \"should've\", 'can', 'it', 'theirs', 'me', 'not', 'that', 'any', 'itself', 'did', 'such', 'as', 'x8f', 'both', 'having', 'under', \"shan't\", 'should', \"mustn't\", 'with', 'x87', 'whom', 'couldn', 'be', \"weren't\", 'myself', 'd', \"didn't\", 've', 'have', 'up', 'same', 'above', 'all', 'after', 'so', \"aren't\", 'his', 'of', 'between', 'll', 'what', 'those', \"wouldn't\", 'your', 'when', 'haven', \"won't\", 'below', 'her', 'she', 'until', 'why', 'its', 'down', 'the', 'here', 'where', 'has', 'own', 'wouldn', 'aren', \"don't\", 'doing', 'xb8', 'themselves', 'this', 'there', 'how', 's', 'don', 'ain', 'x9f', 'to', 'xc3', 'now', 't', 'again', 'is', \"you've\", \"doesn't\", \"wasn't\", 'during', 'x9d', 'xa2', \"isn't\", 'very', 'x90', 'through', 'from', 'ourselves', 'in', 'out', 'on', 'are', \"couldn't\", 'didn', 'an', 'ma', 'do', 'been', 'they', \"it's\", 'mightn', 'y', \"you're\", \"haven't\", 'each', 'because', \"hadn't\", 'other', 'their', 'my', 'off', \"needn't\", 'was', 'hers', 'some', 'weren', 'xa4', 'were', 'or', 'shan', 'hasn', 'a', 'he', 'no', 'over', 'xe2', 'xef', 'before', 'by', 'will', 'i', 'about', 'am', 'our', 'shouldn', 'just', 'xf0', 're', 'had', 'who', 'm', 'hadn', 'you', \"shouldn't\", 'won', 'while', 'yourselves', 'but', 'needn', 'against', 'too', \"you'll\", 'yours', 'being', 'does', 'xad', 'x80', \"that'll\", 'few', 'we', 'herself', 'him', 'then', 'himself', 'isn', 'than', \"mightn't\", 'for', 'once', 'them', 'these', 'more', 'and', 'doesn', 'mustn', 'most', \"hasn't\", 'xbf', 'wasn', \"you'd\", 'into', 'which', 'nor', 'if', 'ours', 'o', 'yourself', 'only', 'x89'])\nunit = set([\"cm\",\"gr\",\"mm\",\"m\",\"kg\",\"ml\",\"g\",\"mg\",\"l\"]) \n\nremove_word = b\nstopwords = a|c \n\ndef remove_stopwords(text):\n    sentence = []\n    for word in text.split(): \n        word = word.lower()\n        if DEBUG:\n            print(word)\n        # remove 1 character \n        x = []\n        for c in word:\n            if c in remove_word:\n                continue\n            x.append(c)\n        word = \"\".join(x)\n        if word.isnumeric():\n            continue\n        elif word in stopwords:\n            continue\n        elif len(word) < 3:\n            continue\n        sentence.append(word)\n    return \" \".join(sentence)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## translate","metadata":{}},{"cell_type":"code","source":"dictionary = {\"wanita\": \"woman\", \"anak\": \"child\", \"bayi\": \"baby\", \"tas\": \"bag\", \"masker\": \"face mask\", \"pria\": \"men\", \"murah\": \"cheap\", \"tangan\": \"hand\", \"alat\": \"tool\", \"motif\": \"motive\", \"warna\": \"color\", \"bahan\": \"material\", \"celana\": \"pants\", \"baju\": \"clothes\", \"kaos\": \"t-shirt\", \"sepatu\": \"shoes\", \"rambut\": \"hair\", \"mainan\": \"toy\", \"sarung\": \"holster\", \"polos\": \"plain\", \"rak\": \"rack\", \"botol\": \"bottle\", \"sabun\": \"soap\", \"kain\": \"fabric\", \"panjang\": \"long\", \"kabel\": \"cable\", \"buku\": \"book\", \"plastik\": \"plastic\", \"mobil\": \"car\", \"hitam\": \"black\", \"karakter\": \"character\", \"putih\": \"white\", \"dompet\": \"purse\", \"kaki\": \"feet\", \"pembersih\": \"cleaners\", \"lipat\": \"folding\", \"silikon\": \"silicone\", \"minyak\": \"oil\", \"isi\": \"contents\", \"paket\": \"package\", \"susu\": \"milk\", \"gamis\": \"robe\", \"mandi\": \"bath\", \"madu\": \"honey\", \"kulit\": \"skin\", \"serbaguna\": \"multipurpose\", \"bisa\": \"can\", \"kacamata\": \"spectacles\", \"pendek\": \"short\", \"tali\": \"rope\", \"selempang\": \"sash\", \"topi\": \"hat\", \"obat\": \"drug\", \"gantungan\": \"hanger\", \"tahun\": \"year\", \"jilbab\": \"hijab\", \"dapur\": \"kitchen\", \"dinding\": \"wall\", \"kuas\": \"brush\", \"perempuan\": \"woman\", \"katun\": \"cotton\", \"sepeda\": \"bike\", \"lucu\": \"funny\", \"lengan\": \"arm\", \"kaca\": \"glass\", \"garansi\": \"warranty\", \"bunga\": \"flower\", \"handuk\": \"towel\", \"dewasa\": \"adult\", \"elektrik\": \"electric\", \"timbangan\": \"balance\", \"besar\": \"big\", \"bahan\": \"ingredient\", \"ransel\": \"backpack\", \"kertas\": \"paper\",'bahan' : 'ingredient', 'bisa' : 'can', 'rak' : 'rack', 'panjang' : 'long', 'untuk' : 'to', 'rambut' : 'hair', 'bayi' : 'baby', 'celana' : 'pants', 'isi' : 'contents', 'grosir' : 'wholesaler', 'tas' : 'bag', 'kaki' : 'feet', 'kaos' : 't-shirt', 'lampu' : 'light', 'tali' : 'rope', 'pria' : 'men', 'dan' : 'and', 'plastik' : 'plastic', 'baju' : 'clothes', 'putih' : 'white', 'alat' : 'tool', 'paket' : 'package', 'mobil' : 'car', 'gamis' : 'robe', 'tempat' : 'the place', 'anak' : 'child', 'warna' : 'color', 'dompet' : 'purse', 'wanita' : 'women', 'wajah' : 'face', 'termurah' : 'cheapest', 'mainan' : 'toy', 'sabun' : 'soap', 'dengan' : 'with', 'jilbab' : 'hijab', 'hitam' : 'black', 'tangan' : 'hand', 'karakter' : 'character', 'murah' : 'cheap', 'sarung' : 'scabbard', 'sepatu' : 'shoes', 'pendek' : 'short', 'botol' : 'bottle', 'kain' : 'fabric'}\ndef translate(text):\n    sentence = [] \n    for word in text.split():\n        word = word.lower()\n        if DEBUG:\n            print(word)\n        if word in dictionary:\n            sentence.append(dictionary[word]) \n        else:\n            sentence.append(word)\n    return \" \".join(sentence)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## stemmer","metadata":{}},{"cell_type":"code","source":"def lemmatize_stemming(text):\n    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## data cleansing","metadata":{}},{"cell_type":"code","source":"timer = Timer() \nlogger = Logger() \n\nif CFG.remove_stopword:\n    timer.start()\n    print(\"Remove stopword...\")\n    texts = df.title.values \n    for i in range(len(df)):\n        if DEBUG:\n            print(\"-\"*100)\n            print(f\"[{i}]\")\n        text = texts[i]\n        df.loc[i,\"title\"] = remove_stopwords(text)\n    timer.stop()\n        \nif CFG.translate:\n    timer.start()\n    print(\"Translate...\")\n    texts = df.title.values \n    for i in range(len(df)):\n        if DEBUG:\n            print(\"-\"*100)\n            print(f\"[{i}]\")\n        text = texts[i]\n        df.loc[i,\"title\"] = translate(text)\n    timer.stop() \n\nif CFG.stemmer:\n    timer.start()\n    print(\"Stemming...\")\n    texts = df.title.values \n    for i in range(len(df)):\n        if DEBUG:\n            print(\"-\"*100)\n            print(f\"[{i}]\")\n        text = texts[i]\n        df.loc[i,\"title\"] = lemmatize_stemming(text)\n    timer.stop() \n\nif not SUBMIT:\n    logger.dump(remove_word,f\"remove_word_nb{NB}_ver{VERSION}.pkl\")\n    logger.dump(stopwords,f\"stopword_nb{NB}_ver{VERSION}.pkl\")\n    logger.dump(dictionary,f\"dictionary_nb{NB}_ver{VERSION}.pkl\")\n    \ndf.title.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TFidf","metadata":{}},{"cell_type":"code","source":"if TYPE == \"GPU\":\n    df_gf = cudf.DataFrame(df) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Computing text embeddings...')\nif TYPE == \"GPU\":\n    model = TfidfVectorizer(stop_words='english', binary=True, max_features=CFG.max_features) \n    text_embeddings = model.fit_transform(df_gf.title).toarray()\nelse:\n    model = TfidfVectorizer(stop_words='english', binary=True, max_features=CFG.max_features) \n    text_embeddings = model.fit_transform(df.title).toarray()\nprint('text embeddings shape',text_embeddings.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = []\nn = df.shape[0] \nbs = n//N_BATCH \nprint('Finding similar titles...')\n\nfor i in range(N_BATCH):\n    left = bs*i\n    right = bs*(i + 1) \n    if i == N_BATCH - 1:\n        right = n \n    print('chunk',left,'to',right)\n    \n    # COSINE SIMILARITY DISTANCE\n    if TYPE == \"GPU\":\n        cts = cupy.matmul(text_embeddings[left:right], text_embeddings.T) \n    else:\n        cts = text_embeddings[left:right]@text_embeddings.T\n    \n    for k in range(right-left):\n        if TYPE == \"GPU\":\n            IDX = cupy.where(cts[k,]>CFG.th)[0]\n            o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n        else:\n            IDX = np.where(cts[k,]>CFG.th)[0]\n            o = df.iloc[IDX].posting_id.values\n        preds.append(o)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not SUBMIT:\n    logger.dump(text_embeddings,f\"embed_tfidf_nb{NB}_ver{VERSION}_fold{CFG.fold}.pkl\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del model, text_embeddings\n_ = gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['preds_list'] = preds\ndf['preds'] = df[\"preds_list\"].apply(lambda x:\" \".join(x))\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Compute CV","metadata":{}},{"cell_type":"code","source":"if not SUBMIT:\n    scores,score = row_wise_f1_score(df.target,preds) \n    df[\"score\"] = scores \n    print(f\"CV : f1score = {score}\")\nelse:\n    submission = pd.read_csv(\"../input/shopee-product-matching/sample_submission.csv\")\n    submission[\"matches\"] = df[\"preds\"]\n    submission.to_csv(\"submission.csv\",index=False)\n    print(submission.head())  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}