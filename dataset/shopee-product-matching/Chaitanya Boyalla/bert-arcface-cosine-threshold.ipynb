{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd \nimport gc\nfrom tqdm import tqdm\nimport tensorflow as tf\nfrom shutil import copyfile\ncopyfile(src = \"../input/bert-arcface-trained-weights/tokenization.py\", dst = \"../working/tokenization.py\")\nfrom tokenization import FullTokenizer\nimport tensorflow_hub as hub\nimport torch\ntorch.manual_seed(0)\ntorch.backends.cudnn.deterministic = False\ntorch.backends.cudnn.benchmark = True\nfrom sklearn.preprocessing import LabelEncoder\nimport re","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LIMIT = 2.0\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        tf.config.experimental.set_virtual_device_configuration(\n            gpus[0],\n            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*LIMIT)])\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n        #print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n    except RuntimeError as e:\n        print(e)\nprint('TensorFlow Limit {}GB'.format(LIMIT))\nprint('RAPIDS Limit {}GB'.format(16-LIMIT))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess(description):\n  # Actually not required as what we have is titles which usually doesn't contain words that we use for general communication.\n    description=description.lower()\n    description=re.sub('[-\\n\\t]+',' ',description)\n    description= re.sub(r\"won\\'t\", \"will not\",description)\n    description=re.sub(r\"can\\'t\", \"can not\",description)\n    description=re.sub(r\"n\\'t\", \" not\",description)\n    description=re.sub(r\"\\'re\", \" are\",description)\n    description=re.sub(r\"\\'s\", \" is\",description)\n    description=re.sub(r\"\\'d\", \" would\",description)\n    description=re.sub(r\"\\'ll\", \" will\",description)\n    description=re.sub(r\"\\'t\", \" not\",description)\n    description=re.sub(r\"\\'ve\", \" have\",description)\n    description=re.sub(r\"\\'m\", \" am\",description)\n    description=re.sub('[^a-z0-9]+',' ',description)\n    description=re.sub('\\s+',' ',description)\n    return description.strip()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test=pd.read_csv(r'../input/shopee-product-matching/test.csv')\nif len(test)<=3:\n    train=pd.read_csv(r'../input/shopee-product-matching/train.csv')\nelse:\n    train=pd.read_csv(r'../input/shopee-product-matching/test.csv')\n\ntrain.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del test\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clensed_train=[preprocess(title) for title in tqdm(train.title.values)]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_layer = hub.KerasLayer(\"../input/shopee-external-models/bert_en_uncased_L-24_H-1024_A-16_1\", trainable=True)\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n\ntokenizer=FullTokenizer(vocab_file, do_lower_case)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_seq_length=84\ntitle_tokens=[]\nfor i in tqdm(clensed_train):\n    tok=['[CLS]']+tokenizer.tokenize(i)\n    if len(tok)>=max_seq_length-1:\n        tok=tok[:max_seq_length-1]+['[SEP]']\n    else:\n        tok=tok+['[SEP]']\n        tok=tok+(['[PAD]']*(max_seq_length-len(tok)))\n    title_tokens.append(np.array(tokenizer.convert_tokens_to_ids(tok)))\ntitle_tokens=np.array(title_tokens)\n\ntitle_masks=np.array([np.array([1 if j!=0 else 0 for j in i]) for i in title_tokens])\n\n#As we are not using any sentence seperation in titles\ntitle_segments=np.zeros(title_masks.shape)\n\ntitle_tokens.shape, title_masks.shape, title_segments.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ArcFace(tf.keras.layers.Layer):\n    \n    # Implementation reference from https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/blob/master/src/modeling/metric_learning.py\n    \n    def __init__(self, n_classes, scale, margin, **kwargs):\n\n        super(ArcFace, self).__init__(**kwargs)\n\n        self.n_classes = n_classes\n        self.scale = scale\n        self.margin = margin\n        self.cos_m = tf.math.cos(margin)\n        self.sin_m = tf.math.sin(margin)\n\n    def get_config(self):\n\n        config = super().get_config().copy()\n        config.update({'n_classes': self.n_classes,'scale': self.scale,'margin': self.margin})\n        return config\n\n    def build(self, input_shape):\n        super(ArcFace, self).build(input_shape[0])\n\n        self.W = self.add_weight(\n            name='W',\n            shape=(int(input_shape[0][-1]), self.n_classes),\n            initializer='glorot_uniform',\n            dtype='float32',\n            trainable=True,\n            regularizer=None)\n\n    def call(self, inputs):\n        X, y = inputs\n        y = tf.cast(y, dtype=tf.int32)\n\n        # Normalizing vectors( Unit Vectors ) to make dot product depend only on angle between vectors.\n        cosine = tf.matmul(\n            tf.math.l2_normalize(X, axis=1),\n            tf.math.l2_normalize(self.W, axis=0)\n        )\n\n        # Sin(angle)^2 + Cos(angle)^2 = 1\n        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n        \n        # Cos(angle+margin)=Cos(angle)*Cos(margin)-Sin(angle)*Sin(margin)\n        phi = cosine * self.cos_m - sine * self.sin_m\n\n        # Add margin only when angle in greate than 90 degrees(Cos(90)=0)\n        phi = tf.where(cosine > 0, phi, cosine)\n\n        one_hot = tf.cast(tf.one_hot(y, depth=self.n_classes),dtype=cosine.dtype)\n    \n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        #\n        output *= self.scale\n        return output","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.backend.clear_session()\n\nmax_seq_length=84\n\ninput_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_word_ids\")\n\ninput_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_mask\")\n\nsegment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"segment_ids\")\n\nlabel = tf.keras.layers.Input(shape = (), name = 'label')\n\n#bert layer \npooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n\n# margin=0.35 radians= 20 degrees approx, scale=30\narc_face=ArcFace(11014,30,0.35, dtype='float32')([pooled_output, label])\n\nout=tf.keras.layers.Softmax(dtype='float32')(arc_face)\n\nBert_ArcFace_model=tf.keras.models.Model(inputs=[input_word_ids,input_mask,segment_ids,label], outputs=out)\nBert_ArcFace_model.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(Bert_ArcFace_model, show_shapes=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Bert_ArcFace_model.load_weights('../input/bert-arcface-trained-weights/BERT_ArcFace_epoch_26.hdf5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.backend.clear_session()\n\ninput_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_word_ids\")\ninput_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_mask\")\nsegment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"segment_ids\")\n\npooled_embed,seq_embed=Bert_ArcFace_model.layers[3]([input_word_ids, input_mask, segment_ids])\n\nbert_arcface_encoder=tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=[pooled_embed])\ntf.keras.utils.plot_model(bert_arcface_encoder, show_shapes=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del Bert_ArcFace_model\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_embeddings=[]\nchunksize=512\nchunks=len(train)//chunksize if len(train)%chunksize==0 else (len(train)//chunksize)+1\nfor chunk in tqdm(range(chunks)):\n    start=chunk*chunksize\n    end=min(len(train),start+chunksize)\n    embeddings=bert_arcface_encoder.predict([title_tokens[start:end], title_masks[start:end], title_segments[start:end]], batch_size=8)\n    bert_embeddings.append(embeddings)\nbert_embeddings=np.concatenate(np.array(bert_embeddings))\nbert_embeddings.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del bert_arcface_encoder\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_embeddings=np.array([i/np.linalg.norm(i) if np.linalg.norm(i)!=0 else i  for i in bert_embeddings])\nbert_embeddings.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedded_train=torch.from_numpy(bert_embeddings)\nembedded_train=embedded_train.cuda()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del bert_embeddings\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CosSim_thres=0.6\n\nmatches=[]\nchunksize=1024\nchunks=len(train)//chunksize if len(train)%chunksize==0 else (len(train)//chunksize)+1\nfor chunk in tqdm(range(chunks)):\n    start=chunk*chunksize\n    end=min(len(train),start+chunksize)\n    cossim=torch.matmul(embedded_train,embedded_train[start:end].T).T\n    cossim=cossim.data.cpu().numpy()\n    for per_posting in cossim:\n        indices=np.where(per_posting>=CosSim_thres)[0]\n        match=train.iloc[indices].posting_id.values\n        if len(match.tolist())>50:\n            ind=np.where(per_posting>=sorted(per_posting)[-50])[0]\n            match=train.iloc[ind].posting_id.values\n        matches.append(' '.join(match.tolist()))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['matches']=matches\ntrain.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission=train[['posting_id','matches']]\nsubmission.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv',index=False)","metadata":{},"execution_count":null,"outputs":[]}]}