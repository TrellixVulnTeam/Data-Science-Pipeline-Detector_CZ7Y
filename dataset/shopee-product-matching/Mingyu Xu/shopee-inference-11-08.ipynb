{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nfrom shutil import copyfile\n\ncopyfile(src = \"../input/shopee-utils/utils.py\", dst = \"../working/utils.py\")\nsys.path.append(\"../input/timm-pytorch-image-models/pytorch-image-models-master\")","metadata":{"execution":{"iopub.status.busy":"2021-11-12T06:53:28.613359Z","iopub.execute_input":"2021-11-12T06:53:28.613682Z","iopub.status.idle":"2021-11-12T06:53:28.643145Z","shell.execute_reply.started":"2021-11-12T06:53:28.613593Z","shell.execute_reply":"2021-11-12T06:53:28.642186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class config:\n    PATH = \"../input/shopee-product-matching/\"\n    \n    image_model_name = \"eca_nfnet_l0\"\n    image_model_path = \"../input/shopeemodel/eca_nfnet_l0_flexibleMargin_epoch_8.pt\"\n    text_model_name = \"distilbert-base-multilingual-cased\"\n    text_model_path = \"../input/shopeemodel/distilbert-base-multilingual-cased_epoch_6.pt\"\n    \n    n_classes = 9024\n    batch_size = 8","metadata":{"execution":{"iopub.status.busy":"2021-11-12T06:53:28.645187Z","iopub.execute_input":"2021-11-12T06:53:28.645548Z","iopub.status.idle":"2021-11-12T06:53:28.650377Z","shell.execute_reply.started":"2021-11-12T06:53:28.645509Z","shell.execute_reply":"2021-11-12T06:53:28.649372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torchvision import transforms\nfrom utils import ShopeeTrainDataset, ShopeeImageDataset, ShopeeTextTrainDataset, ShopeeTextDataset\nfrom utils import get_metric, validate\n\nfrom transformers import AutoTokenizer, AutoModel\nimport timm\n\nimport cudf\nimport cuml\n\nimport os\nfrom tqdm import tqdm\nimport math","metadata":{"execution":{"iopub.status.busy":"2021-11-12T06:53:28.651554Z","iopub.execute_input":"2021-11-12T06:53:28.652323Z","iopub.status.idle":"2021-11-12T06:53:39.927914Z","shell.execute_reply.started":"2021-11-12T06:53:28.652286Z","shell.execute_reply":"2021-11-12T06:53:39.927139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ArcFace(nn.Module):\n    \"\"\" NN module for projecting extracted embeddings onto the sphere surface \"\"\"\n    \n    def __init__(self, in_features, out_features, s=30, m=0.5):\n        super(ArcFace, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.s = s\n        self.m = m\n        self.cos_m = math.cos(self.m)\n        self.sin_m = math.sin(self.m)\n        self.arc_min = math.cos(math.pi - self.m)\n        self.margin_min = math.sin(math.pi - self.m) * self.m\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n    \n    def _update_margin(self, new_margin):\n        self.m = new_margin\n        self.cos_m = math.cos(self.m)\n        self.sin_m = math.sin(self.m)\n        self.arc_min = math.cos(math.pi - self.m)\n        self.margin_min = math.sin(math.pi - self.m) * self.m\n\n    def forward(self, embedding, label):\n        cos = F.linear(F.normalize(embedding), F.normalize(self.weight))\n        sin = torch.sqrt(1.0 - torch.pow(cos, 2)).clamp(0, 1)\n        phi = cos * self.cos_m - sin * self.sin_m\n        phi = torch.where(cos > self.arc_min, phi, cos - self.margin_min)\n\n        one_hot = torch.zeros(cos.size(), device=device)\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        logits = one_hot * phi + (1.0 - one_hot) * cos\n        logits *= self.s\n        return logits","metadata":{"execution":{"iopub.status.busy":"2021-11-12T06:53:39.929571Z","iopub.execute_input":"2021-11-12T06:53:39.929818Z","iopub.status.idle":"2021-11-12T06:53:39.943445Z","shell.execute_reply.started":"2021-11-12T06:53:39.92978Z","shell.execute_reply":"2021-11-12T06:53:39.942041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, model_name, n_classes, margin=0.5, fc_dim=1024):\n        super(Model, self).__init__()\n        print(\"Building Model Backbone for {} model\".format(model_name))\n        self.model_name = model_name\n        self.backbone = timm.create_model(model_name)\n        \n        if \"eca_nfnet\" in model_name:\n            feat_size = self.backbone.head.fc.in_features\n            self.backbone.head.fc = nn.Identity()\n            self.backbone.head.global_pool = nn.Identity()\n                \n        elif \"efficientnet\" in model_name:\n            feat_size = self.backbone.classifier.in_features\n            self.backbone.classifier = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n        \n        self.pooling =  nn.AdaptiveAvgPool2d(1)\n        self.dropout = nn.Dropout(p=0.1)\n        self.fc = nn.Linear(feat_size, fc_dim)\n        self.bn = nn.BatchNorm1d(fc_dim)\n        self.margin = ArcFace(fc_dim, n_classes, m=margin)\n        self._init_params()\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, x, labels=None):\n        batch_size = x.shape[0]\n        x = self.backbone(x)\n        x = self.pooling(x).view(batch_size, -1)\n        \n        x = self.dropout(x)\n        x = self.fc(x)\n        x = self.bn(x)\n        x = F.normalize(x,dim=1)\n        if labels is not None:\n            return self.margin(x,labels)\n        else:\n            return x","metadata":{"execution":{"iopub.status.busy":"2021-11-12T06:53:39.946594Z","iopub.execute_input":"2021-11-12T06:53:39.946884Z","iopub.status.idle":"2021-11-12T06:53:39.96032Z","shell.execute_reply.started":"2021-11-12T06:53:39.946803Z","shell.execute_reply":"2021-11-12T06:53:39.959559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TextModel(nn.Module):\n    def __init__(self, model_name, n_classes, margin=0.5, fc_dim=1024):\n        super(TextModel, self).__init__()\n        print(\"Building Model Backbone for {} model\".format(model_name))\n        self.model_name = model_name\n        self.tokenizer = AutoTokenizer.from_pretrained(\"../input/huggingface-bert-variants/{model_name}/{model_name}/\".format(model_name=model_name), TOKENIZERS_PARALLELISM=False)\n        self.backbone = AutoModel.from_pretrained(\"../input/huggingface-bert-variants/{model_name}/{model_name}/\".format(model_name=model_name))\n        self.feat_size = self.backbone.config.hidden_size\n\n        self.pooling =  nn.AdaptiveAvgPool2d(1)\n        self.dropout = nn.Dropout(p=0.1)\n        self.fc = nn.Linear(self.feat_size, fc_dim)\n        self.bn = nn.BatchNorm1d(fc_dim)\n        self.margin = ArcFace(fc_dim, n_classes, m=margin)\n        self._init_params()\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, text, labels=None):\n        inputs = self.tokenizer(text, max_length=100, truncation=True, padding=True, return_tensors=\"pt\")\n        output = self.backbone(input_ids = inputs[\"input_ids\"].to(device), attention_mask = inputs[\"attention_mask\"].to(device))\n        embedding = output[0][:, 0, :] \n        x = self.dropout(embedding)\n        x = self.fc(x)\n        x = self.bn(x)\n        x = F.normalize(x,dim=1)\n        if labels is not None:\n            return self.margin(x,labels)\n        else:\n            return x","metadata":{"execution":{"iopub.status.busy":"2021-11-12T06:53:39.962055Z","iopub.execute_input":"2021-11-12T06:53:39.962574Z","iopub.status.idle":"2021-11-12T06:53:39.97664Z","shell.execute_reply.started":"2021-11-12T06:53:39.962536Z","shell.execute_reply":"2021-11-12T06:53:39.97596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_dataset(name=\"train\"):\n    assert name in {\"train\", \"test\"}\n    df = pd.read_csv(config.PATH + '{}.csv'.format(name))\n    df[\"image_path\"] = config.PATH + '{}_images/'.format(name) + df['image']\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-11-12T06:53:39.978507Z","iopub.execute_input":"2021-11-12T06:53:39.978707Z","iopub.status.idle":"2021-11-12T06:53:39.989241Z","shell.execute_reply.started":"2021-11-12T06:53:39.97868Z","shell.execute_reply":"2021-11-12T06:53:39.988563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(config.PATH + 'test.csv')\nif len(test) > 3:\n    TRAIN = False\nelse:\n    TRAIN = True\n    \nTRAIN = False","metadata":{"execution":{"iopub.status.busy":"2021-11-12T06:53:39.991479Z","iopub.execute_input":"2021-11-12T06:53:39.992113Z","iopub.status.idle":"2021-11-12T06:53:40.008298Z","shell.execute_reply.started":"2021-11-12T06:53:39.992076Z","shell.execute_reply":"2021-11-12T06:53:40.007661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if TRAIN:\n    df = read_dataset(\"train\")\n    label_group_dict = df.groupby(\"label_group\").posting_id.agg(\"unique\").to_dict()\n    df['target'] = df.label_group.map(label_group_dict)\nelse:\n    df = read_dataset(\"test\")\n\nid_to_idx = df.reset_index().set_index(\"posting_id\")[\"index\"].to_dict()\nidx_to_id = df[\"posting_id\"].to_dict()\n    \nif torch.cuda.is_available():\n    df_cu = cudf.DataFrame(df) \n    \ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-12T06:53:40.011238Z","iopub.execute_input":"2021-11-12T06:53:40.011419Z","iopub.status.idle":"2021-11-12T06:53:48.006452Z","shell.execute_reply.started":"2021-11-12T06:53:40.011396Z","shell.execute_reply":"2021-11-12T06:53:48.005698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n])\n\ndf_dataset = ShopeeImageDataset(df, transform = transform)\ndf_dataloader = torch.utils.data.DataLoader(df_dataset, batch_size=16, shuffle=False, num_workers=2)\n\ndf_text_dataset = ShopeeTextDataset(df)\ndf_text_dataloader = torch.utils.data.DataLoader(df_text_dataset, batch_size=16, shuffle=False, num_workers=2)","metadata":{"execution":{"iopub.status.busy":"2021-11-12T06:53:48.008081Z","iopub.execute_input":"2021-11-12T06:53:48.008347Z","iopub.status.idle":"2021-11-12T06:53:48.014938Z","shell.execute_reply.started":"2021-11-12T06:53:48.008301Z","shell.execute_reply":"2021-11-12T06:53:48.013906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n\ndef get_model(model_name, model_path, n_classes):\n    model = Model(model_name, n_classes)\n    model.load_state_dict(torch.load(model_path, map_location=device))\n    return model.to(device)\n\ndef get_text_model(model_name, model_path, n_classes):\n    model = TextModel(model_name, n_classes)\n    model.load_state_dict(torch.load(model_path, map_location=device))\n    return model.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-11-12T06:53:48.016248Z","iopub.execute_input":"2021-11-12T06:53:48.017332Z","iopub.status.idle":"2021-11-12T06:53:48.026607Z","shell.execute_reply.started":"2021-11-12T06:53:48.017273Z","shell.execute_reply":"2021-11-12T06:53:48.025806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_image_feature(model, dataloader):\n    image_features = []\n    with torch.no_grad():\n        for images in tqdm(dataloader):\n            images = images.to(device)\n            features = model(images)\n            image_features.append(features)\n            del images\n    image_features = torch.cat(image_features, axis=0)\n\n    torch.cuda.empty_cache()   \n    return image_features\n\n\ndef get_tfidf_feature(df, max_features):\n    if torch.cuda.is_available():\n        from cuml.feature_extraction.text import TfidfVectorizer\n    else:\n        from sklearn.feature_extraction.text import TfidfVectorizer\n    \n    model = TfidfVectorizer(stop_words='english', max_features=max_features)\n    model.fit(df.title)\n\n    tfidf_features = model.transform(df.title).toarray()\n    tfidf_features = torch.Tensor(tfidf_features).to(device)\n    return tfidf_features\n\ndef get_bert_feature(model, dataloader):\n    text_features = []\n    with torch.no_grad():\n        for text in tqdm(dataloader):\n            text = list(text)\n            features = model(text)\n            text_features.append(features)\n            del text\n    text_features = torch.cat(text_features, axis=0)\n\n    torch.cuda.empty_cache()   \n    return text_features","metadata":{"execution":{"iopub.status.busy":"2021-11-12T06:53:48.028894Z","iopub.execute_input":"2021-11-12T06:53:48.030142Z","iopub.status.idle":"2021-11-12T06:53:48.040523Z","shell.execute_reply.started":"2021-11-12T06:53:48.030105Z","shell.execute_reply":"2021-11-12T06:53:48.03977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CHUNK = 1024\n\ndef cosine_dist(x,y):\n    m, n = x.size(0), y.size(0)\n    \n    norm_x = x.norm(p=2, dim=1, keepdim=True).expand(m,n)\n    norm_y = y.norm(p=2, dim=1, keepdim=True).expand(n,m).t()\n    dist = torch.matmul(x, y.t()) / (norm_x * norm_y)\n    return dist\n\ndef KNN(embeddings, k=50):\n    topK_index, topK_distance = list(), list()\n    \n    n = (embeddings.size(0) + CHUNK - 1) // CHUNK\n    with torch.no_grad():\n        for i in range(n):\n            a = i*CHUNK\n            b = min((i+1)*CHUNK, embeddings.size(0))\n            x = embeddings[a:b]\n            y = embeddings\n            chunk_distance = x @ y.T\n            topK = torch.topk(chunk_distance, k=min(k, embeddings.size(0)))\n            topK_idx, topK_dist = topK[1].detach().cpu().numpy(), topK[0].detach().cpu().numpy() # size: [chunk, k]\n                \n            topK_index.append(topK_idx)\n            topK_distance.append(topK_dist)\n    \n    topK_index = np.concatenate(topK_index, axis=0)\n    topK_distance = np.concatenate(topK_distance, axis=0)\n    \n    return topK_index, topK_distance\n\ndef DistancePredict(embeddings, df, threshold=0.9, least_threshold=0.5, k=50):\n    predict = []\n\n    n = (embeddings.size(0) + CHUNK - 1) // CHUNK\n    with torch.no_grad():\n        for i in range(n):\n            a = i*CHUNK\n            b = min((i+1)*CHUNK, embeddings.size(0))\n            x = embeddings[a:b]\n            y = embeddings\n            chunk_distance = cosine_dist(x,y)\n            topK = torch.topk(chunk_distance, k=min(k, embeddings.size(0)))\n            topK_idx, topK_dist = topK[1].detach().cpu().numpy(), topK[0].detach().cpu().numpy() # size: [chunk, k]\n\n            for j, (idx, dist) in enumerate(zip(topK_idx, topK_dist)):\n                mask = dist >= threshold\n                # release threshold if match < 2\n                if not mask[1]:\n                    mask[1] = True if dist[1] >= least_threshold else False\n                target_index = idx[mask]\n                pred = df.iloc[target_index].posting_id.to_numpy()\n                predict.append(pred)\n            \n    return predict\n\ndef QueryExpansion(query_embeddings, embeddings, alpha=2, k=3):\n    qe_embeddings = []\n\n    n = (query_embeddings.size(0) + CHUNK - 1) // CHUNK\n    with torch.no_grad():\n        for i in range(n):\n            a = i*CHUNK\n            b = min((i+1)*CHUNK, embeddings.size(0))\n            \n            dist = cosine_dist(query_embeddings[a:b], embeddings)\n            topK = torch.topk(dist, k=min(k, embeddings.size(0)))\n            topK_idx, topK_dist = topK[1], topK[0] # size: [CHUNK, k]\n\n            coef = topK_dist ** alpha\n            qe_embedding = (embeddings[topK_idx] * coef.unsqueeze(-1)).sum(-2)\n            qe_embedding = F.normalize(qe_embedding, dim=-1)\n\n            qe_embeddings.append(qe_embedding)\n\n            del dist, topK, topK_idx, topK_dist, coef\n            torch.cuda.empty_cache()    \n            \n    qe_embeddings = torch.cat(qe_embeddings, dim=0)\n\n    return qe_embeddings","metadata":{"execution":{"iopub.status.busy":"2021-11-12T06:53:48.041798Z","iopub.execute_input":"2021-11-12T06:53:48.042592Z","iopub.status.idle":"2021-11-12T06:53:48.066124Z","shell.execute_reply.started":"2021-11-12T06:53:48.042538Z","shell.execute_reply":"2021-11-12T06:53:48.065341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"IMAGE","metadata":{}},{"cell_type":"code","source":"IMAGE_THRESHOLD = 0.85\nIMAGE_LEAST_THRESHOLD = 0.6\n\nmodel = get_model(config.image_model_name, config.image_model_path, config.n_classes)\nmodel.eval()\nimage_features = get_image_feature(model, df_dataloader)\nimage_features_aug = QueryExpansion(image_features, image_features, k=2)\n\nimage_pred = DistancePredict(image_features_aug, df, threshold=IMAGE_THRESHOLD, least_threshold=IMAGE_LEAST_THRESHOLD)\ndf[\"image_pred\"] = image_pred\n\ndel model, image_features_aug\ntorch.cuda.empty_cache()\n\nif TRAIN:\n    f1, prec, rec = get_metric(df[\"target\"], df[\"image_pred\"])\n    print(\"Mean F1: {:f}\".format(f1))\n    print(\"Mean Precision: {:f}\".format(prec))\n    print(\"Mean Recall: {:f}\".format(rec))","metadata":{"execution":{"iopub.status.busy":"2021-11-12T07:02:12.960293Z","iopub.execute_input":"2021-11-12T07:02:12.960553Z","iopub.status.idle":"2021-11-12T07:02:13.909115Z","shell.execute_reply.started":"2021-11-12T07:02:12.960526Z","shell.execute_reply":"2021-11-12T07:02:13.908257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BERT_THRESHOLD = 0.85\nBERT_LEAST_THRESHOLD = 0.65\n\ntext_model = get_text_model(config.text_model_name, config.text_model_path, config.n_classes)\ntext_model.eval()\nbert_features = get_bert_feature(text_model, df_text_dataloader)\nbert_features_aug = QueryExpansion(bert_features, bert_features, k=2)\n\nbert_pred = DistancePredict(bert_features_aug, df, threshold=BERT_THRESHOLD, least_threshold=BERT_LEAST_THRESHOLD)\ndf[\"bert_pred\"] = bert_pred\n\ndel text_model, bert_features_aug\ntorch.cuda.empty_cache()\n\nif TRAIN:\n    f1, prec, rec = get_metric(df[\"target\"], df[\"bert_pred\"])\n    print(\"Mean F1: {:f}\".format(f1))\n    print(\"Mean Precision: {:f}\".format(prec))\n    print(\"Mean Recall: {:f}\".format(rec))","metadata":{"execution":{"iopub.status.busy":"2021-11-12T07:02:13.911225Z","iopub.execute_input":"2021-11-12T07:02:13.911661Z","iopub.status.idle":"2021-11-12T07:02:17.162746Z","shell.execute_reply.started":"2021-11-12T07:02:13.911621Z","shell.execute_reply":"2021-11-12T07:02:17.161924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TFIDF_FEATURES = 25000\n# TFIDF_THRESHOLD = 0.7\n\n# if torch.cuda.is_available():\n#     tfidf_features = get_tfidf_feature(df_cu, TFIDF_FEATURES)\n# else:\n#     tfidf_features = get_tfidf_feature(df, TFIDF_FEATURES)\n\n# tfidf_pred = DistancePredict(df, tfidf_features, threshold = TFIDF_THRESHOLD, distance_type=\"cosine\")\n# df[\"tfidf_pred\"] = tfidf_pred\n\n# if TRAIN:\n#     f1, prec, rec = get_metric(df[\"target\"], df[\"tfidf_pred\"])\n#     print(\"Mean F1: {:f}\".format(f1))\n#     print(\"Mean Precision: {:f}\".format(prec))\n#     print(\"Mean Recall: {:f}\".format(rec))","metadata":{"execution":{"iopub.status.busy":"2021-11-12T07:02:17.164413Z","iopub.execute_input":"2021-11-12T07:02:17.165021Z","iopub.status.idle":"2021-11-12T07:02:17.169155Z","shell.execute_reply.started":"2021-11-12T07:02:17.164974Z","shell.execute_reply":"2021-11-12T07:02:17.168449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CONCAT_THRESHOLD = 0.65\nCONCAT_LEAST_THRESHOLD = 0.5\n\nconcat_features = torch.cat([image_features, bert_features], axis=1)\nconcat_features_aug = QueryExpansion(concat_features, concat_features, k=2)\n\n\nconcat_pred = DistancePredict(concat_features_aug, df, threshold=CONCAT_THRESHOLD, least_threshold=CONCAT_LEAST_THRESHOLD)\ndf[\"concat_pred\"] = concat_pred\n\ndel image_features, bert_features, concat_features, concat_features_aug\ntorch.cuda.empty_cache()\n\nif TRAIN:\n    f1, prec, rec = get_metric(df[\"target\"], df[\"concat_pred\"])\n    print(\"Mean F1: {:f}\".format(f1))\n    print(\"Mean Precision: {:f}\".format(prec))\n    print(\"Mean Recall: {:f}\".format(rec))","metadata":{"execution":{"iopub.status.busy":"2021-11-12T07:02:17.171831Z","iopub.execute_input":"2021-11-12T07:02:17.172684Z","iopub.status.idle":"2021-11-12T07:02:17.183729Z","shell.execute_reply.started":"2021-11-12T07:02:17.172648Z","shell.execute_reply":"2021-11-12T07:02:17.182973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# candidate_pairs = []\n\n# knn_idx, knn_dist = KNN(image_features, k=50)\n# df[\"img_recall\"] = list(knn_idx)\n\n# knn_idx, knn_dist = KNN(bert_features, k=50)\n# df[\"bert_recall\"] = list(knn_idx)\n\n# knn_idx, knn_dist = KNN(concat_features, k=50)\n# df[\"concat_recall\"] = list(knn_idx)\n\n# def union(x,y):\n#     return np.union1d(x,y)\n\n# def intersect(x,y):\n#     return np.intersect1d(x,y)\n\n# df[\"recall\"] = df.apply(lambda row: union(row['img_recall'], row[\"bert_recall\"]),axis=1)\n# df[\"recall\"] = df.apply(lambda row: union(row['recall'], row[\"concat_recall\"]),axis=1)\n\n\n# for i, row in df.iterrows():\n#     for j in row.recall:\n#         candidate_pairs.append({\"subject\": i, \"object\": j})\n\n\n# del knn_idx, knn_dist\n# torch.cuda.empty_cache()\n\n# candidate_pairs = pd.DataFrame.from_records(candidate_pairs)\n# candidate_pairs.drop_duplicates(inplace=True)\n# candidate_pairs.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-12T07:02:17.185573Z","iopub.execute_input":"2021-11-12T07:02:17.185782Z","iopub.status.idle":"2021-11-12T07:02:17.19435Z","shell.execute_reply.started":"2021-11-12T07:02:17.185758Z","shell.execute_reply":"2021-11-12T07:02:17.193596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def get_anchor_feature(embeddings, feat_type = \"median\"):\n#     features = []\n    \n#     n = (embeddings.size(0) + CHUNK - 1) // CHUNK\n#     with torch.no_grad():\n#         for i in range(n):\n#             a = i*CHUNK\n#             b = min((i+1)*CHUNK, embeddings.size(0))\n#             x = embeddings[a:b]\n#             y = embeddings\n#             chunk_distance = cosine_dist(x, y)\n#             if feat_type == \"median\":\n#                 chunk_feat = chunk_distance.median(dim=1).values\n#             elif feat_type == \"mean\":\n#                 chunk_feat = chunk_distance.mean(dim=1)\n#             elif feat_type == \"max\":\n#                 chunk_feat = chunk_distance.max(dim=1).values\n#             elif feat_type == \"min\":\n#                 chunk_feat = chunk_distance.min(dim=1).values\n                \n#             features.append(chunk_feat)        \n\n#     features = torch.cat(features, dim=0)\n    \n#     return features.detach().cpu().numpy()\n\n# from torch.nn import CosineSimilarity\n# cos = CosineSimilarity(dim=-1)\n\n# def get_dist(embeddings, df):\n#     distance = []\n    \n#     n = (len(df) + CHUNK - 1) // CHUNK\n#     with torch.no_grad():\n#         for i in range(n):\n#             a = i*CHUNK\n#             b = min((i+1) * CHUNK, len(df))\n#             sub_idx = df.iloc[a:b].subject.to_numpy()\n#             obj_idx = df.iloc[a:b].object.to_numpy()\n#             dist = cos(embeddings[sub_idx], embeddings[obj_idx]).detach().cpu().numpy()\n#             distance.append(dist)\n            \n#     distance = np.concatenate(distance, axis=0)\n    \n#     return distance","metadata":{"execution":{"iopub.status.busy":"2021-11-12T07:02:17.195657Z","iopub.execute_input":"2021-11-12T07:02:17.196084Z","iopub.status.idle":"2021-11-12T07:02:17.204466Z","shell.execute_reply.started":"2021-11-12T07:02:17.196048Z","shell.execute_reply":"2021-11-12T07:02:17.20371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# candidate_pairs[\"img_dist\"] = get_dist(image_features, candidate_pairs)\n# candidate_pairs[\"bert_dist\"] = get_dist(bert_features, candidate_pairs)\n# candidate_pairs[\"concat_dist\"] = get_dist(concat_features, candidate_pairs)\n\n# print(\"done 0\")\n\n# for feat_type in [\"median\", \"mean\", \"max\", \"min\"]:\n#     img_dist_type = get_anchor_feature(image_features, feat_type = feat_type)\n#     bert_dist_type = get_anchor_feature(bert_features, feat_type = feat_type)\n#     concat_dist_type = get_anchor_feature(concat_features, feat_type = feat_type)\n    \n#     candidate_pairs[\"img_dist_{}\".format(feat_type)] = candidate_pairs.subject.apply(lambda x: img_dist_type[x])\n#     candidate_pairs[\"bert_dist_{}\".format(feat_type)] = candidate_pairs.subject.apply(lambda x: bert_dist_type[x])\n#     candidate_pairs[\"concat_dist_{}\".format(feat_type)] = candidate_pairs.subject.apply(lambda x: concat_dist_type[x])\n    \n# print(\"done 1\")\n    \n# for feat in [\"img\", \"bert\", \"concat\"]:\n#     candidate_pairs[\"{}_dist_max_percent\".format(feat)] = candidate_pairs[\"{}_dist\".format(feat)] / candidate_pairs[\"{}_dist_max\".format(feat)]\n#     candidate_pairs[\"{}_dist_min_percent\".format(feat)] = candidate_pairs[\"{}_dist\".format(feat)] / candidate_pairs[\"{}_dist_min\".format(feat)]\n\n# candidate_pairs.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-12T07:02:17.20666Z","iopub.execute_input":"2021-11-12T07:02:17.208781Z","iopub.status.idle":"2021-11-12T07:02:17.218595Z","shell.execute_reply.started":"2021-11-12T07:02:17.208753Z","shell.execute_reply":"2021-11-12T07:02:17.217875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import xgboost as xgb\n\n# xgb_model = xgb.XGBClassifier()\n# xgb_model.load_model(\"../input/shopeemodel/xgb(2).json\")\n\n# feature = candidate_pairs.iloc[:, 2:]\n\n# pair_predict = xgb_model.predict_proba(feature)\n# candidate_pairs[\"predict_proba\"] = pair_predict[:, 1]\n# candidate_pairs.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-12T07:02:17.219987Z","iopub.execute_input":"2021-11-12T07:02:17.220697Z","iopub.status.idle":"2021-11-12T07:02:17.23015Z","shell.execute_reply.started":"2021-11-12T07:02:17.220663Z","shell.execute_reply":"2021-11-12T07:02:17.229378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pred_df = candidate_pairs[[\"subject\", \"object\",\"predict_proba\"]]\n# pred_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-12T07:02:17.231582Z","iopub.execute_input":"2021-11-12T07:02:17.231852Z","iopub.status.idle":"2021-11-12T07:02:17.239667Z","shell.execute_reply.started":"2021-11-12T07:02:17.231808Z","shell.execute_reply":"2021-11-12T07:02:17.238728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# THRESHOLD = 0.8\n\n# final_df = []\n\n# pred_df = pred_df[pred_df.predict_proba >= THRESHOLD]\n\n# for i in range(len(df)):\n#     matchs = []\n#     for _, row in pred_df[pred_df.subject == i].iterrows():\n#         matchs.append(idx_to_id[np.int64(row.object)])\n#     final_df.append({\"posting_id\": idx_to_id[i], \"pred\": matchs})\n\n# final_df = pd.DataFrame.from_records(final_df)\n\n# if TRAIN:\n#     f1, prec, rec = get_metric(df[\"target\"], final_df[\"pred\"])\n#     print(\"Mean F1: {:f}\".format(f1))\n#     print(\"Mean Precision: {:f}\".format(prec))\n#     print(\"Mean Recall: {:f}\".format(rec))","metadata":{"execution":{"iopub.status.busy":"2021-11-12T07:02:17.243559Z","iopub.execute_input":"2021-11-12T07:02:17.24388Z","iopub.status.idle":"2021-11-12T07:02:17.249852Z","shell.execute_reply.started":"2021-11-12T07:02:17.243832Z","shell.execute_reply":"2021-11-12T07:02:17.249186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def submission(row):\n#     return ' '.join(row)\n\n# final_df[\"matches\"] = final_df[\"pred\"].apply(lambda x: submission(x))\n\n# # submit\n# final_df[['posting_id','matches']].to_csv('submission.csv',index=False)\n# submission = pd.read_csv('submission.csv')\n# submission.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-12T07:02:17.251353Z","iopub.execute_input":"2021-11-12T07:02:17.251618Z","iopub.status.idle":"2021-11-12T07:02:17.262175Z","shell.execute_reply.started":"2021-11-12T07:02:17.251586Z","shell.execute_reply":"2021-11-12T07:02:17.261407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def union(x,y):\n#     return np.union1d(x,y)\n\n# def intersect(x,y):\n#     return np.intersect1d(x,y)\n\n# df[\"pred\"] = df.apply(lambda row: union(row['image_pred'], row[\"bert_pred\"]),axis=1)\n# df[\"pred\"] = df.apply(lambda row: union(row['pred'], row[\"concat_pred\"]),axis=1)\n\n# #df[\"wait\"] = df.apply(lambda row: intersect(row['image_pred_wait'], row[\"text_pred_wait\"]),axis=1)\n# #df[\"pred\"] = df.apply(lambda row: union(row['pred'], row[\"wait\"]),axis=1)\n\n# if TRAIN:\n#     f1, prec, rec = get_metric(df[\"target\"], df[\"pred\"])\n#     print(\"Mean F1: {:f}\".format(f1))\n#     print(\"Mean Precision: {:f}\".format(prec))\n#     print(\"Mean Recall: {:f}\".format(rec))","metadata":{"execution":{"iopub.status.busy":"2021-11-12T07:02:17.263565Z","iopub.execute_input":"2021-11-12T07:02:17.263838Z","iopub.status.idle":"2021-11-12T07:02:17.270885Z","shell.execute_reply.started":"2021-11-12T07:02:17.263795Z","shell.execute_reply":"2021-11-12T07:02:17.27018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def union(x,y):\n    return np.union1d(x,y)\n\ndef intersect(x,y):\n    return np.intersect1d(x,y)\n\ndef submission(row):\n    return ' '.join(row)\n\ndf[\"pred\"] = df.apply(lambda row: union(row['image_pred'], row[\"bert_pred\"]),axis=1)\ndf[\"pred\"] = df.apply(lambda row: union(row['pred'], row[\"concat_pred\"]),axis=1)\n\ndf[\"matches\"] = df[\"pred\"].apply(lambda x: submission(x))\n# submit\ndf[['posting_id','matches']].to_csv('submission.csv',index=False)\nsubmission = pd.read_csv('submission.csv')\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-12T07:02:17.272269Z","iopub.execute_input":"2021-11-12T07:02:17.272608Z","iopub.status.idle":"2021-11-12T07:02:17.298942Z","shell.execute_reply.started":"2021-11-12T07:02:17.272573Z","shell.execute_reply":"2021-11-12T07:02:17.298225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}