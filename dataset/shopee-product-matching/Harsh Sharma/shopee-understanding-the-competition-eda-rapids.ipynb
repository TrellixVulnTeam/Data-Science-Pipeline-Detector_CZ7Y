{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center><h1>Shopee - Price Match Guarantee</h1></center>","metadata":{}},{"cell_type":"markdown","source":"<h2>Let’s understand the problem first-</h2>\n\n\nShopee is an e-commerce platform which provides ‘Lowest Price Guaranteed’ feature to thousands of products listed. To ensure lowest price Shopee must find the duplicate/similar items listed in other retailer’s websites. To perform these matches automatically we have to a ML algorithm which can cluster similar items irrespective of different images, titles, description etc. It is given that we can find at most 49 similar products to a given product.\n","metadata":{}},{"cell_type":"markdown","source":"<h2>Let’s Understand the DATA now-</h2>\n\n\nThe training data provided has following features - posting_id, image name, image, image_phash (perceptual hash of the image), the title of the image and image group. Image group is basically ID code for all postings that map to the same product. \nThe test data has -  3 samples but the model will be evaluated on more samples (about 70K images) privately when submitted. The submission file should consist of 2 rows:\n\n\n•posting_id: The Posting Id of the image (taken from the test file)\n\n•matches: All the different matches to the current image by their posting id. Keep in mind, all images are a self-match \nfor first (i.e: all images also match themselves, so you would have to include that in your entry too). Different posting ids will be separated by space.\n","metadata":{}},{"cell_type":"markdown","source":"<h2>Evaluation Metric</h2>\n\nThis competition will be judged on F1 score evaluation. The major difference between ‘accuracy’ and ‘F1’ is that accuracy is dependent on ‘True Positives’ and ‘True Negatives’ while F1 is also dependent on ‘False Positives’ and ‘False Negatives’.\n","metadata":{}},{"cell_type":"markdown","source":"<h2>Libraries</h2>","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport os\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nfrom wordcloud import WordCloud, STOPWORDS\nimport glob\nimport random","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Load Data</h2>","metadata":{}},{"cell_type":"markdown","source":"<h3> Train Data </h3","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv(\"../input/shopee-product-matching/train.csv\")\ntrain_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Test Data </h3>","metadata":{}},{"cell_type":"code","source":"test_data = pd.read_csv(\"../input/shopee-product-matching/test.csv\")\ntest_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> Sample Submission</h3>","metadata":{}},{"cell_type":"code","source":"sample_sub = pd.read_csv(\"../input/shopee-product-matching/sample_submission.csv\")\nsample_sub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Exploratory Data Analysis</h3>","metadata":{}},{"cell_type":"markdown","source":"<h5>Data Information</h5>","metadata":{}},{"cell_type":"code","source":"train_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h5>There is no 'null value' in the dataset:)</h5>","metadata":{}},{"cell_type":"markdown","source":"<h5> Dataset Size </h5>","metadata":{}},{"cell_type":"code","source":"print(f\"Training Dataset Shape: {train_data.shape}\")\nprint(f\"Test Dataset Shape: {test_data.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h5> Column-wise Unique values </h5>","metadata":{}},{"cell_type":"code","source":"for col in train_data.columns:\n    print(col + \":\" + (str(len(train_data[col].unique()))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h5>Except posting_id column all columns have duplicate values </h5>","metadata":{}},{"cell_type":"markdown","source":"<h5> Train & Test Image Count </h5>","metadata":{}},{"cell_type":"code","source":"train_jpg_directory = '../input/shopee-product-matching/train_images'\ntest_jpg_directory = '../input/shopee-product-matching/test_images'\ndef getImagePaths(path):\n    image_names = []\n    for dirname, _, filenames in os.walk(path):\n        for filename in filenames:\n            fullpath = os.path.join(dirname, filename)\n            image_names.append(fullpath)\n    return image_names\ntrain_images_path = getImagePaths(train_jpg_directory)\ntest_images_path = getImagePaths(test_jpg_directory)\nprint(f\"Number of train images: {len(train_images_path)}\")\nprint(f\"Number of test images:  {len(test_images_path)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h5>Display Images</h5>","metadata":{}},{"cell_type":"code","source":"def display_img(images_paths, rows, cols):\n    figure, ax = plt.subplots(nrows=rows,ncols=cols,figsize=(16,8) )\n    for ind,image_path in enumerate(images_paths):\n        image=cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n        try:\n            ax.ravel()[ind].imshow(image)\n            ax.ravel()[ind].set_axis_off()\n        except:\n            continue;\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h5>Train Images</h5>","metadata":{}},{"cell_type":"code","source":"display_img(train_images_path[:50], 5, 5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h5>Test Images</h5>","metadata":{}},{"cell_type":"code","source":"display_img(test_images_path, 1, 3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h5> Image Label Groups by No. of Images </h5>","metadata":{}},{"cell_type":"code","source":"top10_names = train_data['label_group'].value_counts().index.tolist()[:15]\ntop10_values = train_data['label_group'].value_counts().tolist()[:15]\n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=top10_names, y=top10_values)\nplt.xticks(rotation=45)\nplt.xlabel(\"Label Group\")\nplt.ylabel(\"Image Count\")\nplt.title(\"Top-15 Label Groups by Image Count\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h5> Duplicate Count per Label</h5>","metadata":{}},{"cell_type":"code","source":"groups = train_data.label_group.value_counts()\nplt.figure(figsize=(20,5))\nplt.plot(np.arange(len(groups)),groups.values)\nplt.ylabel('Duplicate Count',size=14)\nplt.xlabel('Index of Unique Item',size=14)\nplt.title('Duplicate Count vs. Unique Item Count',size=16)\nplt.show()\n\nplt.figure(figsize=(20,5))\nplt.bar(groups.index.values[:50].astype('str'),groups.values[:50])\nplt.xticks(rotation = 45)\nplt.ylabel('Duplicate Count',size=14)\nplt.xlabel('Label Group',size=14)\nplt.title('Top 50 Duplicated Items',size=16)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4> So we have gathered a good in-dpeth knowledge about Data. Lets try Model now </h4>","metadata":{}},{"cell_type":"markdown","source":"<h2>RAPIDS</h2","metadata":{}},{"cell_type":"code","source":"import cudf, cuml, cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors\nimport tensorflow as tf\nimport nltk\nfrom cuml.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer as CV\nfrom wordcloud import WordCloud,STOPWORDS\nfrom tensorflow.keras.applications import ResNet101\nprint('TF',tf.__version__)\nprint('RAPIDS',cuml.__version__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> Finding Similar Titles using RAPIDS </h3>","metadata":{}},{"cell_type":"markdown","source":"To find similar items in train data using only the title's text, first we will extract text embeddings using RAPIDS cuML's TfidfVectorizer. This will turn every title into a one-hot-encoding of the words present. We will then compare one-hot-encodings with RAPIDS cuML KNN to find title's that are similar.","metadata":{}},{"cell_type":"code","source":"# Load Data\ntrain_data = cudf.read_csv('../input/shopee-product-matching/train.csv')\ntrain_data.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Extract Text Embeddings with RAPIDS TfidfVectorizer</h4>\nTfidfVectorizer returns a cupy sparse matrix. Afterward we convert to a cupy dense matrix and feed that into RAPIDS cuML KNN.","metadata":{}},{"cell_type":"code","source":"model = TfidfVectorizer(stop_words='english', binary=True)\ntext_embeddings = model.fit_transform(train_data.title).toarray()\nprint('text embeddings shape is',text_embeddings.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find similar 'Titles' with RAPIDS KNN\nKNN = 50\nmodel = NearestNeighbors(n_neighbors=KNN)\nmodel.fit(text_embeddings)\ndistances, indices = model.kneighbors(text_embeddings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for k in range(5):\n    plt.figure(figsize=(20,3))\n    plt.plot(np.arange(50),cupy.asnumpy(distances[k,]),'o-')\n    plt.title('Text Distance From Train Row %i to Other Train Rows'%k,size=16)\n    plt.ylabel('Distance to Train Row %i'%k,size=14)\n    plt.xlabel('Index Sorted by Distance to Train Row %i'%k,size=14)\n    plt.show()\n    \n    print( train_data.loc[cupy.asnumpy(indices[k,:10]),['title','label_group']] )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4> Matching Images Usings RAPIDS... In Progress</h4>","metadata":{}},{"cell_type":"markdown","source":"<h4>Refrences</h4>\n\nhttps://www.kaggle.com/cdeotte/rapids-cuml-tfidfvectorizer-and-knn\n\n\nhttps://www.kaggle.com/ishandutta/v7-shopee-indepth-eda-one-stop-for-all-your-needs","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}