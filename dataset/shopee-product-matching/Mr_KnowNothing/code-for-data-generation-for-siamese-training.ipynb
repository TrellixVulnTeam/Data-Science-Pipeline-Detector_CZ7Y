{"cells":[{"metadata":{},"cell_type":"markdown","source":"# About this Notebook\n\nFew weeks ago I shared code for Siamese Style Training Strategy with contrastive loss [here](https://www.kaggle.com/tanulsingh077/siamese-style-training-efficient-net-b0) and the dataset used was published [here](https://www.kaggle.com/tanulsingh077/shopee-siamese-training).\n\nThis is the helper code to generate data for Siamese Style Training with Contrastive Loss . @xhlulu has been kind enough to share code to generate data for Siamese Style training with Triplet Loss [here](https://www.kaggle.com/xhlulu/shopee-generate-data-for-triplet-loss)\n\nNow we all can train models with these two different losses and see what works for us . I will also upload the inference kernels as soon as I get promising results by any of these approaches\n\nThe Logic used in the following code is almost similar as xhlulu's only it differs in the way I extract titles . The one thing which I feel the data suffers from is the number of positive examples since the negative pairs will be much larger than the positive pairs for products having just 2 items\n\nPlease let me know in the comments if you don't understand the code ,I will update this notebook with line by line explanantion then"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/shopee-product-matching/train.csv')\ndata_without_dup = data.drop_duplicates(subset='label_group')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"posting_id_dict = data.groupby('label_group')['posting_id'].unique().to_dict()\ntitle_dict = data.groupby('label_group')['title'].unique().to_dict()\nimage_dict = data.groupby('label_group')['image'].unique().to_dict()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_groups = data_without_dup.label_group.values.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_data = []\nfor i,label_group in tqdm(enumerate(data_without_dup['label_group'])):\n    matches = posting_id_dict[label_group].tolist()\n    titles = title_dict[label_group].tolist()\n    images = image_dict[label_group].tolist()\n    \n    index = np.random.randint(2,len(label_groups)-2)\n    while (index== i):\n        index = np.random.randint(0,len(label_groups))\n    \n    if len(matches) == 2:\n        if len(titles) == 2:\n            matches.extend(titles)\n            matches.extend(images)\n            matches.extend([1])\n        else:\n            matches.extend([titles[0],titles[0]])\n            matches.extend(images)\n            matches.extend([1])\n        new_data.append(matches)\n        new_data.append([matches[0],posting_id_dict[label_groups[index]][0],titles[0],title_dict[label_groups[index]][0],images[0],image_dict[label_groups[index]][0],0])\n        new_data.append([matches[0],posting_id_dict[label_groups[index+1]][0],titles[0],title_dict[label_groups[index+1]][0],images[0],image_dict[label_groups[index+1]][0],0])\n        new_data.append([matches[0],posting_id_dict[label_groups[index-1]][0],titles[0],title_dict[label_groups[index-1]][0],images[0],image_dict[label_groups[index-1]][0],0])\n    \n    else:\n        for match,title,image in zip(matches[1:],titles[1:],images[1:]):\n            new_data.append([matches[0],match,titles[0],title,images[0],image,1])\n            new_data.append([matches[0],posting_id_dict[label_groups[index]][0],titles[0],title_dict[label_groups[index]][0],images[0],image_dict[label_groups[index]][0],0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"siamese_data = pd.DataFrame(new_data,columns=['posting_id_1','posting_id_2','title_1','title_2','image_1','image_2','label'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"siamese_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"siamese_data.to_csv('siamese_data.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}