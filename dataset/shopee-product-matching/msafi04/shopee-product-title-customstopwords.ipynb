{"cells":[{"metadata":{},"cell_type":"markdown","source":"Many thanks to Chris Deotte @cdeotte for his great notebooks!!"},{"metadata":{},"cell_type":"markdown","source":"In this notebook, I try to find the effect preprocessing of text has on the F1 score (considering only the title). I have tried:\n1. Using the title without any pre-processing\n2. Pre-process by removing punctuations, numbers and special characters\n3. Removing stopwords of the language used in the title\n4. Remove stopwords and numbers/special characters"},{"metadata":{},"cell_type":"markdown","source":"__Shopee is the leading e-commerce platform in Southeast Asia and Taiwan.__"},{"metadata":{},"cell_type":"markdown","source":"# Competition Goal\n\n__In this competition, youâ€™ll apply your machine learning skills to build a model that predicts which items are the same products.__"},{"metadata":{},"cell_type":"markdown","source":"# Evaluation Metric\n\n__Submissions will be evaluated based on their mean F1 score.__"},{"metadata":{},"cell_type":"markdown","source":"# Code Requirements\n\nSubmissions to this competition must be made through Notebooks. In order for the \"Submit\" button to be active after a commit, the following conditions must be met:\n\n- CPU Notebook <= 9\n- GPU Notebook <= 2\n- Internet access disabled\n- Freely & publicly available external data is allowed, including pre-trained models\n- Submission file must be named \"submission.csv\""},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%matplotlib inline\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display\n\nimport cudf, cuml, cupy\nfrom textwrap import wrap\n\nimport gc\n\nimport itertools\nimport collections\nfrom collections import Counter\n\nimport re\nfrom wordcloud import WordCloud\n\nimport os\nprint(os.listdir('/kaggle/input/shopee-product-matching/'))\n\nfrom time import time, strftime, gmtime\nstart = time()\nimport datetime\nprint(str(datetime.datetime.now()))\n\nimport warnings\nwarnings.simplefilter('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_dir = '/kaggle/input/shopee-product-matching/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(base_dir + 'train.csv')\nprint(train.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(base_dir + 'test.csv')\nprint(test.shape)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv(base_dir + 'sample_submission.csv')\nprint(sub.shape)\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Number of train images: {len(os.listdir(base_dir + \"train_images/\"))}')\nprint(f'Number of test images: {len(os.listdir(base_dir + \"test_images/\"))}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['image_path'] = base_dir + 'train_images/' + train['image']\ntest['image_path'] = base_dir + 'test_images/' + test['image']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Baseline F1 score using Image Phash provided__"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = train.groupby('label_group')['posting_id'].agg('unique').to_dict()\ntrain['target'] = train['label_group'].map(temp)\ntrain.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_f1score(col):\n    def f1score(row):\n        n = len(np.intersect1d(row.target, row[col]))\n        return 2 * n / (len(row.target) + len(row[col]))\n    return f1score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#To calculate F1 score - local\ntemp = train.groupby('image_phash')['posting_id'].agg('unique').to_dict()\ntrain['oof'] = train['image_phash'].map(temp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['f1_base'] = train.apply(get_f1score('oof'), axis = 1)\nprint(f\"Train F1 Score: {round(train['f1_base'].mean(), 3)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#For submission test set will be replaced with bigger(70k) dataset\nif len(test) == 3:\n    df = train\n    img_dir = '../input/shopee-product-matching/train_images/'\n    df_text = df[['title']]\n    print(df.shape)\nelse:\n    df = test\n    img_dir = '../input/shopee-product-matching/test_images/'\n    df_text = df[['title']]\n    print(df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Find similar images using Text (title) embeddings"},{"metadata":{},"cell_type":"markdown","source":"# 1. F1 Metric using Clean Text\n- Clean text of numbers, special characters, punctuations"},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\n\ndef preprocess(x):\n    try:\n        x = x.lower() #lower case\n        x = x.strip() #white space\n        x = x.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation))) #remove punctuations\n        x = re.sub(r'[^a-z]', ' ', x) #Remove numbers and special characters\n    except:\n        None\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_text['title_clean'] = df_text['title'].apply(lambda x: preprocess(x))\n#Convert to cudf to speed up\ntitle_text = cudf.DataFrame.from_pandas(df_text)['title_clean']\ntitle_text.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from cuml.feature_extraction.text import TfidfVectorizer\n\ntfid = TfidfVectorizer(stop_words = 'english', binary = True, max_features = 25000)\n\ntext_embeddings = tfid.fit_transform(title_text).toarray()\nprint(f\"Title Text Embeddings shape: {text_embeddings.shape}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Finding similar titles with Cosine Similarity__"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\npreds = []\nchunk_size = 4096\n\nchunk_it = np.arange(np.ceil(len(df) / chunk_size))\n\nfor j in chunk_it: \n    a = int(j * chunk_size)\n    b = int((j + 1) * chunk_size)\n    b = min(b, len(df))\n    print('Processing chunk', a, 'to', b)\n    sim = cupy.matmul(text_embeddings, text_embeddings[a: b].T).T\n    for k in range(b - a):\n        idx = cupy.where(sim[k, ] > 0.75)[0]\n        post_ids = df.iloc[cupy.asnumpy(idx)]['posting_id'].values\n        preds.append(post_ids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['preds_txt1'] = preds\ndf.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"F1 Score for Text  : {round(df.apply(get_f1score('preds_txt1'), axis = 1).mean(), 3)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. F1 Metric using Clean Text with numbers/spl chars\n- Remove punctuations, lower case and strip white space"},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\n\ndef preprocess(x):\n    try:\n        x = x.lower() #lower case\n        x = x.strip() #white space\n        x = x.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation))) #remove punctuations\n        #x = re.sub(r'[^a-z]', ' ', x) #Remove numbers and special characters\n    except:\n        None\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_text['title_clean'] = df_text['title'].apply(lambda x: preprocess(x))\n#Convert to cudf to speed up\ntitle_text = cudf.DataFrame.from_pandas(df_text)['title_clean']\ntitle_text.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfid = TfidfVectorizer(stop_words = 'english', binary = True, max_features = 25000)\n\ntext_embeddings = tfid.fit_transform(title_text).toarray()\nprint(f\"Title Text Embeddings shape: {text_embeddings.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\npreds = []\nchunk_size = 4096\n\nchunk_it = np.arange(np.ceil(len(df) / chunk_size))\n\nfor j in chunk_it: \n    a = int(j * chunk_size)\n    b = int((j + 1) * chunk_size)\n    b = min(b, len(df))\n    print('Processing chunk', a, 'to', b)\n    sim = cupy.matmul(text_embeddings, text_embeddings[a: b].T).T\n    for k in range(b - a):\n        idx = cupy.where(sim[k, ] > 0.75)[0]\n        post_ids = df.iloc[cupy.asnumpy(idx)]['posting_id'].values\n        preds.append(post_ids)\n\ndel text_embeddings\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['preds_txt2'] = preds\nprint(f\"F1 Score for Text  : {round(df.apply(get_f1score('preds_txt2'), axis = 1).mean(), 3)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. F1 Metric using custom Stopwords with numbers/spl chars\n- Remove punctuations and remove malay stopwords"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#from https://github.com/stopwords-iso/stopwords-id\nindo_stopwords = [\n                  \"ada\",\"adalah\",\"adanya\",\"adapun\",\"agak\",\"agaknya\",\"agar\",\"akan\",\"akankah\",\"akhir\",\"akhiri\",\n                  \"akhirnya\",\"aku\",\"akulah\",\"amat\",\"amatlah\",\"anda\",\"andalah\",\"antar\",\"antara\",\"antaranya\",\"apa\",\n                  \"apaan\",\"apabila\",\"apakah\",\"apalagi\",\"apatah\",\"artinya\",\"asal\",\"asalkan\",\"atas\",\"atau\",\"ataukah\",\n                  \"ataupun\",\"awal\",\"awalnya\",\"bagai\",\"bagaikan\",\"bagaimana\",\"bagaimanakah\",\"bagaimanapun\",\"bagi\",\n                  \"bagian\",\"bahkan\",\"bahwa\",\"bahwasanya\",\"baik\",\"bakal\",\"bakalan\",\"balik\",\"banyak\",\"bapak\",\"baru\",\n                  \"bawah\",\"beberapa\",\"begini\",\"beginian\",\"beginikah\",\"beginilah\",\"begitu\",\"begitukah\",\"begitulah\",\n                  \"begitupun\",\"bekerja\",\"belakang\",\"belakangan\",\"belum\",\"belumlah\",\"benar\",\"benarkah\",\"benarlah\",\n                  \"berada\",\"berakhir\",\"berakhirlah\",\"berakhirnya\",\"berapa\",\"berapakah\",\"berapalah\",\"berapapun\",\n                  \"berarti\",\"berawal\",\"berbagai\",\"berdatangan\",\"beri\",\"berikan\",\"berikut\",\"berikutnya\",\"berjumlah\",\n                  \"berkali-kali\",\"berkata\",\"berkehendak\",\"berkeinginan\",\"berkenaan\",\"berlainan\",\"berlalu\",\n                  \"berlangsung\",\"berlebihan\",\"bermacam\",\"bermacam-macam\",\"bermaksud\",\"bermula\",\"bersama\",\n                  \"bersama-sama\",\"bersiap\",\"bersiap-siap\",\"bertanya\",\"bertanya-tanya\",\"berturut\",\"berturut-turut\",\n                  \"bertutur\",\"berujar\",\"berupa\",\"besar\",\"betul\",\"betulkah\",\"biasa\",\"biasanya\",\"bila\",\"bilakah\",\n                  \"bisa\",\"bisakah\",\"boleh\",\"bolehkah\",\"bolehlah\",\"buat\",\"bukan\",\"bukankah\",\"bukanlah\",\"bukannya\",\n                  \"bulan\",\"bung\",\"cara\",\"caranya\",\"cukup\",\"cukupkah\",\"cukuplah\",\"cuma\",\"dahulu\",\"dalam\",\"dan\",\"dapat\",\n                  \"dari\",\"daripada\",\"datang\",\"dekat\",\"demi\",\"demikian\",\"demikianlah\",\"dengan\",\"depan\",\"di\",\"dia\",\n                  \"diakhiri\",\"diakhirinya\",\"dialah\",\"diantara\",\"diantaranya\",\"diberi\",\"diberikan\",\"diberikannya\",\n                  \"dibuat\",\"dibuatnya\",\"didapat\",\"didatangkan\",\"digunakan\",\"diibaratkan\",\"diibaratkannya\",\"diingat\",\n                  \"diingatkan\",\"diinginkan\",\"dijawab\",\"dijelaskan\",\"dijelaskannya\",\"dikarenakan\",\"dikatakan\",\n                  \"dikatakannya\",\"dikerjakan\",\"diketahui\",\"diketahuinya\",\"dikira\",\"dilakukan\",\"dilalui\",\"dilihat\",\n                  \"dimaksud\",\"dimaksudkan\",\"dimaksudkannya\",\"dimaksudnya\",\"diminta\",\"dimintai\",\"dimisalkan\",\"dimulai\",\n                  \"dimulailah\",\"dimulainya\",\"dimungkinkan\",\"dini\",\"dipastikan\",\"diperbuat\",\"diperbuatnya\",\n                  \"dipergunakan\",\"diperkirakan\",\"diperlihatkan\",\"diperlukan\",\"diperlukannya\",\"dipersoalkan\",\n                  \"dipertanyakan\",\"dipunyai\",\"diri\",\"dirinya\",\"disampaikan\",\"disebut\",\"disebutkan\",\"disebutkannya\",\n                  \"disini\",\"disinilah\",\"ditambahkan\",\"ditandaskan\",\"ditanya\",\"ditanyai\",\"ditanyakan\",\"ditegaskan\",\n                  \"ditujukan\",\"ditunjuk\",\"ditunjuki\",\"ditunjukkan\",\"ditunjukkannya\",\"ditunjuknya\",\"dituturkan\",\n                  \"dituturkannya\",\"diucapkan\",\"diucapkannya\",\"diungkapkan\",\"dong\",\"dua\",\"dulu\",\"empat\",\"enggak\",\n                  \"enggaknya\",\"entah\",\"entahlah\",\"guna\",\"gunakan\",\"hal\",\"hampir\",\"hanya\",\"hanyalah\",\"hari\",\"harus\",\n                  \"haruslah\",\"harusnya\",\"hendak\",\"hendaklah\",\"hendaknya\",\"hingga\",\"ia\",\"ialah\",\"ibarat\",\"ibaratkan\",\n                  \"ibaratnya\",\"ibu\",\"ikut\",\"ingat\",\"ingat-ingat\",\"ingin\",\"inginkah\",\"inginkan\",\"ini\",\"inikah\",\"inilah\",\n                  \"itu\",\"itukah\",\"itulah\",\"jadi\",\"jadilah\",\"jadinya\",\"jangan\",\"jangankan\",\"janganlah\",\"jauh\",\"jawab\",\n                  \"jawaban\",\"jawabnya\",\"jelas\",\"jelaskan\",\"jelaslah\",\"jelasnya\",\"jika\",\"jikalau\",\"juga\",\"jumlah\",\n                  \"jumlahnya\",\"justru\",\"kala\",\"kalau\",\"kalaulah\",\"kalaupun\",\"kalian\",\"kami\",\"kamilah\",\"kamu\",\n                  \"kamulah\",\"kan\",\"kapan\",\"kapankah\",\"kapanpun\",\"karena\",\"karenanya\",\"kasus\",\"kata\",\"katakan\",\n                  \"katakanlah\",\"katanya\",\"ke\",\"keadaan\",\"kebetulan\",\"kecil\",\"kedua\",\"keduanya\",\"keinginan\",\n                  \"kelamaan\",\"kelihatan\",\"kelihatannya\",\"kelima\",\"keluar\",\"kembali\",\"kemudian\",\"kemungkinan\",\n                  \"kemungkinannya\",\"kenapa\",\"kepada\",\"kepadanya\",\"kesampaian\",\"keseluruhan\",\"keseluruhannya\",\n                  \"keterlaluan\",\"ketika\",\"khususnya\",\"kini\",\"kinilah\",\"kira\",\"kira-kira\",\"kiranya\",\"kita\",\"kitalah\",\n                  \"kok\",\"kurang\",\"lagi\",\"lagian\",\"lah\",\"lain\",\"lainnya\",\"lalu\",\"lama\",\"lamanya\",\"lanjut\",\"lanjutnya\",\n                  \"lebih\",\"lewat\",\"lima\",\"luar\",\"macam\",\"maka\",\"makanya\",\"makin\",\"malah\",\"malahan\",\"mampu\",\"mampukah\",\n                  \"mana\",\"manakala\",\"manalagi\",\"masa\",\"masalah\",\"masalahnya\",\"masih\",\"masihkah\",\"masing\",\n                  \"masing-masing\",\"mau\",\"maupun\",\"melainkan\",\"melakukan\",\"melalui\",\"melihat\",\"melihatnya\",\"memang\",\n                  \"memastikan\",\"memberi\",\"memberikan\",\"membuat\",\"memerlukan\",\"memihak\",\"meminta\",\"memintakan\",\n                  \"memisalkan\",\"memperbuat\",\"mempergunakan\",\"memperkirakan\",\"memperlihatkan\",\"mempersiapkan\",\n                  \"mempersoalkan\",\"mempertanyakan\",\"mempunyai\",\"memulai\",\"memungkinkan\",\"menaiki\",\"menambahkan\",\n                  \"menandaskan\",\"menanti\",\"menanti-nanti\",\"menantikan\",\"menanya\",\"menanyai\",\"menanyakan\",\"mendapat\",\n                  \"mendapatkan\",\"mendatang\",\"mendatangi\",\"mendatangkan\",\"menegaskan\",\"mengakhiri\",\"mengapa\",\n                  \"mengatakan\",\"mengatakannya\",\"mengenai\",\"mengerjakan\",\"mengetahui\",\"menggunakan\",\"menghendaki\",\n                  \"mengibaratkan\",\"mengibaratkannya\",\"mengingat\",\"mengingatkan\",\"menginginkan\",\"mengira\",\"mengucapkan\",\n                  \"mengucapkannya\",\"mengungkapkan\",\"menjadi\",\"menjawab\",\"menjelaskan\",\"menuju\",\"menunjuk\",\"menunjuki\",\n                  \"menunjukkan\",\"menunjuknya\",\"menurut\",\"menuturkan\",\"menyampaikan\",\"menyangkut\",\"menyatakan\",\n                  \"menyebutkan\",\"menyeluruh\",\"menyiapkan\",\"merasa\",\"mereka\",\"merekalah\",\"merupakan\",\"meski\",\"meskipun\",\n                  \"meyakini\",\"meyakinkan\",\"minta\",\"mirip\",\"misal\",\"misalkan\",\"misalnya\",\"mula\",\"mulai\",\"mulailah\",\n                  \"mulanya\",\"mungkin\",\"mungkinkah\",\"nah\",\"naik\",\"namun\",\"nanti\",\"nantinya\",\"nyaris\",\"nyatanya\",\"oleh\",\n                  \"olehnya\",\"pada\",\"padahal\",\"padanya\",\"pak\",\"paling\",\"panjang\",\"pantas\",\"para\",\"pasti\",\"pastilah\",\n                  \"penting\",\"pentingnya\",\"per\",\"percuma\",\"perlu\",\"perlukah\",\"perlunya\",\"pernah\",\"persoalan\",\"pertama\",\n                  \"pertama-tama\",\"pertanyaan\",\"pertanyakan\",\"pihak\",\"pihaknya\",\"pukul\",\"pula\",\"pun\",\"punya\",\"rasa\",\n                  \"rasanya\",\"rata\",\"rupanya\",\"saat\",\"saatnya\",\"saja\",\"sajalah\",\"saling\",\"sama\",\"sama-sama\",\n                  \"sambil\",\"sampai\",\"sampai-sampai\",\"sampaikan\",\"sana\",\"sangat\",\"sangatlah\",\"satu\",\"saya\",\n                  \"sayalah\",\"se\",\"sebab\",\"sebabnya\",\"sebagai\",\"sebagaimana\",\"sebagainya\",\"sebagian\",\"sebaik\",\n                  \"sebaik-baiknya\",\"sebaiknya\",\"sebaliknya\",\"sebanyak\",\"sebegini\",\"sebegitu\",\"sebelum\",\n                  \"sebelumnya\",\"sebenarnya\",\"seberapa\",\"sebesar\",\"sebetulnya\",\"sebisanya\",\"sebuah\",\"sebut\",\n                  \"sebutlah\",\"sebutnya\",\"secara\",\"secukupnya\",\"sedang\",\"sedangkan\",\"sedemikian\",\"sedikit\",\n                  \"sedikitnya\",\"seenaknya\",\"segala\",\"segalanya\",\"segera\",\"seharusnya\",\"sehingga\",\"seingat\",\n                  \"sejak\",\"sejauh\",\"sejenak\",\"sejumlah\",\"sekadar\",\"sekadarnya\",\"sekali\",\"sekali-kali\",\"sekalian\",\n                  \"sekaligus\",\"sekalipun\",\"sekarang\",\"sekecil\",\"seketika\",\"sekiranya\",\"sekitar\",\"sekitarnya\",\n                  \"sekurang-kurangnya\",\"sekurangnya\",\"sela\",\"selagi\",\"selain\",\"selaku\",\"selalu\",\"selama\",\n                  \"selama-lamanya\",\"selamanya\",\"selanjutnya\",\"seluruh\",\"seluruhnya\",\"semacam\",\"semakin\",\n                  \"semampu\",\"semampunya\",\"semasa\",\"semasih\",\"semata\",\"semata-mata\",\"semaunya\",\"sementara\",\n                  \"semisal\",\"semisalnya\",\"sempat\",\"semua\",\"semuanya\",\"semula\",\"sendiri\",\"sendirian\",\"sendirinya\",\n                  \"seolah\",\"seolah-olah\",\"seorang\",\"sepanjang\",\"sepantasnya\",\"sepantasnyalah\",\"seperlunya\",\n                  \"seperti\",\"sepertinya\",\"sepihak\",\"sering\",\"seringnya\",\"serta\",\"serupa\",\"sesaat\",\"sesama\",\n                  \"sesampai\",\"sesegera\",\"sesekali\",\"seseorang\",\"sesuatu\",\"sesuatunya\",\"sesudah\",\"sesudahnya\",\n                  \"setelah\",\"setempat\",\"setengah\",\"seterusnya\",\"setiap\",\"setiba\",\"setibanya\",\"setidak-tidaknya\",\n                  \"setidaknya\",\"setinggi\",\"seusai\",\"sewaktu\",\"siap\",\"siapa\",\"siapakah\",\"siapapun\",\"sini\",\"sinilah\",\n                  \"soal\",\"soalnya\",\"suatu\",\"sudah\",\"sudahkah\",\"sudahlah\",\"supaya\",\"tadi\",\"tadinya\",\"tahu\",\"tahun\",\n                  \"tak\",\"tambah\",\"tambahnya\",\"tampak\",\"tampaknya\",\"tandas\",\"tandasnya\",\"tanpa\",\"tanya\",\"tanyakan\",\n                  \"tanyanya\",\"tapi\",\"tegas\",\"tegasnya\",\"telah\",\"tempat\",\"tengah\",\"tentang\",\"tentu\",\"tentulah\",\n                  \"tentunya\",\"tepat\",\"terakhir\",\"terasa\",\"terbanyak\",\"terdahulu\",\"terdapat\",\"terdiri\",\"terhadap\",\n                  \"terhadapnya\",\"teringat\",\"teringat-ingat\",\"terjadi\",\"terjadilah\",\"terjadinya\",\"terkira\",\n                  \"terlalu\",\"terlebih\",\"terlihat\",\"termasuk\",\"ternyata\",\"tersampaikan\",\"tersebut\",\"tersebutlah\",\n                  \"tertentu\",\"tertuju\",\"terus\",\"terutama\",\"tetap\",\"tetapi\",\"tiap\",\"tiba\",\"tiba-tiba\",\"tidak\",\n                  \"tidakkah\",\"tidaklah\",\"tiga\",\"tinggi\",\"toh\",\"tunjuk\",\"turut\",\"tutur\",\"tuturnya\",\"ucap\",\"ucapnya\",\n                  \"ujar\",\"ujarnya\",\"umum\",\"umumnya\",\"ungkap\",\"ungkapnya\",\"untuk\",\"usah\",\"usai\",\"waduh\",\"wah\",\"wahai\",\n                  \"waktu\",\"waktunya\",\"walau\",\"walaupun\",\"wong\",\"yaitu\",\"yakin\",\"yakni\",\"yang\"\n                 ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(x):\n    try:\n        x = x.lower() #lower case\n        x = x.strip() #white space\n        x = x.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation))) #remove punctuations\n        x = ' '.join([word for word in x.split() if word not in indo_stopwords]) #stopwords\n        #x = re.sub(r'[^a-z]', ' ', x) #Remove numbers and special characters\n    except:\n        None\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_text['title_clean'] = df_text['title'].apply(lambda x: preprocess(x))\n#Convert to cudf to speed up\ntitle_text = cudf.DataFrame.from_pandas(df_text)['title_clean']\ntitle_text.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfid = TfidfVectorizer(stop_words = 'english', binary = True, max_features = 25000)\n\ntext_embeddings = tfid.fit_transform(title_text).toarray()\nprint(f\"Title Text Embeddings shape: {text_embeddings.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\npreds = []\nchunk_size = 4096\n\nchunk_it = np.arange(np.ceil(len(df) / chunk_size))\n\nfor j in chunk_it: \n    a = int(j * chunk_size)\n    b = int((j + 1) * chunk_size)\n    b = min(b, len(df))\n    print('Processing chunk', a, 'to', b)\n    sim = cupy.matmul(text_embeddings, text_embeddings[a: b].T).T\n    for k in range(b - a):\n        idx = cupy.where(sim[k, ] > 0.75)[0]\n        post_ids = df.iloc[cupy.asnumpy(idx)]['posting_id'].values\n        preds.append(post_ids)\n\ndel text_embeddings\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['preds_txt3'] = preds\nprint(f\"F1 Score for Text  : {round(df.apply(get_f1score('preds_txt3'), axis = 1).mean(), 3)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. F1 Metric using custom Stopwords\n- Remove punctuations numbers, special chars and indonesian stopwords"},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(x):\n    try:\n        x = x.lower() #lower case\n        x = x.strip() #white space\n        x = x.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation))) #remove punctuations\n        x = ' '.join([word for word in x.split() if word not in indo_stopwords]) #stopwords\n        x = re.sub(r'[^a-z]', ' ', x) #Remove numbers and special characters\n    except:\n        None\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_text['title_clean'] = df_text['title'].apply(lambda x: preprocess(x))\n#Convert to cudf to speed up\ntitle_text = cudf.DataFrame.from_pandas(df_text)['title_clean']\ntitle_text.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfid = TfidfVectorizer(stop_words = 'english', binary = True, max_features = 25000)\n\ntext_embeddings = tfid.fit_transform(title_text).toarray()\nprint(f\"Title Text Embeddings shape: {text_embeddings.shape}\")\n\npreds = []\nchunk_size = 4096\n\nchunk_it = np.arange(np.ceil(len(df) / chunk_size))\n\nfor j in chunk_it: \n    a = int(j * chunk_size)\n    b = int((j + 1) * chunk_size)\n    b = min(b, len(df))\n    print('Processing chunk', a, 'to', b)\n    sim = cupy.matmul(text_embeddings, text_embeddings[a: b].T).T\n    for k in range(b - a):\n        idx = cupy.where(sim[k, ] > 0.75)[0]\n        post_ids = df.iloc[cupy.asnumpy(idx)]['posting_id'].values\n        preds.append(post_ids)\n\ndel text_embeddings\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['preds_txt4'] = preds\nprint(f\"F1 Score for Text  : {round(df.apply(get_f1score('preds_txt4'), axis = 1).mean(), 3)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. F1 Metric title\n- No change"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Convert to cudf to speed up\ntitle_text = cudf.DataFrame.from_pandas(df_text)['title']\ntitle_text.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfid = TfidfVectorizer(stop_words = 'english', binary = True, max_features = 25000)\n\ntext_embeddings = tfid.fit_transform(title_text).toarray()\nprint(f\"Title Text Embeddings shape: {text_embeddings.shape}\")\n\npreds = []\nchunk_size = 4096\n\nchunk_it = np.arange(np.ceil(len(df) / chunk_size))\n\nfor j in chunk_it: \n    a = int(j * chunk_size)\n    b = int((j + 1) * chunk_size)\n    b = min(b, len(df))\n    print('Processing chunk', a, 'to', b)\n    sim = cupy.matmul(text_embeddings, text_embeddings[a: b].T).T\n    for k in range(b - a):\n        idx = cupy.where(sim[k, ] > 0.75)[0]\n        post_ids = df.iloc[cupy.asnumpy(idx)]['posting_id'].values\n        preds.append(post_ids)\n\ndel text_embeddings\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['preds_txt5'] = preds\nprint(f\"F1 Score for Text  : {round(df.apply(get_f1score('preds_txt5'), axis = 1).mean(), 3)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n- Removing numbers and special characters results in higher F1 score (only Title)\n- Using custom stop words has a little effect"},{"metadata":{"trusted":true},"cell_type":"code","source":"finish = time()\nprint(strftime(\"%H:%M:%S\", gmtime(finish - start)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}