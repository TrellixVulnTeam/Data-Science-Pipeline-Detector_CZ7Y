{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-30T14:27:37.969723Z","iopub.execute_input":"2022-04-30T14:27:37.970257Z","iopub.status.idle":"2022-04-30T14:27:37.973986Z","shell.execute_reply.started":"2022-04-30T14:27:37.970218Z","shell.execute_reply":"2022-04-30T14:27:37.973252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd, gc\nimport matplotlib.pyplot as plt\nimport cv2\n\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\n\nimport sys\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\nimport timm\n\nimport cudf, cuml, cupy\nfrom cuml.neighbors import NearestNeighbors\n\nfrom tqdm import tqdm\n# timm.list_models(pretrained=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T14:27:38.038011Z","iopub.execute_input":"2022-04-30T14:27:38.038454Z","iopub.status.idle":"2022-04-30T14:27:38.045303Z","shell.execute_reply.started":"2022-04-30T14:27:38.038426Z","shell.execute_reply":"2022-04-30T14:27:38.0445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Constant\nTRAIN_PATH=\"../input/shopee-product-matching/train.csv\"\nTEST_PATH=\"../input/shopee-product-matching/test.csv\"\nTRAIN_IMAGE_PATH=\"../input/shopee-product-matching/train_images/\" # + image id\nTEST_IMAGE_PATH=\"../input/shopee-product-matching/test_images/\"\n\n'''\nefficientnet_b0\nefficientnet_b4\nefficientnetv2_rw_m\nvgg16\nswin_large_patch4_window12_384\ngluon_resnext101_32x4d\nadv_inception_v3\n'''\nBASE_MODEL = \"efficientnet_b0\"\n\nDIM = (512, 512)\n# DIM = (384, 384)\n# DIM = (320, 320)\n# DIM = (256, 256)\nN_CLASS = 11014\n\nBATCH_SIZE=32\n\nCOMPUTE_CV = False","metadata":{"execution":{"iopub.status.busy":"2022-04-30T14:27:38.152017Z","iopub.execute_input":"2022-04-30T14:27:38.152226Z","iopub.status.idle":"2022-04-30T14:27:38.157078Z","shell.execute_reply.started":"2022-04-30T14:27:38.152202Z","shell.execute_reply":"2022-04-30T14:27:38.15637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_device():\n    if torch.cuda.is_available():\n        device = torch.device('cuda:0')\n        torch.backends.cudnn.benchmark = True\n    else:\n        device = torch.device('cpu') # don't have GPU \n    return device\nDEVICE = get_device()\nprint(DEVICE)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T14:27:38.271597Z","iopub.execute_input":"2022-04-30T14:27:38.273795Z","iopub.status.idle":"2022-04-30T14:27:38.281565Z","shell.execute_reply.started":"2022-04-30T14:27:38.273745Z","shell.execute_reply":"2022-04-30T14:27:38.280723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(TRAIN_PATH)\ntest_df = pd.read_csv(TEST_PATH)\n\ntrain_df['target'] = train_df.label_group.map(\n    train_df.groupby('label_group').posting_id.agg('unique').to_dict()\n)\n\nN_CLASS = train_df['label_group'].nunique()\nprint(\"n_trian: {} n_unique: {} n_per_images: {}\"\n         .format(train_df['label_group'].shape[0],\n                 train_df['label_group'].nunique(),\n                 train_df['label_group'].shape[0]/train_df['label_group'].nunique())\n         )\n\nsample_img = cv2.imread(TRAIN_IMAGE_PATH+train_df.loc[0, 'image'])\nsample_img = cv2.cvtColor(sample_img, cv2.COLOR_BGR2RGB)\n# plt.imshow(sample_img)\n# plt.show()\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-30T14:27:38.336752Z","iopub.execute_input":"2022-04-30T14:27:38.337581Z","iopub.status.idle":"2022-04-30T14:27:39.24804Z","shell.execute_reply.started":"2022-04-30T14:27:38.337542Z","shell.execute_reply":"2022-04-30T14:27:39.247149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Transform only image'''\ndef train_transforms():\n    return A.Compose([\n        A.Normalize(\n            max_pixel_value=255.0, always_apply=True\n        ),\n        ToTensorV2()\n    ])\n\nx = train_transforms()(image=sample_img)['image']\nprint(x.shape, x.min(), x.max())","metadata":{"execution":{"iopub.status.busy":"2022-04-30T14:27:39.250113Z","iopub.execute_input":"2022-04-30T14:27:39.25043Z","iopub.status.idle":"2022-04-30T14:27:39.283765Z","shell.execute_reply.started":"2022-04-30T14:27:39.250386Z","shell.execute_reply":"2022-04-30T14:27:39.282793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp = train_df.groupby('image_phash').posting_id.agg('unique').to_dict()\ntrain_df['oof'] = train_df.image_phash.map(tmp)\ndef getMetric(col):\n    def f1score(row):\n        n = len( np.intersect1d(row.target,row[col]) )\n        return 2*n / (len(row.target)+len(row[col]))\n    return f1score\ntrain_df['f1'] = train_df.apply(getMetric('oof'),axis=1)\nprint('CV score for baseline =',train_df.f1.mean())","metadata":{"execution":{"iopub.status.busy":"2022-04-30T14:27:39.285454Z","iopub.execute_input":"2022-04-30T14:27:39.285858Z","iopub.status.idle":"2022-04-30T14:27:43.822622Z","shell.execute_reply.started":"2022-04-30T14:27:39.285821Z","shell.execute_reply":"2022-04-30T14:27:43.821198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if COMPUTE_CV:\n    test = pd.read_csv('../input/shopee-product-matching/train.csv')\n    test_gf = cudf.DataFrame(test)\n    print('Using train as test to compute CV (since commit notebook). Shape is', test_gf.shape )\nelse:\n    test = pd.read_csv('../input/shopee-product-matching/test.csv')\n    test_gf = cudf.read_csv('../input/shopee-product-matching/test.csv')\n    print('Test shape is', test_gf.shape )\ntest_gf.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-30T14:27:43.824789Z","iopub.execute_input":"2022-04-30T14:27:43.825047Z","iopub.status.idle":"2022-04-30T14:27:43.950037Z","shell.execute_reply.started":"2022-04-30T14:27:43.825013Z","shell.execute_reply":"2022-04-30T14:27:43.949333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ImageModel(nn.Module):\n    def __init__(self, model_name=BASE_MODEL, pretrained=True):\n        self.model_name = model_name\n        self.pretrained=pretrained\n        super().__init__()\n        \n        if model_name==\"resnet50\":\n            self.backbone_model = torchvision.models.resnet50(pretrained=True)\n        \n        else:\n            self.backbone_model = timm.create_model(model_name, pretrained=self.pretrained)\n            if not self.pretrained:\n                checkpoint_path = '../input/new-efficentnetb0weights/efficientnet_b0_ra-3dd342df.pth'\n                pretrained_weights = torch.load(checkpoint_path)\n#                 print(pretrained_weights)\n                self.backbone_model.load_state_dict(pretrained_weights)\n            \n        \n    def forward(self, image):\n        output = self.backbone_model(image)\n        return output\n","metadata":{"execution":{"iopub.status.busy":"2022-04-30T14:27:43.951298Z","iopub.execute_input":"2022-04-30T14:27:43.951971Z","iopub.status.idle":"2022-04-30T14:27:43.959068Z","shell.execute_reply.started":"2022-04-30T14:27:43.951931Z","shell.execute_reply":"2022-04-30T14:27:43.958381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ShopeeDataset(Dataset):\n    def __init__(self, image,labels,dim=DIM, augmentation=None, is_train=True):\n        self.image = image\n        self.labels = labels\n        self.dim = dim\n        self.is_train = is_train\n        self.augmentation = augmentation\n        \n    def get_image(self, image_path, is_train=True):\n        if self.is_train:\n            img = cv2.imread(os.path.join(TRAIN_IMAGE_PATH, image_path))\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            img = cv2.resize(img, self.dim)\n        else:\n            img = cv2.imread(os.path.join(TEST_IMAGE_PATH, image_path))\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            img = cv2.resize(img, self.dim)\n        return img\n        \n    def __len__(self):\n        return len(self.image)\n    \n    def __getitem__(self, idx):\n        img = self.image[idx]\n        img = self.get_image(img)\n        \n        if self.augmentation:\n            tmp = self.augmentation(image=img)\n            img = tmp['image']\n            \n        return img\n        ","metadata":{"execution":{"iopub.status.busy":"2022-04-30T14:27:43.960647Z","iopub.execute_input":"2022-04-30T14:27:43.961183Z","iopub.status.idle":"2022-04-30T14:27:43.972405Z","shell.execute_reply.started":"2022-04-30T14:27:43.961145Z","shell.execute_reply":"2022-04-30T14:27:43.97171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# BASE = '../input/shopee-product-matching/test_images/'\n# if COMPUTE_CV: BASE = '../input/shopee-product-matching/train_images/'\n\nmodel = ImageModel(pretrained=False)\nmodel = model.to(DEVICE)\nembeds = []\n# print(len(train_df['label_group'].values.tolist()))\n\nif COMPUTE_CV:\n    train_ds = ShopeeDataset(\n        image = train_df['image'].values.tolist(),\n        labels = 0,\n        dim = DIM,\n        is_train = True,\n        augmentation=train_transforms(),\n    )\nelse:\n    train_ds = ShopeeDataset(\n        image = test_df['image'].values.tolist(),\n        labels = 0,\n        dim = DIM,\n        is_train = False,\n        augmentation=train_transforms(),\n    )\n    print(COMPUTE_CV, test_df['image'].values.tolist())\n\ntrain_dl = DataLoader(\n    train_ds,\n    batch_size=32,\n#     pin_memory=True,\n    num_workers=2\n)\n\n# for e,  in train_dl:\n#     print(e)\n\nprint('Computing image embeddings...')\nfor i, tmp in enumerate(tqdm(train_dl)):\n    with torch.no_grad():\n        tmp = tmp.to(DEVICE)\n        image_embeddings = model(tmp)\n        embeds.append(image_embeddings.cpu())\n\nimage_embeddings = np.concatenate(embeds)\ndel train_ds, train_dl, embeds, model\n_ = gc.collect()\nprint('image embeddings shape',image_embeddings.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T14:27:43.973877Z","iopub.execute_input":"2022-04-30T14:27:43.974393Z","iopub.status.idle":"2022-04-30T14:37:07.355235Z","shell.execute_reply.started":"2022-04-30T14:27:43.974348Z","shell.execute_reply":"2022-04-30T14:37:07.354384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"KNN = 50\nif len(test)==3: KNN = 2\nmodel = NearestNeighbors(n_neighbors=KNN)\nmodel.fit(image_embeddings)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T14:37:07.356691Z","iopub.execute_input":"2022-04-30T14:37:07.357116Z","iopub.status.idle":"2022-04-30T14:37:07.505834Z","shell.execute_reply.started":"2022-04-30T14:37:07.357074Z","shell.execute_reply":"2022-04-30T14:37:07.505018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(TS=18.0, print_data=True):\n    preds = []\n    CHUNK = 1024\n\n    print('Finding similar images...')\n    CTS = len(image_embeddings)//CHUNK\n    if len(image_embeddings)%CHUNK!=0: CTS += 1\n    for j in range( CTS ):\n\n        a = j*CHUNK\n        b = (j+1)*CHUNK\n        b = min(b,len(image_embeddings))\n        if print_data: print('chunk',a,'to',b)\n        distances, indices = model.kneighbors(image_embeddings[a:b,])\n\n        for k in range(b-a):\n            IDX = np.where(distances[k,]<TS)[0]\n            IDS = indices[k,IDX]\n            o = test.iloc[IDS].posting_id.values\n            preds.append(o)\n    return preds\n\npreds = predict()\n# preds\n# del model, distances, indices, image_embeddings, embeds\n# _ = gc.collect()\n# best ts for efficientnet_b0: 18.0 f1_score: 0.6358098027565081 (512, 512)\n# best ts for efficientnet_b0: 24.0 f1_score: 0.6201323875459158 (256, 256)\n# best ts for efficientnetv2_rw_m: 13.5 f1_score: 0.5539241111324119 (320, 320) from paper\n# best ts for efficientnetv2_rw_m: 12.0 f1_score: 0.5642019980069831 (512, 512)\n# best ts for vgg16: 17.0 f1_score: 0.6318011447485347 (512, 512)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T14:37:07.507352Z","iopub.execute_input":"2022-04-30T14:37:07.507693Z","iopub.status.idle":"2022-04-30T14:37:21.303393Z","shell.execute_reply.started":"2022-04-30T14:37:07.507638Z","shell.execute_reply":"2022-04-30T14:37:21.301846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['preds'] = preds\ntmp = [e.shape[0] for e in test['preds']]\nprint(max(tmp))\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-30T14:37:21.310828Z","iopub.execute_input":"2022-04-30T14:37:21.311168Z","iopub.status.idle":"2022-04-30T14:37:21.370567Z","shell.execute_reply.started":"2022-04-30T14:37:21.311129Z","shell.execute_reply":"2022-04-30T14:37:21.369563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def combine_for_sub(row):\n    x = np.concatenate([row.preds,row.preds2, row.preds3])\n    return ' '.join( np.unique(x) )\n\ndef combine_for_cv(row):\n    x = np.concatenate([row.preds,row.preds2, row.preds3])\n    return np.unique(x)\n\ndef my_combine_for_sub(row):\n    x = row.preds\n    return ' '.join( np.unique(x) )\n\ndef my_for_cv(row):\n    return row.preds","metadata":{"execution":{"iopub.status.busy":"2022-04-30T14:37:21.37222Z","iopub.execute_input":"2022-04-30T14:37:21.372788Z","iopub.status.idle":"2022-04-30T14:37:21.380708Z","shell.execute_reply.started":"2022-04-30T14:37:21.372718Z","shell.execute_reply":"2022-04-30T14:37:21.379764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_score(combine=my_for_cv):\n    if COMPUTE_CV:\n        tmp = test.groupby('label_group').posting_id.agg('unique').to_dict()\n        test['target'] = test.label_group.map(tmp)\n        test['oof'] = test.apply(combine,axis=1)\n        test['f1'] = test.apply(getMetric('oof'),axis=1)\n        print('CV Score =', test.f1.mean() )\n\nget_score()","metadata":{"execution":{"iopub.status.busy":"2022-04-30T14:37:21.387098Z","iopub.execute_input":"2022-04-30T14:37:21.387488Z","iopub.status.idle":"2022-04-30T14:37:24.880029Z","shell.execute_reply.started":"2022-04-30T14:37:21.387427Z","shell.execute_reply":"2022-04-30T14:37:24.878549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def something():\n    if COMPUTE_CV:\n        log = []\n        index_log = []\n        for i in np.arange(9.0, 30.0, 0.5):\n            test['preds'] = predict(i, print_data=False)\n            tmp = test.groupby('label_group').posting_id.agg('unique').to_dict()\n            test['target'] = test.label_group.map(tmp)\n            test['oof'] = test.apply(my_for_cv,axis=1)\n            test['f1'] = test.apply(getMetric('oof'),axis=1)\n            print('TS = {},CV Score = {}'.format(i, test.f1.mean()))\n            index_log.append(i)\n            log.append(test.f1.mean())\n    \nsomething()","metadata":{"execution":{"iopub.status.busy":"2022-04-30T14:37:24.881294Z","iopub.execute_input":"2022-04-30T14:37:24.881693Z","iopub.status.idle":"2022-04-30T14:42:10.569127Z","shell.execute_reply.started":"2022-04-30T14:37:24.881633Z","shell.execute_reply":"2022-04-30T14:42:10.567676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['matches'] = test.apply(my_combine_for_sub,axis=1)\ntest[['posting_id','matches']].to_csv('submission.csv',index=False)\nsub = pd.read_csv('submission.csv')\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-30T14:42:10.570358Z","iopub.status.idle":"2022-04-30T14:42:10.57078Z","shell.execute_reply.started":"2022-04-30T14:42:10.570553Z","shell.execute_reply":"2022-04-30T14:42:10.570573Z"},"trusted":true},"execution_count":null,"outputs":[]}]}