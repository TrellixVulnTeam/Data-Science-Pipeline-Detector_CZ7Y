{"cells":[{"metadata":{},"cell_type":"markdown","source":"## The aim of this notebook is to make a pHash distance matrix which can help in the analysis of the dataset, and modelling the solution."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport numpy as np\nimport pandas as pd\n\nimport hashlib \n\nfrom scipy.spatial.distance import pdist\n\nimport pickle ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"root_dir = '/kaggle/input/shopee-product-matching/'\ntrain_imgs_dir = root_dir+'train_images/'\ntest_imgs_dir = root_dir+'test_images/'\n\ntrain = pd.read_csv(root_dir+'train.csv')\ntest = pd.read_csv(root_dir+'test.csv')\nsubmission = pd.read_csv(root_dir+'sample_submission.csv')\n\n# add target column to training set\ntmp = train.groupby('label_group').posting_id.agg('unique').to_dict()\ntrain['target'] = train.label_group.map(tmp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The hamming distance is a glorified way of saying how many characters are different between two given strings."},{"metadata":{"trusted":true},"cell_type":"code","source":"def hamming_distance(hash1, hash2):\n    \"Calculates hamming distance between two hashes\"    \n    return sum([c1 != c2 for c1, c2 in zip(hash1, hash2)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# example for hamming distance between two images of the same label\ntrain_rand_label = np.random.choice(train.label_group.value_counts().index)\ntmp = train[train.label_group == train_rand_label]\nimg1, hash1 = tmp[['image', 'image_phash']].sample(1).values[0]\nimg2, hash2 = tmp[['image', 'image_phash']].sample(1).values[0]\n\nprint('Hamming distance:', hamming_distance(hash1, hash2))\n\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.imshow(mpimg.imread(train_imgs_dir+img1))\nplt.title(hash1)\nplt.subplot(1, 2, 2)\nplt.imshow(mpimg.imread(train_imgs_dir+img2))\nplt.title(hash2)\nplt.axis('off');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# example for hamming distance between two random images\nimg1, hash1 = train[['image', 'image_phash']].sample(1).values[0]\nimg2, hash2 = train[['image', 'image_phash']].sample(1).values[0]\n\nprint('Hamming distance:', hamming_distance(hash1, hash2))\n\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.imshow(mpimg.imread(train_imgs_dir+img1))\nplt.title(hash1)\nplt.subplot(1, 2, 2)\nplt.imshow(mpimg.imread(train_imgs_dir+img2))\nplt.title(hash2)\nplt.axis('off');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### At first I tried usin numpy's pdist, but 32450x32450 pairwise distance adds up to more than a billion pairs.\n\n#### After that I divised a solution which converts the pHash to a one hot encoded version consisting of 16 features for each of the 16 characters of the hash. Where the hamming distance is just 16 - dot(hash1, hash2)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n\n# # calculate pairwise distance matrix between postings using image_phash\n# hashes = train.image_phash.values.reshape(-1, 1)\n# dm = pdist(hashes, hamming_distance)\n\n# save mat for later used\n# pickle.dump(dm, open('phash_dist_mat.pkl', 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dm = squareform(pdist(hashes[:10000], lambda s1, s2: sum([c1 != c2 for h1, h2 in zip(s1, s2) for c1, c2 in zip(h1, h2)])))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class HashOneHot:\n    def __init__(self):\n        hexa = '0123456789abcdef'\n        self.hexa_to_idx = {c: i for i, c in enumerate(hexa)}\n        self.idx_to_hexa = {i: c for i, c in enumerate(hexa)}\n    \n    def encode_hash(self, hash_string):\n        encoding = np.zeros((1, 16*len(hash_string)))\n        for i, hexa in enumerate(hash_string):\n            encoding[0, (i * 16) + self.hexa_to_idx[hexa]] = 1\n        return encoding\n    \n    def decode_hash(self, encoding):\n        hash_string = []\n        hexa_onehot = list(np.where(encoding == 1)[-1])\n        for i, hexa in zip(np.arange(0, 256, 16), hexa_onehot):\n            hash_string.append(self.idx_to_hexa[hexa - i])\n        return ''.join(hash_string)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# testing\nhashes = train.image_phash.values\n\nencoder = HashOneHot()\nphash = hashes[0][0]\nencoding = encoder.encode_hash(phash)\ndecoding = encoder.decode_hash(encoding)\ndecoding == phash","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"phash_onehot = np.empty((hashes.shape[0], 16*16))\n\nfor i, phash in enumerate(hashes):\n    phash_onehot[i, :] = encoder.encode_hash(phash)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# another test\nfor i, phash in enumerate(phash_onehot):\n    encoder.decode_hash(phash) == hashes[i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"phash_dist = np.empty(shape=[train.shape[0], train.shape[0]], dtype=np.int8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Vanilla matrix multiplcation was too much for the memory to handle, so I opted for making the matrix 1000 posts at a time, which takes around 7 minutes."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfor i in np.arange(0, 32450, 1000):\n    phash_dist[i:i+1000] = 16 - (np.matmul(phash_onehot[i:i+1000], phash_onehot.T))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Using this matrix, we can find the n closest posts to any given post. For example, this function plots the top 50 closest posts are those and puts the hash and the hamming distance in the title."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_50_closest_posts(i):\n    closest_50_posts = np.argsort(phash_dist[i])[:50]\n    closest_50_images = train.loc[closest_50_posts, 'image'].tolist()\n    closest_50_hashes = train.loc[closest_50_posts, 'image_phash'].tolist()\n\n    n_rows = 5\n    n_cols = 10\n\n    plt.figure(figsize=(n_cols*3.2, n_rows*3.2))\n    for row in range(n_rows):\n        for col in range(n_cols):\n            idx = row * n_cols + col\n            plt.subplot(n_rows, n_cols, idx+1)\n\n            img = mpimg.imread(train_imgs_dir+closest_50_images[idx])\n            plt.imshow(img)\n            plt.axis('off')\n            plt.title(closest_50_hashes[idx] + '-' + str(phash_dist[i, closest_50_posts[idx]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rand_post = np.random.choice(32450)\nplot_50_closest_posts(rand_post)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### One thing I noticed while fooling around with this function is that the similar posts won't probably appear in the top 50 closest posts. Definitely more analysis could carried out using this matrix, so I hope you enjoy it."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}