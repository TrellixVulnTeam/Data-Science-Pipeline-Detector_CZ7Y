{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nos.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nfrom numpy.linalg import norm\nimport pickle\nfrom tqdm import tqdm, tqdm_notebook\nimport time\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3,preprocess_input\nimport pandas as pd\nimport math","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = InceptionV3(weights='imagenet', include_top=False,\n                 input_shape=(299, 299, 3),pooling=\"max\")\ndef extract_features(img_path, model):\n    input_shape = (299, 299, 3)\n    img = image.load_img(img_path, target_size=(\n        input_shape[0], input_shape[1]))\n    img_array = image.img_to_array(img)\n    expanded_img_array = np.expand_dims(img_array, axis=0)\n    preprocessed_img = preprocess_input(expanded_img_array)\n    features = model.predict(preprocessed_img)\n    flattened_features = features.flatten()\n    normalized_features = flattened_features / norm(flattened_features)\n    return normalized_features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/shopee-product-matching/train.csv')\ntest_df = pd.read_csv('../input/shopee-product-matching/test.csv')\nlen_data_train  = len(train_df)\nlen_data_test  = len(test_df)\nBATCH_SIZE = 32\nTRAIN_BATCHES = math.ceil(len_data_train/BATCH_SIZE)\nTEST_BATCHES = math.ceil(len_data_test/BATCH_SIZE)\ntrain_images = '../input/shopee-product-matching/train_images'\ntest_images = '../input/shopee-product-matching/test_images'\nIMG_SIZE = 299","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def myzip(s,t):\n    return [(s[i], t[i]) for i in range(len(s))]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = list(set(train_df.label_group.tolist()))\n \nlabels.sort()\nno_classes = len(labels)\n\nlabel=[]\nmapped=[]\nfor index,value in enumerate(labels):\n    label.append(value)\n    mapped.append(index)\nzipper = myzip(label, mapped)\nreverse = myzip(mapped,label)\nlabel_dict = dict(zipper)\nreverse_dict = dict(reverse)\n\nfor index,label in enumerate(train_df.label_group):    \n    train_df.at[index,'label_group'] = label_dict[label]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['image'] = train_df.image.map(lambda x: '../input/shopee-product-matching/train_images/' + x)\ntest_df['image'] = test_df.image.map(lambda x: '../input/shopee-product-matching/test_images/' + x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xs_train_image = train_df.image.to_numpy()\nxs_test_image = test_df.image.to_numpy()\nclass_ids = train_df.label_group.to_numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filenames = xs_train_image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_list = []\nfor i in tqdm_notebook(range(len(filenames))):\n    feature_list.append(extract_features(filenames[i], model))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.neighbors import NearestNeighbors\nneighbors = NearestNeighbors(n_neighbors=5, algorithm='brute',\nmetric='euclidean').fit(feature_list)\ndistances, indices = neighbors.kneighbors([feature_list[0]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n%matplotlib inline  \nplt.imshow(mpimg.imread(filenames[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_images(similar_image_paths, distances): \n    plt.figure(figsize=(20,20))\n    for i,imagepath in enumerate(similar_image_paths[:25]):\n        plt.subplot(5,5,i+1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.grid(False)\n        plt.imshow(mpimg.imread(imagepath))\n        ipath = \"/\".join(imagepath.split(\"/\")[-2:])\n        if i == 0:\n            plt.xlabel(f'Original: self d {distances[i]:.1f}')\n            plt.ylabel(f'{ipath}')\n        else:\n            plt.xlabel(f'Near match: {distances[i]:.5f}')\n            plt.ylabel(f'{ipath}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_images = len(filenames)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(6):\n    random_image_index = random.randint(0,num_images)\n    distances, indices = neighbors.kneighbors([feature_list[random_image_index]])\n    similar_image_paths = [   filenames[random_image_index]   ] + [     filenames[    indices[0][i]   ] for i in range(1,len(indices[0]) )     ]\n    plot_images(similar_image_paths, distances[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform PCA over the features\nnum_feature_dimensions=100      # Set the number of features\npca = PCA(n_components = num_feature_dimensions)\npca.fit(feature_list)\nfeature_list_compressed = pca.transform(feature_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# partial clusters.\nselected_features = feature_list_compressed[:1000]\nselected_class_ids = class_ids[:1000]\nselected_filenames = filenames[:1000]\n\ntsne_results = TSNE(n_components=2,verbose=1,metric='euclidean').fit_transform(selected_features)\n\n# Plot a scatter plot from the generated t-SNE results\ncolormap = plt.cm.get_cmap('coolwarm')\nscatter_plot = plt.scatter(tsne_results[:,0],tsne_results[:,1], c = selected_class_ids, cmap=colormap)\nplt.colorbar(scatter_plot)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib.offsetbox import OffsetImage, AnnotationBbox\nfrom matplotlib.cbook import get_sample_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def imscatter(x, y, images, ax=None, zoom=1):\n    if ax is None:\n        ax = plt.gca()\n    \n    x, y = np.atleast_1d(x, y)\n    artists = []\n    for i,(x0, y0) in enumerate(zip(x, y)):\n        try:\n            img = plt.imread(images[i])\n        except TypeError:\n            pass\n        im = OffsetImage(img, zoom=zoom)\n\n        ab = AnnotationBbox(im, (x0, y0), xycoords='data', frameon=False)\n        artists.append(ax.add_artist(ab))\n    ax.update_datalim(np.column_stack([x, y]))\n    ax.autoscale()\n    return artists","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12,12), dpi=100)\nimscatter(tsne_results[:,0],tsne_results[:,1], selected_filenames, zoom=0.1, ax=ax)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}