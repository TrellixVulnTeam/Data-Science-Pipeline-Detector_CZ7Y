{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\nfrom tqdm import tqdm  \nfrom keras.preprocessing.text import Tokenizer\n\nimport numpy as np, pandas as pd\nimport cv2,matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\nfrom tensorflow.keras.preprocessing import image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/shopee-product-matching/train.csv')\ntest_df = pd.read_csv('../input/shopee-product-matching/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no_classes = len(train_df.label_group.unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len_data_train  = len(train_df)\nlen_data_test  = len(test_df)\nBATCH_SIZE = 32\nTRAIN_BATCHES = math.ceil(len_data_train/BATCH_SIZE)\nTEST_BATCHES = math.ceil(len_data_test/BATCH_SIZE)\nIMG_SIZE = 32","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataGenerator(tf.keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, df, img_size=IMG_SIZE, batch_size=BATCH_SIZE, path=''): \n        self.df = df\n        self.img_size = img_size\n        self.batch_size = batch_size\n        self.path = path\n        self.indexes = np.arange( len(self.df) )\n        \n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        batches = math.ceil(len(self.df) / self.batch_size)\n        \n        return batches\n\n    def __getitem__(self, batch):\n        'Generate one batch of data'\n        demarrer=batch*self.batch_size\n        fin=min(self.batch_size*batch + self.batch_size,len(self.df))\n        indexes = self.indexes[demarrer:fin]\n        X = self.__data_generation(indexes)\n        return X\n            \n    def __data_generation(self, indexes):\n        'Generates data containing batch_size samples' \n        X = np.zeros((len(indexes),self.img_size,self.img_size,3),dtype='float32')\n        df = self.df.iloc[indexes]\n        for i,(index,row) in enumerate(df.iterrows()):            \n            img = image.load_img(self.path+row.image, target_size=(32, 32))\n            x = image.img_to_array(img)\n            x = np.array(x)\n            x = preprocess_input(x)\n            X[i,] = x\n        return X","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN = '../input/shopee-product-matching/train_images/'\nTEST = '../input/shopee-product-matching/test_images/'\n\nCHUNK_SIZE = 1024*4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ndef get_embeds(df,image_path):\n    WIMGN = '../input/vgg16imagenet/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n    \n    model = VGG16(include_top=False, weights=WIMGN, input_shape=None,pooling='avg')\n    \n    embeds = []\n    \n    print('Computing image embeddings...')\n    BATCHES = math.ceil(len(df)/CHUNK_SIZE)\n\n    for i,batch in enumerate( range( BATCHES ) ):\n    \n        a = batch * CHUNK_SIZE\n        b = min(CHUNK_SIZE*batch + CHUNK_SIZE,len_data_train)\n    \n        print('chunk',a,'to',b)\n    \n        image_gen = DataGenerator(df.iloc[a:b], batch_size=BATCH_SIZE, path=image_path)\n        image_embeddings = model.predict(image_gen,verbose=1,use_multiprocessing=True, workers=4)\n        embeds.append(image_embeddings)\n    \n    image_embeddings = np.concatenate(embeds)\n    return image_embeddings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_embeds = get_embeds(train_df,TRAIN)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_embeds = get_embeds(test_df,TEST)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_embeds = np.concatenate((train_embeds,test_embeds), axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del test_embeds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.text import text_to_word_sequence","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_words():\n    with open('../input/infochimps/words_alpha.txt', 'r') as file:\n        data = file.read().replace('\\n', ' ')\n    words = np.array(   list(set(text_to_word_sequence(data)))    )\n    return words","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"words = get_words()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def double(x):\n    return x + x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def word_embed(image_embeddings):\n    mymax=np.array(np.max(image_embeddings, axis=0))\n    mymin=np.array(np.min(image_embeddings, axis=0))\n    mystd=double(np.array(np.std(image_embeddings, axis=0)))\n    \n    bins={}\n    for i in range(0,len(mymax)):\n        amax=mymax[i]\n        amin=mymin[i]\n        astd=mystd[i]\n        abin=[]\n        counter = amin\n        while counter <= amax:\n            abin.append(counter)\n            counter+=astd\n        bins[i]=abin\n        \n    alen = len(image_embeddings.T)\n    blen = len(image_embeddings)\n    \n    digitized=np.empty([alen,blen])\n    \n    for index in range(alen):\n        inds = np.digitize(image_embeddings[:,index], bins[index])\n        digitized[index]=inds\n    \n    digitized = digitized.T\n    \n    digitized_words={}   \n    prev_len = 0;\n    for index in range(alen):\n        worded=[]    \n        for row in digitized[:,index]:\n            getit=int(row+prev_len)\n            worded.append(words[getit])\n        prev_len += max(digitized[:,index])+1         \n        digitized_words[index]=np.array(worded)\n        \n    del digitized\n        \n    digitized_words = np.array(list(digitized_words.values()),dtype=object)\n    digitized_words = digitized_words.T\n    \n    train_words = digitized_words[:len_data_train,:]\n    test_words = digitized_words[len_data_train:,:]\n    del digitized_words\n    train_words = dict(zip(range(len(train_words)),train_words))\n    test_words = dict(zip(range(len(test_words)),test_words))\n    \n    return train_words,test_words","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_words,test_words = word_embed(train_embeds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['image_net']=train_df.index.map(train_words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train_words","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['image_net']=test_df.index.map(test_words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndel test_words","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def unpack(parte):\n    good_stuff = [y for x,y in parte]    \n    return good_stuff","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def transformer(dictd,col1,col2):\n    dict_list=[]\n    for key in dictd:\n        value = dictd[key]\n        if not key.isnumeric() and len(key) > 1:\n            dict_list.append([value,key])\n    dict_list.sort(reverse=True)\n    temp = pd.DataFrame(dict_list,columns=[col1,col2])\n    return temp.set_index('words')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def candy_man(batch_tokens,batch,y_hats,refs):\n    for indexa,tokens in enumerate(batch_tokens):\n        award = {}\n        for index,tokens2 in enumerate(refs):\n            local_award = 0\n            for index2,word in enumerate(tokens): \n                twerk = 0\n                for index3,word2 in enumerate(tokens2):\n                    twerk+=1                  \n                    if word == word2:\n                        local_award+=1\n                        break\n                if twerk >= len(tokens2):\n                    break\n                \n                \n            if local_award > 0:\n                award[index] = local_award\n        award_list=[]\n        label_list=[]\n        for key in award:\n            value = award[key]\n            award_list.append([value,key])\n        if len(award_list) > 0:\n            award_list.sort(reverse=True)\n\n        for (award,indexer) in award_list:\n            label_list.append([award,train_df.loc[indexer,].label_group])\n        y_hats[BATCH_SIZE*batch+indexa]= label_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def anotate_y_hat(sampler):\n    if len(sampler) == 0:\n        clas = ''\n    else:\n        clas = unpack(sampler)[0]\n    return clas","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prime_test():    \n    y_hats={}\n    \n    for batch in range(TEST_BATCHES):\n        candy_man(test_df.image_net[batch * BATCH_SIZE : min(BATCH_SIZE*batch + BATCH_SIZE,len_data_test)],batch,y_hats,test_df.image_net)\n    return y_hats","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_test(y_hats):\n    test_df['y_hat_weights']=test_df.index.map(y_hats)\n    test_df['y_hat_labels']=test_df.y_hat_weights.map(anotate_y_hat)\n    return","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def my_concatenate(array1,array2):\n    return ' '.join(np.unique(np.concatenate((array1,array2), axis = None)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def submit_test():\n    \n    randmn = -100000\n    for index,row in test_df.iterrows():\n        if row['y_hat_labels'] == \"\":\n            test_df.loc[index,'y_hat_labels'] = randmn\n            randmn+=1\n    temp = test_df.groupby('y_hat_labels').posting_id.agg('unique').to_dict()\n    test_df['y_hat_postings'] = test_df.y_hat_labels.map(temp)\n    \n    temp = test_df.groupby('image_phash').posting_id.agg('unique').to_dict()\n    test_df['duplicates'] = test_df.image_phash.map(temp)\n    \n    test_df['matches'] = [my_concatenate(item[0],item[1]) for item in test_df[['duplicates','y_hat_postings']].values ]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_test():\n    y_hats = prime_test()\n    predict_test(y_hats)\n    submit_test() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run_test()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[['posting_id','matches']].to_csv('submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}