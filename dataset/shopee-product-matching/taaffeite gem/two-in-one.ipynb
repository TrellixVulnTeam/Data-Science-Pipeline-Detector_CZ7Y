{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport math\nimport numpy as np\nfrom tqdm import tqdm  \nfrom keras.preprocessing.text import Tokenizer\nimport matplotlib.pyplot as plt\nimport tensorflow as tf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def unpack(parte):\n    good_stuff = [y for x,y in parte]    \n    return good_stuff","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def phasher(phash,ref_phashes):\n    same = {}\n    for index,ref_phash in enumerate(ref_phashes):\n        similarity = same_same(phash,ref_phash)\n        if similarity >= the_line:\n            same[index] = similarity       \n    same_list=[]    \n    for key in same:\n        value = same[key]\n        same_list.append([value,key])\n    same_list.sort(reverse=True)\n    same_list = unpack(same_list)\n    same_list = [int(i) for i in same_list]\n    same_list = train_df.loc[same_list,].label_group.to_numpy()\n    return same_list[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def self_phasher(phashes):\n    same={}\n    pumpkina = len(phashes)\n    for indexa in range(pumpkina):\n        self_same = []\n        pumpkin = len(phashes)-indexa\n        pumpkinb = pumpkin-1\n        phasha = phashes[indexa:]\n        for index in range(pumpkin):\n            similarity = same_same(phasha[0],phasha[pumpkinb-(pumpkinb-index)])\n            self_same.append(similarity)\n        same[indexa] = np.array(self_same)       \n    return [item for sublist in same.values() for item in sublist]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def  countSetBits(n):\n    count = 0\n    while (n):\n        count += n & 1\n        n >>= 1\n    return count","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def same_same(phash1,phash2):\n    result =phash1 ^ phash2\n    result = countSetBits(int(result))\n    result = 1-result/64\n    return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def transformer(dictd,col1,col2):\n    dict_list=[]\n    for key in dictd:\n        value = dictd[key]\n        if not key.isnumeric() and len(key) > 1:\n            dict_list.append([value,key])\n    dict_list.sort(reverse=True)\n    temp = pd.DataFrame(dict_list,columns=[col1,col2])\n    return temp.set_index('words')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tf_idf(astring):\n    t2 = Tokenizer()\n    t2.fit_on_texts([astring])\n    word_counts=transformer(t2.word_counts,'countd','words')\n    total_words = len(word_counts)\n    word_tf_idf={}\n    for row in word_counts.iterrows():\n        try:\n            tf = list(row)[1][0]/total_words\n            idf = word_docs.loc[row[0],].idf\n            tf_idf_value = tf * idf\n            word_tf_idf[row[0]]=tf_idf_value\n        except:\n            pass\n    dict_list=[]\n    for key in word_tf_idf:\n        value = word_tf_idf[key]\n        dict_list.append([value,key])\n    dict_list.sort(reverse=True)\n    return dict_list   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def candy_man(batch_tokens,batch,y_hats):\n    for indexa,title_tokens in enumerate(batch_tokens):\n        award = {}\n        for index,title_tokens2 in enumerate(ref_tf_idfs):\n            local_award = 0\n            for index2,(weight,word) in enumerate(title_tokens): \n                twerk = 0\n                for index3,(weight2,word2) in enumerate(title_tokens2):\n                    twerk+=1\n                    if word == word2 and weight <= weight2:\n                        local_award+=2 * weight\n                        break\n                    elif word == word2:\n                        local_award+=1 * weight\n                        break\n                if twerk >= len(title_tokens2):\n                    break\n                \n                \n            if local_award > 0:\n                award[index] = local_award\n        award_list=[]\n        label_list=[]\n        for key in award:\n            value = award[key]\n            award_list.append([value,key])\n        if len(award_list) > 0:\n            award_list.sort(reverse=True)\n\n        for (award,indexer) in award_list:\n            label_list.append([award,train_df.loc[indexer,].label_group])\n        y_hats[BATCH_SIZE*batch+indexa]= label_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def anotate_y_hat(yaya,refs):\n    sampler,phash = yaya\n    if len(sampler) == 0:\n        clas = phasher(phash,refs)\n    else:\n        choco = unpack(sampler)\n        getter = min(len(choco),1)\n        clas = choco[:getter][0]\n    return clas","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def annotate_postings(batch_test_class,batch,df,ref_classes_all,y_hat_postings):\n    for indexa,test_class in enumerate(batch_test_class):\n        local_postings=[]\n        for index,ref_class in enumerate(ref_classes_all):              \n                \n            if test_class == ref_class:\n                local_postings.append(df.loc[index,].posting_id)\n                \n        \n        y_hat_postings[BATCH_SIZE*batch+indexa]= local_postings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prime_test():\n    test_df['tf_idf']=test_df.title.map(lambda x: tf_idf(x))\n    y_hats={}\n    for batch in tqdm(range(TEST_BATCHES)):\n        candy_man(test_df.tf_idf[batch * BATCH_SIZE : min(BATCH_SIZE*batch + BATCH_SIZE,len_data_test)],batch,y_hats)\n    return y_hats","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prime_train():\n    y_hats={}\n    for batch in tqdm(range(TRAIN_BATCHES)):\n        candy_man(train_df.tf_idf[batch * BATCH_SIZE : min(BATCH_SIZE*batch + BATCH_SIZE,len_data_train)],batch,y_hats)\n    return y_hats","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_test(y_hats):\n    test_df['y_hat_weights']=test_df.index.map(y_hats)\n    test_df['yaya']=[test_df[['y_hat_weights']].values.tolist()[i] + test_df[['image_phash']].values.tolist()[i]\n                     for i in range(0, len(test_df[['image_phash']].values.tolist()))]\n    test_df['y_hat_labels']=test_df.yaya.map(lambda x: anotate_y_hat(x,train_df.image_phash))\n\n    y_hat_postings={}\n    ref_classes_all = test_df.y_hat_labels\n    for batch in tqdm(range(TEST_BATCHES)):\n        annotate_postings(test_df.y_hat_labels[batch * BATCH_SIZE : min(BATCH_SIZE*batch + BATCH_SIZE,len_data_test)],batch,test_df,ref_classes_all,y_hat_postings)\n    return y_hat_postings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_train(y_hats):\n    train_df['y_hat_weights']=train_df.index.map(y_hats)    \n    train_df['yaya']=[train_df[['y_hat_weights']].values.tolist()[i] + train_df[['image_phash']].values.tolist()[i]\n                     for i in range(0, len(train_df[['image_phash']].values.tolist()))]\n    train_df['y_hat_labels']=train_df.yaya.map(lambda x: anotate_y_hat(x,train_df.image_phash))\n\n    y_hat_postings={}\n    ref_classes_all = train_df.y_hat_labels\n    for batch in tqdm(range(TRAIN_BATCHES)):\n        annotate_postings(train_df.y_hat_labels[batch * BATCH_SIZE : min(BATCH_SIZE*batch + BATCH_SIZE,len_data_train)],batch,train_df,ref_classes_all,y_hat_postings)\n    return y_hat_postings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def submit_test(y_hat_postings):\n    test_df['y_hat_postings']=test_df.index.map(y_hat_postings)\n    temp = test_df.groupby('image_phash').posting_id.agg('unique').to_dict()\n    test_df['smoother'] = test_df.image_phash.map(temp)\n\n\n    test_df['matches'] =[     ' '.join(np.unique([item for item in np.array(test_df[['smoother']].values[i][0]).flatten()] + [item for item in np.array(test_df[['y_hat_postings']].values[i][0],dtype=object).flatten()]))\n                     for i in range(0, len(test_df[['y_hat_postings']].values.tolist()))      ]\n\n                                          \n    test_df[['posting_id','matches']].to_csv('submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def submit_train(y_hat_postings):\n    train_df['y_hat_postings']=train_df.index.map(y_hat_postings)\n    temp = train_df.groupby('image_phash').posting_id.agg('unique').to_dict()\n    train_df['smoother'] = train_df.image_phash.map(temp)\n\n\n    train_df['matches'] =[     ' '.join(np.unique([item for item in np.array(train_df[['smoother']].values[i][0]).flatten()] + [item for item in np.array(train_df[['y_hat_postings']].values[i][0],dtype=object).flatten()]))\n                     for i in range(0, len(train_df[['y_hat_postings']].values.tolist()))      ]\n\n                                          \n   \n    \n    temp = train_df.groupby('label_group').posting_id.agg('unique').to_dict()\n    train_df['target'] = train_df.label_group.map(temp)\n    \n    train_df['f1'] = train_df.apply(getMetric('matches'),axis=1)\n    print('CV score for baseline =',train_df.f1.mean())\n    train_df[['posting_id','matches']].to_csv('submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def scope_test():\n    google_backend()\n    predictions = test_df.y_hat_labels.to_numpy()\n    for index,prediction in enumerate(predictions):\n        matches = train_df.loc[train_df['label_group'] == prediction].index.values.tolist()\n        plt.figure(figsize=(10,10))\n        for i,match in enumerate(matches[:25]):\n            image = train_df.iloc[match].image\n            path = os.path.join(train_images,image)\n            img = tf.keras.preprocessing.image.load_img(path) \n        \n            plt.subplot(5,5,i+1)\n            plt.xticks([])\n            plt.yticks([])\n            plt.grid(False)\n            plt.imshow(img, cmap=plt.cm.binary)\n            plt.xlabel(prediction)\n        plt.show()\n        posting_id = test_df.iloc[index,:].posting_id\n    \n        image = test_df.iloc[index,:].image\n        path = os.path.join(test_images,image)\n        img = tf.keras.preprocessing.image.load_img(path)     \n        plt.figure(figsize=(32,32))\n        plt.subplot(5,5,1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.grid(False)\n        plt.imshow(img, cmap=plt.cm.binary)\n        plt.xlabel(posting_id)\n        plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def google_frontend():\n    y_hats = prime_train()\n    y_hat_postings=predict_train(y_hats)\n    submit_train(y_hat_postings)\n    scope_test()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def google_backend():\n    y_hats = prime_test()\n    y_hat_postings=predict_test(y_hats)\n    submit_test(y_hat_postings) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#https://www.kaggle.com/cdeotte\ndef getMetric(col):\n    def f1score(row):\n        n = len( np.intersect1d(row.target,np.array(row[col].split())) )\n        return 2*n / (len(row.target)+len(np.array(row[col].split())))\n    return f1score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/shopee-product-matching/train.csv')\ntest_df = pd.read_csv('../input/shopee-product-matching/test.csv')\ntrain_images = '../input/shopee-product-matching/train_images'\ntest_images = '../input/shopee-product-matching/test_images'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['image_phash']= train_df.image_phash.apply(lambda x: '0x'+x)\ntest_df['image_phash']= test_df.image_phash.apply(lambda x: '0x'+x)\ntrain_df['image_phash']=train_df.image_phash.apply(lambda x: int(x, 16))\ntest_df['image_phash']=test_df.image_phash.apply(lambda x: int(x, 16))\ntemp = train_df.groupby('label_group')['image_phash'].apply(list).to_dict()\ntrain_df['phashes']= train_df.label_group.map(temp)\ntrain_df['image_phash_weights']= train_df.phashes.apply(lambda x: self_phasher(x))\ntrain_df['means']= train_df.image_phash_weights.apply(    lambda x:np.array(x).mean()     )\ntrain_df['stds']= train_df.image_phash_weights.apply(     lambda x:np.array(x).std()     )\ntrain_df['mins']= train_df.image_phash_weights.apply(     lambda x:np.array(x).min()     )\nthe_line = train_df.mins.mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xs_train = train_df.title.to_numpy()\nxs_test = test_df.title.to_numpy()\nt = Tokenizer()\nt.fit_on_texts(xs_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_docs = t.document_count\nword_docs=transformer(t.word_docs,'docs','words')\nword_docs['idf']=word_docs.docs.map(lambda x: math.log10(total_docs/    (x+0.00000000000000001)        ))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len_data_train  = len(train_df)\nlen_data_test  = len(test_df)\nBATCH_SIZE = 32\nTRAIN_BATCHES = math.ceil(len_data_train/BATCH_SIZE)\nTEST_BATCHES = math.ceil(len_data_test/BATCH_SIZE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['tf_idf']=train_df.title.map(lambda x: tf_idf(x))\nref_tf_idfs=train_df.tf_idf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ref_phashes = train_df.image_phash","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"front = True\nif len_data_test > 3:\n    front = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if front:\n    google_frontend()\nelse:\n    google_backend()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}