{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Zero-shot learning for images and text with CLIP\n\nIn this very minimalist notebook I use the [Sentence-transformers](https://www.sbert.net/examples/applications/image-search/README.html) implementation [OpenAI's CLIP](https://openai.com/blog/clip/) model to generate embeddings for both text and images, and then use KNN to find similar products in the resulting embedding space. \n\nThis is an example of zero-shot learning, an \"extreme\" version of transfer learning where the model isn't fine-tuned on the target task before inference. \n\nThis approach is useful to quickly get results and assess the difficulty of the task at hand while going a step further than a baseline model, which in this case would be simply predicting products with identical ```image_phash``` values as identical.  \n\nPlus it's fun to experiment with cutting-edge models and the possibility they offer, like projecting images and sentences in a same embedding space as shown in the Sentence-transformers [documentation](https://www.sbert.net/examples/applications/image-search/README.html):\n> SentenceTransformers provides models that allow to embed images and text into the same vector space.  \n> This allows to find similar images as well as to implement image search.  \n>  \n> ![](https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/ImageSearch.png)","metadata":{}},{"cell_type":"markdown","source":"In the end we compare the F1 score for 4 approaches:\n1. Using the *image_phash* values\n2. Using KNN on the images embeddings\n3. Using KNN on the titles embeddings\n4. Combining the previous 3 predictions, which yields a surprisingly passable score\n\nThe next steps for developping this notebook would be to:\n- Try different letrics (i. e. cosine) for the ```NearestNeighbors``` model\n- Add a function to find the optimal threshold defining embeddings representing the same product\n- And obviously fine-tune CLIP :P\n  \n  \nIf you have other ideas and/or suggestions, please leave a comment below. ","metadata":{}},{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install ../input/sentencetransformer/sentence-transformers-1.0.4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom typing import List, Optional, Tuple\n\nimport numpy as np\nimport pandas as pd\nfrom cuml.neighbors import NearestNeighbors\nfrom PIL import Image\nfrom sentence_transformers import SentenceTransformer\nfrom tqdm.notebook import tqdm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SUBMIT = False\n\ndf = pd.read_csv(\"../input/shopee-product-matching/test.csv\")\nif len(df) > 3:\n    SUBMIT = True\n\n    \nclass CFG:\n    data_dir = \"../input/shopee-product-matching/\"\n    csv = \"test.csv\" if SUBMIT else \"train.csv\"\n    images_dir = \"test_images/\" if SUBMIT else \"train_images/\"\n    model_name = \"../input/clip-model/clip\"\n    batch_size = 512","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"code","source":"def create_dataset(\n    data_dir: str, \n    csv: str, \n    images_dir: str\n) -> Tuple[pd.DataFrame, List[str], List[str]]:\n\n    df = pd.read_csv(os.path.join(data_dir, csv))\n    df[\"image_path\"] = df.image.apply(lambda x: os.path.join(data_dir, images_dir, x))\n    tmp = df.groupby(\"image_phash\").posting_id.agg(\"unique\").to_dict()\n    df[\"preds_phash\"] = df.image_phash.map(tmp)\n    df[\"preds_phash\"] = df.preds_phash.apply(lambda x: \" \".join(x))\n    return df, df.image_path.to_list(), df.title.to_list()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CLIP embeddings","metadata":{}},{"cell_type":"code","source":"def create_embeddings(\n    model: SentenceTransformer,\n    batch_size: int,\n    data: List[str],\n    is_image: Optional[bool] = False,\n) -> np.ndarray:\n\n    embeddings = np.empty((0, 512))\n    CTS = int(np.ceil(len(data) / batch_size))\n    for i in tqdm(range(CTS), total=CTS):\n        a = i * batch_size\n        b = (i + 1) * batch_size\n        b = min(b, len(data))\n        batch_data = data[a:b]\n        if is_image:\n            batch_data = [Image.open(filepath) for filepath in batch_data]\n        batch_emb = model.encode(batch_data, convert_to_numpy=True, show_progress_bar=False)\n        embeddings = np.concatenate((embeddings, batch_emb), axis=0)\n    return embeddings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Neighbors","metadata":{}},{"cell_type":"code","source":"def get_neighbors(\n    df: pd.DataFrame,\n    embeddings: np.ndarray,\n    n_neighbors: int,\n    metric: str,\n    threshold: float,\n) -> List[str]:\n\n    model = NearestNeighbors(n_neighbors=n_neighbors, metric=metric)\n    model.fit(embeddings)\n    distances, indices = model.kneighbors(embeddings)\n    predictions = []\n    for k in tqdm(range(embeddings.shape[0])):\n        idx = np.where(distances[k, ] < threshold)[0]\n        ids = indices[k, idx]\n        posting_ids = \" \".join(df[\"posting_id\"].iloc[ids].values)\n        predictions.append(posting_ids)\n    return predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Score","metadata":{}},{"cell_type":"code","source":"def combine_predictions(row: pd.Series) -> str:\n    x = \" \".join((row[\"preds_phash\"], row[\"preds_images\"], row[\"preds_titles\"]))\n    return \" \".join([*{*x.split()}])\n\n\ndef f1_score(y_true: pd.Series, y_pred: pd.Series) -> float:\n    y_true = y_true.apply(lambda x: set(x.split()))\n    y_pred = y_pred.apply(lambda x: set(x.split()))\n    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n    len_y_pred = y_pred.apply(lambda x: len(x)).values\n    len_y_true = y_true.apply(lambda x: len(x)).values\n    f1 = 2 * intersection / (len_y_pred + len_y_true)\n    return f1\n\n\ndef print_f1_score(df: pd.DataFrame) -> None:\n    tmp = df.groupby([\"label_group\"]).posting_id.unique().to_dict()\n    df[\"target\"] = df.label_group.map(tmp)\n    df[\"target\"] = df.target.apply(lambda x: \" \".join(x))\n    for column in [\"preds_phash\", \"preds_images\", \"preds_titles\", \"matches\"]:\n        df[\"f1\"] = f1_score(df[column], df[\"target\"])\n        score = df[\"f1\"].mean()\n        print(f\"\\tF1 score associated with columns {column} is: {score}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Engine","metadata":{}},{"cell_type":"code","source":"print(\"Loading data and model...\")\ndf, images, titles = create_dataset(CFG.data_dir, CFG.csv, CFG.images_dir)\nmodel = SentenceTransformer(CFG.model_name)\n\nprint(\"Gettings images embeddings...\")\nimages_emb = create_embeddings(model, CFG.batch_size, images, is_image=True)\n\nprint(\"Gettings titles embeddings...\")\ntitles_emb = create_embeddings(model, CFG.batch_size, titles)\n\nprint(\"Gettings images predictions...\")\ndf[\"preds_images\"] = get_neighbors(df, images_emb, n_neighbors=50, metric=\"euclidean\", threshold=4.5)\n\nprint(\"Getting titles predictions...\")\ndf[\"preds_titles\"] = get_neighbors(df, titles_emb, n_neighbors=50, metric=\"euclidean\", threshold=3)\n\ndf[\"matches\"] = df.apply(combine_predictions, axis=1)\n\nif not SUBMIT:\n    print(\"Getting scores...\")\n    print_f1_score(df)\n\ndf[[\"posting_id\", \"matches\"]].to_csv(\"submission.csv\", index=False)\nprint(\"\\nFile 'submission.csv' successfully saved.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}