{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Online","metadata":{}},{"cell_type":"code","source":"#! pip download -d wheels torch==1.7.1 torchvision==0.8.2 pytorch-lightning==1.2.6 pytorch-metric-learning==0.9.98 faiss-gpu==1.7.0 pydantic==1.7 imbalanced-learn==0.7 aurum==0.3.1 stripping==0.2.6 minio==6.0 albumentations==0.5.2 scikit-image==0.16.2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#! rm wheels/PyYAML-*\n#! rm wheels/urllib3-*\n#! rm wheels/tensorboard-*\n#! rm wheels/numpy-*\n#! rm wheels/Pillow-*","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import os\n#from glob import glob\n#for tar_file in glob(\"wheels/*.tar.gz\"):\n#    os.rename(tar_file, tar_file.replace(\".tar.gz\", \".xyz\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#! zip -r wheels.zip wheels/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from IPython.display import FileLink\n#FileLink(r'wheels.zip')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Offline","metadata":{}},{"cell_type":"code","source":"! mkdir -p /tmp/wheels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! cp /kaggle/input/shopeewheels/wheels/* /tmp/wheels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom glob import glob\n\ntargzs = glob(\"/tmp/wheels/*.xyz\")\nfor targz in targzs:\n    print(targz)\n    if \"timm\" in targz:\n        print(targz)\n        os.remove(targz)\n    else:\n        os.rename(targz, targz.replace(\".xyz\", \".tar.gz\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targzs = glob(\"/tmp/wheels/*\")\nfor targz in targzs:\n     if \"timm\" in targz:\n        print(targz)\n        os.remove(targz)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip install --no-index --no-deps /tmp/wheels/*","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import timm\ntimm.__version__","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"experiment_ids = [\"b89623cd-164a-4ce9-9458-96a2a7df21a1\", \"f04628b3-85b4-4182-8110-8f08a07b188a\", \"e4cae871-accf-4ed1-b316-7dcc2aae9883\", \"c0eb698d-adb4-46b9-a841-f6f4157725e1\"]\nexperiment_id_no_slashes = experiment_ids[-1].replace(\"-\", \"\")\ndistance_thresholds = [[0.8]] * 4\n#distance_threshold_regressor_paths = [\"39261eb9d94646a8b5ba649eddb29c73-mlp-thr-reg/39261eb9-d946-46a8-b5ba-649eddb29c73_mlp_thr_reg.pt\"]\ndistance_threshold_regressor_paths = [None] * 4\nensemble_method = \"AFFIRMATIVE\"\napply_lower_case=False\napply_clean_numbers=False\napply_clean_characters=False\napply_normalize_title=False\napply_remove_stopwords=False\ntext_embedding_dim=25000\ntf_idf_embedding_strategy = \"part_of_ensemble\" # [\"union_after_ensemble\", \"part_of_ensemble\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! git config --global user.email \"fernando@xnv.io\"\n! git config --global user.name \"Fernando Camargo\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! git init\n! au init","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! cp -r /kaggle/input/$experiment_id_no_slashes/* .","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    import aurum as au\nexcept:\n    pass # It's raising an exception the first time for some reason","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from typing import *\nimport pickle\nfrom multiprocessing import Pool\nimport functools\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom sklearn.base import RegressorMixin\nfrom torch.utils.data.dataset import Dataset\nimport PIL.Image\nfrom tqdm.notebook import tqdm\nimport albumentations as A\nfrom albumentations import Compose\nimport faiss\n\nfrom shopee.model.generic_triplet import *\nfrom shopee.model.generic_metric_learner import *\nfrom shopee.data_preparation import ImagePreprocessor, TextPreprocessor\nfrom shopee.dataset import ShopeeDataset\nfrom shopee.ensembling import ensemble, EnsembleMethod\nfrom shopee.nlp.sequence import pad_sequences\n#from shopee.torch.utils import transfer_batch_to_device","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport math\nimport random \nimport os \nimport cv2\nimport timm\n\nfrom tqdm import tqdm \n\nimport albumentations as A \nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torch \nfrom torch.utils.data import Dataset \nfrom torch import nn\nimport torch.nn.functional as F \n\nimport gc\nimport cudf\nimport cuml\nimport cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors\nimport nltk\nnltk.download(\"stopwords\")\nfrom nltk.corpus import stopwords\nimport re\nimport unicodedata\nimport os","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import PIL.Image\n\nfrom shopee.data_preparation import PIL_INTERPOLATION_METHODS\nfrom shopee.dataset import NoOpImagePreprocessor\nfrom shopee.utils import (\n    apply_clahe,\n    resize_keeping_aspect_ratio,\n)\n\ndef transfer_batch_to_device(\n    batch: Union[torch.Tensor, list, dict], device: torch.device\n) -> Union[torch.Tensor, list, dict]:\n    # base case\n    if isinstance(batch, torch.Tensor):\n        return batch.to(device)\n\n    # when list/tuple\n    elif isinstance(batch, list) or isinstance(batch, tuple):\n        for i, x in enumerate(batch):\n            batch[i] = transfer_batch_to_device(x, device)\n        return batch\n\n    # when dict\n    elif isinstance(batch, dict):\n        for k, v in batch.items():\n            batch[k] = transfer_batch_to_device(v, device)\n\n        return batch\n\n    # nothing matches, return the value as is without transform\n    return batch\n\nclass ImagePreprocessor(object):\n    def __init__(\n        self,\n        image_size: Tuple[int, int],\n        keep_aspect_ratio: bool,\n        apply_clahe: bool,\n        interpolation_method: int,\n        center_crop: bool,\n    ) -> None:\n        super().__init__()\n        self._image_size = image_size\n        self._keep_aspect_ratio = keep_aspect_ratio\n        self._apply_clahe = apply_clahe\n        self._interpolation_method = interpolation_method\n        self._center_crop = center_crop\n\n    @property\n    def suffix(self) -> str:\n        suffix = \"%d_%d\" % self._image_size\n        if self._keep_aspect_ratio:\n            suffix += \"_kept_aspect_ratio\"\n        if self._apply_clahe:\n            suffix += \"_clahe\"\n        if self._center_crop:\n            suffix += \"_cropped\"\n        suffix += f\"_interpolation={self._interpolation_method}\"\n\n        return suffix\n\n    def preprocess_image(self, image: PIL.Image.Image) -> PIL.Image.Image:\n        if self._keep_aspect_ratio:\n            image = resize_keeping_aspect_ratio(\n                image, self._image_size, self._interpolation_method\n            )\n        elif self._center_crop:\n            width, height = image.size\n            if width < self._image_size[0] or height < self._image_size[1]:\n                image = image.resize(self._image_size, self._interpolation_method)\n            image = np.array(image)\n            transform = A.CenterCrop(\n                height=self._image_size[0], width=self._image_size[1]\n            )\n            image = PIL.Image.fromarray(transform(image=image)[\"image\"])\n\n        else:\n            image = image.resize(self._image_size, self._interpolation_method)\n        if self._apply_clahe:\n            image = apply_clahe(image)\n        return image\n\n    def preprocess_file(self, image_filepath: str, output_dir: str) -> PIL.Image.Image:\n        image = PIL.Image.open(image_filepath)\n        image = self.preprocess_image(image)\n        image.save(\n            os.path.join(output_dir, os.path.split(image_filepath)[1]), quality=100\n        )\n\n    def preprocess_dir(self, input_dir: str, output_dir: str):\n        image_paths = glob(os.path.join(input_dir, \"*.jpg\"))\n        with Pool(os.cpu_count()) as pool:\n            list(\n                tqdm(\n                    pool.imap(\n                        functools.partial(self.preprocess_file, output_dir=output_dir),\n                        image_paths,\n                    ),\n                    total=len(image_paths),\n                )\n            )\n            \n\nclass ShopeeDataset(Dataset):\n    def __init__(\n        self,\n        df: pd.DataFrame,\n        images_dir: str,\n        image_preprocessing_function: Callable[[np.ndarray], np.ndarray],\n        image_preprocessor: ImagePreprocessor = NoOpImagePreprocessor(),\n        original_images_dir: str = None,\n        text_embeddings: np.ndarray = None,\n        augmenter: A.Compose = None,\n        auto_augment: str = None,\n        test: bool = False,\n    ) -> None:\n        super().__init__()\n\n        self._df = df\n        self._images_dir = images_dir\n        self._image_preprocessing_function = image_preprocessing_function\n        self._image_preprocessor = image_preprocessor\n        self._original_images_dir = original_images_dir\n        self._text_embeddings = text_embeddings\n        self._augmenter = augmenter\n        self._auto_augment = (\n            auto_augment_transform(auto_augment, None) if auto_augment else None\n        )\n        self._test = test\n\n    def __len__(self) -> int:\n        return len(self._df)\n\n    def get_label_groups(self) -> np.ndarray:\n        return self._df[\"label_group\"].values\n\n    def _get_preprocessed_image(self, image_filename: str) -> np.ndarray:\n        image_path = os.path.join(self._images_dir, image_filename)\n\n        if not os.path.exists(image_path):\n            original_image_path = os.path.join(self._original_images_dir, image_filename)\n            image = self._image_preprocessor.preprocess_file(original_image_path, self._images_dir)\n            \n        image = PIL.Image.open(image_path)\n        \n        if self._auto_augment is not None:\n            image = self._auto_augment(image)\n        return np.array(image)\n\n    def __getitem__(\n        self, index: int\n    ) -> Union[\n        Tuple[Dict[str, torch.Tensor], torch.Tensor, torch.Tensor],\n        Dict[str, torch.Tensor],\n    ]:\n        item = self._df.iloc[index]\n\n        image = self._get_preprocessed_image(item[\"image\"])\n\n        if self._augmenter is not None:\n            image = self._augmenter(image=image)[\"image\"]\n\n        image = self._image_preprocessing_function(image)\n        image = image.transpose(2, 0, 1)\n\n        image_tensor = torch.tensor(image, dtype=torch.float32)\n\n        inputs = {\"images\": image_tensor}\n\n        if self._text_embeddings is not None:\n            text_tensor = torch.tensor(self._text_embeddings[index], dtype=torch.float32)\n            inputs['texts'] = text_tensor\n\n        if self._test:\n            return inputs\n        else:\n            return (\n                inputs,\n                torch.tensor(item[\"label_group\"], dtype=torch.long),\n                torch.tensor(item[\"sample_weight\"], dtype=torch.float32)\n                if \"sample_weight\" in item\n                else torch.tensor(1.0, dtype=torch.float32),\n            )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import display, clear_output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_images_dir(image_preprocessor: ImagePreprocessor) -> str:\n    return os.path.join(f\"/tmp/images_{image_preprocessor.suffix}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data.dataloader import DataLoader\nfrom pytorch_metric_learning.distances import (\n    CosineSimilarity,\n    DotProductSimilarity,\n    LpDistance,\n    SNRDistance,\n)\n\ndef _predict_matches(\n    self, loader: DataLoader, device: torch.device, index: faiss.Index, is_inverted: bool, k: int = 50, distance_threshold: float = np.inf, distance_threshold_regressor: RegressorMixin = None,\n) -> List[np.ndarray]:\n    self.eval()\n\n    embeddings_list: List[torch.Tensor] = []\n    if distance_threshold_regressor is not None:\n        distance_thresholds_list = []\n    with torch.no_grad():\n        for inputs in tqdm(loader, total=len(loader)):\n            emb = self(transfer_batch_to_device(inputs, device))\n            #if isinstance(self.distance, CosineSimilarity):\n            emb = F.normalize(emb)\n            embeddings = emb.cpu().numpy()\n            embeddings_list.append(embeddings)\n            if distance_threshold_regressor is not None:\n                if isinstance(distance_threshold_regressor, torch.jit.ScriptModule):\n                    distance_thresholds_list.append(distance_threshold_regressor(emb).flatten().cpu().numpy())\n                else:\n                    distance_thresholds_list.append(distance_threshold_regressor.predict(embeddings))\n            \n    self.to(torch.device(\"cpu\"))\n    torch.cuda.empty_cache()\n\n    embeddings = np.concatenate(embeddings_list, axis=0)\n    \n    res = faiss.StandardGpuResources()\n    index = faiss.index_cpu_to_gpu(res, 0, index)\n    index.add(embeddings)\n    all_distances, all_indices = index.search(embeddings, k)\n\n    def match(distance: float, distance_threshold: float) -> bool:\n        return (\n            distance >= distance_threshold\n            if is_inverted\n            else distance <= distance_threshold\n        )\n    \n    if distance_threshold_regressor is not None:\n        distance_thresholds = np.concatenate(distance_thresholds_list, axis=0)\n        return [\n            np.array([i for distance, i in zip(distances, indices) if match(distance, min(distance_threshold + 0.1, 1.0))])\n            for distances, indices, distance_threshold in zip(all_distances, all_indices, distance_thresholds)\n        ]\n    else:\n        return [\n            np.array([i for distance, i in zip(distances, indices) if match(distance, distance_threshold)])\n            for distances, indices in zip(all_distances, all_indices)\n        ]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_matches(experiment_id: str, test_df: pd.DataFrame, input_images_dir: str, distance_thresholds: List[float], distance_threshold_regressor_path: str = None, tta_steps: int = 0, device: torch.device = torch.device(\"cpu\"), augmenter: Compose = None) -> List[np.ndarray]:\n    display(f\"Predicting using {experiment_id}...\")\n    \n    if distance_threshold_regressor_path:\n        try:\n            distance_threshold_regressor = torch.jit.load(os.path.join(\"/kaggle/input\", distance_threshold_regressor_path)).to(device)\n        except:\n            with open(os.path.join(\"/kaggle/input\", distance_threshold_regressor_path), \"rb\") as f:\n                distance_threshold_regressor = pickle.load(f)\n    else:\n        distance_threshold_regressor = None\n    \n    base_model_path = f\"/kaggle/input/{experiment_id.replace('-', '')}/output/{experiment_id}/models\"\n\n    posting_ids = test_df[\"posting_id\"].values\n    \n    folds_outputs = []\n\n    for n_fold, distance_threshold in zip(sorted(os.listdir(base_model_path)), distance_thresholds):\n        if not os.path.isdir(os.path.join(base_model_path, n_fold)):\n            continue\n            \n        df = test_df.copy()\n        \n        display(f\"Loading model for {n_fold}...\")\n        model_path = os.path.join(base_model_path, n_fold, \"model.pt\")\n        script_model_path = os.path.join(base_model_path, n_fold, \"script_model.pt\")\n        if os.path.exists(script_model_path):\n            display(f\"Found TorchScript model for {n_fold}\")\n            model = torch.load(model_path, map_location=torch.device(\"cpu\")).to(torch.device(\"cpu\"))\n            script_model = torch.jit.load(script_model_path, map_location=device).to(device)\n        else:\n            display(f\"TorchScript model for {n_fold} not found\")\n            model = torch.load(model_path, map_location=device).to(device)\n            script_model = None\n            \n        with open(os.path.join(base_model_path, n_fold, \"image_preprocessor.pkl\"), \"rb\") as f:\n            image_preprocessor = pickle.load(f)\n            \n        if os.path.exists(os.path.join(base_model_path, n_fold, \"text_preprocessor.pkl\")):\n            with open(os.path.join(base_model_path, n_fold, \"text_preprocessor.pkl\"), \"rb\") as f:\n                text_preprocessor = pickle.load(f)\n                text_preprocessor.preprocess_text(df)\n                text_embeddings = text_preprocessor.transform(df)\n                text_embeddings = pad_sequences(text_embeddings, maxlen=15)\n        else:\n            text_embeddings = None\n            \n        if augmenter is None:\n            with open(os.path.join(base_model_path, n_fold, \"augmenter.pkl\"), \"rb\") as f:\n                augmenter = pickle.load(f)\n                \n        with open(os.path.join(base_model_path, n_fold, \"image_preprocessing_function.pkl\"), \"rb\") as f:\n            image_preprocessing_function = pickle.load(f)\n        \n        images_dir = get_images_dir(image_preprocessor)\n        if not os.path.exists(images_dir):\n            #display(\"Preprocessing images...\")\n            os.makedirs(images_dir)\n            #image_preprocessor.preprocess_dir(input_images_dir, images_dir)\n            #display(\"Images preprocessed successfully.\")\n        \n        dataset = ShopeeDataset(\n            df,\n            images_dir,\n            image_preprocessing_function,\n            image_preprocessor=image_preprocessor,\n            original_images_dir=input_images_dir,\n            text_embeddings=text_embeddings,\n            #image_preprocessor=image_preprocessor,\n            augmenter=augmenter if tta_steps else None,\n            test=True,\n        )\n        \n        loader = DataLoader(\n            dataset,\n            batch_size=model.training_config.batch_size * 2,\n            shuffle=False,\n            pin_memory=True,\n            num_workers=os.cpu_count(),\n            prefetch_factor=10,\n        )\n\n        display(\"Running the model...\")\n        index = model._faiss_index_type(model.model_config.embedding_dim if getattr(model.model_config, \"use_output_fc\", True)  else model.get_embedding_dim())\n        match_indices = _predict_matches(\n            script_model if script_model is not None else model,\n            loader,\n            device,\n            index,\n            model.distance.is_inverted,\n            distance_threshold=distance_threshold,\n            distance_threshold_regressor=distance_threshold_regressor\n        )\n        #match_indices = model.predict_matches(dataset, distance_threshold=distance_threshold)\n        matches = [posting_ids[indices].tolist() for indices in match_indices]\n        \n        folds_outputs.append(matches)\n        #folds_outputs.append(predict_proba(model, dataset, tta_steps=tta_steps))\n        display(\"Model ran successfully.\")\n        \n    return folds_outputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/shopee-product-matching/test.csv\")\ninput_images_dir = \"/kaggle/input/shopee-product-matching/test_images\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df = pd.read_csv(\"/kaggle/input/shopee-product-matching/train.csv\")\n#input_images_dir = \"/kaggle/input/shopee-product-matching/train_images\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"experiment_tta_matches = [\n    predict_matches(experiment_id, df, input_images_dir, distance_thresholds=exp_distance_thresholds, distance_threshold_regressor_path=distance_threshold_regressor_path, device=torch.device(\"cuda\"))\n    for experiment_id, exp_distance_thresholds, distance_threshold_regressor_path in zip(experiment_ids, distance_thresholds, distance_threshold_regressor_paths)\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_matches = [matches for tta_matches in experiment_tta_matches for matches in tta_matches]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_dataset():\n    df_cu = cudf.DataFrame(df)\n    image_paths = '../input/shopee-product-matching/test_images/' + df['image']\n    return df_cu, image_paths","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cachedStopWords = stopwords.words(\"indonesian\")\ndef clean_numbers(x):\n\n    x = re.sub('[0-9]', '0', x)\n    return x\n\n\ndef clean_characters(x):\n    print(\"Cleaning\")\n\n    x = str(x)\n    for punct in \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&':\n        x = x.replace(punct, ' ')\n    return x\n\ndef normalize_title(title):\n    return unicodedata.normalize('NFKD', title.lower()).encode('ASCII', 'ignore').decode('utf8')\n\n\ndef remove_stopwords(title):\n\n    return ' '.join([word for word in title.split() if word not in cachedStopWords])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_text(dataframe):\n    if apply_lower_case:\n        print(\"Applying lower case\")\n        dataframe[\"title\"] = dataframe[\"title\"].str.lower()\n    \n    if apply_clean_numbers:\n        print(\"Applying clean numbers\")\n        dataframe[\"title\"] = dataframe[\"title\"].apply(\n            lambda x: clean_numbers(x)\n        )\n\n    if apply_clean_characters:\n        print(\"Applying clean characters\")\n        dataframe[\"title\"] = dataframe[\"title\"].apply(\n            lambda x: clean_characters(x)\n        )\n\n    if apply_normalize_title:\n        print(\"Applying normalize title\")\n        dataframe[\"title\"] = dataframe[\"title\"].apply(\n            lambda x: normalize_title(x)\n        )\n        \n    if apply_remove_stopwords:\n        print(\"Removing stop words\")\n        dataframe[\"title\"] = dataframe[\"title\"].apply(\n            lambda x: remove_stopwords(x)\n        )\n    return dataframe","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = preprocess_text(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_cu,image_paths = read_dataset()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_text_predictions(df, max_features = 10000):\n    \n    model = TfidfVectorizer(stop_words = 'english', binary = True, max_features = max_features)\n    text_embeddings = model.fit_transform(df_cu['title']).toarray()\n    preds = []\n    CHUNK = 1024*4\n\n    print('Finding similar titles...')\n    CTS = len(df)//CHUNK\n    if len(df)%CHUNK!=0: CTS += 1\n    for j in range( CTS ):\n\n        a = j*CHUNK\n        b = (j+1)*CHUNK\n        b = min(b,len(df))\n        print('chunk',a,'to',b)\n\n        # COSINE SIMILARITY DISTANCE\n        cts = cupy.matmul( text_embeddings, text_embeddings[a:b].T).T\n\n        for k in range(b-a):\n            IDX = cupy.where(cts[k,]>0.75)[0]\n            o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n            preds.append(o)\n    \n    del model,text_embeddings\n    gc.collect()\n    return preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf_idf_matches = get_text_predictions(df, max_features = text_embedding_dim)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\nfrom collections import Counter\nfrom typing import List, Union\n\ndef _intersection(*matches: List[Union[int, str]]) -> List[Union[int, str]]:\n    sets = iter(map(set, matches))\n    result = next(sets)\n    for s in sets:\n        result = result.intersection(s)\n    return list(result)\n\n\ndef _union(*matches: List[Union[int, str]]) -> List[Union[int, str]]:\n    sets = iter(map(set, matches))\n    result = next(sets)\n    for s in sets:\n        result = result.union(s)\n    return list(result)\n\n\ndef ensemble(\n    model_matches: List[List[Union[int, str]]],\n    num_models: int = None,\n    ensemble_method: EnsembleMethod = EnsembleMethod.CONSENSUS,\n) -> List[Union[int, str]]:\n    if num_models is None:\n        num_models = len(model_matches)\n\n    if ensemble_method == EnsembleMethod.CONSENSUS:\n        counter = Counter([match for matches in model_matches for match in matches])\n        cutoff = math.ceil(num_models / 2)\n        return [match for match, count in counter.items() if count >= cutoff]\n    elif ensemble_method == EnsembleMethod.UNANIMOUS:\n        return _intersection(*model_matches)\n    else:\n        return _union(*model_matches)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if tf_idf_embedding_strategy == \"union_after_ensemble\": # [\"union_after_ensemble\", \"part_of_ensemble\"]\n    with Pool(os.cpu_count()) as pool:\n        ensembled_matches = pool.map(functools.partial(ensemble, ensemble_method=EnsembleMethod[ensemble_method]), list(zip(*all_matches)))\n        ensembled_matches = pool.map(functools.partial(ensemble, ensemble_method=EnsembleMethod.AFFIRMATIVE), list(zip(ensembled_matches, tf_idf_matches)))\nelse:\n    all_matches.append(tf_idf_matches)\n    with Pool(os.cpu_count()) as pool:\n        ensembled_matches = pool.map(functools.partial(ensemble, ensemble_method=EnsembleMethod[ensemble_method]), list(zip(*all_matches)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = df[[\"posting_id\"]]\nsubmission_df[\"matches\"] = [\" \".join(matches) for matches in ensembled_matches]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}