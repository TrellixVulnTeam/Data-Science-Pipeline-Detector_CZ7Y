{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\n\nimport timm\nfrom tqdm import tqdm\nimport math\nimport random\nimport os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.neighbors import NearestNeighbors\n\n# Visuals and CV2\nimport cv2\n\n# albumentations for augs\nimport albumentations\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torch\nimport timm\nimport torch.nn as nn\nfrom torch.nn import Parameter\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset,DataLoader\n\nfrom torchvision import datasets, transforms","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Config","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IM_FOLDER = '../input/shopee-product-matching/test_images'\nMODEL_PATH = '../input/shopeekfoldevaluation/kfolds_strategies_evaluation/kfolds_strategies_evaluation/train_n_lbgr_88_3fold/Fold02_Valid0.725_Train0.724_Ep003.pth'\n\nFOIS = [0, 1, 2]\nSAMPLE = None\n\nDIM = (512,512)\nEMB_SIZE = 1536\n\nNUM_WORKERS = 4\nTRAIN_BATCH_SIZE = 16\nVALID_BATCH_SIZE = 16\nSEED = 2020\nLR = 3e-4\n\n################################################# MODEL ####################################################################\nmodel_name = 'efficientnet_b3' #efficientnet_b0-b7\n\n################################################ Metric Loss and its params #######################################################\nloss_module = 'arcface' #'cosface' #'adacos'\ns = 30.0\nm = 0.5 \nls_eps = 0.0\neasy_margin = False\n\n############################################## Model Params ###############################################################\nmodel_params = {\n    'model_name':'efficientnet_b3',\n    'use_fc':False,\n    'fc_dim':512,\n    'dropout':0.0,\n    'loss_module':loss_module,\n    's':30.0,\n    'margin':28.6, # degree (0.5 radian)\n    'ls_eps':0.0,\n    'theta_zero':0.785,\n    'pretrained':None\n}\n\n########### Device ###########\nDEVICE = torch.device(\"cuda\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_valid_transforms():\n\n    return albumentations.Compose(\n        [\n            albumentations.Resize(DIM[0],DIM[1],always_apply=True),\n            albumentations.Normalize(),\n        ToTensorV2(p=1.0)\n        ]\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ShopeeDataset(Dataset):\n    def __init__(self, csv, transforms=None):\n\n        self.csv = csv.reset_index()\n        self.augmentations = transforms\n        \n        if('label_group' in self.csv.columns):\n            self.is_test = False\n        else:\n            self.is_test = True\n\n    def __len__(self):\n        return self.csv.shape[0]\n\n    def __getitem__(self, index):\n        row = self.csv.iloc[index]\n        \n        text = row.title\n        \n        image = cv2.imread(row.filepath)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.augmentations:\n            augmented = self.augmentations(image=image)\n            image = augmented['image']       \n        \n        if(self.is_test):\n            return image\n        else:\n            return image, torch.tensor(row.label_group)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ShopeeNet(nn.Module):\n\n    def __init__(self,\n                 model_name='efficientnet_b0',\n                 use_fc=False,\n                 fc_dim=512,\n                 dropout=0.0,\n                 loss_module='softmax',\n                 s=30.0,\n                 margin=0.50,\n                 ls_eps=0.0,\n                 theta_zero=0.785,\n                 pretrained=None):\n        \"\"\"\n        :param n_classes:\n        :param model_name: name of model from pretrainedmodels\n            e.g. resnet50, resnext101_32x4d, pnasnet5large\n        :param loss_module: One of ('arcface', 'cosface', 'softmax')\n        \"\"\"\n        super(ShopeeNet, self).__init__()\n        print('Building Model Backbone for {} model'.format(model_name))\n\n        self.backbone = timm.create_model(model_name, pretrained=False)\n        if(pretrained):\n            print('Loading pretrained model from:', pretrained)\n            self.backbone.load_state_dict(torch.load(pretrained, map_location='cpu'))\n            \n        final_in_features = self.backbone.classifier.in_features\n        \n        self.backbone.classifier = nn.Identity()\n        self.backbone.global_pool = nn.Identity()\n        \n        self.pooling =  nn.AdaptiveAvgPool2d(1)\n            \n        self.use_fc = use_fc\n        if use_fc:\n            print('use_fc')\n            self.dropout = nn.Dropout(p=dropout)\n            self.fc = nn.Linear(final_in_features, fc_dim)\n            self.bn = nn.BatchNorm1d(fc_dim)\n            self._init_params()\n            final_in_features = fc_dim\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        x = self.backbone(x)\n        x = self.pooling(x).view(batch_size, -1)\n\n        if self.use_fc:\n            x = self.dropout(x)\n            x = self.fc(x)\n            x = self.bn(x)\n        \n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Load model","metadata":{}},{"cell_type":"code","source":"# Test loading model properly\nmodel = ShopeeNet(**model_params)\nmodel.to(DEVICE)\nmodel.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Predict","metadata":{}},{"cell_type":"code","source":"def getMetric(col):\n    def f1score(row):\n        n = len(np.intersect1d(row.target,row[col]))\n        return 2*n / (len(row.target)+len(row[col]))\n    return f1score\n\ndef compute_f1(df, pred_col='preds'):\n    res_df = df.copy()\n    target_dict = res_df.groupby('label_group').posting_id.agg('unique').to_dict()\n    res_df['target'] = res_df.label_group.map(target_dict)\n    res_df['f1'] = res_df.apply(getMetric(pred_col),axis=1)\n    return res_df.f1.mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_fn(dataloader, df, model, k=50, metric='cosine', threshold=0.4):\n    emb_list = []\n    for images in tqdm(dataloader):\n        if(type(images) == tuple or type(images) == list):\n            images = images[0]\n        images = images.to(DEVICE)\n        embeddings = model(images).detach().cpu().numpy()\n        \n        # l2 norm\n        embeddings /= np.linalg.norm(embeddings, 2, axis=1, keepdims=True)\n        \n        emb_list.append(embeddings)\n        \n    emb_vectors = np.vstack(emb_list)\n    print(emb_vectors.shape)\n    \n    model = NearestNeighbors(n_neighbors=k, metric=metric)\n    model.fit(emb_vectors)\n    distances, indices = model.kneighbors(emb_vectors)\n\n    res_df = df.copy()\n    preds = []\n    for dist, inds in tqdm(zip(distances, indices)):\n        IDX = np.where(dist<threshold)[0]\n        IDS = inds[IDX]\n        o = df.iloc[IDS].posting_id.values\n        preds.append(o)\n            \n    res_df['preds'] = preds\n    \n    return res_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('../input/shopee-product-matching/test.csv')\ntest_df['filepath'] = test_df['image'].apply(lambda x: os.path.join(IM_FOLDER, x))\n\ntest_dataset = ShopeeDataset(csv=test_df, transforms=get_valid_transforms(),)\ntest_loader = torch.utils.data.DataLoader(test_dataset,batch_size=VALID_BATCH_SIZE,num_workers=NUM_WORKERS,\n                                               shuffle=False,pin_memory=True,drop_last=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"K = 50 if(50 < len(test_df)) else len(test_df)\n    \ntest_res_df =  predict_fn(test_loader, test_df, model, k=K)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_res_df['matches'] = test_res_df['preds'].map(lambda x: ' '.join(x.tolist()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = test_res_df[['posting_id', 'matches']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# valid_df = pd.read_csv('../input/shopeekfoldevaluation/train_vanila_88_3fold.csv').sort_values('label_group')\n# valid_df = valid_df[valid_df.fold==0]\n# valid_df['filepath'] = valid_df['image'].apply(lambda x: os.path.join('../input/shopee-product-matching/train_images', x))\n\n# le = LabelEncoder()\n# valid_df['label_group'] = le.fit_transform(valid_df.label_group)\n\n# valid_dataset = ShopeeDataset(csv=valid_df, transforms=get_valid_transforms(),)\n# valid_loader = torch.utils.data.DataLoader(valid_dataset,batch_size=16,num_workers=NUM_WORKERS,\n#                                                shuffle=False,pin_memory=True,drop_last=False)\n\n# valid_res_df = predict_fn(valid_loader, valid_df, model)\n\n# compute_f1(valid_res_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# all_df = pd.read_csv('../input/shopeekfoldevaluation/train_vanila_88_3fold.csv').sort_values('label_group')\n# all_df['filepath'] = all_df['image'].apply(lambda x: os.path.join('../input/shopee-product-matching/train_images', x))\n\n# le = LabelEncoder()\n# all_df['label_group'] = le.fit_transform(all_df.label_group)\n\n# all_dataset = ShopeeDataset(csv=all_df, transforms=get_valid_transforms(),)\n# all_loader = torch.utils.data.DataLoader(all_dataset,batch_size=16,num_workers=NUM_WORKERS,\n#                                                shuffle=False,pin_memory=True,drop_last=False)\n\n# all_res_df = predict_fn(all_loader, all_df, model)\n\n# compute_f1(all_res_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# a = next(iter(all_loader))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}