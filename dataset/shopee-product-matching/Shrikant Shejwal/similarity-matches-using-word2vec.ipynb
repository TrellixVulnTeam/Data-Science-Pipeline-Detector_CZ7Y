{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **This notebook is about finding similarity between two titles of the product using word2vec and cosine similarity. This feature can be used with image to find the similar product.**","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\nimport numpy as np\nnp.random.seed(2018)\nfrom gensim.models import Word2Vec\nimport nltk\nnltk.download('wordnet')\nstemmer = SnowballStemmer('english')\n\nfrom numpy import dot\nfrom numpy.linalg import norm\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-09T09:32:26.793065Z","iopub.execute_input":"2022-02-09T09:32:26.793984Z","iopub.status.idle":"2022-02-09T09:32:29.353161Z","shell.execute_reply.started":"2022-02-09T09:32:26.793824Z","shell.execute_reply":"2022-02-09T09:32:29.351865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/shopee-product-matching/train.csv')\ntest_df = pd.read_csv('../input/shopee-product-matching/test.csv')\nDATA_PATH = '../input/shopee-product-matching/'","metadata":{"execution":{"iopub.status.busy":"2022-02-09T09:32:29.355377Z","iopub.execute_input":"2022-02-09T09:32:29.355737Z","iopub.status.idle":"2022-02-09T09:32:29.607523Z","shell.execute_reply.started":"2022-02-09T09:32:29.355701Z","shell.execute_reply":"2022-02-09T09:32:29.606419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Add target column in the dataframe","metadata":{}},{"cell_type":"code","source":"train_df['image'] = DATA_PATH + 'train_images/' + train_df['image']\ntmp = train_df.groupby('label_group').posting_id.agg('unique').to_dict()\ntrain_df['target'] = train_df.label_group.map(tmp)\ntrain_df","metadata":{"execution":{"iopub.status.busy":"2022-02-09T09:32:29.608952Z","iopub.execute_input":"2022-02-09T09:32:29.609309Z","iopub.status.idle":"2022-02-09T09:32:30.578715Z","shell.execute_reply.started":"2022-02-09T09:32:29.609274Z","shell.execute_reply":"2022-02-09T09:32:30.577674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['title'][5]\n","metadata":{"execution":{"iopub.status.busy":"2022-02-09T09:32:30.580052Z","iopub.execute_input":"2022-02-09T09:32:30.580467Z","iopub.status.idle":"2022-02-09T09:32:30.587821Z","shell.execute_reply.started":"2022-02-09T09:32:30.580429Z","shell.execute_reply":"2022-02-09T09:32:30.586891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning","metadata":{}},{"cell_type":"code","source":"\ndef lemmatize_stemming(text):\n    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n\ndef preprocess(text):\n    result = []\n    for token in gensim.utils.simple_preprocess(text):\n        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n            if token == 'xxxx':\n                continue\n            result.append(lemmatize_stemming(token))\n    \n    return result","metadata":{"execution":{"iopub.status.busy":"2022-02-09T09:32:30.590788Z","iopub.execute_input":"2022-02-09T09:32:30.59142Z","iopub.status.idle":"2022-02-09T09:32:30.604097Z","shell.execute_reply.started":"2022-02-09T09:32:30.591378Z","shell.execute_reply":"2022-02-09T09:32:30.602592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed_docs = train_df['title'].map(preprocess)\nprocessed_docs =list(processed_docs)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-09T09:32:30.605856Z","iopub.execute_input":"2022-02-09T09:32:30.60651Z","iopub.status.idle":"2022-02-09T09:32:40.239679Z","shell.execute_reply.started":"2022-02-09T09:32:30.606464Z","shell.execute_reply":"2022-02-09T09:32:40.238387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed_docs[:10] # clean document","metadata":{"execution":{"iopub.status.busy":"2022-02-09T09:32:40.241813Z","iopub.execute_input":"2022-02-09T09:32:40.242307Z","iopub.status.idle":"2022-02-09T09:32:40.253577Z","shell.execute_reply.started":"2022-02-09T09:32:40.242257Z","shell.execute_reply":"2022-02-09T09:32:40.251947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Word2vec model\n\nI choose embedding dim of size 50. This means that each word will be represented by a vector of size 50","metadata":{}},{"cell_type":"code","source":"def word2vec_model():\n    w2v_model = Word2Vec(min_count=1,\n                     window=3,\n                     vector_size=50,\n                     sample=6e-5, \n                     alpha=0.03, \n                     min_alpha=0.0007, \n                     negative=20)\n    \n    w2v_model.build_vocab(processed_docs)\n    w2v_model.train(processed_docs, total_examples=w2v_model.corpus_count, epochs=300, report_delay=1)\n    \n    return w2v_model","metadata":{"execution":{"iopub.status.busy":"2022-02-09T09:32:40.255564Z","iopub.execute_input":"2022-02-09T09:32:40.256028Z","iopub.status.idle":"2022-02-09T09:32:40.266478Z","shell.execute_reply.started":"2022-02-09T09:32:40.255973Z","shell.execute_reply":"2022-02-09T09:32:40.265393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w2v_model = word2vec_model()\nw2v_model.save('word2vec_model')","metadata":{"execution":{"iopub.status.busy":"2022-02-09T09:32:40.26763Z","iopub.execute_input":"2022-02-09T09:32:40.267971Z","iopub.status.idle":"2022-02-09T09:34:54.641822Z","shell.execute_reply.started":"2022-02-09T09:32:40.267936Z","shell.execute_reply":"2022-02-09T09:34:54.640646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Getting embedding vector","metadata":{}},{"cell_type":"code","source":"emb_vec = w2v_model.wv","metadata":{"execution":{"iopub.status.busy":"2022-02-09T09:34:54.643014Z","iopub.execute_input":"2022-02-09T09:34:54.643638Z","iopub.status.idle":"2022-02-09T09:34:54.648198Z","shell.execute_reply.started":"2022-02-09T09:34:54.643594Z","shell.execute_reply":"2022-02-09T09:34:54.647293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emb_vec['anak'] # It will return vector representation of the word anak","metadata":{"execution":{"iopub.status.busy":"2022-02-09T09:34:54.64936Z","iopub.execute_input":"2022-02-09T09:34:54.649813Z","iopub.status.idle":"2022-02-09T09:34:54.668185Z","shell.execute_reply.started":"2022-02-09T09:34:54.649778Z","shell.execute_reply":"2022-02-09T09:34:54.666967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Finding similarity between two vector using cosine similarity","metadata":{}},{"cell_type":"code","source":"\ndef find_similarity(sen1, sen2, model):\n    p_sen1 = preprocess(sen1)\n    p_sen2 = preprocess(sen2)\n    \n    sen_vec1 = np.zeros(50)\n    sen_vec2 = np.zeros(50)\n    for val in p_sen1:\n        sen_vec1 = np.add(sen_vec1, model[val])\n\n    for val in p_sen2:\n        sen_vec2 = np.add(sen_vec2, model[val])\n    \n    return dot(sen_vec1,sen_vec2)/(norm(sen_vec1)*norm(sen_vec2))\n    ","metadata":{"execution":{"iopub.status.busy":"2022-02-09T09:34:54.669804Z","iopub.execute_input":"2022-02-09T09:34:54.670137Z","iopub.status.idle":"2022-02-09T09:34:54.6814Z","shell.execute_reply.started":"2022-02-09T09:34:54.670107Z","shell.execute_reply":"2022-02-09T09:34:54.680202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"find_similarity('Atasan Rajut Wanita LISDIA SWEATER', 'Atasan Rajut Wanita LISDIA',emb_vec )","metadata":{"execution":{"iopub.status.busy":"2022-02-09T09:34:54.683453Z","iopub.execute_input":"2022-02-09T09:34:54.683912Z","iopub.status.idle":"2022-02-09T09:34:54.698312Z","shell.execute_reply.started":"2022-02-09T09:34:54.683876Z","shell.execute_reply":"2022-02-09T09:34:54.697046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"find_similarity('Atasan Rajut Wanita LISDIA SWEATER', 'CELANA WANITA  (BB 45-84 KG)Harem wanita (bisa cod)',emb_vec )","metadata":{"execution":{"iopub.status.busy":"2022-02-09T09:34:54.699966Z","iopub.execute_input":"2022-02-09T09:34:54.700444Z","iopub.status.idle":"2022-02-09T09:34:54.711252Z","shell.execute_reply.started":"2022-02-09T09:34:54.700389Z","shell.execute_reply":"2022-02-09T09:34:54.710268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fell free to use this notebook and please upvote if you like the work.\n# Thank You","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}