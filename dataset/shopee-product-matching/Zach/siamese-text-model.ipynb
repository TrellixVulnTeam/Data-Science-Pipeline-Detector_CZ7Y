{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport keras\nimport tensorflow as tf \nfrom math import ceil\nfrom tqdm import tqdm\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import FunctionTransformer, OrdinalEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n#import efficientnet.tfkeras as efn\n\nIMAGE_SIZE = [512, 512]\nN_CLASSES = 11014\nCHANNELS = 3\n\n# References\n# https://www.kaggle.com/ragnar123/unsupervised-baseline-arcface/data\n\n# Load the data\ndf_train = pd.read_csv('../input/shopee-product-matching/train.csv')\ndf_train_pairs = pd.read_csv('../input/pairwise-dataset/pairwise.csv')\ndf_test = pd.read_csv('../input/shopee-product-matching/test.csv')\n\n#Full image path\ndf_train['image'] = '../input/train_images/' + df_train['image']\ndf_train_pairs['image_1'] = '../input/train_images/' + df_train_pairs['image_1']\ndf_train_pairs['image_2'] = '../input/train_images/' + df_train_pairs['image_2']\ndf_test['image'] = '../input/test_images/' + df_test['image']\n\nassert len(df_train['label_group'].unique()) == N_CLASSES\nassert len(df_train_pairs['label_group_1'].unique()) == N_CLASSES\nassert len(df_train_pairs['label_group_2'].unique()) == N_CLASSES","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"####################################################################\n# Add matches for validation\n####################################################################\ndef add_matches(df):\n  tmp = df.groupby(['label_group'])['posting_id'].unique().to_dict()\n  df['matches'] = df['label_group'].map(tmp)\n  # df['matches'] = df['matches'].apply(lambda x: ' '.join(x))\n  return df\n\ndf_train = add_matches(df_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"####################################################################\n# Text Preprocessing\n####################################################################\n\n# Imputer for text\ndef string_impute(x):\n  return x.fillna('missing').astype(str)\n\nimputer = FunctionTransformer(string_impute, validate=False)\n\ntext_word_pipeline = make_pipeline(\n  imputer,\n  TfidfVectorizer(\n    strip_accents='unicode',\n    lowercase=True,\n    analyzer='word',\n    token_pattern=r\"(?u)\\b\\w+\\b\",  # Default is \"2 or more letters\", lets use 1 letter\n    ngram_range=(1, 3),\n    max_df=.5,\n    min_df=10,\n    binary=False,\n    norm='l2',\n    use_idf=True,\n    smooth_idf=True,\n    sublinear_tf=False,\n    dtype=np.float32\n  ),\n  'passthrough'\n)\n\ntext_train = text_word_pipeline.fit_transform(df_train['title'])\ntext_test = text_word_pipeline.transform(df_test['title'])\n\ntext_train_pairs_1 = text_word_pipeline.transform(df_train_pairs['title_1'])\ntext_train_pairs_2 = text_word_pipeline.transform(df_train_pairs['title_2'])\n\n# Sort indexes\ntext_train.sort_indices()\ntext_test.sort_indices()\n\ntext_train_pairs_1.sort_indices()\ntext_train_pairs_2.sort_indices()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"####################################################################\n# Define models\n####################################################################\n\ndef build_cosine_sim_model(\n  input_shape,\n  sparse_input,\n  embed_layer,\n  opt,\n  **kwargs\n):\n  \"\"\"\n  Class to build a model that compares 2 inputs using cosine similarity\n  \n  input_shape = shape of the inputs\n  \n  sparse_input = True or False.  Only workers if layer_source==keras.layers\n  \n  layer_source = keras or tf.keras.  Keras supports sparse input, tf.keras supports images\n  \n  embed_layer = A function to generate the shared layer to use to compare the 2 inputs\n  \n  opt = optimizer to use for mode\n  \n  **kwargs = passed to embed_layer\n  \"\"\"\n  \n  # 2 inputs\n  input_1 = tf.keras.layers.Input(shape = input_shape, name = 'input_1', sparse=sparse_input)\n  input_2 = tf.keras.layers.Input(shape = input_shape, name = 'input_2', sparse=sparse_input)\n  \n  # Pass both inputs through the SAME dense layer to map them to a shared embedding\n  shared_embed = embed_layer(**kwargs)\n  embed_1 = shared_embed(input_1)\n  embed_2 = shared_embed(input_2)\n  \n  # Use the Dot layer to compate them\n  # Dot with normalize = True computes cosine similarity\n  sim = tf.keras.layers.Dot(-1, normalize=True, name='sim')([embed_1, embed_2])\n\n  # Make a model that inputs 2 texts and outputs similarity\n  model = tf.keras.models.Model(inputs=[input_1, input_2], outputs = sim)\n  \n  # Compile the model\n  # Consider CosineSimilarity() loss too\n  model.compile(optimizer = opt, loss = [tf.keras.losses.MeanSquaredError()])\n  model.summary()\n  return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"####################################################################\n# Fit text model\n####################################################################\nfrom annoy import AnnoyIndex\nfrom sklearn.preprocessing import normalize\n\nEMBED_DIM = 512\n\n# Build text model\ndef build_text_model():\n  return build_cosine_sim_model(\n    input_shape=(text_train.shape[1]),\n    sparse_input=True,\n    embed_layer=tf.keras.layers.Dense,\n    opt=tf.keras.optimizers.Adam(1e-4),\n    units=EMBED_DIM,\n    use_bias=False,\n    name='shared_embed')\n\ntext_model = build_text_model()\n\nx = {'input_1': text_train_pairs_1, 'input_2': text_train_pairs_2}\ny = df_train_pairs['target'].to_numpy()\n\n# Fit model\ntext_model.fit(\n  x, y, \n  epochs=6,\n  batch_size=256)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"####################################################################\n# Predict text model\n####################################################################\n\n# Predict embeddings\n# TODO: make this an actual MODEL when we define the fit\ntext_embeddings_model = tf.keras.models.Model(\n  inputs = text_model.inputs[0], \n  outputs = text_model.layers[-2].output)\n  \n# Function to map text to pairs in a given data frame\ndef text_to_matches(df, seed=42):\n  \n  # Use sklearn to vectorize the text\n  text_sparse = text_word_pipeline.transform(df['title'])\n  text_sparse.sort_indices()\n  \n  # Now use our embedding model to turn the sparse text vectors into a dense embedding\n  text_embed = text_embeddings_model.predict(text_sparse, batch_size=256)\n  text_embed = normalize(text_embed)\n  \n  # Build a knn model for matches\n  knn_model = AnnoyIndex(EMBED_DIM, 'dot')  \n  for i in range(text_embed.shape[0]):\n      knn_model.add_item(i, text_embed[i,:])\n  knn_model.set_seed(42)\n  knn_model.build(100)\n  \n  # Loop over each row and match\n  all_neighbors = []\n  all_dist = []\n  N = 50\n  for i in tqdm(range(text_embed.shape[0])):\n    neighbors, sim = knn_model.get_nns_by_vector(text_embed[i,:], n=N, include_distances=True)\n    neighbors = np.asarray(neighbors)\n    sim = np.asarray(sim)\n    \n    all_neighbors.append([df['posting_id'].iloc[x] for x in neighbors])\n    all_dist.append(sim)\n    \n  df['all_neighbors'] = all_neighbors\n  df['all_dist'] = all_dist\n  return df\n\ndf_train = text_to_matches(df_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"####################################################################\n# Find a good threshold\n####################################################################\nfrom scipy.optimize import minimize, minimize_scalar\n\ndef f1_score(y_true, y_pred):\n    intersection = len(np.intersect1d(y_true, y_pred))\n    len_y_pred = y_true.shape[0]\n    len_y_true = y_pred.shape[0]\n    f1 = 2 * intersection / (len_y_pred + len_y_true)\n    return f1\n\ndef matches_above_thresh(thresh, df):\n  y_pred = []\n  for i in range(df.shape[0]):\n    dist = df['all_dist'][i]\n    nn = np.array(df['all_neighbors'][i])\n    out = nn[np.where(dist > thresh)[0]]\n    if len(out) < 2:\n      out = nn[0:2]\n    y_pred.append(out)\n    \n  return y_pred\n    \ndef calc_f1(thresh, df=df_train):\n  \n  y_true = df['matches']\n  y_pred = matches_above_thresh(thresh, df)\n  \n  assert len(y_true) == len(y_pred)\n  \n  f1 = 0\n  for i in range(len(y_true)):\n    f1 += f1_score(y_true[i], y_pred[i])\n  f1 = f1 / len(y_true) \n\n  return -1 * f1\n  \ncalc_f1(-1)\ncalc_f1(0)\ncalc_f1(.5)\ncalc_f1(.8)\ncalc_f1(1)\n\nbest_thresh = minimize_scalar(calc_f1)\nprint(best_thresh['x'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"####################################################################\n# Submit\n####################################################################\n\ndf_test = text_to_matches(df_test)\nmatches_test = matches_above_thresh(best_thresh['x'], df_test)\ndf_test['matches'] = [' '.join(x) for x in matches_test]\ndf_test[['posting_id', 'matches']].to_csv('submission.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}