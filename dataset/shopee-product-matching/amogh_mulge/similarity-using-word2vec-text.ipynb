{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **This notebook is about finding similarity between two titles of the product using word2vec and cosine similarity. This feature can be used with image to find the similar product.**","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\nimport numpy as np\nnp.random.seed(2018)\nfrom gensim.models import Word2Vec\nimport nltk\nnltk.download('wordnet')\nstemmer = SnowballStemmer('english')\n\nfrom numpy import dot\nfrom numpy.linalg import norm\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/shopee-product-matching/train.csv')\ntest_df = pd.read_csv('../input/shopee-product-matching/test.csv')\nDATA_PATH = '../input/shopee-product-matching/'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Add target column in the dataframe","metadata":{}},{"cell_type":"code","source":"train_df['image'] = DATA_PATH + 'train_images/' + train_df['image']\ntmp = train_df.groupby('label_group').posting_id.agg('unique').to_dict()\ntrain_df['target'] = train_df.label_group.map(tmp)\ntrain_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['title'][5]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning","metadata":{}},{"cell_type":"code","source":"\ndef lemmatize_stemming(text):\n    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n\ndef preprocess(text):\n    result = []\n    for token in gensim.utils.simple_preprocess(text):\n        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n            if token == 'xxxx':\n                continue\n            result.append(lemmatize_stemming(token))\n    \n    return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed_docs = train_df['title'].map(preprocess)\nprocessed_docs =list(processed_docs)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed_docs[:10] # clean document","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Word2vec model\n\nI choose embedding dim of size 50. This means that each word will be represented by a vector of size 50","metadata":{}},{"cell_type":"code","source":"def word2vec_model():\n    w2v_model = Word2Vec(min_count=1,\n                     window=3,\n                     vector_size=50,\n                     sample=6e-5, \n                     alpha=0.03, \n                     min_alpha=0.0007, \n                     negative=20)\n    \n    w2v_model.build_vocab(processed_docs)\n    w2v_model.train(processed_docs, total_examples=w2v_model.corpus_count, epochs=300, report_delay=1)\n    \n    return w2v_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w2v_model = word2vec_model()\nw2v_model.save('word2vec_model')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Getting embedding vector","metadata":{}},{"cell_type":"code","source":"emb_vec = w2v_model.wv","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emb_vec['anak'] # It will return vector representation of the word anak","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Finding similarity between two vector using cosine similarity","metadata":{}},{"cell_type":"code","source":"\ndef find_similarity(sen1, sen2, model):\n    p_sen1 = preprocess(sen1)\n    p_sen2 = preprocess(sen2)\n    \n    sen_vec1 = np.zeros(50)\n    sen_vec2 = np.zeros(50)\n    for val in p_sen1:\n        sen_vec1 = np.add(sen_vec1, model[val])\n\n    for val in p_sen2:\n        sen_vec2 = np.add(sen_vec2, model[val])\n    \n    return dot(sen_vec1,sen_vec2)/(norm(sen_vec1)*norm(sen_vec2))\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"find_similarity('Bubble Wrap ( Hanya tambahan packing)', 'Bubble wrap',emb_vec )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"find_similarity('Atasan Rajut Wanita LISDIA SWEATER', 'CELANA WANITA  (BB 45-84 KG)Harem wanita (bisa cod)',emb_vec )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def combine_for_sub(row):\n    x = np.concatenate([row.preds,row.preds2, row.preds3])\n    return ' '.join( np.unique(x) )\n\ndef combine_for_cv(row):\n    x = np.concatenate([row.preds,row.preds2, row.preds3])\n    return np.unique(x)\nif COMPUTE_CV:\n    tmp = test.groupby('label_group').posting_id.agg('unique').to_dict()\n    test['target'] = test.label_group.map(tmp)\n    test['oof'] = test.apply(combine_for_cv,axis=1)\n    test['f1'] = test.apply(getMetric('oof'),axis=1)\n    print('Accuracy or CV Score =', test.f1.mean() )\n\ntest['matches'] = test.apply(combine_for_sub,axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Accuracy or CV Score = 0.9248077230326005\n","metadata":{}},{"cell_type":"markdown","source":"# Fell free to use this notebook and please upvote if you like the work.\n# Thank You","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}