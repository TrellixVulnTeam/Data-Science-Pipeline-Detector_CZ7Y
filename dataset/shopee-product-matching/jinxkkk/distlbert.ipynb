{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Load Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nimport torch\nimport transformers as ppb\nimport warnings\nwarnings.filterwarnings('ignore')\nimport tqdm\nfrom sklearn.neighbors import NearestNeighbors","metadata":{"execution":{"iopub.status.busy":"2021-11-21T13:22:12.745414Z","iopub.execute_input":"2021-11-21T13:22:12.745915Z","iopub.status.idle":"2021-11-21T13:22:14.954721Z","shell.execute_reply.started":"2021-11-21T13:22:12.745813Z","shell.execute_reply":"2021-11-21T13:22:14.953889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# root = '../input'","metadata":{"execution":{"iopub.status.busy":"2021-11-21T13:22:21.913192Z","iopub.execute_input":"2021-11-21T13:22:21.913455Z","iopub.status.idle":"2021-11-21T13:22:21.917239Z","shell.execute_reply.started":"2021-11-21T13:22:21.913426Z","shell.execute_reply":"2021-11-21T13:22:21.916386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # download the pretrained model and save it to local\n\n# # load the pretrained model and weights\n# model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n\n# # Load pretrained model/tokenizer\n# tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n# model = model_class.from_pretrained(pretrained_weights)\n\n# # model.save_pretrained(root)\n# # tokenizer.save_pretrained(root)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T13:22:24.577652Z","iopub.execute_input":"2021-11-21T13:22:24.578343Z","iopub.status.idle":"2021-11-21T13:22:24.582292Z","shell.execute_reply.started":"2021-11-21T13:22:24.578301Z","shell.execute_reply":"2021-11-21T13:22:24.581127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load the train data\n# path1 = root + '/shopee-product-matching/train.csv'\n# path2 = root + '/shopee-product-matching/test.csv'\n# train = pd.read_csv(path1)\n# df_test = test = pd.read_csv(path2)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T13:22:28.820347Z","iopub.execute_input":"2021-11-21T13:22:28.821021Z","iopub.status.idle":"2021-11-21T13:22:28.824332Z","shell.execute_reply.started":"2021-11-21T13:22:28.820982Z","shell.execute_reply":"2021-11-21T13:22:28.823473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# divide train set into train(80%) and dev(20%) sets\n\n# df_dev = train.sample(frac=0.2)\n# df_train = train[~train.index.isin(df_dev.index)]","metadata":{"execution":{"iopub.status.busy":"2021-11-21T13:22:29.769161Z","iopub.execute_input":"2021-11-21T13:22:29.771963Z","iopub.status.idle":"2021-11-21T13:22:29.776371Z","shell.execute_reply.started":"2021-11-21T13:22:29.771907Z","shell.execute_reply":"2021-11-21T13:22:29.77566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load the tokenizer and model from distlBERT","metadata":{}},{"cell_type":"code","source":"transformer_model = '../input/distilbert-base-indonesian/distilbert-base-indonesian'\nTOKENIZER = ppb.AutoTokenizer.from_pretrained(transformer_model)\nMODEL = ppb.AutoModel.from_pretrained(transformer_model)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T13:22:31.993781Z","iopub.execute_input":"2021-11-21T13:22:31.994094Z","iopub.status.idle":"2021-11-21T13:22:43.431027Z","shell.execute_reply.started":"2021-11-21T13:22:31.994058Z","shell.execute_reply":"2021-11-21T13:22:43.430234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting random seed and device\nSEED = 567\n\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda:0\" if use_cuda else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2021-11-21T13:22:45.113535Z","iopub.execute_input":"2021-11-21T13:22:45.113829Z","iopub.status.idle":"2021-11-21T13:22:45.178681Z","shell.execute_reply.started":"2021-11-21T13:22:45.113791Z","shell.execute_reply":"2021-11-21T13:22:45.177894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generate Dataset, DataLoader","metadata":{}},{"cell_type":"code","source":"# generate the dataset\nfrom torch.utils.data import Dataset\n\nclass Task2Dataset(Dataset):\n    # preprocess dsv files and initialize some parameters\n    def __init__(self, mode, df, tokenizer):\n        assert mode in [\"train\", \"dev\", \"test\"] \n        self.mode = mode\n        self.df = df\n        self.len = len(self.df)\n        self.tokenizer = tokenizer  # use BERT tokenizer\n\n    # define how to return back a sample\n    def __getitem__(self, idx):\n        if self.mode == \"test\":\n            #text_a, text_b = self.df.iloc[idx, :2].values\n#             text = self.df.iloc[idx, 3]\n            text = self.df.title[idx]\n            \n            #label = self.df.iloc[idx, 1]\n            label_tensor = torch.tensor(0)\n        \n        else:\n            text = self.df.iloc[idx, 3]\n            label = self.df.iloc[idx, 4]\n            label_tensor = torch.tensor(label)\n\n      #text, label = self.df.iloc[idx, :].values\n        \n      # encode_plus returns 3 tensors: 'input_ids', 'token_type_ids', 'attention_mask'\n        encoded_text = self.tokenizer.encode_plus(text, add_special_tokens=True, return_token_type_ids = True)\n      \n        tokens_tensor = torch.tensor(encoded_text['input_ids'])\n        segments_tensor = torch.tensor(encoded_text['token_type_ids'])\n      # masks_tensor =torch.tensor(encoded_text['attention_mask'])\n      \n      #label_tensor = torch.tensor(label)\n      \n        return (tokens_tensor, segments_tensor, label_tensor)\n\n    def __len__(self):\n        \n        return self.len\n    \n    \n# initialize Datesets, use 'distilbert-base-uncased' tokenizer\n# trainset = Task2Dataset(\"train\", df_train, tokenizer=TOKENIZER)\n# devset = Task2Dataset(\"dev\", df_dev, tokenizer=TOKENIZER)\n# testset = Task2Dataset(\"test\", df_test, tokenizer=TOKENIZER)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T13:22:52.437671Z","iopub.execute_input":"2021-11-21T13:22:52.438338Z","iopub.status.idle":"2021-11-21T13:22:52.44765Z","shell.execute_reply.started":"2021-11-21T13:22:52.438304Z","shell.execute_reply":"2021-11-21T13:22:52.446702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # test\n# idx = 1\n# text = train.iloc[idx, 3]\n# label = train.iloc[idx, 4]\n# label_tensor = torch.tensor(label)\n\n# encoded_text = TOKENIZER.encode_plus(text, add_special_tokens=True, return_token_type_ids = True)\n# encoded_text","metadata":{"execution":{"iopub.status.busy":"2021-11-21T13:23:11.573514Z","iopub.execute_input":"2021-11-21T13:23:11.573808Z","iopub.status.idle":"2021-11-21T13:23:11.577669Z","shell.execute_reply.started":"2021-11-21T13:23:11.573777Z","shell.execute_reply":"2021-11-21T13:23:11.576599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generate the dataloader\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\n\ndef collate_fn_padd(samples):\n    \n    tokens_tensors = [s[0] for s in samples]\n    segments_tensors = [s[1] for s in samples]\n    \n    label_ids = torch.stack([s[2] for s in samples])\n    \n    tokens_tensors = pad_sequence(tokens_tensors, batch_first=True)\n    segments_tensors = pad_sequence(segments_tensors, batch_first=True)\n    \n    masks_tensors = torch.zeros(tokens_tensors.shape, dtype=torch.long)\n    masks_tensors = masks_tensors.masked_fill(tokens_tensors != 0, 1)\n    \n    return tokens_tensors, segments_tensors, masks_tensors, label_ids\n\n\nBATCH_SIZE = 16\n\n# trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, \n#                          collate_fn=collate_fn_padd)\n# devloader = DataLoader(devset, batch_size=BATCH_SIZE, collate_fn=collate_fn_padd)\n# # testloader = DataLoader(testset, batch_size=BATCH_SIZE, \n# #                          collate_fn=collate_fn_padd)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T13:23:13.672952Z","iopub.execute_input":"2021-11-21T13:23:13.673823Z","iopub.status.idle":"2021-11-21T13:23:13.68123Z","shell.execute_reply.started":"2021-11-21T13:23:13.673765Z","shell.execute_reply":"2021-11-21T13:23:13.68034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # test\n# data = next(iter(trainloader))\n\n# tokens_tensors, segments_tensors, \\\n#     masks_tensors, label_ids = data\n\n# print(f\"\"\"\n# tokens_tensors.shape   = {tokens_tensors.shape} \n# {tokens_tensors}\n# ------------------------\n# segments_tensors.shape = {segments_tensors.shape}\n# {segments_tensors}\n# ------------------------\n# masks_tensors.shape    = {masks_tensors.shape}\n# {masks_tensors}\n# ------------------------\n# label_ids.shape        = {label_ids.shape}\n# {label_ids}\n# \"\"\")","metadata":{"execution":{"iopub.status.busy":"2021-11-21T13:23:13.907176Z","iopub.execute_input":"2021-11-21T13:23:13.907476Z","iopub.status.idle":"2021-11-21T13:23:13.913683Z","shell.execute_reply.started":"2021-11-21T13:23:13.907434Z","shell.execute_reply":"2021-11-21T13:23:13.912285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # the model information\n# print(\"\"\"\n# name            module\n# ----------------------\"\"\")\n# for name, module in MODEL.named_children():\n#     if name == \"bert\":\n#         for n, _ in module.named_children():\n#             print(f\"{name}: {n}\")\n#     else:\n#         print(\"{:15} {}\".format(name, module))","metadata":{"execution":{"iopub.status.busy":"2021-11-21T13:23:14.28576Z","iopub.execute_input":"2021-11-21T13:23:14.286019Z","iopub.status.idle":"2021-11-21T13:23:14.289842Z","shell.execute_reply.started":"2021-11-21T13:23:14.285989Z","shell.execute_reply":"2021-11-21T13:23:14.288805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## test prediction function","metadata":{}},{"cell_type":"code","source":"def test_predict(model, dataloader, device):\n\n    model.eval()\n    _predicted_metrics = []\n    _true_labels = []\n\n    with torch.no_grad():    \n        for batch in dataloader:\n            \n            tokens_tensors, segments_tensors, masks_tensors, labels = batch\n            \n            tokens_tensors, segments_tensors, masks_tensors, labels = tokens_tensors.to(device), \\\n          segments_tensors.to(device), masks_tensors.to(device), labels.to(device)\n\n          # inputs,attention_masks = inputs.to(device),attention_masks.to(device)\n          # features = model(inputs,attention_masks)[0][:,0,:].detach()\n\n          # features = model(input_ids=tokens_tensors, \n          # token_type_ids=segments_tensors, \n          # attention_mask=masks_tensors,\n          # labels=labels)\n\n            _ = model.to(device)\n\n            features = model(input_ids=tokens_tensors, \n            attention_mask=masks_tensors)[0][:,0,:].detach()\n\n\n            if len(features.shape) != 2:\n                features = torch.nn.AdaptiveAvgPool2d(1)(features).cpu().view(-1,features.shape[1]).detach().numpy()\n            else:\n                features = features.detach().cpu().numpy()\n\n            metric = features.reshape(features.shape[0], features.shape[1])\n            _predicted_metrics.append(metric)\n\n    return np.concatenate(_predicted_metrics)\n\ndef get_similar(df,embeddings,threshold=0.36,KNN = 50):\n    \n    if len(df)==3: KNN = 2\n    model = NearestNeighbors(n_neighbors=KNN,metric='cosine')\n    model.fit(embeddings)\n    \n    preds = []\n    \n    CHUNK = 1024*4\n\n    print('Finding similar images...')\n    CTS = len(embeddings)//CHUNK\n    if len(embeddings)%CHUNK!=0: CTS += 1\n    \n    # for j in tqdm(range( CTS )):  \n    for j in range( CTS ):\n    # j = 0\n    # while j <= 2:\n\n        a = j*CHUNK\n        b = (j+1)*CHUNK\n        b = min(b,len(embeddings))\n        distances, indices = model.kneighbors(embeddings[a:b,])\n        # print(distances)\n        # j+=1\n\n#         x_avg=np.mean(distances) # 平均値(定義上は0)\n#         x_std=np.std(distances) # 標準偏差(定義上は1)\n#         lsl_1=x_avg-x_std*1.55\n\n        for k in range(b-a):\n            IDX = np.where(distances[k,]<threshold)[0]\n            IDS = indices[k,IDX]\n            o = df.iloc[IDS].posting_id.values\n            preds.append(o)\n\n#     print(lsl_1)\n    print(f\"embed={embeddings.shape[1]}_KNN={KNN}_distances={threshold}\")\n#     plt.hist(distances.flatten(),bins=100)\n#     plt.show()\n#     del model, distances, indices, image_embeddings#, embeds\n#     _ = gc.collect()\n    \n    return preds\n\n\ndef compute_F1(df):\n    \n    n,m = df.shape\n    s = 0\n    for i in range(n):\n        row = df.iloc[i,:]\n        preds = row[-1]\n        label_group = row[-2]\n        true_pics = df[df['label_group'] == label_group].posting_id.values\n    \n        Precission = len(set(preds) & set(true_pics)) /len(preds)\n        Recall = len(set(preds) & set(true_pics)) /len(true_pics)\n    \n        if Precission == 0.0 and Recall == 0.0 and Precission + Recall == 0.0:\n            f1 = 0\n        else:\n            f1 = 2*Precission*Recall/(Precission+Recall)\n        s += f1\n    return s/n","metadata":{"execution":{"iopub.status.busy":"2021-11-21T13:23:21.286933Z","iopub.execute_input":"2021-11-21T13:23:21.287221Z","iopub.status.idle":"2021-11-21T13:23:21.306217Z","shell.execute_reply.started":"2021-11-21T13:23:21.28719Z","shell.execute_reply":"2021-11-21T13:23:21.305243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## generate the submit file\n\n","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv('../input/shopee-product-matching/test.csv')\ntestset = Task2Dataset(\"test\", test, tokenizer=TOKENIZER)\n\ntestloader = DataLoader(testset, batch_size=BATCH_SIZE, \n                         collate_fn=collate_fn_padd)\n\ntest_embeddings = test_predict(MODEL,testloader,device)\ntest['matches'] = get_similar(test,test_embeddings,0.9/100)\n\ntest['matches'] = test['matches'].apply(lambda x: \" \".join(x))\n\ntest[['posting_id','matches']].to_csv('submission.csv',index=False)\nsub = pd.read_csv('submission.csv')\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-21T13:23:27.649138Z","iopub.execute_input":"2021-11-21T13:23:27.649404Z","iopub.status.idle":"2021-11-21T13:23:33.201461Z","shell.execute_reply.started":"2021-11-21T13:23:27.649373Z","shell.execute_reply":"2021-11-21T13:23:33.200681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # train = pd.read_csv('../input/shopee-product-matching/train.csv')\n# # trainset = Task2Dataset(\"train\", train, tokenizer=TOKENIZER)\n\n# # trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, \n# #                          collate_fn=collate_fn_padd)\n\n# dev_embeddings = test_predict(MODEL,devloader,device)\n# df_dev['matches'] = get_similar(df_dev,dev_embeddings,0.9/100)\n\n# df_dev['matches'] = df_dev['matches'].apply(lambda x: \" \".join(x))\n\n# df_dev[['posting_id','matches']].to_csv('submission.csv',index=False)\n# sub = pd.read_csv('submission.csv')\n# sub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train[train['label_group'] == 4093212188]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}