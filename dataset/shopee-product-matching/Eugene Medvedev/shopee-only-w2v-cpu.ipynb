{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nfrom gensim.models import Word2Vec\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.manifold import TSNE\nimport random\nfrom collections import Counter\nimport plotly.express as px\nfrom sklearn.neighbors import NearestNeighbors\nfrom nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\nfrom sklearn.neighbors import NearestNeighbors\nimport seaborn as sns \nfrom collections import defaultdict\nfrom wordcloud import WordCloud, STOPWORDS\nstopwords = set(STOPWORDS)\nimport cv2\nimport os","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/shopee-product-matching/train.csv\")\ntest = pd.read_csv(\"../input/shopee-product-matching/test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def len_seq(data): \n    return len(data.split()) \ntrain[\"number_tokens\"] = train[\"title\"].apply(lambda x : len_seq(x)) \nfig = px.histogram(\n    train, \n    x=\"number_tokens\",\n    width=800,\n    height=500,\n    title='Number tokens distribution'\n)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution helps to set parameters: window, negative ","metadata":{}},{"cell_type":"code","source":"TOKEN_RE = re.compile(r'[\\w]+')\ndef tokenize_text_simple_regex(txt, min_token_size=2):\n    txt = str(txt).lower()\n    all_tokens = TOKEN_RE.findall(txt)\n    return [wordnet_lemmatizer.lemmatize(token, pos=\"v\") for token in all_tokens if len(token) >= min_token_size]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_corpus(texts, tokenizer=tokenize_text_simple_regex, **tokenizer_kwargs):\n    return [tokenizer(text, **tokenizer_kwargs) for text in texts]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['title'] = train['image_phash'] + ' ' + train['title'] + ' ' + train['image_phash']\ntest['title'] = test['image_phash'] + ' ' + test['title'] + ' ' + test['image_phash']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus = tokenize_corpus(list(pd.concat([train['title'], test['title']])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Word2Vec(\n        sentences=corpus,\n        vector_size=100, \n        window=15, \n        min_count=1, \n        sg=1, #skip-gram\n        negative=7, \n        epochs=1000, \n        seed=42,\n        workers=6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_vectors(vectors, labels, how='tsne', ax=None):\n    if how == 'tsne':\n        projections = TSNE().fit_transform(vectors)\n    elif how == 'svd':\n        projections = TruncatedSVD().fit_transform(vectors)\n    x = projections[:, 0]\n    y = projections[:, 1]\n    ax.scatter(x, y)\n    for cur_x, cur_y, cur_label in zip(x, y, labels):\n        ax.annotate(cur_label, (cur_x, cur_y))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def n_grams(ngram, data):\n    freq_dict = defaultdict(int)\n    for text in data:\n        tokens = [w for w in text.lower().split() if w != \" \" if w not in stopwords]\n        ngrams = zip(*[tokens[i:] for i in range(ngram)])\n        list_grams = [\" \".join(ngram) for ngram in ngrams]\n        for word in list_grams:\n            freq_dict[word] += 1\n    df_ngram =  pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])   \n    df_ngram.columns = [\"word\", \"wordcount\"]\n    return df_ngram ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_3_grams = n_grams(3, train[\"title\"]) \nprint(df_3_grams.head(20))\nprint(df_3_grams.tail())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_words = ['jam','tangan','wanita','xiaomi','redmi','note','somebymi','yuja','niacin','mm','3m']\ngensim_words = [w for w in test_words if w in model.wv.index_to_key]\ngensim_vectors = np.stack([model.wv[w] for w in gensim_words])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots()\nfig.set_size_inches((10, 10))\nplot_vectors(gensim_vectors, test_words, how='svd', ax=ax)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example_hash = 'a6f319f924ad708c'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def draw_img_hash(hash):\n    plot_list = train[train['image_phash'] == example_hash]['image'].tolist()[0]\n    plt.figure(figsize=(5, 5))\n    image = cv2.imread(os.path.join('../input/shopee-product-matching/train_images/', plot_list))\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    plt.title(plot_list, fontsize=12)\n    plt.axis(\"off\")\n    plt.imshow(image)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"draw_img_hash(example_hash)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#similar words\nnn = model.wv.most_similar(example_hash)\nnn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"draw_img_hash('e69999663199cc93')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"draw_img_hash('bce5c11a96393cc6')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"W2V is working :)","metadata":{}},{"cell_type":"code","source":"import gc\ndel train\ndel test \ndel corpus\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if len(pd.read_csv(\"../input/shopee-product-matching/test.csv\")) > 3: \n    df = pd.read_csv(\"../input/shopee-product-matching/test.csv\")\nelse: \n    df = pd.read_csv(\"../input/shopee-product-matching/train.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeds = []\nfor phash in df['image_phash'].tolist():\n    try:\n        embeds.append(model.wv[phash].tolist())\n    except KeyError:\n        embeds.append(np.zeros((100), dtype='float32').tolist())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"neighbors_model = NearestNeighbors(n_neighbors = 50, metric='cosine').fit(embeds)\ntext_distances, text_indices = neighbors_model.kneighbors(embeds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = []\nfor k in range(df.shape[0]):\n    idx_text = np.where(text_distances[k,] < 0.17)[0]\n    ids_text = text_indices[k,idx_text]\n    posting_ids = ' '.join(df.iloc[ids_text]['posting_id'].values)\n    predictions.append(posting_ids)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['matches'] = predictions\ndf[['posting_id', 'matches']].to_csv('submission.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}