{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Shopee-Product-Matching\n![Shopee](https://cdn.lynda.com/course/563030/563030-636270778700233910-16x9.jpg)\n","metadata":{}},{"cell_type":"markdown","source":"1. If you want to learn more about this amazing competition hosted by [Shopee](https://www.kaggle.com/c/shopee-product-matching), Please visit following [Shopee EDA Image AutoEncoder](https://www.kaggle.com/code/chiragtagadiya/shopee-basic-autoencoder).\n2. This Notebook contains EDA and Image AutoEncoder solution.","metadata":{}},{"cell_type":"code","source":"%config Completer.use_jedi = False","metadata":{"execution":{"iopub.status.busy":"2022-03-20T18:49:29.109269Z","iopub.execute_input":"2022-03-20T18:49:29.10984Z","iopub.status.idle":"2022-03-20T18:49:29.125061Z","shell.execute_reply.started":"2022-03-20T18:49:29.109796Z","shell.execute_reply":"2022-03-20T18:49:29.124189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Packages","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append('../input/timmmaster')\nimport timm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-20T18:49:29.126846Z","iopub.execute_input":"2022-03-20T18:49:29.127309Z","iopub.status.idle":"2022-03-20T18:49:29.131146Z","shell.execute_reply.started":"2022-03-20T18:49:29.127274Z","shell.execute_reply":"2022-03-20T18:49:29.130399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\nimport os\nimport numpy as np\nimport cv2\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nimport timm\nimport torch\nfrom torch import nn \nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F \nimport albumentations\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom torch.optim import lr_scheduler\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom datetime import date\nfrom sklearn.metrics import f1_score, accuracy_score","metadata":{"execution":{"iopub.status.busy":"2022-03-20T18:49:29.132698Z","iopub.execute_input":"2022-03-20T18:49:29.133252Z","iopub.status.idle":"2022-03-20T18:49:29.14064Z","shell.execute_reply.started":"2022-03-20T18:49:29.133216Z","shell.execute_reply":"2022-03-20T18:49:29.139776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configuration Options\n","metadata":{}},{"cell_type":"code","source":"TRAIN_DIR = '../input/shopee-product-matching/train_images'\nTEST_DIR = '../input/shopee-product-matching/test_images'\nTRAIN_CSV = '../input/crossvalidationfolds/folds.csv'\nMODEL_PATH = './'\n\n\nclass CFG:\n    seed = 123 \n    img_size = 512\n    classes = 11014\n    fc_dim = 512\n    epochs = 15\n    batch_size = 32\n    num_workers = 3\n    model_name = 'tf_efficientnet_b4'\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    scheduler_params = {\n        \"lr_start\": 1e-3,\n        \"lr_max\": 1e-5 * batch_size,\n        \"lr_min\": 1e-6,\n        \"lr_ramp_ep\": 5,\n        \"lr_sus_ep\": 0,\n        \"lr_decay\": 0.8,\n    }\n    model_path='../input/21-mar-lr-large/2022-03-20_softmax_512x512_tf_efficientnet_b4.pt'\n    isTraining=False\n    ","metadata":{"execution":{"iopub.status.busy":"2022-03-20T18:49:29.142417Z","iopub.execute_input":"2022-03-20T18:49:29.143012Z","iopub.status.idle":"2022-03-20T18:49:29.150354Z","shell.execute_reply.started":"2022-03-20T18:49:29.142977Z","shell.execute_reply":"2022-03-20T18:49:29.149596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Solution Approach\n\n* In this competition it is given that,if two or more images have **same label group** then they are **similar products.** \n* Basically we can use this information to transfer the business problem into **multi class classification** problem.\n* From Image EDA, I found out that we have **11014** different classes, and dataset is **not balanced dataset**\n* If you see below plot, we can clearly see that there are **hardly 1000 data points having more than 10 products per label.*\n* In this notebook I used **Weighted Sampler technique used in pytorch for handling imbalanced classification problem**\n","metadata":{}},{"cell_type":"code","source":"train_df=pd.read_csv('../input/shopee-product-matching/train.csv')\nlabelGroups = train_df.label_group.value_counts()\n# print(labelGroups)\nplt.figure(figsize=(15,5))\nplt.plot(np.arange(len(labelGroups)), labelGroups.values)\nplt.xlabel(\"Index for unique label_group_item\", size=12)\nplt.ylabel(\"Number of product data for label \", size=12)\nplt.title(\"label vs label frequency\", size=15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-20T19:05:44.152803Z","iopub.execute_input":"2022-03-20T19:05:44.153115Z","iopub.status.idle":"2022-03-20T19:05:44.418054Z","shell.execute_reply.started":"2022-03-20T19:05:44.153081Z","shell.execute_reply":"2022-03-20T19:05:44.417397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Custom DataSet","metadata":{}},{"cell_type":"code","source":"class ShopeeDataset(Dataset):\n    \n    def __init__(self, df,root_dir, isTraining=False, transform=None):\n        self.df = df\n        self.transform = transform\n        self.root_dir = root_dir\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        # get row at index idx\n#         print(\"idx\",idx)\n        \n        row = self.df.iloc[idx]\n#         print(row)\n        label = row.label_group\n        image_path = os.path.join(self.root_dir, row.image)\n        \n        # read image convert to RGB and apply augmentation\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        if self.transform:\n            aug = self.transform(image=image)\n            image = aug['image']\n        \n        return image, torch.tensor(label).long()\n            \n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-20T18:49:29.152179Z","iopub.execute_input":"2022-03-20T18:49:29.152478Z","iopub.status.idle":"2022-03-20T18:49:29.162354Z","shell.execute_reply.started":"2022-03-20T18:49:29.152443Z","shell.execute_reply":"2022-03-20T18:49:29.161498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Data Augmentation For training and validation Data","metadata":{}},{"cell_type":"code","source":"\ndef getAugmentation(IMG_SIZE, isTraining=False):\n    \n    if isTraining:\n        return albumentations.Compose([\n            albumentations.Resize(IMG_SIZE, IMG_SIZE, always_apply=True),\n            albumentations.HorizontalFlip(p=0.5),\n            albumentations.VerticalFlip(p=0.5),\n            albumentations.Rotate(limit=120, p=0.75),\n            albumentations.RandomBrightness(limit=(0.09, 0.6), p=0.5),\n            albumentations.Normalize(\n                mean = [0.485, 0.456, 0.406],\n                std = [0.229, 0.224, 0.225]\n            ),\n            ToTensorV2(p=1.0)\n        ])\n    else:\n        return albumentations.Compose([\n            albumentations.Resize(IMG_SIZE, IMG_SIZE, always_apply=True),\n            albumentations.Normalize(\n                mean = [0.485, 0.456, 0.406],\n                std = [0.229, 0.224, 0.225]\n            ),\n            ToTensorV2(p=1.0)\n        ])","metadata":{"execution":{"iopub.status.busy":"2022-03-20T18:49:29.163293Z","iopub.execute_input":"2022-03-20T18:49:29.163489Z","iopub.status.idle":"2022-03-20T18:49:29.173344Z","shell.execute_reply.started":"2022-03-20T18:49:29.16346Z","shell.execute_reply":"2022-03-20T18:49:29.172659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build Model","metadata":{}},{"cell_type":"code","source":"class ShopeeLabelGroupClassfier(nn.Module):\n    \n    def __init__(self,\n                     model_name='tf_efficientnet_b0',\n                     loss_fn='softmax',\n                     classes = CFG.classes,\n                     fc_dim = CFG.fc_dim,\n                     pretrained=True,\n                     use_fc=True,\n                     isTraining=False\n                ):\n        \n        \n        super(ShopeeLabelGroupClassfier,self).__init__()\n        \n        # create bottlenack backbone network from pretrained model \n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n        in_features = self.backbone.classifier.in_features\n        # we will put FC layers over backbone to classfy images based on label groups\n        self.backbone.classifier = nn.Identity()\n        self.backbone.global_pool = nn.Identity()\n        self.pooling = nn.AdaptiveAvgPool2d(1)\n        self.use_fc = use_fc\n        self.loss_fn =loss_fn\n        \n        # build top fc layers\n        if self.use_fc:\n            self.dropout = nn.Dropout(0.2)\n            self.fc = nn.Linear(in_features,fc_dim )\n            self.bn = nn.BatchNorm1d(fc_dim)\n            in_features = fc_dim\n        self.loss_fn = loss_fn\n        \n        if self.loss_fn=='softmax':\n            self.final = nn.Linear(in_features, CFG.classes)\n    \n    def forward(self, image, label):\n        features = self.get_features(image)\n        \n        if self.loss_fn=='softmax':\n            logits = self.final(features)\n            \n        return logits\n    \n    def get_features(self,inp):\n        batch_dim = inp.shape[0]\n        inp = self.backbone(inp)\n        inp = self.pooling(inp).view(batch_dim, -1)\n        if self.use_fc:\n            inp = self.dropout(inp)\n            inp = self.fc(inp)\n            inp = self.bn(inp)\n        \n        return inp\n    \n    \n# shoppe_label_classfier = ShopeeLabelGroupClassfier()\n","metadata":{"execution":{"iopub.status.busy":"2022-03-20T18:49:29.174659Z","iopub.execute_input":"2022-03-20T18:49:29.175028Z","iopub.status.idle":"2022-03-20T18:49:29.189026Z","shell.execute_reply.started":"2022-03-20T18:49:29.174995Z","shell.execute_reply":"2022-03-20T18:49:29.188257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training  Single Epoch\n","metadata":{}},{"cell_type":"code","source":"def training_one_epoch(epoch_num,model, dataloader,optimizer, scheduler, device, loss_criteria):\n    avgloss = 0.0\n    # put model in traning model\n    model.train()\n    tq = tqdm(enumerate(dataloader), total=len(dataloader))\n    \n    for idx, data in tq:\n        batch_size = data[0].shape[0]\n        images = data[0]\n        targets = data[1]\n        # zero out gradient\n        optimizer.zero_grad()\n        # put input and target to device\n        images = images.to(device)\n        targets = targets.to(device)\n        # pass input to the model\n        output = model(images,targets)\n        # get loss\n        loss = loss_criteria(output,targets)\n        # backpropogation \n        loss.backward()\n        # update learning rate step\n        optimizer.step() \n        # avg loss\n        avgloss += loss.item() \n\n        tq.set_postfix({'loss' : '%.6f' %float(avgloss/(idx+1)), 'LR' : optimizer.param_groups[0]['lr']})\n        \n    # lr scheduler step after each epoch\n    scheduler.step()\n    return avgloss / len(dataloader)\n    \n    \n    \n    \n    \n    ","metadata":{"execution":{"iopub.status.busy":"2022-03-20T18:49:29.2022Z","iopub.execute_input":"2022-03-20T18:49:29.202448Z","iopub.status.idle":"2022-03-20T18:49:29.211553Z","shell.execute_reply.started":"2022-03-20T18:49:29.202415Z","shell.execute_reply":"2022-03-20T18:49:29.210656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validating Single Epoch","metadata":{}},{"cell_type":"code","source":"\n\ndef validation_one_epoch(model, dataloader, epoch, device, loss_criteria):\n    avgloss = 0.0\n    # put model in traning model\n    model.eval()\n    tq = tqdm(enumerate(dataloader), desc = \"Training Epoch { }\" + str(epoch+1))\n    \n    y_true=[]\n    y_pred=[]\n    with torch.no_grad():\n        for idx, data in tq:\n            batch_size = data[0].shape[0]\n            images = data[0]\n            targets = data[1]\n\n            images = images.to(device)\n            targets = targets.to(device)\n            output = model(images,targets)\n            predicted_label=torch.argmax(output,1)\n            y_true.extend(targets.detach().cpu().numpy())\n            y_pred.extend(predicted_label.detach().cpu().numpy())\n            loss = loss_criteria(output,targets)\n\n            avgloss += loss.item() \n\n            tq.set_postfix({'validation loss' : '%.6f' %float(avgloss/(idx+1))})\n    f1_score_metric = f1_score(y_true, y_pred, average='micro')\n    tq.set_postfix({'validation f1 score' : '%.6f' %float(f1_score_metric)})\n    return avgloss / len(dataloader),f1_score_metric\n        \n","metadata":{"execution":{"iopub.status.busy":"2022-03-20T18:49:29.213537Z","iopub.execute_input":"2022-03-20T18:49:29.213832Z","iopub.status.idle":"2022-03-20T18:49:29.224515Z","shell.execute_reply.started":"2022-03-20T18:49:29.213796Z","shell.execute_reply":"2022-03-20T18:49:29.223708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Helper Function for Handling class imbalanced data","metadata":{}},{"cell_type":"code","source":"import numpy as np \ndef get_class_weights(data):\n    weight_dict=dict()\n    # Format of row : PostingId, Image, ImageHash, Title, LabelGroup\n    # LabelGroup index is 4 and it is representating class information\n    for row in data.values:\n        weight_dict[row[4]]=0\n    # Word dictionary keys will be label and value will be frequency of label in dataset\n    for row in data.values:\n        weight_dict[row[4]]+=1\n    # for each data point get label count data\n    class_sample_count= np.array([weight_dict[row[4]] for row in data.values])\n    # each data point weight will be inverse of frequency\n    weight = 1. / class_sample_count\n    weight=torch.from_numpy(weight)\n    return weight","metadata":{"execution":{"iopub.status.busy":"2022-03-20T18:49:29.226043Z","iopub.execute_input":"2022-03-20T18:49:29.226855Z","iopub.status.idle":"2022-03-20T18:49:29.235094Z","shell.execute_reply.started":"2022-03-20T18:49:29.226762Z","shell.execute_reply":"2022-03-20T18:49:29.234258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Loop","metadata":{}},{"cell_type":"code","source":"def run_training():\n    data = pd.read_csv('../input/crossvalidationfolds/folds.csv')\n    \n    # label encoding\n    labelencoder= LabelEncoder()\n    data['label_group_original']=data['label_group']\n    data['label_group'] = labelencoder.fit_transform(data['label_group'])\n    #data['weights'] = data['label_group'].map(1/data['label_group'].value_counts())\n    # create training_data and validation data initially not using k fold\n    train_data = data[data['fold']!=0]\n    # get weights for  classes\n    samples_weight=get_class_weights(train_data)\n    \n    print(\"samples_weight\", len(samples_weight))\n    validation_data = data[data['fold']==0]\n    \n    # training augmentation\n    train_aug = getAugmentation(CFG.img_size,isTraining=True )\n    validation_aug = getAugmentation(CFG.img_size, isTraining=False)\n    # create custom train and validation dataset\n    \n    trainset = ShopeeDataset(train_data, TRAIN_DIR, isTraining=True, transform = train_aug)\n    validset = ShopeeDataset(validation_data, TRAIN_DIR, isTraining=False, transform = validation_aug)\n    print(len(data), len(samples_weight))\n    print(len(trainset))\n    # create data sampler\n                  \n    sampler = torch.utils.data.sampler.WeightedRandomSampler(samples_weight, num_samples=len(samples_weight))   \n    \n    # create custom training and validation data loader num_workers=CFG.num_workers,\n    train_dataloader = DataLoader(trainset, batch_size=CFG.batch_size,\n                          drop_last=True,pin_memory=True, sampler=sampler)\n    \n    validation_dataloader = DataLoader(validset, batch_size=CFG.batch_size,\n                         drop_last=True,pin_memory=True)\n    \n    \n    # define loss function\n    loss_criteria = nn.CrossEntropyLoss()\n    loss_criteria.to(CFG.device)\n    # define model\n    \n    model = ShopeeLabelGroupClassfier()\n    model.to(CFG.device)\n    \n    # define optimzer\n    optimizer = torch.optim.Adam(model.parameters(),lr= CFG.scheduler_params['lr_start'])\n    \n    # learning rate scheudler\n    scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=7, T_mult=1, eta_min=1e-6, last_epoch=-1)\n    \n    history = {'train_loss':[],'validation_loss':[]}\n    for epoch in range(CFG.epochs):\n        \n        # get current epoch training loss\n        avg_train_loss = training_one_epoch(epoch_num = epoch,\n                                           model = model,\n                                           dataloader = train_dataloader,\n                                           optimizer = optimizer,\n                                           scheduler = scheduler,\n                                           device = CFG.device, \n                                           loss_criteria = loss_criteria)\n        \n        # get current epoch validation loss\n        avg_validation_loss = validation_one_epoch(model = model,\n                                           dataloader = validation_dataloader,\n                                           epoch = epoch,\n                                           device = CFG.device,\n                                           loss_criteria = loss_criteria)\n        \n        \n        history['train_loss'].append(avg_train_loss)\n        history['validation_loss'].append(avg_validation_loss)\n        \n        # save model\n        torch.save(model.state_dict(), MODEL_PATH + str(date.today()) +'_softmax_512x512_{}.pt'.format(CFG.model_name))\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer': optimizer.state_dict(),\n#             'scheduler': lr_scheduler.state_dict()\n            },\n            MODEL_PATH + str(date.today()) +'_softmax_512x512_{}_checkpoints.pt'.format(CFG.model_name)\n        )\n        \n    return model, history","metadata":{"execution":{"iopub.status.busy":"2022-03-20T18:49:29.237649Z","iopub.execute_input":"2022-03-20T18:49:29.237998Z","iopub.status.idle":"2022-03-20T18:49:29.255688Z","shell.execute_reply.started":"2022-03-20T18:49:29.237935Z","shell.execute_reply":"2022-03-20T18:49:29.254971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history=None\nif CFG.isTraining:\n    model, history = run_training()\n    ","metadata":{"execution":{"iopub.status.busy":"2022-03-20T18:49:29.257204Z","iopub.execute_input":"2022-03-20T18:49:29.257469Z","iopub.status.idle":"2022-03-20T18:49:29.261789Z","shell.execute_reply.started":"2022-03-20T18:49:29.257434Z","shell.execute_reply":"2022-03-20T18:49:29.260969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot Training and Validation Loss and Accuracy","metadata":{}},{"cell_type":"code","source":"if CFG.isTraining:\n    epoch_lst = [ i+1 for i in range(15)]\n    plt.plot(epoch_lst,history['train_loss'])\n\n    plt.xlabel(\"Epoch number\")\n    plt.ylabel('Training Loss')\n    plt.title('Training Loss SoftMax Loss Function')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-20T18:49:29.263399Z","iopub.execute_input":"2022-03-20T18:49:29.264055Z","iopub.status.idle":"2022-03-20T18:49:29.269956Z","shell.execute_reply.started":"2022-03-20T18:49:29.264019Z","shell.execute_reply":"2022-03-20T18:49:29.269283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CFG.isTraining:\n    plt.plot(epoch_lst,history['validation_loss'])\n    plt.xlabel(\"Epoch number\")\n    plt.ylabel('Validation Loss')\n    plt.title('Validation Loss SoftMax Loss Function')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-20T18:49:29.271464Z","iopub.execute_input":"2022-03-20T18:49:29.271988Z","iopub.status.idle":"2022-03-20T18:49:29.278847Z","shell.execute_reply.started":"2022-03-20T18:49:29.271952Z","shell.execute_reply":"2022-03-20T18:49:29.278118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction ","metadata":{}},{"cell_type":"code","source":"def prediction(model):\n    data = pd.read_csv('../input/crossvalidationfolds/folds.csv')\n\n    # label encoding\n    labelencoder= LabelEncoder()\n    data['label_group'] = labelencoder.fit_transform(data['label_group'])\n    # Prepare Validation data\n    validation_data = data[data['fold']==0]\n    validation_aug = getAugmentation(CFG.img_size,isTraining=False)\n    validset = ShopeeDataset(validation_data, TRAIN_DIR, isTraining=False, transform = validation_aug)\n    test_data_loader = torch.utils.data.DataLoader(validset,batch_size=CFG.batch_size)\n    \n    # put model in evalution mode\n    \n    model.eval()\n    \n    tq = tqdm(enumerate(test_data_loader))\n    y_true=[]\n    y_pred=[]\n    with torch.no_grad():\n        for idx, data in tq:\n            images = data[0]\n            targets = data[1]\n            \n            images = images.to(CFG.device)\n            targets = targets.to(CFG.device)\n            y_true.extend(targets.detach().cpu().numpy())\n            output = model(images,targets)\n            outputs=torch.argmax(output,1)\n            y_pred.extend(outputs.detach().cpu().numpy())\n        \n    f1_score_metric = f1_score(y_true, y_pred, average='micro')\n    return f1_score_metric\n    ","metadata":{"execution":{"iopub.status.busy":"2022-03-20T18:49:29.279604Z","iopub.execute_input":"2022-03-20T18:49:29.280618Z","iopub.status.idle":"2022-03-20T18:49:29.290967Z","shell.execute_reply.started":"2022-03-20T18:49:29.280581Z","shell.execute_reply":"2022-03-20T18:49:29.290138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not CFG.isTraining:\n    model = ShopeeLabelGroupClassfier(pretrained=False).to(CFG.device)\n    model.load_state_dict(torch.load(CFG.model_path))\n    f1=prediction(model)\n    print(\"F1 score {}\".format(f1))","metadata":{"execution":{"iopub.status.busy":"2022-03-20T18:49:29.29233Z","iopub.execute_input":"2022-03-20T18:49:29.292637Z","iopub.status.idle":"2022-03-20T18:51:54.794823Z","shell.execute_reply.started":"2022-03-20T18:49:29.292604Z","shell.execute_reply":"2022-03-20T18:51:54.794116Z"},"trusted":true},"execution_count":null,"outputs":[]}]}