{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nsys.path.append('../input/timmmaster')\nimport timm","metadata":{"_uuid":"69763b81-c7c2-4a51-9a34-601f10c275b9","_cell_guid":"2f1831eb-2b76-4536-863c-df6810af59a1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-18T04:14:24.128956Z","iopub.execute_input":"2022-04-18T04:14:24.129312Z","iopub.status.idle":"2022-04-18T04:14:32.768351Z","shell.execute_reply.started":"2022-04-18T04:14:24.12923Z","shell.execute_reply":"2022-04-18T04:14:32.767151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preliminaries\nfrom tqdm import tqdm\nimport math\nimport random\nimport os\nimport pandas as pd\nimport numpy as np\n\n# Visuals and CV2\nimport cv2\n\n# albumentations for augs\nimport albumentations\nfrom albumentations.pytorch.transforms import ToTensorV2\n\n#torch\nimport torch\nimport timm\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset,DataLoader\n\n\nimport gc\nimport matplotlib.pyplot as plt\nimport cudf\nimport cuml\nimport cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml import PCA\nfrom cuml.neighbors import NearestNeighbors","metadata":{"_uuid":"c59548a1-cb49-4a42-adfb-3a2443310cee","_cell_guid":"950e4d7b-1263-4ffb-a67b-316bd31f5d58","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-18T04:14:32.771442Z","iopub.execute_input":"2022-04-18T04:14:32.772092Z","iopub.status.idle":"2022-04-18T04:14:38.351401Z","shell.execute_reply.started":"2022-04-18T04:14:32.772032Z","shell.execute_reply":"2022-04-18T04:14:38.350448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# configuration class\n\nclass CFG:\n    loss_module='ArcFace'\n    TRAIN_DIR='../input/shopee-product-matching/train_images'\n    TEST_DIR='../input/shopee-product-matching/test_images'\n    seed = 123 \n    img_size = 512\n    classes = 11014\n    fc_dim = 512\n    epochs = 25\n    batch_size = 12\n    num_workers = 3\n    model_name = 'tf_efficientnet_b3'\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    model_path_arcface='../input/pretrained-b3/Train_F1_score_0.9061769859813084valid_f1_score0.4245035046728972_Epoch_0_lr_start_2.23e-05_lr_max_0.00016_softmax_512x512_tf_efficientnet_b0.pt'\n    model_path_softmax = '../input/label-classfier-model/2022-04-15_softmax_512x512_tf_efficientnet_b4.pt'\n    # # check true when we want to train the model\n    isTraining=False","metadata":{"_uuid":"4a618b9d-6c35-4692-9c42-e290609687df","_cell_guid":"06175366-72c7-4cff-8bb4-2ad767a0819b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-18T04:14:38.354362Z","iopub.execute_input":"2022-04-18T04:14:38.354751Z","iopub.status.idle":"2022-04-18T04:14:38.360547Z","shell.execute_reply.started":"2022-04-18T04:14:38.354694Z","shell.execute_reply":"2022-04-18T04:14:38.359676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading Data","metadata":{"_uuid":"f672af75-837f-43be-a1a8-3e58e4078804","_cell_guid":"0bdf3efa-a839-4029-a4c5-77d750945c1b","trusted":true}},{"cell_type":"code","source":"def read_dataset():\n\n    # if not in testing phase read train dataset else test dataset\n    df = pd.read_csv('../input/shopee-product-matching/train.csv')\n    # we have information that label_group is same for similar kind of product\n    # let's use this to get F1 score for our final model\n    tmp = df.groupby(['label_group'])['posting_id'].unique().to_dict()\n    df['matches'] = df['label_group'].map(tmp)\n    df['matches'] = df['matches'].apply(lambda x: ' '.join(x))\n    # get cuda frame for faster GPU computation\n    df_cu = cudf.DataFrame(df)\n    \n        \n    return df, df_cu","metadata":{"_uuid":"1292e0e5-ed4d-447d-b334-6c2475ce6da0","_cell_guid":"20cfe4b7-41a2-49a7-801c-e9274b4ef33e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-18T04:14:38.363853Z","iopub.execute_input":"2022-04-18T04:14:38.364594Z","iopub.status.idle":"2022-04-18T04:14:38.380309Z","shell.execute_reply.started":"2022-04-18T04:14:38.364537Z","shell.execute_reply":"2022-04-18T04:14:38.379139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Dataset","metadata":{"_uuid":"4be546f7-7bd8-4913-8b7f-cb3856e361d7","_cell_guid":"81a47eda-3044-47c7-b476-30bdc89270ed","trusted":true}},{"cell_type":"code","source":"class ShopeeQueryDataset(Dataset):\n    \n    def __init__(self, imagePath, transform=None):\n        self.imagePath = imagePath\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.imagePath)\n    \n    def __getitem__(self, idx):\n \n        row = self.imagePath[idx]\n        # read image convert to RGB and apply augmentation\n        image = cv2.imread(row)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        # apply transformation\n        if self.transform:\n            aug = self.transform(image=image)\n            image = aug['image']\n        \n        return image, torch.tensor(1).long()","metadata":{"_uuid":"f914437a-1ef9-411f-9cc9-bd3a18ca019c","_cell_guid":"1d979582-d579-4598-b91f-7bc4c5c0c571","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-18T04:14:38.3821Z","iopub.execute_input":"2022-04-18T04:14:38.382453Z","iopub.status.idle":"2022-04-18T04:14:38.397354Z","shell.execute_reply.started":"2022-04-18T04:14:38.382409Z","shell.execute_reply":"2022-04-18T04:14:38.39596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_test_transforms():\n\n    return albumentations.Compose(\n        [\n            albumentations.Resize(CFG.img_size,CFG.img_size,always_apply=True),\n            albumentations.Normalize(),\n        ToTensorV2(p=1.0)\n        ]\n    )","metadata":{"_uuid":"e2ea00f5-e051-48d8-bbd1-35ccc3d35441","_cell_guid":"c229224d-5fce-4b7e-980d-317681783c65","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-18T04:14:38.398668Z","iopub.execute_input":"2022-04-18T04:14:38.40034Z","iopub.status.idle":"2022-04-18T04:14:38.409207Z","shell.execute_reply.started":"2022-04-18T04:14:38.400294Z","shell.execute_reply":"2022-04-18T04:14:38.408218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"528f4140-3aa7-41a9-9c6d-9e0148dea70e","_cell_guid":"a2bc1915-a067-4094-95c1-932b41222910","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{"_uuid":"70d8679c-9971-4dc2-a64a-bb0ab67ca5d1","_cell_guid":"71b79eda-5391-4bac-a255-f74c3bc5aad0","trusted":true}},{"cell_type":"markdown","source":"### Model 1 : Product Classfier Softmax Loss","metadata":{"_uuid":"7f5dbad2-57da-4a26-98d9-0e70db08371a","_cell_guid":"f63c9d05-6ff5-40f8-ac2e-cf94f60093cd","trusted":true}},{"cell_type":"code","source":"class ShopeeLabelGroupClassfier(nn.Module):\n    \n    def __init__(self,\n                     model_name='tf_efficientnet_b0',\n                     loss_fn='softmax',\n                     classes = CFG.classes,\n                     fc_dim = CFG.fc_dim,\n                     pretrained=False,\n                     use_fc=True,\n                     isTraining=False\n                ):\n        \n        \n        super(ShopeeLabelGroupClassfier,self).__init__()\n        \n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n        in_features = self.backbone.classifier.in_features\n        self.backbone.classifier = nn.Identity()\n        self.backbone.global_pool = nn.Identity()\n        self.pooling = nn.AdaptiveAvgPool2d(1)\n        self.use_fc = use_fc\n        self.loss_fn =loss_fn\n        self.isTraining = isTraining\n        \n        if self.use_fc:\n            self.dropout = nn.Dropout(0.2)\n            self.fc = nn.Linear(in_features,fc_dim )\n            self.bn = nn.BatchNorm1d(fc_dim)\n            in_features = fc_dim\n        self.loss_fn = loss_fn\n        \n        if self.loss_fn=='softmax':\n            self.final = nn.Linear(in_features, CFG.classes)\n    \n    def forward(self, image, label):\n        features = self.get_features(image)\n        if self.loss_fn=='softmax' and CFG.isTraining:\n            logits = self.final(features)\n            return logits\n        else:\n            return features\n    \n    def get_features(self,inp):\n        batch_dim = inp.shape[0]\n        inp = self.backbone(inp)\n        inp = self.pooling(inp).view(batch_dim, -1)\n        if self.use_fc and self.isTraining:\n            inp = self.dropout(inp)\n            inp = self.fc(inp)\n            inp = self.bn(inp)\n        return inp","metadata":{"_uuid":"c71c2aea-f918-453c-8a84-3f6efc47b73f","_cell_guid":"5c48bfb3-4207-45da-9c5a-db13a5f9e542","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-18T04:14:38.411065Z","iopub.execute_input":"2022-04-18T04:14:38.411727Z","iopub.status.idle":"2022-04-18T04:14:38.426756Z","shell.execute_reply.started":"2022-04-18T04:14:38.41167Z","shell.execute_reply":"2022-04-18T04:14:38.425705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model 2: Product Classfier ArcFace Loss","metadata":{"_uuid":"89d146b0-5da2-4ab2-bc08-0655646d93b3","_cell_guid":"cf4ab3c7-8d60-4ae2-8e40-ecc184d08192","trusted":true}},{"cell_type":"code","source":"class ArcFaceModule(nn.Module):\n    def __init__(self, in_features, out_features, scale, margin, easy_margin=False, ls_eps=0.0 ):\n        super(ArcFaceModule, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.scale = scale\n        self.margin = margin\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n        self.easy_margin=easy_margin\n        self.ls_eps=ls_eps\n        self.cos_m = math.cos(margin)\n        self.sin_m = math.sin(margin)\n        self.th = math.cos(math.pi - margin)\n        self.mm = math.sin(math.pi - margin) * margin\n        \n        \n        \n    \n    def forward(self, input, label):\n        \n        # cosine = X.W = ||X|| .||W|| . cos(theta) \n        # if X and W are normalize then dot product X, W = will be cos theta\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n        # phi = cos(theta + margin) = cos theta . cos(margin) -  sine theta .  sin(margin)\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n            \n        one_hot = torch.zeros(cosine.size(), device=CFG.device)\n        # one hot encoded\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n        #  output = label == True ? phi : cosine  \n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        # scale the output\n        output *= self.scale\n        # return cross entropy loss on scalled output\n        return output, nn.CrossEntropyLoss()(output,label)","metadata":{"_uuid":"ce74556d-16df-494c-940b-d92eea177a95","_cell_guid":"b7987449-4b63-4c73-ae97-64b58abf8cdd","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-18T04:14:38.428675Z","iopub.execute_input":"2022-04-18T04:14:38.429399Z","iopub.status.idle":"2022-04-18T04:14:38.446099Z","shell.execute_reply.started":"2022-04-18T04:14:38.429326Z","shell.execute_reply":"2022-04-18T04:14:38.44509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"   \nclass ShopeeEncoderBackBone(nn.Module):\n    \n    def __init__(self,\n                     model_name='tf_efficientnet_b3',\n                     loss_fn='ArcFace',\n                     classes = CFG.classes,\n                     fc_dim = CFG.fc_dim,\n                     pretrained=False,\n                     use_fc=True,\n                     isTraining=False\n                ):\n        \n        \n        super(ShopeeEncoderBackBone,self).__init__()\n        \n        # create bottlenack backbone network from pretrained model \n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n        in_features = self.backbone.classifier.in_features\n        self.backbone.classifier = nn.Identity()\n        self.backbone.global_pool = nn.Identity()\n        self.pooling = nn.AdaptiveAvgPool2d(1)\n        self.use_fc = use_fc\n        self.loss_fn =loss_fn\n        self.isTraining =isTraining\n        \n        # build top fc layers (Embedding that we are looking at testing time to represent the entire image)\n        # this will work as regularizer\n        if self.use_fc:\n            self.dropout = nn.Dropout(0.2)\n            self.fc = nn.Linear(in_features,fc_dim )\n            self.bn = nn.BatchNorm1d(fc_dim)\n            self.init_params()\n            in_features = fc_dim\n        self.loss_fn = loss_fn\n        if self.loss_fn=='softmax':\n            self.final = nn.Linear(in_features, CFG.classes)\n        elif self.loss_fn =='ArcFace':\n            self.final = ArcFaceModule( in_features,\n                                        CFG.classes,\n                                        scale = 30,\n                                        margin = 0.5,\n                                        easy_margin = False,\n                                        ls_eps = 0.0)\n            \n    def forward(self, image, label):\n        features = self.get_features(image)\n        if self.isTraining:\n            logits = self.final(features, label)\n            return logits\n        else:\n            return features\n    \n    def init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias,0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n        \n        \n    def get_features(self,inp):\n        batch_dim = inp.shape[0]\n        inp = self.backbone(inp)\n        inp = self.pooling(inp).view(batch_dim, -1)\n        if self.use_fc and self.isTraining:\n            inp = self.dropout(inp)\n            inp = self.fc(inp)\n            inp = self.bn(inp)\n            \n        return inp","metadata":{"_uuid":"be7433ed-4177-43c8-99a2-0fc966e3f055","_cell_guid":"4f170598-1947-459c-ac35-b4e1f906ece5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-18T04:14:38.448233Z","iopub.execute_input":"2022-04-18T04:14:38.448572Z","iopub.status.idle":"2022-04-18T04:14:38.467758Z","shell.execute_reply.started":"2022-04-18T04:14:38.448531Z","shell.execute_reply":"2022-04-18T04:14:38.466624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load trained model\n\ndef getPretrainedModel(loss_module='ArcFace', model_path=CFG.model_path_arcface, device=CFG.device) :\n    \n    if loss_module== 'ArcFace':\n        # load arcface loss classfier\n        model = ShopeeEncoderBackBone()\n        model.load_state_dict(torch.load(CFG.model_path_arcface, map_location=CFG.device))\n        model = model.to(CFG.device)\n        return model\n    else:\n        #load softmax classfier\n        model = ShopeeLabelGroupClassfier()\n        model.load_state_dict(torch.load(CFG.model_path_softmax, map_location=CFG.device))\n        model = model.to(CFG.device)\n        return model","metadata":{"_uuid":"57a5db41-a180-4377-bb80-0ebe3803fe70","_cell_guid":"edbb7aa3-04ce-40c4-ab9d-23b2b743fc78","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-18T04:14:38.472388Z","iopub.execute_input":"2022-04-18T04:14:38.472707Z","iopub.status.idle":"2022-04-18T04:14:38.482831Z","shell.execute_reply.started":"2022-04-18T04:14:38.472667Z","shell.execute_reply":"2022-04-18T04:14:38.481857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generate Embeddings","metadata":{"_uuid":"7e4386fa-c6ea-4723-af8f-086c4021c155","_cell_guid":"ba4f9580-cb17-46fc-8c3e-d8b39cf402e6","trusted":true}},{"cell_type":"code","source":"def get_images_path(df, root_dir,isRandomImage=False):\n    if not isRandomImage:\n        imagepaths = [ root_dir + \"/\"+image for image in df['image'].tolist()]\n        return imagepaths\n    else :\n        return []","metadata":{"_uuid":"13c52127-99c2-445f-b4c7-bc0589c857cb","_cell_guid":"49fd2c07-d13b-4765-8332-b3512cc4d5f2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-18T04:14:38.484675Z","iopub.execute_input":"2022-04-18T04:14:38.485007Z","iopub.status.idle":"2022-04-18T04:14:38.494025Z","shell.execute_reply.started":"2022-04-18T04:14:38.484953Z","shell.execute_reply":"2022-04-18T04:14:38.492972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getEmbeddings(queryImagesPath, model, transform=None):\n    # create dataset from image paths\n    query_dataset = ShopeeQueryDataset(queryImagesPath,  transform = transform)\n    \n    # create dataloader\n    query_dataloader = torch.utils.data.DataLoader(\n                                                query_dataset,\n        batch_size=16\n    )\n    \n    \n    # put model in evaluation mode\n    model.eval()\n    embeddings = []\n    with torch.no_grad():\n         \n        for idx, datax  in tqdm(enumerate(query_dataloader)):\n            image, label = datax\n            image = image.to(CFG.device)\n            label = label.to(CFG.device)\n            # forward pass to get features\n            features = model(image, label)\n            image_embeddings = features.detach().cpu().numpy()\n            embeddings.append(image_embeddings)\n            \n            \n    image_embeddings = np.concatenate(embeddings)\n            \n    return image_embeddings","metadata":{"_uuid":"33669ecb-a70b-4734-9e26-1839615ee5fc","_cell_guid":"0b6dbca5-0cec-4d83-8be7-4d2cd182638f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-18T04:14:38.495267Z","iopub.execute_input":"2022-04-18T04:14:38.498135Z","iopub.status.idle":"2022-04-18T04:14:38.50727Z","shell.execute_reply.started":"2022-04-18T04:14:38.498103Z","shell.execute_reply":"2022-04-18T04:14:38.506107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_neighbors( train_embeddings, query_embeddings,KNN=50, metric_param='cosine'):\n    # we can get top neighbors based on different distance metric,in our case we are using \n    # cosine and euclidean metric\n    if metric_param == 'cosine':\n        # fit cosine distance medal on train image embddings\n        cosine_knnModel = get_knn_model(train_embeddings, KNN=KNN, metric='cosine')\n        # get top k neighbors distances and indices given metric for query embeddings\n        distances, indices = cosine_knnModel.kneighbors(query_embeddings)\n\n    else:\n        # fit euclidean distance modal on image embeddings\n        eucl_knnModel = get_knn_model(train_embeddings, KNN=KNN, metric='minkowski')\n        # get top k neighbors distances and indices given metric for query embeddings\n        distances, indices = eucl_knnModel.kneighbors(query_embeddings)\n    \n    return distances, indices","metadata":{"execution":{"iopub.status.busy":"2022-04-18T04:14:38.508979Z","iopub.execute_input":"2022-04-18T04:14:38.509698Z","iopub.status.idle":"2022-04-18T04:14:38.521578Z","shell.execute_reply.started":"2022-04-18T04:14:38.509657Z","shell.execute_reply":"2022-04-18T04:14:38.520453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get nearest neighbors distances and index information\n\ndef get_knn_model(embeddings,KNN=50, metric='cosine'):\n    knnModel = NearestNeighbors(n_neighbors=KNN,metric=metric)\n    knnModel.fit(embeddings)\n#         distances, indices = knnModel.kneighbors(image_embeddings)\n    \n    return knnModel","metadata":{"_uuid":"af7131f1-32f0-44a4-8652-593fc9e13e18","_cell_guid":"ca53a8d9-4bc0-433b-9651-367143c0ceb1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-18T04:14:38.523352Z","iopub.execute_input":"2022-04-18T04:14:38.523711Z","iopub.status.idle":"2022-04-18T04:14:38.5328Z","shell.execute_reply.started":"2022-04-18T04:14:38.523668Z","shell.execute_reply":"2022-04-18T04:14:38.531755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/shopee-product-matching/train.csv')\n# get  Training image  path\ntrain_image_paths = get_images_path(train_df, CFG.TRAIN_DIR)","metadata":{"_uuid":"105d3ab1-d4fa-4630-9099-b821360d1c9a","_cell_guid":"4208943c-fafe-4081-afc8-3ad2e8f2d04c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-18T04:14:38.534946Z","iopub.execute_input":"2022-04-18T04:14:38.535234Z","iopub.status.idle":"2022-04-18T04:14:38.734996Z","shell.execute_reply.started":"2022-04-18T04:14:38.535166Z","shell.execute_reply":"2022-04-18T04:14:38.733976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  get Training Image embeddings and save it for later use\ntest_transform =get_test_transforms()\nshopee_model = getPretrainedModel(loss_module='ArcFace',model_path=CFG.model_path_arcface, device=CFG.device)\ntrain_image_embeddings = getEmbeddings(train_image_paths, shopee_model, transform=test_transform)\nnp.save(\"training_image_embeddings\", train_image_embeddings)","metadata":{"_uuid":"896557e3-d1a2-4298-88a8-db938be9a6c7","_cell_guid":"32ad3495-0bb7-4890-8982-5fd94de0476b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-18T04:14:38.737599Z","iopub.execute_input":"2022-04-18T04:14:38.738221Z","iopub.status.idle":"2022-04-18T04:35:03.672036Z","shell.execute_reply.started":"2022-04-18T04:14:38.73816Z","shell.execute_reply":"2022-04-18T04:35:03.671003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's get Query Embeddings ","metadata":{}},{"cell_type":"code","source":"train_image_embeddings.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-18T04:35:03.673798Z","iopub.execute_input":"2022-04-18T04:35:03.674129Z","iopub.status.idle":"2022-04-18T04:35:03.683213Z","shell.execute_reply.started":"2022-04-18T04:35:03.674082Z","shell.execute_reply":"2022-04-18T04:35:03.682157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from os import listdir\nfrom os.path import isfile, join\nTEST_DIR='../input/sample-testing'\ntest_images_list = [join(TEST_DIR, f) for f in listdir(TEST_DIR) if isfile(join(TEST_DIR, f))]\n\ntest_images_list","metadata":{"execution":{"iopub.status.busy":"2022-04-18T04:35:03.684984Z","iopub.execute_input":"2022-04-18T04:35:03.685722Z","iopub.status.idle":"2022-04-18T04:35:03.705927Z","shell.execute_reply.started":"2022-04-18T04:35:03.685678Z","shell.execute_reply":"2022-04-18T04:35:03.704812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"query_image_embeddings = getEmbeddings(test_images_list, shopee_model, transform=test_transform)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-18T04:35:03.707379Z","iopub.execute_input":"2022-04-18T04:35:03.70786Z","iopub.status.idle":"2022-04-18T04:35:03.993753Z","shell.execute_reply.started":"2022-04-18T04:35:03.707818Z","shell.execute_reply":"2022-04-18T04:35:03.992752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metric_param = 'cosine'\nquery_cosine_distances, query_cosine_indices = get_neighbors(\n                                    train_embeddings = train_image_embeddings,\n                                    query_embeddings = query_image_embeddings,\n                                    KNN=50,\n                                    metric_param='cosine'\n                                )\n\nprint(query_cosine_distances.shape)\nprint(query_cosine_indices.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T04:35:03.995629Z","iopub.execute_input":"2022-04-18T04:35:03.996259Z","iopub.status.idle":"2022-04-18T04:35:06.130753Z","shell.execute_reply.started":"2022-04-18T04:35:03.996217Z","shell.execute_reply":"2022-04-18T04:35:06.129758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's visualize our Model Results","metadata":{"_uuid":"926b748f-c474-4a1c-8a1f-27b286afcffb","_cell_guid":"1a17c65a-eef4-44c3-b63c-6dd755870980","trusted":true}},{"cell_type":"code","source":"\n\ndef plot_canvas(train, COLS=4, ROWS=2, path=CFG.TRAIN_DIR+\"/\",img_list=[],k=0):\n    \n    for m in range(ROWS): \n        plt.figure(figsize=(20,5))\n        for j in range(COLS): \n            if j == 0 and m == 0:\n                title = \"Query Image \\n\"\n                title += \"Downloaded from internet : \\n\"\n                img = cv2.imread(img_list[k])\n            else:\n                row = COLS*m + j \n                name = train.iloc[row-1,1]\n                img = cv2.imread(path+name)\n\n                title = \"Recommended Image {} \\n\".format(row-1)\n                orig_title = train.iloc[row-1,3]\n                punctuation= '''!()-[]{};:'\"\\, <>./?@#$%^&*_~'''\n                for x in punctuation:\n                    orig_title=orig_title.replace(x,\"\")\n                title  += \"title :\" + orig_title[:min(15, len(orig_title))] + \" \\n\"\n            img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB )\n            plt.subplot(1,COLS,j+1)\n            plt.title(title)\n            plt.axis('off')\n            plt.imshow(img)\n        plt.show()\n\n","metadata":{"_uuid":"bf857e23-23c0-4e7e-8387-98cc57c279cc","_cell_guid":"1030216c-aee1-4d25-9a16-93cdf3a33707","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-18T04:35:06.132151Z","iopub.execute_input":"2022-04-18T04:35:06.132993Z","iopub.status.idle":"2022-04-18T04:35:06.146011Z","shell.execute_reply.started":"2022-04-18T04:35:06.132951Z","shell.execute_reply":"2022-04-18T04:35:06.144626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize Top k Recommendation","metadata":{"_uuid":"6a601aad-205d-482f-8334-27f70eae1dcd","_cell_guid":"9c963628-2f74-47fa-9c30-2cae67902bf6","trusted":true}},{"cell_type":"markdown","source":"### Visualize Results based on Cosine Distance","metadata":{"_uuid":"f974b957-0a0a-4fac-acf4-1009cc5d7cfb","_cell_guid":"51c71a46-a4a6-4c50-89f3-3f747dc46141","trusted":true}},{"cell_type":"code","source":"indices = [0,1,2,3,4,12]\nfor k in indices:\n\n    plt.figure(figsize=(20,3))\n    plt.plot(np.arange(50),cupy.asnumpy(query_cosine_distances[k,]),'o-')\n    plt.title('Image {} Distance From Train Row {} to Other Train Rows'.format(\"cosine\",k),size=16)\n    plt.ylabel('{} Distance to Train Row {}'.format(\"cosine\", k),size=14)\n    plt.xlabel('Index Sorted by {} Distance to Train Row {}'.format(\"cosine\",k),size=14)\n    plt.show()\n    \n    cluster = train_df.loc[cupy.asnumpy(query_cosine_indices[k,:8])] \n    plot_canvas(cluster, COLS=5, ROWS=1, path=CFG.TRAIN_DIR+\"/\",img_list=test_images_list,k=k)\n#     plot_canvas(cluster, random=False, ROWS=2, COLS=4, isRecommending=True, test_images_list=test_images_list,k=k)","metadata":{"_uuid":"f42e0c10-0b27-4248-a63f-e70c02b319a1","_cell_guid":"853b0216-1167-497a-88f5-4edcbd407dbe","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-18T04:35:06.147463Z","iopub.execute_input":"2022-04-18T04:35:06.147845Z","iopub.status.idle":"2022-04-18T04:35:11.437928Z","shell.execute_reply.started":"2022-04-18T04:35:06.147803Z","shell.execute_reply":"2022-04-18T04:35:11.437082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualize Results based on Euclidean Distance","metadata":{"_uuid":"ec676eb7-8322-4089-9d47-afd9cfaa0176","_cell_guid":"e8e03293-8aab-48e8-a61f-62fedf2c9021","trusted":true}},{"cell_type":"code","source":"\nquery_euc_distances, query_euc_indices = get_neighbors(\n                                    train_embeddings = train_image_embeddings,\n                                    query_embeddings = query_image_embeddings,\n                                    KNN=50,\n                                    metric_param='euclidean'\n                                )\n\n","metadata":{"_uuid":"06f3a633-2594-4c91-bb10-15766741bca1","_cell_guid":"bbd7ca5e-a606-4ad5-b8f0-addecbb03fd7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-18T04:35:11.43935Z","iopub.execute_input":"2022-04-18T04:35:11.440188Z","iopub.status.idle":"2022-04-18T04:35:11.647333Z","shell.execute_reply.started":"2022-04-18T04:35:11.440133Z","shell.execute_reply":"2022-04-18T04:35:11.646454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for k in (indices):\n\n    plt.figure(figsize=(20,3))\n    plt.plot(np.arange(50),cupy.asnumpy(query_euc_distances[k,]),'o-')\n    plt.title('Image {} Distance From Train Row {} to Other Train Rows'.format(\"euclidean\",k),size=16)\n    plt.ylabel('{} Distance to Train Row {}'.format(\"euclidean\", k),size=14)\n    plt.xlabel('Index Sorted by {} Distance to Train Row {}'.format(\"euclidean\",k),size=14)\n    plt.show()\n    \n    cluster = train_df.loc[cupy.asnumpy(query_euc_indices[k,:8])] \n    plot_canvas(cluster, COLS=5, ROWS=1, path=CFG.TRAIN_DIR+\"/\",img_list=test_images_list,k=k)\n    #plot_canvas(cluster, random=False, ROWS=2, COLS=4, isRecommending=True, test_images_list=test_images_list,k=k)","metadata":{"_uuid":"43e60236-edfc-46da-97db-ad6587ce1664","_cell_guid":"ec1afb61-35e8-4f28-b186-0d2d66c95643","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-18T04:35:11.648922Z","iopub.execute_input":"2022-04-18T04:35:11.64961Z","iopub.status.idle":"2022-04-18T04:35:17.802954Z","shell.execute_reply.started":"2022-04-18T04:35:11.649538Z","shell.execute_reply":"2022-04-18T04:35:17.802045Z"},"trusted":true},"execution_count":null,"outputs":[]}]}