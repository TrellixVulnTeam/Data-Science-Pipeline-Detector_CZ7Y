{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nsys.path.append('../input/timmmaster')\nimport timm","metadata":{"_uuid":"69763b81-c7c2-4a51-9a34-601f10c275b9","_cell_guid":"2f1831eb-2b76-4536-863c-df6810af59a1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-16T22:52:02.408634Z","iopub.execute_input":"2022-04-16T22:52:02.408888Z","iopub.status.idle":"2022-04-16T22:52:02.416555Z","shell.execute_reply.started":"2022-04-16T22:52:02.40886Z","shell.execute_reply":"2022-04-16T22:52:02.415635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preliminaries\nfrom tqdm import tqdm\nimport math\nimport random\nimport os\nimport pandas as pd\nimport numpy as np\n\n# Visuals and CV2\nimport cv2\n\n# albumentations for augs\nimport albumentations\nfrom albumentations.pytorch.transforms import ToTensorV2\n\n#torch\nimport torch\nimport timm\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset,DataLoader\n\n\nimport gc\nimport matplotlib.pyplot as plt\nimport cudf\nimport cuml\nimport cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml import PCA\nfrom cuml.neighbors import NearestNeighbors","metadata":{"_uuid":"c59548a1-cb49-4a42-adfb-3a2443310cee","_cell_guid":"950e4d7b-1263-4ffb-a67b-316bd31f5d58","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-16T22:52:02.418718Z","iopub.execute_input":"2022-04-16T22:52:02.419252Z","iopub.status.idle":"2022-04-16T22:52:02.427667Z","shell.execute_reply.started":"2022-04-16T22:52:02.419216Z","shell.execute_reply":"2022-04-16T22:52:02.426889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# configuration class\n\nclass CFG:\n    loss_module='ArcFace'\n    TRAIN_DIR='../input/shopee-product-matching/train_images'\n    TEST_DIR='../input/shopee-product-matching/test_images'\n    seed = 123 \n    img_size = 512\n    classes = 11014\n    fc_dim = 512\n    epochs = 25\n    batch_size = 12\n    num_workers = 3\n    model_name = 'tf_efficientnet_b3'\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    model_path_arcface='../input/pretrained-b3/Train_F1_score_0.9061769859813084valid_f1_score0.4245035046728972_Epoch_0_lr_start_2.23e-05_lr_max_0.00016_softmax_512x512_tf_efficientnet_b0.pt'\n    model_path_softmax = '../input/label-classfier-model/2022-04-15_softmax_512x512_tf_efficientnet_b4.pt'\n    # # check true when we want to train the model\n    isTraining=False","metadata":{"_uuid":"4a618b9d-6c35-4692-9c42-e290609687df","_cell_guid":"06175366-72c7-4cff-8bb4-2ad767a0819b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-16T22:52:02.429106Z","iopub.execute_input":"2022-04-16T22:52:02.429407Z","iopub.status.idle":"2022-04-16T22:52:02.44124Z","shell.execute_reply.started":"2022-04-16T22:52:02.429374Z","shell.execute_reply":"2022-04-16T22:52:02.440487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading Data","metadata":{"_uuid":"f672af75-837f-43be-a1a8-3e58e4078804","_cell_guid":"0bdf3efa-a839-4029-a4c5-77d750945c1b","trusted":true}},{"cell_type":"code","source":"def read_dataset():\n\n    # if not in testing phase read train dataset else test dataset\n    df = pd.read_csv('../input/shopee-product-matching/train.csv')\n    # we have information that label_group is same for similar kind of product\n    # let's use this to get F1 score for our final model\n    tmp = df.groupby(['label_group'])['posting_id'].unique().to_dict()\n    df['matches'] = df['label_group'].map(tmp)\n    df['matches'] = df['matches'].apply(lambda x: ' '.join(x))\n    # get cuda frame for faster GPU computation\n    df_cu = cudf.DataFrame(df)\n    \n        \n    return df, df_cu","metadata":{"_uuid":"1292e0e5-ed4d-447d-b334-6c2475ce6da0","_cell_guid":"20cfe4b7-41a2-49a7-801c-e9274b4ef33e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-16T22:52:02.442397Z","iopub.execute_input":"2022-04-16T22:52:02.44274Z","iopub.status.idle":"2022-04-16T22:52:02.451902Z","shell.execute_reply.started":"2022-04-16T22:52:02.442685Z","shell.execute_reply":"2022-04-16T22:52:02.451244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Dataset","metadata":{"_uuid":"4be546f7-7bd8-4913-8b7f-cb3856e361d7","_cell_guid":"81a47eda-3044-47c7-b476-30bdc89270ed","trusted":true}},{"cell_type":"code","source":"class ShopeeQueryDataset(Dataset):\n    \n    def __init__(self, imagePath, transform=None):\n        self.imagePath = imagePath\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.imagePath)\n    \n    def __getitem__(self, idx):\n \n        row = self.imagePath[idx]\n        # read image convert to RGB and apply augmentation\n        image = cv2.imread(row)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        # apply transformation\n        if self.transform:\n            aug = self.transform(image=image)\n            image = aug['image']\n        \n        return image, torch.tensor(1).long()","metadata":{"_uuid":"f914437a-1ef9-411f-9cc9-bd3a18ca019c","_cell_guid":"1d979582-d579-4598-b91f-7bc4c5c0c571","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-16T22:52:02.453836Z","iopub.execute_input":"2022-04-16T22:52:02.454173Z","iopub.status.idle":"2022-04-16T22:52:02.461604Z","shell.execute_reply.started":"2022-04-16T22:52:02.454138Z","shell.execute_reply":"2022-04-16T22:52:02.460742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_test_transforms():\n\n    return albumentations.Compose(\n        [\n            albumentations.Resize(CFG.img_size,CFG.img_size,always_apply=True),\n            albumentations.Normalize(),\n        ToTensorV2(p=1.0)\n        ]\n    )","metadata":{"_uuid":"e2ea00f5-e051-48d8-bbd1-35ccc3d35441","_cell_guid":"c229224d-5fce-4b7e-980d-317681783c65","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-16T22:52:02.462941Z","iopub.execute_input":"2022-04-16T22:52:02.463464Z","iopub.status.idle":"2022-04-16T22:52:02.473193Z","shell.execute_reply.started":"2022-04-16T22:52:02.463324Z","shell.execute_reply":"2022-04-16T22:52:02.472387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"528f4140-3aa7-41a9-9c6d-9e0148dea70e","_cell_guid":"a2bc1915-a067-4094-95c1-932b41222910","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{"_uuid":"70d8679c-9971-4dc2-a64a-bb0ab67ca5d1","_cell_guid":"71b79eda-5391-4bac-a255-f74c3bc5aad0","trusted":true}},{"cell_type":"markdown","source":"### Model 1 : Product Classfier Softmax Loss","metadata":{"_uuid":"7f5dbad2-57da-4a26-98d9-0e70db08371a","_cell_guid":"f63c9d05-6ff5-40f8-ac2e-cf94f60093cd","trusted":true}},{"cell_type":"code","source":"class ShopeeLabelGroupClassfier(nn.Module):\n    \n    def __init__(self,\n                     model_name='tf_efficientnet_b0',\n                     loss_fn='softmax',\n                     classes = CFG.classes,\n                     fc_dim = CFG.fc_dim,\n                     pretrained=False,\n                     use_fc=True,\n                     isTraining=False\n                ):\n        \n        \n        super(ShopeeLabelGroupClassfier,self).__init__()\n        \n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n        in_features = self.backbone.classifier.in_features\n        self.backbone.classifier = nn.Identity()\n        self.backbone.global_pool = nn.Identity()\n        self.pooling = nn.AdaptiveAvgPool2d(1)\n        self.use_fc = use_fc\n        self.loss_fn =loss_fn\n        self.isTraining = isTraining\n        \n        if self.use_fc:\n            self.dropout = nn.Dropout(0.2)\n            self.fc = nn.Linear(in_features,fc_dim )\n            self.bn = nn.BatchNorm1d(fc_dim)\n            in_features = fc_dim\n        self.loss_fn = loss_fn\n        \n        if self.loss_fn=='softmax':\n            self.final = nn.Linear(in_features, CFG.classes)\n    \n    def forward(self, image, label):\n        features = self.get_features(image)\n        if self.loss_fn=='softmax' and CFG.isTraining:\n            logits = self.final(features)\n            return logits\n        else:\n            return features\n    \n    def get_features(self,inp):\n        batch_dim = inp.shape[0]\n        inp = self.backbone(inp)\n        inp = self.pooling(inp).view(batch_dim, -1)\n        if self.use_fc and self.isTraining:\n            inp = self.dropout(inp)\n            inp = self.fc(inp)\n            inp = self.bn(inp)\n        return inp","metadata":{"_uuid":"c71c2aea-f918-453c-8a84-3f6efc47b73f","_cell_guid":"5c48bfb3-4207-45da-9c5a-db13a5f9e542","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-16T22:52:02.475154Z","iopub.execute_input":"2022-04-16T22:52:02.475504Z","iopub.status.idle":"2022-04-16T22:52:02.490975Z","shell.execute_reply.started":"2022-04-16T22:52:02.47547Z","shell.execute_reply":"2022-04-16T22:52:02.490264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model 2: Product Classfier ArcFace Loss","metadata":{"_uuid":"89d146b0-5da2-4ab2-bc08-0655646d93b3","_cell_guid":"cf4ab3c7-8d60-4ae2-8e40-ecc184d08192","trusted":true}},{"cell_type":"code","source":"class ArcFaceModule(nn.Module):\n    def __init__(self, in_features, out_features, scale, margin, easy_margin=False, ls_eps=0.0 ):\n        super(ArcFaceModule, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.scale = scale\n        self.margin = margin\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n        self.easy_margin=easy_margin\n        self.ls_eps=ls_eps\n        self.cos_m = math.cos(margin)\n        self.sin_m = math.sin(margin)\n        self.th = math.cos(math.pi - margin)\n        self.mm = math.sin(math.pi - margin) * margin\n        \n        \n        \n    \n    def forward(self, input, label):\n        \n        # cosine = X.W = ||X|| .||W|| . cos(theta) \n        # if X and W are normalize then dot product X, W = will be cos theta\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n        # phi = cos(theta + margin) = cos theta . cos(margin) -  sine theta .  sin(margin)\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n            \n        one_hot = torch.zeros(cosine.size(), device=CFG.device)\n        # one hot encoded\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n        #  output = label == True ? phi : cosine  \n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        # scale the output\n        output *= self.scale\n        # return cross entropy loss on scalled output\n        return output, nn.CrossEntropyLoss()(output,label)","metadata":{"_uuid":"ce74556d-16df-494c-940b-d92eea177a95","_cell_guid":"b7987449-4b63-4c73-ae97-64b58abf8cdd","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-16T22:52:02.492571Z","iopub.execute_input":"2022-04-16T22:52:02.493546Z","iopub.status.idle":"2022-04-16T22:52:02.508389Z","shell.execute_reply.started":"2022-04-16T22:52:02.493493Z","shell.execute_reply":"2022-04-16T22:52:02.507569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"   \nclass ShopeeEncoderBackBone(nn.Module):\n    \n    def __init__(self,\n                     model_name='tf_efficientnet_b3',\n                     loss_fn='ArcFace',\n                     classes = CFG.classes,\n                     fc_dim = CFG.fc_dim,\n                     pretrained=False,\n                     use_fc=True,\n                     isTraining=False\n                ):\n        \n        \n        super(ShopeeEncoderBackBone,self).__init__()\n        \n        # create bottlenack backbone network from pretrained model \n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n        in_features = self.backbone.classifier.in_features\n        self.backbone.classifier = nn.Identity()\n        self.backbone.global_pool = nn.Identity()\n        self.pooling = nn.AdaptiveAvgPool2d(1)\n        self.use_fc = use_fc\n        self.loss_fn =loss_fn\n        self.isTraining =isTraining\n        \n        # build top fc layers (Embedding that we are looking at testing time to represent the entire image)\n        # this will work as regularizer\n        if self.use_fc:\n            self.dropout = nn.Dropout(0.2)\n            self.fc = nn.Linear(in_features,fc_dim )\n            self.bn = nn.BatchNorm1d(fc_dim)\n            self.init_params()\n            in_features = fc_dim\n        self.loss_fn = loss_fn\n        if self.loss_fn=='softmax':\n            self.final = nn.Linear(in_features, CFG.classes)\n        elif self.loss_fn =='ArcFace':\n            self.final = ArcFaceModule( in_features,\n                                        CFG.classes,\n                                        scale = 30,\n                                        margin = 0.5,\n                                        easy_margin = False,\n                                        ls_eps = 0.0)\n            \n    def forward(self, image, label):\n        features = self.get_features(image)\n        if self.isTraining:\n            logits = self.final(features, label)\n            return logits\n        else:\n            return features\n    \n    def init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias,0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n        \n        \n    def get_features(self,inp):\n        batch_dim = inp.shape[0]\n        inp = self.backbone(inp)\n        inp = self.pooling(inp).view(batch_dim, -1)\n        if self.use_fc and self.isTraining:\n            inp = self.dropout(inp)\n            inp = self.fc(inp)\n            inp = self.bn(inp)\n            \n        return inp","metadata":{"_uuid":"be7433ed-4177-43c8-99a2-0fc966e3f055","_cell_guid":"4f170598-1947-459c-ac35-b4e1f906ece5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-16T22:52:02.509727Z","iopub.execute_input":"2022-04-16T22:52:02.510086Z","iopub.status.idle":"2022-04-16T22:52:02.525238Z","shell.execute_reply.started":"2022-04-16T22:52:02.510052Z","shell.execute_reply":"2022-04-16T22:52:02.524358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load trained model\n\ndef getPretrainedModel(loss_module='ArcFace', model_path=CFG.model_path_arcface, device=CFG.device) :\n    \n    if loss_module== 'ArcFace':\n        # load arcface loss classfier\n        model = ShopeeEncoderBackBone()\n        model.load_state_dict(torch.load(CFG.model_path_arcface, map_location=CFG.device))\n        model = model.to(CFG.device)\n        return model\n    else:\n        #load softmax classfier\n        model = ShopeeLabelGroupClassfier()\n        model.load_state_dict(torch.load(CFG.model_path_softmax, map_location=CFG.device))\n        model = model.to(CFG.device)\n        return model","metadata":{"_uuid":"57a5db41-a180-4377-bb80-0ebe3803fe70","_cell_guid":"edbb7aa3-04ce-40c4-ab9d-23b2b743fc78","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-16T22:52:02.527884Z","iopub.execute_input":"2022-04-16T22:52:02.5283Z","iopub.status.idle":"2022-04-16T22:52:02.537566Z","shell.execute_reply.started":"2022-04-16T22:52:02.528263Z","shell.execute_reply":"2022-04-16T22:52:02.536762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generate Embeddings","metadata":{"_uuid":"7e4386fa-c6ea-4723-af8f-086c4021c155","_cell_guid":"ba4f9580-cb17-46fc-8c3e-d8b39cf402e6","trusted":true}},{"cell_type":"code","source":"def get_images_path(df, root_dir):\n    imagepaths = [ root_dir + \"/\"+image for image in df['image'].tolist()]\n    return imagepaths","metadata":{"_uuid":"13c52127-99c2-445f-b4c7-bc0589c857cb","_cell_guid":"49fd2c07-d13b-4765-8332-b3512cc4d5f2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-16T22:52:02.539409Z","iopub.execute_input":"2022-04-16T22:52:02.53966Z","iopub.status.idle":"2022-04-16T22:52:02.545808Z","shell.execute_reply.started":"2022-04-16T22:52:02.539626Z","shell.execute_reply":"2022-04-16T22:52:02.545059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getEmbeddings(queryImagesPath, model, transform=None):\n    # create dataset from image paths\n    query_dataset = ShopeeQueryDataset(queryImagesPath,  transform = transform)\n    \n    # create dataloader\n    query_dataloader = torch.utils.data.DataLoader(\n                                                query_dataset,\n        batch_size=16\n    )\n    \n    \n    # put model in evaluation mode\n    model.eval()\n    embeddings = []\n    with torch.no_grad():\n         \n        for idx, datax  in tqdm(enumerate(query_dataloader)):\n            image, label = datax\n            image = image.to(CFG.device)\n            label = label.to(CFG.device)\n            # forward pass to get features\n            features = model(image, label)\n            image_embeddings = features.detach().cpu().numpy()\n            embeddings.append(image_embeddings)\n            \n            \n    image_embeddings = np.concatenate(embeddings)\n            \n    return image_embeddings","metadata":{"_uuid":"33669ecb-a70b-4734-9e26-1839615ee5fc","_cell_guid":"0b6dbca5-0cec-4d83-8be7-4d2cd182638f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-16T22:52:02.547161Z","iopub.execute_input":"2022-04-16T22:52:02.547678Z","iopub.status.idle":"2022-04-16T22:52:02.557079Z","shell.execute_reply.started":"2022-04-16T22:52:02.54756Z","shell.execute_reply":"2022-04-16T22:52:02.556318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def get_neighbors(df, embeddings, KNN = 50, isImage=False):\n    \n\n#     # cuml neighbors\n#     model = NearestNeighbors(n_neighbors = KNN)\n#     model.fit(embeddings)\n#     distances, indices = model.kneighbors(embeddings)\n#     print(distances.shape)\n#     predictions = []\n#     for k in tqdm(range(embeddings.shape[0])):\n#         if isImage:\n#             idx = np.where(distances[k,] < 3.1)[0]\n#         else:\n#             idx = np.where(distances[k,] < 0.70)[0]\n#         ids = indices[k,idx]\n#         posting_ids = df['posting_id'].iloc[ids].values\n#         predictions.append(posting_ids)\n        \n#     del model, distances, indices\n#     gc.collect()\n#     return df, predictions","metadata":{"_uuid":"9f2ebca7-075b-4221-a49f-afa3ef102844","_cell_guid":"dbf2cb5f-7dbf-44e2-aee5-05323759beb7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-16T22:52:02.558665Z","iopub.execute_input":"2022-04-16T22:52:02.559063Z","iopub.status.idle":"2022-04-16T22:52:02.568254Z","shell.execute_reply.started":"2022-04-16T22:52:02.558925Z","shell.execute_reply":"2022-04-16T22:52:02.567489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get nearest neighbors distances and index information\n\ndef get_knn_model(embeddings,KNN=50, metric='cosine'):\n    knnModel = NearestNeighbors(n_neighbors=KNN,metric=metric)\n    knnModel.fit(embeddings)\n#         distances, indices = knnModel.kneighbors(image_embeddings)\n    \n    return knnModel","metadata":{"_uuid":"af7131f1-32f0-44a4-8652-593fc9e13e18","_cell_guid":"ca53a8d9-4bc0-433b-9651-367143c0ceb1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-16T22:52:02.569669Z","iopub.execute_input":"2022-04-16T22:52:02.569982Z","iopub.status.idle":"2022-04-16T22:52:02.577448Z","shell.execute_reply.started":"2022-04-16T22:52:02.56988Z","shell.execute_reply":"2022-04-16T22:52:02.576607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def search_similar_images(queryFeatures, index, maxResults=5):\n#     results=[]\n    \n#     # loop over our index\n#     for i in range(0, len(index[\"features\"])):\n#         # compute the  distance euclidean between our query features\n#         # and the features for the current image in our index, then\n#         dist = euclidean(queryFeatures, index[\"features\"][i])\n#         results.append((dist, index['indexes'][i]))\n\n#     # sort the results and grab the top ones\n#     results = sorted(results)[:maxResults]\n\n#     # return the list of results\n#     return results","metadata":{"_uuid":"9637a14d-0e26-4fdc-9ec2-add5a05a6d80","_cell_guid":"ec543450-aa89-4875-8d80-38243e8caf95","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-16T22:52:02.578709Z","iopub.execute_input":"2022-04-16T22:52:02.579305Z","iopub.status.idle":"2022-04-16T22:52:02.585032Z","shell.execute_reply.started":"2022-04-16T22:52:02.579269Z","shell.execute_reply":"2022-04-16T22:52:02.584276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/shopee-product-matching/train.csv')\n# get  Training image  path\ntrain_image_paths = get_images_path(train_df, CFG.TRAIN_DIR)","metadata":{"_uuid":"105d3ab1-d4fa-4630-9099-b821360d1c9a","_cell_guid":"4208943c-fafe-4081-afc8-3ad2e8f2d04c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-16T22:52:02.588564Z","iopub.execute_input":"2022-04-16T22:52:02.589254Z","iopub.status.idle":"2022-04-16T22:52:02.689579Z","shell.execute_reply.started":"2022-04-16T22:52:02.589181Z","shell.execute_reply":"2022-04-16T22:52:02.688895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  get Training Image embeddings and save it for later use\ntest_transform =get_test_transforms()\nshopee_model = getPretrainedModel(loss_module='ArcFace',model_path=CFG.model_path_arcface, device=CFG.device)\ntrain_image_embeddings = getEmbeddings(train_image_paths, shopee_model, transform=test_transform)\nnp.save(\"training_image_embeddings\", train_image_embeddings)","metadata":{"_uuid":"896557e3-d1a2-4298-88a8-db938be9a6c7","_cell_guid":"32ad3495-0bb7-4890-8982-5fd94de0476b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-16T22:52:02.691467Z","iopub.execute_input":"2022-04-16T22:52:02.691663Z","iopub.status.idle":"2022-04-16T23:06:35.894489Z","shell.execute_reply.started":"2022-04-16T22:52:02.691639Z","shell.execute_reply":"2022-04-16T23:06:35.893404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's visualize our Model Results","metadata":{"_uuid":"926b748f-c474-4a1c-8a1f-27b286afcffb","_cell_guid":"1a17c65a-eef4-44c3-b63c-6dd755870980","trusted":true}},{"cell_type":"code","source":"# punctuation= '''!()-[]{};:'\"\\, <>./?@#$%^&*_~'''\n# myString= \"Python.:F}or{Beg~inn;ers\"\n# print(\"Input String is:\")\n# print(myString)\n# emptyString=\"\"\n# for x in punctuation:\n#     myString=myString.replace(x,emptyString)\n# print(\"Output String is:\")\n# print(myString)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ndef plot_canvas(train, random=False, COLS=4, ROWS=2, path=CFG.TRAIN_DIR+\"/\", isRecommending=False):\n    \n    for k in range(ROWS): \n        plt.figure(figsize=(20,5))\n        for j in range(COLS): \n            if random: row = np.random.randint(0,len(train))\n            else: row = COLS*k + j \n            name = train.iloc[row,1]\n            title = train.iloc[row,3]\n            posting_id = train.iloc[row,0]\n            label_g = train.iloc[row,4]\n            if isRecommending:\n                if j == 0 and row==0 :\n                    title_with_return = \"Query Image \\n\"\n                    title_with_return += \"-\"*20 + \"\\n Title :\"\n                else:\n                    title_with_return = \"Recommended Image {} \\n\".format(row)\n                    title_with_return += \"-\"*20 + \"\\n Title : \"\n                punctuation= '''!()-[]{};:'\"\\, <>./?@#$%^&*_~'''\n                for x in punctuation:\n                    title=title.replace(x,\"\")\n                title_with_return+= \" \"+ title[:min(len(title), 20)] \n            else:\n                title_with_return = \"\"\n            \n            img = cv2.imread(path+name)\n            img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB )\n            plt.subplot(1,COLS,j+1)\n            plt.title(title_with_return)\n            plt.axis('off')\n            plt.imshow(img)\n        plt.show()\n        \nplot_canvas(train_df,random=True)","metadata":{"_uuid":"bf857e23-23c0-4e7e-8387-98cc57c279cc","_cell_guid":"1030216c-aee1-4d25-9a16-93cdf3a33707","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:18:09.435683Z","iopub.execute_input":"2022-04-17T00:18:09.435954Z","iopub.status.idle":"2022-04-17T00:18:10.461525Z","shell.execute_reply.started":"2022-04-17T00:18:09.435914Z","shell.execute_reply":"2022-04-17T00:18:10.460916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize Top k Recommendation","metadata":{"_uuid":"6a601aad-205d-482f-8334-27f70eae1dcd","_cell_guid":"9c963628-2f74-47fa-9c30-2cae67902bf6","trusted":true}},{"cell_type":"code","source":"def get_neighbors( train_embeddings, query_embeddings,KNN=50, metric_param='cosine'):\n    # we can get top neighbors based on different distance metric,in our case we are using \n    # cosine and euclidean metric\n    if metric_param == 'cosine':\n        # fit cosine distance medal on train image embddings\n        cosine_knnModel = get_knn_model(train_embeddings, KNN=KNN, metric='cosine')\n        # get top k neighbors distances and indices given metric for query embeddings\n        distances, indices = cosine_knnModel.kneighbors(query_embeddings)\n\n    else:\n        # fit euclidean distance modal on image embeddings\n        eucl_knnModel = get_knn_model(train_embeddings, KNN=KNN, metric='minkowski')\n        # get top k neighbors distances and indices given metric for query embeddings\n        distances, indices = eucl_knnModel.kneighbors(query_embeddings)\n    \n    return distances, indices","metadata":{"_uuid":"03ce58de-c093-4da5-a1cf-eb661e8b162c","_cell_guid":"f932f23a-b6e1-4d15-88a0-a0d9bc76365c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-16T23:06:39.364626Z","iopub.execute_input":"2022-04-16T23:06:39.365331Z","iopub.status.idle":"2022-04-16T23:06:39.372145Z","shell.execute_reply.started":"2022-04-16T23:06:39.365295Z","shell.execute_reply":"2022-04-16T23:06:39.371305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metric_param = 'cosine'\ncosine_distances, cosine_indices = get_neighbors(\n                                    train_embeddings = train_image_embeddings,\n                                    query_embeddings = train_image_embeddings,\n                                    KNN=50,\n                                    metric_param='cosine'\n                                )\n\nnp.save(\"cosine_distance_training_f1_0.9\", cosine_distances)\n\nnp.save(\"top_50_similar_cosine_indices_training_f1_0.9\", cosine_indices)","metadata":{"_uuid":"023c02d9-4cc7-4d47-8f6a-56e20f29f97e","_cell_guid":"48ca7372-70ee-4b28-8436-d26279878130","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-16T23:06:39.373464Z","iopub.execute_input":"2022-04-16T23:06:39.373747Z","iopub.status.idle":"2022-04-16T23:06:40.160844Z","shell.execute_reply.started":"2022-04-16T23:06:39.37371Z","shell.execute_reply":"2022-04-16T23:06:40.160038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualize Results based on Cosine Distance","metadata":{"_uuid":"f974b957-0a0a-4fac-acf4-1009cc5d7cfb","_cell_guid":"51c71a46-a4a6-4c50-89f3-3f747dc46141","trusted":true}},{"cell_type":"code","source":"### Visualize Results based on Cosine Distance","metadata":{"_uuid":"abc76aae-7c38-49d5-bd0e-5cc0050346d6","_cell_guid":"0a924b82-7b70-4987-8b3e-ff7fcb862bfc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-16T23:06:40.162084Z","iopub.execute_input":"2022-04-16T23:06:40.162472Z","iopub.status.idle":"2022-04-16T23:06:40.166306Z","shell.execute_reply.started":"2022-04-16T23:06:40.16244Z","shell.execute_reply":"2022-04-16T23:06:40.165447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testing_sample_id =[505,506,3,9,33,94]","metadata":{"_uuid":"7d11a22a-93d4-4d5d-b0df-aa1be01e46f7","_cell_guid":"fd26a45f-9e04-4e5e-82ac-0589581ea136","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:22:17.65958Z","iopub.execute_input":"2022-04-17T00:22:17.659847Z","iopub.status.idle":"2022-04-17T00:22:17.666539Z","shell.execute_reply.started":"2022-04-17T00:22:17.659816Z","shell.execute_reply":"2022-04-17T00:22:17.66578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for k in (testing_sample_id):\n\n    plt.figure(figsize=(20,3))\n    plt.plot(np.arange(50),cupy.asnumpy(cosine_distances[k,]),'o-')\n    plt.title('Image {} Distance From Train Row {} to Other Train Rows'.format(\"cosine\",k),size=16)\n    plt.ylabel('{} Distance to Train Row {}'.format(\"cosine\", k),size=14)\n    plt.xlabel('Index Sorted by {} Distance to Train Row {}'.format(\"cosine\",k),size=14)\n    plt.show()\n    \n    cluster = train_df.loc[cupy.asnumpy(cosine_indices[k,:5])] \n#     print(cluster)\n    plot_canvas(cluster, random=False, ROWS=1, COLS=5, isRecommending=True)","metadata":{"_uuid":"f42e0c10-0b27-4248-a63f-e70c02b319a1","_cell_guid":"853b0216-1167-497a-88f5-4edcbd407dbe","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:22:19.985814Z","iopub.execute_input":"2022-04-17T00:22:19.986163Z","iopub.status.idle":"2022-04-17T00:22:24.941466Z","shell.execute_reply.started":"2022-04-17T00:22:19.986128Z","shell.execute_reply":"2022-04-17T00:22:24.940844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualize Results based on Euclidean Distance","metadata":{"_uuid":"ec676eb7-8322-4089-9d47-afd9cfaa0176","_cell_guid":"e8e03293-8aab-48e8-a61f-62fedf2c9021","trusted":true}},{"cell_type":"code","source":"\neuc_distances, euc_indices = get_neighbors(\n                                    train_embeddings = train_image_embeddings,\n                                    query_embeddings = train_image_embeddings,\n                                    KNN=50,\n                                    metric_param='euclidean'\n                                )\n\nnp.save(\"euclidean_distance_training_f1_0.9\", euc_distances)\n\nnp.save(\"top_50_similar_euclidean_indices_training_f1_0.9\", euc_indices)","metadata":{"_uuid":"06f3a633-2594-4c91-bb10-15766741bca1","_cell_guid":"bbd7ca5e-a606-4ad5-b8f0-addecbb03fd7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-16T23:07:01.492555Z","iopub.execute_input":"2022-04-16T23:07:01.493206Z","iopub.status.idle":"2022-04-16T23:07:02.253933Z","shell.execute_reply.started":"2022-04-16T23:07:01.49317Z","shell.execute_reply":"2022-04-16T23:07:02.25312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for k in (testing_sample_id):\n    plt.figure(figsize=(20,3))\n    plt.plot(np.arange(50),cupy.asnumpy(euc_distances[k,]),'o-')\n    plt.title('Image {} Distance From Train Row {} to Other Train Rows'.format(\"euclidean\",k),size=16)\n    plt.ylabel('{} Distance to Train Row {}'.format(\"euclidean\", k),size=14)\n    plt.xlabel('Index Sorted by Distance to Train Row {}'.format(k),size=14)\n    plt.show()\n    \n    cluster = train_df.loc[cupy.asnumpy(euc_indices[k,:5])] \n    plot_canvas(cluster, random=False, ROWS=1, COLS=5,isRecommending=True)","metadata":{"_uuid":"43e60236-edfc-46da-97db-ad6587ce1664","_cell_guid":"ec1afb61-35e8-4f28-b186-0d2d66c95643","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:22:28.981386Z","iopub.execute_input":"2022-04-17T00:22:28.98194Z","iopub.status.idle":"2022-04-17T00:22:34.323447Z","shell.execute_reply.started":"2022-04-17T00:22:28.9819Z","shell.execute_reply":"2022-04-17T00:22:34.322779Z"},"trusted":true},"execution_count":null,"outputs":[]}]}