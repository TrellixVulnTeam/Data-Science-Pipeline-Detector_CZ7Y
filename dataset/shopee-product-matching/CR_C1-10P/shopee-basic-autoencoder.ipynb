{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Shopee-Product-Matching\n![Shopee](https://cdn.lynda.com/course/563030/563030-636270778700233910-16x9.jpg)\n","metadata":{}},{"cell_type":"markdown","source":"1. [Business Problem](#motivation)  \n2. [EDA and Data Exploration](#eda)\n3. [Convolution AutoEncoder](#cae)\n4. [Visualize Prediction](#testing)\n5. [Creating Index](#indexing)\n","metadata":{}},{"cell_type":"markdown","source":"# 1. Business Problem\n<!-- <div id=\"motivation\"></div>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#483d8b; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center></center></h1> -->\n\n\nShopee is the leading e-commerce platform in Southeast Asia and Taiwan. Customers appreciate its easy, secure, and fast online shopping experience tailored to their region. The company also provides strong payment and logistical support along with a 'Lowest Price Guaranteed' feature on thousands of Shopee's listed products.\n\nFinding near-duplicates in large datasets is an important problem for many online businesses. In Shopee's case, everyday users can upload their own images and write their own product descriptions, adding an extra layer of challenge. Your task is to identify which products have been posted repeatedly. The differences between related products may be subtle while photos of identical products may be wildly different!\n\nTwo different images of similar wares may represent the same product or two completely different items. Retailers want to avoid misrepresentations and other issues that could come from conflating two dissimilar products. Currently, a combination of deep learning and traditional machine learning analyzes image and text information to compare similarity. But major differences in images, titles, and product descriptions prevent these methods from being entirely effective.\n\nIn this competition, weâ€™ll apply our machine learning skills to build a model that predicts which items are the same products.\n","metadata":{}},{"cell_type":"markdown","source":"## 2. EDA and Dataset Exploration\n<!-- \n\n<div id=\"eda\"></div>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#483d8b; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center></center></h1> -->\n\nIn this competition, we have items with an image and title. For the train data, the column label_group indicates the ground truth of which items are similar. We need to build a model that finds these similar images based on their image and title's text. In this notebook we explore some tools to help us.\n","metadata":{}},{"cell_type":"markdown","source":"### 2.1 Load Libraries","metadata":{}},{"cell_type":"code","source":"%config Completer.use_jedi = False","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:27:35.336334Z","iopub.execute_input":"2022-02-25T19:27:35.336979Z","iopub.status.idle":"2022-02-25T19:27:35.370228Z","shell.execute_reply.started":"2022-02-25T19:27:35.336878Z","shell.execute_reply":"2022-02-25T19:27:35.369477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Load Libraries\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as im\nimport tqdm\nimport cv2\n%matplotlib inline\nimport PIL\nimport gc\nimport time\nfrom skimage import io, transform\nimport pickle\n## Deep Learning Pytorch library\nimport tensorflow as tf\nimport tensorflow.keras.layers as layers\nimport tensorflow.keras.models as models\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:27:35.372103Z","iopub.execute_input":"2022-02-25T19:27:35.372625Z","iopub.status.idle":"2022-02-25T19:27:41.889252Z","shell.execute_reply.started":"2022-02-25T19:27:35.372587Z","shell.execute_reply":"2022-02-25T19:27:41.888409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2 Load Dataset","metadata":{}},{"cell_type":"code","source":"# configuration params\nBASE_PATH = '../input/shopee-product-matching/'\nTRAIN_PATH = BASE_PATH + \"train_images/\"\nTEST_PATH = BASE_PATH + \"test_images/\"\n\nBATCH_SIZE=64\nINP_WIDTH=256\nINP_HEIGHT=256\nMODEL_PATH='output/'","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:27:41.89065Z","iopub.execute_input":"2022-02-25T19:27:41.890908Z","iopub.status.idle":"2022-02-25T19:27:41.896574Z","shell.execute_reply.started":"2022-02-25T19:27:41.890872Z","shell.execute_reply":"2022-02-25T19:27:41.895935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(BASE_PATH+'test.csv')\ntrain_df = pd.read_csv(BASE_PATH+'train.csv')\n","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:27:41.89762Z","iopub.execute_input":"2022-02-25T19:27:41.897965Z","iopub.status.idle":"2022-02-25T19:27:42.06845Z","shell.execute_reply.started":"2022-02-25T19:27:41.897932Z","shell.execute_reply":"2022-02-25T19:27:42.067541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.3 Display Duplicated items\n\nusing the label_group feature which indicated the item posting belongs to same product, we can display duplicated products.","metadata":{}},{"cell_type":"code","source":"labelGroups = train_df.label_group.value_counts()\nplt.figure(figsize=(15,5))\nplt.plot(np.arange(len(labelGroups)), labelGroups.values)\nplt.xlabel(\"Index for unique label_group_item\", size=12)\nplt.ylabel(\"duplicated image count\", size=12)\nplt.title(\"duplicated item vs duplicated image count\", size=15)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:27:42.070862Z","iopub.execute_input":"2022-02-25T19:27:42.071129Z","iopub.status.idle":"2022-02-25T19:27:42.335636Z","shell.execute_reply.started":"2022-02-25T19:27:42.071101Z","shell.execute_reply":"2022-02-25T19:27:42.334894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,5))\nplt.bar(labelGroups.index[:30].astype('str'), labelGroups.values[:30])\nplt.xlabel(\"label_group\", size=14)\nplt.xticks(rotation = 45)\nplt.ylabel(\"duplicated  count\", size=14)\nplt.title(\"top 30  duplicated image count\", size=16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:27:42.336954Z","iopub.execute_input":"2022-02-25T19:27:42.337409Z","iopub.status.idle":"2022-02-25T19:27:42.703791Z","shell.execute_reply.started":"2022-02-25T19:27:42.337369Z","shell.execute_reply":"2022-02-25T19:27:42.70307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_image(df, COLS=6, ROWS=4, path=BASE_PATH, random=False):\n    # iterate over rows     \n    for k in range(ROWS):\n        # for each row we will set the size of figure\n        plt.figure(figsize=(20,5))\n        # iterate over all the columns\n        for j in range(COLS):\n            # if random flag is true get the random index from data frame\n            if random: \n                row = np.random.randint(0,len(df))\n            else:\n                \n                row = COLS*k + j\n                \n            # image name this will help collect the true path of image\n            name = df.iloc[row,1]\n            # title of the image\n            title = df.iloc[row,3]\n            img = im.imread(path+name)\n            plt.subplot(1,COLS,j+1)\n            plt.title(title[:30])\n            plt.axis('off')\n            # display image\n            plt.imshow(img)\n        plt.show()\n        \n","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:27:42.705267Z","iopub.execute_input":"2022-02-25T19:27:42.705688Z","iopub.status.idle":"2022-02-25T19:27:42.713961Z","shell.execute_reply.started":"2022-02-25T19:27:42.705647Z","shell.execute_reply":"2022-02-25T19:27:42.713035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_group_sample =train_df[train_df['label_group'] == 994676122]\ndisplay_image(label_group_sample, random=False, ROWS=1, COLS=4, path = BASE_PATH + 'train_images/')","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:27:42.71536Z","iopub.execute_input":"2022-02-25T19:27:42.71585Z","iopub.status.idle":"2022-02-25T19:27:43.52358Z","shell.execute_reply.started":"2022-02-25T19:27:42.715807Z","shell.execute_reply":"2022-02-25T19:27:43.522906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.4 Observation\n\n- We can clearly see that all the above images belongs to same label_group, some of the images **(first row)** are different than others. \n- There are certain images that is almost identical, only difference is **zoom level,backgroud and orientation**.\n- **Zoom Level Variation :** [Row 1, Col 2] [Row 2, Col 3] \n- **Background and Orientation Variation :** [ last Row ]\n- If we check the title of this products we will find slight different.\n- This variation makes this problem very interesting!\n","metadata":{}},{"cell_type":"code","source":"for k in range(2):\n    print('*'*40)\n    print('*** TOP %i DUPLICATED ITEM:'%(k+1),labelGroups.index[k])\n    print('*'*40)\n    top = train_df.loc[train_df.label_group==labelGroups.index[k]]\n    display_image(top, random=False, ROWS=1, COLS=4, path = BASE_PATH + 'train_images/')","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:27:43.524545Z","iopub.execute_input":"2022-02-25T19:27:43.524767Z","iopub.status.idle":"2022-02-25T19:27:44.968733Z","shell.execute_reply.started":"2022-02-25T19:27:43.524734Z","shell.execute_reply":"2022-02-25T19:27:44.968035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['title'].describe()","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:27:44.970116Z","iopub.execute_input":"2022-02-25T19:27:44.970853Z","iopub.status.idle":"2022-02-25T19:27:45.00497Z","shell.execute_reply.started":"2022-02-25T19:27:44.970813Z","shell.execute_reply":"2022-02-25T19:27:45.004259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_frequent_title_df = train_df[train_df['title'] == 'Koko syubbanul muslimin koko azzahir koko baju']\ntop_frequent_title_df\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:27:45.006225Z","iopub.execute_input":"2022-02-25T19:27:45.006981Z","iopub.status.idle":"2022-02-25T19:27:45.029866Z","shell.execute_reply.started":"2022-02-25T19:27:45.006942Z","shell.execute_reply":"2022-02-25T19:27:45.029124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.5 Top Frequent Title and Label groups","metadata":{}},{"cell_type":"code","source":"\n# create subplot 4 rows 3 columns\nfigure, ax = plt.subplots(nrows=2, ncols=3, figsize=(20,10))\nax = ax.flatten()\n\nfor idx,imageIndexId in enumerate(top_frequent_title_df.index[:6]):\n    imageId = train_df.loc[imageIndexId]['image']\n    target_label = train_df.loc[imageIndexId]['label_group']\n    ax[idx].imshow(im.imread(\"../input/shopee-product-matching/train_images/{}\".format(imageId)).squeeze())\n    ax[idx].title.set_text(\"Label: {}\".format(target_label))\n    ","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:27:45.031151Z","iopub.execute_input":"2022-02-25T19:27:45.031444Z","iopub.status.idle":"2022-02-25T19:27:47.043395Z","shell.execute_reply.started":"2022-02-25T19:27:45.031407Z","shell.execute_reply":"2022-02-25T19:27:47.042761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n### 2.6 Top Frequent imageHash and labelGroups\n","metadata":{}},{"cell_type":"code","source":"top_frequent_image_hash = train_df[train_df['image_phash'] == 'fad28daa2ad05595' ] \ntop_frequent_image_hash = top_frequent_image_hash.sort_values(by='title')\ntop_frequent_image_hash","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:27:47.044414Z","iopub.execute_input":"2022-02-25T19:27:47.044764Z","iopub.status.idle":"2022-02-25T19:27:47.075903Z","shell.execute_reply.started":"2022-02-25T19:27:47.044732Z","shell.execute_reply":"2022-02-25T19:27:47.075137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# create subplot 4 rows 3 columns\nfigure, ax = plt.subplots(nrows=1, ncols=3, figsize=(20,10))\n# flatten the index so we can easily put image using indexing\nax = ax.flatten()\n\nfor idx,imageIndexId in enumerate(top_frequent_image_hash.index[:3]):\n    \n    \n    imageId = train_df.loc[imageIndexId]['image']\n    ax[idx].imshow(im.imread(\"../input/shopee-product-matching/train_images/{}\".format(imageId)).squeeze())\n    ax[idx].title.set_text(\"imageId: {}\".format(imageId))\n    ","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:27:47.080174Z","iopub.execute_input":"2022-02-25T19:27:47.080487Z","iopub.status.idle":"2022-02-25T19:27:48.064461Z","shell.execute_reply.started":"2022-02-25T19:27:47.080458Z","shell.execute_reply":"2022-02-25T19:27:48.063717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.7 Top Frequent image url and label group\n","metadata":{}},{"cell_type":"code","source":"top_frequent_image_url = train_df[train_df['image'] == \"0cca4afba97e106abd0843ce72881ca4.jpg\"]\ntop_frequent_image_url","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:27:48.06575Z","iopub.execute_input":"2022-02-25T19:27:48.066108Z","iopub.status.idle":"2022-02-25T19:27:48.090222Z","shell.execute_reply.started":"2022-02-25T19:27:48.066075Z","shell.execute_reply":"2022-02-25T19:27:48.089506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see from the above table that we have two different label_group ( label_group : 2403374241, 4198148727) for same image.","metadata":{}},{"cell_type":"code","source":"# create subplot 4 rows 3 columns\nfigure, ax = plt.subplots(nrows=1, ncols=5, figsize=(15,15))\nax = ax.flatten()\n\nfor idx,imageIndexId in enumerate(top_frequent_image_url.index[:5]):\n    imageId = train_df.loc[imageIndexId]['image']\n    label_group = train_df.loc[imageIndexId]['label_group']\n    ax[idx].imshow(im.imread(\"../input/shopee-product-matching/train_images/{}\".format(imageId)).squeeze())\n    ax[idx].title.set_text(\"label_group: {}\".format(label_group))\n    ","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:27:48.091519Z","iopub.execute_input":"2022-02-25T19:27:48.092293Z","iopub.status.idle":"2022-02-25T19:27:49.497454Z","shell.execute_reply.started":"2022-02-25T19:27:48.092253Z","shell.execute_reply":"2022-02-25T19:27:49.496611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_frequent_image_group_by_labels = top_frequent_image_url.groupby('label_group')\ntop_frequent_image_group_by_labels.groups","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:27:49.498801Z","iopub.execute_input":"2022-02-25T19:27:49.499316Z","iopub.status.idle":"2022-02-25T19:27:49.511734Z","shell.execute_reply.started":"2022-02-25T19:27:49.499139Z","shell.execute_reply":"2022-02-25T19:27:49.510863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filtered_2403374241 = train_df[train_df['label_group'] == 2403374241]\nfiltered_2403374241","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:27:49.512895Z","iopub.execute_input":"2022-02-25T19:27:49.514467Z","iopub.status.idle":"2022-02-25T19:27:49.5299Z","shell.execute_reply.started":"2022-02-25T19:27:49.514424Z","shell.execute_reply":"2022-02-25T19:27:49.529165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filtered_4198148727 = train_df[train_df['label_group'] == 4198148727]\nfiltered_4198148727","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:27:49.531372Z","iopub.execute_input":"2022-02-25T19:27:49.531831Z","iopub.status.idle":"2022-02-25T19:27:49.547089Z","shell.execute_reply.started":"2022-02-25T19:27:49.531791Z","shell.execute_reply":"2022-02-25T19:27:49.546402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n# 3 Convolution AutoEncoder","metadata":{}},{"cell_type":"code","source":"# Plan of action\nfrom matplotlib.pyplot import figure\n\nfigure(figsize=(25, 20), dpi=80)\nplt.imshow(plt.imread(\"../input/cbir-pipelin/CBIR.jpeg\") )","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:27:49.548457Z","iopub.execute_input":"2022-02-25T19:27:49.548973Z","iopub.status.idle":"2022-02-25T19:27:50.461269Z","shell.execute_reply.started":"2022-02-25T19:27:49.548934Z","shell.execute_reply":"2022-02-25T19:27:50.460608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfigure(figsize=(15, 10), dpi=80)\nplt.imshow(plt.imread(\"../input/encoder-decoder-arch/Encoder_decoder.png\"))\n","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:27:50.462707Z","iopub.execute_input":"2022-02-25T19:27:50.463161Z","iopub.status.idle":"2022-02-25T19:27:51.277912Z","shell.execute_reply.started":"2022-02-25T19:27:50.463123Z","shell.execute_reply":"2022-02-25T19:27:51.277233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.1 Dataset Creation and preprocessing\n","metadata":{}},{"cell_type":"code","source":"# train_df = pd.read_csv(BASE_PATH+'train.csv')\ntrain_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:27:51.279048Z","iopub.execute_input":"2022-02-25T19:27:51.279435Z","iopub.status.idle":"2022-02-25T19:27:51.298808Z","shell.execute_reply.started":"2022-02-25T19:27:51.279397Z","shell.execute_reply":"2022-02-25T19:27:51.298208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nPreprocess the images to feed into convolution neural network\n1) Read Images\n2) decode images to jpeg or proper format\n3) resize or apply transformation\n4) convert to tensor\n\n\n\"\"\"\ndef preprocessImages(path,_):\n    \n    path = TRAIN_PATH + path\n    # read the file using tf.io\n    image = tf.io.read_file(path)\n    # decode image to jpeg\n    image = tf.image.decode_jpeg(image, channels=3)\n    # resize the image\n    image = tf.image.resize(image, [INP_WIDTH,INP_HEIGHT])\n    # convert to tensor and normalize\n    image = tf.cast(image, tf.float32)/255.0\n    \n    return image, image\n\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:27:51.299911Z","iopub.execute_input":"2022-02-25T19:27:51.300253Z","iopub.status.idle":"2022-02-25T19:27:51.313153Z","shell.execute_reply.started":"2022-02-25T19:27:51.300219Z","shell.execute_reply":"2022-02-25T19:27:51.31233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create train dataset \ntrain_dataset = tf.data.Dataset.from_tensor_slices((train_df['image'].values, train_df['label_group'].values))\n# preprocess the train dataset to feed into deep learning model\ntrain_dataset = train_dataset.map(preprocessImages)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:27:51.314753Z","iopub.execute_input":"2022-02-25T19:27:51.315662Z","iopub.status.idle":"2022-02-25T19:27:53.74431Z","shell.execute_reply.started":"2022-02-25T19:27:51.315624Z","shell.execute_reply":"2022-02-25T19:27:53.743553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2 Visualize dataset\n\n\n","metadata":{}},{"cell_type":"code","source":"images = next(iter(train_dataset))\nplt.imshow(images[0])","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:27:53.745562Z","iopub.execute_input":"2022-02-25T19:27:53.745997Z","iopub.status.idle":"2022-02-25T19:27:54.077763Z","shell.execute_reply.started":"2022-02-25T19:27:53.745942Z","shell.execute_reply":"2022-02-25T19:27:54.077057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert dataset to batches\ntrain_dataset = train_dataset.batch(BATCH_SIZE).prefetch(buffer_size = tf.data.experimental.AUTOTUNE)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:27:54.079028Z","iopub.execute_input":"2022-02-25T19:27:54.079439Z","iopub.status.idle":"2022-02-25T19:27:54.087982Z","shell.execute_reply.started":"2022-02-25T19:27:54.0794Z","shell.execute_reply":"2022-02-25T19:27:54.087063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n## 3.3 Build Model\n\n\n","metadata":{}},{"cell_type":"code","source":"filters =[16,32,64]\n\ndef build_convolution_auto_encoder(input_size, filters=[16,32,64]):\n    # create input layer\n    inputs = layers.Input(shape=input_size)\n    \n    # Conv\n    # BAtchNorm\n    # maxPool\n    # iterate over filters and pass input to layers and get latern features\n    for idx,_filter in enumerate(filters):\n        if idx==0:\n            latent_features = layers.Conv2D(filters=_filter, kernel_size=(3,3), padding='same', activation='relu')(inputs)\n        else:\n            latent_features = layers.Conv2D(filters=_filter, kernel_size=(3,3), padding='same', activation='relu')(latent_features)\n        latent_features= layers.BatchNormalization()(latent_features)\n        latent_features = layers.MaxPooling2D(pool_size=(2,2), padding=\"same\")(latent_features)\n        \n    \n    \n    # iterate over the filters in reverse order\n    # and use transposed convolution to reconstruct the same iamge again using latent features\n    \n    # Conv \n    # UpSampling\n    for idx,_filter in enumerate(reversed(filters)):\n        if idx==0:\n            decoded_features = layers.Conv2D(filters=_filter,kernel_size=(3,3), padding=\"same\", activation=\"relu\")(latent_features)\n        else: \n            decoded_features = layers.Conv2D(filters=_filter,kernel_size=(3,3), padding=\"same\", activation=\"relu\")(decoded_features)\n        decoded_features = layers.UpSampling2D(size = (2, 2))(decoded_features)\n        \n    decoded_features = layers.Conv2D(filters = 3, kernel_size = (3, 3), padding = \"same\", activation = \"sigmoid\")(decoded_features)\n    \n    \n    # Encoder Part model only\n    encoder_model  = models.Model(inputs=inputs, outputs = latent_features ) \n    # Encoder Decoder model\n    encoder_decoder = models.Model(inputs = inputs, outputs = decoded_features)\n    encoder_decoder.compile(optimizer = \"Adam\", loss = \"binary_crossentropy\")\n    return encoder_decoder, encoder_model\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:27:54.089735Z","iopub.execute_input":"2022-02-25T19:27:54.090058Z","iopub.status.idle":"2022-02-25T19:27:54.105563Z","shell.execute_reply.started":"2022-02-25T19:27:54.090015Z","shell.execute_reply":"2022-02-25T19:27:54.104602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.backend.clear_session()\nencoder_decoder, encoder = build_convolution_auto_encoder((256, 256, 3))\nencoder_decoder.summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:27:54.107098Z","iopub.execute_input":"2022-02-25T19:27:54.10742Z","iopub.status.idle":"2022-02-25T19:27:54.269518Z","shell.execute_reply.started":"2022-02-25T19:27:54.107376Z","shell.execute_reply":"2022-02-25T19:27:54.268771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IS_TRAINING = False\n# Fit Model\n# use callbacks\nif IS_TRAINING:\n    history = encoder_decoder.fit(\n        train_dataset, epochs = 3,\n        callbacks = [\n            tf.keras.callbacks.EarlyStopping(monitor = \"train_loss\", patience = 3, mode = \"min\"),\n            tf.keras.callbacks.ModelCheckpoint(filepath = \"encoder_decoder.h5\", monitor = \"train_loss\", mode = \"min\", save_best_only = True, save_weights_only = True)\n        ]\n    )\n    # Save model to disk\n    encoder_decoder.save_weights('encoder_decoder.h5')\n    encoder.save_weights('encoder.h5')\nelse:\n    encoder_decoder.load_weights('../input/output-model/encoder_decoder.h5')\n    encoder.load_weights('../input/output-model/encoder.h5')","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:27:54.270858Z","iopub.execute_input":"2022-02-25T19:27:54.27112Z","iopub.status.idle":"2022-02-25T19:27:54.354629Z","shell.execute_reply.started":"2022-02-25T19:27:54.271074Z","shell.execute_reply":"2022-02-25T19:27:54.353792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.4 Visualize Predictions\n\n","metadata":{}},{"cell_type":"code","source":"def visualize_predictions(predictions,truth, samples=5):\n    # initialize our list of output images\n    outputs = None\n#     tf.enable_eager_execution()\n    # loop over our number of output samples\n    for i in range(0, samples):\n        # grab the original image and reconstructed image\n        true_image = (truth[i].numpy() * 255).astype(\"uint8\")\n        predicted_image = (predictions[i] * 255).astype(\"uint8\")\n\n        # stack the original and reconstructed image side-by-side\n        output = np.hstack([true_image, predicted_image])\n\n        # if the outputs array is empty, initialize it as the current\n        # side-by-side image display\n        if outputs is None:\n            outputs = output\n\n        # otherwise, vertically stack the outputs\n        else:\n            outputs = np.vstack([outputs, output])\n\n    # return the output images\n#     plt.imshow(outputs)\n    return outputs","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:27:54.355943Z","iopub.execute_input":"2022-02-25T19:27:54.356253Z","iopub.status.idle":"2022-02-25T19:27:54.363954Z","shell.execute_reply.started":"2022-02-25T19:27:54.356184Z","shell.execute_reply":"2022-02-25T19:27:54.362935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create samples and prepare to array\n\nrandom_sample = []\npredictions_sample = []\nfor idx, batch in enumerate(train_dataset):\n    # Extract first 5 sample from 1st batch\n    random_sample=list(batch[0][:5,:])\n    # get prediction for first 5 sample from 1st batch\n    predictions_sample = list(encoder_decoder.predict(batch[0][:5,:]))\n    break\n","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:27:54.365639Z","iopub.execute_input":"2022-02-25T19:27:54.36618Z","iopub.status.idle":"2022-02-25T19:28:14.850068Z","shell.execute_reply.started":"2022-02-25T19:27:54.36613Z","shell.execute_reply":"2022-02-25T19:28:14.849255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nvis = visualize_predictions(predictions_sample, random_sample)\n# write image to output\ncv2.imwrite('viz.jpeg', vis)\nplt.figure(figsize=(15, 30))\nplt.imshow(vis)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:28:14.851903Z","iopub.execute_input":"2022-02-25T19:28:14.852158Z","iopub.status.idle":"2022-02-25T19:28:15.917698Z","shell.execute_reply.started":"2022-02-25T19:28:14.852123Z","shell.execute_reply":"2022-02-25T19:28:15.916886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.5  Observation\n\nIn a single epoch of training, we are able to reconstruct the image which looks like a original image, generated images are blury, possible reason could be MSE loss and architectural design and training issue.","metadata":{}},{"cell_type":"markdown","source":"\n# 4 Load Models and create indexing\n\n\n","metadata":{}},{"cell_type":"code","source":"tf.keras.backend.clear_session()\nencoder_decoder, encoder = build_convolution_auto_encoder((256, 256, 3))\nencoder_decoder.load_weights('../input/output-model/encoder_decoder.h5')\nencoder.load_weights('../input/output-model/encoder.h5')","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:28:15.918925Z","iopub.execute_input":"2022-02-25T19:28:15.920341Z","iopub.status.idle":"2022-02-25T19:28:16.111089Z","shell.execute_reply.started":"2022-02-25T19:28:15.920299Z","shell.execute_reply":"2022-02-25T19:28:16.110144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocessImages(path,index):\n    \n    path = TRAIN_PATH + path\n    # read the file using tf.io\n    image = tf.io.read_file(path)\n    # decode image to jpeg\n    image = tf.image.decode_jpeg(image, channels=3)\n    # resize the image\n    image = tf.image.resize(image, [INP_WIDTH,INP_HEIGHT])\n    # convert to tensor and normalize\n    image = tf.cast(image, tf.float32)/255.0\n    \n    return image, index\n# create train dataset \ntrain_dataset = tf.data.Dataset.from_tensor_slices((train_df['image'].values, train_df.index.values))\n# preprocess the train dataset to feed into deep learning model\ntrain_dataset = train_dataset.map(preprocessImages)\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:28:16.113083Z","iopub.execute_input":"2022-02-25T19:28:16.113408Z","iopub.status.idle":"2022-02-25T19:28:16.20234Z","shell.execute_reply.started":"2022-02-25T19:28:16.11335Z","shell.execute_reply":"2022-02-25T19:28:16.201552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset_sample = train_df.sample(n = 1000,random_state=10)\nindexes=train_dataset_sample.index\ntrain_dataset_sample = tf.data.Dataset.from_tensor_slices((train_dataset_sample['image'].values, train_dataset_sample.index.values))\ntrain_dataset_sample = train_dataset_sample.map(preprocessImages)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:28:16.204242Z","iopub.execute_input":"2022-02-25T19:28:16.204705Z","iopub.status.idle":"2022-02-25T19:28:16.226048Z","shell.execute_reply.started":"2022-02-25T19:28:16.204651Z","shell.execute_reply":"2022-02-25T19:28:16.225386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make predictions over train_dataset\nfrom tqdm import tqdm\ntest_encoded = []\n\nfor image in tqdm(train_dataset_sample.batch(64)):\n    batch_size = image[0].shape[0]\n    encoded = encoder.predict(image[0])\n    test_encoded.append(encoded.reshape(batch_size, -1))\n    \ntest_encoded = np.concatenate(test_encoded, axis = 0)\n    \n","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:28:16.227384Z","iopub.execute_input":"2022-02-25T19:28:16.227909Z","iopub.status.idle":"2022-02-25T19:28:26.557308Z","shell.execute_reply.started":"2022-02-25T19:28:16.227867Z","shell.execute_reply":"2022-02-25T19:28:26.556494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"data_dict = {\"indexes\": indexes, \"features\": test_encoded}\n\n# write the data dictionary to disk\nprint(\"[INFO] saving index...\")\nf = open('indexing1.pickle', \"wb\")\nf.write(pickle.dumps(data_dict))\nf.close()","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:28:26.558575Z","iopub.execute_input":"2022-02-25T19:28:26.560372Z","iopub.status.idle":"2022-02-25T19:28:27.147792Z","shell.execute_reply.started":"2022-02-25T19:28:26.560323Z","shell.execute_reply":"2022-02-25T19:28:27.146926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def euclidean(a, b):\n    return np.linalg.norm(a - b)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:28:27.149394Z","iopub.execute_input":"2022-02-25T19:28:27.149679Z","iopub.status.idle":"2022-02-25T19:28:27.154523Z","shell.execute_reply.started":"2022-02-25T19:28:27.149632Z","shell.execute_reply":"2022-02-25T19:28:27.15363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5 Visualize top similar product","metadata":{}},{"cell_type":"code","source":"def search_similar_images(queryFeatures, index, maxResults=5):\n    results=[]\n    \n    # loop over our index\n    for i in range(0, len(index[\"features\"])):\n        # compute the  distance euclidean between our query features\n        # and the features for the current image in our index, then\n        dist = euclidean(queryFeatures, index[\"features\"][i])\n        results.append((dist, index['indexes'][i]))\n\n    # sort the results and grab the top ones\n    results = sorted(results)[:maxResults]\n\n    # return the list of results\n    return results","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:28:27.15631Z","iopub.execute_input":"2022-02-25T19:28:27.156682Z","iopub.status.idle":"2022-02-25T19:28:27.165299Z","shell.execute_reply.started":"2022-02-25T19:28:27.156635Z","shell.execute_reply":"2022-02-25T19:28:27.164318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# searchSimialar image\nindex = pickle.loads(open('../input/indexingfile/indexing.pickle', \"rb\").read())\n\n\n    \n","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:28:27.166819Z","iopub.execute_input":"2022-02-25T19:28:27.167536Z","iopub.status.idle":"2022-02-25T19:28:30.576462Z","shell.execute_reply.started":"2022-02-25T19:28:27.167493Z","shell.execute_reply":"2022-02-25T19:28:30.575649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_indexes = index['indexes']\ntrain_Features = index['features']\n\nindexes_Features=dict()\nfor k,v in  zip(train_indexes,train_Features):\n    indexes_Features[k]=v","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:28:30.58264Z","iopub.execute_input":"2022-02-25T19:28:30.582864Z","iopub.status.idle":"2022-02-25T19:28:30.589171Z","shell.execute_reply.started":"2022-02-25T19:28:30.582836Z","shell.execute_reply":"2022-02-25T19:28:30.588441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# read the file using tf.io\nqimage = tf.io.read_file('../input/shopee-product-matching/train_images/0000a68812bc7e98c42888dfb1c07da0.jpg')\n# decode image to jpeg\nqimage = tf.image.decode_jpeg(qimage, channels=3)\n# resize the image\nqimage = tf.image.resize(qimage, [INP_WIDTH,INP_HEIGHT])\n# convert to tensor and normalize\nqimage = tf.cast(qimage, tf.float32)/255.0","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:28:30.591146Z","iopub.execute_input":"2022-02-25T19:28:30.591663Z","iopub.status.idle":"2022-02-25T19:28:30.613855Z","shell.execute_reply.started":"2022-02-25T19:28:30.591621Z","shell.execute_reply":"2022-02-25T19:28:30.613072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"qimage =tf.expand_dims(qimage, 0)\nqimage = encoder.predict(qimage)\nfeature_query = qimage.reshape(1, -1)\nfeature_query.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:28:30.615027Z","iopub.execute_input":"2022-02-25T19:28:30.615772Z","iopub.status.idle":"2022-02-25T19:28:30.698936Z","shell.execute_reply.started":"2022-02-25T19:28:30.615733Z","shell.execute_reply":"2022-02-25T19:28:30.698134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def search_similar_images(queryFeatures, index, maxResults=5):\n    results=[]\n    \n    # loop over our index\n    for i in range(0, len(index[\"features\"])):\n        # compute the  distance euclidean between our query features\n        # and the features for the current image in our index, then\n        dist = euclidean(queryFeatures, index[\"features\"][i])\n        if index['indexes'][i] == 13168:\n            print(dist)\n        results.append((dist, index['indexes'][i]))\n\n    # sort the results and grab the top ones\n    results = sorted(results)[:maxResults]\n\n    # return the list of results\n    return results","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:28:30.700101Z","iopub.execute_input":"2022-02-25T19:28:30.700377Z","iopub.status.idle":"2022-02-25T19:28:30.707916Z","shell.execute_reply.started":"2022-02-25T19:28:30.70034Z","shell.execute_reply":"2022-02-25T19:28:30.707145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = search_similar_images(feature_query,index)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:28:30.709373Z","iopub.execute_input":"2022-02-25T19:28:30.709987Z","iopub.status.idle":"2022-02-25T19:28:30.784943Z","shell.execute_reply.started":"2022-02-25T19:28:30.709924Z","shell.execute_reply":"2022-02-25T19:28:30.784007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results\n# ../input/shopee-product-matching/train_images/0000a68812bc7e98c42888dfb1c07da0.jpg\nres_index= [0]\nres_index.extend([i[1] for i in results])\ndistance_matrix=[(0,0)]\ndistance_matrix.extend(results)\nprint(res_index)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:28:30.786633Z","iopub.execute_input":"2022-02-25T19:28:30.78691Z","iopub.status.idle":"2022-02-25T19:28:30.792674Z","shell.execute_reply.started":"2022-02-25T19:28:30.786869Z","shell.execute_reply":"2022-02-25T19:28:30.791944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# take indexes\nsample_indexes = res_index\n# set seed \nnp.random.seed(100)\n\n# create subplot 4 rows 3 columns\nfigure, ax = plt.subplots(nrows=2, ncols=3, figsize=(20,15))\nax = ax.flatten()\n\nfor idx,data in enumerate(distance_matrix):\n    img_id = train_df.loc[data[1]][\"image\"]\n    ax[idx].imshow(im.imread(\"../input/shopee-product-matching/train_images/{}\".format(img_id)).squeeze())\n    \n    if idx == 0:\n        ax[idx].title.set_text(\"Query Image :   dist {}\".format( data[0]))\n    else:\n        ax[idx].title.set_text(\"prediction Image :   dist {}\".format( data[0]) )                      \n        ","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:28:30.793984Z","iopub.execute_input":"2022-02-25T19:28:30.794961Z","iopub.status.idle":"2022-02-25T19:28:32.728754Z","shell.execute_reply.started":"2022-02-25T19:28:30.794917Z","shell.execute_reply":"2022-02-25T19:28:32.72807Z"},"trusted":true},"execution_count":null,"outputs":[]}]}