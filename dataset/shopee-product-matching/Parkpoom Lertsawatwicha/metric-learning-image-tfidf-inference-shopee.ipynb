{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# About this Notebook\n\nHi all this is the inference notebook for the training notebook found [here](https://www.kaggle.com/tanulsingh077/pytorch-metric-learning-pipeline-only-images?scriptVersionId=57596864) and is a Pytorch Implementation of kernel given by @ragnar from [here](https://www.kaggle.com/ragnar123/unsupervised-baseline-arcface)\n\nWhat we are using in inference :\n* Effnet-B3 Trained with arc-face and Cross Entropy loss for Images\n* TFiDF for texts\n\nI am able to achieve 0.712 lb using the training and this inference notebook without any changes on the baseline. I will be adding training and inference code for transformer model on texts as well\n\nThis notebook runs without errors for all the efficientnet architectures\n\nI am quick saving notebook for now as I don't have GPU left , I will commit and get an lb score on this on the weekend .","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append('../input/pytorch-image-models/pytorch-image-models-master')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-17T15:08:19.335082Z","iopub.execute_input":"2021-06-17T15:08:19.335519Z","iopub.status.idle":"2021-06-17T15:08:19.341164Z","shell.execute_reply.started":"2021-06-17T15:08:19.335395Z","shell.execute_reply":"2021-06-17T15:08:19.340326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preliminaries\nfrom tqdm import tqdm\nimport math\nimport random\nimport os\nimport pandas as pd\nimport numpy as np\n\n# Visuals and CV2\nimport cv2\n\n# albumentations for augs\nimport albumentations\nfrom albumentations.pytorch.transforms import ToTensorV2\n\n#torch\nimport torch\nimport timm\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.nn import Parameter\n\n\nimport gc\nimport matplotlib.pyplot as plt\nimport cudf\nimport cuml\nimport cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml import PCA\nfrom cuml.neighbors import NearestNeighbors\nfrom sklearn.preprocessing import Normalizer\n\n\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset,DataLoader\n\nimport transformers","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:08:19.342903Z","iopub.execute_input":"2021-06-17T15:08:19.343395Z","iopub.status.idle":"2021-06-17T15:08:26.972239Z","shell.execute_reply.started":"2021-06-17T15:08:19.343348Z","shell.execute_reply":"2021-06-17T15:08:26.971448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DIM = (512,512)\n\nNUM_WORKERS = 4\nBATCH_SIZE = 16\nSEED = 42\n\ndevice = torch.device('cuda')\n\nCLASSES = 11014\n\n################################################  ADJUSTING FOR CV OR SUBMIT ##############################################\n\nCHECK_SUB = False\nGET_CV = True\n\ntest = pd.read_csv('../input/shopee-product-matching/test.csv')\nif len(test)>3: GET_CV = False\nelse: print('this submission notebook will compute CV score, but commit notebook will not')\n\n\n################################################# MODEL ####################################################################\n\nmodel_name = 'efficientnet_b3' #efficientnet_b0-b7\n\ntransformer_model = '../input/sam-export-model/sentence-transformers/stsb-distilbert-base/'\nTOKENIZER = transformers.AutoTokenizer.from_pretrained(transformer_model)\n\n################################################ MODEL PATH ###############################################################\n\nIMG_MODEL_PATH = '../input/pytorch-metric-learning-pipeline-only-images/model_efficientnet_b3_IMG_SIZE_512_arcface.bin'\nTEXT_MODEL_PATH = '../input/sam-metric-learning-pipeline-only-text-sbert/sentence_transfomer_xlm_best_loss_num_epochs_50_arcface.bin'\n\n################################################ Metric Loss and its params #######################################################\nloss_module = 'arcface' #'cosface' #'adacos'\ns = 30.0\nm = 0.5 \nls_eps = 0.0\neasy_margin = False\n\nmodel_params = {\n    'n_classes':11014,\n    'model_name':transformer_model,\n    'use_fc':False,\n    'fc_dim':512,\n    'dropout':0.3,\n}","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:08:26.973776Z","iopub.execute_input":"2021-06-17T15:08:26.974129Z","iopub.status.idle":"2021-06-17T15:08:27.428388Z","shell.execute_reply.started":"2021-06-17T15:08:26.974094Z","shell.execute_reply":"2021-06-17T15:08:27.427428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Data","metadata":{}},{"cell_type":"code","source":"def read_dataset():\n    if GET_CV:\n        df = pd.read_csv('../input/shopee-product-matching/train.csv')\n        tmp = df.groupby(['label_group'])['posting_id'].unique().to_dict()\n        df['matches'] = df['label_group'].map(tmp)\n        df['matches'] = df['matches'].apply(lambda x: ' '.join(x))\n        if CHECK_SUB:\n            df = pd.concat([df, df], axis = 0)\n            df.reset_index(drop = True, inplace = True)\n        df_cu = cudf.DataFrame(df)\n        image_paths = '../input/shopee-product-matching/train_images/' + df['image']\n    else:\n        df = pd.read_csv('../input/shopee-product-matching/test.csv')\n        df_cu = cudf.DataFrame(df)\n        image_paths = '../input/shopee-product-matching/test_images/' + df['image']\n        \n    return df, df_cu, image_paths","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:08:27.43058Z","iopub.execute_input":"2021-06-17T15:08:27.43083Z","iopub.status.idle":"2021-06-17T15:08:27.441701Z","shell.execute_reply.started":"2021-06-17T15:08:27.430805Z","shell.execute_reply":"2021-06-17T15:08:27.44084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"def seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_torch(SEED)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:08:27.443936Z","iopub.execute_input":"2021-06-17T15:08:27.444446Z","iopub.status.idle":"2021-06-17T15:08:27.45656Z","shell.execute_reply.started":"2021-06-17T15:08:27.444409Z","shell.execute_reply":"2021-06-17T15:08:27.455698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def f1_score(y_true, y_pred):\n    y_true = y_true.apply(lambda x: set(x.split()))\n    y_pred = y_pred.apply(lambda x: set(x.split()))\n    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n    len_y_pred = y_pred.apply(lambda x: len(x)).values\n    len_y_true = y_true.apply(lambda x: len(x)).values\n    f1 = 2 * intersection / (len_y_pred + len_y_true)\n    return f1","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:08:27.457856Z","iopub.execute_input":"2021-06-17T15:08:27.458204Z","iopub.status.idle":"2021-06-17T15:08:27.465804Z","shell.execute_reply.started":"2021-06-17T15:08:27.458168Z","shell.execute_reply":"2021-06-17T15:08:27.464811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def combine_predictions(row):\n    x = np.concatenate([row['image_predictions'], row['text_predictions']])\n    return ' '.join( np.unique(x) )","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:08:27.467358Z","iopub.execute_input":"2021-06-17T15:08:27.468074Z","iopub.status.idle":"2021-06-17T15:08:27.473739Z","shell.execute_reply.started":"2021-06-17T15:08:27.468035Z","shell.execute_reply":"2021-06-17T15:08:27.472825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_neighbors(df, embeddings, KNN = 50, image = True):\n    '''\n    https://www.kaggle.com/ragnar123/unsupervised-baseline-arcface?scriptVersionId=57121538\n    '''\n\n    model = NearestNeighbors(n_neighbors = KNN)\n    model.fit(embeddings)\n    distances, indices = model.kneighbors(embeddings)\n    \n    # Iterate through different thresholds to maximize cv, run this in interactive mode, then replace else clause with a solid threshold\n    if GET_CV:\n        if image:\n            thresholds = list(np.arange(2,4,0.1))\n        else:\n            thresholds = list(np.arange(0.1, 1, 0.1))\n        scores = []\n        for threshold in thresholds:\n            predictions = []\n            for k in range(embeddings.shape[0]):\n                idx = np.where(distances[k,] < threshold)[0]\n                ids = indices[k,idx]\n                posting_ids = ' '.join(df['posting_id'].iloc[ids].values)\n                predictions.append(posting_ids)\n            df['pred_matches'] = predictions\n            df['f1'] = f1_score(df['matches'], df['pred_matches'])\n            score = df['f1'].mean()\n            print(f'Our f1 score for threshold {threshold} is {score}')\n            scores.append(score)\n        thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\n        max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n        best_threshold = max_score['thresholds'].values[0]\n        best_score = max_score['scores'].values[0]\n        print(f'Our best score is {best_score} and has a threshold {best_threshold}')\n        \n        # Use threshold\n        predictions = []\n        for k in range(embeddings.shape[0]):\n            # Because we are predicting the test set that have 70K images and different label groups, confidence should be smaller\n            if image:\n                idx = np.where(distances[k,] < 2.7)[0]\n            else:\n                idx = np.where(distances[k,] < 0.60)[0]\n            ids = indices[k,idx]\n            posting_ids = df['posting_id'].iloc[ids].values\n            predictions.append(posting_ids)\n    \n    # Because we are predicting the test set that have 70K images and different label groups, confidence should be smaller\n    else:\n        predictions = []\n        for k in tqdm(range(embeddings.shape[0])):\n            if image:\n                idx = np.where(distances[k,] < 2.7)[0]\n            else:\n                idx = np.where(distances[k,] < 0.60)[0]\n            ids = indices[k,idx]\n            posting_ids = df['posting_id'].iloc[ids].values\n            predictions.append(posting_ids)\n        \n    del model, distances, indices\n    gc.collect()\n    return df, predictions","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:08:27.4766Z","iopub.execute_input":"2021-06-17T15:08:27.476838Z","iopub.status.idle":"2021-06-17T15:08:27.491297Z","shell.execute_reply.started":"2021-06-17T15:08:27.476815Z","shell.execute_reply":"2021-06-17T15:08:27.490448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using Images","metadata":{}},{"cell_type":"code","source":"def get_test_transforms():\n\n    return albumentations.Compose(\n        [\n            albumentations.Resize(DIM[0],DIM[1],always_apply=True),\n            albumentations.Normalize(),\n        ToTensorV2(p=1.0)\n        ]\n    )","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:08:27.492623Z","iopub.execute_input":"2021-06-17T15:08:27.493038Z","iopub.status.idle":"2021-06-17T15:08:27.502242Z","shell.execute_reply.started":"2021-06-17T15:08:27.493002Z","shell.execute_reply":"2021-06-17T15:08:27.501609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ShopeeDataset(Dataset):\n    def __init__(self, image_paths, transforms=None):\n\n        self.image_paths = image_paths\n        self.augmentations = transforms\n\n    def __len__(self):\n        return self.image_paths.shape[0]\n\n    def __getitem__(self, index):\n        image_path = self.image_paths[index]\n        \n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.augmentations:\n            augmented = self.augmentations(image=image)\n            image = augmented['image']       \n        \n        \n        return image,torch.tensor(1)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:08:27.50327Z","iopub.execute_input":"2021-06-17T15:08:27.503611Z","iopub.status.idle":"2021-06-17T15:08:27.512683Z","shell.execute_reply.started":"2021-06-17T15:08:27.503577Z","shell.execute_reply":"2021-06-17T15:08:27.511666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ShopeeDataset_text(Dataset):\n    def __init__(self, csv):\n        self.csv = csv.reset_index()\n\n    def __len__(self):\n        return self.csv.shape[0]\n\n    def __getitem__(self, index):\n        row = self.csv.iloc[index]\n        \n        text = row.title\n        \n        text = TOKENIZER(text, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")\n        input_ids = text['input_ids'][0]\n        attention_mask = text['attention_mask'][0]  \n        \n        return input_ids, attention_mask","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:08:27.514156Z","iopub.execute_input":"2021-06-17T15:08:27.514593Z","iopub.status.idle":"2021-06-17T15:08:27.521845Z","shell.execute_reply.started":"2021-06-17T15:08:27.51456Z","shell.execute_reply":"2021-06-17T15:08:27.520648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ShopeeNet(nn.Module):\n\n    def __init__(self,\n                 n_classes,\n                 model_name='efficientnet_b0',\n                 use_fc=False,\n                 fc_dim=512,\n                 dropout=0.0,\n                 loss_module='softmax',\n                 s=30.0,\n                 margin=0.50,\n                 ls_eps=0.0,\n                 theta_zero=0.785,\n                 pretrained=False):\n        \"\"\"\n        :param n_classes:\n        :param model_name: name of model from pretrainedmodels\n            e.g. resnet50, resnext101_32x4d, pnasnet5large\n        :param pooling: One of ('SPoC', 'MAC', 'RMAC', 'GeM', 'Rpool', 'Flatten', 'CompactBilinearPooling')\n        :param loss_module: One of ('arcface', 'cosface', 'softmax')\n        \"\"\"\n        super(ShopeeNet, self).__init__()\n        print('Model building for {} backbone'.format(model_name))\n\n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n        final_in_features = self.backbone.classifier.in_features\n        \n        self.backbone.classifier = nn.Identity()\n        self.backbone.global_pool = nn.Identity()\n        \n        self.pooling =  nn.AdaptiveAvgPool2d(1)\n            \n        self.use_fc = use_fc\n        if use_fc:\n            self.dropout = nn.Dropout(p=dropout)\n            self.fc = nn.Linear(final_in_features, fc_dim)\n            self.bn = nn.BatchNorm1d(fc_dim)\n            self._init_params()\n            final_in_features = fc_dim\n\n        self.loss_module = loss_module\n        if loss_module == 'arcface':\n            self.final = ArcMarginProduct(final_in_features, n_classes,\n                                          s=s, m=margin, easy_margin=False, ls_eps=ls_eps)\n        elif loss_module == 'cosface':\n            self.final = AddMarginProduct(final_in_features, n_classes, s=s, m=margin)\n        elif loss_module == 'adacos':\n            self.final = AdaCos(final_in_features, n_classes, m=margin, theta_zero=theta_zero)\n        else:\n            self.final = nn.Linear(final_in_features, n_classes)\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, x, label):\n        feature = self.extract_feat(x)\n        if self.loss_module in ('arcface', 'cosface', 'adacos'):\n            logits = self.final(feature, label)\n        else:\n            logits = self.final(feature)\n        return feature,logits\n\n    def extract_feat(self, x):\n        batch_size = x.shape[0]\n        x = self.backbone(x)\n        x = self.pooling(x).view(batch_size, -1)\n\n        if self.use_fc:\n            x = self.dropout(x)\n            x = self.fc(x)\n            x = self.bn(x)\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:08:27.523323Z","iopub.execute_input":"2021-06-17T15:08:27.523764Z","iopub.status.idle":"2021-06-17T15:08:27.539196Z","shell.execute_reply.started":"2021-06-17T15:08:27.523731Z","shell.execute_reply":"2021-06-17T15:08:27.538262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ShopeeNet_text(nn.Module):\n\n    def __init__(self,\n                 n_classes,\n                 model_name='bert-base-uncased',\n                 use_fc=False,\n                 fc_dim=512,\n                 dropout=0.0):\n        \"\"\"\n        :param n_classes:\n        :param model_name: name of model from pretrainedmodels\n            e.g. resnet50, resnext101_32x4d, pnasnet5large\n        :param pooling: One of ('SPoC', 'MAC', 'RMAC', 'GeM', 'Rpool', 'Flatten', 'CompactBilinearPooling')\n        :param loss_module: One of ('arcface', 'cosface', 'softmax')\n        \"\"\"\n        super(ShopeeNet_text, self).__init__()\n\n        self.transformer = transformers.AutoModel.from_pretrained(model_name)\n        final_in_features = self.transformer.config.hidden_size\n        \n        self.use_fc = use_fc\n    \n        if use_fc:\n            self.dropout = nn.Dropout(p=dropout)\n            self.fc = nn.Linear(final_in_features, fc_dim)\n            self.bn = nn.BatchNorm1d(fc_dim)\n            self._init_params()\n            final_in_features = fc_dim\n\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, input_ids,attention_mask):\n        feature = self.extract_feat(input_ids,attention_mask)\n        return F.normalize(feature)\n\n    def extract_feat(self, input_ids,attention_mask):\n        x = self.transformer(input_ids=input_ids,attention_mask=attention_mask)\n        \n        features = x[0]\n        features = features[:,0,:]\n\n        if self.use_fc:\n            features = self.dropout(features)\n            features = self.fc(features)\n            features = self.bn(features)\n\n        return features","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:08:27.54056Z","iopub.execute_input":"2021-06-17T15:08:27.540954Z","iopub.status.idle":"2021-06-17T15:08:27.553742Z","shell.execute_reply.started":"2021-06-17T15:08:27.540919Z","shell.execute_reply":"2021-06-17T15:08:27.552962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AdaCos(nn.Module):\n    def __init__(self, in_features, out_features, m=0.50, ls_eps=0, theta_zero=math.pi/4):\n        super(AdaCos, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.theta_zero = theta_zero\n        self.s = math.log(out_features - 1) / math.cos(theta_zero)\n        self.m = m\n        self.ls_eps = ls_eps  # label smoothing\n        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n    def forward(self, input, label):\n        # normalize features\n        x = F.normalize(input)\n        # normalize weights\n        W = F.normalize(self.weight)\n        # dot product\n        logits = F.linear(x, W)\n        # add margin\n        theta = torch.acos(torch.clamp(logits, -1.0 + 1e-7, 1.0 - 1e-7))\n        target_logits = torch.cos(theta + self.m)\n        one_hot = torch.zeros_like(logits)\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n        output = logits * (1 - one_hot) + target_logits * one_hot\n        # feature re-scale\n        with torch.no_grad():\n            B_avg = torch.where(one_hot < 1, torch.exp(self.s * logits), torch.zeros_like(logits))\n            B_avg = torch.sum(B_avg) / input.size(0)\n            theta_med = torch.median(theta)\n            self.s = torch.log(B_avg) / torch.cos(torch.min(self.theta_zero * torch.ones_like(theta_med), theta_med))\n        output *= self.s\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:08:27.554824Z","iopub.execute_input":"2021-06-17T15:08:27.555568Z","iopub.status.idle":"2021-06-17T15:08:27.568817Z","shell.execute_reply.started":"2021-06-17T15:08:27.555543Z","shell.execute_reply":"2021-06-17T15:08:27.567595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ArcMarginProduct(nn.Module):\n    r\"\"\"Implement of large margin arc distance: :\n        Args:\n            in_features: size of each input sample\n            out_features: size of each output sample\n            s: norm of input feature\n            m: margin\n            cos(theta + m)\n        \"\"\"\n    def __init__(self, in_features, out_features, s=30.0, m=0.50, easy_margin=False, ls_eps=0.0):\n        super(ArcMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.s = s\n        self.m = m\n        self.ls_eps = ls_eps  # label smoothing\n        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(m)\n        self.sin_m = math.sin(m)\n        self.th = math.cos(math.pi - m)\n        self.mm = math.sin(math.pi - m) * m\n\n    def forward(self, input, label):\n        # --------------------------- cos(theta) & phi(theta) ---------------------------\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n        # --------------------------- convert label to one-hot ---------------------------\n        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n        one_hot = torch.zeros(cosine.size(), device='cuda')\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.s\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:08:27.570367Z","iopub.execute_input":"2021-06-17T15:08:27.570739Z","iopub.status.idle":"2021-06-17T15:08:27.583606Z","shell.execute_reply.started":"2021-06-17T15:08:27.570704Z","shell.execute_reply":"2021-06-17T15:08:27.582457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AddMarginProduct(nn.Module):\n    r\"\"\"Implement of large margin cosine distance: :\n    Args:\n        in_features: size of each input sample\n        out_features: size of each output sample\n        s: norm of input feature\n        m: margin\n        cos(theta) - m\n    \"\"\"\n\n    def __init__(self, in_features, out_features, s=30.0, m=0.40):\n        super(AddMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.s = s\n        self.m = m\n        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n    def forward(self, input, label):\n        # --------------------------- cos(theta) & phi(theta) ---------------------------\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        phi = cosine - self.m\n        # --------------------------- convert label to one-hot ---------------------------\n        one_hot = torch.zeros(cosine.size(), device='cuda')\n        # one_hot = one_hot.cuda() if cosine.is_cuda else one_hot\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)  # you can use torch.where if your torch.__version__ is 0.4\n        output *= self.s\n        # print(output)\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:08:27.584902Z","iopub.execute_input":"2021-06-17T15:08:27.585315Z","iopub.status.idle":"2021-06-17T15:08:27.594727Z","shell.execute_reply.started":"2021-06-17T15:08:27.585281Z","shell.execute_reply":"2021-06-17T15:08:27.593963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_image_embeddings(image_paths):\n    embeds = []\n    \n    model = ShopeeNet(n_classes=CLASSES,model_name=model_name)\n    model.eval()\n    \n    model.load_state_dict(torch.load(IMG_MODEL_PATH),strict=False)\n    model = model.to(device)\n\n    image_dataset = ShopeeDataset(image_paths=image_paths,transforms=get_test_transforms())\n    image_loader = torch.utils.data.DataLoader(\n        image_dataset,\n        batch_size=BATCH_SIZE,\n        pin_memory=True,\n        drop_last=False,\n        num_workers=NUM_WORKERS\n    )\n    \n    \n    with torch.no_grad():\n        for img,label in tqdm(image_loader): \n            img = img.cuda()\n            label = label.cuda()\n            feat, _ = model(img,label)\n            image_embeddings = feat.detach().cpu().numpy()\n            embeds.append(image_embeddings)\n    \n    \n    del model\n    image_embeddings = np.concatenate(embeds)\n    print(f'Our image embeddings shape is {image_embeddings.shape}')\n    del embeds\n    gc.collect()\n    return image_embeddings","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:08:27.596302Z","iopub.execute_input":"2021-06-17T15:08:27.596693Z","iopub.status.idle":"2021-06-17T15:08:27.605538Z","shell.execute_reply.started":"2021-06-17T15:08:27.596659Z","shell.execute_reply":"2021-06-17T15:08:27.6046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using Texts with TFiDF","metadata":{}},{"cell_type":"code","source":"# def get_text_embeddings(df_cu, max_features = 15000, n_components = 5000):\n#     model = TfidfVectorizer(stop_words = 'english', binary = True, max_features = max_features)\n#     text_embeddings = model.fit_transform(df_cu['title']).toarray()\n#     pca = PCA(n_components = n_components)\n#     text_embeddings = pca.fit_transform(text_embeddings).get()\n#     print(f'Our title text embedding shape is {text_embeddings.shape}')\n#     del model, pca\n#     gc.collect()\n#     return text_embeddings","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:08:27.607054Z","iopub.execute_input":"2021-06-17T15:08:27.607707Z","iopub.status.idle":"2021-06-17T15:08:27.614059Z","shell.execute_reply.started":"2021-06-17T15:08:27.60767Z","shell.execute_reply":"2021-06-17T15:08:27.613444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using BERT","metadata":{}},{"cell_type":"code","source":"def get_text_embeddings(df):\n    embeds = []\n    \n    model = ShopeeNet_text(**model_params)\n    model.eval()\n    \n    model.load_state_dict(dict(list(torch.load(TEXT_MODEL_PATH).items())[:-1]))\n    model = model.to(device)\n\n    text_dataset = ShopeeDataset_text(df)\n    text_loader = torch.utils.data.DataLoader(\n        text_dataset,\n        batch_size=BATCH_SIZE,\n        pin_memory=True,\n        drop_last=False,\n        num_workers=NUM_WORKERS\n    )\n    \n    \n    with torch.no_grad():\n        for input_ids, attention_mask in tqdm(text_loader): \n            input_ids = input_ids.cuda()\n            attention_mask = attention_mask.cuda()\n            feat = model(input_ids, attention_mask)\n            text_embeddings = feat.detach().cpu().numpy()\n            embeds.append(text_embeddings)\n    \n    \n    del model\n    text_embeddings = np.concatenate(embeds)\n    print(f'Our text embeddings shape is {text_embeddings.shape}')\n    del embeds\n    gc.collect()\n    return text_embeddings","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:08:27.618117Z","iopub.execute_input":"2021-06-17T15:08:27.6184Z","iopub.status.idle":"2021-06-17T15:08:27.625888Z","shell.execute_reply.started":"2021-06-17T15:08:27.618355Z","shell.execute_reply":"2021-06-17T15:08:27.624993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Calculating Predictions","metadata":{}},{"cell_type":"code","source":"df,df_cu,image_paths = read_dataset()\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:08:27.627708Z","iopub.execute_input":"2021-06-17T15:08:27.628051Z","iopub.status.idle":"2021-06-17T15:08:35.542889Z","shell.execute_reply.started":"2021-06-17T15:08:27.628018Z","shell.execute_reply":"2021-06-17T15:08:35.542033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_embeddings = get_image_embeddings(image_paths.values)\ntext_embeddings = get_text_embeddings(df)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:08:35.544209Z","iopub.execute_input":"2021-06-17T15:08:35.544752Z","iopub.status.idle":"2021-06-17T15:18:50.12617Z","shell.execute_reply.started":"2021-06-17T15:08:35.544714Z","shell.execute_reply":"2021-06-17T15:18:50.125251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get neighbors for image_embeddings\ndf,image_predictions = get_neighbors(df, image_embeddings, KNN = 50, image = True)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:18:50.127574Z","iopub.execute_input":"2021-06-17T15:18:50.128065Z","iopub.status.idle":"2021-06-17T15:19:51.452427Z","shell.execute_reply.started":"2021-06-17T15:18:50.128027Z","shell.execute_reply":"2021-06-17T15:19:51.451613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:19:51.455243Z","iopub.execute_input":"2021-06-17T15:19:51.455595Z","iopub.status.idle":"2021-06-17T15:19:51.471408Z","shell.execute_reply.started":"2021-06-17T15:19:51.455565Z","shell.execute_reply":"2021-06-17T15:19:51.470671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get neighbors for text_embeddings\ndf, text_predictions = get_neighbors(df, text_embeddings, KNN = 50, image = False)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:19:51.473078Z","iopub.execute_input":"2021-06-17T15:19:51.473567Z","iopub.status.idle":"2021-06-17T15:20:19.169765Z","shell.execute_reply.started":"2021-06-17T15:19:51.473531Z","shell.execute_reply":"2021-06-17T15:20:19.168902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:20:19.170942Z","iopub.execute_input":"2021-06-17T15:20:19.171263Z","iopub.status.idle":"2021-06-17T15:20:19.184343Z","shell.execute_reply.started":"2021-06-17T15:20:19.17123Z","shell.execute_reply":"2021-06-17T15:20:19.183506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing Submission","metadata":{}},{"cell_type":"code","source":"if GET_CV:\n    df['image_predictions'] = image_predictions\n    df['text_predictions'] = text_predictions\n    df['pred_matches'] = df.apply(combine_predictions, axis = 1)\n    df['f1'] = f1_score(df['matches'], df['pred_matches'])\n    score = df['f1'].mean()\n    print(f'Our final f1 cv score is {score}')\n    df['matches'] = df['pred_matches']\n    df[['posting_id', 'matches']].to_csv('submission.csv', index = False)\nelse:\n    df['image_predictions'] = image_predictions\n    df['text_predictions'] = text_predictions\n    df['matches'] = df.apply(combine_predictions, axis = 1)\n    df[['posting_id', 'matches']].to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:20:19.185686Z","iopub.execute_input":"2021-06-17T15:20:19.18617Z","iopub.status.idle":"2021-06-17T15:20:21.10629Z","shell.execute_reply.started":"2021-06-17T15:20:19.186135Z","shell.execute_reply":"2021-06-17T15:20:21.105522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}