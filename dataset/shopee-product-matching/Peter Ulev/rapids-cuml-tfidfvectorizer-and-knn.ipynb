{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# RAPIDS cuML TfidfVectorizer and KNN to find similar Text and Images\nIn this notebook we use RAPIDS cuML's TfidfVectorizer and cuML's KNN to find items with similar titles and items with similar images. First we use RAPIDS cuML TfidfVectorizer to extract text embeddings of each item's title and then compare the embeddings using RAPIDS cuML KNN. Next we extract image embeddings of each item with EffNetB0 and compare them using RAPIDS cuML KNN.[](http://)","metadata":{}},{"cell_type":"markdown","source":"# Load Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport cv2, matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.applications import EfficientNetB0\nprint('TF',tf.__version__)\nimport torch\nfrom IPython.display import Image, clear_output  # to display images\n\nclear_output()\nprint(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-03T22:59:38.793642Z","iopub.execute_input":"2021-06-03T22:59:38.794056Z","iopub.status.idle":"2021-06-03T22:59:45.477093Z","shell.execute_reply.started":"2021-06-03T22:59:38.793966Z","shell.execute_reply":"2021-06-03T22:59:45.475984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip3 install psycopg2-binary \n!pip3 install langdetect","metadata":{"execution":{"iopub.status.busy":"2021-06-03T22:59:45.47921Z","iopub.execute_input":"2021-06-03T22:59:45.479886Z","iopub.status.idle":"2021-06-03T22:59:57.720849Z","shell.execute_reply.started":"2021-06-03T22:59:45.479829Z","shell.execute_reply":"2021-06-03T22:59:57.719914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import abc\n\n\nclass Embedder(abc.ABC):\n    @abc.abstractmethod\n    def encode(self, X):\n        pass\n\n    @abc.abstractmethod\n    def fit(self, X):\n        pass\n","metadata":{"execution":{"iopub.status.busy":"2021-06-03T22:59:57.724374Z","iopub.execute_input":"2021-06-03T22:59:57.724668Z","iopub.status.idle":"2021-06-03T22:59:57.731664Z","shell.execute_reply.started":"2021-06-03T22:59:57.724639Z","shell.execute_reply":"2021-06-03T22:59:57.730885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import abc\n\n\nclass Regressor(abc.ABC):\n    @abc.abstractmethod\n    def fit(self, X, y):\n        pass\n\n    @abc.abstractmethod\n    def predict(self, X):\n        pass\n\n    @abc.abstractmethod\n    def save(self, path):\n        pass\n\n    @abc.abstractmethod\n    def load(self, path):\n        pass\n","metadata":{"execution":{"iopub.status.busy":"2021-06-03T22:59:57.734787Z","iopub.execute_input":"2021-06-03T22:59:57.735077Z","iopub.status.idle":"2021-06-03T22:59:57.74152Z","shell.execute_reply.started":"2021-06-03T22:59:57.735049Z","shell.execute_reply":"2021-06-03T22:59:57.740724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom typing import List\nimport numpy as np\n\n\nclass Basepipeline:\n    def __init__(self, Embedder, Regressor):\n        self.embedder = Embedder()\n        self.regressor = Regressor()\n\n    def fit(self, X: List[str], y: List[float]):\n        X_emb = self.embedder.fit(X)\n        #4 X_emb = self.embedder.encode(X)\n        self.regressor.fit(X_emb, y)\n\n    def predict(self, X: List[str]):\n        X_emb = self.embedder.encode(X)\n        return self.regressor.predict(X_emb)\n\n    def save(self, odir):\n        os.makedirs(odir, exist_ok=True)\n        self.embedder.save(f'{odir}/{self.embedder.name}')\n        self.regressor.save(f'{odir}/{self.regressor.name}')\n","metadata":{"execution":{"iopub.status.busy":"2021-06-03T22:59:57.746505Z","iopub.execute_input":"2021-06-03T22:59:57.746889Z","iopub.status.idle":"2021-06-03T22:59:57.755579Z","shell.execute_reply.started":"2021-06-03T22:59:57.746863Z","shell.execute_reply":"2021-06-03T22:59:57.754619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\n\ndef N_distance(y_true, y_pred, N):\n    \"\"\"\n\n    :param y_true:\n    :param y_pred:\n    :return:\n    \"\"\"\n    score = 0\n    for y_i, yhat_i in zip(y_true, y_pred):\n        vals = abs(y_i - yhat_i)\n        if all(a <= N for a in vals):\n            score += 1\n    return score\n\n\ndef rmse(y, yhat):\n    \"\"\"\n\n    :param y: actual M x N matrix\n    :param yhat: predictions M x N matrix\n    :return: rmse\n    \"\"\"\n    return mean_squared_error(y, yhat, squared=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-03T22:59:57.75715Z","iopub.execute_input":"2021-06-03T22:59:57.757497Z","iopub.status.idle":"2021-06-03T22:59:58.32668Z","shell.execute_reply.started":"2021-06-03T22:59:57.757471Z","shell.execute_reply":"2021-06-03T22:59:58.32586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from abc import ABC\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n# from cuml.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import PCA\nfrom nltk import word_tokenize\nfrom nltk.stem.porter import PorterStemmer\nimport numpy as np\nimport pickle\nfrom nltk import download\ndownload('punkt')\n\n\nclass TfidfModel(Embedder, ABC):\n    \"\"\"\n    Tf-idf + PCA.\n    \"\"\"\n\n    def __init__(self):\n        self.name = ''\n        self.model = TfidfVectorizer(lowercase=True, max_features=30000)\n        self.pca = PCA(n_components=100)\n\n    def fit(self, X):\n        print('Fitting the tfidf vectorizer...')\n        tokenized_text = self.tokenize_text(X)\n        matrix = self.model.fit_transform(tokenized_text).todense()\n        matrix = np.squeeze(np.asarray(matrix))\n        print('Dimension of original tfidf matrix: ', matrix.shape)\n\n        self.pca.fit(matrix)\n        reduced_matrix = self.pca.transform(matrix)\n        print('Dimension of reduced matrix: ', reduced_matrix.shape)\n        print('Encoder fitting completed!')\n        return reduced_matrix\n\n    def encode(self, X):\n        print('Encoding data...')\n        tokenized_text = self.tokenize_text(X)\n        matrix = self.model.transform(tokenized_text).todense()\n        matrix = np.squeeze(np.asarray(matrix))\n        reduced_matrix = self.pca.transform(matrix)\n        return reduced_matrix\n\n    def load(self, path):\n        pass\n\n    def save(self, odir):\n        print('Saving model...')\n        pickle.dump(self.model, open(odir + 'tfidf_vectorizer', 'wb'))  # Save tfidf vectorizer\n        pickle.dump(self.pca, open(odir + 'pca_model', 'wb'))  # Save the PCA model\n        print('Model saved!')\n\n    def tokenize_item(self, item):\n        tokens = word_tokenize(item)\n        stems = []\n        for token in tokens:\n            stems.append(PorterStemmer().stem(token))\n        return stems\n\n    def tokenize_text(self, text):\n        return [' '.join(self.tokenize_item(txt.lower())) for txt in text]\n","metadata":{"execution":{"iopub.status.busy":"2021-06-03T22:59:58.328963Z","iopub.execute_input":"2021-06-03T22:59:58.329228Z","iopub.status.idle":"2021-06-03T22:59:59.237655Z","shell.execute_reply.started":"2021-06-03T22:59:58.329195Z","shell.execute_reply":"2021-06-03T22:59:59.236706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.datasets import make_regression\nfrom sklearn.model_selection import RepeatedKFold\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nimport keras\nfrom keras import callbacks\n\nimport tensorflow as tf\n\nprint(tf.__version__)\nprint(tf.config.list_physical_devices())\n\n\nclass NeuralNetMulti(Regressor):\n    def __init__(self):\n        self.name = 'keras-sequential'\n        self.model = Sequential()\n        # self.earlystopping = callbacks.EarlyStopping(monitor=\"mae\",\n        #                                              mode=\"min\", patience=5,\n        #                                              restore_best_weights=True)\n\n    def fit(self, X, y):\n        print('Fitting into the neural net...')\n        n_inputs = X.shape[1]\n        n_outputs = y.shape[1]\n        self.model.add(Dense(30, input_dim=n_inputs, kernel_initializer='he_uniform', activation='relu'))\n        self.model.add(Dense(20, activation='relu'))\n        self.model.add(Dense(20, activation='relu'))\n        self.model.add(Dense(10, activation='relu'))\n        self.model.add(Dense(n_outputs))\n        self.model.summary()\n        self.model.compile(loss='mae', optimizer='adam', metrics=['mse', 'mae'])\n        self.model.fit(X, y, verbose=1, epochs=1000)\n        # self.model.fit(X, y, verbose=1, epochs=1000, callbacks=[self.earlystopping])\n        print('Fitting completed!')\n\n    def predict(self, X):\n        print('Predicting...')\n        predictions = self.model.predict(X)\n        print('Predicted!')\n        return predictions\n\n    def save(self, path):\n        print('Saving model to ', path, '...')\n        self.model.save(path)\n        print('Model saved')\n\n    def load(self, path):\n        print('Loading model...')\n        model = keras.models.load_model(path)\n        print('Model loaded!')\n\n    # def get_dataset(self):\n    #     X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, n_targets=3, random_state=2)\n    #     return X, y\n\n    # evaluate a model using repeated k-fold cross-validation\n    # def evaluate_model(self, X, y):\n    #     results = list()\n    #     n_inputs, n_outputs = X.shape[1], y.shape[1]\n    #     # define evaluation procedure\n    #     cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n    #     # enumerate folds\n    #     for train_ix, test_ix in cv.split(X):\n    #         # prepare data\n    #         X_train, X_test = X[train_ix], X[test_ix]\n    #         y_train, y_test = y[train_ix], y[test_ix]\n    #         # define model\n    #         model = get_model(n_inputs, n_outputs)\n    #         # fit model\n    #         model.fit(X_train, y_train, verbose=0, epochs=100)\n    #         # evaluate model on test set\n    #         mae = model.evaluate(X_test, y_test, verbose=0)\n    #         # store result\n    #         print('>%.3f' % mae)\n    #         results.append(mae)\n    #     return results\n# READ THIIIIIIIIIIS\n# https://stackoverflow.com/questions/56299770/units-in-dense-layer-in-keras/56302896\n","metadata":{"execution":{"iopub.status.busy":"2021-06-03T22:59:59.239042Z","iopub.execute_input":"2021-06-03T22:59:59.239568Z","iopub.status.idle":"2021-06-03T22:59:59.457436Z","shell.execute_reply.started":"2021-06-03T22:59:59.239528Z","shell.execute_reply":"2021-06-03T22:59:59.456675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport psycopg2\n\n\n# query = '''SELECT big5_openness, big5_conscientiousness, big5_extraversion, big5_agreeableness, big5_neuroticism, input_text  FROM data_personality_analiser_nlp where input_text IS NOT NULL and input_text <> '' '''\n\nclass Connector:\n    def __init__(self):\n        host = '134.213.113.101'\n        user = 'nlp'\n        password = 'p2021nlp-psql'\n        # driver = 'SQL+Server'\n        db = 'nlp-data'\n        port = '5432'\n        self.connection = psycopg2.connect(\n            database=db,\n            user=user,\n            password=password,\n            host=host,\n            port=port\n        )\n\n    def query(self, db_query):\n        print('Quierying database...')\n        df = pd.read_sql_query(db_query, self.connection)\n#         df = pd.read_csv('../data/big5_data.csv')\n#         print(df.shape)\n        print('Database queried!')\n        return df\n","metadata":{"execution":{"iopub.status.busy":"2021-06-03T22:59:59.460478Z","iopub.execute_input":"2021-06-03T22:59:59.460806Z","iopub.status.idle":"2021-06-03T22:59:59.489964Z","shell.execute_reply.started":"2021-06-03T22:59:59.460771Z","shell.execute_reply":"2021-06-03T22:59:59.489156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir data","metadata":{"execution":{"iopub.status.busy":"2021-06-03T22:59:59.491587Z","iopub.execute_input":"2021-06-03T22:59:59.492012Z","iopub.status.idle":"2021-06-03T23:00:00.155871Z","shell.execute_reply.started":"2021-06-03T22:59:59.491973Z","shell.execute_reply":"2021-06-03T23:00:00.154816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport re\nfrom langdetect import detect\n\n\nclass DataLoader:\n    def __init__(self):\n        self.connector = Connector()\n\n    def parse_input(self):\n        # df = pd.read_csv(self.input_file)\n        db_query = '''SELECT big5_openness, big5_conscientiousness, big5_extraversion, big5_agreeableness, big5_neuroticism, input_text  FROM data_personality_analiser_nlp where input_text IS NOT NULL and input_text <> '' '''\n        df = self.connector.query(db_query)\n        df = df[df.input_text.apply(detect).eq('en')]\n        if not os.path.exists('data'):\n            os.makedirs('data')\n#         df.to_csv('data/big5_data.csv')\n        y = df[['big5_openness', 'big5_conscientiousness', 'big5_extraversion', 'big5_agreeableness',\n                'big5_neuroticism']].to_numpy()\n\n        input_texts = df[['input_text']].astype(str).to_numpy()\n        X = [re.sub(r'http\\S+', '', text[0]) for text in input_texts]  # remove links\n        return X, y\n","metadata":{"execution":{"iopub.status.busy":"2021-06-03T23:00:00.157651Z","iopub.execute_input":"2021-06-03T23:00:00.158036Z","iopub.status.idle":"2021-06-03T23:00:00.181162Z","shell.execute_reply.started":"2021-06-03T23:00:00.157998Z","shell.execute_reply":"2021-06-03T23:00:00.180437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RESTRICT TENSORFLOW TO 12GB OF GPU RAM\n# SO THAT WE HAVE GPU RAM FOR RAPIDS CUML KNN\nLIMIT = 12\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n  try:\n    tf.config.experimental.set_virtual_device_configuration(\n        gpus[0],\n        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*LIMIT)])\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n  except RuntimeError as e:\n    print(e)\nprint('Restrict TensorFlow to max %iGB GPU RAM'%LIMIT)\nprint('so RAPIDS can use %iGB GPU RAM'%(16-LIMIT))","metadata":{"execution":{"iopub.status.busy":"2021-06-03T23:00:00.182341Z","iopub.execute_input":"2021-06-03T23:00:00.18269Z","iopub.status.idle":"2021-06-03T23:00:05.616786Z","shell.execute_reply.started":"2021-06-03T23:00:00.182662Z","shell.execute_reply":"2021-06-03T23:00:05.612899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from models.base_pipeline import Basepipeline\n# from models.bert_model import BertModel\n# from models.catboost_regr import CatboostRegr\n# from models.neuralnet_multi import NeuralNetMulti\n# from models.data_loader import DataLoader\n# from models.tfidf_model import TfidfModel\n# from models.scores import rmse\n# from models.scores import N_distance\n\n\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\n\ndef main():\n    dl = DataLoader()\n\n    X, y = dl.parse_input()\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\n    base_model = Basepipeline(TfidfModel, NeuralNetMulti)\n    base_model.fit(X_train, y_train)\n    base_model.save('tfidf_pca_nn_full/')\n    y_pred = base_model.predict(X_test)\n\n    ndist = N_distance(y_test, y_pred, 10)\n    _rmse = rmse(y_test, y_pred)\n    print('Results: ')\n    print('Shape of prediction set size: ', y_test.shape)\n    print('10_distance: ', ndist)\n    print('rmse: ', _rmse)\n\n\nif __name__ == '__main__':\n    main()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-03T23:00:05.618788Z","iopub.execute_input":"2021-06-03T23:00:05.619371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # RESTRICT TENSORFLOW TO 12GB OF GPU RAM\n# # SO THAT WE HAVE GPU RAM FOR RAPIDS CUML KNN\n# LIMIT = 12\n# gpus = tf.config.experimental.list_physical_devices('GPU')\n# if gpus:\n#   try:\n#     tf.config.experimental.set_virtual_device_configuration(\n#         gpus[0],\n#         [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*LIMIT)])\n#     logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n#     print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n#   except RuntimeError as e:\n#     print(e)\n# print('Restrict TensorFlow to max %iGB GPU RAM'%LIMIT)\n# print('so RAPIDS can use %iGB GPU RAM'%(16-LIMIT))","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import cudf, cuml, cupy\n# from cuml.feature_extraction.text import TfidfVectorizer\n# from cuml.neighbors import NearestNeighbors\n# print('RAPIDS',cuml.__version__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = TfidfVectorizer(stop_words='english', binary=True)\n# text_embeddings = model.fit_transform(train_gf.title).toarray()\n# print('text embeddings shape is',text_embeddings.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = EfficientNetB0(weights='imagenet',include_top=False, pooling='avg', input_shape=None)\n# train_gen = DataGenerator(train, batch_size=128)\n# image_embeddings = model.predict(train_gen,verbose=1)\n# print('image embeddings shape is',image_embeddings.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}