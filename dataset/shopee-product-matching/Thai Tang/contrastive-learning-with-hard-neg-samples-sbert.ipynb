{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Training pipline to finetune pre-trained models from [SentenceTransformers](https://www.sbert.net/docs/pretrained_models.html) with Contrastive Loss and Hard Negative Samples\n\n* reference: [CONTRASTIVE LEARNING WITH\nHARD NEGATIVE SAMPLES](https://arxiv.org/pdf/2010.04592.pdf)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install faiss-cpu sentence_transformers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import itertools\nimport os\nimport random as rn\nimport shutil\nfrom scipy.special import comb\n\nimport faiss\nimport numpy as np\nimport pandas as pd\nfrom sentence_transformers import (\n    SentencesDataset,\n    SentenceTransformer,\n    evaluation,\n    losses,\n)\nfrom sentence_transformers.readers import InputExample\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm, notebook","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL_NAME = \"stsb-roberta-base\"\nMODEL = SentenceTransformer(MODEL_NAME)\nMODEL_SAVE_PATH = f\"finetuned-model/{MODEL_NAME}\"\nBATCH_SIZE = 32\nCAP_SIZE = 50","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_prep():\n    df = pd.read_csv(\"../input/shopee-product-matching/train.csv\")\n\n    label_groups = df[\"label_group\"].unique()\n    # rn.shuffle(label_groups)\n    train_df = df.loc[df.label_group.isin(label_groups[: int(0.8 * len(label_groups))])]\n    eval_df = df.loc[df.label_group.isin(label_groups[int(0.8 * len(label_groups)) :])]\n\n    # Prepare train_data according to ContrastiveLoss\n    train_examples = list()\n    train_groups = [\n        train_df.loc[train_df[\"label_group\"] == lg][\"title\"].values.tolist()\n        for lg in train_df[\"label_group\"].unique()\n    ]\n\n    # Build FAISS index to query hard negative samples\n    train_titles = sum(train_groups, [])\n    train_embeddings = MODEL.encode(train_titles)\n    train_index = faiss.IndexFlatL2(train_embeddings.shape[1])\n    train_index.add(train_embeddings)\n    for _, group in enumerate(train_groups):\n        negative_pairs_no = int(max(CAP_SIZE - comb(len(group), 2), comb(len(group), 2)))\n\n        group_embedding = np.ascontiguousarray(\n            np.mean(MODEL.encode(group), axis=0).reshape(1, -1), dtype=np.float32\n        )\n        _, similar_idx = train_index.search(group_embedding, negative_pairs_no * 2)\n        negative_titles = [train_titles[idx] for idx in similar_idx[0]]\n        for title in group:\n            try:\n                negative_titles.remove(title)\n            except:\n                pass\n        negative_titles = negative_titles[:negative_pairs_no]\n\n        positive_pairs = [\n            list(pair)\n            for pair in list(itertools.combinations(group, 2))\n            if (isinstance(pair[0], str) and isinstance(pair[1], str))\n        ]\n        for pair in positive_pairs:\n            train_examples.append(InputExample(texts=pair, label=1))\n        negative_pairs = [\n            [rn.choice(rn.choice(positive_pairs)), negative_title]\n            for negative_title in negative_titles\n        ]\n        for pair in negative_pairs:\n            train_examples.append(InputExample(texts=pair, label=0))\n\n    train_dataset = SentencesDataset(train_examples, MODEL)\n    train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n\n    # Prepare eval_data according to BinaryClassificationEvaluator\n    eval_examples = list()\n    eval_groups = [\n        eval_df.loc[eval_df[\"label_group\"] == lg][\"title\"].values.tolist()\n        for lg in eval_df[\"label_group\"].unique()\n    ]\n    # Build FAISS index to query hard negative samples\n    eval_titles = sum(eval_groups, [])\n    eval_embeddings = MODEL.encode(eval_titles)\n    eval_index = faiss.IndexFlatL2(eval_embeddings.shape[1])\n    eval_index.add(eval_embeddings)\n    for _, group in enumerate(eval_groups):\n        negative_pairs_no = int(max(CAP_SIZE - comb(len(group), 2), comb(len(group), 2)))\n\n        group_embedding = np.ascontiguousarray(\n            np.mean(MODEL.encode(group), axis=0).reshape(1, -1), dtype=np.float32\n        )\n        _, similar_idx = eval_index.search(group_embedding, negative_pairs_no * 2)\n        negative_titles = [eval_titles[idx] for idx in similar_idx[0]]\n        for title in group:\n            try:\n                negative_titles.remove(title)\n            except:\n                pass\n        negative_titles = negative_titles[:negative_pairs_no]\n\n        positive_pairs = [\n            list(pair) + [1]\n            for pair in list(itertools.combinations(group, 2))\n            if (isinstance(pair[0], str) and isinstance(pair[1], str))\n        ]\n        negative_pairs = [\n            [rn.choice(rn.choice(positive_pairs)[:2]), negative_title, 0]\n            for negative_title in negative_titles\n        ]\n        eval_examples.append(positive_pairs)\n        eval_examples.append(negative_pairs)\n\n    eval_examples = sum(eval_examples, [])\n\n    evaluator = evaluation.BinaryClassificationEvaluator(\n        sentences1=list(zip(*eval_examples))[0],\n        sentences2=list(zip(*eval_examples))[1],\n        labels=list(zip(*eval_examples))[2],\n        batch_size=BATCH_SIZE,\n    )\n\n    return train_dataloader, evaluator","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader, evaluator = data_prep()\n\ntrain_loss = losses.ContrastiveLoss(model=MODEL)\n\nif not os.path.exists(MODEL_SAVE_PATH):\n    os.makedirs(MODEL_SAVE_PATH)\nelse:\n    shutil.rmtree(MODEL_SAVE_PATH)\n    os.makedirs(MODEL_SAVE_PATH)\n\nMODEL.fit(\n    train_objectives=[(train_dataloader, train_loss)],\n    epochs=100,\n    warmup_steps=100,\n    evaluation_steps=500,\n    output_path=MODEL_SAVE_PATH,\n    evaluator=evaluator,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}