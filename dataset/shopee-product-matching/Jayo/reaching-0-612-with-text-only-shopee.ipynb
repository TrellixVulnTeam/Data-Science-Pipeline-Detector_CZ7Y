{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# About this Notebook\n\nAfter carefull considerations and doing a lot of experiments with tfidf and Bert-Based models , I strongly feel Bert-based models might do better if trained and used in the right way. [This](https://www.kaggle.com/c/shopee-product-matching/discussion/231510) dicussion thread talks about the usage of different approaches for text and discusses why BERT-base model might be better.\n\nI started with normal Hugging Face BERT type models , but I found Sentence transformers pre-trained models a better idea . As sentence transformer models were already trained in a siamese fashion especially for information retreival and semantic similarity tasks it's much better idea to start with them and then fine-tune it on our data. I have used <b> paraphrase-xlm-r-multilingual-v1 </b> from sentence transformers , one can try with other very good models also . I have uploaded all models for offline use [here](https://www.kaggle.com/tanulsingh077/sentence-transformer-models)\n\nOne more additional thing which has come as a result of experimentation is to train with full data instead of splitting and then saving models on eval set. To avoid overfitting one can use strong regularizers like using fully connected layer on top of bert output , weight decay,etc\n\nThis is the inference notebook , you can find the training notebook [here](https://www.kaggle.com/tanulsingh077/metric-learning-pipeline-only-text-sbert)\n\n# Additional Utils\n\nI have added a function get_neighbours_cos_sim which is cosine similarity equivalent for ragnar's get_neighbours function for knn. Now one can use both to find the best threshold and see which similarity function works best for you ,<b> one thing to note however is that normalize the embeddings if you use cosine similarity</b>","metadata":{}},{"cell_type":"code","source":"!pip install timm","metadata":{"execution":{"iopub.status.busy":"2021-12-29T23:19:30.835928Z","iopub.execute_input":"2021-12-29T23:19:30.836591Z","iopub.status.idle":"2021-12-29T23:19:30.841057Z","shell.execute_reply.started":"2021-12-29T23:19:30.836555Z","shell.execute_reply":"2021-12-29T23:19:30.839429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append('../input/timm-pytorch-image-models/pytorch-imcdage-models-master')","metadata":{"execution":{"iopub.status.busy":"2021-12-29T23:19:30.843824Z","iopub.execute_input":"2021-12-29T23:19:30.844427Z","iopub.status.idle":"2021-12-29T23:19:30.850979Z","shell.execute_reply.started":"2021-12-29T23:19:30.844387Z","shell.execute_reply":"2021-12-29T23:19:30.850123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preliminaries\nfrom tqdm import tqdm\nimport math\nimport random\nimport os\nimport pandas as pd\nimport numpy as np\n\n#torch\nimport torch\nimport timm\nimport torch.nn as nn\nfrom torch.nn import Parameter\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset,DataLoader\n\n# Visuals and CV2\nimport cv2\n\n# albumentations for augs\nimport albumentations\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport transformers\n\nimport gc\nimport matplotlib.pyplot as plt\n# from sklearn.neighbors import NearestNeighbors\nimport cudf\nimport cuml\nimport cupy\n\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml import PCA\nfrom cuml.neighbors import NearestNeighbors\nfrom sklearn.preprocessing import Normalizer","metadata":{"execution":{"iopub.status.busy":"2021-12-29T23:19:30.85363Z","iopub.execute_input":"2021-12-29T23:19:30.85382Z","iopub.status.idle":"2021-12-29T23:19:30.862346Z","shell.execute_reply.started":"2021-12-29T23:19:30.853797Z","shell.execute_reply":"2021-12-29T23:19:30.861488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test Configuration","metadata":{}},{"cell_type":"code","source":"DIM = (512,512)\n\nNUM_WORKERS = 4\nBATCH_SIZE = 16\nSEED = 42\n\ndevice = torch.device('cuda')\n\nCLASSES = 11014\n\n################################################  ADJUSTING FOR CV OR SUBMIT ##############################################\n\nCHECK_SUB = False\nGET_CV = True\n\n#CHECK_SUB = True\n#GET_CV = False\n\ntest = pd.read_csv('../input/shopee-product-matching/test.csv')\nif len(test)>3: GET_CV = False\nelse: print('this submission notebook will compute CV score, but commit notebook will not')\n\n\n################################################# MODEL ###################################################################\n\ntransformer_model = '../input/sentence-transformer-models/paraphrase-xlm-r-multilingual-v1/0_Transformer'\nTOKENIZER = transformers.AutoTokenizer.from_pretrained(transformer_model)\n\nmodel_name_img = 'efficientnet_b3' #efficientnet_b0-b7\n\n################################################ MODEL PATH ###############################################################\n\nTEXT_MODEL_PATH = '../input/best-multilingual-model/sentence_transfomer_xlm_best_loss_num_epochs_25_arcface.bin'\nIMG_MODEL_PATH = '../input/arcface-model-v2/model_efficientnet_b3_IMG_SIZE_512_arcface_24.pth'\n\nmodel_params = {\n    'n_classes':11014,\n    'model_name':transformer_model,\n    'use_fc':False,\n    'fc_dim':512,\n    'dropout':0.3,\n}\n\n############################################## Metric Loss and its params #################################################\nloss_module = 'arcface' # 'arcface' #'cosface' #'adacos'\ns = 30.0\nm = 0.5 \nls_eps = 0.0\neasy_margin = False","metadata":{"execution":{"iopub.status.busy":"2021-12-29T23:19:30.864898Z","iopub.execute_input":"2021-12-29T23:19:30.865579Z","iopub.status.idle":"2021-12-29T23:19:32.380506Z","shell.execute_reply.started":"2021-12-29T23:19:30.865543Z","shell.execute_reply":"2021-12-29T23:19:32.379789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reading Data","metadata":{}},{"cell_type":"code","source":"def read_dataset():\n    if GET_CV:\n        df = pd.read_csv('../input/shopee-product-matching/train.csv')\n        tmp = df.groupby(['label_group'])['posting_id'].unique().to_dict()\n        df['matches'] = df['label_group'].map(tmp)\n        df['matches'] = df['matches'].apply(lambda x: ' '.join(x))\n        if CHECK_SUB:\n            df = pd.concat([df, df], axis = 0)\n            df.reset_index(drop = True, inplace = True)\n        df_cu = cudf.DataFrame(df)\n        image_paths = '../input/shopee-product-matching/train_images/' + df['image']\n    else:\n        df = pd.read_csv('../input/shopee-product-matching/test.csv')\n        df_cu = cudf.DataFrame(df)\n        image_paths = '../input/shopee-product-matching/test_images/' + df['image']\n        \n    return df, df_cu, image_paths","metadata":{"execution":{"iopub.status.busy":"2021-12-29T23:19:32.383373Z","iopub.execute_input":"2021-12-29T23:19:32.383827Z","iopub.status.idle":"2021-12-29T23:19:32.391098Z","shell.execute_reply.started":"2021-12-29T23:19:32.383789Z","shell.execute_reply":"2021-12-29T23:19:32.390358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"def seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_torch(SEED)","metadata":{"execution":{"iopub.status.busy":"2021-12-29T23:19:32.392399Z","iopub.execute_input":"2021-12-29T23:19:32.392833Z","iopub.status.idle":"2021-12-29T23:19:32.405822Z","shell.execute_reply.started":"2021-12-29T23:19:32.392798Z","shell.execute_reply":"2021-12-29T23:19:32.404999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def f1_score(y_true, y_pred):\n    y_true = y_true.apply(lambda x: set(x.split()))\n    y_pred = y_pred.apply(lambda x: set(x.split()))\n    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n    len_y_pred = y_pred.apply(lambda x: len(x)).values\n    len_y_true = y_true.apply(lambda x: len(x)).values\n    f1 = 2 * intersection / (len_y_pred + len_y_true)\n    return f1","metadata":{"execution":{"iopub.status.busy":"2021-12-29T23:19:32.408821Z","iopub.execute_input":"2021-12-29T23:19:32.409018Z","iopub.status.idle":"2021-12-29T23:19:32.416129Z","shell.execute_reply.started":"2021-12-29T23:19:32.408995Z","shell.execute_reply":"2021-12-29T23:19:32.415447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_neighbors(df, embeddings, KNN = 50, image = True):\n    '''\n    https://www.kaggle.com/ragnar123/unsupervised-baseline-arcface?scriptVersionId=57121538\n    '''\n\n    model = NearestNeighbors(n_neighbors = KNN)\n    model.fit(embeddings)\n    distances, indices = model.kneighbors(embeddings)\n    \n    # Iterate through different thresholds to maximize cv, run this in interactive mode, then replace else clause with a solid threshold\n    if GET_CV:\n        if image:\n            thresholds = list(np.arange(2,4,0.1))\n        else:\n            thresholds = list(np.arange(0.1, 1, 0.1))\n        scores = []\n        for threshold in thresholds:\n            predictions = []\n            for k in range(embeddings.shape[0]):\n                idx = np.where(distances[k,] < threshold)[0]\n                ids = indices[k,idx]\n                posting_ids = ' '.join(df['posting_id'].iloc[ids].values)\n                predictions.append(posting_ids)\n            df['pred_matches'] = predictions\n            df['f1'] = f1_score(df['matches'], df['pred_matches'])\n            score = df['f1'].mean()\n            print(f'Our f1 score for threshold {threshold} is {score}')\n            scores.append(score)\n        thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\n        max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n        best_threshold = max_score['thresholds'].values[0]\n        best_score = max_score['scores'].values[0]\n        print(f'Our best score is {best_score} and has a threshold {best_threshold}')\n        \n        # Use threshold\n        predictions = []\n        for k in range(embeddings.shape[0]):\n            # Because we are predicting the test set that have 70K images and different label groups, confidence should be smaller\n            if image:\n                idx = np.where(distances[k,] < 2.7)[0]\n            else:\n                idx = np.where(distances[k,] < 0.60)[0]\n            ids = indices[k,idx]\n            posting_ids = df['posting_id'].iloc[ids].values\n            predictions.append(posting_ids)\n    \n    # Because we are predicting the test set that have 70K images and different label groups, confidence should be smaller\n    else:\n        predictions = []\n        for k in tqdm(range(embeddings.shape[0])):\n            if image:\n                idx = np.where(distances[k,] < 2.7)[0]\n            else:\n                idx = np.where(distances[k,] < 0.60)[0]\n            ids = indices[k,idx]\n            posting_ids = df['posting_id'].iloc[ids].values\n            predictions.append(posting_ids)\n        \n    del model, distances, indices\n    gc.collect()\n    return df, predictions","metadata":{"execution":{"iopub.status.busy":"2021-12-29T23:19:32.418662Z","iopub.execute_input":"2021-12-29T23:19:32.419132Z","iopub.status.idle":"2021-12-29T23:19:32.434356Z","shell.execute_reply.started":"2021-12-29T23:19:32.419075Z","shell.execute_reply":"2021-12-29T23:19:32.433566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# -------------------------------- Text Part --------------------------------","metadata":{}},{"cell_type":"markdown","source":"# [Text] Generating Embeddings","metadata":{}},{"cell_type":"code","source":"class ShopeeDataset(Dataset):\n    def __init__(self, csv):\n        self.csv = csv.reset_index()\n\n    def __len__(self):\n        return self.csv.shape[0]\n\n    def __getitem__(self, index):\n        row = self.csv.iloc[index]\n        \n        text = row.title\n        \n        text = TOKENIZER(text, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")\n        input_ids = text['input_ids'][0]\n        attention_mask = text['attention_mask'][0]  \n        \n        return input_ids, attention_mask","metadata":{"execution":{"iopub.status.busy":"2021-12-29T23:19:32.435659Z","iopub.execute_input":"2021-12-29T23:19:32.436096Z","iopub.status.idle":"2021-12-29T23:19:32.445475Z","shell.execute_reply.started":"2021-12-29T23:19:32.436057Z","shell.execute_reply":"2021-12-29T23:19:32.444807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ShopeeNet(nn.Module):\n\n    def __init__(self,\n                 n_classes,\n                 model_name='bert-base-uncased',\n                 use_fc=False,\n                 fc_dim=512,\n                 dropout=0.0):\n        \"\"\"\n        :param n_classes:\n        :param model_name: name of model from pretrainedmodels\n            e.g. resnet50, resnext101_32x4d, pnasnet5large\n        :param pooling: One of ('SPoC', 'MAC', 'RMAC', 'GeM', 'Rpool', 'Flatten', 'CompactBilinearPooling')\n        :param loss_module: One of ('arcface', 'cosface', 'softmax')\n        \"\"\"\n        super(ShopeeNet, self).__init__()\n\n        self.transformer = transformers.AutoModel.from_pretrained(model_name)\n        final_in_features = self.transformer.config.hidden_size\n        \n        self.use_fc = use_fc\n    \n        if use_fc:\n            self.dropout = nn.Dropout(p=dropout)\n            self.fc = nn.Linear(final_in_features, fc_dim)\n            self.bn = nn.BatchNorm1d(fc_dim)\n            self._init_params()\n            final_in_features = fc_dim\n\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, input_ids,attention_mask):\n        feature = self.extract_feat(input_ids,attention_mask)\n        return F.normalize(feature)\n\n    def extract_feat(self, input_ids,attention_mask):\n        x = self.transformer(input_ids=input_ids,attention_mask=attention_mask)\n        \n        features = x[0]\n        features = features[:,0,:]\n\n        if self.use_fc:\n            features = self.dropout(features)\n            features = self.fc(features)\n            features = self.bn(features)\n\n        return features","metadata":{"execution":{"iopub.status.busy":"2021-12-29T23:19:32.446885Z","iopub.execute_input":"2021-12-29T23:19:32.447183Z","iopub.status.idle":"2021-12-29T23:19:32.460332Z","shell.execute_reply.started":"2021-12-29T23:19:32.447148Z","shell.execute_reply":"2021-12-29T23:19:32.45967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_text_embeddings(df):\n    embeds = []\n    \n    model = ShopeeNet(**model_params)\n    model.eval()\n    \n    model.load_state_dict(dict(list(torch.load(TEXT_MODEL_PATH).items())[:-1]))\n    model = model.to(device)\n\n    text_dataset = ShopeeDataset(df)\n    text_loader = torch.utils.data.DataLoader(\n        text_dataset,\n        batch_size=BATCH_SIZE,\n        pin_memory=True,\n        drop_last=False,\n        num_workers=NUM_WORKERS\n    )\n    \n    \n    with torch.no_grad():\n        for input_ids, attention_mask in tqdm(text_loader): \n            input_ids = input_ids.cuda()\n            attention_mask = attention_mask.cuda()\n            feat = model(input_ids, attention_mask)\n            text_embeddings = feat.detach().cpu().numpy()\n            embeds.append(text_embeddings)\n    \n    \n    del model\n    text_embeddings = np.concatenate(embeds)\n    print(f'Our text embeddings shape is {text_embeddings.shape}')\n    del embeds\n    gc.collect()\n    return text_embeddings","metadata":{"execution":{"iopub.status.busy":"2021-12-29T23:19:32.46267Z","iopub.execute_input":"2021-12-29T23:19:32.462935Z","iopub.status.idle":"2021-12-29T23:19:32.4712Z","shell.execute_reply.started":"2021-12-29T23:19:32.462899Z","shell.execute_reply":"2021-12-29T23:19:32.470533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# -------------------------------- Image Part --------------------------------","metadata":{}},{"cell_type":"markdown","source":"# [IMG] Generating Embeddings","metadata":{}},{"cell_type":"code","source":"def get_test_transforms():\n\n    return albumentations.Compose(\n        [\n            albumentations.Resize(DIM[0],DIM[1],always_apply=True),\n            albumentations.Normalize(),\n        ToTensorV2(p=1.0)\n        ]\n    )","metadata":{"execution":{"iopub.status.busy":"2021-12-29T23:19:32.472581Z","iopub.execute_input":"2021-12-29T23:19:32.473219Z","iopub.status.idle":"2021-12-29T23:19:32.481626Z","shell.execute_reply.started":"2021-12-29T23:19:32.473183Z","shell.execute_reply":"2021-12-29T23:19:32.480951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ShopeeDataset_img(Dataset):\n    def __init__(self, image_paths, transforms=None):\n\n        self.image_paths = image_paths\n        self.augmentations = transforms\n\n    def __len__(self):\n        return self.image_paths.shape[0]\n\n    def __getitem__(self, index):\n        image_path = self.image_paths[index]\n        \n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.augmentations:\n            augmented = self.augmentations(image=image)\n            image = augmented['image']       \n        \n        \n        return image,torch.tensor(1)","metadata":{"execution":{"iopub.status.busy":"2021-12-29T23:19:32.482811Z","iopub.execute_input":"2021-12-29T23:19:32.483438Z","iopub.status.idle":"2021-12-29T23:19:32.491895Z","shell.execute_reply.started":"2021-12-29T23:19:32.483403Z","shell.execute_reply":"2021-12-29T23:19:32.491206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ShopeeNet_img(nn.Module):\n\n    def __init__(self,\n                 n_classes,\n                 model_name_img='efficientnet_b0',\n                 use_fc=False,\n                 fc_dim=512,\n                 dropout=0.0,\n                 loss_module='softmax',\n                 s=30.0,\n                 margin=0.50,\n                 ls_eps=0.0,\n                 theta_zero=0.785,\n                 pretrained=False):\n        \"\"\"\n        :param n_classes:\n        :param model_name_img: name of model from pretrainedmodels\n            e.g. resnet50, resnext101_32x4d, pnasnet5large\n        :param pooling: One of ('SPoC', 'MAC', 'RMAC', 'GeM', 'Rpool', 'Flatten', 'CompactBilinearPooling')\n        :param loss_module: One of ('arcface', 'cosface', 'softmax')\n        \"\"\"\n        super(ShopeeNet_img, self).__init__()\n        print('Model building for {} backbone'.format(model_name_img))\n\n        self.backbone = timm.create_model(model_name_img, pretrained=pretrained)\n        final_in_features = self.backbone.classifier.in_features\n        \n        self.backbone.classifier = nn.Identity()\n        self.backbone.global_pool = nn.Identity()\n        \n        self.pooling =  nn.AdaptiveAvgPool2d(1)\n            \n        self.use_fc = use_fc\n        if use_fc:\n            self.dropout = nn.Dropout(p=dropout)\n            self.fc = nn.Linear(final_in_features, fc_dim)\n            self.bn = nn.BatchNorm1d(fc_dim)\n            self._init_params()\n            final_in_features = fc_dim\n\n        self.loss_module = loss_module\n        if loss_module == 'arcface':\n            self.final = ArcMarginProduct(final_in_features, n_classes,\n                                          s=s, m=margin, easy_margin=False, ls_eps=ls_eps)\n        elif loss_module == 'cosface':\n            self.final = AddMarginProduct(final_in_features, n_classes, s=s, m=margin)\n        elif loss_module == 'adacos':\n            self.final = AdaCos(final_in_features, n_classes, m=margin, theta_zero=theta_zero)\n        else:\n            self.final = nn.Linear(final_in_features, n_classes)\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, x, label):\n        feature = self.extract_feat(x)\n        if self.loss_module in ('arcface', 'cosface', 'adacos'):\n            logits = self.final(feature, label)\n        else:\n            logits = self.final(feature)\n        return feature,logits\n\n    def extract_feat(self, x):\n        batch_size = x.shape[0]\n        x = self.backbone(x)\n        x = self.pooling(x).view(batch_size, -1)\n\n        if self.use_fc:\n            x = self.dropout(x)\n            x = self.fc(x)\n            x = self.bn(x)\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-12-29T23:19:32.495036Z","iopub.execute_input":"2021-12-29T23:19:32.495211Z","iopub.status.idle":"2021-12-29T23:19:32.510804Z","shell.execute_reply.started":"2021-12-29T23:19:32.495189Z","shell.execute_reply":"2021-12-29T23:19:32.510106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AdaCos(nn.Module):\n    def __init__(self, in_features, out_features, m=0.50, ls_eps=0, theta_zero=math.pi/4):\n        super(AdaCos, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.theta_zero = theta_zero\n        self.s = math.log(out_features - 1) / math.cos(theta_zero)\n        self.m = m\n        self.ls_eps = ls_eps  # label smoothing\n        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n    def forward(self, input, label):\n        # normalize features\n        x = F.normalize(input)\n        # normalize weights\n        W = F.normalize(self.weight)\n        # dot product\n        logits = F.linear(x, W)\n        # add margin\n        theta = torch.acos(torch.clamp(logits, -1.0 + 1e-7, 1.0 - 1e-7))\n        target_logits = torch.cos(theta + self.m)\n        one_hot = torch.zeros_like(logits)\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n        output = logits * (1 - one_hot) + target_logits * one_hot\n        # feature re-scale\n        with torch.no_grad():\n            B_avg = torch.where(one_hot < 1, torch.exp(self.s * logits), torch.zeros_like(logits))\n            B_avg = torch.sum(B_avg) / input.size(0)\n            theta_med = torch.median(theta)\n            self.s = torch.log(B_avg) / torch.cos(torch.min(self.theta_zero * torch.ones_like(theta_med), theta_med))\n        output *= self.s\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2021-12-29T23:19:32.513664Z","iopub.execute_input":"2021-12-29T23:19:32.51418Z","iopub.status.idle":"2021-12-29T23:19:32.52657Z","shell.execute_reply.started":"2021-12-29T23:19:32.514075Z","shell.execute_reply":"2021-12-29T23:19:32.525914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ArcMarginProduct(nn.Module):\n    r\"\"\"Implement of large margin arc distance: :\n        Args:\n            in_features: size of each input sample\n            out_features: size of each output sample\n            s: norm of input feature\n            m: margin\n            cos(theta + m)\n        \"\"\"\n    def __init__(self, in_features, out_features, s=30.0, m=0.50, easy_margin=False, ls_eps=0.0):\n        super(ArcMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.s = s\n        self.m = m\n        self.ls_eps = ls_eps  # label smoothing\n        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(m)\n        self.sin_m = math.sin(m)\n        self.th = math.cos(math.pi - m)\n        self.mm = math.sin(math.pi - m) * m\n\n    def forward(self, input, label):\n        # --------------------------- cos(theta) & phi(theta) ---------------------------\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n        # --------------------------- convert label to one-hot ---------------------------\n        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n        one_hot = torch.zeros(cosine.size(), device='cuda')\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.s\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2021-12-29T23:19:32.528028Z","iopub.execute_input":"2021-12-29T23:19:32.528521Z","iopub.status.idle":"2021-12-29T23:19:32.541653Z","shell.execute_reply.started":"2021-12-29T23:19:32.528486Z","shell.execute_reply":"2021-12-29T23:19:32.540925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AddMarginProduct(nn.Module):\n    r\"\"\"Implement of large margin cosine distance: :\n    Args:\n        in_features: size of each input sample\n        out_features: size of each output sample\n        s: norm of input feature\n        m: margin\n        cos(theta) - m\n    \"\"\"\n\n    def __init__(self, in_features, out_features, s=30.0, m=0.40):\n        super(AddMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.s = s\n        self.m = m\n        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n    def forward(self, input, label):\n        # --------------------------- cos(theta) & phi(theta) ---------------------------\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        phi = cosine - self.m\n        # --------------------------- convert label to one-hot ---------------------------\n        one_hot = torch.zeros(cosine.size(), device='cuda')\n        # one_hot = one_hot.cuda() if cosine.is_cuda else one_hot\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)  # you can use torch.where if your torch.__version__ is 0.4\n        output *= self.s\n        # print(output)\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2021-12-29T23:19:32.545419Z","iopub.execute_input":"2021-12-29T23:19:32.545634Z","iopub.status.idle":"2021-12-29T23:19:32.554535Z","shell.execute_reply.started":"2021-12-29T23:19:32.545605Z","shell.execute_reply":"2021-12-29T23:19:32.55382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_image_embeddings(image_paths):\n    embeds = []\n    \n    model = ShopeeNet_img(n_classes=CLASSES,model_name_img=model_name_img)\n    model.eval()\n    \n    model.load_state_dict(torch.load(IMG_MODEL_PATH),strict=False)\n    model = model.to(device)\n\n    image_dataset = ShopeeDataset_img(image_paths=image_paths,transforms=get_test_transforms())\n    image_loader = torch.utils.data.DataLoader(\n        image_dataset,\n        batch_size=BATCH_SIZE,\n        pin_memory=True,\n        drop_last=False,\n        num_workers=NUM_WORKERS\n    )\n    \n    \n    with torch.no_grad():\n        for img,label in tqdm(image_loader): \n            img = img.cuda()\n            label = label.cuda()\n            feat, _ = model(img,label)\n            image_embeddings = feat.detach().cpu().numpy()\n            embeds.append(image_embeddings)\n    \n    \n    del model\n    image_embeddings = np.concatenate(embeds)\n    print(f'Our image embeddings shape is {image_embeddings.shape}')\n    del embeds\n    gc.collect()\n    return image_embeddings","metadata":{"execution":{"iopub.status.busy":"2021-12-29T23:19:32.556091Z","iopub.execute_input":"2021-12-29T23:19:32.556628Z","iopub.status.idle":"2021-12-29T23:19:32.566388Z","shell.execute_reply.started":"2021-12-29T23:19:32.55659Z","shell.execute_reply":"2021-12-29T23:19:32.565639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# -------------------- Calculating Predictions --------------------","metadata":{}},{"cell_type":"code","source":"df,df_cu,image_paths = read_dataset()\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-29T23:19:32.567841Z","iopub.execute_input":"2021-12-29T23:19:32.56812Z","iopub.status.idle":"2021-12-29T23:19:37.100379Z","shell.execute_reply.started":"2021-12-29T23:19:32.568086Z","shell.execute_reply":"2021-12-29T23:19:37.099692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# [Text] Predictions","metadata":{}},{"cell_type":"code","source":"text_embeddings = get_text_embeddings(df)","metadata":{"execution":{"iopub.status.busy":"2021-12-29T23:19:37.101646Z","iopub.execute_input":"2021-12-29T23:19:37.1019Z","iopub.status.idle":"2021-12-29T23:22:04.14398Z","shell.execute_reply.started":"2021-12-29T23:19:37.101869Z","shell.execute_reply":"2021-12-29T23:22:04.143211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df, text_predictions = get_neighbors(df, text_embeddings, KNN = 50, image = False)","metadata":{"execution":{"iopub.status.busy":"2021-12-29T23:22:04.145794Z","iopub.execute_input":"2021-12-29T23:22:04.14629Z","iopub.status.idle":"2021-12-29T23:22:36.066403Z","shell.execute_reply.started":"2021-12-29T23:22:04.146252Z","shell.execute_reply":"2021-12-29T23:22:36.065647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# [IMG] Predictions","metadata":{}},{"cell_type":"code","source":"image_embeddings = get_image_embeddings(image_paths.values)","metadata":{"execution":{"iopub.status.busy":"2021-12-29T23:22:36.067913Z","iopub.execute_input":"2021-12-29T23:22:36.068172Z","iopub.status.idle":"2021-12-29T23:22:39.775628Z","shell.execute_reply.started":"2021-12-29T23:22:36.068139Z","shell.execute_reply":"2021-12-29T23:22:39.774676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df,image_predictions = get_neighbors(df, image_embeddings, KNN = 50, image = True)","metadata":{"execution":{"iopub.status.busy":"2021-12-29T23:22:39.776707Z","iopub.status.idle":"2021-12-29T23:22:39.777627Z","shell.execute_reply.started":"2021-12-29T23:22:39.777376Z","shell.execute_reply":"2021-12-29T23:22:39.777402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# image phash","metadata":{}},{"cell_type":"code","source":"tmp = df.groupby('image_phash').posting_id.agg('unique').to_dict()\ndf['phash_preds'] = df.image_phash.map(tmp)","metadata":{"execution":{"iopub.status.busy":"2021-12-29T23:22:39.778651Z","iopub.status.idle":"2021-12-29T23:22:39.779485Z","shell.execute_reply.started":"2021-12-29T23:22:39.77923Z","shell.execute_reply":"2021-12-29T23:22:39.779256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# -------------------- Preparing Submission --------------------","metadata":{}},{"cell_type":"code","source":"def combine_predictions(row):\n#     x = np.concatenate([row['image_predictions'], row['phash_preds']])\n#     return ' '.join( np.unique(x) )\n\n    x = np.concatenate([row['image_predictions'], row['text_predictions'], row['phash_preds']])\n    return ' '.join( np.unique(x) )","metadata":{"execution":{"iopub.status.busy":"2021-12-29T23:22:39.780575Z","iopub.status.idle":"2021-12-29T23:22:39.781515Z","shell.execute_reply.started":"2021-12-29T23:22:39.781211Z","shell.execute_reply":"2021-12-29T23:22:39.781237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not GET_CV:\n    # For Text\n    text_predictions = [' '.join(text_preds) for text_preds in text_predictions]\n    df['matches'] = text_predictions\n    \n    # For Image&Text Combination\n    #df['image_predictions'] = image_predictions\n    #df['matches'] = df.apply(combine_predictions, axis = 1)\n    \n    df[['posting_id','matches']].to_csv('submission.csv',index=False)\n    \nelse:\n    # For Text\n    df['matches'] = text_predictions\n    \n    # For Image&Text Combination\n    #df['image_predictions'] = image_predictions\n    #df['pred_matches'] = df.apply(combine_predictions, axis = 1)\n    #df['f1'] = f1_score(df['matches'], df['pred_matches'])\n    #score = df['f1'].mean()\n    #print(f'Our final f1 cv score is {score}')\n    #df['matches'] = df['pred_matches']\n    \n    df[['posting_id','matches']].to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-29T23:22:39.78288Z","iopub.status.idle":"2021-12-29T23:22:39.783504Z","shell.execute_reply.started":"2021-12-29T23:22:39.78327Z","shell.execute_reply":"2021-12-29T23:22:39.783294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}