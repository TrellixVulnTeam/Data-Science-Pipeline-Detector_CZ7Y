{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Basic\nimport pandas as pd\npd.set_option('display.max_columns', None)\nimport numpy as np\nimport os\nimport random\nfrom tqdm.autonotebook import tqdm\nimport string\nfrom collections import Counter\nimport re\n\n# Visualizations\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS\n\n# NLP\nimport spacy\nnlp = spacy.load('en_core_web_lg', disable=['parser', 'ner'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dir = '../input/shopee-product-matching'\n\ntrain_file_path = os.path.join(data_dir, 'train.csv')\ntest_file_path = os.path.join(data_dir, 'test.csv')\nsample_sub_file_path = os.path.join(data_dir, 'sample_submission.csv')\ntrain_images_path = os.path.join(data_dir, 'train_images')\ntest_images_path = os.path.join(data_dir, 'test_images')\n\nprint(f'Train file: {train_file_path}')\nprint(f'Test file: {test_file_path}')\nprint(f'Sample Sub file: {sample_sub_file_path}')\nprint(f'Train Imaes Path: {train_images_path}')\nprint(f'Test Images Path: {test_images_path}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RANDOM_SEED = 42","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed=RANDOM_SEED):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    random.seed(seed)\nseed_everything()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ntrain_df = pd.read_csv(train_file_path)\ntest_df = pd.read_csv(test_file_path)\nsub_df = pd.read_csv(sample_sub_file_path)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.sample(5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.nunique()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_image(class_num, examples=2, train_df=train_df, train_images_path=train_images_path):\n    image_list = train_df[train_df['label_group'] == class_num]['image'].sample(frac=1)[:examples].to_list()\n    plt.figure(figsize=(20,10))\n    for i, img in enumerate(image_list):\n        full_path = os.path.join(train_images_path, img)\n        img = Image.open(full_path)\n        plt.subplot(1 ,examples, i%examples +1)\n        plt.axis('off')\n        plt.imshow(img)\n        plt.title(f'Class: {class_num}')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_count = [len(x.split()) for x in train_df['title'].tolist()]\nbarplot_dim = (12, 6)\nax = plt.subplots(figsize =barplot_dim);\nax = sns.distplot(word_count, kde=False);\nax.set_ylabel('No. of Observations', size=15)\nax.set_xlabel('No. of Words', size=15)\nax.set_title('Title Word Count Distribution', size=20);\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def text_cleaning(text):\n    '''\n    Converts all text to lower case, Removes special charecters, emojis and multiple spaces\n    text - Sentence that needs to be cleaned\n    '''\n    text = ''.join([k for k in text if k not in string.punctuation])\n    text = str(text).lower()\n    text = re.sub('[^a-zA-Z]', ' ', text)\n    text = re.sub(' +', ' ', text)\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tqdm.pandas()\ntrain_df['title'] = train_df['title'].progress_apply(text_cleaning)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tqdm.pandas()\ntest_df['title'] = test_df['title'].progress_apply(text_cleaning)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_count = [len(x.split()) for x in train_df['title'].tolist()]\nbarplot_dim = (12, 6)\nax = plt.subplots(figsize =barplot_dim);\nax = sns.distplot(word_count, kde=False);\nax.set_ylabel('No. of Observations', size=15)\nax.set_xlabel('No. of Words', size=15)\nax.set_title('Title Word Count Distribution', size=20);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_df = pd.DataFrame()\ntemp_df['temp_list'] = train_df['title'].apply(lambda x :str(x).split())\ntop = Counter([item for sublist in temp_df['temp_list'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(25))\ntemp.columns = ['Common Words', 'Count']\ntemp.style.background_gradient(cmap='Reds')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = ' '.join(train_df['title'])\nwordcloud = WordCloud(background_color='black', stopwords=STOPWORDS, width=2560, height=1440).generate(text)\n\nbarplot_dim = (15, 15)\nax = plt.subplots(figsize=barplot_dim, facecolor='w')\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_text(text, nlp=nlp):\n    '''\n    Returns the text after stop-word removal and lemmatization.\n    text - Sentence to be processed\n    nlp - Spacy NLP model\n    '''\n    doc = nlp(text)\n    lemma_list = [token.lemma_ for token in doc if not token.is_stop]\n    lemmatized_sentence = ' '.join(lemma_list)\n        \n    return lemmatized_sentence\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tqdm.pandas()\ntest_df['title'] = test_df['title'].progress_apply(prepare_text)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from https://www.kaggle.com/isaienkov/shopee-data-understanding-and-analysis\n\nmask = test_df.groupby(['title']).count().reset_index()['title'].tolist()\na = []\nb = []\nfor item in mask:\n    res = test_df[test_df['title']== item]['posting_id'].tolist()\n    ans = ''\n    for id_item in res:\n        ans = ans + str(id_item) + ' '\n    ans = ans[:-1]\n    for id_item in res:\n        a.append(id_item)\n        b.append(ans)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['posting_id'] = a\nsubmission['matches'] = b\nsubmission.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mapping_dict_phash = test_df.groupby('image_phash')['posting_id'].apply(list).to_dict()\ntest_df['matches_temp'] = test_df['image_phash'].map(mapping_dict_phash)\ntest_df['matches_temp'] = test_df['matches_temp'].apply(lambda x: ' '.join(x))\n\nsubmission_map = test_df[['posting_id', 'matches_temp']].set_index('posting_id').to_dict()['matches_temp']\n\nsubmission['matches_temp'] = submission['posting_id'].map(submission_map)\nsubmission['matches_temp'] = submission['matches_temp'] + ' ' + submission['matches']\nsubmission['matches_temp'] = submission['matches_temp'].apply(lambda x: x.split())\nsubmission['matches_temp'] = submission['matches_temp'].apply(lambda x: set(x))\nsubmission['matches'] = submission['matches_temp'].apply(lambda x: ' '.join(x))\nsubmission.drop('matches_temp', axis=1, inplace=True)\nsubmission.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}