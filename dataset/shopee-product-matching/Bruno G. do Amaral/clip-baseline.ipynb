{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Dirty code to make it work..."},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\n!cp -r ../input/openai-clip/CLIP/CLIP-main /tmp/\n\n# Kaggle likes to unpack .gz files in datasets... so we have to pack it back\n!gzip -c /tmp/CLIP-main/clip/bpe_simple_vocab_16e6.txt > /tmp/CLIP-main/clip/bpe_simple_vocab_16e6.txt.gz\nsys.path.append('/tmp/CLIP-main')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\n!pip install ../input/openai-clip/ftfy-5.9/ftfy-5.9 \\\n             ../input/openai-clip/torch-1.7.1+cu110-cp37-cp37m-linux_x86_64.whl \\\n             ../input/openai-clip/torchvision-0.8.2+cu110-cp37-cp37m-linux_x86_64.whl \\\n             ../input/faiss-163/faiss_gpu-1.6.3-cp37-cp37m-manylinux2010_x86_64.whl","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport clip\nfrom PIL import Image\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nimport re\nfrom clip.simple_tokenizer import SimpleTokenizer\nimport faiss\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv('../input/shopee-product-matching/test.csv', index_col='posting_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run train only for commit\nDO_TRAIN = len(df_test) == 3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_tokenizer = SimpleTokenizer()\n\n# Copied from https://github.com/openai/CLIP/blob/beba48f35392a73c6c47ae67ddffced81ad1916d/clip/clip.py#L164\n# but with relaxed exception\ndef tokenize(texts, context_length: int = 77) -> torch.LongTensor:\n    if isinstance(texts, str):\n        texts = [texts]\n\n    sot_token = _tokenizer.encoder[\"<|startoftext|>\"]\n    eot_token = _tokenizer.encoder[\"<|endoftext|>\"]\n    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]\n    result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n\n    for i, tokens in enumerate(all_tokens):\n        n = min(len(tokens), context_length)\n        result[i, :n] = torch.tensor(tokens)[:n]\n        if len(tokens) > context_length:\n            result[i, -1] = tokens[-1]\n\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RE_EMOJI = re.compile(r\"\\\\x[A-Za-z0-9./]+\", flags=re.UNICODE)\n\ndef strip_emoji(text):\n    return RE_EMOJI.sub(r'', text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load CLIP\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load(\"../input/openai-clip/RN50.pt\", device=device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embed_dim = model.text_projection.shape[1]\nembed_dim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(self, df, images_path):\n        super().__init__()\n        self.df = df\n        self.images_path = images_path\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        \n        image = preprocess(Image.open(self.images_path / row['image']))\n        text = tokenize([strip_emoji(row['title'])])[0]\n        \n        return image, text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Generate features for train"},{"metadata":{"trusted":true},"cell_type":"code","source":"if DO_TRAIN:\n    train_images_path = Path('../input/shopee-product-matching/train_images')\n    \n    df_train = pd.read_csv('../input/shopee-product-matching/train.csv', index_col='posting_id')\n\n    dstrain = MyDataset(df_train, train_images_path)\n    dltrain = DataLoader(dstrain, batch_size=32, shuffle=False, num_workers=2)\n\n    train_features = np.empty((len(df_train), 2*embed_dim), dtype=np.float32)\n\n    i = 0\n    for images, texts in tqdm(dltrain):\n        n = len(images)\n        with torch.no_grad():\n            images_features = model.encode_image(images.to(device))\n            texts_features = model.encode_text(texts.to(device))\n\n        train_features[i:i+n, :embed_dim] = images_features.cpu()\n        train_features[i:i+n, embed_dim:] = texts_features.cpu()\n\n        i += n\n    \n    np.save('train_features-no-norm.npy', train_features)\n\n    # l2-normalize\n    train_features /= np.linalg.norm(train_features, 2, axis=1, keepdims=True)\n\n    # Create index\n    index = faiss.IndexFlatIP(2*embed_dim)\n\n    index.add(train_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nif DO_TRAIN:\n    similatiries, indexes = index.search(train_features, 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if DO_TRAIN:\n    np.save('similatiries.npy', similatiries)\n    np.save('indexes.npy', indexes)\n    \n    found_groups = df_train['label_group'].values[indexes]\n\n    is_same_group = (found_groups == found_groups[:, [0]])\n\n    plt.hist([similatiries[is_same_group], similatiries[~is_same_group]], density=True, bins=21,\n             label=['Same group', 'Different group'])\n    plt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Run on test"},{"metadata":{"trusted":true},"cell_type":"code","source":"GROUP_CUT = 0.84  # Use train code to find this number","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_images_path = Path('../input/shopee-product-matching/test_images')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dstest = MyDataset(df_test, test_images_path)\ndltest = DataLoader(dstest, batch_size=32, shuffle=False, num_workers=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features = np.empty((len(df_test), 2*embed_dim), dtype=np.float32)\n\ni = 0\nfor images, texts in tqdm(dltest):\n    n = len(images)\n    with torch.no_grad():\n        images_features = model.encode_image(images.to(device))\n        texts_features = model.encode_text(texts.to(device))\n        \n    test_features[i:i+n, :embed_dim] = images_features.cpu()\n    test_features[i:i+n, embed_dim:] = texts_features.cpu()\n    \n    i += n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# l2-normalize\ntest_features /= np.linalg.norm(test_features, 2, axis=1, keepdims=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create index\nindex_test = faiss.IndexFlatIP(2 * embed_dim)\n\n\nindex_test.add(test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nsimilatiries, indexes = index_test.search(test_features, 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## TODO: try range_search\n# lims, similatiries, indexes = index_test.range_search(test_features, GROUP_CUT)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_are_same_groups = (similatiries > GROUP_CUT)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = []\n\nfor i, (test_is_same_group, index_result) in enumerate(zip(test_are_same_groups, indexes)):\n    row_results = df_test.index[index_result[test_is_same_group]]\n    \n    results.append({\n        'posting_id': df_test.index[i],\n        'matches': ' '.join(row_results)\n    })\n    \ndf_sub = pd.DataFrame(results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!head submission.csv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}