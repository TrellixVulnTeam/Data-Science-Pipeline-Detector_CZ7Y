{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# part 1\nimport numpy as np\nimport pandas as pd\nimport pickle as pkl\nimport matplotlib.pyplot as plt\n\n# additional for part 2\nfrom cuml.neighbors import NearestNeighbors\nimport tensorflow as tf\nfrom tensorflow.keras.applications import EfficientNetB0\nimport cv2\nimport gc","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Credits to cdeotte ([from this kernel](https://www.kaggle.com/cdeotte/part-2-rapids-tfidfvectorizer-cv-0-700)):\n* Restricting tensorflow GPU\n* Commit/submit flags\n* DataGenerator class\n* Weights dataset\n* Embedding code\n* NearestNeighbors code","metadata":{}},{"cell_type":"code","source":"# RESTRICT TENSORFLOW TO 1GB OF GPU RAM\n# SO THAT WE HAVE 15GB RAM FOR RAPIDS\nLIMIT = 1\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n  try:\n    tf.config.experimental.set_virtual_device_configuration(\n        gpus[0],\n        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*LIMIT)])\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n    #print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n  except RuntimeError as e:\n    print(e)\nprint('We will restrict TensorFlow to max %iGB GPU RAM'%LIMIT)\nprint('then RAPIDS can use %iGB GPU RAM'%(16-LIMIT))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 1 - choose cutoff for what's considered an item having a \"close\" image match","metadata":{}},{"cell_type":"markdown","source":"## Read in data from other sources","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"../input/shopee-product-matching/train.csv\")\ntwo_way_close_img_distances_df = pd.read_csv(\"../input/shoppee-efficientnetb0-distances/EfficientNetB0_distances.csv\")\ntwo_way_close_img_distances_df.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"closest_distances = two_way_close_img_distances_df.groupby(\"posting_id\")[\"img_distance\"].min()\nclosest_non_true_distances = two_way_close_img_distances_df.query(\"~true_match\").groupby(\"posting_id\")[\"img_distance\"].min()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Choose cutoff through manual exploration","metadata":{}},{"cell_type":"code","source":"# see the figs below\n# we will probe the test set to see if follows the blue distribution or the orange - or something in between\n# we'll use that as a proxy to estimate product overlap with the training set\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\nax1.set_title(\"distribution of items' closest image distances\")\nclosest_distances.hist(ax=ax1, bins=10)\nax2.set_title(\"distribution of items' closest image distances from different product-groups\")\nclosest_non_true_distances.hist(color=\"orange\", ax=ax2, bins=20)\nplt.ylim([0,12500])\npass","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# manually play around with different cutoffs\n# we want something that is that has a high percentage for the training distribution, but there's a big difference between whether we include true matches or not\ncutoff = .10\ntotal = len(train_df)\nprint(\"Using cutoff of:\", cutoff)\nprint(f\"A. percent of items with matches within {cutoff} cosine distance: {(closest_distances<cutoff).sum() / total: .1%}\")\nprint(f\"B. percent of items with different-group matches within {cutoff} cosine distance:{(closest_non_true_distances<cutoff).sum() / total: .1%}\")\nprint(f\"C. Ratio of B/A: {(closest_distances<cutoff).sum() / (closest_non_true_distances<cutoff).sum(): .1f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set cutoff\n\n# .1 gives us a nice percentage of items with \"close\" matches in the training set,\n# as well as a big difference whether we include same-product matches or not\nCUTOFF = .1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2 - Calculate Percent of items in test set, with \"close\" match in training set","metadata":{}},{"cell_type":"markdown","source":"## load test_df","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv(\"../input/shopee-product-matching/test.csv\")\nif len(test_df) == 3:\n    HAVE_TEST = False\nelse:\n    HAVE_TEST = True\n    \nif not HAVE_TEST:\n    test_df = train_df.iloc[:5000]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## create test_df embeddings","metadata":{}},{"cell_type":"code","source":"if HAVE_TEST:\n    BASE = '../input/shopee-product-matching/test_images/'\nelse:\n    BASE = '../input/shopee-product-matching/train_images/'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataGenerator(tf.keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, df, img_size=256, batch_size=32, path=''): \n        self.df = df\n        self.img_size = img_size\n        self.batch_size = batch_size\n        self.path = path\n        self.indexes = np.arange( len(self.df) )\n        \n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        ct = len(self.df) // self.batch_size\n        ct += int(( (len(self.df)) % self.batch_size)!=0)\n        return ct\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        X = self.__data_generation(indexes)\n        return X\n            \n    def __data_generation(self, indexes):\n        'Generates data containing batch_size samples' \n        X = np.zeros((len(indexes),self.img_size,self.img_size,3),dtype='float32')\n        df = self.df.iloc[indexes]\n        for i,(index,row) in enumerate(df.iterrows()):\n            img = cv2.imread(self.path+row.image)\n            X[i,] = cv2.resize(img,(self.img_size,self.img_size)) #/128.0 - 1.0\n        return X","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"WGT = '../input/effnetb0/efficientnetb0_notop.h5'\nmodel = EfficientNetB0(weights=WGT,include_top=False, pooling='avg', input_shape=None)\n\nembeds = []\nCHUNK = 1024*4\n\nprint('Computing image embeddings...')\nNUM_CHUNKS = len(test_df)//CHUNK\nif len(test_df)%CHUNK!=0: NUM_CHUNKS += 1\n\nfor i in range(NUM_CHUNKS):\n\n    a = i*CHUNK\n    b = (i+1)*CHUNK\n    b = min(b,len(test_df))\n    print('chunk',a,'to',b)\n    \n    train_gen = DataGenerator(test_df.iloc[a:b], batch_size=32, path=BASE)\n    image_embeddings = model.predict(train_gen,verbose=1,use_multiprocessing=True, workers=4)\n    embeds.append(image_embeddings)\n    \ndel model\n_ = gc.collect()\ntest_image_embeddings = np.concatenate(embeds)\nprint('image embeddings shape',test_image_embeddings.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## load train embeddings, to quickly make the nearest neighbor model","metadata":{}},{"cell_type":"code","source":"with open(\"../input/shoppee-create-distance-sets-df/train_img_embeddings.pkl\", \"rb\") as f:\n    train_image_embeddings = pkl.load(f)\n    \nif HAVE_TEST:\n    n_neighbors=1\nelse:\n    n_neighbors=2 # first neighbor in train will always be itself\n    \nmodel = NearestNeighbors(n_neighbors=n_neighbors, metric=\"cosine\")\nmodel.fit(train_image_embeddings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Find Neighbors","metadata":{}},{"cell_type":"code","source":"CHUNK = 1024*4\n\nprint('Finding similar images...')\nNUM_CHUNKS = len(test_image_embeddings)//CHUNK\nif len(test_image_embeddings)%CHUNK!=0: NUM_CHUNKS += 1\n\nall_distances = []\n    \nfor j in range(NUM_CHUNKS):\n\n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(test_image_embeddings))\n    print('chunk',a,'to',b)\n    distances, indices = model.kneighbors(test_image_embeddings[a:b,])\n\n    distances = np.round(distances,4)\n    \n    all_distances.append(distances)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_distances = np.concatenate(all_distances)\nall_distances = all_distances[:,n_neighbors-1] # see code above for how we set n_neighbors","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"percent = (all_distances<CUTOFF).sum() / len(test_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}