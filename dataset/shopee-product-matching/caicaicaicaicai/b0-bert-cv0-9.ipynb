{"cells":[{"metadata":{"_uuid":"34fd2006-7b83-4fae-bea1-17bf326af36f","_cell_guid":"6c1a6b10-6e20-405c-8b45-149014e95997","trusted":true},"cell_type":"markdown","source":"thanks to https://www.kaggle.com/cdeotte/part-2-rapids-tfidfvectorizer-cv-0-700"},{"metadata":{"_uuid":"05f06942-f4a7-489a-9934-dec0f780a554","_cell_guid":"e444446a-42d2-4333-9a31-3d35b959b0ca","trusted":true},"cell_type":"markdown","source":"# Load Libraries"},{"metadata":{"_uuid":"b4cb4519-0d65-43d6-84ac-4b0d83e63570","_cell_guid":"12b86128-98f9-492a-bd67-977503d31456","trusted":true},"cell_type":"code","source":"import sys\nsys.path = [\n    '../input/geffnet-20200820'  \n] + sys.path\n\nimport numpy as np, pandas as pd, gc\nimport cv2, matplotlib.pyplot as plt\nimport cudf, cuml, cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors\n\nimport albumentations\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport geffnet\nfrom transformers import *","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"87aa4b0f-313f-4ebc-95bc-9519556bed44","_cell_guid":"2a2f8589-602f-4365-b988-c1b94d9fdf8f","trusted":true},"cell_type":"code","source":"import re\nimport string\n\ndef cleanReview(title):\n    # 利用正则匹配\\x开头的特殊字符\n    result = re.findall(r'\\\\x[a-f0-9]{2}', title)\n    for x in result:\n        title = title.replace(x, '')\n    title = title.lower()\n    # 替换所有标点符号\n    for c in string.punctuation:\n        title = title.replace(c, \" \")\n    # 利用正则匹配ml或者cm或者mm等结束的特殊字符\n    unitlist = ['mm', 'cm', 'm', 'ml', 'l', 'gr', 'kg', '%', 'w']\n    maplist = {'mm':'millimeter', 'cm':'centimeter', 'm':'meter', 'ml':'millilitre', 'l':'litre', 'gr':'gram', 'kg':'kilogram', '%':'percentage', 'w':'watt'}\n    for u in unitlist:\n        result = re.findall(r'[0-9\\s]'+u+'[\\s]', title) + re.findall(r'[0-9\\s]'+u+'\\Z', title)\n        for x in result:\n            title = title.replace(x, x.strip()[:-len(u)]+' '+ maplist[u] +' ')\n    title = ' '.join(title.split())\n    return title","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64f42601-027b-40da-8f17-33ba31c8bad7","_cell_guid":"fe9bc023-921c-48b9-ac06-9e892f88ed97","trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/shopee-product-matching/test.csv')\ntest['title'] = test['title'].apply(cleanReview)\ntest.to_csv('test.csv',index=False)\n\ntrain = pd.read_csv('../input/shopee-product-matching/train.csv')\ntrain['title'] = train['title'].apply(cleanReview)\ntrain.to_csv('train.csv',index=False)\n\ntrain_fold = pd.read_csv('../input/shopee-folds/train_fold.csv')\ntrain_fold['title'] = train_fold['title'].apply(cleanReview)\ntrain_fold.to_csv('train_fold.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e3d58116-3d31-45b7-a5b9-21bdac4d092f","_cell_guid":"6e8b9769-d563-4c16-be3f-e552f967f3a7","trusted":true},"cell_type":"markdown","source":"# Load Train Data\nFirst we load the train data and create a target column of ground truths to help us compute CV score. Note how the variable `COMPUTE_CV` will change to `False` when we **submit** this notebook but it is `True` now because you are reading a **commit** notebook."},{"metadata":{"_uuid":"47567f32-e128-41c4-ac8a-c27ee577a1e3","_cell_guid":"62159c8e-285f-4e22-9826-c4a651c91465","trusted":true},"cell_type":"code","source":"COMPUTE_CV = True\n\ntest = pd.read_csv('test.csv')\nif len(test)>3: COMPUTE_CV = False\nelse: print('this submission notebook will compute CV score, but commit notebook will not')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2b2f4b35-3b82-41dc-b3a6-4fc57d653591","_cell_guid":"d8c7ce31-66f1-44e6-9b2f-7f7dda26d782","trusted":true},"cell_type":"code","source":"train = pd.read_csv('train.csv')\ntmp = train.groupby('label_group').posting_id.agg('unique').to_dict()\ntrain['target'] = train.label_group.map(tmp)\nprint('train shape is', train.shape )\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c0c73ca1-f9c6-4eb5-82f9-fe21e3a28ed9","_cell_guid":"4c9a73bf-6a65-4122-b07b-89884ddd1624","trusted":true},"cell_type":"markdown","source":"# Compute Baseline CV Score\nA baseline is to predict all items with the same `image_phash` as being duplicate. Let's calcuate the CV score for this submission."},{"metadata":{"_uuid":"0ffe1c1d-8a75-416b-ae23-16de68c4d4e2","_cell_guid":"5c751e52-b765-42b6-a84a-d39790bbbfbc","trusted":true},"cell_type":"code","source":"tmp = train.groupby('image_phash').posting_id.agg('unique').to_dict()\ntrain['oof'] = train.image_phash.map(tmp)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a43aa11-e630-457b-b327-9007baf56d9b","_cell_guid":"17d56236-88c7-4925-a810-920ee71a5d11","trusted":true},"cell_type":"code","source":"def getMetric(col):\n    def f1score(row):\n        n = len( np.intersect1d(row.target,row[col]) )\n        return 2*n / (len(row.target)+len(row[col]))\n    return f1score","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"706ad7d9-7e6d-43d6-9c1b-2ca6d1108de4","_cell_guid":"2b7d8dc9-864c-4bd3-b50a-2dcd8f6c02c8","trusted":true},"cell_type":"code","source":"train['f1'] = train.apply(getMetric('oof'),axis=1)\nprint('CV score for baseline =',train.f1.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d9d814e-d41f-422b-b51d-e4fdc27fa092","_cell_guid":"cd39878e-121e-41d1-bdae-7e65af0bc38f","trusted":true},"cell_type":"markdown","source":"# Compute RAPIDS Model CV and Infer Submission\nWe will now use image embeddings, text embeddings, and phash to create a better model with better CV. We will also infer submission csv.\n\nNote how the variable `COMPUTE_CV` is only `True` when we **commit** this notebook. Right now you are reading a **commit** notebook, so we see test replaced with train and computed CV score. When we **submit** this notebook, the variable `COMPUTE_CV` will be `False` and the **submit** notebook will **not** compute CV. Instead it will load the real test dataset with 70,000 rows and find duplicates in the real test dataset."},{"metadata":{"_uuid":"2fa1056d-a595-4415-b586-9fbb42a76bf7","_cell_guid":"8d122225-21aa-4374-8613-d8a5b3e26d29","trusted":true},"cell_type":"code","source":"if COMPUTE_CV:\n    test = pd.read_csv('train_fold.csv')\n#     test = test[test.fold==0]\n    test_gf = cudf.DataFrame(test)\n    print('Using train as test to compute CV (since commit notebook). Shape is', test_gf.shape )\nelse:\n    test = pd.read_csv('test.csv')\n    test_gf = cudf.read_csv('test.csv')\n    print('Test shape is', test_gf.shape )\ntest_gf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"948cb1dd-c234-4fc8-9560-7feff39a04ce","_cell_guid":"705d0b6b-41a0-4028-a4c3-2044b9133200","trusted":true},"cell_type":"markdown","source":"# Use Image Embeddings\nTo prevent memory errors, we will compute image embeddings in chunks. And we will find similar images with RAPIDS cuML KNN in chunks."},{"metadata":{"_uuid":"7782e7b2-e9b3-4c29-9e1c-40214b046a21","_cell_guid":"c8825a08-092a-45e7-9691-c816ea737885","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import os\n\ndef get_transforms(img_size=256):\n    return  albumentations.Compose([\n                albumentations.Resize(300, 300),\n                albumentations.CenterCrop(img_size,img_size, p=1.0),\n#                 albumentations.RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n                albumentations.Normalize()\n            ])\n\n\nclass LandmarkDataset(Dataset):\n    def __init__(self, csv, split, mode, transforms=get_transforms(img_size=256), tokenizer=None):\n\n        self.csv = csv.reset_index()\n        self.split = split\n        self.mode = mode\n        self.transform = transforms\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return self.csv.shape[0]\n\n    def __getitem__(self, index):\n        row = self.csv.iloc[index]\n        \n        text = row.title\n        \n        image = cv2.imread(row.filepath)\n        image = image[:, :, ::-1]\n        \n        res0 = self.transform(image=image)\n        image0 = res0['image'].astype(np.float32)\n        image = image0.transpose(2, 0, 1)        \n\n        text = self.tokenizer(text, padding='max_length', truncation=True, max_length=16, return_tensors=\"pt\")\n        input_ids = text['input_ids'][0]\n        attention_mask = text['attention_mask'][0]\n\n        if self.mode == 'test':\n            return torch.tensor(image), input_ids, attention_mask\n        else:\n            return torch.tensor(image), input_ids, attention_mask, torch.tensor(row.label_group)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cf6f23be-419b-4eae-9cf1-71ff260480bc","_cell_guid":"f5e923df-c219-4a22-be36-f08769cc7453","trusted":true},"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('../input/bert-base-uncased')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"182db2aa-1e4e-4b0b-aa07-c05545d7fd28","_cell_guid":"c5961f69-d2fc-4357-99f9-53bb98657085","trusted":true},"cell_type":"code","source":"if not COMPUTE_CV: \n    df_sub = pd.read_csv('test.csv')\n\n    df_test = df_sub.copy()\n    df_test['filepath'] = df_test['image'].apply(lambda x: os.path.join('../input/shopee-product-matching/', 'test_images', x))\n\n    dataset_test = LandmarkDataset(df_test, 'test', 'test', transforms=get_transforms(img_size=256), tokenizer=tokenizer)\n    test_loader = DataLoader(dataset_test, batch_size=16, num_workers=4)\n\n    print(len(dataset_test),dataset_test[0])\nelse:\n    df_sub = test\n\n    df_test = df_sub.copy()\n    df_test['filepath'] = df_test['image'].apply(lambda x: os.path.join('../input/shopee-product-matching/', 'train_images', x))\n\n    dataset_test = LandmarkDataset(df_test, 'test', 'test', transforms=get_transforms(img_size=256), tokenizer=tokenizer)\n    test_loader = DataLoader(dataset_test, batch_size=16, num_workers=4)\n\n    print(len(dataset_test),dataset_test[0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"daf24bdf-ce3b-4d1b-abe6-9c6d8f9f0b30","_cell_guid":"ecd0e962-2108-4dee-b4e9-c0a066fa1939","trusted":true},"cell_type":"code","source":"class ArcMarginProduct_subcenter(nn.Module):\n    def __init__(self, in_features, out_features, k=3):\n        super().__init__()\n        self.weight = nn.Parameter(torch.FloatTensor(out_features*k, in_features))\n        self.reset_parameters()\n        self.k = k\n        self.out_features = out_features\n        \n    def reset_parameters(self):\n        stdv = 1. / math.sqrt(self.weight.size(1))\n        self.weight.data.uniform_(-stdv, stdv)\n        \n    def forward(self, features):\n        cosine_all = F.linear(F.normalize(features), F.normalize(self.weight))\n        cosine_all = cosine_all.view(-1, self.out_features, self.k)\n        cosine, _ = torch.max(cosine_all, dim=2)\n        return cosine \n    \nsigmoid = torch.nn.Sigmoid()\n\nclass Swish(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, i):\n        result = i * sigmoid(i)\n        ctx.save_for_backward(i)\n        return result\n    @staticmethod\n    def backward(ctx, grad_output):\n        i = ctx.saved_variables[0]\n        sigmoid_i = sigmoid(i)\n        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))\n\nclass Swish_module(nn.Module):\n    def forward(self, x):\n        return Swish.apply(x)\n\n    \n \n    \nclass enet_arcface_FINAL(nn.Module):\n\n    def __init__(self, enet_type, out_dim):\n        super(enet_arcface_FINAL, self).__init__()\n        self.bert = AutoModel.from_pretrained('../input/bert-base-uncased')\n        self.enet = geffnet.create_model(enet_type.replace('-', '_'), pretrained=None)\n        self.feat = nn.Linear(self.enet.classifier.in_features+self.bert.config.hidden_size, 512)\n        self.swish = Swish_module()\n        self.dropout = nn.Dropout(0.5)\n        self.metric_classify = ArcMarginProduct_subcenter(512, out_dim)\n        self.enet.classifier = nn.Identity()\n \n    def forward(self, x,input_ids, attention_mask):\n        x = self.enet(x)\n        text = self.bert(input_ids=input_ids, attention_mask=attention_mask)[1]\n        x = torch.cat([x, text], 1)\n        x = self.swish(self.feat(x))\n        return F.normalize(x), self.metric_classify(x)\n    \ndef load_model(model, model_file):\n    state_dict = torch.load(model_file)\n    if \"model_state_dict\" in state_dict.keys():\n        state_dict = state_dict[\"model_state_dict\"]\n    state_dict = {k[7:] if k.startswith('module.') else k: state_dict[k] for k in state_dict.keys()}\n#     del state_dict['metric_classify.weight']\n    model.load_state_dict(state_dict, strict=True)\n    print(f\"loaded {model_file}\")\n    model.eval()    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"93636f88-cd44-41f6-aef5-8f857b9f2c6f","_cell_guid":"f6db9280-7ff4-4d09-af01-1cd0cedf124d","trusted":true},"cell_type":"code","source":"import math\nfrom tqdm import tqdm\n\nWGT = '../input/shopee-b0-bert/b0ns_256_bert_20ep_fold0_epoch27.pth'\n\nmodel = enet_arcface_FINAL('tf_efficientnet_b0_ns', out_dim=11014).cuda()\nmodel = load_model(model, WGT)\n\n\nembeds = []\n\nwith torch.no_grad():\n    for img, input_ids, attention_mask in tqdm(test_loader): \n        img, input_ids, attention_mask = img.cuda(), input_ids.cuda(), attention_mask.cuda()\n        feat, _ = model(img, input_ids, attention_mask)\n        image_embeddings = feat.detach().cpu().numpy()\n        embeds.append(image_embeddings)\n\n    \ndel model\n_ = gc.collect()\nimage_embeddings = np.concatenate(embeds)\nprint('image embeddings shape',image_embeddings.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a5a19f6c-c852-445d-a247-1074ad928675","_cell_guid":"ecc51ad8-18b0-4efc-8c8a-a044fa1028f5","trusted":true},"cell_type":"code","source":"KNN = 50\nif len(test)==3: KNN = 2\nmodel = NearestNeighbors(n_neighbors=KNN)\nmodel.fit(image_embeddings)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"57c9029a-836d-4df6-bc81-e1dddde7858e","_cell_guid":"3dbec7a3-0d61-4efa-be4b-9a5556709d34","trusted":true},"cell_type":"code","source":"image_embeddings = cupy.array(image_embeddings)\npreds = []\nCHUNK = 1024*4\n\nprint('Finding similar images...')\nCTS = len(image_embeddings)//CHUNK\nif len(image_embeddings)%CHUNK!=0: CTS += 1\nfor j in range( CTS ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(image_embeddings))\n    print('chunk',a,'to',b)\n   \n    cts = cupy.matmul(image_embeddings, image_embeddings[a:b].T).T\n    \n    for k in range(b-a):\n#         print(sorted(cts[k,], reverse=True))\n        IDX = cupy.where(cts[k,]>0.5)[0]\n        o = test.iloc[cupy.asnumpy(IDX)].posting_id.values\n        preds.append(o)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aca10acf-b4bd-44e0-ade6-5b0a3ede1c31","_cell_guid":"e9ede2b4-8f32-416b-a53a-822e672b1e85","trusted":true},"cell_type":"code","source":"test['preds2'] = preds\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f8c369a4-35ae-456b-96f6-c6eb91d51db9","_cell_guid":"fc839f47-a631-41cf-8f55-6a47406b3882","trusted":true},"cell_type":"markdown","source":"# Use Text Embeddings\nTo prevent memory errors, we will find similar titles in chunks. To faciliate this, we will use cosine similarity between text embeddings instead of KNN."},{"metadata":{"_uuid":"6032bbe6-f861-43db-9be8-ebe8d7e92ba5","_cell_guid":"23788230-0496-4d89-abfc-0675b066d25f","trusted":true},"cell_type":"code","source":"print('Computing text embeddings...')\nmodel = TfidfVectorizer(stop_words=None, \n                        binary=True, \n                        max_features=25000)\ntext_embeddings = model.fit_transform(test_gf.title).toarray()\nprint('text embeddings shape',text_embeddings.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"32b036e7-a409-4b36-ab5e-743b9c618403","_cell_guid":"19e44374-8388-456a-8d98-c37166a3ad26","trusted":true},"cell_type":"code","source":"preds = []\nCHUNK = 1024*4\n\nprint('Finding similar titles...')\nCTS = len(test)//CHUNK\nif len(test)%CHUNK!=0: CTS += 1\nfor j in range( CTS ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(test))\n    print('chunk',a,'to',b)\n    \n    #COSINE SIMILARITY DISTANCE\n    cts = cupy.matmul(text_embeddings, text_embeddings[a:b].T).T\n    \n    for k in range(b-a):\n        IDX = cupy.where(cts[k,]>0.75)[0]\n        o = test.iloc[cupy.asnumpy(IDX)].posting_id.values\n        preds.append(o)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c596a646-a151-4a5f-b2cc-ceb18ebf4358","_cell_guid":"a4129a57-5096-44e6-bb53-9f3d77039c5f","trusted":true},"cell_type":"code","source":"test['preds'] = preds\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1bcf8986-004f-4b0b-887b-883126221024","_cell_guid":"a0f9e41f-b737-4684-8ba4-a67411edf727","trusted":true},"cell_type":"markdown","source":"# Use Phash Feature\nWe will predict all items with the same phash as duplicates"},{"metadata":{"_uuid":"6d6408fd-6532-4a0d-a02c-549603f49071","_cell_guid":"ebd87fa0-cd9e-419f-828b-844b9f01bc7a","trusted":true},"cell_type":"code","source":"tmp = test.groupby('image_phash').posting_id.agg('unique').to_dict()\ntest['preds3'] = test.image_phash.map(tmp)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e3db8d8-b560-410e-8fa5-0c82285bd507","_cell_guid":"f50bbc56-ed1a-4cf5-a588-3be314ba65ec","trusted":true},"cell_type":"markdown","source":"# Compute CV Score\nThis simple model scores a high CV of 0.700+!"},{"metadata":{"_uuid":"e9c99d0c-e135-4e54-b8bd-80e487c6bf23","_cell_guid":"b21d0b22-eea1-4037-9966-e35abd6441ff","trusted":true},"cell_type":"code","source":"def combine_for_sub(row):\n    x = np.concatenate([row.preds, row.preds2, row.preds3])\n    return ' '.join( np.unique(x) )\n\ndef combine_for_cv(row):\n    x = np.concatenate([row.preds, row.preds2, row.preds3])\n    return np.unique(x)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5cb12577-16da-4f95-b81e-cfb05da923c9","_cell_guid":"a584175d-8d18-48e5-bfce-195ae7729eb6","trusted":true},"cell_type":"code","source":"if COMPUTE_CV:\n    tmp = test.groupby('label_group').posting_id.agg('unique').to_dict()\n    test['target'] = test.label_group.map(tmp)\n    test['oof'] = test.apply(combine_for_cv,axis=1)\n    test['f1'] = test.apply(getMetric('oof'),axis=1)\n    print('CV Score =', test.f1.mean() )\n\ntest['matches'] = test.apply(combine_for_sub,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d25f5f52-52ea-4c85-9678-64893c572333","_cell_guid":"f3bf3751-3a5d-4618-b4eb-3cf6a147903c","trusted":true},"cell_type":"code","source":"print(\"CV for image :\", round(test.apply(getMetric('preds2'),axis=1).mean(), 3))\nprint(\"CV for text  :\", round(test.apply(getMetric('preds'),axis=1).mean(), 3))\nprint(\"CV for phash :\", round(test.apply(getMetric('preds3'),axis=1).mean(), 3))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7241ad5b-3d93-4b15-9313-33b47a7ecc76","_cell_guid":"32dc85a4-d2f7-4a75-8512-9895cfbae9d1","trusted":true},"cell_type":"code","source":"test","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b38cb0eb-89a1-4806-94e8-26cfeb307b91","_cell_guid":"47130a16-43b9-441f-9889-b8bbea006f10","trusted":true},"cell_type":"markdown","source":"# Write Submission CSV\nIn this notebook, the submission file below looks funny containing train information. But when we submit this notebook, the size of `test.csv` dataframe will be longer than 3 rows and the variable `COMPUTE_CV` will subsequently set to `False`. Then our submission notebook will compute the correct matches using the real test dataset and our submission csv for LB will be ok."},{"metadata":{"_uuid":"6ccccfd0-0244-4fc0-8743-ae20cf7ec1d0","_cell_guid":"5160b706-7383-4a67-aa24-ef614a08d6b8","trusted":true},"cell_type":"code","source":"test[['posting_id','matches']].to_csv('submission.csv',index=False)\nsub = pd.read_csv('submission.csv')\nsub.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}