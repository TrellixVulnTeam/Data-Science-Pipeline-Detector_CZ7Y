{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Shopee Product Matching EDA and Cleaning\n### Import Required Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport cv2\nimport seaborn as sns\nimport sklearn\nimport matplotlib.pyplot as plt\nfrom textwrap import wrap\nimport pytesseract\nimport re,string\nfrom wordcloud import WordCloud, STOPWORDS\nfrom tqdm.notebook import tqdm\nfrom joblib import dump, load\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"path = '../input/shopee-product-matching'\ntrain_path = '../input/shopee-product-matching/train_images'\ntest_path = '../input/shopee-product-matching/test_images'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's have a look at the data head**"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"data = pd.read_csv(path+'/'+'train.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Basic Details about the data"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"print(f\"The Shape of the train data : {data.shape}\")\nprint(f\"Duplicate Rows : {data.shape[0] - len(data['posting_id'].unique())}\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"print(\"Number unique label_groups = {}\".format( len(data[\"label_group\"].unique()) ))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Here one label group indicates similar products, i.e: all products with same label_id are similar**"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"num_label_groups = {}\nfor i in data['label_group']:\n    num_label_groups[i] = data[data['label_group'] == i]\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"len_label_groups = {}\nfor i in num_label_groups:\n    len_label_groups[i] = len(num_label_groups[i])\n\nprint(f\"Max of all the label groups : {max(len_label_groups.values())}\")\nprint(f\"Min of all the label groups : {min(len_label_groups.values())}\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"label_names = list(num_label_groups.keys())\nnum_label_groups[label_names[5]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's Visualize some Similar products**\n\nThis Function allows you to input label_id and view top 10 (or less where applicable) similar products."},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"def visualize_sim_products(label_id):\n    sns.set_style(\"whitegrid\")\n    plt.rcParams['font.size'] = '28'\n    plt.figure(figsize=(50,50))\n    length = len_label_groups[label_id]\n    if length > 10:\n        length = 10\n    for i in range(length):\n        img = plt.imread(train_path + '/' + num_label_groups[label_id]['image'].iloc[i])\n        plt.subplot(length,2,i+1)\n        plt.imshow(img)\n        plt.title(\"\\n\".join(wrap(num_label_groups[label_id]['title'].iloc[i],60)))\n        plt.axis('off')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visualize_sim_products(label_names[14])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Perceptual Hashing\n\nIn this challenge perpetual hashing is provided, so I read up on it, According to wikipedia : \n> Perceptual hashing is the use of an algorithm that produces a snippet or fingerprint of various forms of multimedia.[1][2] Perceptual hash functions are analogous if features of the multimedia are similar, whereas cryptographic hashing relies on the avalanche effect of a small change in input value creating a drastic change in output value. Perceptual hash functions are widely used in finding cases of online copyright infringement as well as in digital forensics because of the ability to have a correlation between hashes so similar data can be found (for instance with a differing watermark).\n\n*So maybe similar objects have similar Perceptual hash values.* \nThis is my hypothesis, let's check if this is true."},{"metadata":{"trusted":true},"cell_type":"code","source":"num_label_groups[label_names[17]]['image_phash']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" **Seems like my hypotheses was wrong.** \n\nBut we can use the hamming distance between these phashes as a feature (Feature Engineering), that will be for another notebook though. \n\nSeems like exact same images have same pHash."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def hamming(s1, s2):\n    return float(sum(c1 != c2 for c1, c2 in zip(s1, s2))) / float(len(s1))\nhamming('bf38f0e08397c712','bf38f0e083d7c710')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's find out how many same exact images are there"},{"metadata":{"trusted":true},"cell_type":"code","source":"copies = {}\nfor i in data['image_phash']:\n    copies[i] = data[data['image_phash'] == i]\nphash_list = list(copies.keys())\n\ncopies[phash_list[14]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"copies_len = {}\nfor i in copies.keys():\n    copies_len[i] = len(copies[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"copies_len = pd.DataFrame({'phash':copies_len.keys(),'count':copies_len.values()})\n# copies_len.reset_index(inplace=True)\ncopies_len.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"copies_len.sort_values(by='count',ascending = False, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Top 10 duplicate images' phashes"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(70,50))\nsns.barplot(x = copies_len.iloc[:10]['phash'],y=copies_len.iloc[:10]['count'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's view some exact copies"},{"metadata":{"trusted":true},"cell_type":"code","source":"def visualize_sim_phashes(phash):\n    sns.set_style(\"whitegrid\")\n    plt.rcParams['font.size'] = '28'\n    plt.figure(figsize=(50,50))\n    length = len(copies[phash])\n    if length > 10:\n        length = 10\n    for i in range(length):\n        img = plt.imread(train_path + '/' + copies[phash]['image'].iloc[i])\n        plt.subplot(length,2,i+1)\n        plt.imshow(img)\n        plt.title(\"\\n\".join(wrap(copies[phash]['title'].iloc[i],60)))\n        plt.axis('off')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visualize_sim_phashes(phash_list[14])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# NLP Based EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"title_text = data['title'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean(title):\n    stop = stopwords.words('english')\n    title = [x for x in title.split() if not x in stop]\n    title = \" \".join(title)\n    title = title.lower()\n    title = re.sub(r\"\\-\",\"\",title)\n    title = re.sub(r\"\\+\",\"\",title)\n    title = re.sub (r\"&\",\"and\",title)\n    title = re.sub(r\"\\|\",\"\",title)\n    title = re.sub(r\"\\\\\",\"\",title)\n    title = re.sub(r\"\\W\",\" \",title)\n    for p in string.punctuation :\n        title = re.sub(r\"f{p}\",\"\",title)\n    \n    title = re.sub(r\"\\s+\",\" \",title)\n    \n    return title","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## WordClouds"},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords_wc = set(STOPWORDS) \ntoken_text = ''\n\nfor i in tqdm(title_text):\n    token_l = i.split()\n    token_text += \" \".join(token_l) + \" \" ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### WordCloud for Title Text"},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = stopwords_wc, \n                min_font_size = 10).generate(token_text)\n\nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's Extract everything we can from the images, maybe used as a feature in the future**"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Use this for OCR extraction\n# ocr_text = []\n# for i in tqdm(range(data.shape[0])):\n#     img = cv2.imread(train_path + '/' + data['image'].iloc[i])\n#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n#     text = pytesseract.image_to_string(img)\n#     text = \" \".join(text.split())\n#     if len(text) != 0:\n#         ocr_text.append(text)\n#     else:\n#         ocr_text.append('Nothing Found')\n\n# data['ocr_text'] = ocr_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# data.to_csv('cleaned_and _raw_ocr.csv')\ndata = pd.read_csv('../input/cleaned-shopee-data-with-ocr/cleaned_title_and_ocr.csv')\n# Cleaning titles and ocr text again for stopwords, which I missed before\ndata.drop(['cleaned_title','cleaned_ocr_text'],axis=1,inplace=True)\ndata['cleaned_title'] = data['title'].map(clean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data['cleaned_ocr_text'] = data['ocr_text'].map(clean)\ndata.drop(['Unnamed: 0','Unnamed: 0.1'],axis=1,inplace=True)\ndata.to_csv('cleaned_title_and_ocr_sw.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### WordCloud For the OCR data"},{"metadata":{"trusted":true},"cell_type":"code","source":"title_text = data['cleaned_ocr_text'].values\nstopwords_wc = list(STOPWORDS)\ntoken_text = ''\n\nfor i in tqdm(title_text):\n    if i.strip() != 'Nothing Found'.lower():\n        token_l = i.split()\n        token_text += \" \".join(token_l) + \" \"\n\nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = stopwords_wc, \n                min_font_size = 10,contour_color='steelblue').generate(token_text)\n\nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud,interpolation='bilinear') \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show()     ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's have a look at title and OCR text as well\n\n### Distribution of Title text Lengths"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 6))\nsns.set_style(\"whitegrid\")\nsns.kdeplot(data['cleaned_title'].apply(lambda x: len(x)),fill = True,edgecolor='black',alpha=0.9)\nplt.xlabel('Title Text Length')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Distribution of Title text Tokens Count"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 6))\nsns.set_style(\"whitegrid\")\nsns.kdeplot(data['cleaned_title'].apply(lambda x: len(x.split())),fill = True,edgecolor='black',alpha=0.9,color='cyan')\nplt.xlabel('Title Text Tokens Count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Distribution of OCR text lengths"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 6))\nsns.set_style(\"whitegrid\")\nsns.kdeplot(data['cleaned_ocr_text'].apply(lambda x: len(x)),fill = True,color = 'red',edgecolor='black',alpha=0.9)\nplt.xlabel('OCR Text Length')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Distribution of OCR text tokens count"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 6))\nsns.set_style(\"whitegrid\")\nsns.kdeplot(data['cleaned_ocr_text'].apply(lambda x: len(x.split())),fill = True,color = 'maroon',edgecolor='black',alpha=0.9)\nplt.xlabel('OCR Text Tokens Count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Image shapes EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"image_shapes_h = []\nimage_shapes_w = []\nimage_shapes_c = []\nfor i in tqdm(range(data.shape[0])):\n    img = cv2.imread(train_path + '/' + data['image'].iloc[i])\n    h, w, c = img.shape\n    image_shapes_h.append(h)\n    image_shapes_w.append(w)\n    image_shapes_c.append(c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dump(image_shapes_h,'heights.pkl')\ndump(image_shapes_w,'widths.pkl')\ndump(image_shapes_c,'channels.pkl')\n\nimage_shapes_h = load('heights.pkl')\nimage_shapes_w = load('widths.pkl')\nimage_shapes_c = load('channels.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set(image_shapes_c)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**There aren't any B&W/Gray images**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style(\"white\")\nsns.axes_style('whitegrid')\nh = sns.JointGrid(x =  image_shapes_h,y = image_shapes_w,height=8)\nh.plot_joint(sns.scatterplot)\nh.plot_marginals(sns.histplot, kde=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Insights to the Data (Compilation)\n\n+ There are a total of 34250 products in the database which are unique, that means there are no duplicate rows.\n+ There are 11014 label_groups provided in the dataset.\n+ Max of all the label groups : 51\n+ Min of all the label groups : 2\n+ Perceptual hash of all the images in a label group are not the same, but there are some identical images with different titles.\n+ Phash hamming distance might be a good feature in the future to use.\n+ We can refer the respective wordclouds to get an idea of frequenty used words in the title and ocr text.\n+ The OCR text might be a good feature for the model.\n+ Title text length  seem to be less than 90 characters for most of the data.\n+ Title text tokens count seems to be less than 20 words for most of the data.\n+ Similarly OCR text length also seems to be less than 90-100 characters.\n+ The count of OCR text tokens seem to be less than 18-20 tokens (most of these tokens are garbled noise from bad OCR)\n+ All images are 3-channeled RGB images, there are no B&W or gray images in the dataset. The jointplot can be refered to get and idea of images heights and widths distribution."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}