{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"font-size:60px\"><center>Shopee Product Matching</center></h1>\n\n![shopee logo](https://i.imgur.com/GvmrZK0.png)"},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering Notebook\n\n\n    \nHere I try to create as many features as possible so that in the modelling phase I can get good results. I am using the cleaned title and OCR data that I did in my previous [Notebook](https://www.kaggle.com/mohneesh7/shopee-challenge-eda-nlp-on-title-ocr). If you haven't followed it, please check it out.\n\nI haven't lemmatized the title text there, I will try to use lemmatized/stemmed words as another feature. Let's see if it gives good results.\n    \n"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# import sys\n# !cp ../input/rapids/rapids.0.18.0 /opt/conda/envs/rapids.tar.gz\n# !cd /opt/conda/envs/ && tar -xzvf rapids.tar.gz > /dev/null\n# sys.path = [\"/opt/conda/envs/rapids/lib/python3.7/site-packages\"] + sys.path\n# sys.path = [\"/opt/conda/envs/rapids/lib/python3.7\"] + sys.path\n# sys.path = [\"/opt/conda/envs/rapids/lib\"] + sys.path \n# !cp /opt/conda/envs/rapids/lib/libxgboost.so /opt/conda/lib/","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Import Required Packages"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nimport cv2\nfrom joblib import dump, load\nfrom tqdm.notebook import tqdm\nimport re\nimport nltk\nfrom nltk.stem.porter import *\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom tqdm.notebook import tqdm\n# from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nimport cudf, cuml, cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom cuml.neighbors import NearestNeighbors","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"### Add required paths so as to avoid confusion later on"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"path = '../input/shopee-product-matching'\ntrain_path = '../input/shopee-product-matching/train_images'\ntest_path = '../input/shopee-product-matching/test_images'\ncleaned_data_path = '../input/cleaned-shopee-data-with-ocr/cleaned_title_and_ocr_sw.csv'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's load in the data and have a quick look"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(cleaned_data_path)\ndata.drop('Unnamed: 0',axis=1,inplace=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Approaches :\n\n+ **Euclidean Distannces between title/ocr text using word2vec or tfidf vectors.**\n+ **Image Similarity using the phash value, setting a threshold for the hamming distance might be a good idea(subject to choosing the best hyperparamter). we will do this in modelling phase**\n+ **Use all features to find nearest neighbours and group them together (more resource and time intensive,selecting k neighbours will be key)**\n+ **Maybe we can turn this into a clasification problem with number of classes equal to the number of unique label_groups, I am a bit confused on this though.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"s_stemmer = SnowballStemmer(language='english')\nwords = data['cleaned_title'].iloc[4587].split()\nlemmatizer = WordNetLemmatizer()\nfor word in words:\n    print(word+' --> '+s_stemmer.stem(word))\n    print(word+' --> '+lemmatizer.lemmatize(word))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**I have decided not to do lemmatization/Stemming as most of the words are not native english words, there are other language words in english that might not get stemmed properly, better to leave it for now, I will come back here if I need to fine tune model and if this helps.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train = pd.read_csv(cleaned_data_path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Text Features\n\n+ **Length & Word count of titles and OCR text**"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"tqdm.pandas()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train['len_title'] = data_train['cleaned_title'].progress_apply(lambda x: len(x))\ndata_train['word_count_title'] = data_train['cleaned_title'].progress_apply(lambda x: len(x.split()))\ndata_train['len_ocr'] = data_train['cleaned_ocr_text'].progress_apply(lambda x: len(str(x)))\ndata_train['word_count_ocr'] = data_train['cleaned_ocr_text'].progress_apply(lambda x: len(str(x).split()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"+ **Average Word lengths in title and OCR text**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train['avg_word_length_title'] = data_train['len_title']/data_train['word_count_title']\ndata_train['avg_word_length_ocr'] = data_train['len_ocr']/data_train['word_count_ocr']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def n_gram_count(text,n):\n    word_vectorizer = CountVectorizer(ngram_range=(n,n), analyzer='word', stop_words=None,max_df=0.8)\n    if len(text.split()) == 1:\n        return 1\n    if len(text.split()) == 0:\n        return 0\n    print(text)\n    sparse_matrix = word_vectorizer.fit_transform([text])\n    frequencies = len(sum(sparse_matrix).toarray()[0])\n    return frequencies","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train = cudf.DataFrame.from_pandas(data_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\n_ = gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_vec = TfidfVectorizer(stop_words='english', \n                            binary=True, \n#                             max_df = 0.5,\n#                             min_df = 2\n                           )\ntitle_embeddings = tfidf_vec.fit_transform(data_train['cleaned_title']).toarray().astype(np.float32)\ntitle_embeddings.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_vec_2 = TfidfVectorizer(stop_words='english', \n                            binary=True, \n#                             max_df = 0.5,\n#                             min_df = 2\n                           )\nocr_embeddings = tfidf_vec_2.fit_transform(data_train['cleaned_ocr_text']).toarray().astype(np.float32)\nocr_embeddings.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def cosine(v1, v2):\n#     v1 = np.array(v1)\n#     v2 = np.array(v2)\n\n#     return np.dot(v1, v2) / (np.sqrt(np.sum(v1**2)) * np.sqrt(np.sum(v2**2)))\n# sims = []\n# for i in tqdm(range(len(title_embeddings))):\n#     sims_temp = []\n#     for j in range(i,len(title_embeddings)):\n#         sim = cosine(title_embeddings[i],title_embeddings[j])\n#         if sim >= 0.5:\n#             sims_temp.append(data_train['posting_id'].iloc[j])\n#     sims.append(sims_temp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Not using the above code, It works perfectly but is very slow as it runs on CPU, using RAPIDS is the only alternative for now it seems. the codde from the cell below has been taken from chris Deotte's notebook** [ Check it Out Here](https://www.kaggle.com/cdeotte/part-2-rapids-tfidfvectorizer-cv-0-700)"},{"metadata":{},"cell_type":"markdown","source":"Title Embeddings Cosine Similarity"},{"metadata":{"trusted":true},"cell_type":"code","source":"title_embeddings[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = []\nCHUNK = 1024\n\nprint('Finding similar titles...')\nCTS = len(title_embeddings)//CHUNK\nif len(title_embeddings)%CHUNK!=0: CTS += 1\nfor j in range( CTS ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(title_embeddings))\n    print('chunk',a,'to',b)\n    \n    # COSINE SIMILARITY DISTANCE\n    cts = cupy.matmul(title_embeddings, title_embeddings[a:b].T).T\n    \n    for k in range(b-a):\n        IDX = cupy.where(cts[k,]>0.7)[0]\n        o = data.iloc[cupy.asnumpy(IDX)]['posting_id'].values\n        preds.append(o)\n        \n# del tfidf_vec, text_embeddings\n# _ = gc.collect()\ndata_train['title_cos_sim>0.7'] = preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OCR Text Cosine similarity"},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_ocr = []\nCHUNK = 1024\n\nprint('Finding similar titles...')\nCTS = len(ocr_embeddings)//CHUNK\nif len(ocr_embeddings)%CHUNK!=0: CTS += 1\nfor j in range( CTS ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(ocr_embeddings))\n    print('chunk',a,'to',b)\n    \n    # COSINE SIMILARITY DISTANCE\n    cts = cupy.matmul(ocr_embeddings, ocr_embeddings[a:b].T).T\n    \n    for k in range(b-a):\n        IDX = cupy.where(cts[k,]>0.7)[0]\n        o = data.iloc[cupy.asnumpy(IDX)].posting_id.values\n        preds_ocr.append(o)\n        \n# del tfidf_vec, text_embeddings\n# _ = gc.collect()\ndata_train['ocr_cos_sim>0.7'] = preds_ocr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**To be on the safe side lets save all features till now (also I dont want to waste GPU time)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train.head()\ndata_train = data_train.to_pandas()\ndata_train.to_csv('features_till_cos_sim.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train = pd.read_csv('features_till_cos_sim.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Work in Progress\n\nI am working on implementing more features."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}