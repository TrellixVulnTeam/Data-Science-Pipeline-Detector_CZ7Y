{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Shopee Product Matching**\n![](https://aseanrecords.world/wp-content/uploads/product/userfiles/upload/images/cach-seo-tren-shopee.png)","metadata":{}},{"cell_type":"markdown","source":"# Table Of Contents\n\n* Description\n* Import Libraries\n* Load The Data\n* Exploratory Data Analysis\n* Prediction By Text \n* KNN Model\n* Prediction By Image \n","metadata":{}},{"cell_type":"markdown","source":"# Description (from Kaggle)\n\n\nShopee is the leading e-commerce platform in Southeast Asia and Taiwan. Customers appreciate its easy, secure, and fast online shopping experience tailored to their region. The company also provides strong payment and logistical support along with a 'Lowest Price Guaranteed' feature on thousands of Shopee's listed products.\n\nTwo different images of similar wares may represent the same product or two completely different items. Retailers want to avoid misrepresentations and other issues that could come from conflating two dissimilar products. Currently, a combination of deep learning and traditional machine learning analyzes image and text information to compare similarity. But major differences in images, titles, and product descriptions prevent these methods from being entirely effective.","metadata":{}},{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"# Load libraries\nimport os\nimport re\nimport numpy as np\nimport pandas as pd\nfrom tqdm.autonotebook import tqdm\nimport cv2\nimport random\n\nimport re\nimport nltk\nnltk.download('popular')\n\nimport torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.models as models\nimport torchvision.transforms as transforms\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport cudf, cuml, cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS\n","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-10-18T13:30:22.722246Z","iopub.execute_input":"2021-10-18T13:30:22.722621Z","iopub.status.idle":"2021-10-18T13:30:22.739333Z","shell.execute_reply.started":"2021-10-18T13:30:22.722571Z","shell.execute_reply":"2021-10-18T13:30:22.737632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function that displays images\ndef plot_img(df_plot,cols=4,rows=4):\n    path='../input/shopee-product-matching/train_images/'\n    for k in range(rows):\n        plt.figure(figsize=(20,5))\n        for j in range(cols):\n            row = cols*k + j\n            image = df_plot.iloc[row,1]\n            img = cv2.imread(path+image)\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            plt.subplot(1,cols,j+1)\n            plt.axis('off')\n            plt.imshow(img)\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-18T13:30:22.743965Z","iopub.execute_input":"2021-10-18T13:30:22.74442Z","iopub.status.idle":"2021-10-18T13:30:22.75233Z","shell.execute_reply.started":"2021-10-18T13:30:22.744382Z","shell.execute_reply":"2021-10-18T13:30:22.751122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load The Data","metadata":{}},{"cell_type":"code","source":"path = '../input/shopee-product-matching/'\ntrain_path = '../input/shopee-product-matching/train_images'\ntest_path = '../input/shopee-product-matching/test_images'\ndata = pd.read_csv(path + 'train.csv')\n\n# Creat full path feature\ndata[\"path\"] = [os.path.join(train_path,s) for s in data[\"image\"]]","metadata":{"execution":{"iopub.status.busy":"2021-10-18T13:30:22.755574Z","iopub.execute_input":"2021-10-18T13:30:22.756071Z","iopub.status.idle":"2021-10-18T13:30:22.921543Z","shell.execute_reply.started":"2021-10-18T13:30:22.756036Z","shell.execute_reply":"2021-10-18T13:30:22.920636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"# Show the first 3 rows\ndata.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-10-18T13:30:22.922965Z","iopub.execute_input":"2021-10-18T13:30:22.923368Z","iopub.status.idle":"2021-10-18T13:30:22.935447Z","shell.execute_reply.started":"2021-10-18T13:30:22.923328Z","shell.execute_reply":"2021-10-18T13:30:22.934513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot first 8 images\nplot_img(data.iloc[0:8],4,2)","metadata":{"execution":{"iopub.status.busy":"2021-10-18T13:30:22.939455Z","iopub.execute_input":"2021-10-18T13:30:22.940136Z","iopub.status.idle":"2021-10-18T13:30:24.018052Z","shell.execute_reply.started":"2021-10-18T13:30:22.940091Z","shell.execute_reply":"2021-10-18T13:30:24.01728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the first 8 titles\ndata['title'].iloc[0:8]","metadata":{"execution":{"iopub.status.busy":"2021-10-18T13:30:24.020362Z","iopub.execute_input":"2021-10-18T13:30:24.02093Z","iopub.status.idle":"2021-10-18T13:30:24.027781Z","shell.execute_reply.started":"2021-10-18T13:30:24.020889Z","shell.execute_reply":"2021-10-18T13:30:24.026858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the data shape\nprint(f'Shape: {data.shape}')\n# Print how many unique labels\nprint('Unique label_groups = {}'.format( len(data[\"label_group\"].unique()) ))","metadata":{"execution":{"iopub.status.busy":"2021-10-18T13:30:24.029081Z","iopub.execute_input":"2021-10-18T13:30:24.029655Z","iopub.status.idle":"2021-10-18T13:30:24.042124Z","shell.execute_reply.started":"2021-10-18T13:30:24.029611Z","shell.execute_reply":"2021-10-18T13:30:24.040897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the min\\max sampels for label\n\nnum_groups = {}\nlen_groups = {}\nfor i in data['label_group']:\n    num_groups[i] = data[data['label_group'] == i]\n    \nfor i in num_groups:\n    len_groups[i] = len(num_groups[i])\n\nprint(f'Maximum sum of label groups : {max(len_groups.values())}')\nprint(f'Minimum sum of label groups : {min(len_groups.values())}')","metadata":{"execution":{"iopub.status.busy":"2021-10-18T13:30:24.043511Z","iopub.execute_input":"2021-10-18T13:30:24.043942Z","iopub.status.idle":"2021-10-18T13:30:45.444049Z","shell.execute_reply.started":"2021-10-18T13:30:24.043908Z","shell.execute_reply":"2021-10-18T13:30:45.442284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction By Text ","metadata":{}},{"cell_type":"markdown","source":"## Wordcloud","metadata":{}},{"cell_type":"code","source":"stopwords = set(STOPWORDS) \nwordcloud = WordCloud(width = 800, \n                      height = 800,\n                      background_color ='white',\n                      min_font_size = 10,\n                      stopwords = stopwords,).generate(' '.join(data['title'])) \n\n# Plot the WordCloud image                        \nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n\nplt.show() \n","metadata":{"execution":{"iopub.status.busy":"2021-10-18T13:30:45.445226Z","iopub.execute_input":"2021-10-18T13:30:45.44556Z","iopub.status.idle":"2021-10-18T13:30:49.288725Z","shell.execute_reply.started":"2021-10-18T13:30:45.445532Z","shell.execute_reply":"2021-10-18T13:30:49.287683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cleaning The Title","metadata":{}},{"cell_type":"code","source":"# Define cleaning function for the title\ndef clean_text(text):\n\n    lst_stopwords = nltk.corpus.stopwords.words(\"english\")\n    \n    # Clean \n    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n            \n    # Tokenize (convert from string to list)\n    lst_text = text.split()\n    \n    # remove Stopwords\n    lst_text = [word for word in lst_text if word not in \n                lst_stopwords]\n    \n    # Stemming (remove -ing, -ly)\n    ps = nltk.stem.porter.PorterStemmer()\n    lst_text = [ps.stem(word) for word in lst_text]                \n    \n    # back to string from list\n    text = \" \".join(lst_text)\n    return text\n\ndata[\"title\"] = data[\"title\"].apply(lambda x: clean_text(x))","metadata":{"execution":{"iopub.status.busy":"2021-10-18T13:30:49.290214Z","iopub.execute_input":"2021-10-18T13:30:49.29056Z","iopub.status.idle":"2021-10-18T13:31:02.149422Z","shell.execute_reply.started":"2021-10-18T13:30:49.290525Z","shell.execute_reply":"2021-10-18T13:31:02.148679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transfrom from pandas to cudf\ndata_cu = cudf.read_csv('../input/shopee-product-matching/train.csv')\ndata_cu['title']=data['title'].values\ndata_cu['path']=data['path'].values","metadata":{"execution":{"iopub.status.busy":"2021-10-18T13:31:02.15086Z","iopub.execute_input":"2021-10-18T13:31:02.151367Z","iopub.status.idle":"2021-10-18T13:31:02.184449Z","shell.execute_reply.started":"2021-10-18T13:31:02.151329Z","shell.execute_reply":"2021-10-18T13:31:02.183791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tfidf Vectorization\n![](https://miro.medium.com/max/638/1*Uucq42G4ntPGJKzI84b3aA.png)","metadata":{}},{"cell_type":"code","source":"# Sentence embeddings by Tfidf\nmodel = TfidfVectorizer( binary=True)\ntext_embeddings = model.fit_transform(data_cu.title).toarray()","metadata":{"execution":{"iopub.status.busy":"2021-10-18T13:31:02.185768Z","iopub.execute_input":"2021-10-18T13:31:02.186084Z","iopub.status.idle":"2021-10-18T13:31:02.468247Z","shell.execute_reply.started":"2021-10-18T13:31:02.18605Z","shell.execute_reply":"2021-10-18T13:31:02.46752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## KNN Model\n","metadata":{}},{"cell_type":"code","source":"# Fot the titls to KNN model with 51 neighbors\nmodel = NearestNeighbors(n_neighbors=51)\nmodel.fit(text_embeddings)\ndistances, indices = model.kneighbors(text_embeddings)","metadata":{"execution":{"iopub.status.busy":"2021-10-18T13:31:02.469871Z","iopub.execute_input":"2021-10-18T13:31:02.470203Z","iopub.status.idle":"2021-10-18T13:31:10.504265Z","shell.execute_reply.started":"2021-10-18T13:31:02.470166Z","shell.execute_reply":"2021-10-18T13:31:10.503383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For our test group, 3 samples were randomly selected (samples with more than 10 samples in their group for presentation purposes)","metadata":{}},{"cell_type":"code","source":"DISTANCE = 1.15\n\n# Function for plotting the nearest neighbors\ndef plot_knn_test(index_test):\n    plt.figure(figsize=(10,3))\n    plt.plot(np.arange(51),cupy.asnumpy(distances[index_test,]),'o-')\n    plt.ylabel('Distance') \n    plt.xlabel('Index')\n    plt.show()\n    \n    counter=0\n    for dis in enumerate(cupy.asnumpy(distances[index_test,])):\n        if ( dis[1] < DISTANCE ):\n            counter +=1\n            \n    predictions= data_cu.loc[cupy.asnumpy(indices[index_test,: counter])]\n    predictions=predictions.to_pandas()\n    print(predictions[['title','label_group']])\n    return predictions\n\n\nplot_img(plot_knn_test(177), 4,4)    \nplot_img(plot_knn_test(183), 3,1)    \nplot_img(plot_knn_test(187), 4,4)   ","metadata":{"execution":{"iopub.status.busy":"2021-10-18T13:31:10.505631Z","iopub.execute_input":"2021-10-18T13:31:10.505965Z","iopub.status.idle":"2021-10-18T13:31:16.510444Z","shell.execute_reply.started":"2021-10-18T13:31:10.50593Z","shell.execute_reply":"2021-10-18T13:31:16.506465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction By Image ","metadata":{}},{"cell_type":"markdown","source":"## Resnet18 model for feature extraction","metadata":{}},{"cell_type":"markdown","source":"Inspired by : \nhttps://becominghuman.ai/extract-a-feature-vector-for-any-image-with-pytorch-9717561d1d4c","metadata":{}},{"cell_type":"markdown","source":"![](https://miro.medium.com/max/3960/1*1BEDb6N5T4ZRZVb31IpKsw.png)","metadata":{}},{"cell_type":"code","source":"# Load the pretrained model\nmodel = models.resnet18(pretrained=True)\n# Use the model object to select the desired layer\nlayer = model._modules.get('avgpool')\n# Set model to evaluation mode\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2021-10-18T13:31:23.941047Z","iopub.execute_input":"2021-10-18T13:31:23.941402Z","iopub.status.idle":"2021-10-18T13:31:27.851937Z","shell.execute_reply.started":"2021-10-18T13:31:23.941367Z","shell.execute_reply":"2021-10-18T13:31:27.851046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining transformation for resize, crop, normalization\ntransforms = torchvision.transforms.Compose([\n    torchvision.transforms.Resize(256),\n    torchvision.transforms.CenterCrop(224),\n    torchvision.transforms.ToTensor(),\n    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])","metadata":{"execution":{"iopub.status.busy":"2021-10-18T13:31:27.853547Z","iopub.execute_input":"2021-10-18T13:31:27.853924Z","iopub.status.idle":"2021-10-18T13:31:27.859986Z","shell.execute_reply.started":"2021-10-18T13:31:27.853888Z","shell.execute_reply":"2021-10-18T13:31:27.858379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract the feature vector function\ndef get_vector(image):\n    # Create a PyTorch tensor with the transformed image\n    t_img = transforms(image)\n    # Create a vector of zeros that will hold our feature vector\n    # The 'avgpool' layer has an output size of 512\n    my_embedding = torch.zeros(512)\n\n    # Define a function that will copy the output of a layer\n    def copy_data(m, i, o):\n        my_embedding.copy_(o.flatten())                 # <-- flatten\n\n    # Attach that function to our selected layer\n    h = layer.register_forward_hook(copy_data)\n    # Run the model on our transformed image\n    with torch.no_grad():                               \n        model(t_img.unsqueeze(0))                       \n    # Detach our copy function from the layer\n    h.remove()\n    # Return the feature vector\n    return my_embedding","metadata":{"execution":{"iopub.status.busy":"2021-10-18T13:31:27.861407Z","iopub.execute_input":"2021-10-18T13:31:27.861927Z","iopub.status.idle":"2021-10-18T13:31:27.875011Z","shell.execute_reply.started":"2021-10-18T13:31:27.86189Z","shell.execute_reply":"2021-10-18T13:31:27.874028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature extraction & vectorization for all the images\n\nimg_list_vectors =[]\nfor comp_index in tqdm( range(len(data)) ):\n    img_list_vectors.append( get_vector(Image.open(data[\"path\"][comp_index])))\n","metadata":{"execution":{"iopub.status.busy":"2021-10-18T13:31:27.87643Z","iopub.execute_input":"2021-10-18T13:31:27.877128Z","iopub.status.idle":"2021-10-18T14:14:22.42608Z","shell.execute_reply.started":"2021-10-18T13:31:27.87709Z","shell.execute_reply":"2021-10-18T14:14:22.425217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the nearest samples by cosin similarity\ndef plot_test_pic(test_pic):\n    THRESHOLD=0.8\n    predictions =[]\n    # Compare Cosine Similarity between the test and all the images\n    for comp_index in tqdm( range(len(img_list_vectors)) ):\n        cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n        cos_sim = cos(test_pic.unsqueeze(0),img_list_vectors[comp_index].unsqueeze(0))\n        if (cos_sim > THRESHOLD):\n            predictions.append(data.iloc[comp_index])\n            \n    return  pd.DataFrame(predictions) ","metadata":{"execution":{"iopub.status.busy":"2021-10-18T14:14:22.427409Z","iopub.execute_input":"2021-10-18T14:14:22.427936Z","iopub.status.idle":"2021-10-18T14:14:22.435045Z","shell.execute_reply.started":"2021-10-18T14:14:22.427897Z","shell.execute_reply":"2021-10-18T14:14:22.434181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = plot_test_pic( get_vector(Image.open(data[\"path\"][177])) )\nplot_img(predictions,4,4)","metadata":{"execution":{"iopub.status.busy":"2021-10-18T14:14:22.436293Z","iopub.execute_input":"2021-10-18T14:14:22.436909Z","iopub.status.idle":"2021-10-18T14:14:27.620352Z","shell.execute_reply.started":"2021-10-18T14:14:22.436872Z","shell.execute_reply":"2021-10-18T14:14:27.619456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = plot_test_pic( get_vector(Image.open(data[\"path\"][183])) )\nplot_img(predictions,4,1)","metadata":{"execution":{"iopub.status.busy":"2021-10-18T14:14:27.621667Z","iopub.execute_input":"2021-10-18T14:14:27.622023Z","iopub.status.idle":"2021-10-18T14:14:31.995292Z","shell.execute_reply.started":"2021-10-18T14:14:27.621985Z","shell.execute_reply":"2021-10-18T14:14:31.994448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = plot_test_pic( get_vector(Image.open(data[\"path\"][187])) )\nplot_img(predictions,5,3)","metadata":{"execution":{"iopub.status.busy":"2021-10-18T14:14:31.996851Z","iopub.execute_input":"2021-10-18T14:14:31.997189Z","iopub.status.idle":"2021-10-18T14:14:36.746347Z","shell.execute_reply.started":"2021-10-18T14:14:31.997153Z","shell.execute_reply":"2021-10-18T14:14:36.745408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Improvement that needs to be done:\n* Find the optimal distance in KNN\n* Finding the optimal threshold in cosin similarity\n* Combining the predictions received from the two models into one model\n* Finding a solution to the runtime problem when comparing cosin similarity","metadata":{}}]}