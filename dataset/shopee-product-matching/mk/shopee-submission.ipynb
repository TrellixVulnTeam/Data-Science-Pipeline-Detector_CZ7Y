{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-01T20:12:45.445294Z","iopub.execute_input":"2021-06-01T20:12:45.445643Z","iopub.status.idle":"2021-06-01T20:12:45.45343Z","shell.execute_reply.started":"2021-06-01T20:12:45.445569Z","shell.execute_reply":"2021-06-01T20:12:45.452654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport tqdm\nfrom tqdm.auto import tqdm as tqdmp\ntqdmp.pandas()\n\n# Work with phash\nimport imagehash\n\nimport cv2, os\nimport skimage.io as io\nfrom PIL import Image\n\n# ignoring warnings\nimport warnings\n\nwarnings.simplefilter(\"ignore\")\nimport torch\nfrom torchvision import transforms\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport tqdm\nfrom tqdm.auto import tqdm as tqdmp\ntqdmp.pandas()\n\nimport cv2, os\nimport skimage.io as io\nfrom PIL import Image\n\n# ignoring warnings\nimport warnings\n\nwarnings.simplefilter(\"ignore\")\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport numpy as np\nimport torchvision\nfrom torchvision import datasets, models, transforms\nimport matplotlib.pyplot as plt\nimport time\nimport os\nimport copy\nimport torch.nn.functional as F\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\nimport matplotlib.pyplot as plt\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torch.utils.data import DataLoader\n\nclass DataLoader:\n    def __init__(self, path, outputpath):\n        self.path = path\n        self.outputpath = outputpath\n\n    def load_df(self, csvpath, imagepath):\n        self.csvpath = ('/').join([self.path, csvpath])\n        self.df = pd.read_csv(self.csvpath)\n        return self.df\n\n    def generate_image_path(self):\n        self.train_df = self.load_df('train.csv', 'train_images')\n        self.test_df = self.load_df('test.csv', 'test_images')\n\n        print(f\"train: {self.train_df.shape}  test: {self.test_df.shape}\")\n        print(f\"unique labels: {self.train_df.label_group.nunique()}\")\n\n        self.preprocess_train(self.train_df)\n        self.preprocess_test(self.test_df)\n\n    def preprocess_train(self, df):\n        df['path'] = self.path + '/train_images/' + df['image']\n        df.to_csv(self.outputpath + '/train_proc.csv')\n\n    def preprocess_test(self, df):\n        df['path'] = self.path + '/test_images/' + df['image']\n        df.to_csv(self.outputpath + '/test_proc.csv')\n\n\nclass ImageHandler:\n    def image_shape(self, image_path):\n        im = cv2.imread(image_path)\n        return str(im.shape)\n\n    def standardize_image(self, width, height, orig_path, new_path):\n        # Generate shape of image, resize them to 200 x 200, as that is the min found in this set\n        # Then generate the array which represents each image\n        # Divide them all by 255 to scale them\n        # Persist these images in disk before next steps\n\n        torch.manual_seed(17)\n        self.image_viz(orig_path)\n        image = cv2.imread(orig_path)\n        dim = (width, height)\n        # resize image\n        resized = cv2.resize(image, dim, interpolation=cv2.INTER_AREA)\n        tran = transforms.ToTensor()  # Convert the numpy array (C, H, W) Tensor format and /255 normalize to [0, 1.0]\n        img_tensor = tran(resized) # (C,H,W), channel order (B, G, R)\n        torch.save(img_tensor, new_path)\n\n    def image_viz(self, image_path):\n        img = cv2.imread(image_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        plt.imshow(img)\n        plt.axis('off')\n\n    def plot_image_pairs(self, df, index1, index2):\n        for idx, path in enumerate([df.loc[index1, 'path'], df.loc[index2, 'path']]):\n            plt.subplot(1, 2, idx + 1)\n            self.image_viz(path)\n        plt.show()\n\n    def plot_image(self, df, index):\n        for idx, path in enumerate([df.loc[index, 'path']]):\n            plt.subplot(1, 2, idx + 1)\n            self.image_viz(path)\n        plt.show()\n\n\n\nclass MatchFinder:\n    def match_matrix(self, phash_array):\n        \"\"\" A function that checks for matches by phash value,\n        takes phash values as input, outputs diff matrix as a df \"\"\"\n        phashs = phash_array.apply(lambda x: imagehash.hex_to_hash(x))\n        phash_matrix = pd.DataFrame()\n        for idx, i in enumerate(phash_array):\n            phash_matrix = pd.concat([phash_matrix, phashs - imagehash.hex_to_hash(i)], axis=1)\n        phash_matrix.columns = range(len(phash_array))\n        return phash_matrix\n\n\nclass GenerateDataset:\n\n    def generate_matching_pairs(self, path, outputpath, matches):\n        matchingsets = []\n        for i in range(len(matches)):\n            matchingsets.append(matches.iloc[i, :][matches.iloc[i, :] == 0].index.values)\n        matchingsets = pd.Series(matchingsets)\n        pairs = matchingsets[matchingsets.apply(lambda x: len(x) == 2)]\n        triplets = matchingsets[matchingsets.apply(lambda x: len(x) == 3)]\n        quartets = matchingsets[matchingsets.apply(lambda x: len(x) == 4)]\n\n        matching_pairs = []\n        for idx, val in pairs.items():\n            matching_pairs.append(val)\n\n        for idx, value in triplets.items():\n            matching_pairs.append([value[0], value[1]])\n            matching_pairs.append([value[1], value[2]])\n            matching_pairs.append([value[0], value[2]])\n\n        for idx, value in quartets.items():\n            matching_pairs.append([value[0], value[1]])\n            matching_pairs.append([value[1], value[2]])\n            matching_pairs.append([value[0], value[2]])\n            matching_pairs.append([value[0], value[3]])\n            matching_pairs.append([value[1], value[3]])\n            matching_pairs.append([value[2], value[3]])\n\n        final = list(set([tuple(t) for t in matching_pairs]))\n\n        matching = pd.DataFrame().from_records(final, columns=['one', 'two'])\n        matching = matching.sort_values(by=['one'])\n        matching.to_csv(outputpath + '/matching_pairs.csv')\n\n        non_matching_pairs = []\n        counter = 0\n        for j in range(0, len(matching_pairs)):\n            if j not in matching['one']:\n                #doesn't work, manually edit if there is a subsequent image pair match\n                #non_matching_pairs.append([j, j+ 1])\n                counter += 1\n            if counter == matching.shape[0]:\n                break\n        non_matching = pd.DataFrame().from_records(non_matching_pairs, columns=['one', 'two'])\n        non_matching = non_matching.sort_values(by=['one'])\n        non_matching.to_csv(outputpath + '/non_matching_pairs.csv')\n        return\n\n    def merged_dataset(self, index_df, train_df):\n        # Merge data from training set with image pairs [posting_id, image, image_phash, title, label_group]\n        # from train_df\n        train_df['index'] = train_df.index\n\n        dataset_one = train_df.loc[index_df['one']]\n        dataset_one['idx'] = index_df.index\n        dataset_two = train_df.loc[index_df['two']]\n        dataset_two['idx'] = index_df.index\n\n        dataset = pd.merge(dataset_one, dataset_two, on='idx', suffixes=['_1', '_2'])\n        dataset = pd.merge(dataset, index_df, left_index=True, right_index=True)\n        return dataset","metadata":{"execution":{"iopub.status.busy":"2021-06-01T20:12:46.727189Z","iopub.execute_input":"2021-06-01T20:12:46.727492Z","iopub.status.idle":"2021-06-01T20:12:49.227152Z","shell.execute_reply.started":"2021-06-01T20:12:46.727465Z","shell.execute_reply":"2021-06-01T20:12:49.226353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def initial_processing(path, outputpath):\n    dataLoader = DataLoader(path, outputpath)\n    imageHandler = ImageHandler()\n\n    dataLoader.generate_image_path()\n\n    train_df = pd.read_csv(outputpath + '/train_proc.csv', 'train_images')\n    test_df = pd.read_csv(outputpath + '/test_proc.csv', 'test_images')\n#     print(train_df.head)\n#     imageHandler.plot_image_pairs(train_df, 11, 12)\n#     imageHandler.plot_image(train_df, 11)\n\n    # Since the process of composing a matrix is quite resource-intensive,\n    # for clarity, we will take only the first thousand images.\n    # TODO:\n    # A good strategy to expand on this matching dataset\n    # is to look for same labelgroup and generate matching pairs within each labelgroup\n\n    train_1000 = train_df.iloc[:1000, :]\n#     train_1000 = train_1000.drop(columns=['Unnamed: 0'])\n    matchFinder = MatchFinder()\n    matches = matchFinder.match_matrix(train_1000['image_phash'])\n\n    generator = GenerateDataset()\n    generator.generate_matching_pairs(path, matches)\n\n    # We have a dataset generated with first 1000 entries from the train dataset\n    df1 = pd.read_csv(outputpath + '/matching_pairs.csv')\n    df1['label'] = True\n    df2 = pd.read_csv(outputpath + '/non_matching_df.csv')\n    df1 = df1.drop(columns=['Unnamed: 0'])\n    df2 = df2.drop(columns=['Unnamed: 0'])\n#     df2['label'] = False\n    merged = pd.concat([df1, df2])\n    merged = merged.reset_index(drop=True)\n    merged.to_csv(outputpath + '/merged.csv')\n\n    # start with 'merged.csv' in path\n    index_df = pd.read_csv(outputpath + '/merged.csv')\n    train_df = pd.read_csv(outputpath + '/train_proc.csv')\n    train_df = train_df.drop(columns=['Unnamed: 0'])\n    index_df = index_df.drop(columns=['Unnamed: 0'])\n    train_1000 = train_df.iloc[:1000, :]\n    generator = GenerateDataset()\n    merged_df = generator.merged_dataset(index_df, train_1000)\n    pd.set_option('display.max_columns', None)\n    merged_df = merged_df.reset_index(drop=True)\n    merged_df.to_csv(outputpath + '/merged_with_data.csv')","metadata":{"execution":{"iopub.status.busy":"2021-05-30T22:57:48.564264Z","iopub.execute_input":"2021-05-30T22:57:48.564587Z","iopub.status.idle":"2021-05-30T22:57:48.574497Z","shell.execute_reply.started":"2021-05-30T22:57:48.564555Z","shell.execute_reply":"2021-05-30T22:57:48.573678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '/kaggle/input/shopee-product-matching'\n# train_df = initial_processing(path, '/kaggle/working')","metadata":{"execution":{"iopub.status.busy":"2021-06-01T19:29:11.992511Z","iopub.execute_input":"2021-06-01T19:29:11.992889Z","iopub.status.idle":"2021-06-01T19:29:11.996368Z","shell.execute_reply.started":"2021-06-01T19:29:11.992857Z","shell.execute_reply":"2021-06-01T19:29:11.995345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '/kaggle/input/shopee-product-matching'\noutput = '/kaggle/working'\ntrain_df = pd.read_csv(path + '/train.csv')\nsimilar_products = train_df.groupby(['label_group'])['image'].apply(list)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T20:19:04.553708Z","iopub.execute_input":"2021-06-01T20:19:04.55405Z","iopub.status.idle":"2021-06-01T20:19:04.939611Z","shell.execute_reply.started":"2021-06-01T20:19:04.554019Z","shell.execute_reply":"2021-06-01T20:19:04.938765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build a data set with similar product images\n# make equal pairs of similar and dissimilar image pairs, labeled true and false for whether they are for similar products or not\ndef generate_matching_pairs(similar_products):\n    pairs = []\n    triplets = []\n    quartets = []\n    \n    for x in similar_products:\n        if(len(x) == 2):\n            pairs.append(x)\n        elif len(x) == 3:\n            triplets.append(x)\n        else:\n            quartets.append(x)\n\n    matching_pairs = []\n    for val in pairs:\n        matching_pairs.append(val)\n\n    for value in triplets:\n        matching_pairs.append([value[0], value[1]])\n        matching_pairs.append([value[1], value[2]])\n        matching_pairs.append([value[0], value[2]])\n\n    for value in quartets:\n        matching_pairs.append([value[0], value[1]])\n        matching_pairs.append([value[1], value[2]])\n        matching_pairs.append([value[0], value[2]])\n        matching_pairs.append([value[0], value[3]])\n        matching_pairs.append([value[1], value[3]])\n        matching_pairs.append([value[2], value[3]])\n\n    final = list(set([tuple(t) for t in matching_pairs]))\n    matching = pd.DataFrame().from_records(final, columns=['image1', 'image2'])\n    matching['label'] = 1\n    return matching","metadata":{"execution":{"iopub.status.busy":"2021-06-01T20:19:05.939064Z","iopub.execute_input":"2021-06-01T20:19:05.939491Z","iopub.status.idle":"2021-06-01T20:19:05.964803Z","shell.execute_reply.started":"2021-06-01T20:19:05.93945Z","shell.execute_reply":"2021-06-01T20:19:05.96301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = generate_matching_pairs(similar_products.tolist())\n\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-01T20:19:08.263466Z","iopub.execute_input":"2021-06-01T20:19:08.263799Z","iopub.status.idle":"2021-06-01T20:19:08.299032Z","shell.execute_reply.started":"2021-06-01T20:19:08.263763Z","shell.execute_reply":"2021-06-01T20:19:08.298076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv(output + '/matching_pairs_all.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-01T20:19:11.076515Z","iopub.execute_input":"2021-06-01T20:19:11.076834Z","iopub.status.idle":"2021-06-01T20:19:11.409633Z","shell.execute_reply.started":"2021-06-01T20:19:11.076803Z","shell.execute_reply":"2021-06-01T20:19:11.408837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_non_matching_pairs(similar_products, size):\n    non_matching_pairs = []\n    for i in range(1, len(similar_products)):\n        list1 = similar_products[i-1:i][0]\n        list2 = similar_products[i:i+1][0]\n        list3 = zip(list1, list2)\n        for item in list3:\n            non_matching_pairs.append(item)\n        if len(non_matching_pairs) > size: \n            break\n        \n    final = list(set([tuple(t) for t in non_matching_pairs]))\n    non_matching = pd.DataFrame().from_records(final, columns=['image1', 'image2'])\n    non_matching['label'] = 0\n    return non_matching","metadata":{"execution":{"iopub.status.busy":"2021-06-01T20:19:13.360122Z","iopub.execute_input":"2021-06-01T20:19:13.360463Z","iopub.status.idle":"2021-06-01T20:19:13.36648Z","shell.execute_reply.started":"2021-06-01T20:19:13.360434Z","shell.execute_reply":"2021-06-01T20:19:13.365535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"non_df = generate_non_matching_pairs(similar_products.tolist(), 24000)\nnon_df.to_csv(output + '/non_matching_pairs_all.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-01T20:19:15.398329Z","iopub.execute_input":"2021-06-01T20:19:15.39865Z","iopub.status.idle":"2021-06-01T20:19:15.53677Z","shell.execute_reply.started":"2021-06-01T20:19:15.398619Z","shell.execute_reply":"2021-06-01T20:19:15.535971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"non_df.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-01T20:19:17.954122Z","iopub.execute_input":"2021-06-01T20:19:17.954511Z","iopub.status.idle":"2021-06-01T20:19:17.959967Z","shell.execute_reply.started":"2021-06-01T20:19:17.954471Z","shell.execute_reply":"2021-06-01T20:19:17.959086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = pd.concat([df, non_df])\ndataset.to_csv(output + '/dataset.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-01T20:19:19.471639Z","iopub.execute_input":"2021-06-01T20:19:19.471995Z","iopub.status.idle":"2021-06-01T20:19:19.684034Z","shell.execute_reply.started":"2021-06-01T20:19:19.471964Z","shell.execute_reply":"2021-06-01T20:19:19.683205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ShopeeImagePairDataset(Dataset):\n\n    def __init__(self, size, csv_file):\n        df = pd.read_csv(csv_file)\n        \n        #24k samples for each class. Randomize inputs     \n        similar = df.head(24000).sample(n=size,replace=False)\n        dissimilar = df.tail(24000).sample(n=size,replace=False)\n        self.dataframe = pd.concat([similar, dissimilar], ignore_index=True)\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        img1_path = '../input/shopee-product-matching/train_images/' + self.dataframe['image1'][idx]\n        img2_path = '../input/shopee-product-matching/train_images/' + self.dataframe['image2'][idx]\n        \n        image1 = self.transform(img1_path)\n        image2 = self.transform(img2_path)\n        \n        label = torch.tensor(self.dataframe['label'][idx])\n        return image1, image2, label.int()\n    \n    def transform(self, image_path):\n        image = io.imread(image_path)\n        dim = (200, 200)\n        resized = cv2.resize(image, dim, interpolation=cv2.INTER_AREA)\n        tran = transforms.ToTensor()  # Convert the numpy array (C, H, W) Tensor format and /255 normalize to [0, 1.0]\n        img_tensor = tran(resized) # (C,H,W), channel order (B, G, R)\n        return img_tensor","metadata":{"execution":{"iopub.status.busy":"2021-06-01T20:19:20.947381Z","iopub.execute_input":"2021-06-01T20:19:20.947677Z","iopub.status.idle":"2021-06-01T20:19:20.956669Z","shell.execute_reply.started":"2021-06-01T20:19:20.94765Z","shell.execute_reply":"2021-06-01T20:19:20.955579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nimage_path = output + '/dataset.csv'\n","metadata":{"execution":{"iopub.status.busy":"2021-06-01T20:19:23.611316Z","iopub.execute_input":"2021-06-01T20:19:23.611622Z","iopub.status.idle":"2021-06-01T20:19:23.615998Z","shell.execute_reply.started":"2021-06-01T20:19:23.611594Z","shell.execute_reply":"2021-06-01T20:19:23.615067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_data_sets(size, batch_size):\n    size = size\n    batch_size = batch_size\n    validation_split = .2\n    shuffle_dataset = True\n    random_seed= 42\n\n    # Creating data indices for training and validation splits:\n    shopee_dataset = ShopeeImagePairDataset(size, image_path)\n    dataset_size = len(shopee_dataset)\n    indices = list(range(dataset_size))\n    split = int(np.floor(validation_split * dataset_size))\n    if shuffle_dataset :\n        np.random.seed(random_seed)\n        np.random.shuffle(indices)\n    train_indices, val_indices = indices[split:], indices[:split]\n\n    # Creating PT data samplers and loaders:\n    train_sampler = SubsetRandomSampler(train_indices)\n    valid_sampler = SubsetRandomSampler(val_indices)\n\n    train_loader = torch.utils.data.DataLoader(shopee_dataset, batch_size=batch_size, \n                                               sampler=train_sampler)\n    validation_loader = torch.utils.data.DataLoader(shopee_dataset, batch_size=batch_size,\n                                                    sampler=valid_sampler)\n#     print(len(train_loader))\n#     print(len(validation_loader))\n    return train_loader, validation_loader","metadata":{"execution":{"iopub.status.busy":"2021-06-01T20:19:24.808076Z","iopub.execute_input":"2021-06-01T20:19:24.808393Z","iopub.status.idle":"2021-06-01T20:19:24.815314Z","shell.execute_reply.started":"2021-06-01T20:19:24.808364Z","shell.execute_reply":"2021-06-01T20:19:24.814401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training and validation loss were calculated after every epoch\ndef train(model, train_loader, val_loader, num_epochs):\n    train_losses = []\n    val_losses = []\n    accuracy = []\n    cur_step = 0\n    \n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        model.train()\n        print(\"Starting epoch \" + str(epoch+1))\n        for i, data in enumerate(train_loader,0):\n            img0, img1, label = data\n            img0, img1, label = img0.to(device), img1.to(device) , label.to(device)\n            optimizer.zero_grad()\n             # Forward\n            target = model(img0,img1)  \n            target = target.squeeze(1)\n            loss = criterion(target, label.float())\n            \n            # Backward and optimize\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        avg_train_loss = running_loss / len(train_loader)\n        train_losses.append(avg_train_loss)\n        val_running_loss = 0.0\n        correct = 0.0\n#         right, error = 0, 0\n        #check validation loss after every epoch\n        with torch.no_grad():\n            model.eval()\n            for i, data in enumerate(val_loader,0):\n                img1, img2, label = data\n                img1 = img1.to(device)\n                img2 = img2.to(device)\n                label = label.to(device)       \n                target = model(img1,img2)  \n                output = target.data.cpu().numpy()\n#                 pred = np.argmax(output)\n#                 if pred == label: \n#                     right += 1\n#                     correct += 1\n#                 else: \n#                     error += 1\n                \n                target = target.squeeze(1)\n                loss = criterion(target, label.float())\n                val_running_loss += loss.item()\n#         print(right, error, right + error)\n#         accuracy.append(right*1.0/(right+error))    \n        avg_val_loss = val_running_loss / len(val_loader)\n        val_losses.append(avg_val_loss)\n        accuracy_for_epoch = correct/len(val_loader)\n#         accuracy.append(accuracy_for_epoch)\n       \n        print('Epoch [{}/{}],Train Loss: {:.4f}, Valid Loss: {:.8f}, Accuracy: {:.8f}'\n            .format(epoch+1, num_epochs, avg_train_loss, avg_val_loss, accuracy_for_epoch))\n    print(\"Finished Training\")  \n    return train_losses, val_losses, accuracy\n","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:02:23.39955Z","iopub.execute_input":"2021-05-31T07:02:23.399883Z","iopub.status.idle":"2021-05-31T07:02:23.411563Z","shell.execute_reply.started":"2021-05-31T07:02:23.399852Z","shell.execute_reply":"2021-05-31T07:02:23.410375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ShopeeImageNet(nn.Module):\n    def __init__(self):\n        super(ShopeeImageNet, self).__init__()\n        \n        # Conv2d(input_channels, output_channels, kernel_size)\n        self.conv1 = nn.Conv2d(3, 64, 3) \n        self.conv2 = nn.Conv2d(64, 128, 3)  \n        self.conv3 = nn.Conv2d(128, 128, 3)\n        self.conv4 = nn.Conv2d(128, 256, 3)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.bn2 = nn.BatchNorm2d(128)\n        self.bn3 = nn.BatchNorm2d(128)\n        self.bn4 = nn.BatchNorm2d(256)\n        self.dropout1 = nn.Dropout(0.1)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc1 = nn.Linear(256 * 21 * 21, 200)\n        self.fcOut = nn.Linear(200, 1)\n        self.sigmoid = nn.Sigmoid()\n    \n    def convs(self, x): \n        #32, 3, 200, 200\n#         print(1, x.shape)\n        x = F.relu(self.bn1(self.conv1(x)))\n        # 32, 64, 198, 198\n#         print(2, x.shape)\n        x = F.max_pool2d(x, (2,2))\n#         print(3, x.shape)\n        # 32, 64, 99, 99\n        x = F.relu(self.bn2(self.conv2(x)))\n#         print(4, x.shape)\n        # 32, 128, 97, 97\n        x = F.max_pool2d(x, (2,2))\n#         print(5, x.shape)\n        # 32, 128, 48, 48\n        x = F.relu(self.bn3(self.conv3(x)))\n#         print(6, x.shape)\n        # 32, 128, 46, 46\n        x = F.max_pool2d(x, (2,2))\n#         print(7, x.shape)\n        # 32, 128, 23, 23\n        x = F.relu(self.bn4(self.conv4(x)))\n#         print(8, x.shape)\n        # 32, 256, 21, 21\n        return x\n\n    def forward(self, x1, x2):        \n        x1 = self.convs(x1)\n#         print('forward')\n#         print(9, x1.shape)\n        x1 = x1.view(-1, 256 * 21 * 21)\n#         print(10, x1.shape)\n        x1 = self.sigmoid(self.fc1(x1))\n#         print(11, x1.shape)\n        x2 = self.convs(x2)\n#         print(12, x2.shape)\n        x2 = x2.view(-1, 256 * 21 * 21)\n#         print(13, x2.shape)\n        x2 = self.sigmoid(self.fc1(x2))\n#         print(14, x2.shape)\n        x = torch.abs(x1 - x2)\n        x = self.fcOut(x)\n#         print(15, x.shape)\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:02:26.519848Z","iopub.execute_input":"2021-05-31T07:02:26.520184Z","iopub.status.idle":"2021-05-31T07:02:26.535341Z","shell.execute_reply.started":"2021-05-31T07:02:26.520154Z","shell.execute_reply":"2021-05-31T07:02:26.534085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def imshow(img,text=None,should_save=False):\n#     fig = plt.figure()\n    plt.rcParams[\"figure.figsize\"] = (40,20)\n    npimg = img.numpy()\n    plt.axis(\"off\")\n    if text:\n        plt.text(75, 8, text, style='italic',fontweight='bold',\n            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()    \n\ndef show_plot(iteration,loss):\n    plt.plot(iteration,loss)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-01T20:19:35.951461Z","iopub.execute_input":"2021-06-01T20:19:35.951793Z","iopub.status.idle":"2021-06-01T20:19:35.959075Z","shell.execute_reply.started":"2021-06-01T20:19:35.951736Z","shell.execute_reply":"2021-06-01T20:19:35.957998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n\ndataiter = iter(validation_loader)\n\n\nexample_batch = next(dataiter)\nconcatenated = torch.cat((example_batch[0],example_batch[1]),0)\nimshow(torchvision.utils.make_grid(concatenated))\nprint(example_batch[2].numpy())\n","metadata":{"execution":{"iopub.status.busy":"2021-05-30T23:21:41.912573Z","iopub.execute_input":"2021-05-30T23:21:41.913097Z","iopub.status.idle":"2021-05-30T23:21:42.822176Z","shell.execute_reply.started":"2021-05-30T23:21:41.913054Z","shell.execute_reply":"2021-05-30T23:21:42.821347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_only(model, train_loader, num_epochs):\n    train_losses = []\n    val_losses = []\n    accuracy = []\n    cur_step = 0\n    \n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        model.train()\n        print(\"Starting epoch \" + str(epoch+1))\n        for i, data in enumerate(train_loader,0):\n            img0, img1, label = data\n            img0, img1, label = img0.to(device), img1.to(device) , label.to(device)\n            optimizer.zero_grad()\n             # Forward\n            target = model(img0,img1)  \n            target = target.squeeze(1)\n            loss = criterion(target, label.float())\n            \n            # Backward and optimize\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        avg_train_loss = running_loss / len(train_loader)\n        train_losses.append(avg_train_loss)\n        \n        print('Epoch [{}/{}],Train Loss: {:.4f}'\n            .format(epoch+1, num_epochs, avg_train_loss))\n    print(\"Finished Training\")  \n    return model, train_losses","metadata":{"execution":{"iopub.status.busy":"2021-06-01T19:27:27.373867Z","iopub.execute_input":"2021-06-01T19:27:27.374255Z","iopub.status.idle":"2021-06-01T19:27:27.384047Z","shell.execute_reply.started":"2021-06-01T19:27:27.374209Z","shell.execute_reply":"2021-06-01T19:27:27.382686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ContrastiveLoss(torch.nn.Module):\n    \"\"\"\n    Contrastive loss function.\n    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n    \"\"\"\n\n    def __init__(self, margin=2.0):\n        super(ContrastiveLoss, self).__init__()\n        self.margin = margin\n\n    def forward(self, output1, output2, label):\n        euclidean_distance = F.pairwise_distance(output1, output2)\n        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n        return loss_contrastive","metadata":{"execution":{"iopub.status.busy":"2021-06-01T19:27:30.534704Z","iopub.execute_input":"2021-06-01T19:27:30.535267Z","iopub.status.idle":"2021-06-01T19:27:30.544841Z","shell.execute_reply.started":"2021-06-01T19:27:30.535214Z","shell.execute_reply":"2021-06-01T19:27:30.543387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# Train the model\nnet = ShopeeImageNet().to(device)\nmodel, train_loss = train_only(net, train_loader, 20)\ntorch.save(model.state_dict(), 'model_20_epochs.pth')\nprint(\"Model Saved Successfully\")","metadata":{"execution":{"iopub.status.busy":"2021-05-30T23:28:10.804504Z","iopub.execute_input":"2021-05-30T23:28:10.804941Z","iopub.status.idle":"2021-05-30T23:42:46.988023Z","shell.execute_reply.started":"2021-05-30T23:28:10.804906Z","shell.execute_reply":"2021-05-30T23:42:46.987105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model, val_loader):\n    train_losses = []\n    val_losses = []\n    accuracy = []\n    cur_step = 0\n    val_running_loss = 0.0\n    correct = 0.0\n    right, error = 0, 0\n    with torch.no_grad():\n        model.eval()\n        for i, data in enumerate(val_loader,0):\n            img1, img2, label = data\n            img1 = img1.to(device)\n            img2 = img2.to(device)\n            label = label.to(device)       \n            target = model(img1,img2)              \n            target = target.squeeze(1)\n            output = target.data.cpu().numpy()\n            \n            pred = np.argmax(output)\n            if pred == 0: \n                right += 1\n                correct += 1\n            else: \n                error += 1\n            print(label, target, pred)\n            loss = criterion(target, label.float())\n            val_running_loss += loss.item()\n        print(right, error, right + error)\n        accuracy.append(right*1.0/(right+error))    \n        avg_val_loss = val_running_loss / len(val_loader)\n        val_losses.append(avg_val_loss)\n        accuracy_for_epoch = correct/len(val_loader)\n        accuracy.append(accuracy_for_epoch)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T23:47:46.323221Z","iopub.execute_input":"2021-05-30T23:47:46.323654Z","iopub.status.idle":"2021-05-30T23:47:46.339728Z","shell.execute_reply.started":"2021-05-30T23:47:46.323604Z","shell.execute_reply":"2021-05-30T23:47:46.338805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict(model, validation_loader)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T23:47:48.853183Z","iopub.execute_input":"2021-05-30T23:47:48.853529Z","iopub.status.idle":"2021-05-30T23:47:58.255638Z","shell.execute_reply.started":"2021-05-30T23:47:48.853497Z","shell.execute_reply":"2021-05-30T23:47:58.254812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#transfer learning\nfrom torchvision import *\nnet = models.resnet18(pretrained=True)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:01:02.064144Z","iopub.execute_input":"2021-05-31T07:01:02.064456Z","iopub.status.idle":"2021-05-31T07:01:04.170356Z","shell.execute_reply.started":"2021-05-31T07:01:02.064427Z","shell.execute_reply":"2021-05-31T07:01:04.169508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0.9)\n\ndef accuracy(out, labels):\n    _,pred = torch.max(out, dim=1)\n    return torch.sum(pred==labels).item()\n\nnum_ftrs = net.fc.in_features\nnet.fc = nn.Linear(num_ftrs, 128)\nnet.fc = net.fc.cuda() ","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:04:15.541031Z","iopub.execute_input":"2021-05-31T07:04:15.541442Z","iopub.status.idle":"2021-05-31T07:04:19.868516Z","shell.execute_reply.started":"2021-05-31T07:04:15.541408Z","shell.execute_reply":"2021-05-31T07:04:19.867681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader, validation_dataloader = create_data_sets(1000, 32)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:05:56.034648Z","iopub.execute_input":"2021-05-31T07:05:56.035012Z","iopub.status.idle":"2021-05-31T07:05:56.103579Z","shell.execute_reply.started":"2021-05-31T07:05:56.034982Z","shell.execute_reply":"2021-05-31T07:05:56.102793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_epochs = 5\nprint_every = 10\nvalid_loss_min = np.Inf\nval_loss = []\nval_acc = []\ntrain_loss = []\ntrain_acc = []\ntotal_step = len(train_dataloader)\nfor epoch in range(1, n_epochs+1):\n    running_loss = 0.0\n    correct = 0\n    total=0\n    print(f'Epoch {epoch}\\n')\n    for i, data in enumerate(train_dataloader,0):\n        img0, img1, label = data\n        img0, img1, label = img0.to(device), img1.to(device) , label.to(device)\n#     for batch_idx, (data_, target_) in enumerate(train_dataloader):\n#         data_, target_ = data_.to(device), target_.to(device)\n        optimizer.zero_grad()\n        \n        outputs = net(img0, img1)\n        loss = criterion(outputs, target_)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _,pred = torch.max(outputs, dim=1)\n        correct += torch.sum(pred==target_).item()\n        total += target_.size(0)\n        if (batch_idx) % 20 == 0:\n            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n                   .format(epoch, n_epochs, batch_idx, total_step, loss.item()))\n    train_acc.append(100 * correct / total)\n    train_loss.append(running_loss/total_step)\n    print(f'\\ntrain-loss: {np.mean(train_loss):.4f}, train-acc: {(100 * correct/total):.4f}')\n    batch_loss = 0\n    total_t=0\n    correct_t=0\n    with torch.no_grad():\n        net.eval()\n#         for data_t, target_t in (test_dataloader):\n#             data_t, target_t = data_t.to(device), target_t.to(device)\n        for i, data in enumerate(validation_dataloader,0):\n            img0, img1, label = data\n            img0, img1, label = img0.to(device), img1.to(device) , label.to(device)\n            outputs_t = net(img0, img1)\n            loss_t = criterion(outputs_t, target_t)\n            batch_loss += loss_t.item()\n            _,pred_t = torch.max(outputs_t, dim=1)\n            correct_t += torch.sum(pred_t==target_t).item()\n            total_t += target_t.size(0)\n        val_acc.append(100 * correct_t/total_t)\n        val_loss.append(batch_loss/len(validation_dataloader))\n        network_learned = batch_loss < valid_loss_min\n        print(f'validation loss: {np.mean(val_loss):.4f}, validation acc: {(100 * correct_t/total_t):.4f}\\n')\n\n        \n        if network_learned:\n            valid_loss_min = batch_loss\n            torch.save(net.state_dict(), 'resnet.pt')\n            print('Improvement-Detected, save-model')\n    net.train()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:08:49.700905Z","iopub.execute_input":"2021-05-31T07:08:49.701226Z","iopub.status.idle":"2021-05-31T07:08:50.935936Z","shell.execute_reply.started":"2021-05-31T07:08:49.701199Z","shell.execute_reply":"2021-05-31T07:08:50.933884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Img2VecResnet18():\n    def __init__(self):\n        self.device = torch.device(\"cpu\")\n        self.numberFeatures = 512\n        self.modelName = \"resnet-18\"\n        self.model, self.featureLayer = self.getFeatureLayer()\n        self.model = self.model.to(self.device)\n        self.model.eval()\n        self.toTensor = transforms.ToTensor()\n        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        \ndef getFeatureLayer(self):\n    cnnModel = models.resnet18(pretrained=True)\n    layer = cnnModel._modules.get('avgpool')\n    self.layer_output_size = 512\n    \n    return cnnModel, layer\n\ndef getVec(self, img):\n    image = self.normalize(self.toTensor(img)).unsqueeze(0).to(self.device)\n    embedding = torch.zeros(1, self.numberFeatures, 1, 1)\n    def copyData(m, i, o): embedding.copy_(o.data)\n    h = self.featureLayer.register_forward_hook(copyData)\n    self.model(image)\n    h.remove()\n    return embedding.numpy()[0, :, 0, 0]","metadata":{"execution":{"iopub.status.busy":"2021-06-01T00:04:41.986111Z","iopub.execute_input":"2021-06-01T00:04:41.986436Z","iopub.status.idle":"2021-06-01T00:04:41.996772Z","shell.execute_reply.started":"2021-06-01T00:04:41.986365Z","shell.execute_reply":"2021-06-01T00:04:41.995653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getSimilarityMatrix(vectors):\n    v = np.array(list(vectors.values())).T\n    sim = np.inner(v.T, v.T) / ((np.linalg.norm(v, axis=0).reshape(-1,1)) * ((np.linalg.norm(v, axis=0).reshape(-1,1)).T))\n    keys = list(vectors.keys())\n    matrix = pd.DataFrame(sim, columns = keys, index = keys)\n    return matrix\n\nimg2vec = Img2VecResnet18()\nallVectors = {}\nfor image in tqdm(os.listdir(\"inputImagesCNN\")):\n    I = Image.open(os.path.join(\"inputImagesCNN\", image))\n    vec = img2vec.getVec(I)\n    allVectors[image] = vec\n    I.close()\nsimilarityMatrix = getSimilarityMatrix(allVectors)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T00:06:18.234441Z","iopub.execute_input":"2021-06-01T00:06:18.234786Z","iopub.status.idle":"2021-06-01T00:06:18.260417Z","shell.execute_reply.started":"2021-06-01T00:06:18.234754Z","shell.execute_reply":"2021-06-01T00:06:18.258513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SiameseNetwork1(nn.Module):\n    def __init__(self):\n        super(SiameseNetwork1, self).__init__()\n        # Conv2d(input_channels, output_channels, kernel_size)\n        self.conv1 = nn.Conv2d(3, 64, 3) \n        self.conv2 = nn.Conv2d(64, 128, 3)  \n        self.conv3 = nn.Conv2d(128, 128, 3)\n        self.conv4 = nn.Conv2d(128, 256, 3)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.bn2 = nn.BatchNorm2d(128)\n        self.bn3 = nn.BatchNorm2d(128)\n        self.bn4 = nn.BatchNorm2d(256)\n        self.dropout1 = nn.Dropout(0.1)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc1 = nn.Linear(256 * 21 * 21, 200)\n        self.fcOut = nn.Linear(200, 1)\n        self.sigmoid = nn.Sigmoid()\n    \n    def convs(self, x): \n        #32, 3, 200, 200\n#         print(1, x.shape)\n        x = F.relu(self.bn1(self.conv1(x)))\n        # 32, 64, 198, 198\n#         print(2, x.shape)\n        x = F.max_pool2d(x, (2,2))\n#         print(3, x.shape)\n        # 32, 64, 99, 99\n        x = F.relu(self.bn2(self.conv2(x)))\n#         print(4, x.shape)\n        # 32, 128, 97, 97\n        x = F.max_pool2d(x, (2,2))\n#         print(5, x.shape)\n        # 32, 128, 48, 48\n        x = F.relu(self.bn3(self.conv3(x)))\n#         print(6, x.shape)\n        # 32, 128, 46, 46\n        x = F.max_pool2d(x, (2,2))\n#         print(7, x.shape)\n        # 32, 128, 23, 23\n        x = F.relu(self.bn4(self.conv4(x)))\n#         print(8, x.shape)\n        # 32, 256, 21, 21\n        return x\n\n    def forward(self, x1, x2):        \n        x1 = self.convs(x1)\n#         print('forward')\n#         print(9, x1.shape)\n        x1 = x1.view(-1, 256 * 21 * 21)\n#         print(10, x1.shape)\n        x1 = self.fc1(x1)\n#         print(11, x1.shape)\n        x2 = self.convs(x2)\n#         print(12, x2.shape)\n        x2 = x2.view(-1, 256 * 21 * 21)\n#         print(13, x2.shape)\n        x2 = self.fc1(x2)   \n#         print(14, x2.shape)\n        return x1, x2\n\n#     def forward_once(self, x):\n#         output = self.convs(x)\n#         output = output.view(output.size()[0], -1)\n#         output = self.fc1(output)\n#         return output\n\n#     def forward(self, input1, input2):\n#         output1 = self.forward_once(input1)\n#         output2 = self.forward_once(input2)\n#         return output1, output2\n\n\nclass ContrastiveLoss(torch.nn.Module):\n    \"\"\"\n    Contrastive loss function.\n    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n    \"\"\"\n\n    def __init__(self, margin=2.0):\n        super(ContrastiveLoss, self).__init__()\n        self.margin = margin\n\n    def forward(self, output1, output2, label):\n        euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)\n        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n\n\n        return loss_contrastive","metadata":{"execution":{"iopub.status.busy":"2021-06-01T20:19:56.267901Z","iopub.execute_input":"2021-06-01T20:19:56.268208Z","iopub.status.idle":"2021-06-01T20:19:56.282516Z","shell.execute_reply.started":"2021-06-01T20:19:56.268181Z","shell.execute_reply":"2021-06-01T20:19:56.281315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_only(model, train_loader, num_epochs):\n    train_losses = []\n    val_losses = []\n    accuracy = []\n    cur_step = 0\n    \n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        model.train()\n        print(\"Starting epoch \" + str(epoch+1))\n        for i, data in enumerate(train_loader,0):\n            img0, img1, label = data\n            img0, img1, label = img0.to(device), img1.to(device) , label.to(device)\n            optimizer.zero_grad()\n             # Forward\n            output1,output2 = model(img0,img1)\n            loss_contrastive = criterion(output1,output2,label)\n            loss_contrastive.backward()\n            optimizer.step()\n            running_loss += loss_contrastive.item()\n        avg_train_loss = running_loss / len(train_loader)\n        train_losses.append(avg_train_loss)\n        \n        print('Epoch [{}/{}],Train Loss: {:.4f}'\n            .format(epoch+1, num_epochs, avg_train_loss))\n    print(\"Finished Training\")  \n    return model, train_losses\n","metadata":{"execution":{"iopub.status.busy":"2021-06-01T19:30:04.144749Z","iopub.execute_input":"2021-06-01T19:30:04.145072Z","iopub.status.idle":"2021-06-01T19:30:04.152293Z","shell.execute_reply.started":"2021-06-01T19:30:04.145041Z","shell.execute_reply":"2021-06-01T19:30:04.151342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Train with 64 batch size at a time\n\ntrain_loader, validation_loader = create_data_sets(5000, batch_size = 64) #randomized samples\nmodel2 = SiameseNetwork1().cuda()\ncriterion = ContrastiveLoss()\noptimizer = optim.Adam(model2.parameters(),lr = 0.0005)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T19:30:13.154249Z","iopub.execute_input":"2021-06-01T19:30:13.154625Z","iopub.status.idle":"2021-06-01T19:30:13.425794Z","shell.execute_reply.started":"2021-06-01T19:30:13.154589Z","shell.execute_reply":"2021-06-01T19:30:13.424937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_out, train_loss = train_only(model2, train_loader, num_epochs=10)\n\nfig = plt.figure()\nplt.plot(train_loss, label ='Train Loss')\nplt.legend()\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-06-01T19:30:14.496289Z","iopub.execute_input":"2021-06-01T19:30:14.496633Z","iopub.status.idle":"2021-06-01T20:10:19.936146Z","shell.execute_reply.started":"2021-06-01T19:30:14.496594Z","shell.execute_reply":"2021-06-01T20:10:19.935255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model_out.state_dict(), 'model__7000_128__6_1_11_45.pth')\nprint(\"Model Saved Successfully\")","metadata":{"execution":{"iopub.status.busy":"2021-06-01T20:10:19.937834Z","iopub.execute_input":"2021-06-01T20:10:19.938193Z","iopub.status.idle":"2021-06-01T20:10:20.31709Z","shell.execute_reply.started":"2021-06-01T20:10:19.938154Z","shell.execute_reply":"2021-06-01T20:10:20.316146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = SiameseNetwork1().cuda()\nmodel.load_state_dict(torch.load('../input/shopee/model__7000_128__6_1_11_45.pth'))\nmodel.eval()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-01T20:17:58.946728Z","iopub.execute_input":"2021-06-01T20:17:58.94714Z","iopub.status.idle":"2021-06-01T20:18:01.084283Z","shell.execute_reply.started":"2021-06-01T20:17:58.947106Z","shell.execute_reply":"2021-06-01T20:18:01.083513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader, validation_loader = create_data_sets(5000, batch_size = 8) #randomized samples\ncount = 80\ncorrect = 0\nfor i in range(0,10):\n    dataiter = iter(validation_loader)\n\n    example_batch = next(dataiter)\n    concatenated = torch.cat((example_batch[0],example_batch[1]),0)\n    labels = example_batch[2]\n    \n    print(labels)\n    img0, img1 , label = example_batch[0].cuda(), example_batch[1].cuda() , example_batch[2].cuda()\n\n    output1,output2 = model(img0,img1)\n    euclidean_distance = F.pairwise_distance(output1, output2)\n    print(euclidean_distance)\n    target = torch.randn(1, 8, dtype=torch.double).to(device)\n    target.fill_(1)\n    output_labels = (euclidean_distance < target).long()\n    print(output_labels)\n#     correct = (labels == output_labels).long().sum()\n#     print(correct)\n    correct += torch.sum(label == output_labels).item()\n    print(f\"correct = {correct}\")\n    imshow(torchvision.utils.make_grid(concatenated))\n#     correct_t += torch.sum(pred_t==target_t).item()\n#     total_t += target_t.size(0)\n    \naccuracy = correct/count\nprint(f\"Accuracy={accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-01T20:38:43.556902Z","iopub.execute_input":"2021-06-01T20:38:43.557217Z","iopub.status.idle":"2021-06-01T20:38:54.831524Z","shell.execute_reply.started":"2021-06-01T20:38:43.557188Z","shell.execute_reply":"2021-06-01T20:38:54.830736Z"},"trusted":true},"execution_count":null,"outputs":[]}]}