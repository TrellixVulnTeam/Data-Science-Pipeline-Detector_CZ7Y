{"cells":[{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/24286/logos/header.png?t=2021-01-07-16-57-37\" style=\"width:80%;\">\n\nDo you scan online retailers in search of the best deals? You're joined by the many savvy shoppers who don't like paying extra for the same product depending on where they shop. Retail companies use a variety of methods to assure customers that their products are the cheapest. Among them is product matching, which allows a company to offer products at rates that are competitive to the same product sold by another retailer. To perform these matches automatically requires a thorough machine learning approach, which is where your data science skills could help.\n\nTwo different images of similar wares may represent the same product or two completely different items. Retailers want to avoid misrepresentations and other issues that could come from conflating two dissimilar products. Currently, a combination of deep learning and traditional machine learning analyzes image and text information to compare similarity. But major differences in images, titles, and product descriptions prevent these methods from being entirely effective.\n\nShopee is the leading e-commerce platform in Southeast Asia and Taiwan. Customers appreciate its easy, secure, and fast online shopping experience tailored to their region. The company also provides strong payment and logistical support along with a 'Lowest Price Guaranteed' feature on thousands of Shopee's listed products.\n\nIn this competition, youâ€™ll apply your machine learning skills to build a model that predicts which items are the same products.\n\nThe applications go far beyond Shopee or other retailers. Your contributions to product matching could support more accurate product categorization and uncover marketplace spam. Customers will benefit from more accurate listings of the same or similar products as they shop. Perhaps most importantly, this will aid you and your fellow shoppers in your hunt for the very best deals."},{"metadata":{},"cell_type":"markdown","source":"Hi everyone, \n\nIn this notebook we will explore the Shopee Dataset with simple visualizations. This is initial and not complete version for now. I will also share some solution ideas in the upcoming days.\n\nUpvotes are too much appreciated if you like the notebook."},{"metadata":{},"cell_type":"markdown","source":"# Let's start with the imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BASE_DATA_DIR = Path(\"../input/shopee-product-matching/\")\n!ls {BASE_DATA_DIR}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have only 3 public test image for the submission. We will use them to prepare our prediction pipeline. The notebook will run on approximately 70,000 images after the submission."},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls {BASE_DATA_DIR / \"test_images\"}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading the files"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(BASE_DATA_DIR / \"train.csv\")\ndf_test = pd.read_csv(BASE_DATA_DIR / \"test.csv\")\ndf_sub = pd.read_csv(BASE_DATA_DIR / \"sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Here we group the images based on the `label_group`. Definition from the competitions page is: \n\n> ID code for all postings that map to the same product. Not provided for the test set.\n\nlike this. So any image that belongs to the same group will be considered as the same product. We will also visualize and explore those images later in the notebook."},{"metadata":{},"cell_type":"markdown","source":"# Grouping based on label_group"},{"metadata":{"trusted":true},"cell_type":"code","source":"group_by_label_images = df_train.groupby(\"label_group\")[\"image\"].apply(list)\nlen_groups = group_by_label_images.apply(len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Grouped images\ngroup_by_label_images.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Length of the groups\nlen_groups.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check out the statistics about the grouped images (by length):"},{"metadata":{"trusted":true},"cell_type":"code","source":"len_groups.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we can see that the most of the items have at most two other item similar to itself. Let's investigate this a bit further by plotting a histogram. We plot two histogram with the same data. Right image is the same as the left one but on a log scale. Since often, most of the items have a less similar items, we plot on a log scale.  "},{"metadata":{},"cell_type":"markdown","source":"# Histogram plots"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(16, 6))\nsns.histplot(len_groups, bins=20, binwidth=3, ax=ax[0])\nsns.histplot(len_groups, bins=20, binwidth=3, ax=ax[1])\nax[0].set_title(\"Histogram of grouped images lengths\")\nax[1].set_title(\"Histogram of grouped images lengths (log scale)\")\nax[1].set_yscale('log')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets visualize some of the similar items to see how similar they are. First of all, we define several helper functions to read the images and plot them."},{"metadata":{},"cell_type":"markdown","source":"# Image Visualizations"},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_img_and_cvt_format(img_path, clr_format=cv2.COLOR_BGR2RGB):\n    return cv2.cvtColor(cv2.imread(img_path), clr_format)\n\n\ndef visualize_batch(label_group, img_ids, texts, nrows=3, ncols=3, figsize=(24, 14)):\n    \n    plt.figure(figsize=figsize)\n    plt.suptitle(f\"Label group: {label_group}\")\n    for idx, (img_id, text) in enumerate(zip(img_ids, texts)):\n        plt.subplot(nrows, ncols, idx + 1)\n        img_fn = str(BASE_DATA_DIR / \"train_images\" / img_id)\n        img = read_img_and_cvt_format(img_fn)\n        plt.imshow(img)\n        plt.title(f\"{text}\", fontsize=8, wrap=True)\n        plt.axis(\"off\")\n        \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's choose some ids for plotting.\nlen_groups.sort_values(ascending=False)[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"single_label_group = 1163569239\ndf_single = df_train[df_train.label_group == single_label_group]\ndf_single.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's take the first 16 image\nimg_ids, texts = df_single.image.values[:16], df_single.title.values[:16]\n\nvisualize_batch(single_label_group, img_ids, texts, nrows=4, ncols=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"single_label_group = 159351600\ndf_single = df_train[df_train.label_group == single_label_group]\ndf_single.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Again, let's visualize the first 16 images\nimg_ids, texts = df_single.image.values[:16], df_single.title.values[:16]\n\nvisualize_batch(single_label_group, img_ids, texts, nrows=4, ncols=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"single_label_group = 3627744656\ndf_single = df_train[df_train.label_group == single_label_group]\ndf_single.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_ids, texts = df_single.image.values[:16], df_single.title.values[:16]\n\nvisualize_batch(single_label_group, img_ids, texts, nrows=4, ncols=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before going further, let's stop here and explore some of the images together. Please, state your thoughts on the comment section as well. \n\n* So, first thing that I noticed here is the **diversity** both in the images and the texts. Some of the images seems to be taken at home, some of them seems to be a bit more professional, some of them are just catalog images. Also there are some texts on the images as well. Our models should be robust to that diversity in the images.\n\n* Second thing to notice is titles of the images. They also seem to be diverse. They mostly include the brand of the product, but there is no particular format. Careful preprocessing would bring additional improvements here. \n\nNow, let's visualize some of the images from the other tail of the line:"},{"metadata":{"trusted":true},"cell_type":"code","source":"len_groups.sort_values()[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we will visualize the two length grouped images together on the same row. "},{"metadata":{"trusted":true},"cell_type":"code","source":"all_image_ids = []\nall_titles = []\nall_label_groups = []\n\nfor index in len_groups.sort_values()[:5].index:\n    df_single = df_train[df_train.label_group == index]\n    image_ids, titles = df_single.image.values, df_single.title.values\n    all_image_ids.extend(image_ids.tolist())\n    all_titles.extend(titles.tolist())\n    all_label_groups.append(index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_label_groups","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visualize_batch(\"\\n\" + \"\\n\".join(map(str, all_label_groups)), \n                all_image_ids, all_titles, nrows=5, ncols=2, figsize=(14, 18))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Starter Ideas"},{"metadata":{},"cell_type":"markdown","source":"* Opposite to high length groups, here images are much more similar and less diverse. \n\nOne thing to notice here is the language of the titles. There is no only English titles but Indonesian as well as we might expected. So, if we want to extract features from a language model like BERT, we should also consider the language detection or multi-lingual models as well."},{"metadata":{},"cell_type":"markdown","source":"# To Be Continued..."},{"metadata":{},"cell_type":"markdown","source":"So, I will stop here for now. I plan to extend this notebook with some text exploration as well. I will also share some potential solution approaches like metric learning in the upcoming days.\n\nUpvotes would be too much appreciated if you liked this notebook. Thanks and stay safe! ðŸ¤— ðŸ¤—"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}