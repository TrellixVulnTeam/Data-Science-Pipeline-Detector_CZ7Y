{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np \nimport pandas as pd\nimport cv2,math,gc,sys\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\nfrom torch.nn import Parameter\n\n!pip install \"../input/faissgpuwheel/faiss_gpu-1.7.0-cp37-cp37m-manylinux2014_x86_64.whl\"\n#!pip install faiss-cpu\nimport faiss\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nimport cudf, cuml, cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors\n\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\nimport timm\n\n\nimport warnings\nwarnings.simplefilter('ignore')\n\ntorch.backends.cudnn.benchmark = True\n\nimport transformers\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup,get_cosine_schedule_with_warmup\nfrom transformers import get_cosine_with_hard_restarts_schedule_with_warmup","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class cfg:\n    NUM_WORKERS = 2\n    BATCH_SIZE = 8  \n    transformer_model = 'bert-base-uncased'\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nTOKENIZER1 = transformers.AutoTokenizer.from_pretrained('../input/bert-base-uncased')\nTOKENIZER2 = transformers.AutoTokenizer.from_pretrained('../input/paraphrase-xlm-r-multilingual-v1/0_Transformer')\n\n    \nmodel_params = {\n    'n_classes':8812,\n    #'n_classes':11014,\n    'model_name':\"../input/bert-base-uncased\",\n    'pooling':'clf',\n    'use_fc':False,\n    'fc_dim':1280,\n    'dropout':0.0,\n    #'loss_module':loss_module,\n    's':30.0,\n    'margin':0.50,\n    'ls_eps':0.0,\n    'theta_zero':0.785\n}\n\nmodel_params2 = {\n    'n_classes':8811,\n    #'n_classes':11014,\n    'model_name':\"../input/bert-base-uncased\",\n    'pooling':'clf',\n    'use_fc':False,\n    'fc_dim':1280,\n    'dropout':0.0,\n    #'loss_module':loss_module,\n    's':30.0,\n    'margin':0.50,\n    'ls_eps':0.0,\n    'theta_zero':0.785\n}\n\nmodel_params_2 = {\n    'n_classes':8811,\n    #'n_classes':11014,\n    'model_name':'../input/paraphrase-xlm-r-multilingual-v1/0_Transformer',\n    'pooling':'clf',\n    'use_fc':False,\n    'fc_dim':1280,\n    'dropout':0.0,\n    #'loss_module':loss_module,\n    's':30.0,\n    'margin':0.50,\n    'ls_eps':0.0,\n    'theta_zero':0.785\n}\n    \nclass ShopeeDataset1(Dataset):\n    def __init__(self, csv):\n        self.csv = csv.reset_index()\n\n    def __len__(self):\n        return self.csv.shape[0]\n\n    def __getitem__(self, index):\n        row = self.csv.iloc[index]\n        text = row.title\n        text = TOKENIZER1(text, padding='max_length', truncation=True, max_length=64, return_tensors=\"pt\")\n        input_ids = text['input_ids'][0]\n        attention_mask = text['attention_mask'][0]  \n        \n        return input_ids, attention_mask\n    \nclass ShopeeDataset2(Dataset):\n    def __init__(self, csv):\n        self.csv = csv.reset_index()\n\n    def __len__(self):\n        return self.csv.shape[0]\n\n    def __getitem__(self, index):\n        row = self.csv.iloc[index]\n        text = row.title\n        text = TOKENIZER2(text, padding='max_length', truncation=True, max_length=64, return_tensors=\"pt\")\n        input_ids = text['input_ids'][0]\n        attention_mask = text['attention_mask'][0]  \n        \n        return input_ids, attention_mask","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"COMPUTE_CV = False\n\n#make target clustering\nif COMPUTE_CV:\n    df = pd.read_csv(\"../input/shopee-product-matching/train.csv\")\n    tmp = df.groupby('label_group').posting_id.agg('unique').to_dict()\n    df['target'] = df.label_group.map(tmp)\n    df['target'] = df['target'].apply(lambda x: ' '.join(x))\n    #df_cu = cudf.DataFrame(df)\nelse:\n    df = pd.read_csv(\"../input/shopee-product-matching/test.csv\")\n    df_cu = cudf.DataFrame(df)\n    if len(df)==3:\n        cfg.batch = 3\n    \nprint('df shape is', df.shape )\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\ndf_unique = df.label_group.unique()\nu0, u1, u2, u3, u4 = np.array_split(df_unique, 5)\nu_train = np.concatenate([u0,u1,u2,u3],axis=0)\nu_test = u4\n\ndf_train = df[df.label_group.isin(u_train)]\ndf_test = df[df.label_group.isin(u_test)]\n\nprint(len(df_train),len(df_test),len(df_train)+len(df_test))\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ArcMarginProduct(nn.Module):\n    def __init__(self, in_features, out_features, s=30.0, m=0.70, easy_margin=False, ls_eps=0.0):\n        super(ArcMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.s = s\n        self.m = m\n        self.ls_eps = ls_eps  # label smoothing\n        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(m)\n        self.sin_m = math.sin(m)\n        self.th = math.cos(math.pi - m)\n        self.mm = math.sin(math.pi - m) * m\n\n    def forward(self, input, label):\n        # --------------------------- cos(theta) & phi(theta) ---------------------------\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n        # --------------------------- convert label to one-hot ---------------------------\n        one_hot = torch.zeros(cosine.size(), device=\"cuda\")\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.s\n\n        return output\n    \nclass ShopeeNet(nn.Module):\n\n    def __init__(self,\n                 n_classes,\n                 model_name='../input/bert-base-uncased',\n                 pooling='mean_pooling',\n                 use_fc=False,\n                 fc_dim=512,\n                 dropout=0.0,\n                 loss_module='softmax',\n                 s=30.0,\n                 margin=0.50,\n                 ls_eps=0.0,\n                 theta_zero=0.785):\n\n        super(ShopeeNet, self).__init__()\n\n        self.transformer = transformers.AutoModel.from_pretrained(model_name)\n        final_in_features = self.transformer.config.hidden_size\n        \n        self.pooling = pooling\n        self.use_fc = use_fc\n        self.fc = nn.Linear(final_in_features, fc_dim)\n        self._init_params()\n        self.loss_module = loss_module\n        #self.final = ArcMarginProduct(final_in_features, n_classes)\n        self.final = ArcMarginProduct(fc_dim, n_classes)\n        \n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n        #nn.init.constant_(self.bn.weight, 1)\n        #nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, input_ids,attention_mask, label=None):\n        feature = self.extract_feat(input_ids,attention_mask)\n        feature = self.fc(feature)\n        if label is not None:\n            logits = self.final(feature, label)\n        else:\n            logits = feature\n        return F.normalize(logits,dim=1)\n\n    def extract_feat(self, input_ids,attention_mask):\n        x = self.transformer(input_ids=input_ids,attention_mask=attention_mask)\n        features = x[0]\n        features = features[:,0,:]\n\n        if self.use_fc:\n            features = self.dropout(features)\n            features = self.fc(features)\n            features = self.bn(features)\n            features = self.relu(features)\n\n        return features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_embedding_fn(df):\n    test_dataset = ShopeeDataset1(csv=df)\n    loader = torch.utils.data.DataLoader(\n        test_dataset,\n        batch_size=cfg.BATCH_SIZE,\n        shuffle=False,\n        pin_memory=True,\n        drop_last=False,\n        num_workers=cfg.NUM_WORKERS)\n    model1 = ShopeeNet(**model_params).to(device)\n    model1.load_state_dict(torch.load('../input/shopee-weight/bertmodel_acc0.813.pt'))\n    model2 = ShopeeNet(**model_params2).to(device)\n    model2.load_state_dict(torch.load('../input/shopee-weight/bertmodel_fold2_acc0.774.pt'))\n    model1.eval()\n    model2.eval()\n    print('start collection')\n    embedded = np.empty((0,1280),dtype='float32')\n    with torch.no_grad():\n        for idx,d in enumerate(loader):\n            input_ids, attention_mask = d[0].to(device),d[1].to(device)\n            outputs1 = model1(input_ids,attention_mask)\n            outputs2 = model2(input_ids,attention_mask)\n            outputs = (outputs1 + outputs2)/2\n            embedded = np.append(embedded, outputs.cpu().detach().numpy(),axis=0)\n\n            if idx%500==0:\n                print(idx,len(loader)) \n                print(embedded.shape)\n    print(embedded.shape)\n    return embedded\n\ndef get_embedding_para(df):\n    test_dataset = ShopeeDataset2(csv=df)\n    loader = torch.utils.data.DataLoader(\n        test_dataset,\n        batch_size=cfg.BATCH_SIZE,\n        shuffle=False,\n        pin_memory=True,\n        drop_last=False,\n        num_workers=cfg.NUM_WORKERS)\n    model = ShopeeNet(**model_params_2).to(device)\n    model.load_state_dict(torch.load('../input/shopee-weight/paraphrasemodel_fold3_acc0.73.pt'))\n    model.eval()\n    print('start collection')\n    embedded = np.empty((0,1280),dtype='float32')\n    with torch.no_grad():\n        for idx,d in enumerate(loader):\n            input_ids, attention_mask = d[0].to(device),d[1].to(device)\n            outputs = model(input_ids,attention_mask)\n            embedded = np.append(embedded, outputs.cpu().detach().numpy(),axis=0)\n\n            if idx%500==0:\n                print(idx,len(loader)) \n                print(embedded.shape)\n    print(embedded.shape)\n    return embedded\n\ndef f1_score(y_true, y_pred):\n    y_true = y_true.apply(lambda x: set(x.split()))\n    y_pred = y_pred.apply(lambda x: set(x.split()))\n    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n    len_y_pred = y_pred.apply(lambda x: len(x)).values\n    len_y_true = y_true.apply(lambda x: len(x)).values\n    f1 = 2 * intersection / (len_y_pred + len_y_true)\n    return f1\n\ndef predict_bert(df,embeddings,topk=50,threshold=0.63):\n    N,D = embeddings.shape\n    cpu_index = faiss.IndexFlatL2(D)\n    gpu_index = faiss.index_cpu_to_all_gpus(cpu_index)\n    gpu_index.add(embeddings)\n    cluster_distance,cluster_index = gpu_index.search(x=embeddings, k=topk)\n    \n    df['pred_bert'] = ''\n    pred = []\n    for k in range(embeddings.shape[0]):\n        idx = np.where(cluster_distance[k,] < threshold)[0]\n        ids = cluster_index[k,idx]\n        #posting_ids = ' '.join(df['posting_id'].iloc[ids].values)\n        posting_ids = df['posting_id'].iloc[ids].values\n        pred.append(posting_ids)\n    df['pred_bert'] = pred\n    if COMPUTE_CV:\n        df['pred_bertonly'] = df.pred_bert.apply(lambda x: ' '.join(x))\n        df['f1_bert'] = f1_score(df['target'], df['pred_bertonly'])\n        score = df['f1_bert'].mean()\n        print(f'Our f1 score for threshold {threshold} is {score}')\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_embedding = get_embedding_fn(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#para_embedding = get_embedding_para(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#trans_embedding = bert_embedding","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"theresholds=np.linspace(0.32,0.38,10)\nif COMPUTE_CV:\n    df = predict_bert(df,bert_embedding,topk=50,threshold=0.346)\n    #df = predict_bert(df,para_embedding,topk=50,threshold=0.573)\n    #df = predict_bert(df,trans_embedding,topk=50,threshold=0.27)\n    #for topk in [49,50,51,60]:\n    #for threshold in theresholds:\n        #df = predict_bert(df,bert_embedding,topk=50,threshold=threshold)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = predict_bert(df,bert_embedding,topk=50,threshold=0.24)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['matches'] = df['pred_bert'].apply(lambda x: ' '.join(x))\nif COMPUTE_CV:\n    df['f1'] = f1_score(df['target'], df['matches'])\n    score = df['f1'].mean()\n    print(f'Final f1 score is {score}')\nelse:\n    with open('submission.csv', 'w') as outf:\n        print('posting_id,matches', file=outf)\n        for i,(idnum,match) in enumerate(zip(df['posting_id'],df['matches'])):\n            print(f'{idnum},{match}', file=outf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if COMPUTE_CV:\n    df.to_csv('result.csv')\nelse:\n    df_t = pd.read_csv(\"submission.csv\")\n    print(df_t)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}