{"cells":[{"metadata":{},"cell_type":"markdown","source":"<p style = \"font-family:courier,arial,helvetica;font-size:300%;\">\nShopee - Price Match Gaurentee\n</p>"},{"metadata":{},"cell_type":"markdown","source":"![](https://img.lovepik.com/original_origin_pic/18/05/29/d49c1a025873dc18ca5919263197a3cc.png_wh300.png)"},{"metadata":{},"cell_type":"markdown","source":"<p style = \"font-family:courier,arial,helvetica;font-size:300%;\">\nRecent Update :\n<ol style=\"font-family:courier,arial,helvetica;font-size:200%\"><li>I added Phash Similarity Checker With a metric which is fast :)</li></ol>\n</p>"},{"metadata":{},"cell_type":"markdown","source":"<p style = \"font-family:courier,arial,helvetica;font-size:300%;\">\nImporting Packages</p>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, BertModel\nimport logging\nimport matplotlib.pyplot as plt\nimport transformers\nfrom transformers import *\nfrom tokenizers import BertWordPieceTokenizer\nimport pandas as pd\nimport numpy as np\nfrom scipy.spatial.distance import cosine\nimport cudf, cuml, cupy\nimport gc\nimport numpy as np\nfrom sklearn.preprocessing import normalize\nfrom scipy.spatial.distance import hamming\nfrom IPython.display import clear_output\nimport imagehash","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style = \"font-family:courier,arial,helvetica;font-size:300%;\">\nImporting Saved Tokenizer</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('../input/bert-large-tokenizer')\nfast_tokenizer = BertWordPieceTokenizer('../input/bert-large-tokenizer/vocab.txt', lowercase=True )\nfast_tokenizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style = \"font-family:courier,arial,helvetica;font-size:300%;\">\nImporting Test Data</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"test=pd.read_csv('../input/shopee-product-matching/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style = \"font-family:courier,arial,helvetica;font-size:300%;\">\nEncoding The Data</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def fast_encode(texts, tokenizer, chunk_size=35000, maxlen=200):\n    tokenizer.enable_truncation(max_length=maxlen)\n    tokenizer.enable_padding()\n    all_ids = []\n    \n    for i in range(0, len(texts), chunk_size):\n        text_chunk = texts[i:i+chunk_size].tolist()\n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n    \n    return np.array(all_ids)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style = \"font-family:courier,arial,helvetica;font-size:300%;\">\nFinding Best Results</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings = fast_encode(test['title'].values, fast_tokenizer, maxlen=200)\nembeddings=normalize(embeddings)\npreds = []\nCHUNK = 1024*4\ntext_embeddings=cupy.array(embeddings)\nprint('Finding similar titles...')\nCTS = len(test)//CHUNK\nif len(test)%CHUNK!=0: CTS += 1\nfor j in range( CTS ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(test))\n    print('chunk',a,'to',b)\n    \n    # COSINE SIMILARITY DISTANCE\n    cts = cupy.matmul( text_embeddings, text_embeddings[a:b].T).T\n    \n    for k in range(b-a):\n        IDX = cupy.where(cts[k,]>0.995)[0]\n        o = test.iloc[cupy.asnumpy(IDX)].posting_id.values\n        preds.append(o)\ndel text_embeddings,embeddings\n_=gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"posting=test['posting_id'].values\nans=[]\nfor i in range(len(test)):\n    s=posting[i]\n    if len(preds[i])>1 :\n        for j in preds[i]:\n            if j!=posting[i]:\n                s+=' '+j\n    ans.append(s)\npreds=[]\nfor i in ans:\n    preds.append(i.split(' '))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style = \"font-family:courier,arial,helvetica;font-size:300%;\">\nBest Results Using PHASH</p>"},{"metadata":{},"cell_type":"markdown","source":"<p style = \"font-family:courier,arial,helvetica;font-size:200%;\">\nHow is this working?? ðŸ¤”ðŸ¤”\n</p>\n<p style = \"font-family:courier,arial,helvetica;font-size:100%;\">\n    Well i had a basic idea of how i can filter True-False or False-True from the arrays so i used the XNOR function like this and took out the sum of them and subtracted them from 64 which is the highest value which made a distance . So if is closer to 0 means that the hash code has high similarity from the choosen hash code otherwise it is different from it :)\n    <br>\n    It was better than the time it took for hamming distance to run and seems to be good :)\n</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"hash_codes=np.array([imagehash.hex_to_flathash(i,64).hash for i in test['image_phash'].values])\nhash_preds=[]\ncount=1\nfor i in range(len(hash_codes)) :\n    print(count*100/len(hash_codes),' % is done')\n    clear_output(wait=True)\n    count+=1\n    x=sum((hash_codes[i]*hash_codes).T)\n    y=sum((~hash_codes[i]*~hash_codes).T)\n    t=x+y\n    t=64-t\n    IDX = np.where(t<10)[1]\n    o = test.iloc[IDX].posting_id.values\n    hash_preds.append(o)\nhash_preds=[list(i) for i in hash_preds]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hash_preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style = \"font-family:courier,arial,helvetica;font-size:300%;\">\nResult of Both Tokenizer and BaseLine Model Combined</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntest_list = test[['posting_id', 'title', 'image', 'image_phash']].reset_index(drop=True).values.tolist()\nsubmission_list = []\n# ==========================================\n# Case. title[1] & image[2] & image_phash[3]\n# ==========================================\ncount=0\nfor item in test_list:\n    res1 = test[test['title']==item[1]]['posting_id'].unique().tolist()\n    res2 = test[test['image']==item[2]]['posting_id'].unique().tolist()\n    res2=list(set(res2+preds[count]))\n    res3 = test[test['image_phash']==item[3]]['posting_id'].unique().tolist()\n    res3=list(set(res3+hash_preds[count]))\n    res = list(set(res1 + res2 + res3))\n    res.remove(item[0])\n    matches = ' '.join(map(str, res))\n    matches = f'{item[0]} {matches}'\n    submission_list.append([item[0], matches])\n    count+=1\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame(submission_list, columns = ['posting_id','matches'])\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reference For Baseline model from : https://www.kaggle.com/cdeotte/part-2-rapids-tfidfvectorizer-cv-0-700 by chris deotte :)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}