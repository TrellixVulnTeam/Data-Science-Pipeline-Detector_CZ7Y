{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#       print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport time\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/shopee-product-matching/train.csv')\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/shopee-product-matching/test.csv')\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Removing stopwords from titles and finding matches","metadata":{}},{"cell_type":"code","source":"same_group = train.iloc[np.where(train['label_group'] == 3627744656)]\nsame_group.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(same_group.title.value_counts())\nprint(len(np.unique(same_group.title)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see how many of the titles in this group have similar words, but are still slightly different. Let's see if we can combine some of these by removing stopwords.","metadata":{}},{"cell_type":"code","source":"lowercase = [title.lower() for title in same_group.title]\ntokenized = [word_tokenize(title) for title in lowercase]\n\nstopwords_punctuation = stopwords.words('english')\npunctuation = ['&', '-', '(', ')', '/', '|']\nstopwords_punctuation.extend(punctuation)\n\ntitles_no_stopwords = []\nfor title in tokenized:\n    title_no_sw = [word for word in title if not word in stopwords_punctuation]\n    titles_no_stopwords.append(title_no_sw)\n    \nprint(titles_no_stopwords)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"same_group['title_no_sw'] = titles_no_stopwords\nsame_group.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(same_group.title_no_sw.value_counts())\nprint(len(np.unique(same_group.title_no_sw)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks like we have reduced the number of unique values in the titles! Let's try expanding this to the entire dataset.","metadata":{}},{"cell_type":"code","source":"lowercase = [title.lower() for title in train.title]\ntokenized = [word_tokenize(title) for title in lowercase]\n\nstopwords_punctuation = stopwords.words('english')\npunctuation = [ '!', '\"', '#', '$', '%', '&', '(', ')', '*', '+', '-', '.', '/',  '\\\\', ':', ';', '<', '=', '>',\n           '?', '@', '[', ']', '^', '_', '`', '{', '|', '}', '\\t','\\n',\"'\",\",\",'~' , '—']\nstopwords_punctuation.extend(punctuation)\n\ntitles_no_stopwords = []\nfor title in tokenized:\n    title_no_sw = [word for word in title if not word in stopwords_punctuation]\n    title_string = ' '.join(title_no_sw)\n    titles_no_stopwords.append(title_string)\n    \ntrain['title_no_sw'] = titles_no_stopwords\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = time.time()\n\nids = []\nitem_group = [] # tracks which group an item is in\ngroups = {} # tracks which items are in each group\nfor ind in train.index:\n    \n    indices_phash = train.loc[(train['image_phash'] == train['image_phash'][ind])].index \n    indices_title = train.loc[(train['title_no_sw'] == train['title_no_sw'][ind])].index \n    indices_image = train.loc[(train['image'] == train['image'][ind])].index\n    indices = set(indices_phash).union(set(indices_title)).union(set(indices_image))\n    \n    # check if an item this matches already has a group\n    match_ids = list(train.loc[indices]['posting_id'])\n    if set(match_ids).intersection(ids):\n        existing_group = item_group[ids.index(list(set(match_ids).intersection(ids))[0])]\n        item_group.append(existing_group)\n        groups[existing_group] = set(groups[existing_group]).union(match_ids)\n    else:\n        item_group.append(len(groups))\n        groups[len(groups)] = match_ids\n        \n    ids.append(train['posting_id'][ind])\n    \nmatches = [' '.join(list(groups[ind])) for ind in item_group]\n\nprint(time.time() - start_time)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now lets apply this to the test set.","metadata":{}},{"cell_type":"code","source":"lowercase = [title.lower() for title in test.title]\ntokenized = [word_tokenize(title) for title in lowercase]\n\nstopwords_punctuation = stopwords.words('english')\npunctuation = [ '!', '\"', '#', '$', '%', '&', '(', ')', '*', '+', '-', '.', '/',  '\\\\', ':', ';', '<', '=', '>',\n           '?', '@', '[', ']', '^', '_', '`', '{', '|', '}', '\\t','\\n',\"'\",\",\",'~' , '—']\nstopwords_punctuation.extend(punctuation)\n\ntitles_no_stopwords = []\nfor title in tokenized:\n    title_no_sw = [word for word in title if not word in stopwords_punctuation]\n    title_string = ' '.join(title_no_sw)\n    titles_no_stopwords.append(title_string)\n    \ntest['title_no_sw'] = titles_no_stopwords\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = time.time()\n\nids = []\nitem_group = [] # tracks which group an item is in\ngroups = {} # tracks which items are in each group\nfor ind in test.index:\n    \n    indices_phash = test.loc[(test['image_phash'] == test['image_phash'][ind])].index \n    indices_title = test.loc[(test['title_no_sw'] == test['title_no_sw'][ind])].index \n    indices_image = test.loc[(test['image'] == test['image'][ind])].index\n    indices = set(indices_phash).union(set(indices_title)).union(set(indices_image))\n    \n    # check if an item this matches already has a group\n    match_ids = list(test.loc[indices]['posting_id'])\n    if set(match_ids).intersection(ids):\n        existing_group = item_group[ids.index(list(set(match_ids).intersection(ids))[0])]\n        item_group.append(existing_group)\n        groups[existing_group] = set(groups[existing_group]).union(match_ids)\n    else:\n        item_group.append(len(groups))\n        groups[len(groups)] = match_ids\n        \n    ids.append(test['posting_id'][ind])\n    \nmatches = [' '.join(list(groups[ind])) for ind in item_group]\n\nprint(time.time() - start_time)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({'posting_id': ids, 'matches': matches})\nsubmission.to_csv('submission.csv', index = False)\nsubmission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Finding Similar Titles with RAPIDS and kNN\n\nhttps://www.kaggle.com/ruchi798/shopee-eda-rapids-preprocessing\n\nhttps://www.kaggle.com/cdeotte/rapids-cuml-tfidfvectorizer-and-knn#Find-Similar-Titles-with-RAPIDS!\n\nRAPIDS is a suite of open source software liraries that allow you to run code on GPUs. cuDF is a Python GPU DataFrame library. CuPy is a NumPy-compatible array library. cuML is a suite of GPU-accelerated ML algorithms.","metadata":{}},{"cell_type":"code","source":"import cuml, cudf, cupy\nimport nltk\nimport tensorflow as tf\n\nimport cudf, cuml, cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors\nprint('RAPIDS',cuml.__version__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load data to use with RAPIDS\ntrain_c = cudf.from_pandas(train)\ntrain_c.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing: TfidVectorizer\n\nhttps://medium.com/rapids-ai/natural-language-processing-text-preprocessing-and-vectorizing-at-rocking-speed-with-rapids-cuml-74b8d751812e\n\nTfidfVectorizer performs fast text vectorizing on GPUs, which can be almost 5 times faster than sklearn's `TfidfVectorizer` on CPUs. The TF-IDF score is the product of the term frequency and the inverse document frequency. Term frequency is the number of times each term appears in a document divided by the total number of words in the document. Inverse document frequency is the log of the number of documents divided by the number of documents that contain the word. `TfidfVectorizer` returns a cuPy sparse matrix.","metadata":{}},{"cell_type":"code","source":"model = TfidfVectorizer(stop_words='english', binary=True)\ntext_embeddings = model.fit_transform(train_c.title).toarray()\nprint('text embeddings shape is',text_embeddings.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fit a kNN model to find similar titles","metadata":{}},{"cell_type":"code","source":"model = NearestNeighbors(n_neighbors = 50)\nmodel.fit(text_embeddings)\ndistances, indices_kNN = model.kneighbors(text_embeddings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's look at the nearest neighbors of the first few rows of our dataset.","metadata":{}},{"cell_type":"code","source":"for k in range(5):\n   plt.figure(figsize=(20,3))\n   plt.plot(np.arange(50),cupy.asnumpy(distances[k,]),'o-')\n   plt.title('Text Distance From Train Row %i to Other Train Rows'%k,size=16)\n   plt.ylabel('Distance to Train Row %i'%k,size=14)\n   plt.xlabel('Index Sorted by Distance to Train Row %i'%k,size=14)\n   plt.show()\n    \n   print(train_c.loc[cupy.asnumpy(indices_kNN[k,:10]),['title','label_group']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For 4 out of these 5 rows, the neighbors with a distance less than 0.9 seem to be in the same label group. In train row 3, the 3 nearest neighbors all have titles **very** similar to the original, but are surprisingly in different groups. We can try using 0.9 as our threshold and see where that gets us.","metadata":{}},{"cell_type":"markdown","source":"# Assembling the Overall Model","metadata":{}},{"cell_type":"code","source":"start_time = time.time()\n\ntrain_ids = []\ntrain_item_group = [] # tracks which group an item is in\ntrain_groups = {} # tracks which items are in each group\nfor ind in train.index:\n    \n    indices_phash = np.where(train.image_phash == train['image_phash'][ind])[0]\n    indices_title = indices_kNN[ind,][cupy.where(distances[ind,] <= 0.9)]\n    indices_image = np.where(train['image'] == train['image'][ind])[0]\n    indices = set(indices_phash).union(set(list(cupy.asnumpy(indices_title)))).union(set(indices_image))\n    \n    # check if an item this matches already has a group\n    match_ids = list(train.loc[indices]['posting_id'])\n    if set(match_ids).intersection(train_ids):\n        existing_group = train_item_group[train_ids.index(list(set(match_ids).intersection(train_ids))[0])]\n        train_item_group.append(existing_group)\n        train_groups[existing_group] = set(train_groups[existing_group]).union(match_ids)\n    else:\n        train_item_group.append(len(train_groups))\n        train_groups[len(train_groups)] = match_ids\n        \n    train_ids.append(train['posting_id'][ind])\n    \ntrain_matches = [' '.join(list(train_groups[ind])) for ind in train_item_group]\n\nprint(time.time() - start_time)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add column with ground truth\ntmp = train.groupby('label_group').posting_id.agg('unique').to_dict()\ntrain['target'] = train.label_group.map(tmp)\n\ntrain['oof'] = [list(train_groups[ind]) for ind in train_item_group]\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getMetric(col):\n    def f1score(row):\n        n = len( np.intersect1d(row.target,row[col]) )\n        return 2*n / (len(row.target)+len(row[col]))\n    return f1score\n\ntrain['f1'] = train.apply(getMetric('oof'),axis=1)\nprint('CV score =',train.f1.mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is a big improvement in the training score! Now let's apply this to test data.","metadata":{}},{"cell_type":"markdown","source":"# Fitting to Test Data and Generating Submission","metadata":{}},{"cell_type":"code","source":"# load data to use with RAPIDS\ntest_c = cudf.read_csv('../input/shopee-product-matching/test.csv')\ntest_c.head()\n#train_2 = cudf.from_pandas(train)\n#test_c = cudf.concat([train_2,train_2],axis=0,ignore_index=True)\n#test_c.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = TfidfVectorizer(stop_words='english', binary=True)\ntext_embeddings = model.fit_transform(test_c.title).toarray()\nprint('text embeddings shape is',text_embeddings.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = NearestNeighbors(n_neighbors = 3)\nif np.shape(test)[0] > 3:\n    model = NearestNeighbors(n_neighbors = 50)\n\nmodel.fit(text_embeddings)\ndistances, indices_kNN = model.kneighbors(text_embeddings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = time.time()\n\nids = []\nitem_group = [] # tracks which group an item is in\ngroups = {} # tracks which items are in each group\nfor ind in test.index:\n    \n    indices_phash = np.where(test.image_phash == test['image_phash'][ind])[0]\n    indices_title = indices_kNN[ind,][cupy.where(distances[ind,] <= 0.9)]\n    indices_image = np.where(test['image'] == test['image'][ind])[0]\n    indices = set(indices_phash).union(set(list(cupy.asnumpy(indices_title)))).union(set(indices_image))\n    \n    # check if an item this matches already has a group\n    match_ids = list(test.loc[indices]['posting_id'])\n    if set(match_ids).intersection(ids):\n        existing_group = item_group[ids.index(list(set(match_ids).intersection(ids))[0])]\n        item_group.append(existing_group)\n        groups[existing_group] = set(groups[existing_group]).union(match_ids)\n    else:\n        item_group.append(len(groups))\n        groups[len(groups)] = match_ids\n        \n    ids.append(test['posting_id'][ind])\n    \nmatches = [' '.join(list(groups[ind])) for ind in item_group]\n\nprint(time.time() - start_time)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({'posting_id': ids, 'matches': matches})\nsubmission.to_csv('submission.csv', index = False)\nsubmission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Unfortunately, finding the nearest neighbor distances and indices gives us a memory error when running on the test set. In later notebooks we use a chunking method to avoid memory errors.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}