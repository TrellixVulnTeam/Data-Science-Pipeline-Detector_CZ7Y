{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Shopee - Price Match Guarantee competition.\n## We are given a set of products, each represented by the image and text description. The goal is to discover products that should have equal price.\n#### **NOTE**: the goal is quite different from looking for simply duplicated items, which is brilliantly explained by [Roman Glushko](https://www.kaggle.com/glushko) in [this discussion](https://www.kaggle.com/c/shopee-product-matching/discussion/236496)","metadata":{}},{"cell_type":"markdown","source":"### This notebook will walk you through the competition. We will perform EDA, have a look at images and explore perceptual hashing, peek into textual data. \n### For modeling part we will create a basic training pipelines with PyTorch and RAPIDS and do inference with separate models for image and text data.","metadata":{}},{"cell_type":"markdown","source":"## Before we start, I'd like to thank these guys for their insightful and much inspiring work:\n[Chris Deotte](https://www.kaggle.com/cdeotte) - on awesome introductions to ML with CUDA, found in the top of the Shopee competition notebooks\n\n[ragnar](https://www.kaggle.com/ragnar123) - https://www.kaggle.com/ragnar123/shopee-efficientnetb3-arcmarginproduct, https://www.kaggle.com/ragnar123/shopee-inference-efficientnetb1-tfidfvectorizer\n\n[Mr_KnowNothing](https://www.kaggle.com/tanulsingh077) - https://www.kaggle.com/tanulsingh077/pytorch-metric-learning-pipeline-only-images\n\n[Parth Dhameliya](https://www.kaggle.com/parthdhameliya77) - https://www.kaggle.com/parthdhameliya77/pytorch-eca-nfnet-l0-image-tfidf-inference","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\nsys.path.append('../input/shopee-competition-rgr')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%bash\nmkdir -p ./src && \\\ncp ../input/shopee-competition-rgr/*.py ./src","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nimport os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom tqdm import tqdm\n\n\nimport cudf\nimport cuml\nimport cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors\nimport cv2\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch import nn\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Subset, DataLoader\n\nfrom src.config import CFG\nfrom src.dataset import ShopeeDataset\nfrom src.loss import Mish, replace_activations\nfrom src.model import ShopeeCNNModel\nfrom src.train import train_fn, eval_fn\nfrom src.transforms import get_train_transforms, get_test_transforms\nfrom src.utils import read_dataset\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BASE_DIR = \"../input/shopee-product-matching\"","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(f'{BASE_DIR}/train.csv')\ntest = pd.read_csv(f'{BASE_DIR}/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.posting_id = train.posting_id.str.replace('train_', '')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.label_group.value_counts()[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Explore our target","metadata":{}},{"cell_type":"code","source":"ax = plt.axes()\nsns.boxplot(train.label_group.value_counts(), ax=ax)\nax.set_xlabel(\"Count\")\nax.set_ylabel(\"Labels groups\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"duplicated_labels= train[\"label_group\"].value_counts()[:20]\nplt.xticks(range(len(duplicated_labels)), duplicated_labels.index, rotation=90)\nplt.bar(range(len(duplicated_labels)), duplicated_labels.values)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualization of these duplicates can provide us with insights into features to account for","metadata":{}},{"cell_type":"code","source":"labels_to_examine = duplicated_labels.iloc[np.random.randint(0, len(duplicated_labels), 3)].to_frame()\nlabels_to_examine","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train), train[\"image\"].nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Interesting! Identical images denote different label groups. \n### This suggests that description will play quite a role when deciding whether two products have the same price","metadata":{}},{"cell_type":"markdown","source":"# Explore the image data","metadata":{}},{"cell_type":"code","source":"def visualize_similar_imgs(random=False, COLS=6, ROWS=4, base_path=BASE_DIR):\n    root = f'{base_path}/train_images'\n    for k in range(ROWS):\n        plt.figure(figsize=(20,5))\n        for j in range(COLS):\n            if random: row = np.random.randint(0,len(train))\n            else: row = COLS*k + j\n            name = train.iloc[row,1]\n            title = train.iloc[row,3]\n            title_with_return = \"\"\n            for i,ch in enumerate(title):\n                title_with_return += ch\n                if (i!=0)&(i%20==0): title_with_return += '\\n'\n                img = cv2.imread(str(Path(root).joinpath(name)))\n            plt.subplot(1,COLS,j+1)\n            plt.title(title_with_return)\n            plt.axis('off')\n            plt.imshow(img)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_similar_imgs()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_visualize = 5\n\ndef add_newlines_to_title(title: str):\n    max_chars = 15\n    idxs = [i*max_chars for i in range(0, len(title) // max_chars)]\n    for i in idxs:\n        title = title[:i]+'\\n'+title[i:]\n    return title+'\\n'\n\ndef visualize_dupl_images():\n    for k, (lg, dup_n) in enumerate(labels_to_examine.iterrows()):\n        plt.figure(figsize=(20,5))\n        samples = train[train.label_group==lg][:to_visualize]\n        title = samples.loc[:,'title'].values\n        names = samples.loc[:,'image'].values\n        for j in range(to_visualize):\n            img_path = str(Path(BASE_DIR).joinpath(f'train_images/{names[j]}'))\n            img = cv2.imread(img_path)\n            plt.subplot(1,to_visualize,j+1)\n            img_title = add_newlines_to_title(title[j])\n            plt.title(img_title)\n            plt.axis('off')\n            plt.imshow(img)\n    plt.show()\n\nvisualize_dupl_images()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Both images and descriptions give clear scent of duplication\n### Images with same product contain the product with some distortions (rotation, brightness, gamma transforms), noisy objects. There are complete duplicates as well.\n### Descriptions contain name of the product with some extra text, which mildly contributes to higher algorithm performance","metadata":{}},{"cell_type":"code","source":"target = \"label_group\"\nlabels = train[target]\ndescr = train.title\nphash = train.image_phash\nimages = train.image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"descr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Image perceptual hash should be similar (not identical!) if the image is considered similar","metadata":{}},{"cell_type":"code","source":"phash, phash.nunique(), labels.nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### If the phash is identical - the images are complete copies. We have a plenty of such cases","metadata":{}},{"cell_type":"markdown","source":"### Let's examine perceptual hashing","metadata":{}},{"cell_type":"markdown","source":"#### Different hashing techniques use different image features. Maybe we can guess the hash that was used?","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nimport imagehash\nimg_path1 = f'{BASE_DIR}/train_images/{images[0]}'\nimg_path2 = f'{BASE_DIR}/train_images/{images[1]}'\n\ndef find_img_phashes(img):\n    avg_hash = imagehash.average_hash(img)\n    diff_hash = imagehash.dhash(img)\n    dct_hash = imagehash.phash(img)\n    wavelet_hash = imagehash.whash(img)\n    print('Hashes:')\n    print('AVG: ' + str(avg_hash))\n    print('DIFF: ' + str(diff_hash))\n    print('DCT: ' + str(dct_hash))\n    print('Wavelet: ' + str(wavelet_hash))\n    print('\\nTrue hash: '+phash[0])\n    return avg_hash, diff_hash, dct_hash, wavelet_hash\n\nprint(\"Image 1 perceptual hashes:\")\navg_hash1, diff_hash1, dct_hash1, wavelet_hash1 = find_img_phashes(Image.open(img_path1))\nprint(\"\\nImage 2 perceptual hashes:\")\navg_hash2, diff_hash2, dct_hash2, wavelet_hash2 = find_img_phashes(Image.open(img_path2))\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 10)) \nax[0].imshow(Image.open(img_path1))\nax[1].imshow(Image.open(img_path2))\nif(dct_hash1 == dct_hash2):\n    print(\"\\nThe pictures are perceptually the same !\")\nelse:\n    print(f\"\\nThe pictures are different, distance: {dct_hash1 - dct_hash2}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### So, our perceptual hashing algorithm is DCT","metadata":{}},{"cell_type":"markdown","source":"### But! DCT is tolerant to minor transformations that we have a lot in our data (acc. to http://www.hackerfactor.com/blog/?/archives/432-Looks-Like-It.html).\n### This implies using additional perceptual hashes and some voting rule?!","metadata":{}},{"cell_type":"markdown","source":"# Explore product descriptions","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### What text appears to be most frequently used?","metadata":{}},{"cell_type":"code","source":"text = ' '.join(descr)\nwordcloud = WordCloud(width=400, height=400, min_font_size=8, max_font_size=64, background_color='white').generate(text)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The most frequent words can provide some noise to model and we should handle them accordingly","metadata":{}},{"cell_type":"code","source":"descr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(descr.map(lambda x: x.split(' ')).map(len), axlabel='#words in description')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### There are quite a few samples where description is either too short or too long, giving the average length of ~8.\n### Smart way to concatenate image- and text-based predictions with these looks beneficial","metadata":{}},{"cell_type":"markdown","source":"# Cooking the model","metadata":{}},{"cell_type":"code","source":"def run_training(base_dir):\n    data = pd.read_csv(base_dir)\n\n    present_imgs = os.listdir(f\"{base_dir}/train_images/\")\n    data = data[data['image'].isin(present_imgs)]\n    data['image'] = data['image'].apply(lambda x: f\"{base_dir}/train_images/\" + x)\n\n    encoder = LabelEncoder()\n    data['label_group'] = encoder.fit_transform(data['label_group'])\n    train_dataset = ShopeeDataset(data, transforms=get_train_transforms())\n\n    torch.cuda.empty_cache()\n    TRAIN_IDXS = int(0.9 * len(train_dataset))\n\n    indices = np.arange(len(train_dataset))\n    train_indices, test_indices = train_test_split(indices, train_size=TRAIN_IDXS)\n\n    train_subset = Subset(train_dataset, train_indices)\n    val_subset = Subset(train_dataset, test_indices)\n\n    train_dataloader = DataLoader(dataset=train_subset,\n                                  batch_size=CFG.batch_size,\n                                  num_workers=CFG.num_workers,\n                                  shuffle=True,\n                                  pin_memory=True,\n                                  drop_last=True)\n    val_dataloader = DataLoader(dataset=val_subset,\n                                batch_size=CFG.batch_size,\n                                shuffle=True,\n                                pin_memory=True,\n                                drop_last=True)\n\n    model = ShopeeCNNModel('efficientnet_b3')\n    model.to(CFG.device)\n\n    existing_layer = torch.nn.SiLU\n    new_layer = Mish()\n    model = replace_activations(model, existing_layer, new_layer)\n\n    lr_start = 1e-2\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr_start)\n    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, 3, eta_min=lr_start * 1e-4)\n\n    history = {'train': [], 'val': []}\n\n    for i in range(CFG.train_epochs):\n        epoch_loss_train = train_fn(model, train_dataloader, criterion, optimizer, scheduler, i)\n        epoch_loss_val = eval_fn(model, val_dataloader, i)\n        history['train'].append(epoch_loss_train)\n        history['val'].append(epoch_loss_val)\n        torch.save(model.state_dict(), 'arcface_512x512_efficientnet_b3.pt')\n\n    print(history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"markdown","source":"## Image data","metadata":{}},{"cell_type":"code","source":"TEST_PATH = f\"{BASE_DIR}/test.csv\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_image_embeddings(base_dir, model_name, model_path):\n    embeds = []\n\n    model = ShopeeCNNModel(model_name=model_name)\n    model.to(CFG.device)\n    model.eval()\n\n    model.load_state_dict(torch.load(model_path))\n    model = model.to(CFG.device)\n\n    test_data = pd.read_csv(f\"{base_dir}/test.csv\")\n\n    test_data['image'] = test_data['image'].apply(lambda x: f\"{base_dir}/test_images/\" + x)\n\n    test_dataset = ShopeeDataset(test_data, transforms=get_test_transforms(), is_training=False)\n    image_loader = torch.utils.data.DataLoader(\n        test_dataset,\n        batch_size=CFG.batch_size,\n        pin_memory=True,\n        drop_last=False,\n        num_workers=4\n    )\n\n    with torch.no_grad():\n        for img, label in tqdm(image_loader):\n            img = img.cuda()\n            label = label.cuda()\n            feat = model(img, label)\n            image_embeddings = feat.detach().cpu().numpy()\n            embeds.append(image_embeddings)\n\n    del model\n    image_embeddings = np.concatenate(embeds)\n    print(f'Our image embeddings shape is {image_embeddings.shape}')\n    del embeds\n    gc.collect()\n    return image_embeddings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_image_predictions(df, embeddings, threshold=0.0):\n    if len(df) > 3:\n        KNN = 50\n    else:\n        KNN = 3\n\n    model = NearestNeighbors(n_neighbors=KNN, metric='cosine')\n    model.fit(embeddings)\n    distances, indices = model.kneighbors(embeddings)\n\n    predictions = []\n    for k in tqdm(range(embeddings.shape[0])):\n        idx = np.where(distances[k,] < threshold)[0]  # check if the distance is small enough for a 'neighbour'\n        ids = indices[k, idx]  # select indices in KNN dataframe that match the distance req for item k\n        posting_ids = df['posting_id'].iloc[ids].values  # obtain posting ids from indices, including identity\n        predictions.append(posting_ids)\n\n    del model, distances, indices\n    gc.collect()\n    return predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text data","metadata":{}},{"cell_type":"markdown","source":"#### Since the hidden test set contains approx. 70k samples, it requires GPU optimized ML to keep iterations relatively quick","metadata":{}},{"cell_type":"code","source":"def get_text_predictions(df, df_cu, max_features = 20_000):\n    \n    model = TfidfVectorizer(stop_words = 'english', binary = True, max_features = max_features)\n    text_embeddings = model.fit_transform(df_cu['title']).toarray()\n    preds = []\n    CHUNK = 1024*4\n\n    print('Finding similar titles...')\n    CTS = len(df)//CHUNK\n    if len(df)%CHUNK!=0: CTS += 1\n    for j in range( CTS ):\n\n        a = j*CHUNK\n        b = (j+1)*CHUNK\n        b = min(b,len(df))\n        print('chunk',a,'to',b)\n\n        # COSINE SIMILARITY DISTANCE\n        cts = cupy.matmul( text_embeddings, text_embeddings[a:b].T).T\n\n        for k in range(b-a):\n            IDX = cupy.where(cts[k,]>0.75)[0]\n            o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n            preds.append(o)\n    \n    del model,text_embeddings\n    gc.collect()\n    return preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"KNN_DISTANCE_THRESH = 0.21\nMAX_TEXT_TOKENS = 15_000","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = pd.read_csv(f\"{BASE_DIR}/test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df,df_cu,image_paths = read_dataset(BASE_DIR)\n\nimage_embeddings = get_image_embeddings(BASE_DIR, CFG.model_name2, CFG.model_path2)\nimage_predictions = get_image_predictions(df, image_embeddings, threshold = KNN_DISTANCE_THRESH)\ntext_predictions = get_text_predictions(df, df_cu, max_features = MAX_TEXT_TOKENS)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def combine_predictions(row):\n    x = np.concatenate([row['image_predictions'], row['text_predictions']])\n    return ' '.join( np.unique(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['image_predictions'] = image_predictions\ndf['text_predictions'] = text_predictions\ndf['matches'] = df.apply(combine_predictions, axis = 1)\ndf[['posting_id', 'matches']].to_csv('submission.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.read_csv('submission.csv').head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}