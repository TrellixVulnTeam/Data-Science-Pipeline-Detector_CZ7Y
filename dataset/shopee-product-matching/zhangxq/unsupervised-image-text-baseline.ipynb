{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 要添加一个新单元，输入 '# %%'\n# 要添加一个新的标记单元，输入 '# %% [markdown]'","metadata":{"_uuid":"0ec39db5-abe8-46d1-953b-b39c057775c4","_cell_guid":"945db099-b380-4c38-8d99-8d9f1f87f464","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 环境配置","metadata":{"_uuid":"0587d897-4fd2-42ea-bbc0-c9f58dadecdf","_cell_guid":"0ccaa254-44ba-48d4-b84d-ee5b0712cc14","trusted":true}},{"cell_type":"code","source":"\nEXT_PATH=r''\n# EXT_PATH=r'/home/aistudio/external-libraries'\n\nDATA_PATH=r'../input/shopee-product-matching/'\n\nCNN_MODEL='resnet34'\nCNN_MODEL_PATH='../input/shopee-models/shopee34_119.pth'\n\nIMG_DIST_THRESHOLD = 0.7\nIMG_CHUNK = 1024*4\n\nNUM_WORKERS=2\n\nimport os\nif EXT_PATH:\n    os.sys.path.insert(0, EXT_PATH)","metadata":{"_uuid":"4e49b41d-0ca8-4c97-8615-1ddc6c2154dc","_cell_guid":"72548e1b-62e6-4424-bab9-25559709d7bb","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n## import","metadata":{"_uuid":"3e195816-a701-4433-a1fe-1a08ae108742","_cell_guid":"d3ed892a-1372-40b4-bc62-41d9c873fad0","trusted":true}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm.notebook import tqdm\n# from tqdm import tqdm\nimport torch\nfrom sklearn.feature_extraction.text import TfidfVectorizer","metadata":{"_uuid":"22e1b36b-c049-4896-aeed-3f39b4bc9e12","_cell_guid":"d4206f21-82f7-442b-b034-d5193c26329b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n## 配置","metadata":{"_uuid":"ae260b15-8e85-4090-9cdd-f5a0fe655960","_cell_guid":"56d309d7-4b71-43c1-a9f8-f28b0e83b8c8","trusted":true}},{"cell_type":"code","source":"# 计算交叉验证\nCOMPUTE_CV = True\nHAS_CUDA = torch.cuda.is_available()\nDEVICE = 'cuda' if HAS_CUDA else 'cpu'\nif COMPUTE_CV:\n    print('this submission notebook will compute CV score, but commit notebook will not')\nelse:\n    print('this submission notebook will not compute CV score')\n\ntest = pd.read_csv(DATA_PATH + 'test.csv')\nif len(test)>3: COMPUTE_CV = False\n\nprint('COMPUTE_CV:', COMPUTE_CV)","metadata":{"_uuid":"615dafff-c35c-496e-9902-da2c7fc5cf86","_cell_guid":"40e8b9a3-16c9-4839-a624-cdcc54621b69","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n## utils","metadata":{"_uuid":"d82405a6-2ab3-484f-aa28-7a9aa9e12edd","_cell_guid":"85bd19e6-ec6b-48a0-a950-2507d7c486be","trusted":true}},{"cell_type":"code","source":"def getMetric(col_name):\n    \"\"\"计算f1_score\n\n    Args:\n        col_name: 预测列\n    \"\"\"\n    def f1score(row):\n        # f1 = 2tp/(tp+fn+tp+fp) = 2tp/(len(target)+len(predict))\n        n = len( np.intersect1d(row.target,row[col_name]) )\n        return 2*n / (len(row.target)+len(row[col_name]))\n    return f1score","metadata":{"_uuid":"a2733d3c-61c2-4218-b1a2-075aec6af553","_cell_guid":"45647399-8bbb-4b1a-ba55-222ff3d0c0ed","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n## 加载数据","metadata":{"_uuid":"54f5b08c-4400-4b1c-9ba7-00122fd3bf38","_cell_guid":"dc54a508-4c97-4a0d-bc0b-232df3c78a17","trusted":true}},{"cell_type":"code","source":"if COMPUTE_CV:\n    train = pd.read_csv(DATA_PATH + 'train.csv')\n    train['image'] = DATA_PATH + 'train_images/' + train['image']\n    tmp = train.groupby('label_group').posting_id.agg('unique').to_dict()\n    # target: [pid1, pid2, ...]\n    train['target'] = train.label_group.map(tmp)\nelse:\n    train = pd.read_csv(DATA_PATH + 'test.csv')\n    train['image'] = DATA_PATH + 'test_images/' + train['image']\n    \nprint('train shape is', train.shape )\ntrain.head()","metadata":{"_uuid":"0da311dc-1939-4fbb-bbd9-a9490a5abf29","_cell_guid":"30896235-fbf9-4b31-abb7-9eb1bdf1bc70","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n## image hash 特征","metadata":{"_uuid":"1616088b-1949-49be-a31e-02aaffb75354","_cell_guid":"83d54f8a-c9f6-4c54-8c54-6224da13cf8b","trusted":true}},{"cell_type":"code","source":"tmp = train.groupby('image_phash').posting_id.agg('unique').to_dict()\ntrain['oof_hash'] = train.image_phash.map(tmp)","metadata":{"_uuid":"9d090572-e541-4ce3-92ab-04796d584d72","_cell_guid":"ae6a3755-0cbb-4b4f-8e6f-19c43535f998","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if COMPUTE_CV:\n    train['f1'] = train.apply(getMetric('oof_hash'),axis=1)\n    print('CV score for baseline =',train.f1.mean())","metadata":{"_uuid":"eb51ab1b-3605-4397-98b1-dc5f2a9d49a6","_cell_guid":"84eef99a-d86d-4775-9af5-b47a826d538c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n## image CNN 特征","metadata":{"_uuid":"e3296dff-d0d4-4092-9d45-c035a91d3d86","_cell_guid":"9c407edb-3f74-4ea8-9ab5-c255d65483a0","trusted":true}},{"cell_type":"code","source":"# 拷贝预训练模型\nimport os\nimport shutil\npretrained_pytorch_models = r'../input/pretrained-pytorch-models/'\nif os.path.isdir(pretrained_pytorch_models):    \n    pretrained_dir = f'{torch.hub.get_dir()}/checkpoints/'\n    os.makedirs(pretrained_dir, exist_ok=True)\n    shutil.copy(os.path.join(pretrained_pytorch_models, 'resnet18-5c106cde.pth'), pretrained_dir)","metadata":{"_uuid":"c4bb80f2-54c7-4e25-806f-e27d3e25ae24","_cell_guid":"01e83c78-5780-420d-a090-b24490e41463","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n### Models","metadata":{"_uuid":"f3486052-5662-437e-9c5e-74e0bfbf2234","_cell_guid":"4b951998-13b1-4ebc-a158-6209931829e8","trusted":true}},{"cell_type":"code","source":"\nimport torch\ntorch.manual_seed(0)\ntorch.backends.cudnn.deterministic = False\ntorch.backends.cudnn.benchmark = True\nfrom torch.nn import functional as F\nfrom torch.nn import DataParallel\n\nimport torch\nimport torch.nn as nn\n\nclass ChannelAttention(nn.Module):\n    def __init__(self, in_planes, ratio=16):\n        super(ChannelAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n\n        self.fc1   = nn.Conv2d(in_planes, in_planes // 16, 1, bias=False)\n        self.relu1 = nn.ReLU()\n        self.fc2   = nn.Conv2d(in_planes // 16, in_planes, 1, bias=False)\n\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n        out = avg_out + max_out\n        return self.sigmoid(out)\n\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):\n        super(SpatialAttention, self).__init__()\n\n        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n        padding = 3 if kernel_size == 7 else 1\n\n        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        x = torch.cat([avg_out, max_out], dim=1)\n        x = self.conv1(x)\n        return self.sigmoid(x)\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\nclass BasicBlockShopee(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, use_se=True):\n        super(BasicBlockShopee, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.use_se = use_se\n        self.ca = ChannelAttention(planes)\n        self.sa = SpatialAttention()\n\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.use_se:            \n            out = self.ca(out) * out\n            out = self.sa(out) * out\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\nclass ResNetShopee(nn.Module):\n    def __init__(self, block, layers, use_se=True):\n        self.inplanes = 64\n        self.use_se = use_se\n        super(ResNetShopee, self).__init__()\n        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.prelu = nn.PReLU()\n        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.bn4 = nn.BatchNorm2d(512)\n        self.dropout = nn.Dropout()\n        self.fc5 = nn.Linear(512 * 8 * 8, 512)\n        self.bn5 = nn.BatchNorm1d(512)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, use_se=self.use_se))\n        self.inplanes = planes\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, use_se=self.use_se))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.prelu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.bn4(x)\n        x = self.dropout(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc5(x)\n        x = self.bn5(x)\n\n        return x\n\ndef resnet_shopee18(use_se=True, **kwargs):\n    model = ResNetShopee(BasicBlockShopee, [2, 2, 2, 2], use_se=use_se, **kwargs)\n    return model\n\ndef resnet_shopee34(use_se=True, **kwargs):\n    model = ResNetShopee(BasicBlockShopee, [3, 4, 6, 3], use_se=use_se, **kwargs)\n    return model\n\ndef get_model(model, model_path=None, device='cuda', use_se=True) -> nn.Module:\n    if model == 'resnet18':\n        model = resnet_shopee18(use_se=use_se)\n    elif model == 'resnet34':\n        model = resnet_shopee34(use_se=use_se)\n    else:\n        raise NotImplemented\n    model = DataParallel(model)\n    model.load_state_dict(torch.load(model_path, map_location=device))\n    model = model.to(device)\n    return model","metadata":{"_uuid":"ce5e80d1-8c7c-4488-a746-fd15c33d1cb7","_cell_guid":"905a3bee-8ee1-4317-b15c-8c923449454f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n### Datasets","metadata":{"_uuid":"65582d01-9641-4098-a790-059884f340f5","_cell_guid":"1dc3bd8e-5fcc-42d8-af2a-255fee5fb44f","trusted":true}},{"cell_type":"code","source":"from torch.utils.data.dataset import Dataset\nfrom PIL import Image\nimport torch\nimport numpy as np\nfrom torchvision import transforms as T\nimport pandas as pd\n\nclass ShopeeImageDataset(Dataset):\n\n    def __init__(self, imgs, input_shape=(1, 128, 128)):\n        self.input_shape = input_shape\n        # columns: 'posting_id', 'image', 'image_phash', 'title', 'label_group'\n\n        self.imgs = imgs\n        self.transforms = T.Compose([\n            # T.CenterCrop(self.input_shape[1:]),\n            T.Resize(self.input_shape[1:]),\n            T.ToTensor(),\n            T.Normalize(mean=[0.5], std=[0.5])\n        ])\n\n    def __getitem__(self, index):\n        img_path = self.imgs[index]\n        data = Image.open(img_path)\n        data = data.convert('L')\n        data = self.transforms(data)\n        return data.float()\n\n    def __len__(self):\n        return len(self.imgs)","metadata":{"_uuid":"27b5fa97-16b0-41ec-bec3-6146be186bb5","_cell_guid":"081399fc-f4a6-4a29-aee5-a40d114fe7e3","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 加载数据集","metadata":{"_uuid":"c4173f2b-f370-47f3-8ca2-fb185f59fc17","_cell_guid":"550d77d5-29e5-446e-a43f-a27d108221b5","trusted":true}},{"cell_type":"code","source":"imagedataset = ShopeeImageDataset(train['image'].values)\n\nimageloader = torch.utils.data.DataLoader(\n    imagedataset,\n    batch_size=10, shuffle=False,  num_workers=NUM_WORKERS,drop_last=False,\n)","metadata":{"_uuid":"ffb32e7a-7d58-49da-aba3-9534f35a667f","_cell_guid":"870b1636-4f06-4bad-8970-2c1d67036087","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 加载模型","metadata":{"_uuid":"e97dcbce-2e83-483f-9be2-7b5ce8f890a5","_cell_guid":"bcc53677-a737-483a-9b61-378730ed194e","trusted":true}},{"cell_type":"code","source":"# backbone, model_path=None, device='cuda', use_se=False\n\nprint('CNN_MODEL:', CNN_MODEL)\nprint('CNN_MODEL_PATH:', CNN_MODEL_PATH)\n\nimgmodel = get_model(CNN_MODEL, os.path.join(CNN_MODEL_PATH), device=DEVICE)\nimgmodel = imgmodel.to(DEVICE)","metadata":{"_uuid":"4b2d189e-0d1c-4015-a356-7cf78d6e38c0","_cell_guid":"b0da3193-c58d-4e7e-8fad-7608bc226c37","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 计算 image CNN 特征","metadata":{"_uuid":"9919d0fc-3190-471f-8379-409cbecdf38d","_cell_guid":"375fa225-c3a9-4f9c-98a4-9f2d3129fe42","trusted":true}},{"cell_type":"code","source":"imgmodel.eval()\nimagefeat = []\nwith torch.no_grad():\n    for data in tqdm(imageloader):\n        data = data.to(DEVICE)\n        feat = imgmodel(data)\n        feat = feat.reshape(feat.shape[0], feat.shape[1])\n        # feat = feat.data.cpu().numpy()\n        \n        imagefeat.append(feat)\n\nimagefeat = torch.cat(imagefeat)\nimagefeat = F.normalize(imagefeat)\n\nprint('img embeddings shape',imagefeat.shape)\nprint('img embeddings device',imagefeat.device)","metadata":{"_uuid":"ffd84cd8-5595-4356-a5c4-7ddd9159a568","_cell_guid":"3e987723-2142-4f9a-8b55-5ce928198dfa","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 计算 image CNN 预测结果","metadata":{"_uuid":"4c275402-2d14-4581-a4c4-2419ab78c5ea","_cell_guid":"c25780bd-641e-4128-a4b4-d1ef7c58f937","trusted":true}},{"cell_type":"code","source":"preds = []\nCHUNK = IMG_CHUNK\n\nprint('Finding similar images...')\nCTS = len(imagefeat)//CHUNK\nif len(imagefeat)%CHUNK!=0: CTS += 1\nfor j in range( CTS ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b, len(imagefeat))\n    print('chunk',a,'to',b)\n    \n    distances = torch.matmul(imagefeat[a:b], imagefeat.T)\n    \n    for k in range(b-a):\n        # dists, IDX = torch.topk(distances[k,], limit_count)\n        # IDX = IDX[dists > IMG_DIST_THRESHOLD]\n\n        IDX = torch.where(distances[k,]>IMG_DIST_THRESHOLD)[0]\n        top_idx = torch.topk(distances[k,][IDX], min(len(IDX), 51))[1]\n        IDX = IDX[top_idx]\n\n        # IDX = torch.where(distances[k,]>IMG_DIST_THRESHOLD)[0]\n        o = train.iloc[IDX.data.cpu().numpy()].posting_id.values\n        preds.append(o)\n\n    del distances","metadata":{"_uuid":"7c87d1aa-6552-4d32-965d-14c92ab651e9","_cell_guid":"f79bc51a-520f-4ed0-b4f0-25e2f5a3cfe5","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del imgmodel, imagefeat\ntrain['oof_cnn'] = preds\nif COMPUTE_CV:\n    train['f1'] = train.apply(getMetric('oof_cnn'),axis=1)\n    print('CV score for baseline =',train.f1.mean())","metadata":{"_uuid":"7e7625eb-154e-40b3-9a84-2b1ade2d10bd","_cell_guid":"ab650ce4-9867-4a2a-af3a-c26d02822823","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n## title TFIDF","metadata":{"_uuid":"7f87e04d-fa5c-4210-a0bc-6d8e86df76e8","_cell_guid":"4122537f-7f53-4350-8853-6dda8df4e988","trusted":true}},{"cell_type":"code","source":"model = TfidfVectorizer(stop_words=None, binary=True, max_features=25000)\ntext_embeddings = model.fit_transform(train.title).toarray()\n\ntext_embeddings = torch.from_numpy(text_embeddings).to(DEVICE)\nprint('text embeddings shape',text_embeddings.shape)\nprint('text embeddings device',text_embeddings.device)","metadata":{"_uuid":"b63ff575-214e-4faa-bd4b-e19203850300","_cell_guid":"d2480071-5968-4d82-9fe4-d8d48a44f4ef","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = []\nCHUNK = 1024*4\n\nprint('Finding similar titles...')\nCTS = len(text_embeddings)//CHUNK\nif len(text_embeddings)%CHUNK!=0: CTS += 1\nfor j in range( CTS ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(text_embeddings))\n    print('chunk',a,'to',b)\n    \n    # COSINE SIMILARITY DISTANCE\n    # cts = cupy.matmul(text_embeddings, text_embeddings[a:b].T).T\n    cts = torch.matmul(text_embeddings[a:b], text_embeddings.T)\n\n    for k in range(b-a):\n        # dists, IDX = torch.topk(cts[k,], limit_count)\n        # IDX = IDX[dists > 0.7]\n        IDX = torch.where(cts[k,]>0.7)[0]\n        top_idx = torch.topk(cts[k,][IDX], min(len(IDX), 51))[1]\n        IDX = IDX[top_idx]\n\n        # IDX = np.where(cts[k,]>0.7)[0]\n        # IDX = cupy.where(cts[k,]>0.7)[0]\n        o = train.iloc[IDX.data.cpu().numpy()].posting_id.values\n        preds.append(o)\n    del cts","metadata":{"_uuid":"2602afe2-abdc-4521-ad2b-461b85e00e91","_cell_guid":"9524cbd4-3d26-4e49-9433-e80c4547a049","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del model, text_embeddings\ntrain['oof_text'] = preds\n\nif COMPUTE_CV:\n    train['f1'] = train.apply(getMetric('oof_text'),axis=1)\n    print('CV score for baseline =',train.f1.mean())","metadata":{"_uuid":"f076b99f-d9fc-4fb1-88cd-7096267b3260","_cell_guid":"f994f38f-dd1c-43a0-8c1f-66b85d0db519","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n## 组合所有特征","metadata":{"_uuid":"b89aebd5-22e7-417c-b78c-1b8c68bb6559","_cell_guid":"6d10b02c-489b-4157-9480-625b68b0835f","trusted":true}},{"cell_type":"code","source":"def combine_for_sub(row):\n    x = np.concatenate([row.oof_text,row.oof_cnn, row.oof_hash])\n    return ' '.join( np.unique(x) )\n\ndef combine_for_cv(row):\n    x = np.concatenate([row.oof_text,row.oof_cnn, row.oof_hash])\n    return np.unique(x)","metadata":{"_uuid":"26da8dec-1c07-4bb7-a1ae-43e7011b1a9f","_cell_guid":"b3007228-249f-41ba-8603-245e36a6f0b7","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if COMPUTE_CV:\n    tmp = train.groupby('label_group').posting_id.agg('unique').to_dict()\n    train['target'] = train.label_group.map(tmp)\n    train['oof'] = train.apply(combine_for_cv,axis=1)\n    train['f1'] = train.apply(getMetric('oof'),axis=1)\n    print('CV Score =', train.f1.mean() )\n\ntrain['matches'] = train.apply(combine_for_sub,axis=1)","metadata":{"_uuid":"e0043bb2-11d2-453d-959a-8b50776fa377","_cell_guid":"0fe0e556-3c3b-4fb6-abc4-4130ff4d4cb3","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[['posting_id','matches']].to_csv('submission.csv',index=False)\nsub = pd.read_csv('submission.csv')\nsub.head()","metadata":{"_uuid":"85956b5d-3a01-4ece-841e-e8a5ca608e46","_cell_guid":"7ba06b47-9d64-44bc-8281-e02bcfe2652d","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}