{"cells":[{"metadata":{},"cell_type":"markdown","source":"# EfficientNet Image Embeddings + DBSCAN Baseline [CV 0.658]\n\nUsing EfficientNet image embeddings feature extraction from this [notebook](https://www.kaggle.com/cdeotte/part-2-rapids-tfidfvectorizer-cv-0-700), we cluster the embeddings with [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) and match images / postings within the same cluster."},{"metadata":{},"cell_type":"markdown","source":"# Load Libraries & Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport gc\nimport pickle\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nfrom tqdm.notebook import tqdm\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MaxAbsScaler\nfrom sklearn.cluster import DBSCAN\nfrom sklearn import metrics\n\nimport tensorflow as tf\nfrom tensorflow.keras.applications import EfficientNetB0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dir = '../input/shopee-product-matching/'\ntest = pd.read_csv(data_dir + 'test.csv')\n\nif len(test) != 3:\n    SUBMISSION = True\n    train = test.copy()\n    BASE = '../input/shopee-product-matching/test_images/'\nelse:\n    SUBMISSION = False\n    train = pd.read_csv(data_dir + 'train.csv')\n    BASE = '../input/shopee-product-matching/train_images/'\n\nprint(train.shape)\ntrain.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# F1 Score & pHash Baseline\nsource: https://www.kaggle.com/cdeotte/part-2-rapids-tfidfvectorizer-cv-0-700/"},{"metadata":{"trusted":true},"cell_type":"code","source":"def getMetric(col):\n    def f1score(row):\n        n = len(np.intersect1d(row.target, row[col]))\n        return 2 * n / (len(row.target) + len(row[col]))\n    return f1score\n\ntmp = train.groupby('image_phash')['posting_id'].agg('unique').to_dict()\ntrain['matches_phash'] = train['image_phash'].map(tmp)\n\nif not SUBMISSION:\n    tmp = train.groupby('label_group')['posting_id'].agg('unique').to_dict()\n    train['target'] = train['label_group'].map(tmp)\n\n    train['f1_phash'] = train.apply(getMetric('matches_phash'), axis=1)\n    print('Train F1 Score - method:pHash =', train['f1_phash'].mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Image Embeddings via EfficientNet\nsource: https://www.kaggle.com/cdeotte/part-2-rapids-tfidfvectorizer-cv-0-700/"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"class DataGenerator(tf.keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, df, img_size=256, batch_size=32, path=''): \n        self.df = df\n        self.img_size = img_size\n        self.batch_size = batch_size\n        self.path = path\n        self.indexes = np.arange( len(self.df) )\n        \n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        ct = len(self.df) // self.batch_size\n        ct += int(( (len(self.df)) % self.batch_size)!=0)\n        return ct\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        X = self.__data_generation(indexes)\n        return X\n            \n    def __data_generation(self, indexes):\n        'Generates data containing batch_size samples' \n        X = np.zeros((len(indexes),self.img_size,self.img_size,3),dtype='float32')\n        df = self.df.iloc[indexes]\n        for i,(index,row) in enumerate(df.iterrows()):\n            img = cv2.imread(self.path+row.image)\n            X[i,] = cv2.resize(img,(self.img_size,self.img_size)) #/128.0 - 1.0\n        return X","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"WGT = '../input/effnetb0/efficientnetb0_notop.h5'\nmodel = EfficientNetB0(weights=WGT, include_top=False, pooling='avg', input_shape=None)\n\nembeds = []\nCHUNK = 1024*4\n\nprint('Computing image embeddings...')\nCTS = len(train)//CHUNK\nif len(train)%CHUNK!=0: CTS += 1\nfor i,j in enumerate( range( CTS ) ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(train))\n    print('chunk',a,'to',b)\n    \n    train_gen = DataGenerator(train.iloc[a:b], batch_size=32, path=BASE)\n    image_embeddings = model.predict(train_gen,verbose=1,use_multiprocessing=True, workers=4)\n    embeds.append(image_embeddings)\n    \ndel model\n_ = gc.collect()\nimage_embeddings = np.concatenate(embeds)\n\n# with open('../working/ShopeeEffNetEmbeddings.pkl', 'wb') as handle:\n    # pickle.dump(image_embeddings, handle)\n\nprint('image embeddings shape',image_embeddings.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DBSCAN Clustering"},{"metadata":{},"cell_type":"markdown","source":"### Functions: fit, match, and combine"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# source: https://www.kaggle.com/cdeotte/part-2-rapids-tfidfvectorizer-cv-0-700/\n\ndef dbscan_fit_match(train_sample, embeddings_sample, n_dims=200, eps=2, min_samp=2, show_vc=True, verbose=True, metric='euclidean', sub=False):\n\n    use_train = train_sample.copy()\n    use_image_embeddings = embeddings_sample.copy()\n    \n    if verbose:\n        print('fitting dbscan using {} samples and params: n_dims={}, eps={}'.format(len(use_train), n_dims, eps))\n\n    # fit dbscan\n    db = DBSCAN(eps=eps, \n                min_samples=min_samp, \n                metric=metric, \n                n_jobs=-1).fit(use_image_embeddings[:, :n_dims])\n    labels = db.labels_\n    use_train['clusters'] = labels\n\n    # use cluster labels to match items & compute, add their own posting_id for unclustered items\n    if show_vc:\n        display(use_train['clusters'].value_counts())\n    clustered = (use_train['clusters'] != -1)\n    tmp = use_train.loc[clustered].groupby('clusters')['posting_id'].agg('unique').to_dict()\n    tmp[-1] = []\n    for key in tmp:\n        if len(tmp[key]) > 50:\n            tmp[key] = tmp[key][:50]  # dirty solution to keep cluster sizes up to 50 only\n    use_train['matches_dbscan'] = use_train['clusters'].map(tmp)\n    # add self if not yet in matches\n    use_train['matches_dbscan'] = use_train.apply(match_self, axis=1)\n    \n    if not sub:\n        use_train['f1_dbscan'] = use_train.apply(getMetric('matches_dbscan'), axis=1)\n        return db, use_train['clusters'], use_train['matches_dbscan'], use_train['f1_dbscan']\n    else:\n        return db, use_train['clusters'], use_train['matches_dbscan'], None\n\ndef match_self(row):\n    if row['posting_id'] not in row['matches_dbscan']:\n        return [row['posting_id']] + row['matches_dbscan']\n    else:\n        return row['matches_dbscan']\n\ndef combine_for_cv(row, match_cols):\n    x = np.concatenate([row[col] for col in match_cols])\n    return np.unique(x)\n\ndef combine_for_sub(row, match_cols):\n    x = np.concatenate([row[col] for col in match_cols])\n    return ' '.join(np.unique(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Optimize eps param on euclidean distance"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"if not SUBMISSION:\n    # use few samples for selecting eps for faster runtime\n    sample_labels = pd.Series(train['label_group'].unique()).sample(frac=0.1, random_state=0)\n    train_sample = train.loc[train['label_group'].isin(sample_labels)].copy()\n    image_embeddings_sample = image_embeddings[train_sample.index].copy()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"if not SUBMISSION:\n    n_dims = 1280\n    eps_range = np.arange(1, 11, 1)\n    min_samp = 2\n    \n    opt_matrix = []\n    for eps in tqdm(eps_range, total=len(eps_range)):\n        print('fitting dbscan w/ eps={}...'.format(round(eps, 3)))\n        db, clusters, train_sample['matches_dbscan'], train_sample['f1_dbscan'] = dbscan_fit_match(train_sample, image_embeddings_sample, n_dims=n_dims, eps=eps, min_samp=min_samp, show_vc=False, verbose=False, metric='euclidean')\n\n        # combine dbscan matches and pHash matches\n        train_sample['matches'] = train_sample.apply(combine_for_cv, \n                                                     axis=1, \n                                                     match_cols=['matches_phash', 'matches_dbscan'])\n        train_sample['f1_combined'] = train_sample.apply(getMetric('matches'),axis=1)\n        opt_row = [n_dims,\n                   eps,\n                   clusters.value_counts(),\n                   train_sample['f1_phash'].mean(), \n                   train_sample['f1_dbscan'].mean(), \n                   train_sample['f1_combined'].mean()]\n        opt_matrix.append(opt_row)\n        \n    opt_df = pd.DataFrame(opt_matrix, columns=['n_dims', 'eps', 'counts', 'f1_phash', 'f1_dbscan', 'f1_combined'])\n    display(opt_df[['n_dims', 'eps', 'f1_phash', 'f1_dbscan', 'f1_combined']])\n    \n    print('best dbscan f1 score = {}'.format(opt_df['f1_dbscan'].max()))\n    display(opt_df.sort_values(by='f1_dbscan', ascending=False)['counts'].iloc[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Optimize eps param on cosine distance"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"if not SUBMISSION:\n    # first attempt was np.arange(0.5, 1, 0.5) then used finer ranges\n    n_dims=1280\n    eps_range = np.arange(0.15, 0.26, 0.01)\n    min_samp= 2\n\n    opt_matrix = []\n\n    for eps in tqdm(eps_range, total=len(eps_range)):\n        print('fitting dbscan w/ eps={}...'.format(round(eps, 3)))\n        db, clusters, train_sample['matches_dbscan'], train_sample['f1_dbscan'] = dbscan_fit_match(train_sample, image_embeddings_sample, n_dims=n_dims, eps=eps, min_samp=min_samp, show_vc=False, verbose=False, metric='cosine')\n\n        # combine dbscan matches and pHash matches\n        train_sample['matches'] = train_sample.apply(combine_for_cv, \n                                                     axis=1, \n                                                     match_cols=['matches_phash', 'matches_dbscan'])\n        train_sample['f1_combined'] = train_sample.apply(getMetric('matches'),axis=1)\n        opt_row = [n_dims,\n                   eps,\n                   clusters.value_counts(),\n                   train_sample['f1_phash'].mean(), \n                   train_sample['f1_dbscan'].mean(), \n                   train_sample['f1_combined'].mean()]\n        opt_matrix.append(opt_row)\n\n    opt_df = pd.DataFrame(opt_matrix, columns=['n_dims', 'eps', 'counts', 'f1_phash', 'f1_dbscan', 'f1_combined'])\n    display(opt_df[['n_dims', 'eps', 'f1_phash', 'f1_dbscan', 'f1_combined']])\n\n    print('best dbscan f1 score = {}'.format(opt_df['f1_dbscan'].max()))\n    display(opt_df.sort_values(by='f1_dbscan', ascending=False)['counts'].iloc[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fit on whole train set (~18 mins run time)\n- time complexity seems $O(n^{2})$, estimating 72 mins run time in test\n- using eps=0.15 because performance dropped on whole train sample on best eps=0.22 from above"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndb, clusters, train['matches_dbscan'], f1_dbscan = dbscan_fit_match(train, image_embeddings, n_dims=1280, eps=0.15, min_samp=2, show_vc=True, verbose=True, metric='cosine', sub=SUBMISSION)\n\nif not SUBMISSION:\n    train['f1_dbscan'] = f1_dbscan\n    # combine dbscan matches and pHash matches\n    train['matches'] = train.apply(combine_for_cv, axis=1, match_cols=['matches_phash', 'matches_dbscan'])\n    train['f1_combined'] = train.apply(getMetric('matches'),axis=1)\n\n    # print F1 Score for each method\n    print('Train F1 Score - method:pHash =', train['f1_phash'].mean())\n    print('Train F1 Score - method:dbscan =', train['f1_dbscan'].mean())\n    print('Train F1 Score - combined =', train['f1_combined'].mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"matches_to_combine = ['matches_phash', 'matches_dbscan']\ntrain['matches'] = train.apply(combine_for_sub, axis=1, match_cols=matches_to_combine)\ntrain[['posting_id', 'matches']].to_csv('submission.csv',index=False)\nsub = pd.read_csv('submission.csv')\nsub.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}