{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Import Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\nimport os\nimport re\nimport numpy as np\nimport pandas as pd\nfrom tqdm.autonotebook import tqdm\nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport spacy\nimport unidecode\nimport codecs\nfrom wordcloud import WordCloud, STOPWORDS\nfrom fuzzywuzzy import fuzz\nimport cudf\nimport cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import Counter\nfrom cuml.common.sparsefuncs import csr_row_normalize_l2\nimport gc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(0)\nnlp = spacy.load('en')#, disable=[\"tagger\", \"parser\", \"ner\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Helper funtions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_bar_chart(x, y, title, rotation_angle=45):\n    plt.figure(figsize = (20, 15))\n    sns.barplot(x=x, y=y).set_title(title)\n    plt.xticks(rotation=rotation_angle)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_images(dataframe, column_name, value):\n    '''\n    Plot images using image_path, based on the column & value filter\n    '''\n    plt.figure(figsize = (30, 30))\n    value_filter = dataframe[dataframe[column_name] == value]\n    image_paths = value_filter['image_path'].to_list()\n    print(f'Total images: {len(image_paths)}')\n    posting_id = dataframe['posting_id'].to_list()\n    for i, j in enumerate(zip(image_paths, posting_id)):\n        plt.subplot(10, 10, i + 1)\n        img = cv2.cvtColor(cv2.imread(j[0]), cv2.COLOR_BGR2RGB)\n        plt.title(j[1])\n        plt.axis(\"off\")\n        plt.tight_layout()\n        plt.imshow(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_matched_images(images_path, posting_id):\n    plt.figure(figsize = (50, 50))\n    for i, j in enumerate(zip(images_path, posting_id)):\n        plt.subplot(10, 10, i + 1)\n        img = cv2.cvtColor(cv2.imread(j[0]), cv2.COLOR_BGR2RGB)\n        plt.title(j[1])\n        plt.axis(\"off\")\n        plt.tight_layout()\n        plt.imshow(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_images_by_label_group(label):\n    plt.figure(figsize = (30, 30))\n    label_filter = train_df[train_df['label_group'] == label]\n    image_paths = label_filter['image_path'].to_list()\n    print(f'Total images: {len(image_paths)}')\n    posting_id = label_filter['posting_id'].to_list()\n    for i, j in enumerate(zip(image_paths, posting_id)):\n        plt.subplot(10, 10, i + 1)\n        img = cv2.cvtColor(cv2.imread(j[0]), cv2.COLOR_BGR2RGB)\n        plt.title(j[1])\n        plt.axis(\"off\")\n        plt.tight_layout()\n        plt.imshow(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_images_by_phash(image_phash):\n    '''\n    Plots image by phash value from train_df dataframe\n    '''\n    plt.figure(figsize = (30, 30))\n    phash_filter = train_df[train_df['image_phash'] == image_phash]\n    image_paths = phash_filter['image_path'].to_list()\n    print(f'Total images: {len(image_paths)}')\n    posting_id = phash_filter['posting_id'].to_list()\n    for i, j in enumerate(zip(image_paths, posting_id)):\n        plt.subplot(10, 10, i + 1)\n        img = cv2.cvtColor(cv2.imread(j[0]), cv2.COLOR_BGR2RGB)\n        plt.title(j[1])\n        plt.axis(\"off\")\n        plt.tight_layout()\n        plt.imshow(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def hamming_distance(phash1, phash2):\n    '''\n    helper function to calculate phash similarity\n    '''\n    phash1 = bin(int(phash1, 16))[2:].zfill(64)\n    phash2 = bin(int(phash2, 16))[2:].zfill(64)\n    distance = np.sum([i != j for i, j in zip(phash1, phash2)])\n    return distance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def hamming_distance_bin(phash1, phash2):\n    '''\n    helper function to calculate phash similarity\n    '''\n    return np.sum([i != j for i, j in zip(phash1, phash2)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_record_from_df(dataframe, column_name, value):\n    '''\n    Returns records from dataframe for the given value & column\n    '''\n    return dataframe[dataframe[column_name] == value]\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cosine_similarity(string1, string2):\n    d1 = nlp(string1)\n    d2 = nlp(string2)\n    return d2.similarity(d2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_matches(posting_id, dataframe, dist_thr=10, title_thr=60):\n    '''\n    posting_id: posting_id \n    dataframe: train/test dataframe from which the phash & title can be pulled\n    dist_thr: phash distance/score threshold\n    title_thr: title score threshold from 100\n    '''\n    results = {}\n    phash_value = dataframe[dataframe['posting_id'] == posting_id].image_phash.to_list()[0]\n    title_value = dataframe[dataframe['posting_id'] == posting_id].clean_title.to_list()[0]\n    print(title_value)\n    for i in dataframe.itertuples():\n        phash_dist = hamming_distance(phash_value, i.image_phash)\n        title_score = fuzz.token_set_ratio(title_value.lower(), i.clean_title.lower())\n\n        if phash_dist <= dist_thr:\n            # print(i.posting_id, \" ::: \", i.title, phash_dist)\n            # results.append([i.posting_id, i.image_path])\n            results[i.posting_id] = i.image_path\n            continue\n        \n        if title_score > title_thr:\n            # print(i.posting_id, \" ::: \", i.title, title_score)\n            # results.append([i.posting_id, i.image_path])\n            results[i.posting_id] = i.image_path\n    return results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ProductMatch:\n    '''\n    Aggregating phash | fuzzymatch | cosine similarity\n    '''\n    def __init__(self, cudf_df, pro_df):\n        self.vectorizer = TfidfVectorizer(stop_words='english')\n        self.tfidf_matrix = self.vectorizer.fit_transform(cudf_df['clean_title'])\n        self.pro_df = pro_df\n        \n        \n    def find_phash_fuzz_match(self, posting_id, dist_thr=10, title_thr=60):\n        phash_val = self.pro_df.loc[self.pro_df['posting_id'] == posting_id].hash.to_list()[0]\n        title_val = self.pro_df.loc[self.pro_df['posting_id'] == posting_id].clean_title.to_list()[0]\n\n        self.pro_df['image_phash_score'] = self.pro_df['hash'].apply(lambda x: hamming_distance_bin(phash_val, x))\n        self.pro_df['title_score'] = self.pro_df['clean_title'].apply(lambda x: fuzz.token_set_ratio(title_val, x))\n        self.pro_df.sort_values(by='title_score', ascending=False, inplace=True)\n        i_score = self.pro_df.loc[self.pro_df['image_phash_score'] <= dist_thr]\n        t_score = self.pro_df.loc[self.pro_df['title_score'] > title_thr]\n\n        self.fuz_ph = {**dict(zip(i_score.posting_id.to_list()[:50], i_score.image_path.to_list()[:50])), **dict(zip(\n            t_score.posting_id.to_list()[:50], t_score.image_path.to_list()[:50]))}\n\n        return self.fuz_ph\n    \n    \n    # Ref: https://medium.com/rapids-ai/natural-language-processing-text-preprocessing-and-vectorizing-at-rocking-speed-with-rapids-cuml-74b8d751812e\n    def efficient_csr_cosine_similarity(self, query, matrix_normalized=False):\n        query = csr_row_normalize_l2(query, inplace=False)\n        if not matrix_normalized:\n            self.tfidf_matrix = csr_row_normalize_l2(self.tfidf_matrix, inplace=False)\n        return self.tfidf_matrix.dot(query.T)\n\n    def cos_match(self, df, query, cos_thr=0.2, top_n=50):\n        query = self.pro_df.loc[self.pro_df['posting_id'] == query].clean_title.to_list()[0]\n        query_vec = self.vectorizer.transform(cudf.Series([query]))\n        similarities = self.efficient_csr_cosine_similarity(query_vec, matrix_normalized=True)\n        similarities = similarities.todense().reshape(-1)\n        best_idx = similarities.argsort()[-top_n:][::-1]\n        op_df = cudf.DataFrame({\n            'posting_id': df['posting_id'].iloc[best_idx],\n            # 'title': df['clean_title'].iloc[best_idx],\n            'image_path': df['image_path'].iloc[best_idx],\n            'similarity': similarities[best_idx]\n        })\n        cos_df = op_df.to_pandas()\n        cos_df = cos_df[~cos_df['posting_id'].isin([list(self.fuz_ph.keys())])]\n        cos_df = cos_df.loc[cos_df['similarity'] > cos_thr]\n        cos_df = dict(zip(cos_df.posting_id.to_list()[:50 - len(self.fuz_ph.keys())], cos_df.image_path.to_list()[:50 - len(self.fuz_ph.keys())]))\n        return cos_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ref: https://medium.com/rapids-ai/natural-language-processing-text-preprocessing-and-vectorizing-at-rocking-speed-with-rapids-cuml-74b8d751812e\n\ndef efficient_csr_cosine_similarity(query, tfidf_matrix, matrix_normalized=False):\n    query = csr_row_normalize_l2(query, inplace=False)\n    if not matrix_normalized:\n        tfidf_matrix = csr_row_normalize_l2(tfidf_matrix, inplace=False)\n    return tfidf_matrix.dot(query.T)\n\ndef product_match(df, query, vectorizer, tfidf_matrix, top_n=50):\n    print(f\"Product match: {query}\")\n    query_vec = vectorizer.transform(cudf.Series([query]))\n    similarities = efficient_csr_cosine_similarity(query_vec, tfidf_matrix, matrix_normalized=True)\n    similarities = similarities.todense().reshape(-1)\n    best_idx = similarities.argsort()[-top_n:][::-1]\n    op_df = cudf.DataFrame({\n        'posting_id': df['posting_id'].iloc[best_idx],\n        'title': df['clean_title'].iloc[best_idx],\n        'image_path': df['image_path'].iloc[best_idx],\n        'similarity': similarities[best_idx]\n    })\n    return op_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"digit_check = re.compile('\\d')\ndef check_alpha_num(token):\n    # check if the token id alphanumeric\n    return bool(digit_check.search(token))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def handle_consecutive_char(string):\n    # check & fix for 3 or more consecutive characters\n    return re.sub(r'(.)\\1+\\1+', r'\\1', string)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Getting started with data"},{"metadata":{"trusted":true},"cell_type":"code","source":"source_path = '../input/shopee-product-matching'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(f'{source_path}/train.csv')\ntest_df = pd.read_csv(f'{source_path}/test.csv')\nsample_submission_df = pd.read_csv(f'{source_path}/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Is there any NaN values?: {train_df.isnull().values.any()}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preparing image paths"},{"metadata":{"trusted":true},"cell_type":"code","source":"tqdm.pandas()\ntrain_df['image_path'] = train_df['image'].progress_apply(lambda x: f\"{source_path}/train_images/{x}\")\ntest_df['image_path'] = test_df['image'].progress_apply(lambda x: f\"{source_path}/test_images/{x}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tqdm.pandas()\ntrain_df['hash'] = train_df['image_phash'].progress_apply(lambda x: bin(int(x, 16))[2:].zfill(64))\ntest_df['hash'] = train_df['image_phash'].progress_apply(lambda x: bin(int(x, 16))[2:].zfill(64))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Trainset: {train_df.shape} \\nTestset: {test_df.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Label Group"},{"metadata":{},"cell_type":"markdown","source":"Label group count"},{"metadata":{"trusted":true},"cell_type":"code","source":"label_group_count = train_df.groupby(['label_group']).size().reset_index()\nlabel_group_count.columns = ['label_group', 'count']\nlabel_group_count.sort_values(by='count', ascending=False, inplace=True)\nlabel_group_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'No. of Duplicate label group: {train_df[train_df[\"label_group\"].duplicated() == True].shape[0]}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Minimum product under single label group: {label_group_count['count'].min()}\\nMaximum product under single label group: {label_group_count['count'].max()}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x, y = label_group_count['label_group'][:50], label_group_count['count'][:50]\nplot_bar_chart(x, y, title='Label Group Chart')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking the similarity of image phash that falls under same group"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[train_df['label_group'] == 509010932]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hamming_distance('eab5c295966ac368', 'efc096b0d38e98c3')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cannot rely on phash similarity, title similarity along with phash can be considered"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_images(train_df, 'label_group', 509010932)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Images"},{"metadata":{},"cell_type":"markdown","source":"Image count"},{"metadata":{"trusted":true},"cell_type":"code","source":"image_count = train_df.groupby(['image']).size().reset_index()\nimage_count.columns = ['image', 'count']\nimage_count.sort_values(by='count', ascending=False, inplace=True)\nimage_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'No. of Duplicate images: {train_df[train_df[\"image\"].duplicated() == True].shape[0]}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x, y = image_count['image'][:50], image_count['count'][:50]\nplot_bar_chart(x, y, title='Image count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check if same image have same phash"},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp_image = train_df[train_df[\"image\"].duplicated() == True]\n\nfor i in tmp_image.itertuples():\n    cnt = len(set(train_df[train_df['image'] == i.image].image_phash.to_list()))\n    if cnt != 1:\n        print(f'phash mismatch: {i}')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_images(train_df, 'label_group', 159351600)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Image phash"},{"metadata":{},"cell_type":"markdown","source":"* Perpetual hashing acts as the image fingerprint which is generated by analyzing the content of the mathematically. \n* Its a 64-bits representation. \n* We can calculate the distance between two phash using hamming distance to derive the semantics of both images. The lower the score; more they are likely to be identical. (The example is shown below)\n* It is also widely used for use-cases of copyright-infringement. "},{"metadata":{},"cell_type":"markdown","source":"[Read more about phash](https://en.wikipedia.org/wiki/Perceptual_hashing)"},{"metadata":{"trusted":true},"cell_type":"code","source":"phash_count = train_df.groupby(['image_phash']).size().reset_index()\nphash_count.columns = ['image_phash', 'count']\nphash_count.sort_values(by='count', ascending=False, inplace=True)\nphash_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_images(train_df, 'image_phash', 'e992966d4ba49761')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All images belongs to different **posting_id** and visually they are same."},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculating distance between 2 phash for similarity\ndistance = []\nfor i in phash_count['image_phash']:\n    d = hamming_distance('fad28daa2ad05595', i)\n    if d <10:\n        distance.append([i, d])\nprint(distance)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Calculating distance between 2 image phash for similarity\n* Considering fad28daa2ad05595 for reference to compare it with other hash for similarity\n* Higher score = less similar"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_images(train_df, 'image_phash', 'fad28daa2ad05595')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_images(train_df, 'image_phash', 'f2728d8b8ad055b5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_images(train_df, 'image_phash', 'fad28dab22d05595')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Here, we have visualised all the images from above mentioned phash values based on the hamming distance \n* It's visually similar except the watermark in the center part"},{"metadata":{},"cell_type":"markdown","source":"# Title"},{"metadata":{},"cell_type":"markdown","source":"Title count"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'No. of Duplicate titles: {train_df[train_df[\"title\"].duplicated() == True].shape[0]}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[train_df[\"title\"].duplicated() == True]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Handling unicode data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['title'][4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tqdm.pandas()\ntrain_df['unicode_handled_title'] = train_df['title'].progress_apply(lambda x: unidecode.unidecode(codecs.decode(x, 'unicode_escape')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['unicode_handled_title'][4]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# WordCloud"},{"metadata":{"trusted":true},"cell_type":"code","source":"title_data = ' '.join(i for i in train_df['unicode_handled_title'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud = WordCloud(width = 3000, height = 2000, random_state=1, background_color='black', colormap='Set2', collocations=False, stopwords = STOPWORDS).generate(title_data)\nplt.figure(figsize=(40, 30))\nplt.imshow(wordcloud)  \nplt.axis(\"off\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = CountVectorizer(stop_words='english')\ncount_m = vectorizer.fit_transform(train_df['unicode_handled_title'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_df = pd.DataFrame({'tokens': vectorizer.get_feature_names(), 'count': count_m.toarray().sum(axis=0).tolist()})\ncount_df.sort_values(by='count', ascending=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15, 15))\nsns.pointplot(x=count_df['tokens'][:50], y=count_df['count'][:50], linestyles=\"-\")\nplt.xlabel(\"tokens\")\nplt.ylabel(\"frequency\")\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15, 15))\nsns.pointplot(x=count_df['tokens'][-50:], y=count_df['count'][-50:], color = \"green\", linestyles=\"-\")\nplt.xlabel(\"tokens\")\nplt.ylabel(\"frequency\")\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, there is alot of numbers, punctuation, consecutive characters. Hence, cleaning this data.\n* Dropping all the alphanumeric tokens\n* Fixing consecutive characters\n* Applied regex to filter non-alphabetic content from tokens"},{"metadata":{"trusted":true},"cell_type":"code","source":"tqdm.pandas()\ntrain_df['clean_title'] = train_df['unicode_handled_title'].progress_apply(lambda x: ' '.join(handle_consecutive_char(i) for i in str(\n                re.sub('[^A-Za-z0-9]', ' ', x.lower().strip())).split() if i.strip() and not check_alpha_num(i.strip()) and not (i.strip(\n                ) == len(i.strip()) * i.strip()[0])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Product Match"},{"metadata":{},"cell_type":"markdown","source":"Earlier, we saw that phash similarity is not enough. Hence we considered the phash + title matching.\n\nPlotting matched products/posting_id from the data frame for the given posting_id. Here, the results seem pretty interesting but ofcourse there are mismatched products in the results below.\n\nIn the below test, the first image is from the source posting_id and the images in the grid are the match from other records in the data frame."},{"metadata":{},"cell_type":"markdown","source":"**The results have improved after text cleaning**"},{"metadata":{"trusted":true},"cell_type":"code","source":"result = find_matches('train_1638187876', train_df)\nplot_images(train_df, 'posting_id', 'train_1638187876')\nplot_matched_images(result.values(), result.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = find_matches('train_3193897481', train_df)\nplot_images(train_df, 'posting_id', 'train_3193897481')\nplot_matched_images(result.values(), result.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = find_matches('train_2767483557', train_df)\nplot_images(train_df, 'posting_id', 'train_2767483557')\nplot_matched_images(list(result.values())[:50], list(result.keys())[:50])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# result = find_matches('train_2928592022', train_df)\n# plot_images(train_df, 'posting_id', 'train_2928592022')\n# plot_matched_images([i[1] for i in result], [i[0] for i in result])\nresult = find_matches('train_2406599165', train_df)\nplot_images(train_df, 'posting_id', 'train_2406599165')\nplot_matched_images(result.values(), result.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = find_matches('train_4085449742', train_df)\nplot_images(train_df, 'posting_id', 'train_4085449742')\nplot_matched_images(result.values(), result.keys())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RAPIDS TfidfVectorizer cosine similarity match"},{"metadata":{},"cell_type":"markdown","source":"Here, I am trying TfidfVectorizer + cosine similarity with product titles. The results look reasonable and of course, the matched products that are returned seem correct as compare to the above approach results. And yes, there are mismatches. I need to experiment with the score threshold. "},{"metadata":{},"cell_type":"markdown","source":"***Aggregating phash, fuzzymatch, cosine similarity***"},{"metadata":{"trusted":true},"cell_type":"code","source":"cudf_df = cudf.DataFrame(train_df)\nobj = ProductMatch(cudf_df, train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def con(obj, posting_id, df, dist_thr=10, title_thr=60, cos_thr=0.2):\n    ph = obj.find_phash_fuzz_match(posting_id, dist_thr=10, title_thr=60)\n    cs = obj.cos_match(df, posting_id, cos_thr=0.2, top_n=50)\n    return {**ph, **cs}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nresult = con(obj, 'train_1638187876', cudf_df, dist_thr=10, title_thr=60, cos_thr=0.2)\nplot_matched_images(result.values(), result.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nresult = con(obj, 'train_3193897481', cudf_df, dist_thr=10, title_thr=60, cos_thr=0.2)\nplot_matched_images(result.values(), result.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = con(obj, 'train_2767483557', cudf_df, dist_thr=10, title_thr=60, cos_thr=0.2)\nplot_matched_images(result.values(), result.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = con(obj, 'train_1827962737', cudf_df, dist_thr=10, title_thr=60, cos_thr=0.2)\nplot_matched_images(result.values(), result.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = con(obj, 'train_4085449742', cudf_df, dist_thr=10, title_thr=60, cos_thr=0.2)\nplot_matched_images(result.values(), result.keys())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3 align=\"center\" style=\"background-color:green;\">WIP</h3> "},{"metadata":{},"cell_type":"markdown","source":"# Baseline"},{"metadata":{},"cell_type":"markdown","source":"This is based on the fuzzy matching + phash approach."},{"metadata":{"trusted":true},"cell_type":"code","source":"tqdm.pandas()\ntest_df['unicode_handled_title'] = test_df['title'].progress_apply(lambda x: unidecode.unidecode(codecs.decode(x, 'unicode_escape')))\ntest_df['clean_title'] = test_df['unicode_handled_title'].progress_apply(lambda x: ' '.join(handle_consecutive_char(i) for i in str(\n                re.sub('[^A-Za-z]', ' ', x.lower().strip())).split() if i.strip() and not check_alpha_num(i.strip()) and not (i.strip(\n                ) == len(i.strip()) * i.strip()[0])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cudf_test_df = cudf.DataFrame(test_df)\nobj = ProductMatch(cudf_test_df, test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tqdm.pandas()\ntest_df['matches'] = test_df['posting_id'].progress_apply(lambda x: ' '.join(con(obj, x, cudf_test_df, dist_thr=10, title_thr=60, cos_thr=0.2).keys()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_csv = pd.DataFrame({'posting_id': test_df['posting_id'].to_list(), 'matches': test_df['matches'].to_list()})\nsubmission_csv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_csv.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Please check out my another kernel: [Locality Sensitive Hashing(LSH)](https://www.kaggle.com/srcecde/shoppee-locality-sensitive-hashing-lsh-jaccard)"},{"metadata":{},"cell_type":"markdown","source":"<h3 align=\"center\" style=\"background-color:#003300;color:white;\">Thanks! More updates to come. WIP</h3> "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}