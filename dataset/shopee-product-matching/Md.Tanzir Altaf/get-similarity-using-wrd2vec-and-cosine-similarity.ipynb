{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Finding the similarity between title and product text using word2vec and cosine similarity","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\nimport numpy as np\nnp.random.seed(2018)\nfrom gensim.models import Word2Vec\nimport nltk\nnltk.download('wordnet')\nstemmer = SnowballStemmer('english')\nimport unicodedata\nfrom numpy import dot\nfrom numpy.linalg import norm\n\nimport cv2\n%matplotlib inline\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/shopee-product-matching/train.csv')\ntest_df = pd.read_csv('/kaggle/input/shopee-product-matching/test.csv')\nDATA_PATH = '/kaggle/input/shopee-product-matching/'\nprint(gensim.__version__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data preprocessing","metadata":{}},{"cell_type":"code","source":"def cleanData(dataParse):\n    data = unicodedata.normalize('NFKC', dataParse)\n    data = re.sub(r'【.*】', '', data)\n    data = re.sub(r'\\[.*\\]', '', data)\n    data = re.sub(r'「.*」', '', data)\n    data = re.sub(r'\\(.*\\)', '', data)\n    data = re.sub(r'\\<.*\\>', '', data)\n    data = re.sub(r'[※@◎].*$', '', data)\n    return data.lower() #Returns the parsed tweets\n\n\ndef lemmatize_stemming(text):\n    return stemmer.stem(WordNetLemmatizer().lemmatize(cleanData(text), pos='v'))\n\ndef preprocess(text):\n    result = []\n    for token in gensim.utils.simple_preprocess(text):\n        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n            if token == 'xxxx':\n                continue\n            result.append(lemmatize_stemming(token))\n    \n    return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed_docs = train_df['title'].map(preprocess)\nprocessed_docs =list(processed_docs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed_docs[:5] # clean document","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load model","metadata":{}},{"cell_type":"code","source":"w2v_model = Word2Vec.load(\"/kaggle/input/train-model/word2vec_model.model\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Find the similarity between two vector","metadata":{}},{"cell_type":"code","source":"# Generate the average word2vec for the each title description\n\ndef vectors_test(): #test_df\n    \n    # Creating a list for storing the vectors (description into vectors)\n    global word_embeddings_test\n    word_embeddings_test = []\n    test_processed_docs = test_df['title'].map(preprocess)\n    \n    # Reading the each book description \n    for line in test_processed_docs:\n        avgword2vec = None\n        count = 0\n        for word in line:\n            if word in w2v_model.wv:\n                count += 1\n                if avgword2vec is None:\n                    avgword2vec = w2v_model.wv[word]\n                else:\n                    avgword2vec = avgword2vec + w2v_model.wv[word]\n                \n        if avgword2vec is not None:\n            avgword2vec = avgword2vec / count\n            word_embeddings_test.append(avgword2vec)\n        else:\n            word_embeddings_test.append(np.array([0]*50, dtype='float32'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectors_test()#train_df\n# The Top 50 similar matches\ndef similarity(title):\n    \n    # finding cosine similarity for the vectors\n    cosine_similarities = cosine_similarity(word_embeddings_test, word_embeddings_test)\n\n    cntx = test_df[['title', 'posting_id']]\n    #Reverse mapping of the index\n    indices = pd.Series(test_df.index, index = test_df['title']).drop_duplicates()\n         \n    idx = indices[title]\n    sim_scores = list(enumerate(cosine_similarities[idx]))\n    sim_scores = sorted(sim_scores, key = lambda x: x[1], reverse = True)\n    uplimit = len(test_df['title']) if len(test_df['title']) < 50 else 50\n    sim_scores = sim_scores[0:uplimit]\n    post_indices = [i[0] for i in sim_scores if i[1] > .9]\n    recommend = cntx.iloc[post_indices]\n    output=[]\n    output.append(cntx.iloc[idx,1])\n    for index, row in recommend.iterrows():\n        output.append(row['posting_id'])\n\n    return ' '.join( np.unique(output))\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['matches']=test_df['title'].apply(similarity)\n    \ntest_df[['posting_id', 'matches']].to_csv('submission.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[['posting_id', 'matches']].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# keep learning and enrich your intuition.Don't forgot to upvote !! :)","metadata":{}}]}