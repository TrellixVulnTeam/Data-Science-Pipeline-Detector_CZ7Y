{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os \nimport numpy as np \nimport pandas as pd \nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport tensorflow as tf \nimport re\nimport string\nimport shutil\nshutil.copy(src=\"../input/tokenization/tokenization.py\",dst = \"./\")\nimport tokenization\nimport tensorflow_hub as hub\nimport math\nfrom tqdm import tqdm\nimport gc\nfrom tensorflow.keras.utils import Sequence\nimport cv2 as cv","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load datas:","metadata":{}},{"cell_type":"code","source":"submission = False\ntest = pd.read_csv(\"../input/shopee-product-matching/test.csv\")\nimages_path = \"../input/shopee-product-matching/test_images\"\nif len(test) <=3 :\n   test = pd.read_csv(\"../input/shopee-product-matching/train.csv\")\n   #test = pd.concat([test,test,test.iloc[:5000]])\n   images_path = \"../input/shopee-product-matching/train_images\"\n   submission = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Usefuls functions:","metadata":{}},{"cell_type":"code","source":"def max_length(text):\n    max_l = 0\n    for tx in text :\n        l = len(tx.split())\n        if l > max_l :\n            max_l = l\n    max_l = min(512,max_l)\n    return max_l","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text (title) :\n    \"\"\"This function, allows to clean title from useless characters and symbols.\n    \n    @ params :\n    title(str) : the title text that the function will clean up.\n    \n    @ returns :\n    title(str) : cleaned title\n\n    \n    \"\"\"\n    title = re.sub(r\"\\-\",\" \",title)\n    title = re.sub(r\"\\+\",\" \",title)\n    title = re.sub (r\"&\",\"and\",title)\n    title = re.sub(r\"\\|\",\" \",title)\n    title = re.sub(r\"\\\\\",\" \",title)\n    title = re.sub(r\"\\W\",\" \",title)\n    for p in string.punctuation :\n        title = re.sub(r\"f{p}\",\" \",title)\n    \n    title = re.sub(r\"\\s+\",\" \",title)\n    \n    return title","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def f1_score(y_true, y_pred):\n    y_true = y_true.apply(lambda x: set(x.split()))\n    y_pred = y_pred.apply(lambda x: set(x.split()))\n    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n    len_y_pred = y_pred.apply(lambda x: len(x)).values\n    len_y_true = y_true.apply(lambda x: len(x)).values\n    f1 = 2 * intersection / (len_y_pred + len_y_true)\n    return f1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getMetric(col):\n    def f1score(row):\n        n = len( np.intersect1d(row.target,row[col]) )\n        return 2*n / (len(row.target)+len(row[col]))\n    return f1score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def combine(row):\n    x = np.concatenate([row.pred_eff,row.pred_bert])\n   \n    return np.unique(x)\ndef combine_matches(row):\n    return \" \".join(row.pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ArcMarginProduct(tf.keras.layers.Layer):\n    '''\n    Implements large margin arc distance.\n\n    Reference:\n        https://arxiv.org/pdf/1801.07698.pdf\n        https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/\n            blob/master/src/modeling/metric_learning.py\n    '''\n    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n                 ls_eps=0.0, **kwargs):\n\n        super(ArcMarginProduct, self).__init__(**kwargs)\n\n        self.n_classes = n_classes\n        self.s = s\n        self.m = m\n        self.ls_eps = ls_eps\n        self.easy_margin = easy_margin\n        self.cos_m = tf.math.cos(m)\n        self.sin_m = tf.math.sin(m)\n        self.th = tf.math.cos(math.pi - m)\n        self.mm = tf.math.sin(math.pi - m) * m\n\n    def get_config(self):\n\n        config = super().get_config().copy()\n        config.update({\n            'n_classes': self.n_classes,\n            's': self.s,\n            'm': self.m,\n            'ls_eps': self.ls_eps,\n            'easy_margin': self.easy_margin,\n        })\n        return config\n\n    def build(self, input_shape):\n        super(ArcMarginProduct, self).build(input_shape[0])\n\n        self.W = self.add_weight(\n            name='W',\n            shape=(int(input_shape[0][-1]), self.n_classes),\n            initializer='glorot_uniform',\n            dtype='float32',\n            trainable=True,\n            regularizer=None)\n\n    def call(self, inputs):\n        X, y = inputs\n        y = tf.cast(y, dtype=tf.int32)\n        cosine = tf.matmul(\n            tf.math.l2_normalize(X, axis=1),\n            tf.math.l2_normalize(self.W, axis=0)\n        )\n        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = tf.where(cosine > 0, phi, cosine)\n        else:\n            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n        one_hot = tf.cast(\n            tf.one_hot(y, depth=self.n_classes),\n            dtype=cosine.dtype\n        )\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.s\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bert_encode(texts,tokenizer,max_len = 512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    for tx in texts :\n        token = tokenizer.tokenize(tx)\n        token = token[:max_len -2]\n        token = [\"[CLS]\"] + token + [\"[SEP]\"]\n        tokens = tokenizer.convert_tokens_to_ids(token)\n        le = len(tokens)\n        pad = max_len - le \n        tokens = tokens + [0] * pad \n        masks = [1] * le + [0] * pad \n        segments = [0] * max_len \n        all_tokens.append(tokens)\n        all_masks.append(masks)\n        all_segments.append(segments)\n    return np.array(all_tokens),np.array(all_masks),np.array(all_segments)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def BertToolsConfiguration():\n    bert_layer = hub.KerasLayer(\"../input/bert-en-uncased-l-12-h-768-a-12-1\",trainable=True)\n    vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n    do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n    tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n    max_le = max_length(test[\"cleaned_title\"].values)\n    return bert_layer,tokenizer,max_le","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def BertImplementation(bert_layer,tokenizer,max_len,N_CLASSES):\n    margin = ArcMarginProduct(\n            n_classes = N_CLASSES, \n            s = 30, \n            m = 0.5, \n            name='head/arc_margin', \n            dtype='float32'\n            )\n    inputs_ids = tf.keras.layers.Input(shape=(max_len,),dtype=tf.int32,name=\"inputs_ids\")\n    inputs_masks = tf.keras.layers.Input(shape=(max_len,),dtype=tf.int32,name=\"inputs_masks\")\n    inputs_segments = tf.keras.layers.Input(shape=(max_len,),dtype=tf.int32,name=\"inputs_segments\")\n    labels = tf.keras.layers.Input(shape=(None,),dtype=tf.int32,name=\"labels\")\n    _,sequence_output = bert_layer([inputs_ids,inputs_masks,inputs_segments])\n    clf_output = sequence_output[:,0,:]\n    clf_output = tf.keras.layers.BatchNormalization()(clf_output)\n    clf_output = tf.keras.layers.Dropout(0.4)(clf_output)\n    x = margin([clf_output,labels])\n    output = tf.keras.layers.Softmax()(x)\n    model = tf.keras.models.Model(inputs=[inputs_ids,inputs_masks,inputs_segments,labels],\\\n                                  outputs=[output])\n    model.load_weights(\"../input/bert-arc-face-training/bert_weight.h5\")\n    model = tf.keras.models.Model(inputs=model.input[:3],outputs=model.layers[-6].output)\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataGenerateur(Sequence):\n    \n    def __init__(self,df,img_size=256,batch_size = 4 ,path=images_path ):\n        self.df = df \n        self.path = path \n        self.batch_size = batch_size \n        self.img_size = img_size\n        self.indices = np.arange(len(self.df))\n    def __len__(self):\n        cls = (len(self.df))//(self.batch_size)\n        cls += int (((len(self.df)) % (self.batch_size))!=0)\n        return cls \n    def __getitem__(self,index):\n        ind = self.indices [index * self.batch_size : (index+1) * self.batch_size]\n        return self.__datagenerator(ind)\n    def __datagenerator(self,ind):\n        dff = self.df.iloc[ind]\n        images = np.zeros((len(dff),self.img_size,self.img_size,3),dtype=np.float)\n        \n        for i ,(j,row) in enumerate(dff.iterrows()):\n            img = os.path.join(self.path,row.image)\n            img = cv.imread(img)\n            img = cv.resize(img,(self.img_size,self.img_size))\n            images[i,] = img \n        return images ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def effnetImplementation(N_CLASSES) :\n    margin = ArcMarginProduct(\n            n_classes = N_CLASSES, \n            s = 30, \n            m = 0.5, \n            name='head/arc_margin', \n            dtype='float32'\n            )\n    WGT = \"../input/effnetb0/efficientnetb0_notop.h5\"\n    inp = tf.keras.layers.Input(shape=(256,256,3),dtype=tf.float32)\n    labels = tf.keras.layers.Input(shape=(None,),dtype=tf.int32,name=\"labels\")\n    effnet = tf.keras.applications.EfficientNetB0(weights=None,include_top= False,input_shape=None,\\\n                                                 pooling=\"AVG\")\n    out = effnet(inp)\n    #out = tf.keras.layers.BatchNormalization()(out)\n    #out = tf.keras.layers.Dropout(0.3)(out)\n    out = tf.keras.layers.GlobalAveragePooling2D()(out)\n    x = margin([out,labels])\n    output = tf.keras.layers.Softmax()(x)\n    model = tf.keras.models.Model(inputs=[inp,labels],outputs=[output])\n    model.load_weights(\"../input/effnetb0-arcface-training/effnet_weights.h5\")\n    \n    model = tf.keras.models.Model(inputs=model.input[0],outputs=model.layers[-4].output)\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Datas preparation :","metadata":{}},{"cell_type":"code","source":"if submission : \n   tmp = test.groupby(\"label_group\").posting_id.unique()\n   test[\"target\"] = test[\"label_group\"].map(tmp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[\"cleaned_title\"] = test[\"title\"].map(clean_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_CLASSES = test[\"label_group\"].nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(N_CLASSES)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Duplicated products based on images:","metadata":{}},{"cell_type":"code","source":"LIMIT = 1\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus :\n    try :\n       tf.config.experimental.set_virtual_device_configuration(gpus[0],\\\n                                                           [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*LIMIT)])\n       logical_gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n    \n    except RuntimeError as e :\n       print(e)\nprint('We will restrict TensorFlow to max %iGB GPU RAM'%LIMIT)\nprint('then RAPIDS can use %iGB GPU RAM'%(16-LIMIT))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"effnet_model = effnetImplementation(11014)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_embedding = []\nchunk = 4 * 1024\ncls = len(test) // chunk\ncls += int((len(test) % chunk) !=0)\nfor i in tqdm(range(cls)) :\n    a = i * chunk \n    b = (i+1) * chunk \n    b = min(b,len(test))\n    data = DataGenerateur(test.iloc[a:b])\n    image_embedding.append(effnet_model.predict(data,use_multiprocessing=True,workers = 4))\n    \ndel (effnet_model)\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_embedding = np.concatenate(image_embedding,axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from cuml.neighbors import NearestNeighbors\nimport cupy \nimage_embedding = cupy.array(image_embedding)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mm = NearestNeighbors(n_neighbors=50)\nmm.fit(image_embedding)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from cupy.linalg import norm\n#norme = norm(image_embedding,axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#norme = norme.reshape(-1,1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#normed_embedding = image_embedding / norme","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cudf \ncudf_test = cudf.DataFrame(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = []\nfor i in tqdm(range(cls)) :\n    a = i * chunk \n    b = (i+1) * chunk \n    b = min(b,len(test))\n    distances,indices = mm.kneighbors(image_embedding[a:b,])\n    for j in range(b-a) :\n           distance = distances[j,]\n           ind = cupy.where(distance < 4.5) [0]\n           ind = cupy.asnumpy(ind)\n           ind = indices[j,ind]\n           ind = cupy.asnumpy(ind)\n           pred.append(test.iloc[ind].posting_id.values)\n       \n     ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[\"pred_eff\"] = pred \nif submission : \n   test[\"f2\"] = test.apply(getMetric(\"pred_eff\"),axis=1)\n    \n   print('CV score for arc face bert  embedding text =',test.f2.mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Duplicated products based on text :","metadata":{}},{"cell_type":"code","source":"bert_layer,tokenizer,max_len = BertToolsConfiguration()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(max_len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_model = BertImplementation(bert_layer,tokenizer,82,11014)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_tokens,all_masks,all_segments = bert_encode(test[\"cleaned_title\"].values,tokenizer,\\\n                                                max_len=82)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding = []\nchunk = 1024 *4 \ncls = len(test) // chunk \ncls += int((len(test) % chunk) !=0)\nfor i in tqdm(range(cls)) :\n    a = i * chunk \n    b = (i+1) * chunk \n    b = min(b,len(test))\n    embedding.append(bert_model.predict([all_tokens[a:b,],all_masks[a:b,],all_segments[a:b,]]))\nembedding = np.concatenate(embedding,axis = 0 )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del(bert_model)\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from cuml.neighbors import NearestNeighbors\nnn = NearestNeighbors(n_neighbors=50)\nnn.fit(embedding)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cupy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = []\nchunk = 4 * 1024 \ncls = len(test) // chunk \ncls += int((len(test) % chunk)!=0)\nfor i in tqdm(range(cls)):\n    a = i * chunk\n    b = (i+1) * chunk \n    b = min(b,len(test))\n    distances,indices = nn.kneighbors(embedding[a:b,])\n    for j in range(b-a) :\n        distance = distances[j,:]\n        ind = np.where(distance < 8.5)[0]\n        ind = indices[j,ind]\n        ind = cupy.asnumpy(ind)\n        pred.append(test.iloc[ind].posting_id)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[\"pred_bert\"] = pred \nif submission :\n   test[\"f0\"] = test.apply(getMetric(\"pred_bert\"),axis=1)\n    \n   print('CV score for bert arc face prediction =',test.f0.mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### tf_idf","metadata":{}},{"cell_type":"markdown","source":"from cuml.feature_extraction.text import TfidfVectorizer\ntf_idf = TfidfVectorizer(stop_words=\"english\",max_features=25000,binary=True)","metadata":{}},{"cell_type":"markdown","source":"embedding = tf_idf.fit_transform(cudf_test[\"cleaned_title\"]).toarray()","metadata":{}},{"cell_type":"markdown","source":"from cuml.neighbors import NearestNeighbors \nnn = NearestNeighbors(n_neighbors=50)\nnn.fit(embedding)","metadata":{}},{"cell_type":"markdown","source":"pred = []\nfor i in tqdm(range(cls)) :\n    a = i * chunk \n    b = (i+1) * chunk \n    b = min(b,len(test))\n    distances , indices = nn.kneighbors(embedding[a:b,])\n    for j in range(b-a) :\n        distance = distances[j,:]\n        ind = np.where(distance < 1.0)[0]\n        proches = indices[j,ind]\n        proches = cupy.asnumpy(proches)\n        pred.append(test.iloc[proches].posting_id.values)","metadata":{}},{"cell_type":"markdown","source":"test[\"pred_tf_idf\"] = pred \nif submission :\n   test[\"f2\"] = test.apply(getMetric(\"pred_tf_idf\"),axis=1)\n    \n   print('CV score for arc face combined prediction =',test.f2.mean())","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"### tf ","metadata":{}},{"cell_type":"markdown","source":"from cuml.feature_extraction.text import CountVectorizer\ntf = CountVectorizer(stop_words=\"english\",max_features=25000,binary=True)\nembedding = tf.fit_transform(cudf_test[\"cleaned_title\"]).toarray()","metadata":{}},{"cell_type":"markdown","source":"nb = NearestNeighbors(n_neighbors=50,metric=\"cosine\")\nnb.fit(embedding)","metadata":{}},{"cell_type":"markdown","source":"pred = []\nfor i in tqdm(range(cls)) :\n    a = i * chunk \n    b = (i+1) * chunk \n    b = min(b,len(test))\n    distances , indices = nb.kneighbors(embedding[a:b,])\n    for j in range(b-a) :\n        distance = distances[j,:]\n        ind = np.where(distance < 0.4)[0]\n        proches = indices[j,ind]\n        proches = cupy.asnumpy(proches)\n        pred.append(test.iloc[proches].posting_id.values)","metadata":{}},{"cell_type":"markdown","source":"test[\"pred_tf\"] = pred \nif submission :\n   test[\"f4\"] = test.apply(getMetric(\"pred_tf\"),axis=1)\n    \n   print('CV score for arc face combined prediction =',test.f4.mean())","metadata":{}},{"cell_type":"markdown","source":"def intersect_tx(row):\n    x = np.intersect1d(row[\"pred_tf_idf\"],row[\"pred_tf\"])\n    return x \ndef concat_tx(row):\n    x = np.concatenate([row[\"pred_tf_idf\"],row[\"pred_tf\"]])\n    x = np.unique(x)\n    return x ","metadata":{}},{"cell_type":"markdown","source":"test[\"tfettf_idf\"] = test.apply(intersect_tx,axis=1)\nif submission :\n   test[\"f5\"] = test.apply(getMetric(\"tfettf_idf\"),axis=1)\n    \n   print('CV score for arc face combined prediction =',test.f5.mean())","metadata":{}},{"cell_type":"markdown","source":"test[\"tfoutf_idf\"] = test.apply(concat_tx,axis=1)\nif submission :\n   test[\"f5\"] = test.apply(getMetric(\"tfoutf_idf\"),axis=1)\n    \n   print('CV score for arc face combined prediction =',test.f5.mean())","metadata":{}},{"cell_type":"markdown","source":"# Combined prediction :","metadata":{}},{"cell_type":"code","source":"test[\"pred\"] = test.apply(combine,axis=1)\n#test[\"pred\"] = pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if submission :\n   test[\"f3\"] = test.apply(getMetric(\"pred\"),axis=1)\n    \n   print('CV score for arc face combined prediction =',test.f3.mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[\"matches\"] = test.apply(combine_matches,axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[[\"posting_id\",\"matches\"]].to_csv(\"submission.csv\",index = False)\nsub = pd.read_csv('submission.csv')\nsub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}