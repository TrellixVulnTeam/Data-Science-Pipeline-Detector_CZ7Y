{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport os \nimport random\nfrom PIL import Image\nimport cv2 as cv\nimport tensorflow as tf \nimport tensorflow_hub as hub \nimport shutil \nshutil.copy(src=\"../input/tokenization/tokenization.py\",dst=\"./\")\n! cp  -r \"../input/bert-en-uncased-l-12-h-768-a-12-1\" \"./\"\nimport tokenization\nimport re\nimport string\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport math\nfrom tensorflow.keras.utils import Sequence \nfrom tensorflow.keras.applications import EfficientNetB0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Datas :","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv(\"../input/shopee-product-matching/test.csv\")\nimages_path = \"../input/shopee-product-matching/test_images\"\nif len(test) <= 3 :\n    test = pd.read_csv(\"../input/shopee-product-matching/train.csv\")\n    images_path = \"../input/shopee-product-matching/train_images\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Useful functions :","metadata":{}},{"cell_type":"code","source":"SEED = 42","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ArcMarginProduct(tf.keras.layers.Layer):\n    '''\n    Implements large margin arc distance.\n\n    Reference:\n        https://arxiv.org/pdf/1801.07698.pdf\n        https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/\n            blob/master/src/modeling/metric_learning.py\n    '''\n    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n                 ls_eps=0.0, **kwargs):\n\n        super(ArcMarginProduct, self).__init__(**kwargs)\n\n        self.n_classes = n_classes\n        self.s = s\n        self.m = m\n        self.ls_eps = ls_eps\n        self.easy_margin = easy_margin\n        self.cos_m = tf.math.cos(m)\n        self.sin_m = tf.math.sin(m)\n        self.th = tf.math.cos(math.pi - m)\n        self.mm = tf.math.sin(math.pi - m) * m\n\n    def get_config(self):\n\n        config = super().get_config().copy()\n        config.update({\n            'n_classes': self.n_classes,\n            's': self.s,\n            'm': self.m,\n            'ls_eps': self.ls_eps,\n            'easy_margin': self.easy_margin,\n        })\n        return config\n\n    def build(self, input_shape):\n        super(ArcMarginProduct, self).build(input_shape[0])\n\n        self.W = self.add_weight(\n            name='W',\n            shape=(int(input_shape[0][-1]), self.n_classes),\n            initializer='glorot_uniform',\n            dtype='float32',\n            trainable=True,\n            regularizer=None)\n\n    def call(self, inputs):\n        X, y = inputs\n        y = tf.cast(y, dtype=tf.int32)\n        cosine = tf.matmul(\n            tf.math.l2_normalize(X, axis=1),\n            tf.math.l2_normalize(self.W, axis=0)\n        )\n        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = tf.where(cosine > 0, phi, cosine)\n        else:\n            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n        one_hot = tf.cast(\n            tf.one_hot(y, depth=self.n_classes),\n            dtype=cosine.dtype\n        )\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.s\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean(title):\n    \"\"\"This function, allows to clean title from useless characters and symbols.\n    \n    @ params :\n    title(str) : the title text that the function will clean up.\n    \n    @ returns :\n    title(str) : cleaned title\n\n    \n    \"\"\"\n    title = re.sub(r\"\\-\",\" \",title)\n    title = re.sub(r\"\\+\",\" \",title)\n    title = re.sub (r\"&\",\"and\",title)\n    title = re.sub(r\"\\|\",\" \",title)\n    title = re.sub(r\"\\\\\",\" \",title)\n    title = re.sub(r\"\\W\",\" \",title)\n    for p in string.punctuation :\n        title = re.sub(r\"f{p}\",\" \",title)\n    \n    title = re.sub(r\"\\s+\",\" \",title)\n    \n    return title","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\ndef get_lr_callback():\n    lr_start   = 0.000001\n    lr_max     = 0.000005 * BATCH_SIZE\n    lr_min     = 0.000001\n    lr_ramp_ep = 5\n    lr_sus_ep  = 0\n    lr_decay   = 0.8\n   \n    def lrfn(epoch):\n        if epoch < lr_ramp_ep:\n            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start   \n        elif epoch < lr_ramp_ep + lr_sus_ep:\n            lr = lr_max    \n        else:\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min    \n        return lr\n\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n    return lr_callback\n\ndef max_length(text):\n    max_l = 0\n    for tx in text :\n        l = len(tx.split())\n        if l > max_l :\n            max_l = l\n    max_l = min(512,max_l)\n    return max_l\ndef processing_data(df):\n    lb = LabelEncoder()\n    df[\"encoded_label_group\"] = lb.fit_transform(df[\"label_group\"])\n    N_CLASSES = df[\"encoded_label_group\"].nunique()\n    xtr,xts,ytr,yts = train_test_split(df[\"cleaned_title\"].values,df[\"encoded_label_group\"].\\\n                                       values,stratify=df[\"encoded_label_group\"].values,\\\n                                       test_size =0.33,random_state=SEED)\n    return N_CLASSES,xtr,xts,ytr,yts\ndef bert_encode(text,tokenizer,max_len=512): \n    \"\"\" This function allows to return tokens,masks and segments for series or text array\"\"\"\n    \n   \n    all_tokens = []\n    all_mask = []\n    all_segments = []\n    max_l = 0\n    \n    for tx in text:\n        tx = tokenizer.tokenize(tx)\n        tokens = tx[:max_len -2]\n        tokens = ['[CLS]'] + tokens + ['[SEP]']\n        tokens = tokenizer.convert_tokens_to_ids(tokens)\n        seq_len = len(tokens)\n        pad = max_len - seq_len\n        tokens = tokens + [0] * pad \n        mask = [1] * seq_len + [0] * pad \n        segment = [0] * max_len \n        all_tokens.append(tokens)\n        all_mask.append(mask)\n        all_segments.append(segment)\n    \n    return np.array(all_tokens),np.array(all_mask),np.array(all_segments)\n\ndef build_bert_model (bert_layer,max_len=512) :\n    \n    margin = ArcMarginProduct(\n            n_classes = N_CLASSES, \n            s = 30, \n            m = 0.5, \n            name='head/arc_margin', \n            dtype='float32'\n            )\n    inputs_ids = tf.keras.layers.Input(shape=(max_len,),dtype=tf.int32,name=\"input_ids\")\n    inputs_mask = tf.keras.layers.Input(shape=(max_len,),dtype=tf.int32,name=\"inputs_mask\")\n    inputs_segment = tf.keras.layers.Input(shape=(max_len,),dtype=tf.int32,name=\"inputs_segment\")\n    label = tf.keras.layers.Input(shape=(),dtype=tf.int32,name=\"label\")\n    \n    _,sequence_output = bert_layer([inputs_ids,inputs_mask,inputs_segment])\n    clf_output = sequence_output[:,0,:]\n    clf_output = tf.keras.layers.BatchNormalization()(clf_output)\n    clf_output = tf.keras.layers.Dropout(0.4)(clf_output)\n    x = margin([clf_output,label])\n    output = tf.keras.layers.Softmax()(x)\n    model = tf.keras.models.Model(inputs=[inputs_ids,inputs_mask,inputs_segment,label],outputs=\\\n                                 [output])\n    \n    model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),loss=tf.keras.losses.\\\n                  sparse_categorical_crossentropy,metrics=\"accuracy\")\n    return model \n\ndef bert_model_trainAndsave(xtr,xts,ytr,yts):\n    seed_everything(SEED)\n    bert_layer = hub.KerasLayer(\"../input/bert-en-uncased-l-12-h-768-a-12-1\",\\\n                               trainable=True)\n    vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n    do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n    tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n    max_le = max_length(test[\"cleaned_title\"].values)\n    xtr_encoded = bert_encode(xtr,tokenizer,max_len=max_le)\n    xts_encoded = bert_encode(xts,tokenizer,max_len=max_le)\n    y_train = ytr\n    y_val = yts\n    x_train = (xtr_encoded[0],xtr_encoded[1],xtr_encoded[2],y_train)\n    x_val = (xts_encoded[0],xts_encoded[1],xts_encoded[2],y_val)\n    md = build_bert_model(bert_layer,max_len=max_le)\n    checkpoints = tf.keras.callbacks.ModelCheckpoint(f\"bert_weight.h5\",\\\n                                                monitor=\"val_loss\",\\\n                                                verbose=VERBOSE,\\\n                                                save_best_only = True,\\\n                                                save_weights_only = True,\\\n                                                mode = \"min\")\n    \n    histrory = md.fit(x_train,y_train,validation_data=(x_val,y_val),epochs=EPOCHS,callbacks=\\\n                     [checkpoints,get_lr_callback()],batch_size = BATCH_SIZE,verbose=VERBOSE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data preparation :","metadata":{}},{"cell_type":"code","source":"label_group = test.groupby(\"label_group\").posting_id.unique()\ntest[\"target\"] = test[\"label_group\"].map(label_group)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[\"cleaned_title\"] = test[\"title\"].map(clean)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_CLASSES,xtr,xts,ytr,yts = processing_data(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling :","metadata":{}},{"cell_type":"code","source":"VERBOSE = 1\nEPOCHS = 30\nBATCH_SIZE = 32\nSEED = 42","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_model_trainAndsave(xtr,xts,ytr,yts)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}