{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":22.965448,"end_time":"2021-04-20T01:18:05.538055","exception":false,"start_time":"2021-04-20T01:17:42.572607","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfrom nltk.stem.porter import *\nimport numpy as np\nnp.random.seed(2018)\nfrom numpy import dot\nfrom numpy.linalg import norm","metadata":{"papermill":{"duration":2.399614,"end_time":"2021-04-20T01:18:08.058596","exception":false,"start_time":"2021-04-20T01:18:05.658982","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport tensorflow as tf\nfrom tensorflow.keras import datasets, layers, models\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\nimport gc","metadata":{"papermill":{"duration":7.707208,"end_time":"2021-04-20T01:18:15.882591","exception":false,"start_time":"2021-04-20T01:18:08.175383","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\nimport numpy as np\nnp.random.seed(2018)\nfrom gensim.models import Word2Vec\nimport nltk\n# nltk.download('wordnet')\nstemmer = SnowballStemmer('english')\n\nfrom numpy import dot\nfrom numpy.linalg import norm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/shopee-product-matching/train.csv')\n# train_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('../input/shopee-product-matching/test.csv')\n# test_df","metadata":{"papermill":{"duration":0.194567,"end_time":"2021-04-20T01:18:16.193949","exception":false,"start_time":"2021-04-20T01:18:15.999382","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_title_list = train_df['title'].tolist()\ntrain_label_list = train_df['label_group'].tolist()\n# train_title_list\n# train_label_list","metadata":{"papermill":{"duration":0.119453,"end_time":"2021-04-20T01:18:16.429051","exception":false,"start_time":"2021-04-20T01:18:16.309598","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_title_list = test_df['title'].tolist()\ntest_title_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lemmatize_stemming(text):\n    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n\ndef preprocess(text):\n    result = []\n    for token in gensim.utils.simple_preprocess(text):\n        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n            if token == 'xxxx':\n                continue\n            result.append(lemmatize_stemming(token))\n    \n    return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Process title field for train df","metadata":{}},{"cell_type":"code","source":"processed_docs = train_df['title'].map(preprocess)\nprocessed_docs =list(processed_docs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def word2vec_model():\n    w2v_model = Word2Vec(min_count=1,\n                     window=3,\n                     vector_size=50,\n                     sample=6e-5, \n                     alpha=0.03, \n                     min_alpha=0.0007, \n                     negative=20)\n    \n    w2v_model.build_vocab(processed_docs)\n    w2v_model.train(processed_docs, total_examples=w2v_model.corpus_count, epochs=300, report_delay=1)\n    \n    return w2v_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w2v_model = word2vec_model()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Getting embedding vector","metadata":{}},{"cell_type":"code","source":"emb_vec = w2v_model.wv\nmodel = emb_vec","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Finding similarity between two vector using cosine similarity","metadata":{}},{"cell_type":"code","source":"def find_similarity(sen1, sen2, model):\n    p_sen1 = preprocess(sen1)\n    p_sen2 = preprocess(sen2)\n    \n    sen_vec1 = np.zeros(50)\n    sen_vec2 = np.zeros(50)\n    for val in p_sen1:\n        sen_vec1 = np.add(sen_vec1, model[val])\n#     print(\"sen_vec1 \", sen_vec1)\n    for val in p_sen2:\n        if val in model.key_to_index:\n            sen_vec2 = np.add(sen_vec2, model[val])\n#     print(\"sen_vec2 \", sen_vec2)\n    return dot(sen_vec1,sen_vec2)/(norm(sen_vec1)*norm(sen_vec2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Some optimization","metadata":{}},{"cell_type":"code","source":"sen_vec1_arr = []\nfor train_data in train_title_list:\n    p_sen1 = preprocess(train_data)\n    sen_vec1 = np.zeros(50)\n    for val in p_sen1:\n        sen_vec1 = np.add(sen_vec1, model[val])\n    sen_vec1_arr.append(sen_vec1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datetime import datetime\nstart_time = datetime.now()\npred_df = []\npcounter = 0\nmodel = emb_vec\n    \nfor test_data in test_title_list:\n    sim_score = {}\n    p_sen2 = preprocess(test_data)\n    sen_vec2 = np.zeros(50)\n    for val in p_sen2:\n        if val in model.key_to_index:\n            sen_vec2 = np.add(sen_vec2, model[val])\n    index = 0\n    for train_data in train_title_list:\n        sen_vec1 = sen_vec1_arr[index]\n        distance = dot(sen_vec1,sen_vec2)/(norm(sen_vec1)*norm(sen_vec2))\n#         if distance > 0.6:\n        sim_score[train_data] = distance\n        index = index+1\n    \n    sorted_dict = {k: v for k, v in sorted(sim_score.items(), key=lambda item: item[1], reverse=True)}\n\n    first = list(sorted_dict.keys())[0]\n#     sim_score = next(iter(sorted_dict.values()))\n#     print(\"pcounter \", pcounter)\n#     print(\"sim_score key \", first)\n#     print(\"sim_score score \", sim_score)    \n#     print(\"test_data \", test_data)\n    index = train_title_list.index(first)\n#     print(\"index \", index)\n#     print(\"train_title_list[index] \", train_label_list[index])\n    pred_df.append(train_label_list[index])\n    pcounter = pcounter+1\nend_time = datetime.now()\nprint('Program took : {}'.format(end_time - start_time))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pred_df = []\n# for test_data in test_title_list:\n#     sim_score = {}\n#     for train_data in train_title_list:\n#         sim_score[train_data] = find_similarity(train_data,test_data,emb_vec)\n    \n#     sorted_dict = {k: v for k, v in sorted(sim_score.items(), key=lambda item: item[1], reverse=True)}\n    \n#     first = next(iter(sorted_dict.keys()))\n# #     print(\"sim_score \", first)\n# #     print(\"test_data \", test_data)\n#     index = train_title_list.index(first)\n# #     print(\"index \", index)\n# #     print(\"train_title_list[index] \", train_label_list[index])\n#     pred_df.append(train_label_list[index])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = test_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df['pred'] = [element for element in pred_df]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"drop_cols = ['image','image_phash','title']\nsubmission_df = submission_df.drop(columns=drop_cols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df['matches'] = submission_df.groupby(['pred'])['posting_id'].transform(lambda x : ' '.join(x))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = submission_df.drop(columns='pred')\nsubmission_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv('submission.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.106991,"end_time":"2021-04-20T01:18:22.844302","exception":false,"start_time":"2021-04-20T01:18:22.737311","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}