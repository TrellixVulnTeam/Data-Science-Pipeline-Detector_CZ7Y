{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **相似度计算专题**","metadata":{}},{"cell_type":"markdown","source":"### 此次Kernel的核心目的为后处理的**相似度计算代码填空**专题，关于图像和文本的特征张量已生成（对于图像已训练调优模型，直接加载使用即可），你需要利用这两个特征张量分别/结合进行内部相似度计算。\n\n### 上传至Kaggle时，需要在dataset添加Add data：使用url搜索添加以下两个包：\n### timm 图像模型构件库\n### https://www.kaggle.com/kozodoi/timm-pytorch-image-models\n### 已训练好的nfnet网络\n### https://www.kaggle.com/winniy/pretrain-nfnet","metadata":{}},{"cell_type":"markdown","source":"## **第一步，简单了解预处理及特征提取部分**","metadata":{}},{"cell_type":"markdown","source":"### 以下部分仅需了解即可，可自行查阅谷歌或官方文档了解语句用途","metadata":{}},{"cell_type":"markdown","source":"### **1.1 导包**","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\nimport numpy as np \nimport pandas as pd \n\nimport math\nimport random \nimport os \nimport cv2\n\nfrom tqdm import tqdm \n\nimport torch \nfrom torch.utils.data import Dataset \nfrom torch import nn\nimport torch.nn.functional as F \n\nimport gc\nimport cudf\nimport cuml\nimport cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors\n\n# 新增：albumentations 是一个图像增强库\nimport albumentations as A \nfrom albumentations.pytorch.transforms import ToTensorV2\n# timm 是一个快速构建图像网络的模型库\nimport timm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport transformers\n\nimport gc\nimport matplotlib.pyplot as plt\nimport cudf\nimport cuml\nimport cupy\nfrom cuml import PCA\nfrom cuml.neighbors import NearestNeighbors\nfrom sklearn.preprocessing import Normalizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **1.2 基本参数设置**","metadata":{}},{"cell_type":"code","source":"class CFG:\n    \n    img_size = 512\n    batch_size = 12\n    seed = 2021\n    \n    device = 'cuda'\n    classes = 11014\n    \n    model_name = 'eca_nfnet_l0'\n    model_path = '../input/pretrain-nfnet/nfnet_epoch_15.pt'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 随机种子固定\ndef seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_torch(CFG.seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **1.3 加载数据集和Pytorch的数据集读取loader**","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda')\nNUM_WORKERS = 4\nBATCH_SIZE = 16\nSEED = 42\nCHECK_SUB = False\nGET_CV = True\ntransformer_model = '../input/sentence-transformer-models/paraphrase-xlm-r-multilingual-v1/0_Transformer'\nTOKENIZER = transformers.AutoTokenizer.from_pretrained(transformer_model)\n\n################################################ MODEL PATH ###############################################################\n\nTEXT_MODEL_PATH = '../input/best-multilingual-model/sentence_transfomer_xlm_best_loss_num_epochs_25_arcface.bin'\n\nmodel_params = {\n    'n_classes':11014,\n    'model_name':transformer_model,\n    'use_fc':False,\n    'fc_dim':512,\n    'dropout':0.3,\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_dataset(mode):\n    if(mode=='train'):\n        text_path='./input/shopee-product-matching/'+ mode +'.csv'\n        df = pd.read_csv(text_path)\n        tmp = df.groupby(['label_group'])['posting_id'].unique().to_dict()\n        df['matches'] = df['label_group'].map(tmp)\n        df['matches'] = df['matches'].apply(lambda x: ' '.join(x))\n        if CHECK_SUB:\n            df = pd.concat([df, df], axis = 0)\n            df.reset_index(drop = True, inplace = True)\n        df_cu = cudf.DataFrame(df)\n    else:\n        df = pd.read_csv('../input/shopee-product-matching/test.csv')\n        df_cu = cudf.DataFrame(df)\n    image_paths = '../input/shopee-product-matching/'+ mode +'_images/' + df['image']\n    return df, df_cu, image_paths\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ShopeeTextDataset(Dataset):\n    def __init__(self, csv):\n        self.csv = csv.reset_index()\n\n    def __len__(self):\n        return self.csv.shape[0]\n\n    def __getitem__(self, index):\n        row = self.csv.iloc[index]\n        \n        text = row.title\n        \n        text = TOKENIZER(text, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")\n        input_ids = text['input_ids'][0]\n        attention_mask = text['attention_mask'][0]  \n        \n        return input_ids, attention_mask","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ShopeeNet(nn.Module):\n\n    def __init__(self,\n                 n_classes,\n                 model_name='bert-base-uncased',\n                 use_fc=False,\n                 fc_dim=512,\n                 dropout=0.0):\n        \"\"\"\n        :param n_classes:\n        :param model_name: name of model from pretrainedmodels\n            e.g. resnet50, resnext101_32x4d, pnasnet5large\n        :param pooling: One of ('SPoC', 'MAC', 'RMAC', 'GeM', 'Rpool', 'Flatten', 'CompactBilinearPooling')\n        :param loss_module: One of ('arcface', 'cosface', 'softmax')\n        \"\"\"\n        super(ShopeeNet, self).__init__()\n\n        self.transformer = transformers.AutoModel.from_pretrained(model_name)\n        final_in_features = self.transformer.config.hidden_size\n        \n        self.use_fc = use_fc\n    \n        if use_fc:\n            self.dropout = nn.Dropout(p=dropout)\n            self.fc = nn.Linear(final_in_features, fc_dim)\n            self.bn = nn.BatchNorm1d(fc_dim)\n            self._init_params()\n            final_in_features = fc_dim\n\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, input_ids,attention_mask):\n        feature = self.extract_feat(input_ids,attention_mask)\n        return F.normalize(feature)\n\n    def extract_feat(self, input_ids,attention_mask):\n        x = self.transformer(input_ids=input_ids,attention_mask=attention_mask)\n        \n        features = x[0]\n        features = features[:,0,:]\n\n        if self.use_fc:\n            features = self.dropout(features)\n            features = self.fc(features)\n            features = self.bn(features)\n\n        return features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_xlmR_text_embeddings(df):\n    embeds = []\n    \n    model = ShopeeNet(**model_params)\n    model.eval()\n    \n    model.load_state_dict(dict(list(torch.load(TEXT_MODEL_PATH).items())[:-1]))\n    model = model.to(device)\n\n    text_dataset = ShopeeTextDataset(df)\n    text_loader = torch.utils.data.DataLoader(\n        text_dataset,\n        batch_size=BATCH_SIZE,\n        pin_memory=True,\n        drop_last=False,\n        num_workers=NUM_WORKERS\n    )\n    \n    \n    with torch.no_grad():\n        for input_ids, attention_mask in tqdm(text_loader): \n            input_ids = input_ids.cuda()\n            attention_mask = attention_mask.cuda()\n            feat = model(input_ids, attention_mask)\n            text_embeddings = feat.detach().cpu().numpy()\n            embeds.append(text_embeddings)\n    \n    \n    del model\n    text_embeddings = np.concatenate(embeds)\n    print(f'Our text embeddings shape is {text_embeddings.shape}')\n    del embeds\n    gc.collect()\n    return text_embeddings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def f1_score(y_true, y_pred):\n    y_true = y_true.apply(lambda x: set(x.split()))\n    y_pred = y_pred.apply(lambda x: set(x.split()))\n    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n    len_y_pred = y_pred.apply(lambda x: len(x)).values\n    len_y_true = y_true.apply(lambda x: len(x)).values\n    f1 = 2 * intersection / (len_y_pred + len_y_true)\n    return f1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_neighbors_knn(df, embeddings, KNN = 50):\n    '''\n    https://www.kaggle.com/ragnar123/unsupervised-baseline-arcface?scriptVersionId=57121538\n    '''\n\n    model = NearestNeighbors(n_neighbors = KNN)\n    model.fit(embeddings)\n    distances, indices = model.kneighbors(embeddings)\n    # Iterate through different thresholds to maximize cv, run this in interactive mode, then replace else clause with a solid threshold\n    if GET_CV:\n        thresholds = list(np.arange(0,2,0.1))\n        \n        scores = []\n        for threshold in thresholds:\n            predictions = []\n            for k in range(embeddings.shape[0]):\n                idx = np.where(distances[k,] < threshold)[0]\n                ids = indices[k,idx]\n                posting_ids = ' '.join(df['posting_id'].iloc[ids].values)\n                predictions.append(posting_ids)\n            df['pred_matches'] = predictions\n            df['f1'] = f1_score(df['matches'], df['pred_matches'])\n            score = df['f1'].mean()\n            print(f'Our f1 score for threshold {threshold} is {score}')\n            scores.append(score)\n        thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\n        max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n        best_threshold = max_score['thresholds'].values[0]\n        best_score = max_score['scores'].values[0]\n        print(f'Our best score is {best_score} and has a threshold {best_threshold}')\n        \n        # Use threshold\n        predictions = []\n        for k in range(embeddings.shape[0]):\n            # Because we are predicting the test set that have 70K images and different label groups, confidence should be smaller\n            idx = np.where(distances[k,] < 0.60)[0]\n            ids = indices[k,idx]\n            posting_ids = ' '.join(df['posting_id'].iloc[ids].values)\n            predictions.append(posting_ids)\n    \n    # Because we are predicting the test set that have 70K images and different label groups, confidence should be smaller\n    else:\n        predictions = []\n        for k in tqdm(range(embeddings.shape[0])):\n            idx = np.where(distances[k,] < 0.60)[0]\n            ids = indices[k,idx]\n            posting_ids = df['posting_id'].iloc[ids].values\n            predictions.append(posting_ids)\n        \n    del model, distances, indices\n    gc.collect()\n    return df, predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_neighbours_cos_sim(df,embeddings):\n    '''\n    When using cos_sim use normalized features else use normal features\n    '''\n    embeddings = cupy.array(embeddings)\n    \n    if GET_CV:\n        thresholds = list(np.arange(0.5,0.7,0.05))\n\n        scores = []\n        for threshold in thresholds:\n            \n################################################# Code for Getting Preds #########################################\n            preds = []\n            CHUNK = 1024*4\n\n            print('Finding similar titles...for threshold :',threshold)\n            CTS = len(embeddings)//CHUNK\n            if len(embeddings)%CHUNK!=0: CTS += 1\n\n            for j in range( CTS ):\n                a = j*CHUNK\n                b = (j+1)*CHUNK\n                b = min(b,len(embeddings))\n\n                cts = cupy.matmul(embeddings,embeddings[a:b].T).T\n\n                for k in range(b-a):\n                    IDX = cupy.where(cts[k,]>threshold)[0]\n                    o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n                    o = ' '.join(o)\n                    preds.append(o)\n# ######################################################################################################################\n#             df['pred_matches'] = preds\n#             df['f1'] = f1_score(df['matches'], df['pred_matches'])\n#             score = df['f1'].mean()\n#             print(f'Our f1 score for threshold {threshold} is {score}')\n#             scores.append(score)\n            \n#         thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\n#         max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n#         best_threshold = max_score['thresholds'].values[0]\n#         best_score = max_score['scores'].values[0]\n#         print(f'Our best score is {best_score} and has a threshold {best_threshold}')\n            \n    else:\n        preds = []\n        CHUNK = 1024*4\n        threshold = 0.6\n\n        print('Finding similar texts...for threshold :',threshold)\n        CTS = len(embeddings)//CHUNK\n        if len(embeddings)%CHUNK!=0: CTS += 1\n\n        for j in range( CTS ):\n            a = j*CHUNK\n            b = (j+1)*CHUNK\n            b = min(b,len(embeddings))\n            print('chunk',a,'to',b)\n\n            cts = cupy.matmul(embeddings,embeddings[a:b].T).T\n\n            for k in range(b-a):\n                IDX = cupy.where(cts[k,]>threshold)[0]\n                o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n                preds.append(o)\n                    \n    return df, preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ShopeeDataset(Dataset):\n    def __init__(self, image_paths):\n\n        self.image_paths = image_paths\n        self.augmentations = A.Compose(\n        [\n            A.Resize(CFG.img_size,CFG.img_size,always_apply=True),\n            A.Normalize(),\n            ToTensorV2(p=1.0)\n        ]\n        )\n\n    def __len__(self):\n        return self.image_paths.shape[0]\n\n    def __getitem__(self, index):\n        image_path = self.image_paths[index]\n        \n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.augmentations:\n            augmented = self.augmentations(image=image)\n            image = augmented['image']       \n    \n        return image,torch.tensor(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **1.4 图像特征提取模型构建**","metadata":{}},{"cell_type":"code","source":"class ShopeeModel(nn.Module):\n\n    def __init__(self,n_classes = CFG.classes,model_name = CFG.model_name,fc_dim = 512,use_fc = True,pretrained = False):\n\n        super(ShopeeModel,self).__init__()\n\n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n        final_in_features = self.backbone.head.fc.in_features\n        self.backbone.head.fc = nn.Identity()\n        self.final = nn.Identity()\n        self.backbone.head.global_pool = nn.Identity()\n        self.pooling =  nn.AdaptiveAvgPool2d(1)\n        self.use_fc = use_fc\n        self.dropout = nn.Dropout(p=0.0)\n        self.fc = nn.Linear(final_in_features, fc_dim)\n        self.bn = nn.BatchNorm1d(fc_dim)\n        final_in_features = fc_dim\n\n    def forward(self, image, label):\n        feature = self.extract_feat(image)\n        return feature\n\n    def extract_feat(self, x):\n        batch_size = x.shape[0]\n        x = self.backbone(x)\n        x = self.pooling(x).view(batch_size, -1)\n\n        if self.use_fc:\n            x = self.dropout(x)\n            x = self.fc(x)\n            x = self.bn(x)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **1.5 图像特征图、文本特征图抽取（关键）**","metadata":{}},{"cell_type":"markdown","source":"#### **图像特征图**","metadata":{}},{"cell_type":"code","source":"def get_image_embeddings(image_paths, model_name = CFG.model_name):\n    embeds = []\n\n    model = ShopeeModel(model_name = model_name)\n    model.eval()\n    \n    checkpoint = torch.load(CFG.model_path)\n    del checkpoint['final.weight']\n    model.load_state_dict(checkpoint,strict=True)\n    model = model.to(CFG.device)\n    \n\n    image_dataset = ShopeeDataset(image_paths=image_paths)\n    image_loader = torch.utils.data.DataLoader(image_dataset,batch_size=CFG.batch_size,pin_memory=True,drop_last=False,num_workers=4)\n    \n    \n    with torch.no_grad():\n        for img,label in tqdm(image_loader): \n            img = img.cuda()\n            label = label.cuda()\n            feat = model(img,label)\n            image_embeddings = feat.detach().cpu().numpy()\n            embeds.append(image_embeddings)\n    \n    image_embeddings = np.concatenate(embeds)\n    del model, embeds\n    gc.collect()\n    return cupy.asarray(image_embeddings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **文本特征图**","metadata":{}},{"cell_type":"markdown","source":"## **第二步，相似度计算**","metadata":{}},{"cell_type":"code","source":"# 运行读取数据集\ndf,df_cu,image_paths = read_dataset('test')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 获得特征张量embedding\nimage_embeddings = get_image_embeddings(image_paths.values)\ntext_embeddings = get_xlmR_text_embeddings(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(image_embeddings.shape)\nprint(text_embeddings.shape)\nprint(type(image_embeddings))\nprint(type(text_embeddings))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **现在你拥有两个特征张量，image_embeddings为提取的图像特征，text_embeddings为提取的文本特征。**\n### **在这里，Query和Features Datebase为同一个，即内部检索相似度。**\n### **张量的每一行为某一件商品，请匹配与之相近的商品。**\n### **你可以两个张量分别检索相似度，再将结果合并去重。**\n### **你也可以同时使用跨模态的两个张量，直接检索得到结果。**","metadata":{}},{"cell_type":"markdown","source":"----------------","metadata":{}},{"cell_type":"markdown","source":"## **现成例子演示，仅使用【文本】计算乘法相似度**\n## **方法来源：https://www.kaggle.com/finlay/unsupervised-image-text-baseline-in-20min/data#image-CNN**\n## **图像相似度KNN K近邻计算相似方法：https://www.kaggle.com/parthdhameliya77/pytorch-resnext50-32x4d-image-tfidf-inference**\n## **你还可以使用其他计算相似度的方法**\n## **参考文献：https://arxiv.org/pdf/1611.01747.pdf Figure1**","metadata":{}},{"cell_type":"code","source":"def get_image_predictions(df, embeddings,threshold = 1.2):\n    \n    if len(df) > 3:\n        KNN = 50\n    else : \n        KNN = 3\n    \n    model = NearestNeighbors(n_neighbors = KNN)\n    model.fit(embeddings)\n    distances, indices = model.kneighbors(embeddings)\n    \n    predictions = []\n    for k in tqdm(range(embeddings.shape[0])):\n        idx = np.where(distances[k,] < threshold)[0]\n        ids = indices[k,idx]\n        posting_ids = df['posting_id'].iloc[cupy.asnumpy(ids)].values\n        predictions.append(posting_ids)\n        \n    del model, distances, indices\n    gc.collect()\n    return predictions\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def get_predictions(df, embeddings, threshold=0.75):\n    \n#     preds = []\n    \n#     # 面对大数据时，矩阵过大计算成本较高，一般做分块处理\n#     CHUNK = 1024*2\n#     CTS = len(df)//CHUNK\n#     if len(df)%CHUNK!=0: CTS += 1\n#     for j in range( CTS ):\n\n#         a = j*CHUNK\n#         b = (j+1)*CHUNK\n#         b = min(b,len(df))\n#         print('chunk',a,'to',b)\n\n#         # ================ 你需要修改这里 ================\n#         # 乘法相似度计算\n#         # ==============================================\n#         cts = cupy.matmul( embeddings, embeddings[a:b].T).T\n        \n#         # ==============================================\n        \n        \n#         for k in range(b-a):\n#             IDX = cupy.where(cts[k,]>threshold)[0]\n#             o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n#             preds.append(o)\n    \n#     del embeddings\n#     gc.collect()\n#     return preds\ndf,text_predictions = get_neighbours_cos_sim(df,text_embeddings)\nprint((text_predictions))\ntext_pre=[]\nfor p in text_predictions:\n    list1=[]\n    list1.append(p)\n    c=np.array(list1)\n    text_pre.append(c)\nprint(text_pre)\n# text_prediction=[array(['test_2255846744'], dtype=object), array(['test_3588702337'], dtype=object), array(['test_4015706929'], dtype=object)]\n# if not GET_CV:\n#     text_predictions = [' '.join(text_preds) for text_preds in text_predictions]\n#     df['matches'] = text_predictions\n#     df[['posting_id','matches']].to_csv('submission.csv',index=False)\n# else:\n#     df['matches'] = text_predictions\n#     df[['posting_id','matches']].to_csv('submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# text_predictions = get_predictions(df, text_embeddings, 0.75)\nimage_predictions = get_image_predictions(df, image_embeddings, threshold = 4.8)\ndel text_embeddings\ndel image_embeddings\nprint((image_predictions))\nprint(type(image_predictions))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-----------------------","metadata":{}},{"cell_type":"markdown","source":"## **第三步，提交答案**","metadata":{}},{"cell_type":"code","source":"# 用于提交答案\ndef combine_predictions(row):\n    x = np.concatenate([row['image_predictions'], row['text_predictions']])\n    return ' '.join( np.unique(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 使用图像维度记得修改此处👇\ndf['image_predictions'] = image_predictions\ndf['text_predictions'] = text_pre\ndf['matches'] = df.apply(combine_predictions, axis = 1)\ndf[['posting_id', 'matches']].to_csv('submission.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}