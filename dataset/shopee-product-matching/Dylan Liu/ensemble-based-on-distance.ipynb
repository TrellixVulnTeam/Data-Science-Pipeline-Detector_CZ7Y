{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import os\n\n# os.listdir('../input/k/ragnar123/')","metadata":{"execution":{"iopub.execute_input":"2021-04-27T10:21:18.230682Z","iopub.status.busy":"2021-04-27T10:21:18.229326Z","iopub.status.idle":"2021-04-27T10:21:18.231701Z","shell.execute_reply":"2021-04-27T10:21:18.232197Z"},"papermill":{"duration":0.017899,"end_time":"2021-04-27T10:21:18.232426","exception":false,"start_time":"2021-04-27T10:21:18.214527","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thanks for the code: https://www.kaggle.com/muhammad4hmed/b3-tfidf-knn-boom-p","metadata":{}},{"cell_type":"code","source":"import cudf, cuml, cupy","metadata":{"execution":{"iopub.execute_input":"2021-04-27T10:21:18.254866Z","iopub.status.busy":"2021-04-27T10:21:18.254362Z","iopub.status.idle":"2021-04-27T10:21:22.890706Z","shell.execute_reply":"2021-04-27T10:21:22.889875Z"},"papermill":{"duration":4.648892,"end_time":"2021-04-27T10:21:22.890872","exception":false,"start_time":"2021-04-27T10:21:18.24198","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install ../input/shopee-external-models/Keras_Applications-1.0.8-py3-none-any.whl\n!pip install ../input/shopee-external-models/efficientnet-1.1.0-py3-none-any.whl\n#!pip install keras==2.1.6\nimport numpy as np\nimport pandas as pd\nimport gc\nimport matplotlib.pyplot as plt\nimport cudf\nimport cuml\nimport cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml import PCA\nfrom cuml.neighbors import NearestNeighbors\nimport tensorflow as tf\nimport efficientnet.tfkeras as efn\nfrom tqdm.notebook import tqdm\nimport math\nfrom shutil import copyfile\ncopyfile(src = \"../input/bert-baseline/tokenization.py\", dst = \"../working/tokenization.py\")\nimport tokenization\nimport tensorflow_hub as hub\nfrom numba import cuda\nfrom keras import backend as K","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2021-04-27T10:21:22.930766Z","iopub.status.busy":"2021-04-27T10:21:22.920005Z","iopub.status.idle":"2021-04-27T10:22:20.320471Z","shell.execute_reply":"2021-04-27T10:22:20.319242Z"},"papermill":{"duration":57.420117,"end_time":"2021-04-27T10:22:20.320619","exception":false,"start_time":"2021-04-27T10:21:22.900502","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Arcmarginproduct class keras layer\nclass ArcMarginProduct(tf.keras.layers.Layer):\n    '''\n    Implements large margin arc distance.\n\n    Reference:\n        https://arxiv.org/pdf/1801.07698.pdf\n        https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/\n            blob/master/src/modeling/metric_learning.py\n    '''\n    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n                 ls_eps=0.0, **kwargs):\n\n        super(ArcMarginProduct, self).__init__(**kwargs)\n\n        self.n_classes = n_classes\n        self.s = s\n        self.m = m\n        self.ls_eps = ls_eps\n        self.easy_margin = easy_margin\n        self.cos_m = tf.math.cos(m)\n        self.sin_m = tf.math.sin(m)\n        self.th = tf.math.cos(math.pi - m)\n        self.mm = tf.math.sin(math.pi - m) * m\n\n    def get_config(self):\n\n        config = super().get_config().copy()\n        config.update({\n            'n_classes': self.n_classes,\n            's': self.s,\n            'm': self.m,\n            'ls_eps': self.ls_eps,\n            'easy_margin': self.easy_margin,\n        })\n        return config\n\n    def build(self, input_shape):\n        super(ArcMarginProduct, self).build(input_shape[0])\n\n        self.W = self.add_weight(\n            name='W',\n            shape=(int(input_shape[0][-1]), self.n_classes),\n            initializer='glorot_uniform',\n            dtype='float32',\n            trainable=True,\n            regularizer=None)\n\n    def call(self, inputs):\n        X, y = inputs\n        y = tf.cast(y, dtype=tf.int32)\n        cosine = tf.matmul(\n            tf.math.l2_normalize(X, axis=1),\n            tf.math.l2_normalize(self.W, axis=0)\n        )\n        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = tf.where(cosine > 0, phi, cosine)\n        else:\n            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n        one_hot = tf.cast(\n            tf.one_hot(y, depth=self.n_classes),\n            dtype=cosine.dtype\n        )\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.s\n        return output","metadata":{"execution":{"iopub.execute_input":"2021-04-27T10:22:20.366277Z","iopub.status.busy":"2021-04-27T10:22:20.364384Z","iopub.status.idle":"2021-04-27T10:22:20.366932Z","shell.execute_reply":"2021-04-27T10:22:20.367332Z"},"papermill":{"duration":0.033855,"end_time":"2021-04-27T10:22:20.367541","exception":false,"start_time":"2021-04-27T10:22:20.333686","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cd ../input/bert-baseline; ls","metadata":{"execution":{"iopub.execute_input":"2021-04-27T10:22:20.396221Z","iopub.status.busy":"2021-04-27T10:22:20.395474Z","iopub.status.idle":"2021-04-27T10:22:21.036537Z","shell.execute_reply":"2021-04-27T10:22:21.036043Z"},"papermill":{"duration":0.657048,"end_time":"2021-04-27T10:22:21.036658","exception":false,"start_time":"2021-04-27T10:22:20.37961","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For tf.dataset\nAUTO = tf.data.experimental.AUTOTUNE\n\n# Configuration\nIMAGE_SIZE = [512, 512]\nBATCH_SIZE = 8\n# Seed\nSEED = 42\n# Verbosity\nVERBOSE = 1\n# Number of classes\nN_CLASSES = 11014\n\nIMAGE_THR = 3.8*0.7\nTEXT_THR = 24.5*0.7\nTEXT_THR2 = 0.7*1.25","metadata":{"execution":{"iopub.execute_input":"2021-04-27T10:22:21.068988Z","iopub.status.busy":"2021-04-27T10:22:21.068143Z","iopub.status.idle":"2021-04-27T10:22:21.070462Z","shell.execute_reply":"2021-04-27T10:22:21.070847Z"},"papermill":{"duration":0.02071,"end_time":"2021-04-27T10:22:21.070974","exception":false,"start_time":"2021-04-27T10:22:21.050264","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RESTRICT TENSORFLOW TO 2GB OF GPU RAM\n# SO THAT WE HAVE 14GB RAM FOR RAPIDS\nLIMIT = 2.0\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        tf.config.experimental.set_virtual_device_configuration(\n            gpus[0],\n            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*LIMIT)])\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n        #print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n    except RuntimeError as e:\n        print(e)\nprint('We will restrict TensorFlow to max %iGB GPU RAM'%LIMIT)\nprint('then RAPIDS can use %iGB GPU RAM'%(16-LIMIT))","metadata":{"execution":{"iopub.execute_input":"2021-04-27T10:22:25.825343Z","iopub.status.busy":"2021-04-27T10:22:25.824674Z","iopub.status.idle":"2021-04-27T10:22:25.830035Z","shell.execute_reply":"2021-04-27T10:22:25.829179Z"},"papermill":{"duration":4.746074,"end_time":"2021-04-27T10:22:25.83016","exception":false,"start_time":"2021-04-27T10:22:21.084086","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Flag to get cv score\nGET_CV = True\n# Flag to check ram allocations (debug)\nCHECK_SUB = False\n\ndf = cudf.read_csv('../input/shopee-product-matching/test.csv')\n# If we are comitting, replace train set for test set and dont get cv\nif len(df) > 3:\n    GET_CV = False\ndel df\n\n# Function to get our f1 score\ndef f1_score(y_true, y_pred):\n    y_true = y_true.apply(lambda x: set(x.split()))\n    y_pred = y_pred.apply(lambda x: set(x.split()))\n    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n    len_y_pred = y_pred.apply(lambda x: len(x)).values\n    len_y_true = y_true.apply(lambda x: len(x)).values\n    f1 = 2 * intersection / (len_y_pred + len_y_true)\n    return f1\n\n# Function to combine predictions\ndef combine_predictions(row):\n    x = np.concatenate([row['image_predictions'], row['text_predictions'], row['oof_text']])\n    #x = row['image_predictions']\n    return ' '.join( np.unique(x) )\n\n# Function to read out dataset\ndef read_dataset():\n    if GET_CV:\n        df = pd.read_csv('../input/shopee-product-matching/train.csv')\n        tmp = df.groupby(['label_group'])['posting_id'].unique().to_dict()\n        df['matches'] = df['label_group'].map(tmp)\n        df['matches'] = df['matches'].apply(lambda x: ' '.join(x))\n        if CHECK_SUB:\n            df = pd.concat([df, df], axis = 0)\n            df.reset_index(drop = True, inplace = True)\n        df_cu = cudf.DataFrame(df)\n        image_paths = '../input/shopee-product-matching/train_images/' + df['image']\n    else:\n        df = pd.read_csv('../input/shopee-product-matching/test.csv')\n        df_cu = cudf.DataFrame(df)\n        image_paths = '../input/shopee-product-matching/test_images/' + df['image']\n        \n    return df, df_cu, image_paths\n\n# Function to decode our images\ndef decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels = 3)\n    image = tf.image.resize(image, IMAGE_SIZE)\n    image = tf.cast(image, tf.float32) / 255.0\n    return image\n\n# Function to read our test image and return image\ndef read_image(image):\n    image = tf.io.read_file(image)\n    image = decode_image(image)\n    return image\n\n# Function to get our dataset that read images\ndef get_dataset(image):\n    dataset = tf.data.Dataset.from_tensor_slices(image)\n    dataset = dataset.map(read_image, num_parallel_calls = AUTO)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\n# Function to get the embeddings of our images with the fine-tuned model\ndef get_image_embeddings(image_paths, cfg):\n    embeds = []\n    \n    \"\"\"\n    margin = ArcMarginProduct(\n            n_classes = N_CLASSES, \n            s = 30, \n            m = 0.5, \n            name='head/arc_margin', \n            dtype='float32'\n            )\n    \"\"\"\n    margin = cfg.margin\n\n    inp = tf.keras.layers.Input(shape = (*IMAGE_SIZE, 3), name = 'inp1')\n    label = tf.keras.layers.Input(shape = (), name = 'inp2')\n    #x = efn.EfficientNetB5(weights = None, include_top = False)(inp)\n    x = cfg.model[0].__dict__[cfg.model[1]](weights = None, include_top = False)(inp)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = margin([x, label])\n        \n    output = tf.keras.layers.Softmax(dtype='float32')(x)\n\n    model = tf.keras.models.Model(inputs = [inp, label], outputs = [output])\n    #model.load_weights('../input/shopee-efficientnet-arcmarginproduct-model/EfficientNetB3_512_42.h5')\n    model.load_weights(cfg.checkpoint_path)\n    model = tf.keras.models.Model(inputs = model.input[0], outputs = model.layers[-4].output)\n    chunk = 5000\n    iterator = np.arange(np.ceil(len(df) / chunk))\n    for j in iterator:\n        a = int(j * chunk)\n        b = int((j + 1) * chunk)\n        image_dataset = get_dataset(image_paths[a:b])\n        image_embeddings = model.predict(image_dataset)\n        embeds.append(image_embeddings)\n    del model\n    image_embeddings = np.concatenate(embeds)\n    print(f'Our image embeddings shape is {image_embeddings.shape}')\n    del embeds\n    gc.collect()\n    tf.keras.backend.clear_session()\n    #cuda.select_device(0)\n    #cuda.close()\n    return image_embeddings\n\n# Return tokens, masks and segments from a text array or series\ndef bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n\n# Function to get our text title embeddings using a pre-trained bert model\ndef get_text_embeddings(df, max_len = 70):\n    embeds = []\n    module_url = \"../input/shopee-external-models/bert_en_uncased_L-24_H-1024_A-16_1\"\n    bert_layer = hub.KerasLayer(module_url, trainable = True)\n    vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n    do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n    tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n    text = bert_encode(df['title'].values, tokenizer, max_len = max_len)\n    \n    margin = ArcMarginProduct(\n            n_classes = 11014, \n            s = 30, \n            m = 0.5, \n            name='head/arc_margin', \n            dtype='float32'\n            )\n    \n    input_word_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n    label = tf.keras.layers.Input(shape = (), name = 'label')\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    x = margin([clf_output, label])\n    output = tf.keras.layers.Softmax(dtype='float32')(x)\n    model = tf.keras.models.Model(inputs = [input_word_ids, input_mask, segment_ids, label], outputs = [output])\n    \n    model.load_weights('../input/bert-baseline/Bert_123.h5')\n    model = tf.keras.models.Model(inputs = model.input[0:3], outputs = model.layers[-4].output)\n    chunk = 5000\n    iterator = np.arange(np.ceil(len(df) / chunk))\n    for j in iterator:\n        a = int(j * chunk)\n        b = int((j + 1) * chunk)\n        text_chunk = ((text[0][a:b], text[1][a:b], text[2][a:b]))\n        text_embeddings = model.predict(text_chunk, batch_size = BATCH_SIZE)\n        embeds.append(text_embeddings)\n    del model\n    text_embeddings = np.concatenate(embeds)\n    print(f'Our text embeddings shape is {text_embeddings.shape}')\n    del embeds\n    gc.collect()\n    tf.keras.backend.clear_session()\n    return text_embeddings\n    \n# Function to get 50 nearest neighbors of each image and apply a distance threshold to maximize cv\ndef get_neighbors(df, embedding_list, KNN = 50, image = True):\n    all_distances = []\n    index_list = []\n    index_set = []\n    distance_list = []\n    for embeddings in embedding_list:\n        model = NearestNeighbors(n_neighbors = KNN)\n        model.fit(embeddings)\n        distances, indices = model.kneighbors(embeddings)\n        index_list.append(indices)\n        if len(index_set) == 0:\n            index_set = index_list[-1]\n        else:\n            index_set = np.intersect1d(index_set, index_list[-1])\n            assert len(index_set) > 0  \n        distance_list.append(np.array(distances))\n    indexes = index_set\n    all_distances = 0\n    for i in range(len(index_list)):\n        index_indexes = np.where(index_list[i] in indexes)\n        if i == 0:\n            all_distances = distance_list[i][index_indexes]\n        else:\n            all_distances += distance_list[i][index_indexes]\n    all_distances /= len(index_list)\n    \n    # Iterate through different thresholds to maximize cv, run this in interactive mode, then replace else clause with a solid threshold\n    if GET_CV:\n        if image:\n            thresholds = list(np.arange(3.0, 5.0, 0.05))\n        else:\n            thresholds = list(np.arange(15, 35, 0.25))\n        scores = []\n        for threshold in thresholds:\n            predictions = []\n            for k in range(embeddings.shape[0]):\n                idx = np.where(distances[k,] < threshold)[0]\n                ids = indices[k,idx]\n                posting_ids = ' '.join(df['posting_id'].iloc[ids].values)\n                predictions.append(posting_ids)\n            df['pred_matches'] = predictions\n            df['f1'] = f1_score(df['matches'], df['pred_matches'])\n            score = df['f1'].mean()\n            print(f'Our f1 score for threshold {threshold} is {score}')\n            scores.append(score)\n        thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\n        max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n        best_threshold = max_score['thresholds'].values[0]\n        best_score = max_score['scores'].values[0]\n        print(f'Our best score is {best_score} and has a threshold {best_threshold}')\n        \n        # Use threshold\n        predictions = []\n        for k in range(embeddings.shape[0]):\n            # Because we are predicting the test set that have 70K images and different label groups, confidence should be smaller\n            if image:\n                idx = np.where(distances[k,] < IMAGE_THR)[0]\n            else:\n                idx = np.where(distances[k,] < TEXT_THR)[0]\n                #print('distances[k,]:', distances[k,])\n            ids = indices[k,idx]\n            posting_ids = df['posting_id'].iloc[ids].values\n            predictions.append(posting_ids)\n    \n    # Because we are predicting the test set that have 70K images and different label groups, confidence should be smaller\n    else:\n        predictions = []\n        for k in tqdm(range(embeddings.shape[0])):\n            if image:\n                idx = np.where(distances[k,] < IMAGE_THR)[0]\n            else:\n                idx = np.where(distances[k,] < TEXT_THR)[0]\n            ids = indices[k,idx]\n            posting_ids = df['posting_id'].iloc[ids].values\n            predictions.append(posting_ids)\n        \n    del model, distances, indices\n    gc.collect()\n    return df, predictions\n    \ndf, df_cu, image_paths = read_dataset()\nfrom cuml.feature_extraction.text import TfidfVectorizer\n\nmodel = TfidfVectorizer(stop_words=None, binary=True, max_features=25000)\ntext_embeddings2 = model.fit_transform(df_cu.title).toarray()\nprint('text embeddings shape',text_embeddings2.shape)","metadata":{"execution":{"iopub.execute_input":"2021-04-27T10:22:25.876977Z","iopub.status.busy":"2021-04-27T10:22:25.86106Z","iopub.status.idle":"2021-04-27T10:22:43.041152Z","shell.execute_reply":"2021-04-27T10:22:43.041709Z"},"papermill":{"duration":17.197268,"end_time":"2021-04-27T10:22:43.041925","exception":false,"start_time":"2021-04-27T10:22:25.844657","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.012837,"end_time":"2021-04-27T10:22:43.068783","exception":false,"start_time":"2021-04-27T10:22:43.055946","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfgs = []\n\n#3\nclass CONFIG():\n    margin = ArcMarginProduct(\n                                n_classes = N_CLASSES, \n                                s = 30, \n                                m = 0.5, \n                                name='head/arc_margin', \n                                dtype='float32'\n                                )\n    model = [efn, 'EfficientNetB0']\n    checkpoint_path = '../input/shopee-b0/EfficientNetB0_512_2_seed42.h5'\ncfgs.append(CONFIG())\n#4\nclass CONFIG():\n    margin = ArcMarginProduct(\n                                n_classes = N_CLASSES, \n                                s = 30, \n                                m = 0.5, \n                                name='head/arc_margin', \n                                dtype='float32'\n                                )\n    model = [efn, 'EfficientNetB0']\n    checkpoint_path = '../input/shopee-b0/EfficientNetB0_512_3_seed42.h5'\ncfgs.append(CONFIG())\n\n#1\nclass CONFIG():\n    margin = ArcMarginProduct(\n                                n_classes = N_CLASSES, \n                                s = 30, \n                                m = 0.5, \n                                name='head/arc_margin', \n                                dtype='float32'\n                                )\n    model = [efn, 'EfficientNetB1']\n    checkpoint_path = '../input/shopee-efficientnetb1-arcmarginproduc/EfficientNetB1_512_0_seed42.h5'\ncfgs.append(CONFIG())\n#2\nclass CONFIG():\n    margin = ArcMarginProduct(\n                                n_classes = N_CLASSES, \n                                s = 30, \n                                m = 0.5, \n                                name='head/arc_margin', \n                                dtype='float32'\n                                )\n    model = [efn, 'EfficientNetB1']\n    checkpoint_path = '../input/shopee-efficientnetb1-arcmarginproduc/EfficientNetB1_512_1_seed42.h5'\ncfgs.append(CONFIG())\n\n\nimage_embedding_list = []\nwhile len(cfgs) > 0:\n    image_embedding_list.append(get_image_embeddings(image_paths, cfgs[0]))\n    #del cfgs[0].margin\n    del cfgs[0]\n    gc.collect()\n    tf.keras.backend.clear_session()\n    tf.compat.v1.reset_default_graph()\n    #reset_seeds()\ntext_embeddings = get_text_embeddings(df)\ngc.collect()","metadata":{"execution":{"iopub.execute_input":"2021-04-27T10:22:43.114134Z","iopub.status.busy":"2021-04-27T10:22:43.113576Z","iopub.status.idle":"2021-04-27T11:45:49.575919Z","shell.execute_reply":"2021-04-27T11:45:49.576334Z"},"papermill":{"duration":4986.493326,"end_time":"2021-04-27T11:45:49.576493","exception":false,"start_time":"2021-04-27T10:22:43.083167","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.016182,"end_time":"2021-04-27T11:45:49.609004","exception":false,"start_time":"2021-04-27T11:45:49.592822","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = []\nCHUNK = 1024*4\n\nprint('Finding similar titles...')\nCTS = len(df_cu)//CHUNK\nif len(df_cu)%CHUNK!=0: CTS += 1\nfor j in range( CTS ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(df_cu))\n    print('chunk',a,'to',b)\n    \n    # COSINE SIMILARITY DISTANCE\n    # cts = np.dot( text_embeddings, text_embeddings[a:b].T).T\n    cts = cupy.matmul(text_embeddings2, text_embeddings2[a:b].T).T\n    \n    for k in range(b-a):\n        # IDX = np.where(cts[k,]>0.7)[0]\n        IDX = cupy.where(cts[k,]>TEXT_THR2)[0]\n        o = df_cu.iloc[cupy.asnumpy(IDX)].posting_id.to_pandas().values\n        preds.append(o)\n        \ndel model, text_embeddings2","metadata":{"execution":{"iopub.execute_input":"2021-04-27T11:45:49.652377Z","iopub.status.busy":"2021-04-27T11:45:49.65182Z","iopub.status.idle":"2021-04-27T11:48:15.160887Z","shell.execute_reply":"2021-04-27T11:48:15.161321Z"},"papermill":{"duration":145.536174,"end_time":"2021-04-27T11:48:15.161494","exception":false,"start_time":"2021-04-27T11:45:49.62532","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_cu['oof_text'] = preds","metadata":{"execution":{"iopub.execute_input":"2021-04-27T11:48:15.233493Z","iopub.status.busy":"2021-04-27T11:48:15.232723Z","iopub.status.idle":"2021-04-27T11:48:15.241114Z","shell.execute_reply":"2021-04-27T11:48:15.240659Z"},"papermill":{"duration":0.060136,"end_time":"2021-04-27T11:48:15.241236","exception":false,"start_time":"2021-04-27T11:48:15.1811","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get neighbors for image_embeddings\ndf, image_predictions = get_neighbors(df, image_embedding_list, KNN = 100, image = True)","metadata":{"execution":{"iopub.execute_input":"2021-04-27T11:48:15.287635Z","iopub.status.busy":"2021-04-27T11:48:15.28674Z","iopub.status.idle":"2021-04-27T11:50:27.58319Z","shell.execute_reply":"2021-04-27T11:50:27.582169Z"},"papermill":{"duration":132.323003,"end_time":"2021-04-27T11:50:27.583334","exception":false,"start_time":"2021-04-27T11:48:15.260331","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get neighbors for text_embeddings\ndf, text_predictions = get_neighbors(df, [text_embeddings], KNN = 100, image = False)","metadata":{"execution":{"iopub.execute_input":"2021-04-27T11:50:27.649144Z","iopub.status.busy":"2021-04-27T11:50:27.647765Z","iopub.status.idle":"2021-04-27T11:54:42.690227Z","shell.execute_reply":"2021-04-27T11:54:42.689694Z"},"papermill":{"duration":255.076353,"end_time":"2021-04-27T11:54:42.690381","exception":false,"start_time":"2021-04-27T11:50:27.614028","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concatenate image predctions with text predictions\nif GET_CV:\n    df['image_predictions'] = image_predictions\n    df['text_predictions'] = text_predictions\n    df['oof_text'] = df_cu['oof_text'].to_pandas().values\n    df['pred_matches'] = df.apply(combine_predictions, axis = 1)\n    df['f1'] = f1_score(df['matches'], df['pred_matches'])\n    score = df['f1'].mean()\n    print(f'Our final f1 cv score is {score}')\n    df['matches'] = df['pred_matches']\n    df[['posting_id', 'matches']].to_csv('submission.csv', index = False)\nelse:\n    df['image_predictions'] = image_predictions\n    df['oof_text'] = df_cu['oof_text'].to_pandas().values\n    df['text_predictions'] = text_predictions\n    df['matches'] = df.apply(combine_predictions, axis = 1)\n    df[['posting_id', 'matches']].to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.execute_input":"2021-04-27T11:54:42.800702Z","iopub.status.busy":"2021-04-27T11:54:42.80019Z","iopub.status.idle":"2021-04-27T11:54:44.543587Z","shell.execute_reply":"2021-04-27T11:54:44.54265Z"},"papermill":{"duration":1.802388,"end_time":"2021-04-27T11:54:44.543733","exception":false,"start_time":"2021-04-27T11:54:42.741345","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}