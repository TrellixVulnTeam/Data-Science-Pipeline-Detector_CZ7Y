{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Finding the similarity between title and product text using word2vec and cosine similarity","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\nimport numpy as np\nnp.random.seed(2018)\nfrom gensim.models import Word2Vec\nimport nltk\n# nltk.download('wordnet')\nstemmer = SnowballStemmer('english')\nimport unicodedata\nfrom numpy import dot\nfrom numpy.linalg import norm\n# Lemmatize with POS Tag\nfrom nltk.corpus import wordnet\nimport gc\nimport cudf\nimport cupy\nimport torch\n%matplotlib inline\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/shopee-product-matching/train.csv')\nDATA_PATH = '/kaggle/input/shopee-product-matching/'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('../input/shopee-product-matching/test.csv')\ndf_cu = cudf.DataFrame(test_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data preprocessing","metadata":{}},{"cell_type":"code","source":"def cleanData(dataParse):\n    data = unicodedata.normalize('NFKC', dataParse)\n    data = re.sub(r'【.*】', '', data)\n    data = re.sub(r'\\[.*\\]', '', data)\n    data = re.sub(r'「.*」', '', data)\n    data = re.sub(r'\\(.*\\)', '', data)\n    data = re.sub(r'\\<.*\\>', '', data)\n    data = re.sub(r'[※@◎].*$', '', data)\n    return data.lower() #Returns the parsed tweets\n\n\ndef lemmatize_stemming(text):\n    return stemmer.stem(WordNetLemmatizer().lemmatize(cleanData(text), pos='v'))\n\ndef preprocess(text):\n    result = []\n    for token in gensim.utils.simple_preprocess(text):\n        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n            if token == 'xxxx':\n                continue\n            result.append(lemmatize_stemming(token))\n    \n    return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load model parameter for traing","metadata":{}},{"cell_type":"code","source":"# Load word2vec model\n\nw2v_model = Word2Vec.load(\"../input/word2vec-model/word2vec_model\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Find the similarity between two vector","metadata":{}},{"cell_type":"code","source":"# Generate the average word2vec for the each title description\nnon_dup_train_df=test_df.drop_duplicates(subset=['title'])\nnon_dup_train_df=non_dup_train_df.reset_index()\ndef vectors_test(): #test_df\n    \n    # Creating a list for storing the vectors (description into vectors)\n    word_embeddings_test = []\n    test_processed_docs = non_dup_train_df['title'].map(preprocess)\n    \n    # Reading the each book description \n    for line in test_processed_docs:\n        avgword2vec = None\n        count = 0\n        for word in line:\n            if word in w2v_model.wv:\n                count += 1\n                if avgword2vec is None:\n                    avgword2vec = w2v_model.wv[word]\n                else:\n                    avgword2vec = avgword2vec + w2v_model.wv[word]\n                \n        if avgword2vec is not None:\n            avgword2vec = avgword2vec / count\n            word_embeddings_test.append(avgword2vec)\n        else:\n            word_embeddings_test.append(np.array([0]*50, dtype='float32'))\n    return word_embeddings_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_embeddings_test = cupy.array(vectors_test(), dtype=cupy.float32)#test_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def similarity1():\n    preds = []\n    CHUNK = 1024*4\n\n    print('Finding similar titles...')\n    CTS = len(test_df)//CHUNK\n    if len(test_df)%CHUNK!=0: CTS += 1\n    for j in range( CTS ):\n\n        a = j*CHUNK\n        b = (j+1)*CHUNK\n        b = min(b,len(test_df))\n        print('chunk',a,'to',b)\n\n        # COSINE SIMILARITY DISTANCE\n        cts = cupy.matmul( word_embeddings_test, word_embeddings_test[a:b].T).T        \n\n        for k in range(b-a):\n            IDX = cupy.where(cts[k,]>9)[0]\n            o = test_df.iloc[cupy.asnumpy(IDX)].posting_id.values\n            preds.append(o)\n\n    \n    return preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def combine_predictions(row):\n    return ' '.join( np.unique(row))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"non_dup_train_df['test_matches']=similarity1()\nnon_dup_train_df['matches'] = non_dup_train_df['test_matches'].apply(combine_predictions)\ndel word_embeddings_test\ndel w2v_model\ngc.collect()\nnon_dup_train_df[['posting_id', 'matches']].to_csv('submission.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"non_dup_train_df[['posting_id', 'matches']].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# keep learning and enrich your intuition.Don't forgot to upvote !! :)","metadata":{}}]}