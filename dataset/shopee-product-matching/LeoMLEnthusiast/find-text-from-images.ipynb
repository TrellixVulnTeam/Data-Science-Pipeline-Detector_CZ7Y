{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# RAPIDS cuML TfidfVectorizer and KNN to find similar Text and Images\nIn this notebook we use RAPIDS cuML's TfidfVectorizer and cuML's KNN to find items with similar titles and items with similar images. First we use RAPIDS cuML TfidfVectorizer to extract text embeddings of each item's title and then compare the embeddings using RAPIDS cuML KNN. Next we extract image embeddings of each item with EffNetB0 and compare them using RAPIDS cuML KNN.[](http://)","metadata":{}},{"cell_type":"markdown","source":"# Load Libraries","metadata":{}},{"cell_type":"code","source":"import pip\ntry:\n     __import__(\"imutils\")\nexcept ImportError:\n    pip.main([\"install\", \"imutils\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport  matplotlib.pyplot as plt\nimport tensorflow as tf\nimport pandas as pd\nimport cv2\nfrom imutils.object_detection import non_max_suppression\nimport pytesseract\nimport os\nimport glob\nos.chdir('/kaggle/working')\nBASE = '../input/shopee-product-matching/train_images/'\nEAST_PATH = \"/kaggle/input/east-data-leo/frozen_east_text_detection/frozen_east_text_detection.pb\"\nRESIZE_IMG_SHAPE = (500,500)\nimg_paths = glob.glob(f\"{BASE}*jpg\") ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Train Data\nIn this competition, we have items with an image and title. For the train data, the column `label_group` indicates the ground truth of which items are similar. We need to build a model that finds these similar images based on their image and title's text. In this notebook we explore some tools to help us.","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/shopee-product-matching/train.csv')\nprint('train shape is', train.shape )\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Display Duplicated Items from Train Data\nUsing the column `label_group` which is the ground truth, we can display examples of duplicated items.","metadata":{}},{"cell_type":"code","source":"groups = train.label_group.value_counts()\nplt.figure(figsize=(20,5))\nplt.plot(np.arange(len(groups)),groups.values)\nplt.ylabel('Duplicate Count',size=14)\nplt.xlabel('Index of Unique Item',size=14)\nplt.title('Duplicate Count vs. Unique Item Count',size=16)\nplt.show()\n\nplt.figure(figsize=(20,5))\nplt.bar(groups.index.values[:50].astype('str'),groups.values[:50])\nplt.xticks(rotation = 45)\nplt.ylabel('Duplicate Count',size=14)\nplt.xlabel('Label Group',size=14)\nplt.title('Top 50 Duplicated Items',size=16)\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def img_proc(img,offset =20):\n    img =img[offset:-offset,offset:-offset]\n    img =cv2.resize(img ,RESIZE_IMG_SHAPE)\n    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n    gray = cv2.medianBlur(gray,5)\n    ret, thresh = cv2.threshold(gray,150,255,cv2.THRESH_BINARY)\n    thresh = cv2.adaptiveThreshold(gray,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\\\n            cv2.THRESH_BINARY_INV,21,2)\n    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5,5))\n    opening = cv2.erode(thresh,kernel, iterations = 1)\n    closing = cv2.dilate(opening,kernel,iterations=1)\n    contours = cv2.findContours(closing, \n                            cv2.RETR_EXTERNAL,\n                            cv2.CHAIN_APPROX_NONE)\n    canvas =np.zeros((500,500))\n    cv2.drawContours(canvas, contours[0], -1, (255,0,255), thickness = 2) \n    return canvas\n\n\n\ndef displayDF(train, random=False, COLS=6, ROWS=4, path=BASE):\n    for k in range(ROWS):\n        plt.figure(figsize=(20,5))\n        for j in range(COLS):\n            if random: row = np.random.randint(0,len(train))\n            else: row = COLS*k + j\n            name = train.iloc[row,1]\n            title = train.iloc[row,3]\n            title_with_return = \"\"\n            for i,ch in enumerate(title):\n                title_with_return += ch\n                if (i!=0)&(i%20==0): title_with_return += '\\n'\n            img = img_proc(cv2.imread(path+name))\n            plt.subplot(1,COLS,j+1)\n            plt.title(title_with_return)\n            plt.axis('off')\n            plt.imshow(img)\n        plt.show()\n        \ndisplayDF(train,random=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predictions(prob_score, geo,min_conf):\n    (numR, numC) = prob_score.shape[2:4]\n    boxes = []\n    confidence_val = []\n    for y in range(0, numR):\n        scoresData = prob_score[0, 0, y]\n        x0 = geo[0, 0, y]\n        x1 = geo[0, 1, y]\n        x2 = geo[0, 2, y]\n        x3 = geo[0, 3, y]\n        anglesData = geo[0, 4, y]\n        for i in range(0, numC):\n            if scoresData[i] < min_conf:\n                continue\n            (offX, offY) = (i * 4.0, y * 4.0)\n            angle = anglesData[i]\n            cos = np.cos(angle)\n            sin = np.sin(angle)\n            h = x0[i] + x2[i]\n            w = x1[i] + x3[i]\n            endX = int(offX + (cos * x1[i]) + (sin * x2[i]))\n            endY = int(offY - (sin * x1[i]) + (cos * x2[i]))\n            startX = int(endX - w)\n            startY = int(endY - h)\n            boxes.append((startX, startY, endX, endY))\n            confidence_val.append(scoresData[i])\n\n    return (boxes, confidence_val)\n\ndef get_text(img,img_text ='Output',min_conf = 0.7, east = EAST_PATH):\n    image = cv2.imread(img)\n    orig = image.copy()\n    (origH, origW) = image.shape[:2]\n    (newW, newH) = (320,320)\n    rW = origW / float(newW)\n    rH = origH / float(newH)\n    image = cv2.resize(image, (newW, newH))\n    (H, W) = image.shape[:2]\n    blob = cv2.dnn.blobFromImage(image, 1.0, (W, H),\n        (123.68, 116.78, 103.94), swapRB=True, crop=False)\n    net = cv2.dnn.readNet(east)\n    # The following two layer need to pulled from EAST model for achieving this. \n    layerNames = [\n        \"feature_fusion/Conv_7/Sigmoid\",\n        \"feature_fusion/concat_3\"]\n\n    #Forward pass the blob from the image to get the desired output layers\n    net.setInput(blob)\n    (scores, geometry) = net.forward(layerNames)\n    (boxes, confidence_val) = predictions(scores, geometry,min_conf)\n    boxes = non_max_suppression(np.array(boxes), probs=confidence_val)\n    \n    results = []\n    for (startX, startY, endX, endY) in boxes:\n        startX = int(startX * rW)\n        startY = int(startY * rH)\n        endX = int(endX * rW)\n        endY = int(endY * rH)\n        r = orig[startY:endY, startX:endX]\n        configuration = (\"-l eng --oem 1 --psm 8\")\n        text = pytesseract.image_to_string(r, config=configuration)\n        results.append(((startX, startY, endX, endY), text))\n\n    orig_image = orig.copy()\n    txts=[]\n    for ((start_X, start_Y, end_X, end_Y), text) in results:\n        text = \"\".join([x if ord(x) < 128 else \"\" for x in text]).strip()\n        cv2.rectangle(orig_image, (start_X, start_Y), (end_X, end_Y),\n            (0, 0, 255), 2)\n        cv2.putText(orig_image, text, (start_X, start_Y + 30),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.7,(0,255, 0), 4)\n        txts.append(text)\n    plt.figure(figsize=(10,10))\n    plt.imshow(orig_image)\n    plt.title(img_text)\n    plt.show()\n    print(txts)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"it = iter(range(500))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"row = next(it)\npath = BASE + train.iloc[row,1]\ntitle = train.iloc[row,3]\nget_text(path,title,min_conf=0.9)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Find Similar Titles with RAPIDS!\nWe will now ignore the ground truth and try to find similar items in train data using only the title's text. First we will extract text embeddings using RAPIDS cuML's TfidfVectorizer. This will turn every title into a one-hot-encoding of the words present. We will then compare one-hot-encodings with RAPIDS cuML KNN to find title's that are similar.","metadata":{}},{"cell_type":"code","source":"!pip install cudf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cudf, cuml, cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors\nprint('RAPIDS',cuml.__version__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LOAD TRAIN UNTO THE GPU WITH CUDF\ntrain_gf = cudf.read_csv('../input/shopee-product-matching/train.csv')\nprint('train shape is', train_gf.shape )\ntrain_gf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Extract Text Embeddings with RAPIDS TfidfVectorizer\nTfidfVectorizer returns a cupy sparse matrix. Afterward we convert to a cupy dense matrix and feed that into RAPIDS cuML KNN.","metadata":{}},{"cell_type":"code","source":"model = TfidfVectorizer(stop_words='english', binary=True)\ntext_embeddings = model.fit_transform(train_gf.title).toarray()\nprint('text embeddings shape is',text_embeddings.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Find Similar Titles with RAPIDS KNN\nAfter fitting KNN, we will display some example rows of train and their 10 closest other titles in train (based on word count one-hot-encoding).","metadata":{}},{"cell_type":"code","source":"KNN = 50\nmodel = NearestNeighbors(n_neighbors=KNN)\nmodel.fit(text_embeddings)\ndistances, indices = model.kneighbors(text_embeddings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for k in range(5):\n    plt.figure(figsize=(20,3))\n    plt.plot(np.arange(50),cupy.asnumpy(distances[k,]),'o-')\n    plt.title('Text Distance From Train Row %i to Other Train Rows'%k,size=16)\n    plt.ylabel('Distance to Train Row %i'%k,size=14)\n    plt.xlabel('Index Sorted by Distance to Train Row %i'%k,size=14)\n    plt.show()\n    \n    print( train_gf.loc[cupy.asnumpy(indices[k,:10]),['title','label_group']] )","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Find Matching Images with RAPIDS!\nWe will now ignore the ground truth and try to find similar items in train data using only the item's image. First we will extract image embeddings using EffNetB0. We will then compare image embeddings with RAPIDS cuML KNN to find images that are similar.\n\n## Extract Image Embeddings with EffNetB0","metadata":{}},{"cell_type":"code","source":"class DataGenerator(tf.keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, df, img_size=256, batch_size=32, path=BASE): \n        self.df = df\n        self.img_size = img_size\n        self.batch_size = batch_size\n        self.path = path\n        self.indexes = np.arange( len(self.df) )\n        \n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        ct = len(self.df) // self.batch_size\n        ct += int(( (len(self.df)) % self.batch_size)!=0)\n        return ct\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        X = self.__data_generation(indexes)\n        return X\n            \n    def __data_generation(self, indexes):\n        'Generates data containing batch_size samples' \n        X = np.zeros((len(indexes),self.img_size,self.img_size,3),dtype='float32')\n        df = self.df.iloc[indexes]\n        for i,(index,row) in enumerate(df.iterrows()):\n            img = cv2.imread(self.path+row.image)\n            X[i,] = cv2.resize(img,(self.img_size,self.img_size)) #/128.0 - 1.0\n        return X","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = EfficientNetB0(weights='imagenet',include_top=False, pooling='avg', input_shape=None)\ntrain_gen = DataGenerator(train, batch_size=128)\nimage_embeddings = model.predict(train_gen,verbose=1)\nprint('image embeddings shape is',image_embeddings.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Find Similar Images with RAPIDS KNN\nAfter fitting KNN, we will display some example rows of train and their 8 closest other images in train (based EffNetB0 image embeddings).","metadata":{}},{"cell_type":"code","source":"KNN = 50\nmodel = NearestNeighbors(n_neighbors=KNN)\nmodel.fit(image_embeddings)\ndistances, indices = model.kneighbors(image_embeddings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for k in range(180,190):\n    plt.figure(figsize=(20,3))\n    plt.plot(np.arange(50),cupy.asnumpy(distances[k,]),'o-')\n    plt.title('Image Distance From Train Row %i to Other Train Rows'%k,size=16)\n    plt.ylabel('Distance to Train Row %i'%k,size=14)\n    plt.xlabel('Index Sorted by Distance to Train Row %i'%k,size=14)\n    plt.show()\n    \n    cluster = train.loc[cupy.asnumpy(indices[k,:8])] \n    displayDF(cluster, random=False, ROWS=2, COLS=4)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test Data\nThe next thing to do is apply the above methods on the test data and create a submission notebook! Have fun!","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv('../input/shopee-product-matching/test.csv')\nprint( test.shape )\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv('../input/shopee-product-matching/sample_submission.csv')\nprint( sub.shape )\nsub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}