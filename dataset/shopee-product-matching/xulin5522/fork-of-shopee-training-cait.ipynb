{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# About Notebook\n\n* Thanks to PyTorch Arcface Implementation by @tanulsingh077 from [here](https://www.kaggle.com/tanulsingh077/pytorch-metric-learning-pipeline-only-images)\n\n* One can train any EfficientNet(b0-b7) model by changing `model_name` in **CFG**.\n\n* Inference Notebook for the same can be found [here](https://www.kaggle.com/vatsalmavani/eff-b4-tfidf-0-727)\n\n#### **NOTE:** \n*     If you are using kaggle GPU, you must have to change `batch_size`. In addition, you also have to change `CFG.lr_max = 1e-5 * 32`","metadata":{}},{"cell_type":"markdown","source":"# Import Packages","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append('../input/utils-shopee')\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\nimport timm\nimport torch\nfrom torch import nn \nimport torch.nn.functional as F \n\nimport engine\nfrom dataset import ShopeeDataset\nfrom custom_scheduler import ShopeeScheduler\nfrom augmentations import get_train_transforms\nimport gc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config and Directories","metadata":{}},{"cell_type":"code","source":"DATA_DIR = '../input/shopee-product-matching/train_images'\nTRAIN_CSV = '../input/utils-shopee/folds.csv'\nMODEL_PATH = './'\n\n\nclass CFG:\n    debug=False\n    seed = 54\n    img_size = 384\n    classes = 11014\n    scale = 30\n    margin = 0.5\n    fc_dim = 512\n    epochs = 5\n    batch_size = 80\n    num_workers = 4\n    model_name = 'M36'\n#     'tf_efficientnet_b4'\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    scheduler_params = {\n        \"lr_start\": 1e-4,\n        \"lr_max\": 1e-4 * batch_size,     # 1e-5 * 32 (if batch_size(=32) is different then)\n        \"lr_min\": 1e-5,\n        \"lr_ramp_ep\": 5,\n        \"lr_sus_ep\": 0,\n        \"lr_decay\": 0.8,\n    }\n    retrain = False\n    start_epoch = 0\n    model_path = '../input/shopeecait/arcface_512x512_M48_checkpoints3.pt'\nSHOPEE_MODEL = '../input/shopee-pytorch-models/arcface_512x512_nfnet_l0 (mish).pt'\nCAIT_MODEL = f'../input/fair-cait/M36_384.pth'\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(TRAIN_CSV)\ndf['length'] = df['title'].apply(lambda x: len(x.split()))\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CFG.debug:\n    df = df[:10]\n    CFG.epochs = 1\n    CFG.batch_size = 5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CaiT","metadata":{}},{"cell_type":"code","source":"# Copyright (c) 2015-present, Facebook, Inc.\n# All rights reserved.\n\nimport torch\nimport torch.nn as nn\nfrom functools import partial\n\nfrom timm.models.vision_transformer import Mlp, PatchEmbed , _cfg\nfrom timm.models.registry import register_model\nfrom timm.models.layers import trunc_normal_\n\n\n__all__ = [\n    'cait_M48', 'cait_M36', 'cait_M4',\n    'cait_S36', 'cait_S24','cait_S24_224',\n    'cait_XS24','cait_XXS24','cait_XXS24_224',\n    'cait_XXS36','cait_XXS36_224'\n]\n\n\nclass Class_Attention(nn.Module):\n    # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n    # with slight modifications to do CA \n    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n\n        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n        self.k = nn.Linear(dim, dim, bias=qkv_bias)\n        self.v = nn.Linear(dim, dim, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    \n    def forward(self, x ):\n        \n        B, N, C = x.shape\n        q = self.q(x[:,0]).unsqueeze(1).reshape(B, 1, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n        k = self.k(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n\n        q = q * self.scale\n        v = self.v(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n\n        attn = (q @ k.transpose(-2, -1)) \n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x_cls = (attn @ v).transpose(1, 2).reshape(B, 1, C)\n        x_cls = self.proj(x_cls)\n        x_cls = self.proj_drop(x_cls)\n        \n        return x_cls     \n        \nclass LayerScale_Block_CA(nn.Module):\n    # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n    # with slight modifications to add CA and LayerScale\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, Attention_block = Class_Attention,\n                 Mlp_block=Mlp,init_values=1e-4):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention_block(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp_block(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n        self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n        self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n\n    \n    def forward(self, x, x_cls):\n        \n        u = torch.cat((x_cls,x),dim=1)\n        \n        \n        x_cls = x_cls + self.drop_path(self.gamma_1 * self.attn(self.norm1(u)))\n        \n        x_cls = x_cls + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x_cls)))\n        \n        return x_cls \n        \n        \nclass Attention_talking_head(nn.Module):\n    # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n    # with slight modifications to add Talking Heads Attention (https://arxiv.org/pdf/2003.02436v1.pdf)\n    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        \n        self.num_heads = num_heads\n        \n        head_dim = dim // num_heads\n        \n        self.scale = qk_scale or head_dim ** -0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        \n        self.proj = nn.Linear(dim, dim)\n        \n        self.proj_l = nn.Linear(num_heads, num_heads)\n        self.proj_w = nn.Linear(num_heads, num_heads)\n        \n        self.proj_drop = nn.Dropout(proj_drop)\n\n\n    \n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0] * self.scale , qkv[1], qkv[2] \n    \n        attn = (q @ k.transpose(-2, -1)) \n        \n        attn = self.proj_l(attn.permute(0,2,3,1)).permute(0,3,1,2)\n                \n        attn = attn.softmax(dim=-1)\n  \n        attn = self.proj_w(attn.permute(0,2,3,1)).permute(0,3,1,2)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n    \nclass LayerScale_Block(nn.Module):\n    # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n    # with slight modifications to add layerScale\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm,Attention_block = Attention_talking_head,\n                 Mlp_block=Mlp,init_values=1e-4):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention_block(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp_block(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n        self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n        self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n\n    def forward(self, x):        \n        x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x)))\n        x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n        return x \n    \n    \n    \n    \nclass cait_models(nn.Module):\n    # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n    # with slight modifications to adapt to our cait models\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12,\n                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n                 drop_path_rate=0., norm_layer=nn.LayerNorm, global_pool=None,\n                 block_layers = LayerScale_Block,\n                 block_layers_token = LayerScale_Block_CA,\n                 Patch_layer=PatchEmbed,act_layer=nn.GELU,\n                 Attention_block = Attention_talking_head,Mlp_block=Mlp,\n                init_scale=1e-4,\n                Attention_block_token_only=Class_Attention,\n                Mlp_block_token_only= Mlp, \n                depth_token_only=2,\n                mlp_ratio_clstk = 4.0):\n        super().__init__()\n        \n\n            \n        self.num_classes = num_classes\n        self.num_features = self.embed_dim = embed_dim  \n\n        self.patch_embed = Patch_layer(\n                img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n        \n        num_patches = self.patch_embed.num_patches\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        dpr = [drop_path_rate for i in range(depth)] \n        self.blocks = nn.ModuleList([\n            block_layers(\n                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n                act_layer=act_layer,Attention_block=Attention_block,Mlp_block=Mlp_block,init_values=init_scale)\n            for i in range(depth)])\n        \n\n        self.blocks_token_only = nn.ModuleList([\n            block_layers_token(\n                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio_clstk, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                drop=0.0, attn_drop=0.0, drop_path=0.0, norm_layer=norm_layer,\n                act_layer=act_layer,Attention_block=Attention_block_token_only,\n                Mlp_block=Mlp_block_token_only,init_values=init_scale)\n            for i in range(depth_token_only)])\n            \n        self.norm = norm_layer(embed_dim)\n\n\n        self.feature_info = [dict(num_chs=embed_dim, reduction=0, module='head')]\n        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n        trunc_normal_(self.pos_embed, std=.02)\n        trunc_normal_(self.cls_token, std=.02)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'pos_embed', 'cls_token'}\n\n\n    def forward_features(self, x):\n        B = x.shape[0]\n        x = self.patch_embed(x)\n\n        cls_tokens = self.cls_token.expand(B, -1, -1)  \n        \n        x = x + self.pos_embed\n        x = self.pos_drop(x)\n\n        for i , blk in enumerate(self.blocks):\n            x = blk(x)\n            \n        for i , blk in enumerate(self.blocks_token_only):\n            cls_tokens = blk(x,cls_tokens)\n\n        x = torch.cat((cls_tokens, x), dim=1)\n            \n                \n        x = self.norm(x)\n        return x[:, 0]\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        \n        x = self.head(x)\n\n        return x \n        \n@register_model\ndef cait_XXS24_224(pretrained=False, **kwargs):\n    model = cait_models(\n        img_size= 224,patch_size=16, embed_dim=192, depth=24, num_heads=4, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_scale=1e-5,\n        depth_token_only=2,**kwargs)\n    \n    model.default_cfg = _cfg()\n    if pretrained:\n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/deit/XXS24_224.pth\",\n            map_location=\"cpu\", check_hash=True\n        )\n        checkpoint_no_module = {}\n        for k in model.state_dict().keys():\n            checkpoint_no_module[k] = checkpoint[\"model\"]['module.'+k]\n            \n        model.load_state_dict(checkpoint_no_module)\n        \n    return model \n\n@register_model\ndef cait_XXS24(pretrained=False, **kwargs):\n    model = cait_models(\n        img_size= 384,patch_size=16, embed_dim=192, depth=24, num_heads=4, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_scale=1e-5,\n        depth_token_only=2,**kwargs)\n    \n    model.default_cfg = _cfg()\n    if pretrained:\n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/deit/XXS24_384.pth\",\n            map_location=\"cpu\", check_hash=True\n        )\n        checkpoint_no_module = {}\n        for k in model.state_dict().keys():\n            checkpoint_no_module[k] = checkpoint[\"model\"]['module.'+k]\n            \n        model.load_state_dict(checkpoint_no_module)\n        \n    return model \n@register_model\ndef cait_XXS36_224(pretrained=False, **kwargs):\n    model = cait_models(\n        img_size= 224,patch_size=16, embed_dim=192, depth=36, num_heads=4, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_scale=1e-5,\n        depth_token_only=2,**kwargs)\n    \n    model.default_cfg = _cfg()\n    if pretrained:\n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/deit/XXS36_224.pth\",\n            map_location=\"cpu\", check_hash=True\n        )\n        checkpoint_no_module = {}\n        for k in model.state_dict().keys():\n            checkpoint_no_module[k] = checkpoint[\"model\"]['module.'+k]\n            \n        model.load_state_dict(checkpoint_no_module)\n        \n    return model \n\n@register_model\ndef cait_XXS36(pretrained=False, **kwargs):\n    model = cait_models(\n        img_size= 384,patch_size=16, embed_dim=192, depth=36, num_heads=4, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_scale=1e-5,\n        depth_token_only=2,**kwargs)\n    \n    model.default_cfg = _cfg()\n    if pretrained:\n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/deit/XXS36_384.pth\",\n            map_location=\"cpu\", check_hash=True\n        )\n        checkpoint_no_module = {}\n        for k in model.state_dict().keys():\n            checkpoint_no_module[k] = checkpoint[\"model\"]['module.'+k]\n            \n        model.load_state_dict(checkpoint_no_module)\n        \n    return model \n\n@register_model\ndef cait_XS24(pretrained=False, **kwargs):\n    model = cait_models(\n        img_size= 384,patch_size=16, embed_dim=288, depth=24, num_heads=6, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_scale=1e-5,\n        depth_token_only=2,**kwargs)\n    \n    model.default_cfg = _cfg()\n    if pretrained:\n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/deit/XS24_384.pth\",\n            map_location=\"cpu\", check_hash=True\n        )\n        checkpoint_no_module = {}\n        for k in model.state_dict().keys():\n            checkpoint_no_module[k] = checkpoint[\"model\"]['module.'+k]\n            \n        model.load_state_dict(checkpoint_no_module)\n        \n    return model \n\n\n\n\n@register_model\ndef cait_S24_224(pretrained=False, **kwargs):\n    model = cait_models(\n        img_size= 224,patch_size=16, embed_dim=384, depth=24, num_heads=8, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_scale=1e-5,\n        depth_token_only=2,**kwargs)\n    \n    model.default_cfg = _cfg()\n    if pretrained:\n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/deit/S24_224.pth\",\n            map_location=\"cpu\", check_hash=True\n        )\n        checkpoint_no_module = {}\n        for k in model.state_dict().keys():\n            checkpoint_no_module[k] = checkpoint[\"model\"]['module.'+k]\n            \n        model.load_state_dict(checkpoint_no_module)\n        \n    return model \n\n@register_model\ndef cait_S24(pretrained=False, **kwargs):\n    model = cait_models(\n        img_size= 384,patch_size=16, embed_dim=384, depth=24, num_heads=8, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_scale=1e-5,\n        depth_token_only=2,**kwargs)\n    \n    model.default_cfg = _cfg()\n    if pretrained:\n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/deit/S24_384.pth\",\n            map_location=\"cpu\", check_hash=True\n        )\n        checkpoint_no_module = {}\n        for k in model.state_dict().keys():\n            checkpoint_no_module[k] = checkpoint[\"model\"]['module.'+k]\n            \n        model.load_state_dict(checkpoint_no_module)\n        \n    return model \n\n@register_model\ndef cait_S36(pretrained=False, **kwargs):\n    model = cait_models(\n        img_size= 384,patch_size=16, embed_dim=384, depth=36, num_heads=8, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_scale=1e-6,\n        depth_token_only=2,**kwargs)\n    \n    model.default_cfg = _cfg()\n    if pretrained:\n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/deit/S36_384.pth\",\n            map_location=\"cpu\", check_hash=True\n        )\n        checkpoint_no_module = {}\n        for k in model.state_dict().keys():\n            checkpoint_no_module[k] = checkpoint[\"model\"]['module.'+k]\n            \n        model.load_state_dict(checkpoint_no_module)\n\n    return model \n\n\n\n\n\n@register_model\ndef cait_M36(pretrained=False, **kwargs):\n    model = cait_models(\n        img_size= 384, patch_size=16, embed_dim=768, depth=36, num_heads=16, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_scale=1e-6,\n        depth_token_only=2,**kwargs)\n    \n    model.default_cfg = _cfg()\n    if pretrained:\n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/deit/M36_384.pth\",\n            map_location=\"cpu\", check_hash=True\n        )\n        checkpoint_no_module = {}\n        for k in model.state_dict().keys():\n            checkpoint_no_module[k] = checkpoint[\"model\"]['module.'+k]\n            \n        model.load_state_dict(checkpoint_no_module)\n\n    return model \n\n\n@register_model\ndef cait_M48(pretrained=False, **kwargs):\n    model = cait_models(\n        img_size= 448 , patch_size=16, embed_dim=768, depth=48, num_heads=16, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_scale=1e-6,\n        depth_token_only=2,**kwargs)\n    \n    model.default_cfg = _cfg()\n    if pretrained:\n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=\"https://dl.fbaipublicfiles.com/deit/M48_448.pth\",\n            map_location=\"cpu\", check_hash=True\n        )\n        checkpoint_no_module = {}\n        for k in model.state_dict().keys():\n            checkpoint_no_module[k] = checkpoint[\"model\"]['module.'+k]\n            \n        model.load_state_dict(checkpoint_no_module)\n        \n    return model         \n","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = cait_M48(pretrained=False)\n# load_cait_pretrained(model)\n# model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_cait_pretrained(model):\n    checkpoint = torch.load(CAIT_MODEL, map_location=torch.device('cpu'))\n    checkpoint_no_module = {}\n    for k in model.state_dict().keys():\n        checkpoint_no_module[k] = checkpoint[\"model\"]['module.'+k]\n    model.load_state_dict(checkpoint_no_module)\n    del checkpoint, checkpoint_no_module\n    gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def load_pretrained(model, CLIP_MODEL):\n#     state_dict = torch.jit.load(CLIP_MODEL, map_location=\"cpu\").state_dict()\n#     remove_list = [\"input_resolution\", \"context_length\", \"vocab_size\", \"token_embedding.weight\", 'visual.attnpool']\n#     remove_list += [  \"visual.attnpool.positional_embedding\", \"positional_embedding\", 'visual.proj']\n#     keys = []\n#     for key in state_dict.keys():\n#         for item in remove_list:\n#             if item in key:\n#                 keys.append(key)\n#                 break\n\n#     for key in  keys:           \n#         del state_dict[key]\n#     for key in list(state_dict.keys()):\n#         state_dict[key.replace('visual.', '')] = state_dict.pop(key)\n#     model.load_state_dict(state_dict, strict=False)\n#     del state_dict\n#     gc.collect()\n#     return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def freeze_model(model, requires_grad=False):\n    for param in model.parameters():\n        param.requires_grad = requires_grad\n        \ndef partial_unfreeze1(model):      \n    for m in [model.backbone.blocks_token_only, model.backbone.norm]:\n        freeze_model(m, requires_grad=True)\n#     if 'RN50' in CFG.model_name:\n#         for m in [model.attnpool]:\n#             freeze_model(m, requires_grad=True)\n#     elif 'ViT' in CFG.model_name:\n#         for m in [model.positional_embedding, model.proj]:\n#             m.requires_grad = True\n    \n\ndef partial_unfreeze2(model):\n    if 'RN50' in CFG.model_name:\n        for m in [model.layer4]:\n            freeze_model(m, requires_grad=True)\n    elif 'ViT' in CFG.model_name:\n        for m in [model.ln_post, model.transformer.resblocks[-1]]: \n            freeze_model(m, requires_grad=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Model","metadata":{}},{"cell_type":"code","source":"def load_final_pretrained(final):\n    checkpoint = torch.load(SHOPEE_MODEL, map_location=torch.device('cpu'))\n    checkpoint_no_module = {}\n    checkpoint_no_module['weight'] = checkpoint['final.weight']\n    final.load_state_dict(checkpoint_no_module)\n    del checkpoint, checkpoint_no_module\n    gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ArcMarginProduct(nn.Module):\n    def __init__(self, in_features, out_features, scale=30.0, margin=0.50, easy_margin=False, ls_eps=0.0):\n        super(ArcMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.scale = scale\n        self.margin = margin\n        self.ls_eps = ls_eps  # label smoothing\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(margin)\n        self.sin_m = math.sin(margin)\n        self.th = math.cos(math.pi - margin)\n        self.mm = math.sin(math.pi - margin) * margin\n\n    def forward(self, input, label):\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            if device == 'cuda':\n                phi = phi.to(torch.float16)\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n    \n        one_hot = torch.zeros(cosine.size(), device=device)\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.scale\n        return output, nn.CrossEntropyLoss()(output,label)\n\n\nclass ShopeeModel(nn.Module):\n\n    def __init__(\n        self,\n        n_classes = CFG.classes,\n        model_name = CFG.model_name,\n        fc_dim = CFG.fc_dim,\n        margin = CFG.margin,\n        scale = CFG.scale,\n        use_fc = False,\n        pretrained = True):\n\n        super(ShopeeModel,self).__init__()\n        \n        self.backbone = cait_M36(pretrained=False)\n        if pretrained and not CFG.retrain:\n            load_cait_pretrained(self.backbone)\n        self.backbone.head = nn.Identity()\n        freeze_model(self.backbone)\n#         partial_unfreeze1(self.backbone)\n        self.use_fc = use_fc\n        final_in_features = 768\n        if use_fc:\n            self.dropout = nn.Dropout(p=0.1)\n            self.classifier = nn.Linear(final_in_features, fc_dim)\n            self.bn = nn.BatchNorm1d(fc_dim)\n            self._init_params()\n            final_in_features = fc_dim\n\n        self.final = ArcMarginProduct(\n            final_in_features,\n            n_classes,\n            scale = scale,\n            margin = margin,\n            easy_margin = False,\n            ls_eps = 0.0\n        )\n#         if pretrained and not CFG.retrain:\n#             load_final_pretrained(self.final)\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.classifier.weight)\n        nn.init.constant_(self.classifier.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, image, label):\n        features = self.extract_features(image)\n        if self.training:\n            logits = self.final(features, label)\n            return logits\n        else:\n            return features\n\n    def extract_features(self, x):\n        batch_size = x.shape[0]\n        x = self.backbone.forward_features(x)\n#         x = self.pooling(x).view(batch_size, -1)\n\n        if self.use_fc and self.training:\n            x = self.dropout(x)\n            x = self.classifier(x)\n            x = self.bn(x)\n        return x\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = build_model(embed_dim=512, image_resolution=512, vision_layers=12, vision_width=768, vision_patch_size=32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm\n\nfrom torch.cuda.amp import autocast, GradScaler\nscaler = GradScaler()\n\ndef train_fn(model, data_loader, optimizer, scheduler, epoch, device):\n    model.train()\n    fin_loss = 0.0\n    tk = tqdm(data_loader, desc = \"Training epoch: \" + str(epoch+1))\n\n    for t,data in enumerate(tk):\n        optimizer.zero_grad()\n        for k,v in data.items():\n            data[k] = v.to(device)\n        \n        with autocast():\n            _, loss = model(**data)\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n#         loss.backward()\n#         optimizer.step() \n        fin_loss += loss.item() \n        scheduler.step()\n        tk.set_postfix({'loss' : '%.6f' %float(fin_loss/(t+1)), 'LR' : optimizer.param_groups[0]['lr']})\n\n    \n    return fin_loss / len(data_loader)\n\n\ndef eval_fn(model, data_loader, epoch, device):\n    model.eval()\n    fin_loss = 0.0\n    tk = tqdm(data_loader, desc = \"Validation epoch: \" + str(epoch+1))\n\n    with torch.no_grad():\n        for t,data in enumerate(tk):\n            for k,v in data.items():\n                data[k] = v.to(device)\n\n            _, loss = model(**data)\n            fin_loss += loss.item() \n\n            tk.set_postfix({'loss' : '%.6f' %float(fin_loss/(t+1))})\n        return fin_loss / len(data_loader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_training():\n    \n#     df = pd.read_csv(TRAIN_CSV)\n\n    labelencoder= LabelEncoder()\n    df['label_group'] = labelencoder.fit_transform(df['label_group'])\n\n    trainset = ShopeeDataset(df,\n                             DATA_DIR,\n                             transform = get_train_transforms(img_size = CFG.img_size))\n\n    trainloader = torch.utils.data.DataLoader(\n        trainset,\n        batch_size = CFG.batch_size,\n        num_workers = CFG.num_workers,\n        pin_memory = True,\n        shuffle = True,\n        drop_last = True\n    )\n\n    model = ShopeeModel(use_fc = False)\n    if CFG.retrain:\n        model.load_state_dict(torch.load(CFG.model_path, map_location=torch.device('cpu')), strict=False)\n    model.to(CFG.device)\n    partial_unfreeze1(model)\n\n    optimizer = torch.optim.Adam(model.parameters(),\n                                 lr = CFG.scheduler_params['lr_start'])\n#     scheduler = ShopeeScheduler(optimizer, **CFG.scheduler_params)\n    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer,\n                                                  steps_per_epoch=len(trainloader),\n                                                  pct_start=0.1,\n                                                  div_factor=1e2, \n                                                  max_lr=3e-3,\n                                                  epochs=CFG.epochs)\n\n    for epoch in range(CFG.start_epoch, CFG.start_epoch + CFG.epochs):#         \n#         freeze_model(model, requires_grad=True)\n#         if epoch == 1:\n        \n#             freeze_model(model, requires_grad=True)\n        avg_loss_train = train_fn(model, trainloader, optimizer, scheduler, epoch, CFG.device)\n        torch.save(model.state_dict(), 'arcface_512x512_{}.pt'.format(CFG.model_name))\n        if (epoch+1) % 2 == 0:\n            torch.save(model.state_dict(), f'arcface_512x512_{CFG.model_name}_checkpoints{epoch+1}.pt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run_training()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}