{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<img src=\"https://www.videoandaudiocenter.com/v/vspfiles/assets/images/PriceMatchGuar.jpg\" width = 250 height = 100>\n\n# Description\n\nLet me begin by thanking [Chris Deotte](https://www.kaggle.com/cdeotte) for his outstanding contributions to the data science community. I have been learning a ton from him, and for that, I am forever grateful. This notebook is my submission to the [Shopee - Price Match Guarantee](https://www.kaggle.com/c/shopee-product-matching) competition, and is mainly built upon Chris's great notebooks published [here](https://www.kaggle.com/cdeotte/rapids-cuml-tfidfvectorizer-and-knn) and [here](https://www.kaggle.com/cdeotte/part-2-rapids-tfidfvectorizer-cv-0-700). The goal of the competition is to build a model that predicts which listed items are the same product, so that customers can purchase their desired product at its lowest price. \n\nWe will be using Keras' EfficientNetB0 as well as RAPIDS cuML's TfidfVectorizer and KNN, in order to find items with similar titles and/or images. First we use RAPIDS cuML TfidfVectorizer to extract text embeddings of each item's title and then compare the embeddings using RAPIDS cuML KNN. Next we extract image embeddings of each item with Keras' EfficientNetB0 and compare them using RAPIDS cuML KNN. \n\n#### With that, let the adventure begin! ","metadata":{}},{"cell_type":"code","source":"import numpy as np, pandas as pd, gc\nimport cv2, matplotlib.pyplot as plt\nimport cudf, cuml, cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors\nimport tensorflow as tf\nfrom tensorflow.keras.applications import EfficientNetB0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that to avoid memory issues, we can restrict TensorFlow to 1GB of GPU RAM so that we have 15GB RAM for RAPIDS. According to Chris Deotte's comment in [this post](https://www.kaggle.com/cdeotte/rapids-cuml-tfidfvectorizer-and-knn#1244649), when TensorFlow sees a GPU, it will reserve all the GPU RAM for itself. So we can trick TensorFlow by making a fake GPU with only 1GB RAM. Then TensorFlow only takes the 1GB and the remaining 15GB is left over for RAPIDS to use.","metadata":{}},{"cell_type":"code","source":"LIMIT = 1\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n  try:\n    tf.config.experimental.set_virtual_device_configuration(\n        gpus[0],\n        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*LIMIT)])\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n    #print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n  except RuntimeError as e:\n    print(e)","metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization\n\nIn this section, we will load the training data and create a target column of ground truths so as to compute the CV score. Note that *in order to submit this notebook we should change the variable `COMPUTE_CV` to `False`. However, this variable should be set to `True` when we want to commit the notebook.*","metadata":{}},{"cell_type":"code","source":"COMPUTE_CV = True\n\ntest = pd.read_csv('../input/shopee-product-matching/test.csv')\nif len(test)>3: COMPUTE_CV = False\nelse: print('The submission notebook will compute CV score, but the commit notebook will not')","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/shopee-product-matching/train.csv')\ntmp = train.groupby('label_group').posting_id.agg('unique').to_dict()\ntrain['target'] = train.label_group.map(tmp)\nprint('Train shape is', train.shape )\ntrain.head()","metadata":{"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To start, let's randomly display 20 images out of the training data: ","metadata":{}},{"cell_type":"code","source":"BASE = '../input/shopee-product-matching/train_images/'\n\ndef displayDF(train, random=False, COLS=5, ROWS=4, path=BASE):\n    for k in range(ROWS):\n        plt.figure(figsize=(20,5))\n        for j in range(COLS):\n            if random: row = np.random.randint(0,len(train))\n            else: row = COLS*k + j\n            name = train.iloc[row,1]\n            title = train.iloc[row,3]\n            title_with_return = \"\"\n            for i,ch in enumerate(title):\n                title_with_return += ch\n                if (i!=0)&(i%20==0): title_with_return += '\\n'\n            img = cv2.imread(path+name)\n            plt.subplot(1,COLS,j+1)\n            plt.title(title_with_return)\n            plt.axis('off')\n            plt.imshow(img)\n        plt.show()\n        \ndisplayDF(train,random=True)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's display the top 5 duplicated items, using the column `label_group`, which represents the ground truth in the training data. For brievity, we will only show 5 images for each duplicated item:","metadata":{}},{"cell_type":"code","source":"groups = train.label_group.value_counts()\n\nfor k in range(5):\n    \n    print('-'*22)\n    print('Top', k+1, 'Duplicated Item')  \n    print('-'*22)\n    top = train.loc[train.label_group==groups.index[k]]\n    displayDF(top, random=False, ROWS=1, COLS=5)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that we can also find similar items using the title's text. To achieve this, we can first extract text embeddings using RAPIDS cuML's TfidfVectorizer. This will turn each title into a one-hot-encoding vector. We can then compare the one-hot-encoding vectors with RAPIDS cuML KNN in order to find the similar titles. I am going to skip plotting the similar titles in this section, but we will be using similar titles to create our ML model. \n\n# Modeling\n\nWe now ignore the ground truth of which items are similar and create a model that can identify duplicate items. Let's start by creating a baseline model, where we predict all products with the same `image_phash` as duplicate items.","metadata":{}},{"cell_type":"code","source":"tmp = train.groupby('image_phash').posting_id.agg('unique').to_dict()\ntrain['oof'] = train.image_phash.map(tmp)\n\ndef getMetric(col):\n    def f1score(row):\n        n = len( np.intersect1d(row.target,row[col]) )\n        return 2*n / (len(row.target)+len(row[col]))\n    return f1score\n\ntrain['f1'] = train.apply(getMetric('oof'),axis=1)\nprint('Baseline CV Score =',train.f1.mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's use image embeddings, text embeddings, and phash all together in order to create a more accurate model. As mentioned earlier, in order to submit this notebook we should change the variable `COMPUTE_CV` to `False`. However, this variable should be set to `True` when we want to commit the notebook.","metadata":{}},{"cell_type":"code","source":"if COMPUTE_CV:\n    test = pd.read_csv('../input/shopee-product-matching/train.csv')\n    test_gf = cudf.DataFrame(test)\n    print('This is a commit notebook! Test shape is', test_gf.shape)\nelse:\n    test = pd.read_csv('../input/shopee-product-matching/test.csv')\n    test_gf = cudf.read_csv('../input/shopee-product-matching/test.csv')\n    print('This is a submission notebook! Test shape is', test_gf.shape)\ntest_gf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Method I: Image Embeddings\n\nWe will compute image embeddings in chunks in order to prevent memory errors, and will find similar images with RAPIDS cuML KNN in chunks.","metadata":{}},{"cell_type":"code","source":"class DataGenerator(tf.keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, df, img_size=256, batch_size=32, path=''): \n        self.df = df\n        self.img_size = img_size\n        self.batch_size = batch_size\n        self.path = path\n        self.indexes = np.arange( len(self.df) )\n        \n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        ct = len(self.df) // self.batch_size\n        ct += int(( (len(self.df)) % self.batch_size)!=0)\n        return ct\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        X = self.__data_generation(indexes)\n        return X\n            \n    def __data_generation(self, indexes):\n        'Generates data containing batch_size samples' \n        X = np.zeros((len(indexes),self.img_size,self.img_size,3),dtype='float32')\n        df = self.df.iloc[indexes]\n        for i,(index,row) in enumerate(df.iterrows()):\n            img = cv2.imread(self.path+row.image)\n            X[i,] = cv2.resize(img,(self.img_size,self.img_size)) #/128.0 - 1.0\n        return X","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BASE = '../input/shopee-product-matching/test_images/'\nif COMPUTE_CV: BASE = '../input/shopee-product-matching/train_images/'\n\nWGT = '../input/effnetb0/efficientnetb0_notop.h5'\nmodel = EfficientNetB0(weights=WGT,include_top=False, pooling='avg', input_shape=None)\n\nembeds = []\nCHUNK = 1024*4\n\nprint('Computing image embeddings ...')\nCTS = len(test)//CHUNK\nif len(test)%CHUNK!=0: CTS += 1\nfor i,j in enumerate( range( CTS ) ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(test))\n    print('chunk',a,'to',b)\n    \n    test_gen = DataGenerator(test.iloc[a:b], batch_size=32, path=BASE)\n    image_embeddings = model.predict(test_gen,verbose=1,use_multiprocessing=True, workers=4)\n    embeds.append(image_embeddings)\n\n    #if i>=1: break\n    \ndel model\n_ = gc.collect()\nimage_embeddings = np.concatenate(embeds)\nprint('Image embeddings shape is',image_embeddings.shape)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"KNN = 50\nif len(test)==3: KNN = 2\nmodel = NearestNeighbors(n_neighbors=KNN)\nmodel.fit(image_embeddings)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = []\nCHUNK = 1024*4\n\nprint('Finding similar images ...')\nCTS = len(image_embeddings)//CHUNK\nif len(image_embeddings)%CHUNK!=0: CTS += 1\nfor j in range( CTS ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(image_embeddings))\n    print('chunk',a,'to',b)\n    distances, indices = model.kneighbors(image_embeddings[a:b,])\n    \n    for k in range(b-a):\n        IDX = np.where(distances[k,]<6.0)[0]\n        IDS = indices[k,IDX]\n        o = test.iloc[IDS].posting_id.values\n        preds.append(o)\n        \n_ = gc.collect()","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['preds2'] = preds\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we have fitted the KNN classifier, let's display 5 different items and their 4 closest other images in the train data based on EffNetB0 image embeddings:","metadata":{}},{"cell_type":"code","source":"for k in range(180,185):\n    \n    print('-'*9)\n    print('Example', k-179)\n    print('-'*9)\n    cluster = train.loc[cupy.asnumpy(indices[k,:8])] \n    displayDF(cluster, random=False, ROWS=1, COLS=5)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Method II:  Text Embeddings\n\nSimilarly, we will find similar titles in chunks in order to prevent memory errors. To faciliate this, we will use cosine similarity between text embeddings instead of KNN.","metadata":{}},{"cell_type":"code","source":"print('Computing text embeddings ...')\nmodel = TfidfVectorizer(stop_words='english', binary=True, max_features=25_000)\ntext_embeddings = model.fit_transform(test_gf.title).toarray()\nprint('Text embeddings shape is',text_embeddings.shape)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = []\nCHUNK = 1024*4\n\nprint('Finding similar titles ...')\nCTS = len(test)//CHUNK\nif len(test)%CHUNK!=0: CTS += 1\nfor j in range( CTS ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(test))\n    print('chunk',a,'to',b)\n    \n    # COSINE SIMILARITY DISTANCE\n    cts = cupy.matmul( text_embeddings, text_embeddings[a:b].T).T\n    \n    for k in range(b-a):\n        IDX = cupy.where(cts[k,]>0.7)[0]\n        o = test.iloc[cupy.asnumpy(IDX)].posting_id.values\n        preds.append(o)\n        \ndel model, text_embeddings\n_ = gc.collect()","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['preds'] = preds\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Method III: Phash Feature\n\nFinally, we will be using the phash feature and predict all items with the same phash as duplicates.","metadata":{}},{"cell_type":"code","source":"tmp = test.groupby('image_phash').posting_id.agg('unique').to_dict()\ntest['preds3'] = test.image_phash.map(tmp)\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Method IV: Ensemble Learning\n\nLet's combine the previous models and calculate the CV score:","metadata":{}},{"cell_type":"code","source":"def combine_for_sub(row):\n    x = np.concatenate([row.preds,row.preds2, row.preds3])\n    return ' '.join( np.unique(x) )\n\ndef combine_for_cv(row):\n    x = np.concatenate([row.preds,row.preds2, row.preds3])\n    return np.unique(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if COMPUTE_CV:\n    tmp = test.groupby('label_group').posting_id.agg('unique').to_dict()\n    test['target'] = test.label_group.map(tmp)\n    test['oof'] = test.apply(combine_for_cv,axis=1)\n    test['f1'] = test.apply(getMetric('oof'),axis=1)\n    print('CV Score =', test.f1.mean() )\n\ntest['matches'] = test.apply(combine_for_sub,axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we need to generate a submission file:","metadata":{}},{"cell_type":"code","source":"test[['posting_id','matches']].to_csv('submission.csv',index=False)\nsub = pd.read_csv('submission.csv')\nsub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hope you enjoyed this notebook. Make sure you also check Chris Deotte's [post](https://www.kaggle.com/c/shopee-product-matching/discussion/238033) on how to improve the CV score by using a better decision boundary and removing false negative and false positive which increase metric F1 score. Happy Kaggling :) ","metadata":{}}]}