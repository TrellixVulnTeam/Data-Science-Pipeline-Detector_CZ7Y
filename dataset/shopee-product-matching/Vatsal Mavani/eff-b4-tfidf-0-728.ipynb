{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# About Notebook\n\n* This notebook works with any EfficientNet(B0 - B7) Model.\n* Training Notebook for EfficientNet-B4 can be found [here](https://www.kaggle.com/vatsalmavani/shopee-training-eff-b4)\n\n#### **NOTE:**\n* By changing `threshold` value in `get_image_neighbors` function, one may get higher F1 score.\n* In new version, `text_threshold = 0.75` which increase lb from 0.727 --> 0.728\n","metadata":{}},{"cell_type":"markdown","source":"# Import Packages","metadata":{"papermill":{"duration":0.012782,"end_time":"2021-04-08T07:22:17.192971","exception":false,"start_time":"2021-04-08T07:22:17.180189","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import sys\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.020168,"end_time":"2021-04-08T07:22:17.224865","exception":false,"start_time":"2021-04-08T07:22:17.204697","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport cv2\nimport math\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport albumentations\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torch\nimport timm\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset,DataLoader\n\nimport gc\nimport matplotlib.pyplot as plt\nimport cudf\nimport cuml\nimport cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml import PCA\nfrom cuml.neighbors import NearestNeighbors","metadata":{"papermill":{"duration":8.559641,"end_time":"2021-04-08T07:22:25.796278","exception":false,"start_time":"2021-04-08T07:22:17.236637","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config","metadata":{"papermill":{"duration":0.013167,"end_time":"2021-04-08T07:22:25.823801","exception":false,"start_time":"2021-04-08T07:22:25.810634","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class CFG:\n    seed = 54\n    classes = 11014 \n    scale = 30 \n    margin = 0.5\n    model_name =  'tf_efficientnet_b4'\n    fc_dim = 512\n    img_size = 512\n    batch_size = 20\n    num_workers = 4\n    device = device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    model_path = '../input/utils-shopee/arcface_512x512_tf_efficientnet_b4_LR.pt'","metadata":{"papermill":{"duration":0.029829,"end_time":"2021-04-08T07:22:25.865901","exception":false,"start_time":"2021-04-08T07:22:25.836072","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{"papermill":{"duration":0.012898,"end_time":"2021-04-08T07:22:25.892415","exception":false,"start_time":"2021-04-08T07:22:25.879517","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def read_dataset():\n\n    df = pd.read_csv('../input/shopee-product-matching/test.csv')\n    df_cu = cudf.DataFrame(df)\n    image_paths = '../input/shopee-product-matching/test_images/' + df['image']\n\n    return df, df_cu, image_paths","metadata":{"papermill":{"duration":0.02193,"end_time":"2021-04-08T07:22:25.928194","exception":false,"start_time":"2021-04-08T07:22:25.906264","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_torch(CFG.seed)","metadata":{"papermill":{"duration":0.024009,"end_time":"2021-04-08T07:22:25.964788","exception":false,"start_time":"2021-04-08T07:22:25.940779","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def f1_score(y_true, y_pred):\n    y_true = y_true.apply(lambda x: set(x.split()))\n    y_pred = y_pred.apply(lambda x: set(x.split()))\n    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n    len_y_pred = y_pred.apply(lambda x: len(x)).values\n    len_y_true = y_true.apply(lambda x: len(x)).values\n    f1 = 2 * intersection / (len_y_pred + len_y_true)\n    return f1","metadata":{"papermill":{"duration":0.021228,"end_time":"2021-04-08T07:22:25.998786","exception":false,"start_time":"2021-04-08T07:22:25.977558","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def combine_predictions(row):\n    x = np.concatenate([row['image_predictions'], row['text_predictions']])\n    return ' '.join( np.unique(x) )","metadata":{"papermill":{"duration":0.020011,"end_time":"2021-04-08T07:22:26.031784","exception":false,"start_time":"2021-04-08T07:22:26.011773","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image Predictions","metadata":{"papermill":{"duration":0.012915,"end_time":"2021-04-08T07:22:26.05774","exception":false,"start_time":"2021-04-08T07:22:26.044825","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Create Model\n\n\nclass ArcMarginProduct(nn.Module):\n    def __init__(self, in_features, out_features, scale=30.0, margin=0.50, easy_margin=False, ls_eps=0.0):\n        super(ArcMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.scale = scale\n        self.margin = margin\n        self.ls_eps = ls_eps\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(margin)\n        self.sin_m = math.sin(margin)\n        self.th = math.cos(math.pi - margin)\n        self.mm = math.sin(math.pi - margin) * margin\n\n    def forward(self, input, label):\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n    \n        one_hot = torch.zeros(cosine.size(), device='cuda')\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.scale\n\n        return output, nn.CrossEntropyLoss()(output,label)\n\n\nclass ShopeeModel(nn.Module):\n\n    def __init__(\n        self,\n        n_classes = CFG.classes,\n        model_name = CFG.model_name,\n        fc_dim = CFG.fc_dim,\n        margin = CFG.margin,\n        scale = CFG.scale,\n        use_fc = True,\n        pretrained = True):\n\n        super(ShopeeModel,self).__init__()\n        print('Building Model Backbone for {} model'.format(model_name))\n\n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n        in_features = self.backbone.classifier.in_features\n        self.backbone.classifier = nn.Identity()\n        self.backbone.global_pool = nn.Identity()\n        self.pooling =  nn.AdaptiveAvgPool2d(1)\n        self.use_fc = use_fc\n\n        if use_fc:\n            self.dropout = nn.Dropout(p=0.1)\n            self.classifier = nn.Linear(in_features, fc_dim)\n            self.bn = nn.BatchNorm1d(fc_dim)\n            self._init_params()\n            in_features = fc_dim\n\n        self.final = ArcMarginProduct(\n            in_features,\n            n_classes,\n            scale = scale,\n            margin = margin,\n            easy_margin = False,\n            ls_eps = 0.0\n        )\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.classifier.weight)\n        nn.init.constant_(self.classifier.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, image, label):\n        features = self.extract_features(image)\n        if self.training:\n            logits = self.final(features, label)\n            return logits\n        else:\n            return features\n\n    def extract_features(self, x):\n        batch_size = x.shape[0]\n        x = self.backbone(x)\n        x = self.pooling(x).view(batch_size, -1)\n\n        if self.use_fc and self.training:\n            x = self.dropout(x)\n            x = self.classifier(x)\n            x = self.bn(x)\n        return x\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_image_neighbors(df, embeddings, KNN=50):\n\n    model = NearestNeighbors(n_neighbors = KNN)\n    model.fit(embeddings)\n    distances, indices = model.kneighbors(embeddings)\n    \n    threshold = 4.5\n    predictions = []\n    for k in tqdm(range(embeddings.shape[0])):\n        idx = np.where(distances[k,] < threshold)[0]\n        ids = indices[k,idx]\n        posting_ids = df['posting_id'].iloc[ids].values\n        predictions.append(posting_ids)\n        \n    del model, distances, indices\n    gc.collect()\n    return df, predictions","metadata":{"papermill":{"duration":0.028647,"end_time":"2021-04-08T07:22:26.099725","exception":false,"start_time":"2021-04-08T07:22:26.071078","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_test_transforms():\n    return albumentations.Compose([\n        albumentations.Resize(CFG.img_size, CFG.img_size, always_apply=True),\n        albumentations.Normalize(),\n        ToTensorV2(p=1.0)\n    ])","metadata":{"papermill":{"duration":0.020889,"end_time":"2021-04-08T07:22:26.133794","exception":false,"start_time":"2021-04-08T07:22:26.112905","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ShopeeDataset(Dataset):\n\n    def __init__(self, image_paths, transforms=None):\n        self.image_paths = image_paths\n        self.augmentations = transforms\n\n    def __len__(self):\n        return self.image_paths.shape[0]\n\n    def __getitem__(self, index):\n        image_path = self.image_paths[index]\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        if self.augmentations:\n            augmented = self.augmentations(image=image)\n            image = augmented['image']\n        \n        return image, torch.tensor(1)","metadata":{"papermill":{"duration":0.022922,"end_time":"2021-04-08T07:22:26.169849","exception":false,"start_time":"2021-04-08T07:22:26.146927","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_image_embeddings(image_paths):\n\n    model = ShopeeModel(pretrained=False).to(CFG.device)\n    model.load_state_dict(torch.load(CFG.model_path))\n    model.eval()\n\n    image_dataset = ShopeeDataset(image_paths=image_paths, transforms=get_test_transforms())\n    image_loader = torch.utils.data.DataLoader(\n        image_dataset,\n        batch_size=CFG.batch_size,\n        num_workers=CFG.num_workers\n    )\n\n    embeds = []\n    with torch.no_grad():\n        for img,label in tqdm(image_loader): \n            img = img.cuda()\n            label = label.cuda()\n            features = model(img,label)\n            image_embeddings = features.detach().cpu().numpy()\n            embeds.append(image_embeddings)\n\n    del model\n    image_embeddings = np.concatenate(embeds)\n    print(f'Our image embeddings shape is {image_embeddings.shape}')\n    del embeds\n    gc.collect()\n    return image_embeddings","metadata":{"papermill":{"duration":0.023211,"end_time":"2021-04-08T07:22:26.206301","exception":false,"start_time":"2021-04-08T07:22:26.18309","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text Predictions","metadata":{"papermill":{"duration":0.013634,"end_time":"2021-04-08T07:22:26.233358","exception":false,"start_time":"2021-04-08T07:22:26.219724","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def get_text_predictions(df, max_features=25_000):\n    \n    model = TfidfVectorizer(stop_words='english',\n                            binary=True,\n                            max_features=max_features)\n    text_embeddings = model.fit_transform(df_cu['title']).toarray()\n\n    print('Finding similar titles...')\n    CHUNK = 1024 * 4\n    CTS = len(df) // CHUNK\n    if (len(df)%CHUNK) != 0:\n        CTS += 1\n\n    preds = []\n    for j in range( CTS ):\n        a = j * CHUNK\n        b = (j+1) * CHUNK\n        b = min(b, len(df))\n        print('chunk', a, 'to', b)\n\n        # COSINE SIMILARITY DISTANCE\n        cts = cupy.matmul(text_embeddings, text_embeddings[a:b].T).T\n        for k in range(b-a):\n            IDX = cupy.where(cts[k,]>0.75)[0]\n            o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n            preds.append(o)\n\n    del model,text_embeddings\n    gc.collect()\n    return preds","metadata":{"papermill":{"duration":0.021709,"end_time":"2021-04-08T07:22:26.268516","exception":false,"start_time":"2021-04-08T07:22:26.246807","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Calculating Predictions","metadata":{"papermill":{"duration":0.013521,"end_time":"2021-04-08T07:22:26.295982","exception":false,"start_time":"2021-04-08T07:22:26.282461","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df,df_cu,image_paths = read_dataset()\ndf.head()","metadata":{"papermill":{"duration":8.204288,"end_time":"2021-04-08T07:22:34.513817","exception":false,"start_time":"2021-04-08T07:22:26.309529","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get neighbors for image_embeddings\n\nimage_embeddings = get_image_embeddings(image_paths.values)\ntext_predictions = get_text_predictions(df, max_features=25_000)\ndf, image_predictions = get_image_neighbors(df, image_embeddings, KNN=50 if len(df)>3 else 3)\ndf.head()","metadata":{"papermill":{"duration":728.942245,"end_time":"2021-04-08T07:34:43.469993","exception":false,"start_time":"2021-04-08T07:22:34.527748","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing Submission","metadata":{"papermill":{"duration":0.451128,"end_time":"2021-04-08T07:34:44.38534","exception":false,"start_time":"2021-04-08T07:34:43.934212","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df['image_predictions'] = image_predictions\ndf['text_predictions'] = text_predictions\ndf['matches'] = df.apply(combine_predictions, axis=1)\ndf[['posting_id', 'matches']].to_csv('submission.csv', index=False)","metadata":{"papermill":{"duration":1.056403,"end_time":"2021-04-08T07:34:45.894444","exception":false,"start_time":"2021-04-08T07:34:44.838041","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]}]}