{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Shopee product matching triplet data conversion to TFRecords","metadata":{}},{"cell_type":"markdown","source":"Kaggle allows for competitors to expedite deep learning model training via tensor processing units (TPUs). However, it is necessary to convert the data into a TFRecord format and feed these files through Google Cloud Storage (GCS). [Conversion to TFRecords enables one to fully take advantage of the extra processing power TPUs provide by avoiding data bottlenecks](https://www.kaggle.com/docs/tpu)\n\nThis notebook demonstrates the encoding of the Shopee image data into TFRecords. Based on [mattbast's work in the Google Landmark Retrieval 2020 competition](https://www.kaggle.com/mattbast/google-landmarks-2020-create-a-tfrecord-dataset/notebook)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport io\nfrom PIL import Image\n\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom kaggle_datasets import KaggleDatasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For large datasets, it is good practice to split TFRecords into \"shards\". ","metadata":{}},{"cell_type":"code","source":"NUM_SHARDS = 16\nIMAGE_SIZE = (600, 600)\nSEED=42\n\nIMAGE_DIR = '../input/shopee-product-matching/train_images'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Basic EDA for confirmation","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/shopee-product-matching/train.csv')\ndisplay(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.groupby(['label_group'])['posting_id'].nunique().sort_values()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This confirms that the maximum size of a label group to be 50, as stated in the competition rules.","metadata":{}},{"cell_type":"markdown","source":"## Detecting matches\n\nThe following two cells create a list of matches. This is not required, but may be useful for model training.","metadata":{}},{"cell_type":"code","source":"match_map = df.groupby(['label_group'])['posting_id'].unique().to_dict()\ndf['matches'] = df['label_group'].map(match_map)\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_mapper = dict(zip(df['label_group'].unique(), np.arange(len(df['label_group'].unique()))))\ndf['label_group'] = df['label_group'].map(label_mapper)\ndisplay(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Convert dataframe to TFRecords","metadata":{}},{"cell_type":"code","source":"def encode_image(filepath, method='bilinear'):\n    image_string = tf.io.read_file(filepath)\n    image = tf.image.decode_jpeg(image_string, channels=3)\n    #Must convert dtype to float32 for most resizing methods to work\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    image = tf.image.resize(image, IMAGE_SIZE, method=method, antialias=True)\n    #Convert dtype to uint8 to be encoded to bytestring for tfrec\n    image = tf.image.convert_image_dtype(image, tf.uint8)\n    image = tf.image.encode_jpeg(image, optimize_size=True)\n    return image\n\ndef featurize(val):\n    if isinstance(val, (bytes, str, tf.Tensor)):\n        if isinstance(val, type(tf.constant(0))):\n            val = val.numpy() \n        elif isinstance(val, str):\n            val = str.encode(val)\n        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[val]))\n    elif isinstance(val, (int, np.integer)):\n        return tf.train.Feature(int64_list=tf.train.Int64List(value=[val]))\n    elif isinstance(val, (float, np.floating)):\n        return tf.train.Feature(float_list=tf.train.FloatList(value=[val]))\n    else:\n        raise Exception(f'Cannot featurize due to type {type(val)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def serialize_example(row):    \n    feature = row.to_dict()\n    img_path = os.path.join(IMAGE_DIR, feature['image'])\n    feature['image'] = encode_image(img_path)\n    feature['matches'] = tf.io.serialize_tensor(tf.convert_to_tensor(feature['matches']))\n    for k,v in feature.items():\n        feature[k] = featurize(v)\n        \n    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n    return example_proto.SerializeToString()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def write_tfr(df, filepath, filename, file_index, file_size, image_indexes):       \n    with tf.io.TFRecordWriter(f'{filepath}/{filename}%.2i.tfrec'%(file_index)) as writer:\n        start = file_size * file_index\n        end = file_size * (file_index + 1)\n        for i in tqdm(image_indexes[start:end]):\n            example = serialize_example(df.loc[i])\n            writer.write(example)\n\ndef to_tfr(df, filepath, filename):\n    if not os.path.exists(filepath):\n        os.makedirs(filepath)\n    image_indexes = df.index.values\n    file_size = len(image_indexes) // 15\n    file_count = len(image_indexes) // file_size + int(len(image_indexes) % file_size != 0)\n    for file_index in range(file_count):\n        print('Writing TFRecord %i of %i...'%(file_index, file_count))\n        write_tfr(df, filepath, filename, file_index, file_size, image_indexes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_tfr(df, './train', 'train')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}