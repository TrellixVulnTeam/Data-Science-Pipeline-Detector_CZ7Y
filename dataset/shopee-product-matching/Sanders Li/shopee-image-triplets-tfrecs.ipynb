{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Shopee product matching triplet data conversion to TFRecords","metadata":{}},{"cell_type":"markdown","source":"[To avoid data bottlenecks in TPU, convert data to TFRecords](https://www.kaggle.com/docs/tpu)\n\nThis notebook demonstrates the encoding of triplet sets into TFRecords. Based on [mattbast's work in the Google Landmark Retrieval 2020 competition](https://www.kaggle.com/mattbast/google-landmarks-2020-create-a-tfrecord-dataset/notebook)\n\nFor the construction of triplet sets, refer to [xhlulu's excellent notebook](https://www.kaggle.com/xhlulu/shopee-generate-data-for-triplet-loss)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport io\nfrom PIL import Image\n\nfrom tqdm.notebook import tqdm\nimport random\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom kaggle_datasets import KaggleDatasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_SHARDS = 16\nIMAGE_SIZE = (224, 224)\nSEED=42","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/shopee-product-matching/train.csv')\ndisplay(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_triplets(df):\n    random.seed(SEED)\n    label_group = dict(list(df.groupby('label_group')))\n    \n    def aux(row):\n        anchor = row['image']\n        \n        # We sample a positive data point from the same group, but\n        # exclude the anchor itself\n        ids = label_group[row['label_group']]['image'].tolist()\n        ids.remove(row['image'])\n        positive = random.choice(ids)\n        \n        # Now, this will sample a group from all possible groups, then sample \n        # a product from that group\n        groups = list(label_group.keys())\n        groups.remove(row['label_group'])\n        neg_group = random.choice(groups)\n        negative = random.choice(label_group[neg_group]['image'].tolist())\n\n        return anchor, positive, negative\n    \n    return aux","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"triplet_sets = df.apply(generate_triplets(df), axis=1).tolist()\ntriplet_sets = pd.DataFrame(triplet_sets, columns=['anchor', 'positive', 'negative'])\ndisplay(triplet_sets)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#triplet_sets = pd.read_csv('../input/generate-triplet-data/triplet_sets.csv')\ntriplet_paths = triplet_sets.applymap(lambda x: os.path.join('../input/shopee-product-matching/train_images', x))\ndisplay(triplet_paths)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_paths, test_paths = train_test_split(triplet_paths, train_size=0.8, random_state=SEED)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data pipeline prototyping","metadata":{}},{"cell_type":"markdown","source":"### What method should be used to resize images?","metadata":{}},{"cell_type":"code","source":"#Compare resize methods\nfilepath = train_paths.iloc[1,0]\n\nmethods = ['bilinear', 'lanczos3', 'lanczos5', 'bicubic', 'gaussian', 'nearest', 'area', 'mitchellcubic']\nfig = plt.figure(figsize=(30, 30))\nax = fig.subplots(3,3)\n\nimage_string = tf.io.read_file(filepath)\noriginal = tf.image.decode_jpeg(image_string, channels=3)\nax[0,0].imshow(original)\nax[0,0].set_title('original', fontsize=24)\n\nfor i, method in enumerate(methods):\n    image = original\n    \n    #Must convert dtype to float32 for most resizing methods to work\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    image = tf.image.resize(image, IMAGE_SIZE, method=method, antialias=True)\n    \n    subplot = (i+1)//3, (i+1)%3\n    ax[subplot].imshow(image)\n    ax[subplot].set_title(method, fontsize=24)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All look pretty comparable. \n\nRanking: \"Bilinear\" > ... > \"area\" > \"nearest\"","metadata":{}},{"cell_type":"markdown","source":"### When should resizing occur in the data pipeline?","metadata":{}},{"cell_type":"code","source":"def encode(image, method, resize=True):\n    if resize:\n        #Must convert dtype to float32 for most resizing methods to work\n        image = tf.image.convert_image_dtype(image, tf.float32)\n        image = tf.image.resize(image, IMAGE_SIZE, method=method, antialias=True)\n        #Convert dtype to uint8 to be encoded to bytestring for tfrec\n        image = tf.image.convert_image_dtype(image, tf.uint8)\n    image = tf.image.encode_jpeg(image, optimize_size=True)\n    return image\n\n# Example decoding func\ndef decode(image, method, resize=True):\n    image = tf.image.decode_jpeg(image, channels=3)\n    if resize:\n        image = tf.cast(image, tf.uint8) / 255\n        image = tf.image.resize(image, IMAGE_SIZE, method=method, antialias=True) \n    return image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Compare resize methods\nfilepath = train_paths.iloc[1,0]\n\nmethods = ['bilinear', 'lanczos3', 'lanczos5', 'bicubic', 'gaussian', 'nearest', 'area', 'mitchellcubic']\nfig = plt.figure(figsize=(24, 9))\nax = fig.subplots(3,8)\n\nimage_string = tf.io.read_file(filepath)\noriginal = tf.image.decode_jpeg(image_string, channels=3)\n\n# Mimic the data pipeline as shown in \n# Encoding to tfrec - https://www.kaggle.com/mattbast/google-landmarks-2020-create-a-tfrecord-dataset/notebook\n# Decoding from tfrec - https://www.kaggle.com/mattbast/google-landmark-retrieval-triplet-loss/data\nfor j in range(3):\n    if j == 0:\n        encode_resize, decode_resize = True, True\n    elif j == 1:\n        encode_resize, decode_resize = True, False\n    else:\n        encode_resize, decode_resize = False, True\n    \n    for i, method in enumerate(methods):\n        image = original\n        #encode to tfrec\n        image = encode(image, method, resize=encode_resize)\n        #decode from tfrec\n        image = decode(image, method, resize=decode_resize)\n        \n        ax[j,i].imshow(image)\n        title = ' resize' + (' encode' if encode_resize else '') + (' decode' if decode_resize else '')\n        ax[j,i].set_title(method + title)\n        ax[j,i].axis('off')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Seems that only \"bilinear\", \"gaussian\", \"nearest\", and \"area\" survive the original pipeline implementation for whatever reason","metadata":{}},{"cell_type":"code","source":"#Compare resize methods\nfilepath = train_paths.iloc[1,0]\n\nmethods = ['bilinear', 'gaussian', 'nearest', 'area']\nfig = plt.figure(figsize=(32, 24))\nax = fig.subplots(3,4)\n\nimage_string = tf.io.read_file(filepath)\noriginal = tf.image.decode_jpeg(image_string, channels=3)\n\nfor j in range(3):\n    if j == 0:\n        encode_resize, decode_resize = True, True\n    elif j == 1:\n        encode_resize, decode_resize = True, False\n    else:\n        encode_resize, decode_resize = False, True\n    \n    for i, method in enumerate(methods):\n        image = original\n        #encode to tfrec\n        image = encode(image, method, resize=encode_resize)\n        #decode from tfrec\n        image = decode(image, method, resize=decode_resize)\n        \n        ax[j,i].imshow(image)\n        title = ' resize' + (' encode' if encode_resize else '') + (' decode' if decode_resize else '')\n        ax[j,i].set_title(method + title, fontsize=24)\n        ax[j,i].axis('off')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Best results seem to occur when images are resized during preprocessing. \n\nThe remainder of the code in this notebook will resize the images via the \"bilinear\" method to 224x224 for EfficientNetb0","metadata":{}},{"cell_type":"markdown","source":"## Triplet encoding to tfrecs","metadata":{}},{"cell_type":"code","source":"def encode_image(filepath, method='bilinear'):\n    image_string = tf.io.read_file(filepath)\n    image = tf.image.decode_jpeg(image_string, channels=3)\n    #Must convert dtype to float32 for most resizing methods to work\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    image = tf.image.resize(image, IMAGE_SIZE, method=method, antialias=True)\n    #Convert dtype to uint8 to be encoded to bytestring for tfrec\n    image = tf.image.convert_image_dtype(image, tf.uint8)\n    image = tf.image.encode_jpeg(image, optimize_size=True)\n    return image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize(df_paths):\n\n    def show(ax, image):\n        ax.imshow(image)\n        ax.axis('off')\n\n    fig = plt.figure(figsize=(9, 9))\n\n    axs = fig.subplots(3, 3)\n    for i in range(3):\n        triplet = df_paths.iloc[i, 0:3]\n        for j in range(3):\n            image = encode_image(triplet[j])\n            image = tf.image.decode_jpeg(image, channels=3)\n            show(axs[i,j], image)\n\nvisualize(train_paths)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _bytes_feature(value):\n    if isinstance(value, type(tf.constant(0))):\n        value = value.numpy() \n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def serialize_example(row):    \n    anchor_img = encode_image(row['anchor'])\n    positive_img = encode_image(row['positive'])\n    negative_img = encode_image(row['negative'])\n    \n    feature = {\n        'anchor_img': _bytes_feature(anchor_img),\n        'positive_img': _bytes_feature(positive_img),\n        'negative_img': _bytes_feature(negative_img),\n    }\n    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n    return example_proto.SerializeToString()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def write_tfrecord_file(df, filepath, filename, file_index, file_size, image_indexes):       \n    with tf.io.TFRecordWriter(f'{filepath}/{filename}%.2i.tfrec'%(file_index)) as writer:\n        start = file_size * file_index\n        end = file_size * (file_index + 1)\n        for i in tqdm(image_indexes[start:end]):\n            example = serialize_example(df.loc[i])\n            writer.write(example)\n\ndef df_to_tfrecords(df, filepath, filename):\n    if not os.path.exists(filepath):\n        os.makedirs(filepath)\n    image_indexes = df.index.values\n    file_size = len(image_indexes) // 15\n    file_count = len(image_indexes) // file_size + int(len(image_indexes) % file_size != 0)\n    for file_index in range(file_count):\n        print('Writing TFRecord %i of %i...'%(file_index, file_count))\n        write_tfrecord_file(df, filepath, filename, file_index, file_size, image_indexes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_to_tfrecords(train_paths, './train', 'train')\ndf_to_tfrecords(test_paths, './test', 'test')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}