{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### About me \nI am a physicist, performing statistical analysis of PetaBytes of physics data to make new discoveries. I was part of the team who discovered the famous \"God Particle\" or known as Higgs boson. I am here to try my data science skills on the real world industry problems. \n\n\nIf you like this notbook please upvote to motivate me further to add more useful information in future.  \n\n\n\n### Table of contents: \n* [Introduction](#introduction)\n * [What to expect in this article](#expect)\n\n\n* [Load the libraries](#loadlib)\n* [Load the dataset and basic operations](#loaddata)\n* [Explore data with similar phash](#explorephash)\n* [Explore data with similar image label](#exploreimglabel)\n\n### Introduction <a class=\"anchor\" id=\"introduction\"></a>\n\n\n\nAim: The goal of the competetion is to find images which describe or represent same product. The target can be achieved by understanding better the images of the product itself and the metadata associated with these images. The metadata is basically the title of the image. \n\n#### What to expect in this notbook <a class=\"anchor\" id=\"expect\"></a> \nThis notebook use the image data and its associated text metadata to identify the same products.  \n##### Strategy \nWe have three set of information available, \n1. Image \n2. Metadata of Image, i.e. title \n3. image_phash \n\nWe can ideally use all three to identify if two (or more) images correspond to same product or not. \n\nLet me guide you through these three steps and see how useful each of these infomration are and how to combine them and what else can be tried? \n\n","metadata":{}},{"cell_type":"code","source":"\ntrain_img='/kaggle/input/shopee-product-matching/train_images/'\ntrain_csv='/kaggle/input/shopee-product-matching/train.csv'\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load the libraries <a class=\"anchor\" id=\"loadlib\"></a>\n\n","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport cv2\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load the dataset and basic operations <a class=\"anchor\" id=\"loaddata\"></a>","metadata":{}},{"cell_type":"code","source":"\ndf_train  = pd.read_csv(train_csv)\n\n!ls /kaggle/input/shopee-product-matching/train_images/ | wc -l \n\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.columns\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.shape\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df_train.posting_id.unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df_train.image_phash.unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df_train[\"label_group\"].unique())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before we can start playing aruond with the data, it is important to understand what exactly is stored in it. The csv file/ dataframe has 5 columns. \n* 'posting_id': is a unique string associated with each image. \n* 'image: has the name of the image which can be used for traning. Note that train directory has less images when compared with images name listed in the .csv file\n* 'image_phash': phash stands for Perceptual hashing. phash is an algorithm which in simple words creates a finger print for an image. If the finger prints of two images are same then the images are likely same. Looking at the unique values we can notice there are images which have same phash values. One can understand that the present competetion is about creating a new kind of hash which can then be used to identify the similar images.\n* 'title': This is the string which describe the product in the image. \n* 'label_group': represent the groups. There are 11014 unique groups of product in the training dataset. ","metadata":{}},{"cell_type":"markdown","source":"1. ### Explore data with similar phash <a class=\"anchor\" id=\"explorephash\"></a>","metadata":{}},{"cell_type":"code","source":"def listofimages(df, feature):\n    mode_=df[feature].value_counts().index[0]\n    df = df[df[feature]==mode_]\n    return [os.path.join(train_img,iimg) for iimg in df.image.to_list()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.image_phash.value_counts()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"label_group count simply tells that there is no product which has no repetition, so in the input dataset, there is atleast one partner for each of the image. ","metadata":{}},{"cell_type":"code","source":"df_train.label_group.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* The frequency distribution below gives a hint how often the images might have a match in the present dataset. \n* Almost half of the dataset is with those images which have only one matching image and rest have more than two and goes upto 51. ","metadata":{}},{"cell_type":"code","source":"import seaborn as sns \nnlist = [2,3,4,5,6,7,8,9,10]\nx1=np.array(nlist)\ny_=[]\nfor i in  nlist: \n    if i<6:\n        y_.append (np.sum(df_train.label_group.value_counts()==i) )\n    else:\n        y_.append (np.sum(df_train.label_group.value_counts()>=i) )\ny1 = np.array(y_)\n\nsns.barplot(x=x1,y=y1)\n#print (x1,y1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I will use this function very often \n\nimport matplotlib.pyplot as plt \nimport cv2\n\ndef display_multiple_img(images_paths, rows, cols):\n    figure, ax = plt.subplots(nrows=rows,ncols=cols,figsize=(18,88) )\n    for ind,image_path in enumerate(images_paths):\n        image=cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n        try:\n            ax.ravel()[ind].imshow(image)\n            ax.ravel()[ind].set_axis_off()\n        except:\n            continue;\n    plt.tight_layout()\n    plt.show()\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The Notebook using RAPID, Resnet18 and cosine distance (score 0.712) \nYou can checkout the notebook with RAPIDs and Resnet and measuring the similarity using cosine distance at this link: \n\n\n* In the following part discussing some of the corner cases I found while exploring the cases when images are not tagged as same category. \n* The case here refer to 6 images which represent same item but the label_group is given different in the training dataset. \n\n### Let's check what are those 6 images. \nThings to notice: \n1. The 6 images belong to **two** label_groups \n2. Three of them have exact same title, but remaining have slightly different \n3. Image visualisation, they all looks very similar. \n","metadata":{}},{"cell_type":"code","source":"image_list=df_train[ (df_train.posting_id=='train_3386243561') | \n                   (df_train.posting_id=='train_2120597446') |\n                   (df_train.posting_id=='train_3423213080')|\n                   (df_train.posting_id=='train_1816968361') |\n                   (df_train.posting_id=='train_1831941588') |\n                   (df_train.posting_id=='train_3805508898')\n ]\n\nimage_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_list.title.values.tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimage_list_path = [os.path.join(train_img,iimage) for iimage in image_list.image.values.tolist()]\ndisplay_multiple_img(image_list_path,3,2)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When cropping is added in the ","metadata":{}},{"cell_type":"markdown","source":"**Conclusion:** Looking at this one example we can guess that the data is not as clean as one might expect and same will be true for the test dataset as well. And this will effect the performance of the tagging on both seen and unseen dataset. \n\n1. The images in one label_group can be tagged as another very easibly which will lead to low score at the end. \nIf you have some idea to tackle situation like these please write down in comment. \n\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## Looking at the features \n\n* Looking at the title and label_group one can guess why this particular ,image_phash has highest frequency in the dataset. This is what we can say looking at these two features. One more feature of dataset can be seen from image column. A given image is repeated multiple times. They have same hash but they have different entry more likely due to different title. Let's try to visualize these 26 images. ","metadata":{}},{"cell_type":"code","source":"img_TopOcc = df_train[df_train.image_phash==\"fad28daa2ad05595\"].image.to_list()\nimg_TopOcc_ = [os.path.join(train_img,iimg) for iimg in img_TopOcc]\ndisplay_multiple_img(img_TopOcc_,13,2) ## showing first 25\nimg_TopOcc_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As expected, they all are same. Lets check the images based on the label. ","metadata":{}},{"cell_type":"markdown","source":"### Explore data with similar image label <a class=\"anchor\" id=\"exploreimglabel\"></a>","metadata":{}},{"cell_type":"code","source":"list_label = listofimages(df_train,\"label_group\")\nlen(list_label)\ndisplay_multiple_img(list_label,17,3)\n#list_label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[df_train.image=='f9dc2cf9ed811fec7cbc9d5120638f0c.jpg']['title'].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have seen some basic features of the data, lets try to dig in more. \n\nI personally think that we need to first make fragments of the problem and then combine the solutions. Therefore the fragments should be in a way that they can be combined later on, and solutions should be in a form or format that they can be combined as easy as plug and play. \n\nTo me it can be divided into at least three fragments: \n\n1. Text associated with the image, i.e. title.\n2. Image itself has a lot of information, the image of the product can be used to decide whether  two images are similar or not. \n3. I will focus on remaining 1-2 fragments later. \n\nLet's focus on these two for now. \n\n","metadata":{}},{"cell_type":"markdown","source":"### Detecting the image similarity using the images. \n\nFollowing is still work in progress, \n\n","metadata":{}},{"cell_type":"code","source":"#tmp = df_train.groupby('label_group')[\"posting_id\"].agg('unique').to_dict()\n#df_train.label_group.map(tmp)\n\n\ntmp = df_train.groupby(\"image_phash\")[\"posting_id\"].agg('unique').to_dict()\n#df_train.image_phash.map(tmp).values.tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Detecting the image similarity using the title of the image. \n\nFor this purpose I will be using some of the natural processing tools to make a meaningful dataset which can be understood by the numpy and ML algorithms. The process mainly involves converting the text into numbers. Lets see how can be achieve this. ","metadata":{}},{"cell_type":"markdown","source":"Lets check some features of the images now. ","metadata":{}},{"cell_type":"code","source":"import os\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\nimport numpy as np\nnp.random.seed(2018)\nfrom gensim.models import Word2Vec\nimport nltk\nnltk.download('wordnet')\nstemmer = SnowballStemmer('english')\n\nfrom numpy import dot\nfrom numpy.linalg import norm\nimport pandas as pd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\ntext_ = \"\"\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\n'''","metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The very first step is to tokenize the string using nltk and find unique tokens, or lemmatising the verbs. ","metadata":{}},{"cell_type":"code","source":"'''text_tokens_ = nltk.word_tokenize(text_)\ntext_tokens_\ntext_tokens_1 = nltk.Text(text_tokens_)\ntext_tokens_1\nset(text_tokens_)\nwlemma = WordNetLemmatizer()\ndoc1 = [wlemma.lemmatize(w,'v') for w in text_tokens_1]\ndoc2 = doc1 \n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\nvectors = vectorizer.fit_transform([text_])\nvectors\nfeature_names = vectorizer.get_feature_names()\n\ndense = vectors.todense()\ndense\ndenselist = dense.tolist()\ndenselist\ndf = pd.DataFrame(denselist, columns=feature_names)\ndf\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\ntitles = df_train.title.values.tolist()\ntitles_skim = titles[:5000]\ntitles_skim\nvectors = vectorizer.fit_transform(titles_skim)\nfeature_names = vectorizer.get_feature_names()\nlen(feature_names)\n'''\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\ndense = vectors.todense()\ndense\ndenselist = dense.tolist()\ndenselist\ndf = pd.DataFrame(denselist, columns=feature_names)\n\ndf\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stay tuned for more details","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}