{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nimport os\nimport gc\nimport cv2\nimport math\nimport time\nimport tqdm\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import KFold,StratifiedKFold\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, lr_scheduler, AdamW\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom torch.optim.lr_scheduler import (CosineAnnealingWarmRestarts, CosineAnnealingLR, \n                                      ReduceLROnPlateau)\n\nfrom sklearn.preprocessing import LabelEncoder\n\nimport albumentations as A \nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport transformers\nfrom transformers import AutoModel, AutoTokenizer\n\nfrom colorama import Fore, Back, Style\ny_ = Fore.YELLOW\nr_ = Fore.RED\ng_ = Fore.GREEN\nb_ = Fore.BLUE\nm_ = Fore.MAGENTA\nc_ = Fore.CYAN\nsr_ = Style.RESET_ALL","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_path = '../input/shopee-product-matching/train_images/'\ntest_path = '../input/shopee-product-matching/test_images/'\n\ntrain_data = pd.read_csv('../input/shopee-product-matching/train.csv')\ntest_data = pd.read_csv('../input/shopee-product-matching/test.csv')\n\ntrain_data['image_paths'] = train_path + train_data['image']\ntest_data['image_paths'] = test_path + test_data['image']\n\nle = LabelEncoder()\ntrain_data['label'] = le.fit_transform(train_data['label_group'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = {\n    'learning_rate':0.01,\n    'train_batch_size':12,\n    'valid_batch_size':12,\n    'accumulation_step':4,\n    'epochs':15,\n    'nfolds':5,\n    'seed':42,\n    \n    's':30.0,\n    'm':0.5,\n    'ls_eps':0.0,\n    'easy_margin':False,\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(seed=config['seed'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" class ShopeeDataset(nn.Module):\n    def __init__(self,df,tokenizer):\n        self.titles = df['title'].to_numpy()\n        self.labels = df['label'].to_numpy()\n        self.tokenizer = tokenizer\n    \n    def __getitem__(self,idx):\n        label = torch.as_tensor(self.labels[idx],dtype=torch.long)\n        encode = self.tokenizer(self.titles[idx],return_tensors='pt',padding='max_length',truncation=True)\n        return encode,label\n    \n    def __len__(self):\n        return len(self.titles)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ArcMarginProduct(nn.Module):\n    def __init__(self, in_features, out_features, s=30.0, m=0.50, easy_margin=False, ls_eps=0.0):\n        super(ArcMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.s = s\n        self.m = m\n        self.ls_eps = ls_eps  \n        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(m)\n        self.sin_m = math.sin(m)\n        self.th = math.cos(math.pi - m)\n        self.mm = math.sin(math.pi - m) * m\n\n    def forward(self, input, label):\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight)).to('cuda')\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2)).to('cuda')\n        phi = (cosine * self.cos_m - sine * self.sin_m).to('cuda')\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n        one_hot = torch.zeros(cosine.size(), device='cuda')\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.s\n\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self,backbone):\n        super(Model,self).__init__()\n        self.backbone = backbone\n        self.in_features = self.backbone.pooler.dense.in_features\n        self.dropout = nn.Dropout(0.2)\n        self.final = ArcMarginProduct(self.in_features, 11014,\n                                      s=config['s'], m=config['m'],\n                                      easy_margin=config['easy_margin'], \n                                      ls_eps=config['ls_eps'])\n    \n    def forward(self,x,label):\n        output = self.backbone(**x)\n        x = output['last_hidden_state'][:,0,:]\n        x = self.dropout(x)\n        x = self.final(x,label)\n        return x  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL_PATH = 'xlm-roberta-base'\nbackbone = AutoModel.from_pretrained(MODEL_PATH)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\ntokenizer.save_pretrained('bert_tokenizer')\n\n# model.save_pretrained('bert_model')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run(plot_losses=True, verbose=True):\n        \n    def loss_fn(inputs,targets):\n        return nn.CrossEntropyLoss()(inputs,targets)\n    \n    def train_loop(train_loader, model, loss_fn, device,optimizer,lr_scheduler=None):\n        model.train()\n        total_loss = 0\n        model.zero_grad(set_to_none=True)\n        for i, (inputs,targets) in enumerate(train_loader):\n            inputs = {key:val.reshape(val.shape[0],-1).to(device) for key,val in inputs.items()}\n            targets = targets.to(device)\n            outputs = model(inputs,targets)\n            loss = loss_fn(outputs,targets) / config['accumulation_step']\n            loss.backward()\n            if (i + 1) % config['accumulation_step'] == 0:\n                optimizer.step()\n                model.zero_grad(set_to_none=True)\n                \n            total_loss += loss.item()\n        total_loss /= len(train_loader)\n        return total_loss\n    \n#     scaler = torch.cuda.amp.GradScaler()\n    \n#     def train_loop(train_loader, model, loss_fn, device,optimizer,lr_scheduler=None):\n#         model.train()\n#         total_loss = 0\n#         for i, (inputs,targets) in enumerate(train_loader):\n#             inputs, targets = inputs.to(device), targets.to(device)\n#             optimizer.zero_grad()\n#             with torch.cuda.amp.autocast():\n#                 outputs = model(inputs,targets)\n#                 loss = loss_fn(outputs,targets)\n                \n#             scaler.scale(loss).backward()\n#             scaler.step(optimizer)\n#             scaler.update()\n            \n#             total_loss += loss.item()\n#         total_loss /= len(train_loader)\n#         return total_loss\n    \n    def valid_loop(valid_loader, model, loss_fn, device):\n        model.eval()\n        total_loss = 0\n        with torch.no_grad():\n            for i, (inputs,targets) in enumerate(valid_loader):\n                inputs = {key:val.reshape(val.shape[0],-1).to(device) for key,val in inputs.items()}\n                targets = targets.to(device)\n                outputs = model(inputs,targets)\n                loss = loss_fn(outputs,targets)\n                total_loss += loss.item()\n            total_loss /= len(valid_loader)\n        return total_loss \n    \n    fold_train_losses = list()\n    fold_valid_losses = list()\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"{device} is used\")\n    \n#     train = train_data.sample(n=5000).reset_index(drop=True)\n#     kfold = KFold(n_splits=config['nfolds'])\n    train = train_data\n    kfold = StratifiedKFold(n_splits=config['nfolds'])\n    for k , (train_idx,valid_idx) in enumerate(kfold.split(X=train,y=train['label'])):\n        x_train,x_valid = train.loc[train_idx],train.loc[valid_idx]\n\n        train_ds = ShopeeDataset(x_train,tokenizer)\n        train_dl = DataLoader(train_ds,\n                              batch_size = config[\"train_batch_size\"],\n                              shuffle=True,\n                              num_workers = 4,\n                              pin_memory=True\n                             )\n\n        valid_ds = ShopeeDataset(x_valid,tokenizer)\n        valid_dl = DataLoader(valid_ds,\n                              batch_size = config[\"valid_batch_size\"],\n                              shuffle=False,\n                              num_workers = 4,\n                              pin_memory=True,\n                              drop_last=False,\n                             )\n        \n        model = Model(backbone)\n        model.to(device)\n        \n        optimizer = optim.AdamW(model.parameters(),lr=config['learning_rate'])\n#         lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=2, verbose=True)\n#         lr_scheduler = optim.lr_scheduler.StepLR(optimizer,step_size=10,gamma=0.8)\n        lr_scheduler = None\n    \n\n        print(f\"Fold {k}\")\n        best_loss = 99999\n        \n        train_losses = list()\n        valid_losses = list()\n        start = time.time()\n        for i in range(config[\"epochs\"]):\n            train_loss = train_loop(train_dl,model,loss_fn,device,optimizer,lr_scheduler=lr_scheduler)\n            valid_loss = valid_loop(valid_dl,model,loss_fn,device)\n            \n            if lr_scheduler:\n                lr_scheduler.step(valid_loss)\n\n            train_losses.append(train_loss)\n            valid_losses.append(valid_loss)\n            \n            end = time.time()\n            epoch_time = end - start\n            start = end\n                                      \n            if verbose:\n                print(f\"epoch:{i} Training loss:{train_loss} | Validation loss:{valid_loss}| epoch time {epoch_time:.2f}s \")\n\n            if valid_loss <= best_loss:\n                if verbose:\n                    print(f\"{g_}Validation loss Decreased from {best_loss} to {valid_loss}{sr_}\")\n                best_loss = valid_loss\n                torch.save(model.state_dict(),f'bert.bin')\n                \n        fold_train_losses.append(train_losses)\n        fold_valid_losses.append(valid_losses)\n        break\n        \n    if plot_losses == True:\n        plt.figure(figsize=(20,14))\n        for i, (t,v) in enumerate(zip(fold_train_losses,fold_valid_losses)):\n            plt.subplot(2,5,i+1)\n            plt.title(f\"Fold {i}\")\n            plt.plot(t,label=\"train_loss\")\n            plt.plot(v,label=\"valid_loss\")\n            plt.legend()\n        plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}