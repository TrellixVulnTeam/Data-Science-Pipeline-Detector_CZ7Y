{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n'''\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n'''\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd \n\ndf = pd.read_csv('/kaggle/input/shopee-product-matching/train.csv')\nlen(df), df['label_group'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Product matching using cotrastive learning\n\n<img src=\"https://lh6.googleusercontent.com/UVN-ArRhK7YeFcmWaTyPE8Qzqmt1cU_9Krnupz77OIW0qu3cT8GLuDvQEzuwGbN4kWLEwfkkdkaSAyhJPJUC2oY1MvmkU-Ghitj4XzRCvMCnUNgkpfVKphXTLyMhc4tqyF6OhnTB\" width=\"750\" align=\"center\">","metadata":{}},{"cell_type":"markdown","source":"## install 3rd party pip packages","metadata":{}},{"cell_type":"code","source":"#!pip install pytorch-lightning pytorch-metric-learning torchvision faiss-cpu scikit-learn\n!pip install ../input/wheels/faiss_cpu-1.7.0-cp37-cp37m-manylinux2014_x86_64.whl\n!pip install ../input/wheels/pytorch_metric_learning-0.9.98-py3-none-any.whl\n!pip install ../input/wheels/timm-0.4.5-py3-none-any.whl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## define custom dataset","metadata":{}},{"cell_type":"code","source":"# custom dataset\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\n\nfrom PIL import Image\nfrom tqdm import tqdm\n\nimport torch\nimport os, sys\nimport json\nimport pandas as pd\nimport PIL\nimport argparse\nimport random\nimport matplotlib.pyplot as plt\n\nclass ProductPairDataset(Dataset):\n    def __init__(self, df, root_dir, img_size=260, train_mode=True, test_mode=False, transform=None):\n        \"\"\"\n        Args:\n            df (DataFrame): part of entire dataframe\n            root_dir (str): root image path\n            transform (callable, optional): Optional transform to be applied on a sample.\n        \"\"\"\n        self.products_frame = df\n        self.root_dir = root_dir\n        self.train_mode = train_mode\n        self.test_mode = test_mode\n        \n        self.images = []\n        self.labels = []\n\n        if transform is not None:\n            self.transform = transform\n        else:\n            if self.train_mode:  # set default image tranform\n                self.transform = transforms.Compose(\n                    [\n                        transforms.Resize([int(img_size // 0.9), int(img_size // 0.9)]),\n                        transforms.RandomCrop([img_size, img_size]),\n                        transforms.RandomHorizontalFlip(),\n                        transforms.ToTensor(),\n                        transforms.Normalize(\n                            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n                        ),\n                    ]\n                )\n            else:\n                self.transform = transforms.Compose(\n                    [\n                        transforms.Resize([img_size, img_size]),\n                        transforms.ToTensor(),\n                        transforms.Normalize(\n                            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n                        ),\n                    ]\n                )\n\n    def __len__(self):\n        return len(self.products_frame)\n\n    def __getitem__(self, index):\n        image_tensor = self.transform(PIL.Image.open(self.root_dir + os.sep + self.products_frame.iloc[index][\"image\"]))\n        \n        if not self.test_mode:\n            label = int(self.products_frame.iloc[index][\"label_group\"])\n            label_tensor = torch.LongTensor([label])\n\n            return image_tensor, label_tensor\n        \n        return image_tensor","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## implementation background of this","metadata":{}},{"cell_type":"markdown","source":"## ContrastiveLoss based on cosine similarity\n\n![](https://kevinmusgrave.github.io/pytorch-metric-learning/imgs/contrastive_loss_similarity_equation.png)\n\nappropriate values would be **pos_margin = 1 and neg_margin = 0**, in case of cosine similarity","metadata":{}},{"cell_type":"markdown","source":"## using in-batch constrastive loss\n\n![In-batch constrastive learning](https://d3i71xaburhd42.cloudfront.net/bbe55736e6f4681c54ec4a889b9b12b6e4c25b56/2-Figure1-1.png)","metadata":{}},{"cell_type":"markdown","source":"## It restrict only in-batch pairs relation! How to expand it?\n\n![](https://i.ytimg.com/vi/SDKDSvv9oTk/maxresdefault.jpg)","metadata":{}},{"cell_type":"markdown","source":"## model definition","metadata":{}},{"cell_type":"code","source":"## model definition\nfrom torchvision import transforms\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data.sampler import BatchSampler\n\nfrom pytorch_metric_learning import miners, losses\nfrom pytorch_metric_learning.distances import CosineSimilarity\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport pytorch_lightning as pl\n\nimport PIL\nimport pandas as pd\nimport random\n\nclass ProductFeatureNet(nn.Module):\n    def __init__(self, backbone_net, feature_dim=768):\n        super(ProductFeatureNet, self).__init__()\n\n        self.feature_dim = feature_dim\n        self.backbone_net = backbone_net\n        self.feature_layer = nn.Linear(\n            #self.backbone_net.classifier.out_features, self.feature_dim, bias=False\n            self.backbone_net.fc.out_features, self.feature_dim, bias=False\n        )\n        nn.init.xavier_uniform_(self.feature_layer.weight)\n\n    def forward(self, images):\n        features = self.backbone_net(images)\n        features = self.feature_layer(features)\n        features = F.normalize(features)\n\n        return features\n\nclass ProductFeatureEncoder(pl.LightningModule):\n    def __init__(\n        self,\n        model,\n        margin=0.5,\n        lr=1e-3,\n        lr_patience=2,\n        lr_decay_ratio=0.5,\n        memory_batch_max_num=1024,\n    ):\n        super().__init__()\n\n        self.save_hyperparameters()\n\n        self.model = model\n        self.margin = margin\n        self.lr = lr\n        self.lr_patience = lr_patience\n        self.lr_decay_ratio = lr_decay_ratio\n\n        self.memory_batch_max_num = memory_batch_max_num\n        \n        self.loss_func = losses.CrossBatchMemory(\n            losses.ContrastiveLoss(pos_margin=1, neg_margin=0, distance=CosineSimilarity()),\n            self.model.feature_dim, \n            memory_size=self.memory_batch_max_num, \n            miner=miners.MultiSimilarityMiner(epsilon=self.margin)\n        )\n        \n    def forward(self, images):\n        features = self.model(images)\n\n        return features\n\n    def configure_optimizers(self):\n        optim = torch.optim.Adam(\n            [\n                {\"params\": self.model.backbone_net.parameters(), \"lr\": self.lr * 0.1},\n                {\"params\": self.model.feature_layer.parameters()},\n                \n            ],\n            #self.parameters(),\n            lr=self.lr,\n        )\n        \n        return {\n            \"optimizer\": optim,\n            \"lr_scheduler\": ReduceLROnPlateau(\n                optim,\n                patience=self.lr_patience,\n                threshold=1e-8,\n                factor=self.lr_decay_ratio,\n            ),\n            \"monitor\": \"val_loss\",\n        }\n\n    def training_step(self, train_batch, batch_idx):\n        self.model.train()\n\n        images, labels = train_batch\n        features = self.model(images)\n        \n        xbm_loss = self.loss_func(features, labels.squeeze(1))\n        self.log(\"train/loss\", xbm_loss, prog_bar=True)\n        \n        return xbm_loss\n\n    def validation_step(self, validation_batch, batch_idx):\n        self.model.eval()\n\n        images, labels = validation_batch\n        features = self.model(images)\n\n        with torch.no_grad():\n            xbm_loss = self.loss_func(features, labels.squeeze(1))\n            self.log(\"train/loss\", xbm_loss, prog_bar=True)\n            \n            return {\n                \"features\": features,\n                \"labels\": labels,\n                \"val_loss\": xbm_loss,\n            }\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## define custom batch sampler for ensuring positive pair portion including","metadata":{}},{"cell_type":"code","source":"## positive pair augment custom batch sampler\nclass PositivePairAugBatchSampler(BatchSampler):\n    def __init__(self, dataset_df, min_positive_instances=4, num_labels_per_batch=16):\n        #self.max_iter = len(dataset_df) // (num_labels_per_batch)\n        self.max_iter = len(dataset_df) // (min_positive_instances * num_labels_per_batch)\n        self.min_positive_instances = min_positive_instances\n        self.num_labels_per_batch = num_labels_per_batch\n\n        self.label_index_dict = {}  # key: batch, value: [batch_indices]\n        for label in dataset_df[\"label_group\"]:\n            self.label_index_dict[label] = [index for index in list(dataset_df[dataset_df[\"label_group\"] == label].index) if index < len(dataset_df)]\n\n        delete_label_list = []\n        for k, v in self.label_index_dict.items():\n            if len(v) == 0:\n                delete_label_list.append(k)\n        for label in delete_label_list:\n            del self.label_index_dict[label]\n        \n    def __len__(self):\n        return self.max_iter\n\n    def __iter__(self):\n        for _ in range(self.max_iter):\n            batch_indices = []\n\n            selected_labels = random.choices(\n                list(self.label_index_dict.keys()), k=self.num_labels_per_batch\n            )\n\n            for label in selected_labels:\n                batch_indices.extend(\n                    random.choices(\n                        self.label_index_dict[label], k=self.min_positive_instances\n                    )\n                )\n\n            yield batch_indices","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## import pre-trained backbone model for extracting image feature","metadata":{}},{"cell_type":"code","source":"import timm\nimport torch\n\n'''\nbackbone_net = timm.create_model('tf_efficientnet_b0')\nbackbone_net.load_state_dict(torch.load('../input/timm-pretrained-efficientnet/efficientnet/tf_efficientnet_b0_aa-827b6e33.pth'))\n\n\nbackbone_net = timm.create_model('tf_efficientnet_es')\nbackbone_net.load_state_dict(torch.load('../input/timm-pretrained-efficientnet/efficientnet/tf_efficientnet_es-ca1afbfe.pth'))\n\nbackbone_net = timm.create_model('tf_efficientnet_b0_ap')\nbackbone_net.load_state_dict(torch.load('../input/timm-pretrained-efficientnet/efficientnet/tf_efficientnet_b0_ap-f262efe1.pth'))\n\nbackbone_net = timm.create_model('tf_efficientnet_b1_ns')\nbackbone_net.load_state_dict(torch.load('../input/timm-pretrained-efficientnet/efficientnet/tf_efficientnet_b1_ns-99dd0c41.pth'))\n\nbackbone_net = timm.create_model('tf_efficientnet_b2_ns')\nbackbone_net.load_state_dict(torch.load('../input/timm-pretrained-efficientnet/efficientnet/tf_efficientnet_b2_ns-00306e48.pth'))\n'''\n\nbackbone_net = timm.create_model('resnet101')\nbackbone_net.load_state_dict(torch.load('../input/timm-pretrained-resnet/resnet/resnet101-5d3b4d8f.pth'))\n\nbackbone_net","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## model training start(based on pytorch-lightning way)","metadata":{}},{"cell_type":"code","source":"%%time\n\nfrom pytorch_lightning.metrics.functional import f1, accuracy\nfrom pytorch_lightning.callbacks import EarlyStopping\nfrom torch.utils.data import DataLoader, random_split\nfrom sklearn.utils import shuffle\nfrom tqdm import tqdm\n\nimport pandas as pd\nimport multiprocessing\nimport numpy as np\nimport faiss\n\nargs = {\n    # model parameters\n    'feature_dim' : 512,\n    \n    # training parameters\n    'epochs': 20,\n    'margin': 0.4,\n    'lr' : 4e-4,\n    'lr_patience': 2,\n    'early_stop_patience': 4,\n    'lr_decay_ratio':0.1,\n    'batch' : 128,\n    'memory_batch_max_num' : 2048,\n    \n    # dataset parameters\n    'train_portion' : 0.95,\n    'train_csv_file' : \"/kaggle/input/shopee-product-matching/train.csv\",\n    'train_root_dir' : \"/kaggle/input/shopee-product-matching/train_images\"\n}\n\n# Init model\nembedding_net = ProductFeatureNet(\n    backbone_net=backbone_net, \n    feature_dim=args['feature_dim']\n)\nproduct_encoder = ProductFeatureEncoder(\n    model=embedding_net, lr=args['lr'], margin=args['margin'], memory_batch_max_num=args['memory_batch_max_num']\n)\n\n# Init DataLoader from Custom Dataset\ndataset_df = pd.read_csv(args['train_csv_file'])\ndataset_df = shuffle(dataset_df)\n\ntrain_df = dataset_df[: int(len(dataset_df) * args['train_portion'])]\ntrain_batch_sampler = PositivePairAugBatchSampler(train_df)\ntrain_dataset = ProductPairDataset(\n    df=train_df,\n    root_dir=args['train_root_dir'],\n    train_mode=True,\n)\ntrain_loader = DataLoader(\n    train_dataset,\n    num_workers=multiprocessing.cpu_count(),\n    pin_memory=True,\n    batch_sampler = train_batch_sampler\n)\n\nvalid_df = dataset_df[len(train_df):]\nvalid_batch_sampler = PositivePairAugBatchSampler(valid_df)\nvalid_dataset = ProductPairDataset(\n    df=valid_df,\n    root_dir=args['train_root_dir'],\n    train_mode=False,\n)\nvalid_loader = DataLoader(\n    valid_dataset, \n    num_workers=multiprocessing.cpu_count(),\n    batch_sampler = valid_batch_sampler\n)\n\ntest_loader = DataLoader(\n    valid_dataset, \n    num_workers=multiprocessing.cpu_count(),\n    batch_size=args['batch']\n)\n\nearly_stopping = EarlyStopping(\"val_loss\", patience=args['early_stop_patience'])\n\n# Initialize a trainer\ntrainer = pl.Trainer(\n    gpus=torch.cuda.device_count(),\n    progress_bar_refresh_rate=1,\n    accelerator=\"ddp\",\n    max_epochs=args['epochs'],\n    #callbacks=[early_stopping],\n    tpu_cores=8,\n    replace_sampler_ddp=False\n)\n\n# Train the model\nprint ('training start!')\ntrainer.fit(product_encoder, train_loader, valid_loader)\nprint ('training end!')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## pulling image features using trained model","metadata":{}},{"cell_type":"code","source":"#del train_dataset\n#del train_loader\n\ndataset_df = pd.read_csv(args['train_csv_file'])\n\nvalid_dataset = ProductPairDataset(\n    df=dataset_df,\n    root_dir=args['train_root_dir'],\n    train_mode=False,\n)\nvalid_loader = DataLoader(\n    valid_dataset, batch_size=args['batch'], num_workers=multiprocessing.cpu_count(), shuffle=False\n)\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nproduct_encoder.model = product_encoder.model.to(device)\nproduct_encoder.model.eval()\n\nembeddings = None\n\n# store image feature embedding iterating over data\nfor images, labels in tqdm(valid_loader, desc='storing image features ...'):\n    images = images.to(device)\n    with torch.no_grad():\n        features = product_encoder.model(images)\n        \n        if embeddings is None:\n            embeddings = features.cpu()\n        else:\n            embeddings = torch.cat([embeddings, features.cpu()])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## find similarity threshold for optimal f1 score","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(args['train_csv_file'])\n\nmatches_column = []\nfor i in tqdm(range(len(df)), desc='matching target posting_ids ...'):\n    matches_column.append(' '.join(list(df[df['label_group']==df.iloc[i]['label_group']]['posting_id'])))\ndf['matches'] = matches_column\n\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"index = faiss.IndexFlatIP(args['feature_dim'])\nindex.add(embeddings.numpy())\n\ndistances, indices = index.search(embeddings.numpy(), k=50) # search max 50 candidates\ndistances, indices","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# search similarity threshold for optimal f1 score\ndef f1_score(y_true, y_pred):\n    '''\n    precision = len(intersection(y_true, y_pred)) / len(y_pred)\n    recall = len(intersection(y_true, y_pred)) / len(y_true)\n    f1 = (2 * (precision * recall)) / (precision + recall) => (2 * len(intersection(y_true, y_pred))) / (len(y_true) + len(y_pred))\n    '''\n    #print (f'y_true: {y_true}')\n    #print (f'y_pred: {y_pred}')\n    \n    intersection = list(set(y_true) & set(y_pred))\n    \n    return (2 * len(intersection)) / float(len(y_true) + len(y_pred))\n\n\nmax_f1 = 0.0\nsimilarity_threshold = 0.0\n\nfor threshold in tqdm(np.arange(0.6, 1.0, 0.05), desc='searching similarity threshold for optimal f1 score ...'):\n    matches_pred = []\n    for distance, index in zip(distances, indices):\n        selected_distance = list(np.where(distance >= threshold))[0]\n        each_matches_pred = []\n        for selected_index in selected_distance:\n            each_matches_pred.append(df.iloc[index[selected_index]].values[0]) # posting_id\n            \n        matches_pred.append(' '.join(each_matches_pred))\n\n    df['matches_pred'] = matches_pred\n    #print (df.head())\n    \n    df['f1'] = df.apply(lambda row: f1_score(row['matches'], row['matches_pred']), axis=1)\n\n    print (f\"f1 score of similarity threshold({threshold}): {df['f1'].mean()}\")\n    \n    if df['f1'].mean() > max_f1:\n        similarity_threshold = threshold\n        max_f1 = df['f1'].mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generate submission file\ntest_dataset = ProductPairDataset(\n    df=pd.read_csv('../input/shopee-product-matching/test.csv'),\n    root_dir='../input/shopee-product-matching/test_images',\n    train_mode=False,\n    test_mode=True\n)\ntest_loader = DataLoader(\n    test_dataset, batch_size=args['batch'], num_workers=multiprocessing.cpu_count(), shuffle=False\n)\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nproduct_encoder.model = product_encoder.model.to(device)\nproduct_encoder.model.eval()\n\nembeddings = None\n\n# store image feature embedding iterating over data\nfor images in tqdm(test_loader, desc='storing image features ...'):\n    images = images.to(device)\n    with torch.no_grad():\n        features = product_encoder.model(images)\n        \n        if embeddings is None:\n            embeddings = features.cpu()\n        else:\n            embeddings = torch.cat([embeddings, features.cpu()])\n            \nindex = faiss.IndexFlatIP(args['feature_dim'])\nindex.add(embeddings.numpy())\n\ndistances, indices = index.search(embeddings.numpy(), k=50) # search max 50 candidates\n\ndistances, indices, similarity_threshold","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## generate submission file","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/shopee-product-matching/test.csv')\n\nmatches_pred = []\nfor distance, index in zip(distances, indices):\n    selected_distance = list(np.where(distance >= similarity_threshold))[0]\n    each_matches_pred = []\n    for selected_index in selected_distance:\n        each_matches_pred.append(df.iloc[index[selected_index]].values[0]) # posting_id\n\n    matches_pred.append(' '.join(each_matches_pred))\n    \ndf['matches_pred'] = matches_pred\n\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\ntorch.mm(embeddings[0].unsqueeze(0), embeddings[33231].unsqueeze(0).t())\n\nimg_array = np.array(Image.open(f\"{args['train_root_dir']}/{df.iloc[0]['image']}\"))\nplt.imshow(img_array)\n\nimg_array = np.array(Image.open(f\"{args['train_root_dir']}/{df.iloc[33231]['image']}\"))\nplt.imshow(img_array)\n'''\n\nwith open('submission.csv', 'w') as resultFile:\n    resultFile.write('posting_id,matches')\n    for i in range(len(df)):\n        resultFile.write(f\"\\n{df.iloc[i]['posting_id']},{df.iloc[i]['matches_pred']}\")\n        \n!cat submission.csv","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}