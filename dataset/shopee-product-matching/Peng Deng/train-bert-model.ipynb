{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport copy\nimport math\nimport pandas as pd\nimport numpy as np\nfrom tqdm.autonotebook import tqdm\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nimport transformers\nfrom transformers import (BertTokenizer, BertModel,\n                          DistilBertTokenizer, DistilBertModel)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/shopee-product-matching/train.csv\")\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title_lengths = train['title'].apply(lambda x: len(x.split(\" \"))).to_numpy()\nprint(f\"MIN words: {title_lengths.min()}, MAX words: {title_lengths.max()}\")\nplt.hist(title_lengths)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    DistilBERT = True # if set to False, BERT model will be used\n    bert_hidden_size = 768\n    \n    batch_size = 64\n    epochs = 100\n    num_workers = 4\n    learning_rate = 1e-5 \n    scheduler = \"ReduceLROnPlateau\"\n    step = 'epoch'\n    patience = 2\n    factor = 0.8\n    dropout = 0.5\n    model_path = \"/kaggle/working\"\n    max_length = 30\n    model_save_name = \"bert_model.pt\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CFG.DistilBERT:\n    model_name='cahya/distilbert-base-indonesian'\n    tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n    bert_model = DistilBertModel.from_pretrained(model_name)\nelse:\n    model_name='cahya/bert-base-indonesian-522M'\n    tokenizer = BertTokenizer.from_pretrained(model_name)\n    bert_model = BertModel.from_pretrained(model_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## pre-process","metadata":{}},{"cell_type":"code","source":"lbl_encoder = LabelEncoder()\ntrain['label_code'] = lbl_encoder.fit_transform(train['label_group'])\nNUM_CLASSES = train['label_code'].nunique()\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TextDataset(torch.utils.data.Dataset):\n    def __init__(self, dataframe, tokenizer, mode=\"train\", max_length=None):\n        self.dataframe = dataframe\n        if mode != \"test\":\n            self.targets = dataframe['label_code'].values\n        texts = list(dataframe['title'].apply(lambda o: str(o)).values)\n        self.encodings = tokenizer(texts, \n                                   padding=True, \n                                   truncation=True, \n                                   max_length=max_length)\n        self.mode = mode\n        \n        \n    def __getitem__(self, idx):\n        # putting each tensor in front of the corresponding key from the tokenizer\n        # HuggingFace tokenizers give you whatever you need to feed to the corresponding model\n        item = {key: torch.tensor(values[idx]) for key, values in self.encodings.items()}\n        # when testing, there are no targets so we won't do the following\n        if self.mode != \"test\":\n            item['labels'] = torch.tensor(self.targets[idx]).long()\n        return item\n    \n    def __len__(self):\n        return len(self.dataframe)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ArcMarginProduct(nn.Module):\n    def __init__(self, in_features, out_features, s=30.0, m=0.50, easy_margin=False):\n        super(ArcMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.s = s\n        self.m = m\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(m)\n        self.sin_m = math.sin(m)\n        self.th = math.cos(math.pi - m)\n        self.mm = math.sin(math.pi - m) * m\n\n    def forward(self, input, label):\n        # --------------------------- cos(theta) & phi(theta) ---------------------------\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt((1.0 - torch.pow(cosine, 2)).clamp(0, 1))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n        # --------------------------- convert label to one-hot ---------------------------\n        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n        one_hot = torch.zeros(cosine.size(), device=CFG.device)\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)  # you can use torch.where if your torch.__version__ is 0.4\n        output *= self.s\n        \n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, \n                 bert_model, \n                 num_classes=NUM_CLASSES, \n                 last_hidden_size=CFG.bert_hidden_size):\n        \n        super().__init__()\n        self.bert_model = bert_model\n        self.arc_margin = ArcMarginProduct(last_hidden_size, \n                                           num_classes, \n                                           s=30.0, \n                                           m=0.50, \n                                           easy_margin=False)\n    \n    def get_bert_features(self, batch):\n        output = self.bert_model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n        last_hidden_state = output.last_hidden_state # shape: (batch_size, seq_length, bert_hidden_dim)\n        CLS_token_state = last_hidden_state[:, 0, :] # obtaining CLS token state which is the first token.\n        return CLS_token_state\n    \n    def forward(self, batch):\n        CLS_hidden_state = self.get_bert_features(batch)\n        output = self.arc_margin(CLS_hidden_state, batch['labels'])\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loss_list = []\ntrain_acc_list = []\nval_loss_list = []\nval_acc_list = []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AvgMeter:\n    def __init__(self, name=\"Metric\"):\n        self.name = name\n        self.reset()\n    \n    def reset(self):\n        self.avg, self.sum, self.count = [0]*3\n    \n    def update(self, val, count=1):\n        self.count += count\n        self.sum += val * count\n        self.avg = self.sum / self.count\n    \n    def __repr__(self):\n        text = f\"{self.name}: {self.avg:.4f}\"\n        return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def one_epoch(model, \n              criterion, \n              loader,\n              optimizer=None, \n              lr_scheduler=None, \n              mode=\"train\", \n              step=\"batch\"):\n    \n    loss_meter = AvgMeter()\n    acc_meter = AvgMeter()\n    \n    tqdm_object = tqdm(loader, total=len(loader))\n    for batch in tqdm_object:\n        batch = {k: v.to(CFG.device) for k, v in batch.items()}\n        preds = model(batch)\n        loss = criterion(preds, batch['labels'])\n        if mode == \"train\":\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            if step == \"batch\":\n                lr_scheduler.step()\n                \n        count = batch['input_ids'].size(0)\n        loss_meter.update(loss.item(), count)\n        \n        accuracy = get_accuracy(preds.detach(), batch['labels'])\n        acc_meter.update(accuracy.item(), count)\n        if mode == \"train\":\n            tqdm_object.set_postfix(train_loss=loss_meter.avg, accuracy=acc_meter.avg, lr=get_lr(optimizer))\n\n        else:\n            tqdm_object.set_postfix(valid_loss=loss_meter.avg, accuracy=acc_meter.avg)\n    \n    return loss_meter, acc_meter","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group[\"lr\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_accuracy(preds, targets):\n    preds = preds.argmax(dim=1)\n    acc = (preds == targets).float().mean()\n    return acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_eval(epochs, model, train_loader, valid_loader, \n               criterion, optimizer, lr_scheduler=None):\n    \n    best_loss = float('inf')\n    best_model_weights = copy.deepcopy(model.state_dict())\n    \n    for epoch in range(epochs):\n        print(\"*\" * 30)\n        print(f\"Epoch {epoch + 1}\")\n        current_lr = get_lr(optimizer)\n        \n        model.train()\n        train_loss, train_acc = one_epoch(model, \n                                          criterion, \n                                          train_loader, \n                                          optimizer=optimizer,\n                                          lr_scheduler=lr_scheduler,\n                                          mode=\"train\",\n                                          step=CFG.step)                     \n        train_loss_list.append(train_loss.avg)\n        train_acc_list.append(train_acc.avg)\n        model.eval()\n        with torch.no_grad():\n            valid_loss, valid_acc = one_epoch(model, \n                                              criterion, \n                                              valid_loader, \n                                              optimizer=None,\n                                              lr_scheduler=None,\n                                              mode=\"valid\")\n            val_loss_list.append(valid_loss.avg)\n            val_acc_list.append(valid_acc.avg)\n        \n        if valid_loss.avg < best_loss:\n            best_loss = valid_loss.avg\n            best_model_weights = copy.deepcopy(model.state_dict())\n            torch.save(model.state_dict(), f'{CFG.model_path}/{CFG.model_save_name}')\n            print(\"Saved best model!\")\n        \n        if isinstance(lr_scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n            lr_scheduler.step(valid_loss.avg)\n            if current_lr != get_lr(optimizer):\n                print(\"Loading best model weights!\")\n                model.load_state_dict(torch.load(f'{CFG.model_path}/{CFG.model_save_name}', \n                                                 map_location=CFG.device))\n        \n        print(\"*\" * 30)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df, valid_df = train_test_split(train, \n                                      test_size=0.33, \n                                      shuffle=True, \n                                      random_state=42, \n                                      stratify=train['label_code'])\n\ntrain_dataset = TextDataset(train_df, tokenizer, max_length=CFG.max_length)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, \n                                           batch_size=CFG.batch_size, \n                                           num_workers=CFG.num_workers, \n                                           shuffle=True)\n\nvalid_dataset = TextDataset(valid_df, tokenizer, max_length=CFG.max_length)\nvalid_loader = torch.utils.data.DataLoader(valid_dataset, \n                                           batch_size=CFG.batch_size, \n                                           num_workers=CFG.num_workers, \n                                           shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(bert_model).to(CFG.device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=CFG.learning_rate)\nif CFG.scheduler == \"ReduceLROnPlateau\":\n    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n                                                              mode=\"min\", \n                                                              factor=CFG.factor, \n                                                              patience=CFG.patience)\n\ntrain_eval(CFG.epochs, model, train_loader, valid_loader,\n           criterion, optimizer, lr_scheduler=lr_scheduler)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loss_list = np.array(train_loss_list)\ntrain_acc_list = np.array(train_acc_list)\nval_loss_list = np.array(val_loss_list)\nval_acc_list = np.array(val_acc_list)\n\nnp.save('bert_train_loss_list.npy', train_loss_list)\nnp.save('bert_train_acc_list.npy', train_acc_list)\nnp.save('bert_val_loss_list.npy', val_loss_list)\nnp.save('bert_val_acc_list.npy', val_acc_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir tokenizer\ntokenizer.save_pretrained(\"./tokenizer\")\ntorch.save(model.state_dict(), \"final.pt\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(train_loss_list)\nplt.title('BERT')\nplt.xlabel('epoch')\nplt.ylabel('training loss')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(train_acc_list)\nplt.title('BERT')\nplt.xlabel('epoch')\nplt.ylabel('training accuracy')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(val_loss_list)\nplt.title('BERT')\nplt.xlabel('epoch')\nplt.ylabel('validation loss')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(val_acc_list)\nplt.title('BERT')\nplt.xlabel('epoch')\nplt.ylabel('validation accuracy')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}