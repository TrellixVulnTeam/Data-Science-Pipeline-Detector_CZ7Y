{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Workflow\n\n- Image\n    - Extract **image embeddings of each item** with EffNetB0\n    - Compare them using RAPIDS cuML KNN\n- Text\n    - RAPIDS cuML TfidfVectorizer to extract **text embeddings of each item's title**\n    - Compare the embeddings using RAPIDS cuML KNN/ Cosine Similarity \n    \nIn this notebook, we use embedding to find a baseline model. To **further improve your score** click here to explore more: [Eff-B4 + TFIDF w/ CV for threshold_searching\n](https://www.kaggle.com/chienhsianghung/eff-b4-tfidf-w-cv-for-threshold-searching)","metadata":{"id":"kG530zAnB3fC"}},{"cell_type":"markdown","source":"# Setting","metadata":{"id":"s7AEKHeZCuUf"}},{"cell_type":"markdown","source":"## Colab","metadata":{"id":"0IJI3y8_C7Lt"}},{"cell_type":"code","source":"COLAB = False\ndefault_dir = None\n\nif COLAB:\n    from google.colab import drive\n    drive.mount('/content/drive')\n    default_dir = '/content'\n\n    # Install RAPIDS\n    !git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n    !bash rapidsai-csp-utils/colab/rapids-colab.sh\n\n    import sys, os\n\n    dist_package_index = sys.path.index('/usr/local/lib/python3.6/dist-packages')\n    # ValueError: '/usr/local/lib/python3.6/dist-packages' is not in list\n    sys.path = sys.path[:dist_package_index] + ['/usr/local/lib/python3.6/site-packages'] + sys.path[dist_package_index:]\n    sys.path\n    exec(open('rapidsai-csp-utils/colab/update_modules.py').read(), globals())\n\n    # intall miniconda\n    !wget -c https://repo.continuum.io/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\n    !chmod +x Miniconda3-4.5.4-Linux-x86_64.sh\n    !bash ./Miniconda3-4.5.4-Linux-x86_64.sh -b -f -p /usr/local\n\n    # install RAPIDS packages\n    !conda install -q -y --prefix /usr/local -c conda-forge \\\n    -c rapidsai-nightly/label/cuda10.0 -c nvidia/label/cuda10.0 \\\n    cudf cuml\n\n    # set environment vars\n    import shutil\n    sys.path.append('/usr/local/lib/python3.6/site-packages/')\n    os.environ['NUMBAPRO_NVVM'] = '/usr/local/cuda/nvvm/lib64/libnvvm.so'\n    os.environ['NUMBAPRO_LIBDEVICE'] = '/usr/local/cuda/nvvm/libdevice/'\n\n    # copy .so files to current working dir\n    # FileNotFoundError: [Errno 2] No such file or directory: '/usr/local/lib/libcudf.so'\n    for fn in ['libcudf.so', 'librmm.so']:\n        shutil.copy('/usr/local/lib/'+fn, os.getcwd())\n\n    # ModuleNotFoundError: No module named 'conda'\n    !conda install -c nvidia nvstrings==0.1.0\n    !conda install -c rapidsai -c numba -c conda-forge -c defaults cudf=0.4.0\n\n    # intall miniconda\n    !wget -c https://repo.continuum.io/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\n    !chmod +x Miniconda3-4.5.4-Linux-x86_64.sh\n    !bash ./Miniconda3-4.5.4-Linux-x86_64.sh -b -f -p /usr/local\n\n    !conda install -c pytorch faiss-gpu cuda92\n\n    # install RAPIDS packages\n    !conda install -q -y --prefix /usr/local -c conda-foFileNotFoundError: [Errno 2] No such file or directory: '/usr/local/lib/libcudf.so'rge \\\n    -c rapidsai-nightly/label/cuda10.0 -c nvidia/label/cuda10.0 \\\n    cudf cuml\n    # FileNotFoundError: [Errno 2] No such file or directory: '/usr/local/lib/libcudf.so'\n    # https://github.com/rapidsai/cuml/issues/25\n    # Running conda install -c pytorch faiss-gpu cuda92 prior to the cuML install resolves the issue.\n\n    # set environment vars\n    import sys, os, shutil\n    sys.path.append('/usr/local/lib/python3.6/site-packages/')\n    os.environ['NUMBAPRO_NVVM'] = '/usr/local/cuda/nvvm/lib64/libnvvm.so'\n    os.environ['NUMBAPRO_LIBDEVICE'] = '/usr/local/cuda/nvvm/libdevice/'\n\n    # copy .so files to current working dir\n    # FileNotFoundError: [Errno 2] No such file or directory: '/usr/local/lib/libcudf.so'\n    for fn in ['libcudf.so', 'librmm.so']:\n        shutil.copy('/usr/local/lib/'+fn, os.getcwd())\n\nelse: default_dir = '../input/shopee-product-matching'","metadata":{"id":"xfNE3DIbCFJV","executionInfo":{"status":"ok","timestamp":1618034054767,"user_tz":-480,"elapsed":28545,"user":{"displayName":"洪健翔","photoUrl":"","userId":"06910228611361480085"}},"outputId":"648c1953-0398-4b3d-97bf-b338ae0d6451","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I've downloaded the data to MyDrive. If you want to use Kaggle API, please check [here](https://www.kaggle.com/questions-and-answers/135301). It's a more safe way to prevent file missing problem while unzipping data to VM I think.","metadata":{"id":"GwOIRkGlLL3x"}},{"cell_type":"code","source":"if COLAB:\n    !cp /content/drive/MyDrive/ML/Shopee\\ -\\ Price\\ Match\\ Guarantee/shopee-product-matching.zip /content\n    !unzip \\*.zip  && rm *.zip\n\n    # To check wether there is any missing file in train_images\n    # https://askubuntu.com/questions/370697/how-to-count-number-of-files-in-a-directory-but-not-recursively\n    %cd train_images/\n    !ls -F |grep -v / | wc -l\n    %cd ..","metadata":{"id":"5YwNWK0GH0fS","executionInfo":{"status":"ok","timestamp":1618031860094,"user_tz":-480,"elapsed":118548,"user":{"displayName":"洪健翔","photoUrl":"","userId":"06910228611361480085"}},"outputId":"2cfa0939-6464-4525-bb21-ebae92f8732e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Libraries","metadata":{"id":"vAMe-dpzB3fJ"}},{"cell_type":"code","source":"import tensorflow as tf\nprint('tf version:', tf.__version__)\n\nimport pandas as pd\nimport numpy as np\nimport cv2, matplotlib.pyplot as plt\n\nfrom os.path import join\n\nfrom tensorflow.keras.applications import EfficientNetB0, EfficientNetB3, EfficientNetB5, EfficientNetB6\nimport gc","metadata":{"id":"telKlQqwB3fJ","executionInfo":{"status":"error","timestamp":1618035147553,"user_tz":-480,"elapsed":3040,"user":{"displayName":"洪健翔","photoUrl":"","userId":"06910228611361480085"}},"outputId":"33c79d39-928d-4ff2-8b17-a3d950de5216","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## RAM Restriction\nRestrict Tensorflow to 1 GP of GPU RAM so that we have GPU RAM for RAPIDS cuML KNN, and be able to submit final result. See [\"Submission CSV Not Found\" - struggling to submit](https://www.kaggle.com/c/shopee-product-matching/discussion/229672).","metadata":{"id":"k20EaSR7B3fK"}},{"cell_type":"code","source":"LIMIT = 1\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        tf.config.experimental.set_virtual_device_configuration(\n        gpus[0],\n        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*LIMIT)])\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n        print(len(gpus), 'Physical GPUs', len(logical_gpus), 'Logical GPUs')\n    except RuntimeError as e:\n        print(e)\n    print('Restrict TensorFlow to max %iGB GPU RAM'%LIMIT)\n    print('so RAPIDS can use %iGB GPU RAM'%(16-LIMIT))\nelse:\n    print('Non Accelerator detected')","metadata":{"id":"1-OfBWUEB3fK","executionInfo":{"status":"ok","timestamp":1618031996643,"user_tz":-480,"elapsed":5912,"user":{"displayName":"洪健翔","photoUrl":"","userId":"06910228611361480085"}},"outputId":"9172bd80-809a-4ccc-feb2-a647dd4defd6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Data Load In\n\nWe have image, title (X) and column `label_group` (Y) indicates the ground trugh of which items are similar.","metadata":{"id":"buJmhhHRB3fL"}},{"cell_type":"markdown","source":"## COMPUTE_CV\n\n*This committed notebook computes CV score but when we submit this notebook it does not compute CV. Instead it will load the 70,000 row test.csv file and compute matches in the test dataset. Because the variable `COMPUTE_CV = True` when we commit this notebook. But when we submit this notebook to Kaggle then the length of test.csv will be longer than 3 and the if-statement below will change to `COMPUTE_CV=False`.*","metadata":{"id":"CwyDnuLjB3fL"}},{"cell_type":"code","source":"COMPUTE_CV = True\n\ntest = pd.read_csv(join(default_dir, 'test.csv'))\nif len(test)>3: COMPUTE_CV = False\nelse: print('this submission notebook will compute CV score but commit notebook will not')","metadata":{"id":"ibcKsU9iB3fL","executionInfo":{"status":"ok","timestamp":1618032145326,"user_tz":-480,"elapsed":1705,"user":{"displayName":"洪健翔","photoUrl":"","userId":"06910228611361480085"}},"outputId":"1ca742ae-0dc3-4585-e905-486d4ee6ec62","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(join(default_dir, 'train.csv'))\ntmp = train.groupby('label_group').posting_id.agg('unique').to_dict()\ntrain['target'] = train.label_group.map(tmp)\nprint('train shape is', train.shape)\ntrain.head()","metadata":{"id":"DeZbsvGzB3fM","executionInfo":{"status":"ok","timestamp":1618032148340,"user_tz":-480,"elapsed":1779,"user":{"displayName":"洪健翔","photoUrl":"","userId":"06910228611361480085"}},"outputId":"8cda7006-35b4-4b7d-ca77-a150ae1b0a65","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Images Check Randomly","metadata":{"id":"LKMoFf9TB3fM"}},{"cell_type":"code","source":"BASE = join(default_dir, 'train_images/')\n\ndef displayDF(train, random=False, COLS=6, ROWS=4, path=BASE):\n    for k in range(ROWS):\n        plt.figure(figsize=(20,5))\n        for j in range(COLS):\n            if random: row = np.random.randint(0,len(train))\n            else: row = COLS*k + j\n            name = train.iloc[row,1]\n            title = train.iloc[row,3]\n            title_with_return = \"\"\n            for i,ch in enumerate(title):\n                title_with_return += ch\n                if (i!=0)&(i%20==0): title_with_return += '\\n'\n            img = cv2.imread(path+name)\n            \n            # color fixing\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            \n            plt.subplot(1,COLS,j+1)\n            plt.title(title_with_return)\n            plt.axis('off')\n            plt.imshow(img)\n        plt.show()\n        \ndisplayDF(train,random=True)","metadata":{"id":"vZGEECR_B3fM","executionInfo":{"status":"ok","timestamp":1618032290486,"user_tz":-480,"elapsed":6931,"user":{"displayName":"洪健翔","photoUrl":"","userId":"06910228611361480085"}},"outputId":"792cbc8b-d3db-4a64-c51d-ebb18eba86cf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Duplicated Items","metadata":{"id":"AiAZxA3-B3fM"}},{"cell_type":"code","source":"groups = train.label_group.value_counts()\nplt.figure(figsize=(20, 5))\nplt.plot(np.arange(len(groups)), groups.values)\nplt.ylabel('Duplicate Count', size=14)\nplt.xlabel('Index of Unique Item', size=14)\nplt.title('Duplicate Count vs. Unique Item Count', size=16)\nplt.show()\n\nplt.figure(figsize=(20,5))\nplt.bar(groups.index.values[:50].astype('str'),groups.values[:50])\nplt.xticks(rotation = 45)\nplt.ylabel('Duplicate Count',size=14)\nplt.xlabel('Label Group',size=14)\nplt.title('Top 50 Duplicated Items',size=16)\nplt.show()","metadata":{"id":"KgTajJeHB3fN","executionInfo":{"status":"ok","timestamp":1618032312728,"user_tz":-480,"elapsed":1823,"user":{"displayName":"洪健翔","photoUrl":"","userId":"06910228611361480085"}},"outputId":"2acfcd95-5c2f-4b31-bb6a-da36f24f08ee","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for k in range(2):\n    print('#'*40)\n    print('### TOP %i DUPLICATED ITEM:'%(k+1),groups.index[k])\n    print('#'*40)\n    top = train.loc[train.label_group==groups.index[k]]\n    displayDF(top, random=False, ROWS=2, COLS=4)","metadata":{"id":"NwC0MwdzB3fN","executionInfo":{"status":"ok","timestamp":1618032323384,"user_tz":-480,"elapsed":9519,"user":{"displayName":"洪健翔","photoUrl":"","userId":"06910228611361480085"}},"outputId":"d8f7b07b-7245-4a5b-eff9-14bf1d2d5201","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Baseline CV Score\n\nA baseline is to predict all items with the same `image_phash` as being duplicate.","metadata":{"id":"AD5LLsDdB3fN"}},{"cell_type":"code","source":"tmp = train.groupby('image_phash').posting_id.agg('unique').to_dict()\ntrain['oof'] = train.image_phash.map(tmp)\ntrain.head()","metadata":{"id":"bluiREgJB3fN","executionInfo":{"status":"ok","timestamp":1618032333647,"user_tz":-480,"elapsed":3130,"user":{"displayName":"洪健翔","photoUrl":"","userId":"06910228611361480085"}},"outputId":"090f9db3-f82a-4f10-fbec-0e4fa575e56d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getMetric(col):\n    def f1score(row):\n        n = len( np.intersect1d(row.target, row[col]) )\n        return 2*n / (len(row.target) + len(row[col]))\n    return f1score","metadata":{"id":"oURJ18E1B3fO","executionInfo":{"status":"ok","timestamp":1618032335688,"user_tz":-480,"elapsed":1050,"user":{"displayName":"洪健翔","photoUrl":"","userId":"06910228611361480085"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['f1'] = train.apply(getMetric('oof'), axis=1)\nprint('CV score for baseline =', train.f1.mean())\n\ndel train","metadata":{"id":"nwJt73GdB3fO","executionInfo":{"status":"ok","timestamp":1618032339785,"user_tz":-480,"elapsed":3054,"user":{"displayName":"洪健翔","photoUrl":"","userId":"06910228611361480085"}},"outputId":"92637c81-e205-4b90-c95d-d6b77f4008cc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Compute RAPIDS Model CV and Infer Submission\n\nNote how the variable `COMPUTE_CV` is only `True` when we **commit** this notebook. Right now you are reading a **commit** notebook, so we see test replaced with train and computed CV score. When we **submit** this notebook, the variable `COMPUTE_CV` will be `False` and the **submit** notebook will **not** compute CV. Instead it will load the real test dataset with 70,000 rows and find duplicates in the real test dataset.","metadata":{"id":"Ez7N24A6P7ei"}},{"cell_type":"code","source":"import cudf\n\nif COMPUTE_CV:\n    test = pd.read_csv(join(default_dir, 'train.csv'))\n    test_gf = cudf.DataFrame(test)\n    print('Using train as test to compute CV (since commit notebook). Shape is', test_gf.shape)\nelse:\n    test = pd.read_csv(join(default_dir, 'test.csv'))\n    test_gf = cudf.DataFrame(test)\n    print('Test shape is', test_gf.shape)\ntest_gf.head()","metadata":{"id":"LAzyZTGgQCxF","executionInfo":{"status":"error","timestamp":1618032759228,"user_tz":-480,"elapsed":637,"user":{"displayName":"洪健翔","photoUrl":"","userId":"06910228611361480085"}},"outputId":"c078b738-fb4c-47b4-a683-de379fcf558d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{"id":"SU31cVt4B3fO"}},{"cell_type":"markdown","source":"## Use Image Embeddings\n\nTo prevent memory errors, we will compute image embeddings in chunks. And we will find similar images with RAPIDS cuML KNN in chunks.","metadata":{}},{"cell_type":"code","source":"class DataGenerator(tf.keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, df, img_size=256, batch_size=32, path=''): \n        self.df = df\n        self.img_size = img_size\n        self.batch_size = batch_size\n        self.path = path\n        self.indexes = np.arange( len(self.df) )\n        \n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        ct = len(self.df) // self.batch_size\n        ct += int(( (len(self.df)) % self.batch_size)!=0)\n        return ct\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        X = self.__data_generation(indexes)\n        return X\n            \n    def __data_generation(self, indexes):\n        'Generates data containing batch_size samples' \n        X = np.zeros((len(indexes),self.img_size,self.img_size,3),dtype='float32')\n        df = self.df.iloc[indexes]\n        for i,(index,row) in enumerate(df.iterrows()):\n            img = cv2.imread(self.path+row.image)\n            X[i,] = cv2.resize(img,(self.img_size,self.img_size)) #/128.0 - 1.0\n        return X","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BASE = join(default_dir, 'test_images/')\nif COMPUTE_CV: BASE = join(default_dir, 'train_images/')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Keras implementation of EfficientNet\n\nBecause training EfficientNet on ImageNet takes a tremendous amount of resources and several techniques that are not a part of the model architecture itself. Hence the Keras implementation by default loads pre-trained weights obtained via training with **AutoAugment**.\n\nFor B0 to B7 base models, the input shapes are different. [Here](https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/) is a list of input shape expected for each model.\n\n\n#### ResourceExhaustedError\n\nResourceExhaustedError:  OOM when allocating tensor with shape[8,192,256,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc<br>\n\t [[node efficientnetb6/block2a_expand_bn/FusedBatchNormV3 (defined at <ipython-input-16-5b6f43f107b0>:42) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.<br>\n [Op:__inference_predict_function_16157]<br>\n\nFunction call stack:<br>\npredict_function\n\n* [OOM when allocating tensor with shape #16768](https://github.com/tensorflow/tensorflow/issues/16768)\n* [How to add report_tensor_allocations_upon_oom to RunOptions in Keras](https://stackoverflow.com/questions/49665757/how-to-add-report-tensor-allocations-upon-oom-to-runoptions-in-keras)","metadata":{}},{"cell_type":"code","source":"MODEL = EfficientNetB0\nSAVE_IMGEMBEDDING = False\n\nif not COMPUTE_CV or SAVE_IMGEMBEDDING:\n    \n    if MODEL == EfficientNetB0:\n        WGT = '../input/effnetb0/efficientnetb0_notop.h5'\n        model = EfficientNetB0(weights=WGT, include_top=False, pooling='avg', input_shape=None)\n    elif MODEL == EfficientNetB3:\n        WGT = '../input/tfkerasefficientnetimagenetnotop/efficientnetb3_notop.h5'\n        model = EfficientNetB3(weights=WGT, include_top=False, pooling='avg', input_shape=None)\n    elif MODEL == EfficientNetB5:\n        WGT = '../input/tfkerasefficientnetimagenetnotop/efficientnetb5_notop.h5'\n        model = EfficientNetB5(weights=WGT, include_top=False, pooling='avg', input_shape=None)\n    elif MODEL == EfficientNetB6:\n        WGT = '../input/tfkerasefficientnetimagenetnotop/efficientnetb6_notop.h5'\n        model = EfficientNetB6(weights=WGT, include_top=False, pooling='avg', input_shape=None)\n\n    embeds = []\n    CHUNK = 1024 * 4\n\n    print('Computing image embeddings...')\n    CTS = len(test) // CHUNK\n    if len(test) % CHUNK != 0: CTS += 1\n    for i, j in enumerate(range(CTS)):\n\n        a = j * CHUNK\n        b = (j+1) * CHUNK\n        b = min(b, len(test))\n        print('chunk', a, 'to', b)\n        \n        if MODEL == EfficientNetB6:\n            test_gen = DataGenerator(test.iloc[a:b], img_size=512, batch_size=6, path=BASE)\n        else:\n            test_gen = DataGenerator(test.iloc[a:b], batch_size=32, path=BASE)\n            \n        image_embeddings = model.predict(test_gen, verbose=1, use_multiprocessing=True, workers=4)\n        embeds.append(image_embeddings)\n\n        #if i>=1: break\n\n    del model\n    _ = gc.collect()\n    image_embeddings = np.concatenate(embeds)\n\n    # Saving a NumPy Array to CSV File\n    if SAVE_IMGEMBEDDING: np.savetxt('image_embeddings_EfficientNetB6.csv', image_embeddings, delimiter=',')\n\nelse:\n    print('Loading image embeddings...')\n    if EfficientNetB0:\n        image_embeddings = np.loadtxt('../input/shopee-price-match-guarantee-embeddings/image_embeddings.csv',\n                                 delimiter=',')\n    else: raise ValueError('Please select the correspondent model and embeddings in \"../input/shopee-price-match-guarantee-embeddings\".')\n\nprint('image embeddings shape',image_embeddings.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Please Note! As stated in competition's evaluation page:<br>\n*Group sizes were capped at 50, so there is no benefit to predict more than 50 matches.*\n\n[I predicted 100 matches](https://www.kaggle.com/muhammad4hmed/you-need-more-tensors-in-neighbourhood) in hope of finding the actual neighborsand it did improve the score on LB a very little (3rd digits maybe).","metadata":{}},{"cell_type":"code","source":"from cuml.neighbors import NearestNeighbors\n\nKNN = 100\nif len(test) == 3: KNN = 2\nmodel = NearestNeighbors(n_neighbors=KNN)\nmodel.fit(image_embeddings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = []\nCHUNK = 1024*4\n\nprint('Finding similar images...')\nCTS = len(image_embeddings) // CHUNK\nif len(image_embeddings) % CHUNK != 0: CTS += 1\nfor j in range(CTS):\n    \n    a = j * CHUNK\n    b = (j+1) * CHUNK\n    b = min(b, len(image_embeddings))\n    print('chunk', a, 'to', b)\n    distances, indices = model.kneighbors(image_embeddings[a:b, ])\n    \n    for k in range(b-a):\n        IDX = np.where(distances[k, ] < 6.0)[0]\n        IDS = indices[k, IDX]\n        o = test.iloc[IDS].posting_id.values\n        preds.append(o)\n        \ndel model, distances, indices, image_embeddings # embeds\n_ = gc.collect()\n\ntest['preds2'] = preds\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Use Text Embeddings","metadata":{}},{"cell_type":"code","source":"from cuml.feature_extraction.text import TfidfVectorizer\n\nprint('Computing text embeddings...')\nmodel = TfidfVectorizer(stop_words=None, binary=True, max_features=25_000)\ntext_embeddings = model.fit_transform(test_gf.title).toarray()\n\n# Saving a NumPy Array to CSV File\n# np.savetxt('text_embeddings.csv', text_embeddings, delimiter=',')\n# Warning: It's not np type\n\nprint('text embeddings shape',text_embeddings.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To prevent memory errors, we will find similar titles in chunks. \n# To faciliate this, we will use cosine similarity between text embeddings instead of KNN.\nCOSINE_SIMILARITY = True\nText_KNN_Follow_Up = False\n\nif not COSINE_SIMILARITY:\n    KNN = 100\n    if len(test) == 3: KNN = 2\n    model = NearestNeighbors(n_neighbors = KNN)\n    model.fit(text_embeddings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here is an amazing post-searching method.\n- [@kirderf](https://www.kaggle.com/kirderf)\n - [Features and post processing that might help](https://www.kaggle.com/c/shopee-product-matching/discussion/233626)","metadata":{}},{"cell_type":"code","source":"import cupy\n\npreds = []\nCHUNK = 1024*4\n\nprint('Finding similar titles...')\n\nCTS = len(test) // CHUNK\nif len(test) % CHUNK != 0: CTS += 1\n\nfor j in range(CTS):\n    \n    a = j * CHUNK\n    b = (j+1) * CHUNK\n    b = min(b, len(test))\n    print('chunk', a, 'to', b)\n    \n    if COSINE_SIMILARITY:\n        # COSINE SIMILARITY DISTANCE\n        cts = cupy.matmul(text_embeddings, text_embeddings[a:b].T).T\n\n        for k in range(b-a):\n            IDX = cupy.where(cts[k, ] > 0.7)[0]\n            o = test.iloc[cupy.asnumpy(IDX)].posting_id.values\n            for ii in np.arange(0.7, 0.5, -0.02):\n                if ii > 0.5 and o.shape[0] <= 1:\n                    IDX = cupy.where(cts[k, ] > ii)[0]\n                    o = test.iloc[cupy.asnumpy(IDX)].posting_id.values\n            preds.append(o)\n    \n    else:\n        # KNN\n        distances, indices = model.kneighbors(text_embeddings[a:b,])\n        \n        for k in range(b-a):\n            IDX = cupy.where(indices[k, ] < 6.0)[0]\n            o = test.iloc[cupy.asnumpy(IDX)].posting_id.values\n            preds.append(o)\n            \n            # IDX = np.where(distances[k, ] < 6.0)[0]\n            # IDS = indices[k, IDX]\n            # o = test.iloc[IDS].posting_id.values\n            # preds.append(o)\n            \n            # TypeError: Implicit conversion to a NumPy array is not allowed. Please use `.get()` to construct a NumPy array explicitly.\n            # https://stackoverflow.com/questions/65008297/attempting-numpy-conversion-when-not-needed-in-cupy\n            \nif not Text_KNN_Follow_Up: del model, text_embeddings\nelse: del model\n_ = gc.collect()\n\ntest['preds'] = preds\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text KNN Follow Up\n\nA thought from [here](https://www.kaggle.com/muhammad4hmed/b3-tfidf-knn-boom-p/comments).","metadata":{}},{"cell_type":"code","source":"if Text_KNN_Follow_Up:\n    KNN = 100\n    if len(test) == 3: KNN = 2\n    model = NearestNeighbors(n_neighbors = KNN)\n    model.fit(text_embeddings)\n\n    preds = []\n    CHUNK = 1024*4\n\n    print('Finding similar titles using KNN...')\n\n    CTS = len(test) // CHUNK\n    if len(test) % CHUNK != 0: CTS += 1\n\n    for j in range(CTS):\n\n        a = j * CHUNK\n        b = (j+1) * CHUNK\n        b = min(b, len(test))\n        print('chunk', a, 'to', b)\n\n        distances, indices = model.kneighbors(text_embeddings[a:b,])\n\n        for k in range(b-a):\n            IDX = cupy.where(indices[k, ] < 2.0)[0]\n            o = test.iloc[cupy.asnumpy(IDX)].posting_id.values\n            preds.append(o)\n\n            # IDX = np.where(distances[k, ] < 6.0)[0]\n            # IDS = indices[k, IDX]\n            # o = test.iloc[IDS].posting_id.values\n            # preds.append(o)\n\n            # TypeError: Implicit conversion to a NumPy array is not allowed. Please use `.get()` to construct a NumPy array explicitly.\n            # https://stackoverflow.com/questions/65008297/attempting-numpy-conversion-when-not-needed-in-cupy\n\n    del distances, indices, model, text_embeddings\n    _ = gc.collect()\n\n    test['preds_txt2'] = preds\n    test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Use Phash Feature\n\nWe will predict all items with the same phash as duplicates:<br>\nCV Score = 0.7191099726819047 <br>\nCV Score = 0.7190606455051343 (w/o Phash)","metadata":{}},{"cell_type":"code","source":"tmp = test.groupby('image_phash').posting_id.agg('unique').to_dict()\ntest['preds3'] = test.image_phash.map(tmp)\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Compute CV Score","metadata":{}},{"cell_type":"code","source":"def combine_for_sub(row):\n    x = np.concatenate([row.preds, row.preds2, row.preds3])\n    return ' '.join( np.unique(x) )\n\ndef combine_for_cv(row):\n    x = np.concatenate([row.preds, row.preds2, row.preds3])\n    return np.unique(x)\n\ndef combine_for_cv_txt2(row):\n    x = np.concatenate([row.preds, row.preds2, row.preds3, row.preds_txt2])\n    return np.unique(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if COMPUTE_CV:        \n    tmp = test.groupby('label_group').posting_id.agg('unique').to_dict()\n    test['target'] = test.label_group.map(tmp)\n    \n    if Text_KNN_Follow_Up: \n        test['oof'] = test.apply(combine_for_cv_txt2, axis=1)\n    else: \n        test['oof'] = test.apply(combine_for_cv, axis=1)\n        \n    test['f1'] = test.apply(getMetric('oof'),axis=1)\n    print('CV Score =', test.f1.mean())\n    print(f'COSINE_SIMILARITY = {COSINE_SIMILARITY}, Text_KNN_Follow_Up = {Text_KNN_Follow_Up}')\n\ntest['matches'] = test.apply(combine_for_sub,axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Baseline CV Score After Embedding\n\nCV Score = 0.6393196101848189 (preds2)<br>\nCV Score = 0.6137154152579091 (preds)<br>\nCV Score = 0.5530933399167943 (preds3)","metadata":{}},{"cell_type":"markdown","source":"# Write Submission CSV\n\nIn this notebook, the submission file below looks funny containing train information. But when we submit this notebook, the size of `test.csv` dataframe will be longer than 3 rows and the variable `COMPUTE_CV` will subsequently set to `False`. Then our submission notebook will compute the correct matches using the real test dataset and our submission csv for LB will be ok.","metadata":{}},{"cell_type":"code","source":"test[['posting_id','matches']].to_csv('submission.csv',index=False)\nsub = pd.read_csv('submission.csv')\nsub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Appendix","metadata":{}},{"cell_type":"markdown","source":"## Ignore Ground Truth (RAPIDS Only)\n\nWe will now ignore the ground truth and try to find similar items in train data using only the title's text. First we will extract text embeddings using **RAPIDS cuML's TfidfVectorizer**. This will turn every title into a one-hot-encoding of the words present. We will then compare one-hot-encodings with **RAPIDS cuML KNN** to find title's that are similar.","metadata":{"id":"k9J3x1RpB3fP"}},{"cell_type":"code","source":"import cuml, cupy\nprint('RAPIDS', cuml.__version__)","metadata":{"id":"aPzOhj0xB3fP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(join(default_dir, 'train.csv'))\n\n# LOAD TRAIN UNTO THE GPU WITH CUDF\ntrain_gf = cudf.read_csv(join(default_dir, 'train.csv'))\nprint('train_gf shape is', train_gf.shape, '\\ntrain shape is', train.shape)\ntrain_gf.head()","metadata":{"id":"bS7uVFa2B3fP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Similar Titles (TfidfVectorizer)\n\n**TfidfVectorizer** returns a cupy sparse matrix. Afterward we convert to a cupy dense matrix and feed that into **RAPIDS cuML KNN**.","metadata":{"id":"B29rXDPLB3fP"}},{"cell_type":"code","source":"model = TfidfVectorizer(stop_words='english', binary=True)\ntext_embeddings = model.fit_transform(train_gf.title).toarray()\nprint('text embeddings shape is',text_embeddings.shape)","metadata":{"id":"LM49GRgjB3fP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Similar Titles (KNN)","metadata":{"id":"43BeDk85B3fQ"}},{"cell_type":"code","source":"KNN = 50\nmodel = NearestNeighbors(n_neighbors=KNN)\nmodel.fit(text_embeddings)\ndistances, indices = model.kneighbors(text_embeddings)","metadata":{"id":"KeIu_PueB3fQ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for k in range(5):\n    plt.figure(figsize=(20,3))\n    plt.plot(np.arange(50),cupy.asnumpy(distances[k,]),'o-')\n    plt.title('Text Distance From Train Row %i to Other Train Rows'%k,size=16)\n    plt.ylabel('Distance to Train Row %i'%k,size=14)\n    plt.xlabel('Index Sorted by Distance to Train Row %i'%k,size=14)\n    plt.show()\n    \n    print( train_gf.loc[cupy.asnumpy(indices[k,:10]),['title','label_group']] )","metadata":{"id":"qIK8KtqJB3fQ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Similar Images (EfficientNetB0)\n\nAgain, we will now ignore the ground truth and try to find similar items in train data using only the item's image. First we will extract image embeddings using **EffNetB0**. We will then compare image embeddings with **RAPIDS cuML KNN** to find images that are similar.","metadata":{"id":"-K_FKnLPB3fQ"}},{"cell_type":"code","source":"# model = EfficientNetB0(weights='../input/effnetb0/efficientnetb0_notop.h5', include_top=False, pooling='avg', input_shape=None)\n\n# train_gen = DataGenerator(train, batch_size=32, path=BASE)\n# image_embeddings = model.predict(train_gen, verbose=1)\n\nimage_embeddings = np.loadtxt('../input/shopee-price-match-guarantee-embeddings/image_embeddings.csv',\n                             delimiter=',')\nprint('image embeddings shape is',image_embeddings.shape)","metadata":{"id":"k3x1YiJ3B3fR","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Similar Images (KNN)","metadata":{"id":"5Vjx5eTiB3fR"}},{"cell_type":"code","source":"KNN = 50\nmodel = NearestNeighbors(n_neighbors=KNN)\nmodel.fit(image_embeddings)\ndistances, indices = model.kneighbors(image_embeddings)","metadata":{"id":"zjacHUhjB3fR","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for k in range(180,190):\n    plt.figure(figsize=(20,3))\n    plt.plot(np.arange(50),cupy.asnumpy(distances[k,]),'o-')\n    plt.title('Image Distance From Train Row %i to Other Train Rows'%k,size=16)\n    plt.ylabel('Distance to Train Row %i'%k,size=14)\n    plt.xlabel('Index Sorted by Distance to Train Row %i'%k,size=14)\n    plt.show()\n    \n    cluster = train.loc[cupy.asnumpy(indices[k,:8])] \n    displayDF(cluster, random=False, ROWS=2, COLS=4)","metadata":{"id":"YvrNSUejB3fR","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Next: Hyperparameters\nWhat is the importance of hyperparameter tuning?<br>\nHyperparameters are crucial as they control the overall behaviour of a machine learning model. The ultimate goal is to find an optimal combination of hyperparameters that gives better results. To further improve your score, click to see here: [Eff-B4 + TFIDF w/ CV for threshold_searching](https://www.kaggle.com/chienhsianghung/eff-b4-tfidf-w-cv-for-threshold-searching)\n\nFurther discussion: [The Best Hyperparameters](https://www.kaggle.com/c/tabular-playground-series-apr-2021/discussion/231152)","metadata":{}},{"cell_type":"markdown","source":"# References\n\n* [RAPIDS cuML TfidfVectorizer and KNN](https://www.kaggle.com/cdeotte/rapids-cuml-tfidfvectorizer-and-knn)\n* [[PART 2] - RAPIDS TfidfVectorizer - [CV 0.700]](https://www.kaggle.com/cdeotte/part-2-rapids-tfidfvectorizer-cv-0-700/data#Use-Image-Embeddings)\n* [Multiple ways to download output file generated in KAGGLE Kernel !](https://www.kaggle.com/getting-started/168312)\n* [How to Save a NumPy Array to File for Machine Learning](https://machinelearningmastery.com/how-to-save-a-numpy-array-to-file-for-machine-learning/)","metadata":{"id":"fZAri1tFB3fR"}}]}