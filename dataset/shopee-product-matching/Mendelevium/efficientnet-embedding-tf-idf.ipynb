{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport cv2\nimport re\nimport gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.image import imread\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\nfrom tqdm import tqdm\nimport tensorflow as tf\nimport tensorflow.keras as K\nfrom keras import applications","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Config and loading data","metadata":{}},{"cell_type":"code","source":"# Configuration\n\n# For Kaggle submission, set to True\nSUBMIT = True\n\n# Use only 1000 images when True\nDEBUG = False\n\n# Which part of the model is used\nIMG = True\nEXTRA_LAYERS = False\nTEXT = True\nPHASH = False\nSIM_PHASH = False\n\n\nPATH = '../input/shopee-product-matching/'\nIMG_SIZE = 456 # avg img size = 444, EfficientNetB5 = 456\nBATCH = 5000\nIMG_SIM_THRESHOLD = 0.25\nTEXT_SIM_THRESHOLD = 0.45\nPHASH_SIM_THRESHOLD = 6","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load data\ntest = pd.read_csv(PATH + 'test.csv')\ntrain = pd.read_csv(PATH + 'train.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DEBUG:\n    train = train.sample(n = 1000).reset_index(drop = True)\n\nif SUBMIT: \n    img_path = PATH +\"/test_images/\"\n    train = test \nelse:\n    img_path = PATH +\"/train_images/\"\n    # convert label_group to target\n    tmp = train.groupby('label_group').posting_id.agg('unique').to_dict()\n    train['target'] = train.label_group.map(tmp)\n    train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Image predictions","metadata":{}},{"cell_type":"code","source":"# images pre-processing\n\nclass DataGenerator(tf.keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, df, img_size=IMG_SIZE, batch_size=32, path=''): \n        self.df = df\n        self.img_size = img_size\n        self.batch_size = batch_size\n        self.path = path\n        self.indexes = np.arange( len(self.df) )\n        \n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        ct = len(self.df) // self.batch_size\n        ct += int(( (len(self.df)) % self.batch_size)!=0)\n        return ct\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        X = self.__data_generation(indexes)\n        return X\n            \n    def __data_generation(self, indexes):\n        'Generates data containing batch_size samples' \n        X = np.zeros((len(indexes),self.img_size,self.img_size,3),dtype='float32')\n        df = self.df.iloc[indexes]\n        for i,(index,row) in enumerate(df.iterrows()):\n            img = cv2.imread(self.path + row['image'])\n            X[i,] = cv2.resize(img,(self.img_size,self.img_size))\n        return X","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### image embedding with EfficentNet","metadata":{}},{"cell_type":"code","source":"# load pre-trained model\n# \"tf keras efficientnet imagenet no top\" must be added with \"+ Add data\"\nif IMG:\n    \n    WGT = \"../input/tfkerasefficientnetimagenetnotop/efficientnetb5_notop.h5\"\n    eff_net = K.applications.EfficientNetB5(weights=WGT, input_shape=None, include_top=False, pooling=\"avg\", drop_connect_rate=0.2)\n    #WGT = \"../input/tfkerasefficientnetimagenetnotop/efficientnetb0_notop.h5\"\n    #eff_net = K.applications.EfficientNetB0(weights=WGT, input_shape=None, include_top=False, pooling=\"avg\", drop_connect_rate=0.2)\n    \n    if EXTRA_LAYERS == False: model = eff_net","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# apply images to pre-trained model (EfficientNet)\nif IMG:\n    \n    #chunk = BATCH \n    #cls = len(train) // chunk \n    #cls += int (len(train) % chunk != 0)\n    #image_embedding = []\n    #for i in tqdm(range(cls)) :\n\n    #    a = i * chunk \n    #    b = (i+1) * chunk \n    #    b = min(b,len(train))\n    #    data = DataGenerator(train.iloc[a:b], path=img_path)\n    #    emb = model.predict(data, use_multiprocessing=True, workers=8)\n    #    image_embedding.append(emb)\n\n    data = DataGenerator(train, path=img_path)\n    image_embedding = model.predict(data, use_multiprocessing=True, workers=8)\n    \n    del(model)\n    gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### NearestNeighbors on image embeddings","metadata":{}},{"cell_type":"code","source":"#IMG_SIM_THRESHOLD = 0.15\n# get images prediction from nearest image embeddings\n\nif IMG:\n    \n    nn = 50\n    # prevent bug while submiting to kaggle\n    if len(test)==3: nn = 3\n    knn = NearestNeighbors(n_neighbors=3, metric =\"cosine\")\n    knn.fit(image_embedding)\n    \n    chunk = BATCH\n    cl = len(train) // chunk \n    cl += int((len(train) % chunk) !=0)\n    pred_img = []\n    for i in tqdm(range(cl)) :\n\n        a = i * chunk\n        b = (i+1) * chunk\n        b = min(len(train),b)\n        distances,indices = knn.kneighbors(image_embedding[a:b,])\n        for j in range(b-a):\n            distance = distances[j,:]\n            ind = np.where(distance < IMG_SIM_THRESHOLD)[0]\n            IND = indices[j,ind]\n            pred_img.append(train.iloc[IND].posting_id.values)\n\n    train[\"pred_img\"] = pred_img","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text predictions","metadata":{}},{"cell_type":"code","source":"# text pre-processing\n\ndef letters_only(t):\n    return re.sub(\"[^a-zA-Z]\", \" \", t)\n\ndef lowercase_only(t):\n    return t.lower()\n\ndef remove_patterns(t):\n    '''\n    Some titles contain \" x \" as separators.\n    Example: train_3369186413\n    '''\n    patterns = {' x ': ' ',\n               ' a ': ' '}\n    for k, v in patterns.items():\n        t = t.replace(k,v)\n    return t\n\ndef replace_multispace_by_space(t):\n    return re.sub('\\s+',' ',t)\n\n#cm_map adjusted - Source of original cm_map : https://www.kaggle.com/c/shopee-product-matching/discussion/228358\ncm_map = {\"wanita\": \"woman\", \n          \"anak\": \"child\", \n          \"bayi\": \"baby\",\n          \"tas\": \"bag\", \n          \"masker\": \"face mask\", \n          \"pria\": \"men\",\n          \"murah\": \"cheap\",\n          \"tangan\": \"hand\", \n          \"alat\": \"tool\", \n          \"motif\": \"motive\", \n          \"warna\": \"color\", \n          \"bahan\": \"material\", \n          \"celana\": \"pants\", \n          \"baju\": \"clothes\", \n          \"kaos\": \"t-shirt\", \n          \"sepatu\": \"shoes\", \n          \"rambut\": \"hair\", \n          \"mainan\": \"toy\", \n          \"sarung\": \"holster\", \n          \"polos\": \"plain\", \n          \"rak\": \"rack\", \n          \"botol\": \"bottle\", \n          \"sabun\": \"soap\", \n          \"kain\": \"fabric\", \n          \"panjang\": \"long\", \n          \"kabel\": \"cable\", \n          \"buku\": \"book\", \n          \"plastik\": \"plastic\", \n          \"mobil\": \"car\", \n          \"hitam\": \"black\", \n          \"karakter\": \"character\", \n          \"putih\": \"white\", \n          \"dompet\": \"purse\", \n          \"kaki\": \"feet\", \n          \"pembersih\": \"cleaners\", \n          \"lipat\": \"folding\", \n          \"silikon\": \"silicone\", \n          \"minyak\": \"oil\", \n          \"isi\": \"contents\", \n          \"paket\": \"package\", \n          \"susu\": \"milk\", \n          \"gamis\": \"robe\", \n          \"mandi\": \"bath\", \n          \"madu\": \"honey\", \n          \"kulit\": \"skin\", \n          \"serbaguna\": \"multipurpose\", \n          \"bisa\": \"can\", \n          \"kacamata\": \"spectacles\", \n          \"pendek\": \"short\", \n          \"tali\": \"rope\", \n          \"selempang\": \"sash\",\n          \"topi\": \"hat\", \n          \"obat\": \"drug\", \n          \"gantungan\": \"hanger\", \n          \"tahun\": \"year\", \n          \"jilbab\": \"hijab\", \n          \"dapur\": \"kitchen\", \n          \"dinding\": \"wall\",\n          \"kuas\": \"brush\",\n          \"perempuan\": \"woman\",\n          \"katun\": \"cotton\", \n          \"sepeda\": \"bike\", \n          \"lucu\": \"funny\", \n          \"lengan\": \"arm\", \n          \"kaca\": \"glass\", \n          \"garansi\": \"warranty\", \n          \"bunga\": \"flower\", \n          \"handuk\": \"towel\", \n          \"dewasa\": \"adult\", \n          \"elektrik\": \"electric\", \n          \"timbangan\": \"balance\", \n          \"besar\": \"big\", \n          \"bahan\": \"ingredient\", \n          \"ransel\": \"backpack\", \n          \"kertas\": \"paper\",\n          \"lampu\" : \"light\",\n          \"sepatu\": \"shoes\",\n          \"tempat\": \"place\"}\n\ndef translate_ind_to_eng(t):\n    res = \" \".join(cm_map.get(w, w) for w in t.split())    \n    return res","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# text clean up\n# removed .apply(letters_only)\nif TEXT:\n    X = (train['title'].apply(lowercase_only)\n                       .apply(remove_patterns)\n                       .apply(replace_multispace_by_space)\n                       .apply(translate_ind_to_eng))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TEXT_SIM_THRESHOLD = 0.45\n# get images prediction from nearest image embeddings\n\nif TEXT:\n    #vectorizer = CountVectorizer()\n    vectorizer =  TfidfVectorizer(min_df=2, max_df=0.95, stop_words='english')\n    X_mat = vectorizer.fit_transform(X)\n\n    nn = 50\n    # prevent bug while submitting to kaggle\n    if len(test)==3: nn = 3\n    knn = NearestNeighbors(n_neighbors=3, metric =\"cosine\")\n    knn.fit(X_mat)\n\n    # get images prediction from nearest image embeddings\n    chunk = BATCH\n    cl = len(train) // chunk \n    cl += int((len(train) % chunk) !=0)\n    pred_text = []\n    for i in tqdm(range(cl)) :\n\n        a = i * chunk\n        b = (i+1) * chunk\n        b = min(len(train),b)\n        distances,indices = knn.kneighbors(X_mat[a:b,])\n        for j in range(b-a):\n            distance = distances[j,:]\n            ind = np.where(distance < TEXT_SIM_THRESHOLD)[0]\n            IND = indices[j,ind]\n            pred_text.append(train.iloc[IND].posting_id.values)\n\n    train[\"pred_text\"] = pred_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## pHash predictions","metadata":{}},{"cell_type":"code","source":"# group identical phash\nif PHASH:\n    tmp = train.groupby('image_phash').posting_id.agg('unique').to_dict()\n    train['pred_phash'] = train.image_phash.map(tmp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# group similar phash\nif SIM_PHASH:\n    def hamming_distance(hash1, hash2):  \n        return sum([c1 != c2 for c1, c2 in zip(hash1, hash2)])\n\n    # very long to run and there's no gain compared to pred_img\n    \n    # check all phash with hamming distance < 6\n    train['pred_phash'] = np.nan\n    for i in tqdm(range(train.shape[0])):\n        train['dist'] = train['image_phash'].apply(lambda x: hamming_distance(x,train['image_phash'].iloc[i]))\n        train['pred_phash'].iloc[i] =  [x for x in train['posting_id'].loc[train['dist'] < PHASH_SIM_THRESHOLD]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Scoring","metadata":{}},{"cell_type":"code","source":"# f1 score\ndef getMetric(col):\n    def f1score(row):\n        n = len( np.intersect1d(row.target,row[col]) )\n        return 2*n / (len(row.target)+len(row[col]))\n    return f1score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if SUBMIT == False: \n    if PHASH:\n        print(\"f1 score for phash :\", round(train.apply(getMetric('pred_phash'),axis=1).mean(), 3))\n    if TEXT:\n        print(\"f1 score for text :\", round(train.apply(getMetric('pred_text'),axis=1).mean(), 3))\n    if IMG:\n        print(\"f1 score for images :\", round(train.apply(getMetric('pred_img'),axis=1).mean(), 3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def combine_predictions(row):\n    if PHASH and TEXT and IMG:\n        y = np.concatenate([row['pred_phash'], row['pred_text'], row['pred_img']])\n    if TEXT and IMG:    \n        y = np.concatenate([row['pred_text'], row['pred_img']])\n    if PHASH and TEXT:    \n        y = np.concatenate([row['pred_phash'], row['pred_text']])\n    if PHASH and IMG:    \n        y = np.concatenate([row['pred_phash'], row['pred_img']])\n    return list(np.unique(y))\n                        \ntrain['pred'] = train.apply(lambda x: combine_predictions(x),axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if SUBMIT == False: \n    print(\"f1 score for combined pred :\", round(train.apply(getMetric('pred'),axis=1).mean(), 3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"# prepare results for submission\n\nsubmission = pd.DataFrame()\n\ndef combine_pred(row):\n    if PHASH and TEXT and IMG:\n        y = np.concatenate([row['pred_phash'], row['pred_text'], row['pred_img']])\n    if TEXT and IMG:\n        y = np.concatenate([row['pred_text'], row['pred_img']]) \n    if PHASH and TEXT:\n        y = np.concatenate([row['pred_phash'], row['pred_text']])    \n    if PHASH and IMG:    \n        y = np.concatenate([row['pred_phash'], row['pred_img']])\n    return \" \".join(np.unique(y)) \n\nsubmission['posting_id'] = train['posting_id']\nsubmission['matches'] = train.apply(lambda x: combine_pred(x),axis=1)\n\nsubmission.to_csv(\"submission.csv\",index = False)\nsubmission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}