{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# How to Bert - Shopee Price Match Guarantee, Part I\nIn this notebook we will experiment with Bert on the dataset from the Shopee competition. Hopefully I can show you guys how to set up a proper pipeline for this kind of task. I hope this will be of use to some of you.\n\nNote that for the competition itself internet is not allowed, so you'll have to either train locally or make a seperate notebook to finetune and save your model before importing it to your submission notebook.\n\nThis is part I where we only deal with the titles of each item. There will be a part II also which uses image embeddings. But before we begin...","metadata":{}},{"cell_type":"markdown","source":"# How to really understand Shopee's unsupervised classification challenge\nI have read many of your notebooks, and I am so grateful to being introduced to many of the models and approaches that you are all implementing, for example I didn't even know about EfficientNet before this challenge. I want to just summarize my thoughts about this challenge and the reason behind my approach.\n\nThis is unsupervised learning and because of this we have the problem of having to classify unseen data based on labels we also have not seen. As if this wasn't bad enough, there are two reasons why this challenge in particular is hard.\n\n1. We don't know the number of classes\n2. Many of the classes have only two instances\n\nTo be specific the training set has 34250 samples with 11014 labels. The number of labels which only occur twice is 6979. We don't know what these numbers are for the hidden test set. Let's think about it carefully for a moment. The first problem is that while we can specify the number of classes for some clustering algorithm on the training data, this is not going to work in general. Many clustering methods rely on knowing the number of classes beforehand and then they define some centroid $\\mu_i$ for each class, based on which regions in feature space can be identified defining each class. The objective is then to optimize $\\mu_i$ (along with model parameters) such that the classifier minimizes the average distance to the closest centroid.\n\nI see a lot of notebooks which just apply k-means clustering in-spite of the fact that we don't know k outside of the training data. We might get lucky and estimate a good k, but still, we essentially don't know.\n\nFor this challenge, in my opinion, we're much better off defining some distance measure $D(z_i,z_j)$ that tells us the distance between two embeddings $z_i,z_j$ and then defining joint binary probabilies based on this $q(D(z_i,z_j))$. Instead of thinking in terms of classes, we should really be thinking in terms of distances. Then the problem we're faced with is to train our model to return embeddings $z_i$ which are far apart for samples which represent different products and close when the products are the same.\n\nIt is challenging to train a model to do this, since if we just pick out random pairs of embeddings, by far most of them will belong to distinct items. We can assign these the target joint probability $p(z_i,z_j)=0$ and then minimize the cross-entropy, but then we most likely will end up training our model in such a way that the $z_i$'s are just pushed away from each other in the embedding space, regardless of how close they actually are. So one of the problems we're facing is properly preparing the data. Some of this could be mitigated by picking the right $D(z_i,z_j)$ and $q(x)$ as well.\n\nThe crucial thing here is to set up a data-pipeline so we can experiment thoroughly with these three things:\n1. How to split training data between sample pairs with hard labels $p(z_i,z_j)=0$ and $p(z_i,z_j)=1$.\n2. How $D(\\cdot)$ and $q(\\cdot)$ should be defined?\n3. What models to use to generate $z_i$.\n\nWith this in mind, I made the notebook that you are now reading. Hope it helps!\n\nEdit: I realized that this methodology is essentially already described in [this well known paper](https://arxiv.org/pdf/1908.10084.pdf), which you should definitely read. Also there are really good notebooks which describe some of this stuff already. [zzy](https://www.kaggle.com/zzy990106/b0-bert-cv0-9#Use-Text-Embeddings), [Mr_KnowNothing](https://www.kaggle.com/tanulsingh077/metric-learning-pipeline-only-text-sbert) and please also take a look at [this duscussion](https://www.kaggle.com/c/shopee-product-matching/discussion/231510)","metadata":{}},{"cell_type":"code","source":"from tqdm.notebook import tqdm, trange\n\nimport random\nimport os\nimport gc\nimport pickle\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport pandas as pd\n\nfrom transformers import BertTokenizer, BertModel, BertConfig, AdamW\n\nEPOCHS = 25\nLR = 5e-5\nBATCH_SIZE = 400\nVALIDGROUPS = 20\n\nDATAPREP = False\nTRAINING = False\nSUBMIT = True\n\n\nENABLE_CUDA = True\nSEED = 147\nTOKENMAX = 20\nDISTANCE_FUN = 'COS' \nLOSS_FUN = 'MSE' #BCE\nMODEL_NAME = 'bert-base-multilingual-cased'\n\n\nGTSPLIT = 0.2 # Fraction of paired training set samples belong to the same label_group\nTHRESHOLD = 0.5 \nPATIENCE = 7\n\n\n\n\n\nDATA = '../input/shopee-product-matching'\nTRAINIMAGES = f'{DATA}/train_images'\nTESTIMAGES = f'{DATA}/test_images'\nTRAINHEADER = f'{DATA}/train.csv'\nPREPTRAINHEADER = f'{DATA}/preptrain.csv'\nTESTHEADER = f'{DATA}/test.csv'\n\nSAVEFOLDER = '../input/shoppeebertmodel/'\nTOKENIZER = '../input/berttokenizer/'\nSUBMISSIONFOLDER = './'\n\nif ENABLE_CUDA and torch.cuda.is_available():\n    dev = torch.device('cuda')\nelse:\n    dev = torch.device('cpu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef seed_everything(seed=1001):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nclass ScoreTracker:\n    def __init__(self, patience=7, mode=\"max\", delta=0.001):\n        self.patience = patience\n        self.counter = 0\n        self.mode = mode\n        self.best_score = None\n        self.early_stop = False\n        self.delta = delta\n        if self.mode == \"min\":\n            self.val_score = np.Inf\n        else:\n            self.val_score = -np.Inf\n        \n    def __call__(self, epoch_score, model, model_path):\n        \n        if self.mode == \"min\":\n            score = -1.0 * epoch_score\n        else:\n            score = np.copy(epoch_score)\n            \n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n        elif score <= self.best_score:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n            self.counter = 0\n        \n    def save_checkpoint(self, epoch_score, model, model_path):\n        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n            torch.save(model.state_dict(),model_path)\n            \n        self.val_score = epoch_score\n        \nclass ShopeeDataset(Dataset):\n    def __init__(self, df):\n        super()\n        self.df = df\n        self.tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n        self.ids = df['posting_id']\n        tokens = self.tokenizer(df['title'].to_list(),return_tensors='pt', padding=True, truncation=True, max_length=TOKENMAX)\n        self.titles = tokens['input_ids']\n        self.title_masks = tokens['attention_mask']\n        self.partners = self.titles[df['partner_index']]\n        self.partner_masks = self.title_masks[df['partner_index']]\n        self.paired = torch.tensor(df['paired'].to_numpy(),dtype=torch.float)\n        \n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        return {\n            'ids': self.ids[idx],\n            'titles': self.titles[idx].to(dev),\n            'title_masks': self.title_masks[idx].to(dev),\n            'partners': self.partners[idx].to(dev),\n            'partner_masks': self.partner_masks[idx].to(dev),\n            'paired': self.paired[idx].to(dev)\n            }\n\nclass ShopeeTestDataset(Dataset):\n    def __init__(self, df, tokenizer):\n        super()\n        self.df = df\n        self.tokenizer = tokenizer\n        self.ids = df['posting_id']\n        tokens = self.tokenizer(df['title'].to_list(),return_tensors='pt', padding=True, truncation=True, max_length=TOKENMAX)\n        self.titles = tokens['input_ids']\n        self.title_masks = tokens['attention_mask']\n        \n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        return {\n            'ids': self.ids[idx],\n            'titles': self.titles[idx].to(dev),\n            'title_masks': self.title_masks[idx].to(dev)\n            }\n    \nclass BertUnsupervisedClassifier(nn.Module):\n    def __init__(self, internet=True):\n        super(BertUnsupervisedClassifier,self).__init__()\n        if internet:\n            self.base = BertModel.from_pretrained(MODEL_NAME)\n        else:\n            self.base = BertModel(BertConfig(vocab_size=119547))\n        self.d = self.distance(DISTANCE_FUN,2)\n        self.q = self.binary(DISTANCE_FUN)\n    \n    def distance(self,key,*argv):\n        if key == 'COS':\n            cos = nn.CosineSimilarity(dim = 1)\n            return cos\n        elif key == 'PDIST':\n            return nn.PairwiseDistance(p = argv[0])\n    \n    def binary(self,key,*argv):\n        if key == 'COS':\n            if LOSS_FUN == 'BCE':\n                eps = 1e-7\n                prob = lambda x: 1/2-x/2+eps\n            elif LOSS_FUN == 'MSE':\n                prob = lambda x: x\n                \n        elif key == 'PDIST':\n            prob = lambda x: 1/(x+1)\n        \n        return prob\n    \n    def forward(self,x1,m1,x2,m2):\n        x1 = self.base(x1,attention_mask=m1,output_hidden_states=False)\n        x1 = x1['pooler_output']\n        x2 = self.base(x2,attention_mask=m2,output_hidden_states=False)\n        x2 = x2['pooler_output']\n        \n        \n        x = self.d(x1,x2)\n        x = self.q(x)\n        \n        return x\n\ndef f1_score(row):\n    n = len(np.intersect1d(row['matches'],row['target']))\n    return 2*n/(len(row['matches'])+len(row['target']))\n\ndef match(model, dataset, df, threshold):\n    model = model.eval()\n    \n    index = df.index.values\n    df = df.reset_index(drop=True)\n    \n    \n    with torch.no_grad():\n        x = model.base(dataset[index]['titles'],attention_mask=dataset[index]['title_masks'],output_hidden_states=False)\n        x = x['pooler_output']\n        n = len(x)\n        matches = [[] for i in range(n)]\n        \n        for i in range(n):\n            matches[i].append(df.loc[i,'posting_id'])\n            for j in range(i+1,n):\n                q = model.q(model.d(x[i].unsqueeze(0),x[j].unsqueeze(0))).item()\n                if q > threshold:\n                    matches[i].append(df.loc[j,'posting_id'])\n                    matches[j].append(df.loc[i,'posting_id'])\n\n    df['matches'] = matches\n    \n    return df\n\ndef match_cosinesim(model, dataloader, df, threshold):\n    model = model.eval()\n    \n    with torch.no_grad():\n        x = torch.tensor([]).to(dev)\n        for dataset in dataloader:\n            out = model.base(dataset['titles'],attention_mask=dataset['title_masks'],output_hidden_states=False)\n            x = torch.cat((x,out['pooler_output']))\n        x = F.normalize(x,2,1)\n        n = len(x)\n        \n        n_batches = n // BATCH_SIZE\n        if n %BATCH_SIZE != 0:\n            n_batches += 1\n        \n        matches = []\n        q = torch.tensor([],device=dev)\n        for b in range(n_batches):\n            l = b*BATCH_SIZE\n            r = min((b+1)*BATCH_SIZE,n)\n            \n            y = x[l:r].transpose(0,1)\n            qq = torch.matmul(x,y).transpose(0,1)\n            qq = qq > threshold\n            qq = qq.cpu().numpy()\n            for j in range(r-l):\n                ind = np.flatnonzero(qq[j])\n                matches.append(df.iloc[ind].posting_id.to_list())\n\n    df['matches'] = matches\n    \n    return df\n\ndef data_prep(df,gtsplit):\n    \n    df['paired'] = False\n    trueindices = np.random.permutation(len(df))[:int(gtsplit*len(df))]\n    df.loc[trueindices,'paired'] = True\n    \n    df['partner_id'] = None\n    df['partner_index'] = None\n    \n    for i in tqdm(df.index):\n        if df.loc[i,'paired']:\n            ingroup = df[df['label_group'] == df.loc[i,'label_group']]\n            j = random.choice(ingroup.index.drop(i))\n            df.loc[i,'partner_id'] = ingroup.loc[j,'posting_id']\n            df.loc[i,'partner_index'] = j\n        else:\n            notingroup = df[df['label_group'] != df.loc[i,'label_group']]\n            j = random.choice(notingroup.index)\n            df.loc[i,'partner_id'] = notingroup.loc[j,'posting_id']\n            df.loc[i,'partner_index'] = j\n    \n    return df\n\ndef train_fn(model,dataloader,validdf,loss_fn,optimizer,epochs):\n    tracker = ScoreTracker(PATIENCE)\n    scores = []\n    \n    n_batches = len(dataloader)\n    \n    validdataset = ShopeeTestDataset(validdf)\n    \n    \n    bar = tqdm(range(epochs))\n    bar.set_description('Training')\n    \n    for i in bar:\n        model = model.train()\n        lossav = 0\n        \n        for d in dataloader:\n            \n            optimizer.zero_grad()\n            \n            titles = d['titles']\n            titlemasks = d['title_masks']\n            partners = d['partners']\n            partner_masks = d['partner_masks']\n            target = d['paired']\n            \n            outputs = model(titles,titlemasks,partners,partner_masks)\n            loss = loss_fn(outputs,target)\n            loss.backward()\n            optimizer.step()\n            loss = loss.detach().item()\n            lossav += loss\n        lossav /= n_batches\n        scores.append(lossav)\n        \n        validdf = match(model,validdataset,validdf,THRESHOLD)\n        validdf['f1score'] = validdf.apply(f1_score,axis=1)\n        score = validdf['f1score'].mean()\n        scores.append(score)\n        tracker(score,model,f'{SAVEFOLDER}model.pt')\n        bar.write(f\"Epoch: {i} Loss: {lossav} Score: {score}\")\n        \n        if tracker.early_stop:\n            break\n    del outputs\n        \n    return scores","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DATAPREP:\n    train = pd.read_csv(TRAINHEADER)\n    train = data_prep(train,GTSPLIT)\n    train.to_csv(PREPTRAINHEADER,index=False)\n\n\n\nif TRAINING:\n    if not DATAPREP:\n        train = pd.read_csv(PREPTRAINHEADER)\n    \n    \n    group_dicts = train.groupby('label_group')['posting_id'].unique().to_dict()\n    train['target'] = train['label_group'].map(group_dicts)\n    labels = train['label_group'].unique().tolist()\n    labels = random.sample(labels,VALIDGROUPS)\n    \n    validset = train.loc[[ train.loc[i,'label_group'] in labels for i in range(len(train)) ]]\n    validset = validset.reset_index(drop=True)\n    \n    dataset = ShopeeDataset(train)\n    \n    vN = len(validset)\n    \n    baseline = [[validset.loc[i,'posting_id']] for i in range(vN) ]\n    \n    validset['matches'] = baseline\n    validset['f1score'] = validset.apply(f1_score,axis=1)\n    \n    print(f\"Baseline score for validationset is {validset['f1score'].mean()}\")\n    \n    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE)\n    if LOSS_FUN == 'MSE':\n        loss_fn = nn.MSELoss()\n    elif LOSS_FUN == 'BCE':\n        loss_fn = nn.BCELoss()\n\n    model = BertUnsupervisedClassifier().to(dev)\n    optimizer = AdamW(model.parameters(), lr=LR)\n    \n    \n    results = train_fn(model,dataloader,validset,loss_fn,optimizer,EPOCHS)\n        \n    del model\n\nif SUBMIT:\n    test = pd.read_csv(TESTHEADER)\n    \n    with open(f'{TOKENIZER}tokenizer.pickle', 'rb') as f:\n        tokenizer = pickle.load(f)\n    \n    testdataset = ShopeeTestDataset(test,tokenizer)\n    dataloader = DataLoader(testdataset, batch_size=BATCH_SIZE)\n    \n    model = BertUnsupervisedClassifier(internet = False).to(dev)\n    model.eval()\n    model.load_state_dict(torch.load(f'{SAVEFOLDER}model.pt'))\n    \n    \n    test = match_cosinesim(model,dataloader,test,THRESHOLD)\n    \n    matches = test['matches']\n    \n    for i in range(len(matches)):\n        for j in range(len(matches[i])):\n            matches[i][j] += ' '\n    \n    matches =  [str.join('',matches[i]) for i in range(len(test))]\n    \n    submission = pd.DataFrame({'posting_id': test['posting_id'], 'matches': matches})\n    \n    \n    \n    submission.to_csv(f'{SUBMISSIONFOLDER}submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}