{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Shopee Product Matching\n* [Mô tả bài toán](#section-1)\n* [Phân tích dữ liệu](#section-2)\n* [Convert, clean dữ liệu](#section-3)\n* [Thuật toán mô hình được chọn](#section-4)\n* [Kết quả thu được](#section-5)\n* [Kết quả khi submit](#section-4)\n ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-1\"></a>\n# Mô tả bài toán\nTìm các sản phẩm trùng lặp đóng vai trò rất quan trọng trong sự cạnh tranh của các công ty thương mại điện tử. <br>\nHai hình ảnh khác nhau cùng một kho có thể là cùng một sản phẩm hoặc hai sản phẩm hoàn toàn tách biệt. Ta cần dự đoán những sản phẩm nào là cùng một nhóm giống nhau. <br>\nTập dữ liệu đầu vào là một file csv có chứa những thông tin sau:\n* `posting_id` - Mã ID của bài đăng \n* `image` - Hình ảnh của sản phẩm(Mã md5sum)\n* `image_phash` - Mã perceptual hash của sản phẩm\n* `title` - tiêu đề của sản phẩm\n* `label_group` - Mã ID của của các sản phẩm cùng loại(không chứa trong bộ dữ liệu test) \n<br>\n<br>\nTập đầu ra là các sản phẩm liên quan đến bài đăng được đưa ra file csv và có định dạng sau:\n* `posting_id` - mã ID của bài đăng\n* `matches` Các sản phẩm có cùng mã ID bài đăng. Số lượng sản phẩm trong mỗi nhóm cần tìm không vượt quá 50 và các bài đăng luôn trùng với chính nó","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\n!pip install stylecloud\n!pip install editdistance","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-08T14:35:22.111659Z","iopub.execute_input":"2022-01-08T14:35:22.112132Z","iopub.status.idle":"2022-01-08T14:35:41.341179Z","shell.execute_reply.started":"2022-01-08T14:35:22.112041Z","shell.execute_reply":"2022-01-08T14:35:41.340269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport math\nimport random \nimport os \nimport cv2\nimport timm\nimport string\nimport cv2, matplotlib.pyplot as plt\n\nfrom tqdm import tqdm \n\nimport albumentations as A \nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torch \nfrom torch.utils.data import Dataset \nfrom torch import nn\nimport torch.nn.functional as F \n\nimport gc\nimport cudf\nimport cuml\nimport cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors","metadata":{"execution":{"iopub.status.busy":"2022-01-08T14:35:41.343693Z","iopub.execute_input":"2022-01-08T14:35:41.344051Z","iopub.status.idle":"2022-01-08T14:35:49.231507Z","shell.execute_reply.started":"2022-01-08T14:35:41.344012Z","shell.execute_reply":"2022-01-08T14:35:49.230478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Khởi tạo môi trường","metadata":{}},{"cell_type":"code","source":"class CFG:\n    \n    img_size = 512\n    batch_size = 12\n    seed = 2021\n    \n    device = 'cuda'\n    classes = 11014\n    \n    scale = 30 \n    margin = 0.5","metadata":{"execution":{"iopub.status.busy":"2022-01-08T14:35:49.233542Z","iopub.execute_input":"2022-01-08T14:35:49.233877Z","iopub.status.idle":"2022-01-08T14:35:49.241004Z","shell.execute_reply.started":"2022-01-08T14:35:49.23384Z","shell.execute_reply":"2022-01-08T14:35:49.24016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_torch(seed=2021):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_torch(CFG.seed)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T14:35:49.243736Z","iopub.execute_input":"2022-01-08T14:35:49.244327Z","iopub.status.idle":"2022-01-08T14:35:49.25359Z","shell.execute_reply.started":"2022-01-08T14:35:49.244232Z","shell.execute_reply":"2022-01-08T14:35:49.252542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Đọc dữ liệu","metadata":{}},{"cell_type":"code","source":"def read_dataset(name=\"train\"):\n    df = pd.read_csv('../input/shopee-product-matching/{}.csv'.format(name))\n    df_cu = cudf.read_csv('../input/shopee-product-matching/{}.csv'.format(name))\n    image_paths = '../input/shopee-product-matching/{}_images/'.format(name) + df['image']\n    return df, df_cu, image_paths\n\ndf_train,df_cu_train,image_paths_train = read_dataset(name=\"train\")\ndf,df_cu,image_paths = read_dataset(name=\"test\")\nlen_d = len(df_cu)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T14:35:49.2564Z","iopub.execute_input":"2022-01-08T14:35:49.256916Z","iopub.status.idle":"2022-01-08T14:35:57.197676Z","shell.execute_reply.started":"2022-01-08T14:35:49.256886Z","shell.execute_reply":"2022-01-08T14:35:57.196729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-2\"></a>\n# Phân tích dữ liệu\n## In ra 5 hàng dầu tiên của bộ dữ liệu train","metadata":{"execution":{"iopub.status.busy":"2022-01-07T07:58:34.66973Z","iopub.execute_input":"2022-01-07T07:58:34.669987Z","iopub.status.idle":"2022-01-07T07:58:34.674132Z","shell.execute_reply.started":"2022-01-07T07:58:34.669961Z","shell.execute_reply":"2022-01-07T07:58:34.67334Z"}}},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-08T14:35:57.199Z","iopub.execute_input":"2022-01-08T14:35:57.199561Z","iopub.status.idle":"2022-01-08T14:35:57.224621Z","shell.execute_reply.started":"2022-01-08T14:35:57.19952Z","shell.execute_reply":"2022-01-08T14:35:57.223581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Trường `image` chứa đuôi .jpg là file dẫn đến tệp hình ảnh","metadata":{}},{"cell_type":"markdown","source":"## In ra những hàng đầu tiên của bộ test","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-08T14:35:57.225915Z","iopub.execute_input":"2022-01-08T14:35:57.226306Z","iopub.status.idle":"2022-01-08T14:35:57.262991Z","shell.execute_reply.started":"2022-01-08T14:35:57.226266Z","shell.execute_reply":"2022-01-08T14:35:57.261917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## In ra các sản phẩm ngẫu nhiên bộ train ,test","metadata":{}},{"cell_type":"code","source":"WORKING_DIR = '../input/shopee-product-matching/'\ndef displayImgDF(df, random=False, COLS=6, ROWS=4, train=True):\n    for k in range(ROWS):\n        plt.figure(figsize=(20,5))\n        for j in range(COLS):\n            if random: row = np.random.randint(0,len(df))\n            else: row = COLS*k + j\n            name = df.iloc[row,1]\n            title = df.iloc[row,3]\n            title_return = \"\"\n            for i,ch in enumerate(title):\n                title_return += ch\n                if (i != 0) and (i % 20 ==0 ): title_return += '\\n' # Cắt các title quá dài\n            if train: path = 'train_images/'\n            else: path = 'test_images/'\n            img = cv2.imread('../input/shopee-product-matching/' + path + name)\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            plt.subplot(1,COLS,j+1)\n            plt.title(title_return)\n            plt.axis('off')\n            plt.imshow(img)\n        \n        plt.show()\n        \nprint('Bộ dữ liệu train')        \ndisplayImgDF(df_train,random=True)\nprint('Bộ dữ liệu test')\ndisplayImgDF(df, random=False, COLS=3, ROWS=1, train=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T14:35:57.266194Z","iopub.execute_input":"2022-01-08T14:35:57.266684Z","iopub.status.idle":"2022-01-08T14:36:00.727052Z","shell.execute_reply.started":"2022-01-08T14:35:57.266654Z","shell.execute_reply":"2022-01-08T14:36:00.72622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## WordCloud","metadata":{}},{"cell_type":"code","source":"import stylecloud\nstylecloud.gen_stylecloud(text=' '.join(df_train['title']),\n                          icon_name='fas fa-shopping-cart',\n                          palette='colorbrewer.qualitative.Accent_8',\n                          background_color='black',\n                          gradient='horizontal',\n                          size=1024)\n\nfrom IPython.display import Image\nImage(filename=\"./stylecloud.png\", width=604, height=604)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T14:36:00.729881Z","iopub.execute_input":"2022-01-08T14:36:00.73035Z","iopub.status.idle":"2022-01-08T14:36:16.320974Z","shell.execute_reply.started":"2022-01-08T14:36:00.730312Z","shell.execute_reply":"2022-01-08T14:36:16.318593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Từ wordcloud trên có thể thấy bộ dữ liệu chủ yếu là tiếng Indonesia và 1 phần tiếng Anh","metadata":{}},{"cell_type":"markdown","source":"## Phân bố độ dài các tiêu đề","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\ndf_train['Độ dài tiêu đề sản phẩm'] = df_train['title'].apply(lambda x: len(x))\ndef plot_distribution(x, title):\n\n    fig = px.histogram(\n    df_train, \n    x = x,\n    width = 800,\n    height = 500,\n    title = title\n    )\n    \n    fig.show()\nplot_distribution(x = 'Độ dài tiêu đề sản phẩm', title = 'Phân bố độ dài tiêu đề sản phẩm')\n","metadata":{"execution":{"iopub.status.busy":"2022-01-08T14:36:16.32307Z","iopub.execute_input":"2022-01-08T14:36:16.32362Z","iopub.status.idle":"2022-01-08T14:36:17.842841Z","shell.execute_reply.started":"2022-01-08T14:36:16.323583Z","shell.execute_reply":"2022-01-08T14:36:17.841396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Có thể thấy độ dài các tiêu đề sản phẩm tập trung nhiều nhất vào khoảng 30-50. Số ít có độ dài hơn 100","metadata":{}},{"cell_type":"markdown","source":"## Hiển thị các sản phẩm trùng nhau","metadata":{}},{"cell_type":"code","source":"import editdistance\ndef plot_edit_distance(df):\n    x_axis = []\n    y_axis = []\n    root_title = df.iloc[0, 3]\n    len_df = len(df)\n    for i in range(1, len_df):\n        x_axis.append(i)\n        y_axis.append(editdistance.eval(root_title, df.iloc[i, 3]))\n    plt.plot(x_axis, y_axis)\n    plt.xlabel(\"Chỉ số tiêu đề\")\n    plt.ylabel(\"Khoảng cách edit distance\")\n    plt.show()\n    \n","metadata":{"execution":{"iopub.status.busy":"2022-01-08T14:36:17.844214Z","iopub.execute_input":"2022-01-08T14:36:17.844758Z","iopub.status.idle":"2022-01-08T14:36:17.854204Z","shell.execute_reply.started":"2022-01-08T14:36:17.844711Z","shell.execute_reply":"2022-01-08T14:36:17.853299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"groups = df_train.label_group.value_counts()\ntop = df_train.loc[df_train.label_group==groups.index[0]]\ndisplayImgDF(top, random=False, ROWS=2, COLS=4)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T14:36:17.85718Z","iopub.execute_input":"2022-01-08T14:36:17.857529Z","iopub.status.idle":"2022-01-08T14:36:19.61283Z","shell.execute_reply.started":"2022-01-08T14:36:17.8575Z","shell.execute_reply":"2022-01-08T14:36:19.611194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Edit Distance của các sản phẩm Top 1 trùng nhau","metadata":{}},{"cell_type":"code","source":"plot_edit_distance(top)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T14:36:19.61424Z","iopub.execute_input":"2022-01-08T14:36:19.614842Z","iopub.status.idle":"2022-01-08T14:36:19.752093Z","shell.execute_reply.started":"2022-01-08T14:36:19.614804Z","shell.execute_reply":"2022-01-08T14:36:19.751152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top = df_train.loc[df_train.label_group==groups.index[1]]\ndisplayImgDF(top, random=False, ROWS=2, COLS=4)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T14:36:19.753204Z","iopub.execute_input":"2022-01-08T14:36:19.753453Z","iopub.status.idle":"2022-01-08T14:36:21.069703Z","shell.execute_reply.started":"2022-01-08T14:36:19.753428Z","shell.execute_reply":"2022-01-08T14:36:21.068929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Edit Distance của các sản phẩm Top 2 trùng nhau","metadata":{}},{"cell_type":"code","source":"plot_edit_distance(top)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T14:36:21.070851Z","iopub.execute_input":"2022-01-08T14:36:21.071359Z","iopub.status.idle":"2022-01-08T14:36:21.194303Z","shell.execute_reply.started":"2022-01-08T14:36:21.071311Z","shell.execute_reply":"2022-01-08T14:36:21.193367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Nhận xét:  Dựa trên quá trình phân tích trên, các sản phẩm cùng loại có thể có tiêu đề không liên quan nhiều đến nhau. Hình ảnh lại có thể được xác định một cách rõ ràng hơn. Tuy vậy, nhiều hình ảnh với các góc chụp khác nhau có thể gây khó khăn cho mô hình. Kết hợp giữa hình ảnh và tiêu đề rõ ràng cho một kết quả chính xác hơn","metadata":{"execution":{"iopub.status.busy":"2022-01-08T11:30:18.635593Z","iopub.execute_input":"2022-01-08T11:30:18.636163Z","iopub.status.idle":"2022-01-08T11:30:18.643826Z","shell.execute_reply.started":"2022-01-08T11:30:18.636119Z","shell.execute_reply":"2022-01-08T11:30:18.64187Z"}}},{"cell_type":"markdown","source":"<a id=\"section-3\"></a>\n# Convert, clean dữ liệu\nTừ quá trình phân tích trên có thể thấy tiêu đề các sản phẩm chứa nhiều các `punctuation` như `[|*`. Nhiều sản phẩm khá giống nhau về tiêu đề, nhưng có tiêu đề in hoa, có tiêu đề không. Vì vậy, để quá trình nhận dạng hiệu quả hơn cần convert về chữ thường và bỏ các `punctuation` <br>\nNgoải ra, các `stopword` - những từ phổ biến đã được trình bày ở wordcloud có thể gây ra nhiễu và cần được loại bỏ","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n \ndef remove_stopwords(): # loại bỏ các stopword\n    stop_words = set(stopwords.words('indonesian'))\n    filtered_sentence = []\n    for i in range(len_d):\n        word_tokens = word_tokenize(df_cu['title'][i])\n        filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n        df_cu['title'][i] = ' '.join(filtered_sentence)\ndef lower_text(): # convert về chữ thường\n    for i in range(len_d):\n        df_cu['title'][i] = df_cu['title'][i].lower()\n        \nPUNCT_TO_REMOVE = string.punctuation\ndef remove_punctuation(): # loại bỏ các punctuation\n    for i in range(len_d):\n        df_cu['title'][i] = df_cu['title'][i].translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n    ","metadata":{"execution":{"iopub.status.busy":"2022-01-08T14:36:21.19578Z","iopub.execute_input":"2022-01-08T14:36:21.196125Z","iopub.status.idle":"2022-01-08T14:36:21.559007Z","shell.execute_reply.started":"2022-01-08T14:36:21.196088Z","shell.execute_reply":"2022-01-08T14:36:21.558089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_cu['title'].head())\nlower_text()\nremove_punctuation()\nremove_stopwords()\nprint('Sau khi được convert, clean')\nprint(df_cu['title'].head())","metadata":{"execution":{"iopub.status.busy":"2022-01-08T14:36:21.560322Z","iopub.execute_input":"2022-01-08T14:36:21.560689Z","iopub.status.idle":"2022-01-08T14:36:22.451486Z","shell.execute_reply.started":"2022-01-08T14:36:21.560653Z","shell.execute_reply":"2022-01-08T14:36:22.450581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tạo dataset","metadata":{}},{"cell_type":"code","source":"def get_test_transforms():\n\n    return A.Compose(\n        [\n            A.Resize(CFG.img_size,CFG.img_size,always_apply=True),\n            A.Normalize(),\n        ToTensorV2(p=1.0)\n        ]\n    )","metadata":{"execution":{"iopub.status.busy":"2022-01-08T14:36:22.452745Z","iopub.execute_input":"2022-01-08T14:36:22.453238Z","iopub.status.idle":"2022-01-08T14:36:22.458736Z","shell.execute_reply.started":"2022-01-08T14:36:22.453198Z","shell.execute_reply":"2022-01-08T14:36:22.45774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ShopeeDataset(Dataset):\n    def __init__(self, image_paths, transforms=None):\n\n        self.image_paths = image_paths\n        self.augmentations = transforms\n\n    def __len__(self):\n        return self.image_paths.shape[0]\n\n    def __getitem__(self, index):\n        image_path = self.image_paths[index]\n        \n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.augmentations:\n            augmented = self.augmentations(image=image)\n            image = augmented['image']       \n    \n        return image,torch.tensor(1)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T14:36:22.460072Z","iopub.execute_input":"2022-01-08T14:36:22.460412Z","iopub.status.idle":"2022-01-08T14:36:22.46947Z","shell.execute_reply.started":"2022-01-08T14:36:22.460376Z","shell.execute_reply":"2022-01-08T14:36:22.468625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-4\"></a>\n# Thuật toán mô hình được chọn\n## Additive Angular Margin Loss\n### Giới thiệu\nhttps://arxiv.org/pdf/1801.07698.pdf <br>\nHàm lỗi `softmax` truyền thống không có khả năng tối ưu các vector đặc trưng để làm tăng sự tương đồng giữa các đối tượng cùng lớp. Nói cách khác, hàm lỗi chưa làm được việc tăng sự khác biệt giữa các lớp. Ta có thể thấy qua hình minh họa sau:\n![](https://i0.wp.com/s1.uphinh.org/2022/01/07/ml1.png) <br>\nHàm `softmax` phân tách được các vector đặc trưng nhưng lại mập mờ giữa các ranh giới quyết định(decison boundaries). Trong khi, `ArcFace` lại phân biệt rõ ràng giữa các lớp gần nhau\n### Các bước tiến hành\nDựa trên đặc trưng $x_i$ và trọng số $W$ sau khi được chuẩn hóa, chúng ta có được giá trị $cos_{\\theta_j}$(logit) được tính theo công thức sau:\n$$\\begin{align}\n\\large cos_{\\theta_j} = W_j^Tx_i\n\\end{align}$$\nBước chuẩn hóa đặc trưng và trọng số khiến cho việc dự đoán chỉ phụ thuộc vào góc giữa chúng. Các vector đặc trưng được học sau đó sẽ được phân bố trên một hình cầu với bán kính $s$. Chính vì được phân bố như vậy, chúng ta sẽ thêm một hệ số $m$ vào góc giữa $x_i$ và $W_{y_i}$ để tăng sự tương đồng trong nội bộ lớp và sự khác biệt giữa các lớp. Sau đó, tính giá trị $cos(\\theta_{y_i}+m)$ và nhân tất cả giá trị logit với hệ số $s$. Các logits lại được đưa vào hàm `softmax` và hàm lỗi `cross entropy` <br>\nCuối cùng ta thu được hàm lỗi:\n$$\\begin{align}\n\\large L_3=-\\frac{1}{n}\\sum_{i=1}^n log\\frac{e^{s \\thinspace cos\\theta_{y_i}}}{e^{s \\thinspace cos\\theta_{y_i}}+\\sum_{j=1,j\\#{y_i}}^n}e^{s \\thinspace cos\\theta_{y_i}}\n\\end{align}$$\nPiepline của toàn bộ quá trình như sau:\n![](https://i0.wp.com/s1.uphinh.org/2022/01/07/ml2.png)\n","metadata":{}},{"cell_type":"code","source":"class ArcMarginProduct(nn.Module):\n    def __init__(self, in_features, out_features, scale=30.0, margin=0.50, easy_margin=False, ls_eps=0.0):\n        super(ArcMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.scale = scale # hệ số s\n        self.margin = margin # hệ số m\n        self.ls_eps = ls_eps \n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(margin)\n        self.sin_m = math.sin(margin)\n        self.th = math.cos(math.pi - margin) \n        self.mm = math.sin(math.pi - margin) * margin\n\n    def forward(self, input, label):\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight)) # tính cos(phi_j)\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2)) #tính sin(phi_j)\n        phi = cosine * self.cos_m - sine * self.sin_m # tính góc giữa x_i và Wy_i\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n        one_hot = torch.zeros(cosine.size(), device='cuda')\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.scale # nhân với hệ số s\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-01-08T14:36:22.472149Z","iopub.execute_input":"2022-01-08T14:36:22.472575Z","iopub.status.idle":"2022-01-08T14:36:22.485722Z","shell.execute_reply.started":"2022-01-08T14:36:22.472547Z","shell.execute_reply":"2022-01-08T14:36:22.484722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model\nBài toán được giải dựa trên 2 mạng NFNet và EfficientNet <br>\n#### Tại sao lại là NFNet và EfficientNet?\nBộ test có gần **70000** dữ liệu và thời gian chạy GPU submit bị giới hạn 2 tiếng. Vì vậy, đòi hỏi phải có các model hiệu quả về mặt thời gian nhưng cũng phải đáp ứng cao về performance. NFNet và EfficientNet đều tỏ ra tối ưu về thời gian huấn luyện cũng như suy luận. Về độ chính xác, EfficentNet đạt được kết quả tốt hơn so với các mạng ConvNet trước đó. Cỵ thể, mô hình đạt được 84.3% top-1 accuracy trong bộ dữ liệu ImageNet và nhỏ hơn 8.4 lần và suy luận nhanh hơn 6.1 lần các mạng Conv. NFNet thậm chí còn nhanh hơn 8.7 lần EfficientNet khi huấn luyện và đạt được 86.5% top-1 accuracy ImageNet. Cụ thể hơn, các model được sử dụng là `EfficentNetB3`, `EfficentNetB5` và `NFNetL0` đã được pretrain dựa trên hàm lỗi ArcMargin\n## EfficientNet\nhttps://arxiv.org/pdf/1905.11946.pdf <br>\nCác tác giả có nhận xét cân bằng được các yếu tố độ sâu mạng, chiều rộng và độ phân giải cho ra một mô hình tốt hơn. Dựa trên quan sát đó, ta có thể scale chiều sâu/chiều rông/độ phân giải bằng một hệ số được gọi là `compound coefficient`\n#### Compound Scaling\nCompound Scaling sử dụng hệ số compound coefficient $\\phi$ để scale đều độ sâu, rộng và phân giải theo công thức sau:\n$$\\begin{align}\n\\textrm{độ sâu} &: d=\\alpha^{\\phi} \\\\\n\\textrm{độ rộng} &: w=\\beta^{\\phi} \\\\\n\\textrm{độ phân giải} &: r=\\gamma^{\\phi} \\\\\n\\textrm{với} &: \\alpha.\\beta^2.\\gamma^2 \\approx 2 \\\\\n& \\alpha \\ge 1, \\beta \\ge 1, \\gamma \\ge 1\n\\end{align}$$\n<br>\n$\\alpha, \\beta, \\gamma$ là hằng số và có thể xác định bằng các thuật toán tìm kiếm lưới(grid search). $\\phi$ giúp kiểm soát số lượng tài nguyên(resources) sẵn có cho model scaling, trong khi  $\\alpha, \\beta, \\gamma$ mô tả lượng tài nguyên thêm vào độ rông, sâu , phân giải. Dựa trên các ý tương trên, mạng cơ sở `EfficientNet-B0` đã được xây dựng để tối ưu về cả performance và FLOPS <br>\nKiến trúc của mạng `EfficentNet-B0`: <br>\n![](https://i.upanh.org/2022/01/08/ml3.png) <br>\nTừ mạng cơ sở `EfficentNet-B0` được scale up với các hệ số $\\phi$ khác nhau và thu được các mô hình từ B1 đến B7\n### NFNet\nhttps://arxiv.org/pdf/2102.06171.pdf <br>\nNFNet được cải tiến dựa trên ResNet với các điểm khác biệt sau:\n- Sửa đối các nhánh residual và tích chập với các trọng số được scale <br>\n    - Để train các mạng ResNet sâu mà không cần Batch Normalization, cần bóp các hàm kích hoạt tại mỗi nhánh thặng dư. NFNet dùng 2 scalar $\\alpha$ và $\\beta$ để thực hiện điều đó\n![](https://i.upanh.org/2022/01/08/ml4.png)    \n- Adaptive Gradient Clipping\n    - AGC được sử dụng để train mô hình với batch size và learning rate lớn \n- Tối ưu kiến trúc để thu được mô hình có độ chính xác cao và tăng tốc độ training\n    - Mặc dù, sửa đối các nhánh thặng dư, tích chập hay thêm AGC nhưng model vẫn chưa đạt được kết quả cao như EfficientNet. Các tác giả sử dụng SE-ResNeXt-D làm mô hình cơ sở và thay đổi để đạt được performance cao","metadata":{}},{"cell_type":"code","source":"class ShopeeModel(nn.Module):\n\n    def __init__(\n        self,\n        n_classes = CFG.classes,\n        model_name = None,\n        fc_dim = 512,\n        margin = CFG.margin,\n        scale = CFG.scale,\n        use_fc = True,\n        pretrained = False):\n\n\n        super(ShopeeModel,self).__init__()\n        print('Building Model Backbone for {} model'.format(model_name))\n\n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n\n        if model_name == 'resnext50_32x4d':\n            final_in_features = self.backbone.fc.in_features\n            self.backbone.fc = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n\n        elif 'efficientnet' in model_name:\n            final_in_features = self.backbone.classifier.in_features\n            self.backbone.classifier = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n        \n        elif model_name == 'eca_nfnet_l0':\n            final_in_features = self.backbone.head.fc.in_features\n            self.backbone.head.fc = nn.Identity()\n            self.backbone.head.global_pool = nn.Identity()\n\n        self.pooling =  nn.AdaptiveAvgPool2d(1)\n\n        self.use_fc = use_fc\n        \n        self.dropout = nn.Dropout(p=0.0)\n        self.fc = nn.Linear(final_in_features, fc_dim)\n        self.bn = nn.BatchNorm1d(fc_dim)\n        self._init_params()\n        final_in_features = fc_dim\n        # load các trọng số ArcFace đã được pretrain\n        self.final = ArcMarginProduct(\n            final_in_features,\n            n_classes,\n            scale = scale,\n            margin = margin,\n            easy_margin = False,\n            ls_eps = 0.0\n        )\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, image, label):\n        feature = self.extract_feat(image)\n        return feature\n\n    def extract_feat(self, x):\n        batch_size = x.shape[0]\n        x = self.backbone(x)\n        x = self.pooling(x).view(batch_size, -1)\n\n        if self.use_fc:\n            x = self.dropout(x)\n            x = self.fc(x)\n            x = self.bn(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-01-08T14:36:22.48716Z","iopub.execute_input":"2022-01-08T14:36:22.487814Z","iopub.status.idle":"2022-01-08T14:36:22.504153Z","shell.execute_reply.started":"2022-01-08T14:36:22.487771Z","shell.execute_reply":"2022-01-08T14:36:22.503175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model(model_name = None, model_path = None, n_classes = None):\n    model = ShopeeModel(model_name = model_name)\n    model.eval()\n    model.load_state_dict(torch.load(model_path))\n    model = model.to(CFG.device)\n    return model ","metadata":{"execution":{"iopub.status.busy":"2022-01-08T14:36:22.505991Z","iopub.execute_input":"2022-01-08T14:36:22.506767Z","iopub.status.idle":"2022-01-08T14:36:22.515991Z","shell.execute_reply.started":"2022-01-08T14:36:22.506728Z","shell.execute_reply":"2022-01-08T14:36:22.51512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ensemble Learning\nCác mô hình đề cập phía trên sẽ được kết hợp lại và lấy trung bình cộng","metadata":{}},{"cell_type":"code","source":"class EnsembleModel(nn.Module):\n    \n    def __init__(self):\n        super(EnsembleModel,self).__init__()\n        self.m1 = get_model('eca_nfnet_l0','../input/shopee-pytorch-models/arcface_512x512_nfnet_l0 (mish).pt')\n        self.m2 = get_model('tf_efficientnet_b5_ns','../input/shopee-pytorch-models/arcface_512x512_eff_b5_.pt')\n        self.m3 = get_model('tf_efficientnet_b3_ns','../input/shopee-pytorch-models/arcface_512x512_eff_b3.pt')\n    def forward(self,img,label):\n        feat1 = self.m1(img, label)\n        feat2 = self.m2(img, label)\n        feat3 = self.m3(img, label)\n    \n        return (feat1 + feat2 + feat3) / 3","metadata":{"execution":{"iopub.status.busy":"2022-01-08T14:36:22.517279Z","iopub.execute_input":"2022-01-08T14:36:22.518015Z","iopub.status.idle":"2022-01-08T14:36:22.528009Z","shell.execute_reply.started":"2022-01-08T14:36:22.517973Z","shell.execute_reply":"2022-01-08T14:36:22.526951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Trích xuất các đặc trưng từ ảnh","metadata":{}},{"cell_type":"code","source":"def get_image_embeddings(image_paths, model_name = None, model_path = None):\n    embeds = []\n    \n    model = EnsembleModel()\n    \n    image_dataset = ShopeeDataset(image_paths=image_paths,transforms=get_test_transforms())\n    image_loader = torch.utils.data.DataLoader(\n        image_dataset,\n        batch_size=CFG.batch_size,\n        pin_memory=True,\n        drop_last=False,\n        num_workers=4\n    )\n    \n    \n    with torch.no_grad():\n        for img,label in tqdm(image_loader): \n            img = img.cuda()\n            label = label.cuda()\n            feat = model(img,label)\n            image_embeddings = feat.detach().cpu().numpy()\n            embeds.append(image_embeddings)\n    \n    \n    del model\n    image_embeddings = np.concatenate(embeds)\n    print(f'Our image embeddings shape is {image_embeddings.shape}')\n    del embeds\n    gc.collect()\n    return image_embeddings","metadata":{"execution":{"iopub.status.busy":"2022-01-08T14:36:22.53058Z","iopub.execute_input":"2022-01-08T14:36:22.530827Z","iopub.status.idle":"2022-01-08T14:36:22.540125Z","shell.execute_reply.started":"2022-01-08T14:36:22.530803Z","shell.execute_reply":"2022-01-08T14:36:22.539331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_embeddings = get_image_embeddings(image_paths.values)\nprint('Kết thúc image embeddings')","metadata":{"execution":{"iopub.status.busy":"2022-01-08T14:36:22.541384Z","iopub.execute_input":"2022-01-08T14:36:22.541985Z","iopub.status.idle":"2022-01-08T14:36:32.30633Z","shell.execute_reply.started":"2022-01-08T14:36:22.541949Z","shell.execute_reply":"2022-01-08T14:36:32.305437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Matching các đặc trưng ảnh dựa trên KNN\n- 1. Chọn $k$ ảnh có đặc trưng gần nhất với ảnh đang xét. \n- 2. Nếu kết quả trả về ít hơn 2, nâng ngưỡng lên nhằm tránh việc không thu được kết quả nào thỏa mãn. Tuy nhiên do ngưỡng được tăng lên, có thể thu được nhiều kết quả không chính xác vì vậy chỉ dừng lại lấy 2 đặc trưng gần nhất. \n- 3. Ngược lại, kết quả trả về nhiều hơn 2 ảnh, giảm ngưỡng đi 1 khoảng `0.08888`. Nếu kết quả trả về nhỏ hơn 2 quay lại bước 2","metadata":{}},{"cell_type":"code","source":"def get_image_predictions(df, embeddings,threshold = 0.0):\n    \n    if len(df) > 3: # bộ test là bộ đang submit\n        KNN = 50 # do giới hạn của cuộc thi tối đa là 50 matching\n    else : \n        KNN = 3\n    \n    model = NearestNeighbors(n_neighbors = KNN, metric = 'cosine')\n    model.fit(embeddings)\n    distances, indices = model.kneighbors(embeddings)\n    \n    predictions = []\n    for k in tqdm(range(embeddings.shape[0])):\n        idx = np.where(distances[k,] < threshold)[0] #lấy các chỉ số của các khoảng cách nhỏ hơn ngưỡng\n        ids = indices[k,idx] # tìm các id dựa trên chỉ số\n        posting_ids = df['posting_id'].iloc[ids].values\n        if len(posting_ids) >= 2: # kết quả trả về lớn hơn 2\n            idx_s = np.where(distances[k,] < threshold - 0.08888)[0] # giảm ngưỡng\n            ids_s = indices[k, idx_s] \n            posting_ids_b = df['posting_id'].iloc[ids_s].values\n            if len(posting_ids_b) >= 2:\n                predictions.append(posting_ids_b) # chọn theo ngưỡng đã giảm\n            else:\n                predictions.append(posting_ids) # chọn theo ngưỡng ban đầu\n        else:\n            idx = np.where(distances[k,] < 0.51313)[0]\n            ids = indices[k,idx]\n            posting_ids = df['posting_id'].iloc[ids].values\n            predictions.append(posting_ids[:2]) # chỉ lấy 2 ảnh gần nhất\n            \n        \n    del model, distances, indices\n    gc.collect()\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2022-01-08T14:36:32.307941Z","iopub.execute_input":"2022-01-08T14:36:32.308537Z","iopub.status.idle":"2022-01-08T14:36:32.318802Z","shell.execute_reply.started":"2022-01-08T14:36:32.308499Z","shell.execute_reply":"2022-01-08T14:36:32.31786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Matching các đặc trưng text dựa trên cosine similarity\nCác `title` sẽ được Tfidf xử lý thành các vector đặc trưng. Sau đó chia thành các chunk nhỏ. Từng vector đặc trưng trong các chunk được tính cosine similarity với toàn bộ các vector đang có. Tuy nhiên, ngược với matching ảnh do cosine similarity càng lớn 2 vector đặc trưng càng giống nhau(cosine distance bằng 0). Các phép toán so sánh sẽ ngược với phần trên nhưng toàn bộ quá trình chọn ngưỡng vẫn tương tự như vậy","metadata":{}},{"cell_type":"code","source":"def get_text_predictions(df_cu, max_features = 25_000):\n    \n    model = TfidfVectorizer(binary = True, max_features = max_features)\n    text_embeddings = model.fit_transform(df_cu['title']).toarray()\n    preds = []\n    CHUNK = 1024 * 4 # chunk size\n\n    num_chunk = len(df) // CHUNK # số lượng chunk\n    if len(df) % CHUNK != 0: \n        num_chunk += 1\n    for j in range(num_chunk):\n\n        a = j * CHUNK # chỉ số bắt đầu chunk j\n        b = (j + 1) * CHUNK # chỉ số kết thúc chunk j\n        b = min(b, len(df))\n        print('chunk', a, 'to', b)\n\n        cos_similarity = cupy.matmul(text_embeddings, text_embeddings[a:b].T).T # tính cosine similarity\n\n        for k in range(b - a):\n            indexes = cupy.where(cos_similarity[k,] > 0.7705)[0]\n            ids = df_cu.iloc[cupy.asnumpy(indexes)].posting_id.to_pandas().values\n            if len(ids) >= 2:\n                indexes_lower_thres = cupy.where(indexes[k,] > 0.80105)[0]\n                ids_lower_thres = df_cu.iloc[cupy.asnumpy(indexes_lower_thres)].posting_id.to_pandas().values\n                if len(o) >= 2:\n                    preds.append(ids_lower_thres)\n                else:\n                    preds.append(ids)\n            else:\n                indexes = cupy.where(cos_similarity[k,]>0.6555)[0]\n                ids = df_cu.iloc[cupy.asnumpy(indexes)].posting_id.to_pandas().values\n                preds.append(ids[:2])\n    \n    del model,text_embeddings\n    gc.collect()\n    return preds","metadata":{"execution":{"iopub.status.busy":"2022-01-08T14:36:32.322386Z","iopub.execute_input":"2022-01-08T14:36:32.322693Z","iopub.status.idle":"2022-01-08T14:36:32.334106Z","shell.execute_reply.started":"2022-01-08T14:36:32.322667Z","shell.execute_reply":"2022-01-08T14:36:32.332983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Image, Text Prediction","metadata":{}},{"cell_type":"code","source":"image_predictions = get_image_predictions(df, image_embeddings, threshold = 0.36)\ntext_predictions = get_text_predictions(df_cu, max_features = 25_000)\nprint('Kết thúc image + text prediction')","metadata":{"execution":{"iopub.status.busy":"2022-01-08T14:36:32.33559Z","iopub.execute_input":"2022-01-08T14:36:32.33597Z","iopub.status.idle":"2022-01-08T14:36:48.048485Z","shell.execute_reply.started":"2022-01-08T14:36:32.335936Z","shell.execute_reply":"2022-01-08T14:36:48.047031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Kết hợp Text, Image prediction ","metadata":{}},{"cell_type":"code","source":"def combine_predictions(row):\n    x = np.concatenate([row['image_predictions'], row['text_predictions']])\n    return ' '.join(np.unique(x))","metadata":{"execution":{"iopub.status.busy":"2022-01-08T14:36:48.049843Z","iopub.execute_input":"2022-01-08T14:36:48.050215Z","iopub.status.idle":"2022-01-08T14:36:48.057092Z","shell.execute_reply.started":"2022-01-08T14:36:48.050177Z","shell.execute_reply":"2022-01-08T14:36:48.056213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Pipeine của toàn bộ quá trình được tóm gọn bằng sơ đồ sau** \n<br>\n<br>\n![so_do](https://i.upanh.org/2022/01/08/Untitled-Diagram.drawio1.png)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-5\"></a>\n# Kết quả thu được","metadata":{}},{"cell_type":"code","source":"df['image_predictions'] = image_predictions\ndf['text_predictions'] = text_predictions\ndf['matches'] = df.apply(combine_predictions, axis = 1)\ndf[['posting_id', 'matches']].to_csv('submission.csv', index = False)\ndf[['posting_id', 'matches']].head()\n###out put the result","metadata":{"execution":{"iopub.status.busy":"2022-01-08T14:36:48.058341Z","iopub.execute_input":"2022-01-08T14:36:48.058884Z","iopub.status.idle":"2022-01-08T14:36:48.085805Z","shell.execute_reply.started":"2022-01-08T14:36:48.058845Z","shell.execute_reply":"2022-01-08T14:36:48.08487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Bộ test khi chưa submit chỉ có 3 hàng dữ liệu, vì vậy khi chạy đều cho ra kết quả chính xác. Model đã được pretrain với bộ train, khi chạy bộ train sẽ cho ra kết quả không khách quan","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-6\"></a>\n# Kết quả khi submit\n![](https://i.upanh.org/2022/01/08/ml6.png)","metadata":{}}]}