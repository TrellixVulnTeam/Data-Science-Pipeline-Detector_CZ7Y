{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport csv\nimport os\nimport sys\nimport sklearn\nimport tensorflow as tf\nfrom tensorflow import keras\nimport math\n\nimport gc\n\n#modeling\nfrom keras import backend as K\nfrom keras.models import Model\nfrom keras.layers import *\nimport tensorflow_addons as tfa\nfrom sklearn.model_selection import train_test_split\n#import transformers\n\n#image preprocessing\nfrom keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\nfrom PIL import Image\nimport cv2\n\n#visualization\nimport matplotlib.pyplot as plt\n#import seaborn as sns\n\n#warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nimport cudf, cuml, cupy\nfrom cuml.neighbors import NearestNeighbors\n\n\n# RESTRICT TENSORFLOW TO 1GB OF GPU RAM\n# SO THAT WE HAVE 15GB RAM FOR RAPIDS\nLIMIT = 14\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n  try:\n    tf.config.experimental.set_virtual_device_configuration(\n        gpus[0],\n        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*LIMIT)])\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n    #print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n  except RuntimeError as e:\n    print(e)\n\n\nK.clear_session()\nnp.random.seed(2021); tf.random.set_seed(2021)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/shopee-product-matching/train.csv')\n#N_tot = len(train)\n#N_index = np.arange(N_tot)\n#train['index'] = N_index\n\ntrain_jpg_directory = '../input/shopee-product-matching/train_images'\n\ntrain_image_path = []\nfor img in train.image:\n    train_image_path.append(os.path.join(train_jpg_directory, img))\ntrain['img_path'] = train_image_path","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GroupKFold\nskf = GroupKFold(5)\ntrain['fold'] = -1\nfor i, (train_idx, valid_idx) in enumerate(skf.split(X=train, groups=train['label_group'])):\n    train.loc[valid_idx, 'fold'] = i","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_set = []\nvalidation_set = []\n\nfor i in range(5):\n    train_fold = train[train['fold']!=i].reset_index(drop=True)\n    label_dict = train_fold.groupby('label_group').posting_id.agg('unique').to_dict()\n    train_fold['target'] = train_fold.label_group.map(label_dict)\n    #train_fold['indexing'] = np.arange(len(train_fold))\n    \n    valid_fold = train[train['fold']==i].reset_index(drop=True)\n    label_dict = valid_fold.groupby('label_group').posting_id.agg('unique').to_dict()\n    valid_fold['target'] = valid_fold.label_group.map(label_dict)\n    \n    training_set.append(train_fold)\n    validation_set.append(valid_fold)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMAGE_SIZE = [256, 256]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_batch(train, batch_index, label_dist):\n    x_inputs = np.zeros((len(batch_index), 256, 256, 3))\n    x_sim = np.ones((len(batch_index), len(batch_index))) * -1\n    for i, iindex in enumerate(batch_index):\n        x_anchor = train.iloc[iindex].img_path\n        x_anchor = tf.io.read_file(x_anchor)\n        x_anchor = tf.image.decode_jpeg(x_anchor, channels=3)\n        x_anchor = tf.image.resize(x_anchor, IMAGE_SIZE)\n\n        x_inputs[i] = x_anchor\n        for k, rindex in enumerate(batch_index):\n            #print(train.iloc[iindex].indexing)\n            if rindex in label_dist[train.iloc[iindex].indexing]:\n                x_sim[i, k] = 1.0\n    \n    return x_inputs, x_sim","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications import EfficientNetB0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model():\n    model = EfficientNetB0(include_top=False, weights='../input/effnetb0/efficientnetb0_notop.h5', pooling='avg', input_shape=None)\n    x = Dropout(rate = 0.2, name = \"added_dropout\")(model.output)\n    x = Dense(units = 128, name = \"added_fc_layer\")(x)\n    x = BatchNormalization(name = \"added_bn_layer\")(x)\n    x = Lambda(lambda x: K.l2_normalize(x,axis=1))(x)\n    model = Model(inputs=model.input, outputs=x)\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_batch(train, batch_index):\n    x_inputs = np.zeros((len(batch_index), 256, 256, 3))\n    for i, iindex in enumerate(batch_index):\n        x_anchor = train.iloc[iindex].img_path\n        x_anchor = tf.io.read_file(x_anchor)\n        x_anchor = tf.image.decode_jpeg(x_anchor, channels=3)\n        x_anchor = tf.image.resize(x_anchor, IMAGE_SIZE)\n\n        x_inputs[i] = x_anchor\n    return x_inputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pair_loss(x_output, x_cur, similarity):\n    \n    prod = tf.matmul(x_output, x_cur.T)\n    \n    loss = tf.maximum(0, 0.5 - tf.multiply(prod, similarity))\n    \n    return tf.reduce_mean(loss)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_feats = []\n\nfor i in range(5):\n    train = training_set[i]\n    valid = validation_set[i]\n    \n    train['indexing'] = np.arange(len(train))\n    label_dict = train.groupby('label_group').indexing.agg('unique').to_dict()\n    train['target'] = train.label_group.map(label_dict)\n    \n    N = len(train)\n    print(N)\n    \n    label_list = []\n    similar_img = {} \n    for il in range(N):\n        similar_img[il] = train.iloc[il].target.tolist()    \n    \n    for k, v in label_dict.items():\n        label_list.append(v.tolist())\n        \n    N = len(label_list)\n    \n    N_index = np.arange(N)\n    \n    batchsize = 15\n    \n    ITER = N // batchsize\n    if N%batchsize != 0:\n        ITER += 1\n    \n    K.clear_session()\n    op = keras.optimizers.Adam(lr=1e-5, beta_1=0.9, beta_2=0.999, epsilon=None, amsgrad=False)\n    model = build_model()\n    \n    loss_fn = pair_loss\n\n    for ip in range(8):\n        np.random.shuffle(N_index)\n        ITER = 0\n        out_it = 0\n        \n        while ITER<N:\n            curlen = 0\n            batch_index = []\n            while ITER < N:\n                QQQ = len(label_list[N_index[ITER]])\n                if curlen + QQQ < 80:\n                    batch_index.extend(label_list[N_index[ITER]])  \n                    curlen += QQQ\n                    ITER += 1\n                else:\n                    break\n            if len(batch_index) == 0:\n                print(\"batch_index ==== 0!!! wrong\")\n                exit()\n            x_input, sim = create_batch(train, batch_index, similar_img)\n            #print(x_input.shape)\n            x_cur = model.predict(x_input)\n            #print(x_cur.shape)\n\n            with tf.GradientTape() as tape:\n                x_output = model([x_input], training=True)\n                loss_value = loss_fn(x_output, x_cur, sim)\n            grads = tape.gradient(loss_value, model.trainable_weights)\n            op.apply_gradients(zip(grads, model.trainable_weights))\n            if ITER // 1000 == out_it:\n                print(\"In fold %d, epoch %d, iteration %d, loss value = %.4f\"%(i, ip, ITER, loss_value) )\n                out_it += 1\n\n    N = len(valid)\n    N_index = np.arange(N)\n    batchsize = 128\n    ITER = N // batchsize\n    if N%batchsize != 0:\n        ITER += 1\n    valid_feat = np.zeros((N, 128))\n    print(N)\n    for it in range(ITER):\n        train_batch = N_index[it*batchsize: min((it + 1)*batchsize, N)]\n        x_batch = get_batch(valid, train_batch)\n        x_cur = model.predict(x_batch)\n        valid_feat[train_batch] = x_cur\n    \n    valid_feats.append(valid_feat)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"thresholdlist = list(np.linspace(0.3,0.8,15))\nprint(thresholdlist)\n\nacc_threshold = np.zeros((5, len(thresholdlist)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def computeFscore(true, pred):\n    n = len( np.intersect1d(true, pred) )\n    return 2*n / (len(true)+len(pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 1024\n\nfor thi, thval in enumerate(thresholdlist):\n    print(\"begining\")\n    scores = []\n    score = 0\n    for i in range(5):\n        valid = validation_set[i]\n        valid_feat = valid_feats[i]\n        N_size = valid_feat.shape[0]\n        valid_feat = cupy.asarray(valid_feat)\n             \n        N_index = np.arange(N_size)\n        num_part = N_size//batch_size\n        if N_size%batch_size !=0:\n            num_part += 1\n\n        f1_score = 0.0\n        \n        for k in range(num_part):\n            batch_index = N_index[i*batch_size: min((i+1)*batch_size, len(N_index))]\n            batch_feat = valid_feat[batch_index]\n            distances = cupy.matmul(batch_feat, valid_feat.T)\n            for s in range(len(batch_index)):\n                IDX = cupy.where(distances[s] >= thval)[0]\n                #print(IDX)\n                pred = valid.iloc[cupy.asnumpy(IDX)].posting_id.values\n                f1_score += computeFscore(valid.iloc[batch_index[s]].target, pred)\n        f1_score = f1_score/N_size\n        \n        acc_threshold[i, thi] = f1_score\n        print(f1_score)\nprint(acc_threshold)\nprint(np.mean(acc_threshold, axis = 0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}