{"cells":[{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://i.imgur.com/gODdzoI.png\">\n\n<center><h1>-Data Preprocessing-</h1></center>\n\n# 1. Introduction\n\n### 🟢 Goal:\n> Building a model that can identify which images contain the same product/s.\n\n### 🟠 Challenges:\n* Finding near-duplicates of the product (and NOT the image)\n* Erasing the impact of the background (the area surrounding the product) \n* Using the description of the image (or the title )\n\n\n### 📚 Libraries + W&B\n* Create an account on https://wandb.ai\n* Input your personal key of the project (mine will be secret, as it is confidential 🙃)\n* You can find my project in the W&B Dashboard by [clicking here](https://wandb.ai/andrada/shopee-kaggle?workspace=user-andrada)."},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"pip install textfeatures","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install pandarallel","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!jupyter nbextension enable --py widgetsnbextension","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Libraries\nimport wandb\nimport os\nimport cv2\nfrom PIL import Image\nimport string\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport albumentations as alb\nfrom albumentations.augmentations.transforms import RGBShift, HueSaturationValue, HorizontalFlip,\\\n                                                    CLAHE, RandomCrop, RandomGamma, Rotate,\\\n                                                    CenterCrop, MedianBlur, VerticalFlip, InvertImg\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk import pos_tag, ne_chunk\nfrom textblob import TextBlob\nimport textfeatures\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom wordcloud import WordCloud, ImageColorGenerator\nfrom wordcloud import STOPWORDS as stopwords_wc\n\nfrom pandarallel import pandarallel\n# Enable progress tracking\npandarallel.initialize(progress_bar=True)\n\n# Environment check\nos.environ[\"WANDB_SILENT\"] = \"true\"\n\n# Secrets 🤫\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\npersonal_key_for_api = user_secrets.get_secret(\"wandb\")\n\n# Color scheme\nmy_colors = [\"#EDAC54\", \"#F4C5B7\", \"#DD7555\", \"#B95F18\", \"#475A20\"]\n\nclass color:\n    BOLD = '\\033[1m' + '\\033[93m'\n    END = '\\033[0m'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! wandb login $personal_key_for_api","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n    '''Plots the value at the end of the a seaborn barplot.\n    axs: the ax of the plot\n    h_v: weather or not the barplot is vertical/ horizontal'''\n    \n    def _show_on_single_plot(ax):\n        if h_v == \"v\":\n            for p in ax.patches:\n                _x = p.get_x() + p.get_width() / 2\n                _y = p.get_y() + p.get_height()\n                value = int(p.get_height())\n                ax.text(_x, _y, format(value, ','), ha=\"center\") \n        elif h_v == \"h\":\n            for p in ax.patches:\n                _x = p.get_x() + p.get_width() + float(space)\n                _y = p.get_y() + p.get_height()\n                value = int(p.get_width())\n                ax.text(_x, _y, format(value, ','), ha=\"left\")\n\n    if isinstance(axs, np.ndarray):\n        for idx, ax in np.ndenumerate(axs):\n            _show_on_single_plot(ax)\n    else:\n        _show_on_single_plot(axs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Data - Images\n\n* **train**: \n    * `train_images` : the product photos (~ 32,400 files)\n    * `train.csv` : the corresponding metadata; each product is assigned a label_group that marks the images with identical products.\n* **test**: \n    * `test_images` : the product photos to be predicted (~ 70,000 hidden files, only 3 showing)\n    * `test.csv` : the corresponding metadata"},{"metadata":{"trusted":true},"cell_type":"code","source":"run = wandb.init(project=\"shopee-kaggle\", name=\"image-discover\")\n\n# Read in data\ntrain_base = \"../input/shopee-product-matching/train_images\"\ntrain_df = pd.read_csv(\"../input/shopee-product-matching/train.csv\")\n\nprint(\"Files in train image folder: {:,}\".format(len(os.listdir(train_base))), \"\\n\" +\n      \"Rows in train dataframe: {:,}\".format(train_df.shape[0]))\n\n# Log into W&B\nwandb.log({'Files in train image folder': len(os.listdir(train_base)), \n           'Rows in train dataframe' : train_df.shape[0]})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.1 Duplicated Images\n\nThere are 1,246 images that have 2 or more apparitions:\n* The `title` differs for most of them\n* The `label_group` is usually the same, but there are a few cases where it differs as well"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the count of apparitions per image\nimage_count = train_df[\"image\"].value_counts().reset_index()\nimage_count.columns = [\"image\", \"count\"]\nimage_count_duplicates = image_count[image_count[\"count\"] > 1]\nprint(\"Total no. of images with duplicates: {:,}\".format(len(image_count_duplicates)))\n\n#Plot\nfig, ax = plt.subplots(figsize=(16, 7))\nplt.bar(x=image_count_duplicates.iloc[::16][\"image\"],\n        height=image_count_duplicates.iloc[::16][\"count\"],\n        color=my_colors[4])\nplt.title(\"Duplicated Images: How many apparitions?\", fontsize=20)\nplt.xticks([])\nplt.xlabel(\"Image ID\", fontsize=16)\nplt.ylabel(\"Count\", fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# --- Make a custom plot to save into W&B ---\n\n# Prepare data\nn = len(image_count_duplicates.iloc[::16][\"image\"])\nlabels = [\"id_\" + str.zfill(str(i), 2) for i in range(n)]\nvalues = image_count_duplicates.iloc[::16][\"count\"]\n\ndata = [[label, val] for (label, val) in zip(labels, values)]\n\n# Create Table & .log() the plot\ntable = wandb.Table(data=data, columns = [\"Image_ID\", \"count\"])\nwandb.log({\"image_chart\" : wandb.plot.bar(table, \"Image_ID\", \"count\",\n                                          title=\"Duplicated Images: How many apparitions?\")})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> You can check the plot in the W&B Project:\n<img src=\"https://i.imgur.com/bmAG5Zh.png\" width=650>\n\nI also wanted to look at how these images with same \"image name\" look and what trully differentiates them:\n* The description usually reffers to the same object, but the wording is different\n* The Group ID can sometimes be different, although the image is exactly the same - this means that the text description is the one that is indicating the category in these instances.\n\n> **📌 Note**: If you're using `cv2` to visualize the images, note that they will be displayed in the `BGR` colorspace (blue-green-red order - for some reason this is the default of this library). To correct that, you can use `cv2.cvtColor` to display them in the `RGB` colorspace. "},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"def get_image_info(name):\n    '''Displays a photo of the image and information on it.\n    name: a string containing the code of the image (.jpg format)'''\n    \n    # Read in the image & corresponding metadata\n    sample_image = cv2.imread(train_base + \"/\" + name)\n    sample_image = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\n    sample_df = train_df[train_df[\"image\"] == name]\n    \n    print(color.BOLD + \"Apparitions for this image:\" + color.END, len(sample_df), \"\\n\" +\n          color.BOLD + \"Some titles:\" + color.END, sample_df[\"title\"].value_counts().index[:5].values, \"\\n\" +\n          color.BOLD + \"No. of unique groups:\" + color.END, sample_df[\"label_group\"].value_counts().shape[0])\n\n    # Plot image\n    plt.figure(figsize=(16, 7))\n    plt.imshow(sample_image)\n    plt.axis(\"off\")\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Example 1\nsample_name = image_count_duplicates.iloc[0][\"image\"]\nget_image_info(name = sample_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Example 2\nsample_name = image_count_duplicates.iloc[11][\"image\"]\nget_image_info(name = sample_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Example 3\nsample_name = image_count_duplicates.iloc[12][\"image\"]\nget_image_info(name = sample_name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 💾 Clean Duplicates Function\n\nHence, we'll clean these duplicates by selecting only the first appearence for each."},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"def clean_duplicates(df, train=True):\n    '''Intakes the original dataframe and returns it wo duplicated images.\n    Converts label_group to string as well.'''\n    \n    if train == True:\n        df[\"label_group\"] = df[\"label_group\"].astype(str)\n    df = df.drop_duplicates(subset=['image']).reset_index(drop=True)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Clean duplicates\ntrain_df = clean_duplicates(df=train_df)\n\nprint(\"Is train metadata now the same length as the image train folder?\", \"\\n\",\n      train_df.shape[0] == len(os.listdir(train_base)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Label Group\n\n> 📌**Note**: If there are 2 or more images with the same `label_group` it means that these have been already mapped as being identical."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get count of values on each group\ngroups_df = train_df[\"label_group\"].value_counts().reset_index()\ngroups_df.columns = [\"group\", \"count\"]\n\n# Print info\nprint(\"No. of unique groups: {:,}\".format(len(groups_df)), \"\\n\" +\n      \"Max no. of apparitions in 1 group: {}\".format(groups_df[\"count\"].max()), \"\\n\" +\n      \"Min no. of apparitions in 1 group: {}\".format(groups_df[\"count\"].min()))\nwandb.log({\"No. of unique groups\": len(groups_df)})\n\n#Plot\nfig, ax = plt.subplots(figsize=(16, 7))\nplt.bar(x=groups_df.iloc[::100][\"group\"],\n        height=groups_df.iloc[::100][\"count\"],\n        color=my_colors[3])\nplt.title(\"Group Count Distribution\", fontsize=20)\nplt.xticks([])\nplt.xlabel(\"Group ID\", fontsize=16)\nplt.ylabel(\"Count\", fontsize=16)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# --- Make a custom plot to save into W&B ---\n\n# Prepare data\nn = len(groups_df.iloc[::100][\"group\"])\nlabels = [\"id_\" + str.zfill(str(i), 3) for i in range(n)]\nvalues = groups_df.iloc[::100][\"count\"]\n\ndata = [[label, val] for (label, val) in zip(labels, values)]\n\n# Create Table & .log() the plot\ntable = wandb.Table(data=data, columns = [\"Group_ID\", \"count\"])\nwandb.log({\"group_chart\" : wandb.plot.bar(table, \"Group_ID\", \"count\",\n                                          title=\"Group Count Distribution\")})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> And this is the plot in the W&B Project:\n<img src=\"https://i.imgur.com/eOHWhjQ.png\" width=650>\n\nLet's also observe the images within some of the groups:\n* There is definitely a resemblance between products (for the human eye 👁)\n* For some groups however, the overall structure of the images is very different"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def get_group_info(group_name):\n    '''This function shows a sample of 6 images within a group.\n    group_name: a string representing the desired group code'''\n    \n    # Retrieve a sample of 6 images from this group\n    sample_names = train_df[train_df[\"label_group\"] == group_name][\"image\"].\\\n                    sample(6, random_state=24).values\n    sample_text = train_df[train_df[\"label_group\"] == group_name][\"title\"].\\\n                    sample(1, random_state=1).values\n\n    # Plot\n    fig = plt.figure(figsize=(16, 8))\n    plt.suptitle(f\"Group: {sample_group}\", fontsize=20)\n    plt.title(f\"{sample_text}\", fontsize=15)\n    plt.axis(\"off\")\n    for k, name in enumerate(sample_names):\n        image = cv2.imread(train_base + \"/\" + name)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        fig.add_subplot(2, 3, k+1)\n        plt.imshow(image)\n        plt.axis(\"off\")\n    \n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Example 1\nsample_group = groups_df[\"group\"][3]\nget_group_info(group_name=sample_group)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Example 2\nsample_group = groups_df[\"group\"][99]\nget_group_info(group_name=sample_group)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Example 3\nsample_group = groups_df[\"group\"][300]\nget_group_info(group_name=sample_group)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.3 Image Augmentation\n\nAnother aspect I wanted to explore was the different kinds of augmentation that might be performed on the images, so that the model can better pick up unique patterns.\n\n> **📌 Note**: From my research, the best performing augmentations for this type of problem were flips (vertical flip, horizontal flip, etc.), crops (center crop, random crop, etc.) and rotations, as they \"display\" the product in different positions without changing its color or texture attributes.\n\nBelow you can see an example of an image and 11 different applied augmentations.\n\n*You can find the [albumentations documentation here](https://vfdev-5-albumentations.readthedocs.io/en/docs_pytorch_fix/api/augmentations.html).*"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def display_augmentations(path):\n    '''Displays different types of augmentations on a chosen image.\n    path: the direct path to the desired image.'''\n    \n    # Read in original image\n    original = cv2.imread(path)\n    original = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)\n\n    # Transformations\n    transform_rgb = alb.Compose([ RGBShift(g_shift_limit=50, always_apply=True) ])\n    transform_hsv = alb.Compose([ HueSaturationValue(hue_shift_limit=100, sat_shift_limit=60, always_apply=True) ])\n    transform_hf = alb.Compose([ HorizontalFlip(always_apply=True) ])\n    transform_clahe = alb.Compose([ CLAHE(clip_limit=10.0, always_apply=True) ])\n    transform_rc = alb.Compose([ RandomCrop(height=300, width=300, always_apply=True) ])\n    transform_rg = alb.Compose([ RandomGamma(gamma_limit=(200, 400), always_apply=True) ])\n    transform_rot = alb.Compose([ Rotate(limit=90, always_apply=True) ])\n    transform_cc = alb.Compose([ CenterCrop(height=450, width=450, always_apply=True) ])\n    transform_mb = alb.Compose([ MedianBlur(blur_limit=103, always_apply=True) ])\n    transform_vf = alb.Compose([ VerticalFlip(always_apply=True) ])\n    transform_ii = alb.Compose([ InvertImg(always_apply=True) ])\n\n    # Apply transformations\n    transformed_rgb = transform_rgb(image=original)[\"image\"]\n    transformed_hsv = transform_hsv(image=original)[\"image\"]\n    transformed_hf = transform_hf(image=original)[\"image\"]\n    transformed_clahe = transform_clahe(image=original)[\"image\"]\n    transformed_rc = transform_rc(image=original)[\"image\"]\n    transformed_rg = transform_rg(image=original)[\"image\"]\n    transformed_rot = transform_rot(image=original)[\"image\"]\n    transformed_cc = transform_cc(image=original)[\"image\"]\n    transformed_mb = transform_mb(image=original)[\"image\"]\n    transformed_vf = transform_vf(image=original)[\"image\"]\n    transformed_ii = transform_ii(image=original)[\"image\"]\n\n    all_transformations = [original, transformed_rgb, transformed_hsv, transformed_hf, \n                           transformed_clahe, transformed_rc, transformed_rg, transformed_rot, \n                           transformed_cc, transformed_mb, transformed_vf, transformed_ii]\n    all_names = [\"Original\", \"RGBShift\", \"HueStaurationValue\", \"HorizontalFlip\", \"CLAHE\",\n                 \"RandomCrop\", \"RandomGamma\", \"Rotate\", \"CenterCrop\", \"MedianBlur\",\n                 \"VerticalFlip\", \"InvertImg\"]\n    \n    # Plot\n    fig = plt.figure(figsize=(20, 14))\n    plt.suptitle(f\"Image Augmentations\", fontsize=20)\n    for k, image in enumerate(all_transformations):\n        fig.add_subplot(3, 4, k+1)\n        plt.title(all_names[k])\n        plt.imshow(image)\n        plt.axis(\"off\")\n        \n        wandb.log({\"Image Augmentations\": plt})\n\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"example_path = \"../input/shopee-product-matching/test_images/0006c8e5462ae52167402bac1c2e916e.jpg\"\ndisplay_augmentations(path = example_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ~ END of EXPERIMENT ~\nwandb.finish()\n# ~~~~~~~~~~~~~~~~~~~~~","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Data - Texts\n\n## 3.1 Text Preprocessing - step by step\nBefore analyzing the text, we'll have to prepare it a little bit, so the insights we'll gain afterwards will be as accurate as possible.\n\n> **❗ Disclaimer**: I chose NOT to remove numbers - as they might be very important when it comes to how many ml or how many pieces are in the package of a product. I am thinking numbers might actually give a huge insight for our prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Original\noriginal = train_df[\"title\"][100]\nprint(color.BOLD + \"Before:\" + color.END, original, \"\\n\")\n\n# ~~~~~ Convert to lower case ~~~~~\nlower = original.lower()\nprint(color.BOLD + \"Lower case:\" + color.END, lower)\n\n# ~~~~~ Remove punctuation ~~~~~\n### !”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~]:\nwo_punct = lower.translate(str.maketrans('','',string.punctuation))\nprint(color.BOLD + \"Remove punctuation:\" + color.END, wo_punct)\n\n# ~~~~~ Remove whitespaces ~~~~~\nwo_whitespaces = wo_punct.strip()\nprint(color.BOLD + \"Remove whitespaces:\" + color.END, wo_whitespaces)\n\n# ~~~~~ Tokenize words ~~~~~\ntokenize = word_tokenize(wo_whitespaces)\nprint(color.BOLD + \"Tokenized:\" + color.END, tokenize)\n\n# ~~~~~ Remove stopwords ~~~~~\ntext_wo_sw = [word for word in tokenize if not word in stopwords.words()]\nprint(color.BOLD + \"Remove stopwords:\" + color.END, text_wo_sw)\n\n# ~~~~~ Lemmatization ~~~~~\nlemmatizer = WordNetLemmatizer()\nlemmatized_text = [lemmatizer.lemmatize(word) for word in text_wo_sw]\nprint(color.BOLD + \"Lemmatization:\" + color.END, lemmatized_text)\n\n# ~~~~~ Part of speech tagging ~~~~~\npos_text = TextBlob(' '.join(lemmatized_text))\nprint(color.BOLD + \"POS:\" + color.END, pos_text.tags)\n\n# # ~~~~~ Named entity recognition ~~~~~\n# ner_text = ne_chunk(pos_tag(lemmatized_text))\n# print(\"NER:\", ner_text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 💾 Text Preprocessing Function\n> **📌 Note**: This function takes ~ 30 mins in the Kaggle environment. But [Maxim Vlah](https://www.kaggle.com/maximvlah) came in the comments with the amazing library called `pandarallel`, which enables parallelisation when applying `.apply()` function in `pandas`. You can check the GitHub repo [here](https://github.com/nalepae/pandarallel). Now the same function takes about 8-9 minutes.\n\n**Check out preprocessing methodology below ⬇️**"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def preprocess_title(title):\n    '''Text Preprocessing Performance.\n    title: the string that needs prepped.'''\n    \n    # Lower Case\n    title = title.lower()\n    # Remove Punctuation\n    title = title.translate(str.maketrans('','',string.punctuation))\n    # Remove whitespaces\n    title = title.strip()\n    # Tokenize\n    tokens_title = word_tokenize(title)\n    # Remove stopwords\n    tokens_title = [word for word in tokens_title if not word in stopwords.words()]\n    # Lemmatization\n    lemmatizer = WordNetLemmatizer()\n    lemm_text = [lemmatizer.lemmatize(word) for word in tokens_title]\n    prepped_title = ' '.join(lemm_text)\n\n    return prepped_title\n    \n\ndef get_POS(prepped_title):\n    '''Gets Part of Speech.\n    prepped_text: the already prepped text'''\n    \n    # Part of speech tagging\n    pos_text = TextBlob(prepped_title)\n    pos_text = ' '.join([j for (i, j) in pos_text.tags])\n\n    return pos_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Process preprocessed title\ntrain_df[\"title_prep\"] = train_df[\"title\"].\\\n                          parallel_apply(lambda x: preprocess_title(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add part of speech\ntrain_df[\"pos\"] = train_df[\"title_prep\"].\\\n                          parallel_apply(lambda x: get_POS(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read in prepped data\ntrain_df_prep = pd.read_csv(\"../input/shopee-preprocessed-data/train_title_prepped.csv\")\ntrain_df_prep[\"label_group\"] = train_df_prep[\"label_group\"].astype(str)\n\n# Save also as artifact\nrun = wandb.init(project='shopee-kaggle', name='df_title_prepped')\nartifact = wandb.Artifact(name='preprocessed', \n                          type='dataset')\n\nartifact.add_file(\"../input/shopee-preprocessed-data/train_title_prepped.csv\")\n\nwandb.log_artifact(artifact)\nwandb.finish()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.2 Text Features Extraction\n\nAnother method was to extract features from the `title` column, in an attempt to feed into the final model more useful information. You can check out [this article](https://towardsdatascience.com/textfeatures-library-for-extracting-basic-features-from-text-data-f98ba90e3932) for more about the textfeatures library.\n\nThe extractions were:\n* `word_count` : counts how many words are in a sentence\n* `char_count` : counts how many characters are in a sentence\n* `avg_word_length` : counts what's the average word length in a sentence\n* `stopwords_count` : counts how many stopwords are in a sentence\n* `numerics_count` : counts how many numbers are in a sentence"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"def extract_title_features(df_prep):\n    '''Extracts features from the unprocessed title column.'''\n    \n    # Extract Features\n    df_prep = textfeatures.word_count(df_prep, \"title\", \"word_count\")\n    df_prep = textfeatures.char_count(df_prep, \"title\", \"char_count\")\n    df_prep = textfeatures.avg_word_length(df_prep, \"title\", \"avg_word_length\")\n    df_prep = textfeatures.stopwords_count(df_prep, \"title\", \"stopwords_count\")\n    df_prep = textfeatures.numerics_count(df_prep, \"title\", \"numerics_count\")\n    \n    return df_prep","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df_prep = extract_title_features(df_prep=train_df_prep)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Explore the Features \n\nWithin out `title` variable, the texts are usually ~10 words long, with ~ 50 characters and containing 1 to 2 numerics."},{"metadata":{"trusted":true},"cell_type":"code","source":"title_features = ['word_count', 'char_count', 'avg_word_length',\n                  'stopwords_count', 'numerics_count']\n\n# Plot\nfig, axs = plt.subplots(nrows=2, ncols=3, figsize=(16, 10), squeeze=False)\nplt.suptitle(f\"Title : Features Extracted\", fontsize=20)\nrows = [0, 0, 0, 1, 1, 1]\ncols = [0, 1, 2, 0, 1, 2]\naxs[1,2].set_visible(False)\n\nfor k, (name, i, j) in enumerate(zip(title_features, rows, cols)):\n    sns.kdeplot(train_df_prep[name], ax=axs[i, j], color=my_colors[k],\n                shade=\"fill\", lw=3)\n    axs[i, j].set_title(name, fontsize=15)\n    axs[i, j].set_xlabel(\"\", fontsize=16)\n    axs[i, j].set_ylabel(\"\", fontsize=16)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.3 Text Exploration\n\nNow let's look at the newly created `title_prep`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Another W&B Experiment\nrun = wandb.init(project=\"shopee-kaggle\", name=\"text-discover\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"# Get bag of words from the title\ntitle_prep = train_df_prep[\"title_prep\"].values.astype('U')\nvectorizer = TfidfVectorizer()\nvectorizer.fit_transform(title_prep)\n\nbag_of_words = pd.DataFrame({'word' : vectorizer.vocabulary_.keys(),\n                             'freq' : vectorizer.vocabulary_.values()})\n\n# Plot\nplt.figure(figsize=(16, 14))\nplot = sns.barplot(data=bag_of_words.head(25).sort_values('freq', ascending=False),\n                   y=\"word\", x=\"freq\", color=my_colors[4])\nshow_values_on_bars(plot, h_v=\"h\", space=0.4)\nplt.title(\"Example of words & frequencies\", fontsize=20)\nplt.yticks(fontsize=15)\nplt.xticks([],)\nplt.xlabel(\"Frequency of apparition\", fontsize=16)\nplt.ylabel(\"\", fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Create Custom Plot for W&B ⬇"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"# --- Make a custom plot to save into W&B ---\n\n# Prepare data\nlabels = bag_of_words.head(25).sort_values('freq', ascending=False)[\"word\"]\nvalues = bag_of_words.head(25).sort_values('freq', ascending=False)[\"freq\"]\n\ndata = [[label, val] for (label, val) in zip(labels, values)]\n\n# Create Table & .log() the plot\ntable = wandb.Table(data=data, columns = [\"Word\", \"Frequency\"])\nwandb.log({\"text_chart\" : wandb.plot.bar(table, \"Word\", \"Frequency\",\n                                          title=\"Example of words & frequencies\")})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's also explore the `pos` (Part of Speech) column**:\n* **WRB** - wh- adverb (how)\n* **WP** - wh- pronoun (who)\n* **VBZ** - verb, present tense with 3rd person singular (bases)\n* **VBP** - verb, present tense not 3rd person singular (wrap)\n* **RP** - particle (about)\n\nYou can find [full list here](https://www.guru99.com/pos-tagging-chunking-nltk.html#:~:text=POS%20Tagging%20in%20NLTK%20is,each%20word%20of%20the%20sentence.)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get bag of words from `pos` column\npos_prep = train_df_prep[\"pos\"].values.astype('U')\nvectorizer = CountVectorizer()\nvectorizer.fit_transform(pos_prep)\n\nbag_of_words = pd.DataFrame({'pos' : vectorizer.vocabulary_.keys(),\n                             'freq' : vectorizer.vocabulary_.values()})\n\n# Plot\nplt.figure(figsize=(16, 10))\nplot = sns.barplot(data=bag_of_words.head(25).sort_values('freq', ascending=False),\n                   y=\"pos\", x=\"freq\", color=my_colors[0])\nshow_values_on_bars(plot, h_v=\"h\", space=0)\nplt.title(\"Part of Speech: Frequencies\", fontsize=20)\nplt.yticks(fontsize=15)\nplt.xticks([],)\nplt.xlabel(\"Frequency of apparition\", fontsize=16)\nplt.ylabel(\"\", fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Create Custom Plot for W&B ⬇"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# --- Make a custom plot to save into W&B ---\n\n# Prepare data\nlabels = bag_of_words.head(25).sort_values('freq', ascending=False)[\"pos\"]\nvalues = bag_of_words.head(25).sort_values('freq', ascending=False)[\"freq\"]\n\ndata = [[label, val] for (label, val) in zip(labels, values)]\n\n# Create Table & .log() the plot\ntable = wandb.Table(data=data, columns = [\"POS\", \"Frequency\"])\nwandb.log({\"pos_chart\" : wandb.plot.bar(table, \"POS\", \"Frequency\",\n                                          title=\"Part of Speech: Frequencies\")})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ☁ Wordcloud"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get all titles\ntext_for_wc = \" \".join(title for title in train_df_prep[\"title\"])\n\n# Wordcloud\nfont_path = \"../input/shopee-preprocessed-data/ACETONE.otf\"\nstopwords_wc = set(stopwords_wc)\n# stopwords_wc.update([\"yes\"])\n\nwordcloud = WordCloud(stopwords=stopwords_wc, font_path=font_path,\n                      max_words=4000,\n                      max_font_size=200, random_state=42,\n                      width=1600, height=800,\n                      colormap = \"spring\")\nwordcloud.generate(text_for_wc)\n\n# Plot\nplt.figure(figsize = (16, 8))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ~ END of EXPERIMENT ~\nwandb.finish()\n# ~~~~~~~~~~~~~~~~~~~~~","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.4 Create Text Embeddings\n\nLet's append now the `TF-IDF` & `CountVectorizer` embeddings explored above to our training dataframe.\n\n> **📌 Note**: We'll end up with 26,705 columns (instead of the 12 we are working with now, or 5 in the original training dataset)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_embeddings(df):\n    '''Gets the word embeddings for `title_prep` and `pos` columns.\n    df: dataframe which contains cleaned title & part of speech'''\n    \n    # Vectorizer functions don't support NAs, so we need to remove if any\n    df = df[df[\"title_prep\"].isna() != True].reset_index(drop=True)\n    \n    # `title` vectorizer\n    title_vectorizer = TfidfVectorizer()\n    title_matrix = title_vectorizer.fit_transform(df[\"title_prep\"]).toarray()\n    # Create dataframe\n    title_matrix_df = pd.DataFrame(title_matrix)\n    title_matrix_df.columns = [f\"title_{i}\" for i in range(0, title_matrix_df.shape[1])]\n    \n    # `pos` vectorizer\n    pos_vectorizer = CountVectorizer()\n    pos_matrix = pos_vectorizer.fit_transform(df[\"pos\"]).toarray()\n    # Create dataframe\n    pos_matrix_df = pd.DataFrame(pos_matrix)\n    pos_matrix_df.columns = [f\"pos_{i}\" for i in range(0, pos_matrix_df.shape[1])]\n    \n    # Concatenate all data together\n    final_df = pd.concat([df, title_matrix_df, pos_matrix_df], axis=1)\n    \n    return final_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get `title` and `pos` embeddings\ntrain_df_prep_final = get_embeddings(df=train_df_prep)\n\nprint(\"Train df shape - Before: {}\".format(train_df_prep.shape), '\\n' +\n      \"Train df shape - After: {}\".format(train_df_prep_final.shape))\n\ntrain_df_prep_final.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's also save it to W&B project\nrun = wandb.init(project='shopee-kaggle', name='df_title_pos_embeddings')\nartifact = wandb.Artifact(name='preprocessed_embeddings', \n                          type='dataset')\n\nartifact.add_file(\"../input/shopee-preprocessed-data/train_title_prepped_embeddings.parquet\")\n\nwandb.log_artifact(artifact)\nwandb.finish()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Final Preprocessing Function\n\nWe'll need the functions we created in this notebook to preprocess the `test` dataframe as well, before applying the ML model. Hence, is best to create a `preprocess_df` function that contains the necessary metadata process pipeline.\n\n> **📌 Remember**: We'll use an Unsupervised ML Technique to make our prediction for this competition. Hence, all methodologies we'll apply for the **CV** score we'll also need to use for the **submission** notebook. More on that in my [next notebook](https://www.kaggle.com/andradaolteanu/ii-shopee-model-training-with-pytorch-x-rapids)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_df(df):\n    '''Preprocessing pipeline.'''\n    \n    # Clean duplicates\n    df = clean_duplicates(df, train=False)\n    # Preprocess title + get POS\n    df[\"title_prep\"] = df[\"title\"].apply(lambda x: preprocess_title(x))\n    df[\"pos\"] = df[\"title_prep\"].apply(lambda x: get_POS(x))\n    # Extract title features\n    df = extract_title_features(df)\n    # Get embeddings from title and pos\n    df = get_embeddings(df)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test the preprocess_df function \ntest = pd.read_csv(\"../input/shopee-product-matching/test.csv\")\npreprocess_df(df=test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### - Training & Submission Notebook here: [🛒II. Shopee: Model Training with Pytorch x RAPIDS](https://www.kaggle.com/andradaolteanu/ii-shopee-model-training-with-pytorch-x-rapids) -\n\n<img src=\"https://i.imgur.com/cUQXtS7.png\">\n\n# Specs on how I prepped & explored ⌨️🎨¶\n### (on my local machine)\n* Z8 G4 Workstation 🖥\n* 2 CPUs & 96GB Memory 💾\n* NVIDIA Quadro RTX 8000 🎮\n* RAPIDS version 0.17 🏃🏾‍♀️"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}