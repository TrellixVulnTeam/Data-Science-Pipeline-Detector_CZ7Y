{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np \nimport pandas as pd\nimport cv2,math,gc\nimport timm\nfrom tqdm import tqdm \nimport albumentations as A \n\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\nfrom torch.nn import Parameter\n\n!pip install \"../input/efficient-net/dist/efficientnet_pytorch-0.7.0.tar\"\nfrom efficientnet_pytorch import EfficientNet\n\n!pip install \"../input/faissgpuwheel/faiss_gpu-1.7.0-cp37-cp37m-manylinux2014_x86_64.whl\"\nimport faiss\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nimport cudf, cuml, cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors\n\nimport warnings\nwarnings.simplefilter('ignore')\n\ntorch.backends.cudnn.benchmark = True","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class cfg:\n    img_size = (380,380)\n    feavec_num1 = 512\n    feavec_num2 = 1280\n    fea_norm = 64\n    margin = 0.35\n    batch = 50\n    wpath = [\"../input/shopee-weight/w_eff6_s380_cl8812_fold1_v2.pt\",\n             \"../input/shopee-weight/w_effb3_s380_cl8811_fold2_0.80.pt\",\n             \"../input/shopee-weight/w_effb5_s380_cl8811_fold3.pt\",\n             \"../input/shopee-weight/w_effb4_s380_cl8811_fold4.pt\",\n             \"../input/shopee-weight/w_effb3_s380_cl8811_fold5_m0.35.pt\"]\n    mname = ['efficientnet-b6','efficientnet-b3','efficientnet-b5','efficientnet-b4','efficientnet-b3']\n    clsize = [8812,8811,8811,8811,8811]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#印度小哥\nclass CFG:\n    \n    img_size = 512\n    batch_size = 12\n    seed = 2020\n    \n    device = 'cuda'\n    classes = 11014\n    \n    model_name = 'eca_nfnet_l0'\n    model_path = '../input/shopee-pytorch-models/arcface_512x512_nfnet_l0 (mish).pt'\n    \n    scale = 30 \n    margin = 0.5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"COMPUTE_CV = False\n\n#make target clustering\nif COMPUTE_CV:\n    df = pd.read_csv(\"../input/shopee-product-matching/train.csv\")\n    tmp = df.groupby('label_group').posting_id.agg('unique').to_dict()\n    df['target'] = df.label_group.map(tmp)\n    df['target'] = df['target'].apply(lambda x: ' '.join(x))\n    df_cu = cudf.DataFrame(df)\nelse:\n    df = pd.read_csv(\"../input/shopee-product-matching/test.csv\")\n    df_cu = cudf.DataFrame(df)\n    #image_paths = '../input/shopee-product-matching/test_images/' + df['image']\n    if len(df)==3:\n        cfg.batch = 3\n    \nprint('df shape is', df.shape )\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#印度小哥\ndef read_dataset():\n    df1 = pd.read_csv('../input/shopee-product-matching/test.csv')\n    df_cu1 = cudf.DataFrame(df)\n    image_paths = '../input/shopee-product-matching/test_images/' + df['image']\n    return df1, df_cu1, image_paths\n\nimage_paths = '../input/shopee-product-matching/test_images/' + df['image']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#印度小哥\nimport random\nimport os \ndef seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_torch(CFG.seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#印度小哥image_predictions\ndef get_image_predictions_L0(df, embeddings,threshold = 0.0):\n    \n    if len(df) > 3:\n        KNN = 50\n    else : \n        KNN = 3\n    \n    model = NearestNeighbors(n_neighbors = KNN, metric = 'cosine')\n    model.fit(embeddings)\n    distances, indices = model.kneighbors(embeddings)\n    \n    predictions = []\n    for k in tqdm(range(embeddings.shape[0])):\n        idx = np.where(distances[k,] < threshold)[0]\n        ids = indices[k,idx]\n        posting_ids = df['posting_id'].iloc[ids].values\n        predictions.append(posting_ids)\n        \n    del model, distances, indices\n    gc.collect()\n    return predictions\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#印度小哥 \ndef get_test_transforms():\n\n    return A.Compose(\n        [\n            A.Resize(CFG.img_size,CFG.img_size,always_apply=True),\n            A.Normalize(),\n        ToTensorV2(p=1.0)\n        ]\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#印度小哥 dataset image读取\nimage_paths = '../input/shopee-product-matching/test_images/' + df['image']\n\nclass ShopeeDataset(Dataset):\n    def __init__(self, image_paths, transforms=None):\n\n        self.image_paths = image_paths\n        self.augmentations = transforms\n\n    def __len__(self):\n        return self.image_paths.shape[0]\n\n    def __getitem__(self, index):\n        image_path = self.image_paths[index]\n        \n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.augmentations:\n            augmented = self.augmentations(image=image)\n            image = augmented['image']       \n    \n        return image,torch.tensor(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Use Image Embeddings","metadata":{}},{"cell_type":"code","source":"class ArcMarginProduct(nn.Module):\n    def __init__(self, in_features, out_features, s=30.0, m=0.30, easy_margin=False):\n        super(ArcMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.s = s\n        self.m = m\n        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(m)\n        self.sin_m = math.sin(m)\n        self.th = math.cos(math.pi - m)\n        self.mm = math.sin(math.pi - m) * m\n\n    def forward(self, input, label):\n        # --------------------------- cos(theta) & phi(theta) ---------------------------\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt((1.0 - torch.pow(cosine, 2)).clamp(0, 1))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n        # --------------------------- convert label to one-hot ---------------------------\n        one_hot = torch.zeros(cosine.size(), device=device)\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)  # you can use torch.where if your torch.__version__ is 0.4\n        output *= self.s\n        return output\n\n\nclass Model(nn.Module):\n    def __init__(self,name,clustersize,feavec=512):\n        super(Model, self).__init__()\n        self.eff = EfficientNet.from_name(name)\n        self.out = nn.Linear(1000,feavec)\n        self.margin = ArcMarginProduct(in_features=feavec, \n                                       out_features = clustersize, \n                                       s=cfg.fea_norm, \n                                       m=cfg.margin)      \n\n    def forward(self, x, labels=None):\n        x = self.eff(x)\n        x = self.out(x)\n        if labels is not None:\n            return self.margin(x,labels)\n        return F.normalize(x,dim=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ArcMarginProduct_L0(nn.Module):\n    def __init__(self, in_features, out_features, scale=30.0, margin=0.50, easy_margin=False, ls_eps=0.0):\n        super(ArcMarginProduct_L0, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.scale = scale\n        self.margin = margin\n        self.ls_eps = ls_eps  # label smoothing\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(margin)\n        self.sin_m = math.sin(margin)\n        self.th = math.cos(math.pi - margin)\n        self.mm = math.sin(math.pi - margin) * margin\n\n    def forward(self, input, label):\n        # --------------------------- cos(theta) & phi(theta) ---------------------------\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n        # --------------------------- convert label to one-hot ---------------------------\n        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n        one_hot = torch.zeros(cosine.size(), device='cuda')\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.scale\n\n        return output\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#印度小哥的model\nclass ShopeeModel(nn.Module):\n\n    def __init__(\n        self,\n        n_classes = CFG.classes,\n        model_name = None,\n        fc_dim = 512,\n        margin = CFG.margin,\n        scale = CFG.scale,\n        use_fc = True,\n        pretrained = False):\n\n\n        super(ShopeeModel,self).__init__()\n        print('Building Model Backbone for {} model'.format(model_name))\n\n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n\n        if model_name == 'resnext50_32x4d':\n            final_in_features = self.backbone.fc.in_features\n            self.backbone.fc = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n\n        elif 'efficientnet' in model_name:\n            final_in_features = self.backbone.classifier.in_features\n            self.backbone.classifier = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n        \n        elif model_name == 'eca_nfnet_l0':\n            final_in_features = self.backbone.head.fc.in_features\n            self.backbone.head.fc = nn.Identity()\n            self.backbone.head.global_pool = nn.Identity()\n\n        self.pooling =  nn.AdaptiveAvgPool2d(1)\n\n        self.use_fc = use_fc\n\n        self.dropout = nn.Dropout(p=0.0)\n        self.fc = nn.Linear(final_in_features, fc_dim)\n        self.bn = nn.BatchNorm1d(fc_dim)\n        self._init_params()\n        final_in_features = fc_dim\n\n        self.final = ArcMarginProduct_L0(\n            final_in_features,\n            n_classes,\n            scale = scale,\n            margin = margin,\n            easy_margin = False,\n            ls_eps = 0.0\n        )\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, image, label):\n        feature = self.extract_feat(image)\n        #logits = self.final(feature,label)\n        return feature\n\n    def extract_feat(self, x):\n        batch_size = x.shape[0]\n        x = self.backbone(x)\n        x = self.pooling(x).view(batch_size, -1)\n\n        if self.use_fc:\n            x = self.dropout(x)\n            x = self.fc(x)\n            x = self.bn(x)\n        return x\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#印度小哥的mish_func\nclass Mish_func(torch.autograd.Function):\n    \n    \"\"\"from: https://github.com/tyunist/memory_efficient_mish_swish/blob/master/mish.py\"\"\"\n    \n    @staticmethod\n    def forward(ctx, i):\n        result = i * torch.tanh(F.softplus(i))\n        ctx.save_for_backward(i)\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        i = ctx.saved_variables[0]\n  \n        v = 1. + i.exp()\n        h = v.log() \n        grad_gh = 1./h.cosh().pow_(2) \n\n        # Note that grad_hv * grad_vx = sigmoid(x)\n        #grad_hv = 1./v  \n        #grad_vx = i.exp()\n        \n        grad_hx = i.sigmoid()\n\n        grad_gx = grad_gh *  grad_hx #grad_hv * grad_vx \n        \n        grad_f =  torch.tanh(F.softplus(i)) + i * grad_gx \n        \n        return grad_output * grad_f \n\n\nclass Mish(nn.Module):\n    def __init__(self, **kwargs):\n        super().__init__()\n        pass\n    def forward(self, input_tensor):\n        return Mish_func.apply(input_tensor)\n\n\ndef replace_activations(model, existing_layer, new_layer):\n    \n    \"\"\"A function for replacing existing activation layers\"\"\"\n    \n    for name, module in reversed(model._modules.items()):\n        if len(list(module.children())) > 0:\n            model._modules[name] = replace_activations(module, existing_layer, new_layer)\n\n        if type(module) == existing_layer:\n            layer_old = module\n            layer_new = new_layer\n            model._modules[name] = layer_new\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#印度小哥LO embedding\ndef get_image_embeddings(image_paths, model_name = CFG.model_name):\n    embeds = []\n    \n    model = ShopeeModel(model_name = model_name)\n    model.eval()\n    \n    if model_name == 'eca_nfnet_l0':\n        model = replace_activations(model, torch.nn.SiLU, Mish())\n\n    model.load_state_dict(torch.load(CFG.model_path))\n    model = model.to(CFG.device)\n    \n    image_paths = '../input/shopee-product-matching/test_images/' + df['image']\n    image_dataset = ShopeeDataset(image_paths=image_paths,transforms=get_test_transforms())\n    image_loader = torch.utils.data.DataLoader(\n        image_dataset,\n        batch_size=CFG.batch_size,\n        pin_memory=True,\n        drop_last=False,\n        num_workers=4\n    )\n    \n    \n    with torch.no_grad():\n        for img,label in tqdm(image_loader): \n            img = img.cuda()\n            label = label.cuda()\n            feat = model(img,label)\n            image_embeddings = feat.detach().cpu().numpy()\n            embeds.append(image_embeddings)\n    \n    \n    del model\n    image_embeddings_L0 = np.concatenate(embeds)\n    print(f'Our image_embeddings_L0 shape is {image_embeddings_L0.shape}')\n    del embeds\n    gc.collect()\n    return image_embeddings_L0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#印度小哥LO embedding\nimage_embeddings_L0 = get_image_embeddings(image_paths.values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#印度小哥LO prediction\nimage_predictions_L0 = get_image_predictions_L0(df, image_embeddings_L0, threshold = 0.31)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#印度小哥LO prediction 结束\ndf['image_predictions_L0'] = image_predictions_L0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1 = Model(name=cfg.mname[0],clustersize=cfg.clsize[0]).to(device).half()\nmodel1.load_state_dict(torch.load(cfg.wpath[0], map_location=device))\n\nmodel2 = Model(name=cfg.mname[1],clustersize=cfg.clsize[1]).to(device).half()\nmodel2.load_state_dict(torch.load(cfg.wpath[1], map_location=device))\n\nmodel3 = Model(name=cfg.mname[2],clustersize=cfg.clsize[2]).to(device).half()\nmodel3.load_state_dict(torch.load(cfg.wpath[2], map_location=device))\n\nmodel4 = Model(name=cfg.mname[3],clustersize=cfg.clsize[3]).to(device).half()\nmodel4.load_state_dict(torch.load(cfg.wpath[3], map_location=device))\n\nmodel5 = Model(name=cfg.mname[4],clustersize=cfg.clsize[4]).to(device).half()\nmodel5.load_state_dict(torch.load(cfg.wpath[4], map_location=device))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make image Datasets\ndef load_image(file_name):\n    if COMPUTE_CV:\n        file_path = f'/kaggle/input/shopee-product-matching/train_images/{file_name}'\n    else:\n        file_path = f'/kaggle/input/shopee-product-matching/test_images/{file_name}'\n\n    img = cv2.imread(file_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, cfg.img_size)\n    tensor_img = torch.tensor(img)\n    tensor_img = tensor_img.permute(( 2, 0, 1)).float()/255.0\n    return tensor_img\n\nclass valDataset(Dataset):\n    def __init__(self, df):\n        self.img = df.image.values\n        \n    def __len__(self):\n        return len(self.img)\n\n    def __getitem__(self, idx):\n        img = self.img[idx]\n        img = load_image(img)\n        return img","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def image_embeddings(df):\n    dataset = valDataset(df)\n    loader = DataLoader(dataset,\n                        batch_size=cfg.batch,\n                        shuffle=False,\n                        num_workers=2,\n                        pin_memory=True,\n                        drop_last=False)\n    \n    model1.eval()\n    model2.eval()\n    model3.eval()\n    model4.eval()\n    model5.eval()\n    print('start collection')\n    feavec = 512\n    embedded1 = np.empty((0,feavec),dtype='float32')\n    embedded2 = np.empty((0,feavec),dtype='float32')\n    embedded3 = np.empty((0,feavec),dtype='float32')\n    embedded4 = np.empty((0,feavec),dtype='float32')\n    embedded5 = np.empty((0,feavec),dtype='float32')\n    with torch.no_grad():\n        for idx,images in enumerate(loader):\n            images = images.to(device,non_blocking=True).half()\n            outputs = model1(images)\n            embedded1 = np.append(embedded1, outputs.cpu().detach().numpy(),axis=0)\n            outputs = model2(images)\n            embedded2 = np.append(embedded2, outputs.cpu().detach().numpy(),axis=0)\n            outputs = model3(images)\n            embedded3 = np.append(embedded3, outputs.cpu().detach().numpy(),axis=0)\n            outputs = model4(images)\n            embedded4 = np.append(embedded4, outputs.cpu().detach().numpy(),axis=0)\n            outputs = model5(images)\n            embedded5 = np.append(embedded5, outputs.cpu().detach().numpy(),axis=0)\n\n            if idx%100==0:\n                print(idx,len(loader)) \n                print(embedded1.shape)\n                print(embedded2.shape)\n                print(embedded3.shape)\n                print(embedded4.shape)\n                print(embedded5.shape)\n    #del model1,model2,model3,model4\n    return embedded1,embedded2,embedded3,embedded4,embedded5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def f1_score(y_true, y_pred):\n    y_true = y_true.apply(lambda x: set(x.split()))\n    y_pred = y_pred.apply(lambda x: set(x.split()))\n    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n    len_y_pred = y_pred.apply(lambda x: len(x)).values\n    len_y_true = y_true.apply(lambda x: len(x)).values\n    f1 = 2 * intersection / (len_y_pred + len_y_true)\n    return f1\n\ndef predict_img(df,embeddings,topk=50,threshold=0.63):\n    N,D = embeddings.shape\n    cpu_index = faiss.IndexFlatL2(D)\n    gpu_index = faiss.index_cpu_to_all_gpus(cpu_index)\n    gpu_index.add(embeddings)\n    cluster_distance,cluster_index = gpu_index.search(x=embeddings, k=topk)\n    \n    df['pred_images'] = ''\n    pred = []\n    for k in range(embeddings.shape[0]):\n        idx = np.where(cluster_distance[k,] < threshold)[0]\n        ids = cluster_index[k,idx]\n        #posting_ids = ' '.join(df['posting_id'].iloc[ids].values)\n        posting_ids = df['posting_id'].iloc[ids].values\n        pred.append(posting_ids)\n    df['pred_images'] = pred\n    if COMPUTE_CV:\n        df['pred_imgonly'] = df.pred_images.apply(lambda x: ' '.join(x))\n        df['f1_img'] = f1_score(df['target'], df['pred_imgonly'])\n        score = df['f1_img'].mean()\n        print(f'Our f1 score for threshold {threshold} is {score}')\n    return df\n\ndef predict_text(df,embeddings,topk=50,threshold=0.63):\n    N,D = embeddings.shape\n    cpu_index = faiss.IndexFlatL2(D)\n    gpu_index = faiss.index_cpu_to_all_gpus(cpu_index)\n    gpu_index.add(embeddings)\n    cluster_distance,cluster_index = gpu_index.search(x=embeddings, k=topk)\n    \n    df['pred_text'] = ''\n    pred = []\n    for k in range(embeddings.shape[0]):\n        idx = np.where(cluster_distance[k,] < threshold)[0]\n        ids = cluster_index[k,idx]\n        #posting_ids = ' '.join(df['posting_id'].iloc[ids].values)\n        posting_ids = df['posting_id'].iloc[ids].values\n        pred.append(posting_ids)\n    df['pred_text'] = pred\n    if COMPUTE_CV:\n        df['pred_textonly'] = df.pred_images.apply(lambda x: ' '.join(x))\n        df['f1_text'] = f1_score(df['target'], df['pred_textonly'])\n        score = df['f1_text'].mean()\n        print(f'Our f1 score for threshold {threshold} is {score}')\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Use Text Embeddings","metadata":{}},{"cell_type":"code","source":"def get_text_predictions(df, max_features = 25000,threshold=0.7):\n    from cuml.feature_extraction.text import TfidfVectorizer\n    model = TfidfVectorizer(stop_words = 'english', binary = True, max_features = max_features)\n    text_embeddings = model.fit_transform(df_cu.title).toarray()\n    #print(text_embeddings)\n    preds = []\n    CHUNK = 1024*4\n    \n    print('Finding similar titles...')\n    CTS = len(df)//CHUNK\n    if len(df)%CHUNK!=0: CTS += 1\n    for j in range( CTS ):\n\n        a = j*CHUNK\n        b = (j+1)*CHUNK\n        b = min(b,len(df))\n        print('chunk',a,'to',b)\n\n        # COSINE SIMILARITY DISTANCE\n        cts = cupy.matmul( text_embeddings, text_embeddings[a:b].T).T\n\n        for k in range(b-a):\n            IDX = cupy.where(cts[k,]>threshold)[0]\n            o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n            preds.append(o)\n    df['pred_text'] = preds\n    del model,text_embeddings\n    gc.collect()\n    if COMPUTE_CV:\n        df['pred_textonly'] = df.pred_text.apply(lambda x: ' '.join(x))\n        df['f1_text'] = f1_score(df['target'], df['pred_textonly'])\n        score = df['f1_text'].mean()\n        print(f'Our f1 score for threshold {threshold} is {score}')\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class textvalDataset(Dataset):\n    def __init__(self, textlist):\n        self.text = textlist\n        \n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, idx):\n        text = torch.tensor(self.text[idx])\n        text = text.float()\n        return text\n\nclass Model(nn.Module):\n    def __init__(self,clustersize,feavec=512):\n        super(Model, self).__init__()\n        self.linear1 = nn.Linear(24939,4000)\n        self.linear2 = nn.Linear(4000,feavec)\n        self.dropout = nn.Dropout(p=0.5)\n        self.relu = nn.ReLU()\n        self.margin = ArcMarginProduct(in_features=feavec, \n                                       out_features = clustersize, \n                                       s=64, \n                                       m=0.7)      \n\n    def forward(self, x, labels=None):\n        x = self.linear1(x)\n        #x = self.relu(x)\n        x = self.linear2(x)\n        #x = self.relu(x)\n        x = self.dropout(x)\n        if labels is not None:\n            return self.margin(x,labels)\n        return F.normalize(x,dim=1)\n    \n\ndef get_deeptext_predictions(df):\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    df_t = pd.read_csv(\"../input/shopee-product-matching/train.csv\")\n    models = TfidfVectorizer(stop_words = 'english', binary = True, max_features = 24939)\n    models.fit(pd.concat([df,df_t],axis=0).title)\n    text = models.transform(df.title).toarray()\n    batch = 100\n    if len(df)==3:\n        batch=3\n    test_dataset = textvalDataset(text)\n    test_loader = DataLoader(test_dataset,\n                            batch_size=batch,\n                            shuffle=False,\n                            num_workers=2,\n                            pin_memory=True)\n    model_t1 = Model(8811)\n    model_t2 = Model(8811)\n    model_t1 = model_t1.to(device)\n    model_t2 = model_t2.to(device)\n    model_t1.load_state_dict(torch.load('../input/shopee-weight/w_lin_e5_fold1.pt'))\n    model_t2.load_state_dict(torch.load('../input/shopee-weight/w_lin_e5_fold2.pt'))\n    #model.load_state_dict(torch.load('../input/shopee-weight-text/w_lin_e5_fold0.pt'))\n    model_t1.eval()\n    model_t2.eval()\n    print('start collection')\n    embedded1 = np.empty((0,512),dtype='float32')\n    embedded2 = np.empty((0,512),dtype='float32')\n    with torch.no_grad():\n        for idx,(images) in enumerate(test_loader):\n            images = images.to(device,non_blocking=True)\n            outputs = model_t1(images)\n            embedded1 = np.append(embedded1, outputs.cpu().detach().numpy(),axis=0)\n            outputs = model_t2(images)\n            embedded2 = np.append(embedded2, outputs.cpu().detach().numpy(),axis=0)\n\n            if idx%100==0:\n                print(idx,len(test_loader)) \n                print(embedded1.shape)\n                print(embedded2.shape)\n    print(embedded1.shape,embedded2.shape)\n    return embedded1,embedded2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_embeddings1, text_embeddings2 = get_deeptext_predictions(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Carry out image prediction","metadata":{}},{"cell_type":"code","source":"image_embeddings1,image_embeddings2,image_embeddings3, image_embeddings4, image_embeddings5 = image_embeddings(df)\n\n#image_embeddings2 = image_embeddings(df,cfg.wpath2,cfg.mname2,cfg.feavec_num1)\n#image_embeddings3 = image_embeddings(df,cfg.wpath3,cfg.mname3,cfg.feavec_num1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embed_reuse = False\nif embed_reuse:\n    image_embeddings1 = np.load(\"../input/shopeeinferoutput/fold1_512.npy\")\n    image_embeddings2 = np.load(\"../input/shopeeinferoutput/fold2_512.npy\")\n    image_embeddings3 = np.load(\"../input/shopeeinferoutput/fold3_512.npy\")\n    image_embeddings4 = np.load(\"../input/shopeeinferoutput/fold4_512.npy\")\n    image_embeddings5 = np.load(\"../input/shopeeinferoutput/fold5_512.npy\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#w = np.array([1.2,0.8,1.1,0.8,0.7])\n#image_embeddings = (w[0]*image_embeddings1+w[1]*image_embeddings2+w[2]*image_embeddings3+w[3]*image_embeddings4+w[4]*image_embeddings5)/w.sum()\nw = np.array([1.2,0.8,1.1,0.8,0.7])\nimage_embeddings = (w[0]*image_embeddings1+w[1]*image_embeddings2+w[2]*image_embeddings3+w[3]*image_embeddings4+w[4]*image_embeddings5)/w.sum()\nwt = np.array([1,1])\ntext_embeddings = (wt[0]*text_embeddings1+wt[1]*text_embeddings2)/wt.sum()\nimg_text_embeddings = (image_embeddings + 0.4*text_embeddings)/1.4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#img_text_embeddings_L0 = np.hstack([img_text_embeddings,image_embeddings_L0])\n#print('img_text_embeddings_L0:',img_text_embeddings_L0,img_text_embeddings_L0.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#image_embeddings = np.average([image_embeddings1, image_embeddings2, image_embeddings3, image_embeddings4, image_embeddings5], axis = 0)\n#w = np.array([0.80,1.05,1.4,1.05,1.15])\n#image_embeddings = np.average([image_embeddings2, image_embeddings3, image_embeddings4], axis = 0)\n#image_embeddings = np.average([image_embeddings3, image_embeddings4], axis = 0)\nif COMPUTE_CV:\n    df = predict_img(df,image_embeddings1,topk=50,threshold=0.88)\n    df = predict_img(df,image_embeddings2,topk=50,threshold=0.88)\n    df = predict_img(df,image_embeddings3,topk=50,threshold=0.88)\n    df = predict_img(df,image_embeddings4,topk=50,threshold=0.88)\n    df = predict_img(df,image_embeddings5,topk=50,threshold=0.88)\n    df = predict_img(df,image_embeddings,topk=50,threshold=0.196)\n    \n    df = predict_img(df,img_text_embeddings,topk=50,threshold=0.138)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if COMPUTE_CV:\n    np.save('fold1_512.npy', image_embeddings1)\n    np.save('fold2_512.npy', image_embeddings2)\n    np.save('fold3_512.npy', image_embeddings3)\n    np.save('fold4_512.npy', image_embeddings4)\n    np.save('fold5_512.npy', image_embeddings5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#2,3,4\n#df = predict_img(df,image_embeddings,topk=50,threshold=0.13)\n#df = predict_img(df,img_text_embeddings,topk=50,threshold=0.11)\n#3,4\n#df = predict_img(df,image_embeddings,topk=50,threshold=0.30)\n\n#df = predict_img(df,image_embeddings,topk=50,threshold=0.60)\ndf = predict_img(df,img_text_embeddings,topk=50,threshold=0.11) #for test img_text_embeddings_L0 is in ['pred_images'] for CV test img_text_embeddings_L0 is in ['pred_imgonly']\n#df = predict_img_l0(df,img_text_embedding,topk=50,threshold=22)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"theresholds=np.linspace(0.13,0.15,10)\nif COMPUTE_CV:\n    #for topk in [49,50,51,60]:\n    for threshold in theresholds:\n        df = predict_img(df,img_text_embeddings_L0,topk=50,threshold=threshold)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Carry out text predictions","metadata":{}},{"cell_type":"code","source":"df = get_text_predictions(df, max_features = 25000,threshold=0.75)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# combine_predictions","metadata":{}},{"cell_type":"code","source":"def combine_predictions(row):\n    x = np.concatenate([row['pred_images'], row['pred_text'],row['image_predictions_L0']])\n    return ' '.join( np.unique(x) )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['matches'] = df.apply(combine_predictions, axis=1)\n#df['matches'] = df['pred_images'].apply(lambda x: ' '.join(x))\nif COMPUTE_CV:\n    df['f1'] = f1_score(df['target'], df['matches'])\n    score = df['f1'].mean()\n    print(f'Final f1 score is {score}')\nelse:\n    with open('submission.csv', 'w') as outf:\n        print('posting_id,matches', file=outf)\n        for i,(idnum,match) in enumerate(zip(df['posting_id'],df['matches'])):\n            print(f'{idnum},{match}', file=outf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_t = pd.read_csv(\"submission.csv\")\nprint(df_t)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}