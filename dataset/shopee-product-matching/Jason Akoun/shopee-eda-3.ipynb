{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-03T23:27:24.524204Z","iopub.execute_input":"2021-06-03T23:27:24.524523Z","iopub.status.idle":"2021-06-03T23:27:24.528708Z","shell.execute_reply.started":"2021-06-03T23:27:24.524493Z","shell.execute_reply":"2021-06-03T23:27:24.527732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nfrom PIL import Image\nimport torch\nimport numpy as np\nfrom torch import nn\nfrom torchvision import transforms\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.notebook import tqdm\nimport torchvision.models as models\nfrom torch.utils.data import random_split\nfrom sklearn.neighbors import NearestNeighbors\n\nimport matplotlib.pyplot as plt\n\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk import pos_tag, ne_chunk\nfrom textblob import TextBlob\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom tqdm import tqdm\nfrom textblob import TextBlob\n","metadata":{"execution":{"iopub.status.busy":"2021-06-03T23:27:24.803369Z","iopub.execute_input":"2021-06-03T23:27:24.803644Z","iopub.status.idle":"2021-06-03T23:27:24.809993Z","shell.execute_reply.started":"2021-06-03T23:27:24.803618Z","shell.execute_reply":"2021-06-03T23:27:24.809195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Device available now:', device)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T23:27:25.158031Z","iopub.execute_input":"2021-06-03T23:27:25.158318Z","iopub.status.idle":"2021-06-03T23:27:25.163021Z","shell.execute_reply.started":"2021-06-03T23:27:25.15829Z","shell.execute_reply":"2021-06-03T23:27:25.162025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(\"../input/shopee-product-matching/train.csv\")\ndf_train.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T23:27:25.546715Z","iopub.execute_input":"2021-06-03T23:27:25.54891Z","iopub.status.idle":"2021-06-03T23:27:25.789771Z","shell.execute_reply.started":"2021-06-03T23:27:25.548866Z","shell.execute_reply":"2021-06-03T23:27:25.789002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#See different titles for same product\ndf = pd.DataFrame(index = [0, 1, 2], columns = [\"Title 1\", \"Title 2\"])\nlabels = [df_train[\"label\"].values[0], df_train[\"label\"].values[1], df_train[\"label\"].values[2]]\nn= len(labels)\nfor l in range(n):\n    indexes = df_train[df_train[\"label_group\"]==labels[l]].index.values \n    titles = df_train[\"title\"].iloc[indexes].values\n    df.iloc[l] = titles\n\ndf","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Print a pair of product\nindexes = df_train[df_train[\"label_group\"]==df_train[\"label\"].values[0]].index.values \nfig, ax = plt.subplots(1,2)\nimg_A = np.array(Image.open('../input/shopee-product-matching/train_images/' +  df_train[\"image\"].iloc[indexes[0]]))\nimg_B = np.array(Image.open('../input/shopee-product-matching/train_images/' +  df_train[\"image\"].iloc[indexes[1]]))\nax[0].imshow(img_A)\nax[1].imshow(img_B)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp = df_train.groupby('label_group').posting_id.agg('unique').to_dict()\ndf_train['target'] = df_train.label_group.map(tmp)\ndf_train.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T23:27:26.436822Z","iopub.execute_input":"2021-06-03T23:27:26.437352Z","iopub.status.idle":"2021-06-03T23:27:27.215507Z","shell.execute_reply.started":"2021-06-03T23:27:26.437283Z","shell.execute_reply":"2021-06-03T23:27:27.214382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d = dict(zip(df_train.label_group.unique(), range(len(df_train.label_group))))\ndf_train.label_group.replace(d, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T23:27:27.219468Z","iopub.execute_input":"2021-06-03T23:27:27.219816Z","iopub.status.idle":"2021-06-03T23:27:29.104745Z","shell.execute_reply.started":"2021-06-03T23:27:27.21978Z","shell.execute_reply":"2021-06-03T23:27:29.103928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T23:27:29.106601Z","iopub.execute_input":"2021-06-03T23:27:29.107032Z","iopub.status.idle":"2021-06-03T23:27:29.11908Z","shell.execute_reply.started":"2021-06-03T23:27:29.106996Z","shell.execute_reply":"2021-06-03T23:27:29.118114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating shopee dataset","metadata":{}},{"cell_type":"code","source":"class ShopeeDataset(Dataset):\n    \n    def __init__(self, csv, train=True):\n        self.csv = csv.reset_index()\n        self.train = train\n        self.transform = transforms.Compose([\n                                             transforms.Resize((256, 256)),\n                                             transforms.ToTensor()\n                                            ])\n        \n    def __len__(self):\n        return len(self.csv)\n    \n    def __getitem__(self, index):\n        if self.train:\n            image = Image.open('../input/shopee-product-matching/train_images/' +\n                               self.csv.image[index])\n        else:\n            image = Image.open('../input/shopee-product-matching/test_images/' +\n                               self.csv.image[index])\n\n        image = self.transform(image)\n        \n        if self.train:\n            label = torch.tensor(self.csv.label_group[index])\n            return image, label\n        \n        else:\n            return image","metadata":{"execution":{"iopub.status.busy":"2021-06-03T23:27:29.120774Z","iopub.execute_input":"2021-06-03T23:27:29.121156Z","iopub.status.idle":"2021-06-03T23:27:29.130513Z","shell.execute_reply.started":"2021-06-03T23:27:29.121119Z","shell.execute_reply":"2021-06-03T23:27:29.129488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 16","metadata":{"execution":{"iopub.status.busy":"2021-06-03T23:27:29.131813Z","iopub.execute_input":"2021-06-03T23:27:29.13214Z","iopub.status.idle":"2021-06-03T23:27:29.141547Z","shell.execute_reply.started":"2021-06-03T23:27:29.132106Z","shell.execute_reply":"2021-06-03T23:27:29.140763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = ShopeeDataset(csv=df_train)\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=4)\n\nprint(\"Dataset length: {}\".format(len(train_dataset)), \"\\n\" +\n      \"Shape of images: {}\".format(train_dataset[0][0].shape))","metadata":{"execution":{"iopub.status.busy":"2021-06-03T23:27:29.142773Z","iopub.execute_input":"2021-06-03T23:27:29.143136Z","iopub.status.idle":"2021-06-03T23:27:29.229734Z","shell.execute_reply.started":"2021-06-03T23:27:29.143068Z","shell.execute_reply":"2021-06-03T23:27:29.228997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Importing model ResNet18","metadata":{}},{"cell_type":"code","source":"resnet18 = models.resnet18(pretrained=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T23:53:28.275481Z","iopub.execute_input":"2021-06-03T23:53:28.275824Z","iopub.status.idle":"2021-06-03T23:53:28.577769Z","shell.execute_reply.started":"2021-06-03T23:53:28.275792Z","shell.execute_reply":"2021-06-03T23:53:28.576905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Extracting embeddings from train images using ResNet18","metadata":{}},{"cell_type":"code","source":"def extract_embeddings(model, dataloader):\n    embeddings = []\n\n    with torch.no_grad():\n        for image, label in tqdm(dataloader):\n            if torch.cuda.is_available():\n                image = image.to('cuda')\n                model.to('cuda')\n\n            img_emd = model(image)\n            img_emd = img_emd.detach().cpu().numpy()\n            embeddings.append(img_emd)\n\n    embeddings = np.concatenate(embeddings)\n    print(\"Shape of embeddings: {}\".format(embeddings.shape))\n    \n    return embeddings","metadata":{"execution":{"iopub.status.busy":"2021-06-03T18:22:27.932621Z","iopub.execute_input":"2021-06-03T18:22:27.932975Z","iopub.status.idle":"2021-06-03T18:22:27.939235Z","shell.execute_reply.started":"2021-06-03T18:22:27.932942Z","shell.execute_reply":"2021-06-03T18:22:27.938207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#embeddings = extract_embeddings(resnet18, train_dataloader)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:20:20.407144Z","iopub.execute_input":"2021-06-03T14:20:20.407611Z","iopub.status.idle":"2021-06-03T14:26:20.182926Z","shell.execute_reply.started":"2021-06-03T14:20:20.407573Z","shell.execute_reply":"2021-06-03T14:26:20.181948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#np.save(\"image_embeddings\", embeddings)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:26:20.184775Z","iopub.execute_input":"2021-06-03T14:26:20.185132Z","iopub.status.idle":"2021-06-03T14:26:20.29757Z","shell.execute_reply.started":"2021-06-03T14:26:20.185091Z","shell.execute_reply":"2021-06-03T14:26:20.296663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text prediction","metadata":{}},{"cell_type":"code","source":"class color:\n   PURPLE = '\\033[95m'\n   CYAN = '\\033[96m'\n   DARKCYAN = '\\033[36m'\n   BLUE = '\\033[94m'\n   GREEN = '\\033[92m'\n   YELLOW = '\\033[93m'\n   RED = '\\033[91m'\n   BOLD = '\\033[1m'\n   UNDERLINE = '\\033[4m'\n   END = '\\033[0m'\n    \n# Preprocess titles\noriginal_titles = df_train[\"title\"]\nlemmatizer = WordNetLemmatizer()\n\ndef preprocess_title(title):\n    title_clean = title.lower().translate(str.maketrans('','',string.punctuation)).strip()\n    title_tokenize = word_tokenize(title_clean)\n    title_w_stopword = [w for w in title_tokenize if not w in stopwords.words()]\n    title_lemmatized = [lemmatizer.lemmatize(w) for w in title_w_stopword]\n    return \" \".join(title_lemmatized)\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(5):\n    print(color.BOLD + \"Orginial Title -\" + color.END, original_titles.iloc[i])\n    print(color.BOLD + \"Preprocessed Title -\" + color.END, preprocess_title(original_titles.iloc[i]))\n    print('\\n')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics.pairwise import linear_kernel\n\ndef predict_text(val_df, threshold = 0.4, get_score=False):\n    preprocess_title  = val_df[\"preprocess_title\"].values\n    tf_idf = TfidfVectorizer(stop_words='english', binary=True, max_features=25000)\n    text_embeddings = tf_idf.fit_transform(preprocess_title).toarray()\n    \n    predictions = []\n    \n    for index in range(len(val_df)):\n        cosine_similarities = linear_kernel(text_embeddings[index].reshape(1,-1), text_embeddings).flatten()\n        pair_indexes = cosine_similarities.argsort()[:-10:-1]\n        for pair_index in pair_indexes:\n            if cosine_similarities[pair_index] > threshold:\n                pred = val_df.iloc[index].posting_id\n                predictions.append(pred)\n        \n    val_df['matches'] = predictions\n    \n    if get_score:\n        val_df['f1_score'] = get_f1_score(val_df['target'], val_df['matches'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Preprocess the titles\ntqdm.pandas()\n\n#df_train[\"preprocess_title\"] = df_train[\"title\"].progress_apply(lambda x: preprocess_title(x))\n#df_train[\"preprocess_title\"] = df_train[\"preprocess_title\"].astype('U')\n#preprocess_title = df_train[\"preprocess_title\"].values\n\n\n#Creating Train, Val dataframe\nN = len(df_train)\nN_train = int(0.8 * N)\nidx = np.array(range(N))\nnp.random.shuffle(idx)\n\nidx_train, idx_val = idx[:N_train], idx[N_train:]\ntrain = df_train.loc[idx_train].reset_index(drop=True)\nval = df_train.loc[idx_val].reset_index(drop=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_text(val, threshold=0.4, get_score=True)\nprint(\"F1 score: {}\".format(val_df['f1_score'].mean()))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Making predictions","metadata":{}},{"cell_type":"code","source":"def get_f1_score(y, y_pred):\n    intr_sect = np.array([len(set(x1).intersection(x2)) for x1, x2 in zip(y, y_pred)])\n    len_y = y.apply(lambda x: len(x)).values\n    len_y_pred = y_pred.apply(lambda x: len(x)).values\n    f1 = 2 * intr_sect / (len_y + len_y_pred)\n    \n    return f1","metadata":{"execution":{"iopub.status.busy":"2021-06-03T18:14:33.543848Z","iopub.execute_input":"2021-06-03T18:14:33.544313Z","iopub.status.idle":"2021-06-03T18:14:33.553485Z","shell.execute_reply.started":"2021-06-03T18:14:33.544277Z","shell.execute_reply":"2021-06-03T18:14:33.55268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model, train_emd, val_emd, \n            train_df, val_df, threshold, get_score=False):\n    predictions = []\n    \n    knn_model = NearestNeighbors(n_neighbors=50, algorithm='brute', metric='cosine')\n    knn_model.fit(train_emd)\n    dists, idx = knn_model.kneighbors(val_emd)\n    \n    for i in range(val_emd.shape[0]):\n        mask = dists[i] < threshold\n        pred = train_df.loc[idx[i][mask]].posting_id.values\n        predictions.append(pred)\n        \n    val_df['matches'] = predictions\n    \n    if get_score:\n        val_df['f1_score'] = get_f1_score(val_df['target'], val_df['matches'])","metadata":{"execution":{"iopub.status.busy":"2021-06-03T18:14:33.716403Z","iopub.execute_input":"2021-06-03T18:14:33.71671Z","iopub.status.idle":"2021-06-03T18:14:33.724583Z","shell.execute_reply.started":"2021-06-03T18:14:33.716683Z","shell.execute_reply":"2021-06-03T18:14:33.723776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nnum_imgs = len(train_dataset)\nnum_train_imgs = int(num_imgs*.8)\nidx = np.array(range(num_imgs))\nnp.random.shuffle(idx)\nidx_train, idx_val = idx[:num_train_imgs], idx[num_train_imgs:]\ntrain_df = df_train.loc[idx_train].reset_index(drop=True)\nval_df = df_train.loc[idx_val].reset_index(drop=True)\ntrain_set = ShopeeDataset(csv=train_df)\ntrain_loader = DataLoader(train_set, batch_size=batch_size, num_workers=4)\nval_set = ShopeeDataset(csv=val_df)\nval_loader = DataLoader(val_set, batch_size=batch_size, num_workers=4)\ntrain_embd = extract_embeddings(resnet18, train_loader)\nval_embd = extract_embeddings(resnet18, val_loader)\npredict(resnet18, train_embd, val_embd, train_df, val_df, threshold=.3, get_score=True)\nprint(\"F1 score: {}\".format(val_df['f1_score'].mean()))\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-06-03T15:43:59.346127Z","iopub.execute_input":"2021-06-03T15:43:59.346466Z","iopub.status.idle":"2021-06-03T15:43:59.356192Z","shell.execute_reply.started":"2021-06-03T15:43:59.346437Z","shell.execute_reply":"2021-06-03T15:43:59.355133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#predict(resnet18, train_embd, val_embd, train_df, val_df, threshold=.3, get_score=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T13:19:29.081458Z","iopub.execute_input":"2021-06-03T13:19:29.081888Z","iopub.status.idle":"2021-06-03T13:19:29.16527Z","shell.execute_reply.started":"2021-06-03T13:19:29.081852Z","shell.execute_reply":"2021-06-03T13:19:29.16424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#predict(resnet18, embeddings, embeddings, df_train, df_train, threshold=.2, get_score=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:27:16.432926Z","iopub.execute_input":"2021-06-03T14:27:16.433233Z","iopub.status.idle":"2021-06-03T14:28:03.157154Z","shell.execute_reply.started":"2021-06-03T14:27:16.433203Z","shell.execute_reply":"2021-06-03T14:28:03.156287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(\"F1 score: {}\".format(df_train['f1_score'].mean()))","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:28:03.158723Z","iopub.execute_input":"2021-06-03T14:28:03.159094Z","iopub.status.idle":"2021-06-03T14:28:03.164534Z","shell.execute_reply.started":"2021-06-03T14:28:03.159055Z","shell.execute_reply":"2021-06-03T14:28:03.163719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\ndf_test = pd.read_csv(\"../input/shopee-product-matching/test.csv\")\ndf_test.head(3)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-06-02T21:59:11.283024Z","iopub.execute_input":"2021-06-02T21:59:11.283392Z","iopub.status.idle":"2021-06-02T21:59:11.308503Z","shell.execute_reply.started":"2021-06-02T21:59:11.283358Z","shell.execute_reply":"2021-06-02T21:59:11.307626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test_set = ShopeeDataset(csv=df_test, train=False)\n#test_loader = DataLoader(test_set, batch_size=batch_size, num_workers=4)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T21:52:34.344089Z","iopub.execute_input":"2021-06-02T21:52:34.34442Z","iopub.status.idle":"2021-06-02T21:52:34.353542Z","shell.execute_reply.started":"2021-06-02T21:52:34.344388Z","shell.execute_reply":"2021-06-02T21:52:34.351764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_test_embeddings(model, dataloader):\n    embeddings = []\n    \n    with torch.no_grad():\n        for image in tqdm(dataloader):\n            if torch.cuda.is_available():\n                image = image.to('cuda')\n                model.to('cuda')\n                \n            img_emd = model(image)\n            img_emd = img_emd.detach().cpu().numpy()\n            embeddings.append(img_emd)\n            \n    embeddings = np.concatenate(embeddings)\n    print(\"Shape of embeddings: {}\".format(embeddings.shape))\n\n    return embeddings","metadata":{"execution":{"iopub.status.busy":"2021-06-02T21:52:34.914421Z","iopub.execute_input":"2021-06-02T21:52:34.914745Z","iopub.status.idle":"2021-06-02T21:52:34.920654Z","shell.execute_reply.started":"2021-06-02T21:52:34.914714Z","shell.execute_reply":"2021-06-02T21:52:34.919681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test_embd = extract_test_embeddings(resnet18, test_loader)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T21:52:38.564124Z","iopub.execute_input":"2021-06-02T21:52:38.564443Z","iopub.status.idle":"2021-06-02T21:52:38.841235Z","shell.execute_reply.started":"2021-06-02T21:52:38.564411Z","shell.execute_reply":"2021-06-02T21:52:38.840093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#predict(resnet18, embeddings, test_embd, df_train, df_test, threshold=.5)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T21:59:18.248807Z","iopub.execute_input":"2021-06-02T21:59:18.249123Z","iopub.status.idle":"2021-06-02T21:59:18.457428Z","shell.execute_reply.started":"2021-06-02T21:59:18.249095Z","shell.execute_reply":"2021-06-02T21:59:18.456497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df_test['matches'] = df_test['matches'].apply(lambda x: \" \".join(x))","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:00:11.953215Z","iopub.execute_input":"2021-06-02T22:00:11.953616Z","iopub.status.idle":"2021-06-02T22:00:11.959928Z","shell.execute_reply.started":"2021-06-02T22:00:11.953568Z","shell.execute_reply":"2021-06-02T22:00:11.958884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df_test[['posting_id', 'matches']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:00:13.192872Z","iopub.execute_input":"2021-06-02T22:00:13.193184Z","iopub.status.idle":"2021-06-02T22:00:13.201033Z","shell.execute_reply.started":"2021-06-02T22:00:13.193155Z","shell.execute_reply":"2021-06-02T22:00:13.200236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ResNet18 with ArcFace","metadata":{}},{"cell_type":"code","source":"class ArcFace(nn.Module):\n    \n    def __init__(self, in_feat, out_feat, s=30.0, m=0.30, margin=False):\n        super(ArcFace, self).__init__()\n        self.in_feat = in_feat\n        self.out_feat = out_feat\n        self.s = torch.tensor(m)\n        self.m = torch.tensor(m)\n        \n        with torch.no_grad():\n            weights = nn.Parameter(torch.FloatTensor(out_feat, in_feat))\n            nn.init.xavier_uniform_(weights)\n        \n        if torch.cuda.is_available():\n            self.weights = weights.to('cuda')\n\n        self.margin = margin\n        self.cos_m = torch.cos(self.m)\n        self.sin_m = torch.sin(self.m)\n        \n        tmp = torch.tensor(np.pi - m)\n        self.theta = torch.cos(tmp)\n        self.mm = torch.sin(tmp) * m\n\n    def forward(self, x, label):\n        cos = F.linear(F.normalize(x), F.normalize(self.weights))\n        sin = (1. - cos**2).clamp(0, 1)**.5\n        phi = cos * self.cos_m - sin * self.sin_m\n        if self.margin:\n            phi = torch.where(cos > 0, phi, cos)\n        else:\n            phi = torch.where(cos > self.theta, phi, cos - self.mm)\n\n        one_hot = torch.zeros(cos.size(), device=device)\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        output = (one_hot * phi) + ((1. - one_hot) * cos)\n        output *= self.s\n        del cos, sin, phi, one_hot\n        \n        return output","metadata":{"execution":{"iopub.status.busy":"2021-06-03T23:34:08.628591Z","iopub.execute_input":"2021-06-03T23:34:08.629001Z","iopub.status.idle":"2021-06-03T23:34:08.640187Z","shell.execute_reply.started":"2021-06-03T23:34:08.628967Z","shell.execute_reply":"2021-06-03T23:34:08.639255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ResNetWithArcFaceNet(nn.Module):\n\n    def __init__(self, num_classes, model, s=30.0, margin=0.50):\n        super(ResNetWithArcFaceNet, self).__init__()\n\n        self.model = nn.Sequential(*list(resnet18.children())[:-1]).cuda()\n        self.num_classes = num_classes\n        self.s = s\n        self.margin = margin\n        #self.fc = nn.Linear(1000, 512)\n        self.flatten = nn.Sequential(nn.Flatten())\n        self.arcface = ArcFace(512, self.num_classes, s=self.s, m=self.margin)\n        #self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x, label):\n        out = self.model(x)\n        out = self.flatten(out)\n        #out = self.fc(out)\n        out = self.arcface(out, label)\n        #out = self.softmax(out)\n\n        return out","metadata":{"execution":{"iopub.status.busy":"2021-06-04T00:08:04.825334Z","iopub.execute_input":"2021-06-04T00:08:04.82567Z","iopub.status.idle":"2021-06-04T00:08:04.834921Z","shell.execute_reply.started":"2021-06-04T00:08:04.825624Z","shell.execute_reply":"2021-06-04T00:08:04.83413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_rate = 1e-3\nnum_classes = df_train.label_group.unique().shape[0]\nmodel = ResNetWithArcFaceNet(num_classes, resnet18)\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\nloss = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T00:08:06.23636Z","iopub.execute_input":"2021-06-04T00:08:06.236664Z","iopub.status.idle":"2021-06-04T00:08:06.285712Z","shell.execute_reply.started":"2021-06-04T00:08:06.236635Z","shell.execute_reply":"2021-06-04T00:08:06.284863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, optimizer, num_classes, train_loader, criterion, n_epoch=5):\n    \n    #model.eval()\n    loss_train = []\n    if torch.cuda.is_available():\n        model = model.to('cuda')\n    \n    \n    for epoch in range(n_epoch): \n        for images, labels in tqdm(train_loader):\n            if torch.cuda.is_available():\n                images = images.to('cuda')\n                labels = labels.to('cuda')\n            \n            optimizer.zero_grad()\n            outputs = model(images, labels)\n\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            loss_train.append(loss.item())\n\n        if epoch % 1 == 0:\n            print(\"Epoch {}\".format(epoch))\n            print(\"Loss: {}\".format(loss.item()))\n    \n        torch.save({\n                    'epoch': epoch,\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'loss': loss_train[-1],\n                    }, 'model_arcface_epoch' + str(epoch) + '.pt')\n            \n    print('Finished Training')","metadata":{"execution":{"iopub.status.busy":"2021-06-04T00:08:06.635035Z","iopub.execute_input":"2021-06-04T00:08:06.63533Z","iopub.status.idle":"2021-06-04T00:08:06.643371Z","shell.execute_reply.started":"2021-06-04T00:08:06.635302Z","shell.execute_reply":"2021-06-04T00:08:06.642327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train(model, optimizer, num_classes, train_dataloader, loss, n_epoch=50)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T00:08:07.095212Z","iopub.execute_input":"2021-06-04T00:08:07.095505Z","iopub.status.idle":"2021-06-04T00:35:37.23751Z","shell.execute_reply.started":"2021-06-04T00:08:07.095477Z","shell.execute_reply":"2021-06-04T00:35:37.235484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save({\n            'epoch': 50,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            }, 'model_arcface_epoch50.pt')","metadata":{},"execution_count":null,"outputs":[]}]}