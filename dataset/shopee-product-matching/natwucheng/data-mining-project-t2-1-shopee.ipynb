{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import ","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')","metadata":{"execution":{"iopub.status.busy":"2021-12-31T00:38:16.373941Z","iopub.execute_input":"2021-12-31T00:38:16.374416Z","iopub.status.idle":"2021-12-31T00:38:16.380057Z","shell.execute_reply.started":"2021-12-31T00:38:16.374352Z","shell.execute_reply":"2021-12-31T00:38:16.379081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \n\nimport math\nimport random \nimport os \nimport cv2\nimport timm\n\nfrom tqdm import tqdm \n\nimport albumentations as A \nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torch \nfrom torch.utils.data import Dataset \nfrom torch import nn\nimport torch.nn.functional as F \n\nimport gc\nimport cudf\nimport cuml\nimport cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-31T00:38:16.386691Z","iopub.execute_input":"2021-12-31T00:38:16.387063Z","iopub.status.idle":"2021-12-31T00:38:19.454071Z","shell.execute_reply.started":"2021-12-31T00:38:16.387027Z","shell.execute_reply":"2021-12-31T00:38:19.453194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"class CFG:\n    TEST = True\n    img_size = 512\n    batch_size = 12\n    seed = 2020\n    \n    device = 'cuda'\n    classes = 11014\n    \n    model_name = 'efficientnet_b3'\n    model_path = '../input/shopee-pytorch-models/arcface_512x512_eff_b3.pt'\n    \n    scale = 30 \n    margin = 0.5","metadata":{"execution":{"iopub.status.busy":"2021-12-31T00:38:19.45667Z","iopub.execute_input":"2021-12-31T00:38:19.457042Z","iopub.status.idle":"2021-12-31T00:38:19.466308Z","shell.execute_reply.started":"2021-12-31T00:38:19.457006Z","shell.execute_reply":"2021-12-31T00:38:19.465402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"def read_dataset():\n    if CFG.TEST :\n        df = pd.read_csv('../input/shopee-product-matching/test.csv')\n        df_cu = cudf.DataFrame(df)\n        image_paths = '../input/shopee-product-matching/test_images/' + df['image']\n    else:\n        df = pd.read_csv('../input/shopee-product-matching/train.csv')\n        tmp = df.groupby(['label_group'])['posting_id'].unique().to_dict()\n        df['matches'] = df['label_group'].map(tmp)\n        df['matches'] = df['matches'].apply(lambda x: ' '.join(x))\n        df_cu = cudf.DataFrame(df)\n        image_paths = '../input/shopee-product-matching/train_images/' + df['image']\n    return df, df_cu, image_paths","metadata":{"execution":{"iopub.status.busy":"2021-12-31T00:38:19.467789Z","iopub.execute_input":"2021-12-31T00:38:19.468426Z","iopub.status.idle":"2021-12-31T00:38:19.47771Z","shell.execute_reply.started":"2021-12-31T00:38:19.468385Z","shell.execute_reply":"2021-12-31T00:38:19.476896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_torch(CFG.seed)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T00:38:19.481205Z","iopub.execute_input":"2021-12-31T00:38:19.481449Z","iopub.status.idle":"2021-12-31T00:38:19.491268Z","shell.execute_reply.started":"2021-12-31T00:38:19.481427Z","shell.execute_reply":"2021-12-31T00:38:19.49057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def f1_score(y_true, y_pred):\n    y_true = y_true.apply(lambda x: set(x.split()))\n    y_pred = y_pred.apply(lambda x: set(x.split()))\n    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n    len_y_pred = y_pred.apply(lambda x: len(x)).values\n    len_y_true = y_true.apply(lambda x: len(x)).values\n    f1 = 2 * intersection / (len_y_pred + len_y_true)\n    return f1","metadata":{"execution":{"iopub.status.busy":"2021-12-31T00:38:19.495423Z","iopub.execute_input":"2021-12-31T00:38:19.495684Z","iopub.status.idle":"2021-12-31T00:38:19.501712Z","shell.execute_reply.started":"2021-12-31T00:38:19.495651Z","shell.execute_reply":"2021-12-31T00:38:19.500863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def combine_predictions(row):\n#     print([row['image_predictions']])\n#     print(row['text_predictions'])\n    if CFG.TEST:\n        x = np.concatenate([row['image_predictions'], row['text_predictions']])\n    else:\n        x = np.concatenate([[row['image_predictions']], row['text_predictions']])\n    return ' '.join( np.unique(x))","metadata":{"execution":{"iopub.status.busy":"2021-12-31T00:38:19.503393Z","iopub.execute_input":"2021-12-31T00:38:19.504026Z","iopub.status.idle":"2021-12-31T00:38:19.511453Z","shell.execute_reply.started":"2021-12-31T00:38:19.503988Z","shell.execute_reply":"2021-12-31T00:38:19.510622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using Images","metadata":{}},{"cell_type":"code","source":"def get_image_predictions(df, embeddings):\n    \n    if len(df) > 3:\n        KNN = 50\n    else : \n        KNN = 3\n    \n    model = NearestNeighbors(n_neighbors = KNN)\n    model.fit(embeddings)\n    distances, indices = model.kneighbors(embeddings)\n    \n    if CFG.TEST:\n        predictions = []\n        for k in tqdm(range(embeddings.shape[0])):\n            idx = np.where(distances[k,] < 4.4)[0]\n            ids = indices[k,idx]\n            posting_ids = df['posting_id'].iloc[ids].values\n            predictions.append(posting_ids)\n    else:\n        thresholds = list(np.arange(3.0, 5.0, 0.1))\n        scores = []\n        for threshold in thresholds:\n            predictions = []\n            for k in range(embeddings.shape[0]):\n                idx = np.where(distances[k,] < threshold)[0]\n                ids = indices[k,idx]\n                posting_ids = ' '.join(df['posting_id'].iloc[ids].values)\n                predictions.append(posting_ids)\n            df['pred_matches'] = predictions\n            df['f1'] = f1_score(df['matches'], df['pred_matches'])\n            score = df['f1'].mean()\n            print(f'Our f1 score for threshold {threshold} is {score}')\n            scores.append(score)\n        thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\n        max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n        best_threshold = max_score['thresholds'].values[0]\n        best_score = max_score['scores'].values[0]\n        print(f'Our best score is {best_score} and has a threshold {best_threshold}')\n        \n    del model, distances, indices\n    gc.collect()\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2021-12-31T00:38:19.51428Z","iopub.execute_input":"2021-12-31T00:38:19.514604Z","iopub.status.idle":"2021-12-31T00:38:19.525702Z","shell.execute_reply.started":"2021-12-31T00:38:19.514573Z","shell.execute_reply":"2021-12-31T00:38:19.524824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_test_transforms():\n\n    return A.Compose(\n        [\n            A.Resize(CFG.img_size,CFG.img_size,always_apply=True),\n            A.Normalize(),\n        ToTensorV2(p=1.0)\n        ]\n    )","metadata":{"execution":{"iopub.status.busy":"2021-12-31T00:38:19.52718Z","iopub.execute_input":"2021-12-31T00:38:19.527782Z","iopub.status.idle":"2021-12-31T00:38:19.537342Z","shell.execute_reply.started":"2021-12-31T00:38:19.527732Z","shell.execute_reply":"2021-12-31T00:38:19.536589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ShopeeDataset(Dataset):\n    def __init__(self, image_paths, transforms=None):\n\n        self.image_paths = image_paths\n        self.augmentations = transforms\n\n    def __len__(self):\n        return self.image_paths.shape[0]\n\n    def __getitem__(self, index):\n        image_path = self.image_paths[index]\n        \n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.augmentations:\n            augmented = self.augmentations(image=image)\n            image = augmented['image']       \n    \n        return image,torch.tensor(1)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T00:38:19.53876Z","iopub.execute_input":"2021-12-31T00:38:19.539277Z","iopub.status.idle":"2021-12-31T00:38:19.546809Z","shell.execute_reply.started":"2021-12-31T00:38:19.539237Z","shell.execute_reply":"2021-12-31T00:38:19.545865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ArcMarginProduct(nn.Module):\n    def __init__(self, in_features, out_features, scale=30.0, margin=0.50, easy_margin=False, ls_eps=0.0):\n        super(ArcMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.scale = scale\n        self.margin = margin\n        self.ls_eps = ls_eps  # label smoothing\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(margin)\n        self.sin_m = math.sin(margin)\n        self.th = math.cos(math.pi - margin)\n        self.mm = math.sin(math.pi - margin) * margin\n\n    def forward(self, input, label):\n        # --------------------------- cos(theta) & phi(theta) ---------------------------\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n        # --------------------------- convert label to one-hot ---------------------------\n        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n        one_hot = torch.zeros(cosine.size(), device='cuda')\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.scale\n\n        return output\n\nclass ShopeeModel(nn.Module):\n\n    def __init__(\n        self,\n        n_classes = CFG.classes,\n        model_name = CFG.model_name,\n        fc_dim = 512,\n        margin = CFG.margin,\n        scale = CFG.scale,\n        use_fc = False,\n        pretrained = False):\n\n\n        super(ShopeeModel,self).__init__()\n        print('Building Model Backbone for {} model'.format(model_name))\n\n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n\n#         if model_name == 'efficientnet_b3':\n        final_in_features = self.backbone.classifier.in_features\n        self.backbone.classifier = nn.Identity()\n        self.backbone.global_pool = nn.Identity()\n\n        self.pooling =  nn.AdaptiveAvgPool2d(1)\n\n        self.use_fc = use_fc\n\n        self.dropout = nn.Dropout(p=0.0)\n        self.fc = nn.Linear(final_in_features, fc_dim)\n        self.bn = nn.BatchNorm1d(fc_dim)\n        self._init_params()\n        final_in_features = fc_dim\n\n        self.final = ArcMarginProduct(\n            final_in_features,\n            n_classes,\n            scale = scale,\n            margin = margin,\n            easy_margin = False,\n            ls_eps = 0.0\n        )\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, image, label):\n        feature = self.extract_feat(image)\n        #logits = self.final(feature,label)\n        return feature\n\n    def extract_feat(self, x):\n        batch_size = x.shape[0]\n        x = self.backbone(x)\n        x = self.pooling(x).view(batch_size, -1)\n\n        if self.use_fc:\n            x = self.dropout(x)\n            x = self.fc(x)\n            x = self.bn(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-12-31T00:38:19.548236Z","iopub.execute_input":"2021-12-31T00:38:19.548693Z","iopub.status.idle":"2021-12-31T00:38:19.569671Z","shell.execute_reply.started":"2021-12-31T00:38:19.548656Z","shell.execute_reply":"2021-12-31T00:38:19.568856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_image_embeddings(image_paths, model_name = CFG.model_name):\n    embeds = []\n    \n    model = ShopeeModel(model_name = model_name)\n    model.eval()\n    model.load_state_dict(torch.load(CFG.model_path))\n    model = model.to(CFG.device)\n\n    image_dataset = ShopeeDataset(image_paths=image_paths,transforms=get_test_transforms())\n    image_loader = torch.utils.data.DataLoader(\n        image_dataset,\n        batch_size=CFG.batch_size,\n        pin_memory=True,\n        drop_last=False,\n        num_workers=4\n    )\n    \n    \n    with torch.no_grad():\n        for img,label in tqdm(image_loader): \n            img = img.cuda()\n            label = label.cuda()\n            feat = model(img,label)\n            image_embeddings = feat.detach().cpu().numpy()\n            embeds.append(image_embeddings)\n    \n    \n    del model\n    image_embeddings = np.concatenate(embeds)\n    print(f'Our image embeddings shape is {image_embeddings.shape}')\n    del embeds\n    gc.collect()\n    return image_embeddings","metadata":{"execution":{"iopub.status.busy":"2021-12-31T00:38:19.57139Z","iopub.execute_input":"2021-12-31T00:38:19.572001Z","iopub.status.idle":"2021-12-31T00:38:19.582049Z","shell.execute_reply.started":"2021-12-31T00:38:19.571918Z","shell.execute_reply":"2021-12-31T00:38:19.581258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df,df_cu,image_paths = read_dataset()\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-31T00:38:19.583602Z","iopub.execute_input":"2021-12-31T00:38:19.584075Z","iopub.status.idle":"2021-12-31T00:38:22.526597Z","shell.execute_reply.started":"2021-12-31T00:38:19.584038Z","shell.execute_reply":"2021-12-31T00:38:22.525701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using Text with Tfidf","metadata":{}},{"cell_type":"code","source":"from cuml.feature_extraction.text import TfidfVectorizer\n\nmodel = TfidfVectorizer(stop_words=None, binary=True, max_features=25000)\ntext_embeddings = model.fit_transform(df_cu.title).toarray()\nprint('text embeddings shape',text_embeddings.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T00:38:22.527933Z","iopub.execute_input":"2021-12-31T00:38:22.52829Z","iopub.status.idle":"2021-12-31T00:38:23.063614Z","shell.execute_reply.started":"2021-12-31T00:38:22.528251Z","shell.execute_reply":"2021-12-31T00:38:23.062575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = []\nCHUNK = 1024*4\n\nprint('Finding similar titles...')\nCTS = len(df_cu)//CHUNK\nif len(df_cu)%CHUNK!=0: CTS += 1\nfor j in range( CTS ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(df_cu))\n    print('chunk',a,'to',b)\n    \n    # COSINE SIMILARITY DISTANCE\n    # cts = np.dot( text_embeddings, text_embeddings[a:b].T).T\n    cts = cupy.matmul(text_embeddings, text_embeddings[a:b].T).T\n    \n    for k in range(b-a):\n        # IDX = np.where(cts[k,]>0.7)[0]\n        IDX = cupy.where(cts[k,]>0.75)[0]\n        o = df_cu.iloc[cupy.asnumpy(IDX)].posting_id.to_pandas().values\n        preds.append(o)\n        \ndel model, text_embeddings","metadata":{"execution":{"iopub.status.busy":"2021-12-31T00:38:23.064951Z","iopub.execute_input":"2021-12-31T00:38:23.065301Z","iopub.status.idle":"2021-12-31T00:38:23.293144Z","shell.execute_reply.started":"2021-12-31T00:38:23.065253Z","shell.execute_reply":"2021-12-31T00:38:23.292148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_cu['text_predictions'] = preds","metadata":{"execution":{"iopub.status.busy":"2021-12-31T00:38:23.300727Z","iopub.execute_input":"2021-12-31T00:38:23.301164Z","iopub.status.idle":"2021-12-31T00:38:23.316554Z","shell.execute_reply.started":"2021-12-31T00:38:23.301121Z","shell.execute_reply":"2021-12-31T00:38:23.315562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_embeddings = get_image_embeddings(image_paths.values)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T00:38:23.32084Z","iopub.execute_input":"2021-12-31T00:38:23.322111Z","iopub.status.idle":"2021-12-31T00:38:24.707293Z","shell.execute_reply.started":"2021-12-31T00:38:23.322075Z","shell.execute_reply":"2021-12-31T00:38:24.706294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_predictions = get_image_predictions(df, image_embeddings)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T00:38:24.708908Z","iopub.execute_input":"2021-12-31T00:38:24.709509Z","iopub.status.idle":"2021-12-31T00:38:25.021023Z","shell.execute_reply.started":"2021-12-31T00:38:24.709468Z","shell.execute_reply":"2021-12-31T00:38:25.020058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing submission ","metadata":{}},{"cell_type":"code","source":"if CFG.TEST:\n    df['image_predictions'] = image_predictions\n    df['text_predictions'] = df_cu['text_predictions'].to_pandas().values\n    df['matches'] = df.apply(combine_predictions, axis = 1)\n    df[['posting_id', 'matches']].to_csv('submission.csv', index = False)\nelse:\n    df['image_predictions'] = image_predictions\n    df['text_predictions'] = df_cu['text_predictions'].to_pandas().values\n    df['pred_matches'] = df.apply(combine_predictions, axis = 1)\n    df['f1'] = f1_score(df['matches'], df['pred_matches'])\n    score = df['f1'].mean()\n    print(f'Our final f1 cv score is {score}')\n    df['matches'] = df['pred_matches']\n    df[['posting_id', 'matches']].to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T00:38:25.022341Z","iopub.execute_input":"2021-12-31T00:38:25.022698Z","iopub.status.idle":"2021-12-31T00:38:25.135185Z","shell.execute_reply.started":"2021-12-31T00:38:25.022659Z","shell.execute_reply":"2021-12-31T00:38:25.134443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}