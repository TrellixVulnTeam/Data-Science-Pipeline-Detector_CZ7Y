{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <font color='brown' size=4>Objective:</font> \n        \n<p> This notebook explores different approaches for learning discriminative embeddings using popular algorithms from both text and image. We also get to explore about a concept called <b>Metric learning</b> which is very relevant for this competition</p>"},{"metadata":{},"cell_type":"markdown","source":"> **ðŸ“Œ Note**: Here we gradually learn from basic representation to the advanced ones as always"},{"metadata":{},"cell_type":"markdown","source":"<font color='brown' size=4>Ride on ðŸš€</font><br>"},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"!pip install sentence_transformers\n!pip install timm\n!pip install -q pytorch-metric-learning[with-hooks]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfrom sentence_transformers import SentenceTransformer, InputExample, losses\nfrom torch.utils.data import DataLoader\nimport logging\nfrom datetime import datetime\nimport math\nfrom sklearn.model_selection import train_test_split\nfrom sentence_transformers import SentenceTransformer, models\nfrom torch import nn\nfrom sentence_transformers.evaluation import BinaryClassificationEvaluator\n\n\nimport cv2\nimport torch.nn.init as init\nimport torch\nfrom PIL import Image, ImageFilter\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nfrom torch.optim import Adam, SGD, RMSprop\nimport time\nfrom torch.autograd import Variable\nimport torch.functional as F\nfrom tqdm import tqdm\nfrom sklearn import metrics\nimport urllib\nimport pickle\nimport cv2\nimport torch.nn.functional as F\nfrom torchvision import models as torchmodels\nimport seaborn as sns\nimport random\nimport timm\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn import preprocessing\n\nimport sys\n\n%matplotlib inline\nfrom pytorch_metric_learning import losses as pml_loss, miners, samplers, trainers, testers\nfrom pytorch_metric_learning.utils import common_functions\nimport pytorch_metric_learning.utils.logging_presets as logging_presets\nimport torchvision\nfrom torchvision import datasets, transforms\nfrom PIL import Image\nimport logging\nimport matplotlib.pyplot as plt\nimport umap\nfrom cycler import cycler\nimport record_keeper\nimport pytorch_metric_learning\nimport albumentations\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n\nlogging.getLogger().setLevel(logging.INFO)\nlogging.info(\"VERSION %s\"%pytorch_metric_learning.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Table of Contents\n\n- 1. Embedding representations\n   - 1.1 Text embeddings\n   - 1.2 Image embeddings\n\n- 2. Sbert overview\n   - 2.1 Architecture\n      \n- 3. Introduction to metric learning\n   - 3.1 Pytorch metric learning toolkit\n\n- 4. Finetuning\n   - 4.1 Sentence bert finetuning\n   - 4.2 Pytorch metric learning(PML) for image embedding\n   - 4.3 Custom pipeline + PML\n       - 4.4 Utils\n       - 4.5 Engine\n       \n- 5. Acknowledgements"},{"metadata":{},"cell_type":"markdown","source":"# <font color='brown' size=4>1. Embedding representations</font>"},{"metadata":{},"cell_type":"markdown","source":"<p>An image or text can be represented in many ways. But what makes it unique for the any machine learning model to learn is the distribution it learns underneath.</p> \n    \n\n  <p>Ideally an embedding is a relatively low-dimensional space into which you can translate high-dimensional vectors. Ideally, an embedding captures some of the semantics of the input by placing semantically similar inputs close together. An embedding can be learned and reused across models. </p>"},{"metadata":{},"cell_type":"markdown","source":"## <font color='brown' size=4>1.1 Text embeddings</font>"},{"metadata":{},"cell_type":"markdown","source":"<p> Machine learning models take vectors (arrays of numbers) as input. When working with text, the first thing you must do is come up with a strategy to convert strings to numbers (or to \"vectorize\" the text) before feeding it to the model.\n\nNow, there are again many ways to do it, lets explain some of the basic ones and move to the recent advancements in the NLP world</p>"},{"metadata":{},"cell_type":"markdown","source":"<b>One-hot encodings:</b>\n\nAs a first idea, you might \"one-hot\" encode each word in your vocabulary. Consider the sentence \"The cat sat on the mat\". The vocabulary (or unique words) in this sentence is (cat, mat, on, sat, the). To represent each word, you will create a zero vector with length equal to the vocabulary, then place a one in the index that corresponds to the word. This approach is shown in the following diagram."},{"metadata":{},"cell_type":"markdown","source":"<img src='https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/one-hot.png?raw=1' width=1000>\n<div align=\"center\"><font size=\"3\">Source: Google</font></div>"},{"metadata":{},"cell_type":"markdown","source":"<b>Indexing words:</b>\n\nA second approach you might try is to encode each word using a unique number. Continuing the example above, you could assign 1 to \"cat\", 2 to \"mat\", and so on. You could then encode the sentence \"The cat sat on the mat\" as a dense vector like [5, 1, 4, 3, 5, 2]. This appoach is efficient. Instead of a sparse vector, you now have a dense one (where all elements are full). But it also has downsides to it"},{"metadata":{},"cell_type":"markdown","source":"<b>Word embeddings:</b>\n\nThird approach is is word embeddings. Word embeddings give us a way to use an efficient, dense representation in which similar words have a similar encoding. Importantly, you do not have to specify this encoding by hand. An embedding is a dense vector of floating point values (the length of the vector is a parameter you specify). Instead of specifying the values for the embedding manually, they are trainable parameters (weights learned by the model during training, in the same way a model learns weights for a dense layer)"},{"metadata":{},"cell_type":"markdown","source":"<img src='https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/embedding2.png?raw=1' width=1000>\n<div align=\"center\"><font size=\"3\">Source: Google</font></div>"},{"metadata":{},"cell_type":"markdown","source":"> **ðŸ“Œ Note**: From the invention of word embeddings,n number of approaches came out to learn the representation from text. With the transformers ruling the NLP space, we will look into one of its architecture which has been specifically designed for learning sentence representation"},{"metadata":{},"cell_type":"markdown","source":"## <font color='brown' size=4>1.2 Image embeddings</font>"},{"metadata":{},"cell_type":"markdown","source":"<p> Image embeddings are lot more intuitive than text, in a lot of use cases we can either use the pretrained models like VGG,Resnet,Effnet to get features out of it or finetune a model for custom use and use it for lot of exciting applications like similarity match,image retrieval etc </p>"},{"metadata":{},"cell_type":"markdown","source":"> **ðŸ“Œ Note**: Lot of pretrained backbone models can be used for getting embeddings, here we will train efficientnet and resnet based model with metric learning to learn the image embeddings from our product images"},{"metadata":{},"cell_type":"markdown","source":"# <font color='brown' size=4>2. Sbert overview</font>"},{"metadata":{},"cell_type":"markdown","source":"<p>Sentence-BERT (SBERT), a modification of the BERT network uses siamese and triplet networks to derive semantically meaningful sentence embeddings\n    \nFor sentence / text embeddings, we want to map a variable length input text to a fixed sized dense vector. The most basic network architecture we can use is the following:\n</p>"},{"metadata":{},"cell_type":"markdown","source":"<img src='https://www.sbert.net/_images/SBERT_Architecture.png' width=1000>\n<div align=\"center\"><font size=\"3\">Source: Sbert</font></div>\n"},{"metadata":{},"cell_type":"markdown","source":"> **ðŸ“Œ Note**: We feed the input sentence or text into a transformer network like BERT. BERT produces contextualized word embeddings for all input tokens in our text. As we want a fixed-sized output representation (vector u), we need a pooling layer. Different pooling options are available, the most basic one is mean-pooling: We simply average all contextualized word embeddings BERT is giving us. This gives us a fixed 768 dimensional output vector independet how long our input text was."},{"metadata":{},"cell_type":"markdown","source":"## <font color='brown' size=4>2.1 Architecture</font>"},{"metadata":{},"cell_type":"markdown","source":"<p>The most simple way is to have sentence pairs annotated with a score indicating their similarity, e.g. on a scale 0 to 1. We can then train the network with a Siamese Network Architecture</p>"},{"metadata":{},"cell_type":"markdown","source":"<img src='https://www.sbert.net/_images/SBERT_Siamese_Network1.png' width=1000>\n<div align=\"center\"><font size=\"3\">Source: Sbert</font></div>"},{"metadata":{},"cell_type":"markdown","source":"> **ðŸ“Œ Note**: For each sentence pair, we pass sentence A and sentence B through our network which yields the embeddings u und v. The similarity of these embeddings is computed using cosine similarity and the result is compared to the gold similarity score. This allows our network to be fine-tuned and to recognize the similarity of sentences."},{"metadata":{},"cell_type":"markdown","source":"# <font color='brown' size=4>3. Introduction to metric learning</font>"},{"metadata":{},"cell_type":"markdown","source":"<p>The goal of Metric Learning is to learn a representation function that maps objects into an embedded space. The distance in the embedded space should preserve the objectsâ€™ similarity â€” similar objects get close and dissimilar objects get far away. Various loss functions have been developed for Metric Learning.</p>\n    \n   <p>For example, the <b>contrastive loss</b> guides the objects from the same class to be mapped to the same point and those from different classes to be mapped to different points whose distances are larger than a margin. <b>Triplet loss</b> is also popular, which requires the distance between the anchor sample and the positive sample to be smaller than the distance between the anchor sample and the negative sample.</p>"},{"metadata":{},"cell_type":"markdown","source":"## <font color='brown' size=4>3.1 Pytorch metric learning toolkit</font>"},{"metadata":{},"cell_type":"markdown","source":"<p>PyTorch Metric Learning is an open source library that aims to do metric based learning elegantly. The modular and flexible design allows users to easily try out different combinations of algorithms in their existing code. It also comes with complete train/test workflows, for users who want results fast.</p>"},{"metadata":{},"cell_type":"markdown","source":"**Major components:**\n\n* Miners: Determine the best way to create batches\n* Losses: Loss functions work similarly to many regular PyTorch loss functions\n* Distances: Distance metrics used in loss function for comparison of embeddings\n* Reducers: Losses are typically computed per element, pair, or triplet, and are then                   reduced to a single value by some operation, such as averaging"},{"metadata":{},"cell_type":"markdown","source":"<img src='https://kevinmusgrave.github.io/pytorch-metric-learning/imgs/high_level_loss_function_overview.png' width=1000>\n<div align=\"center\"><font size=\"3\">Source: PML</font></div>"},{"metadata":{},"cell_type":"markdown","source":"> **ðŸ“Œ Note**: Please refer to their documentation for detailed explanation, link is attached in the acknowledgement section"},{"metadata":{},"cell_type":"markdown","source":"# <font color='brown' size=4>4. Finetuning</font>"},{"metadata":{},"cell_type":"markdown","source":"<p>Let's now finetune text and images of the product to get more relevant embeddings which can later be used for finding similarity</p>"},{"metadata":{},"cell_type":"markdown","source":"## <font color='brown' size=4>4.1 Sentence bert finetuning</font>"},{"metadata":{},"cell_type":"markdown","source":"<b>Dataset credit</b> : https://www.kaggle.com/tanulsingh077/shopee-siamese-training"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df=pd.read_csv('../input/shopee-product-matching/train.csv')\ntest_df=pd.read_csv('../input/shopee-product-matching/test.csv')\nsiam_data=pd.read_csv('../input/shopee-siamese-training/siamese_data.csv')\n\ntrain_image_path='../input/shopee-product-matching/train_images/'\n\nsiam_data['image_1_path']=siam_data['image_1'].apply(lambda x: train_image_path+x)\nsiam_data['image_2_path']=siam_data['image_2'].apply(lambda x: train_image_path+x)\n\ntrain_df['image_path']=train_df['image'].apply(lambda x: train_image_path+x)\nlabels_list=train_df['label_group'].value_counts().index[:100].tolist() ##filtering only 100 labels\n\ntrain_df=train_df[train_df['label_group'].isin(labels_list)].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"siam_data.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"siam_data['label'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#shuffle the data for split\nsiam_data=siam_data.sample(frac=0.5).reset_index(drop=True) #taking only sample of data\ntrain,val = train_test_split(siam_data, test_size=0.2, random_state=42,\n                                                       stratify=siam_data['label'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name = 'bert-base-uncased'\ntrain_batch_size = 16\nnum_epochs = 1\nmodel_save_path = 'output/training_stsbenchmark_continue_training-'+model_name+'-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get the bert base uncased embedding and pass it into pooling layer and a linear layer for findal representation\nword_embedding_model = models.Transformer(model_name, max_seq_length=512)\npooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\ndense_model = models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(), out_features=512, activation_function=nn.Tanh())\n\nmodel = SentenceTransformer(modules=[word_embedding_model, pooling_model, dense_model])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Prepare dataset\ntrain_samples = []\nval_samples = []\n\nfor row in train.iterrows():\n        inp_example = InputExample(texts=[row[1]['title_1'], row[1]['title_2']], label=row[1]['label'])\n        train_samples.append(inp_example)         \n\nfor row in val.iterrows():\n        inp_example = InputExample(texts=[row[1]['title_1'], row[1]['title_2']], label=row[1]['label'])\n        val_samples.append(inp_example)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size)\ntrain_loss = losses.ContrastiveLoss(model=model)\n\n\n# Development set: Measure correlation between cosine score and gold labels\nlogging.info(\"Read evaluation dev dataset\")\nevaluator = BinaryClassificationEvaluator.from_input_examples(val_samples, name='contrast-dev')\n\n\n# Configure the training. We skip evaluation in this example\nwarmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1) #10% of train data for warm-up\nlogging.info(\"Warmup-steps: {}\".format(warmup_steps))\n\n\n# Train the model\nmodel.fit(train_objectives=[(train_dataloader, train_loss)],\n          evaluator=evaluator,\n          epochs=num_epochs,\n          evaluation_steps=1000,\n          warmup_steps=warmup_steps,\n          output_path=model_save_path)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color='brown' size=4>4.2 Pytorch metric learning(PML) for image embedding</font>"},{"metadata":{},"cell_type":"markdown","source":"> **ðŸ“Œ Note**: Here we will train the model using metric learning, So that same class gets similar representation, this can be even tried with siamese architecture"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dummy model for the pipeline building, it can be replaced with any model\n\nclass MLP(nn.Module):\n    # layer_sizes[0] is the dimension of the input\n    # layer_sizes[-1] is the dimension of the output\n    def __init__(self, layer_sizes, final_relu=False):\n        super().__init__()\n        layer_list = []\n        layer_sizes = [int(x) for x in layer_sizes]\n        num_layers = len(layer_sizes) - 1\n        final_relu_layer = num_layers if final_relu else num_layers - 1\n        for i in range(len(layer_sizes) - 1):\n            input_size = layer_sizes[i]\n            curr_size = layer_sizes[i + 1]\n            if i < final_relu_layer:\n                layer_list.append(nn.ReLU(inplace=False))\n            layer_list.append(nn.Linear(input_size, curr_size))\n        self.net = nn.Sequential(*layer_list)\n        self.last_linear = self.net[-1]\n\n    def forward(self, x):\n        return self.net(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Set trunk model and replace the softmax layer with an identity function\ntrunk = torchvision.models.resnet18(pretrained=True) #any model can be used as trunk/backbone\ntrunk_output_size = trunk.fc.in_features\ntrunk.fc = common_functions.Identity()\ntrunk = trunk.to(device)\n\n# Set embedder model. This takes in the output of the trunk and outputs 512 dimensional embeddings\nembedder = MLP([trunk_output_size, 512]).to(device)\n\n# Set optimizers\ntrunk_optimizer = torch.optim.Adam(trunk.parameters(), lr=0.00001, weight_decay=0.0001)\nembedder_optimizer = torch.optim.Adam(embedder.parameters(), lr=0.0001, weight_decay=0.0001)\n\n# Set the image transforms\ntrain_transform = transforms.Compose([transforms.Resize(512),\n                                    transforms.RandomResizedCrop(scale=(0.16, 1), ratio=(0.75, 1.33), size=64),\n                                    transforms.RandomHorizontalFlip(0.5),\n                                    transforms.ToTensor(),\n                                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n\nval_transform = transforms.Compose([transforms.Resize(512),\n                                    transforms.ToTensor(),\n                                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <font color='brown' size=4>Transforms</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_train_transforms():\n    return albumentations.Compose(\n        [\n            albumentations.HorizontalFlip(p=0.5),\n            albumentations.VerticalFlip(p=0.5),\n            albumentations.Rotate(limit=120, p=0.8),\n            albumentations.RandomBrightness(limit=(0.09, 0.6), p=0.5),\n            albumentations.Cutout(num_holes=8, max_h_size=8, max_w_size=8, fill_value=0, always_apply=False, p=0.5),\n            albumentations.ShiftScaleRotate(\n                shift_limit=0.25, scale_limit=0.1, rotate_limit=0\n            ),\n            albumentations.Normalize(\n                [0.485, 0.456, 0.406], [0.229, 0.224, 0.225], max_pixel_value=255.0, always_apply=True\n            ),\n        \n            ToTensorV2(p=1.0),\n        ]\n    )\n\ndef get_valid_transforms():\n\n    return albumentations.Compose(\n        [albumentations.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225], max_pixel_value=255.0, always_apply=True),\n        ToTensorV2(p=1.0)\n        ]\n    )\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <font color='brown' size=4>Dataset</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"class MyDataset(Dataset):\n    \n    def __init__(self, dataframe,dim=(512,512), transform=None, test=False):\n        self.df = dataframe\n        self.transform = transform\n        self.test = test\n        self.dim=dim\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        \n        label = self.df.label.values[idx]\n        p_path = self.df.image_path.values[idx]\n        \n            \n        image = cv2.imread(p_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n#         image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n        #image = transforms.ToPILImage()(image)\n        if self.dim:\n            image = cv2.resize(image,self.dim)\n            \n        if self.transform:\n            image = self.transform(image=image)\n        \n        return image['image'], label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le = preprocessing.LabelEncoder()\ntrain_df['label'] = le.fit_transform(train_df['label_group'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data,val_data = train_test_split(train_df, test_size=0.4, random_state=42,\n                                                       stratify=train_df['label'])\n\ntrain_dataset=MyDataset(train_data,transform=get_train_transforms())\nval_dataset=MyDataset(val_data,transform=get_valid_transforms())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set the loss function, here i use triplet loss\nloss = pml_loss.TripletMarginLoss(margin=0.1)\n\n# Set the mining function\nminer = miners.MultiSimilarityMiner(epsilon=0.1)\n\n# Set the dataloader sampler\nsampler = samplers.MPerClassSampler(train_data.label, m=4, length_before_new_iter=len(train_data))\n\n# Set other training parameters\nbatch_size = 32\nnum_epochs = 1\n\n# Package the above stuff into dictionaries.\nmodels = {\"trunk\": trunk, \"embedder\": embedder}\noptimizers = {\"trunk_optimizer\": trunk_optimizer, \"embedder_optimizer\": embedder_optimizer}\nloss_funcs = {\"metric_loss\": loss}\nmining_funcs = {\"tuple_miner\": miner}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"record_keeper, _, _ = logging_presets.get_record_keeper(\"example_logs\", \"example_tensorboard\")\nhooks = logging_presets.get_hook_container(record_keeper)\ndataset_dict = {\"val\": val_dataset}\nmodel_folder = \"example_saved_models\"\n\n#To visualize the embeddings of different classes after one epoch training\ndef visualizer_hook(umapper, umap_embeddings, labels, split_name, keyname, *args):\n    logging.info(\"UMAP plot for the {} split and label set {}\".format(split_name, keyname))\n    label_set = np.unique(labels)\n    num_classes = len(label_set)\n    fig = plt.figure(figsize=(20,15))\n    plt.gca().set_prop_cycle(cycler(\"color\", [plt.cm.nipy_spectral(i) for i in np.linspace(0, 0.9, num_classes)]))\n    for i in range(num_classes):\n        idx = labels == label_set[i]\n        plt.plot(umap_embeddings[idx, 0], umap_embeddings[idx, 1], \".\", markersize=1)   \n    plt.show()\n\n# Create the tester\ntester = testers.GlobalEmbeddingSpaceTester(end_of_testing_hook = hooks.end_of_testing_hook, \n                                            visualizer = umap.UMAP(), \n                                            visualizer_hook = visualizer_hook,\n                                            dataloader_num_workers = 4)\n\nend_of_epoch_hook = hooks.end_of_epoch_hook(tester, \n                                            dataset_dict, \n                                            model_folder, \n                                            test_interval = 1,\n                                            patience = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer = trainers.MetricLossOnly(models,\n                                optimizers,\n                                batch_size,\n                                loss_funcs,\n                                mining_funcs,\n                                train_dataset,\n                                sampler=sampler,\n                                dataloader_num_workers = 4,\n                                end_of_iteration_hook = hooks.end_of_iteration_hook,\n                                end_of_epoch_hook = end_of_epoch_hook)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer.train(num_epochs=num_epochs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <font color='brown' size=4>4.3 Custom pipeline + PML</font>"},{"metadata":{},"cell_type":"markdown","source":"> **ðŸ“Œ Note**: Here we use metric learning but with our custom model with effnet backbone"},{"metadata":{},"cell_type":"markdown","source":"Reference: https://www.kaggle.com/tanulsingh077/siamese-style-training-efficient-net-b0"},{"metadata":{},"cell_type":"markdown","source":"## <font color='brown' size=4>4.4 Utils</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"DIM = (512,512)\n\nNUM_WORKERS = 4\nTRAIN_BATCH_SIZE = 16\nVALID_BATCH_SIZE = 16\nEPOCHS = 1\nSEED = 2020\nLR = 3e-4\n\n\nDEVICE = \"cuda\"\n\nMEAN = [0.485, 0.456, 0.406]\nSTD = [0.229, 0.224, 0.225]\n\n\n################################################# MODEL ####################################################################\n\nMODEL_NAME = 'efficientnet_b0' #efficientnet_b3 #efficientnetb5 #efficientnetb7\n\nSCHEDULER = 'CosineAnnealingWarmRestarts' #'CosineAnnealingLR'\nT_0=3 # CosineAnnealingWarmRestarts\nmin_lr=1e-6","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n    \n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n    \n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SiameseModel(nn.Module):\n    def __init__(self, model_name='efficientnet_b0',out_features=2,pretrained=True):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained)\n        n_features = self.model.classifier.in_features\n        \n        self.model.global_pool = nn.Identity()\n        self.model.classifier = nn.Identity()\n        self.pooling = nn.AdaptiveAvgPool2d(1)\n        self.classifier = nn.Sequential(nn.Linear(n_features, n_features//2),\n                                nn.ReLU(),\n                                nn.Linear(n_features//2, out_features)\n                                )\n\n    def forward(self, x):\n        bs = x.size(0)\n        output = self.model(x)\n        output = self.pooling(output).view(bs, -1)\n        \n        output = self.classifier(output)\n        return output","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color='brown' size=4>4.5 Engine</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_fn(dataloader,model,criterion,optimizer,device,scheduler,epoch):\n    model.train()\n    loss_score = AverageMeter()\n    \n    tk0 = tqdm(dataloader, total=len(dataloader))\n    for img_0,label in tk0:\n        \n        img_0 = img_0.to(device)\n\n        label = label.to(device)\n        \n        batch_size = img_0.shape[0]\n        \n        optimizer.zero_grad()\n        \n        output_1 = model(img_0)\n        \n        loss = criterion(output_1,label)\n        \n        loss.backward()\n        optimizer.step()\n        \n        if scheduler is not None:\n            scheduler.step()\n        \n        loss_score.update(loss.detach().item(), batch_size)\n        \n        \n        tk0.set_postfix(Train_Loss=loss_score.avg,Epoch=epoch)\n        \n    return loss_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run():\n    df=pd.read_csv('../input/shopee-product-matching/train.csv')\n    df['image_path']=df['image'].apply(lambda x: train_image_path+x)\n    df=df.sample(frac=0.5).reset_index(drop=True) #taking only sample of data\n    \n    df['label'] = le.fit_transform(df['label_group'])\n    \n    # Defining DataSet\n    train_dataset = MyDataset(df,transform=get_train_transforms())\n        \n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=TRAIN_BATCH_SIZE,\n        pin_memory=True,\n        drop_last=True,\n        num_workers=NUM_WORKERS\n    )\n    \n    # Defining Device\n    device = torch.device(\"cuda\")\n    \n    # Defining Model for specific fold\n    model = SiameseModel(model_name=MODEL_NAME,out_features=128,pretrained=False)\n    model.to(device)\n    \n    #DEfining criterion - using arcface loss\n    criterion = pml_loss.ArcFaceLoss(num_classes=11014,embedding_size=128)\n\n    optimizer = torch.optim.Adam(criterion.parameters(), lr=LR)\n    #Defining LR SCheduler\n    scheduler = CosineAnnealingWarmRestarts(optimizer,T_0=T_0)\n    \n    # THE ENGINE LOOP\n    best_loss = 10000\n    for epoch in range(EPOCHS):\n        train_loss = train_fn(train_loader, model,criterion, optimizer, device,scheduler=scheduler,epoch=epoch)\n        \n        if train_loss.avg < best_loss:\n            best_loss = train_loss.avg\n            torch.save(model.state_dict(),f'model_best_loss.bin')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### <b>Cheers, happy kaggling!!!</b>"},{"metadata":{},"cell_type":"markdown","source":"# <font color='brown' size=4>5. Acknowledgements</font>"},{"metadata":{},"cell_type":"markdown","source":"* https://www.tensorflow.org/tutorials/text/word_embeddings\n* https://www.sbert.net/docs/training/overview.html\n* https://kevinmusgrave.github.io/pytorch-metric-learning/"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}