{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import timm\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom tqdm.notebook import tqdm\nimport numpy as np \nimport pandas as pd \nimport sqlite3\nimport warnings\nimport cv2\nwarnings.filterwarnings(\"ignore\")\nimport concurrent\nimport os\nimport gc\ntorch.cuda.empty_cache()\n\ntorch.backends.cudnn.benchmark = True\n\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\n\nimport albumentations as A\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MEAN = (0.485, 0.456, 0.406)\nSTD = 0.229, 0.224, 0.225\nBATCH_SIZE = 256\nDEVICE = torch.device('cuda:0')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform = A.Compose([ A.Resize(384, 384, p=1),\n                        A.Normalize(\n                        MEAN, STD, max_pixel_value=255.0, always_apply=True),\n                               ])\ndef load_image(file_path, transform=transform):\n#     file_path = f'/kaggle/input/shopee-product-matching/train_images/{file_name}'\n\n    img = cv2.imread(file_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    tensor_img = transform(image=img)['image']\n    tensor_img = torch.tensor(tensor_img)\n    tensor_img = tensor_img.permute(( 2, 0, 1)).float()\n    \n    return tensor_img\ndef load_images(file_paths):\n    images = []\n    for file_path in file_paths:\n        image = load_image(file_path)\n        images.append(image.unsqueeze(0))\n    images = torch.cat(images)\n    return images","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Get embedding","metadata":{}},{"cell_type":"code","source":"model = timm.create_model('densenet121', pretrained=False)\nmodel.load_state_dict(torch.load('../input/shopee-model-siamese/model_epoch3_loss0.7067989706993103.pt'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.to(DEVICE)\n_ = model.eval()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_path_root = '../input/shopee-product-matching/train_images/'\ntrain = pd.read_csv('../input/shopee-product-matching/train.csv')\nimages_name = train.image.tolist()\nimages_name = np.array(list(map(lambda x: image_path_root+x, images_name)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_iter = len(images_name) // BATCH_SIZE\nembedding = []\nfor i in tqdm(range(n_iter+1)):\n    file_paths = images_name[BATCH_SIZE*i:BATCH_SIZE*(i+1)]\n    images = load_images(file_paths)\n#     print(images.shape)\n    with torch.no_grad():\n        output = model(images.to(DEVICE).float()).detach().cpu().numpy()\n        embedding.append(output)\n    del images\n    gc.collect()\n    torch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding = np.concatenate(embedding)\nembedding.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## KNN","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import NearestNeighbors","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if len(train) > 100:\n    k = 100\nelse:\n    k = len(train)\nnbrs = NearestNeighbors(n_neighbors=k, metric='cosine').fit(embedding) \n#  algorithm='brute', metric='cosine'\ndistances, indices = nbrs.kneighbors(embedding)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Make tripple","metadata":{}},{"cell_type":"code","source":"train_image = np.array(train.image.tolist())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"neighbors = {}\nfor image, label_group in tqdm(zip(train.image, train.label_group), total=len(train)):\n    neighbors[image] = train[train.label_group==label_group].image.tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tripple = pd.DataFrame(columns=['anchor', 'positives', 'negatives'])\ncount = 0\nfor i, n_indices in tqdm(enumerate(indices), total=len(indices)):\n    anchor = train_image[i]\n    all_neighbor = train_image[n_indices]\n    positives = list(filter(lambda x: x in neighbors[anchor], all_neighbor))  # to keep order/distance\n    # positives = neighbors[anchor]  # no order\n    negatives = list(filter(lambda x: x not in neighbors[anchor], all_neighbor))\n    df_tripple.loc[i] = [anchor, ' '.join(positives), ' '.join(negatives)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tripple.to_csv('df_tripple.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}