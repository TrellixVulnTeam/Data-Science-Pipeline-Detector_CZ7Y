{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Comments\n\nThanks to Chris for this great notebook https://www.kaggle.com/cdeotte/part-2-rapids-tfidfvectorizer-cv-0-700. \n\nHere is the script for the EfficientNetb3 ArcFace Model https://www.kaggle.com/ragnar123/shopee-efficientnetb3-arcmarginproduct\n\nHere is the script for the Bert Model https://www.kaggle.com/ragnar123/bert-baseline","metadata":{"_uuid":"2eded935-d518-4420-987f-69214826c48e","_cell_guid":"cb2222a4-5ef0-4e46-9538-e91a20d0a545","papermill":{"duration":0.018885,"end_time":"2021-04-02T08:25:59.682065","exception":false,"start_time":"2021-04-02T08:25:59.66318","status":"completed"},"tags":[],"trusted":true}},{"cell_type":"code","source":"!pip install ../input/external-libraries-shopee-product-matching/Keras_Applications-1.0.8-py3-none-any.whl\n!pip install ../input/external-libraries-shopee-product-matching/efficientnet-1.1.1-py3-none-any.whl\n\n# !pip install ../input/d/mhilmiasyrofi/external-libraries-shopee-product-matching/Keras_Applications-1.0.8-py3-none-any.whl\n# !pip install ../input/d/mhilmiasyrofi/external-libraries-shopee-product-matching/efficientnet-1.1.1-py3-none-any.whl","metadata":{"_uuid":"46c09037-a692-4a29-9085-dc04ba6fc8b7","_cell_guid":"63b66ad9-8182-44dc-9ff7-171ad3bbf8b3","collapsed":false,"papermill":{"duration":53.428181,"end_time":"2021-04-02T08:26:53.170217","exception":false,"start_time":"2021-04-02T08:25:59.742036","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\n!cp ../input/rapids/rapids.0.19.0 /opt/conda/envs/rapids.tar.gz\n!cd /opt/conda/envs/ && tar -xzvf rapids.tar.gz > /dev/null\nsys.path = [\"/opt/conda/envs/rapids/lib/python3.7/site-packages\"] + sys.path\nsys.path = [\"/opt/conda/envs/rapids/lib/python3.7\"] + sys.path\nsys.path = [\"/opt/conda/envs/rapids/lib\"] + sys.path \n!cp /opt/conda/envs/rapids/lib/libxgboost.so /opt/conda/lib/","metadata":{"_uuid":"a18ae121-3b87-4d12-a6bc-9f9a9393a005","_cell_guid":"0b66f1ab-a32f-491c-86c3-6b5d5926f462","collapsed":false,"papermill":{"duration":149.266195,"end_time":"2021-04-02T08:29:22.457404","exception":false,"start_time":"2021-04-02T08:26:53.191209","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport random\nimport math\nimport numpy as np\nimport pandas as pd\nimport gc\nimport matplotlib.pyplot as plt\nimport cudf\nimport cuml\nimport cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml import PCA\nfrom cuml.neighbors import NearestNeighbors\nimport tensorflow as tf\nimport efficientnet.tfkeras as efn\nfrom tqdm.notebook import tqdm\nfrom shutil import copyfile\nimport tensorflow_hub as hub\nimport sklearn\nimport torch\nfrom shutil import copyfile\n\nfrom tqdm.autonotebook import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom sklearn.preprocessing import LabelEncoder\n\nimport transformers","metadata":{"_uuid":"c792f7bc-208b-42f1-9e01-511fa7a108c8","_cell_guid":"1d301630-4d39-4206-bb88-7d03bd1c4cea","collapsed":false,"papermill":{"duration":8.955425,"end_time":"2021-04-02T08:29:31.434195","exception":false,"start_time":"2021-04-02T08:29:22.47877","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For tf.dataset\nAUTO = tf.data.experimental.AUTOTUNE\n\n# Configuration\nBATCH_SIZE = 8\nIMAGE_SIZE = [512, 512]\n# Seed\nSEED = 42\n# Verbosity\nVERBOSE = 1\n# Number of classes\nN_CLASSES = 11014\n\n# Flag to get cv score\nGET_CV = True\n# Flag to check ram allocations (debug)\nCHECK_SUB = False\n\nRECOMPUTE_IMAGE_EMBEDDING = False\nRECOMPUTE_NFNET_EMBEDDING = False\nRECOMPUTE_TEXT_EMBEDDING = False\nRECOMPUTE_TFIDF_EMBEDDING = False\nRECOMPUTE_INDOBERT_EMBEDDING = False\n\n\nKAGGLE_ENV = True\n\nINPUT_DIR = \"../input/\"\nWORK_DIR = \"../working/\"","metadata":{"_uuid":"584e4bd5-f3ea-4972-94c8-cc59bd6f9e1a","_cell_guid":"01e0357d-8c39-4f8e-a1ff-9b49b1b8ce82","collapsed":false,"papermill":{"duration":0.031777,"end_time":"2021-04-02T08:29:31.486416","exception":false,"start_time":"2021-04-02T08:29:31.454639","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"copyfile(src = INPUT_DIR + 'external-modules-shopee-product-matching/utils.py', dst = WORK_DIR + 'utils.py')\n\nfrom utils import clean_text","metadata":{"_uuid":"8b39731c-8c3f-47b7-9f32-06e3a707d4dd","_cell_guid":"6d10e008-2288-4ce8-bc67-1280be8846ea","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"copyfile(src = INPUT_DIR + 'external-modules-shopee-product-matching/tokenization.py', dst = WORK_DIR + 'tokenization.py')\n\nimport tokenization","metadata":{"_uuid":"9482f7a5-6a33-4f7b-a173-940dc38de82b","_cell_guid":"3dbdf917-2c19-4b34-a923-37322c6d8d25","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RESTRICT TENSORFLOW TO 2GB OF GPU RAM\n# SO THAT WE HAVE 14GB RAM FOR RAPIDS\nLIMIT = 2.0\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        tf.config.experimental.set_virtual_device_configuration(\n            gpus[0],\n            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*LIMIT)])\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n        #print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n    except RuntimeError as e:\n        print(e)\nprint('We will restrict TensorFlow to max %iGB GPU RAM'%LIMIT)\nprint('then RAPIDS can use %iGB GPU RAM'%(16-LIMIT))","metadata":{"_uuid":"0e16fd88-c157-47f6-ab81-5666b7bcf3b8","_cell_guid":"6ac6377a-c4ce-4459-b755-3cb9a8bf44a7","collapsed":false,"papermill":{"duration":1.425091,"end_time":"2021-04-02T08:29:32.931743","exception":false,"start_time":"2021-04-02T08:29:31.506652","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(INPUT_DIR + 'shopee-product-matching/test.csv')\n# If we are comitting, replace train set for test set and dont get cv\nif len(df) > 3:\n    GET_CV = False\n    RECOMPUTE_IMAGE_EMBEDDING = True\n    RECOMPUTE_TEXT_EMBEDDING = True\ndel df\n\n# Function to get our f1 score\ndef f1_score(y_true, y_pred):\n    y_true = y_true.apply(lambda x: set(x.split()))\n    y_pred = y_pred.apply(lambda x: set(x.split()))\n    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n    len_y_pred = y_pred.apply(lambda x: len(x)).values\n    len_y_true = y_true.apply(lambda x: len(x)).values\n    f1 = 2 * intersection / (len_y_pred + len_y_true)\n    return f1\n\n# Function to combine predictions\ndef combine_predictions(row1, row2):\n    x = np.concatenate([row1, row2])\n    return np.unique(x)\n\n# Function to combine predictions\ndef aggregate_predictions(preds1, preds2): \n    \n    memory = {}\n    connections = {}\n    \n    def add_to_memory(idx, value):\n        if value != \"\":\n            if value in memory:\n                memory[value].add(idx)\n                return\n            memory[value] = {idx}\n            \n    combineds = []\n\n    for i, pred in enumerate(preds1) :\n        combineds.append(set())\n        for v in pred :\n            add_to_memory(i, v)\n            combineds[i].add(v)\n\n    for i, pred in enumerate(preds2) :\n        for v in pred :\n            add_to_memory(i, v)\n            combineds[i].add(v)\n    \n    for ids in memory.values():\n        current_connection = set(ids)\n\n        for uid in ids:\n            if uid in connections:\n                current_connection.update(connections[uid])\n\n        for uid in current_connection:\n            connections[uid] = current_connection\n            \n    del memory\n    \n    connections = sorted(connections.items())\n    res = []\n    for i in range(len(combineds)):\n        combined = set()\n        for idx in connections[i][1] :\n            combined.update(combineds[idx])\n        res.append(list(combined))\n\n    del connections\n    \n    gc.collect()\n    \n    return res\n\ndef reformat_labels(arr): \n    \"\"\"\n    Convert arrray of strings into concatenated strings separated with space\n    \"\"\"\n    return ' '.join(np.unique(arr))","metadata":{"_uuid":"d12b5607-8ca5-4d6d-b923-0f0e35a47c55","_cell_guid":"b9bef495-704c-4e88-98d7-3f3e708b4a8c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to read out dataset\ndef read_dataset():\n    data_type = \"test\"\n    if GET_CV :\n        data_type = \"train\"\n        \n    df = pd.read_csv(INPUT_DIR + 'shopee-product-matching/' + data_type + '.csv')\n    \n    df[\"title\"] = df[\"title\"].apply(clean_text)\n    \n    if GET_CV :\n        tmp = df.groupby(['label_group'])['posting_id'].unique().to_dict()\n        df['matches'] = df['label_group'].map(tmp)\n        df['matches'] = df['matches'].apply(lambda x: ' '.join(x))\n        if CHECK_SUB:\n            df = pd.concat([df, df], axis = 0)\n            df.reset_index(drop = True, inplace = True)\n    \n    df_cu = cudf.DataFrame(df)\n    image_paths = INPUT_DIR + 'shopee-product-matching/' + data_type + '_images/' + df['image']\n        \n    return df, df_cu, image_paths","metadata":{"_uuid":"de09019d-467b-4fc8-b4ef-9cf98b154350","_cell_guid":"a6bf9a2b-c706-45e9-aa11-0f29529976c9","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to decode our images\ndef decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels = 3)\n    image = tf.image.resize(image, IMAGE_SIZE)\n    image = tf.cast(image, tf.float32) / 255.0\n    return image\n\n# Function to read our test image and return image\ndef read_image(image):\n    image = tf.io.read_file(image)\n    image = decode_image(image)\n    return image\n\n# Function to get our dataset that read images\ndef get_dataset(image):\n    dataset = tf.data.Dataset.from_tensor_slices(image)\n    dataset = dataset.map(read_image, num_parallel_calls = AUTO)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\n# Arcmarginproduct class keras layer\nclass ArcMarginProduct(tf.keras.layers.Layer):\n    '''\n    Implements large margin arc distance.\n\n    Reference:\n        https://arxiv.org/pdf/1801.07698.pdf\n        https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/\n            blob/master/src/modeling/metric_learning.py\n    '''\n    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n                 ls_eps=0.0, **kwargs):\n\n        super(ArcMarginProduct, self).__init__(**kwargs)\n\n        self.n_classes = n_classes\n        self.s = s\n        self.m = m\n        self.ls_eps = ls_eps\n        self.easy_margin = easy_margin\n        self.cos_m = tf.math.cos(m)\n        self.sin_m = tf.math.sin(m)\n        self.th = tf.math.cos(math.pi - m)\n        self.mm = tf.math.sin(math.pi - m) * m\n\n    def get_config(self):\n\n        config = super().get_config().copy()\n        config.update({\n            'n_classes': self.n_classes,\n            's': self.s,\n            'm': self.m,\n            'ls_eps': self.ls_eps,\n            'easy_margin': self.easy_margin,\n        })\n        return config\n\n    def build(self, input_shape):\n        super(ArcMarginProduct, self).build(input_shape[0])\n\n        self.W = self.add_weight(\n            name='W',\n            shape=(int(input_shape[0][-1]), self.n_classes),\n            initializer='glorot_uniform',\n            dtype='float32',\n            trainable=True,\n            regularizer=None)\n\n    def call(self, inputs):\n        X, y = inputs\n        y = tf.cast(y, dtype=tf.int32)\n        cosine = tf.matmul(\n            tf.math.l2_normalize(X, axis=1),\n            tf.math.l2_normalize(self.W, axis=0)\n        )\n        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = tf.where(cosine > 0, phi, cosine)\n        else:\n            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n        one_hot = tf.cast(\n            tf.one_hot(y, depth=self.n_classes),\n            dtype=cosine.dtype\n        )\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.s\n        return output","metadata":{"_uuid":"c8c4b032-7ee7-4f28-b6e5-e75eb335b099","_cell_guid":"c8eccfb3-bf38-4f0b-865f-8e699139a259","collapsed":false,"papermill":{"duration":0.07729,"end_time":"2021-04-02T08:29:33.030437","exception":false,"start_time":"2021-04-02T08:29:32.953147","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def call_function_by_model_name(model_name): \n    return {\n        \"EfficientNetB3\": efn.EfficientNetB3,\n        \"EfficientNetB4\": efn.EfficientNetB4,\n        \"EfficientNetB5\": efn.EfficientNetB5,\n        \"EfficientNetB6\": efn.EfficientNetB6,\n        \"EfficientNetB7\": efn.EfficientNetB7\n    }[model_name]\n\n# Function to get the embeddings of our images with the fine-tuned model\ndef get_image_embeddings(image_paths, model_name=\"EfficientNetB3\"):\n    embeds = []\n    \n    margin = ArcMarginProduct(\n            n_classes = N_CLASSES, \n            s = 30, \n            m = 0.7, \n            name='head/arc_margin', \n            dtype='float32'\n            )\n\n    inp = tf.keras.layers.Input(shape = (*IMAGE_SIZE, 3), name = 'inp1')\n    label = tf.keras.layers.Input(shape = (), name = 'inp2')\n    x = call_function_by_model_name(model_name)(weights = None, include_top = False)(inp)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = margin([x, label])\n        \n    output = tf.keras.layers.Softmax(dtype='float32')(x)\n\n    model = tf.keras.models.Model(inputs = [inp, label], outputs = [output])\n    model.load_weights(INPUT_DIR + 'external-models-shopee-product-matching/' + model_name + '_512_42.h5')\n    model = tf.keras.models.Model(inputs = model.input[0], outputs = model.layers[-4].output)\n    chunk = 5000\n    iterator = np.arange(np.ceil(len(df) / chunk))\n    for j in iterator:\n        a = int(j * chunk)\n        b = int((j + 1) * chunk)\n        image_dataset = get_dataset(image_paths[a:b])\n        image_embeddings = model.predict(image_dataset)\n        embeds.append(image_embeddings)\n    del model\n    image_embeddings = np.concatenate(embeds)\n    print(f'Our image embeddings shape is {image_embeddings.shape}')\n    del embeds\n    gc.collect()\n    return image_embeddings","metadata":{"_uuid":"3fa6367a-2d70-49c1-9f2d-4e925a5c47a5","_cell_guid":"d11fa666-f73b-4785-b32f-f9a62bbd6000","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Return tokens, masks and segments from a text array or series\ndef bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n\n# Function to get our text title embeddings using a pre-trained bert model\ndef get_text_embeddings(df, batch_size=32, max_len = 128):\n    embeds = []\n    module_url = INPUT_DIR + 'external-models-shopee-product-matching/bert_en_uncased_L-24_H-1024_A-16_1'\n    bert_layer = hub.KerasLayer(module_url, trainable = True)\n    vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n    do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n    tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n    text = bert_encode(df['title'].values, tokenizer, max_len = max_len)\n    \n    margin = ArcMarginProduct(\n            n_classes = 11014, \n            s = 30, \n            m = 0.5, \n            name='head/arc_margin', \n            dtype='float32'\n            )\n    \n    input_word_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n    label = tf.keras.layers.Input(shape = (), name = 'label')\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    x = margin([clf_output, label])\n    output = tf.keras.layers.Softmax(dtype='float32')(x)\n    model = tf.keras.models.Model(inputs = [input_word_ids, input_mask, segment_ids, label], outputs = [output])\n    \n    model.load_weights(INPUT_DIR + 'external-models-shopee-product-matching/Bert_42.h5')\n    model = tf.keras.models.Model(inputs = model.input[0:3], outputs = model.layers[-4].output)\n    chunk = 5000\n    iterator = np.arange(np.ceil(len(df) / chunk))\n    for j in iterator:\n        a = int(j * chunk)\n        b = int((j + 1) * chunk)\n        text_chunk = ((text[0][a:b], text[1][a:b], text[2][a:b]))\n        text_embeddings = model.predict(text_chunk, batch_size = batch_size)\n        embeds.append(text_embeddings)\n    del model\n    text_embeddings = np.concatenate(embeds)\n    print(f'Our text embeddings shape is {text_embeddings.shape}')\n    del embeds\n    gc.collect()\n    return text_embeddings","metadata":{"_uuid":"33d1fcb4-054e-429e-9524-60497453906a","_cell_guid":"fb8c77fb-5701-4b3b-b296-799eafa07cb2","collapsed":false,"papermill":{"duration":0.046046,"end_time":"2021-04-02T08:29:33.097605","exception":false,"start_time":"2021-04-02T08:29:33.051559","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed=SEED):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","metadata":{"_uuid":"7535a195-1526-45dc-afaf-aabcad26075c","_cell_guid":"6d4d5610-6b01-44d7-b2a5-0e625deb8a36","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df, df_cu, image_paths = read_dataset()","metadata":{"_uuid":"563c3a96-fe66-452f-b920-a0a84529cf3f","_cell_guid":"c560daec-6398-4cb6-ada1-d7cf13eb8f58","collapsed":false,"papermill":{"duration":3.531351,"end_time":"2021-04-02T08:29:36.717807","exception":false,"start_time":"2021-04-02T08:29:33.186456","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Image Phash","metadata":{"_uuid":"7a780de5-7a24-4cb2-8a1d-92e5637a3e58","_cell_guid":"0ad2cfd2-507a-45d8-981c-cc3e945c89d1","trusted":true}},{"cell_type":"code","source":"tmp = df.groupby('image_phash').posting_id.agg('unique').to_dict()\ndf['phash_predictions'] = df.image_phash.map(tmp)","metadata":{"_uuid":"16e9535f-4bf5-4a3a-9396-c1b797e0ae1e","_cell_guid":"fd6fdccf-5d6e-4b11-8562-460392d80d46","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if GET_CV:\n    df['f1'] = f1_score(df['matches'], df['phash_predictions'].apply(lambda x: ' '.join( np.unique(x) )))\n    print('CV score for baseline =', df.f1.mean())","metadata":{"_uuid":"9995cc75-0bd8-47c6-999c-d36099db932a","_cell_guid":"2accaaaf-27a7-452a-a241-226dbb7101f1","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Image Embeddings","metadata":{"_uuid":"26b042ca-62c9-49cd-a3e1-54114d86757a","_cell_guid":"42564052-f3cc-4f63-abbc-5f55d7a90602","trusted":true}},{"cell_type":"code","source":"set_seed(42)\nimage_model_name = \"EfficientNetB3\"\n\nif GET_CV :\n    image_embedding_path = INPUT_DIR + 'external-embeddings-shopee-product-matching/image_embeddings_' + image_model_name\n    if RECOMPUTE_IMAGE_EMBEDDING :\n        image_embeddings = get_image_embeddings(image_paths, image_model_name)\n        if KAGGLE_ENV : \n            np.save('image_embeddings', image_embeddings)\n        else :\n            np.save(image_embedding_path, image_embeddings)\n    else :\n        image_embeddings = np.load(image_embedding_path + \".npy\")\nelse :\n    image_embeddings = get_image_embeddings(image_paths, image_model_name)","metadata":{"_uuid":"76c19201-c6e3-4073-af21-bac236ceabba","_cell_guid":"353db060-c318-4876-ad82-39654fc4ecdc","collapsed":false,"papermill":{"duration":4.227596,"end_time":"2021-04-02T08:29:40.967052","exception":false,"start_time":"2021-04-02T08:29:36.739456","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text Embeddings","metadata":{"_uuid":"aba5d912-d5f2-44bf-9d5e-387d0b30fab0","_cell_guid":"cb20c821-dc7b-49d2-aac2-9a847402a4b3","trusted":true}},{"cell_type":"code","source":"set_seed(42)\n\nif GET_CV :\n    text_embedding_path = INPUT_DIR + 'external-embeddings-shopee-product-matching/text_embeddings_Bert42'\n    if RECOMPUTE_TEXT_EMBEDDING :\n        text_embeddings = get_text_embeddings(df)\n        if KAGGLE_ENV : \n            np.save('text_embeddings', text_embeddings)\n        else :\n            np.save(text_embedding_path, text_embeddings)\n    else :\n        text_embeddings = np.load(text_embedding_path + '.npy')\nelse :\n    text_embeddings = get_text_embeddings(df)","metadata":{"_uuid":"0be68a7c-4627-40fc-aec8-b3722fdca18d","_cell_guid":"9a8b676f-7d71-4d3d-956f-c6f2f1253621","collapsed":false,"papermill":{"duration":379.864357,"end_time":"2021-04-02T08:36:00.853117","exception":false,"start_time":"2021-04-02T08:29:40.98876","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"_uuid":"55941984-fea7-44ab-bae3-679863294452","_cell_guid":"94ceba2a-9a48-4340-9ea4-56f96ca74d88","collapsed":false,"papermill":{"duration":1.27765,"end_time":"2021-04-02T08:36:02.153499","exception":false,"start_time":"2021-04-02T08:36:00.875849","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Function to get 50 nearest neighbors of each image and apply a distance threshold to maximize cv\ndef get_distances_indices(embeddings, KNN=50, normalize=False, metric='cosine'):\n    if metric:\n        model = NearestNeighbors(n_neighbors = KNN, metric=metric)\n    else :\n        model = NearestNeighbors(n_neighbors = KNN)\n    model.fit(embeddings)\n    distances, indices = model.kneighbors(embeddings)\n    if normalize: distances = sklearn.preprocessing.normalize(distances)\n    del model\n    gc.collect()\n    return distances, indices\n    \ndef get_neighbors_from_distance_indices(df, distances, indices, thresholds, choosen_threshold) :\n    # Iterate through different thresholds to maximize cv, run this in interactive mode, then replace else clause with a solid threshold\n    if GET_CV:\n#         scores = []\n#         for threshold in thresholds:\n#             predictions = []\n#             for k in range(indices.shape[0]):\n#                 idx = np.where(distances[k,] < threshold)[0]\n#                 ids = indices[k,idx]\n#                 posting_ids = ' '.join(df['posting_id'].iloc[ids].values)\n#                 predictions.append(posting_ids)\n#             df['pred_matches'] = predictions\n#             df['f1'] = f1_score(df['matches'], df['pred_matches'])\n#             score = df['f1'].mean()\n#             print(f'Our f1 score for threshold {threshold} is {score}')\n#             scores.append(score)\n#             df = df.drop(columns=['pred_matches','f1'])\n#             del predictions\n#             gc.collect()\n#         thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\n#         max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n#         best_threshold = max_score['thresholds'].values[0]\n#         best_score = max_score['scores'].values[0]\n#         print(f'Our best score is {best_score} and has a threshold {best_threshold}')\n        \n#         # Use threshold\n        predictions = []\n        for k in range(indices.shape[0]):\n            # Because we are predicting the test set that have 70K images and different label groups, confidence should be smaller\n            idx = np.where(distances[k,] < choosen_threshold)[0]\n            ids = indices[k,idx]\n            posting_ids = df['posting_id'].iloc[ids].values\n            predictions.append(posting_ids)\n            \n        # Modified\n#         predictions = []\n#         for k in range(indices.shape[0]):\n#             dist = distances[k,]\n#             posting_ids = np.array([])\n#             for threshold in np.arange(choosen_threshold, choosen_threshold + 0.03, 0.005):\n#                 if posting_ids.shape[0] <= 1:\n#                     idx = np.where(dist < choosen_threshold)[0]\n#                     ids = indices[k,idx]\n#                     posting_ids = df['posting_id'].iloc[ids].values\n#             predictions.append(posting_ids)\n\n    \n    # Because we are predicting the test set that have 70K images and different label groups, confidence should be smaller\n    else:\n        predictions = []\n        for k in tqdm(range(indices.shape[0])):\n            idx = np.where(distances[k,] < choosen_threshold)[0]\n            ids = indices[k,idx]\n            posting_ids = df['posting_id'].iloc[ids].values\n            predictions.append(posting_ids)\n#         predictions = []\n#         for k in range(indices.shape[0]):\n#             dist = distances[k,]\n#             posting_ids = np.array([])\n#             for threshold in np.arange(choosen_threshold, choosen_threshold + 0.03, 0.005):\n#                 if posting_ids.shape[0] <= 1:\n#                     idx = np.where(dist < choosen_threshold)[0]\n#                     ids = indices[k,idx]\n#                     posting_ids = df['posting_id'].iloc[ids].values\n#             predictions.append(posting_ids)\n        \n    del distances, indices\n    gc.collect()\n    return df, predictions","metadata":{"_uuid":"ea3576ed-d172-472f-85c0-a5ff943ff1a6","_cell_guid":"442fa694-541b-41a8-9403-1edf7eac5e88","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NORMALIZE_IMAGE_DISTANCE = True\nNORMALIZE_TEXT_DISTANCE = True\n\nKNN=100","metadata":{"_uuid":"db764821-e908-4544-902a-115fe5b5cc09","_cell_guid":"21a501f7-727d-4b9c-8c0c-898aeca043a6","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nGet neighbors for image_embeddings\n\"\"\"\n\n## calculate image distances and get indices\nimage_distances, image_indices = get_distances_indices(image_embeddings, KNN=KNN, normalize=NORMALIZE_IMAGE_DISTANCE, metric='cosine')\n    \nif NORMALIZE_IMAGE_DISTANCE :\n    ## threshold for normalized image distances\n    image_thresholds = list(np.arange(0.03, 0.08, 0.005))\n    choosen_image_threshold=0.035\n\ndf, image_predictions = get_neighbors_from_distance_indices(df, image_distances, image_indices, thresholds=image_thresholds, choosen_threshold=choosen_image_threshold)\n","metadata":{"_uuid":"efe20641-ced7-4ab7-87ff-56b0a46056ae","_cell_guid":"9a6b2724-f877-40e0-a1ec-40d56c80ae25","collapsed":false,"papermill":{"duration":58.80761,"end_time":"2021-04-02T08:37:00.98378","exception":false,"start_time":"2021-04-02T08:36:02.17617","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nGet neighbors for text_embeddings\n\"\"\"\n\n## calculate text distances and get indices\ntext_distances, text_indices = get_distances_indices(text_embeddings, KNN=KNN, normalize=NORMALIZE_TEXT_DISTANCE)\n\nif NORMALIZE_TEXT_DISTANCE :\n    ## threshold for normalized embeddings\n    text_thresholds = list(np.arange(0.03, 0.08, 0.005))\n    choosen_text_threshold = 0.035\n    \ndf, text_predictions  = get_neighbors_from_distance_indices(df, text_distances, text_indices, thresholds=text_thresholds, choosen_threshold=choosen_text_threshold)","metadata":{"_uuid":"21c502b6-6a84-43e9-970f-d6a883261265","_cell_guid":"3073b2e1-a563-4402-a763-42ad5c6687b5","collapsed":false,"papermill":{"duration":92.104929,"end_time":"2021-04-02T08:38:33.117087","exception":false,"start_time":"2021-04-02T08:37:01.012158","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\nfrom cuml import PCA\nfrom cuml.neighbors import NearestNeighbors\nimport tensorflow as tf\nimport efficientnet.tfkeras as efn\n\nimport tensorflow_hub as hub\n\nimport os\nimport cv2\nimport random\nfrom tqdm import tqdm\n\nimport albumentations\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torch\nimport timm\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset,DataLoader\nimport fasttext as ft","metadata":{"_uuid":"82199c37-4e66-4f6e-b55f-6b1c5d52c9e9","_cell_guid":"eb808ca6-4b29-40e9-b83c-9f2c2e30f734","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import albumentations as A \nfrom albumentations.pytorch.transforms import ToTensorV2","metadata":{"_uuid":"cb9a52fd-6027-407c-ae4d-aba95bab11b2","_cell_guid":"e298390f-2a27-4a48-b799-d21263c015e6","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    \n    img_size = 512\n    batch_size = 12\n    seed = 2020\n    \n    device = 'cuda'\n    classes = 11014\n    \n    model_name = 'eca_nfnet_l0'\n    model_path = '../input/shopee-pytorch-models/arcface_512x512_nfnet_l0 (mish).pt'\n    \n    scale = 30 \n    margin = 0.5","metadata":{"_uuid":"1a906c54-2750-404b-b3a7-cde7a0317991","_cell_guid":"d8e8cc45-f47d-45dc-97de-f1900a658e53","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"set_seed(CFG.seed)","metadata":{"_uuid":"874362a4-6e32-4b84-b97e-84df6b8583e9","_cell_guid":"a6fb7af6-cb85-4675-a1e9-37e862fc8830","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_test_transforms():\n\n    return A.Compose(\n        [\n            A.Resize(CFG.img_size,CFG.img_size,always_apply=True),\n            A.Normalize(),\n        ToTensorV2(p=1.0)\n        ]\n    )","metadata":{"_uuid":"3c31521d-be92-4f6f-bfa5-96c3d6dda8e3","_cell_guid":"609e4b82-a178-4b13-a4be-5da2c82198e8","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ShopeeDataset(Dataset):\n    def __init__(self, image_paths, transforms=None):\n\n        self.image_paths = image_paths\n        self.augmentations = transforms\n\n    def __len__(self):\n        return self.image_paths.shape[0]\n\n    def __getitem__(self, index):\n        image_path = self.image_paths[index]\n        \n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.augmentations:\n            augmented = self.augmentations(image=image)\n            image = augmented['image']       \n    \n        return image,torch.tensor(1)","metadata":{"_uuid":"76472f71-ff3b-408a-939e-2eb2c1000d22","_cell_guid":"d3a86b4e-9da7-47b4-90aa-8121f7574a17","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ArcMarginProduct_Image(nn.Module):\n    def __init__(self, in_features, out_features, scale=30.0, margin=0.50, easy_margin=False, ls_eps=0.0):\n        super(ArcMarginProduct_Image, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.scale = scale\n        self.margin = margin\n        self.ls_eps = ls_eps  # label smoothing\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(margin)\n        self.sin_m = math.sin(margin)\n        self.th = math.cos(math.pi - margin)\n        self.mm = math.sin(math.pi - margin) * margin\n        \n    def forward(self, input, label):\n        # --------------------------- cos(theta) & phi(theta) ---------------------------\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n        # --------------------------- convert label to one-hot ---------------------------\n        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n        one_hot = torch.zeros(cosine.size(), device='cuda')\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.scale\n        \n        return output","metadata":{"_uuid":"21705d96-5984-4932-8900-43d5ab5d615d","_cell_guid":"2584036b-8a0f-4b9f-a39c-fc716dbb9926","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ShopeeModel(nn.Module):\n\n    def __init__(\n        self,\n        n_classes = CFG.classes,\n        model_name = CFG.model_name,\n        fc_dim = 512,\n        margin = CFG.margin,\n        scale = CFG.scale,\n        use_fc = True,\n        pretrained = False):\n\n\n        super(ShopeeModel,self).__init__()\n        print('Building Model Backbone for {} model'.format(model_name))\n        \n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n\n        if model_name == 'resnext50_32x4d':\n            final_in_features = self.backbone.fc.in_features\n            self.backbone.fc = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n\n        elif model_name == 'efficientnet_b3':\n            final_in_features = self.backbone.classifier.in_features\n            self.backbone.classifier = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n\n        elif model_name == 'tf_efficientnet_b5_ns':\n            final_in_features = self.backbone.classifier.in_features\n            self.backbone.classifier = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n            \n        elif model_name == 'eca_nfnet_l0':\n            final_in_features = self.backbone.head.fc.in_features\n            self.backbone.head.fc = nn.Identity()\n            self.backbone.head.global_pool = nn.Identity()\n\n        self.pooling =  nn.AdaptiveAvgPool2d(1)\n\n        self.use_fc = use_fc\n\n        self.dropout = nn.Dropout(p=0.0)\n        self.fc = nn.Linear(final_in_features, fc_dim)\n        self.bn = nn.BatchNorm1d(fc_dim)\n        self._init_params()\n        final_in_features = fc_dim\n\n        self.final = ArcMarginProduct_Image(\n            final_in_features,\n            n_classes,\n            scale = scale,\n            margin = margin,\n            easy_margin = False,\n            ls_eps = 0.0\n        )\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n        \n    def forward(self, image, label):\n        feature = self.extract_feat(image)\n        #logits = self.final(feature,label)\n        return feature\n\n    def extract_feat(self, x):\n        batch_size = x.shape[0]\n        x = self.backbone(x)\n        x = self.pooling(x).view(batch_size, -1)\n\n        if self.use_fc:\n            x = self.dropout(x)\n            x = self.fc(x)\n            x = self.bn(x)\n        return x","metadata":{"_uuid":"322085a2-444d-4a4d-bded-1f2ab0df1127","_cell_guid":"0be80643-f698-47b2-a921-c5ab4da826f8","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Mish_func(torch.autograd.Function):\n    \n    \"\"\"from: https://github.com/tyunist/memory_efficient_mish_swish/blob/master/mish.py\"\"\"\n    \n    @staticmethod\n    def forward(ctx, i):\n        result = i * torch.tanh(F.softplus(i))\n        ctx.save_for_backward(i)\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        i = ctx.saved_variables[0]\n  \n        v = 1. + i.exp()\n        h = v.log() \n        grad_gh = 1./h.cosh().pow_(2)\n        \n        # Note that grad_hv * grad_vx = sigmoid(x)\n        #grad_hv = 1./v  \n        #grad_vx = i.exp()\n        \n        grad_hx = i.sigmoid()\n\n        grad_gx = grad_gh *  grad_hx #grad_hv * grad_vx \n        \n        grad_f =  torch.tanh(F.softplus(i)) + i * grad_gx \n        \n        return grad_output * grad_f \n    \nclass Mish(nn.Module):\n    def __init__(self, **kwargs):\n        super().__init__()\n        pass\n    def forward(self, input_tensor):\n        return Mish_func.apply(input_tensor)\n    \n    \ndef replace_activations(model, existing_layer, new_layer):\n    \n    \"\"\"A function for replacing existing activation layers\"\"\"\n    \n    for name, module in reversed(model._modules.items()):\n        if len(list(module.children())) > 0:\n            model._modules[name] = replace_activations(module, existing_layer, new_layer)\n\n        if type(module) == existing_layer:\n            layer_old = module\n            layer_new = new_layer\n            model._modules[name] = layer_new\n    return model","metadata":{"_uuid":"6ef469da-3ef1-4047-842b-1ec2e042aa57","_cell_guid":"2791a3eb-fad4-4658-865f-aed27041a101","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_image_embeddings1(image_paths, model_name = CFG.model_name):\n    embeds = []\n    \n    model = ShopeeModel(model_name = model_name)\n    model.eval()\n    \n    if model_name == 'eca_nfnet_l0':\n        model = replace_activations(model, torch.nn.SiLU, Mish())\n\n    model.load_state_dict(torch.load(CFG.model_path))\n    model = model.to(CFG.device)\n    \n\n    image_dataset = ShopeeDataset(image_paths=image_paths,transforms=get_test_transforms())\n    image_loader = torch.utils.data.DataLoader(\n        image_dataset,\n        batch_size=CFG.batch_size,\n        pin_memory=True,\n        drop_last=False,\n        num_workers=4\n    )\n    \n    with torch.no_grad():\n        for img,label in tqdm(image_loader): \n            img = img.cuda()\n            label = label.cuda()\n            feat = model(img,label)\n            image_embeddings = feat.detach().cpu().numpy()\n            embeds.append(image_embeddings)\n    \n    \n    del model\n    \n    image_embeddings = np.concatenate(embeds)\n    print(f'Our image embeddings shape is {image_embeddings.shape}')\n    del embeds\n    gc.collect()\n    return image_embeddings","metadata":{"_uuid":"9cccc329-b475-4f99-8377-c9def87c699b","_cell_guid":"61afe07b-2d0f-459d-aa63-ef5f0fc87731","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_image_predictions(df, embeddings,threshold = 0.0):\n    \n    if len(df) > 3:\n        KNN = 100\n    else : \n        KNN = 3\n    \n    model = NearestNeighbors(n_neighbors = KNN, metric = 'cosine')\n    model.fit(embeddings)\n    distances, indices = model.kneighbors(embeddings)\n    \n    predictions = []\n    for k in tqdm(range(embeddings.shape[0])):\n        idx = np.where(distances[k,] < threshold)[0]\n        ids = indices[k,idx]\n        posting_ids = df['posting_id'].iloc[ids].values\n        predictions.append(posting_ids)\n        \n    del model, distances, indices\n    gc.collect()\n    return predictions","metadata":{"_uuid":"c38e0bd5-eec5-4f14-9665-0724feb5b662","_cell_guid":"23898dbf-db1a-410c-8b78-75c73b8b6a3d","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_image,df_image_cu,image_paths = read_dataset()\ndf_image.head()","metadata":{"_uuid":"a8882778-6b5c-47e6-9a9e-e118b59ab190","_cell_guid":"c747b5e3-1ab9-44b6-8f13-e706f6c50d32","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if GET_CV :\n    nfnet_embedding_path = INPUT_DIR + 'external-embeddings-shopee-product-matching/image_embeddings_nfnet'\n    if RECOMPUTE_NFNET_EMBEDDING :\n        nfnet_embeddings = get_image_embeddings1(image_paths.values)\n        if KAGGLE_ENV : \n            np.save('image_embeddings_nfnet', nfnet_embeddings)\n        else :\n            np.save(nfnet_embedding_path, nfnet_embeddings)\n    else :\n        nfnet_embeddings = np.load(nfnet_embedding_path + '.npy')\nelse :\n    nfnet_embeddings = get_image_embeddings1(image_paths.values)","metadata":{"_uuid":"23dad395-8656-4424-ad5a-5985b9a4a10c","_cell_guid":"17a686c9-6e8e-4962-b2a8-b52b6684af6d","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## calculate image distances and get indices\n# nfnet_distances, nfnet_indices = get_distances_indices(nfnet_embeddings, KNN=KNN, normalize=False, metric='cosine')\nnfnet_distances, nfnet_indices = get_distances_indices(nfnet_embeddings, KNN=KNN, normalize=True, metric='cosine')\n\n## threshold for normalized image distances\nnfnet_thresholds = list(np.arange(0.03, 0.05, 0.005))\nchoosen_nfnet_threshold = 0.045\n\ndf, nfnet_predictions = get_neighbors_from_distance_indices(df, nfnet_distances, nfnet_indices, thresholds=nfnet_thresholds, choosen_threshold=choosen_nfnet_threshold)","metadata":{"_uuid":"5469a27f-9cc2-4673-995f-6892f08c35de","_cell_guid":"7bef8aa5-ffc6-4014-881e-299ba8c289e4","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cuml\nfrom cuml.feature_extraction.text import TfidfVectorizer\n\ndef get_tfidf_embeddings(df_cu):\n    model = TfidfVectorizer(stop_words=None, binary=True, use_idf=True, max_features=25000)\n#     model = TfidfVectorizer(stop_words=None, binary=True, ngram_range=(1,2), use_idf=True, max_features=25000)\n    tfidf_embeddings = model.fit_transform(df_cu.title).toarray()\n    print('text embeddings shape',tfidf_embeddings.shape)\n    del model\n    gc.collect()\n    return tfidf_embeddings\n\ntfidf_embeddings = get_tfidf_embeddings(df_cu)","metadata":{"_uuid":"2257e741-aad9-4584-92aa-02f2e012a8ee","_cell_guid":"e03956c0-2634-45a1-89fa-a076e0063eb1","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CHECK_SUB = True\nif GET_CV and not CHECK_SUB:\n    tfidf_preds_path = INPUT_DIR + 'external-embeddings-shopee-product-matching/tfidf_preds.npy'\n    tfidf_preds = np.load(tfidf_preds_path, allow_pickle=True)\nelse :\n    tfidf_preds = []\n    tfidf_low_preds = []\n    CHUNK = 1024*4\n\n    print('Finding similar titles...')\n    CTS = len(df_cu)//CHUNK\n    if len(df_cu)%CHUNK!=0: CTS += 1\n    for j in range( CTS ):\n\n        a = j*CHUNK\n        b = (j+1)*CHUNK\n        b = min(b,len(df_cu))\n        print('chunk',a,'to',b)\n\n        # COSINE SIMILARITY DISTANCE\n        cts = cupy.matmul(tfidf_embeddings, tfidf_embeddings[a:b].T).T\n\n        for k in range(b-a):\n            llll = cts[k,]\n            o = np.array([])\n            for ii in np.arange(0.775,0.50, -0.02):\n                if ii>0.5 and o.shape[0] <= 1:\n                    IDX = cupy.where(llll>ii)[0]\n                    o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n            tfidf_preds.append(o)\n            \n        for k in range(b-a):\n            IDX = cupy.where(cts[k,]>0.1)[0]\n            o = df_cu.iloc[cupy.asnumpy(IDX)].posting_id.to_pandas().values\n            tfidf_low_preds.append(cupy.asnumpy(o))\n\n    del tfidf_embeddings\n\n    gc.collect()","metadata":{"_uuid":"bd5e8e27-ca44-4916-847f-95e1b8c6289a","_cell_guid":"1507eb54-3a07-43df-806f-a7646f26fe9d","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_cu['oof_text'] = tfidf_preds","metadata":{"_uuid":"0c924e5b-d73a-428c-a1d2-6ae28ded8c41","_cell_guid":"cae906ae-e680-4176-a254-c803e7bbca8f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"USE_PHASH_PREDICTION = False\n\nAGGREGATE_IMAGE_PREDICTION = False\nAGGREGATE_TEXT_PREDICTION = False\n\ndf['phash_predictions'] = df['phash_predictions']\ndf['image_predictions'] = image_predictions\ndf['text_predictions'] = text_predictions\n# df['tfidf_predictions'] = df_cu['oof_text'].to_pandas().values\ndf['tfidf_predictions'] = tfidf_preds\ndf['nfnet_predictions'] = nfnet_predictions","metadata":{"_uuid":"1e510db4-1b7c-4b61-a18d-fe676a8f29e2","_cell_guid":"1cb2b9d2-97a6-4981-93ea-65eb3bae9d3b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TFIDF ","metadata":{}},{"cell_type":"code","source":"submission_column = \"matches\"\nif GET_CV:\n    submission_column = \"pred_matches\"\n    \ndf[submission_column] = df['tfidf_predictions']\n\n# df[submission_column] = df.apply(lambda x: combine_predictions(x[submission_column],x['text_predictions']), axis=1)\n\ndf[submission_column] = df[submission_column].apply(reformat_labels)\n\nif GET_CV: \n    df['f1'] = f1_score(df['matches'], df['pred_matches'])\n    score = df['f1'].mean()\n    print(f'Our final f1 cv score is {score}')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**TFIDF Baseline**\n\nOur final f1 cv score is 0.6470817084617556\n","metadata":{}},{"cell_type":"markdown","source":"### Aggregate Phash, Then Combine with Image + Text\n\nNote: I have tried to use image only or text only feature, but the performance is worse than combining it.","metadata":{"_uuid":"da43266f-4702-4fea-a014-5c907997d845","_cell_guid":"c77eea3f-ed6f-41c8-a251-6d963ed4fb9d","trusted":true}},{"cell_type":"code","source":"submission_column = \"matches\"\nif GET_CV:\n    submission_column = \"pred_matches\"\n\nif USE_PHASH_PREDICTION :\n    df[submission_column] = aggregate_predictions(df['phash_predictions'], df['phash_predictions'])\nelse :\n    df[submission_column] = df['image_predictions']\n    \nif AGGREGATE_IMAGE_PREDICTION :\n    df[submission_column] = aggregate_predictions(df[submission_column], df['image_predictions'])\nelse :\n    df[submission_column] = df.apply(lambda x: combine_predictions(x[submission_column],x['image_predictions']), axis=1)\n\nif AGGREGATE_TEXT_PREDICTION :\n    df[submission_column] = aggregate_predictions(df[submission_column], df['text_predictions'])\nelse :\n    df[submission_column] = df.apply(lambda x: combine_predictions(x[submission_column],x['text_predictions']), axis=1)\n\n\ndf[submission_column] = df.apply(lambda x: combine_predictions(x[submission_column],x['tfidf_predictions']), axis=1)\ndf[submission_column] = df.apply(lambda x: combine_predictions(x[submission_column],x['nfnet_predictions']), axis=1)\n\ndf[submission_column] = df[submission_column].apply(reformat_labels)\n\nif GET_CV: \n    df['f1'] = f1_score(df['matches'], df['pred_matches'])\n    score = df['f1'].mean()\n    print(f'Our final f1 cv score is {score}')\n\n# df[['posting_id', 'matches']].to_csv('submission.csv', index = False)","metadata":{"_uuid":"6ee20e43-2c5c-44c5-a7e9-cbfc3fe3b6d9","_cell_guid":"33de7ebe-dcd5-4af0-9925-435bebbaac6e","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## F1 Score Analysis","metadata":{"_uuid":"9ac89781-795d-49e7-919e-2bedfbe3b25b","_cell_guid":"8f6d01e6-76d0-4746-b59b-520a3c6a1ebc","trusted":true}},{"cell_type":"code","source":"# df[df[\"f1\"] < 1][[\"matches\", \"phash_predictions\", \"image_predictions\", \"text_predictions\", \"pred_matches\", \"f1\"]]","metadata":{"_uuid":"18bdc30f-d730-4e11-9296-11bfacc4c531","_cell_guid":"0e41ced7-597f-46fd-b10b-f50b6b1ff8d6","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def len_token(tokens) :\n    return len(tokens.split(\" \"))\n\ndef f1_analysis(df) :\n    d = df[df[\"f1\"] < 1]\n    d[\"len_pred\"] = d[\"pred_matches\"].apply(len_token)\n    d[\"len_label\"] = d[\"matches\"].apply(len_token)\n    \n    equal = d[\"len_pred\"] == d[\"len_label\"]\n\n    ## number the len prediction that is less than the len label\n    ## example pred=\"train_1\" ; label=\"train_1 train_2\"\n    less_than = d[\"len_pred\"] < d[\"len_label\"]\n\n    ## number the len prediction that is greater than the len label\n    ## example pred=\"train_1 train_2 train_3\" ; label=\"train_1 train_2\"\n    greater_than = d[\"len_pred\"] > d[\"len_label\"]\n    \n    print(f\"Error which length pred equal to length match: {sum(equal)}\")\n    print(f\"Error which length pred less than length match: {sum(less_than)}\")\n    print(f\"Error which length pred greater than length match: {sum(greater_than)}\")\n    \n    del d, equal\n    gc.collect()\n    \n    return less_than, greater_than","metadata":{"_uuid":"d5ee5dd2-4680-40c1-91d5-90ccafa01f60","_cell_guid":"86ceed68-ecc3-4e91-bf07-26f481610a3e","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"less_than, greater_than = f1_analysis(df)","metadata":{"_uuid":"d926ca98-8143-4e89-98d0-dd2613fccfae","_cell_guid":"110d64ab-3fb2-43ed-b22a-db6a81e6a1f8","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I think it's better to find a threshold where the number of error comes from them are balance","metadata":{"_uuid":"88327e30-e3f5-4e80-939b-17c0ff311f1f","_cell_guid":"c2a19623-5aa6-4938-ae00-f014dacd4bd4","trusted":true}},{"cell_type":"code","source":"# df[[\"label_group\", \"matches\", \"pred_matches\", \"f1\"]][less_than].sort_values(by=[\"f1\"])","metadata":{"_uuid":"c16bd85e-786e-41dc-a8d4-36b023e256a8","_cell_guid":"55f33861-ebec-4333-9cbe-f729b4746c71","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df[[\"label_group\", \"matches\", \"pred_matches\", \"f1\"]][greater_than].sort_values(by=[\"f1\"])","metadata":{"_uuid":"c825f05a-6b80-48e2-8c1a-d3ad5d495c80","_cell_guid":"bf6b73cb-521c-4648-9e94-1b85490daa78","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_distances, image_indices = get_distances_indices(image_embeddings, KNN=KNN, normalize=NORMALIZE_IMAGE_DISTANCE)\ntext_distances, text_indices = get_distances_indices(text_embeddings, KNN=KNN, normalize=NORMALIZE_TEXT_DISTANCE)\nnfnet_distances, nfnet_indices = get_distances_indices(nfnet_embeddings, KNN=KNN, normalize=NORMALIZE_IMAGE_DISTANCE)","metadata":{"_uuid":"02dd88cd-18fb-4c6a-8cb6-d9ef682b284e","_cell_guid":"4eba48be-68e8-41a2-95d2-27426327f240","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# GROUP = 2008989859\n# # N = 24839\n# N = df.groupby(\"label_group\").get_group(GROUP).index.values[0]\n# df.groupby(\"label_group\").get_group(GROUP)[[ \"f1\", \"matches\", \"pred_matches\", \"image_predictions\", \"text_predictions\", \"title\"]]","metadata":{"_uuid":"58941619-ed3b-44e3-8851-eed29f924b6d","_cell_guid":"69649457-1b77-470e-9f5a-a1c28fe43ce7","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(f\"Image Threshold: {choosen_image_threshold}\")\n# print(f\"Text Threshold: {choosen_text_threshold}\")","metadata":{"_uuid":"09062df1-5a50-4328-8917-9ad96abb3dc9","_cell_guid":"4f1cc82e-9a7b-4394-bda0-c229be88fcfc","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_pair(pair, threshold): \n    print()\n    print(\"Predicted Less Than Threshold\")\n    print([(a, b) for (a, b) in pair if b < threshold])\n    print()\n    print(\"Predicted Greater Than Threshold\")\n    print([(a, b) for (a, b) in pair if b > threshold])\n\n# ## True label\n# print(\"True Label\")\n# print(df.groupby(\"label_group\").get_group(GROUP).index.values)\n\n# ## Label predicted from images      \n# for indexs, distances in zip(image_indices[N:N+1], image_distances[N:N+1]) :\n#     pair = sorted(zip(indexs, distances), key = lambda x: x[1])\n#     print_pair(pair, choosen_image_threshold)","metadata":{"_uuid":"debb036f-6b28-4f88-8dd7-9c9b25b3a992","_cell_guid":"328f6a5d-b245-401b-bbce-8c09fea305c8","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ## True label\n# print(\"True Label\")\n# print(df.groupby(\"label_group\").get_group(GROUP).index.values)\n\n\n# ## Label predicted from texts\n# for i, d in zip(text_indices[N:N+1], text_distances[N:N+1]) :\n#     pair = sorted(zip(i, d), key = lambda x: x[1])\n#     print_pair(pair, choosen_text_threshold)","metadata":{"_uuid":"0c19f64a-dbfa-4795-80f7-6879e8b667a9","_cell_guid":"f311ad4f-6ee9-44b4-a96c-0dbbffede561","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Combine Image and Text Distance","metadata":{"_uuid":"c93a844f-34ce-4b8b-88e3-1246d794f076","_cell_guid":"f64725d1-aeec-4e98-af6a-f5a50f3a2560","trusted":true}},{"cell_type":"code","source":"def list_intersection(a, b) :\n    return list(set(a) & set(b))\n\ndef get_neighbors_from_combined_image_text_distance_indices(df, image_distances, image_indices, choosen_image_threshold, image_multipliers, choosen_image_multiplier, text_distances, text_indices, choosen_text_threshold, text_multipliers, choosen_text_multiplier, PARAMETER_SEARCH) :\n    ## Iterate through different thresholds to maximize cv, run this in interactive mode, then replace else clause with a solid threshold\n    if GET_CV and PARAMETER_SEARCH:\n        scores = []\n        keys = []\n        for im in image_multipliers:\n            for tm in text_multipliers:\n                predictions = []\n                for k in range(image_indices.shape[0]):        \n                    image_idx = np.where(image_distances[k,] < (choosen_image_threshold * im))[0]\n                    image_ids = image_indices[k,image_idx]\n                    text_idx = np.where(text_distances[k,] < (choosen_text_threshold * tm))[0]\n                    text_ids = text_indices[k,text_idx]\n                    ids = list_intersection(image_ids, text_ids)\n                    posting_ids = df['posting_id'].iloc[ids].values\n                    predictions.append(posting_ids)\n                df['pred_matches'] = predictions\n                df['pred_matches'] = df['pred_matches'].apply(reformat_labels)\n                df['f1'] = f1_score(df['matches'], df['pred_matches'])\n                score = df['f1'].mean()\n                print(\"Our f1 score for im-{:.2f} and tm-{:.2f} is {:.3f}\".format(im, tm, score))\n                scores.append(score)\n                keys.append(f'{im}-{tm}')\n                df = df.drop(columns=['pred_matches','f1'])\n                del predictions\n                gc.collect()\n                \n        keys_scores = pd.DataFrame({'keys': keys, 'scores': scores})\n        max_score = keys_scores[keys_scores['scores'] == keys_scores['scores'].max()]\n        best_key = max_score['keys'].values[0]\n        best_score = max_score['scores'].values[0]\n        print(f'Our best score is {best_score} and has a threshold {best_key}')\n        \n    # Because we are predicting the test set that have 70K images and different label groups, confidence should be smaller\n    predictions = []\n    for k in range(image_indices.shape[0]):        \n        image_idx = np.where(image_distances[k,] < choosen_image_threshold * choosen_image_multiplier)[0]\n        image_ids = image_indices[k,image_idx]\n        text_idx = np.where(text_distances[k,] < choosen_text_threshold * choosen_text_multiplier)[0]\n        text_ids = text_indices[k,text_idx]\n        ids = list_intersection(image_ids, text_ids)\n        posting_ids = df['posting_id'].iloc[ids].values\n        predictions.append(posting_ids)\n\n\n    del image_distances, image_indices, text_distances, text_indices\n    gc.collect()\n    return df, predictions","metadata":{"_uuid":"b55399c8-3f5a-4ea1-bddd-1a93cd0d6bb8","_cell_guid":"7d35242b-b299-4233-99e8-a42f149e3eea","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_distances, image_indices = get_distances_indices(image_embeddings, KNN=KNN, normalize=NORMALIZE_IMAGE_DISTANCE)\ntext_distances, text_indices = get_distances_indices(text_embeddings, KNN=KNN, normalize=NORMALIZE_TEXT_DISTANCE)\nnfnet_distances, nfnet_indices = get_distances_indices(nfnet_embeddings, KNN=KNN, normalize=NORMALIZE_IMAGE_DISTANCE)","metadata":{"_uuid":"6899c7ed-906a-4fb7-8e05-df1fb152c2e7","_cell_guid":"8563e524-f63f-4df9-8a2c-42fa128dff11","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PARAMETER_SEARCH = False\n\nimage_multipliers = list(np.arange(1.3, 1.7, 0.1))\nchoosen_image_multiplier = 1.5\n\ntext_multipliers = list(np.arange(1.3, 1.7, 0.1))\nchoosen_text_multiplier = 1.5\n\nnfnet_multipliers = list(np.arange(1.3, 1.7, 0.1))\nchoosen_nfnet_multiplier = 1.5","metadata":{"_uuid":"cc3f5697-43e8-45f7-9bda-969ba32e22f6","_cell_guid":"97c99dc6-87c8-4c66-9815-527feb5bcc33","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df, image_text_predictions = get_neighbors_from_combined_image_text_distance_indices(df, image_distances, image_indices, choosen_image_threshold, image_multipliers, choosen_image_multiplier, text_distances, text_indices, choosen_text_threshold, text_multipliers, choosen_text_multiplier, PARAMETER_SEARCH)","metadata":{"_uuid":"b627818d-42e5-44d7-a4a5-234700c75a91","_cell_guid":"cb49e316-87b6-46f0-9315-a21a1a57f806","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df, nfnet_text_predictions = get_neighbors_from_combined_image_text_distance_indices(df, nfnet_distances, nfnet_indices, choosen_nfnet_threshold, nfnet_multipliers, choosen_nfnet_multiplier, text_distances, text_indices, choosen_text_threshold, text_multipliers, choosen_text_multiplier, PARAMETER_SEARCH)","metadata":{"_uuid":"44014649-56fb-4766-8f45-a3344f8f371c","_cell_guid":"964435ad-82eb-4398-ac39-d4daaf67b576","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df, image_nfnet_predictions = get_neighbors_from_combined_image_text_distance_indices(df, image_distances, image_indices, choosen_image_threshold, image_multipliers, choosen_image_multiplier, nfnet_distances, nfnet_indices, choosen_nfnet_threshold, nfnet_multipliers, choosen_nfnet_multiplier, PARAMETER_SEARCH)","metadata":{"_uuid":"9177f06e-1952-486e-8ccd-bee307be8686","_cell_guid":"9a2c48af-484f-4ddd-8a26-65f026ff802f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def list_intersection(a, b) :\n    return list(set(a) & set(b))\n\ndef get_predictions_from_combined_with_tfidf(df, tfidf_low_preds, text_distances, text_indices, choosen_text_threshold, text_multipliers, choosen_text_multiplier, PARAMETER_SEARCH=True) :\n    ## Iterate through different thresholds to maximize cv, run this in interactive mode, then replace else clause with a solid threshold\n    if GET_CV and PARAMETER_SEARCH:\n        scores = []\n        keys = []\n        for tm in text_multipliers:\n            predictions = []\n            for k in range(image_indices.shape[0]):        \n                text_idx = np.where(text_distances[k,] < (choosen_text_threshold * tm))[0]\n                text_ids = text_indices[k,text_idx]\n                text_post_ids = df['posting_id'].iloc[text_ids].values\n                tfidf_ids = tfidf_low_preds[k]\n#                 if k in [1,2,3,4] :\n#                     print(tfidf_ids)\n#                     print(text_post_ids)\n                posting_ids = list_intersection(tfidf_ids, text_post_ids)\n                predictions.append(posting_ids)\n            df['pred_matches'] = predictions\n            df['pred_matches'] = df['pred_matches'].apply(reformat_labels)\n            df['f1'] = f1_score(df['matches'], df['pred_matches'])\n            score = df['f1'].mean()\n            print(\"Our f1 score for tm-{:.2f} is {:.3f}\".format(tm, score))\n            scores.append(score)\n            keys.append(f'{tm}')\n            df = df.drop(columns=['pred_matches','f1'])\n            del predictions\n            gc.collect()\n                \n        keys_scores = pd.DataFrame({'keys': keys, 'scores': scores})\n        max_score = keys_scores[keys_scores['scores'] == keys_scores['scores'].max()]\n        best_key = max_score['keys'].values[0]\n        best_score = max_score['scores'].values[0]\n        print(f'Our best score is {best_score} and has a threshold {best_key}')\n        \n    # Because we are predicting the test set that have 70K images and different label groups, confidence should be smaller\n    predictions = []\n    for k in range(image_indices.shape[0]):        \n        text_idx = np.where(text_distances[k,] < choosen_text_threshold * choosen_text_multiplier)[0]\n        text_ids = text_indices[k,text_idx]\n        tfidf_ids = tfidf_low_preds[k]\n        ids = list_intersection(tfidf_ids, text_ids)\n        posting_ids = df['posting_id'].iloc[ids].values\n        predictions.append(posting_ids)\n\n\n    del text_distances, text_indices\n    gc.collect()\n    return predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_text_predictions = get_predictions_from_combined_with_tfidf(df, tfidf_low_preds, text_distances, text_indices, choosen_text_threshold, text_multipliers, choosen_text_multiplier)\ntfidf_nfnet_predictions = get_predictions_from_combined_with_tfidf(df, tfidf_low_preds, nfnet_distances, nfnet_indices, choosen_nfnet_threshold, nfnet_multipliers, choosen_nfnet_multiplier)\ntfidf_image_predictions = get_predictions_from_combined_with_tfidf(df, tfidf_low_preds, image_distances, image_indices, choosen_image_threshold, image_multipliers, choosen_image_multiplier)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AGGREGATE_IMAGE_TEXT_PREDICTION = False\n\ndf[\"image_text_predictions\"] = image_text_predictions\ndf[\"nfnet_text_predictions\"] = nfnet_text_predictions\ndf[\"image_nfnet_predictions\"] = image_nfnet_predictions\n\ndf[\"tfidf_image_predictions\"] = tfidf_image_predictions\ndf[\"tfidf_text_predictions\"] = tfidf_text_predictions\ndf[\"tfidf_nfnet_predictions\"] = tfidf_nfnet_predictions","metadata":{"_uuid":"02191e40-57e5-44b1-80e2-957a8ec9e944","_cell_guid":"92076307-66ef-4528-a825-251b5bc1d7da","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_column = \"matches\"\nif GET_CV:\n    submission_column = \"pred_matches\"\n\nif USE_PHASH_PREDICTION :\n    df[submission_column] = aggregate_predictions(df['phash_predictions'], df['phash_predictions'])\nelse :\n    df[submission_column] = df['image_predictions']\n\n    \nif AGGREGATE_IMAGE_PREDICTION :\n    df[submission_column] = aggregate_predictions(df[submission_column], df['image_predictions'])\nelse :\n    df[submission_column] = df.apply(lambda x: combine_predictions(x[submission_column],x['image_predictions']), axis=1)\n\nif AGGREGATE_TEXT_PREDICTION :\n    df[submission_column] = aggregate_predictions(df[submission_column], df['text_predictions'])\nelse :\n    df[submission_column] = df.apply(lambda x: combine_predictions(x[submission_column],x['text_predictions']), axis=1)\n\nif AGGREGATE_IMAGE_TEXT_PREDICTION :\n    df[submission_column] = aggregate_predictions(df[submission_column], df['image_text_predictions'])\nelse :\n    df[submission_column] = df.apply(lambda x: combine_predictions(x[submission_column],x['image_text_predictions']), axis=1)\n\ndf[submission_column] = df.apply(lambda x: combine_predictions(x[submission_column],x['nfnet_text_predictions']), axis=1)\ndf[submission_column] = df.apply(lambda x: combine_predictions(x[submission_column],x['image_nfnet_predictions']), axis=1)\n\ndf[submission_column] = df.apply(lambda x: combine_predictions(x[submission_column],x['tfidf_predictions']), axis=1)\ndf[submission_column] = df.apply(lambda x: combine_predictions(x[submission_column],x['nfnet_predictions']), axis=1)\n\ndf[submission_column] = df.apply(lambda x: combine_predictions(x[submission_column],x['tfidf_text_predictions']), axis=1)\ndf[submission_column] = df.apply(lambda x: combine_predictions(x[submission_column],x['tfidf_nfnet_predictions']), axis=1)\ndf[submission_column] = df.apply(lambda x: combine_predictions(x[submission_column],x['tfidf_image_predictions']), axis=1)\n\ndf[submission_column] = df[submission_column].apply(reformat_labels)\n\nif GET_CV: \n    df['f1'] = f1_score(df['matches'], df['pred_matches'])\n    score = df['f1'].mean()\n    print(f'Our final f1 cv score is {score}')\n\ndf[['posting_id', 'matches']].to_csv('submission.csv', index = False)","metadata":{"_uuid":"aa479a92-5a32-49da-850a-b5667cbde8b3","_cell_guid":"d2eb2454-67f9-4f26-b161-5051371577d0","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"less_than, greater_than = f1_analysis(df)","metadata":{"_uuid":"8973661d-bd02-4cd3-8295-6c35b67abc2c","_cell_guid":"b820cdb7-0ca9-497c-b83b-57f426f59fe2","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Baseline 0.740**\n\nOur final f1 cv score is 0.9144889569967541\n\nError which length pred equal to length match: 993\n\nError which length pred less than length match: 2650\n\nError which length pred greater than length match: 8816\n\n\n**Baseline 0.740**\n\nOur final f1 cv score is 0.9373126574577614\n\nError which length pred equal to length match: 295\n\nError which length pred less than length match: 2374\n\nError which length pred greater than length match: 6575\n\n\n**Baseline 0.740**\n\nOur final f1 cv score is 0.9249745345112435\n\nError which length pred equal to length match: 410\n\nError which length pred less than length match: 3844\n\nError which length pred greater than length match: 6281\n\n\n**Baseline 0.737**\n\nOur final f1 cv score is 0.9282814981347284\n\nError which length pred equal to length match: 401\n\nError which length pred less than length match: 3825\n\nError which length pred greater than length match: 6163","metadata":{"_uuid":"f7e8f25a-20a4-4872-aa3d-e187124eea01","_cell_guid":"223f7d75-18ff-402b-927c-84a69c896fbd","trusted":true}},{"cell_type":"markdown","source":"## TODO\n- combine distance for lower parameter, let say by multiply the text distance and image distance\n- use image embedding to compare with embedding from the training data\n- use IndoBERT instead of English","metadata":{"_uuid":"69c1a7cd-9c3d-4f64-b7a6-2e2395d317ea","_cell_guid":"3ea8c259-932e-4665-9ab3-3cbea5345b37","trusted":true}},{"cell_type":"code","source":"","metadata":{"_uuid":"53c611f6-5061-4a19-86e4-ad41ff506f70","_cell_guid":"f8896f43-c84e-46f9-a1ff-67aa908c0e96","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}