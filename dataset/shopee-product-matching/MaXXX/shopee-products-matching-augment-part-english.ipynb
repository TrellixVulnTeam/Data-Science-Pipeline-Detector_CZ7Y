{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this notebook, some augmentaion methos is introduced. And i will give some insight and product method in shopee compte. If you find usefule, please give upvote, thx.\n\n- Image Augmentation\n- Text Augmentaion\n- Product Augmentaion\n\n在本个notebook中，介绍了一些数据扩增方法。我也会在介绍使用的同时，结合shopee比赛给出一些建议。如果你感谢内容对你有帮助，请给我点赞，谢谢。\n\n\nYou can check my other notebooks:\n\n- [Shopee Products Matching: Image Part [English+中文]](https://www.kaggle.com/finlay/shopee-products-matching-image-part-english)\n- [Shopee Products Matching: Text Part [English+中文]](https://www.kaggle.com/finlay/shopee-products-matching-text-part-english)\n- [Shopee Products Matching: BoF Part [English+中文]](https://www.kaggle.com/finlay/shopee-products-matching-bof-part-english)\n- [Shopee Products Matching: Augment Part [English中文]](https://www.kaggle.com/finlay/shopee-products-matching-augment-part-english)\n- [[Unsupervised] Image + Text Baseline in 20min](https://www.kaggle.com/finlay/unsupervised-image-text-baseline-in-20min)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# DATA_PATH = '../input/'\nDATA_PATH = '../input/shopee-product-matching/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport cv2, matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\nimport gc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"COMPUTE_CV = True\n\ntest = pd.read_csv(DATA_PATH + 'test.csv')\nif len(test)>3: COMPUTE_CV = False\nelse: print('this submission notebook will compute CV score, but commit notebook will not')\n\n# COMPUTE_CV = False\n\nif COMPUTE_CV:\n    train = pd.read_csv(DATA_PATH + 'train.csv')\n    train['image'] = DATA_PATH + 'train_images/' + train['image']\n    tmp = train.groupby('label_group').posting_id.agg('unique').to_dict()\n    train['target'] = train.label_group.map(tmp)\n#     train_gf = cudf.read_csv(DATA_PATH + 'train.csv')\nelse:\n    train = pd.read_csv(DATA_PATH + 'test.csv')\n    train['image'] = DATA_PATH + 'test_images/' + train['image']\n#     train_gf = cudf.read_csv(DATA_PATH + 'test.csv')\n    \nprint('train shape is', train.shape )\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Image Augmentation\n\n图像数据扩增\n\nhttps://github.com/albumentations-team/albumentations"},{"metadata":{"trusted":true},"cell_type":"code","source":"import albumentations as A\n\ndef visualize(image):\n    plt.figure(figsize=(6, 6))\n    plt.axis('off')\n    plt.imshow(image)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OpenCV reads an image in BGR format (so color channels of the image have the following order: Blue, Green, Red). Albumentations uses the most common and popular RGB image format. So when using OpenCV, we need to convert the image format to RGB explicitly."},{"metadata":{"trusted":true},"cell_type":"code","source":"image = cv2.imread(train['image'].iloc[0])\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nvisualize(image)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = A.HorizontalFlip(p=1)\naugmented_image = transform(image=image)['image']\nvisualize(augmented_image)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = A.ShiftScaleRotate(p=1)\naugmented_image = transform(image=image)['image']\nvisualize(augmented_image)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define an augmentation pipeline using Compose, pass the image to it and receive the augmented image"},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = A.Compose([\n    A.RandomRotate90(),\n    A.Transpose(),\n    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.50, rotate_limit=45, p=.75),\n    A.Blur(blur_limit=3),\n    A.OpticalDistortion(),\n    A.GridDistortion(),\n    A.HueSaturationValue(),\n])\naugmented_image = transform(image=image)['image']\nvisualize(augmented_image)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = A.Compose([\n        A.RandomRotate90(),\n        A.Flip(),\n        A.Transpose(),\n        A.OneOf([\n            A.IAAAdditiveGaussianNoise(),\n            A.GaussNoise(),\n        ], p=0.2),\n        A.OneOf([\n            A.MotionBlur(p=.2),\n            A.MedianBlur(blur_limit=3, p=0.1),\n            A.Blur(blur_limit=3, p=0.1),\n        ], p=0.2),\n        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=45, p=0.2),\n        A.OneOf([\n            A.OpticalDistortion(p=0.3),\n            A.GridDistortion(p=.1),\n            A.IAAPiecewiseAffine(p=0.3),\n        ], p=0.2),\n        A.OneOf([\n            A.CLAHE(clip_limit=2),\n            A.IAASharpen(),\n            A.IAAEmboss(),\n            A.RandomBrightnessContrast(),            \n        ], p=0.3),\n        A.HueSaturationValue(p=0.3),\n    ])\naugmented_image = transform(image=image)['image']\nvisualize(augmented_image)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Augmentation\n\n* https://github.com/makcedward/nlpaug\n* https://github.com/jasonwei20/eda_nlp/"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install nlpaug","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nlpaug.augmenter.char as nac\nimport nlpaug.augmenter.word as naw\nimport nlpaug.augmenter.sentence as nas\nimport nlpaug.flow as nafc\n\nfrom nlpaug.util import Action","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Swap character randomly¶\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"aug = nac.RandomCharAug(action=\"swap\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"texts = train['title'].iloc[:10]\n\nfor text in texts:\n    augmented_text = aug.augment(text)\n    \n    print('-'*20)\n    print('Original Input:{}'.format(text))\n    print('Agumented Output:{}'.format(augmented_text))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Delete character randomly\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"aug = nac.RandomCharAug(action=\"delete\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"texts = train['title'].iloc[:10]\n\nfor text in texts:\n    augmented_text = aug.augment(text)\n    \n    print('-'*20)\n    print('Original Input:{}'.format(text))\n    print('Agumented Output:{}'.format(augmented_text))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n## Word Augmenter\n\nUse of word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), fasttext (Joulin et al., 2016), BERT(Devlin et al., 2018) and wordnet to insert and substitute similar word. Word2vecAug, GloVeAug and FasttextAug use word embeddings to find most similar group of words to replace original word. On the other hand, BertAug use language models to predict possible target word. WordNetAug use statistics way to find the similar group of words."},{"metadata":{"trusted":true},"cell_type":"code","source":"text = train['title'].iloc[0]\n\n# model_type: word2vec, glove or fasttext\naug = naw.WordEmbsAug(\n    model_type='fasttext', model_path='../input/fasttext-wikinews/wiki-news-300d-1M.vec')\naugmented_text = aug.augment(text)\nprint(\"Original:\")\nprint(text)\nprint(\"Augmented Text:\")\nprint(augmented_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for text in train['title'].iloc[:10]:\n    augmented_text = aug.augment(text)\n    print('-'*20)\n    print('Original Input:{}'.format(text))\n    print('Agumented Output:{}'.format(augmented_text))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Contextual Word Embeddings Augmenter (BERT)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Augment French by BERT\naug = naw.ContextualWordEmbsAug(model_path='bert-base-uncased', aug_p=0.1)\ntext = \"Bonjour, J'aimerais une attestation de l'employeur certifiant que je suis en CDI.\"\naugmented_text = aug.augment(text)\nprint(\"Original:\")\nprint(text)\nprint(\"Augmented Text:\")\nprint(augmented_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for text in train['title'].iloc[:10]:\n    augmented_text = aug.augment(text)\n    print('-'*20)\n    print('Original Input:{}'.format(text))\n    print('Agumented Output:{}'.format(augmented_text))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## TF-IDF Augmenter"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport nlpaug.model.word_stats as nmw\n\ndef _tokenizer(text, token_pattern=r\"(?u)\\b\\w\\w+\\b\"):\n    token_pattern = re.compile(token_pattern)\n    return token_pattern.findall(text)\n\n# Tokenize input\ntrain_x_tokens = [_tokenizer(x) for x in train['title']]\n\n# Train TF-IDF model\ntfidf_model = nmw.TfIdf()\ntfidf_model.train(train_x_tokens)\ntfidf_model.save('.')\n\n# Load TF-IDF augmenter\naug = naw.TfIdfAug(model_path='.', tokenizer=_tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"texts = train['title'].iloc[:10]\n\nfor text in texts:\n    augmented_text = aug.augment(text)\n    \n    print('-'*20)\n    print('Original Input:{}'.format(text))\n    print('Agumented Output:{}'.format(augmented_text))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Contextual Word Embeddings for Sentence Augmenter"},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_path: xlnet-base-cased or gpt2\naug = nas.ContextualWordEmbsForSentenceAug(model_path='xlnet-base-cased')\naugmented_texts = aug.augment(text, n=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"texts = train['title'].iloc[:10]\n\nfor text in texts:\n    augmented_text = aug.augment(text)\n    \n    print('-'*20)\n    print('Original Input:{}'.format(text))\n    print('Agumented Output:{}'.format(augmented_text))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Product/Pair Augmentation\n\nwe random swap the image and title in same group to get new product.\n\n将商品图像和title，重新组合得到新的商品。"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aug_image_title_pairs = []\nfor df in train.groupby('label_group'):\n    for idx1, img in enumerate(df[1]['image']):\n        for idx2, title in enumerate(df[1]['title']):\n            if idx1 == idx2:\n                continue\n            \n            aug_image_title_pairs.append([img, title])\n            \n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(aug_image_title_pairs, columns=['image', 'title'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we see this group has 3 product, now we get 6 new product!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}