{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Shopee - Detecting Product Similarity (Texts, Images, and Phash)\nThe task of this competition is to \"build a model that predicts which items are the same products.\" We tackle this task by implementing KNN to group similar products by their title and images, and [image phash](https://en.wikipedia.org/wiki/Perceptual_hashing). We also use RAPIDS to accelerate KNN computations on the GPU. A huge thanks to the authors of the following kernel as well.\n* Chris Deoette - [[PART 2] - RAPIDS TfidfVectorizer - [CV 0.700]](https://www.kaggle.com/cdeotte/part-2-rapids-tfidfvectorizer-cv-0-700/notebook#Use-Image-Embeddings)","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\nimport math\nimport tensorflow as tf\nimport sys\nimport gc\nimport math\nfrom tqdm import tqdm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## RAPIDS Installation & Imports\nKaggle has the 0.16 version of RAPIDS installed, but this version's `fit` and `fit_transform` methods [do not support sparse cupy matrices](https://www.kaggle.com/c/shopee-product-matching/discussion/230152) like more recent versions. It only supports dense matrices which require much more memory, so we install version 0.18 in order to save GPU memory.","metadata":{}},{"cell_type":"code","source":"!cp ../input/rapids/rapids.0.18.0 /opt/conda/envs/rapids.tar.gz\n!cd /opt/conda/envs/ && tar -xzvf rapids.tar.gz > /dev/null\nsys.path = [\"/opt/conda/envs/rapids/lib/python3.7/site-packages\"] + sys.path\nsys.path = [\"/opt/conda/envs/rapids/lib/python3.7\"] + sys.path\nsys.path = [\"/opt/conda/envs/rapids/lib\"] + sys.path \n!cp /opt/conda/envs/rapids/lib/libxgboost.so /opt/conda/lib/\nprint('RAPIDS 0.18 installation complete')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cuml, cudf, cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Examine Data\n## Load Data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/shopee-product-matching/train.csv')\ntest = pd.read_csv('../input/shopee-product-matching/test.csv')\nsubmission = pd.read_csv('../input/shopee-product-matching/sample_submission.csv')\n\nprint('Train shape: {}'.format(train.shape))\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Test shape: {}'.format(test.shape))\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Submission shape: {}'.format(submission.shape))\nsubmission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check Samples","metadata":{}},{"cell_type":"code","source":"def show_samples(data_df, num, title, img_path = '../input/shopee-product-matching/train_images/'):\n    cols = 6\n    rows = math.ceil(num / cols)\n    height = 5 * rows\n    fig, axs = plt.subplots(rows, cols, figsize = (20, height))\n    fig.suptitle('{}\\n'.format(title), fontsize = 25)\n    for row in range(rows):\n        for col in range(cols):     \n            if rows > 1:\n                ax = axs[row, col]\n            else:\n                ax = axs[col]\n                \n            ax.axis('off')\n            \n            df_row = col + row * cols # examine row by row for dataframe\n            title = data_df.iloc[df_row, 3]\n            title_with_return = ''\n            for i, ch in enumerate(title):\n                title_with_return += ch\n                if (i != 0) & (i % 20 == 0): \n                    title_with_return += '\\n'\n            ax.set_title(title_with_return)\n            \n            posting_id = data_df.iloc[df_row, 1]\n            img_bgr = cv2.imread(img_path + posting_id)\n            img_rgb = cv2.cvtColor(img_bgr,  cv2.COLOR_BGR2RGB) # convert image back to RGB for visualization\n            ax.imshow(img_rgb)    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_samples(data_df = train, num = 12, title = 'Product Samples')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check Duplicates","metadata":{}},{"cell_type":"code","source":"groups = train['label_group'].value_counts()\n\nplt.figure(figsize = (20, 5))\nplt.plot(np.arange(len(groups)),groups.values)\nplt.title('Number of Duplicates',size = 25)\nplt.ylabel('Count', size = 15)\nplt.xlabel('Duplicate Products', size = 15)\nplt.show()\n\nplt.figure(figsize = (20, 5))\nplt.bar(groups.index.values[:50].astype('str'), groups.values[:50])\nplt.xticks(rotation = 45)\nplt.title('Top 50 Duplicates', size = 25)\nplt.ylabel('Count', size = 15)\nplt.xlabel('Label Group', size = 15)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Top Duplicates","metadata":{}},{"cell_type":"code","source":"TOP_N = 3\nCOUNT = 6\n\ntop_duplicates = train['label_group'].value_counts().index[:TOP_N]\nfor i, duplicate_label in enumerate(top_duplicates):\n    duplicate_label_df = train.loc[train['label_group'] == duplicate_label]\n    duplicate_title = 'Top #{} Product With Duplicates'.format(i + 1)\n    show_samples(data_df = duplicate_label_df, num = COUNT, title = duplicate_title)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setup RAPIDS\nWe setup RAPIDS by distributing the memory limits for TensorFlow and RAPIDS. Since we perform the KNN computations on the GPU, we distribute more memory to RAPIDS.\n## Setup GPU Memory","metadata":{}},{"cell_type":"code","source":"LIMIT = 1 # 1 GB of GPU for TensorFlow, 15 GB of GPU for RAPIDS (GPU total: 16 GB)\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        tf.config.experimental.set_virtual_device_configuration(\n            gpus[0],\n            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit = 1024 * LIMIT)])\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n        print('GPU is online.')\n    except RuntimeError as e:\n        print(e)\nelse:\n    print('GPU is offline.')\nprint('TensorFlow GPU memory limit set to {} GB'.format(LIMIT))\nprint('RAPIDS GPU memory limit set to {} GB'.format(16 - LIMIT))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***\n# Title Predictions\n## Extract Text Embeddings with Tf-idf Vectorizer\nWe use tf-idf [to scale down the impact of tokens that occur very frequently in a given corpus](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer). This helps the model to disregard less informative tokens and emphasize more informative ones.","metadata":{}},{"cell_type":"code","source":"def get_text_embeddings(text_df):\n    text_gf = cudf.from_pandas(text_df)\n    text_embed_model = TfidfVectorizer(\n        stop_words = 'english', \n        binary = True) # binary occurence since short titles\n    text_embeddings = text_embed_model.fit_transform(text_gf)\n    \n    return text_embeddings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title_embeddings = get_text_embeddings(train['title'])\ntitle_embeddings.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get Title Distances with KNN\nWe get the distances and indices of the 50 closest neighbors to each text embedding of a product's title. ([Group sizes were capped at 50, so there is no benefit to predict more than 50 matches](https://www.kaggle.com/c/shopee-product-matching/overview/evaluation)) Also, for text classification the cosine distance metric performs well since it disregards the magnitudes of the embeddings, meaning it is better at capturing similarities across varying lengths of text which is our case.","metadata":{}},{"cell_type":"code","source":"def knn_predict_embeddings(embeddings, metric, n_neighbors = 50):\n    BATCH_SIZE = 1024 * 4\n    n_texts = embeddings.shape[0]\n    n_batches = math.ceil(n_texts / BATCH_SIZE)\n    \n    knn_model = cuml.NearestNeighbors(n_neighbors = n_neighbors, metric = metric)\n    knn_model.fit(embeddings)\n    \n    embed_distances = np.zeros((n_texts, n_neighbors))\n    embed_indices = np.zeros((n_texts, n_neighbors))\n\n    with cuml.using_output_type('numpy'): # to output as numpy arrays\n        for i in tqdm(range(n_batches)):\n            a = i * BATCH_SIZE\n            b = min((i + 1) * BATCH_SIZE, n_texts)\n            distances, indices = knn_model.kneighbors(embeddings[a:b])\n            embed_distances[a:b] = distances\n            embed_indices[a:b] = indices\n        \n    return embed_distances, embed_indices","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title_distances, title_indices = knn_predict_embeddings(title_embeddings, 'cosine')\ndel title_embeddings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Title Distance Samples\nWe check the distances of each neighbor from a number of samples.","metadata":{}},{"cell_type":"code","source":"def show_distance_samples(dist_type, distances, indices,  metric = '', n_items = 3, n_neighbors = 5, random = False):\n    for i in range(n_items): \n        if random: \n            i = np.random.randint(0, len(train))  \n        item_title = train.loc[indices[i, 0], 'title']\n        \n        plt.figure(figsize=(15,3))\n        plt.plot(np.arange(50), distances[i,],'o-')\n        plt.title('{} Distances From \"{}\" to Neighbors'.format(dist_type, item_title),size=16)\n        plt.ylabel('{} Distance'.format(metric),size = 20)\n        plt.xlabel('Closest 50 Neighbors',size = 15)\n        plt.show()\n\n        print(train.loc[indices[i, :n_neighbors],['title','label_group']] )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_distance_samples('Title', title_distances, title_indices, 'Cosine')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Extract Match Title Distances\nWe extract the distances of neighbors which are true duplicate matches of each product by grouping them according to each `label_group`.","metadata":{}},{"cell_type":"code","source":"def show_match_distances(title, distances, indices):\n    match_distances = []\n    for i in tqdm(range(distances.shape[0])):\n        for j in range(distances.shape[1]):\n            if train.loc[i, 'label_group'] == train.loc[indices[i, j], 'label_group']:\n                match_distances.append(distances[i, j])\n    if type(match_distances) != list:\n        match_distances = [dist for dist_list in match_distances for dist in dist_list]\n    plt.hist(match_distances, bins = 20)\n    plt.title('{} Neighbor Distance Distribution'.format(title), size = 20)\n    plt.show()\n    print(pd.DataFrame(match_distances, columns=['distances']).describe())\n    del match_distances\n\ndef get_match_distances(distances, indices):\n    match_distances = []\n    for i in tqdm(range(distances.shape[0])):\n        for j in range(distances.shape[1]):\n            if train.loc[i, 'label_group'] == train.loc[indices[i, j], 'label_group']:\n                match_distances.append(distances[i, j])\n    return match_distances\n\ndef show_match_distance_distribution(title, match_distances):\n    if type(match_distances) != list:\n        match_distances = [dist for dist_list in match_distances for dist in dist_list]\n    plt.hist(match_distances, bins = 20)\n    plt.title('{} Neighbor Distance Distribution'.format(title), size = 20)\n    plt.show()\n    print(pd.DataFrame(match_distances, columns=['distances']).describe())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_match_distances('Title', title_distances, title_indices)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Determine Best Title Distance\nWe need to decide upon a maximum distance in order to predict whether a certain neighbor is a duplicate of the product or not. To do so we calculate the mean **f1 score,** which is this competition's evluation metric, of the title predictions based on different maximum distances.","metadata":{}},{"cell_type":"markdown","source":"### Setup Target Values\nWe setup the target values by grouping every product to each `label_group`.","metadata":{}},{"cell_type":"code","source":"target_dict = train.groupby('label_group')['posting_id'].agg('unique').to_dict()\ntrain['target'] = train['label_group'].map(target_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### F1 Score Metric\nWe create a function to calculate the f1 score of each prediction.","metadata":{}},{"cell_type":"code","source":"def get_f1_score(row, pred_col):\n    true_preds = len(np.intersect1d(row['target'], row[pred_col + '_pred']))\n    f1_score = (2 * true_preds) / (len(row['target']) + len(row[pred_col + '_pred']))\n           # = 2 * TP / ((2 * TP) + FN + FP) \n           # = 2 * TP / (TP + TP + FN + FP) \n           # = 2 * TP / ((TP + FN) + (TP + FP)) \n           # = 2 * TP / (len(target) + len(predictions))\n    return f1_score","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Make Predictions and Choose Max Title Distance\nWe make predictions on various maximum distances and find the distance obtaining the highest f1 score.","metadata":{}},{"cell_type":"code","source":"def get_predictions(data_df, distances, indices, max_distance):\n    predictions = []\n    for i in tqdm(range(distances.shape[0])): \n        pred_distances = np.where(distances[i] < max_distance)[0]\n        pred_indices = indices[i, pred_distances]\n        row_pred = data_df.iloc[pred_indices]['posting_id'].values.tolist()\n        predictions.append(row_pred)\n    return predictions\n\ndef show_max_distance_f1_scores(distances, indices, max_distances, pred_col):\n    for max_distance in max_distances:\n        train[pred_col + '_pred'] = get_predictions(train, distances, indices, max_distance)\n        train['f1_score_' + pred_col] = train.apply(get_f1_score, axis = 'columns', pred_col = pred_col)\n        print('{} max distance: {}, {} f1 score = {}'.format(pred_col, max_distance, pred_col, train['f1_score_' + pred_col].mean()))\n    train.drop(columns = 'f1_score_' + pred_col, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_TITLE_DISTANCES = [0.46861,\n                       0.46862,\n                       0.46863,\n                       0.46864,\n                       0.46865]\n\nshow_max_distance_f1_scores(title_distances, title_indices, MAX_TITLE_DISTANCES, 'title')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The maximum distance value of `0.46863` obtained the highest f1 score, so we use this distance for our final predictions.","metadata":{}},{"cell_type":"code","source":"MAX_TITLE_DISTANCE = 0.46863 # best f1 score of 0.6610465706375579","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Attach Title Predictions","metadata":{}},{"cell_type":"code","source":"train['title_pred'] = get_predictions(train, title_distances, title_indices, MAX_TITLE_DISTANCE)\ndel title_distances, title_indices\n_ = gc.collect()\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***\n# Image Predictions\n## Create Data Generator\nWe create a data generator to pass on the image data to our embedding model in batches to prevent memory errors.","metadata":{}},{"cell_type":"code","source":"class DataGenerator(tf.keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, df, img_size, batch_size, path): \n        self.df = df\n        self.img_size = img_size\n        self.batch_size = batch_size\n        self.path = path\n        self.indexes = np.arange(len(self.df))\n        \n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        ct = len(self.df) // self.batch_size\n        ct += int(((len(self.df)) % self.batch_size) != 0)\n        return ct\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n        X = self.__data_generation(indexes)\n        return X\n            \n    def __data_generation(self, indexes):\n        'Generates data containing batch_size samples' \n        X = np.zeros((len(indexes), self.img_size, self.img_size, 3), dtype = 'float32')\n        df = self.df.iloc[indexes]\n        for i,(index, row) in enumerate(df.iteritems()):\n            img = cv2.imread(self.path + row)\n            X[i,] = cv2.resize(img, (self.img_size, self.img_size)) \n        return X","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Extract Image Embeddings with Pretrained Model\nWe use the EfficientNetB0 model to extract image features as vectors to use for KNN. Since submissions must disable internet connection, we need to load the model weights as an input file.","metadata":{}},{"cell_type":"code","source":"def get_image_embeddings(image_df, image_path):\n    image_embed_model = tf.keras.applications.EfficientNetB0( # image distances most evenly distributed among pretrained models\n        weights = '../input/shopee-data/efficientnetb0_notop.h5', # same as calling 'imagenet', but need file import because internet must be disabled\n        include_top = False, \n        pooling = 'avg', \n        input_shape = None)\n    \n    n_images = image_df.shape[0]\n    n_features = image_embed_model.layers[-1].output_shape[1] # size of final output layer\n    image_embeddings = np.zeros((n_images, n_features))\n    \n    input_img_size = 256 # input size of EfficientNetB0\n    BATCH_SIZE = 1024 * 4\n    EPOCHS = math.ceil(n_images / BATCH_SIZE)\n    for i in tqdm(range(EPOCHS)):\n        a = i * BATCH_SIZE\n        b = min((i + 1) * BATCH_SIZE, n_images)\n\n        image_gen = DataGenerator(df = image_df.iloc[a:b], img_size = input_img_size, batch_size = 32, path = image_path)\n        batch_embeddings = image_embed_model.predict(image_gen, verbose = 1, use_multiprocessing = True, workers = 4)\n        image_embeddings[a:b] = batch_embeddings\n    \n    return image_embeddings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN_PATH = '../input/shopee-product-matching/train_images/'\n\nimage_embeddings = get_image_embeddings(train['image'], TRAIN_PATH)\nimage_embeddings.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get Image Distances with KNN\nLike what we did for the titles, we get the distances and indices of the 50 closest neighbors to each image embedding of a product's image. This time we use euclidean distance.","metadata":{}},{"cell_type":"code","source":"image_distances, image_indices = knn_predict_embeddings(image_embeddings, 'euclidean')\ndel image_embeddings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Text Distance Samples\nWe check the distances of each neighbor from a number of samples.","metadata":{}},{"cell_type":"code","source":"show_distance_samples('Image', image_distances, image_indices, 'Euclidean')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Extract Match Image Distances\nWe extract the distances of neighbors which are true duplicate matches of each product by grouping them according to each `label_group`.","metadata":{}},{"cell_type":"code","source":"show_match_distances('Image', image_distances, image_indices)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Determine Best Image Distance\nWe need to decide upon a maximum distance in order to predict whether a certain neighbor is a duplicate of the product or not. To do so we calculate the mean **f1 score,** which is this competition's evluation metric, of the image predictions based on different maximum distances.","metadata":{}},{"cell_type":"code","source":"MAX_IMAGE_DISTANCES = [6.88564,\n                       6.88566,\n                       6.88568,\n                       6.88570,\n                       6.88572]\n\nshow_max_distance_f1_scores(image_distances, image_indices, MAX_IMAGE_DISTANCES, 'image')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The maximum distance value of `6.88568` obtained the highest f1 score, so we use this distance for our final predictions.","metadata":{}},{"cell_type":"code","source":"MAX_IMAGE_DISTANCE = 6.88568 # best f1 score of 0.6469574990306197","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Attach Image Predictions","metadata":{}},{"cell_type":"code","source":"train['image_pred'] = get_predictions(train, image_distances, image_indices, MAX_IMAGE_DISTANCE)\ndel image_distances, image_indices\n_ = gc.collect()\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***\n# Phash Predictions\nWe use the [perceptual hashes](https://en.wikipedia.org/wiki/Perceptual_hashing) of the product images as duplicate predictions by grouping them together.","metadata":{}},{"cell_type":"code","source":"phash_dict = train.groupby('image_phash').posting_id.agg('unique').to_dict()\ntrain['phash_pred'] = train.image_phash.map(phash_dict)\ndel phash_dict\n_ = gc.collect()\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***\n# Compute Validation F1 Score\nWe combine the three predictions we have made to obtain a single match prediction to evalute our model.","metadata":{}},{"cell_type":"code","source":"def combine_preds_validation(row):\n    pred = np.concatenate([row['title_pred'], row['image_pred'], row['phash_pred']])\n    return np.unique(pred)\n\ndef combine_preds_submission(row):\n    pred = np.concatenate([row['title_pred'], row['image_pred'], row['phash_pred']])\n    return ' '.join(np.unique(pred)) # submission must be in 'pred_A pred_B pred_C ...' format","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['match_pred'] = train.apply(combine_preds_validation, axis = 'columns')\ntrain['f1_score'] = train.apply(get_f1_score, axis = 'columns', pred_col = 'match')\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('validation f1 score = {}'.format(train['f1_score'].mean()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***\n# Get Test Predictions and Submit\nWe now perform the same process on the test set and submit these predictions to the competition. We are only provided 3 rows of data for the test set and will be provided the rest hidden upon submission. Thus, we adjust the number of neighbors accordingly.","metadata":{}},{"cell_type":"code","source":"test_neighbors = min(50, test.shape[0])\n\ntitle_embeddings = get_text_embeddings(test['title'])\n\ntitle_distances, title_indices = knn_predict_embeddings(title_embeddings, 'cosine', test_neighbors)\ndel title_embeddings\n\ntest['title_pred'] = get_predictions(test, title_distances, title_indices, MAX_TITLE_DISTANCE)\ndel title_distances, title_indices\n_ = gc.collect()\n\nTEST_PATH = '../input/shopee-product-matching/test_images/'\n\nimage_embeddings = get_image_embeddings(test['image'], TEST_PATH)\n\nimage_distances, image_indices = knn_predict_embeddings(image_embeddings, 'euclidean', test_neighbors)\ndel image_embeddings\n\ntest['image_pred'] = get_predictions(test, image_distances, image_indices, MAX_IMAGE_DISTANCE)\ndel image_distances, image_indices\n_ = gc.collect()\n\nphash_dict = test.groupby('image_phash').posting_id.agg('unique').to_dict()\ntest['phash_pred'] = test.image_phash.map(phash_dict)\ndel phash_dict\n_ = gc.collect()\n\ntest['matches'] = test.apply(combine_preds_submission, axis = 'columns')\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[['posting_id','matches']].to_csv('submission.csv',index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}