{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is a simple, unsupervised approach to clustering products in this competition. For each test record, [sentence embeddings](https://tfhub.dev/google/collections/universal-sentence-encoder) are generated on `title`. Universal sentence encoders are case insensitive and are robust enough to accommodate small typos. Embeddings whose euclidean distance are within a certain threshold (usually less than 1.) are assumed to belong to the same cluster. Since euclidean distances are reflexive ($Cluster(A)=Cluster(A)$) and symmetric ($Cluster(A)=Cluster(B) \\iff Cluster(B)=Cluster(A)$), [all evaluation criteria](https://www.kaggle.com/c/shopee-product-matching/overview/evaluation) are met.\n\nNeedless to say, it makes more sense to combine the embeddings with features generated from images and perceptual hashes for a better feature set.\n\n**Few implementation notes:**\n* This notebook does not filter for the top 50 similar products alone. But that can be easily done by altering `generate_predictions`\n* I load the entire title set (including repetitions) in this notebook. Alternatively, title strings can either be batched and/or repetitions can be ignored. \n* Submission takes approximately 1 hour.\n* Set `IS_TEST = True` during commit to submit. When `IS_TEST = False`, this notebook evaluates on training data instead. \n\n\n**NOTE:** This is my first public notebook and any feedback is most welcome. Thanks. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import pandas as pd, numpy as np\nimport random\nimport tensorflow_hub as hub\nimport tensorflow as tf\nfrom sklearn.metrics import f1_score\nfrom tqdm import tqdm\nfrom tqdm.contrib.concurrent import process_map\ntqdm.pandas()\n\nIS_TEST = False\nDIST_THRESHOLD = .75\nfl = '../input/shopee-product-matching/train.csv' if not IS_TEST else '../input/shopee-product-matching/test.csv'\ndata = pd.read_csv(fl)\ntot_rows = data.shape[0]\n\nsentence_embed = hub.load(\"../input/use-v4/use_v4\")\ntitle_embeddings = sentence_embed(data['title'].values.tolist()) #512-d\ntitle_embeddings = title_embeddings.numpy() \ntitle_embeddings = pd.DataFrame(title_embeddings)\ntitle_embeddings.columns = [f'title_emb_{i}' for i in range(512)]\ndata = pd.merge(data, title_embeddings, left_index=True, right_index=True)\n\nfeat_cols = [f'title_emb_{i}' for i in range(512)] \nall_vecs = data[feat_cols].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predictions using Sentence embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_predictions(ix):\n    dists = np.linalg.norm(all_vecs-all_vecs[ix], axis=1)\n    indices = np.where(dists<=DIST_THRESHOLD)[0]\n    clusters = data.iloc[indices]['posting_id'].values.tolist()\n    return ' '.join(clusters)\n\ndef generate_ground_truths(ix):\n    lbl = data.iloc[ix]['label_group']\n    gt_indices = data[data['label_group']==lbl].index\n    gt_clusters = data.iloc[gt_indices]['posting_id'].values.tolist()\n    return ' '.join(gt_clusters)\n    \nif __name__ == '__main__':\n    data['matches'] = ''\n    data['matches'] = process_map(generate_predictions, list(range(all_vecs.shape[0])), max_workers=4, chunksize=1000)\n    if not IS_TEST:\n        data['gts'] = ''\n        data['gts'] = process_map(generate_ground_truths, list(range(all_vecs.shape[0])), max_workers=4, chunksize=1000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## F1 Stats on training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"if IS_TEST:\n    data[['posting_id', 'matches']].to_csv('submission.csv', index=False)\nelse:\n    data['gts'] = data['gts'].progress_apply(lambda x: x.split())\n    data['matches'] = data['matches'].progress_apply(lambda x: x.split())\n    tp = data.progress_apply(lambda row: len(set(row['gts']).intersection(row['matches'])), axis=1)\n    fp = data.progress_apply(lambda row: len(set(row['matches'])-set(row['gts'])), axis=1)\n    fn = data.progress_apply(lambda row: len(set(row['gts'])-set(row['matches'])), axis=1)\n    data['f1_score'] = tp/(tp + (fp+fn)/2)\n    print('Train mean F1: ', data['f1_score'].mean())\n    \n    stats = pd.DataFrame()\n    stats['tp'] = tp\n    stats['fp'] = fp\n    stats['fn'] = fn\n    print(stats.describe(percentiles=[(ix+1)*.05 for ix in range(19)]))    ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}