{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cuml\nimport spacy\nimport nltk\nimport re\nimport os\nimport gc\nimport cv2\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras import layers, regularizers\n\nfrom cuml import metrics\nfrom tqdm import tqdm\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n# from sklearn import metrics","metadata":{"papermill":{"duration":10.636579,"end_time":"2021-04-08T16:23:41.980678","exception":false,"start_time":"2021-04-08T16:23:31.344099","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LIMIT = 6\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        tf.config.experimental.set_virtual_device_configuration(\n        gpus[0],\n        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*LIMIT)])\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n    except RuntimeError as e:\n        print(e)\nprint('We will restrict TensorFlow to max %iGB GPU RAM' % LIMIT)\nprint('then RAPIDS can use %iGB GPU RAM' % (16-LIMIT))","metadata":{"papermill":{"duration":4.853004,"end_time":"2021-04-08T16:23:46.849538","exception":false,"start_time":"2021-04-08T16:23:41.996534","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH = '../input/shopee-product-matching/'\ntrain = pd.read_csv(PATH + 'train.csv')\ntest = pd.read_csv(PATH + 'test.csv')\nprint(train.shape, test.shape)","metadata":{"papermill":{"duration":0.310565,"end_time":"2021-04-08T16:23:47.177136","exception":false,"start_time":"2021-04-08T16:23:46.866571","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train = train.groupby('label_group').head(2).reset_index(drop=True)\n# train","metadata":{"papermill":{"duration":0.078467,"end_time":"2021-04-08T16:23:47.281803","exception":false,"start_time":"2021-04-08T16:23:47.203336","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gen = keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255)\ngen","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def input_arcface(df, batch_size=32):\n    img_gen = gen.flow_from_dataframe(df, '../input/shopee-product-matching/train_images/',\n                                      x_col='image', \n                                      y_col='label_group',\n                                      target_size=(256, 256), \n                                      batch_size=batch_size,\n                                      color_mode=\"rgb\",\n                                       )\n    while True:\n        X1 = img_gen.next()\n        yield [X1[0], X1[1]], X1[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ArcFace(layers.Layer):\n    def __init__(self, n_classes=10, s=30.0, m=0.50, regularizer=None, **kwargs):\n        super(ArcFace, self).__init__(**kwargs)\n        self.n_classes = n_classes\n        self.s = s\n        self.m = m\n        self.regularizer = regularizers.get(regularizer)\n\n    def build(self, input_shape):\n        super(ArcFace, self).build(input_shape[0])\n        self.W = self.add_weight(name='W',\n                                 shape=(input_shape[0][-1], self.n_classes),\n                                 initializer='glorot_uniform',\n                                 trainable=True,\n                                 regularizer=self.regularizer)\n\n    def call(self, inputs):\n        x, y = inputs\n        c = K.shape(x)[-1]\n        # normalize feature\n        x = tf.nn.l2_normalize(x, axis=1)\n        # normalize weights\n        W = tf.nn.l2_normalize(self.W, axis=0)\n        # dot product\n        logits = x @ W\n        # add margin\n        # clip logits to prevent zero division when backward\n        theta = tf.acos(K.clip(logits, -1.0 + K.epsilon(), 1.0 - K.epsilon()))\n        target_logits = tf.cos(theta + self.m)\n        # sin = tf.sqrt(1 - logits**2)\n        # cos_m = tf.cos(logits)\n        # sin_m = tf.sin(logits)\n        # target_logits = logits * cos_m - sin * sin_m\n        #\n        logits = logits * (1 - y) + target_logits * y\n        # feature re-scale\n        logits *= self.s\n        out = tf.nn.softmax(logits)\n\n        return out\n\n    def compute_output_shape(self, input_shape):\n        return (None, self.n_classes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMG_SIZE = 256\nn_classes = 11014\n\ninp = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\ny = layers.Input(shape=())\nx = keras.applications.EfficientNetB3(include_top=False, weights='../input/tfkerasefficientnetimagenetnotop/efficientnetb3_notop.h5', pooling='avg')(inp)\noutput = ArcFace(n_classes)([x, y])\n\nmodel = keras.models.Model(inputs=[inp, y], outputs=[output])\nkeras.utils.plot_model(model, show_layer_names=True, show_shapes=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = 0.0005\nmodel.compile(optimizer=keras.optimizers.Adam(learning_rate=lr),\n              loss='categorical_crossentropy', \n              metrics=['categorical_accuracy', 'accuracy'])\n\ntrain['label_group'] = train['label_group'].astype(str)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 16\nEPOCHS = 6\n\nhistory = model.fit_generator(input_arcface(train, BATCH_SIZE),\n                              epochs=EPOCHS,\n                              steps_per_epoch=train.shape[0]//BATCH_SIZE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emb_model = keras.models.Model(inputs=model.layers[-3].input, outputs=model.layers[-3].output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = test.copy()\n# train = pd.concat([train, train]).reset_index(drop=True)","metadata":{"papermill":{"duration":1.83632,"end_time":"2021-04-08T17:33:49.587944","exception":false,"start_time":"2021-04-08T17:33:47.751624","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings = []\nmul = 10\nsteps = len(train) // mul\nif(len(train) % mul != 0):\n    steps += 1\n_ = gc.collect()\nfor i in tqdm(range(steps)):\n    a = i*mul\n    b = (i+1)*mul\n    b = min(b, len(train))\n\n    images = []\n\n    for k, idx in enumerate(train[a:b]['image']):\n        k = k+a\n        img = cv2.imread(PATH + 'test_images/' + idx)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n        images.append(img)\n    embeddings_t = emb_model.predict(\n        np.array(images), use_multiprocessing=True, workers=4)\n    embeddings.append(embeddings_t)\n\ndel model, images, embeddings_t, img\n_ = gc.collect()","metadata":{"papermill":{"duration":3.603721,"end_time":"2021-04-08T17:33:58.004738","exception":false,"start_time":"2021-04-08T17:33:54.401017","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings_cat = np.concatenate(embeddings)\n\ndel embeddings\n_ = gc.collect()\n_","metadata":{"papermill":{"duration":2.071167,"end_time":"2021-04-08T17:34:01.69524","exception":false,"start_time":"2021-04-08T17:33:59.624073","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from cuml.neighbors import NearestNeighbors","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"KNN = 50\nif(len(test) <= 3 ): KNN=2\nmodel = NearestNeighbors(n_neighbors=KNN, metric='cosine')\nmodel.fit(embeddings_cat)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = []\nCHUNK = 1024*4\n\nprint('Finding similar images...')\nCTS = len(embeddings_cat)//CHUNK\nif len(embeddings_cat)%CHUNK!=0: CTS += 1\nfor j in range( CTS ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(embeddings_cat))\n    print('chunk',a,'to',b)\n    distances, indices = model.kneighbors(embeddings_cat[a:b,])\n    \n    for k in range(b-a):\n        IDX = np.where(distances[k,]<0.1)[0]\n        IDS = indices[k,IDX]\n        o = train.iloc[IDS].posting_id.values\n        preds.append(o)\n        \ndel model, distances, indices\n_ = gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# img_pred = []\n# mul = 100\n\n# steps = len(embeddings_cat) // mul\n# if(len(embeddings_cat) % mul != 0):\n#     steps += 1\n\n# for i in tqdm(range(steps)):\n#     a = i*mul\n#     b = (i+1)*mul\n#     b = min(b, len(embeddings_cat))\n#     k = metrics.pairwise_distances(\n#         embeddings_cat[a:b], embeddings_cat, metric='cosine')\n#     k = 1-k\n#     for p in range(b-a):\n#         idx = np.where(k[p, ] > 0.9)[0]\n#         tmp = train.iloc[idx].posting_id.values\n#         img_pred.append(tmp)","metadata":{"papermill":{"duration":1.911492,"end_time":"2021-04-08T17:34:05.18485","exception":false,"start_time":"2021-04-08T17:34:03.273358","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['img_preds'] = preds\n\n# del embeddings_cat, k\n_ = gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# NOW TITLE COLUMN\ntrain['title'] = train['title'].str.lower()\ncorpus = train['title']\n\ncv = TfidfVectorizer(max_features=20_000) \nvectors = cv.fit_transform(corpus).toarray()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title_preds = []\nmul = 1000\n\nsteps = len(vectors) // mul\nif(len(vectors) % mul != 0):\n    steps += 1\n\nfor i in tqdm(range(steps)):\n    a = i*mul\n    b = (i+1)*mul\n    b = min(b, len(vectors))\n    k = metrics.pairwise_distances(vectors[a:b], vectors, metric='cosine')\n    k = 1 - k\n\n    for p in range(b-a):\n        idx = np.where(k[p, ] > 0.7)[0]\n        tmp = train.iloc[idx].posting_id.values\n        title_preds.append(tmp)","metadata":{"papermill":{"duration":1.866699,"end_time":"2021-04-08T17:34:08.700903","exception":false,"start_time":"2021-04-08T17:34:06.834204","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['title_preds'] = title_preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tmp = train.groupby('label_group').posting_id.agg('unique').to_dict()\n# train['target'] = train.label_group.map(tmp)\n\ntmp = train.groupby('image_phash').posting_id.agg('unique').to_dict()\ntrain['oof'] = train.image_phash.map(tmp)","metadata":{"papermill":{"duration":1.858242,"end_time":"2021-04-08T17:34:12.147215","exception":false,"start_time":"2021-04-08T17:34:10.288973","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['final_preds'] = train.apply(\n    lambda x: np.union1d(x['oof'], x['img_preds']), axis=1)\n\ntrain['final_preds'] = train.apply(\n    lambda x: np.union1d(x['title_preds'], x['final_preds']), axis=1)","metadata":{"papermill":{"duration":1.62595,"end_time":"2021-04-08T17:34:15.408561","exception":false,"start_time":"2021-04-08T17:34:13.782611","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def getMetric(col):\n#     def f1score(row):\n#         n = len( np.intersect1d(row.target,row[col]) )\n#         return 2*n / (len(row.target)+len(row[col]))\n#     return f1score","metadata":{"papermill":{"duration":1.777053,"end_time":"2021-04-08T17:34:19.177546","exception":false,"start_time":"2021-04-08T17:34:17.400493","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train['f1'] = train.apply(getMetric('oof'),axis=1)\n# train['f1'].mean()","metadata":{"papermill":{"duration":1.821803,"end_time":"2021-04-08T17:34:22.579034","exception":false,"start_time":"2021-04-08T17:34:20.757231","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train['f1'] = train.apply(getMetric('title_preds'),axis=1)\n# train['f1'].mean()","metadata":{"papermill":{"duration":1.630139,"end_time":"2021-04-08T17:34:25.811965","exception":false,"start_time":"2021-04-08T17:34:24.181826","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train['f1'] = train.apply(getMetric('img_preds'),axis=1)\n# train['f1'].mean()","metadata":{"papermill":{"duration":1.588678,"end_time":"2021-04-08T17:34:29.004771","exception":false,"start_time":"2021-04-08T17:34:27.416093","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train['f1'] = train.apply(getMetric('final_preds'),axis=1)\n# train['f1'].mean()","metadata":{"papermill":{"duration":1.588754,"end_time":"2021-04-08T17:34:32.213924","exception":false,"start_time":"2021-04-08T17:34:30.62517","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['matches'] = train['final_preds'].apply(lambda x: ' '.join(x))\ntrain[['posting_id', 'matches']].to_csv('submission.csv', index=False)","metadata":{"papermill":{"duration":1.686874,"end_time":"2021-04-08T17:34:35.715592","exception":false,"start_time":"2021-04-08T17:34:34.028718","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.to_csv('train.csv', index=False)","metadata":{"papermill":{"duration":1.582722,"end_time":"2021-04-08T17:34:38.878454","exception":false,"start_time":"2021-04-08T17:34:37.295732","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":1.586393,"end_time":"2021-04-08T17:34:42.084144","exception":false,"start_time":"2021-04-08T17:34:40.497751","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}