{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\nfrom sklearn .model_selection import StratifiedKFold, GroupKFold\n\nfrom nltk.corpus import stopwords\nimport pandas as pd\nimport numpy as np\nimport string\nimport re\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/shopee-product-matching/train.csv')\ndf['label_group'], unique = df['label_group'].factorize()\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocessing function helper\n# replace word that concatenate with other word\ndef remove_concatenate_2_words(text):\n    list_words = ['khusus']\n    for w in list_words:\n        text = text.replace(w, '')\n    return text\n\nPUNCT_TO_REMOVE = string.punctuation\ndef remove_punctuation(text):\n    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n\nSTOPWORDS_ID = set(stopwords.words('english'))\nSTOPWORDS_EN = set(stopwords.words('indonesian'))\ndef remove_stopwords(list_text):\n    text_not_in_ID = [word for word in list_text if word not in STOPWORDS_EN]\n    text = [word for word in text_not_in_ID if word not in STOPWORDS_ID]\n    return text\n\n# remove big number and split text that contains word and number\ndef remove_big_number(list_text):\n    words = []\n    for w in list_text:\n        sub_w = re.split('(\\d+)',w)\n        for item in sub_w:\n            try:\n                tmp = int(item)\n                if tmp < 7000:\n                    if (tmp>1000) and (tmp % 100 == 0): # for even number\n                        words.append(str(tmp))\n                    elif (tmp<=1000) and (tmp>100) and (tmp % 10 == 0 ):\n                        words.append(str(tmp))\n                    elif (tmp<=100) and (tmp % 2 == 0):\n                        words.append(str(tmp))\n            except:\n                words.append(item)\n    return words\n\ndef remove_zero_val(list_text):\n    return [w for w in list_text if w not in ['0']]\n\ndef remove_common_words(list_text):\n    common_words = \"hari keren kere kw super baik jual jualan quality best free  kwalitas berkualitas kualitas bagus terbaik kembali dijamin beli gratis murah free diskon ongkir cek berkualitas original asli kualitas uang jaminan jamin terjamin buatan buat kirim wilayah luar kota jawa bali jakarta surabaya bulan month year day tahun hari harian anda your nikmat singapore malaysia indonesia vietnam thailand filipina bangkok jepang buy one get dapat dua two satu meriah kirim send pengiriman paket hemat uang kembali dapat guarantee buatan lokal dalam internasional karya termurah paling murah terbaik cheap murah biaya\".split(' ')\n    return [w for w in list_text if w not in common_words]\n\ndef remove_strange_words(list_text):\n    strange_words = ['aaa', 'aaaa', 'aaaaa', 'abc', 'abcd', 'bb', 'bbb', 'bbbb', 'ccc', 'cccc', 'thn', 'th', 'bln']\n    return [w for w in list_text if w not in strange_words]\n\ndef string_escape(s, encoding='utf-8'):\n    return (\n        s.encode('latin1')  # To bytes, required by 'unicode-escape'\n        .decode('unicode-escape')  # Perform the actual octal-escaping decode\n        .encode('latin1')  # 1:1 mapping back to bytes\n        .decode(encoding)\n    )  # Decode original encoding","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_vectorizer(max_features, max_len, vocab):\n    # max_features: Maximum vocab size.\n    # max_len: Sequence length to pad the outputs to.\n    \n    text_dataset = tf.data.Dataset.from_tensor_slices(vocab)\n    \n    # Create the layer.\n    vectorize_layer = TextVectorization(\n        max_tokens = max_features,\n        output_mode = 'int',\n        output_sequence_length = max_len\n    )\n\n    vectorize_layer.adapt(text_dataset.batch(64))\n\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n    model.add(vectorize_layer)\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preprocess df_unseen title & phash\ndf['title'] = df['title'].apply(lambda x: string_escape(x))\ndf['title'] = df['title'].apply(lambda x: remove_concatenate_2_words(x))\ndf['title'] = df['title'].str.lower()\ndf['title'] = df['title'].apply(lambda x: remove_punctuation(x))\ndf['title'] = df['title'].apply(lambda x: str(x).split())\ndf['title'] = df['title'].apply(lambda x: remove_stopwords(x))\n# df['title'] = df['title'].apply(lambda x: remove_big_number(x))\ndf['title'] = df['title'].apply(lambda x: remove_zero_val(x))\ndf['title'] = df['title'].apply(lambda x: remove_common_words(x))\ndf['title'] = df['title'].apply(lambda x: remove_strange_words(x))\ndf['title'] = df['title'].apply(lambda x: list(np.unique(x)))\n\n# title vocab\nwords = list(df['title'])\nwords = list(np.unique(np.concatenate(words)))\n\n# phash vocab\nphash = list(df['image_phash'].apply(lambda x: list(str(x))))\nphash = list(np.unique(np.concatenate(phash)))\n\n# Text vectorizer\nmodel = text_vectorizer(max_features = 25000, max_len = 100, vocab = words)\nlist_text = [' '.join(x) for x in df['title']]\ntitle_vec = model.predict(list_text)\ndf['title_vec'] = list(title_vec)\n\nmodel = text_vectorizer(max_features = 25, max_len = 25, vocab = phash)\nlist_text = [' '.join(x) for x in df['image_phash']]\nphash_vec = model.predict(list_text)\ndf['phash_vec'] = list(phash_vec)\n\n\nn_classes = df['label_group'].nunique()\nprint(f'n_classes: {n_classes}')\n\n# save to file\ndf.to_parquet(f'./train.parquet', engine='pyarrow')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_parquet('./train.parquet', engine='pyarrow')\ndf_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}