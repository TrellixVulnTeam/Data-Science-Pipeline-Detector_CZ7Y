{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip -q install ../input/pysastrawi/Sastrawi-1.0.1-py2.py3-none-any.whl \n\n## for data\nimport json\nimport pandas as pd\nimport numpy as np\nfrom sklearn .model_selection import StratifiedKFold, GroupKFold\n## for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n## for processing\nimport string\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\n## for bag-of-words\nfrom sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing\n## for explainer\nfrom lime import lime_text\n## for word embedding\nimport gensim\n\nimport gensim.downloader as gensim_api\n## for deep learning\nimport tensorflow as tf\nfrom tensorflow.keras import models, layers, preprocessing as kprocessing\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n## for bert language model\nimport transformers\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom transformers import RobertaTokenizer, TFRobertaModel\n\n# from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocessing function helper\n# replace word that concatenate with other word\ndef remove_concatenate_2_words(text):\n    list_words = ['khusus']\n    for w in list_words:\n        text = text.replace(w, '')\n    return text\n\nPUNCT_TO_REMOVE = string.punctuation\ndef remove_punctuation(text):\n    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n\nSTOPWORDS_ID = set(stopwords.words('indonesian'))\nSTOPWORDS_EN = set(stopwords.words('english'))\ndef remove_stopwords(list_text):\n    text_not_in_ID = [word for word in list_text if word not in STOPWORDS_EN]\n    text = [word for word in text_not_in_ID if word not in STOPWORDS_ID]\n    return text\n\n# remove big number and split text that contains word and number\ndef remove_big_number(list_text):\n    words = []\n    for w in list_text:\n        sub_w = re.split('(\\d+)',w)\n        for item in sub_w:\n            try:\n                tmp = int(item)\n                if tmp < 7000:\n                    if (tmp>1000) and (tmp % 100 == 0): # for even number\n                        words.append(str(tmp))\n                    elif (tmp<=1000) and (tmp>100) and (tmp % 10 == 0 ):\n                        words.append(str(tmp))\n                    elif (tmp<=100) and (tmp % 2 == 0):\n                        words.append(str(tmp))\n            except:\n                words.append(item)\n    return words\n\ndef remove_zero_val(list_text):\n    return [w for w in list_text if w not in ['0']]\n\ndef remove_common_words(list_text):\n    common_words = \"hari keren kere kw super baik jual jualan quality best free  kwalitas berkualitas kualitas bagus terbaik kembali dijamin beli gratis murah free diskon ongkir cek berkualitas original asli kualitas uang jaminan jamin terjamin buatan buat kirim wilayah luar kota jawa bali jakarta surabaya bulan month year day tahun hari harian anda your nikmat singapore malaysia indonesia vietnam thailand filipina bangkok jepang buy one get dapat dua two satu meriah kirim send pengiriman paket hemat uang kembali dapat guarantee buatan lokal dalam internasional karya termurah paling murah terbaik cheap murah biaya\".split(' ')\n    return [w for w in list_text if w not in common_words]\n\ndef remove_strange_words(list_text):\n    strange_words = ['aaa', 'aaaa', 'aaaaa', 'abc', 'abcd', 'bb', 'bbb', 'bbbb', 'ccc', 'cccc', 'thn', 'th', 'bln']\n    return [w for w in list_text if w not in strange_words]\n\ndef text_vectorizer(max_features, max_len, vocab):\n    # max_features: Maximum vocab size.\n    # max_len: Sequence length to pad the outputs to.\n    \n    text_dataset = tf.data.Dataset.from_tensor_slices(vocab)\n    \n    # Create the layer.\n    vectorize_layer = TextVectorization(\n        max_tokens = max_features,\n        output_mode = 'int',\n        output_sequence_length = max_len\n    )\n\n    vectorize_layer.adapt(text_dataset.batch(64))\n\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n    model.add(vectorize_layer)\n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\n    ## clean (convert to lowercase and remove punctuations and characters and then strip\n    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n            \n    ## Tokenize (convert from string to list)\n    lst_text = text.split()\n    ## remove Stopwords\n    if lst_stopwords is not None:\n        for stopwords in lst_stopwords:\n            lst_text = [word for word in lst_text if word not in \n                        stopwords]\n                \n    ## Stemming (remove -ing, -ly, ...)\n    if flg_stemm == True:\n        # english stemming\n        ps = nltk.stem.porter.PorterStemmer()\n        lst_text = [ps.stem(word) for word in lst_text]\n        \n        # indonesian stemming\n#         factory = StemmerFactory()\n#         id_stemmer = factory.create_stemmer()\n\n#         lst_text = [id_stemmer.stem(word) for word in lst_text]\n                \n    ## Lemmatisation (convert the word into root word)\n    if flg_lemm == True:\n        lem = nltk.stem.wordnet.WordNetLemmatizer()\n        lst_text = [lem.lemmatize(word) for word in lst_text]\n        \n    # remove_zero_val\n    lst_text = [w for w in lst_text if w not in ['0']]\n    \n    # remove strange words\n    strange_words = ['aaa', 'aaaa', 'aaaaa', 'abc', 'abcd', 'bb', 'bbb', 'bbbb', 'ccc', 'cccc', 'thn', 'th', 'bln']\n    lst_text = [w for w in lst_text if w not in strange_words]\n            \n    ## back to string from list\n    text = \" \".join(lst_text)\n    return text\n\ndef string_escape(s, encoding='utf-8'):\n    return (\n        s.encode('latin1')  # To bytes, required by 'unicode-escape'\n        .decode('unicode-escape')  # Perform the actual octal-escaping decode\n        .encode('latin1')  # 1:1 mapping back to bytes\n        .decode(encoding)\n    )  # Decode original encoding\n\nlst_stopwords_en = nltk.corpus.stopwords.words(\"english\")\nlst_stopwords_id = nltk.corpus.stopwords.words(\"indonesian\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/shopee-product-matching/train.csv')\ndf['label_group'], _ = df['label_group'].factorize()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n    \"\"\"\n    https://www.kaggle.com/xhlulu/jigsaw-tpu-distilbert-with-huggingface-and-keras\n    \"\"\"\n    tokenizer.enable_truncation(max_length=maxlen)\n    tokenizer.enable_padding(max_length=maxlen)\n    all_ids = []\n    \n    for i in tqdm(range(0, len(texts), chunk_size)):\n        text_chunk = texts[i:i+chunk_size].tolist()\n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n    \n    return np.array(all_ids)\n        \ndef regular_encode(texts, tokenizer, maxlen=512):\n    enc_di = tokenizer.batch_encode_plus(\n        texts, \n#         add_special_tokens = True,\n        return_attention_mask = True,\n        return_token_type_ids=True,\n        pad_to_max_length=True,\n        max_length=maxlen\n        )\n    \n    return np.array(enc_di['input_ids']), np.array(enc_di['attention_mask'])\n\nMAX_LEN = 105\nMODEL = '../input/tfroberta-base-indonesian/roberta-base-indonesian-522M'\ntokenizer = RobertaTokenizer.from_pretrained(MODEL)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preprocess df_train title & phash\ndf['tmp'] = df['title'].apply(lambda x: string_escape(x))\ndf[\"tmp\"] = df[\"tmp\"].apply(lambda x: utils_preprocess_text(\n    x, flg_stemm=False, flg_lemm=False, lst_stopwords=None))\n\n# for BERT\nids, att_mask = regular_encode(list(df[\"tmp\"].values), tokenizer, maxlen=MAX_LEN)\ndf['input_ids'] = list(ids)\ndf['att_mask'] = list(att_mask)\ndel ids, att_mask\n\ndf['tmp'] = df['title'].apply(lambda x: string_escape(x))\ndf['tmp'] = df['tmp'].apply(lambda x: remove_concatenate_2_words(x))\ndf['tmp'] = df['tmp'].str.lower()\ndf['tmp'] = df['tmp'].apply(lambda x: remove_punctuation(x))\ndf['tmp'] = df['tmp'].apply(lambda x: str(x).split())\ndf['tmp'] = df['tmp'].apply(lambda x: remove_stopwords(x))\n#     df['tmp'] = df['tmp'].apply(lambda x: remove_big_number(x))\ndf['tmp'] = df['tmp'].apply(lambda x: remove_zero_val(x))\n#     df['tmp'] = df['tmp'].apply(lambda x: remove_common_words(x))\ndf['tmp'] = df['tmp'].apply(lambda x: remove_strange_words(x))\ndf['tmp'] = df['tmp'].apply(lambda x: list(np.unique(x)))\n\n# for mlp input\n# title vocab\nwords = list(df['tmp'])\nwords = list(np.unique(np.concatenate(words)))\n# Text vectorizer\nmodel = text_vectorizer(max_features = 25000, max_len = 100, vocab = words)\nlist_text = [' '.join(x) for x in df['tmp']]\ntitle_vec = model.predict(list_text)\ndf['title_vec'] = list(title_vec)\ndel model, list_text, title_vec, words\n\nn_classes = df['label_group'].nunique()\nprint(f'n_classes: {n_classes}')\n\ndf.to_parquet(f'/kaggle/working/train.parquet', engine='pyarrow')","metadata":{},"execution_count":null,"outputs":[]}]}