{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n<img src = \"https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Shopee.svg/1200px-Shopee.svg.png\" height = \"400\" width = \"400\">\n\n## Description of the competition\n\nTwo different images of similar wares may represent the same product or two completely different items. Retailers want to avoid misrepresentations and other issues that could come from conflating two dissimilar products. Currently, a combination of deep learning and traditional machine learning analyzes image and text information to compare similarity. But major differences in images, titles, and product descriptions prevent these methods from being entirely effective.\n\n## About Shopee\n\n**Website : [Shopee](https://shopee.com/)** <br>\nShopee is the leading e-commerce platform in Southeast Asia and Taiwan. Customers appreciate its easy, secure, and fast online shopping experience tailored to their region. The company also provides strong payment and logistical support along with a 'Lowest Price Guaranteed' feature on thousands of Shopee's listed products.\n\n## What we need to do\nIn this competition, you’ll apply your machine learning skills to build a model that predicts which items are the same products.\n\n## Other Details\n\n- Evaluation criteria : **F1 Score**\n- Accelerator used for Training : **TPU**\n- Technique : **Siamese model**\n- Loss Function  : **Contrastive Loss**\n\n**Note : Thanks to @tanulsingh077  for creating dataset for Siamese model <br>\nDataset : https://www.kaggle.com/tanulsingh077/shopee-siamese-training <br>\nPreparation notebook : https://www.kaggle.com/tanulsingh077/code-for-data-generation-for-siamese-training/ <br>**","metadata":{}},{"cell_type":"code","source":"#----------------------------\n##installing efficient net models\n#----------------------------\n!/opt/conda/bin/python3.7 -m pip install --upgrade pip\n! pip install -q efficientnet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import necessary Libraries","metadata":{}},{"cell_type":"code","source":"#-------------------------\n#importing necessary libraries\n#-------------------------\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport efficientnet.tfkeras as efn\n\nimport numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport PIL\n\nimport os\n\nfrom kaggle_datasets import KaggleDatasets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Checking TPU access","metadata":{}},{"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GCS_DS_PATH = KaggleDatasets().get_gcs_path(\"shopee-product-matching\")\nTRAIN_PATH = GCS_DS_PATH + \"/train_images/\"\n\ntrain_df = pd.read_csv(\"../input/shopee-product-matching/train.csv\")\n\nlabel2id = dict(zip(range(train_df.label_group.nunique()),train_df.label_group.unique()))\nid2label = dict(zip(train_df.label_group.unique(),range(train_df.label_group.nunique())))\ntrain_df[\"label_group\"] = train_df[\"label_group\"].map(id2label)\ntrain_df.index = train_df[\"image\"]\n\n\ntmp = train_df.groupby('label_group').posting_id.agg('unique').to_dict()\ntrain_df['posting_ids'] = train_df.label_group.map(tmp)\ntrain_df['posting_ids'] = train_df['posting_ids'].apply(lambda x: list(x))\n\ntrain_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Visualization","metadata":{}},{"cell_type":"code","source":"HEIGHT,WIDTH = 512,512\nCHANNELS = 3\n\n#--------------------\n#Display Samples\n#--------------------\n\ntrain_df.index = train_df[\"posting_id\"]\n\ndef filepath_to_arr(filepath):\n    img = tf.keras.preprocessing.image.load_img(filepath,target_size= (HEIGHT,WIDTH))\n    arr = tf.keras.preprocessing.image.img_to_array(img)/255.\n    return arr\n\ndef display_img(training_ids):\n    num_imgs = len(training_ids)\n \n    plt.figure(figsize = (5*num_imgs,10))\n    for i,_id in enumerate(training_ids):\n        plt.subplot(1,num_imgs+1,i+1)\n        \n        filepath = os.path.join(\"../input/shopee-product-matching/train_images\",train_df.loc[_id][\"image\"])\n        plt.title(\"Image : \"+str(i+1))\n        arr = filepath_to_arr(filepath)\n        plt.imshow(arr)\n        plt.axis(\"off\")\n    plt.show()\n\nx = np.random.randint(0,30000,size=1)\nfor j in range(5):\n    display_img(train_df.iloc[x[0] + j][\"posting_ids\"])\n    \ntrain_df = train_df.drop_duplicates(subset=['image'])\ntrain_df.index = train_df[\"image\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Siamese pairs","metadata":{}},{"cell_type":"code","source":"#------------------------\n#siamese dataframe\n#------------------------\n\nsiamese_df = pd.read_csv(\"../input/shopee-siamese-training/siamese_data.csv\")\nsiamese_df.replace(1,2,inplace = True)\nsiamese_df.replace(0,1, inplace = True)\nsiamese_df.replace(2,0, inplace = True)\nsiamese_df[\"label\"] = siamese_df[\"label\"].astype(\"float32\")\nsiamese_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Functions for preprocessing","metadata":{}},{"cell_type":"code","source":"#------------------\n##processing image\n#------------------\ndef process_img(filepath):\n    image = tf.io.read_file(filepath)\n    image = tf.image.decode_jpeg(image, channels=CHANNELS)\n    image = tf.image.convert_image_dtype(image, tf.float32) \n    image = tf.image.resize(image, [HEIGHT,WIDTH])\n    return image\n\n#-----------------------------------\n##adding augmentations to image data\n#-----------------------------------\ndef data_augment(image):\n    p_spatial = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_rotate = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_pixel_1 = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_pixel_2 = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_pixel_3 = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_crop = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n            \n    # Flips\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    if p_spatial > .75:\n        image = tf.image.transpose(image)\n        \n    # Rotates\n    if p_rotate > .75:\n        image = tf.image.rot90(image, k=3) \n    elif p_rotate > .5:\n        image = tf.image.rot90(image, k=2) \n    elif p_rotate > .25:\n        image = tf.image.rot90(image, k=1) \n        \n    \n    if p_pixel_1 >= .4:\n        image = tf.image.random_saturation(image, lower=.7, upper=1.3)\n    if p_pixel_2 >= .4:\n        image = tf.image.random_contrast(image, lower=.8, upper=1.2)\n    if p_pixel_3 >= .4:\n        image = tf.image.random_brightness(image, max_delta=.1)\n        \n    \n    if p_crop > .7:\n        if p_crop > .9:\n            image = tf.image.central_crop(image, central_fraction=.7)\n        elif p_crop > .8:\n            image = tf.image.central_crop(image, central_fraction=.8)\n        else:\n            image = tf.image.central_crop(image, central_fraction=.9)\n    elif p_crop > .4:\n        crop_size = tf.random.uniform([], int(HEIGHT*.8), HEIGHT, dtype=tf.int32)\n        image = tf.image.random_crop(image, size=[crop_size, crop_size, CHANNELS])\n    \n    image = tf.image.resize(image, [HEIGHT,WIDTH])\n    return image\n\n\n#------------------\n#concat two arrays of image pairs\n#------------------\ndef process_img_pair(file_pair,label):\n    im1 = process_img(file_pair[0])\n    im1 = tf.expand_dims(im1,axis=-1)\n    \n    im2 = process_img(file_pair[1])\n    im2 = tf.expand_dims(im2,axis=-1)\n    \n    im_pair = tf.concat([im1,im2],axis=-1)\n    \n    return im_pair,label\n\n#------------------------\n#adding augmentation to image pair\n#------------------------\ndef augment_img_pair(image_pair,label):\n    im1 = image_pair[:,:,:,0]\n    im1 = data_augment(im1)\n    im1 = tf.expand_dims(im1,axis=-1)\n    \n    im2 = image_pair[:,:,:,1]\n    im2 = data_augment(im2)\n    im2 = tf.expand_dims(im2,axis=-1)\n    \n    im_pair = tf.concat([im1,im2],axis=-1)\n    return im_pair,label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Pipeline","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 8 * strategy.num_replicas_in_sync\nSPLIT = int(0.8*len(siamese_df))\n\nSTEPS_PER_EPOCH  = SPLIT//BATCH_SIZE\nVALID_STEPS = (len(siamese_df)-SPLIT)//BATCH_SIZE\nSEED = 143\n\nx = siamese_df.image_1.to_list()\ny = siamese_df.image_2.to_list()\nfilepairs = [[os.path.join(TRAIN_PATH,i),os.path.join(TRAIN_PATH,j)] for i,j in zip(x,y)]\n\nlabels = siamese_df.label.to_list()\n\ndataset = tf.data.Dataset.from_tensor_slices((filepairs,labels))\ndataset = dataset.map(process_img_pair,num_parallel_calls=AUTO)\ndataset = dataset.map(augment_img_pair,num_parallel_calls=AUTO)\n\ntrain_ds = dataset.take(SPLIT)\nval_ds = dataset.skip(SPLIT)\n\nAUTO = tf.data.experimental.AUTOTUNE\n\ntrain_ds = train_ds.cache().repeat().shuffle(BATCH_SIZE*20).batch(BATCH_SIZE).prefetch(AUTO)\nval_ds = val_ds.repeat().batch(BATCH_SIZE).prefetch(AUTO)\nprint(\"Data Pipeline\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for batch in val_ds.take(1):\n    im,label = batch\n    print(im.shape)\n    print(label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Model","metadata":{}},{"cell_type":"code","source":"def create_model():\n    inputs = tf.keras.Input(shape = (HEIGHT,WIDTH,CHANNELS,2,))\n    \n    input_a = inputs[:,:,:,:,0]\n    input_b = inputs[:,:,:,:,1]\n    \n    pretrained = efn.EfficientNetB0(include_top=False, weights='noisy-student',input_shape=[HEIGHT,WIDTH, 3])\n            \n    x = pretrained.output\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n        \n    embed_model = tf.keras.Model(pretrained.input, x)\n    \n    embed_a = embed_model(input_a)\n    embed_b = embed_model(input_b)\n    \n    l1_layer = tf.keras.layers.Lambda(lambda tensors:tf.linalg.norm(tensors[0] - tensors[1], axis=1))\n    outputs = l1_layer([embed_a,embed_b])\n    \n    \n    model = tf.keras.Model(inputs,outputs)\n    \n    return model\n\nmodel = create_model()\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Compile the Model","metadata":{}},{"cell_type":"code","source":"def compile_model(model, lr=0.0001):\n    \n    optimizer = tf.keras.optimizers.Adam(lr=lr)\n    \n    loss = tfa.losses.ContrastiveLoss(margin = 1.0,name = 'loss')\n    \n    metrics = [\n       tf.keras.metrics.Accuracy(name='acc')\n    ]\n\n    model.compile(optimizer=optimizer, loss=loss)\n\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating Callbacks","metadata":{}},{"cell_type":"code","source":"metric = \"val_loss\"\nmode = \"min\"\n\ndef create_callbacks():\n    \n    cpk_path = './best_model.h5'\n    \n    reducelr = tf.keras.callbacks.ReduceLROnPlateau(\n        monitor=metric,\n        mode=mode,\n        factor=0.1,\n        patience=3,\n        verbose=0\n    )\n    \n    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n        filepath=cpk_path,\n        monitor=metric,\n        mode=mode,\n        save_best_only=True,\n        verbose=1,\n    )\n\n    earlystop = tf.keras.callbacks.EarlyStopping(\n        monitor= metric,\n        mode=mode,\n        patience=10, \n        verbose=1\n    )\n    \n    callbacks = [checkpoint, reducelr, earlystop]         \n    \n    return callbacks","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"EPOCHS= 2\nVERBOSE =1\n\n\ntf.keras.backend.clear_session()\n\nwith strategy.scope():\n    \n    #model = create_model()\n    model = tf.keras.models.load_model(\"../input/shopee-contrastiveloss-tensorflow-tpu-training/best_model.h5\")\n    model = compile_model(model, lr=0.0001)\n   \n    callbacks = create_callbacks()\n    \n    history = model.fit(train_ds, \n                        epochs=EPOCHS,\n                        callbacks=callbacks,\n                        validation_data = val_ds,\n                        verbose=VERBOSE,\n                        steps_per_epoch = STEPS_PER_EPOCH,\n                        validation_steps=VALID_STEPS\n                       )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# History plotting","metadata":{}},{"cell_type":"code","source":"loss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs_range = range(len(history.history['val_loss']))\nplt.figure(figsize=(8, 8))\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()\nprint(\"Plotting the History\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training embeddings","metadata":{}},{"cell_type":"code","source":"files_ls = tf.io.gfile.glob(TRAIN_PATH + '*.jpg')\nds = tf.data.Dataset.from_tensor_slices(files_ls)\nds = ds.map(process_img,num_parallel_calls=AUTO)\nds = ds.map(data_augment,num_parallel_calls=AUTO)\nds = ds.batch(BATCH_SIZE)\n\nwith strategy.scope():\n    model = tf.keras.models.load_model(\"./best_model.h5\")\n    embed_model = tf.keras.Model(model.layers[-2].input,model.layers[-2].output)\n    embeddings = embed_model.predict(ds)\nnp.save(\"./embeddings.npy\",embeddings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hope this notebook is helpful. If you have any doubts or suggestions feel free to comment here.\n## An upvote will be very much encouraging for me.\n# Happy kaggling❤","metadata":{}}]}