{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n# Introduction\n\nThis lesson is a continuation of the series on studying reinforcement learning in a series of game simulations, in the previous lessons:\n1. we got acquainted with [the Kaggle virtual environment](https://www.kaggle.com/andrej0marinchenko/000-intro-kaggle-environment)  and the simplest agents , and also created an agent that received a final good grade of 550 points, \n2. in the [second lesson](https://www.kaggle.com/andrej0marinchenko/001-getting-started-connectx) we got to know each other and analyzed the basic models of agents in detail and also wrote our own agents, the best of which can calculate several moves ahead.\n\n\n\nI have prepared a whole course of training material for this competition and I will be glad to share with you:\n1. [000.intro Kaggle Environment](https://www.kaggle.com/andrej0marinchenko/000-intro-kaggle-environment)\n1. [001. Getting Started ConnectX](https://www.kaggle.com/andrej0marinchenko/001-getting-started-connectx)\n    1. [00.simple_agent_random](https://www.kaggle.com/andrej0marinchenko/00-simple-agent-random)\n    1. [01.simple_agent_negamax](https://www.kaggle.com/andrej0marinchenko/01-simple-agent-negamax)\n    1. [02.initial_agent](https://www.kaggle.com/andrej0marinchenko/02-initial-agent)\n    1. [03.one_step_lookahead_agent](https://www.kaggle.com/andrej0marinchenko/03-one-step-lookahead-agent)\n    1. [04.Nstep_lookahead_agent](https://www.kaggle.com/andrej0marinchenko/04-nstep-lookahead-agent)\n    1. [05.Fast Nstep lookahead agent](https://www.kaggle.com/andrej0marinchenko/05-fast-nstep-lookahead-agent)\n1. [002. Q-Learning step-by-step instruction](https://www.kaggle.com/andrej0marinchenko/002-q-learning-step-by-step-instruction)\n    1. [06.play_with_me](https://www.kaggle.com/andrej0marinchenko/06-play-with-me)\n    1. [07.try_to_win_my_agent](https://www.kaggle.com/andrej0marinchenko/07-try-to-win-my-agent)\n    \n    \n\n\n| Number | Agent name | Points per leader board | Time 100 simulations | Win rate % | Losses % | Draws % | Link notebook kaggle | Link Github|\n|---|---|---|---|---|---|---|---|---|\n|1| random_agent | 210 | 709s | 50% | 50% | - | [link random agent](https://www.kaggle.com/andrej0marinchenko/00-simple-agent-random)| [link random agent](https://github.com/BEPb/Kaggle_step_by_step_ConnectX/blob/master/02.step_02/01.random.py)\n|2| negamax_agent | 240 | 1 min 52s | 51% | 49% | - | [link negamax agent](https://www.kaggle.com/andrej0marinchenko/01-simple-agent-negamax)| [link negamax agent](https://github.com/BEPb/Kaggle_step_by_step_ConnectX/blob/master/02.step_02/02.negamax.py)\n|3| initial_agent | 500 | 1min 40s | 87% | 5% | 8% | [link initial agent](https://www.kaggle.com/andrej0marinchenko/02-initial-agent)| [link initial agent](https://github.com/BEPb/Kaggle_step_by_step_ConnectX/blob/master/02.step_02/03.initital_agent.py)\n|4| one_step_agent | 850 | 28s | 71% | 1% | 28% | [link one step agent](https://www.kaggle.com/andrej0marinchenko/03-one-step-lookahead-agent)| [link one step agent](https://github.com/BEPb/Kaggle_step_by_step_ConnectX/blob/master/02.step_02/04.step_lookahead_agent.py)\n|5| N_step_agent (3 step) | 950 | 15min 23s | 52% | 46% | 2% | [link N step agent](https://www.kaggle.com/andrej0marinchenko/04-nstep-lookahead-agent)| [link N step agent](https://github.com/BEPb/Kaggle_step_by_step_ConnectX/blob/master/02.step_02/05.Nstep_lookahead_agent.pyy)\n|4| Fast_N_step_agent (3 step)      | 1050 | 31min 06s | 93% | 3% | 4% | [link Fsat N step agent](https://www.kaggle.com/andrej0marinchenko/05-fast-nstep-lookahead-agent)| [link Fsat N step agent](https://github.com/BEPb/Kaggle_step_by_step_ConnectX/blob/master/02.step_02/06.fast_Nstep_lookahead_agent.py)\n\n\n\nIn this tutorial, we will take a detailed look at Q Learning, I will explain to you only what you need to understand how it works, nothing more, nothing less. In my [github repository](https://github.com/BEPb/Kaggle_step_by_step_ConnectX) you can download and see the step-by-step description and development of solutions for this competition.\n\n\n\n**Content:**\n1. [Introduction:](#1)\n    1. [Hello Kagglers](#2)\n    1. [Importing Dependencies, and creating a game environment](#3)\n1. [Preparation for use](#4)\n    1. [Our learning tools](#5)\n    1. [What is a Q-table!?](#6)\n1. [Application example](#7)\n1. [Let's summarize our tutorial](#8)\n\n\n\n<a id=\"2\"></a> <br>\n### Hello Kagglers\nThis post was written by me after taking the lead in this type of competition. I went through many training courses, but it was the topic of simulations that I liked the most and I decided to share my knowledge for those who want to figure out what it is.\nConnect Four is a game where two players take turns dropping colored discs into a vertical grid. Each\n the player uses their color (usually red or yellow) and the object of the game is to be the first to get four discs in a row (by\nvertical, horizontal or diagonal).","metadata":{}},{"cell_type":"markdown","source":"# Importing Dependencies and creating a game environment","metadata":{}},{"cell_type":"code","source":"!pip install kaggle-environments -U\n'''\nPython 3.9 Reinforcement Learning Python Starter Program - Reinforcement Learning\nFilename `002. Q-Learning step-by-step instruction.ipynb`\n\nVersion: 0.1\nAuthor: Andrey Marinchenko\nDate: 2022-02-13\n\nlet's use one of the kaggle_environments test games, in particular, with the \"ConnectX\" environment\n'''\nfrom kaggle_environments import make, evaluate, utils, agent  # connect the library kaggle_environments simulation virtual environment and its functions\n'''\nmake - create an environment written in file `core.py`\nevaluate - the function of evaluating and returning a reward for one or more episodes is written in the file `core.py`\nutils - description of the functions of the utilities used in the environment written in file `utils.py`\nagent - agent classes (local and web agent) are described in file `agent.py`\n'''\n\n# additional libraries, we will need them in this notebook\nimport random  # connect the library to generate random numbers, letters, random selection of sequence elements\nimport numpy as np  # connect the library general mathematical and numerical operations\nimport os  # connect the library operating system\nimport inspect  # connect the library provides several useful functions to help get information about live objects such as modules, classes, methods, functions, tracebacks\nimport time  # connect the library time\nfrom datetime import datetime  # connect the library data\n\n#measure notebook running time\nstart_time = time.time()","metadata":{"execution":{"iopub.status.busy":"2022-02-19T19:29:33.961865Z","iopub.execute_input":"2022-02-19T19:29:33.962199Z","iopub.status.idle":"2022-02-19T19:29:44.237737Z","shell.execute_reply.started":"2022-02-19T19:29:33.96211Z","shell.execute_reply":"2022-02-19T19:29:44.236634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4\"></a> <br>\n# Preparation for use\n<a id=\"5\"></a> <br>\n# Our learning tools","metadata":{}},{"cell_type":"code","source":"# step(actions, logs=None)  # Execute the environment interpreter using the current state and a list of actions. Actions to pair up with the current agent states.\n'''\nin order to see how the values of our environment variables change on each turn, we will apply the following function, but it will have to be applied in a loop and in training mode.\nIn this example, we will use two agents:\nthe first agent will always move to the zero cell, and the second agent will move randomly.\n'''\nenv=make(\"connectx\")  # in this line we create our virtual environment\n\n\ntrainer = env.train([None, \"random\"])  # Training agent in first position (player 1) against the default random agent.\n\ndone = False  # set a boolean variable done\nwhile not done:  # set the loop condition until the game status is false (i.e. it does not end with the victory of one of the players) it continues\n    action = 0 # Action for the agent being trained.\n    '''in next line we get all the necessary information about the state of the game and assign the result of this state to new variables'''\n    obs, reward, done, info = trainer.step(action)\n    \n    '''in next line we print the result of the game state'''\n    print('Playing field state \\n', obs, '\\n Reward: ', reward, ', Done:', done, ', Add info:', info)\n    \nenv.render(mode=\"ipython\",width=500,height=500)  # for ease of display, select a screen size of 500 by 500 pixels","metadata":{"execution":{"iopub.status.busy":"2022-02-19T19:29:44.239956Z","iopub.execute_input":"2022-02-19T19:29:44.240262Z","iopub.status.idle":"2022-02-19T19:29:44.428787Z","shell.execute_reply.started":"2022-02-19T19:29:44.24022Z","shell.execute_reply":"2022-02-19T19:29:44.427637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, at first we printed out the result of each move. The state of the board on each move is a list of 42 integers, before the start of the game it consisted of only 42 zeros, on the first move the first player threw the ball into the zero cell, and on the second move the second player was random. Thus, you see in the board line the last 7 digits characterize the bottom line of the game board, and the first 7 values mean the positions of the top line.\n- obs - observation\n    - 'remainingOverageTime': 60, \n    - 'step': 2 - move number\n    - 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0], checkerboard surface stored as a one-dimensional array, the position of 42 chips on the field, where 0 is no chip, 1 is the chip of the first player, 2 is the chip of the second player. The list starts at the top left token and ends at the bottom right.\n    - 'mark': 1 - player number\n- reward - default 0, in case of victory 1,  in case of loss -1\n- done - variable characterizing the end of the program, false if the game continues and true if the game is over\n- info - Additional Information\n\n\nAt each step, we get a new state, a reward, whether or not the environment is completed.","metadata":{}},{"cell_type":"code","source":"out = env.render(mode=\"ansi\")  # very convenient and simple mode of displaying the result of the game\nprint(out)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T19:29:44.430268Z","iopub.execute_input":"2022-02-19T19:29:44.430527Z","iopub.status.idle":"2022-02-19T19:29:44.43596Z","shell.execute_reply.started":"2022-02-19T19:29:44.430497Z","shell.execute_reply":"2022-02-19T19:29:44.435201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The essence of reinforcement learning lies in continuous learning due to the feedback of the environment in the process of interaction between the agent and the environment. Having received the current state, the agent performs an action according to its own algorithm (i.e., where to make a move on the game board)), the environment will update its state after receiving this action and return the new state and reward to the agent, forming an interaction loop. In this experiment, when the game is not over, the environment feedback reward is 0, when the game is over, the environment feedback agent's reward can be -1, 1, or a decimal close to 0, where -1 means a loss. In this game, 1 means a win, and a decimal close to 0 means a draw.\n\nParameters with which we will work:\n\n- Agents: players participating in chess games;\n- Environment: general environment of the game board;\n- Status: game board surface;\n- Action: Choice of position for the player to throw. Since there are 7 columns and 6 rows in total (default), the selectable position for tossing a chip is an empty column. You can only throw a chip into an empty column.\n- Rewards: 0 means not over, 1 means win, -1 means lose, 0.0001 means draw.\n\n\nQ-learning is a model-free reinforcement learning algorithm. The goal of Q-learning is to learn a policy, which tells an agent what action to take under what circumstances. So, above we see a simulation of the game of two simple agents. Knowing the position of all the tokens on all moves, we could *definitely* come up with some kind of algorithm that could calculate whether our agent wins or not if he goes down this path, or should he change branches and drop the token into another column instead .\n\nSpace for observation - the position of the chips on the playing field. This space can be any size, but the bigger it gets, the bigger the Q-table gets!\n\n<a id=\"6\"></a> <br>\n# What is a Q-table!?\nQ-Learning works in such a way that there is a \"Q\" value for every action possible for every state. This\ncreates a table. To figure out all the possible states, we can either query the environment (if it's enough\nkind enough to let us know) ... or we just need to dive into the environment for a while to figure it out.\n\n### Fill-up your Q-Table\nSo, what is q-learning anyway? It is a technique that, given a state of the environnement, either chooses the best actions amongst the possible actions or take a random action for exploration.\n\nFor Q-learning, you need a Q-table. The Q-table is a matrix that maps each possible state to all possible actions. In our simulation, there are 7 columns, so our agent can perform 7 actions, so the matrix will have 7 columns. Strings are a little more difficult. the board I designed has 3 rows and 7 columns, that is 21 squares. Each square can be 0, 1 or 2.\n\n- 0 - nothing,\n- 1 - player chip 1\n- 2 - player 2's chip.\n\nThus, the number of possible combinations: 3 to the 42nd power = 1.09x10 to the 20th power = 109,000,000,000,000,000,000\n\nIf you really wanted to optimize this, you would see the board stay the same when you flip it, so it halves the chances for an even number of columns. Also, most combinations do not exist. You can't have 1,0,1 for a column. But this will not be considered here.\n\n\n## Q-Learning\nIn the cells above I talked about the term Q-learning and Deep Q-Learning, but are these algorithms? let us learn about them and try to implement them....<br>\nQ-learning lets the agent use the environment's rewards to learn, over time, the best action to take in a given state. A **Q-value** for a particular state-action combination is representative of the \"quality\" of an action taken from that state. Better Q-values imply better chances of getting greater rewards.<br>\nQ-values are initialized to an arbitrary value, and as the agent exposes itself to the environment and receives different rewards by executing different actions, the Q-values are updated using the equation:\n> Q(state,action)←(1−α)Q(state,action)+α(reward+γmaxaQ(next state,all actions))\n\n* α (alpha) is the learning rate (0<α≤1) - Just like in supervised learning settings, α is the extent to which our Q-values are being updated in every iteration.\n* γ (gamma) is the discount factor (0≤γ≤1) - determines how much importance we want to give to future rewards. A high value for the discount factor (close to 1) captures the long-term effective award, whereas, a discount factor of 0 makes our agent consider only immediate reward, hence making it greedy.\n\n\nWe are updating, the Q-value of the agent's current state and action by first taking a weight (1−α) of the old Q-value, then adding the learned value. The learned value is a combination of the reward for taking the current action in the current state, and the discounted maximum reward from the next state we will be in once we take the current action.\n\n\n\n\nmaxaQ(next state,all actions), means that Q-value of the current step is based on the Q-value of the future step. This means that we initialize Q-Values for St and St+1 to some random values at first. In the first training iteration we update Q-Value in the state St based on reward and on those random value of Q-Value in the state St+1. Since reward is still guiding our system this will eventually converge to the best result. we are learning the proper action to take in the current state by looking at the reward for the current state/action combo, and the max rewards for the next state.\n\n\n\nAll these Q-Values are stored inside of the **Q-Table**, which is just the matrix with the rows for states and the columns for actions.\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"7\"></a> <br>\n# Application example","metadata":{}},{"cell_type":"code","source":"#let's use one of the kaggle_environments test games, in particular, with the \"ConnectX\" environment\nfrom kaggle_environments import make, evaluate, utils, agent  # connect the library kaggle_environments simulation virtual environment and its functions\n'''\nmake - create an environment written in file `core.py`\nevaluate - the function of evaluating and returning a reward for one or more episodes is written in the file `core.py`\nutils - description of the functions of the utilities used in the environment written in file `utils.py`\nagent - agent classes (local and web agent) are described in file `agent.py`\n'''\n\n# additional libraries, we will need them in this notebook\nimport random  # connect the library to generate random numbers, letters, random selection of sequence elements\nfrom random import choice  # connect the library to generate choose a random item from a sequence. Here seq can be a list, tuple, string, or any iterable like range.\nimport numpy as np  # connect the library general mathematical and numerical operations\nimport os  # connect the library operating system\nimport gym  # toolkit for developing and comparing reinforcement learning algorithms\nfrom tqdm import tqdm  # connect the library show a smart progress\n\n\nenv = make(\"connectx\")  # create our virtual simulation environment\n\n'''we will start training our new agent, which is equivalent to filling in Q-tables and training against \na random agent. I connected its finished version in my own collection of agents. A little lower, the line \nwith our best agent from the previous lesson is commented out. If you want to test the table against the \nbest agent, you just need to uncomment the following line of code\n'''\n# saved_agent = '../input/d/andrej0marinchenko/connectx/01.random.py'\nsaved_agent = '../input/d/andrej0marinchenko/connectx/06.fast_Nstep_lookahead_agent.py'\n\n\n'''create a Q-table class'''\nclass QTable:\n    def __init__(self, action_space):  # class initialization\n        self.table = dict()  # on input state table\n        self.action_space = action_space #  on input state action space\n\n    def add_item(self, state_key):  # the function of adding a cell, if you have not encountered such a situation before\n        self.table[state_key] = list(np.zeros(self.action_space.n))  # create a list filled with zeros corresponding to the given situation\n\n    def __call__(self, state):\n        board = state.board[:]  # Get a copy board\n        board.append(state.mark)  # we supplement the table with the number of the active player (making a move)\n        state_key = np.array(board).astype(str)  # set a new string variable key reflecting the current state of the gaming table\n        state_key = hex(int(''.join(state_key), 3))[2:]  # convert data to hex number\n        if state_key not in self.table.keys():  # if the state has not been encountered before\n            self.add_item(state_key)  # use the add function\n\n        return self.table[state_key]  # returns a completed Q-table\n\n# Environment parameters, this data is set by default in the game simulation, but I will specify it again in this program for clarity\ncols = 7  # number cols\nrows = 6  # number rows\n\n'''we can only perform limited actions, throw chips into the columns, \nso our actions are the choice between the numbers of the columns and is equal to their number'''\naction_space = gym.spaces.Discrete(cols)\n\n'''our environment, all the possible positions that our chips can take, the product of columns by rows'''\nobservation_space = gym.spaces.Discrete(cols * rows)\n\n# configure hyper-parameters\nalpha =  0.1\ngamma = 0.6\nepsilon = 0.99\nmin_epsilon = 0.1\n\n\n'''next parameter is responsible for how long we will train our agent (fill in the kyu-table), \nand reflects the number of games that he will play with a random agent, i.e. the number of situations that our kyu-table will study\n'''\nepisodes = 10000  # 1-10-100-1000-10000-50000\n\n\nalpha_decay_step = 1000\nalpha_decay_rate = 0.9\nepsilon_decay_rate = 0.9999\n\nq_table = QTable(action_space)  # create an instance of the Q-table class\ntrainer = env.train([None, saved_agent])  # let's start training\n\n# create empty lists\nall_epochs = []\nall_total_rewards = []\nall_avg_rewards = []\nall_q_table_rows = []\nall_epsilons = []\n\n\n'''we fill the table by episodes of the game with a random agent, to display progress we use a very convenient function tqdm'''\nfor i in tqdm(range(episodes)):\n    state = trainer.reset()  # on each episode we reset the training\n    epsilon = max(min_epsilon, epsilon * epsilon_decay_rate)  # calculate the current state of the epsilon variable for this simulation episode\n    epochs, total_rewards = 0, 0  # set initial values for variables\n    done = False # set the initial state of the variable, the game episode is not over\n\n    while not done:  # the cycle will continue until the game ends in a win or a draw\n        if random.uniform(0, 1) < epsilon: # selects a random value from a uniform distribution between 0 and 1 and compares it with the current epsilon value\n            # in next line write in the list of possible actions only those in which the columns are not completely filled\n            action = choice([c for c in range(action_space.n) if state.board[c] == 0])  \n\n        else:\n            row = q_table(state)[:]\n            selected_items = []\n            for j in range(action_space.n):\n                if state.board[j] == 0:\n                    selected_items.append(row[j])\n                else:\n                    selected_items.append(-1e7)\n            action = int(np.argmax(selected_items))\n\n        next_state, reward, done, info = trainer.step(action)\n\n        # apply new rules\n        if done:\n            if reward == 1:\n                reward = 20\n            elif reward == 0:\n                reward = -20\n            else:\n                reward = 10\n\n        else:\n            reward = -0.05\n\n        old_value = q_table(state)[action]\n        next_max = np.argmax(q_table(next_state))\n\n        # update q value\n        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n        q_table(state)[action] = new_value\n\n        state = next_state\n        epochs += 1\n        total_rewards += reward\n\n    all_epochs.append(epochs)\n    all_total_rewards.append(total_rewards)\n    avg_rewards = np.mean(all_total_rewards[max(0, i - 100): (i + 1)])\n    all_avg_rewards.append(avg_rewards)\n    all_q_table_rows.append(len(q_table.table))\n    all_epsilons.append(epsilon)\n\n    if (i + 1) % alpha_decay_step == 0:\n        alpha += alpha_decay_rate\n\ntmp_dict_q_table = q_table.table.copy()\ndict_q_table = dict()\n\nfor k in tmp_dict_q_table:\n    if np.count_nonzero(tmp_dict_q_table[k]) > 0:\n        dict_q_table[k] = int(np.argmax(tmp_dict_q_table[k]))\n\n\ndef my_agent(observation, configuration):\n    from random import choice\n\n    q_table = dict_q_table\n    board = observation.board[:]\n    board.append(observation.mark)\n    state_key = list(map(str, board))\n    state_key = hex(int(''.join(state_key), 3))[2:]\n\n    if state_key not in q_table.keys():\n        return choice([c for c in range(configuration.columns)\n                       if observation.board[c] == 0])\n    action = q_table[state_key]\n\n    if observation.board[action] != 0:\n        return choice([c for c in range(configuration.columns)\n                       if observation.board[c] == 0])\n    return action\n\n\nenv.render(mode=\"ipython\")\n\n\nagent = \"\"\"def my_agent(observation, configuration):\n    from random import choice\n\n    q_table = \"\"\" + str(dict_q_table).replace(\" \", \"\") + \"\"\"\n    board = observation.board[:]\n    board.append(observation.mark)\n    state_key = list(map(str, board))\n    state_key = hex(int(''.join(state_key), 3))[2:]\n\n    if state_key not in q_table.keys():\n        return choice([c for c in range(configuration.columns) \n                       if observation.board[c] == 0])\n    action = q_table[state_key]\n\n    if observation.board[action] != 0:\n        return choice([c for c in range(configuration.columns) \n                       if observation.board[c] == 0])\n    return action \"\"\"\n\n\n# write our agent to the dispatch file \"submission.py\"\nwith open(\"submission.py\", 'w') as f:  # create a file\n    f.write(agent)  # write our agent","metadata":{"execution":{"iopub.status.busy":"2022-02-19T19:29:44.438269Z","iopub.execute_input":"2022-02-19T19:29:44.438509Z","iopub.status.idle":"2022-02-19T19:45:37.917314Z","shell.execute_reply.started":"2022-02-19T19:29:44.43848Z","shell.execute_reply":"2022-02-19T19:45:37.914202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''we can see how long the Q-table was'''\nlen(q_table.table)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T19:45:37.918214Z","iopub.status.idle":"2022-02-19T19:45:37.91853Z","shell.execute_reply.started":"2022-02-19T19:45:37.918363Z","shell.execute_reply":"2022-02-19T19:45:37.918379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Let me remind you about the agent size limit, initially for this competition it was 1Mb, but now the maximum agent size is 100Mb'''\n\"%s Kb\" % round(os.stat('submission.py').st_size/1024)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T19:45:37.920983Z","iopub.status.idle":"2022-02-19T19:45:37.922265Z","shell.execute_reply.started":"2022-02-19T19:45:37.921974Z","shell.execute_reply":"2022-02-19T19:45:37.922006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And now let's compare the two agents with each other, for these purposes we will apply the comparison function, as a result we will get the percentage of winnings of each player and the percentage of games when the player made mistakes, made impossible moves.","metadata":{}},{"cell_type":"code","source":"# Use default Connect Four setup\nconfig = {'rows': 6, 'columns': 7, 'inarow': 4}\n\ndef get_win_percentages(agent1, agent2, n_rounds=100):\n    # Agent 1 goes first (roughly) half the time\n    for n in tqdm(range(n_rounds-n_rounds//2)):\n        outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], 1)\n\n    # Agent 2 goes first (roughly) half the time\n    for n in tqdm(range(n_rounds-n_rounds//2)):\n        for [a,b] in evaluate(\"connectx\", [agent2, agent1], config, [], 1):\n            outcomes += [[b, a]]\n\n\n    print(\"Agent 1 Win Percentage:\", np.round(outcomes.count([1,-1])/len(outcomes), 2))\n    print(\"Agent 2 Win Percentage:\", np.round(outcomes.count([-1,1])/len(outcomes), 2))\n    print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0]))\n    print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0, None]))","metadata":{"execution":{"iopub.status.busy":"2022-02-19T19:45:37.923248Z","iopub.status.idle":"2022-02-19T19:45:37.923887Z","shell.execute_reply.started":"2022-02-19T19:45:37.923587Z","shell.execute_reply":"2022-02-19T19:45:37.923624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"for the objectivity of the result, we will play 1000 games between the tested agents, while half of the games are started by one agent, and the remaining half he already plays second at the beginning of the game.","metadata":{}},{"cell_type":"code","source":"%%time\nagent1 = \"submission.py\"  # just signed up our agent\nagent2 = \"../input/d/andrej0marinchenko/connectx/01.random.py\"  # random agent on which we trained our agent    \nget_win_percentages(agent1, agent2, n_rounds=1000) ","metadata":{"execution":{"iopub.status.busy":"2022-02-19T19:45:37.925781Z","iopub.status.idle":"2022-02-19T19:45:37.92634Z","shell.execute_reply.started":"2022-02-19T19:45:37.926063Z","shell.execute_reply":"2022-02-19T19:45:37.926091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"8\"></a> <br>\n# Let's summarize our tutorial","metadata":{}},{"cell_type":"markdown","source":"# Comparison table of the game against a random (random) agent\n\n| Number |     Agent name       | Time train | Size Kb | Len Q-table | Time 1000 simulations | Win rate % | Losses % | Draws % \n|--------|----------------------|------------|---------|-------------|-----------------------|------------|----------|---------|\n| 1      | Q_table_1episode     | 03s        | 001 Kb  | 13          | 01min 15s             | 51%        | 49%      | -       | \n| 2      | Q_table_10episode    | 25s        | 001 Kb  | 75          | 01min 14s             | 53%        | 46%      | 1%      |\n| 3      | Q_table_100episode   | 04min 37s  | 010 Kb  | 747         | 01min 32s             | 54%        | 45%      | 1%      |\n| 4      | Q_table_1000episodes | 49min 16s  | 078 Kb  | 5854        | 01min 22s             | 55%        | 45%      | -       | \n| 5      | Q_table_10000episodes| -          | 1095 Kb | 78785       | 01min 22s             | 55%        | 45%      | -       |\n\n\n\n# Comparison table of the game against the fast N step agent\ntake our best agent and start training Q-table\n\n| Number |     Agent name       | Time train | Size Kb | Len Q-table | Time 100 simulations | Win rate % | Losses % | Draws % \n|--------|----------------------|------------|---------|-------------|----------------------|------------|----------|---------|\n| 1      | Q_table_1episode     | 02s        | 001 Kb  | 8           | 04min 09s            | 00%        | 100%     | -       | \n| 2      | Q_table_10episode    | 27s        | 002 Kb  | 78          | 01min 14s            | 00%        | 100%     | -       |\n| 3      | Q_table_100episode   | 04min 33s  | 010 Kb  | 730         | 03min 49s            | 00%        | 100%     | -       |\n| 4      | Q_table_1000episodes | 47min 42s  | 077 Kb  | 5818        | 03min 13s            | 00%        | 100%     | -       | \n\n","metadata":{}},{"cell_type":"code","source":"end_time = time.time()\nprint(\"Notebook run time: {:.1f} seconds. Finished at {}\".format(end_time - start_time, datetime.now()) )","metadata":{"execution":{"iopub.status.busy":"2022-02-19T19:45:37.927703Z","iopub.status.idle":"2022-02-19T19:45:37.928155Z","shell.execute_reply.started":"2022-02-19T19:45:37.927912Z","shell.execute_reply":"2022-02-19T19:45:37.927935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Work in progress, will be updated","metadata":{}}]}