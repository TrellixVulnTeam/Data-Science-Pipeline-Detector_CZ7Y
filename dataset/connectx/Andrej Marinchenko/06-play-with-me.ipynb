{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This code is written as part of a series of tutorials and allows you to play with the basic simulation model yourself. All you need to do is run the code and use the keyboard to enter the column number for your move\n1. If you would like to learn more about the environment, please follow [this link](https://www.kaggle.com/andrej0marinchenko/000-intro-kaggle-environment);\n2. If you want to learn basic models as well as write your own, follow [this link](https://www.kaggle.com/andrej0marinchenko/001-getting-started-connectx).","metadata":{}},{"cell_type":"code","source":"!pip install 'kaggle-environments>=0.1.6'\n!pip install 'recordtype'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-18T16:19:30.338462Z","iopub.execute_input":"2022-02-18T16:19:30.338801Z","iopub.status.idle":"2022-02-18T16:19:50.140193Z","shell.execute_reply.started":"2022-02-18T16:19:30.338711Z","shell.execute_reply":"2022-02-18T16:19:50.138899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time\nfrom IPython.display import clear_output\nfrom kaggle_environments import make as make_game\nfrom random import choice\nfrom recordtype import recordtype\n\nExperienceStep = recordtype('ExperienceStep', [\n    'game_id',\n    'current_network_input',\n    'action',\n    'next_network_input',\n    'last_episode_action',\n    'episode_reward',\n])\n\n# Collect user input\ndef get_input(user, observation, configuration):\n    ncol = configuration.columns\n    time.sleep(0.1)\n    input1 = 'Input from player {}: '.format(your_name)\n    while True:\n        try:\n            print('Enter Value from 1 to 7')\n            raw_input = input(input1)\n            user_input = int(raw_input)\n        except ValueError:\n            try:\n                print('Invalid input:', user_input)\n                continue\n            except UnboundLocalError:\n                user_input = -1\n                if raw_input == 'q':\n                    break\n                continue\n        np_board = obs_to_board(observation, configuration)\n        valid_actions = np.where(np_board[0] == 0)[0]\n        if user_input <= 0 or user_input > ncol or (\n                user_input - 1) not in valid_actions:\n            print('invalid input:', user_input)\n            print('Valid actions: {}'.format(valid_actions + 1))\n        else:\n            return user_input - 1\n\n\n# Convert the 1D observation list to a 2D numpy array\ndef obs_to_board(observation, configuration):\n    return np.array(observation.board).reshape(\n        configuration.rows, configuration.columns)\n\n\ndef check_winner(observation):\n    '''\n    This function returns the value of the winner.\n    INPUT:  observation\n    OUTPUT: 1 for user Winner or 2 for Computer Winner\n    '''\n    line1 = observation.board[0:7]  # bottom row\n    line2 = observation.board[7:14]\n    line3 = observation.board[14:21]\n    line4 = observation.board[21:28]\n    line5 = observation.board[28:35]\n    line6 = observation.board[35:42]\n\n    board = [line1, line2, line3, line4, line5, line6]\n\n    # Check rows for winner\n    for row in range(6):\n        for col in range(4):\n            if (board[row][col] == board[row][col + 1] == board[row][col + 2] == (\n                    board[row][col + 3])) and (board[row][col] != 0):\n                return board[row][col]  # Return Number that match row\n\n    # Check columns for winner\n    for col in range(7):\n        for row in range(3):\n            if (board[row][col] == board[row + 1][col] == board[row + 2][col] == (\n                    board[row + 3][col])) and (board[row][col] != 0):\n                return board[row][col]  # Return Number that match column\n\n    # Check diagonal (top-left to bottom-right) for winner\n    for row in range(3):\n        for col in range(4):\n            if (board[row][col] == board[row + 1][col + 1] == board[\n                row + 2][col + 2] == \\\n                board[row + 3][col + 3]) and (board[row][col] != 0):\n                return board[row][col]  # Return Number that match diagonal\n\n    # Check diagonal (bottom-left to top-right) for winner\n    for row in range(5, 2, -1):\n        for col in range(4):\n            if (board[row][col] == board[row - 1][col + 1] == (\n                    board[row - 2][col + 2]) == board[row - 3][col + 3]) and (\n                    board[row][col] != 0):\n                return board[row][col]  # Return Number that match diagonal\n\n    # No winner: return None\n    return None\n\n\n# Custom class to reuse data of subsequent interations with the environment\n# FIFO buffer. Experience buffer (also referred to as the replay buffer).\nclass ExperienceBuffer:\n    def __init__(self, buffer_size):\n        self.buffer_size = buffer_size\n        self.episode_offset = 0\n        self.data = []\n        self.episode_ids = np.array([])\n\n    def add(self, data):\n        episode_ids = np.array([d.game_id for d in data])\n        num_episodes = episode_ids[-1] + 1\n        if num_episodes > self.buffer_size:\n            # Keep most recent experience of the experience batch\n            data = data[\n                   np.where(episode_ids == (num_episodes - self.buffer_size))[0][0]:]\n            self.data = data\n            self.episode_ids = episode_ids\n            self.episode_offset = 0\n            return\n\n        episode_ids = episode_ids + self.episode_offset\n        self.data = data + self.data\n        self.episode_ids = np.concatenate([episode_ids, self.episode_ids])\n\n        unique_episode_ids = pd.unique(self.episode_ids)\n        if unique_episode_ids.size > self.buffer_size:\n            cutoff_index = np.where(self.episode_ids == unique_episode_ids[\n                self.buffer_size])[0][0]\n            self.data = self.data[:cutoff_index]\n            self.episode_ids = self.episode_ids[:cutoff_index]\n        self.episode_offset += num_episodes\n\n    def get_all_data(self):\n        return self.data\n\n    def size(self):\n        return len(self.data)\n\n    def num_episodes(self):\n        return np.unique(self.episode_ids).size\n\n\nyour_name = 'You'  # @param {type:\"string\"}\nplay_against_random = True  # @param [\"False\", \"True\"] {type:\"raw\"}\nplot_resolution = 400  # @param {type:\"slider\", min:200, max:500, step:1}\n\n\n# Here we define an agent that picks a random non-empty column\ndef my_random_agent(observation, configuration):\n    return int(choice([c for c in range(\n        configuration.columns) if observation.board[c] == 0]))\n\n\ndef play_against_agent(opponent_agent):\n    # Play as first position against the opposing agent.\n    env = make_game(\"connectx\", debug=False, configuration={\"timeout\": 10})\n    trainer = env.train([None, opponent_agent])\n    observation = trainer.reset()\n\n    while not env.done:\n        clear_output(wait=True)  # Comment if you want to keep track of every action\n        print(\"{}'s color: Blue\".format(your_name))\n        env.render(mode=\"ipython\", width=plot_resolution, height=plot_resolution,\n                   header=False, controls=False)\n\n        my_action = get_input(your_name, observation, env.configuration)\n        if my_action is None:\n            print(\"Exiting game after pressing q\")\n            return\n\n        observation, reward, done, info = trainer.step(my_action)\n        # print(observation, reward, done, info)\n        if (check_winner(observation) == 1):\n            print(\"You Won, Amazing! \\nGAME OVER\")\n\n        elif (check_winner(observation) == 2):\n            print(\"The opponent Won! \\nGAME OVER\")\n\n    if (check_winner(observation) is None):\n        print(\"That is a draw between you and the opponent\")\n\n    env.render(mode=\"ipython\", width=plot_resolution, height=plot_resolution,\n               header=False, controls=False)\n\n\nif play_against_random:\n    play_against_agent(my_random_agent)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T16:19:50.143206Z","iopub.execute_input":"2022-02-18T16:19:50.143597Z","iopub.status.idle":"2022-02-18T16:20:38.054007Z","shell.execute_reply.started":"2022-02-18T16:19:50.143551Z","shell.execute_reply":"2022-02-18T16:20:38.052726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I have prepared a whole course of training material for this competition and I will be glad to share with you:\n1. [000.intro Kaggle Environment](https://www.kaggle.com/andrej0marinchenko/000-intro-kaggle-environment)\n1. [001. Getting Started ConnectX](https://www.kaggle.com/andrej0marinchenko/001-getting-started-connectx)\n    1. [00.simple_agent_random](https://www.kaggle.com/andrej0marinchenko/00-simple-agent-random)\n    1. [01.simple_agent_negamax](https://www.kaggle.com/andrej0marinchenko/01-simple-agent-negamax)\n    1. [02.initial_agent](https://www.kaggle.com/andrej0marinchenko/02-initial-agent)\n    1. [03.one_step_lookahead_agent](https://www.kaggle.com/andrej0marinchenko/03-one-step-lookahead-agent)\n    1. [04.Nstep_lookahead_agent](https://www.kaggle.com/andrej0marinchenko/04-nstep-lookahead-agent)\n    1. [05.Fast Nstep lookahead agent](https://www.kaggle.com/andrej0marinchenko/05-fast-nstep-lookahead-agent)\n1. [002. Q-Learning step-by-step instruction](https://www.kaggle.com/andrej0marinchenko/002-q-learning-step-by-step-instruction)\n    1. [06.play_with_me](https://www.kaggle.com/andrej0marinchenko/06-play-with-me)\n    1. [07.try_to_win_my_agent](https://www.kaggle.com/andrej0marinchenko/07-try-to-win-my-agent)","metadata":{}}]}