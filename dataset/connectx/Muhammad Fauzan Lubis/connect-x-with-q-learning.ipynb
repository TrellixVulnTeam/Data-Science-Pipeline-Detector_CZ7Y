{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Setup\nDownloading depedencies, and setting up the environment"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Downloading kaggle env\n!pip install 'kaggle-environments>=0.1.6'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make Kaggle Env\nfrom kaggle_environments import evaluate, make, utils\n\nenv = make(\"connectx\", debug=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test Out the environment\nenv.render(mode=\"ipython\", width=500, height=450)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Testing the agents"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating Agents\ndef agent(observation, configuration):\n    from random import choice\n    return choice([c for c in range(configuration.columns) if observation.board[c] == 0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env.reset()\nenv.run([agent, 'random'])\nenv.render(mode=\"ipython\", width=500, height=450)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Q-Learning Time!\nTrying to implement Q-Learning on the problem of connect X"},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import defaultdict\nfrom tqdm.notebook import tqdm\nimport numpy as np\nimport random","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Setting up all of the parameters that's going to be used on the training of the agent"},{"metadata":{"trusted":true},"cell_type":"code","source":"LEARNING_RATE = .1\nDISCOUNT_FACTOR = .6\nEPSILON = .99\nSWITCH_PROBABILITY = .5\n\nPAIR = [None, 'negamax']\n\nepisode = 10000\n\nMIN_EPSILON = .1\nEPSILON_DECAY = .9999\nLR_DECAY = .9\nLR_DECAY_STEP = 1000","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## $\\epsilon-\\text{greedy}$ Policy\n- Get random number $r$ on $[0,1]$\n- $\\max_{Q}(a)$ if $r \\ge \\epsilon$\n- else choose randomly"},{"metadata":{"trusted":true},"cell_type":"code","source":"def epsilon_greedy(n_action, QTable, state):\n    global EPSILON\n    \n    r = random.uniform(0, 1)\n    if r >= EPSILON:\n        curr_state = tuple(state['board'])\n        return np.argmax([QTable[curr_state][c] if state['board'][c] == 0 else -1e9 for c in range(n_action)])\n    else:\n        return random.choice([c for c in range(n_action) if state['board'][c] == 0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Initialize Q-Table $Q(s,a)$ arbitrarily"},{"metadata":{"trusted":true},"cell_type":"code","source":"Q_table = defaultdict(lambda: np.zeros(env.configuration.columns))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The algoritm\n- Observe current state, $s$\n- Choose action $a$ based on the selection of the policies\n- Take action $a$ observe the reward $r$ and the new state $s'$\n- Update the Q-value at the Q table using the observed reward and the maximum reward for the next state\n- Set the new state $s'$ as the state $s$ and repeat until state is terminal"},{"metadata":{"trusted":true},"cell_type":"code","source":"# History\ntotal_reward_per_episode = []\nepoch_per_episode = []\nq_table_row = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer = env.train(PAIR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in tqdm(range(episode)):\n    # Do random change of enemy agent\n    if random.uniform(0, 1) < SWITCH_PROBABILITY:\n        PAIR = PAIR[::-1]\n        trainer = env.train(PAIR)\n    \n    EPSILON = max(MIN_EPSILON, EPSILON * EPSILON_DECAY)\n    state = trainer.reset()\n    done = False\n    \n    epoch, total_reward = 0, 0\n    \n    while not done:\n        action = int(epsilon_greedy(env.configuration.columns, Q_table, state))\n        next_state, reward, done, info = trainer.step(action)\n        \n        if done:\n            if reward == 1:\n                reward = 20\n            elif reward == 0:\n                reward = -20\n            else:\n                reward = 10\n        else:\n            reward = -0.05\n            \n        old_value = Q_table[tuple(state['board'])][action]\n        next_max = np.max(Q_table[tuple(next_state['board'])])\n        \n        new_value = old_value + LEARNING_RATE * (reward + DISCOUNT_FACTOR * next_max - old_value)\n        state = next_state\n        \n        Q_table[tuple(state['board'])][action] = new_value\n        \n        total_reward += reward\n        epoch += 1\n    \n    total_reward_per_episode.append(total_reward)\n    epoch_per_episode.append(epoch)\n    q_table_row.append(len(Q_table))\n    \n    if (i + 1) % LR_DECAY_STEP == 0:\n        LEARNING_RATE *= LR_DECAY","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checking Training Result\nCheck the metrics taken on the training to see how the agent does the training"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(Q_table)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.plot(total_reward_per_episode)\nplt.xlabel('Episode')\nplt.ylabel('Total Rewards')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(epoch_per_episode)\nplt.xlabel('Episode')\nplt.ylabel('Epoch')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(q_table_row)\nplt.xlabel('Episode')\nplt.ylabel('Q-Table Row')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating an Agent\nCreate a python script to submit into Kaggle"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract only the action of the QTable\ntmp_Q_table = Q_table.copy()\naction_dict = dict()\n\nfor a in tmp_Q_table:\n    if np.count_nonzero(tmp_Q_table) > 0:\n        action_dict[a] = int(np.argmax(tmp_Q_table[a]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"agent_file = \"\"\"def my_agent(observation, configuration):\n    from random import choice\n    \n    q_table = \"\"\" + str(action_dict).replace(' ', '') + \"\"\"\n    \n    board = observation.board[:]\n    \n    if tuple(board) not in q_table:\n        return choice([c for c in range(configuration.columns) if observation.board[c] == 0])\n        \n    action = q_table[tuple(board)]\n    \n    if observation.board[action] == 0:\n        return choice([c for c in range(configuration.columns) if observation.board[c] == 0])\n        \n    return action\n    \"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('submission.py', 'w') as sf:\n    sf.write(agent_file)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}