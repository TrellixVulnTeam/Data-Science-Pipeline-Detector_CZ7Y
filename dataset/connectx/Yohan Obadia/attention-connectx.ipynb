{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Attention mechanism for a generic ConnectX\n\nThe purpose here is to trigger a conversation around a possible approach toward a generic model that can play any kind of configurations in terms of number of rows, columns and tokens to align to win.\n\nAn interesting takeaway of such a method is that the training can start with very simple configurations and grow in complexity as the model learns. For example, start with (nrows=3, ncols=3, inarow=2) and finish at (nrows=15, ncols=15, inarow=6).\n\nFor the moment, the results are not satisfying (not even beating negamax with a configuration 4x4 with inarow=3) but we might collectively come up with new ideas to enrich this approach :)\n\nPotential improvement that I already want to try:\n - Add convolutions in the begining (and get rid of positional embedding ?)\n - Improve sampling to focus on interesting board configurations\n - Other optimizer like one-cycle that tends to work well with fewer samples\n\nCredits : \n - jadore801120 for his `Attention-is-all-you-need` package, and especially its `PositionalEmbedding` function.\n - Hieu Phung for his base pytorch Reinforcement Learning ConnectX notebook that I used extensively ! "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Base\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\n\n# Kaggle\nfrom kaggle_environments import make\nfrom kaggle_environments.envs.connectx.connectx import is_win,specification\n\n# Deep learning\nimport gym\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.nn.modules.transformer import TransformerEncoderLayer\nfrom torch.nn.modules.activation import MultiheadAttention\n\nfrom torch.nn import Module\nfrom torch.nn.modules.dropout import Dropout\nfrom torch.nn.modules.linear import Linear\nfrom torch.nn.modules.normalization import LayerNorm\n\nfrom torch.nn import functional as F\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def board_array(observation, env, board3d=False):\n    board = np.array(observation.board).reshape((env.configuration['rows'], env.configuration['columns']))\n    if board3d:\n        b0 = board==0\n        b1 = board==env.mark\n        b2 = board==1+env.mark%2\n        board = np.concatenate([b0,b1,b2], axis=0).reshape((board.shape[0], board.shape[1], 3))\n    return board\n\nclass PositionalEncoding(nn.Module):\n    # Altered function from jadore801120\n    def __init__(self, d_hid, n_position=200, n_dim=1, f=100, concat=False, device=None):\n        super(PositionalEncoding, self).__init__()\n        self.device = device\n        self.concat = concat\n        self.n_dim = n_dim\n        self.f = f\n        # Not a parameter\n        self.register_buffer('pos_table', self._get_sinusoid_encoding_table(n_position, d_hid, n_dim))\n\n    def _get_sinusoid_encoding_table(self, n_position, d_hid, n_dim):\n        ''' Sinusoid position encoding table '''\n        # TODO: make it with torch instead of numpy\n\n        def get_position_angle_vec(position):\n            # Altered formula with a product by Pi\n            return [position * np.pi / np.power(self.f, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)]\n\n        # Only use the cosinus\n        sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)])\n        \n        # Commented version that corresponds to the appropriate positional embedding (not useful here)\n        #sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n        #sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n        \n        # Only keep the cosin version\n        sinusoid_table = np.cos(sinusoid_table[:, 1::2])\n        \n        # Combine the sinusoid_table for both rows and cols (choose the appropriate way to combine them)\n        if n_dim==2:\n            a = sinusoid_table.reshape((n_position, 1,-1))\n            b = sinusoid_table.reshape((1, n_position,-1))\n            sinusoid_table = a*b\n\n        pos_table = torch.FloatTensor(sinusoid_table).unsqueeze(0)\n        if self.device is not None:\n            pos_table = pos_table.to(device)\n        return pos_table\n\n    def forward(self, x):\n        unsqueezed = False\n        if x.ndim-self.n_dim==1:\n            x = x.unsqueeze(0)\n            unsqueezed = True\n        if x.ndim==3:\n            pos = self.pos_table[:, :x.size(1)].clone().detach()\n        elif x.ndim==4:\n            pos = self.pos_table[:, :x.size(1), :x.size(2)].clone().detach()\n        dims = [1]*x.ndim\n        dims[0] = x.size(0)\n        \n        if self.concat:\n            x = torch.cat([x, pos.repeat(dims)], x.ndim-1)\n        else:\n            x + pos.repeat(dims)\n            \n        if unsqueezed:\n            x = x.squeeze(0)\n        return x\n    \nclass ConnectX(gym.Env):\n    def __init__(self, switch_prob=0.5, nrows=6,ncols=7,inarow=4):\n        configuration = {'timeout': 5, 'columns':ncols, 'rows':nrows, 'inarow':inarow, 'steps': 1000}\n        self.env = make('connectx', debug=True, configuration=configuration)\n        self.pair = [None, 'negamax']\n        self.mark = 1\n        self.trainer = self.env.train(self.pair)\n        self.switch_prob = switch_prob\n\n        # Define required gym fields (examples):\n        self.configuration = self.env.configuration\n        self.action_space = gym.spaces.Discrete(self.configuration.columns)\n        self.observation_space = gym.spaces.Discrete(self.configuration.columns * self.configuration.rows)\n\n    def switch_trainer(self):\n        self.pair = self.pair[::-1]\n        self.trainer = self.env.train(self.pair)\n        self.mark = 1+self.mark%2\n\n    def step(self, action):\n        return self.trainer.step(action)\n    \n    def reset(self):\n        #if np.random.random() < self.switch_prob:\n            #self.switch_trainer()\n        return self.trainer.reset()\n    \n    def render(self, **kwargs):\n        return self.env.render(**kwargs)\n    \nclass DeepModel(nn.Module):\n    def __init__(self, n_rows, n_cols, n_heads, d_model, b=5, dropout=0.1, device=None):\n        super(DeepModel, self).__init__()\n        self.n_rows = n_rows\n        self.n_cols = n_cols\n        self.dropout = nn.Dropout(dropout)\n        self.norm1 = LayerNorm(d_model)\n        self.transformer1 = TransformerEncoderLayer(d_model=d_model, nhead=n_heads, dim_feedforward=8)\n        self.linear1 = nn.Linear(d_model, 4)\n        self.transformer2 = TransformerEncoderLayer(d_model=4, nhead=2, dim_feedforward=4)\n        self.linear2 = nn.Linear(4, 4)\n        self.linear3 = nn.Linear(5, 3)\n        self.linear4 = nn.Linear(3, 1)\n\n    def forward(self, x):\n        # Set the upper and lower bounds a bit beyond possible produced values\n        vmax = 25\n        vmin = -22\n        \n        # Collect useful shapes\n        n_rows = x.size(0)\n        n_cols = x.size(1)\n        batch_size = x.size(2)\n        \n        # Get a vector of the number of available slots per columns\n        cnt_left = x[:,:,:,0].sum(dim=0).unsqueeze(2).unsqueeze(0).type(torch.float)\n        cnt_left = torch.cat([cnt_left]*n_rows)\n        \n        # flatten and add the batch dimension\n        x = x.view((n_rows*n_cols, batch_size, -1))\n        \n        # Apply the attention layers followed by a dense one\n        x = self.transformer1(x)\n        x = F.relu(self.linear1(x))\n        x = self.transformer2(x)\n        x = F.relu(self.linear2(x))\n        \n        # Reshape into the game board and batch size\n        x = x.view((n_rows, n_cols,batch_size,-1))\n        \n        \n        #x = x.sum(dim=0)\n        #print(x.shape, cnt_left.shape)\n        # Add the number of tokens left on the column to play\n        x = torch.cat([x,cnt_left],dim=3)\n        \n        # Final dense layer\n        x = F.relu(self.linear3(x))\n        x = self.linear4(x).squeeze(3)\n        \n        # Softmax on the row dim and sum over it squared to keep one value per column\n        x = torch.softmax(x, 0)\n        x = torch.sum(x**2, 0)\n        \n        # Rescale the output in a range slightly beyond the actual range \n        # of possible values to help the model converge\n        x = torch.sigmoid(x) * (vmax-vmin) + vmin\n        \n        return x\n\n\nclass DQN:\n    def __init__(self, num_actions, n_rows, n_cols, n_heads, d_model, gamma, \n                 max_experiences, min_experiences, batch_size, lr, device=None):\n        self.num_actions = num_actions\n        self.batch_size = batch_size\n        self.n_rows = n_rows\n        self.n_cols = n_cols\n        self.n_heads = n_heads\n        self.d_model = d_model\n        self.gamma = gamma\n        self.model = DeepModel(n_rows, n_cols, n_heads, d_model, device=device)\n        self.optimizer = optim.Adam(self.model.parameters(), lr=lr, )\n        self.criterion = nn.MSELoss()\n        self.experience = {'s': [], 'a': [], 'r': [], 's2': [], 'done': []} # The buffer\n        self.max_experiences = max_experiences\n        self.min_experiences = min_experiences\n        self.device = device\n        self.positioner = PositionalEncoding(d_hid=6,n_position=20,n_dim=2,f=100,concat=True,device=device)        \n        \n    def predict(self, inputs):\n        return self.model(inputs)\n\n    def train(self, TargetNet, env):\n        if len(self.experience['s']) < self.min_experiences:\n            # Only start the training process when we have enough experiences in the buffer\n            return 0\n\n        # Randomly select n experience in the buffer, n is batch-size\n        ids = np.random.randint(low=0, high=len(self.experience['s']), size=self.batch_size)\n        states = torch.cat([self.preprocess(self.experience['s'][i], env) for i in ids], dim=2)\n        actions = torch.LongTensor([self.experience['a'][i] for i in ids]).to(self.device)\n        rewards = torch.FloatTensor([self.experience['r'][i] for i in ids]).to(self.device)\n\n        # Prepare labels for training process\n        states_next = torch.cat([self.preprocess(self.experience['s2'][i], env) for i in ids], dim=2)\n        dones = torch.FloatTensor([self.experience['done'][i] for i in ids]).to(self.device).type(torch.bool)\n\n        # Encode actions\n        actions = actions.unsqueeze(1)\n        actions_one_hot = torch.FloatTensor(self.batch_size, self.num_actions).zero_().to(self.device)\n        actions_one_hot = actions_one_hot.scatter_(1, actions, 1).T\n        \n        # Get action values for known past rewards\n        selected_action_values = torch.sum(self.predict(states) * actions_one_hot, dim=0)\n        \n        # Compute the actual values obtained combined with the next expected state-value\n        value_next = TargetNet.predict(states_next).max(dim=0).values\n        actual_values = torch.where(dones, rewards, rewards+self.gamma*value_next).to(device)\n\n        # Update weights\n        self.optimizer.zero_grad()\n        loss = self.criterion(selected_action_values, actual_values)\n        loss.backward()\n        self.optimizer.step()\n        return loss.item()\n\n    # Get an action by using epsilon-greedy\n    def get_action(self, state, env, epsilon):\n        if np.random.random() < epsilon:\n            return int(np.random.choice([c for c in range(self.num_actions) if state.board[c] == 0]))\n        else:\n            prediction = self.predict(self.preprocess(state, env)).detach().cpu().numpy()\n            for i in range(self.num_actions):\n                if state.board[i] != 0:\n                    prediction[i] = -1e7\n            return int(np.argmax(prediction))\n\n    # Method used to manage the buffer\n    def add_experience(self, exp):\n        if len(self.experience['s']) >= self.max_experiences:\n            for key in self.experience.keys():\n                self.experience[key].pop(0)\n        for key, value in exp.items():\n            self.experience[key].append(value)\n\n    def copy_weights(self, TrainNet):\n        self.model.load_state_dict(TrainNet.state_dict())\n\n    def save_weights(self, path):\n        torch.save(self.model.state_dict(), path)\n\n    def load_weights(self, path):\n        self.model.load_state_dict(torch.load(path))\n    \n    # Each state will consist of the board and the mark\n    # in the observations\n    def preprocess(self, state, env):\n        # Convert the observed state into a 3D boolean tensor\n        board = board_array(state, env, True)\n        board = torch.FloatTensor(board)\n        if self.device is not None:\n            board = board.to(self.device)\n            \n        # Add positioning\n        board = self.positioner(board)\n        \n        # add the batch dimension\n        board = board.view((self.n_rows, self.n_cols, 1, -1))\n        \n        return board\n    \ndef play_game(env, TrainNet, TargetNet, epsilon, copy_step):\n    rewards = 0\n    losses = list()\n    iter = 0\n    done = False\n    if np.random.rand()>.5:\n        pass\n        #env.switch_trainer()\n    observations = env.reset()\n    while not done:\n        # Using epsilon-greedy to get an action\n        action = TrainNet.get_action(observations, env, epsilon)\n\n        # Caching the information of current state\n        prev_observations = observations\n\n        # Take action\n        observations, reward, done, _ = env.step(action)\n\n        # Apply new rules\n        if done:\n            if reward == 1: # Won\n                reward = 20\n            elif reward == 0: # Lost\n                reward = -20\n            else: # Draw\n                reward = 10\n        else:\n            # Try to promote the agent to \"struggle\" when playing against negamax agent\n            # as Magolor's (@magolor) idea\n            reward = 0.5\n\n        rewards += reward\n\n        # Adding experience into buffer\n        exp = {'s': prev_observations, 'a': action, 'r': reward, 's2': observations, 'done': done}\n        TrainNet.add_experience(exp)\n\n        # Train the training model by using experiences in buffer and the target model\n        losses.append(TrainNet.train(TargetNet, env))\n        iter += 1\n        if iter % copy_step == 0:\n            # Update the weights of the target model when reaching enough \"copy step\"\n            TargetNet.copy_weights(TrainNet)\n    return rewards, np.mean(losses)\n\ndef rephase_game(env, TrainNet, TargetNet, lr, n_rows, n_cols, inarow):\n    \"\"\"Keep weights of the networks but change everything else that \n    needs to be updated to match the new environment\"\"\"\n    # Update the env\n    env = ConnectX(ncols=n_cols, nrows=n_rows, inarow=inarow)\n    \n    # Reinitialize useful parameters on the TrainNet\n    TrainNet.n_rows = n_rows\n    TrainNet.n_cols = n_cols\n    TrainNet.num_actions = n_cols\n    TrainNet.experience = {'s': [], 'a': [], 'r': [], 's2': [], 'done': []}\n    TrainNet.optimizer = optim.Adam(TrainNet.model.parameters(), lr=lr, )\n    \n    # Reinitialize useful parameters on the TargetNet\n    TargetNet.n_rows = n_rows\n    TargetNet.n_cols = n_cols\n    TargetNet.num_actions = n_cols\n    TargetNet.experience = {'s': [], 'a': [], 'r': [], 's2': [], 'done': []}\n    TargetNet.optimizer = optim.Adam(TargetNet.model.parameters(), lr=lr, )\n    \n    return env, TrainNet, TargetNet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_possible_values = 3 # should never change\nn_pos_dim = 3\nmax_length = 20\n\nn_rows = 3\nn_cols = 3\ninarow = 3\nn_heads = 6\nd_model = n_possible_values + n_pos_dim\n\ngamma = 0.99\ncopy_step = 25\nhidden_units = [100, 200, 200, 100]\nmax_experiences = 10000\nmin_experiences = 100\nbatch_size = 32\nlr = 3e-2\nepsilon = 0.5\ndecay = 0.9999\nmin_epsilon = 0.01\nepisodes = 2000\n\nprecision = 7\n\nenv = ConnectX(ncols=n_cols, nrows=n_rows, inarow=inarow)\nnum_actions = env.action_space.n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_total_rewards = list()\nall_avg_rewards = list()\nall_avg_losses = list()\nall_epsilons = list()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize models\nTrainNet = DQN(num_actions, n_rows, n_cols, n_heads, d_model, gamma, max_experiences, min_experiences, batch_size, lr, device=device)\nTargetNet = DQN(num_actions, n_rows, n_cols, n_heads, d_model, gamma, max_experiences, min_experiences, batch_size, lr, device=device)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"pbar = tqdm(range(episodes))\n\nenv, TrainNet, TargetNet = rephase_game(env, TrainNet,TargetNet,lr,n_rows, n_cols, inarow)\nfor n in pbar:\n    epsilon = max(min_epsilon, epsilon * decay)\n    total_reward, avg_loss = play_game(env, TrainNet, TargetNet, epsilon, copy_step)\n    all_total_rewards.append(total_reward)\n    avg_reward = np.mean(all_total_rewards[-100:])\n    all_avg_rewards.append(avg_reward)\n    all_avg_losses.append(avg_loss)\n    avg_loss = np.mean(all_avg_losses[-100:])\n    all_epsilons.append(epsilon)\n\n    pbar.set_postfix({\n        'episode reward': total_reward,\n        'avg (100 last) reward': avg_reward,\n        'avg (100 last) losses': avg_loss,\n        'epsilon': epsilon\n    })","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Diagnostic tools\n\nWhen changing the parameters (nrows, ncols, inarow) you will observe a drop in reward and an increase in loss."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyze how the loss evolves over time\nplt.plot(all_avg_losses)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyze how the revard evolves over time\nplt.plot(all_avg_rewards)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Have a look at the model playing against negamax\nenv.switch_trainer()\nprint(env.mark)\nplay_game(env, TrainNet, TargetNet, 0.3, copy_step)\nenv.render(mode=\"ipython\", width=500, height=450, )","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}