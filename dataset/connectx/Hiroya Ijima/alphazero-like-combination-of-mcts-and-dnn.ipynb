{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# AlphaZero-like combination of MCTS and DNN","metadata":{}},{"cell_type":"markdown","source":"AlphaZero is a very effective approach, but I lack the resources and the implementation skills to execute it. I suggest an approach inspired by AlphaZero, but note that this approach is different from AlphaZero in many ways.","metadata":{}},{"cell_type":"code","source":"from kaggle_environments import evaluate, make, utils\nfrom kaggle_environments.envs.connectx.connectx import play,is_win\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport time\nimport random\nimport collections\nfrom joblib import Parallel, delayed\nimport joblib","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define Network","metadata":{}},{"cell_type":"code","source":"class Alpha_Net(torch.nn.Module):\n    def __init__(self):\n        super(Alpha_Net,self).__init__()\n        self.block1 = torch.nn.Sequential(\n        torch.nn.Conv2d(2,20,kernel_size=2,padding=0,stride=1),\n        torch.nn.BatchNorm2d(20),\n        torch.nn.LeakyReLU(),\n        torch.nn.Conv2d(20,40,kernel_size=3,padding=0,stride=1),\n        torch.nn.BatchNorm2d(40),\n        torch.nn.LeakyReLU(),\n        torch.nn.Conv2d(40,120,kernel_size=3,padding=0,stride=1),\n        torch.nn.BatchNorm2d(120),\n        torch.nn.LeakyReLU(),\n        )\n        self.block2 = torch.nn.Sequential(\n        torch.nn.Linear(240, 42),\n        torch.nn.LeakyReLU(),\n        torch.nn.Linear(42, 7),\n        )\n        self.t_out = torch.nn.LogSoftmax(dim=1)\n        self.e_out = torch.nn.Softmax(dim=1)\n        self.block3 = torch.nn.Sequential(\n        torch.nn.Linear(240, 16),\n        torch.nn.LeakyReLU(),\n        torch.nn.Linear(16, 1),\n        torch.nn.Tanh()\n        )\n\n    def forward(self, x):\n        x = self.block1(x)\n        x = x.view(-1,240)\n        x1 = self.block2(x)\n        if self.training:\n            x1 = self.t_out(x1)\n        else:\n            x1 = self.e_out(x1)\n        x2 = self.block3(x)\n        x = torch.cat([x1,x2],dim=1)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Monte Carlo tree search with DNN\nI think there are more efficient implementations. I hope you will work on improving this.","metadata":{}},{"cell_type":"code","source":"def alpha_MCTS(net,board,config,start_time=None,info=None,expand=False,root=False,C_p=1000,expand_threshold=1,time_lim=4.0,gamma=0.95,steps=350):#parameter C_p should be tuned\n    if info == None:\n        b1 = np.array([[[1 if p == 1 else 0 for p in board]]]).reshape(1,1,6,7)\n        b2 = np.array([[[1 if p == 2 else 0 for p in board]]]).reshape(1,1,6,7)\n        board2 = np.concatenate([b1,b2],axis=1)\n        board2 = torch.from_numpy(board2).float()\n        pred = net(board2).detach().numpy()[0]\n        policy = pred[:7]\n        value = pred[7]\n        count = [0,0,0,0,0,0,0]\n        info_each = [0,0,0,0,0,0,0]\n        Q = [0,0,0,0,0,0,0]\n    else:\n        policy = info['policy']\n        info_each = info['info_each']\n        count = info['count']\n        Q = info['Q']\n        value = info['value']\n    turn = sum([1 if p != 0 else 0 for p in board])%2 + 1\n    if root:\n        start_time = time.time()\n        for t in range(steps):\n            C = np.log((1+sum(count)+C_p)/C_p) + 1.25\n            select = np.array([Q[i]+C*policy[i]*(np.sqrt(sum(count))/(1+count[i])) for i in range(7)])\n            child_index = int(np.argmax(select))\n            if board[child_index] != 0:\n                Q[child_index] = -1\n            else:\n                next_board = board[:]\n                if is_win(next_board,child_index,turn,config,False):\n                    Q[child_index] = 1\n                else:\n                    play(next_board,child_index,turn,config)\n                    fillness = sum([1 if p != 0 else 0 for p in next_board])\n                    if fillness == 42:\n                        Q[child_index] = 0\n                    else:\n                        if count[child_index] >= expand_threshold:\n                            scc, ie = alpha_MCTS(net,next_board,config,start_time,expand=True,info=info_each[child_index])\n                            Q[child_index] = -scc\n                            info_each[child_index] = ie\n                        else:\n                            if count[child_index] == 0:\n                                scc, pol = alpha_MCTS(net,next_board,config,start_time,info=None)\n                                Q[child_index] = -scc\n                                info_each[child_index] = {'policy':pol,'info_each':[0,0,0,0,0,0,0],'count':[0,0,0,0,0,0,0],'Q':[0,0,0,0,0,0,0],'value':scc}\n            count[child_index] += 1\n            if time.time()-start_time >= time_lim:\n                break\n        return np.power(np.array(count),1) / np.power(np.array(count),1).sum(), np.sum(np.array(Q) * np.array(count)) / sum(count)\n    else:\n        if expand:\n            C = np.log((1+sum(count)+C_p)/C_p) + 1.25\n            select = np.array([Q[i]+C*policy[i]*(np.sqrt(sum(count))/(1+count[i])) for i in range(7)])\n            child_index = int(np.argmax(select))\n            if board[child_index] != 0:\n                Q[child_index] = -1\n            else:\n                next_board = board[:]\n                if is_win(next_board,child_index,turn,config,False):\n                    Q[child_index] = 1\n                else:\n                    play(next_board,child_index,turn,config)\n                    fillness = sum([1 if p != 0 else 0 for p in next_board])\n                    if fillness == 42:\n                        Q[child_index] = 0\n                    else:\n                        if count[child_index] >= expand_threshold:\n                            scc, ie = alpha_MCTS(net,next_board,config,start_time,expand=True,info=info_each[child_index])\n                            Q[child_index] = -gamma*scc\n                            info_each[child_index] = ie\n                        else:\n                            if count[child_index] == 0:\n                                scc, pol = alpha_MCTS(net,next_board,config,start_time,info=None)\n                                Q[child_index] = -gamma*scc\n                                info_each[child_index] = {'policy':pol,'info_each':[0,0,0,0,0,0,0],'count':[0,0,0,0,0,0,0],'Q':[0,0,0,0,0,0,0],'value':scc}\n            count[child_index] += 1\n            re_Q = np.array(Q) * np.array(count)\n            r = np.power(2,sum(count)-1)/(1+np.power(2,sum(count)-1))\n            return np.sum(re_Q)/sum(count)*r + value*(1-r), {'policy':policy,'info_each':info_each,'count':count,'Q':Q,'value':value}\n        else:\n            return value, policy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define functions for training","metadata":{}},{"cell_type":"code","source":"def self_play(net,search=70,gamma=0.95,random_turn=3):\n    env = make('connectx',debug=True)\n    log = []\n    log2 = []\n    turn = 0\n    while True:\n        turn += 1\n        p_log = {}\n        b1 = np.array([[[1 if p == 1 else 0 for p in env.state[0]['observation']['board']]]]).reshape(1,1,6,7)\n        b2 = np.array([[[1 if p == 2 else 0 for p in env.state[0]['observation']['board']]]]).reshape(1,1,6,7)\n        board = np.concatenate([b1,b2],axis=1)\n        liner_board = env.state[0]['observation']['board']\n        p_log['board'] = board[0]\n        pol, value = alpha_MCTS(net,liner_board,env.configuration,root=True,steps=search)\n        p_log['policy'] = pol\n        p_log['reward'] = value  ##different from AlphaZero\n        log.append(p_log)\n        action = random.choices([0,1,2,3,4,5,6],weights=list(pol),k=1)[0]\n        if liner_board[action] != 0 or turn < random_turn:\n            action = random.choice([c for c in range(7) if liner_board[c] == 0])\n        env.step([action,action])\n        if env.state[0]['status'] == 'DONE':\n            break\n    #Augmentation by reflection\n    for i in range(len(log)):\n        b = log[i]['board']\n        board2 = np.array([np.fliplr(b[0]),np.fliplr(b[1])]).copy()\n        policy2 = log[i]['policy'][::-1].copy()\n        log2.append({'board':board2,'policy':policy2,'reward':log[i]['reward']})\n    log = log + log2\n    return log","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class original_loss(torch.nn.Module):\n    def __init__(self):\n        super(original_loss,self).__init__()\n    def forward(self,pred,target):\n        CEL = (-target[:,:7] * pred[:,:7]).sum(dim=1).sum()\n        MSE = torch.pow((target[:,7]-pred[:,7]),2).sum()\n        loss = CEL+MSE\n        return CEL,MSE","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fight(agent,fs,mode=\"random\"):\n    env = make('connectx',debug=True)\n    if fs == 1:\n        trainer = env.train([None,mode])\n    else:\n        trainer = env.train([mode,None])\n    observation = trainer.reset()\n    done = False\n    for step in range(42):\n        action = agent(observation, env.configuration)\n        observation, reward, done, info = trainer.step(action)\n        if done:\n            break\n    return reward","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define Agent to be trained","metadata":{}},{"cell_type":"code","source":"class AlphaN():\n    def __init__(self):\n        self.net = Alpha_Net()\n        self.buff = collections.deque([],maxlen=100000)\n        self.optim = torch.optim.RMSprop(self.net.parameters(),lr=0.001,weight_decay=1e-4)\n        self.criterion = original_loss()\n        self.loss_log = []\n        self.CEL_log = []\n        self.MSE_log = []\n        self.score_log = []\n        self.env = make(\"connectx\", debug=True)\n    def evaluate(self,num,mode='random'):\n        def forward(observation, configuration):\n            policy,_ = alpha_MCTS(self.net,observation.board,configuration,root=True,steps=70)\n            action = random.choices([0,1,2,3,4,5,6],weights=list(policy),k=1)[0]\n            if observation.board[action] != 0:\n                action = random.choice([c for c in range(configuration.columns) if observation.board[c] == 0])\n            return action\n        result1 = Parallel(n_jobs=-1,verbose=0)([delayed(fight)(forward,fs=1,mode=mode) for n in range(num)])\n        result2 = Parallel(n_jobs=-1,verbose=0)([delayed(fight)(forward,fs=2,mode=mode) for n in range(num)])\n        reward1 = sum(result1)\n        reward2 = sum(result2)\n        return reward1 / num, reward2 / num\n    def train(self,num,play_num=40,batch_num=32,batch_size=64,train_loop=50,time_lim=30000):\n        start_time = time.time()\n        for t in range(num):\n            #self play\n            self.net.eval()\n            with torch.no_grad():\n                if t%5 == 0:\n                    score1, score2 = self.evaluate(25,mode='negamax')\n                    self.score_log.append((score1+score2)/2)\n                    print('step: '+str(t)+'  score1: '+str(score1)+'  score2: '+str(score2))\n                if t == 0:\n                    bu = Parallel(n_jobs=-1,verbose=0)([delayed(self_play)(net,search=49) for net in [self.net]*1000])\n                else:\n                    bu = Parallel(n_jobs=-1,verbose=0)([delayed(self_play)(net) for net in [self.net]*play_num])\n                for ebb in bu:\n                    self.buff += ebb\n            #Updating Network\n            self.net.train()\n            for loop in range(train_loop):\n                run_loss = 0\n                run_CEL = 0\n                run_MSE = 0\n                for batchs in range(batch_num):\n                    batch = random.choices(self.buff,k=batch_size)\n                    self.optim.zero_grad()\n                    board = np.array([b['board'] for b in batch])\n                    board = torch.from_numpy(board).float()\n                    policy = np.array([b['policy'] for b in batch])\n                    value = np.array([[b['reward']] for b in batch])\n                    target = np.concatenate([policy,value],axis=1)\n                    target = torch.from_numpy(target).float()\n                    pred = self.net(board)\n                    with torch.autograd.detect_anomaly():\n                        CEL,MSE = self.criterion(pred,target)\n                        loss = CEL + MSE\n                        loss.backward()\n                    self.optim.step()\n                    run_loss += loss.item() / batch_num / batch_size\n                    run_CEL += CEL.item() / batch_num / batch_size\n                    run_MSE += MSE.item() / batch_num / batch_size\n                self.loss_log.append(run_loss)\n                self.CEL_log.append(run_CEL)\n                self.MSE_log.append(run_MSE)\n            print('step: '+str(t)+'  loss: '+str(run_loss))\n            if time.time() - start_time > time_lim:\n                print('time over')\n                break\n        plt.plot(self.score_log)\n        plt.title('Score vsNegaMax')\n        plt.xlabel('step')\n        plt.ylabel('score')\n        plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training","metadata":{}},{"cell_type":"code","source":"agent1 = AlphaN()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"agent1.train(1000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(agent1.loss_log)\nplt.title('total loss')\nplt.xlabel('loop')\nplt.ylabel('loss')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(agent1.MSE_log)\nplt.title('value loss')\nplt.xlabel('loop')\nplt.ylabel('loss')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(agent1.CEL_log)\nplt.title('policy loss')\nplt.xlabel('loop')\nplt.ylabel('loss')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Write the submission file","metadata":{}},{"cell_type":"code","source":"np.set_printoptions(threshold=np.inf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out = \"\"\"\nfrom kaggle_environments.envs.connectx.connectx import play,is_win\nimport time\nimport random\nimport torch\nimport numpy as np\ndef agent(observation,configuration):\n    start_time = time.time()\n    class Alpha_Net(torch.nn.Module):\n        def __init__(self):\n            super(Alpha_Net,self).__init__()\n            self.block1 = torch.nn.Sequential(\n            torch.nn.Conv2d(2,20,kernel_size=2,padding=0,stride=1),\n            torch.nn.BatchNorm2d(20),\n            torch.nn.LeakyReLU(),\n            torch.nn.Conv2d(20,40,kernel_size=3,padding=0,stride=1),\n            torch.nn.BatchNorm2d(40),\n            torch.nn.LeakyReLU(),\n            torch.nn.Conv2d(40,120,kernel_size=3,padding=0,stride=1),\n            torch.nn.BatchNorm2d(120),\n            torch.nn.LeakyReLU(),\n            )\n            self.block2 = torch.nn.Sequential(\n            torch.nn.Linear(240, 42),\n            torch.nn.LeakyReLU(),\n            torch.nn.Linear(42, 7),\n            )\n            self.t_out = torch.nn.LogSoftmax(dim=1)\n            self.e_out = torch.nn.Softmax(dim=1)\n            self.block3 = torch.nn.Sequential(\n            torch.nn.Linear(240, 16),\n            torch.nn.LeakyReLU(),\n            torch.nn.Linear(16, 1),\n            torch.nn.Tanh()\n            )\n\n        def forward(self, x):\n            x = self.block1(x)\n            x = x.view(-1,240)\n            x1 = self.block2(x)\n            if self.training:\n                x1 = self.t_out(x1)\n            else:\n                x1 = self.e_out(x1)\n            x2 = self.block3(x)\n            x = torch.cat([x1,x2],dim=1)\n            return x\n    def alpha_MCTS(net,board,config,start_time=None,info=None,expand=False,root=False,C_p=1000,expand_threshold=1,time_lim=2.0,gamma=0.95,steps=10000000):\n        if info == None:\n            b1 = np.array([[[1 if p == 1 else 0 for p in board]]]).reshape(1,1,6,7)\n            b2 = np.array([[[1 if p == 2 else 0 for p in board]]]).reshape(1,1,6,7)\n            board2 = np.concatenate([b1,b2],axis=1)\n            board2 = torch.from_numpy(board2).float()\n            pred = net(board2).detach().numpy()[0]\n            policy = pred[:7]\n            value = pred[7]\n            count = [0,0,0,0,0,0,0]\n            info_each = [0,0,0,0,0,0,0]\n            Q = [0,0,0,0,0,0,0]\n        else:\n            policy = info['policy']\n            info_each = info['info_each']\n            count = info['count']\n            Q = info['Q']\n            value = info['value']\n        turn = sum([1 if p != 0 else 0 for p in board])%2 + 1\n        if root:\n            for t in range(steps):\n                C = np.log((1+sum(count)+C_p)/C_p) + 1.25\n                select = np.array([Q[i]+C*policy[i]*(np.sqrt(sum(count))/(1+count[i])) for i in range(7)])\n                child_index = int(np.argmax(select))\n                if board[child_index] != 0:\n                    Q[child_index] = -1\n                else:\n                    next_board = board[:]\n                    if is_win(next_board,child_index,turn,config,False):\n                        Q[child_index] = 1\n                    else:\n                        play(next_board,child_index,turn,config)\n                        fillness = sum([1 if p != 0 else 0 for p in next_board])\n                        if fillness == 42:\n                            Q[child_index] = 0\n                        else:\n                            if count[child_index] >= expand_threshold:\n                                scc, ie = alpha_MCTS(net,next_board,config,start_time,expand=True,info=info_each[child_index])\n                                Q[child_index] = -scc\n                                info_each[child_index] = ie\n                            else:\n                                if count[child_index] == 0:\n                                    scc, pol = alpha_MCTS(net,next_board,config,start_time,info=None)\n                                    Q[child_index] = -scc\n                                    info_each[child_index] = {'policy':pol,'info_each':[0,0,0,0,0,0,0],'count':[0,0,0,0,0,0,0],'Q':[0,0,0,0,0,0,0],'value':scc}\n                count[child_index] += 1\n                if time.time()-start_time >= time_lim:\n                    break\n            return np.power(np.array(count),1) / np.power(np.array(count),1).sum()\n        else:\n            if expand:\n                C = np.log((1+sum(count)+C_p)/C_p) + 1.25\n                select = np.array([Q[i]+C*policy[i]*(np.sqrt(sum(count))/(1+count[i])) for i in range(7)])\n                child_index = int(np.argmax(select))\n                if board[child_index] != 0:\n                    Q[child_index] = -1\n                else:\n                    next_board = board[:]\n                    if is_win(next_board,child_index,turn,config,False):\n                        Q[child_index] = 1\n                    else:\n                        play(next_board,child_index,turn,config)\n                        fillness = sum([1 if p != 0 else 0 for p in next_board])\n                        if fillness == 42:\n                            Q[child_index] = 0\n                        else:\n                            if count[child_index] >= expand_threshold:\n                                scc, ie = alpha_MCTS(net,next_board,config,start_time,expand=True,info=info_each[child_index])\n                                Q[child_index] = -gamma*scc\n                                info_each[child_index] = ie\n                            else:\n                                if count[child_index] == 0:\n                                    scc, pol = alpha_MCTS(net,next_board,config,start_time,info=None)\n                                    Q[child_index] = -gamma*scc\n                                    info_each[child_index] = {'policy':pol,'info_each':[0,0,0,0,0,0,0],'count':[0,0,0,0,0,0,0],'Q':[0,0,0,0,0,0,0],'value':scc}\n                count[child_index] += 1\n                re_Q = np.array(Q) * np.array(count)\n                r = np.power(2,sum(count)-1)/(1+np.power(2,sum(count)-1))\n                return np.sum(re_Q)/sum(count)*r + value*(1-r), {'policy':policy,'info_each':info_each,'count':count,'Q':Q,'value':value}\n            else:\n                return value, policy\n    net = Alpha_Net()\n\"\"\"\nfor key in agent1.net.state_dict().keys():\n    if 'num' in key:\n        out += \"    net.state_dict()['\"+key+\"'] = torch.tensor(\"+str(agent1.net.state_dict()[key].item())+\")\\n\"\n    else:\n        out += \"    net.state_dict()['\"+key+\"'][:] = torch.tensor(\"+str(list(agent1.net.state_dict()[key].to(torch.device(\"cpu\")).numpy())).replace('array(', '').replace(')', '').replace(' ', '').replace('\\n', '').replace(',dtype=float32','')+\")\\n\"\nout += \"\"\"\n    net.eval()\n    with torch.no_grad():\n        pol = alpha_MCTS(net,observation.board,configuration,start_time=start_time,root=True)\n    action = int(np.argmax(np.array(pol)))\n    if observation.board[action] != 0:\n        action = random.choice([c for c in range(configuration.columns) if observation.board[c] == 0])\n    return action\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('submission.py', 'w') as f:\n    f.write(out)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test submission","metadata":{}},{"cell_type":"code","source":"from submission import agent","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env = make(\"connectx\", debug=True)\ntrainer = env.train([\"negamax\",None])\n\nobservation = trainer.reset()\n\nwhile not env.done:\n    my_action = agent(observation, env.configuration)\n    print(\"My Action\", my_action)\n    observation, reward, done, info = trainer.step(my_action)\nenv.render()\nprint(reward)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Complete!\nThis is my first public notebook and I'm not an expert. If there are any errors, please point them out to me.","metadata":{}}]}