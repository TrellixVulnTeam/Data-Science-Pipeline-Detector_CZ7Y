{"cells":[{"metadata":{},"cell_type":"markdown","source":"I learned a lot from Alexis's notebook. But one question remains, how do I create a submission file?\nI asked Google and managed to create a submission file, so I'll share it with you.\n\nthanks to:\n* https://www.kaggle.com/alexisbcook/deep-reinforcement-learning\n\n* https://stable-baselines3.readthedocs.io/en/master/\n* https://github.com/DLR-RM/stable-baselines3\n\n* Stable Baselines RL https://www.kaggle.com/c/connectx/discussion/128591\n\n- Hard-coding PyTorch weights into a script\nhttps://www.kaggle.com/c/connectx/discussion/126678","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"UPDATE: \n- Version1: make submission with stable-baseline3/PPO('MlpPolicy')\n- Version2: using custom CNN.\n- Version3: tried using the GPU, but it didn't work at submission.\n- Version4: using GPU only train.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Install stable-baselines3\n\nI happened to find this while looking at the documentation, they use pytorch.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install stable-baselines3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import modules","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import gym\nfrom kaggle_environments import make, evaluate\n\nimport os\nimport numpy as np\nimport torch as th\nfrom torch import nn as nn\nimport torch.nn.functional as F\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.monitor import Monitor\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nfrom stable_baselines3.common.monitor import load_results\nfrom stable_baselines3.common.torch_layers import NatureCNN\nfrom stable_baselines3.common.policies import ActorCriticPolicy, ActorCriticCnnPolicy\nfrom stable_baselines3.common.torch_layers import BaseFeaturesExtractor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Env","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# ConnectX wrapper from Alexis' notebook.\n# Changed shape, channel first.\n# Changed obs/2.0\nclass ConnectFourGym(gym.Env):\n    def __init__(self, agent2=\"random\"):\n        ks_env = make(\"connectx\", debug=True)\n        self.env = ks_env.train([None, agent2])\n        self.rows = ks_env.configuration.rows\n        self.columns = ks_env.configuration.columns\n        # Learn about spaces here: http://gym.openai.com/docs/#spaces\n        self.action_space = gym.spaces.Discrete(self.columns)\n        self.observation_space = gym.spaces.Box(low=0, high=1, \n                                            shape=(1,self.rows,self.columns), dtype=np.float)\n        # Tuple corresponding to the min and max possible rewards\n        self.reward_range = (-10, 1)\n        # StableBaselines throws error if these are not defined\n        self.spec = None\n        self.metadata = None\n    def reset(self):\n        self.obs = self.env.reset()\n        return np.array(self.obs['board']).reshape(1,self.rows,self.columns)/2\n    def change_reward(self, old_reward, done):\n        if old_reward == 1: # The agent won the game\n            return 1\n        elif done: # The opponent won the game\n            return -1\n        else: # Reward 1/42\n            return 1/(self.rows*self.columns)\n    def step(self, action):\n        # Check if agent's move is valid\n        is_valid = (self.obs['board'][int(action)] == 0)\n        if is_valid: # Play the move\n            self.obs, old_reward, done, _ = self.env.step(int(action))\n            reward = self.change_reward(old_reward, done)\n        else: # End the game and penalize agent\n            reward, done, _ = -10, True, {}\n        return np.array(self.obs['board']).reshape(1,self.rows,self.columns)/2, reward, done, _","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create ConnectFour environment\nenv = ConnectFourGym()\nenv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from stable_baselines3.common.env_checker import check_env\n# check_env(env)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create directory for logging training information\nlog_dir = \"log/\"\nos.makedirs(log_dir, exist_ok=True)\n\n# Logging progress\nenv = Monitor(env, log_dir, allow_early_resets=True)\nenv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env = DummyVecEnv([lambda: env])\nenv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env.observation_space.sample()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class Net(BaseFeaturesExtractor):\n    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 512):\n        super(Net, self).__init__(observation_space, features_dim)\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n        self.fc3 = nn.Linear(384, features_dim)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = nn.Flatten()(x)\n        x = F.relu(self.fc3(x))\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"policy_kwargs = {\n    'activation_fn':th.nn.ReLU, \n    'net_arch':[64, dict(pi=[32, 16], vf=[32, 16])],\n    'features_extractor_class':Net,\n}\nlearner = PPO('MlpPolicy', env, policy_kwargs=policy_kwargs)\n\nlearner.policy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nlearner.learn(total_timesteps=100_000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = load_results(log_dir)['r']\ndf.rolling(window=1000).mean().plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Validation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.predict(env.reset())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def testagent(obs, config):\n    import numpy as np\n    obs = np.array(obs['board']).reshape(1, config.rows, config.columns)/2\n    action, _ = learner.predict(obs)\n    return int(action)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_win_percentages(agent1, agent2, n_rounds=100):\n    # Use default Connect Four setup\n    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n    # Agent 1 goes first (roughly) half the time          \n    outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], n_rounds//2)\n    # Agent 2 goes first (roughly) half the time      \n    outcomes += [[b,a] for [a,b] in evaluate(\"connectx\", [agent2, agent1], config, [], n_rounds-n_rounds//2)]\n    print(\"Agent 1 Win Percentage:\", np.round(outcomes.count([1,-1])/len(outcomes), 2))\n    print(\"Agent 2 Win Percentage:\", np.round(outcomes.count([-1,1])/len(outcomes), 2))\n    print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0]))\n    print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0, None]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_win_percentages(agent1=testagent, agent2=\"random\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env = make(\"connectx\", debug=True)\n\n# Two random agents play one game round\nenv.run([testagent, \"random\"])\n\n# Show the game\nenv.render(mode=\"ipython\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Write submission.py","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile submission.py\ndef agent(obs, config):\n    import numpy as np\n    import torch as th\n    from torch import nn as nn\n    import torch.nn.functional as F\n    from torch import tensor\n    \n    class Net(nn.Module):\n        def __init__(self):\n            super(Net, self).__init__()\n            self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n            self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n            self.fc3 = nn.Linear(384, 512)\n            self.shared1 = nn.Linear(512, 64)\n            self.policy1 = nn.Linear(64, 32)\n            self.policy2 = nn.Linear(32, 16)\n            self.action = nn.Linear(16, 7)\n\n        def forward(self, x):\n            x = F.relu(self.conv1(x))\n            x = F.relu(self.conv2(x))\n            x = nn.Flatten()(x)\n            x = F.relu(self.fc3(x))\n            x = F.relu(self.shared1(x))\n            x = F.relu(self.policy1(x))\n            x = F.relu(self.policy2(x))\n            x = self.action(x)\n            x = x.argmax()\n            return x\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.policy.state_dict().keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"th.set_printoptions(profile=\"full\")\n\nagent_path = 'submission.py'\n\nstate_dict = learner.policy.to('cpu').state_dict()\nstate_dict = {\n    'conv1.weight': state_dict['features_extractor.conv1.weight'],\n    'conv1.bias': state_dict['features_extractor.conv1.bias'],\n    'conv2.weight': state_dict['features_extractor.conv2.weight'],\n    'conv2.bias': state_dict['features_extractor.conv2.bias'],\n    'fc3.weight': state_dict['features_extractor.fc3.weight'],\n    'fc3.bias': state_dict['features_extractor.fc3.bias'],\n    \n    'shared1.weight': state_dict['mlp_extractor.shared_net.0.weight'],\n    'shared1.bias': state_dict['mlp_extractor.shared_net.0.bias'],\n    \n    'policy1.weight': state_dict['mlp_extractor.policy_net.0.weight'],\n    'policy1.bias': state_dict['mlp_extractor.policy_net.0.bias'],\n    'policy2.weight': state_dict['mlp_extractor.policy_net.2.weight'],\n    'policy2.bias': state_dict['mlp_extractor.policy_net.2.bias'],\n    \n    'action.weight': state_dict['action_net.weight'],\n    'action.bias': state_dict['action_net.bias'],\n}\n\nwith open(agent_path, mode='a') as file:\n    #file.write(f'\\n    data = {learner.policy._get_data()}\\n')\n    file.write(f'    state_dict = {state_dict}\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile -a submission.py\n\n    model = Net()\n    model = model.float()\n    model.load_state_dict(state_dict)\n    model = model.to('cpu')\n    model = model.eval()\n    obs = tensor(obs['board']).reshape(1, 1, config.rows, config.columns).float()\n    obs = obs / 2\n    action = model(obs)\n    return int(action)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test submission.py","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# load submission.py\nf = open(agent_path)\nsource = f.read()\nexec(source)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# simple test agent\nagent(env.reset()[0]['observation'], env.configuration)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_win_percentages(agent1=agent, agent2=\"random\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env = make(\"connectx\", debug=True)\n\n# Two random agents play one game round\nenv.run([agent, \"random\"])\n\n# Show the game\nenv.render(mode=\"ipython\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}