{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from kaggle_environments import make, evaluate\nfrom kaggle_environments.envs.connectx.connectx import play,is_win\n\nimport os\nfrom pathlib import Path\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as f\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom tqdm import tqdm\n\nimport numpy as np\nimport random\nimport math\nfrom datetime import datetime\nimport copy\nimport time\nimport pickle\n\nif (not os.path.exists(\"logs\")):\n  os.mkdir(\"logs\")\n\nfrom torch.utils.tensorboard import SummaryWriter\n%load_ext tensorboard\n\ntb = SummaryWriter(log_dir=\"logs\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nROWS = 6\nCOLUMNS = 7\n\nN_BLOCKS = 12\nFILTERS = 128\n\nPV_EVALUATE_COUNT = 50\nSP_GAME_COUNT = 500\nSP_TEMPERATURE = 1.0\n\nBATCH_SIZE = 128\nLOOPS = 10\nRN_EPOCHS = 100\nTEST_DATA_NUM = 100\nLEARNING_RATE = 0.01","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Block(nn.Module):\n  def __init__(self, filter):\n    super(Block,self).__init__()\n    self.conv1 = nn.Conv2d(filter, filter, kernel_size=3, padding=1)\n    self.conv2 = nn.Conv2d(filter, filter, kernel_size=3, padding=1)\n\n    self.bn1 = nn.BatchNorm2d(filter)\n    self.bn2 = nn.BatchNorm2d(filter)\n\n    self.relu1 = nn.ReLU()\n    self.relu2 = nn.ReLU()\n\n  def forward(self, x):\n    input = x\n    x = self.conv1(x)\n    x = self.bn1(x)\n    x = self.relu1(x)\n    x = self.conv2(x)\n    x = self.bn2(x)\n    x = x + input\n    x = self.relu2(x)\n    return x\n\nclass ResNet(nn.Module):\n  def __init__(self):\n    super(ResNet,self).__init__()\n    self.conv = nn.Conv2d(2, FILTERS, kernel_size=3, padding=1)\n    self.bn = nn.BatchNorm2d(FILTERS)\n    self.relu = nn.ReLU()\n\n    self.blocks = nn.ModuleList([Block(FILTERS) for _ in range(N_BLOCKS)])\n\n    self.conv_p = nn.Conv2d(FILTERS, 8, kernel_size=1)\n    self.bn_p = nn.BatchNorm2d(8)\n    self.relu_p = nn.ReLU()\n    self.linear_p = nn.Linear(8 * ROWS*COLUMNS, COLUMNS)\n    self.softmax_p = nn.Softmax(dim=1)\n\n    self.conv_v = nn.Conv2d(FILTERS, 4, kernel_size=1)\n    self.bn_v = nn.BatchNorm2d(4)\n    self.relu_v = nn.ReLU()\n    self.linear_v1 = nn.Linear(4 * ROWS*COLUMNS, 64)\n    self.linear_v2 = nn.Linear(64, 1)\n    self.tanh_v = nn.Tanh()\n\n  def forward(self, x):\n    x = self.conv(x)\n    x = self.bn(x)\n    x = self.relu(x)\n\n    for block in self.blocks:\n      x = block(x)\n\n    x_p = self.conv_p(x)\n    x_p = self.bn_p(x_p)\n    x_p = self.relu_p(x_p)\n    x_p = x_p.view(x_p.size(0), -1)\n    x_p = self.linear_p(x_p)\n    policy = self.softmax_p(x_p)\n\n\n    x_v = self.conv_v(x)\n    x_v = self.bn_v(x_v)\n    x_v = self.relu_v(x_v)\n    x_v = x_v.view(x_v.size(0), -1)\n    x_v = self.linear_v1(x_v)\n    x_v = self.linear_v2(x_v)\n    value = self.tanh_v(x_v)\n\n    return value, policy","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model, obs):\n  state_pieces, state_enemy_pieces = get_state(obs)\n  value, policy = model(torch.tensor([state_pieces,state_enemy_pieces], dtype=torch.float).reshape(1, 2, 6, 7).to(DEVICE))\n  value = value[0][0].item()\n  policy = np.array([policy[0].tolist()[c] for c in get_legal_actions(obs)])\n  policy /= sum(policy) if sum(policy) else 1\n  return value, policy.tolist()\n\ndef playout(observation, configuration, action, turn):\n    play(observation['board'], action, turn, configuration)\n    if is_win(observation['board'], action, turn, configuration, True):\n      return 1\n    if len(get_legal_actions(observation)) == 0:\n      return 0\n    next_turn = sum([1 if p != 0 else 0 for p in observation['board']])%2 + 1\n    return -playout(observation, configuration, random_action(observation), next_turn)\n\ndef mcs_action(observation, configuration):\n    next_turn = sum([1 if p != 0 else 0 for p in observation['board']])%2 + 1\n    legal_actions =  [c for c in range(COLUMNS) if observation['board'][c] == 0]\n    values = [0] * len(legal_actions)\n    for i, action in enumerate(legal_actions):\n        for _ in range(10):\n            values[i] += playout(copy.deepcopy(observation), configuration, action, next_turn)\n    return legal_actions[argmax(values)]\n\ndef get_legal_actions(observation):\n  return [c for c in range(COLUMNS) if observation['board'][c] == 0]\n\ndef random_action(observation):\n  return int(random.choice(get_legal_actions(observation)))\n\ndef argmax(collection, key=None):\n    return collection.index(max(collection))\n\ndef get_turn(obs):\n  next_turn = sum([1 if p != 0 else 0 for p in obs['board']])%2 + 1\n  prev_turn = 2 if next_turn == 1 else 1\n  return next_turn, prev_turn\n\ndef get_state(obs):\n  next_turn, prev_turn = get_turn(obs) \n  board = copy.deepcopy(obs['board'])\n  state_pieces = [1 if board[i] == next_turn else 0 for i in range(ROWS*COLUMNS)]\n  state_enemy_pieces = [1 if board[i] == prev_turn else 0 for i in range(ROWS*COLUMNS)]\n  return state_pieces, state_enemy_pieces","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def nodes_to_scores(nodes):\n    scores = []\n    for c in nodes:\n        scores.append(c.n)\n    return scores\n\ndef pv_mcts_scores(observation, configuration, model, temperature):\n  class Node:\n    def __init__(self, obs, conf, p, is_win):\n      self.obs = obs\n      self.conf = conf\n      self.next_turn = sum([1 if p != 0 else 0 for p in self.obs['board']])%2 + 1\n      self.p = p\n      self.is_win = is_win\n      self.w = 0\n      self.n = 0\n      self.child_nodes = None\n\n    def evaluate(self):\n      if self.is_win:\n        value = 1\n        self.w += value\n        self.n += 1\n        return value\n      elif len(get_legal_actions(self.obs)) == 0:\n        value = 0\n        self.w += value\n        self.n += 1\n        return value\n\n      if not self.child_nodes:\n        value, policies = predict(model, self.obs)\n        self.w += value\n        self.n += 1\n        self.expand(policies)\n        return value\n      else:\n        value = -self.next_child_node().evaluate() \n        self.w += value\n        self.n += 1\n        return value\n\n    def expand(self, policies):\n      legal_actions = get_legal_actions(self.obs)\n      self.child_nodes = []\n      for action, policy in zip(legal_actions, policies):\n        next_obs = copy.deepcopy(self.obs)\n        play(next_obs['board'], action, self.next_turn, self.conf)\n        is_done_and_win = is_win(next_obs['board'], action, self.next_turn, self.conf, True)\n        self.child_nodes.append(Node(next_obs, self.conf, policy, is_done_and_win))\n\n    def next_child_node(self):\n      C_PUCT = 1.0\n      t = sum(nodes_to_scores(self.child_nodes))\n      pucb_values = []\n      for child_node in self.child_nodes:\n        pucb_values.append((child_node.w / child_node.n if child_node.n else 0.0) + C_PUCT * child_node.p * math.sqrt(t) / (1 + child_node.n))\n      return self.child_nodes[np.argmax(pucb_values)]\n    \n  root_node = Node(copy.deepcopy(observation), configuration, [], False)\n\n  for _ in range(PV_EVALUATE_COUNT):\n    root_node.evaluate()\n\n  scores = nodes_to_scores(root_node.child_nodes)\n  if temperature == 0:\n    action = np.argmax(scores)\n    scores = np.zeros(len(scores))\n    scores[action] = 1\n  else:\n    scores = boltzman(scores, temperature)\n  return scores\n\ndef pv_mcts_action(model, temperature=0):\n  def pv_mcts_action(observation, configuration):\n    scores = pv_mcts_scores(observation, configuration, model, temperature)\n    legal_actions = get_legal_actions(observation)\n    return int(np.random.choice(legal_actions, p=scores))\n  return pv_mcts_action\n\ndef boltzman(xs, temperature):\n    xs = [x ** (1 / temperature) for x in xs]\n    return [x / sum(xs) for x in xs]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def first_player_value(obs, config, prev_turn, first_turn, prev_action):\n  if is_win(obs['board'], prev_action, prev_turn, config, True):\n    if prev_turn == first_turn:\n      return 1\n    else:\n      return -1\n  return 0\n\ndef write_data(history):\n  now = datetime.now()\n  os.makedirs('./data/', exist_ok=True)\n  path = './data/{:04}{:02}{:02}{:02}{:02}{:02}.history'.format(now.year, now.month, now.day, now.hour, now.minute, now.second)\n  with open(path, mode='wb') as f:\n    pickle.dump(history, f)\n\ndef one_play(model):\n  history = []\n\n  env = make(\"connectx\", debug=True)\n  trainer = env.train([None, \"random\"])\n  observation = trainer.reset()\n\n  first_turn = sum([1 if p != 0 else 0 for p in observation['board']])%2 + 1\n  next_turn = None\n  prev_turn = None\n  prev_action = None\n  is_done = False\n\n  while not is_done:\n\n    next_turn = sum([1 if p != 0 else 0 for p in observation['board']])%2 + 1\n    prev_turn = 2 if next_turn == 1 else 1\n\n    scores = pv_mcts_scores(observation, env.configuration, model, SP_TEMPERATURE)\n    policies = [0] * COLUMNS\n    legal_actions = get_legal_actions(observation)\n    for action, policy in zip(legal_actions, scores):\n      policies[action] = policy\n    state_pieces, state_enemy_pieces = get_state(observation)\n    action = np.random.choice(legal_actions, p=scores)\n    history.append([[state_pieces, state_enemy_pieces], action, None])\n    play(observation['board'], int(action), next_turn, env.configuration)\n    is_done = is_win(observation['board'], int(action), next_turn, env.configuration, True) or len(get_legal_actions(observation))==0\n    prev_action = int(action)\n\n  value = first_player_value(observation, env.configuration, next_turn, first_turn, prev_action)\n\n  for i in range(len(history)):\n    history[i][2] = value\n    value = -value\n  return history\n\ndef self_play(model):\n  history = []\n  for i in range(SP_GAME_COUNT):\n    h = one_play(model)\n    history.extend(h)\n    print('\\rSelfPlay {}/{}'.format(i+1, SP_GAME_COUNT), end='')\n  print('')\n  write_data(history)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_data():\n    history_path = sorted(Path('./data').glob('*.history'))[-1]\n    with history_path.open(mode='rb') as f:\n      return pickle.load(f)\n\ndef train_network(model, times):\n  class DataSet(Dataset):\n    def __init__(self, state, policy, value):\n      self.state = state\n      self.policy = policy\n      self.value = value\n    def __len__(self):\n      return len(self.state)\n    def __getitem__(self, idx):\n      return self.state[idx], self.policy[idx], self.value[idx]\n    \n  history = load_data()\n  xs, y_policies, y_values = zip(*history)\n  l = len(xs)\n  xs = np.array(xs).reshape(l,2,6,7)\n\n  tensor_state = torch.tensor(xs, dtype=torch.float).to(DEVICE)\n  tensor_policy = torch.tensor(y_policies, dtype=torch.long).to(DEVICE)\n  tensor_value = torch.tensor(y_values, dtype=torch.float).to(DEVICE)\n\n  train_state = tensor_state[TEST_DATA_NUM:]\n  train_policy = tensor_policy[TEST_DATA_NUM:]\n  train_value = tensor_value[TEST_DATA_NUM:]\n  test_state = tensor_state[:TEST_DATA_NUM]\n  test_policy = tensor_policy[:TEST_DATA_NUM]\n  test_value = tensor_value[:TEST_DATA_NUM]\n  print(\"train Data\", train_state.shape, train_policy.shape, train_value.shape)\n  print(\"test Data\", test_state.shape, test_policy.shape, test_value.shape)\n\n  trainDS  = DataSet(train_state, train_policy, train_value)\n  trainLoader = DataLoader(trainDS, batch_size=BATCH_SIZE, shuffle=True)\n  testDS = DataSet(test_state, test_policy, test_value)\n  testLoader = DataLoader(testDS, batch_size=100, shuffle=False)\n\n  optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n  policyLossF = nn.CrossEntropyLoss()\n  valueLossF = nn.MSELoss()\n\n  iterate = int(len(xs) / BATCH_SIZE)\n  for epoch in range(RN_EPOCHS):\n    time.sleep(1)\n    losses = [0, 0, 0]\n    accuracy_train = [0, 0]\n    accuracy_test = [0, 0]\n    model.train()\n    ep = tqdm(trainLoader)\n    for i, (X, Y_policy, Y_value) in enumerate(ep):\n      optimizer.zero_grad()\n\n      v, p = model.forward(X)\n      p_loss = policyLossF(p, Y_policy)\n      v_loss = valueLossF(v.view(-1), Y_value)\n\n      #loss = (p_loss * policyVias) + (v_loss * valueVias)\n      loss = (p_loss * 1) + (v_loss * 1)\n      loss.backward()\n      optimizer.step()\n\n      losses[0] += loss.item()\n      losses[1] += p_loss.item()\n      losses[2] += v_loss.item()\n    tb.add_scalar('Loss_{}'.format(times), losses[0], epoch)\n\n  model.eval()\n\n  for i, (X, Y_policy, Y_value) in enumerate(trainLoader):\n    v, p = model.forward(X)\n    _, pred = torch.max(p, dim=1)\n    accuracy_train[0] += pred.eq(Y_policy).sum().item() / Y_value.shape[0]\n    accuracy_train[1] += (v.view(-1).round() == Y_value).sum().item() / Y_value.shape[0]\n\n  for i, (X, Y_policy, Y_value) in enumerate(testLoader):\n    v, p = model.forward(X)\n    _, pred = torch.max(p, dim=1)\n    accuracy_test[0] += pred.eq(Y_policy).sum().item() / Y_value.shape[0]\n    accuracy_test[1] += (v.view(-1).round() == Y_value).sum().item() / Y_value.shape[0]\n\n  tb.add_scalar('Accuracy_Policy_Train', 100.0 * accuracy_train[0] / train_state.shape[0], times)\n  tb.add_scalar('Accuracy_Value_Train', 100.0 * accuracy_train[1] / train_state.shape[0], times)\n\n  tb.add_scalar('Accuracy_Policy_Test', 100.0 * accuracy_test[0] / TEST_DATA_NUM, times)\n  tb.add_scalar('Accuracy_Value_Test', 100.0 * accuracy_test[1] / TEST_DATA_NUM, times)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_win_percentages(agent1, agent2, n_rounds=20):\n    config = {'rows': 6, 'columns': 7, 'inarow': 4}       \n    outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], n_rounds//2)     \n    outcomes += [[b,a] for [a,b] in evaluate(\"connectx\", [agent2, agent1], config, [], n_rounds-n_rounds//2)]\n\n    print(\"Agent 1 Win Percentage:\", np.round(outcomes.count([1,-1])/len(outcomes), 2))\n    print(\"Agent 2 Win Percentage:\", np.round(outcomes.count([-1,1])/len(outcomes), 2))\n    print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0]))\n    print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0, None]))\n    return np.round(outcomes.count([1,-1])/len(outcomes), 2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_model(model, times):\n  agent_func = pv_mcts_action(model, SP_TEMPERATURE)\n  vs_random = get_win_percentages(agent_func, 'random')\n  vs_monte = get_win_percentages(agent_func, mcs_action)\n\n  tb.add_scalar('VS_Random', vs_random, times)\n  tb.add_scalar('VS_Monte', vs_monte, times)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = ResNet()\nmodel = model.to(DEVICE)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(LOOPS):\n    print('Train',i,'====================')\n    # セルフプレイ部\n    self_play(model)\n\n    # パラメータ更新部\n    train_network(model, i)\n\n    evaluate_model(model, i)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tb.close()\n%tensorboard --logdir logs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out = \"\"\"\nfrom kaggle_environments import make, evaluate\nfrom kaggle_environments.envs.connectx.connectx import play,is_win\n\nimport os\nimport sys\nimport io\nimport base64\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as f\n\nimport numpy as np\nimport random\nimport math\n\nimport copy\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nROWS = 6\nCOLUMNS = 7\n\nN_BLOCKS = 12\nFILTERS = 128\n\nPV_EVALUATE_COUNT = 50\nTEMPERATURE = 0.0\n\nencoded_weights = \\\"\\\"\\\"BASE64_PARAMS\\\"\\\"\\\"\n\ndef agent_func(observation, configuration):\n  class Block(nn.Module):\n    def __init__(self, filter):\n      super(Block,self).__init__()\n      self.conv1 = nn.Conv2d(filter, filter, kernel_size=3, padding=1)\n      self.conv2 = nn.Conv2d(filter, filter, kernel_size=3, padding=1)\n\n      self.bn1 = nn.BatchNorm2d(filter)\n      self.bn2 = nn.BatchNorm2d(filter)\n\n      self.relu1 = nn.ReLU()\n      self.relu2 = nn.ReLU()\n\n    def forward(self, x):\n      input = x\n      x = self.conv1(x)\n      x = self.bn1(x)\n      x = self.relu1(x)\n      x = self.conv2(x)\n      x = self.bn2(x)\n      x = x + input\n      x = self.relu2(x)\n      return x\n\n  class ResNet(nn.Module):\n    def __init__(self):\n      super(ResNet,self).__init__()\n      self.conv = nn.Conv2d(2, FILTERS, kernel_size=3, padding=1)\n      self.bn = nn.BatchNorm2d(FILTERS)\n      self.relu = nn.ReLU()\n\n      self.blocks = nn.ModuleList([Block(FILTERS) for _ in range(N_BLOCKS)])\n\n      self.conv_p = nn.Conv2d(FILTERS, 8, kernel_size=1)\n      self.bn_p = nn.BatchNorm2d(8)\n      self.relu_p = nn.ReLU()\n      self.linear_p = nn.Linear(8 * ROWS*COLUMNS, COLUMNS)\n      self.softmax_p = nn.Softmax(dim=1)\n\n      self.conv_v = nn.Conv2d(FILTERS, 4, kernel_size=1)\n      self.bn_v = nn.BatchNorm2d(4)\n      self.relu_v = nn.ReLU()\n      self.linear_v1 = nn.Linear(4 * ROWS*COLUMNS, 64)\n      self.linear_v2 = nn.Linear(64, 1)\n      self.tanh_v = nn.Tanh()\n\n    def forward(self, x):\n      x = self.conv(x)\n      x = self.bn(x)\n      x = self.relu(x)\n\n      for block in self.blocks:\n        x = block(x)\n\n      x_p = self.conv_p(x)\n      x_p = self.bn_p(x_p)\n      x_p = self.relu_p(x_p)\n      x_p = x_p.view(x_p.size(0), -1)\n      x_p = self.linear_p(x_p)\n      policy = self.softmax_p(x_p)\n\n\n      x_v = self.conv_v(x)\n      x_v = self.bn_v(x_v)\n      x_v = self.relu_v(x_v)\n      x_v = x_v.view(x_v.size(0), -1)\n      x_v = self.linear_v1(x_v)\n      x_v = self.linear_v2(x_v)\n      value = self.tanh_v(x_v)\n\n      return value, policy\n  def predict(model, obs):\n    state_pieces, state_enemy_pieces = get_state(obs)\n    value, policy = model(torch.tensor([state_pieces,state_enemy_pieces], dtype=torch.float).reshape(1, 2, 6, 7).to(DEVICE))\n    value = value[0][0].item()\n    policy = np.array([policy[0].tolist()[c] for c in get_legal_actions(obs)])\n    policy /= sum(policy) if sum(policy) else 1\n    return value, policy.tolist()\n\n  def get_legal_actions(observation):\n    return [c for c in range(COLUMNS) if observation['board'][c] == 0]\n\n  def argmax(collection, key=None):\n      return collection.index(max(collection))\n\n  def get_turn(obs):\n    next_turn = sum([1 if p != 0 else 0 for p in obs['board']])%2 + 1\n    prev_turn = 2 if next_turn == 1 else 1\n    return next_turn, prev_turn\n\n  def get_state(obs):\n    next_turn, prev_turn = get_turn(obs) \n    board = copy.deepcopy(obs['board'])\n    state_pieces = [1 if board[i] == next_turn else 0 for i in range(ROWS*COLUMNS)]\n    state_enemy_pieces = [1 if board[i] == prev_turn else 0 for i in range(ROWS*COLUMNS)]\n    return state_pieces, state_enemy_pieces\n\n  def nodes_to_scores(nodes):\n      scores = []\n      for c in nodes:\n          scores.append(c.n)\n      return scores\n\n  def pv_mcts_scores(observation, configuration, model, temperature):\n    class Node:\n      def __init__(self, obs, conf, p, is_win):\n        self.obs = obs\n        self.conf = conf\n        self.next_turn = sum([1 if p != 0 else 0 for p in self.obs['board']])%2 + 1\n        self.p = p\n        self.is_win = is_win\n        self.w = 0\n        self.n = 0\n        self.child_nodes = None\n\n      def evaluate(self):\n        if self.is_win:\n          value = 1\n          self.w += value\n          self.n += 1\n          return value\n        elif len(get_legal_actions(self.obs)) == 0:\n          value = 0\n          self.w += value\n          self.n += 1\n          return value\n\n        if not self.child_nodes:\n          value, policies = predict(model, self.obs)\n          self.w += value\n          self.n += 1\n          self.expand(policies)\n          return value\n        else:\n          value = -self.next_child_node().evaluate() \n          self.w += value\n          self.n += 1\n          return value\n\n      def expand(self, policies):\n        legal_actions = get_legal_actions(self.obs)\n        self.child_nodes = []\n        for action, policy in zip(legal_actions, policies):\n          next_obs = copy.deepcopy(self.obs)\n          play(next_obs['board'], action, self.next_turn, self.conf)\n          is_done_and_win = is_win(next_obs['board'], action, self.next_turn, self.conf, True)\n          self.child_nodes.append(Node(next_obs, self.conf, policy, is_done_and_win))\n\n      def next_child_node(self):\n        C_PUCT = 1.0\n        t = sum(nodes_to_scores(self.child_nodes))\n        pucb_values = []\n        for child_node in self.child_nodes:\n          pucb_values.append((child_node.w / child_node.n if child_node.n else 0.0) + C_PUCT * child_node.p * math.sqrt(t) / (1 + child_node.n))\n        return self.child_nodes[np.argmax(pucb_values)]\n      \n    root_node = Node(copy.deepcopy(observation), configuration, [], False)\n\n    for _ in range(PV_EVALUATE_COUNT):\n      root_node.evaluate()\n\n    scores = nodes_to_scores(root_node.child_nodes)\n    if temperature == 0:\n      action = np.argmax(scores)\n      scores = np.zeros(len(scores))\n      scores[action] = 1\n    else:\n      scores = boltzman(scores, temperature)\n    return scores\n\n  def pv_mcts_action(model, temperature=0):\n    def pv_mcts_action(observation, configuration):\n      scores = pv_mcts_scores(observation, configuration, model, temperature)\n      legal_actions = get_legal_actions(observation)\n      return np.random.choice(legal_actions, p=scores)\n    return pv_mcts_action\n\n  def boltzman(xs, temperature):\n      xs = [x ** (1 / temperature) for x in xs]\n      return [x / sum(xs) for x in xs]\n\n  model = ResNet()\n  decoded = base64.b64decode(encoded_weights)\n  buffer = io.BytesIO(decoded)\n  model = model.to(DEVICE)\n  model.load_state_dict(torch.load(buffer, map_location=DEVICE))\n  model.eval()\n  next_action = pv_mcts_action(model, TEMPERATURE)\n\n  return int(next_action(observation, configuration))\n\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import base64\nimport sys\n\ndict_path = \"net_params.pth\"\ntorch.save(model.state_dict(), dict_path)\n\nINPUT_PATH = dict_path\nOUTPUT_NO_PARAM_PATH = 'submission_no_param.py'\nOUTPUT_PATH = 'submission.py'\n\nwith open(INPUT_PATH, 'rb') as f:\n    raw_bytes = f.read()\n    encoded_weights = base64.encodebytes(raw_bytes).decode()\n\nwith open(OUTPUT_NO_PARAM_PATH, 'w') as f:\n    f.write(out)\n\nwith open(OUTPUT_NO_PARAM_PATH, 'r') as f:\n    data = f.read()\n\ndata = data.replace('BASE64_PARAMS', encoded_weights)\n\nwith open(OUTPUT_PATH, 'w') as f:\n    f.write(data)\n    print('written agent with net parameters to', OUTPUT_PATH)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from submission import agent_func\n\nenv = make(\"connectx\", debug=True)\ntrainer = env.train([\"random\",None])\n\nobservation = trainer.reset()\n\nwhile not env.done:\n    my_action = agent_func(observation, env.configuration)\n    print(\"My Action\", my_action)\n    observation, reward, done, info = trainer.step(my_action)\n\nenv.render(mode=\"ipython\")","metadata":{},"execution_count":null,"outputs":[]}]}