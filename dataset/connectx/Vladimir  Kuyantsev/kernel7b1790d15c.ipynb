{"cells":[{"metadata":{"pycharm":{"is_executing":false,"name":"#%%\n"},"scrolled":true,"id":"yVCELecos7c3","outputId":"8cffdbc6-b9e2-4d3f-c091-24d6dcbbbb49","trusted":false},"cell_type":"code","source":"!pip install 'kaggle-environments>=0.1.6'","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"is_executing":false,"name":"#%%\n"},"id":"En3K8veEs7c9","trusted":false},"cell_type":"code","source":"# !pip install torch","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"is_executing":false,"name":"#%%\n"},"scrolled":true,"id":"OxFVsSuMs7dB","trusted":false},"cell_type":"code","source":"# !pip install tensorflow","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"is_executing":false,"name":"#%%\n"},"id":"4hPrX176s7dE","trusted":false},"cell_type":"code","source":"# import numpy as np\n# import gym\n# import tensorflow as tf\n# import matplotlib.pyplot as plt\n# from tqdm.notebook import tqdm\n# from kaggle_environments import evaluate, make\n","execution_count":null,"outputs":[]},{"metadata":{"id":"22-fSml_DQaw","outputId":"a2f90adb-e18d-4761-b99e-3261562db05a","trusted":false},"cell_type":"code","source":"# class ColumnFilled(RuntimeError):\n#     pass\n\n# cluster_prior = [0, 0, 1, 4, 16]\n\n# num_cols = 7\n# num_rows = 6\n# dims = (num_rows, num_cols)\n\n# def is_valid_ind(ind):\n#     row, col = ind\n#     return 0 <= row < num_rows and 0 <= col < num_cols\n\n# def get_neighbor_indices(ind):\n#     row, col = ind\n#     row = int(row)\n#     col = int(col)\n#     n_inds = []\n#     for i in range(row - 1, row + 2):\n#         for j in range(col - 1, col + 2):\n#             if not is_valid_ind((i,j)):\n#                 continue\n#             if i == row and j == col:\n#                 continue\n#             n_inds.append((i,j))\n#     return n_inds\n\n# def all_neighbors_empty(board, ind):\n#     neighb = get_neighbor_indices(ind)\n#     return sum(board[n] == 0 for n in neighb) == len(neighb)\n\n# def all_neighbors_nonempty(board, ind):\n#     neighb = get_neighbor_indices(ind)\n#     non_em = sum(board[n] != 0 for n in neighb) \n#     return non_em == len(neighb)\n\n# def has_same_neighbors(board, ind, mark):\n#     neighb = get_neighbor_indices(ind)\n#     return sum((board[n] == mark for n in neighb)) > 0\n\n# def has_neighbors(board, ind, mark):\n#     neighb = get_neighbor_indices(ind)\n#     return sum((board[n] != 0 for n in neighb)) > 0\n\n# def check_array_win(arr, mark):\n#     for i in range(len(arr) - 3):\n#         to_check = arr[i:i+4]\n#         check = sum(el == mark for el in to_check)\n#         # print(to_check)\n#         if check == 4:\n#             return True\n#     return False\n\n\n# board = [[0,0,0,0,0,0,0],\n#          [0,0,0,0,0,0,0],\n#          [0,0,2,0,0,0,0],\n#          [0,0,1,0,1,0,0],\n#          [0,0,1,0,1,0,0],\n#          [0,0,1,0,1,0,0]]\n# board = np.array(board)\n\n\n\n# # board = np.array(board).ravel()\n# print(get_neighbor_indices((0,0)))\n# print()\n# print(get_neighbor_indices((5,2)))\n\n# print(is_valid_ind((4,3)))","execution_count":null,"outputs":[]},{"metadata":{"id":"havfpGHJOvdw","outputId":"3d648bc0-65e5-4796-88ed-7e2888a62968","trusted":false},"cell_type":"code","source":"# def check_win(board,place,mark):\n#     reshaped_b = np.reshape(board, dims)\n#     row, col = np.unravel_index(place, dims)\n#     hor = reshaped_b[row, :]\n#     ver = reshaped_b[:, col]\n#     # print(ver)\n#     # print(check_array_win(ver, mark))\n#     # print('end')\n#     offset = col - row\n#     diag1 = np.diagonal(reshaped_b, offset)\n#     diag2 = np.flipud(reshaped_b).diagonal(offset)\n    \n#     return check_array_win(hor, mark) \\\n#            or check_array_win(ver, mark) \\\n#            or check_array_win(diag1, mark) \\\n#            or check_array_win(diag2, mark)\n\n# board = [[2,2,2,2,0,0,0],\n#          [0,0,0,0,0,0,0],\n#          [0,0,2,0,0,1,0],\n#          [0,0,1,0,1,0,0],\n#          [0,0,1,1,1,0,0],\n#          [0,0,1,0,1,0,0]]\n# board = np.array(board).ravel()\n\n# for i in range(len(board)):\n#     if check_win(board, i, 1):\n#         print(1, np.unravel_index(i, dims))\n#     if check_win(board, i, 2):\n#         print(2, np.unravel_index(i, dims))\n\n# # ind = np.ravel_multi_index((2,4), dims)\n# # print(check_win(board,ind, 1))","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"is_executing":false,"name":"#%%\n"},"id":"s1xyz5p5s7dI","outputId":"a96b4df1-135f-4474-8a74-dfb3495abc0d","trusted":false},"cell_type":"code","source":"\n\n# def greedy_strategy(board, mark):\n#     choose_from = []\n#     for i in range(num_cols):\n#         try:\n#             new_board, ind = apply_action(board, mark, i)\n#             new_board = new_board.reshape(dims)\n#             ind = np.unravel_index(ind, dims)\n#             if has_same_neighbors(new_board, ind, mark):\n#                 choose_from.append(i)\n#         except ColumnFilled as e:\n#             pass\n#     if len(choose_from) > 0:\n#         return np.random.choice(choose_from)\n#     return np.random.choice([c for c in range(num_cols) if board[c] == 0])\n\n\n# def greedy_cluster_strategy(board, mark):\n#     choose_from = []\n#     for i in range(num_cols):\n#         try:\n#             new_board, ind = apply_action(board, mark, i)\n#             new_board = new_board.reshape(dims)\n#             ind = np.unravel_index(ind, dims)\n#             if has_neighbors(new_board, ind, mark):\n#                 choose_from.append(i)\n#         except ColumnFilled as e:\n#             pass\n#     if len(choose_from) > 0:\n#         return np.random.choice(choose_from)\n#     return np.random.choice([c for c in range(num_cols) if board[c] == 0])\n\n\n# def random_strategy(board, mark):\n#     return np.random.choice([c for c in range(num_cols) if board[c] == 0])\n\n\n# def check_win(board,place,mark):\n#     reshaped_b = np.reshape(board, dims)\n#     row, col = np.unravel_index(place, dims)\n#     hor = reshaped_b[row, :]\n#     ver = reshaped_b[:, col]\n#     offset = col - row\n#     diag1 = np.diagonal(reshaped_b, offset)\n#     diag2 = np.flipud(reshaped_b).diagonal(offset)\n    \n#     return check_array_win(hor, mark) \\\n#            or check_array_win(ver, mark) \\\n#            or check_array_win(diag1, mark) \\\n#            or check_array_win(diag2, mark)\n\n    \n# def evaluate_position(board, ind, mark):\n#     neighbors = get_neighbor_indices(ind)\n#     counter = 1\n#     num_clusters = 0\n#     for n in neighbors:\n#         if board[n] == mark:\n#             num_clusters += 1\n#             cluster_counter = 1\n#             d = (n[0] - ind[0], n[1] - ind[1])  # direction\n#             next_ind = (ind[0] + d[0], ind[1] + d[1])\n#             while is_valid_ind(next_ind) and board[next_ind] == mark:\n#                 cluster_counter += 1\n#                 next_ind = (next_ind[0] + d[0], next_ind[1] + d[1])\n#             # if d != (0, 1) or d != (1, 0):\n#             #     counter *= 2\n#             # print(mark, ind, cluster_counter)\n#             counter += cluster_prior[cluster_counter] * cluster_counter\n#     point_score = counter * 0.5 * num_clusters\n#     return point_score\n\n\n# def inverse_mark(mark):\n#     return 2 - mark // 2\n\n\n# def sigmoid(x):\n#     return 1/(1 + np.exp(-x))\n\n\n# def get_expected_rewards(raveled_board, mark):\n#     board = raveled_board.reshape(dims)\n#     rewards = [1] * num_cols\n#     for i in range(num_cols):\n#         try:\n#             new_board, ind = apply_action(raveled_board, mark, i)\n#             # my_win = check_win(new_board, ind, mark)\n#             # opp_win = check_win(new_board, ind, inverse_mark(mark))\n#             # if my_win or opp_win:\n#             #     rewards[i] = 10000\n#             #     continue\n#         except ColumnFilled as e:\n#             rewards[i] = 0\n#             continue\n#         ind = np.unravel_index(ind, dims)\n#         gained_score = evaluate_position(board, ind, mark)\n#         blocked_opponent_score = evaluate_position(board, ind, inverse_mark(mark))\n        \n#         rewards[i] += gained_score + blocked_opponent_score\n#     return np.array(rewards)\n\n\n# def get_expected_state_rewards(state):\n#     return get_expected_rewards(state[:-1], state[-1])\n\n\n# def get_expected_batch_rewards(batch):\n#     return np.apply_along_axis(get_expected_state_rewards, 1, batch)\n\n\n# def apply_action(board, mark, action):\n#     # find lowest empty cell in column\n#     board = np.array(board)\n#     # if bottoms is empty, place checker there\n#     bottom_index = np.ravel_multi_index((num_rows - 1, action), dims)\n#     if board[bottom_index] == 0:\n#         board[bottom_index] = mark\n#         return board, bottom_index\n#     for i in range(num_rows):\n#         if board[np.ravel_multi_index((i, action), dims)]:\n#             break\n#     if i == 0:\n#         raise ColumnFilled(\"Column is filled\")\n#     place_index = np.ravel_multi_index((i-1, action), dims)\n#     board[place_index] = mark\n#     return board, place_index\n   \n    \n# board = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n# board = [0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\n# # print(np.reshape(board, dims))\n# board = np.array(board)\n# obs = {\"board\": board, \"mark\": 2}\n\n# board = [[0,0,1,0,0,0,0],\n#          [0,0,2,0,0,0,0],\n#          [0,0,2,0,0,0,0],\n#          [0,0,1,0,1,0,0],\n#          [0,0,1,0,1,0,0],\n#          [0,0,1,0,1,0,0]]\n# board = np.array(board).ravel()\n# # board = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n# # print(greedy_strategy(board, 2))\n# # print(get_expected_rewards(board, 1))\n# print(get_expected_rewards(board, 2))\n# # states = [\n# #     [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 1],\n# #     [0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1, 2]\n# # ]\n# # print(get_expected_batch_rewards(states))","execution_count":null,"outputs":[]},{"metadata":{"id":"UBcInXi8rLm4","trusted":false},"cell_type":"code","source":"# class ConnectX(gym.Env):\n#     def __init__(self, switch_prob=0.5):\n#         self.env = make('connectx', debug=False)\n#         self.pair = [None, 'random']\n#         self.trainer = self.env.train(self.pair)\n#         self.switch_prob = switch_prob\n\n#         # Define required gym fields (examples):\n#         config = self.env.configuration\n#         self.action_space = gym.spaces.Discrete(config.columns)\n#         self.observation_space = gym.spaces.Discrete(config.columns * config.rows)\n\n#     def switch_trainer(self):\n#         self.pair = self.pair[::-1]\n#         self.trainer = self.env.train(self.pair)\n\n#     def step(self, action):\n#         return self.trainer.step(action)\n    \n#     def reset(self):\n#         if np.random.random() < self.switch_prob:\n#             self.switch_trainer()\n#         return self.trainer.reset()\n    \n#     def render(self, **kwargs):\n#         return self.env.render(**kwargs)\n    \n    \n# class DeepModel(tf.keras.Model):\n#     def __init__(self, num_states, hidden_units, num_actions):\n#         super(DeepModel, self).__init__()\n#         self.input_layer = tf.keras.layers.InputLayer(input_shape=(num_states,))\n#         self.hidden_layers = []\n#         for i in hidden_units:\n#             self.hidden_layers.append(tf.keras.layers.Dense(\n#                 i, activation='relu', kernel_initializer='RandomNormal'))\n#         self.output_layer = tf.keras.layers.Dense(\n#             num_actions, activation='sigmoid', kernel_initializer='RandomNormal')\n\n# #     @tf.function\n#     def call(self, inputs):\n#         z = self.input_layer(inputs)\n#         print(z)\n#         for layer in self.hidden_layers:\n#             z = layer(z)\n#             print(z)\n#         output = self.output_layer(z)\n#         return output\n\n\n# class DQN:\n#     def __init__(self, num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr):\n#         self.num_actions = num_actions\n#         self.batch_size = batch_size\n#         self.optimizer = tf.optimizers.Adam(lr)\n#         self.gamma = gamma\n#         self.model = DeepModel(num_states, hidden_units, num_actions)\n#         self.experience = {'s': [], 'a': [], 'r': [], 's2': [], 'done': []} # The buffer\n#         self.max_experiences = max_experiences\n#         self.min_experiences = min_experiences\n\n#     def predict(self, inputs):\n#         return self.model(np.atleast_2d(inputs.astype('float32')))\n\n# #     @tf.function\n#     def train(self, TargetNet):\n#         if len(self.experience['s']) < self.min_experiences:\n#         # if len(self.experience['s']) < 1:\n#             # Only start the training process when we have enough experiences in the buffer\n#             return 0\n\n#         # Randomly select n experience in the buffer, n is batch-size\n#         ## TODO maybe choose them not randomly ? \n#         ids = np.random.randint(low=0, high=len(self.experience['s']), size=self.batch_size)\n#         # ids = np.random.randint(low=0, high=len(self.experience['s']), size=1)\n#         # state before action\n#         states = np.asarray([self.preprocess(self.experience['s'][i]) for i in ids])\n\n#         with tf.GradientTape() as tape:\n#             pred = self.predict(states)\n#             # print('preds', pred)\n#             rewards = get_expected_batch_rewards(states)\n#             # print('rewards', rewards)\n#             loss = -tf.math.reduce_sum(pred * rewards)\n#         variables = self.model.trainable_variables\n#         gradients = tape.gradient(loss, variables)\n#         self.optimizer.apply_gradients(zip(gradients, variables))\n\n#     # Get an action by using epsilon-greedy\n#     def get_action(self, state, epsilon, verbose=False):\n#         if np.random.random() < epsilon:\n#             board = state['board']\n#             mark = state['mark']\n#             return int(greedy_cluster_strategy(board, mark))\n#             # return int(np.random.choice([c for c in range(self.num_actions) if state['board'][c] == 0]))\n#         else:\n#             prediction = self.predict(np.atleast_2d(self.preprocess(state)))[0].numpy()\n#             if verbose:\n#                 print('rew', get_expected_state_rewards(state))\n#                 print('pred',prediction)\n#             for i in range(self.num_actions):\n#                 if state['board'][i] != 0:\n#                     prediction[i] = -1e7\n#             return int(np.argmax(prediction))\n\n#     # Method used to manage the buffer\n#     def add_experience(self, exp):\n#         if len(self.experience['s']) >= self.max_experiences:\n#             for key in self.experience.keys():\n#                 self.experience[key].pop(0)\n#         for key, value in exp.items():\n#             self.experience[key].append(value)\n\n#     def copy_weights(self, TrainNet):\n#         variables1 = self.model.trainable_variables\n#         variables2 = TrainNet.model.trainable_variables\n#         for v1, v2 in zip(variables1, variables2):\n#             v1.assign(v2.numpy())\n\n#     def save_weights(self, path):\n#         self.model.save_weights(path)\n\n#     def load_weights(self, path):\n#         ref_model = tf.keras.Sequential()\n\n#         ref_model.add(self.model.input_layer)\n#         for layer in self.model.hidden_layers:\n#             ref_model.add(layer)\n#         ref_model.add(self.model.output_layer)\n\n#         ref_model.load_weights(path)\n    \n#     # Each state will consist of the board and the mark\n#     # in the observations\n#     def preprocess(self, state):\n#         result = state['board'][:]\n#         result.append(state.mark)\n#         return result\n    \n# def play_game(env, TrainNet, TargetNet, epsilon, copy_step):\n#     rewards = 0\n#     iter = 0\n#     done = False\n#     observations = env.reset()\n#     while not done:\n#         # end.render()\n#         # Using epsilon-greedy to get an action\n#         action = TrainNet.get_action(observations, epsilon)\n\n#         # Caching the information of current state\n#         prev_observations = observations\n\n#         # Take action\n#         observations, reward, done, _ = env.step(action)\n\n#         rewards += reward\n\n#         # Adding experience into buffer\n#         exp = {'s': prev_observations, 'a': action, 'r': reward, 's2': observations, 'done': done}\n#         TrainNet.add_experience(exp)\n\n#         # Train the training model by using experiences in buffer and the target model\n#         TrainNet.train(TargetNet)\n#         iter += 1\n#         if iter > min_experiences and iter % copy_step == 0:\n#             # Update the weights of the target model when reaching enough \"copy step\"\n#             TargetNet.copy_weights(TrainNet)\n#     return rewards\n","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"is_executing":false,"name":"#%%\n"},"id":"uRFPktf4s7dM","outputId":"f26e2257-012b-4451-e7ac-36a7d86d381d","trusted":false},"cell_type":"code","source":"\n# env = ConnectX()\n\n# gamma = 0.99\n# copy_step = 25\n# hidden_units = [100, 200, 200, 100]\n# max_experiences = 10000\n# min_experiences = 100\n# batch_size = 32\n# lr = 1e-2\n# epsilon = 0.99\n# decay = 0.99995\n# min_epsilon = 0.1\n\n# episodes = 30000\n# # episodes = 1\n\n# precision = 7\n\n# # log_dir = 'logs/'\n# # summary_writer = tf.summary.create_file_writer(log_dir)\n\n\n# num_states = env.observation_space.n + 1\n# num_actions = env.action_space.n\n\n# all_total_rewards = np.empty(episodes)\n# all_avg_rewards = np.empty(episodes) # Last 100 steps\n# all_epsilons = np.empty(episodes)\n\n# # Initialize models\n# TrainNet = DQN(num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n# TargetNet = DQN(num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n\n\n# pbar = tqdm(range(episodes))\n# for n in pbar:\n#     epsilon = max(min_epsilon, epsilon * decay)\n#     total_reward = play_game(env, TrainNet, TargetNet, epsilon, copy_step)\n#     all_total_rewards[n] = total_reward\n#     avg_reward = all_total_rewards[max(0, n - 100):(n + 1)].mean()\n#     all_avg_rewards[n] = avg_reward\n#     all_epsilons[n] = epsilon\n\n#     pbar.set_postfix({\n#         'episode reward': total_reward,\n#         'avg (100 last) reward': avg_reward,\n#         'epsilon': epsilon\n#     })\n\n# #     with summary_writer.as_default():\n# #         tf.summary.scalar('episode reward', total_reward, step=n)\n# #         tf.summary.scalar('running avg reward (100)', avg_reward, step=n)\n# #         tf.summary.scalar('epsilon', epsilon, step=n)\n\n","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"is_executing":false,"name":"#%%\n"},"id":"ZWMrR9Dns7dP","outputId":"3d28b124-af2f-47a9-8407-6f0c9054a137","trusted":false},"cell_type":"code","source":"# plt.plot(all_avg_rewards)\n# plt.xlabel('Episode')\n# plt.ylabel('Avg rewards (100)')\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"is_executing":false,"name":"#%%\n"},"id":"2vRssugPs7dU","outputId":"2e2a7564-a15f-4923-e0fd-c4acfef1dc50","trusted":false},"cell_type":"code","source":"# plt.plot(all_epsilons)\n# plt.xlabel('Episode')\n# plt.ylabel('Epsilon')\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"sx38ok_2qbd1","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"is_executing":false,"name":"#%%\n"},"id":"tl7fVMD8s7dX","trusted":false},"cell_type":"code","source":"# fc_layers = []\n\n# # fc_layers.extend([\n# #     TrainNet.model.input_layer.weights[0].numpy().tolist(), # weights\n# #     TrainNet.model.input_layer.weights[1].numpy().tolist() # bias\n# # ])\n\n# # Get all hidden layers' weights\n# for i in range(len(hidden_units)):\n#     fc_layers.extend([\n#         TrainNet.model.hidden_layers[i].weights[0].numpy().tolist(), # weights\n#         TrainNet.model.hidden_layers[i].weights[1].numpy().tolist() # bias\n#     ])\n\n# # Get output layer's weights\n# fc_layers.extend([\n#     TrainNet.model.output_layer.weights[0].numpy().tolist(), # weights\n#     TrainNet.model.output_layer.weights[1].numpy().tolist() # bias\n# ])\n\n# # Convert all layers into usable form before integrating to final agent\n# fc_layers = list(map(\n#     lambda x: str(list(np.round(x, precision))) \\\n#         .replace('array(', '').replace(')', '') \\\n#         .replace(' ', '') \\\n#         .replace('\\n', ''),\n#     fc_layers\n# ))\n# fc_layers = np.reshape(fc_layers, (-1, 2))\n\n# # Create the agent\n# my_agent = '''def my_agent(observation, configuration):\n#     import numpy as np\n\n# '''\n\n# # my_agent += '    il_w = np.array({}, dtype=np.float32)\\n'.format(fc_layers[0][0])\n# # my_agent += '    il_b = np.array({}, dtype=np.float32)\\n'.format(fc_layers[0][1])\n\n# # Write hidden layers\n# for i, (w, b) in enumerate(fc_layers[0:-1]):\n#     my_agent += '    hl{}_w = np.array({}, dtype=np.float32)\\n'.format(i+1, w)\n#     my_agent += '    hl{}_b = np.array({}, dtype=np.float32)\\n'.format(i+1, b)\n# # Write output layer\n# my_agent += '    ol_w = np.array({}, dtype=np.float32)\\n'.format(fc_layers[-1][0])\n# my_agent += '    ol_b = np.array({}, dtype=np.float32)\\n'.format(fc_layers[-1][1])\n\n# my_agent += '''\n#     state = observation['board'][:]\n#     state.append(observation['mark'])\n#     out = np.array(state, dtype=np.float32)\n\n# '''\n\n# # Calculate hidden layers\n# # my_agent += '    out = np.matmul(out, il_w) + il_b\\n'\n# for i in range(len(fc_layers[:-1])):\n#     my_agent += '    out = np.matmul(out, hl{0}_w) + hl{0}_b\\n'.format(i+1)\n    \n#     my_agent += '    out = np.maximum(0,out)\\n' # Relu function\n#     # my_agent += '    out = 1/(1 + np.exp(-out))\\n' # Sigmoid function\n# # Calculate output layer\n# my_agent += '    out = np.matmul(out, ol_w) + ol_b\\n'\n# my_agent += '    out = 1/(1 + np.exp(-out))\\n' # Sigmoid function\n\n# my_agent += '''\n#     for i in range(configuration.columns):\n#         if observation['board'][i] != 0:\n#             out[i] = -1e7\n\n#     return int(np.argmax(out))\n#     '''","execution_count":null,"outputs":[]},{"metadata":{"id":"_oOGN2g36Nnl","trusted":false},"cell_type":"code","source":"# print(my_agent)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"is_executing":false,"name":"#%%\n"},"id":"buoLc-xDs7da","trusted":false},"cell_type":"code","source":"\n# with open('agent_expect_loss.py', 'w') as f:\n#     f.write(my_agent)\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"is_executing":false,"name":"#%%\n"},"id":"NgCK-urCs7de","trusted":false},"cell_type":"code","source":"\n# with open('test_submission.py', 'w') as f:\n#     f.write(my_agent)\n    ","execution_count":null,"outputs":[]},{"metadata":{"id":"SGz9azuYKeJv"},"cell_type":"markdown","source":"#               SHIT ZONE","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}