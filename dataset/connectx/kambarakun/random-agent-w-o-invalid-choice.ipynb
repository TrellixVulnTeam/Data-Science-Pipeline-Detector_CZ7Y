{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Create ConnectX Environment"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install 'kaggle-environments>=0.1.4'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\nfrom kaggle_environments import evaluate, make","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"env = make('connectx', debug=True)\nenv.render()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create my Agent"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile submission.py\nimport numpy as np\n\n\ndef my_agent(obs, config):\n    board = np.array(obs.board).reshape(config.rows, config.columns)\n    return int(np.random.choice(np.where(board[0, :] == 0)[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%run submission.py","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test my Agent"},{"metadata":{"trusted":true},"cell_type":"code","source":"env.reset()\nenv.run([my_agent, 'random'])\nenv.render(mode='ipython', width=400, height=360)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env.reset()\nenv.run([my_agent, my_agent])\nenv.render(mode='ipython', width=400, height=360)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Debug/Train my Agent"},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer = env.train([None, \"random\"])\n\nobservation = trainer.reset()\n\nwhile not env.done:\n    my_action = my_agent(observation, env.configuration)\n    print(\"My Action\", my_action)\n    observation, reward, done, info = trainer.step(my_action)\n    env.render(mode=\"ipython\", width=100, height=90, header=False, controls=False)\nenv.render()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluate my Agent"},{"metadata":{"trusted":true},"cell_type":"code","source":"def mean_reward(rewards):\n    return sum(r[0] for r in rewards) / sum(r[0] + r[1] for r in rewards)\n\n# Run multiple episodes to estimate it's performance.\nprint(\"My Agent vs Random Agent:\",  mean_reward(evaluate(\"connectx\", [my_agent, \"random\"], num_episodes=10)))\nprint(\"My Agent vs Negamax Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=10)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}