{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"install_libraries\"></a>\n# Install libraries\n[Back to Table of Contents](#ToC)","metadata":{}},{"cell_type":"markdown","source":"This is a fork and an upgrade of Hieu Phung Code \n","metadata":{}},{"cell_type":"code","source":"!pip install 'kaggle-environments==0.1.6' > /dev/null 2>&1","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-29T10:34:35.321659Z","iopub.execute_input":"2021-08-29T10:34:35.321981Z","iopub.status.idle":"2021-08-29T10:34:44.017988Z","shell.execute_reply.started":"2021-08-29T10:34:35.321935Z","shell.execute_reply":"2021-08-29T10:34:44.017056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"import_libraries\"></a>\n# Import libraries\n[Back to Table of Contents](#ToC)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport gym\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom kaggle_environments import evaluate, make","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-08-29T10:34:44.020128Z","iopub.execute_input":"2021-08-29T10:34:44.020405Z","iopub.status.idle":"2021-08-29T10:34:49.990745Z","shell.execute_reply.started":"2021-08-29T10:34:44.020358Z","shell.execute_reply":"2021-08-29T10:34:49.989697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"define_useful_classes\"></a>\n# Define useful classes\nNOTE: I use the neural network in [*Siwei Xu's tutorial\n*](https://towardsdatascience.com/deep-reinforcement-learning-build-a-deep-q-network-dqn-to-play-cartpole-with-tensorflow-2-and-gym-8e105744b998) with some proper modifications to adapt to the problem in ConnectX competition and be able to save trained and load pre-trained models.\n\n[Back to Table of Contents](#ToC)","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ConnectX(gym.Env):\n    def __init__(self, switch_prob=0.5):\n        self.env = make('connectx', debug=False)\n        self.pair = [None, 'random']\n        self.trainer = self.env.train(self.pair)\n        self.switch_prob = switch_prob\n\n        # Define required gym fields (examples):\n        config = self.env.configuration\n        self.action_space = gym.spaces.Discrete(config.columns)\n        self.observation_space = gym.spaces.Discrete(config.columns * config.rows)\n\n    def switch_trainer(self):\n        self.pair = self.pair[::-1]\n        self.trainer = self.env.train(self.pair)\n\n    def step(self, action):\n        return self.trainer.step(action)\n    \n    def reset(self):\n        if np.random.random() < self.switch_prob:\n            self.switch_trainer()\n        return self.trainer.reset()\n    \n    def render(self, **kwargs):\n        return self.env.render(**kwargs)\n    \n    \nclass DeepModel(tf.keras.Model):\n    def __init__(self, num_states, hidden_units, num_actions):\n        super(DeepModel, self).__init__()\n        self.input_layer = tf.keras.layers.InputLayer(input_shape=(num_states,))\n        self.hidden_layers = []\n        for i in hidden_units:\n            self.hidden_layers.append(tf.keras.layers.Dense(\n                i, activation='sigmoid', kernel_initializer='RandomNormal'))\n        self.output_layer = tf.keras.layers.Dense(\n            num_actions, activation='linear', kernel_initializer='RandomNormal')\n\n#     @tf.function\n    def call(self, inputs):\n        z = self.input_layer(inputs)\n        for layer in self.hidden_layers:\n            z = layer(z)\n        output = self.output_layer(z)\n        return output\n\n\nclass DQN:\n    def __init__(self, num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr):\n        self.num_actions = num_actions\n        self.batch_size = batch_size\n        self.optimizer = tf.keras.optimizers.Adam(lr)\n        self.gamma = gamma\n        self.model = DeepModel(num_states, hidden_units, num_actions)\n        self.experience = {'s': [], 'a': [], 'r': [], 's2': [], 'done': []} # The buffer\n        self.max_experiences = max_experiences\n        self.min_experiences = min_experiences\n\n    def predict(self, inputs):\n        return self.model(np.atleast_2d(inputs.astype('float32')))\n\n#     @tf.function\n    def train(self, TargetNet):\n        if len(self.experience['s']) < self.min_experiences:\n            # Only start the training process when we have enough experiences in the buffer\n            return 0\n\n        # Randomly select n experience in the buffer, n is batch-size\n        ids = np.random.randint(low=0, high=len(self.experience['s']), size=self.batch_size)\n        states = np.asarray([self.preprocess(self.experience['s'][i]) for i in ids])\n        actions = np.asarray([self.experience['a'][i] for i in ids])\n        rewards = np.asarray([self.experience['r'][i] for i in ids])\n\n        # Prepare labels for training process\n        states_next = np.asarray([self.preprocess(self.experience['s2'][i]) for i in ids])\n        dones = np.asarray([self.experience['done'][i] for i in ids])\n        value_next = np.max(TargetNet.predict(states_next), axis=1)\n        actual_values = np.where(dones, rewards, rewards+self.gamma*value_next)\n\n        with tf.GradientTape() as tape:\n            selected_action_values = tf.math.reduce_sum(\n                self.predict(states) * tf.one_hot(actions, self.num_actions), axis=1)\n            loss = tf.math.reduce_sum(tf.square(actual_values - selected_action_values))\n        variables = self.model.trainable_variables\n        gradients = tape.gradient(loss, variables)\n        self.optimizer.apply_gradients(zip(gradients, variables))\n\n    # Get an action by using epsilon-greedy\n    def get_action(self, state, epsilon):\n        if np.random.random() < epsilon:\n            return int(np.random.choice([c for c in range(self.num_actions) if state.board[c] == 0]))\n        else:\n            prediction = self.predict(np.atleast_2d(self.preprocess(state)))[0].numpy()\n            for i in range(self.num_actions):\n                if state.board[i] != 0:\n                    prediction[i] = -1e7\n            return int(np.argmax(prediction))\n\n    # Method used to manage the buffer\n    def add_experience(self, exp):\n        if len(self.experience['s']) >= self.max_experiences:\n            for key in self.experience.keys():\n                self.experience[key].pop(0)\n        for key, value in exp.items():\n            self.experience[key].append(value)\n\n    def copy_weights(self, TrainNet):\n        variables1 = self.model.trainable_variables\n        variables2 = TrainNet.model.trainable_variables\n        for v1, v2 in zip(variables1, variables2):\n            v1.assign(v2.numpy())\n\n    def save_weights(self, path):\n        self.model.save_weights(path)\n\n    def load_weights(self, path):\n        ref_model = tf.keras.Sequential()\n\n        ref_model.add(self.model.input_layer)\n        for layer in self.model.hidden_layers:\n            ref_model.add(layer)\n        ref_model.add(self.model.output_layer)\n\n        ref_model.load_weights(path)\n    \n    # Each state will consist of the board and the mark\n    # in the observations\n    def preprocess(self, state):\n        result = state.board[:]\n        result.append(state.mark)\n\n        return result","metadata":{"execution":{"iopub.status.busy":"2021-08-29T10:34:49.992533Z","iopub.execute_input":"2021-08-29T10:34:49.992996Z","iopub.status.idle":"2021-08-29T10:34:50.049034Z","shell.execute_reply.started":"2021-08-29T10:34:49.992914Z","shell.execute_reply":"2021-08-29T10:34:50.048153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"define_helper_functions\"></a>\n# Define helper-functions\n[Back to Table of Contents](#ToC)","metadata":{}},{"cell_type":"code","source":"def play_game(env, TrainNet, TargetNet, epsilon, copy_step):\n    rewards = 0\n    iter = 0\n    done = False\n    observations = env.reset()\n    while not done:\n        # Using epsilon-greedy to get an action\n        action = TrainNet.get_action(observations, epsilon)\n\n        # Caching the information of current state\n        prev_observations = observations\n\n        # Take action\n        observations, reward, done, _ = env.step(action)\n\n        # Apply new rules\n        if done:\n            if reward == 1: # Won\n                reward = 20\n            elif reward == 0: # Lost\n                reward = -20\n            else: # Draw\n                reward = -5\n        else:\n            reward = -0.05 # Try to prevent the agent from taking a long move\n\n        rewards += reward\n\n        # Adding experience into buffer\n        exp = {'s': prev_observations, 'a': action, 'r': reward, 's2': observations, 'done': done}\n        TrainNet.add_experience(exp)\n\n        # Train the training model by using experiences in buffer and the target model\n        TrainNet.train(TargetNet)\n        iter += 1\n        if iter % copy_step == 0:\n            # Update the weights of the target model when reaching enough \"copy step\"\n            TargetNet.copy_weights(TrainNet)\n    return rewards","metadata":{"execution":{"iopub.status.busy":"2021-08-29T10:34:50.05034Z","iopub.execute_input":"2021-08-29T10:34:50.050623Z","iopub.status.idle":"2021-08-29T10:34:50.064518Z","shell.execute_reply.started":"2021-08-29T10:34:50.050555Z","shell.execute_reply":"2021-08-29T10:34:50.060551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"create_connectx_environment\"></a>\n# Create ConnectX environment\n[Back to Table of Contents](#ToC)","metadata":{}},{"cell_type":"code","source":"env = ConnectX()\nenv.pair =[None,'random']","metadata":{"execution":{"iopub.status.busy":"2021-08-29T10:34:50.067682Z","iopub.execute_input":"2021-08-29T10:34:50.068247Z","iopub.status.idle":"2021-08-29T10:34:50.125311Z","shell.execute_reply.started":"2021-08-29T10:34:50.068194Z","shell.execute_reply":"2021-08-29T10:34:50.124494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"configure_hyper_parameters\"></a>\n# Configure hyper-parameters\n[Back to Table of Contents](#ToC)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gamma = 0.99\ncopy_step = 25\nhidden_units = [50, 100, 200, 300, 100]\nmax_experiences = 10000\nmin_experiences = 100\nbatch_size = 32\nlr = 0.01\nepsilon = 0.99\ndecay = 0.99999\nmin_epsilon = 0.1\nepisodes = 20000\n\nprecision = 7\n\n# log_dir = 'logs/'\n# summary_writer = tf.summary.create_file_writer(log_dir)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T10:34:50.127172Z","iopub.execute_input":"2021-08-29T10:34:50.127565Z","iopub.status.idle":"2021-08-29T10:34:50.133336Z","shell.execute_reply.started":"2021-08-29T10:34:50.127523Z","shell.execute_reply":"2021-08-29T10:34:50.132689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"train_the_agent\"></a>\n# Train the agent\n[Back to Table of Contents](#ToC)","metadata":{}},{"cell_type":"code","source":"num_states = env.observation_space.n + 1\nnum_actions = env.action_space.n\n\nall_total_rewards = np.empty(episodes)\nall_avg_rewards = np.empty(episodes) # Last 100 steps\nall_epsilons = np.empty(episodes)\n\n# Initialize models\nTrainNet = DQN(num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\nTargetNet = DQN(num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T10:34:50.134577Z","iopub.execute_input":"2021-08-29T10:34:50.135089Z","iopub.status.idle":"2021-08-29T10:34:50.181317Z","shell.execute_reply.started":"2021-08-29T10:34:50.135037Z","shell.execute_reply":"2021-08-29T10:34:50.180533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pbar = tqdm(range(episodes))\nfor n in pbar:\n    epsilon = max(min_epsilon, epsilon * decay)\n    total_reward = play_game(env, TrainNet, TargetNet, epsilon, copy_step)\n    all_total_rewards[n] = total_reward\n    avg_reward = all_total_rewards[max(0, n - 100):(n + 1)].mean()\n    all_avg_rewards[n] = avg_reward\n    all_epsilons[n] = epsilon\n\n    pbar.set_postfix({\n        'episode reward': total_reward,\n        'avg (100 last) reward': avg_reward,\n        'epsilon': epsilon\n    })\n\n#     with summary_writer.as_default():\n#         tf.summary.scalar('episode reward', total_reward, step=n)\n#         tf.summary.scalar('running avg reward (100)', avg_reward, step=n)\n#         tf.summary.scalar('epsilon', epsilon, step=n)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T10:34:50.182456Z","iopub.execute_input":"2021-08-29T10:34:50.182903Z","iopub.status.idle":"2021-08-29T11:35:22.523029Z","shell.execute_reply.started":"2021-08-29T10:34:50.182862Z","shell.execute_reply":"2021-08-29T11:35:22.522112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.plot(all_total_rewards)\n# plt.xlabel('Episode')\n# plt.ylabel('Total rewards')\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T11:35:22.524584Z","iopub.execute_input":"2021-08-29T11:35:22.524911Z","iopub.status.idle":"2021-08-29T11:35:22.532117Z","shell.execute_reply.started":"2021-08-29T11:35:22.524848Z","shell.execute_reply":"2021-08-29T11:35:22.529369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(all_avg_rewards)\nplt.xlabel('Episode')\nplt.ylabel('Avg rewards (100)')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T11:35:22.53505Z","iopub.execute_input":"2021-08-29T11:35:22.535735Z","iopub.status.idle":"2021-08-29T11:35:22.819208Z","shell.execute_reply.started":"2021-08-29T11:35:22.535674Z","shell.execute_reply":"2021-08-29T11:35:22.818172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def agent_random(obs, config):\n    valid_moves = [col for col in range(config.columns) if obs.board[col] == 0]\n    return random.choice(valid_moves)\n\n# Toujour au centre\ndef agent_middle(obs, config):\n    return config.columns//2\n\n# Le plus à gauche des mouves valids\ndef agent_leftmost(obs, config):\n    valid_moves = [col for col in range(config.columns) if obs.board[col] == 0]\n    return valid_moves[0]","metadata":{"execution":{"iopub.status.busy":"2021-08-29T11:35:22.820952Z","iopub.execute_input":"2021-08-29T11:35:22.821506Z","iopub.status.idle":"2021-08-29T11:35:22.831739Z","shell.execute_reply.started":"2021-08-29T11:35:22.821445Z","shell.execute_reply":"2021-08-29T11:35:22.830813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env.pair =[None,agent_leftmost]\n\ndef training(episodes):\n    \n    gamma = 0.99\n    copy_step = 25\n    hidden_units = [100, 200, 200, 100]\n    max_experiences = 10000\n    min_experiences = 100\n    batch_size = 32\n    lr = 0.01\n    epsilon = 0.99\n    decay = 0.99999\n    min_epsilon = 0.1\n\n    precision = 7\n\n    all_total_rewards = np.empty(episodes)\n    all_avg_rewards = np.empty(episodes) # Last 100 steps\n    all_epsilons = np.empty(episodes)\n\n\n    pbar = tqdm(range(episodes))\n    for n in pbar:\n        epsilon = max(min_epsilon, epsilon * decay)\n        total_reward = play_game(env, TrainNet, TargetNet, epsilon, copy_step)\n        all_total_rewards[n] = total_reward\n        avg_reward = all_total_rewards[max(0, n - 100):(n + 1)].mean()\n        all_avg_rewards[n] = avg_reward\n        all_epsilons[n] = epsilon\n\n        pbar.set_postfix({\n            'episode reward': total_reward,\n            'avg (100 last) reward': avg_reward,\n            'epsilon': epsilon\n        })\n    plt.plot(all_avg_rewards)\n    plt.xlabel('Episode')\n    plt.ylabel('Avg rewards (100)')\n    plt.show()\n    def mean_reward(rewards):\n        return sum(r[0] for r in rewards) / sum(r[0] + r[1] for r in rewards)\n\n   ","metadata":{"execution":{"iopub.status.busy":"2021-08-29T11:35:22.833421Z","iopub.execute_input":"2021-08-29T11:35:22.834002Z","iopub.status.idle":"2021-08-29T11:35:22.854657Z","shell.execute_reply.started":"2021-08-29T11:35:22.833939Z","shell.execute_reply":"2021-08-29T11:35:22.853701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training(7000)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T11:35:22.856264Z","iopub.execute_input":"2021-08-29T11:35:22.856894Z","iopub.status.idle":"2021-08-29T11:46:32.534877Z","shell.execute_reply.started":"2021-08-29T11:35:22.856782Z","shell.execute_reply":"2021-08-29T11:46:32.5337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculates score if agent drops piece in selected column\ndef score_move(grid, col, mark, config):\n    next_grid = drop_piece(grid, col, mark, config)\n    score = get_heuristic(next_grid, mark, config)\n    return score\n\ndef score_move_2(grid, col, mark, config):\n    next_grid = drop_piece(grid, col, mark, config)\n    score = get_heuristic_2(next_grid, mark, config)\n    return score\n\n# Helper function for score_move: gets board at next step if agent drops piece in selected column\ndef drop_piece(grid, col, mark, config):\n    next_grid = grid.copy()\n    for row in range(config.rows-1, -1, -1):\n        if next_grid[row][col] == 0:\n            break\n    next_grid[row][col] = mark\n    return next_grid\n\n# Helper function for score_move: calculates value of heuristic for grid\ndef get_heuristic(grid, mark, config):\n    num_threes = count_windows(grid, 3, mark, config)\n    num_fours = count_windows(grid, 4, mark, config)\n    num_threes_opp = count_windows(grid, 3, mark%2+1, config)\n    score = num_threes - 1e2*num_threes_opp + 1e6*num_fours \n    return score\n\ndef get_heuristic_2(grid, mark, config):\n    num_threes = count_windows(grid, 3, mark, config)\n    num_fours = count_windows(grid, 4, mark, config)\n    num_threes_opp = count_windows(grid, 3, mark%2+1, config)\n    score = num_threes - 1e2*num_threes_opp + 1e6*num_fours + get_heuristic(grid, mark, config)*0.8\n    return score\n\n\n# Helper function for get_heuristic: checks if window satisfies heuristic conditions\ndef check_window(window, num_discs, piece, config):\n    return (window.count(piece) == num_discs and window.count(0) == config.inarow-num_discs)\n    \n# Helper function for get_heuristic: counts number of windows satisfying specified heuristic conditions\ndef count_windows(grid, num_discs, piece, config):\n    num_windows = 0\n    # horizontal\n    for row in range(config.rows):\n        for col in range(config.columns-(config.inarow-1)):\n            window = list(grid[row, col:col+config.inarow])\n            if check_window(window, num_discs, piece, config):\n                num_windows += 1\n    # vertical\n    for row in range(config.rows-(config.inarow-1)):\n        for col in range(config.columns):\n            window = list(grid[row:row+config.inarow, col])\n            if check_window(window, num_discs, piece, config):\n                num_windows += 1\n    # positive diagonal\n    for row in range(config.rows-(config.inarow-1)):\n        for col in range(config.columns-(config.inarow-1)):\n            window = list(grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n            if check_window(window, num_discs, piece, config):\n                num_windows += 1\n    # negative diagonal\n    for row in range(config.inarow-1, config.rows):\n        for col in range(config.columns-(config.inarow-1)):\n            window = list(grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n            if check_window(window, num_discs, piece, config):\n                num_windows += 1\n    return num_windows","metadata":{"execution":{"iopub.status.busy":"2021-08-29T11:46:32.536405Z","iopub.execute_input":"2021-08-29T11:46:32.536777Z","iopub.status.idle":"2021-08-29T11:46:32.578387Z","shell.execute_reply.started":"2021-08-29T11:46:32.536707Z","shell.execute_reply":"2021-08-29T11:46:32.5772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def agent_heristique(obs, config):\n    # Get list of valid moves\n    valid_moves = [c for c in range(config.columns) if obs.board[c] == 0]\n    # Convert the board to a 2D grid\n    grid = np.asarray(obs.board).reshape(config.rows, config.columns)\n    # Use the heuristic to assign a score to each possible board in the next turn\n    scores = dict(zip(valid_moves, [score_move(grid, col, obs.mark, config) for col in valid_moves]))\n    # Get a list of columns (moves) that maximize the heuristic\n    max_cols = [key for key in scores.keys() if scores[key] == max(scores.values())]\n    # Select at random from the maximizing columns\n    return random.choice(max_cols)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T11:46:32.580289Z","iopub.execute_input":"2021-08-29T11:46:32.580674Z","iopub.status.idle":"2021-08-29T11:46:32.599379Z","shell.execute_reply.started":"2021-08-29T11:46:32.580602Z","shell.execute_reply":"2021-08-29T11:46:32.598602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env = ConnectX()\nenv.pair =[None,'negamax']","metadata":{"execution":{"iopub.status.busy":"2021-08-29T11:46:32.600481Z","iopub.execute_input":"2021-08-29T11:46:32.600772Z","iopub.status.idle":"2021-08-29T11:46:32.628719Z","shell.execute_reply.started":"2021-08-29T11:46:32.600718Z","shell.execute_reply":"2021-08-29T11:46:32.627816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training(5000)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T11:46:32.630157Z","iopub.execute_input":"2021-08-29T11:46:32.630543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"save_weights\"></a>\n# Save weights\n[Back to Table of Contents](#ToC)","metadata":{}},{"cell_type":"code","source":"TrainNet.save_weights('./weights.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"create_an_agent\"></a>\n# Create an agent\n[Back to Table of Contents](#ToC)","metadata":{}},{"cell_type":"code","source":"fc_layers = []\n\n# Get all hidden layers' weights\nfor i in range(len(hidden_units)):\n    fc_layers.extend([\n        TrainNet.model.hidden_layers[i].weights[0].numpy().tolist(), # weights\n        TrainNet.model.hidden_layers[i].weights[1].numpy().tolist() # bias\n    ])\n\n# Get output layer's weights\nfc_layers.extend([\n    TrainNet.model.output_layer.weights[0].numpy().tolist(), # weights\n    TrainNet.model.output_layer.weights[1].numpy().tolist() # bias\n])\n\n# Convert all layers into usable form before integrating to final agent\nfc_layers = list(map(\n    lambda x: str(list(np.round(x, precision))) \\\n        .replace('array(', '').replace(')', '') \\\n        .replace(' ', '') \\\n        .replace('\\n', ''),\n    fc_layers\n))\nfc_layers = np.reshape(fc_layers, (-1, 2))\n\n# Create the agent\nmy_agent = '''def my_agent(observation, configuration):\n    import numpy as np\n\n'''\n\n# Write hidden layers\nfor i, (w, b) in enumerate(fc_layers[:-1]):\n    my_agent += '    hl{}_w = np.array({}, dtype=np.float32)\\n'.format(i+1, w)\n    my_agent += '    hl{}_b = np.array({}, dtype=np.float32)\\n'.format(i+1, b)\n# Write output layer\nmy_agent += '    ol_w = np.array({}, dtype=np.float32)\\n'.format(fc_layers[-1][0])\nmy_agent += '    ol_b = np.array({}, dtype=np.float32)\\n'.format(fc_layers[-1][1])\n\nmy_agent += '''\n    state = observation.board[:]\n    state.append(observation.mark)\n    out = np.array(state, dtype=np.float32)\n\n'''\n\n# Calculate hidden layers\nfor i in range(len(fc_layers[:-1])):\n    my_agent += '    out = np.matmul(out, hl{0}_w) + hl{0}_b\\n'.format(i+1)\n    my_agent += '    out = 1/(1 + np.exp(-out))\\n' # Sigmoid function\n# Calculate output layer\nmy_agent += '    out = np.matmul(out, ol_w) + ol_b\\n'\n\nmy_agent += '''\n    for i in range(configuration.columns):\n        if observation.board[i] != 0:\n            out[i] = -1e7\n\n    return int(np.argmax(out))\n    '''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('submission.py', 'w') as f:\n    f.write(my_agent)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"evaluate_the_agent\"></a>\n# Evaluate the agent\n[Back to Table of Contents](#ToC)","metadata":{}},{"cell_type":"code","source":"from submission import my_agent","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mean_reward(rewards):\n    return sum(r[0] for r in rewards) / sum(r[0] + r[1] for r in rewards)\n\n# Run multiple episodes to estimate agent's performance.\nprint(\"My Agent vs. Random Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"random\"], num_episodes=10)))\nprint(\"My Agent vs. Negamax Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=10)))\nprint(\"Random Agent vs. My Agent:\", mean_reward(evaluate(\"connectx\", [\"random\", my_agent], num_episodes=10)))\nprint(\"Negamax Agent vs. My Agent:\", mean_reward(evaluate(\"connectx\", [\"negamax\", my_agent], num_episodes=10)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}