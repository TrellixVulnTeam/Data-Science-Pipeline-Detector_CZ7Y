{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Doule DQN\n\n## Installs & Imports","metadata":{}},{"cell_type":"code","source":"# !pip install 'kaggle-environments>=0.1.6'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_environments import evaluate, make, utils\nfrom kaggle_environments import agent as KAgent\nfrom tqdm.notebook import tqdm\nfrom random import choice\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nimport gym\nimport inspect\nimport os\nimport sys","metadata":{"execution":{"iopub.status.busy":"2022-01-27T09:51:39.130917Z","iopub.execute_input":"2022-01-27T09:51:39.131124Z","iopub.status.idle":"2022-01-27T09:51:40.813955Z","shell.execute_reply.started":"2022-01-27T09:51:39.131098Z","shell.execute_reply":"2022-01-27T09:51:40.813237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Environment","metadata":{}},{"cell_type":"markdown","source":"Create an envrionment class to do all the things:-\n- reset\n- render\n- step\n- initialize","metadata":{}},{"cell_type":"code","source":"class connectX(gym.Env):\n    def __init__(self, switch_prob=0.5):\n        \n        # create environment\n        self.env = make('connectx', debug=True)\n        \n        # create opponent\n        self.pair = [None, 'negamax']\n        self.trainer = self.env.train(self.pair)\n        \n        self.switch_prob = switch_prob\n        \n        # initialize action space and observation space\n        config = self.env.configuration\n        \n        self.action_space = gym.spaces.Discrete(config.columns)\n        self.observation_space = gym.spaces.Discrete(config.columns * config.rows)\n    \n    # to switch trainer\n    def switch_trainer(self):\n        self.pair = self.pair[::-1]\n        self.trainer = self.env.train(self.pair)\n    \n    # do the action against trainer\n    def step(self, action):\n        return self.trainer.step(action)\n    \n    # reset trainer\n    def reset(self):\n        if random.uniform(0, 1) < self.switch_prob:\n            self.switch_trainer()\n        return self.trainer.reset()\n    \n    # render environment\n    def render(self, **kwargs):\n        return self.env.render(**kwargs)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T09:51:40.815672Z","iopub.execute_input":"2022-01-27T09:51:40.815909Z","iopub.status.idle":"2022-01-27T09:51:40.826109Z","shell.execute_reply.started":"2022-01-27T09:51:40.815875Z","shell.execute_reply":"2022-01-27T09:51:40.825228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create environment \nenv = connectX()","metadata":{"execution":{"iopub.status.busy":"2022-01-27T09:51:40.827927Z","iopub.execute_input":"2022-01-27T09:51:40.828511Z","iopub.status.idle":"2022-01-27T09:51:40.920477Z","shell.execute_reply.started":"2022-01-27T09:51:40.828471Z","shell.execute_reply":"2022-01-27T09:51:40.919458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"state = env.reset()","metadata":{"execution":{"iopub.status.busy":"2022-01-27T09:51:40.927509Z","iopub.execute_input":"2022-01-27T09:51:40.933551Z","iopub.status.idle":"2022-01-27T09:51:41.517124Z","shell.execute_reply.started":"2022-01-27T09:51:40.9335Z","shell.execute_reply":"2022-01-27T09:51:41.516411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(state)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T09:51:41.5184Z","iopub.execute_input":"2022-01-27T09:51:41.518728Z","iopub.status.idle":"2022-01-27T09:51:41.526573Z","shell.execute_reply.started":"2022-01-27T09:51:41.518689Z","shell.execute_reply":"2022-01-27T09:51:41.52577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Debug/Train Agent","metadata":{}},{"cell_type":"markdown","source":"We will use the Deep Q-learning method. Create a Neural network model and we will train the weights to choose correct action.","metadata":{}},{"cell_type":"markdown","source":"**Deep Learning Model**","metadata":{}},{"cell_type":"code","source":"class model(nn.Module):\n    def __init__(self, num_states, hidden_units, num_actions):\n        super(model, self).__init__()\n        \n        # initialize hidden layers\n        self.hidden_layers = nn.ModuleList([])\n        \n        # add more layers \n        for i in range(len(hidden_units)):\n            if i == 0:\n                self.hidden_layers.append(nn.Linear(num_states, hidden_units[i]))\n            else:\n                self.hidden_layers.append(nn.Linear(hidden_units[i-1], hidden_units[i]))\n        \n        # initialize output layer\n        self.output_layer = nn.Linear(hidden_units[-1], num_actions)\n    \n    # create forward function\n    def forward(self, x):\n        # pass the input through hidden layers\n        for layer in self.hidden_layers:\n            x = torch.sigmoid(layer(x))\n        \n        # pass through output layer\n        x = self.output_layer(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2022-01-27T09:51:41.528083Z","iopub.execute_input":"2022-01-27T09:51:41.52842Z","iopub.status.idle":"2022-01-27T09:51:41.539384Z","shell.execute_reply.started":"2022-01-27T09:51:41.528385Z","shell.execute_reply":"2022-01-27T09:51:41.538333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**DQN Class**","metadata":{}},{"cell_type":"code","source":"class DQN:\n    def __init__(self, num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr):\n        # initialize hyperparameters\n        self.num_actions = num_actions\n        self.batch_size = batch_size\n        self.gamma = gamma\n        \n        # initialize the agent model\n        self.model = model(num_states, hidden_units, num_actions)\n        \n        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n        self.criterion = nn.MSELoss()\n        \n        # create the experience replay buffer\n        self.experience = {'s':[], 'a':[], 'r':[], 's2':[], 'done':[]}\n        \n        self.max_experiences = max_experiences\n        self.min_experiences = min_experiences\n        \n    # predict the q values for different action using model\n    def predict(self, inputs):\n        return self.model(torch.from_numpy(inputs).float())\n    \n    # create method to train the model\n    def train(self, targetNet):\n        \n        # check if we have min experience in the buffer\n        if len(self.experience['s']) < self.min_experiences:\n            return 0\n        \n        # select random indices\n        ids = np.random.randint(low=0, high=len(self.experience['s']), size=self.batch_size)\n        \n        # get batch of states, action, rewards\n        states = np.asarray([self.preprocess(self.experience['s'][i]) for i in ids])\n        actions = np.asarray([self.experience['a'][i] for i in ids])\n        rewards = np.asarray([self.experience['r'][i] for i in ids])\n        \n        # get next states values\n        states_next = np.asarray([self.preprocess(self.experience['s2'][i]) for i in ids])\n        dones = np.asarray([self.experience['done'][i] for i in ids])\n        value_next = np.max(TargetNet.predict(states_next).detach().numpy(), axis=1)\n        actual_values = np.where(dones, rewards, rewards+self.gamma*value_next)\n        \n        # one hot encoding for actions\n        actions = np.expand_dims(actions, axis=1)\n        actions_one_hot = torch.FloatTensor(self.batch_size, self.num_actions).zero_()\n        actions_one_hot = actions_one_hot.scatter_(1, torch.LongTensor(actions), 1)\n        \n        # get the q values for each (state, action)\n        selected_action_values = torch.sum(self.predict(states) * actions_one_hot, dim=1)\n        actual_values = torch.FloatTensor(actual_values)\n        \n        # change the weights\n        self.optimizer.zero_grad()\n        \n        loss = self.criterion(selected_action_values, actual_values)\n        loss.backward()\n        \n        self.optimizer.step()\n        \n    # choose action using epsilon-greedy method\n    def get_action(self, state, epsilon):\n        \n        # choose random action\n        if np.random.random() < epsilon:\n            return int(np.random.choice([c for c in range(self.num_actions) if state['board'][c] == 0]))\n        \n        # return best action\n        else:\n            prediction = self.predict(np.atleast_2d(self.preprocess(state)))[0].detach().numpy()\n            \n            # iterate all possible actions\n            for i in range(self.num_actions):\n                if state['board'][i] != 0:\n                    prediction[i] = -1e7\n            \n            return int(np.argmax(prediction))\n    \n    # define method to add experience to buffer\n    def add_experience(self, exp):\n        \n        # if have required experiences\n        if len(self.experience['s']) >= self.max_experiences:\n            \n            # remove last values \n            for key in self.experience.keys():\n                self.experience[key].pop(0)\n        \n        for key, value in exp.items():\n            self.experience[key].append(value)\n    \n    # extra functions\n    def copy_weights(self, TrainNet):\n        self.model.load_state_dict(TrainNet.state_dict())\n\n    def save_weights(self, path):\n        torch.save(self.model.state_dict(), path)\n\n    def load_weights(self, path):\n        self.model.load_state_dict(torch.load(path))\n    \n    # Each state will consist of the board and the mark\n    # in the observations\n    def preprocess(self, state):\n        result = state['board'][:]\n        result.append(state.mark)\n\n        return result","metadata":{"execution":{"iopub.status.busy":"2022-01-27T09:51:41.540928Z","iopub.execute_input":"2022-01-27T09:51:41.541397Z","iopub.status.idle":"2022-01-27T09:51:41.566021Z","shell.execute_reply.started":"2022-01-27T09:51:41.541347Z","shell.execute_reply":"2022-01-27T09:51:41.565139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Play games Class**","metadata":{}},{"cell_type":"code","source":"def play_game(env, TrainNet, TargetNet, epsilon, copy_step):\n    rewards = 0\n    iter = 0\n    done = False\n    observations = env.reset()\n    while not done:\n        # Using epsilon-greedy to get an action\n        action = TrainNet.get_action(observations, epsilon)\n\n        # Caching the information of current state\n        prev_observations = observations\n\n        # Take action\n        observations, reward, done, _ = env.step(action)\n\n        # Apply new rules\n        if done:\n            if reward == 1: # Won\n                reward = 20\n            elif reward == 0: # Lost\n                reward = -20\n            else:           # Draw\n                reward = 10\n        else:\n#             reward = -0.05 # Try to prevent the agent from taking a long move\n            reward = 0.5\n\n        rewards += reward\n\n        # Adding experience into buffer\n        exp = {'s': prev_observations, 'a': action, 'r': reward, 's2': observations, 'done': done}\n        TrainNet.add_experience(exp)\n\n        # Train the training model by using experiences in buffer and the target model\n        TrainNet.train(TargetNet)\n        iter += 1\n        if iter % copy_step == 0:\n            # Update the weights of the target model when reaching enough \"copy step\"\n            TargetNet.copy_weights(TrainNet)\n    \n    return rewards","metadata":{"execution":{"iopub.status.busy":"2022-01-27T09:51:41.56756Z","iopub.execute_input":"2022-01-27T09:51:41.567874Z","iopub.status.idle":"2022-01-27T09:51:41.579691Z","shell.execute_reply.started":"2022-01-27T09:51:41.567829Z","shell.execute_reply":"2022-01-27T09:51:41.578785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Define hyperparameters**","metadata":{}},{"cell_type":"code","source":"gamma = 0.99\ncopy_step = 25\nhidden_units = [128, 128, 128, 128, 128]   # no of hidden layers\n# hidden layers = 5\nmax_experiences = 10000\nmin_experiences = 100\nbatch_size = 32\nlr = 1e-2\nepsilon = 0.5\ndecay = 0.9999\nmin_epsilon = 0.1\nepisodes = 20000\n\nprecision = 7","metadata":{"execution":{"iopub.status.busy":"2022-01-27T09:51:41.580642Z","iopub.execute_input":"2022-01-27T09:51:41.580824Z","iopub.status.idle":"2022-01-27T09:51:41.59521Z","shell.execute_reply.started":"2022-01-27T09:51:41.580802Z","shell.execute_reply":"2022-01-27T09:51:41.594423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Training**","metadata":{}},{"cell_type":"code","source":"# number of possible states and actions\nnum_states = env.observation_space.n + 1\nnum_actions = env.action_space.n\n\n# create empty array to store results\nall_total_rewards = np.empty(episodes)\nall_avg_rewards = np.empty(episodes) # Last 100 steps\nall_epsilons = np.empty(episodes)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T09:51:41.597893Z","iopub.execute_input":"2022-01-27T09:51:41.598099Z","iopub.status.idle":"2022-01-27T09:51:41.604941Z","shell.execute_reply.started":"2022-01-27T09:51:41.598076Z","shell.execute_reply":"2022-01-27T09:51:41.604204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since we are using the Double DQn algorithm, hence we will create a training network and a target network.","metadata":{}},{"cell_type":"code","source":"# initialize the models\nTrainNet = DQN(num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\nTargetNet = DQN(num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T09:51:41.606186Z","iopub.execute_input":"2022-01-27T09:51:41.606472Z","iopub.status.idle":"2022-01-27T09:51:41.637557Z","shell.execute_reply.started":"2022-01-27T09:51:41.606437Z","shell.execute_reply":"2022-01-27T09:51:41.636906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train the model for episodes. We will use the trained weights for our submission agent.\n\nUsing Double DQN algorithm.","metadata":{}},{"cell_type":"code","source":"for i in tqdm(range(episodes)):\n    # get a epsilon value\n    epsilon = max(min_epsilon, epsilon * decay)\n    \n    # train the training model using target model (for one episode)\n    total_reward = play_game(env, TrainNet, TargetNet, epsilon, copy_step)\n    \n    # append the results\n    all_total_rewards[i] = total_reward\n    avg_reward = all_total_rewards[max(0, i - 100):(i + 1)].mean()\n    all_avg_rewards[i] = avg_reward\n    all_epsilons[i] = epsilon","metadata":{"execution":{"iopub.status.busy":"2022-01-27T09:51:41.638815Z","iopub.execute_input":"2022-01-27T09:51:41.639057Z","iopub.status.idle":"2022-01-27T09:52:05.813358Z","shell.execute_reply.started":"2022-01-27T09:51:41.639025Z","shell.execute_reply":"2022-01-27T09:52:05.81261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Plots**\n\nCreate some plots regarding the training work.","metadata":{}},{"cell_type":"code","source":"plt.plot(all_avg_rewards)\nplt.xlabel('Episode')\nplt.ylabel('Avg rewards (100)')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-27T09:52:05.814702Z","iopub.execute_input":"2022-01-27T09:52:05.814976Z","iopub.status.idle":"2022-01-27T09:52:06.034015Z","shell.execute_reply.started":"2022-01-27T09:52:05.814936Z","shell.execute_reply":"2022-01-27T09:52:06.033379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(all_epsilons)\nplt.xlabel('Episode')\nplt.ylabel('Epsilon')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-27T09:52:06.035252Z","iopub.execute_input":"2022-01-27T09:52:06.035656Z","iopub.status.idle":"2022-01-27T09:52:06.208849Z","shell.execute_reply.started":"2022-01-27T09:52:06.035619Z","shell.execute_reply":"2022-01-27T09:52:06.208172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since we will use the trained weights for creating an agent. Hence we have to save the weights for future use.","metadata":{}},{"cell_type":"code","source":"# save the weights\n\nTrainNet.save_weights('/weights.pth')","metadata":{"execution":{"iopub.status.busy":"2022-01-27T09:52:06.210082Z","iopub.execute_input":"2022-01-27T09:52:06.211514Z","iopub.status.idle":"2022-01-27T09:52:06.218674Z","shell.execute_reply.started":"2022-01-27T09:52:06.211472Z","shell.execute_reply":"2022-01-27T09:52:06.217954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create an Agent","metadata":{}},{"cell_type":"markdown","source":"Extract different layers weights and biases from network to store them in a list.","metadata":{}},{"cell_type":"code","source":"model_layers = []","metadata":{"execution":{"iopub.status.busy":"2022-01-27T09:52:06.220251Z","iopub.execute_input":"2022-01-27T09:52:06.220548Z","iopub.status.idle":"2022-01-27T09:52:06.229025Z","shell.execute_reply.started":"2022-01-27T09:52:06.220513Z","shell.execute_reply":"2022-01-27T09:52:06.228307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for each hidden layer\nfor i in range(len(hidden_units)):\n    model_layers.extend([\n        \n        # add the weights and biases\n        TrainNet.model.hidden_layers[i].weight.T.tolist(),  \n        TrainNet.model.hidden_layers[i].bias.tolist()\n    ])","metadata":{"execution":{"iopub.status.busy":"2022-01-27T09:52:06.230279Z","iopub.execute_input":"2022-01-27T09:52:06.230641Z","iopub.status.idle":"2022-01-27T09:52:06.241491Z","shell.execute_reply.started":"2022-01-27T09:52:06.230603Z","shell.execute_reply":"2022-01-27T09:52:06.24053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# output layer\nmodel_layers.extend([\n    \n    # add the weight and bias\n    TrainNet.model.output_layer.weight.T.tolist(),\n    TrainNet.model.output_layer.bias.tolist()\n])","metadata":{"execution":{"iopub.status.busy":"2022-01-27T09:52:06.244194Z","iopub.execute_input":"2022-01-27T09:52:06.244406Z","iopub.status.idle":"2022-01-27T09:52:06.251109Z","shell.execute_reply.started":"2022-01-27T09:52:06.244374Z","shell.execute_reply":"2022-01-27T09:52:06.250409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reshape the model\nmodel_layers = np.reshape(model_layers, (-1, 2))","metadata":{"execution":{"iopub.status.busy":"2022-01-27T09:52:06.253483Z","iopub.execute_input":"2022-01-27T09:52:06.25397Z","iopub.status.idle":"2022-01-27T09:52:06.265256Z","shell.execute_reply.started":"2022-01-27T09:52:06.25386Z","shell.execute_reply":"2022-01-27T09:52:06.264383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, create an agent that will use the above calculated models (or the trained model weights and biases).","metadata":{}},{"cell_type":"code","source":"def my_agent(observation, configuration):\n    \n    # create list of hidden and output layers\n    hl_w = [] * len(model_layers)  # n hidden layers\n    hl_b = [] * len(model_layers)\n    ol_w = []    # 1 output layer\n    ol_b = []\n    \n    # add hidden layers's weights and biases\n    for i, (w, b) in enumerate(model_layers[:-1]):\n        hl_w.append(np.array(w, dtype=np.float32))\n        hl_b.append(np.array(b, dtype=np.float32))\n    \n    # add output layer's weights and biases\n    ol_w = np.array(model_layers[-1][0], dtype=np.float32)\n    ol_b = np.array(model_layers[-1][1], dtype=np.float32)\n\n    # get current state of environment\n    # board \n    state = observation['board'][:]\n    state.append(observation.mark)\n    \n    # create result array\n    res = np.array(state, dtype=np.float32)\n    \n    # for each hidden layer\n#     for i in range(model_layers[:-1]):  # use the enumerate method\n    for i, (w, b) in enumerate(model_layers[:-1]):\n        # add weights and biases \n        res = np.matmul(res, hl_w[i]) + hl_b[i]\n        \n        # apply sigmoid function\n        res = 1 / (1 + np.exp(-res))\n    \n    # add weights and biases of output layer\n    res = np.matmul(res, ol_w) + ol_b\n    \n    # for unfilled columns set to min\n    for i in range(configuration.columns):\n        if observation['board'][i] != 0:\n            res[i] = 1e-7\n    \n    # return best action\n    return int(np.argmax(res))","metadata":{"execution":{"iopub.status.busy":"2022-01-27T09:52:17.054566Z","iopub.execute_input":"2022-01-27T09:52:17.054841Z","iopub.status.idle":"2022-01-27T09:52:17.0648Z","shell.execute_reply.started":"2022-01-27T09:52:17.054798Z","shell.execute_reply":"2022-01-27T09:52:17.063808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test Agent","metadata":{}},{"cell_type":"markdown","source":"**Against negamax**","metadata":{}},{"cell_type":"code","source":"# reset environment\nenv.reset()\n\n# get the opponent\ntrainer = env.trainer\n\n# get starting configurations\nobservation = trainer.reset()\nconfiguration = env.env.configuration","metadata":{"execution":{"iopub.status.busy":"2022-01-27T09:52:28.071845Z","iopub.execute_input":"2022-01-27T09:52:28.072106Z","iopub.status.idle":"2022-01-27T09:52:28.834807Z","shell.execute_reply.started":"2022-01-27T09:52:28.072074Z","shell.execute_reply":"2022-01-27T09:52:28.833952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"done = False\n\n# while episode is not finished\nwhile not done:\n    my_action = my_agent(observation, env.env.configuration)\n    print(\"My Action\", my_action)\n    \n    # keep playing\n    observation, reward, done, info = trainer.step(my_action)\n\nenv.render(mode=\"ipython\")","metadata":{"execution":{"iopub.status.busy":"2022-01-27T09:52:28.836488Z","iopub.execute_input":"2022-01-27T09:52:28.836745Z","iopub.status.idle":"2022-01-27T09:52:29.444544Z","shell.execute_reply.started":"2022-01-27T09:52:28.836708Z","shell.execute_reply":"2022-01-27T09:52:29.44382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Against random**","metadata":{}},{"cell_type":"code","source":"env.reset()\n\n# play\nenv.env.run([my_agent, \"random\"])\nenv.render(mode=\"ipython\", width=500, height=450)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T09:52:46.330168Z","iopub.execute_input":"2022-01-27T09:52:46.330447Z","iopub.status.idle":"2022-01-27T09:52:46.435808Z","shell.execute_reply.started":"2022-01-27T09:52:46.330416Z","shell.execute_reply":"2022-01-27T09:52:46.435159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate Agent","metadata":{}},{"cell_type":"markdown","source":"Import our agent in the way it will be used by the tester.","metadata":{}},{"cell_type":"code","source":"def mean_reward(rewards):\n    return sum(r[0] for r in rewards) / float(len(rewards))","metadata":{"execution":{"iopub.status.busy":"2022-01-27T09:52:57.250546Z","iopub.execute_input":"2022-01-27T09:52:57.250806Z","iopub.status.idle":"2022-01-27T09:52:57.255516Z","shell.execute_reply.started":"2022-01-27T09:52:57.250775Z","shell.execute_reply":"2022-01-27T09:52:57.254612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# run multiple episodes\nprint(\"My agent vs random agent: \", mean_reward(evaluate(\"connectx\", [my_agent, \"random\"], num_episodes=10)))","metadata":{"execution":{"iopub.status.busy":"2022-01-27T09:52:57.494096Z","iopub.execute_input":"2022-01-27T09:52:57.494333Z","iopub.status.idle":"2022-01-27T09:52:58.618045Z","shell.execute_reply.started":"2022-01-27T09:52:57.494299Z","shell.execute_reply":"2022-01-27T09:52:58.617259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"My agent vs Negamax agent: \", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=10)))","metadata":{"execution":{"iopub.status.busy":"2022-01-27T09:52:58.622521Z","iopub.execute_input":"2022-01-27T09:52:58.623104Z","iopub.status.idle":"2022-01-27T09:53:10.286207Z","shell.execute_reply.started":"2022-01-27T09:52:58.62306Z","shell.execute_reply":"2022-01-27T09:53:10.285429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Write Submission File","metadata":{}},{"cell_type":"markdown","source":"We have to write it as a string because the dictionary will not able to save.","metadata":{}},{"cell_type":"code","source":"# create agent\n\nmy_agent = '''def my_agent(observation, configuration):\n    # import required libraries\n    import numpy as np\n    \n'''\n    \n# NOTE - NO INTENDS\n# create list of hidden and output layers\n\n# add hidden layers's weights and biases\nfor i, (w, b) in enumerate(model_layers[:-1]):\n        \n    my_agent += '    hl{}_w = np.array({}, dtype=np.float32)\\n'.format(i+1, w)\n    my_agent +=  '    hl{}_b = np.array({}, dtype=np.float32)\\n'.format(i+1, b)\n    \n# add output layer's weights and biases\nmy_agent += '    ol_w = np.array({}, dtype=np.float32)\\n'.format(model_layers[-1][0])\nmy_agent += '    ol_b = np.array({}, dtype=np.float32)\\n'.format(model_layers[-1][1])\n\n# get current state of environment\n# board \nmy_agent += '''\n    state = observation['board'][:]\n    state.append(observation.mark)\n    \n    # create result array\n    res = np.array(state, dtype=np.float32)\n    \n'''\n    \n    \n    \n# for each hidden layer\n#     for i in range(model_layers[:-1]):  # use the enumerate method\nfor i, (w, b) in enumerate(model_layers[:-1]):\n    # add weights and biases \n    my_agent += '    res = np.matmul(res, hl{0}_w) + hl{0}_b \\n'.format(i+1)\n        \n    # apply sigmoid function\n    my_agent += '    res = 1 / (1 + np.exp(-res)) \\n'\n    \n# add weights and biases of output layer\nmy_agent += '    res = np.matmul(res, ol_w) + ol_b\\n'\n    \nmy_agent += '''\n    # for unfilled columns set to min\n    for i in range(configuration.columns):\n        if observation['board'][i] != 0:\n            res[i] = 1e-7\n    \n    # return best action\n    return int(np.argmax(res)) \n    '''","metadata":{"execution":{"iopub.status.busy":"2022-01-27T10:04:15.039961Z","iopub.execute_input":"2022-01-27T10:04:15.040694Z","iopub.status.idle":"2022-01-27T10:04:15.105667Z","shell.execute_reply.started":"2022-01-27T10:04:15.040655Z","shell.execute_reply":"2022-01-27T10:04:15.104563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# save our agent in a python file\nwith open('submission.py', 'w') as f:\n    f.write(my_agent)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T10:04:16.195983Z","iopub.execute_input":"2022-01-27T10:04:16.19666Z","iopub.status.idle":"2022-01-27T10:04:16.204026Z","shell.execute_reply.started":"2022-01-27T10:04:16.196619Z","shell.execute_reply":"2022-01-27T10:04:16.203228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Validate Submission","metadata":{}},{"cell_type":"markdown","source":"Play against itself.","metadata":{}},{"cell_type":"code","source":"import sys\nfrom kaggle_environments import agent\n\nout = sys.stdout\nsubmission = utils.read_file(\"/kaggle/working/submission.py\")\na = agent.get_last_callable(submission, path=submission)\nsys.stdout = out","metadata":{"execution":{"iopub.status.busy":"2022-01-27T10:04:16.912985Z","iopub.execute_input":"2022-01-27T10:04:16.913234Z","iopub.status.idle":"2022-01-27T10:04:17.242659Z","shell.execute_reply.started":"2022-01-27T10:04:16.913207Z","shell.execute_reply":"2022-01-27T10:04:17.241916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out = sys.stdout\n\n# read the file\nsubmission = utils.read_file(\"/kaggle/working/submission.py\")\n\n# get the agent\nagent = KAgent.get_last_callable(submission, path=submission)\nsys.stdout = out","metadata":{"execution":{"iopub.status.busy":"2022-01-27T10:04:29.658678Z","iopub.execute_input":"2022-01-27T10:04:29.659144Z","iopub.status.idle":"2022-01-27T10:04:29.866105Z","shell.execute_reply.started":"2022-01-27T10:04:29.659107Z","shell.execute_reply":"2022-01-27T10:04:29.865183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# play aginst itself\nenv = make(\"connectx\", debug=True)\nenv.run([agent, agent])\n\nprint(\"Success!\" if env.state[0].status == env.state[1].status == \"DONE\" else \"Failed...\")","metadata":{"execution":{"iopub.status.busy":"2022-01-27T10:04:32.271848Z","iopub.execute_input":"2022-01-27T10:04:32.272105Z","iopub.status.idle":"2022-01-27T10:04:32.765966Z","shell.execute_reply.started":"2022-01-27T10:04:32.272074Z","shell.execute_reply":"2022-01-27T10:04:32.765095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}