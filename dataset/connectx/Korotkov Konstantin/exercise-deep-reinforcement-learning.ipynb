{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import inspect\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n!pip install 'tensorflow==1.15.0'\n\nimport tensorflow as tf\nfrom kaggle_environments import make, evaluate\nfrom gym import spaces","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!apt-get update\n!apt-get install -y cmake libopenmpi-dev python3-dev zlib1g-dev\n!pip install \"stable-baselines[mpi]==2.9.0\"\n\nfrom stable_baselines.bench import Monitor \nfrom stable_baselines.common.vec_env import DummyVecEnv\nfrom stable_baselines import PPO1, A2C, ACER, ACKTR, TRPO\nfrom stable_baselines.a2c.utils import conv, linear, conv_to_fc\nfrom stable_baselines.common.policies import CnnPolicy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ConnectFourGym:\n    def __init__(self, agent2=\"negamax\"):\n        ks_env = make(\"connectx\", debug=True)\n        self.env = ks_env.train([None, agent2])\n        self.rows = ks_env.configuration.rows\n        self.columns = ks_env.configuration.columns\n        # Learn about spaces here: http://gym.openai.com/docs/#spaces\n        self.action_space = spaces.Discrete(self.columns)\n        self.observation_space = spaces.Box(low=0, high=2, \n                                            shape=(self.rows,self.columns,1), dtype=np.int)\n        # Tuple corresponding to the min and max possible rewards\n        self.reward_range = (-10, 1)\n        # StableBaselines throws error if these are not defined\n        self.spec = None\n        self.metadata = None\n    def reset(self):\n        self.obs = self.env.reset()\n        return np.array(self.obs['board']).reshape(self.rows,self.columns,1)\n    def change_reward(self, old_reward, done):\n        if old_reward == 1: # The agent won the game\n            return 1\n        elif done: # The opponent won the game\n            return -1\n        else: # Reward 1/42\n            return 1/(self.rows*self.columns)\n    def step(self, action):\n        # Check if agent's move is valid\n        is_valid = (self.obs['board'][int(action)] == 0)\n        if is_valid: # Play the move\n            self.obs, old_reward, done, _ = self.env.step(int(action))\n            reward = self.change_reward(old_reward, done)\n        else: # End the game and penalize agent\n            reward, done, _ = -10, True, {}\n        return np.array(self.obs['board']).reshape(self.rows,self.columns,1), reward, done, _","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create ConnectFour environment\nenv = ConnectFourGym(agent2=\"negamax\")\n\n# Create directory for logging training information\nlog_dir = \"log/\"\nos.makedirs(log_dir, exist_ok=True)\n\n# Logging progress\nmonitor_env = Monitor(env, log_dir, allow_early_resets=True)\n\n# Create a vectorized environment\nvec_env = DummyVecEnv([lambda: monitor_env])\n\n# Neural network for predicting action values\ndef modified_cnn(scaled_images, **kwargs):\n    activ = tf.nn.relu\n    layer_1 = activ(conv(scaled_images, 'c1', n_filters=32, filter_size=3, stride=1, \n                         init_scale=np.sqrt(2), **kwargs))\n    layer_2 = activ(conv(layer_1, 'c2', n_filters=64, filter_size=3, stride=1, \n                         init_scale=np.sqrt(2), **kwargs))\n    layer_2 = conv_to_fc(layer_2)\n    return activ(linear(layer_2, 'fc1', n_hidden=512, init_scale=np.sqrt(2)))  \n\nclass CustomCnnPolicy(CnnPolicy):\n    def __init__(self, *args, **kwargs):\n        super(CustomCnnPolicy, self).__init__(*args, **kwargs, cnn_extractor=modified_cnn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize agent\nmodel = PPO1(CustomCnnPolicy, vec_env, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train agent\nmodel.learn(total_timesteps=1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot cumulative reward\nwith open(os.path.join(log_dir, \"monitor.csv\"), 'rt') as fh:    \n    firstline = fh.readline()\n    assert firstline[0] == '#'\n    df = pd.read_csv(fh, index_col=None)['r']\ndf.rolling(window=10).mean().plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(inspect.getsource(PPO1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save(\"ppo1_model\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def agent1(obs, config):\n    # Use the best model to select a column\n    agent = PPO1.load(\"ppo1_model\")\n    col, _ = agent.predict(np.array(obs['board']).reshape(6,7,1))\n    # Check if selected column is valid\n    is_valid = (obs['board'][int(col)] == 0)\n    # If not valid, select random move. \n    if is_valid:\n        return int(col)\n    else:\n        return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the game environment\nenv = make(\"connectx\")\n\n# Two random agents play one game round\nenv.run([agent1, agent1])\n\n# Show the game\nenv.render(mode=\"ipython\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env.play([agent1, None], width=500, height=450)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To learn more about the evaluate() function, check out the documentation here: (insert link here)\ndef get_win_percentages(agent1, agent2, n_rounds=10):\n    # Use default Connect Four setup\n    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n    # Agent 1 goes first (roughly) half the time          \n    outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], n_rounds//2)\n    # Agent 2 goes first (roughly) half the time      \n    outcomes += [[b,a] for [a,b] in evaluate(\"connectx\", [agent2, agent1], config, [], n_rounds-n_rounds//2)]\n    print(\"Agent 1 Win Percentage:\", np.round(outcomes.count([1,0])/len(outcomes), 2))\n    print(\"Agent 2 Win Percentage:\", np.round(outcomes.count([0,1])/len(outcomes), 2))\n    print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0.5]))\n    print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0.5, None]))\n    print(\"Number of Draws (in {} game rounds):\".format(n_rounds), outcomes.count([0.5, 0.5]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get_win_percentages(agent1=agent1, agent2=\"negamax\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import inspect\nimport os\n\ndef write_agent_to_file(function, file):\n    with open(file, \"a\" if os.path.exists(file) else \"w\") as f:\n        f.write(inspect.getsource(function))\n        print(function, \"written to\", file)\n\nwrite_agent_to_file(agent1, \"submission.py\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}