{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# An improved version of Deep Reinforcement Learning\n\n(Original Notebook can be found [here.](https://www.kaggle.com/alexisbcook/deep-reinforcement-learning))\n\n\nImprovements done: \n- Added One Step Look Ahead (OSLA) and Alpha - Beta Pruned Three Step Look Ahead (TSLA) Agents.\n- Assigned 3 different agents as training partners for RL agent Training\n\n","metadata":{"_kg_hide-input":false}},{"cell_type":"code","source":"#First, required libraries are imported:\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Stable Baselines is not compatible with TensorFlow 2.0, thus, downgrading is required.\n!pip install 'tensorflow==1.15.0'","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the current version of tensorflow. It should be \"1.15.0\"\nimport tensorflow as tf\ntf.__version__","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating an alpha-beta pruned game-playing agent","metadata":{}},{"cell_type":"markdown","source":"Firstly, defining helper functions that are provided in the previous modules. ","metadata":{}},{"cell_type":"code","source":"# Helper function for get_heuristic: checks if window satisfies heuristic conditions\ndef check_window(window, num_discs, piece, config):\n    return (window.count(piece) == num_discs and window.count(0) == config.inarow-num_discs)\n    \n# Helper function for get_heuristic: counts number of windows satisfying specified heuristic conditions\ndef count_windows(grid, num_discs, piece, config):\n    num_windows = 0\n    # horizontal\n    for row in range(config.rows):\n        for col in range(config.columns-(config.inarow-1)):\n            window = list(grid[row, col:col+config.inarow])\n            if check_window(window, num_discs, piece, config):\n                num_windows += 1\n    # vertical\n    for row in range(config.rows-(config.inarow-1)):\n        for col in range(config.columns):\n            window = list(grid[row:row+config.inarow, col])\n            if check_window(window, num_discs, piece, config):\n                num_windows += 1\n    # positive diagonal\n    for row in range(config.rows-(config.inarow-1)):\n        for col in range(config.columns-(config.inarow-1)):\n            window = list(grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n            if check_window(window, num_discs, piece, config):\n                num_windows += 1\n    # negative diagonal\n    for row in range(config.inarow-1, config.rows):\n        for col in range(config.columns-(config.inarow-1)):\n            window = list(grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n            if check_window(window, num_discs, piece, config):\n                num_windows += 1\n    return num_windows\n\n# Gets board at next step if agent drops piece in selected column\ndef drop_piece(grid, col, mark, config):\n    next_grid = grid.copy()\n    for row in range(config.rows-1, -1, -1):\n        if next_grid[row][col] == 0:\n            break\n    next_grid[row][col] = mark\n    return next_grid","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Helper function for minimax: calculates value of heuristic for grid\ndef get_heuristic(grid, mark, config):\n    num_threes = count_windows(grid, 3, mark, config)\n    num_fours = count_windows(grid, 4, mark, config)\n    num_threes_opp = count_windows(grid, 3, mark%2+1, config)\n    num_fours_opp = count_windows(grid, 4, mark%2+1, config)\n    score = num_threes - 1e2*num_threes_opp - 1e4*num_fours_opp + 1e6*num_fours\n    return score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Uses minimax to calculate value of dropping piece in selected column\ndef score_move(grid, col, mark, config, nsteps):\n    next_grid = drop_piece(grid, col, mark, config)\n    score = minimax(next_grid, nsteps-1, False, mark, config)\n    return score\n\n# Helper function for minimax: checks if agent or opponent has four in a row in the window\ndef is_terminal_window(window, config):\n    return window.count(1) == config.inarow or window.count(2) == config.inarow\n\n# Helper function for minimax: checks if game has ended\ndef is_terminal_node(grid, config):\n    # Check for draw \n    if list(grid[0, :]).count(0) == 0:\n        return True\n    # Check for win: horizontal, vertical, or diagonal\n    # horizontal \n    for row in range(config.rows):\n        for col in range(config.columns-(config.inarow-1)):\n            window = list(grid[row, col:col+config.inarow])\n            if is_terminal_window(window, config):\n                return True\n    # vertical\n    for row in range(config.rows-(config.inarow-1)):\n        for col in range(config.columns):\n            window = list(grid[row:row+config.inarow, col])\n            if is_terminal_window(window, config):\n                return True\n    # positive diagonal\n    for row in range(config.rows-(config.inarow-1)):\n        for col in range(config.columns-(config.inarow-1)):\n            window = list(grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n            if is_terminal_window(window, config):\n                return True\n    # negative diagonal\n    for row in range(config.inarow-1, config.rows):\n        for col in range(config.columns-(config.inarow-1)):\n            window = list(grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n            if is_terminal_window(window, config):\n                return True\n    return False\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Minimax implementation\ndef minimax(node, depth, maximizingPlayer, mark, config, alpha = -np.Inf, beta = np.Inf):\n    is_terminal = is_terminal_node(node, config)\n    valid_moves = [c for c in range(config.columns) if node[0][c] == 0]\n    if depth == 0 or is_terminal:\n        return get_heuristic(node, mark, config)\n    if maximizingPlayer:\n        value = -np.Inf\n        for col in valid_moves:\n            child = drop_piece(node, col, mark, config)\n            value = max(value, minimax(child, depth-1, False, mark, config, alpha, beta))\n            alpha = max(alpha, value)\n            if alpha >= beta:\n                break\n        return value\n    else:\n        value = np.Inf\n        for col in valid_moves:\n            child = drop_piece(node, col, mark%2+1, config)\n            value = min(value, minimax(child, depth-1, True, mark, config, alpha, beta))\n            beta = min(beta, value)\n            if beta <= alpha:\n                break                \n        return value","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Depthness of the game tree. \nN_STEPS = 3\n\n# Defining the Alpha Pruned 3 Step Look Ahead (TSLA) Agent. \ndef agentTSLA(obs, config):\n    # Get list of valid moves\n    valid_moves = [c for c in range(config.columns) if obs.board[c] == 0]\n    # Convert the board to a 2D grid\n    grid = np.asarray(obs.board).reshape(config.rows, config.columns)\n    # Use the heuristic to assign a score to each possible board in the next step\n    scores = dict(zip(valid_moves, [score_move(grid, col, obs.mark, config, N_STEPS) for col in valid_moves]))\n    # Get a list of columns (moves) that maximize the heuristic\n    max_cols = [key for key in scores.keys() if scores[key] == max(scores.values())]\n    # Select at random from the maximizing columns\n    return random.choice(max_cols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_STEPS = 1\n\n# Defining the One Step Look Ahead (OSLA) Agent.\ndef agentOSLA(obs, config):\n    # Get list of valid moves\n    valid_moves = [c for c in range(config.columns) if obs.board[c] == 0]\n    # Convert the board to a 2D grid\n    grid = np.asarray(obs.board).reshape(config.rows, config.columns)\n    # Use the heuristic to assign a score to each possible board in the next step\n    scores = dict(zip(valid_moves, [score_move(grid, col, obs.mark, config, N_STEPS) for col in valid_moves]))\n    # Get a list of columns (moves) that maximize the heuristic\n    max_cols = [key for key in scores.keys() if scores[key] == max(scores.values())]\n    # Select at random from the maximizing columns\n    return random.choice(max_cols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As the agent defined above, let's check if it works against a random agent:","metadata":{}},{"cell_type":"code","source":"#This cell runs a FSLA agent against a random agent. \nfrom kaggle_environments import make, evaluate\n\n# Create the game environment\nenv = make(\"connectx\")\n\n# Run TSLA agent vs a random agent\nenv.run([agentTSLA, \"random\"])\n\n# Run TSLA agent vs OSLA agent\n#env.run([agentTSLA, agentOSLA])\n\n\n# Show the game\nenv.render(mode=\"ipython\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's define a function to check which decision making algorithm is more likely to win. Random or FSLA?","metadata":{}},{"cell_type":"code","source":"#This function calculates the percentage of wins per \"n\" rounds against the other agent given in the parameter. \ndef get_win_percentages(agent1, agent2, n_rounds=500):\n    # Use default Connect Four setup\n    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n    # Agent 1 goes first (roughly) half the time          \n    outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], n_rounds//2)\n    # Agent 2 goes first (roughly) half the time      \n    outcomes += [[b,a] for [a,b] in evaluate(\"connectx\", [agent1, agent2], config, [], n_rounds-n_rounds//2)]\n    print(\"Agent 1 Win Percentage:\", np.round(outcomes.count([1,-1])/len(outcomes), 2))\n    print(\"Agent 2 Win Percentage:\", np.round(outcomes.count([-1,1])/len(outcomes), 2))\n    print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0]))\n    print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0, None]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_win_percentages(agent1=agentTSLA, agent2=agentOSLA, n_rounds=100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As it is observed, an TSLA agent is more likely to win against a random agent.\nNow, let's train a Reinforcement Learning Agent using the newly defined TSLA agent.","metadata":{}},{"cell_type":"markdown","source":"# Creating an Open AI Gym environment\n\n`ConnectFourGym` class implements ConnectX as an [OpenAI Gym environment](http://gym.openai.com/docs/) and uses several methods:\n- `reset()` will be called at the beginning of every game.  It returns the starting game board as a 2D numpy array with 6 rows and 7 columns.\n- `change_reward()` customizes the rewards that the agent receives.  (_The competition already has its own system for rewards that are used to rank the agents, and this method changes the values to match the rewards system we designed._) \n- `step()` is used to play the agent's choice of action (supplied as `action`), along with the opponent's response.  It returns:\n  - the resulting game board (as a numpy array), \n  - the agent's reward (from the most recent move only: one of `+1`, `-10`, `-1`, or `1/42`), and\n  - whether or not the game has ended (if the game has ended, `done=True`; otherwise, `done=False`).","metadata":{}},{"cell_type":"code","source":"from kaggle_environments import make, evaluate\nfrom gym import spaces\n\nclass ConnectFourGym:\n    def __init__(self, agent2=\"random\"):\n        ks_env = make(\"connectx\", debug=True)\n        self.env = ks_env.train([None, agent2])\n        self.rows = ks_env.configuration.rows\n        self.columns = ks_env.configuration.columns\n        # Learn about spaces here: http://gym.openai.com/docs/#spaces\n        self.action_space = spaces.Discrete(self.columns)\n        self.observation_space = spaces.Box(low=0, high=2, \n                                            shape=(self.rows,self.columns,1), dtype=np.int)\n        # Tuple corresponding to the min and max possible rewards\n        self.reward_range = (-10, 1)\n        # StableBaselines throws error if these are not defined\n        self.spec = None\n        self.metadata = None\n    def reset(self):\n        self.obs = self.env.reset()\n        return np.array(self.obs['board']).reshape(self.rows,self.columns,1)\n    def change_reward(self, old_reward, done):\n        if old_reward == 1: # The agent won the game\n            return 1\n        elif done: # The opponent won the game\n            return -1\n        else: # Reward 1/42\n            return 1/(self.rows*self.columns)\n    def step(self, action):\n        # Check if agent's move is valid\n        is_valid = (self.obs['board'][int(action)] == 0)\n        if is_valid: # Play the move\n            self.obs, old_reward, done, _ = self.env.step(int(action))\n            reward = self.change_reward(old_reward, done)\n        else: # End the game and penalize agent\n            reward, done, _ = -10, True, {}\n        return np.array(self.obs['board']).reshape(self.rows,self.columns,1), reward, done, _","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's train an agent to beat the FSLA agent:","metadata":{}},{"cell_type":"code","source":"# Create ConnectFour environment\nenv = ConnectFourGym(agent2=agentOSLA)\n\n# env = ConnectFourGym(agent2=agentTSLA)\n# env = ConnectFourGym(agent2=\"random\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n[Stable Baselines](https://github.com/hill-a/stable-baselines) is used for the implementation of reinforcement learning algorithm.\n\nIt is a set of improved implementations of reinforcement learning algorithms based on OpenAI Baselines.\n\nStable Baselines requires us to work with [\"vectorized\" environments](https://stable-baselines.readthedocs.io/en/master/guide/vec_envs.html).  For this, we can use the `DummyVecEnv` class.  \n\nThe `Monitor` class lets us watch how the agent's performance gradually improves, as it plays more and more games.","metadata":{}},{"cell_type":"code","source":"!apt-get update\n!apt-get install -y cmake libopenmpi-dev python3-dev zlib1g-dev\n!pip install \"stable-baselines[mpi]==2.9.0\"","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom stable_baselines.bench import Monitor \nfrom stable_baselines.common.vec_env import DummyVecEnv\n\n# Create directory for logging training information\nlog_dir = \"ppo/\"\nos.makedirs(log_dir, exist_ok=True)\n\n# Logging progress\nmonitor_env = Monitor(env, log_dir, allow_early_resets=True)\n\n# Create a vectorized environment\nvec_env = DummyVecEnv([lambda: monitor_env])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The next step is to specify the architecture of the neural network.  In this case, we use a convolutional neural network.  To learn more about how to specify architectures with Stable Baselines, check out the documentation [here](https://stable-baselines.readthedocs.io/en/master/guide/custom_policy.html).\n\nNote that this is the neural network that outputs the probabilities of selecting each column.  Since we use the PPO algorithm (`PPO1` in the code cell below), our network will also output some additional information (called the \"value\" of the input).  This is outside the scope of this course, but you can learn more by reading about \"actor-critic networks\".","metadata":{}},{"cell_type":"code","source":"from stable_baselines import PPO1 \nfrom stable_baselines.a2c.utils import conv, linear, conv_to_fc\nfrom stable_baselines.common.policies import CnnPolicy\n\n# Neural network for predicting action values\ndef modified_cnn(scaled_images, **kwargs):\n    activ = tf.nn.relu\n    layer_1 = activ(conv(scaled_images, 'c1', n_filters=32, filter_size=3, stride=1, \n                         init_scale=np.sqrt(2), **kwargs))\n    layer_2 = activ(conv(layer_1, 'c2', n_filters=64, filter_size=3, stride=1, \n                         init_scale=np.sqrt(2), **kwargs))\n    layer_2 = conv_to_fc(layer_2)\n    return activ(linear(layer_2, 'fc1', n_hidden=512, init_scale=np.sqrt(2)))  \n\nclass CustomCnnPolicy(CnnPolicy):\n    def __init__(self, *args, **kwargs):\n        super(CustomCnnPolicy, self).__init__(*args, **kwargs, cnn_extractor=modified_cnn)\n        \n# Initialize agent\nmodel = PPO1(CustomCnnPolicy, vec_env, verbose=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the code cell above, the weights of the neural network are initially set to random values.\n\nIn the next code cell, we \"train the agent\", which is just another way of saying that we find weights of the neural network that are likely to result in the agent selecting good moves.\n\nWe plot a rolling average of the cumulative reward that the agent received during training.  As evidenced by the increasing function, the agent gradually learned to perform better by playing the game.","metadata":{}},{"cell_type":"code","source":"# Train agent\nmodel.learn(total_timesteps=100000)\n\n# Plot cumulative reward\nwith open(os.path.join(log_dir, \"monitor.csv\"), 'rt') as fh:    \n    firstline = fh.readline()\n    assert firstline[0] == '#'\n    df = pd.read_csv(fh, index_col=None)['r']\ndf.rolling(window=1000).mean().plot()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def agentRL(obs, config):\n    # Use the best model to select a column\n    col, _ = model.predict(np.array(obs['board']).reshape(6,7,1))\n    # Check if selected column is valid\n    is_valid = (obs['board'][int(col)] == 0)\n    # If not valid, select random move. \n    if is_valid:\n        return int(col)\n    else:\n        return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the next code cell, we see the outcome of one game round against a random agent.","metadata":{}},{"cell_type":"code","source":"# Create the game environment\nenv = make(\"connectx\")\n\n# Two random agents play one game round\nenv.run([agentRL, \"random\"])\n#env.run([RLAgent, oslaAgent])\n#env.run([oslaAgent, \"random\"])\n\n# Show the game\nenv.render(mode=\"ipython\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And, we calculate how it performs on average, against the random agent.","metadata":{}},{"cell_type":"code","source":"get_win_percentages(agent1 = agentRL, agent2 = agentOSLA, n_rounds = 1000)\nget_win_percentages(agent1 = agentRL, agent2 = agentTSLA, n_rounds = 1000)\nget_win_percentages(agent1 = agentRL, agent2 = \"random\", n_rounds = 1000)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n","metadata":{}}]}