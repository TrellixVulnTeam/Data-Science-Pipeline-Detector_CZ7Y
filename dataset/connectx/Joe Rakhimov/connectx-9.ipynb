{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install stable-baselines3","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import gym\nfrom kaggle_environments import make, evaluate\n\nimport os\nimport numpy as np\nimport torch as th\nfrom torch import nn as nn\nimport torch.nn.functional as F\nfrom numpy.random import choice\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.monitor import Monitor\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nfrom stable_baselines3.common.monitor import load_results\nfrom stable_baselines3.common.torch_layers import NatureCNN\nfrom stable_baselines3.common.policies import ActorCriticPolicy, ActorCriticCnnPolicy\nfrom stable_baselines3.common.torch_layers import BaseFeaturesExtractor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def minimax_agent(obs, config):\n    \n    ################################\n    # Imports and helper functions #\n    ################################\n    \n    import numpy as np\n    import random\n\n    # Calculates score if agent drops piece in selected column\n    def score_move(grid, col, mark, config):\n        next_grid = drop_piece(grid, col, mark, config)\n        score = get_heuristic(next_grid, mark, config)\n        return score\n\n    # Helper function for score_move: gets board at next step if agent drops piece in selected column\n    def drop_piece(grid, col, mark, config):\n        next_grid = grid.copy()\n        for row in range(config.rows-1, -1, -1):\n            if next_grid[row][col] == 0:\n                break\n        next_grid[row][col] = mark\n        return next_grid\n\n    # Helper function for score_move: calculates value of heuristic for grid\n    def get_heuristic(grid, mark, config):\n        num_twos = count_windows(grid, 2, mark, config)\n        num_threes = count_windows(grid, 3, mark, config)\n        num_fours = count_windows(grid, 4, mark, config)\n        num_twos_opp = count_windows(grid, 2, mark%2+1, config)\n        num_threes_opp = count_windows(grid, 3, mark%2+1, config)\n        score = num_fours * 10000 + num_threes * 10 + num_twos - num_twos_opp * 100 - num_threes_opp * 1000\n        return score\n\n    # Helper function for get_heuristic: checks if window satisfies heuristic conditions\n    def check_window(window, num_discs, piece, config):\n        return (window.count(piece) == num_discs and window.count(0) == config.inarow-num_discs)\n    \n    # Helper function for get_heuristic: counts number of windows satisfying specified heuristic conditions\n    def count_windows(grid, num_discs, piece, config):\n        num_windows = 0\n        # horizontal\n        for row in range(config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[row, col:col+config.inarow])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        # vertical\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns):\n                window = list(grid[row:row+config.inarow, col])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        # positive diagonal\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        # negative diagonal\n        for row in range(config.inarow-1, config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        return num_windows\n    \n    #########################\n    # Agent makes selection #\n    #########################\n    \n    # Get list of valid moves\n    valid_moves = [c for c in range(config.columns) if obs.board[c] == 0]\n    # Convert the board to a 2D grid\n    grid = np.asarray(obs.board).reshape(config.rows, config.columns)\n    # Use the heuristic to assign a score to each possible board in the next turn\n    scores = dict(zip(valid_moves, [score_move(grid, col, obs.mark, config) for col in valid_moves]))\n    # Get a list of columns (moves) that maximize the heuristic\n    max_cols = [key for key in scores.keys() if scores[key] == max(scores.values())]\n    # Select at random from the maximizing columns\n    return random.choice(max_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def board_flip(mark, board):\n    if mark == 1:\n        return board\n    for i in range(board.shape[0]):\n        for j in range(board.shape[1]):\n            if board[i, j, 0] != 0:\n                board[i, j, 0] = board[i, j, 0]%2 + 1\n                return board","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# version 6\nclass ConnectFourGym():\n#     def __init__(self, agent2=\"random\"):\n    def __init__(self, opponent_pool=np.asarray(['random']), distribution='even'):\n        self.ks_env = make(\"connectx\", debug=True)\n#         self.env = self.ks_env.train([None, agent2])\n        self.rows = self.ks_env.configuration.rows\n        self.columns = self.ks_env.configuration.columns\n        # Learn about spaces here: http://gym.openai.com/docs/#spaces\n        self.action_space = gym.spaces.Discrete(self.columns)\n        self.observation_space = gym.spaces.Box(low=0, high=1, \n                                            shape=(1,self.rows,self.columns), dtype=np.float)\n        # Tuple corresponding to the min and max possible rewards\n        self.reward_range = (-10, 1)\n        # StableBaselines throws error if these are not defined\n        self.spec = None\n        self.metadata = None\n        self.last_action = -1\n        self.iter = 0\n        self.opponent_pool = opponent_pool\n        self.distribution = distribution\n        self.init_env()\n        \n    def init_env(self):\n        if self.distribution == 'even':\n            distribution = [1.0 / len(self.opponent_pool)] * len(self.opponent_pool)\n        else:\n            distribution = self.distribution\n        opponent = choice(self.opponent_pool, 1, p=distribution)[0]\n        self.env = self.ks_env.train([None, opponent]) \n#         if self.iter % 2:\n#             self.env = self.ks_env.train([None, opponent])\n#         else:\n#             self.env = self.ks_env.train([opponent, None]) \n        \n    def reset(self):\n        self.iter += 1\n        self.init_env()\n        self.obs = self.env.reset()\n        self.last_action = -1\n        return board_flip(self.obs.mark, np.array(self.obs['board']).reshape(1,self.rows,self.columns)/2)\n\n    def change_reward(self, old_reward, done):\n        if old_reward == 1: # The agent won the game\n            return 1\n        elif done: # The opponent won the game\n            return -1\n        else: # Reward 1/42\n            return 1/(self.rows*self.columns)\n        \n    def step(self, action):\n        # Check if agent's move is valid\n        is_valid = (self.obs['board'][int(action)] == 0)\n        if is_valid: # Play the move\n            self.obs, old_reward, done, _ = self.env.step(int(action))\n            reward = self.change_reward(old_reward, done)\n        else: # End the game and penalize agent\n            reward, done, _ = -10, True, {}\n        return board_flip(self.obs.mark, np.array(self.obs['board']).reshape(1,self.rows,self.columns)/2), reward, done, _","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# env = ConnectFourGym()\nenv = ConnectFourGym([minimax_agent,'random'])\n# env = ConnectFourGym(['random'])\nenv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create directory for logging training information\nlog_dir = \"log/\"\nos.makedirs(log_dir, exist_ok=True)\n\n# Logging progress\nenv = Monitor(env, log_dir, allow_early_resets=True)\nenv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vec_env = DummyVecEnv([lambda: env])\nvec_env","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Net(BaseFeaturesExtractor):\n    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 512):\n        super(Net, self).__init__(observation_space, features_dim)\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n        self.fc3 = nn.Linear(384, features_dim)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = nn.Flatten()(x)\n        x = F.relu(self.fc3(x))\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"policy_kwargs = {\n    'activation_fn':th.nn.ReLU, \n    'net_arch':[64, dict(pi=[32, 16], vf=[32, 16])],\n    'features_extractor_class':Net,\n}\nlearner = PPO('MlpPolicy', vec_env, policy_kwargs=policy_kwargs)\n# learner = PPO('MlpPolicy', vec_env)\n\nlearner.policy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# It is time to learn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nlearner.learn(total_timesteps=300_000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = load_results(log_dir)['r']\ndf.rolling(window=1000).mean().plot()\ndf.tail(1000).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def testagent(obs, config):\n    import numpy as np\n    obs = np.array(obs['board']).reshape(1, config.rows, config.columns)/2\n    action, _ = learner.predict(obs)\n    return int(action)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_win_percentages(agent1, agent2, n_rounds=100):\n    # Use default Connect Four setup\n    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n    # Agent 1 goes first (roughly) half the time          \n    outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], n_rounds//2)\n    # Agent 2 goes first (roughly) half the time      \n    outcomes += [[b,a] for [a,b] in evaluate(\"connectx\", [agent2, agent1], config, [], n_rounds-n_rounds//2)]\n    print(\"Agent 1 Win Percentage:\", np.round(outcomes.count([1,-1])/len(outcomes), 2))\n    print(\"Agent 2 Win Percentage:\", np.round(outcomes.count([-1,1])/len(outcomes), 2))\n    print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0]))\n    print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0, None]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_win_percentages(agent1=testagent, agent2=testagent)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile submission.py\ndef agent(obs, config):\n    import numpy as np\n    import torch as th\n    from torch import nn as nn\n    import torch.nn.functional as F\n    from torch import tensor\n    \n    class Net(nn.Module):\n        def __init__(self):\n            super(Net, self).__init__()\n            self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n            self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n            self.fc3 = nn.Linear(384, 512)\n            self.shared1 = nn.Linear(512, 64)\n            self.policy1 = nn.Linear(64, 32)\n            self.policy2 = nn.Linear(32, 16)\n            self.action = nn.Linear(16, 7)\n\n        def forward(self, x):\n            x = F.relu(self.conv1(x))\n            x = F.relu(self.conv2(x))\n            x = nn.Flatten()(x)\n            x = F.relu(self.fc3(x))\n            x = F.relu(self.shared1(x))\n            x = F.relu(self.policy1(x))\n            x = F.relu(self.policy2(x))\n            x = self.action(x)\n            x = x.argmax()\n            return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"th.set_printoptions(profile=\"full\")\n\nagent_path = 'submission.py'\n\nstate_dict = learner.policy.to('cpu').state_dict()\nstate_dict = {\n    'conv1.weight': state_dict['features_extractor.conv1.weight'],\n    'conv1.bias': state_dict['features_extractor.conv1.bias'],\n    'conv2.weight': state_dict['features_extractor.conv2.weight'],\n    'conv2.bias': state_dict['features_extractor.conv2.bias'],\n    'fc3.weight': state_dict['features_extractor.fc3.weight'],\n    'fc3.bias': state_dict['features_extractor.fc3.bias'],\n    \n    'shared1.weight': state_dict['mlp_extractor.shared_net.0.weight'],\n    'shared1.bias': state_dict['mlp_extractor.shared_net.0.bias'],\n    \n    'policy1.weight': state_dict['mlp_extractor.policy_net.0.weight'],\n    'policy1.bias': state_dict['mlp_extractor.policy_net.0.bias'],\n    'policy2.weight': state_dict['mlp_extractor.policy_net.2.weight'],\n    'policy2.bias': state_dict['mlp_extractor.policy_net.2.bias'],\n    \n    'action.weight': state_dict['action_net.weight'],\n    'action.bias': state_dict['action_net.bias'],\n}\n\nwith open(agent_path, mode='a') as file:\n    #file.write(f'\\n    data = {learner.policy._get_data()}\\n')\n    file.write(f'    state_dict = {state_dict}\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile -a submission.py\n\n    model = Net()\n    model = model.float()\n    model.load_state_dict(state_dict)\n    model = model.to('cpu')\n    model = model.eval()\n    obs = tensor(obs['board']).reshape(1, 1, config.rows, config.columns).float()\n    obs = obs / 2\n    action = model(obs)\n    return int(action)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load submission.py\nf = open(agent_path)\nsource = f.read()\nexec(source)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# agent(env.reset()[0]['observation'], env.configuration)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_win_percentages(agent1=agent, agent2=\"random\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env = make(\"connectx\", debug=True)\n\n# Two random agents play one game round\nenv.run([agent, \"random\"])\n\n# Show the game\nenv.render(mode=\"ipython\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}