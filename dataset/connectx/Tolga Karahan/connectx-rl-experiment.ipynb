{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install 'kaggle-environments>=0.1.6'\n\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.game_ai.ex4 import *\n\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n!pip install 'tensorflow==1.14.0'\n\nimport tensorflow as tf\nfrom kaggle_environments import make, evaluate\nfrom gym import spaces\n\n!apt-get update\n!apt-get install -y cmake libopenmpi-dev python3-dev zlib1g-dev\n!pip install \"stable-baselines[mpi]==2.9.0\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from stable_baselines.bench import Monitor \nfrom stable_baselines.common.vec_env import DummyVecEnv\nfrom stable_baselines import PPO1, A2C, ACER, ACKTR, TRPO\nfrom stable_baselines.a2c.utils import conv, linear, conv_to_fc\nfrom stable_baselines.common.policies import CnnPolicy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ConnectFourGym:\n    def __init__(self, agent2=\"random\"):\n        ks_env = make(\"connectx\", debug=True)\n        self.env = ks_env.train([None, agent2])\n        self.rows = ks_env.configuration.rows\n        self.columns = ks_env.configuration.columns\n        # Learn about spaces here: http://gym.openai.com/docs/#spaces\n        self.action_space = spaces.Discrete(self.columns)\n        self.observation_space = spaces.Box(low=0, high=2, \n                                            shape=(self.rows,self.columns,1), dtype=np.int)\n        # Tuple corresponding to the min and max possible rewards\n        self.reward_range = (-10, 1)\n        # StableBaselines throws error if these are not defined\n        self.spec = None\n        self.metadata = None\n    def reset(self):\n        self.obs = self.env.reset()\n        return np.array(self.obs['board']).reshape(self.rows,self.columns,1)\n    def change_reward(self, old_reward, done):\n        if old_reward == 1: # The agent won the game\n            return 1\n        elif done: # The opponent won the game\n            return -1\n        else: # Reward 1/42\n            return 1/(self.rows*self.columns)\n    def step(self, action):\n        # Check if agent's move is valid\n        is_valid = (self.obs['board'][int(action)] == 0)\n        if is_valid: # Play the move\n            self.obs, old_reward, done, _ = self.env.step(int(action))\n            reward = self.change_reward(old_reward, done)\n        else: # End the game and penalize agent\n            reward, done, _ = -10, True, {}\n        return np.array(self.obs['board']).reshape(self.rows,self.columns,1), reward, done, _","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create ConnectFour environment\nenv = ConnectFourGym(agent2=\"negamax\")\n\n# Create directory for logging training information\nlog_dir = \"log/\"\nos.makedirs(log_dir, exist_ok=True)\n\n# Logging progress\nmonitor_env = Monitor(env, log_dir, allow_early_resets=True)\n\n# Create a vectorized environment\nvec_env = DummyVecEnv([lambda: monitor_env])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Neural network for predicting action values\ndef modified_cnn(scaled_images, **kwargs):\n    activ = tf.nn.relu\n    layer_1 = activ(conv(scaled_images, 'c1', n_filters=32, filter_size=3, stride=1, \n                         init_scale=np.sqrt(2), **kwargs))\n    layer_2 = activ(conv(layer_1, 'c2', n_filters=64, filter_size=3, stride=1, \n                         init_scale=np.sqrt(2), **kwargs))\n    layer_2 = conv_to_fc(layer_2)\n    return activ(linear(layer_2, 'fc1', n_hidden=512, init_scale=np.sqrt(2)))  \n\nclass CustomCnnPolicy(CnnPolicy):\n    def __init__(self, *args, **kwargs):\n        super(CustomCnnPolicy, self).__init__(*args, **kwargs, cnn_extractor=modified_cnn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize agent\nmodel = PPO1(CustomCnnPolicy, vec_env, verbose=0, n_cpu_tf_sess=None)\n\n# Train agent\nmodel.learn(total_timesteps=100000)\n\n# Plot cumulative reward\nwith open(os.path.join(log_dir, \"monitor.csv\"), 'rt') as fh:    \n    firstline = fh.readline()\n    assert firstline[0] == '#'\n    df = pd.read_csv(fh, index_col=None)['r']\ndf.rolling(window=1000).mean().plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch as th\nimport torch.nn as nn\n\nbaselines_cnn_model = model\nclass PyTorchCnnPolicy(nn.Module):\n    def __init__(self):\n        super(PyTorchCnnPolicy, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=0, bias=True)\n        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=0, bias=True)          \n        self.fc1 = nn.Linear(384, 512)\n        self.fc2 = nn.Linear(512, 7)\n        self.relu = nn.ReLU()\n        self.out_activ = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x = self.relu(self.conv1(x))\n        x = self.relu(self.conv2(x))\n        x = x.permute(0, 2, 3, 1).contiguous()\n        x = x.view(x.size(0), -1)\n        x = self.relu(self.fc1(x))\n        x = self.fc2(x)\n        x = self.out_activ(x)                  \n        return x\n\ndef copy_cnn_weights(baselines_model):\n    torch_cnn = PyTorchCnnPolicy()\n    model_params = baselines_model.get_parameters()\n    # Get only the policy parameters\n    policy_keys = [key for key in model_params.keys() if \"pi\" in key or \"c\" in key]\n    policy_params = [model_params[key] for key in policy_keys]\n    \n    for (th_key, pytorch_param), key, policy_param in zip(torch_cnn.named_parameters(), policy_keys, policy_params):\n        param = policy_param.copy()\n        # Copy parameters from stable baselines model to pytorch model\n\n        # Conv layer\n        if len(param.shape) == 4:  \n          # https://gist.github.com/chirag1992m/4c1f2cb27d7c138a4dc76aeddfe940c2\n          # Tensorflow 2D Convolutional layer: height * width * input channels * output channels\n          # PyTorch 2D Convolutional layer: output channels * input channels * height * width\n          param = np.transpose(param, (3, 2, 0, 1))\n        \n        # weight of fully connected layer\n        if len(param.shape) == 2:\n            param = param.T\n        \n        # bias\n        if 'b' in key:\n            param = param.squeeze()\n\n        param = th.from_numpy(param)\n        pytorch_param.data.copy_(param.data.clone())\n        \n    return torch_cnn\n\nth_model = copy_cnn_weights(baselines_cnn_model)\nth.save(th_model.state_dict(), 'thmodel')\n\nimport base64\nwith open('thmodel', 'rb') as f:\n    raw_bytes = f.read()\n    encoded_weights = base64.encodebytes(raw_bytes)\n\nimport io\nimport torch\nfrom torch.autograd import Variable\nimport random\n\nagent_th_model = PyTorchCnnPolicy()\ndecoded = base64.b64decode(encoded_weights)\nbuffer = io.BytesIO(decoded)\nagent_th_model.load_state_dict(torch.load(buffer))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import inspect\nimport os\n\ndef my_agent(observation, configuration):\n    obs = np.array(observation['board'])\n    th_obs = Variable(th.from_numpy(np.array(observation['board']).reshape(1,1,6,7))).float()\n    y = agent_th_model(th_obs)\n    action = th.argmax(agent_th_model(th_obs)).item()\n    if observation.board[action] == 0:\n        return action\n    else:\n        return random.choice([c for c in range(configuration.columns) if observation.board[c] == 0])\n\ndef write_agent_to_file(file):\n#     with open(file, \"a\" if os.path.exists(file) else \"w\") as f:\n    with open(file, \"w\") as f:\n        f.write('import numpy as np\\n')\n        f.write('import random\\n')\n        f.write('import torch as th\\n')\n        f.write('import torch.nn as nn\\n')\n        f.write('import io\\n')\n        f.write('import base64\\n')\n        f.write('from torch.autograd import Variable\\n')\n\n        f.write('class PyTorchCnnPolicy(nn.Module):\\n')\n        f.write('    def __init__(self):\\n')\n        f.write('        super(PyTorchCnnPolicy, self).__init__()\\n')\n        f.write('        self.conv1= nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=0, bias=True)\\n')\n        f.write('        self.conv2= nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=0, bias=True)\\n')\n        f.write('        self.fc1 = nn.Linear(384, 512)\\n') \n        f.write('        self.fc2 = nn.Linear(512, 7)\\n') \n        f.write('        self.relu = nn.ReLU()\\n') \n        f.write('        self.out_activ = nn.Softmax(dim=1)\\n')\n        f.write('    def forward(self, x):\\n')\n        f.write('        x = self.relu(self.conv1(x))\\n')\n        f.write('        x = self.relu(self.conv2(x))\\n')\n        f.write('        x = x.permute(0, 2, 3, 1).contiguous()\\n')\n        f.write('        x = x.view(x.size(0), -1)\\n')\n        f.write('        x = self.relu(self.fc1(x))\\n')\n        f.write('        x = self.fc2(x)\\n')\n        f.write('        x = self.out_activ(x)\\n')\n        f.write('        return x\\n')\n\n        f.write('agent_th_model = PyTorchCnnPolicy()\\n')\n        f.write('encoded_weights =' + str(encoded_weights) + '\\n')\n        f.write('decoded = base64.b64decode(encoded_weights)\\n')\n        f.write('buffer = io.BytesIO(decoded)\\n')\n        f.write('agent_th_model.load_state_dict(th.load(buffer))\\n')\n        \n        f.write(inspect.getsource(my_agent))\n        \nwrite_agent_to_file(\"submission.py\")        ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}