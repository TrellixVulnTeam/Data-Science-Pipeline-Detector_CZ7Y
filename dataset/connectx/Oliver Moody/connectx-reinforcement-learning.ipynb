{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Install kaggle-environments"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# 1. Enable Internet in the Kernel (Settings side pane)\n\n# 2. Curl cache may need purged if v0.1.6 cannot be found (uncomment if needed). \n#!curl -X PURGE https://pypi.org/simple/kaggle-environments\n\n# ConnectX environment was defined in v0.1.6\n!pip install 'kaggle-environments>=0.1.6'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create ConnectX Environment"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from kaggle_environments import evaluate, make, utils\nfrom tqdm.notebook import tqdm\nimport numpy as np\nimport random\nimport collections\nimport json\nimport os\nenv = make(\"connectx\", debug=True)\nenv.render()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Create Q-table**\nload if applicable"},{"metadata":{"trusted":true},"cell_type":"code","source":"# file = \"../input/q-dict-2000/q_dict (4).json\"\n# with open(file, \"r\") as f:\n#     q_dict = json.load(f) \n#     #print(q_dict)\nq_dict = {} \nfile = \"q_dict.json\" \nwith open(file, \"w\") as f: \n    json.dump(q_dict,f)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Getwinmove Helper Function**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(env.configuration)\ndef getwinmove(turn, board, con):\n    cols = con.columns\n    towin = con.inarow\n    rows = con.rows\n    colstreaks = [0]*cols\n    collastgap = [-1]*cols\n    for row in range(rows):\n        streakpregap = 0\n        streak = 0\n        lastgap = -1\n        for pos in range(cols*row,cols*row+cols):\n            #print (pos)\n            if board[pos] == turn:\n                streak+=1\n                colstreaks[pos%cols]+=1\n            elif board[pos] == 0:\n                lastgap = pos\n                collastgap[pos%cols] = pos\n                streakpregap = streak\n                streak = 0\n                colstreaks[pos%cols] = 0\n            else:\n                streak = 0\n                streakpregap = 0\n                lastgap = -1\n                colstreaks[pos%cols] = 0\n                collastgap[pos%cols] = -1\n            if lastgap != -1:\n                if streak + streakpregap >= towin-1:\n                    #print(pos%cols,\"row\",row,board[pos])\n                    if len(board)<= lastgap+cols or board[lastgap+cols]!=0:\n                        return lastgap%cols\n            if collastgap[pos%cols] !=-1:\n                if colstreaks[pos%cols]>=towin-1:\n                    #print(pos%cols,\"col\",board[collastgap[pos%cols]+cols])\n                    if len(board)< collastgap[pos%cols]+cols or board[collastgap[pos%cols]+cols]!=0:\n                        return pos%cols \n    return -1 \n#print(getwinmove(1,[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0],env.configuration))\n                 \n        \n            \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ngames = 10000\nalpha = .1\nalpha_decrease = .9999\nepsilon = 0.1\nepsilon_decrease = .9999\ngamma = 0.5\nk = 1\ndef state_to_str(state):\n    #print(state,state.mark,state[\"board\"])\n    return str(state[\"board\"])\ndef getbest(lis,board):\n    #f(u, n) = u + k/n\n    best = -10000\n    ops = []\n    for i in range(0,env.configuration.columns*2,2):\n        if board[i//2] == 0:\n            u = lis[i]\n            n = lis[i+1]\n            val = u+k/n\n            if val > best:\n                best = val\n                ops = [i]\n            elif val == best:\n                ops.append(i)\n    if ops:\n        return random.choice(ops)\n    return 0\nfile = \"q_dict.json\"\nwith open(file) as f:\n    q_dict = json.load(f)\n#print (q_dict)\n#for d in tqdm(range(1)):\nif True:\n    tot = 0\n    #q_dict = dict()\n    for i in tqdm(range(games)):\n        trainer =  env.train([None, \"negamax\"])\n        first = True\n        if random.random() < .5:\n            trainer = env.train([\"negamax\", None])\n            frist = False\n        state = trainer.reset()\n        gameOver = False\n        while not gameOver:\n            boardstr = state_to_str(state)\n            if boardstr not in q_dict:\n                q_dict[boardstr]=[0,1]*env.configuration.columns\n            winfirst = getwinmove(1,state[\"board\"],env.configuration)\n            winsecond = getwinmove(2,state[\"board\"],env.configuration)\n            if winfirst + winsecond > -2:\n                if first:\n                    if winfirst !=-1:\n                        act= winfirst\n                    else:\n                        act = winsecond\n                else:\n                    if winsecond != -1:\n                        act= winsecond\n                    else:\n                        act =  winfirst\n                act*=2\n            elif random.random() < epsilon:\n                act = random.choice([c for c in range(env.configuration.columns) if state[\"board\"][c] == 0])\n                act*=2\n            else:\n                ops = []\n                best = -10000\n                 \n                act = getbest(q_dict[boardstr],state[\"board\"])\n\n            next_state, reward, gameOver, other = trainer.step(act//2)\n            #print(reward)\n            \n            if gameOver:\n                if reward !=-1:\n                    tot+=reward\n                    #print(tot)\n                reward*=25\n                reward+=10\n\n            else:\n                reward = -.1\n            state = next_state\n            nextboardstr = state_to_str(next_state)\n            if nextboardstr not in q_dict:\n                q_dict[nextboardstr]=[0,1]*env.configuration.columns\n            #Q(s,a) ← Q(s,a) +alpha[R(s,a,s') +  gamma*maxa'f(Q(s',a'),N(s',a')) -Q(s,a)]\n            old_q = q_dict[boardstr][act]\n            next_q = q_dict[nextboardstr][getbest(q_dict[nextboardstr],next_state[\"board\"])]\n            new_q = old_q + alpha*(reward+gamma*next_q - old_q)\n            q_dict[boardstr][act+1]+=1\n            q_dict[boardstr][act]=new_q\n            alpha*=alpha_decrease\n            epsilon*=epsilon_decrease\n\n\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file = \"q_dict.json\"\nprint()\nwith open(file, \"w\") as f:\n    json.dump(q_dict,f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_q_dict = dict()\nfor k in q_dict:\n    #print(q_dict[k])\n    best =  -10000\n    ops = []\n    ind = 0\n    for i in range(0,env.configuration.columns*2,2):\n        val = q_dict[k][i]\n        if val > best:\n                best = val\n                ops = [i]\n        elif val == best:\n                ops.append(i)\n    if ops:\n        ind = random.choice(ops)\n    new_q_dict[k]=ind//2\nq_dict_str = str(new_q_dict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create an Agent\n\nTo create the submission, an agent function should be fully encapsulated (no external dependencies).  \n\nWhen your agent is being evaluated against others, it will not have access to the Kaggle docker image.  Only the following can be imported: Python Standard Library Modules, gym, numpy, scipy, pytorch (1.3.1, cpu only), and more may be added later.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nagent=\"\"\"\n\nimport random\nnew_q_dict =\"\"\"+q_dict_str+\"\"\"\n\ndef my_agent(observation, configuration):   \n    def getwinmove(turn, board, con):\n        cols = con.columns\n        towin = con.inarow\n        rows = con.rows\n        colstreaks = [0]*cols\n        collastgap = [-1]*cols\n        for row in range(rows):\n            streakpregap = 0\n            streak = 0\n            lastgap = -1\n            for pos in range(cols*row,cols*row+cols):\n                #print (pos)\n                if board[pos] == turn:\n                    streak+=1\n                    colstreaks[pos%cols]+=1\n                elif board[pos] == 0:\n                    lastgap = pos\n                    collastgap[pos%cols] = pos\n                    streakpregap = streak\n                    streak = 0\n                    colstreaks[pos%cols] = 0\n                else:\n                    streak = 0\n                    streakpregap = 0\n                    lastgap = -1\n                    colstreaks[pos%cols] = 0\n                    collastgap[pos%cols] = -1\n                if lastgap != -1:\n                    if streak + streakpregap >= towin-1:\n                        print(pos%cols,\"row\",row,board[pos])\n                        if len(board)<= lastgap+cols or board[lastgap+cols]!=0:\n                            return lastgap%cols\n                if collastgap[pos%cols] !=-1:\n                    if colstreaks[pos%cols]>=towin-1:\n                        print(pos%cols,\"col\")\n                        if len(board)< collastgap[pos%cols]+cols or board[collastgap[pos%cols]+cols]!=0:\n                            return pos%cols\n        return -1\n    board = observation.board\n    boardkey = str(board)\n    #print(observation)\n    turnnum = observation.mark\n    otherturnnum = (not (observation.mark -1)) +1\n    h = getwinmove(turnnum, observation.board, configuration)\n    print(h)\n    if h != -1:\n        return h\n    h = getwinmove(otherturnnum, observation.board, configuration)\n    print(h)\n    if h != -1:\n        return h\n    if boardkey not in new_q_dict:\n        return random.choice([c for c in range(configuration.columns) if observation.board[c] == 0])\n    else:\n        action = new_q_dict[boardkey]\n        #print(action)\n    if observation.board[action] != 0:\n        return random.choice([c for c in range(configuration.columns) if observation.board[c] == 0])\n    return action\n\"\"\"\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Write Agent to File"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nwith open('submission.py', 'w') as f:\n    f.write(agent)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from submission import my_agent\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test Agent"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Play as first position against random agent.\ntrainer = env.train([None, \"random\"])\n\nobservation = trainer.reset()\nprint(observation)\nwhile not env.done:\n    my_action = my_agent(observation, env.configuration)\n    print(\"My Action\", my_action)\n    #print(trainer)\n    observation, reward, done, info = trainer.step(my_action)\n    print(my_agent(observation, env.configuration))\n    #print(trainer.step(my_action))\n    env.render(mode=\"ipython\", width=100, height=90, header=False, controls=False)\nenv.render()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluate Agent"},{"metadata":{"trusted":true},"cell_type":"code","source":"def mean_reward(rewards):\n    return sum(r[0] for r in rewards) / float(len(rewards))\n\n# Run multiple episodes to estimate its performance.\nprint(\"My Agent vs Random Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"random\"], num_episodes=10)))\nprint(\"My Agent vs Negamax Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=10)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Validate Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Note: Stdout replacement is a temporary workaround.\nimport sys\nout = sys.stdout\nsubmission = utils.read_file(\"/kaggle/working/submission7.py\")\nagent = my_agent\nsys.stdout = out\n\nenv = make(\"connectx\", debug=True)\nenv.run([agent, agent])\nprint(\"Success!\" if env.state[0].status == env.state[1].status == \"DONE\" else \"Failed...\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}