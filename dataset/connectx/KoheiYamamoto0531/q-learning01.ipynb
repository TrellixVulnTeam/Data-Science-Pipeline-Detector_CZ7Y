{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install 'kaggle-environments>=0.1.6'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-20T22:26:47.652125Z","iopub.execute_input":"2021-07-20T22:26:47.652515Z","iopub.status.idle":"2021-07-20T22:27:14.064607Z","shell.execute_reply.started":"2021-07-20T22:26:47.652463Z","shell.execute_reply":"2021-07-20T22:27:14.063578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_environments import make, utils\n\nenv = make(\"connectx\", debug=True)\nenv.render()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T22:27:14.069106Z","iopub.execute_input":"2021-07-20T22:27:14.069457Z","iopub.status.idle":"2021-07-20T22:27:14.085276Z","shell.execute_reply.started":"2021-07-20T22:27:14.069424Z","shell.execute_reply":"2021-07-20T22:27:14.083919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom random import choice\nfrom tqdm import tqdm\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-07-20T22:27:14.087275Z","iopub.execute_input":"2021-07-20T22:27:14.087587Z","iopub.status.idle":"2021-07-20T22:27:14.100869Z","shell.execute_reply.started":"2021-07-20T22:27:14.087557Z","shell.execute_reply":"2021-07-20T22:27:14.100042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Qテーブル\n\nclass QTable():\n    def __init__(self, actions):\n        self.Q = {}\n        self.actions = actions\n    \n    def get_state_key(self, state):\n        board = state.board[:]\n        board.append(state.mark)\n        state_key = np.array(board).astype(str) \n        return hex(int(''.join(state_key), 3))[2:]        \n        \n    def get_q_values(self, state):\n        state_key = self.get_state_key(state)\n        if state_key not in self.Q.keys(): \n            self.Q[state_key] = [0] * len(self.actions)\n        return self.Q[state_key]\n    \n    def update(self, state, action, add_q):\n        state_key = self.get_state_key(state)\n        self.Q[state_key] = [q + add_q if idx == action else q for idx, q in enumerate(self.Q[state_key])]\n        \n        \n#エージェント\n        \nenv = make(\"connectx\", debug=True)\ntrainer = env.train([None, \"random\"])\n\nclass QLearningAgent():\n    def __init__(self, env, epsilon=0.4):\n        self.env = env\n        self.actions = list(range(self.env.configuration.columns))\n        self.q_table = QTable(self.actions)\n        self.epsilon = epsilon\n        self.reward_log = []  \n        \n    def policy(self, state):\n        if np.random.random() < self.epsilon:\n            return choice([c for c in range(len(self.actions)) if state.board[c] == 0])\n        else:\n            q_values = self.q_table.get_q_values(state)\n            selected_items = [q if state.board[idx] == 0 else -1e7 for idx, q in enumerate(q_values)]\n            return int(np.argmax(selected_items))\n        \n    def custom_reward(self, reward, done):\n        if done:\n            if reward == 1:\n                return 20\n            elif reward == 0:\n                return -30\n            else:\n                return 10\n        else:\n            return -10\n        \n    def learn(self, trainer, episode_cnt=50000, gamma=0.6, \n              learn_rate=0.3):\n        for episode in tqdm(range(episode_cnt)):\n            state = trainer.reset() \n            self.epsilon = 0.4\n            while not env.done:\n                action = self.policy(state) \n                next_state, reward, done, info = trainer.step(action)\n                reward = self.custom_reward(reward, done)\n                gain = reward + gamma * max(self.q_table.get_q_values(next_state))\n                estimate = self.q_table.get_q_values(state)[action]\n                self.q_table.update(state, action, learn_rate * (gain - estimate)) \n                state = next_state\n      \n            self.reward_log.append(reward)\n        \nqa = QLearningAgent(env)\nqa.learn(trainer)\n\n\n#平均報酬\n\nsns.set(style='darkgrid')\npd.DataFrame({'Average Reward': qa.reward_log}).rolling(500).mean().plot(figsize=(10,5))\nplt.show()\n\n\n#Pythonファイル出力\n\ntmp_dict_q_table = qa.q_table.Q.copy()\ndict_q_table = dict()\n\nfor k in tmp_dict_q_table:\n    if np.count_nonzero(tmp_dict_q_table[k]) > 0:\n        dict_q_table[k] = int(np.argmax(tmp_dict_q_table[k]))\n\nmy_agent = '''def my_agent(observation, configuration):\n    from random import choice\n    q_table = ''' \\\n    + str(dict_q_table).replace(' ', '') \\\n    + '''\n    board = observation.board[:]\n    board.append(observation.mark)\n    state_key = list(map(str, board))\n    state_key = hex(int(''.join(state_key), 3))[2:]\n    if state_key not in q_table.keys():\n        return choice([c for c in range(configuration.columns) if observation.board[c] == 0])\n    action = q_table[state_key]\n    if observation.board[action] != 0:\n        return choice([c for c in range(configuration.columns) if observation.board[c] == 0])\n    return action\n    '''\n\nwith open('submission.py', 'w') as f:\n    f.write(my_agent)\n    \nenv.reset()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T22:27:14.102231Z","iopub.execute_input":"2021-07-20T22:27:14.102521Z","iopub.status.idle":"2021-07-20T22:57:05.681153Z","shell.execute_reply.started":"2021-07-20T22:27:14.102494Z","shell.execute_reply":"2021-07-20T22:57:05.680018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#対戦\n\nenv.run([my_agent, \"random\"])\nenv.render(mode=\"ipython\", width=500, height=450)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T22:57:05.68241Z","iopub.execute_input":"2021-07-20T22:57:05.682688Z","iopub.status.idle":"2021-07-20T22:57:31.76091Z","shell.execute_reply.started":"2021-07-20T22:57:05.682661Z","shell.execute_reply":"2021-07-20T22:57:31.75774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#vs randomとvs negamaxとの勝率\n\nfrom kaggle_environments import evaluate\n\ndef mean_reward(rewards):\n    return sum(r[0] for r in rewards) / float(len(rewards))\n\nprint(\"My Agent vs Random Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"random\"], num_episodes=10)))\nprint(\"My Agent vs Negamax Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=10)))","metadata":{"execution":{"iopub.status.busy":"2021-07-20T22:57:31.762224Z","iopub.execute_input":"2021-07-20T22:57:31.762535Z","iopub.status.idle":"2021-07-20T22:58:15.413216Z","shell.execute_reply.started":"2021-07-20T22:57:31.762502Z","shell.execute_reply":"2021-07-20T22:58:15.412163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}