{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# A More Useful Negamax Opponent\n## [This notebook became obsolete as a result of the negamax changes made on Mar 9, 2020]\n## Introduction\n\nLet's say you've built multiple agents for the ConnectX competition.  Is there a relatively simple way to determine which one is best?\n\nYou could always submit them all to the competition and see which one climbs the leaderboard.  Or you could write your own tournament software and let them play one another.\n\nBut to me the most straight-forward way would be to have them all play against a common opponent and see which one is the best.","metadata":{}},{"cell_type":"markdown","source":"## Grading Agents\n\nI'm going to give each agent a grade when playing against a given opponent.  The grade_agent function below is similar to others that have been posted.  Rather than returning the number of wins, loses and draws, I'm going to return the percentage of rewards won.  (This is what we want to know in zero-sum games like ConnectX).  This will evaluate the agent by playing an equal number of games as both player 1 and player 2.","metadata":{}},{"cell_type":"code","source":"!pip install 'kaggle-environments'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_environments import evaluate\nfrom operator import itemgetter\n\ndef grade_agent(game_name, agent, opponent, episodes):\n    as_p1 = evaluate(game_name, [agent, opponent], num_episodes=episodes)\n    as_p1_reward = sum(map(itemgetter(0), as_p1))\n    as_p1_total = sum(map(itemgetter(1), as_p1)) + as_p1_reward\n    \n    as_p2 = evaluate(game_name, [opponent, agent], num_episodes=episodes)\n    as_p2_reward = sum(map(itemgetter(1), as_p2))\n    as_p2_total = sum(map(itemgetter(0), as_p2)) + as_p2_reward\n    \n    return 100 * (as_p1_reward + as_p2_reward) / (as_p1_total + as_p2_total)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Versus Random\n\nI built two agents - agent_A and agent_B.  The screenshot below shows how they did versus the common opponent random.\n","metadata":{}},{"cell_type":"markdown","source":"```python\na_vs_random = grade_agent(\"connectx\", agent_A, \"random\", 50)\nb_vs_random = grade_agent(\"connectx\", agent_B, \"random\", 50)\n\nprint(\"Agent A v Random\", a_vs_random)\nprint(\"Agent B v Random\", b_vs_random)\n```\n\n    Agent A v Random 100.0\n    Agent B v Random 100.0\n\n","metadata":{}},{"cell_type":"markdown","source":"#### Issues\n\nThey both scored 100.  That's good.  But it is not at all helpful in determining which agent is better.  The problem is simply that the random agent is not good.","metadata":{}},{"cell_type":"markdown","source":"### Versus Negamax\n\nThe negamax agent is better than random.  Let's see what happens when my agents play against negamax.","metadata":{}},{"cell_type":"markdown","source":"```python\na_vs_negamax = grade_agent(\"connectx\", agent_A, \"negamax\", 50)\nb_vs_negamax = grade_agent(\"connectx\", agent_B, \"negamax\", 50)\n\nprint(\"Agent A v Negamax\", a_vs_negamax)\nprint(\"Agent B v Negamax\", b_vs_negamax)\n```\n\n    Agent A v Negamax 100.0\n    Agent B v Negamax 100.0\n\n","metadata":{}},{"cell_type":"markdown","source":"#### Issues\n\nThis looks depressingly similar.  We have not recieved any information that let's us distingish between the play of agent_A and agent_B.\n\nBut the situation is even worse than that.  Negamax is a deterministic agent.  By that I mean that negamax will always play the same moves every game against an opponent unless that opponent plays different moves.  My agents A and B are both deterministic too.  This is a problem.\n\nFor agent_A we played 100 games vs negamax (50 as player 1 and 50 as player 2).  But playing multiple games did not give us any additional information, because **all of the games looked the same**.  My agent was always playing the same moves and negamax was always playing the same moves.  We could have gotten the same information by playing 2 games instead (one as player 1 and one as player 2).  This paragraph is obviously also true for agent_B.\n\nSo really getting a score from grade_agent() with a deterministic agent versus negamax (or any other deterministic agent) can only give you 5 values.\n\n- 100.0 - you win as both player 1 and player 2\n- 75.0 - you win as player 1 and tie as player 2 (or vice-versa)\n- 50.0 - you win as player 1, but lose as player 2 (or vice-versa), or you tie twice\n- 0.25 - you lose as player 1, but tie as player 2 (or vice-versa)\n- 0.0 - you lose as both player 1 and player 2\n\nIt looks like playing against negamax can't tell you much either.\n","metadata":{}},{"cell_type":"markdown","source":"## Combining Agents\n\nIt would be nice if negamax didn't play the same game every time.  If it varied we could play negamax multiple times and get different results.  But that is not what negamax is written to do.\n\nOne solution would be if we had a combined agent that played like negamax most of the time, but occasionally played random moves so that our agent would encounter different board positions.","metadata":{}},{"cell_type":"code","source":"import random\n\ndef combined_agent(default_agent, alternate_agent, epsilon):\n    def updated_agent(obs, config):\n        if (random.random() < epsilon):\n            return alternate_agent(obs, config)\n        return default_agent(obs,config)\n    return updated_agent\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### $\\epsilon$-Greedy Negamax\n\nLet's make a new e_greedy_negamax that plays like negamax most of the time, but occasionally makes random moves.  The e_greedy_negamax will **not** be better than negamax, because it will occasionally move randomly.  It will be a **more useful** opponent, because it will play relatively well and it will occasionally present my agents with different board positions.\n","metadata":{}},{"cell_type":"code","source":"from kaggle_environments.envs.connectx.connectx import negamax_agent\nfrom kaggle_environments.envs.connectx.connectx import random_agent\n\ne_greedy_negamax = combined_agent(negamax_agent, random_agent, 0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I don't know if 0.2 sounds too high for you or not.  The longest possible game is 21 moves by each player.  An epsilon of 0.2 means that e_greedy_negamax will play a full game with no random moves about 1% of the time.  If the game ends after only 10 moves by each player than e_greedy_negamax will play with no random moves about 11% of the time.  It sounds reasonable to me.\n\nLet's try this out.","metadata":{}},{"cell_type":"markdown","source":"```python\na_vs_eg_negamax = grade_agent(\"connectx\", agent_A, e_greedy_negamax, 50)\nb_vs_eg_negamax = grade_agent(\"connectx\", agent_B, e_greedy_negamax, 50)\n\nprint(\"Agent A v e-greedy Negamax\", a_vs_eg_negamax)\nprint(\"Agent B v e-greedy Negamax\", b_vs_eg_negamax)\n```\n\n    Agent A v e-greedy Negamax 76.0\n    Agent B v e-greedy Negamax 90.0\n\n","metadata":{}},{"cell_type":"markdown","source":"Playing an $\\epsilon$-greedy negamax has given us some useful information.  Agent B appears to be much better than agent A.","metadata":{}}]}