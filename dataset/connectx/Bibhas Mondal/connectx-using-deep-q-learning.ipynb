{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nfrom torch import nn\nfrom time import time\nfrom torch import optim\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nfrom kaggle_environments import evaluate, make","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ConnectX:\n    def __init__(self, switch_prob=0.5):\n        self.env = make('connectx', debug=False)\n        self.pair = [None, 'negamax']\n        self.trainer = self.env.train(self.pair)\n        self.switch_prob = switch_prob\n        self.config = self.env.configuration\n        self.action_space = self.config.columns\n        self.state_space = self.config.columns * self.config.rows\n        self.rule = {True: {1: 20.0, 0: -20.0, None: 10.0}, False: {0.5: 5.0}}\n\n    def switch_trainer(self):\n        self.pair[1] = np.random.choice(['random','negamax'])\n        self.trainer = self.env.train(self.pair)\n\n    def step(self, action):\n        observations, reward, done, _ = self.trainer.step(action)\n        reward = self.rule[done][reward]\n        return observations.board, reward, done\n\n    def reset(self):\n        if np.random.random() < self.switch_prob:\n            self.switch_trainer()\n        observations = self.trainer.reset()\n        return observations.board\n\n    def render(self, **kwargs):\n        return self.env.render(**kwargs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Experience:\n    class Memory:\n        def __init__(self, curr_state, action, reward, done, next_state):\n            self.curr_state = curr_state\n            self.action = action\n            self.reward = reward\n            self.done = done\n            self.next_state = next_state\n\n    def __init__(self, memory_size):\n        self.memory_size = memory_size\n        self.memories = []\n\n    def choice(self, size):\n        return np.random.choice(self.memories, min(len(self.memories), size))\n\n    def update(self, curr_state, action, reward, done, next_state):\n        # if memory is full, remove the oldest transition\n        self.memories = self.memories[1:] if len(self.memories) >= self.memory_size else self.memories\n        self.memories.append(self.Memory(curr_state, action, reward, done, next_state))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DQN(nn.Module):\n    def __init__(self, num_states, num_actions, hidden_units = 512):\n        super(DQN, self).__init__()\n        self.num_states = num_states\n        self.hidden_units = hidden_units\n        self.num_actions = num_actions\n        self.experience = Experience(10000)\n        \n        self.layer1 = nn.Sequential(\n            nn.Linear(num_states, hidden_units),\n            nn.LayerNorm(hidden_units),\n            nn.ReLU())\n        \n        self.layer2 = nn.Sequential(\n            nn.Linear(hidden_units, num_actions),\n            nn.Sigmoid())\n\n    def forward(self, states):\n        states = states.view(-1, self.num_states)\n        output = self.layer1(states)\n        actions = self.layer2(output)\n        return actions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Trainer:\n    def __init__(self, num_episodes, batch_size = 512):\n        self.num_episodes = num_episodes\n        self.gamma = 0.99\n        self.final_epsilon = 0.0001\n        self.init_epsilon = 0.1\n        self.epsilon_decay = (self.init_epsilon - self.final_epsilon) / (self.num_episodes - 1)\n        self.batch_size = batch_size\n        self.device = torch.device(type = 'cuda' if torch.cuda.is_available() else 'cpu')\n        self.env = ConnectX()\n        self.model = DQN(self.env.state_space, self.env.action_space).to(self.device)\n        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-6)\n        self.criterion = nn.MSELoss()\n        self.rewards = np.array([])\n\n    def epsilon_decrements(self, episode_no):\n        epsilon = self.init_epsilon - self.epsilon_decay * episode_no\n        return epsilon\n\n    def train(self):\n        # sample random minibatch\n        experiences = self.model.experience.choice(self.batch_size)\n        # unpack minibatch\n        curr_state_batch = torch.tensor([experience.curr_state for experience in experiences], device = self.device, dtype = torch.float)\n        action_batch = torch.tensor([experience.action for experience in experiences], device = self.device)\n        reward_batch = torch.tensor([experience.reward for experience in experiences], device = self.device)\n        done_batch = torch.tensor([experience.done for experience in experiences], device = self.device)\n        next_state_batch = torch.tensor([experience.next_state for experience in experiences], device = self.device, dtype = torch.float)\n        # set y_j to r_j for terminal state, otherwise to r_j + gamma*max(Q)\n        y_batch = torch.where(done_batch, reward_batch, self.gamma * self.model(next_state_batch).argmax(1))\n        # extract Q-value\n        q_value = self.model(curr_state_batch)[:,action_batch].sum(dim = 1)\n        return q_value, y_batch\n    \n    def play(self, episode_no):\n        curr_state = self.env.reset()\n        done = False\n        total_reward = 0\n        while not done:\n            # epsilon annealing\n            epsilon = self.epsilon_decrements(episode_no)\n            # epsilon greedy exploration\n            random_action = np.random.uniform() <= epsilon\n            # get output from the neural network or random\n            if random_action:\n                action = torch.randint(self.model.num_actions, torch.Size((1,)))  \n            else:\n                action = self.model(torch.tensor(curr_state, device = self.device, dtype = torch.float)).argmax(1)\n            # get next state and reward\n            next_state, reward, done = self.env.step(action.item())\n            # update experience\n            self.model.experience.update(curr_state, action, reward, done, next_state)\n            # Optize model\n            q_value, y_batch = self.train()\n            # calculate loss\n            loss = self.criterion(q_value, y_batch)\n            # PyTorch accumulates gradients by default, so they need to be reset in each pass\n            self.optimizer.zero_grad()\n            # do backward pass\n            loss.backward()\n            self.optimizer.step()\n            # set curr_state to be next_state\n            curr_state = next_state\n            total_reward += reward\n        return total_reward\n\n    def __call__(self):\n        for n in tqdm(range(len(self.rewards), self.num_episodes)):\n            self.rewards = np.append(self.rewards, self.play(n))\n            if (time() - start_time) / 3600 > 2:\n                break\n\n    def agent(self, observation, config):\n        state = torch.tensor(observation.board, device = self.device, dtype = torch.float)\n        actions = self.model(state).topk(config.columns, dim = 1)[1][0]\n        actions = [action.item() for action in actions if observation.board[action.item()] == 0]\n        return actions[0]\n\n    def load_state_dict(self, path):\n        checkpoint = torch.load(path) if os.path.exists(path) else {}\n        self.model.load_state_dict(checkpoint.get('model', self.model.state_dict()))\n        self.rewards = checkpoint.get('rewards', self.rewards)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer = Trainer(50000)\ntrainer.load_state_dict('../input/connectx-using-deep-q-learning/model.pth')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"avg_rewards = np.array([]) # Last 100 steps\nfor n in range(len(trainer.rewards)):\n    avg_rewards = np.append(avg_rewards, trainer.rewards[max(0, n - 100):(n + 1)].mean())\nplt.figure(figsize = (15,5))\nplt.plot(avg_rewards)\nplt.xlabel('Episode')\nplt.ylabel('Avg rewards (100)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save({'model': trainer.model.state_dict(), 'rewards': trainer.rewards}, 'model.pth')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def my_agent(observation, configuration):\n    return trainer.agent(observation, configuration)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env = make('connectx', debug = True)\n# Test agent\nenv.reset()\n# Play as the first agent against default \"random\" agent.\nenv.run([my_agent, \"negamax\"])\nenv.render(mode=\"ipython\", width=500, height=450)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mean_reward(rewards):\n    return sum(r[0] for r in rewards) / sum(r[0] + r[1] for r in rewards)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run multiple episodes to estimate its performance.\nprint(\"My Agent vs Random Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"random\"], num_episodes=10)))\nprint(\"My Agent vs Negamax Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=10)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"state_dict = dict([(key, value.cpu().numpy().tolist()) for key, value in trainer.model.state_dict().items()])\nstate_dict = str(state_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subission = f'''\nimport torch\nimport numpy as np\nfrom torch import nn\nfrom torch import optim\nfrom collections import OrderedDict\nfrom kaggle_environments import make\n\nenv = make(\"connectx\", debug = False)\ndevice = torch.device(type = 'cuda' if torch.cuda.is_available() else 'cpu')\n\nstate_dict = {state_dict}\nstate_dict = OrderedDict((key, torch.tensor(value, device = device)) for key, value in state_dict.items())\n\nclass DQN(nn.Module):\n    def __init__(self, num_states, num_actions, hidden_units = 512):\n        super(DQN, self).__init__()\n        self.num_states = num_states\n        self.hidden_units = hidden_units\n        self.num_actions = num_actions\n        \n        self.layer1 = nn.Sequential(\n            nn.Linear(num_states, hidden_units),\n            nn.LayerNorm(hidden_units),\n            nn.ReLU())\n        \n        self.layer2 = nn.Sequential(\n            nn.Linear(hidden_units, num_actions),\n            nn.Sigmoid())\n\n    def forward(self, states):\n        states = states.view(-1, self.num_states)\n        output = self.layer1(states)\n        actions = self.layer2(output)\n        return actions\n\nmodel = DQN(env.configuration.columns * env.configuration.rows, env.configuration.columns).to(device)\nmodel.load_state_dict(state_dict)\n\ndef my_agent(observation, configuration):\n    state = torch.tensor(observation.board, device = device, dtype = torch.float)\n    actions = model(state).topk(configuration.columns, dim = 1)[1][0]\n    actions = [action.item() for action in actions if observation.board[action.item()] == 0]\n    return actions[0]\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('submission.py', 'w') as f:\n    f.write(subission)\n\"%s Kb\" % round(os.stat('submission.py').st_size/1024)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/connectx-using-deep-q-learning')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}