{"cells":[{"metadata":{},"cell_type":"markdown","source":"I almost always lose in games. If I am playing a game and you bet that I am going to lose, well, the odds are in your favour. So, I decided to build an AI that almost never loses in a game of Connect 4. Now, if you're like me, you probably knew the game but did not know the name. So, here goes:\n* There are two players in a game with alternating turns.\n* Each turn consists of a player dropping a disc into the game.\n* The player who has four discs together, either horizontally, or vertically, or diagonally, wins.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport random","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"from kaggle_environments import make, evaluate\nenv = make(\"connectx\", debug=True)\n\nprint(list(env.agents))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, if you still don't know how the game works (understandable), let's make two random agents play the game. Just know that these agents are completely random and will play the game extremely poorly. They have no presence of mind and either of the two, if wins, does so by fluke. This is just to get acquainted with the game.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"env.run([\"random\", \"random\"])\n\n# Show the game\nenv.render(mode=\"ipython\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So now that we have a hang of the game, let's go ahead and make some other agents.\n* one will be random as before\n* one will always choose the middle column in the grid\n* one will always choose the leftmost available column in the grid.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def agent_random(obs, config):\n    valid_moves = [col for col in range(config.columns) if obs.board[col] == 0]\n    return random.choice(valid_moves)\n\ndef agent_middle(obs, config):\n    return config.columns//2\n\ndef agent_leftmost(obs, config):\n    valid_moves = [col for col in range(config.columns) if obs.board[col] == 0]\n    return valid_moves[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Left-most agent versus random agent:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"env.run([agent_leftmost, agent_random])\nenv.render(mode = \"ipython\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Left-most agent versus middle agent:","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"env.run([agent_leftmost, agent_middle])\nenv.render(mode = \"ipython\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And finally, middle agent versus random agent:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"env.run([agent_middle, agent_random])\nenv.render(mode = \"ipython\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, clearly, all the agents are, as a matter of fact, quite dumb. So it's safe to say that they play the game exactly like me. You'll see how bad I play later in the notebook, it isn't that better than these agents. But, we need to make these agents better. So, I'll first use a Rudimentary method of rules as a man≈ìuvre.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Making the Agent Make Sense","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Before this, the agent did not know what to do at all. It just dropped the discs where it was told to. Now the agent will be programmed in such a way that, if a move is present which assures the agent that it will win, it ought to select that move. Otherwise, it ought to keep it random. Furthermore, if there are multiple moves which hint towards a win, the agent ought to choose either of them randomly.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def my_agent(obs, config):\n    valid_moves = [col for col in range(config.columns) if obs.board[col] == 0]\n    for col in valid_moves:\n        if check_winning_move(obs, config, col, obs.mark):\n            return col\n    for col in valid_moves:\n        if check_winning_move(obs, config, col, obs.mark%2+1):\n            return col\n    return random.choice(valid_moves)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"However, clearly just doing that will not suffice. The agent must prevent the other player (me) from winning, which shouldn't be a difficult task for a human but the AI is dumb. So, whenever it can, it should block any three discs of the opponent by placing its own disc in place of the apparent fourth disc.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Gets board at next step if agent drops piece in selected column\ndef drop_piece(grid, col, piece, config):\n    next_grid = grid.copy()\n    for row in range(config.rows-1, -1, -1):\n        if next_grid[row][col] == 0:\n            break\n    next_grid[row][col] = piece\n    return next_grid\n\n# Returns True if dropping piece in column results in game win\ndef check_winning_move(obs, config, col, piece):\n    # Convert the board to a 2D grid\n    grid = np.asarray(obs.board).reshape(config.rows, config.columns)\n    next_grid = drop_piece(grid, col, piece, config)\n    # horizontal\n    for row in range(config.rows):\n        for col in range(config.columns-(config.inarow-1)):\n            window = list(next_grid[row,col:col+config.inarow])\n            if window.count(piece) == config.inarow:\n                return True\n    # vertical\n    for row in range(config.rows-(config.inarow-1)):\n        for col in range(config.columns):\n            window = list(next_grid[row:row+config.inarow,col])\n            if window.count(piece) == config.inarow:\n                return True\n    # positive diagonal\n    for row in range(config.rows-(config.inarow-1)):\n        for col in range(config.columns-(config.inarow-1)):\n            window = list(next_grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n            if window.count(piece) == config.inarow:\n                return True\n    # negative diagonal\n    for row in range(config.inarow-1, config.rows):\n        for col in range(config.columns-(config.inarow-1)):\n            window = list(next_grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n            if window.count(piece) == config.inarow:\n                return True\n    return False","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's play! The turquoise discs belong to the agent mine are the white discs.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"from kaggle_environments import evaluate, make, utils\n\nenv = make(\"connectx\", debug=True)\nenv.play([my_agent, None], width=500, height=450)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"env.render(mode = \"ipython\", width=500, height=450)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So while the intelligence of the AI has increased a lot by just using some rudimentary methods, I still won, which means it is a bad AI. However, it has mastered blocking me from winning: when I have three discs together it puts its own disc in the location of the fourth apparent disc. The only reason it lost was because it had no other choice. So, not bad.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### One-Step Look-Ahead Heuristics","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now, what if we make the AI look at all the consequences of each and every move even before we start playing? These are known as look-ahead heuristics since the AI is essentially looking ahead of the present state, it is assessing all the future possiblitites and how they can affect its gameplay. While that seems pretty far-fetched, it is not impossible. But to keep my sanity intact, I will use a one-step look-ahead, i.e. the agent can look one steap ahead of me for each and every move that is possible in the future of the gameplay.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def score_move(grid, col, mark, config):\n    next_grid = drop_piece(grid, col, mark, config)\n    score = get_heuristic(next_grid, mark, config)\n    return score\n\ndef drop_piece(grid, col, mark, config):\n    next_grid = grid.copy()\n    for row in range(config.rows-1, -1, -1):\n        if next_grid[row][col] == 0:\n            break\n    next_grid[row][col] = mark\n    return next_grid\n\ndef get_heuristic(grid, mark, config):\n    num_threes = count_windows(grid, 3, mark, config)\n    num_fours = count_windows(grid, 4, mark, config)\n    num_threes_opp = count_windows(grid, 3, mark%2+1, config)\n    score = num_threes - 1e2*num_threes_opp + 1e6*num_fours\n    return score\n\ndef check_window(window, num_discs, piece, config):\n    return (window.count(piece) == num_discs and window.count(0) == config.inarow-num_discs)\n    \ndef count_windows(grid, num_discs, piece, config):\n    num_windows = 0\n    # horizontal\n    for row in range(config.rows):\n        for col in range(config.columns-(config.inarow-1)):\n            window = list(grid[row, col:col+config.inarow])\n            if check_window(window, num_discs, piece, config):\n                num_windows += 1\n    # vertical\n    for row in range(config.rows-(config.inarow-1)):\n        for col in range(config.columns):\n            window = list(grid[row:row+config.inarow, col])\n            if check_window(window, num_discs, piece, config):\n                num_windows += 1\n    # positive diagonal\n    for row in range(config.rows-(config.inarow-1)):\n        for col in range(config.columns-(config.inarow-1)):\n            window = list(grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n            if check_window(window, num_discs, piece, config):\n                num_windows += 1\n    # negative diagonal\n    for row in range(config.inarow-1, config.rows):\n        for col in range(config.columns-(config.inarow-1)):\n            window = list(grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n            if check_window(window, num_discs, piece, config):\n                num_windows += 1\n    return num_windows","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At this point, I start to treat my agent more or less like a pet dog: \"Good dog\" when it does the trick, \"bad dog\", when it doesn't. What this means is that the agent gets 1 million points if it gets all four of its discs together in a row, or a column or a diagonal; it gets 1 point if it gets three discs together like so and needs only one to win, and -100 if I can manage to get three of my discs together. That gives it enough incentive to make me lose.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"def agent(obs, config):\n    valid_moves = [c for c in range(config.columns) if obs.board[c] == 0]\n    grid = np.asarray(obs.board).reshape(config.rows, config.columns)\n    scores = dict(zip(valid_moves, [score_move(grid, col, obs.mark, config) for col in valid_moves]))\n    max_cols = [key for key in scores.keys() if scores[key] == max(scores.values())]\n    return random.choice(max_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"env = make(\"connectx\", debug=True)\nenv.play([agent, None], width=500, height=450)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"White discs are mine in the game.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"env.render(mode = \"ipython\", width=500, height=450)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, the AI did trap me quite well and won. But I'm not satisfied. I think this was more telling of how I bad I played than how good the AI got. I want the AI to play mind-games with me and make me question everything.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Hence, a little change in heuristics. The agent gets 1 million points if it has four discs together, 100 points if it has three discs together, 1 point if it has two discs together, -1 point if I have two discs together and -1000 points if I have three discs together. This way, the stakes are high and the AI has more to lose if I win.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def get_heuristic_extra(grid, col, mark, config):\n    num_twos = count_windows(grid, 2, mark, config)\n    num_threes = count_windows(grid, 3, mark, config)\n    num_fours = count_windows(grid, 4, mark, config)\n    num_twos_opp = count_windows(grid, 2, mark%2+1, config)\n    num_threes_opp = count_windows(grid, 3, mark%2+1, config)\n    score = A*num_fours + B*num_threes + C*num_twos + D*num_twos_opp + E*num_threes_opp\n    return score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def score_move_extra(grid, col, mark, config):\n    next_grid = drop_piece(grid, col, mark, config)\n    score = get_heuristic_extra(next_grid, col, mark, config)\n    return score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"A = 1e6\nB = 1e2\nC = 1\nD = -1\nE = -1e3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def agent_extra(obs, config):\n    valid_moves = [c for c in range(config.columns) if obs.board[c] == 0]\n    grid = np.asarray(obs.board).reshape(config.rows, config.columns)\n    scores = dict(zip(valid_moves, [score_move_extra(grid, col, obs.mark, config) for col in valid_moves]))\n    max_cols = [key for key in scores.keys() if scores[key] == max(scores.values())]\n    return random.choice(max_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"env = make(\"connectx\", debug=True)\nenv.play([agent_extra, None], width=500, height=450)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"env.render(mode = \"ipython\", width=500, height=450)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And there you go. The AI trapped me, played mind games with me, and won. And I'm still the loser.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}