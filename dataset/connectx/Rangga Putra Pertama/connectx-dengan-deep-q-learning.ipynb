{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <center> ConnectX dengan Deep Q-Learning </center>\n\n<div style = \"text-align: justify ; line-height: 1.75 em\">Kernel ini adalah <b><i>review</b></i> (dengan <b>sedikit perubahan</b> yang menurut saya lebih pas, misal : penggunaan <i>activation function</i>, <i>hyperparameter</i>, aturan decay epsilon, dan aturan pemberian reward) dari kernel yang dibuat oleh <b>Hieu Phung</b> (@phunghieu, <a href = \"https://www.kaggle.com/phunghieu/connectx-with-deep-q-learning-pytorch\">ConnectX with Deep Q-Learning (PyTorch)</a>) dan dibuat untuk mempermudah kita memahami prinsip-prinsip <b><i>Reinforcement Learning</b></i> serta bagaimana implementasinya pada sebuah game bernama <b>ConnectX</b> bagi para pemula dalam bidang ini khususnya yang berasal dari Indonesia. Bagi para pembaca silahkan </b><i>up vote</b></i> kernel tersebut. \n<br>\n<br>\nConnectX ini adalah game yang bertujuan untuk membuat 4 deret <i>checker</i> (bisa vertikal, horizontal, maupun diagonal) sebelum musuh kita dengan setiap putaran secara bergantian kita dan musuh kita menaruh <i>checker</i> pada papan 6 x 7. Menggunakan penggabungan antara <b>Q-Learning</b> (prinsip dasar <i>Reinforcement Learning</i>) dan <i>Neural Network</i> (<b><i>Deep Learning</b></i>) kita akan melatih sebuah agent untuk bermain dan nantinya akan ditandingkan dengan agent-agent lainnya. Sebagai informasi tambahan, ConnectX adalah kompetisi yang berfokus pada pembelajaran (tanpa <i>medal</i>) sehingga fokus utama kita adalah belajar dari kernel - kernel yang ada dan bukan <i>Leader Board</i> (walaupun untuk submisi per tanggal 28 Januari 2020 agent saya secara kebetulan bisa mengalahkan agent Hieu Phung (peace :))). \n<br>\n<br>\nProgram yang ditulis untuk melatih agent pada kernel ini (dan kebanyakan dalam kasus-kasus penyelesaian <i>Reinforcement Learning</i> lainnya) adalah <i>Object Oriented Programming</i> sehingga sangat disarankan bagi bara pembaca yang terbiasa menggunakan pendekatan <i>Procedural Programming</i> untuk melakukan pengolahan data agar belajar terlebih dahulu tentang <i>class</i>, <i>objects</i>, <i>overloading</i>, dan <i>inheritance</i> di Python.\n</div>"},{"metadata":{},"cell_type":"markdown","source":"## Daftar Isi\n\n1. [Reinforcement Learning dan Q-Learning](#pengertianqlearning)\n2. [Prinsip Deep Q-Learning](#deepqlearning)\n3. [Implementasi untuk ConnectX](#implementasi)\n    + [Tahap Persiapan](#persiapan)\n    + [Tahap Pelatihan Agent](#pelatihan)\n    + [Menuliskan Agent ke Sebuah File](#menulis)\n    + [Melihat Hasil Agent](#hasil)"},{"metadata":{},"cell_type":"markdown","source":"<a id = \"pengertianqlearning\"></a>\n\n## Reinforcement Learning dan Q-Learning\n\n<div style = \"text-align: justify ; line-height: 1.75 em\"><b><i>Reinforcement Learning</b></i> adalah sebuah metode <i>machine learning</i> dimana kita <b>melatih sebuah agent</b> untuk belajar mengambil keputusan <b><i>action</b></i>) dalam sebuah kondisi tertentu (<b><i>state</b></i>) di lingkungan tertentu (<b><i>environment</b></i>) sehingga bisa memaksimalkan <b><i>reward</b></i> yang akan didapat (<i>reward</i> bisa bernilai positif atau negatif dan akan didapat bila berhasil melakukan tujuan yang diinginkan). Proses pembelajaran ini dilakukan pada setiap <b><i>episode</b></i> pembelajaran dimana <b><i>episode</b></i> adalah setiap kejadian diantara <i>initial state</i> (kondisi awal) dan <i>terminal state</i> (kondisi akhir). Pada <i>Reinforcement Learning</i>, diasumsikan bahwa kondisi yang akan datang hanya dipengaruhi oleh kondisi saat ini dan bukan kondisi-kondisi sebelumnya. Proses dengan asumsi seperti ini disebut juga Proses Markov</div>\n<br>\n<div style = \"text-align: justify ; line-height: 1.75 em\">Pada setiap prosesnya (setiap <i>state</i>), agent akan melakukan <i>action</i> untuk memaksimalkan <i>reward</i>. Maka kualitas (<b><i>quality</b></i>) dari setiap aksi dinilai dari seberapa besarnya total <i>reward</i> yang didapat. Total <i>reward</i> ini disebut juga dengan <b><i>Q-value</b></i>. Pada tiap <i>episode</i>, agent dilatih untuk berusaha meningkatkan <i>Q-value</i> di <i>state</i> tersebut. Pada setiap pembelajaran, maka akan didapat <i>Q-value</i> akan diperbarui menjadi :</div>\n\n![Proses perbaruan *Q-value*](https://wikimedia.org/api/rest_v1/media/math/render/svg/47fa1e5cf8cf75996a777c11c7b9445dc96d4637)\n\ndengan :\n\n$\\alpha$ = porsi seberapa diperhitungkannya pengetahuan baru menggantikan pengetahuan lama <br>\n$\\gamma$ = porsi seberapa diperhitungkannya reward yang didapat setelah <i>state</i> saat ini (bila 0 maka hanya <i>immadiate reward</i> yang diperhitungkan) <br>\n\n<div style = \"text-align: justify ; line-height: 1.75 em\">Nantinya <i>Q-value</i> ini akan dimasukkan ke dalam <i>Q-table</i> yang berisi daftar <i>reward</i> untuk setiap <i>action</i> pada setiap <i>state</i>. Untuk lebih jelasnya maka perhatikan ilustrasi di bawah ini (diambil dari <a src = \"https://towardsdatascience.com/q-learning-54b841f3f9e4\">artikel ini</a>).\n<br>\n\n<center><img src = \"https://miro.medium.com/max/750/1*tSFotpgBNGurajFg2FH8Cg.png\"></center>\n\nPada sebuah permainan dengan misi mencari jalan terpendek dari 1,1 ke 5,5, terdapat <i>reward</i> yang ditunjukkan dengan warna hijau. Sementara warna merah adalah <i>punishment</i> (<i>reward</i> bernilai negatif sehingga mengurangi <i>Q-value</i>). Maka <i>Q-table</i> yang dihasilkan adalah \n<br>\n<br>\n<center><img src = \"https://miro.medium.com/max/981/1*p6yPonqoDMlK1w_EJKlcAQ.png\"></center>\n<br>\nArtinya, pada titik 1,1, bila kita melakukan aksi bergerak ke atas atau ke kiri kita mendapat <i>punishment</i> sebesar -1000 sementara bila bergerak ke bawah atau ke kanan kita mendapat <i>reward</i> sebesar 1 dan seterusnya. Namun ini baru sistem yang hanya memperhatikan <i>immadiate reward</i> dan sistem ini tidak sempurna. Pada konsisi tertentu, hanya memperhitungkan <i>immadiate reward</i> ($\\gamma$ = 0) punya beberapa kelemahan. Misalkan pada suatu <i>environment</i> yang berbeda yaitu permainan yang sama namun dengan suatu halangan, kita ada di titik 2.2\n<br>\n<center><img src = \"https://miro.medium.com/max/750/1*XV1aCvN2kWkTaos-E1h1yQ.png\"></center>\n\nMaka terdapat 2 pilihan yaitu mengambil <i>immadiate reward</i> yang besar yaitu ke bawah atau mengambil <i>immadiate reward</i> yang lebih kecil dan keluar dari jalan buntu. Hal lainnya yang perlu diperhatikan adalah agent tersebut belum memperhitungkan banyaknya langkah yang harus diambil untuk memaksimalkan <i>Q-value</i>. Bisa saja untuk memaksimalkan <i>Q-value</i> maka agent kita melakukan <i>infinite looping</i> di daerah berwarna hijau dan mengambil <i>reward</i> sebanyak-banyaknya. Selain itu, bila kita perhatikan persamaan matematika di atas, maka agent akan terus mengambil <i>action</i> yang sama pada tiap <i>episode</i> latihan yang membuat <i>Q-value</i> sebesar-besarnya tanpa mengeksplorasi hal baru. Padahal <b>terdapat kemungkinan bahwa solusi terbaik adalah solusi yang belum dicoba</b>. Untuk mengatasi hal tersebut, kita harus membuat parameter lain yaitu <b>$\\epsilon$</b> sehingga <b>bila <i>reward</i> yang didapat kurang dari atau sama dengan $\\epsilon$</b>, kita buat agent tersebut <b>memilih <i>action</i> secara random</b>. \n\nItulah penjelasan tentang <i>Reinforcement Learning</i>, <i>Q-learning</i>, dan beberapa prosedurnya. Dapat dilihat bahwa <i>Q-learning</i> tidak menggunakan model apapun dan hanya berfokus pada <i>action</i> dan <i>reward</i> sehingga disebut juga <i>model-free</i>. Perlu diperhatikan bahwa <b>bila kemungkinan <i>state</i> serta <i>action</i> yang ada sangat banyak</b> maka kelemahan Q-Learning adalah harus <b>mendaftar satu persatu <i>Q-value</i></b> tersebut. Pastinya ini akan memakan banyak memori dan waktu setiap kali kita mengeksplorasi <i>state</i> dan <i>action</i> baru.\n</div>\n\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"<a id = \"deepqlearning\"></a>\n## Prinsip Deep Q-Learning\n\n<div style = \"text-align: justify ; line-height: 1.75 em\">Telah kita ketahui bahwa salah satu kelemahan <i>Q-learning</i> adalah memori dan waktu karena harus mempelajari dan menyimpan <i>Q-value</i> dari setiap <i>state</i> dan <i>action</i>. Bila kita batasi proses pembelajaran agent tersebut untuk menghemat waktu dan memori, maka tidak semua kemungkinan dicoba. Saat agent tersebut selesai belajar dan dijalankan terdapat kemungkinan agent tersebut menemukan <i>state</i> baru. Akibatnya agent tersebut tidak tahu harus berbuat apa. Dengan kata lain, kekurangan <i>Q-learning</i> adalah tidak melakukan generalisasi terhadap <i>state</i> dan <i>action</i> yang mungkin. \n<br>\n<br>\nNamun bagaimana bila kita bisa mengestimasi berbagia kemungkinan <i>Q-value</i>? Itulah yang dilakukan oleh <i>Deep Q-learning</i>. <i>Deep Q-learning</i> berusaha mengestimasi <i>Q-value</i> dari <i>action</i> yang diambil untuk setiap <i>state</i> yang ada. Input dari <i>Deep Q-learning</i> adalah gambar <i>state</i> saat ini.\n\n<center><img src = \"https://pic4.zhimg.com/80/v2-67ef75bb7f5e67b2a42645aa821894bf_hd.png\"></center>\n<center>Ilustrasi Deep Q-learning pada game Atari (sumber <a src = \"https://zhuanlan.zhihu.com/p/25239682\">diambil di sini</a>)</center>\n<br>\n<b>Target dari proses <i>training neural network</i> ini adalah <i>immadiate reward</i> + ($\\gamma$ x <i>estimate of future value</i>)</b> atau suku yang dinamakan <i>learned value</i> pada persamaan <i>update Q-value</i> dan <b><i>Loss function</i></b> yang ingin diminalkan adalah <b>(<i>Q-value</i> prediksi - <i>Q-value</i> sebenarnya)<sup>2</sup></b>. Tapi masalahnya adalah, kita juga tidak tau berapa <i>Q-value</i> sebenarnya saat mengambil <i>action</i> tertentu di <i>state</i> tertentu. Jadi kita harus memprediksi kedua nilai <i>Q-value</i> tersebut sehingga tidak mungkin digunakan satu neural network saja.\n<br>\n<br>\nUntuk mengatasi masalah di atas, maka <b>satu input akan dimasukkan ke dua <i>neural network</i> yang berbeda</b>. Satu <i>neural network</i> berfungsi untuk mengestimasi target, dan <i>neural network</i> lainnya digunakan untuk memprediksi <i>Q-value</i> hasil prediksi. Lalu hasilnya training keduanya dimasukkan ke dalam <i>cost function</i>. Namun, <i>weight</i> yang selalu diupdate hanya <i>weight</i> dari <i>neural network</i> kedua sementara <i>weight</i> dari <i>neural network</i> pertama dibuat semi-konstan. Dengan kata lain, <i>weight neural network</i> pertama diperbarui dengan nilai <i>weight neural network</i> kedua hanya setiap beberapa iterasi sekali. Ini dilakukan terus menerus hingga <i>cost function</i> mencapai batas <i>threshold</i> minimal. Arsitektur dari <i>Deep Q-learning</i> dapat dilihat seperti gambar di bawah ini.\n<img src = \"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2019/04/Screenshot-2019-04-17-at-12.48.05-PM-768x638.png\" height = \"500\" width = \"500\">\n<br>\nSetalah mengetahui arsitekturnya, kita juga harus menentukan bagaimana cara <i>trainingnya</i> agar tidak menghabiskan memori dan waktu untuk melakukan training <i>neural network</i> sepanjang waktu. <b>Cara training</b> yang digunakan adalah <b><i>Experience Replay</i></b>. Jadi pertama kita tentukan berapa banyak <i>batch</i> yang akan dimasukkan ke <i>neural network</i> untuk di<i>train</i>. Kemudian jalankan terlebih dahulu agent untuk mengambil <i>Q-value</i> pada beberapa <i>state</i> dan <i>action</i> dan simpan dalam sebuah memori. Bila <i>Q-value</i> untuk setiap transisi <i>state</i> telah mencapai nilai <i>batch</i>, maka ambil data-data tersebut dan <i>train</i> di <i>neural network</i> dengan arsitektur seperti di atas. Selanjutnya jalankan kembali agent. Setiap terdapat data baru sejumlah <i>batch</i>, maka kita lakukan sampling sebanyak <i>batch</i> dan lakukan <i>training</i> di <i>neural network</i>. Keseluruhan workflow <i>Deep Q-learning</i> dapat dilihat di bawah (diambil dari <a src = \"https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-i-q-learning-sarsa-dqn-ddpg-72a5e0cb6287\">artikel ini</a>).\n<img src = \"https://miro.medium.com/max/1508/1*nb61CxDTTAWR1EJnbCl1cA.png\" height = \"500\" width = \"500\">\n</div>\n"},{"metadata":{},"cell_type":"markdown","source":"<a id = \"implementasi\"></a>\n\n## Implementasi untuk ConnectX\n\nSecara umum, tahap implementasi ini terdiri dari :\n1. Tahap Persiapan : mempersiapkan *environment* ConnectX, *neural network* untuk proses pelatihan (*training*), dan agent\n2. Tahap Pelatihan Agent : mengaktifkan *environment*, mengatur *hyperparameter*, memulai pelatihan, dan melihat hasil pelatihan \n3. Menuliskan Agent ke Sebuah File \n4. Melihat Hasil Agent"},{"metadata":{},"cell_type":"markdown","source":"<a id = \"persiapan\"></a>\n\n## Tahap Persiapan\n\nPanduan untuk tahap ini bisa dilihat di https://www.kaggle.com/ajeffries/connectx-getting-started"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#  ConnectX hanya bisa dijalankan pada kaggle versi 0.1.6\n\n!pip install 'kaggle-environments>=0.1.6' > /dev/null 2>&1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Import modul yang diperlukan\n\n# Modul umum\nimport  numpy as np                # perhitungan matematika\nfrom tqdm.notebook import tqdm     # menampilkan progress bar\nimport matplotlib.pyplot as plt    # menggambar plot\n\n# Modul mempersiapkan environment ConnectX\nimport gym\nfrom kaggle_environments import evaluate, make\n\n# Modul neural network\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mendefinisikan class yang akan dipakai sepanjang kernel ini\n\nclass ConnectX(gym.Env) :\n    \n    # constructor method (inisialisasi)\n    def __init__(self, switch_prob = 0.5) :\n        \n        # membuat environment (method-methodnya dapat dilihat pada kernel Getting Started di atas)\n        self.env = make('connectx', debug = False)\n        # mengambil setting pada environment ConnectX (berisi timeout, columns, rows, inarow atau syarat berhasil, dan steps)   \n        config = self.env.configuration\n        # mendefinisikan jumlah aksi yang dapat dilakukan (mengisi kolom ke berapa) atau action space dan jumlah state atau obs space\n        self.action_space = gym.spaces.Discrete(config.columns)\n        self.observation_space = gym.spaces.Discrete(config.columns * config.rows)\n        \n        # melatih agent kita sebagai player pertama melawan agent Negamax\n        # memasukkan parameter [None, \"negamax\"] ke dalam atribut sendiri agar mudah diakses dan diganti (misal bertukar posisi player) \n        self.pair = [None, 'negamax']     \n        self.trainer = self.env.train(self.pair)\n        # mendefinisikan atribut peluang untuk berganti posisi player dalam proses pelatihan agent nantinya\n        self.switch_prob = switch_prob\n        \n    # method untuk berganti posisi player    \n    def switch_trainer(self) :   \n        # mengganti urutan elemen pada self.pair yang telah diinisiasi\n        self.pair = self.pair[::-1]\n        self.trainer = self.env.train(self.pair)     \n    def reset(self) :\n        if np.random.random() < self.switch_prob :\n            self.switch_trainer()\n        return self.trainer.reset()\n    \n    # method untuk mengakses observasi, reward, status done atau belum, dan info selama pelatihan agent sesuai dengan action yang dilakukan\n    def step(self, action) :\n        return self.trainer.step(action)\n    \n    # method untuk merender state\n    def render(self, **kwargs) :\n        return self.env.render(**kwargs)\n\n# Mendefinisikan kelas untuk neural network untuk proses pelatihan agent (lihat https://pytorch.org/docs/stable/nn.html#containers)\n\nclass DeepModel(nn.Module) :\n    \n    # constructor method (inisialisai)\n    def __init__(self, num_states, hidden_units, num_actions) :\n         \n        super(DeepModel, self).__init__()\n        \n        # mengkonstruksi hidden layer (perhatikan ilustrasi struktur neural network pada gambar di atas)\n        # akan dicoba activation function berupa ReLU \n        self.hidden_layers = []\n        for i in range(len(hidden_units)) :\n            # untuk hidden layer pertama\n            if i == 0 :\n                self.hidden_layers.append(nn.Linear(num_states, hidden_units[i]))\n            # untuk hidden layer berikutnya\n            else :\n                self.hidden_layers.append(nn.Linear(hidden_units[i-1], hidden_units[i]))\n        self.output_layer = nn.Linear(hidden_units[-1], num_actions)\n    \n    def forward(self, x) :\n        for layer in self.hidden_layers :\n            x = layer(x).clamp(min=0)\n        x = self.output_layer(x)\n        return x\n    \n# Mendefinisikan kelas untuk agent\n# Secara umum, agent harus punya 3 kemampuan : bermain beberapa permainan, mengingatnya, dan memperkirakan reward dari tiap state\n# yang muncul pada permainan\n\nclass DQN :\n\n    # constructor method (inisialisasi)\n    def __init__(self, num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr) :\n        \n        #inisialisasi atribut untuk bermain\n        self.num_actions = num_actions   # banyaknya aksi\n        self.gamma = gamma               # porsi future reward terhadap immadiate reward\n        \n        # inisialisasi atribut untuk mekanisme mengingat permainan\n        self.batch_size = batch_size\n        self.experience = {'s' : [], 'a' : [], 'r' : [], 's2' : [], 'done' : []}\n        self.max_experiences = max_experiences\n        self.min_experiences = min_experiences\n        \n        # inisialisasi atribut untuk perkiraan reward dengan neural network di setiap state\n        self.model = DeepModel(num_states, hidden_units, num_actions)\n        self.optimizer = optim.Adam(self.model.parameters(), lr = lr)\n        self.criterion = nn.MSELoss()\n        \n    # fungsi untuk mengatur permainan yang diingat lalu melakukan training neural network dari sana\n    \n    # membuang ingatan state paling awal bila sudah melebihi batas memori\n    def add_experience(self, exp) :\n        if len(self.experience['s']) >= self.max_experiences :\n            for key in self.experience.keys() :\n                self.experience[key].pop(0)\n        for key, value in exp.items() :\n            self.experience[key].append(value)\n    \n    # melakukan training dari neural network \n    # panduannya bisa dilihat di https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n    def train(self, TargetNet) :\n        \n        # hanya melakukan training bila state yang diingat telah melebihi batas minimal\n        if len(self.experience['s']) < self.min_experiences :\n            return 0\n        \n        # mengambil ingatan permainan secara random sesuai ukuran batch   \n        ids = np.random.randint(low = 0, high = len(self.experience['s']), size = self.batch_size)\n        states = np.asarray([self.preprocess(self.experience['s'][i]) for i in ids])\n        actions = np.asarray([self.experience['a'][i] for i in ids])\n        rewards = np.asarray([self.experience['r'][i] for i in ids])\n        \n        # mengambil label\n        next_states = np.asarray([self.preprocess(self.experience['s2'][i]) for i in ids])\n        dones = np.asarray([self.experience['done'][i] for i in ids])\n                \n        # untuk semua ingatan yang diambil, kita harus memprediksi maksimal Q-value dari state setelahnya\n        value_next = np.max(TargetNet.predict(next_states).detach().numpy(), axis = 1)\n        actual_values = np.where(dones, rewards, rewards + self.gamma * value_next)\n        \n        actions = np.expand_dims(actions, axis = 1)\n        actions_one_hot = torch.FloatTensor(self.batch_size, self.num_actions).zero_()\n        actions_one_hot = actions_one_hot.scatter_(1, torch.LongTensor(actions), 1)\n        selected_action_values = torch.sum(self.predict(states) * actions_one_hot, dim = 1)\n        actual_values = torch.FloatTensor(actual_values)\n        \n        self.optimizer.zero_grad()\n        loss = self.criterion(selected_action_values, actual_values)\n        loss.backward()\n        self.optimizer.step()\n    \n    def copy_weights(self, TrainNet) :\n        self.model.load_state_dict(TrainNet.state_dict())\n        \n    def save_weights(self, path) :\n        torch.save(self.model.state_dict(), path)\n        \n    def load_weights(self, path) :\n        self.model.load_state_dict(torch.load(path))      \n        \n    # fungsi untuk menentukan aksi yang diambil untuk bermain berdasarkan hasil prediksi dan aturan eksplorasi\n    \n    # fungsi untuk preprocess state sebelum dimas\n    def preprocess(self, state) :\n        result = state.board[:]\n        result.append(state.mark)\n        return result\n    \n    # fungsi untuk prediksi \n    def predict(self, inputs) :\n        return self.model(torch.from_numpy(inputs).float())\n    \n    def get_action(self, state, epsilon) :\n        # mekanisme eksplorasi, melakukan aksi random bila kolom tersebut kosong \n        if np.random.random() < epsilon :\n            return int(np.random.choice([c for c in range(self.num_actions) if state.board[c] == 0]))\n        else :\n            prediction = self.predict(np.atleast_2d(self.preprocess(state)))[0].detach().numpy()\n            for i in range(self.num_actions) :\n                if state.board[i] != 0 :\n                    prediction[i] = -1e7\n            # melakukan aksi dengan menaruh checker di kolom yang punya hasil prediksi Q value yang paling besar\n            return int(np.argmax(prediction)) \n\n# mendefinisikan aturan permainan yang akan dipelajari agent\ndef play_game(env, TrainNet, TargetNet, epsilon, copy_step) :\n    rewards = 0\n    iter = 0\n    done = False\n    observations = env.reset()\n    \n    while not done :     \n        action = TrainNet.get_action(observations, epsilon)\n        prev_observations = observations\n        observations, reward, done, _ = env.step(action)  \n        if done :\n            # menang\n            if reward == 1 :\n                reward = 20\n            # kalah\n            elif reward == 0 :\n                reward = -50\n            # draw\n            else :\n                reward = 10\n        # membuat agent kita berusaha untuk bermain lebih panjang (selama permainan belum berakhir dia mendapat reward 0.5)\n        # namun harus dibuat threshold agar bila telah melewati batas tertentu maka agen berusaha menang (bukan berusaha main lebih panjang)    \n        else :\n            if rewards <= 2.5 : \n                reward = 0.5\n            else :\n                reward = -0.5\n        rewards += reward\n        \n        # membuat buffer ingatan permainan\n        exp = {'s' : prev_observations, 'a' : action, 'r' : reward, 's2' : observations, 'done' : done}\n        TrainNet.add_experience(exp)\n        TrainNet.train(TargetNet)\n        \n        # ingat bahwa kita membuat network yang memprediksi nilai sebenarnya dari target akan mengcopy weight dari network satu lagi\n        iter+=1\n        if iter % copy_step == 0 :\n            TargetNet.copy_weights(TrainNet)\n            \n    return rewards    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"pelatihan\"></a>\n\n## Tahap Pelatihan Agent"},{"metadata":{"trusted":true},"cell_type":"code","source":"# mengaktifkan environment\n\nenv = ConnectX()\n\n# mengatur hyperparameter\n\ngamma = 0.99 \ncopy_step = 25\nhidden_units = [100, 200, 200, 100]\nmax_experiences = 1000\nmin_experiences = 100\nbatch_size = 32\nlr = 0.01\nepsilon = 0.25    # lebih memilih eksploitasi, namun decay epsilon diperlambat\ndecay = 0.99\nmin_epsilon = 0.05\nepisodes =14000\nprecision = 7","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# inisiasi\n\nnum_states = env.observation_space.n + 1\nnum_actions = env.action_space.n\n\nall_total_rewards = np.empty(episodes) \nall_avg_rewards = np.empty(episodes)\nall_epsilons = np.empty(episodes)\n\nTrainNet = DQN(num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\nTargetNet = DQN(num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n\nprogress_bar = tqdm(range(episodes))\nfor i in progress_bar :\n    if i % 10 == 0 :\n        epsilon = max(min_epsilon, epsilon * decay)\n    else :\n        epsilon = max(min_epsilon, epsilon)\n    total_reward = play_game(env, TrainNet, TargetNet, epsilon, copy_step)\n    all_total_rewards[i] = total_reward\n    avg_reward = all_total_rewards[max(0, i-100) : (i+1)].mean()\n    all_avg_rewards[i] = avg_reward\n    all_epsilons[i] = epsilon\n    progress_bar.set_postfix({\n        'episode_reward' : total_reward,\n        'avg of last 100 reward' : avg_reward,\n        'epsilon' : epsilon\n    })","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# melihat hasil pelatihan agent\n\nplt.plot(all_avg_rewards)\nplt.xlabel('Episode')\nplt.ylabel('Avg Last 100 Rewards ')\nplt.show()\n\n# menyimpan weight dari tiap state\n\nTrainNet.save_weights('./weights.pth')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"menulis\"></a>\n\n## Menuliskan Agent ke Sebuah File"},{"metadata":{"trusted":true},"cell_type":"code","source":"fc_layers = []\n\n# mengambil weight dan bias dari tiap hidden layer serta output layer\nfor i in range(len(hidden_units)):\n    fc_layers.extend([\n        TrainNet.model.hidden_layers[i].weight.T.tolist(), \n        TrainNet.model.hidden_layers[i].bias.tolist() \n    ])\nfc_layers.extend([\n    TrainNet.model.output_layer.weight.T.tolist(), \n    TrainNet.model.output_layer.bias.tolist() \n])\n\n# mengedit hasil dari fc_layers\nfc_layers = list(map(\n    lambda x: str(list(np.round(x, precision))) \\\n        .replace('array(', '').replace(')', '') \\\n        .replace(' ', '') \\\n        .replace('\\n', ''),\n    fc_layers\n))\nfc_layers = np.reshape(fc_layers, (-1, 2))\n\n# Menuliskan agent\n# Agent tersebut harus diinisiasi terlebih dahulu, mempunyai daftar weights tiap state untuk melakukan perhitungan, dan melakukan aksi\n\n# inisiasi agent\nmy_agent = '''def my_agent(observation, configuration):\n    import numpy as np\n\n'''\n\n# memasukkan hasil bobot tiap hidden layer\nfor i, (w, b) in enumerate(fc_layers[:-1]):\n    my_agent += '    hl{}_w = np.array({}, dtype=np.float32)\\n'.format(i+1, w)\n    my_agent += '    hl{}_b = np.array({}, dtype=np.float32)\\n'.format(i+1, b)\nmy_agent += '    ol_w = np.array({}, dtype=np.float32)\\n'.format(fc_layers[-1][0])\nmy_agent += '    ol_b = np.array({}, dtype=np.float32)\\n'.format(fc_layers[-1][1])\n\nmy_agent += '''\n    state = observation.board[:]\n    state.append(observation.mark)\n    out = np.array(state, dtype=np.float32)\n\n'''\n\n# melakukan kalkulasi Q-value berdasarkan weight hidden layer hingga output layer\nfor i in range(len(fc_layers[:-1])):\n    my_agent += '    out = np.matmul(out, hl{0}_w) + hl{0}_b\\n'.format(i+1)\n    my_agent += '    out = np.maximum(out,0)\\n'     # fungsi aktivasi ReLU .clamp(min = 0)\n    \nmy_agent += '    out = np.matmul(out, ol_w) + ol_b\\n'\n\n# melakukan aksi\nmy_agent += '''\n    for i in range(configuration.columns):\n        if observation.board[i] != 0:\n            out[i] = -1e7\n\n    return int(np.argmax(out))\n    '''\nwith open('submission.py', 'w') as f:\n    f.write(my_agent)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"hasil\"></a>\n\n## Melihat Hasil Agent"},{"metadata":{"trusted":true},"cell_type":"code","source":"from submission import my_agent\n\ndef mean_reward(rewards):\n    return sum(r[0] for r in rewards) / sum(r[0] + r[1] for r in rewards)\n\nprint(\"My Agent vs. Random Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"random\"], num_episodes=50)))\nprint(\"My Agent vs. Negamax Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=50)))\nprint(\"Random Agent vs. My Agent:\", mean_reward(evaluate(\"connectx\", [\"random\", my_agent], num_episodes=50)))\nprint(\"Negamax Agent vs. My Agent:\", mean_reward(evaluate(\"connectx\", [\"negamax\", my_agent], num_episodes=50)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}