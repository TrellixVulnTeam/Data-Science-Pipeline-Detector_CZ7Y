{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**This notebook is an exercise in the [Intro to Game AI and Reinforcement Learning](https://www.kaggle.com/learn/intro-to-game-ai-and-reinforcement-learning) course.  You can reference the tutorial at [this link](https://www.kaggle.com/alexisbcook/deep-reinforcement-learning).**\n\n---\n","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n!pip install 'tensorflow==1.15.0'\n\nimport tensorflow as tf\nfrom kaggle_environments import make, evaluate\nfrom gym import spaces\n\n!apt-get update\n!apt-get install -y cmake libopenmpi-dev python3-dev zlib1g-dev\n!pip install \"stable-baselines[mpi]==2.9.0\"\n\nfrom stable_baselines.bench import Monitor \nfrom stable_baselines.common.vec_env import DummyVecEnv\nfrom stable_baselines import PPO1, A2C, ACER, ACKTR, TRPO\nfrom stable_baselines.a2c.utils import conv, linear, conv_to_fc\nfrom stable_baselines.common.policies import CnnPolicy\n\nclass ConnectFourGym:\n    def __init__(self, agent2=\"random\"):\n        ks_env = make(\"connectx\", debug=True)\n        self.env = ks_env.train([None, agent2])\n        self.rows = ks_env.configuration.rows\n        self.columns = ks_env.configuration.columns\n        # Learn about spaces here: http://gym.openai.com/docs/#spaces\n        self.action_space = spaces.Discrete(self.columns)\n        self.observation_space = spaces.Box(low=0, high=2, \n                                            shape=(self.rows,self.columns,1), dtype=np.int)\n        # Tuple corresponding to the min and max possible rewards\n        self.reward_range = (-10, 1)\n        # StableBaselines throws error if these are not defined\n        self.spec = None\n        self.metadata = None\n    def reset(self):\n        self.obs = self.env.reset()\n        return np.array(self.obs['board']).reshape(self.rows,self.columns,1)\n    def change_reward(self, old_reward, done):\n        if old_reward == 1: # The agent won the game\n            return 1\n        elif done: # The opponent won the game\n            return -1\n        else: # Reward 1/42\n            return 1/(self.rows*self.columns)\n    def step(self, action):\n        # Check if agent's move is valid\n        is_valid = (self.obs['board'][int(action)] == 0)\n        if is_valid: # Play the move\n            self.obs, old_reward, done, _ = self.env.step(int(action))\n            reward = self.change_reward(old_reward, done)\n        else: # End the game and penalize agent\n            reward, done, _ = -10, True, {}\n        return np.array(self.obs['board']).reshape(self.rows,self.columns,1), reward, done, _\n    \n# Create ConnectFour environment\nenv = ConnectFourGym(agent2 = \"random\")\n\n# Create directory for logging training information\nlog_dir = \"log/\"\nos.makedirs(log_dir, exist_ok=True)\n\n# Logging progress\nmonitor_env = Monitor(env, log_dir, allow_early_resets=True)\n\n# Create a vectorized environment\nvec_env = DummyVecEnv([lambda: monitor_env])\n\n# Neural network for predicting action values\ndef modified_cnn(scaled_images, **kwargs):\n    activ = tf.nn.relu\n    layer_1 = activ(conv(scaled_images, 'c1', n_filters=32, filter_size=3, stride=1, \n                         init_scale=np.sqrt(2), **kwargs))\n    layer_2 = activ(conv(layer_1, 'c2', n_filters=64, filter_size=3, stride=1, \n                         init_scale=np.sqrt(2), **kwargs))\n    layer_2 = conv_to_fc(layer_2)\n    return activ(linear(layer_2, 'fc1', n_hidden=512, init_scale=np.sqrt(2)))  \n\nclass CustomCnnPolicy(CnnPolicy):\n    def __init__(self, *args, **kwargs):\n        super(CustomCnnPolicy, self).__init__(*args, **kwargs, cnn_extractor=modified_cnn)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T18:06:39.869384Z","iopub.execute_input":"2021-06-04T18:06:39.870075Z","iopub.status.idle":"2021-06-04T18:07:01.204716Z","shell.execute_reply.started":"2021-06-04T18:06:39.869976Z","shell.execute_reply":"2021-06-04T18:07:01.203632Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, run the code cell below to train an agent with PPO and view how the rewards evolved during training.  This code is identical to the code from the tutorial.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Initialize agent\nmodel_pp = PPO1(CustomCnnPolicy, vec_env, verbose=0)\n\n# Train agent\nmodel_pp.learn(total_timesteps=100000)\n\n# Plot cumulative reward\nwith open(os.path.join(log_dir, \"monitor.csv\"), 'rt') as fh:    \n    firstline = fh.readline()\n    assert firstline[0] == '#'\n    df = pd.read_csv(fh, index_col=None)['r']\ndf.rolling(window=1000).mean().plot()\nplt.show()\n\ndef agent_pp01(obs, config):\n    # Use the best model to select a column\n    col, _ = model_pp.predict(np.array(obs['board']).reshape(6,7,1))\n    # Check if selected column is valid\n    is_valid = (obs['board'][int(col)] == 0)\n    # If not valid, select random move. \n    if is_valid:\n        return int(col)\n    else:\n        return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])","metadata":{"execution":{"iopub.status.busy":"2021-06-04T18:08:00.991001Z","iopub.execute_input":"2021-06-04T18:08:00.991623Z","iopub.status.idle":"2021-06-04T19:30:10.027816Z","shell.execute_reply.started":"2021-06-04T18:08:00.991568Z","shell.execute_reply":"2021-06-04T19:30:10.022883Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize agent\nmodel = A2C(CustomCnnPolicy, vec_env, verbose=0)\n\n# Train agent\nmodel.learn(total_timesteps=100000)\n\n# Plot cumulative reward\nwith open(os.path.join(log_dir, \"monitor.csv\"), 'rt') as fh:    \n    firstline = fh.readline()\n    assert firstline[0] == '#'\n    df = pd.read_csv(fh, index_col=None)['r']\ndf.rolling(window=1000).mean().plot()\nplt.show()\n\ndef agent_a2c(obs, config):\n    # Use the best model to select a column\n    col, _ = model.predict(np.array(obs['board']).reshape(6,7,1))\n    # Check if selected column is valid\n    is_valid = (obs['board'][int(col)] == 0)\n    # If not valid, select random move. \n    if is_valid:\n        return int(col)\n    else:\n        return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])","metadata":{"execution":{"iopub.status.busy":"2021-06-04T17:28:37.891331Z","iopub.execute_input":"2021-06-04T17:28:37.891723Z","iopub.status.idle":"2021-06-04T17:41:18.593292Z","shell.execute_reply.started":"2021-06-04T17:28:37.891692Z","shell.execute_reply":"2021-06-04T17:41:18.592114Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getUserInput():\n\n    while True:\n        try:\n            user_input = int(input(\"Input between 1 and 7: \"))\n            if   user_input <= 0 or user_input > 7:\n                print('Invalid input: ', user_input)   \n            else:\n                return user_input -1\n        \n        except Exception as e:\n            print('Invalid input: ', user_input)\n            continue\n","metadata":{"execution":{"iopub.status.busy":"2021-06-04T19:31:42.3638Z","iopub.execute_input":"2021-06-04T19:31:42.36416Z","iopub.status.idle":"2021-06-04T19:31:42.372179Z","shell.execute_reply.started":"2021-06-04T19:31:42.364129Z","shell.execute_reply":"2021-06-04T19:31:42.370877Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_win_percentages(agent1, agent2, n_rounds=100):\n    # Use default Connect Four setup\n    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n    # Agent 1 goes first (roughly) half the time          \n    outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], n_rounds//2)\n    # Agent 2 goes first (roughly) half the time      \n    outcomes += [[b,a] for [a,b] in evaluate(\"connectx\", [agent2, agent1], config, [], n_rounds-n_rounds//2)]\n    print(\"Agent 1 Win Percentage:\", np.round(outcomes.count([1,-1])/len(outcomes), 2))\n    print(\"Agent 2 Win Percentage:\", np.round(outcomes.count([-1,1])/len(outcomes), 2))\n    print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0]))\n    print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0, None]))\n\n# Create the game environment\nenv = make(\"connectx\")\n\n# None vs agent1\n# NOTE: Rendering the board each turn takes some time\ntrainer = env.train([None, agent_a2c])\nobservation = trainer.reset()\nwhile not env.done:\n\n    print(\"Your turn (blue)\")\n        \n    myAction = getUserInput()\n\n    observation, reward, done, info = trainer.step(myAction)\n        \n    env.render(mode=\"ipython\", width=300, height=200, header=False, controls=False)\n    \n# # Create the game environment\n# env = make(\"connectx\")\n\n# # Two random agents play one game round\n# env.run([agent_a2c, agent_pp01])\n\n# # Show the game\n# env.render(mode=\"ipython\")\n\n# get_win_percentages(agent1=agent_pp01, agent2=agent_a2c)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T19:31:44.248241Z","iopub.execute_input":"2021-06-04T19:31:44.248626Z","iopub.status.idle":"2021-06-04T19:33:28.78079Z","shell.execute_reply.started":"2021-06-04T19:31:44.248595Z","shell.execute_reply":"2021-06-04T19:33:28.777932Z"},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n\n\n\n*Have questions or comments? Visit the [Learn Discussion forum](https://www.kaggle.com/learn-forum/161477) to chat with other Learners.*","metadata":{}}]}