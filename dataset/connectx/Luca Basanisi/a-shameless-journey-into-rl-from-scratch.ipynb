{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook wants to record my process of learning and practicing concept of Reinforcement Learning. My current level in this field is nearly 0 and I am currently struggling in acquiring skills from online courses that are somewhat reproducible without guidance. My professional experience never gave me the chance of developing knowledge and skill in this field and I would like to make it a possibility for the future.\n\nSimilarly to what [I am doing for NLP](https://www.kaggle.com/lucabasa/a-shameless-journey-into-nlp-from-scratch), the notebook is meant to be a diary for myself and it is made public to hold myself accountable for making significant progress over the next few months. I will be following [the Kaggle course](https://www.kaggle.com/learn/intro-to-game-ai-and-reinforcement-learning) and try to supplement it with further research in order to make those concepts stick.\n\n**Do not expect a brillian notebook, nor a high scoring one. It will most likely be a fairly pedantic exploration of functionalities I don't know yet. Feel free to drop a suggestion in the comments**\n\n***Day count = 4***","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Environment and Agents\n\nKaggle has already implemented a game environment for us to play with. I will need to play around with it a bit to know more what this environment does. I assume it provides the board with the game rules.","metadata":{}},{"cell_type":"code","source":"from kaggle_environments import make, evaluate\n\nenv = make(\"connectx\", debug=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The first thing I see is `make`, which is a good name for something that creates the environment. I can't find the code at the moment, so the help will do","metadata":{}},{"cell_type":"code","source":"help(make)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I see that in the `envs` folder there are `connectx`, `tictactoe` and `identity`. I wonder if I can simply load a different environment","metadata":{}},{"cell_type":"code","source":"env = make(\"tictactoe\", debug=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Apparently yes, good to know. Moving on, what is this environment?","metadata":{}},{"cell_type":"code","source":"env = make(\"connectx\", debug=True)\n\ndir(env)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env.__state_schema_0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Indeed, it seems to contain all the methods we need to play a game. In particular, it comes with some **agents**","metadata":{}},{"cell_type":"code","source":"env.agents","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And I can find the code of these agents in the file `connectx.py`, nice. In this context, the agent is a python function that we program or train to take decisions given the situation, which is the essence of RL.\n\nThe agents must take 2 arguments, \n\n* `obs`, which has information about the board (`obs.board`) and the player (`obs.mark`, either a player or the other)\n* `config`, which defines the board (rows and columns) and the rule to win (in this case, 4 in a row)\n\nThe agent will return the column number where the next piece will be played. Therefore, I expect the environment to know where the pieces are and to put the new piece in the only available spot given the column number.\n\nSo the random agent takes a random valid move. ","metadata":{}},{"cell_type":"code","source":"import random\nimport numpy as np\n\ndef agent_random(obs, config):\n    print(config)  # I want to see what is inside\n    print(obs.board)\n    valid_moves = [col for col in range(config.columns) if obs.board[col] == 0]\n    return random.choice(valid_moves)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see how it plays","metadata":{}},{"cell_type":"code","source":"# Agents play one game round\nenv.run([agent_random, agent_random])\n\n# Show the game\nenv.render(mode=\"ipython\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Alright, the config is just a dictionary coming from the environment, nothing special there. The board is simply a representation of the board as list, making a move corresponds to putting numbers in the list, 1 for a player, 2 for the other. Therefore, the first 7 zeros correspond to the top row of the board and thus it makes sense that a move is available if those entries are 0 because that would mean that the column is not full.\n\nI am curious about how to break this. Let's say I want to overflow the first column","metadata":{}},{"cell_type":"code","source":"def agent_overflow(obs, config):\n    print(obs.board)\n    return 0\n\nenv.run([agent_overflow, agent_overflow])\n\n# Show the game\nenv.render(mode=\"ipython\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Alright, the game stops, good to know, I might want to build some exeptions in case my agent goes crazy. The second way I want to break it, is to miss the board entirely","metadata":{}},{"cell_type":"code","source":"def agent_drunk(obs, config):\n    print(obs.board)\n    return 7 # which doesn't exist\n\nenv.run([agent_drunk, agent_overflow])\n\n# Show the game\nenv.render(mode=\"ipython\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nice, it breaks differently. Thus the list is just a representation of the board but the rules are enforced in a way that, even thoug the position 7 exists, it is not recognized as a valid move.\n\nThe function that makes everything happen is `run`","metadata":{}},{"cell_type":"code","source":"help(env.run)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Given what I saw from the env object, I would have expected play to be the function to play.","metadata":{}},{"cell_type":"code","source":"help(env.play)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env.play([agent_random, agent_random])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems to be the case, it just renders the game after it runs. However, it seems to call a method that the documentation doesn't mention. I will come back to it.\n\nTo evaluate an agent, the course suggests to run the game many times to get a percentage of wins","metadata":{}},{"cell_type":"code","source":"def get_win_percentages(agent1, agent2, n_rounds=100):\n    import time\n    from joblib import Parallel, delayed\n    from kaggle_environments import evaluate\n    import multiprocessing as mp\n \n    # Use default Connect Four setup\n    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n    \n    cores = mp.cpu_count()\n    half_rounds_per_core = n_rounds // cores // 2\n\n    def evaluate_per_core(half_rounds):\n        outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], half_rounds)\n        outcomes += [[b,a] for [a,b] in evaluate(\"connectx\", [agent2, agent1], config, [], half_rounds)]\n        return outcomes  \n    \n    start_time=time.time()\n    results = Parallel(n_jobs=mp.cpu_count())(delayed(evaluate_per_core)(half_rounds_per_core) for i in range(cores))\n    total_time=time.time()-start_time\n\n    outcomes = [result for results_per_job in results for result in results_per_job]\n\n    print(\"In total, {} episodes have been evaluated using {} CPU's cores.\".format(len(outcomes), cores))\n    print(\"Total time: {:.2f} minutes ({:.2f} seconds per match on average)\".format(total_time/60, total_time/n_rounds))\n    print(\"Agent 1 Won: {:.2%}\".format(outcomes.count([1,-1])/len(outcomes)))\n    print(\"Agent 2 Won: {:.2%}\".format(outcomes.count([-1,1])/len(outcomes)))\n    print(\"Ties:        {:.2%}\".format(outcomes.count([0,0])/len(outcomes)))\n    print(\"Invalid Plays by Agent 1:\", outcomes.count([None, 0]))\n    print(\"Invalid Plays by Agent 2:\", outcomes.count([0, None]))\n    \n    \ndef agent_random(obs, config):\n    valid_moves = [col for col in range(config.columns) if obs.board[col] == 0]\n    return random.choice(valid_moves)\n\n# Selects leftmost valid column\ndef agent_leftmost(obs, config):\n    valid_moves = [col for col in range(config.columns) if obs.board[col] == 0]\n    return valid_moves[0]\n\nget_win_percentages(agent1=agent_leftmost, agent2=agent_random)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The trick is done by a function I imported at the beginning, and I don't know what some of the inputs are but I don't like those lists as default arguments","metadata":{}},{"cell_type":"code","source":"help(evaluate)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Making better agents\n\nThe core functioning of the environment feels clear so far, which is a testament to the effort Kaggle put into it. Thanks to that, I can focus almost exclusively on creating a better agent. The first exercises suggests to create an agent that looks one move ahead to take a decision, which feels a good greedy approach (take the best move available now, defined as the one that gives the highest immediate reward).\n\nIntuitively, I think it should not be difficult to then extend this to looking several moves ahead. \n\nLet's go with order, the first example creates an agent that either picks a winning move, if available, or a random one. To do that, we need to simulate the board one step ahead and test if it is a winning board.","metadata":{}},{"cell_type":"code","source":"# Gets board at next step if agent drops piece in selected column\ndef drop_piece(grid, col, piece, config):\n    next_grid = grid.copy()\n    for row in range(config.rows-1, -1, -1):\n        if next_grid[row][col] == 0:\n            break\n    next_grid[row][col] = piece\n    return next_grid\n\n# Returns True if dropping piece in column results in game win\ndef check_winning_move(obs, config, col, piece):\n    # Convert the board to a 2D grid\n    grid = np.asarray(obs.board).reshape(config.rows, config.columns)\n    next_grid = drop_piece(grid, col, piece, config)\n    # horizontal\n    for row in range(config.rows):\n        for col in range(config.columns-(config.inarow-1)):\n            window = list(next_grid[row,col:col+config.inarow])\n            if window.count(piece) == config.inarow:\n                return True\n    # vertical\n    for row in range(config.rows-(config.inarow-1)):\n        for col in range(config.columns):\n            window = list(next_grid[row:row+config.inarow,col])\n            if window.count(piece) == config.inarow:\n                return True\n    # positive diagonal\n    for row in range(config.rows-(config.inarow-1)):\n        for col in range(config.columns-(config.inarow-1)):\n            window = list(next_grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n            if window.count(piece) == config.inarow:\n                return True\n    # negative diagonal\n    for row in range(config.inarow-1, config.rows):\n        for col in range(config.columns-(config.inarow-1)):\n            window = list(next_grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n            if window.count(piece) == config.inarow:\n                return True\n    return False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ok, with order because there is a lot going on here.\n\nThe board is a list of numbers, we reshape it to make it 2 dimensional","metadata":{}},{"cell_type":"code","source":"a = [0,0,0,0,0,0,0,0,0,0,0,0]\n\nnp.asarray(a).reshape(3, 4) # a simpler board ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then we drop a piece in the board","metadata":{}},{"cell_type":"code","source":"b = np.asarray(a).reshape(3, 4)\n\ndef drop_piece(grid, col, piece, config):\n    next_grid = grid.copy()\n    for row in range(config['rows']-1, -1, -1): # simplified this to work in this experiment\n        if next_grid[row][col] == 0:\n            break\n    next_grid[row][col] = piece\n    return next_grid\n\ndrop_piece(b, 1, 1, {'rows':3})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is a very reasonable thing, \n* try a move by picking a column, \n* check the rows position in this column backwards (thus starting from the highest index to the lowest), \n* stop at the first 0 you find because that's the spot the piece is going to be dropping, \n* fill the spot\n\nThen it just checks if the new board shows positions that have 4 pieces in a row and returns True if it does. It does so by moving a window of 4 in a row on the board and counting the numbers (so it is counting how many `1` or `-1` are in the window.\n\nAlright, then if I want an agent that plays with this strategy, I can do.","metadata":{}},{"cell_type":"code","source":"# need to redefine to work properly\ndef drop_piece(grid, col, piece, config):\n    next_grid = grid.copy()\n    for row in range(config.rows-1, -1, -1):\n        if next_grid[row][col] == 0:\n            break\n    next_grid[row][col] = piece\n    return next_grid","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def agent_winmove(obs, config):\n    valid_moves = [col for col in range(config.columns) if obs.board[col] == 0]\n    # for each valid move, check if it is a winning one\n    for col in valid_moves:\n        if check_winning_move(obs, config, col, obs.mark):\n            return col\n    # otherwise, pick randomly\n    return random.choice(valid_moves)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_win_percentages(agent1=agent_leftmost, agent2=agent_winmove)\nprint('_'*40)\nget_win_percentages(agent1=agent_random, agent2=agent_winmove)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So it wins just a bit more often against leftmost and it wins solidly against random. Good for winmove!\n\nLeftmost seems to be the easiest player to beat but apparently it is still outsmarting my agent. I need to make it capable of stopping a winning move.","metadata":{}},{"cell_type":"code","source":"def agent_winmove_nolose(obs, config):  # the naming might overstate the quality of the agent\n    valid_moves = [col for col in range(config.columns) if obs.board[col] == 0]\n    # for each valid move, check if it is a winning one\n    for col in valid_moves:\n        if check_winning_move(obs, config, col, obs.mark):\n            return col\n    # if no winning move is available, check if the other player has a winning move and stop it\n    for col in valid_moves:\n        if check_winning_move(obs, config, col, obs.mark%2+1): # this is the mark of the opponent\n            return col\n    # otherwise, pick randomly\n    return random.choice(valid_moves)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env.run([agent_leftmost, agent_winmove_nolose])\n\n# Show the game\nenv.render(mode=\"ipython\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems the days we lose against leftmost are over","metadata":{}},{"cell_type":"code","source":"get_win_percentages(agent1=agent_leftmost, agent2=agent_winmove_nolose)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"However, this agent can still lose if the other player has 2 options to win the game in one move. It feels I need to look further, but then the problem would present itself again when meeting an agent that looks 3 steps further and so on. The tutorial suggests another way to tackle the problem\n\n# Heuristics\n\nThis approach assigns points to game boards and make the move that gives the better score. I am guessing that the quality of my agent will heavily depends on how the points are assigned.\n\nThe first agent I will build following the tutorial is still looking only one step ahead, but it is also scoring how valuable is, for example, to get to 3 pieces in a row. With the right point assignment, the effect might reproduce better a player that is forecasting the game a bit more than by only one move.\n\nFirst, I need a function that creates a board after a piece is dropped, and we have that in `drop_piece` defined above. Then I need a function that counts how many pieces my agent and its opponent have in a specified window of the board.\n","metadata":{}},{"cell_type":"code","source":"def check_window(window, num_discs, piece, config):\n    return (window.count(piece) == num_discs and window.count(0) == config.inarow-num_discs)\n\ndef count_windows(grid, num_discs, piece, config):\n    num_windows = 0\n    # horizontal\n    for row in range(config.rows):\n        for col in range(config.columns-(config.inarow-1)):\n            window = list(grid[row, col:col+config.inarow])\n            if check_window(window, num_discs, piece, config):\n                num_windows += 1\n    # vertical\n    for row in range(config.rows-(config.inarow-1)):\n        for col in range(config.columns):\n            window = list(grid[row:row+config.inarow, col])\n            if check_window(window, num_discs, piece, config):\n                num_windows += 1\n    # positive diagonal\n    for row in range(config.rows-(config.inarow-1)):\n        for col in range(config.columns-(config.inarow-1)):\n            window = list(grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n            if check_window(window, num_discs, piece, config):\n                num_windows += 1\n    # negative diagonal\n    for row in range(config.inarow-1, config.rows):\n        for col in range(config.columns-(config.inarow-1)):\n            window = list(grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n            if check_window(window, num_discs, piece, config):\n                num_windows += 1\n    return num_windows","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Which is basically scanning the board for horizontal, vertical, and diagonal patterns, counting how many times it finds a given number of pieces in each window and returning the count. The window size is always of size 4 (given that we need to create 4 in a row) and `num_disc` is telling the function how many pieces of a given player to look for.","metadata":{}},{"cell_type":"code","source":"def get_heuristic(grid, mark, config):\n    num_threes = count_windows(grid, 3, mark, config)\n    num_fours = count_windows(grid, 4, mark, config)\n    num_threes_opp = count_windows(grid, 3, mark%2+1, config)\n    score = num_threes - 1e2*num_threes_opp + 1e6*num_fours\n    return score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Those numbers are something I can tune to find a better strategy. Now I should create a function that, given a move, it scores its board","metadata":{}},{"cell_type":"code","source":"def score_move(grid, col, mark, config):\n    next_grid = drop_piece(grid, col, mark, config)\n    score = get_heuristic(next_grid, mark, config)\n    return score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And I now have everythin from my next agent","metadata":{}},{"cell_type":"code","source":"def agent_heuristic(obs, config):\n    # Get list of valid moves\n    valid_moves = [c for c in range(config.columns) if obs.board[c] == 0]\n    # Convert the board to a 2D grid\n    grid = np.asarray(obs.board).reshape(config.rows, config.columns)\n    # Use the heuristic to assign a score to each possible board in the next turn\n    scores = dict(zip(valid_moves, [score_move(grid, col, obs.mark, config) for col in valid_moves]))\n    # Get a list of columns (moves) that maximize the heuristic\n    max_cols = [key for key in scores.keys() if scores[key] == max(scores.values())]\n    # Select at random from the maximizing columns\n    return random.choice(max_cols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see how it performs against the best agent we have so far","metadata":{}},{"cell_type":"code","source":"env.run([agent_heuristic, agent_winmove_nolose])\n\n# Show the game\nenv.render(mode=\"ipython\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_win_percentages(agent1=agent_heuristic, agent2=agent_winmove_nolose)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Which is a decent performance. I wonder what happens if I had more rules, for example, can I make it more aggressive? or more conservative? It feels I can try several different combination of scores and eventually find a better one but it starts feeling as something that should be learned by playing the game already.","metadata":{}},{"cell_type":"code","source":"def get_heuristic_alt(grid, mark, config):\n    num_twos = count_windows(grid, 2, mark, config)\n    num_threes = count_windows(grid, 3, mark, config)\n    num_fours = count_windows(grid, 4, mark, config)\n    num_twos_opp = count_windows(grid, 2, mark%2+1, config)\n    num_threes_opp = count_windows(grid, 3, mark%2+1, config)\n    score = 1e6*num_fours + 10*num_threes + 1*num_twos - 1*num_twos_opp - 1e3*num_threes_opp\n    return score\n\ndef score_move_alt(grid, col, mark, config):\n    next_grid = drop_piece(grid, col, mark, config)\n    score = get_heuristic_alt(next_grid, mark, config)\n    return score\n\ndef agent_heuristic_alt(obs, config):\n    # Get list of valid moves\n    valid_moves = [c for c in range(config.columns) if obs.board[c] == 0]\n    # Convert the board to a 2D grid\n    grid = np.asarray(obs.board).reshape(config.rows, config.columns)\n    # Use the heuristic to assign a score to each possible board in the next turn\n    scores = dict(zip(valid_moves, [score_move_alt(grid, col, obs.mark, config) for col in valid_moves]))\n    # Get a list of columns (moves) that maximize the heuristic\n    max_cols = [key for key in scores.keys() if scores[key] == max(scores.values())]\n    # Select at random from the maximizing columns\n    return random.choice(max_cols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_win_percentages(agent1=agent_heuristic, agent2=agent_heuristic_alt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before diving in how to make my agent learn, it is worth to explore a more complex use of heuristic\n\n# MinMax algorithm\n\nIf I start making my agent looking ahead one step, I wonder then how to make it look several steps ahead. This will likely include guessing what the opponent will do after my agent's move. Moreover, I wonder how to enforce a strategy that not only picks the best move at this moment (which is what I did so far), but also the best move to eventually win the game.\n\nThe **MinMax algorithm** choses the moves that increases the score of the agent the most and assumes the opponent will chose the move that lowers the score the most.\n\nLet's start with a simple heuristic.","metadata":{}},{"cell_type":"code","source":"def get_heuristic(grid, mark, config):\n    num_threes = count_windows(grid, 3, mark, config)\n    num_fours = count_windows(grid, 4, mark, config)\n    num_threes_opp = count_windows(grid, 3, mark%2+1, config)\n    num_fours_opp = count_windows(grid, 4, mark%2+1, config)\n    score = num_threes - 1e2*num_threes_opp - 1e4*num_fours_opp + 1e6*num_fours\n    return score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Following the [Wikipedia's explanation](https://en.wikipedia.org/wiki/Minimax), I need a few functions. \n\nThe first one checks if a game has ended.","metadata":{}},{"cell_type":"code","source":"def is_terminal_window(window, config):\n    return window.count(1) == config.inarow or window.count(2) == config.inarow\n\ndef is_terminal_node(grid, config):\n    # Check for draw \n    if list(grid[0, :]).count(0) == 0:\n        return True\n    # Check for win: horizontal, vertical, or diagonal\n    # horizontal \n    for row in range(config.rows):\n        for col in range(config.columns-(config.inarow-1)):\n            window = list(grid[row, col:col+config.inarow])\n            if is_terminal_window(window, config):\n                return True\n    # vertical\n    for row in range(config.rows-(config.inarow-1)):\n        for col in range(config.columns):\n            window = list(grid[row:row+config.inarow, col])\n            if is_terminal_window(window, config):\n                return True\n    # positive diagonal\n    for row in range(config.rows-(config.inarow-1)):\n        for col in range(config.columns-(config.inarow-1)):\n            window = list(grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n            if is_terminal_window(window, config):\n                return True\n    # negative diagonal\n    for row in range(config.inarow-1, config.rows):\n        for col in range(config.columns-(config.inarow-1)):\n            window = list(grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n            if is_terminal_window(window, config):\n                return True\n    return False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then the MinMax implementation will look like this","metadata":{}},{"cell_type":"code","source":"def minimax(node, depth, maximizingPlayer, mark, config):\n    is_terminal = is_terminal_node(node, config)\n    valid_moves = [c for c in range(config.columns) if node[0][c] == 0]\n    if depth == 0 or is_terminal:\n        return get_heuristic(node, mark, config)\n    if maximizingPlayer:\n        value = -np.Inf\n        for col in valid_moves:\n            child = drop_piece(node, col, mark, config)\n            value = max(value, minimax(child, depth-1, False, mark, config))\n        return value\n    else:\n        value = np.Inf\n        for col in valid_moves:\n            child = drop_piece(node, col, mark%2+1, config)\n            value = min(value, minimax(child, depth-1, True, mark, config))\n        return value","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In other words\n\n* if we reached the maximum depth (e.g. when we look a predetermined number of steps ahead) or when the game is over, get the heuristic and thus score the board\n* if we are trying to maximize the score, for every valid move, call recursively the function by reducing depth at each step and maximize the value of the board\n* otherwise (e.g. when guessing the opponent's move) do the same but minimize the score\n* note that in the recursion, we alternate the times we try to maximize or minimize the score (simulating one move per player)\n\nBefore continuing, it is not difficult to imagine that this algorithm will run for a long time. To speed up, I can also implement [the alpha-beta pruning](https://en.wikipedia.org/wiki/Alpha%E2%80%93beta_pruning). It is a pruning method and it aims to stop evaluating nodes when there is a proof that those moves are worse than a move previously evaluated.","metadata":{}},{"cell_type":"code","source":"def minimax_ab(node, depth, a, b, maximizingPlayer, mark, config):\n    is_terminal = is_terminal_node(node, config)\n    valid_moves = [c for c in range(config.columns) if node[0][c] == 0]\n    if depth == 0 or is_terminal:\n        return get_heuristic(node, mark, config)\n    if maximizingPlayer:\n        value = -np.Inf\n        for col in valid_moves:\n            child = drop_piece(node, col, mark, config)\n            value = max(value, minimax_ab(child, depth-1, a, b, False, mark, config))\n            a = max(a, value)\n            if a >= b:\n                break # β cutoff\n        return value\n    else:\n        value = np.Inf\n        for col in valid_moves:\n            child = drop_piece(node, col, mark%2+1, config)\n            value = min(value, minimax_ab(child, depth-1, a, b, True, mark, config))\n            b = min(b, value)\n            if b <= a:\n                break # α cutoff\n        return value","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then we need again a function to score the board","metadata":{}},{"cell_type":"code","source":"def score_move(grid, col, mark, config, nsteps):\n    next_grid = drop_piece(grid, col, mark, config)\n    score = minimax_ab(next_grid, nsteps-1, -np.Inf, np.Inf, False, mark, config)\n    return score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The agent will then look like this","metadata":{}},{"cell_type":"code","source":"def agent_minmax3(obs, config):\n    # Get list of valid moves\n    valid_moves = [c for c in range(config.columns) if obs.board[c] == 0]\n    # Convert the board to a 2D grid\n    grid = np.asarray(obs.board).reshape(config.rows, config.columns)\n    # Use the heuristic to assign a score to each possible board in the next step\n    scores = dict(zip(valid_moves, [score_move(grid, col, obs.mark, config, 3) for col in valid_moves]))\n    # Get a list of columns (moves) that maximize the heuristic\n    max_cols = [key for key in scores.keys() if scores[key] == max(scores.values())]\n    # Select at random from the maximizing columns\n    return random.choice(max_cols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In other words,\n\n* For each valid move, start by simulating the new board \n* Score the new board (1 step ahead) with the minmax algorithm. This will simulate the opponent move and then one more move for the player (because we made it look 3 steps ahead).\n* Return the score of each valid move and pick the move with the highest score\n\nSo MinMax is still scoring the next move, but it is looking ahead a given amount of steps to do so.","metadata":{}},{"cell_type":"code","source":"env.run([agent_heuristic_alt, agent_minmax3])\n\n# Show the game\nenv.render(mode=\"ipython\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_win_percentages(agent1=agent_heuristic_alt, agent2=agent_minmax3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our new agent is often winning against our previous champion. I want to start making some submissions for the LB.","metadata":{}},{"cell_type":"code","source":"# redefining as one function for the submission\n\ndef agent_minmax3(obs, config):\n    import numpy as np\n    import random\n    \n    def drop_piece(grid, col, piece, config):\n        next_grid = grid.copy()\n        for row in range(config.rows-1, -1, -1):\n            if next_grid[row][col] == 0:\n                break\n        next_grid[row][col] = piece\n        return next_grid\n    \n    def check_window(window, num_discs, piece, config):\n        return (window.count(piece) == num_discs and window.count(0) == config.inarow-num_discs)\n\n    def count_windows(grid, num_discs, piece, config):\n        num_windows = 0\n        # horizontal\n        for row in range(config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[row, col:col+config.inarow])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        # vertical\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns):\n                window = list(grid[row:row+config.inarow, col])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        # positive diagonal\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        # negative diagonal\n        for row in range(config.inarow-1, config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        return num_windows\n    \n    def score_move(grid, col, mark, config, nsteps):\n        next_grid = drop_piece(grid, col, mark, config)\n        score = minimax_ab(next_grid, nsteps-1, -np.Inf, np.Inf, False, mark, config)\n        return score\n    \n    def minimax_ab(node, depth, a, b, maximizingPlayer, mark, config):\n        is_terminal = is_terminal_node(node, config)\n        valid_moves = [c for c in range(config.columns) if node[0][c] == 0]\n        if depth == 0 or is_terminal:\n            return get_heuristic(node, mark, config)\n        if maximizingPlayer:\n            value = -np.Inf\n            for col in valid_moves:\n                child = drop_piece(node, col, mark, config)\n                value = max(value, minimax_ab(child, depth-1, a, b, False, mark, config))\n                a = max(a, value)\n                if a >= b:\n                    break # β cutoff\n            return value\n        else:\n            value = np.Inf\n            for col in valid_moves:\n                child = drop_piece(node, col, mark%2+1, config)\n                value = min(value, minimax_ab(child, depth-1, a, b, True, mark, config))\n                b = min(b, value)\n                if b <= a:\n                    break # α cutoff\n            return value\n    \n    def is_terminal_window(window, config):\n        return window.count(1) == config.inarow or window.count(2) == config.inarow\n\n    def is_terminal_node(grid, config):\n        # Check for draw \n        if list(grid[0, :]).count(0) == 0:\n            return True\n        # Check for win: horizontal, vertical, or diagonal\n        # horizontal \n        for row in range(config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[row, col:col+config.inarow])\n                if is_terminal_window(window, config):\n                    return True\n        # vertical\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns):\n                window = list(grid[row:row+config.inarow, col])\n                if is_terminal_window(window, config):\n                    return True\n        # positive diagonal\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n                if is_terminal_window(window, config):\n                    return True\n        # negative diagonal\n        for row in range(config.inarow-1, config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n                if is_terminal_window(window, config):\n                    return True\n        return False\n    \n    def get_heuristic(grid, mark, config):\n        num_threes = count_windows(grid, 3, mark, config)\n        num_fours = count_windows(grid, 4, mark, config)\n        num_threes_opp = count_windows(grid, 3, mark%2+1, config)\n        num_fours_opp = count_windows(grid, 4, mark%2+1, config)\n        score = num_threes - 1e2*num_threes_opp - 1e5*num_fours_opp + 1e9*num_fours\n        return score\n    \n    # Get list of valid moves\n    valid_moves = [c for c in range(config.columns) if obs.board[c] == 0]\n    # Convert the board to a 2D grid\n    grid = np.asarray(obs.board).reshape(config.rows, config.columns)\n    # Use the heuristic to assign a score to each possible board in the next step\n    scores = dict(zip(valid_moves, [score_move(grid, col, obs.mark, config, 3) for col in valid_moves]))\n    # Get a list of columns (moves) that maximize the heuristic\n    max_cols = [key for key in scores.keys() if scores[key] == max(scores.values())]\n    # Select at random from the maximizing columns\n    return random.choice(max_cols)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# redefining as one function for the submission\n\ndef agent_heuristic_alt(obs, config):\n    import numpy as np\n    import random\n    \n    def drop_piece(grid, col, piece, config):\n        next_grid = grid.copy()\n        for row in range(config.rows-1, -1, -1):\n            if next_grid[row][col] == 0:\n                break\n        next_grid[row][col] = piece\n        return next_grid    \n    \n    def check_window(window, num_discs, piece, config):\n        return (window.count(piece) == num_discs and window.count(0) == config.inarow-num_discs)\n\n    def count_windows(grid, num_discs, piece, config):\n        num_windows = 0\n        # horizontal\n        for row in range(config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[row, col:col+config.inarow])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        # vertical\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns):\n                window = list(grid[row:row+config.inarow, col])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        # positive diagonal\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        # negative diagonal\n        for row in range(config.inarow-1, config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        return num_windows\n    \n    def get_heuristic_alt(grid, mark, config):\n        num_twos = count_windows(grid, 2, mark, config)\n        num_threes = count_windows(grid, 3, mark, config)\n        num_fours = count_windows(grid, 4, mark, config)\n        num_twos_opp = count_windows(grid, 2, mark%2+1, config)\n        num_threes_opp = count_windows(grid, 3, mark%2+1, config)\n        if num_threes_opp > 1:\n            mult_threes_opp = 1e5\n        else:\n            mult_threes_opp = 1e3\n        if num_threes > 1:\n            mult_threes = 1e4\n        else:\n            mult_threes = 10\n        score = 1e6*num_fours + mult_threes*num_threes + 1*num_twos - 1*num_twos_opp - mult_threes_opp*num_threes_opp\n        return score\n\n    def score_move_alt(grid, col, mark, config):\n        next_grid = drop_piece(grid, col, mark, config)\n        score = get_heuristic_alt(next_grid, mark, config)\n        return score\n    \n    \n    # Get list of valid moves\n    valid_moves = [c for c in range(config.columns) if obs.board[c] == 0]\n    # Convert the board to a 2D grid\n    grid = np.asarray(obs.board).reshape(config.rows, config.columns)\n    # Use the heuristic to assign a score to each possible board in the next turn\n    scores = dict(zip(valid_moves, [score_move_alt(grid, col, obs.mark, config) for col in valid_moves]))\n    # Get a list of columns (moves) that maximize the heuristic\n    max_cols = [key for key in scores.keys() if scores[key] == max(scores.values())]\n    # Select at random from the maximizing columns\n    return random.choice(max_cols)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import inspect\nimport os\n\ndef write_agent_to_file(function, file):\n    with open(file, \"a\" if os.path.exists(file) else \"w\") as f:\n        f.write(inspect.getsource(function))\n        print(function, \"written to\", file)\n\nwrite_agent_to_file(agent_minmax3, \"minmax3_agent.py\")\nwrite_agent_to_file(agent_heuristic_alt, \"heuristic_agent.py\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Deep Reinforcement Learning\n\nSo far, I can see I can tell my agent how to pick a move. In principle, I can look ahead dozens of moves to pick the best move but it will be increasingly slow and the strategy might be very inefficient.\n\nThe basic concept of reinforcement learning I can leverage is instead to let the agent play the game, record if it wins or loses, and backpropagate this information to inform future games and have it making better decisions.\n\nIn this way, I would only need to score wins, losses, and draws rather than every possible configuration of the board.\n\nI must say I am not a comfortable jumping into deep learning right now as it is not clear to me how to train and monitor what my agent is going to do. Google can help me finding the basic concept (or what looks like a building block of what I want to achieve).\n\n## Q-Learning (and simpler games)\n\n[Q-Learning](https://en.wikipedia.org/wiki/Q-learning) is a technique to learn the value of an action in a particular state. The goal is thus to create a policy that, given the configuration of the environment, tells what is the best action to take. The interesting thing is that it does not require a model of the environment. The algorithm works by looking several steps ahead (for example, it can just look till the end of the game), receive a reward for the outcome of those steps, and backpropagate this information to each of the steps.\n\nIn other words, \n* I create a dictionary Q that, for each possible state, has a value for each of the possible actions. At first the values are random (or 0)\n* I let the agent play and record each state and action. \n* At the end of the game, I know if the agent won or lost and give it a reward accordingly.\n* I update the dictionary by using the [Bellman equation](https://en.wikipedia.org/wiki/Bellman_equation) to essentially pass the information that *with this set of actions in these situations you won/lost the game*\n* Eventually, the iterative process converges to the optimal policy, e.g. for each configuration there is an optimal action to win the game.\n\nThe simplest application of this concept I could find is to create an agent able to solve a small grid world puzzle. The rules are simple, there is a world on a grid. You start on a cell, you win if you end up on another cell, you lose if you end up on yet another cell, and there might be cells that are simply unaccessible. A well trained agent should be able to reach the winning cell via the shortest path.\n\n[The code](https://github.com/lucabasa/reinforcement_learning_training/tree/main/GridWorld) is inspired by [this article](https://towardsdatascience.com/reinforcement-learning-implement-grid-world-from-scratch-c5963765ebff) and hidden for readibility.","metadata":{}},{"cell_type":"code","source":"class State:\n    def __init__(self, rows=3, cols=4, \n                 state=None, # default ignored in the init\n                 win_state=None, lose_state=None, # default ignored in the init\n                 forbidden_blocks=None,\n                 deterministic=True, **kwargs):\n        \n        if win_state is None:\n            win_state = (0, cols-1)\n        if lose_state is None:\n            lose_state = (1, cols-1)\n        if state is None:\n            state = (rows-1, 0)\n        if forbidden_blocks is None:\n            forbidden_blocks = []\n        \n        self.rows = rows\n        self.cols = cols\n        self.win_state = win_state\n        self.lose_state = lose_state\n        self.state = state\n        self.board = np.zeros([rows, cols])\n        \n        if not isinstance(forbidden_blocks, list):\n            forbidden_blocks = [forbidden_blocks]\n        if self.win_state in forbidden_blocks:\n            forbidden_blocks = [b for b in forbidden_blocks if b != self.win_state]\n        if self.lose_state in forbidden_blocks:\n            forbidden_blocks = [b for b in forbidden_blocks if b != self.lose_state]\n        if self.state in forbidden_blocks:\n            forbidden_blocks = [b for b in forbidden_blocks if b != self.state] \n        for block in forbidden_blocks:\n            self.board[block] = -1  # Forbidden block \n        self.forbidden_blocks = forbidden_blocks \n        \n        self.isEnd = False\n        self.deterministic = deterministic\n        \n        \n    def giveReward(self):\n        if self.state == self.win_state:\n            return 1\n        elif self.state == self.lose_state:\n            return -1\n        else:\n            return 0\n    \n    \n    def isEndFunc(self):\n        if (self.state == self.win_state) or (self.state == self.lose_state):\n            self.isEnd = True\n    \n    \n    def _chooseActionProb(self, action):\n        # For each action, there is a 10% probability of going in a perpendicular direction\n        if action == \"up\":\n            return np.random.choice([\"up\", \"left\", \"right\"], p=[0.8, 0.1, 0.1])\n        if action == \"down\":\n            return np.random.choice([\"down\", \"left\", \"right\"], p=[0.8, 0.1, 0.1])\n        if action == \"left\":\n            return np.random.choice([\"left\", \"up\", \"down\"], p=[0.8, 0.1, 0.1])\n        if action == \"right\":\n            return np.random.choice([\"right\", \"up\", \"down\"], p=[0.8, 0.1, 0.1])\n    \n    \n    def nxtPosition(self, action):\n        \"\"\"\n        action: up, down, left, right\n        -------------\n        0 | 1 | 2| 3|\n        1 |\n        2 |\n        return next position\n        \"\"\"\n        if self.deterministic:\n            if action == \"up\":\n                nxtState = (self.state[0]-1, self.state[1])\n            elif action == \"down\":\n                nxtState = (self.state[0]+1, self.state[1])\n            elif action == \"left\":\n                nxtState = (self.state[0], self.state[1]-1)\n            else:\n                nxtState = (self.state[0], self.state[1]+1)\n        else:\n            # non-deterministic\n            action = self._chooseActionProb(action)\n            self.deterministic = True\n            nxtState = self.nxtPosition(action)\n            self.deterministic = False\n            \n        # if next state legal\n        if (nxtState[0] >= 0) and (nxtState[0] <= self.rows-1):  # board boundaries\n            if (nxtState[1] >= 0) and (nxtState[1] <= self.cols-1): \n                if nxtState not in self.forbidden_blocks:  # forbidden block\n                    return nxtState\n        return self.state\n    \n    \n    def showBoard(self):\n        self.board[self.state] = 1\n        for i in range(0, self.rows):\n            print('-----'*(self.cols-1)+'--')\n            out = '| '\n            for j in range(0, self.cols):\n                if self.board[i, j] == 1:\n                    token = '*'\n                if self.board[i, j] == -1:\n                    token = 'z'\n                if self.board[i, j] == 0:\n                    token = '0'\n                out += token + ' | '\n            print(out)\n        print('-----'*(self.cols-1)+'--')\n        \n        \nclass Agent:\n    def __init__(self, *, state=None, lr=0.2, exp_rate=0.3, verbose=False):\n        self.states = []\n        self.actions = [\"up\", \"down\", \"left\", \"right\"]\n        if state is None:\n            state = State()\n        self.State = state\n        self.isEnd = self.State.isEnd\n        self.lr = lr\n        self.exp_rate = exp_rate\n        self.orig_state = state\n        self.verbose = verbose\n        \n        np.random.seed(234)\n        \n        # initial state reward\n        self.state_values = {}\n        for i in range(state.rows):  \n            for j in range(state.cols):\n                self.state_values[(i, j)] = 0  # init_reward[i, j]\n    \n    \n    def chooseAction(self):\n        # choose action with most expected value\n        mx_nxt_reward = 0\n        action = \"\"\n        \n        if np.random.uniform(0, 1) <= self.exp_rate:\n            action = np.random.choice(self.actions)\n        else:\n            # greedy action\n            for a in self.actions:\n                # if the action is deterministic\n                nxt_reward = self.state_values[self.State.nxtPosition(a)]\n                if nxt_reward > mx_nxt_reward:\n                    action = a\n                    mx_nxt_reward = nxt_reward\n            # print(\"current pos: {}, greedy aciton: {}\".format(self.State.state, action))\n        if action == '':\n            action = np.random.choice(self.actions)\n        return action\n    \n    \n    def takeAction(self, action):\n        position = self.State.nxtPosition(action)\n        st_dict = self.State.__dict__.copy()\n        st_dict['state'] = position\n        return State(**st_dict)    \n    \n    \n    def reset(self):\n        self.states = []\n        self.State = self.orig_state\n        self.isEnd = self.State.isEnd\n        \n        \n    def _backpropagate(self):\n        # back propagate\n        reward = self.State.giveReward()\n        # explicitly assign end state to reward values\n        self.state_values[self.State.state] = reward\n        if self.verbose:\n            print(\"Game End Reward\", reward)\n        for s in reversed(self.states):\n            reward = self.state_values[s] + self.lr * (reward - self.state_values[s])\n            self.state_values[s] = round(reward, 3)\n        self.reset()\n        \n    \n    def _find_solution(self):\n        action = self.chooseAction()\n        # append trace\n        self.states.append(self.State.nxtPosition(action))\n        if self.verbose:\n            print(\"current position {} action {}\".format(self.State.state, action))\n        # by taking the action, it reaches the next state\n        self.State = self.takeAction(action)\n        # mark is end\n        self.State.isEndFunc()\n        if self.verbose:\n            print(\"nxt state\", self.State.state)\n            print(\"---------------------\")\n        self.isEnd = self.State.isEnd\n    \n    \n    def train(self, rounds=10):\n        i = 0\n        while i < rounds:\n            # to the end of game back propagate reward\n            if self.State.isEnd:\n                self._backpropagate()\n                i += 1\n            else:\n                self._find_solution()\n                \n        self.reset()\n    \n    \n    def play(self, max_steps=100):\n        # We don't need it to explore anymore\n        self.exp_rate_bak = self.exp_rate\n        self.exp_rate = 0\n        self.verbose_bak = self.verbose\n        self.verbose = True\n        i = 0\n        while i < max_steps:\n            if self.State.isEnd:\n                print(f'Solution found in {i} steps')\n                self.exp_rate = self.exp_rate_bak\n                self.verbose = self.verbose_bak\n                self.reset()\n                break\n            else:\n                self._find_solution()\n                i += 1\n    \n    \n    def showValues(self):\n        for i in range(0, self.State.rows):\n            print('----------------------------------')\n            out = '| '\n            for j in range(0, self.State.cols):\n                out += str(self.state_values[(i, j)]) + ' | '\n            print(out)\n        print('----------------------------------')\n        \n\n\nclass Agent_Q(Agent):\n    def __init__(self, *, state=None, decay_gamma=0.9, lr=0.2, exp_rate=0.3, verbose=False):\n        super().__init__(lr=lr, exp_rate=exp_rate, verbose=verbose)\n        if state is None:\n            state = State(deterministic=False)\n        self.State = state\n        self.decay_gamma = decay_gamma\n        self.orig_state = state\n        \n        np.random.seed(234)\n        \n        # initial Q values\n        self.Q_values = {}\n        for i in range(self.State.rows):\n            for j in range(self.State.cols):\n                self.Q_values[(i, j)] = {}\n                for a in self.actions:\n                    self.Q_values[(i, j)][a] = 0  # Q value is a dict of dict\n    \n    \n    def chooseAction(self):\n        # choose action with most expected value\n        mx_nxt_reward = 0\n        action = \"\"\n\n        if np.random.uniform(0, 1) <= self.exp_rate:\n            action = np.random.choice(self.actions)\n        else:\n            # greedy action\n            for a in self.actions:\n                current_position = self.State.state\n                nxt_reward = self.Q_values[current_position][a]\n                if nxt_reward > mx_nxt_reward:\n                    action = a\n                    mx_nxt_reward = nxt_reward\n            # print(\"current pos: {}, greedy aciton: {}\".format(self.State.state, action))\n        if action == '':\n            action = np.random.choice(self.actions)\n        return action\n    \n    \n    def _backpropagate(self):\n        # back propagate\n        reward = self.State.giveReward()\n        for a in self.actions:\n            self.Q_values[self.State.state][a] = reward\n        if self.verbose:\n            print(\"Game End Reward\", reward)\n        for s in reversed(self.states):\n            current_q_value = self.Q_values[s[0]][s[1]]\n            reward = current_q_value + self.lr * (self.decay_gamma * reward - current_q_value)\n            self.Q_values[s[0]][s[1]] = round(reward, 3)\n        self.reset()\n        \n    \n    def _find_solution(self):\n        action = self.chooseAction()\n        # append trace\n        self.states.append([(self.State.state), action])\n        if self.verbose:\n            print(\"current position {} action {}\".format(self.State.state, action))\n        # by taking the action, it reaches the next state\n        self.State = self.takeAction(action)\n        # mark is end\n        self.State.isEndFunc()\n        if self.verbose:\n            print(\"nxt state\", self.State.state)\n            print(\"---------------------\")\n        self.isEnd = self.State.isEnd\n        \n\n    def showValues(self):\n        for pos in self.Q_values.keys():\n            print(pos, self.Q_values[pos])","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's start with the board. We start on the bottom left, the winning cell is by default on the top right, and there are 2 forbidden cells.","metadata":{}},{"cell_type":"code","source":"s = State(forbidden_blocks=[(1, 1), (1,2)])\ns.showBoard()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If I move left, it moves","metadata":{}},{"cell_type":"code","source":"s.state = s.nxtPosition('right')\ns.showBoard()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If I move up from there, I can't because that cell is forbidden","metadata":{}},{"cell_type":"code","source":"s.state = s.nxtPosition('up')\ns.showBoard()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First, I can put an agent that makes use of value iteration to learn the optimal policy. In short, I make an agent that has one value for each of the cells.","metadata":{}},{"cell_type":"code","source":"s = State(rows=5, cols=5, forbidden_blocks=[(1,2), (2,3), (3,4)])\ns.showBoard()\nag = Agent(state=s)\nag.showValues()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The agent doesn't know what are the forbidden blocks (well, it does via the board that is provided) and the board will only tell it when it wins or loses by awarding +1 or -1.\n\nI let the agent train and I backpropagate the reward on each cell. Eventually, I get the following values","metadata":{}},{"cell_type":"code","source":"ag.train(20)\nag.showValues()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Given these values and following the intuition we can get by looking at the board, the agent will go up till the end of the board, then right (or turn right for a cell once before reaching the top of the board). There is a method to apply the learnings and find the right path, let's see","metadata":{}},{"cell_type":"code","source":"ag.play()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There we go. This is however not quite I wanted. I want an agent that maps the state of the board to an action. Here I can finally implement Q-learning for this problem.","metadata":{}},{"cell_type":"code","source":"s = State(rows=5, cols=5, forbidden_blocks=[(1,2), (2,3), (3,4)])\ns.showBoard()\nag = Agent_Q(state=s)\nag.showValues()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At this stage, the agent has no reason to prefer an action over another when it finds itself on a cell. The training is very similar, except that now we give a value to an action at a given cell","metadata":{}},{"cell_type":"code","source":"ag.train(20)\nag.Q_values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ag.play()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"However, this method has a big problem: in the game I want to solve the number of states is very high (4 quadrillion or something), there is no way I can really map each of them to a set of actions.\n\n## Deep Q-Learning","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}