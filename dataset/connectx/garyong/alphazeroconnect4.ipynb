{"cells":[{"metadata":{},"cell_type":"markdown","source":"# AlphaZero Connect4\n## References \nTried to implement using reference from:\n\nhttps://web.stanford.edu/~surag/posts/alphazero.html\n\nhttps://github.com/suragnair/alpha-zero-general\n\nCode might be a abit messy. But should be clear overall how it works","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport copy\nimport random\nfrom pprint import pprint\nimport math\nfrom collections import defaultdict,deque\nfrom tqdm.autonotebook import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom joblib import Parallel, delayed\nfrom collections import deque\nimport gc","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def create_board():\n    return [[0 for i in range(7)] for j in range(6)]\n\ndef reshape4(arr):\n    \"\"\"\n    reshape into 6x7 without numpy \n    \"\"\"\n    line1 = arr[0:7]\n    line2 = arr[7:14]\n    line3 = arr[14:21]\n    line4 = arr[21:28]\n    line5 = arr[28:35]\n    line6 = arr[35:42]\n    board = [line1, line2 , line3, line4, line5, line6] \n    return board\n\ndef drop_piece(board,col,mark):\n    \"\"\"\n    drop piece at next position\n    \"\"\"\n    board = copy.deepcopy(board)\n    for row in range(6-1, -1, -1):\n        if board[row][col] == 0:\n            break\n    board[row][col] = mark\n    return board\n\ndef get_valid_actions(board):\n    \"\"\"\n    get possible valid actions\n    \"\"\"\n    return [c for c in range(0,7) if board[0][c]==0]\n\ndef transform_board(board,a=1,b=2,f=1,s=-1):\n    \"\"\"\n    Map 1 -> -1 and 2 -> 1\n    This is done so that we can easily get the canonical board easily\n    \"\"\"\n    return_board = copy.deepcopy(board)\n    for i in range(len(board)):\n        for j in range(len(board[0])):\n            curr = board[i][j]\n            if curr == a:\n                return_board[i][j] = f\n            elif curr == b:\n                return_board[i][j] = s\n    return return_board\n\ndef inverse_transform(board):\n    return transform_board(board,1,-1,1,2)\n\ndef check_winner(board):\n    # Check rows for winner\n    for row in range(6):\n        for col in range(4):\n            if (board[row][col] == board[row][col + 1] == board[row][col + 2] ==\\\n                board[row][col + 3]) and (board[row][col] != 0):\n                return board[row][col]  #Return Number that match row\n\n    # Check columns for winner\n    for col in range(7):\n        for row in range(3):\n            if (board[row][col] == board[row + 1][col] == board[row + 2][col] ==\\\n                board[row + 3][col]) and (board[row][col] != 0):\n                return board[row][col]  #Return Number that match column\n\n    # Check diagonal (top-left to bottom-right) for winner\n\n    for row in range(3):\n        for col in range(4):\n            if (board[row][col] == board[row + 1][col + 1] == board[row + 2][col + 2] ==\\\n                board[row + 3][col + 3]) and (board[row][col] != 0):\n                return board[row][col] #Return Number that match diagonal\n\n\n    # Check diagonal (bottom-left to top-right) for winner\n\n    for row in range(5, 2, -1):\n        for col in range(4):\n            if (board[row][col] == board[row - 1][col + 1] == board[row - 2][col + 2] ==\\\n                board[row - 3][col + 3]) and (board[row][col] != 0):\n                return board[row][col] #Return Number that match diagonal\n    c = 0\n    for col in range(7):\n        if board[0][col]!=0:\n            c +=1\n    if c == 7:\n        # This is a draw\n        return \"draw\"\n    \n    # No winner: return None\n    return None\n\ndef get_random_agent():\n    def ra(board):\n        return random.choice(get_valid_actions(board))\n    return ra","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Game:\n    def __init__(self):\n        self.board = create_board()\n        \n    def get_action_size(self):\n        return 7\n    \n    def get_next_state(self,board,player,action):\n        board = drop_piece(board,action,player)\n        return board,-player\n    \n    def get_valid_actions(self,board):\n        return get_valid_actions(board)\n    \n    def get_game_ended(self, board, player):\n        result = check_winner(board)\n        # Win\n        if result == player:\n            return 1\n        # Lose\n        elif result == -player:\n            return -1\n        #Draw\n        elif result == 'draw':\n            return 1e-5\n        # Continue playing game\n        elif result == None:\n            return 0\n        else:\n            print(\"error\")\n        \n    def get_canonical_form(self, board, player):\n        # Flip player from 1 to -1 so that NN plays from the 'same' board\n        return (np.array(board) * player).tolist()\n\n    def get_symmetry(self, board, pi):\n        \"\"\"Board is left/right board symmetric\"\"\"\n        reverse_board = [reversed(row) for row in board]\n        return [(board, pi), (reverse_board, pi[::-1])]\n\n    def to_string(self, board):\n        return str(board)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MCTS:\n    def __init__(self,game,nnet,device=None):\n        \"\"\"\n        helper class for Monte Carlo Tree Search\n        \"\"\"\n        self.visited = set()\n        self.game = game\n        self.nnet = nnet\n        self.P = {}\n        self.Qsa = defaultdict(int)\n        self.Ns = defaultdict(int)\n        self.Nsa = defaultdict(int)\n        self.c_puct = 1\n        self.alpha = 1.4\n        if device == None:\n#             self.device = torch.device(\"cpu\")\n            self.device = torch.device( \"cuda\" if torch.cuda.is_available() else \"cpu\" )\n        else:\n            self.device = device\n            \n    def search(self,s):\n        \"\"\"\n        Note negative reward is returned as \n        âˆ’v is the value of the current board from the perspective of the other player\n        \"\"\"\n        self.nnet.eval()\n        self.nnet.to(self.device)\n        reward = self.game.get_game_ended(s,1)\n        if reward!=0: \n            # if game has ended return reward\n            return -reward\n        \n        if str(s) not in self.visited:\n            self.visited.add(str(s))\n            ps, v = self.nnet.predict(torch.tensor(s).float().to(self.device))\n            self.P[str(s)] = ((0.75 * ps) + (0.25 * np.random.dirichlet([self.alpha]) * len(ps))).tolist()\n            return -v.item()\n\n        max_u, best_a = -float(\"inf\"), -1\n        for a in self.game.get_valid_actions(s):\n            sa = str(s) + str(a)\n            if sa in self.Qsa:\n                u = self.Qsa[sa] + self.c_puct*self.P[str(s)][a]*math.sqrt(self.Ns[str(s)])/(1+self.Nsa[str(s)+str(a)])\n            else:\n                u = self.c_puct*self.P[str(s)][a]*math.sqrt(self.Ns[str(s)]+1e-8)\n                \n            if u>max_u:\n                max_u = u\n                best_a = a\n                \n        assert best_a != -1\n        \n        a = best_a\n        \n        # Player makes move and changes player\n        sp,p = self.game.get_next_state(s,1,a)\n        # Inverse board so that NN 'thinks' it is playing as the same player \n        sp = self.game.get_canonical_form(sp, p)\n        v = self.search(sp)\n\n        self.Qsa[str(s)+str(a)] = (self.Nsa[str(s)+str(a)]*self.Qsa[str(s)+str(a)] + v)/(self.Nsa[str(s)+str(a)]+1)\n\n        self.Nsa[str(s)+str(a)] += 1\n        self.Ns[str(s)]+=1\n        return -v","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def policyIterSP(game,nnet,iters,optimizer,eps,n_sims=30):\n    examples = deque([],10000)\n    for i in tqdm(range(iters)):\n        seed = random.randint(0,9999)\n        # Need to seed differently for each episode otherwise MTCS will play the same game\n        example = Parallel(n_jobs=4)(delayed(executeEpisode)(game,nnet,n_sims,seed+e) for e in range(eps))\n        for ex in example:\n            for e in ex:\n                examples.append(e)\n        train_examples = random.sample(examples,min(len(examples),4000))\n        nnet = trainNNet(examples,optimizer,nnet)\n        if i!=0 and i%10==0:\n            torch.save(nnet.cpu().state_dict(),f\"azero_{i}.pth\")\n    return nnet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def executeEpisode(game,nnet,n_sims,seed):\n    random.seed(seed)\n    curr_player = 1\n    examples = []\n    s = create_board()\n    mcts = MCTS(game,nnet)                                           # initialise search tree\n    while True:\n        c_board = game.get_canonical_form(s,curr_player)\n        for _ in range(n_sims):\n            mcts.search(c_board)\n    \n        pi = [mcts.Nsa[str(c_board)+str(a)]/mcts.Ns[str(c_board)] for a in range(7)]\n        examples.append([c_board,pi, None])\n        a = random.choices(range(len(mcts.P[str(c_board)])), weights=pi)    # sample action from improved policy\n        s,curr_player = game.get_next_state(c_board,curr_player,a[0])\n        reward = game.get_game_ended(s,curr_player)\n        if reward!=0:\n            assign_rewards(examples,reward)\n            return examples\n        \ndef assign_rewards(examples,game_reward):\n    for e in examples:\n        e[2] = game_reward","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Connect4Net(nn.Module):\n    \"\"\"\n    input: bs x 6 x 7 \n    output: pi bs x 7, value bs x 1\n    \"\"\"\n    def __init__(self,action_size=7,hidden_size=128):\n        super(Connect4Net, self).__init__()\n        self.action_size = action_size\n        self.hidden_size = hidden_size\n        self.conv1 = nn.Conv2d(1, hidden_size, 3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(hidden_size, hidden_size, 3, stride=1, padding=1)\n        self.conv3 = nn.Conv2d(hidden_size, hidden_size, 3, stride=1,padding=1)\n        self.conv4 = nn.Conv2d(hidden_size, hidden_size, 3, stride=1,padding=1)\n\n        self.bn1 = nn.BatchNorm2d(hidden_size)\n        self.bn2 = nn.BatchNorm2d(hidden_size)\n        self.bn3 = nn.BatchNorm2d(hidden_size)\n        self.bn4 = nn.BatchNorm2d(hidden_size)\n\n        self.fc1 = nn.Linear(hidden_size*6*7, 1024)\n        self.fc_bn1 = nn.BatchNorm1d(1024)\n\n        self.fc2 = nn.Linear(1024, 512)\n        self.fc_bn2 = nn.BatchNorm1d(512)\n        \n        self.drop1 = nn.Dropout(0.5)\n        self.drop2 = nn.Dropout(0.5)\n        \n        self.fc3 = nn.Linear(512, self.action_size)\n\n        self.fc4 = nn.Linear(512, 1)\n\n    def forward(self, s):\n        s = s.view(-1, 1, 6, 7)    \n        s = F.relu(self.bn1(self.conv1(s)))\n        # Skip connection\n        I = s\n        s = F.relu(self.bn2(self.conv2(s)))\n        s = F.relu(self.bn3(self.conv3(s))+I)\n        s = F.relu(self.bn4(self.conv4(s)))\n        s = s.view(-1, self.hidden_size*6*7)\n        \n        s = self.fc1(s)\n        s = self.fc_bn1(s)\n        s = F.relu(s)\n        \n        s = self.fc2(s)\n        s = self.fc_bn2(s)\n        s = F.relu(s)\n        \n        pi = self.fc3(s)\n        v = self.fc4(s)\n        \n        return F.log_softmax(pi, dim=1), torch.tanh(v)\n    \n    @torch.no_grad()\n    def predict(self,s):\n        a,b = self.forward(s.unsqueeze(0))\n        a = torch.exp(a)\n        return a.squeeze(0).cpu().numpy(),b.squeeze(0).cpu().numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ExampleDataset:\n    def __init__(self,examples):\n        self.examples = examples\n    \n    def __getitem__(self,index):\n        return torch.tensor(self.examples[index][0],dtype=torch.float32),torch.tensor(self.examples[index][1],dtype=torch.float32),torch.tensor(self.examples[index][2],dtype=torch.float32)\n    \n    def __len__(self):\n        return len(self.examples)\n\ndef get_trainloader(examples,bs=16):\n    train_dataset = ExampleDataset(examples)\n    return DataLoader(train_dataset,batch_size=bs,shuffle=True,drop_last=True)\n    \nclass NNET:\n    \"\"\"\n    Helper class to train and predict\n    \"\"\"\n    def __init__(self,nnet,device,optimizer,epoch=10):\n        self.nn = nnet\n        self.epoch = epoch\n        self.device = device\n        self.optimizer = optimizer\n    \n    def train(self,examples):\n        optimizer = self.optimizer\n        criterion = self.criterion\n        model = self.nn\n        device = self.device\n        \n        model.to(device)\n        model.train()\n        train_loader = get_trainloader(examples)\n        \n        for e in range(self.epoch):\n            total_loss = 0\n            for d in train_loader:\n                optimizer.zero_grad()\n                board,pi,reward = d\n    \n                board = board.to(device)\n                pi = pi.to(device)\n                reward = reward.to(device)\n\n                p,v = model(board)\n                    \n                loss = self.criterion(p,v,pi,reward)\n                loss.backward()\n                optimizer.step()\n                total_loss += loss.item()\n#             print(f\"Epoch Loss:{total_loss}\")\n            \n    def criterion(self,p,v,pi,reward):\n        return self.loss_pi(p,pi) + self.loss_v(v,reward)\n    \n    def loss_pi(self, targets, outputs):\n        return -torch.sum(targets * outputs) / targets.size()[0]\n\n    def loss_v(self, targets, outputs):\n        return torch.sum((targets - outputs.view(-1)) ** 2) / targets.size()[0]\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def trainNNet(examples,optimizer,nnet):\n    device = torch.device( \"cuda\" if torch.cuda.is_available() else \"cpu\" )\n    nnet = NNET(nnet,optimizer=optimizer,device=device)\n    nnet.train(examples)\n    return nnet.nn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nnet = Connect4Net()\noptimizer = torch.optim.Adam(nnet.parameters(),lr=0.01)\nfinal_net = policyIterSP(game=Game(),nnet=Connect4Net(),optimizer=optimizer,iters=50,eps=50,n_sims=25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(final_net.cpu().state_dict(),'azero_final.pth')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}