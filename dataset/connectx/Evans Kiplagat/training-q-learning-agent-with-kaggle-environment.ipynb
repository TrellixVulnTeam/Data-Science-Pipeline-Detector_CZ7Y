{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is a copy of [Hieu Phung](https://www.kaggle.com/phunghieu/connectx-with-q-learning)\n.He used gym to train the agent but am using kaggle environment"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install kaggle_environments","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"is_executing":false},"trusted":true},"cell_type":"code","source":"from kaggle_environments import evaluate, make\nimport numpy as np\nimport gym\nimport random\nfrom random import choice\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n","is_executing":false},"trusted":true},"cell_type":"code","source":"env = make(\"connectx\")\nenv.render()","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n","is_executing":false},"trusted":true},"cell_type":"code","source":"class QTable:\n    def __init__(self, action_space):\n        self.table = dict()\n        self.action_space = action_space\n        \n    def add_item(self, state_key):\n        self.table[state_key] = list(np.zeros(self.action_space.n))\n    \n    def __call__(self, state):\n        board = state.board[:] # Get a copy\n        board.append(state.mark)\n        state_key = np.array(board).astype(str)\n        state_key = hex(int(''.join(state_key), 3))[2:]\n        if state_key not in self.table.keys():\n            self.add_item(state_key)\n            \n        return self.table[state_key]\n    ","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n","is_executing":false},"trusted":true},"cell_type":"code","source":"# Environment parameters\ncols = 7\nrows = 6\n\naction_space = gym.spaces.Discrete(cols)\nobservation_space = gym.spaces.Discrete(cols * rows)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n","is_executing":false},"trusted":true},"cell_type":"code","source":"# configure hyper-parameters\nalpha =  0.1\ngamma = 0.6\nepsilon = 0.99\nmin_epsilon = 0.1\n\nepisodes = 15000\nalpha_decay_step = 1000\nalpha_decay_rate = 0.9\nepsilon_decay_rate = 0.9999","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n","is_executing":false},"trusted":true},"cell_type":"code","source":"q_table = QTable(action_space)\ntrainer = env.train([None, \"negamax\"])\n\nall_epochs = []\nall_total_rewards = []\nall_avg_rewards = []\nall_q_table_rows = []\nall_epsilons = []\n\nfor i in tqdm(range(episodes)):\n    state = trainer.reset()\n    epsilon = max(min_epsilon, epsilon * epsilon_decay_rate)\n    epochs, total_rewards = 0, 0\n    done = False\n    \n    while not done:\n        if random.uniform(0, 1) < epsilon:\n            action = choice([c for c in range(action_space.n) if state.board[c] == 0])\n        \n        else:\n            row = q_table(state)[:]\n            selected_items = []\n            for j in range(action_space.n):\n                if state.board[j] == 0:\n                    selected_items.append(row[j])\n                else:\n                    selected_items.append(-1e7)\n            action = int(np.argmax(selected_items))\n                \n        next_state, reward, done, info = trainer.step(action)\n        \n        # apply new rules\n        if done:\n            if reward == 1:\n                reward = 20\n            elif reward == 0:\n                reward = -20\n            else:\n                reward = 10\n                \n        else:\n            reward = -0.05\n        \n        old_value = q_table(state)[action]\n        next_max = np.argmax(q_table(next_state))\n        \n        # update q value\n        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n        q_table(state)[action] = new_value\n        \n        state = next_state\n        epochs += 1\n        total_rewards += reward\n    \n    all_epochs.append(epochs)\n    all_total_rewards.append(total_rewards)\n    avg_rewards = np.mean(all_total_rewards[max(0, i - 100) : (i + 1)])\n    all_avg_rewards.append(avg_rewards)\n    all_q_table_rows.append(len(q_table.table))\n    all_epsilons.append(epsilon)\n    \n    if (i +1) % alpha_decay_step == 0:\n        alpha += alpha_decay_rate\n    ","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n","is_executing":false},"trusted":false},"cell_type":"code","source":"tmp_dict_q_table = q_table.table.copy()\ndict_q_table = dict()\n\nfor k in tmp_dict_q_table:\n    if np.count_nonzero(tmp_dict_q_table[k]) > 0:\n        dict_q_table[k] = int(np.argmax(tmp_dict_q_table[k]))","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n","is_executing":false},"trusted":false},"cell_type":"code","source":"def my_agent(observation, configuration):\n    from random import choice\n    \n    q_table = dict_q_table\n    board = observation.board[:]\n    board.append(observation.mark)\n    state_key = list(map(str, board))\n    state_key = hex(int(''.join(state_key), 3))[2:]\n    \n    if state_key not in q_table.keys():\n        return choice([c for c in range(configuration.columns) \n                       if observation.board[c] == 0])\n    action = q_table[state_key]\n    \n    if observation.board[action] != 0:\n        return choice([c for c in range(configuration.columns) \n                       if observation.board[c] == 0])\n    return action","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n","is_executing":false},"trusted":false},"cell_type":"code","source":"# Run against negamax\nenv.reset()\nenv.run([my_agent, \"negamax\"])\nenv.render(mode=\"ipython\")","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n","is_executing":false},"trusted":false},"cell_type":"code","source":"# Play as first position against random agent.\ntrainer = env.train([None, \"random\"])\n\nobservation = trainer.reset()\n\nwhile not env.done:\n    my_action = my_agent(observation, env.configuration)\n    print(\"My Action\", my_action)\n    observation, reward, done, info = trainer.step(my_action)\n    # env.render(mode=\"ipython\", width=100, height=90, header=False, controls=False)\nenv.render()","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% write agent to disk\n","is_executing":false},"trusted":false},"cell_type":"code","source":"agent = \"\"\"def my_agent(observation, configuration):\n    from random import choice\n    \n    q_table = \"\"\"+ str(dict_q_table).replace(\" \", \"\") +\"\"\"\n    board = observation.board[:]\n    board.append(observation.mark)\n    state_key = list(map(str, board))\n    state_key = hex(int(''.join(state_key), 3))[2:]\n    \n    if state_key not in q_table.keys():\n        return choice([c for c in range(configuration.columns) \n                       if observation.board[c] == 0])\n    action = q_table[state_key]\n    \n    if observation.board[action] != 0:\n        return choice([c for c in range(configuration.columns) \n                       if observation.board[c] == 0])\n    return action \"\"\"","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n","is_executing":false},"trusted":false},"cell_type":"code","source":"with open(\"submission.py\", 'w') as f:\n    f.write(agent)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%  \n","is_executing":false},"trusted":false},"cell_type":"code","source":"# import saved agent\nfrom submission import my_agent","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n","is_executing":false},"trusted":false},"cell_type":"code","source":"# Evaluate saved agent\ndef mean_reward(rewards):\n    return sum(r[0] for r in rewards) / sum(r[0] + r[1] for r in rewards)\n\n# Run multiple episodes to estimate its performance.\nprint(\"My Agent vs Random Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"random\"], num_episodes=10)))\nprint(\"My Agent vs Negamax Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=10)))","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n","is_executing":false},"trusted":false},"cell_type":"code","source":"# Play the agent with human\nenv.play([my_agent, None], width=500, height=450)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"pycharm-576fb1ed","language":"python","display_name":"PyCharm (ConnectX)"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.6"},"pycharm":{"stem_cell":{"cell_type":"raw","source":["# \n","This is a copy of [Hieu Phung](https://www.kaggle.com/phunghieu/connectx-with-q-learning)\n",".He used gym to train the agent but am using kaggle environment\n"],"metadata":{"collapsed":false}}}},"nbformat":4,"nbformat_minor":1}