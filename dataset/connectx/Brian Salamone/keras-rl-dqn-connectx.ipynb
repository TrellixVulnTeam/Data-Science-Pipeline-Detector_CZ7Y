{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"code","source":"!pip install keras-rl\n!pip install tensorflow==1.14","metadata":{"id":"RHq1KSbXeFq9","outputId":"2eeb72ac-a285-4b4e-9f7a-f85a3b075132","execution":{"iopub.status.busy":"2022-01-03T20:28:10.298692Z","iopub.execute_input":"2022-01-03T20:28:10.299008Z","iopub.status.idle":"2022-01-03T20:29:02.995238Z","shell.execute_reply.started":"2022-01-03T20:28:10.298956Z","shell.execute_reply":"2022-01-03T20:29:02.994455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_environments import evaluate, make, utils\n\nenv = make(\"connectx\", debug=True)\n#env.render()","metadata":{"id":"1DlsnhlJnto-","execution":{"iopub.status.busy":"2022-01-03T20:29:02.997558Z","iopub.execute_input":"2022-01-03T20:29:02.998022Z","iopub.status.idle":"2022-01-03T20:29:03.104688Z","shell.execute_reply.started":"2022-01-03T20:29:02.997984Z","shell.execute_reply":"2022-01-03T20:29:03.10404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Custom classes","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nfrom rl.core import Processor\nclass CustomProcessor(Processor):\n    def process_state_batch(self, obs_batch):\n        batch = np.array(obs_batch)\n        result = batch.reshape(-1, 6, 7)\n        def group(asd):\n            a, b = asd.copy(), asd.copy()\n            a[a == 1] = 0\n            a[a == 2] = 1\n            b[b == 2] = 0\n            return np.stack((a,b), axis=2)\n        def get_mark(asd):\n            return 1 if np.count_nonzero(asd==2) > np.count_nonzero(asd==1) else 2\n        res1, res2 = [group(a) for a in result], [get_mark(a) for a in result]\n        return [res1, res2]\n    def process_action(self, action):\n        return int(action)","metadata":{"execution":{"iopub.status.busy":"2022-01-03T20:29:03.106007Z","iopub.execute_input":"2022-01-03T20:29:03.106269Z","iopub.status.idle":"2022-01-03T20:29:05.907733Z","shell.execute_reply.started":"2022-01-03T20:29:03.106239Z","shell.execute_reply":"2022-01-03T20:29:05.9068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.utils import shuffle\n\ndef make_idiot():\n    asd = shuffle(list(range(7)))\n    def idiot(observation, configuration):\n        board = observation.board\n        columns = configuration.columns\n        options= [c for c in range(columns) if board[c] == 0]\n        return [a for a in asd if a in options][0]\n    return idiot","metadata":{"execution":{"iopub.status.busy":"2022-01-03T20:29:05.912105Z","iopub.execute_input":"2022-01-03T20:29:05.912394Z","iopub.status.idle":"2022-01-03T20:29:06.523011Z","shell.execute_reply.started":"2022-01-03T20:29:05.912359Z","shell.execute_reply":"2022-01-03T20:29:06.52231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nclass ConnectTrainer():\n    \"\"\"Connect Trainer\n    \"\"\"\n    def __init__(self, env, configuration):\n        self.env =  env\n        self.configuration = configuration\n        self.get_players = lambda self: [None, 'random']\n        \n    def step(self, action):\n        observation, reward, done, info = self.trainer.step(action)\n        observation = observation.board\n        if reward is None:\n            # invalid action\n            reward = -100\n        else:\n            if reward == 0:\n                # lose\n                reward = -10\n            if not done:\n                # game still in course\n                reward = 0\n            # long game equals less penalty and less reward\n            reward *= 1+5/np.count_nonzero(observation)\n        return observation, reward, done, info\n\n    def reset(self):\n        self.trainer = self.env.train(self.get_players(self))\n        observation = self.trainer.reset()\n        observation = observation.board\n        return observation\n    \n    def set_difficulty(self, difficulty):\n        if difficulty < 1:\n            self.get_players = lambda self: shuffle([None, random.choice([\"random\", make_idiot()])])\n        if difficulty >= 1:\n            self.get_players = lambda self: shuffle([None, random.choice([\"negamax\", make_idiot()])])\n        if 'my_agent' in globals():\n            if difficulty >= 2:\n                self.get_players = lambda self: shuffle([None, random.choice([\"negamax\", my_agent])])\n            if difficulty >= 3:\n                self.get_players = lambda self: shuffle([None, my_agent])\n        else:\n            print('Self play not available')\n        print(f'Difficulty set to: {difficulty}.')","metadata":{"execution":{"iopub.status.busy":"2022-01-03T20:29:06.526387Z","iopub.execute_input":"2022-01-03T20:29:06.526867Z","iopub.status.idle":"2022-01-03T20:29:06.540895Z","shell.execute_reply.started":"2022-01-03T20:29:06.526824Z","shell.execute_reply":"2022-01-03T20:29:06.540044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from rl.callbacks import Callback\n\nclass DifficultyChangeCallback(Callback):\n    def __init__(self, initial_difficulty, difficulty_steepness=150):\n        self.difficulty_steepness = difficulty_steepness\n        self.difficulty = initial_difficulty\n        \n    def on_episode_begin(self, episode, logs):\n        if episode and episode % self.difficulty_steepness == 0:\n            self.difficulty += 1\n            self.env.set_difficulty(self.difficulty)","metadata":{"execution":{"iopub.status.busy":"2022-01-03T20:29:06.54432Z","iopub.execute_input":"2022-01-03T20:29:06.544537Z","iopub.status.idle":"2022-01-03T20:29:06.553086Z","shell.execute_reply.started":"2022-01-03T20:29:06.544513Z","shell.execute_reply":"2022-01-03T20:29:06.552469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model definition","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport keras\n\nfrom rl.policy import BoltzmannQPolicy\nfrom rl.memory import SequentialMemory\nfrom rl.agents import DQNAgent\n\n\n# Environment setup\nnb_actions = 7\nenv_wrapper = ConnectTrainer(env, env.configuration)\n\n# Model setup\ninput_shape_1 = keras.Input(shape=(6, 7, 2))\ninput_shape_2 = keras.Input(shape=(1,)) #mark\n\ntower_1 = keras.layers.Conv2D(100, kernel_size=(2, 2), padding='same', activation='relu')(input_shape_1)\ntower_1 = keras.layers.Dropout(0.02)(tower_1)\ntower_1 = keras.layers.Flatten()(tower_1)\ntower_1 = keras.layers.Dense(150, activation='relu')(tower_1)\ntower_1 = keras.layers.Dropout(0.2)(tower_1)\n\ntower_2 = keras.layers.Conv2D(50, kernel_size=(3, 3), padding='same', activation='relu')(input_shape_1)\ntower_2 = keras.layers.Dropout(0.02)(tower_2)\ntower_2 = keras.layers.Flatten()(tower_2)\ntower_2 = keras.layers.Dense(150, activation='relu')(tower_2)\ntower_2 = keras.layers.Dropout(0.2)(tower_2)\n\ntower_3 = keras.layers.Flatten()(input_shape_1)\ntower_3 = keras.layers.Dense(150, activation='relu')(tower_3)\ntower_3 = keras.layers.Dropout(0.2)(tower_3)\n\nmerged = keras.layers.Concatenate(axis=1)([tower_1, tower_2, tower_3, input_shape_2])\nout = keras.layers.Dense(120, activation='relu')(merged)\nout = keras.layers.Dropout(0.3)(out)\nout = keras.layers.Dense(80, activation='relu')(out)\nout = keras.layers.Dropout(0.4)(out)\nout = keras.layers.Dense(50, activation='relu')(out)\nout = keras.layers.Dropout(0.5)(out)\nout = keras.layers.Dense(7, activation='linear')(out)\n\nmodel = keras.models.Model([input_shape_1, input_shape_2], out)\n\n# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n# even the metrics!\nmemory = SequentialMemory(limit=50000, window_length=1)\npolicy = BoltzmannQPolicy(tau=0.3)\ndqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=100,\n               target_model_update=1e-2, policy=policy, processor=CustomProcessor())\ndqn.compile(keras.optimizers.Adam(lr=0.0001), metrics=['mae'])","metadata":{"id":"TkmAiH3gooc0","execution":{"iopub.status.busy":"2022-01-03T20:46:51.799793Z","iopub.execute_input":"2022-01-03T20:46:51.800081Z","iopub.status.idle":"2022-01-03T20:46:53.412471Z","shell.execute_reply.started":"2022-01-03T20:46:51.800052Z","shell.execute_reply":"2022-01-03T20:46:53.411798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"session = tf.Session()\n\nkeras.backend.set_session(session)\ninit = tf.global_variables_initializer()\nsession.run(init)\n\ndef my_agent(obs, conf):\n    import tensorflow as tf\n    import numpy as np\n    import json\n    \n    if 'dqn' in globals():\n        with session.as_default():\n            with session.graph.as_default():\n                q_values = dqn.compute_q_values(obs['board'])\n\n    else:\n        q_net = tf.keras.models.model_from_json('#MODEL')\n        q_net.set_weights(np.array([np.array(a) for a in json.loads('#WEIGHTS')]))\n\n        def process_state_batch(obs_batch):\n            batch = np.array(obs_batch)\n            result = batch.reshape(-1, 6, 7)\n            def group(asd):\n                a, b = asd.copy(), asd.copy()\n                a[a == 1] = 0\n                a[a == 2] = 1\n                b[b == 2] = 0\n                return np.stack((a,b), axis=2)\n            def get_mark(asd):\n                return 1 if np.count_nonzero(asd==2) > np.count_nonzero(asd==1) else 2\n            res1, res2 = np.array([group(a) for a in result]), np.array([get_mark(a) for a in result])\n            return [res1, res2]\n    \n        q_values = q_net.predict(process_state_batch(obs['board']))[0]\n\n    def prune_invalid_actions(board, q_values):\n        for i in range(7):\n            if board[i]:\n                q_values[i] = -1e7\n        return q_values\n\n    def boltzmannize(q_values, tau, clip=(-500., 500.)):\n        nb_actions = q_values.shape[0]\n        exp_values = np.exp(np.clip(q_values / tau, clip[0], clip[1]))\n        probs = exp_values / np.sum(exp_values)\n        return np.random.choice(range(nb_actions), p=probs)\n\n    return int(np.argmax(prune_invalid_actions(obs['board'], q_values)))\n    # return int(boltzmannize(prune_invalid_actions(obs['board'], q_values), 0.3))\n\nmy_agent({'board': [0]*42, 'mark': 1}, {})","metadata":{"execution":{"iopub.status.busy":"2022-01-03T20:46:53.415618Z","iopub.execute_input":"2022-01-03T20:46:53.416149Z","iopub.status.idle":"2022-01-03T20:46:54.115657Z","shell.execute_reply.started":"2022-01-03T20:46:53.416113Z","shell.execute_reply":"2022-01-03T20:46:54.11504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Okay, now it's time to learn something! We visualize the training here for show, but this\n# slows down training quite a lot. You can always safely abort the training prematurely using\n# Ctrl + C.\ndqn.fit(env_wrapper, nb_steps=1200000, visualize=False, verbose=1, callbacks=[DifficultyChangeCallback(-1, 24000)])","metadata":{"execution":{"iopub.status.busy":"2022-01-03T20:46:54.117921Z","iopub.execute_input":"2022-01-03T20:46:54.118372Z","iopub.status.idle":"2022-01-03T20:47:01.131581Z","shell.execute_reply.started":"2022-01-03T20:46:54.118335Z","shell.execute_reply":"2022-01-03T20:47:01.130817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nweights_json = json.dumps([w.tolist() for w in dqn.model.get_weights()])\nmodel_json = dqn.model.to_json()\n\nimport inspect\nimport os\nimport base64\n\ndef write_agent_to_file(function, file):\n    with open(file, \"w\") as f:\n        process = CustomProcessor().process_state_batch\n        source_code = inspect.getsource(function)\n        source_code = source_code.replace('#MODEL', model_json)\n        source_code = source_code.replace('#WEIGHTS', weights_json)\n\n        f.write(source_code)\n        print(function, \"written to\", file)\n        \n    \nwrite_agent_to_file(my_agent, \"submission.py\")","metadata":{"execution":{"iopub.status.busy":"2022-01-03T20:47:01.13303Z","iopub.execute_input":"2022-01-03T20:47:01.133524Z","iopub.status.idle":"2022-01-03T20:47:02.150053Z","shell.execute_reply.started":"2022-01-03T20:47:01.133487Z","shell.execute_reply":"2022-01-03T20:47:02.148559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Note: Stdout replacement is a temporary workaround.\nimport sys\nfrom submission import my_agent as agent\n\nenv = make(\"connectx\", debug=True)\nenv.run([agent, agent])\nprint(\"Success!\" if env.state[0].status == env.state[1].status == \"DONE\" else \"Failed...\")","metadata":{"execution":{"iopub.status.busy":"2022-01-03T20:47:02.152289Z","iopub.execute_input":"2022-01-03T20:47:02.152555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env_wrapper.set_difficulty(0)\ndqn.test(env_wrapper,nb_episodes=10,visualize=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mean_reward(rewards):\n    mr = sum(r[0] for r in rewards) / float(len(rewards))\n    return f'{((mr+1)/2)*100}%'\n    \n\n# Run multiple episodes to estimate its performance.\nprint(\"My Agent vs Random Agent:\", mean_reward(evaluate(\"connectx\", [agent, \"random\"], num_episodes=10)))\nprint(\"My Agent vs Negamax Agent:\", mean_reward(evaluate(\"connectx\", [agent, \"negamax\"], num_episodes=10)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env.reset()\nenv.run(['negamax', agent])\nenv.render(mode=\"ipython\", width=500, height=450)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}