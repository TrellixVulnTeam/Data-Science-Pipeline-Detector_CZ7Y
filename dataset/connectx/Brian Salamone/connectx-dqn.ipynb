{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install kaggle-environments","metadata":{}},{"cell_type":"code","source":"# 1. Enable Internet in the Kernel (Settings side pane)\n\n# 2. Curl cache may need purged if v0.1.6 cannot be found (uncomment if needed). \n# !curl -X PURGE https://pypi.org/simple/kaggle-environments\n\n# ConnectX environment was defined in v0.1.6\n!pip install 'kaggle-environments'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-20T05:32:40.68205Z","iopub.execute_input":"2021-12-20T05:32:40.682483Z","iopub.status.idle":"2021-12-20T05:32:45.887819Z","shell.execute_reply.started":"2021-12-20T05:32:40.68241Z","shell.execute_reply":"2021-12-20T05:32:45.886991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create ConnectX Environment","metadata":{}},{"cell_type":"code","source":"from kaggle_environments import evaluate, make, utils\n\nenv = make(\"connectx\", debug=True)\nenv.render()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-12-20T05:32:45.889777Z","iopub.execute_input":"2021-12-20T05:32:45.89007Z","iopub.status.idle":"2021-12-20T05:32:46.073204Z","shell.execute_reply.started":"2021-12-20T05:32:45.890023Z","shell.execute_reply":"2021-12-20T05:32:46.072459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create an Agent\n\nTo create the submission, an agent function should be fully encapsulated (no external dependencies).  \n\nWhen your agent is being evaluated against others, it will not have access to the Kaggle docker image.  Only the following can be imported: Python Standard Library Modules, gym, numpy, scipy, pytorch (1.3.1, cpu only), and more may be added later.\n\n","metadata":{}},{"cell_type":"markdown","source":"## DQN Agent declaration","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\nclass DqnAgent:\n    \"\"\"\n    DQN Agent: the agent that explores the game and\n    should eventually learn how to play the game.\n    \"\"\"\n    def __init__(self):\n        self.q_net = self._build_dqn_model()\n        self.target_q_net = self._build_dqn_model()\n        self.exploration_decay = 1\n\n    def policy(self, state, mark):\n        \"\"\"\n        Takes a state from the game environment and returns\n        a action that should be taken given the current game\n        environment.\n        \"\"\"\n        if np.random.random() < 0.05 + self.exploration_decay:\n            return int(np.random.choice([c for c in range(7) if state[:,:,:,0].ravel()[c] == 0 and state[:,:,:,1].ravel()[c] == 0]))\n        \n        action_q = self.q_net.predict([state, mark])[0]\n        for i in range(7):\n            if state[:,:,:,0].ravel()[i] != 0 or state[:,:,:,1].ravel()[i] != 0:\n                action_q[i] = -1e7\n        action = int(np.argmax(action_q))\n        return action\n\n    def train(self, batch):\n        state_batch, next_state_batch, action_batch, reward_batch, done_batch, mark_batch = batch\n        current_q = self.q_net.predict([state_batch, mark_batch])\n        target_q = np.copy(current_q)\n        next_q = self.target_q_net.predict([next_state_batch, mark_batch])\n        max_next_q = np.amax(next_q, axis=1)\n        for i in range(state_batch.shape[0]):\n            target_q[i][action_batch[i]] = reward_batch[i] if done_batch[i] else reward_batch[i] + 0.95 * max_next_q[i]\n        result = self.q_net.fit(x=(state_batch, mark_batch), y=target_q, verbose=0)\n        self.exploration_decay *= 0.99\n        return result.history['loss']\n\n    @staticmethod\n    def _build_dqn_model():\n        \"\"\"\n        Builds a deep neural net which predicts the Q values for all possible\n        actions given a state. The input should have the shape of the state\n        (which is rows x columns + who_starts (6x7=42) in ConnectX), and the output should have the same shape as\n        the action space (which is 7 in ConnectX) since we want 1 Q value per\n        possible action.\n\n        :return: the Q network\n        \"\"\"\n        input_shape_1 = tf.keras.Input(shape=(6, 7, 2))\n        input_shape_2 = tf.keras.Input(shape=(1,)) #mark\n        tower_1 = tf.keras.layers.Conv2D(80, kernel_size=(2, 2), padding='same', activation='relu')(input_shape_1)\n        tower_1 = tf.keras.layers.Dropout(0.1)(tower_1)\n        tower_2 = tf.keras.layers.Conv2D(80, kernel_size=(3, 3), padding='same', activation='relu')(input_shape_1)\n        tower_2 = tf.keras.layers.Dropout(0.1)(tower_2)\n        merged = tf.keras.layers.Concatenate(axis=1)([tower_1, tower_2])\n        merged = tf.keras.layers.Flatten()(merged)\n        merged = tf.keras.layers.Concatenate(axis=1)([merged, input_shape_2])\n        out = tf.keras.layers.Dense(80, activation='relu', kernel_initializer='random_normal')(merged)\n        out = tf.keras.layers.Dropout(0.2)(out)\n        out = tf.keras.layers.Dense(50, activation='relu', kernel_initializer='random_normal')(out)\n        out = tf.keras.layers.Dropout(0.5)(out)\n        out = tf.keras.layers.Dense(7, activation='linear', kernel_initializer='random_normal')(out)\n        q_net = tf.keras.models.Model([input_shape_1, input_shape_2], out)\n        \n#         q_net = tf.keras.Sequential()\n#         q_net.add(tf.keras.layers.Dense(128, input_dim=42, activation='relu', kernel_initializer='random_normal'))\n#         q_net.add(tf.keras.layers.Dense(64, activation='relu', kernel_initializer='random_normal'))\n#         q_net.add(tf.keras.layers.Dense(7, activation='linear', kernel_initializer='random_normal'))\n        q_net.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mse')\n        return q_net\n    \n    def update_target_network(self):\n        self.target_q_net.set_weights(self.q_net.get_weights())","metadata":{"execution":{"iopub.status.busy":"2021-12-20T05:32:46.074585Z","iopub.execute_input":"2021-12-20T05:32:46.074893Z","iopub.status.idle":"2021-12-20T05:32:48.007079Z","shell.execute_reply.started":"2021-12-20T05:32:46.074812Z","shell.execute_reply":"2021-12-20T05:32:48.006399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Replay Buffer declaration","metadata":{}},{"cell_type":"code","source":"from collections import deque\nimport numpy as np\nimport random\n\nclass ReplayBuffer:\n    def __init__(self):\n        self.gameplay_experiences = deque(maxlen=1000000)\n\n    def store_gameplay_experience(self, state, next_state, reward, action, done, mark):\n        self.gameplay_experiences.append((state, next_state, reward, action, done, mark))\n\n    def sample_gameplay_batch(self):\n        batch_size = int(len(self.gameplay_experiences)/20)\n        if not batch_size:\n            return [[]]\n#         mu, sigma = 3., 1. # mean and standard deviation\n#         distribution = np.random.lognormal(mu, sigma, len(self.gameplay_experiences))\n#         sampled_gameplay_batch = random.choices(self.gameplay_experiences, distribution, k=batch_size)\n        sampled_gameplay_batch = random.sample(self.gameplay_experiences, batch_size)\n        state_batch, next_state_batch, action_batch, reward_batch, done_batch, mark_batch = [], [], [], [], [], []\n        for gameplay_experience in sampled_gameplay_batch:\n            state_batch.append(*gameplay_experience[0])\n            next_state_batch.append(*gameplay_experience[1])\n            reward_batch.append(gameplay_experience[2])\n            action_batch.append(gameplay_experience[3])\n            done_batch.append(gameplay_experience[4])\n            mark_batch.append(*gameplay_experience[5])\n        return np.array(state_batch), np.array(next_state_batch), action_batch, reward_batch, done_batch, np.array(mark_batch)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T05:32:48.008639Z","iopub.execute_input":"2021-12-20T05:32:48.008926Z","iopub.status.idle":"2021-12-20T05:32:48.020941Z","shell.execute_reply.started":"2021-12-20T05:32:48.008881Z","shell.execute_reply":"2021-12-20T05:32:48.020176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plotting function","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom IPython.display import display, clear_output\nimport numpy as np\nimport random\nfrom collections import deque\n\n\ndef plot_device(deque_mean_len=10):\n    fig = plt.figure(figsize=(16, 8))\n    ax1 = fig.add_subplot(1, 1, 1)\n    ax2 = ax1.twinx()\n    \n    x = []\n    y1 = []\n    y1_mean = deque(maxlen=deque_mean_len)\n    y2 = []\n    y2_mean = deque(maxlen=deque_mean_len)\n    def plot(episode_number, reward, loss):\n        \n        x.append(episode_number)\n        y1_mean.append(reward)\n        y1.append(sum(y1_mean)/len(y1_mean))\n        y2_mean.append(loss)\n        y2.append(sum(y2_mean)/len(y2_mean))\n\n        ax1.set_xlim(x[0], x[-1])\n        ax2.set_xlim(x[0], x[-1])\n        \n        ax1.cla()\n        ax1.plot(x, y1, 'b', label='Reward')\n        ax2.cla()\n        ax2.plot(x, y2, 'r', label='Loss')\n        \n        display(fig)\n        clear_output(wait=True)\n\n\n    return plot","metadata":{"execution":{"iopub.status.busy":"2021-12-20T05:32:48.023922Z","iopub.execute_input":"2021-12-20T05:32:48.024399Z","iopub.status.idle":"2021-12-20T05:32:48.037866Z","shell.execute_reply.started":"2021-12-20T05:32:48.024347Z","shell.execute_reply":"2021-12-20T05:32:48.037036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Putting all together ","metadata":{}},{"cell_type":"code","source":"import random\nimport plotly.graph_objs as go\nfrom IPython.display import display\n\ndef get_state_input(state):\n    state =  state if isinstance(state, np.ndarray) else np.array([state['board']])\n    result = state.reshape(-1, 6, 7)\n    a, b = result.copy(), result.copy()\n    a[a == 1] = 0\n    a[a == 2] = 1\n    b[b == 2] = 0\n    return np.stack((a,b), axis=3)\n\ndef idiot_maker():\n    asd = list(range(7))\n    random.shuffle(asd)\n    def idiot(observation, configuration):\n        board = observation.board\n        columns = configuration.columns\n        options= [c for c in range(columns) if board[c] == 0]\n        return [a for a in asd if a in options][0]\n    return idiot\n\ndef collect_gameplay_experience(env, agent, buffer, difficulty):\n    \"\"\"\n    The collect_gameplay_experience function plays the game \"env\" with the\n    instructions produced by \"agent\" and stores the gameplay experiences\n    into \"buffer\" for later training.\n    \"\"\"\n    players = [idiot_maker()]\n    if difficulty >= 1:\n        players += ['random']\n    if difficulty >= 2:\n        players += [my_agent] if 'my_agent' in globals() else []\n    if difficulty >= 3:\n        players += ['negamax']\n    players = [random.choice(players), None]\n    if difficulty == 0:\n        random.shuffle(players)\n    trainer = env.train(players)\n    state = trainer.reset()\n    mark = np.array([state.mark])\n    state = get_state_input(state)\n    \n    done = False\n    steps = 0\n    action_usage = [0]*7\n    while not done:\n        steps += 1\n        action = agent.policy(state, mark)\n        action_usage[action] += 1\n        next_state, reward, done, _ = trainer.step(action)\n        next_state = get_state_input(next_state)\n        \n        if done:\n            reward *= 1 + 3/steps\n            if reward < 0:\n                reward *= 10\n\n        buffer.store_gameplay_experience(state, next_state, reward, action, done, mark)\n        state = next_state\n    return reward, action_usage\n\n\ndef train_model(env, agent, buffer):\n    \"\"\"\n    Trains a DQN agent to play the ConnectX game\n    \"\"\"\n    pld = plot_device()\n    cum_reward = 0\n    cum_loss = 0\n    cum_action_usage = np.zeros(7)\n    difficulty = 0\n    for episode_cnt in range(50000): # Train the agent for 6000 episodes of the game\n        reward, action_usage = collect_gameplay_experience(env, agent, buffer, difficulty)\n        cum_reward += reward\n        cum_action_usage = cum_action_usage + np.array(action_usage)\n        gameplay_experience_batch = buffer.sample_gameplay_batch()\n        if len(gameplay_experience_batch[0]) < 20:\n            cum_reward = 0\n            continue\n        loss, = agent.train(gameplay_experience_batch)\n        cum_loss += loss\n        if episode_cnt % 20 == 0:\n            print(f'Episode No {episode_cnt}. Training with {len(gameplay_experience_batch[0])}. Explor. {0.05 + agent.exploration_decay:.2f}')\n            print(f'Last 20 episodes: Mean loss - {cum_loss/20:.4f} Mean reward - {cum_reward/20:.2f}')\n            \n#             print(f'Action usage in this episode: {action_usage}')\n            display(go.Figure(go.Bar(x=list(range(7)), y=cum_action_usage/7)))\n            \n    \n            agent.update_target_network()\n            pld(episode_cnt, cum_reward/20, cum_loss/20)\n            \n            difficulty = 0\n            if cum_reward/20 > -10:\n                difficulty = 1\n            if cum_reward/20 > -5:\n                difficulty = 2\n            if cum_reward/20 > 0:\n                difficulty = 3\n            cum_reward = 0\n            cum_loss = 0\n            cum_action_usage = np.zeros(7)\n        \n    print('Done')","metadata":{"execution":{"iopub.status.busy":"2021-12-20T05:39:31.041177Z","iopub.execute_input":"2021-12-20T05:39:31.041959Z","iopub.status.idle":"2021-12-20T05:39:31.068782Z","shell.execute_reply.started":"2021-12-20T05:39:31.0419Z","shell.execute_reply":"2021-12-20T05:39:31.067931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run model","metadata":{}},{"cell_type":"code","source":"experience_buffer = ReplayBuffer()","metadata":{"execution":{"iopub.status.busy":"2021-12-20T05:39:31.070683Z","iopub.execute_input":"2021-12-20T05:39:31.071Z","iopub.status.idle":"2021-12-20T05:39:31.081379Z","shell.execute_reply.started":"2021-12-20T05:39:31.07094Z","shell.execute_reply":"2021-12-20T05:39:31.080526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trained_agent = DqnAgent()","metadata":{"execution":{"iopub.status.busy":"2021-12-20T05:39:31.083104Z","iopub.execute_input":"2021-12-20T05:39:31.083579Z","iopub.status.idle":"2021-12-20T05:39:31.291686Z","shell.execute_reply.started":"2021-12-20T05:39:31.083384Z","shell.execute_reply":"2021-12-20T05:39:31.290977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_model(env, trained_agent, experience_buffer)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T06:10:56.33788Z","iopub.execute_input":"2021-12-20T06:10:56.338213Z","iopub.status.idle":"2021-12-20T06:55:31.647504Z","shell.execute_reply.started":"2021-12-20T06:10:56.338151Z","shell.execute_reply":"2021-12-20T06:55:31.643181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nweights_json = json.dumps([w.tolist() for w in trained_agent.q_net.get_weights()])\nmodel_json = trained_agent.q_net.to_json()","metadata":{"execution":{"iopub.status.busy":"2021-12-20T06:55:31.651474Z","iopub.status.idle":"2021-12-20T06:55:31.652134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def my_agent(observation, configuration):\n    import tensorflow as tf\n    import json\n    import numpy as np\n    \n    if 'trained_agent' in globals():\n        q_net = trained_agent.q_net\n    #MODEL\n    #WEIGHTS\n    \n    mark = np.array([observation.mark])\n    board = np.array(observation.board)\n    board = board[None, :].reshape(-1, 6, 7)\n    a, b = board.copy(), board.copy()\n    a[a == 1] = 0\n    a[a == 2] = 1\n    b[b == 2] = 0\n    result = np.stack((a,b), axis=3)\n    action_q = q_net.predict([result, mark])[0]\n    for i in range(7):\n        if observation.board[i] != 0:\n            action_q[i] = -1e7\n    action = int(np.argmax(action_q))\n    return action\n","metadata":{"execution":{"iopub.status.busy":"2021-12-20T06:55:31.6585Z","iopub.status.idle":"2021-12-20T06:55:31.659144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate your Agent","metadata":{}},{"cell_type":"code","source":"def mean_reward(rewards):\n    mr = sum(r[0] for r in rewards) / float(len(rewards))\n    return f'{((mr+1)/2)*100}%'\n    \n\n# Run multiple episodes to estimate its performance.\nprint(\"My Agent vs Random Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"random\"], num_episodes=100)))\nprint(\"My Agent vs Negamax Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=10)))","metadata":{"execution":{"iopub.status.busy":"2021-12-20T06:55:31.664144Z","iopub.status.idle":"2021-12-20T06:55:31.664935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test your Agent","metadata":{}},{"cell_type":"code","source":"env.reset()\n# Play as the first agent against default \"random\" agent.\nenv.run([\"negamax\", my_agent])\nenv.render(mode=\"ipython\", width=500, height=450)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-12-20T06:55:31.666295Z","iopub.status.idle":"2021-12-20T06:55:31.667276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Write Submission File\n\n","metadata":{}},{"cell_type":"code","source":"import inspect\nimport os\nimport base64\n\ndef write_agent_to_file(function, file):\n    with open(file, \"w\") as f:\n        source_code = inspect.getsource(function)\n        source_code = source_code.replace('#MODEL', f\"q_net = tf.keras.models.model_from_json('{model_json}')\")\n        source_code = source_code.replace('#WEIGHTS', f\"q_net.set_weights(np.array([np.array(a) for a in json.loads('{weights_json}')]))\")\n        \n        f.write(source_code)\n        print(function, \"written to\", file)\n        \n    \nwrite_agent_to_file(my_agent, \"submission.py\")","metadata":{"execution":{"iopub.status.busy":"2021-12-20T06:55:31.668535Z","iopub.status.idle":"2021-12-20T06:55:31.669206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validate Submission\nPlay your submission against itself.  This is the first episode the competition will run to weed out erroneous agents.\n\nWhy validate? This roughly verifies that your submission is fully encapsulated and can be run remotely.","metadata":{}},{"cell_type":"code","source":"# Note: Stdout replacement is a temporary workaround.\nimport sys\nfrom submission import my_agent as agent\n\nenv = make(\"connectx\", debug=True)\nenv.run([agent, agent])\nprint(\"Success!\" if env.state[0].status == env.state[1].status == \"DONE\" else \"Failed...\")","metadata":{"execution":{"iopub.status.busy":"2021-12-20T06:55:31.670435Z","iopub.status.idle":"2021-12-20T06:55:31.671111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submit to Competition\n\n1. Commit this kernel.\n2. View the commited version.\n3. Go to \"Data\" section and find submission.py file.\n4. Click \"Submit to Competition\"\n5. Go to [My Submissions](https://kaggle.com/c/connectx/submissions) to view your score and episodes being played.","metadata":{}}]}