{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this notebook I'd like to explore how the environment is created and how it is passed to the agent.\n\n![](https://i.imgur.com/loGXjIN.png)\n\nAfter inspecting the environment, I give some pointers how to build a good agent with the current version of the kaggle Docker and some sources when Pytorch is available to play around with.\n\nThe environment itself give you a lot of info to work with, and we'll build a 'slightly better than random' agent.\n\nThis kernel is based on the [Getting Started](https://www.kaggle.com/ajeffries/connectx-getting-started) notebook."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"# 1. Enable Internet in the Kernel (Settings side pane)\n\n# 2. Curl cache may need purged if v0.1.4 cannot be found (uncomment if needed). \n# !curl -X PURGE https://pypi.org/simple/kaggle-environments\n\n# ConnectX environment was defined in v0.1.4\n!pip install 'kaggle-environments>=0.1.4'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Investigate ConnectX Environment\nLet's investigate, what Kaggle will be doing with this environment:"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from kaggle_environments import evaluate, make\n\nenv = make(\"connectx\", debug=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are some agents coming baked in, namely the `random` agent, that will create a baseline, if your final agent is doing better than flipping a coin. The `negamax` agent? We'll talk about that one below!"},{"metadata":{"trusted":true},"cell_type":"code","source":"env.agents","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env.configuration","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So clearly we can vary columns and rows, as well as the amount of tokens in a line to win. I guess this may be a nice test case for bigger games.\n\nBut there's also the amount of steps and a timeout variable. I'll venture a guess and say that your move is a maximum of 2 seconds.\n\nWith the specification commend, you'll be able to get a condensed dictionary that has most of the important information!"},{"metadata":{"trusted":true},"cell_type":"code","source":"env.specification","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We get the configuration from before and a bit of fluff. However, there's also the reward with the following values:\n\n- Loss: 0\n- Draw: 0.5\n- Win: 1\n\nand a good description of what to do as a valid action. Choose a column to drop your token in."},{"metadata":{},"cell_type":"markdown","source":"# Inspect inner Agent workings\n\nLet's create a little agent to inspect what objects we're even working with in this gym environment."},{"metadata":{"trusted":true},"cell_type":"code","source":"# This agent random chooses a non-empty column.\ndef my_agent(observation, configuration):\n    from random import choice\n    return choice([c for c in range(configuration.columns) if observation.board[c] == 0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And inspect the first iteration:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Play as first position against random agent.\ntrainer = env.train([None, \"random\"])\n\nobservation = trainer.reset()\n\nprint(\"Observation contains:\\t\", observation)\nprint(\"Configuration contains:\\t\", env.configuration)\n\nmy_action = my_agent(observation, env.configuration)\nprint(\"My Action\", my_action)\nobservation, reward, done, info = trainer.step(my_action)\n# env.render(mode=\"ipython\", width=100, height=90, header=False, controls=False)\nenv.render(mode=\"ipython\", width=100, height=90, header=False, controls=False)\nprint(\"Observation after:\\t\", observation)\n#env.render()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this it is clear that the board contains a flattened 1D array of the $7 \\times 6$ board as described in the specification. Zero contains non-occupied spaces, our tokens represent a one and our opponent has value token 2."},{"metadata":{"trusted":true},"cell_type":"code","source":"def my_comatose_agent(observation, configuration):\n    from random import choice\n    from time import sleep\n    sleep(2)\n    return choice([c for c in range(configuration.columns) if observation.board[c] == 0])\n\ndef my_sleepy_agent(observation, configuration):\n    from random import choice\n    from time import sleep\n    sleep(1)\n    return choice([c for c in range(configuration.columns) if observation.board[c] == 0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(evaluate(\"connectx\", [my_comatose_agent, \"random\"], num_episodes=1))\nprint(evaluate(\"connectx\", [my_sleepy_agent, \"random\"], num_episodes=1))\nprint(evaluate(\"connectx\", [my_agent, \"random\"], num_episodes=1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So each move has to be sub two seconds to be valid. This has some serious implications considering that we're in Python and most of these game strategies rely on some sort of minimax game. That means, we have to somewhat traverse the game-state space to know how well we're doing and that is costly. So essentially, play all hypothetical games and then make the optimal choice on that calculation."},{"metadata":{},"cell_type":"markdown","source":"# Taking The Hint\n\nIf you look in the starter notebook, you'll see that the evaluation is done against the following code:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def mean_reward(rewards):\n    return sum(r[0] for r in rewards) / sum(r[0] + r[1] for r in rewards)\n\nprint(\"My Agent vs Negamax Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=3)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The important bit is the keyword `negamax`. It's a [special version of the minimax](https://en.wikipedia.org/wiki/Negamax) strategy, that optimizes based on the symmetry, that in this two player game you are always doing better when your opponent is doing worse. So essentially, the game state is always \"I'm at score `0.7` so my opponent is at `0.3` -- then you're at `0.4` and they're at `0.6`. Just a fact from a two player game with [perfect information](https://en.wikipedia.org/wiki/Perfect_information).\n\n![](https://i.imgur.com/TBZXsYA.gif)\nCC-BY-SA 3.0 [Maschelos](https://en.wikipedia.org/wiki/File:Plain_Negamax.gif)\n\nIf you're interested in implementing your own Negamax strategy with all the optimizations, I find [this tutorial](http://blog.gamesolver.org/solving-connect-four/03-minmax/) exceptional, despite being in C++. These optimizations probably include [Alpha-Beta Pruning](https://en.wikipedia.org/wiki/Alpha%E2%80%93beta_pruning) and [Transposition Tables](https://en.wikipedia.org/wiki/Transposition_table).\n\nPersonally, I found the [EasyAI](https://github.com/Zulko/easyAI) implementation pretty understandable and worth diving into. \n\nLet's have a look at how kaggle approaches the negamax problem:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import inspect\nimport os\n\nprint(inspect.getsource(env.agents['negamax']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How well is negamax doing against itself?"},{"metadata":{"trusted":true},"cell_type":"code","source":"neg_v_neg = evaluate(\"connectx\", [env.agents['negamax'], \"negamax\"], num_episodes=10)\nprint(neg_v_neg)\nprint(mean_reward(neg_v_neg))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's odd..."},{"metadata":{},"cell_type":"markdown","source":"# Reinforcement Learning\nAs of writing this kernel, the challenge is limited to Standard Python, gym, numpy and scipy. So all the nice things in pytorch are not available yet. Once that is available, you can go on to some cool shenanigans essentially [mimicing AlphaZero](https://towardsdatascience.com/from-scratch-implementation-of-alphazero-for-connect4-f73d4554002a)\n\n![](https://miro.medium.com/max/850/1*4jBLXRsNVeOMBhOqO-8v8w.png)\n\nUntil then, we'll have to do something different. Let's try and do a bit better than random."},{"metadata":{},"cell_type":"markdown","source":"# Let's Try Something Stupid\nHow about, we try random choice, but just not take the step that will make us lose?\n\nAnd yes, this could be the first step toward implementing negamax. Considering, you have to simulate the games in a copy of the environment."},{"metadata":{"trusted":true},"cell_type":"code","source":"def try_not_to_loose_agent(observation, configuration):\n    from random import choice\n    from kaggle_environments import make\n    env = make(\"connectx\", debug=True)\n    trainer = env.train([None, \"negamax\"])\n    \n    cols = list(range(configuration.columns))\n    while cols:\n        # We set the state of the environment, so we can experiment on it.\n        env.state[0]['observation'] = observation\n        env.state[1]['observation'] = observation\n        # Take a random column that is not full\n        my_action = choice([c for c in cols if observation.board[c] == 0])\n        # Simulate the next step\n        out = env.train([None, \"negamax\"]).step(my_action)\n        # If the next step makes us lose, take a different step!\n        if out[2]:\n            cols.pop(my_action)\n        else:\n            return my_action\n    else:\n        # If we run out of steps to take, we just loose with one step.\n        return 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stupid_v_random = evaluate(\"connectx\", [try_not_to_loose_agent, \"random\"], num_episodes=10)\nprint(stupid_v_random)\nprint(mean_reward(stupid_v_random))\n\nstupid_v_neg = evaluate(\"connectx\", [try_not_to_loose_agent, \"negamax\"], num_episodes=10)\nprint(stupid_v_neg)\nprint(mean_reward(stupid_v_neg))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import inspect\nimport os\n\ndef write_agent_to_file(function, file):\n    with open(file, \"a\" if os.path.exists(file) else \"w\") as f:\n        f.write(inspect.getsource(function))\n        print(function, \"written to\", file)\n\nwrite_agent_to_file(try_not_to_loose_agent, \"submission.py\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Good luck!\nHope you find something interesting to work with!"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}