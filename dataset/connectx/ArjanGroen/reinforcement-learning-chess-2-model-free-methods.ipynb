{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Reinforcement Learning Chess \nReinforcement Learning Chess is a series of notebooks where I implement Reinforcement Learning algorithms to develop a chess AI. I start of with simpler versions (environments) that can be tackled with simple methods and gradually expand on those concepts untill I have a full-flegded chess AI. \n\n[**Notebook 1: Policy Iteration**](https://www.kaggle.com/arjanso/reinforcement-learning-chess-1-policy-iteration)  \n[**Notebook 3: Q-networks**](https://www.kaggle.com/arjanso/reinforcement-learning-chess-3-q-networks)  \n[**Notebook 4: Policy Gradients**](https://www.kaggle.com/arjanso/reinforcement-learning-chess-4-policy-gradients)  \n[**Notebook 5: Monte Carlo Tree Search**](https://www.kaggle.com/arjanso/reinforcement-learning-chess-5-tree-search)  "},{"metadata":{},"cell_type":"markdown","source":"# Notebook II: Model-free control\nIn this notebook I use the same move-chess environment as in notebook 1. In this notebook I mentioned that policy evaluation calculates the state value by backing up the successor state values and the transition probabilities to those states. The problem is that these probabilities are usually unknown in real-world problems. Luckily there are control techniques that can work in these unknown environments. These techniques don't leverage any prior knowledge about the environment's dynamics, they are model-free."},{"metadata":{"trusted":true},"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport inspect","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"!pip install --upgrade git+https://github.com/arjangroen/RLC.git  # RLC is the Reinforcement Learning package","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from RLC.move_chess.environment import Board\nfrom RLC.move_chess.agent import Piece\nfrom RLC.move_chess.learn import Reinforce","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The environment\n- The state space is a 8 by 8 grid\n- The starting state S is the top-left square (0,0)\n- The terminal state F is square (5,7). \n- Every move from state to state gives a reward of minus 1\n- Naturally the best policy for this evironment is to move from S to F in the lowest amount of moves possible."},{"metadata":{"trusted":true},"cell_type":"code","source":"env = Board()\nenv.render()\nenv.visual_board","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The agent\n- The agent is a chess Piece (king, queen, rook, knight or bishop)\n- The agent has a behavior policy determining what the agent does in what state"},{"metadata":{"trusted":true},"cell_type":"code","source":"p = Piece(piece='king')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reinforce\n- The reinforce object contains the algorithms for solving move chess\n- The agent and the environment are attributes of the Reinforce object"},{"metadata":{"trusted":true},"cell_type":"code","source":"r = Reinforce(p,env)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.1 Monte Carlo Control"},{"metadata":{},"cell_type":"markdown","source":"**Theory**  \nThe basic intuition is:\n* We do not know the environment, so we sample an episode from beginning to end by running our current policy\n* We try to estimate the action-values rather than the state values. This is because we are working model-free so just knowning state values won't help us select the best actions. \n* The value of a state-action value is defined as the future returns from the first visit of that state-action\n* Based on this we can improve our policy and repeat the process untill the algorithm converges\n\n![](http://incompleteideas.net/book/first/ebook/pseudotmp5.png)"},{"metadata":{},"cell_type":"markdown","source":"**Implementation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(inspect.getsource(r.monte_carlo_learning))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Demo**  \nWe do 100 iterations of monte carlo learning while maintaining a high exploration rate of 0.5:"},{"metadata":{"trusted":true},"cell_type":"code","source":"for k in range(100):\n    eps = 0.5\n    r.monte_carlo_learning(epsilon=eps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r.visualize_policy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Best action value for each state:"},{"metadata":{"trusted":true},"cell_type":"code","source":"r.agent.action_function.max(axis=2).astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.2 Temporal Difference Learning "},{"metadata":{},"cell_type":"markdown","source":"**Theory**\n* Like Policy Iteration, we can back up state-action values from the successor state action without waiting for the episode to end. \n* We update our state-action value in the direction of the successor state action value.\n* The algorithm is called SARSA: State-Action-Reward-State-Action.\n* Epsilon is gradually lowered (the GLIE property)"},{"metadata":{},"cell_type":"markdown","source":"**Implementation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(inspect.getsource(r.sarsa_td))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Demonstration**"},{"metadata":{"trusted":true},"cell_type":"code","source":"p = Piece(piece='king')\nenv = Board()\nr = Reinforce(p,env)\nr.sarsa_td(n_episodes=10000,alpha=0.2,gamma=0.9)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r.visualize_policy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.3 TD-lambda\n**Theory**  \nIn Monte Carlo we do a full-depth backup while in Temporal Difference Learning we de a 1-step backup. You could also choose a depth in-between: backup by n steps. But what value to choose for n?\n* TD lambda uses all n-steps and discounts them with factor lambda\n* This is called lambda-returns\n* TD-lambda uses an eligibility-trace to keep track of the previously encountered states\n* This way action-values can be updated in retrospect"},{"metadata":{},"cell_type":"markdown","source":"**Implementation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(inspect.getsource(r.sarsa_lambda))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Demonstration**"},{"metadata":{"trusted":true},"cell_type":"code","source":"p = Piece(piece='king')\nenv = Board()\nr = Reinforce(p,env)\nr.sarsa_lambda(n_episodes=10000,alpha=0.2,gamma=0.9)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r.visualize_policy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.4 Q-learning"},{"metadata":{},"cell_type":"markdown","source":"**Theory**\n* In SARSA/TD0, we back-up our action values with the succesor action value\n* In SARSA-max/Q learning, we back-up using the maximum action value. "},{"metadata":{},"cell_type":"markdown","source":"**Implementation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(inspect.getsource(r.sarsa_lambda))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Demonstration**"},{"metadata":{"trusted":true},"cell_type":"code","source":"p = Piece(piece='king')\nenv = Board()\nr = Reinforce(p,env)\nr.q_learning(n_episodes=1000,alpha=0.2,gamma=0.9)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r.visualize_policy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r.agent.action_function.max(axis=2).round().astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# References\n1. Reinforcement Learning: An Introduction  \n   Richard S. Sutton and Andrew G. Barto  \n   1st Edition  \n   MIT Press, march 1998\n2. RL Course by David Silver: Lecture playlist  \n   https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}