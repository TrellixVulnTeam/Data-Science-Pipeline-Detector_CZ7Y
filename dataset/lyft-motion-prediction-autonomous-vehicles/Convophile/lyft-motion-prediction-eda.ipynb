{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"<center><h1> Lyft: EDA </h1></center>\n<img src=\"https://storage.googleapis.com/kaggle-media/competitions/Lyft-Kaggle/Motion%20Prediction/BP9I1484%20(1).jpg\">","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 1. What are we asked to predict in this competition?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"So welcome kagglers. In this Lyft competition, we are asked to predict the motion of self driving vehicles. And this knowledge can be used to predict  how cars, cyclists,and pedestrians move in the AV's environment.We have to predict the location of objects agents in the next 50 frames.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Evaluation Metric: Negative log-likelihood \n\nWe calculate the negative log-likelihood of the ground truth data given the multi-modal predictions. You can get more information [here](https://www.kaggle.com/c/lyft-motion-prediction-autonomous-vehicles/overview/evaluation).\n![](https://camo.githubusercontent.com/b3634eea5be5501318957e21086781666018efa1/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f25354362675f77686974652532302535436c617267652532304c2532302533442532302d2532302535436c6f6725323070253238785f253742312532432532302535436c646f747325324325323054253744253243253230795f253742312532432532302535436c646f74732532432532305425374425374363253545253742312532432532302535436c646f74732532432532304b253744253243253230253543626172253742782537445f253742312532432532302535436c646f747325324325323054253744253545253742312532432532302535436c646f74732532432532304b253744253243253230253543626172253742792537445f253742312532432532302535436c646f747325324325323054253744253545253742312532432532302535436c646f74732532432532304b253744253239)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 2. What is the difference between this and previous competition?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"[In the previous competition, we were tasked with detecting three dimensional objects that we see normally on the road to teach AV's how to recognize these. ](https://www.kaggle.com/c/3d-object-detection-for-autonomous-vehicles/)\n<center><img src=\"https://storage.googleapis.com/kaggle-media/competitions/Lyft-Kaggle/Kaggle-01.png\" width=\"100%\"></center>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 3. Previous Competition Winners Solutions","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<center><img src=\"https://i.imgur.com/8dUga6i.jpg\" width=\"1000px\"></center> ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1. [1st place solution](https://www.kaggle.com/c/3d-object-detection-for-autonomous-vehicles/discussion/122820)\n2. [2nd place solution](https://www.kaggle.com/c/3d-object-detection-for-autonomous-vehicles/discussion/123004)\n3. [3rd place solution](https://www.kaggle.com/c/3d-object-detection-for-autonomous-vehicles/discussion/117269)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 4. How files are stored?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The files are stored in the [.zarr file](https://zarr.readthedocs.io/en/stable/) format with Python, which we can easily load using the Level 5 Kit (l5kit for the pip package). Within our training ZARRs, we have the agents, the masks for agents, frames and scenes (which you might recollect from last year) and traffic light faces.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<center><h2>A self driving car in action </h2></center>\nBefore we dive into the technical details of this kernel, let us watch an interesting video of a self-driving car in action.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import HTML\nHTML('<center><iframe width=\"700\" height=\"400\" src=\"https://www.youtube.com/embed/tlThdr3O5Qo?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe></center>')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<center><h2> Let's explore the data</h2></center>\n\n# 5. Necessary Imports\n\n## L5Kit is a library which lets you:\n\n* Load driving scenes from zarr files\n* Read semantic maps\n* Read aerial maps\n* Create birds-eye-view (BEV) images which represent a scene around an AV or another vehicle\n* Sample data\n* Train neural networks\n* Visualize results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pymap3d==2.1.0\n!pip install -U l5kit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import l5kit, os\nfrom l5kit.rasterization import build_rasterizer\nfrom l5kit.configs import load_config_data\nfrom l5kit.visualization import draw_trajectory, TARGET_POINTS_COLOR\nfrom l5kit.geometry import transform_points\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom l5kit.data import PERCEPTION_LABELS\nfrom prettytable import PrettyTable\nfrom l5kit.data import ChunkedDataset, LocalDataManager\nfrom l5kit.dataset import EgoDataset, AgentDataset\nfrom IPython.display import display, clear_output\nimport PIL\nfrom IPython.display import display, clear_output\nfrom IPython.display import HTML\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nfrom PIL import Image\nfrom datetime import time,date\nimport nltk\nimport spacy\nimport re","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's look at the scenes.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# A. Sample Zarr","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"os.environ[\"L5KIT_DATA_FOLDER\"] = \"../input/lyft-motion-prediction-autonomous-vehicles\"\ndm = LocalDataManager()\nsample_path = '../input/lyft-motion-prediction-autonomous-vehicles/scenes/sample.zarr'\nsample_dataset = ChunkedDataset(sample_path)\nsample_dataset.open()\nprint(sample_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_agents = sample_dataset.agents\nsample_agents = pd.DataFrame(sample_agents)\nsample_agents.columns = [\"data\"]; features = ['centroid', 'extent', 'yaw', 'velocity', 'track_id', 'label_probabilities']\n\nfor i, feature in enumerate(features):\n    sample_agents[feature] = sample_agents['data'].apply(lambda x: x[i])\nsample_agents.drop(columns=[\"data\"],inplace=True)\nsample_agents.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del sample_agents","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# B. Test Zarr","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_path = '../input/lyft-motion-prediction-autonomous-vehicles/scenes/test.zarr'\ntest_dataset = ChunkedDataset(test_path)\ntest_dataset.open()\nprint(test_dataset)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# C. Train Zarr","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_path = '../input/lyft-motion-prediction-autonomous-vehicles/scenes/train.zarr'\ntrain_dataset = ChunkedDataset(train_path)\ntrain_dataset.open()\nprint(train_dataset)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# D. Validation Zarr","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_path = '../input/lyft-motion-prediction-autonomous-vehicles/scenes/validate.zarr'\nvalid_dataset = ChunkedDataset(valid_path)\nvalid_dataset.open()\nprint(valid_dataset)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we have seen the basic exploration above for 4 different ZARR's we will now explore them one by one in detail now","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# SEMANTIC VIEW","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cfg = {}\nraster_params = {'raster_size':[512,512],\n                 'pixel_size':[0.5,0.5],\n                 'ego_center':[0.25,0.5],\n                 'map_type':'py_semantic',\n                 'satellite_map_key': 'aerial_map/aerial_map.png',\n                 'semantic_map_key': 'semantic_map/semantic_map.pb',\n                 'dataset_meta_key': 'meta.json',\n                 'filter_agents_threshold': 0.5}\nmodel_params ={'model_architecture': 'effnetB5',\n               'history_num_frames': 0,\n               'history_step_size': 1,\n               'history_delta_time': 0.1,\n               'future_num_frames': 50,\n               'future_step_size': 1,\n               'future_delta_time': 0.1}\ncfg['raster_params'] = raster_params\ncfg['model_params'] = model_params\nrast = build_rasterizer(cfg,dm)\ndataset = EgoDataset(cfg, sample_dataset, rast)\nscene_idx = 2\nindexes = dataset.get_scene_indices(scene_idx)\nimages = []\n\nfor idx in indexes:\n    \n    data = dataset[idx]\n    im = data[\"image\"].transpose(1, 2, 0)\n    im = dataset.rasterizer.to_rgb(im)\n    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n    draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n    clear_output(wait=True)\n    display(PIL.Image.fromarray(im[::-1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>As we can see there's a lot of information in this image itself. Let's discuss about this image now.<h3>\n\n* We see around ten roads in the picture. 4 are main and remaining are inside some area like flat or mall or something like that.But we can see vehicles are on the 4 roads.So we can continue our discussion on these 4 roads alone.\n* The green line represents the autonomous vehicle in the road and its movement we want to predict in this competition.\n* The blue line represents the real vehicle in the road.\n* We can also see the vehicles moving.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cfg = {}\nraster_params = {'raster_size':[512,512],\n                 'pixel_size':[0.5,0.5],\n                 'ego_center':[0.25,0.5],\n                 'map_type':'py_semantic',\n                 'satellite_map_key': 'aerial_map/aerial_map.png',\n                 'semantic_map_key': 'semantic_map/semantic_map.pb',\n                 'dataset_meta_key': 'meta.json',\n                 'filter_agents_threshold': 0.5}\nmodel_params ={'model_architecture': 'effnetB5',\n               'history_num_frames': 0,\n               'history_step_size': 1,\n               'history_delta_time': 0.1,\n               'future_num_frames': 50,\n               'future_step_size': 1,\n               'future_delta_time': 0.1}\ncfg['raster_params'] = raster_params\ncfg['model_params'] = model_params\nrast = build_rasterizer(cfg,dm)\ndataset = EgoDataset(cfg, train_dataset, rast)\nscene_idx = 2\nindexes = dataset.get_scene_indices(scene_idx)\nimages = []\n\nfor idx in indexes:\n    \n    data = dataset[idx]\n    im = data[\"image\"].transpose(1, 2, 0)\n    im = dataset.rasterizer.to_rgb(im)\n    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n    draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n    clear_output(wait=True)\n    display(PIL.Image.fromarray(im[::-1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cfg = {}\nraster_params = {'raster_size':[512,512],\n                 'pixel_size':[0.5,0.5],\n                 'ego_center':[0.25,0.5],\n                 'map_type':'py_semantic',\n                 'satellite_map_key': 'aerial_map/aerial_map.png',\n                 'semantic_map_key': 'semantic_map/semantic_map.pb',\n                 'dataset_meta_key': 'meta.json',\n                 'filter_agents_threshold': 0.5}\nmodel_params ={'model_architecture': 'effnetB5',\n               'history_num_frames': 0,\n               'history_step_size': 1,\n               'history_delta_time': 0.1,\n               'future_num_frames': 50,\n               'future_step_size': 1,\n               'future_delta_time': 0.1}\ncfg['raster_params'] = raster_params\ncfg['model_params'] = model_params\nrast = build_rasterizer(cfg,dm)\ndataset = EgoDataset(cfg, valid_dataset, rast)\nscene_idx = 2\nindexes = dataset.get_scene_indices(scene_idx)\nimages = []\n\nfor idx in indexes:\n    \n    data = dataset[idx]\n    im = data[\"image\"].transpose(1, 2, 0)\n    im = dataset.rasterizer.to_rgb(im)\n    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n    draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n    clear_output(wait=True)\n    display(PIL.Image.fromarray(im[::-1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cfg = {}\nraster_params = {'raster_size':[512,512],\n                 'pixel_size':[0.5,0.5],\n                 'ego_center':[0.25,0.5],\n                 'map_type':'py_semantic',\n                 'satellite_map_key': 'aerial_map/aerial_map.png',\n                 'semantic_map_key': 'semantic_map/semantic_map.pb',\n                 'dataset_meta_key': 'meta.json',\n                 'filter_agents_threshold': 0.5}\nmodel_params ={'model_architecture': 'effnetB5',\n               'history_num_frames': 0,\n               'history_step_size': 1,\n               'history_delta_time': 0.1,\n               'future_num_frames': 50,\n               'future_step_size': 1,\n               'future_delta_time': 0.1}\ncfg['raster_params'] = raster_params\ncfg['model_params'] = model_params\nrast = build_rasterizer(cfg,dm)\ndataset = EgoDataset(cfg, test_dataset, rast)\nscene_idx = 2\nindexes = dataset.get_scene_indices(scene_idx)\nimages = []\n\nfor idx in indexes:\n    \n    data = dataset[idx]\n    im = data[\"image\"].transpose(1, 2, 0)\n    im = dataset.rasterizer.to_rgb(im)\n    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n    draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n    clear_output(wait=True)\n    display(PIL.Image.fromarray(im[::-1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Satellite view","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cfg[\"raster_params\"][\"map_type\"] = \"py_satellite\"\nrast = build_rasterizer(cfg, dm)\ndataset = EgoDataset(cfg, sample_dataset, rast)\nscene_idx = 2\nindexes = dataset.get_scene_indices(scene_idx)\nimages = []\n\nfor idx in indexes:\n    \n    data = dataset[idx]\n    im = data[\"image\"].transpose(1, 2, 0)\n    im = dataset.rasterizer.to_rgb(im)\n    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n    draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n    clear_output(wait=True)\n    display(PIL.Image.fromarray(im[::-1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the satellite view gives us better info than the previous view.\nWe will now see the trajectory path of someother scene.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cfg[\"raster_params\"][\"map_type\"] = \"py_satellite\"\nrast = build_rasterizer(cfg, dm)\ndataset = EgoDataset(cfg, train_dataset, rast)\nscene_idx = 2\nindexes = dataset.get_scene_indices(scene_idx)\nimages = []\n\nfor idx in indexes:\n    \n    data = dataset[idx]\n    im = data[\"image\"].transpose(1, 2, 0)\n    im = dataset.rasterizer.to_rgb(im)\n    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n    draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n    clear_output(wait=True)\n    display(PIL.Image.fromarray(im[::-1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cfg[\"raster_params\"][\"map_type\"] = \"py_satellite\"\nrast = build_rasterizer(cfg, dm)\ndataset = EgoDataset(cfg, test_dataset, rast)\nscene_idx = 2\nindexes = dataset.get_scene_indices(scene_idx)\nimages = []\n\nfor idx in indexes:\n    \n    data = dataset[idx]\n    im = data[\"image\"].transpose(1, 2, 0)\n    im = dataset.rasterizer.to_rgb(im)\n    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n    draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n    clear_output(wait=True)\n    display(PIL.Image.fromarray(im[::-1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cfg[\"raster_params\"][\"map_type\"] = \"py_satellite\"\nrast = build_rasterizer(cfg, dm)\ndataset = EgoDataset(cfg, valid_dataset, rast)\nscene_idx = 2\nindexes = dataset.get_scene_indices(scene_idx)\nimages = []\n\nfor idx in indexes:\n    \n    data = dataset[idx]\n    im = data[\"image\"].transpose(1, 2, 0)\n    im = dataset.rasterizer.to_rgb(im)\n    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n    draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n    clear_output(wait=True)\n    display(PIL.Image.fromarray(im[::-1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cfg[\"raster_params\"][\"map_type\"] = \"py_satellite\"\nrast = build_rasterizer(cfg, dm)\ndataset = EgoDataset(cfg, sample_dataset, rast)\nscene_idx = 34\nindexes = dataset.get_scene_indices(scene_idx)\nimages = []\n\nfor idx in indexes:\n    \n    data = dataset[idx]\n    im = data[\"image\"].transpose(1, 2, 0)\n    im = dataset.rasterizer.to_rgb(im)\n    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n    draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n    clear_output(wait=True)\n    display(PIL.Image.fromarray(im[::-1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above satellite view we can see that the pink line is the vehicle's movement. Well let's animate and try to make into video.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.animation as animation\ncfg[\"raster_params\"][\"map_type\"] = \"py_satellite\"\nrast = build_rasterizer(cfg, dm)\ndataset = EgoDataset(cfg, sample_dataset, rast)\nscene_idx = 34\nindexes = dataset.get_scene_indices(scene_idx)\nimages = []\nfig = plt.figure()\nfor idx in indexes:\n    \n    data = dataset[idx]\n    im = data[\"image\"].transpose(1, 2, 0)\n    im = dataset.rasterizer.to_rgb(im)\n    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n    draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n    clear_output(wait=True)\n    images.append(PIL.Image.fromarray(im[::-1]))\nim = plt.imshow(images[0], animated=True)\nplt.axis('off')\ndef animate(i):\n    im.set_data(images[i])\nani = animation.FuncAnimation(fig, animate, interval=100, blit=False,\n                                repeat_delay=1000)\nHTML(ani.to_html5_video())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above video we can see the movement of vehicles and movement of our Autonomous Vehicle(Pink path). And also we can see that our AV takes a straight path.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import imageio\nimport IPython.display\nimageio.mimsave(\"/tmp/gif.gif\", images, duration=0.0001)\nIPython.display.Image(filename=\"/tmp/gif.gif\", format='png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above gif file gives us better view than the above video","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Aerial Map","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"image = cv2.imread('../input/lyft-motion-prediction-autonomous-vehicles/aerial_map/aerial_map.png')\nimage = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\nplt.figure(figsize=(32,32))\nplt.imshow(image)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The general view of the street us borrowed from [this notebook](https://www.kaggle.com/t3nyks/lyft-working-with-map-api)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from l5kit.data.map_api import MapAPI\nfrom l5kit.rasterization.rasterizer_builder import _load_metadata\n\nsemantic_map_filepath = dm.require(cfg[\"raster_params\"][\"semantic_map_key\"])\ndataset_meta = _load_metadata(cfg[\"raster_params\"][\"dataset_meta_key\"], dm)\nworld_to_ecef = np.array(dataset_meta[\"world_to_ecef\"], dtype=np.float64)\n\nmap_api = MapAPI(semantic_map_filepath, world_to_ecef)\nMAP_LAYERS = [\"junction\", \"node\", \"segment\", \"lane\"]\n\n\ndef element_of_type(elem, layer_name):\n    return elem.element.HasField(layer_name)\n\n\ndef get_elements_from_layer(map_api, layer_name):\n    return [elem for elem in map_api.elements if element_of_type(elem, layer_name)]\n\n\nclass MapRenderer:\n    \n    def __init__(self, map_api):\n        self._color_map = dict(drivable_area='#a6cee3',\n                               road_segment='#1f78b4',\n                               road_block='#b2df8a',\n                               lane='#474747')\n        self._map_api = map_api\n    \n    def render_layer(self, layer_name):\n        fig = plt.figure(figsize=(10, 10))\n        ax = fig.add_axes([0, 0, 1, 1])\n        \n    def render_lanes(self):\n        all_lanes = get_elements_from_layer(self._map_api, \"lane\")\n        fig = plt.figure(figsize=(10, 10))\n        ax = fig.add_axes([0, 0, 1, 1])\n        for lane in all_lanes:\n            self.render_lane(ax, lane)\n        return fig, ax\n        \n    def render_lane(self, ax, lane):\n        coords = self._map_api.get_lane_coords(MapAPI.id_as_str(lane.id))\n        self.render_boundary(ax, coords[\"xyz_left\"])\n        self.render_boundary(ax, coords[\"xyz_right\"])\n        \n    def render_boundary(self, ax, boundary):\n        xs = boundary[:, 0]\n        ys = boundary[:, 1] \n        ax.plot(xs, ys, color=self._color_map[\"lane\"], label=\"lane\")\n        \n        \nrenderer = MapRenderer(map_api)\nfig, ax = renderer.render_lanes()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's Explore Zarr","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"scenes\", sample_dataset.scenes)\nprint(\"scenes[0]\", sample_dataset.scenes[0])\nprint(\"scenes\", test_dataset.scenes)\nprint(\"scenes[0]\", test_dataset.scenes[0])\nprint(\"scenes\", train_dataset.scenes)\nprint(\"scenes[0]\", train_dataset.scenes[0])\nprint(\"scenes\", valid_dataset.scenes)\nprint(\"scenes[0]\", valid_dataset.scenes[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Credits :\n[Lyft: Understanding the data + baseline model by Trigram(@nxrprime)](https://www.kaggle.com/nxrprime/lyft-understanding-the-data-baseline-model)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<center><h1> Thanks for Reading My Kernel </h1></center>\n<center><h2> Please upvote if you find it useful </h2></center>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}