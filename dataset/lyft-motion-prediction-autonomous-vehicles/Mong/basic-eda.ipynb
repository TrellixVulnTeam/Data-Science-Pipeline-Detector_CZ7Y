{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom numpy.testing import assert_equal\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nfrom l5kit.data import LocalDataManager, ChunkedDataset\nfrom l5kit.dataset import AgentDataset, EgoDataset\nfrom l5kit.evaluation import write_pred_csv\nfrom l5kit.rasterization import build_rasterizer\n\nfrom l5kit.visualization import draw_trajectory, TARGET_POINTS_COLOR\nfrom l5kit.geometry import transform_points\n\nfrom IPython.display import display, clear_output\nfrom IPython.display import HTML\nimport PIL\n\nimport tifffile\n\nfrom matplotlib import animation, rc\n\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"multi_submission = pd.read_csv('../input/lyft-motion-prediction-autonomous-vehicles/multi_mode_sample_submission.csv')\nsingle_submission = pd.read_csv('../input/lyft-motion-prediction-autonomous-vehicles/single_mode_sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multi_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"single_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assert_equal(multi_submission['timestamp'].unique(), single_submission['timestamp'].unique())\nassert_equal(multi_submission['track_id'].unique(), single_submission['track_id'].unique())\nassert_equal(multi_submission['conf_0'].unique(), single_submission['conf_0'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multi_submission.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"single_submission.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assert_equal(multi_submission.columns.tolist(), single_submission.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cfg = {\n    'format_version': 4,\n    'model_params': {\n        'history_num_frames': 0,\n        'history_step_size': 1,\n        'history_delta_time': 0.1,\n        'future_num_frames': 50,\n        'future_step_size': 1,\n        'future_delta_time': 0.1\n    },\n    \n    'raster_params': {\n        'raster_size': [300, 300],\n        'pixel_size': [0.5, 0.5],\n        'ego_center': [0.25, 0.5],\n        'map_type': 'py_semantic',\n        'satellite_map_key': 'aerial_map/aerial_map.png',\n        'semantic_map_key': 'semantic_map/semantic_map.pb',\n        'dataset_meta_key': 'meta.json',\n        'filter_agents_threshold': 0.5\n    },\n    'train_data_loader': {\n        'key': 'scenes/train.zarr',\n        'batch_size': 8,\n        'shuffle': True,\n        'num_workers': 4\n    },\n    'test_data_loader': {\n        'key': 'scenes/test.zarr',\n        'batch_size': 32,\n        'shuffle': False,\n        'num_workers': 4\n    }\n}\n\n# set env variable for data\nos.environ[\"L5KIT_DATA_FOLDER\"] = '../input/lyft-motion-prediction-autonomous-vehicles'\ndm = LocalDataManager(None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cfg[\"raster_params\"][\"map_type\"] = \"py_satellite\"\nrast = build_rasterizer(cfg, dm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Scenes"},{"metadata":{},"cell_type":"markdown","source":"## Sample"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_zarr_dataset = ChunkedDataset('../input/lyft-motion-prediction-autonomous-vehicles/scenes/sample.zarr')\nsample_zarr_dataset.open()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sample_zarr_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frames = sample_zarr_dataset.frames\ncoords = np.zeros((len(frames), 2))\nfor idx_coord, idx_data in enumerate(tqdm(range(len(frames)), desc=\"getting centroid to plot trajectory\")):\n    frame = sample_zarr_dataset.frames[idx_data]\n    coords[idx_coord] = frame[\"ego_translation\"][:2]\n\n\nplt.scatter(coords[:, 0], coords[:, 1], marker='.')\naxes = plt.gca()\naxes.set_xlim([-2500, 1600])\naxes.set_ylim([-2500, 1600])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cfg[\"raster_params\"][\"map_type\"] = \"py_satellite\"\nrast = build_rasterizer(cfg, dm)\ndataset_ego = EgoDataset(cfg, sample_zarr_dataset, rast)\ndata = dataset_ego[0]\nim1 = data[\"image\"].transpose(1, 2, 0)\nim1 = dataset_ego.rasterizer.to_rgb(im1)\n\ncfg[\"raster_params\"][\"map_type\"] = \"py_semantic\"\nrast = build_rasterizer(cfg, dm)\ndataset_ego = EgoDataset(cfg, sample_zarr_dataset, rast)\ndata = dataset_ego[0]\nim2 = data[\"image\"].transpose(1, 2, 0)\nim2 = dataset_ego.rasterizer.to_rgb(im2)\ntarget_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\ndraw_trajectory(im2, target_positions_pixels, yaws=data[\"target_yaws\"], radius=1, rgb_color=TARGET_POINTS_COLOR)\n\n_, ax = plt.subplots(1,2, figsize = (7, 7))\nax[0].imshow(im1[::-1])\nax[0].title.set_text('Object Detection')\nax[1].imshow(im2[::-1])\nax[1].title.set_text('Trajectory Simulation')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# keep animation in the notebook\ndef animate_solution(images):\n    def animate(i):\n        im.set_data(images[i])\n        \n    fig, ax = plt.subplots()\n    im = ax.imshow(images[0])\n    \n    return animation.FuncAnimation(fig, animate, frames = len(images), interval = 60)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cfg[\"raster_params\"][\"map_type\"] = \"py_semantic\"\nrast = build_rasterizer(cfg, dm)\ndataset = EgoDataset(cfg, sample_zarr_dataset, rast)\nscene_idx = 2\nindexes = dataset.get_scene_indices(scene_idx)\nimages = []\n\nfor idx in indexes:\n    \n    data = dataset[idx]\n    im = data[\"image\"].transpose(1, 2, 0)\n    im = dataset.rasterizer.to_rgb(im)\n    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n    draw_trajectory(im, target_positions_pixels, yaws=data[\"target_yaws\"], radius=1, rgb_color=TARGET_POINTS_COLOR)\n    clear_output(wait=True)\n    images.append(PIL.Image.fromarray(im[::-1]))\n    \nHTML(animate_solution(images).to_jshtml())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_zarr_dataset = ChunkedDataset('../input/lyft-motion-prediction-autonomous-vehicles/scenes/test.zarr')\ntest_zarr_dataset.open()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_zarr_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cfg[\"raster_params\"][\"map_type\"] = \"py_satellite\"\nrast = build_rasterizer(cfg, dm)\ndataset_ego = EgoDataset(cfg, test_zarr_dataset, rast)\ndata = dataset_ego[0]\nim1 = data[\"image\"].transpose(1, 2, 0)\nim1 = dataset_ego.rasterizer.to_rgb(im1)\n\ncfg[\"raster_params\"][\"map_type\"] = \"py_semantic\"\nrast = build_rasterizer(cfg, dm)\ndataset_ego = EgoDataset(cfg, test_zarr_dataset, rast)\ndata = dataset_ego[0]\nim2 = data[\"image\"].transpose(1, 2, 0)\nim2 = dataset_ego.rasterizer.to_rgb(im2)\ntarget_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\ndraw_trajectory(im2, target_positions_pixels, yaws=data[\"target_yaws\"], radius=1, rgb_color=TARGET_POINTS_COLOR)\n\n_, ax = plt.subplots(1,2, figsize = (7, 7))\nax[0].imshow(im1[::-1])\nax[0].title.set_text('Object Detection')\nax[1].imshow(im2[::-1])\nax[1].title.set_text('Trajectory Simulation')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frames = test_zarr_dataset.frames\ncoords = np.zeros((len(frames), 2))\nfor idx_coord, idx_data in enumerate(tqdm(range(len(frames)), desc=\"getting centroid to plot trajectory\")):\n    frame = test_zarr_dataset.frames[idx_data]\n    coords[idx_coord] = frame[\"ego_translation\"][:2]\n\n\nplt.scatter(coords[:, 0], coords[:, 1], marker='.')\naxes = plt.gca()\naxes.set_xlim([-2500, 1600])\naxes.set_ylim([-2500, 1600])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cfg[\"raster_params\"][\"map_type\"] = \"py_semantic\"\nrast = build_rasterizer(cfg, dm)\ndataset = EgoDataset(cfg, test_zarr_dataset, rast)\nscene_idx = 2\nindexes = dataset.get_scene_indices(scene_idx)\nimages = []\n\nfor idx in indexes:\n    \n    data = dataset[idx]\n    im = data[\"image\"].transpose(1, 2, 0)\n    im = dataset.rasterizer.to_rgb(im)\n    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n    draw_trajectory(im, target_positions_pixels, yaws=data[\"target_yaws\"], radius=1, rgb_color=TARGET_POINTS_COLOR)\n    clear_output(wait=True)\n    images.append(PIL.Image.fromarray(im[::-1]))\n    \nHTML(animate_solution(images).to_jshtml())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_zarr_dataset = ChunkedDataset('../input/lyft-motion-prediction-autonomous-vehicles/scenes/train.zarr')\ntrain_zarr_dataset.open()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_zarr_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cfg[\"raster_params\"][\"map_type\"] = \"py_satellite\"\nrast = build_rasterizer(cfg, dm)\ndataset_ego = EgoDataset(cfg, train_zarr_dataset, rast)\ndata = dataset_ego[0]\nim1 = data[\"image\"].transpose(1, 2, 0)\nim1 = dataset_ego.rasterizer.to_rgb(im1)\n\ncfg[\"raster_params\"][\"map_type\"] = \"py_semantic\"\nrast = build_rasterizer(cfg, dm)\ndataset_ego = EgoDataset(cfg, train_zarr_dataset, rast)\ndata = dataset_ego[0]\nim2 = data[\"image\"].transpose(1, 2, 0)\nim2 = dataset_ego.rasterizer.to_rgb(im2)\ntarget_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\ndraw_trajectory(im2, target_positions_pixels, yaws=data[\"target_yaws\"], radius=1, rgb_color=TARGET_POINTS_COLOR)\n\n_, ax = plt.subplots(1,2, figsize = (7, 7))\nax[0].imshow(im1[::-1])\nax[0].title.set_text('Object Detection')\nax[1].imshow(im2[::-1])\nax[1].title.set_text('Trajectory Simulation')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frames = train_zarr_dataset.frames\ncoords = np.zeros((len(frames), 2))\nfor idx_coord, idx_data in enumerate(tqdm(range(len(frames)), desc=\"getting centroid to plot trajectory\")):\n    frame = train_zarr_dataset.frames[idx_data]\n    coords[idx_coord] = frame[\"ego_translation\"][:2]\n\n\nplt.scatter(coords[:, 0], coords[:, 1], marker='.')\naxes = plt.gca()\naxes.set_xlim([-2500, 1600])\naxes.set_ylim([-2500, 1600])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cfg[\"raster_params\"][\"map_type\"] = \"py_semantic\"\nrast = build_rasterizer(cfg, dm)\ndataset = EgoDataset(cfg, train_zarr_dataset, rast)\nscene_idx = 2\nindexes = dataset.get_scene_indices(scene_idx)\nimages = []\n\nfor idx in indexes:\n    \n    data = dataset[idx]\n    im = data[\"image\"].transpose(1, 2, 0)\n    im = dataset.rasterizer.to_rgb(im)\n    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n    draw_trajectory(im, target_positions_pixels, yaws=data[\"target_yaws\"], radius=1, rgb_color=TARGET_POINTS_COLOR)\n    clear_output(wait=True)\n    images.append(PIL.Image.fromarray(im[::-1]))\n    \nHTML(animate_solution(images).to_jshtml())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Validate"},{"metadata":{"trusted":true},"cell_type":"code","source":"validate_zarr_dataset = ChunkedDataset('../input/lyft-motion-prediction-autonomous-vehicles/scenes/validate.zarr')\nvalidate_zarr_dataset.open()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(validate_zarr_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cfg[\"raster_params\"][\"map_type\"] = \"py_satellite\"\nrast = build_rasterizer(cfg, dm)\ndataset_ego = EgoDataset(cfg, validate_zarr_dataset, rast)\ndata = dataset_ego[0]\nim1 = data[\"image\"].transpose(1, 2, 0)\nim1 = dataset_ego.rasterizer.to_rgb(im1)\n\ncfg[\"raster_params\"][\"map_type\"] = \"py_semantic\"\nrast = build_rasterizer(cfg, dm)\ndataset_ego = EgoDataset(cfg, validate_zarr_dataset, rast)\ndata = dataset_ego[0]\nim2 = data[\"image\"].transpose(1, 2, 0)\nim2 = dataset_ego.rasterizer.to_rgb(im2)\ntarget_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\ndraw_trajectory(im2, target_positions_pixels, yaws=data[\"target_yaws\"], radius=1, rgb_color=TARGET_POINTS_COLOR)\n\n_, ax = plt.subplots(1,2, figsize = (7, 7))\nax[0].imshow(im1[::-1])\nax[0].title.set_text('Object Detection')\nax[1].imshow(im2[::-1])\nax[1].title.set_text('Trajectory Simulation')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frames = validate_zarr_dataset.frames\ncoords = np.zeros((len(frames), 2))\nfor idx_coord, idx_data in enumerate(tqdm(range(len(frames)), desc=\"getting centroid to plot trajectory\")):\n    frame = validate_zarr_dataset.frames[idx_data]\n    coords[idx_coord] = frame[\"ego_translation\"][:2]\n\n\nplt.scatter(coords[:, 0], coords[:, 1], marker='.')\naxes = plt.gca()\naxes.set_xlim([-2500, 1600])\naxes.set_ylim([-2500, 1600])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cfg[\"raster_params\"][\"map_type\"] = \"py_semantic\"\nrast = build_rasterizer(cfg, dm)\ndataset = EgoDataset(cfg, validate_zarr_dataset, rast)\nscene_idx = 2\nindexes = dataset.get_scene_indices(scene_idx)\nimages = []\n\nfor idx in indexes:\n    \n    data = dataset[idx]\n    im = data[\"image\"].transpose(1, 2, 0)\n    im = dataset.rasterizer.to_rgb(im)\n    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n    draw_trajectory(im, target_positions_pixels, yaws=data[\"target_yaws\"], radius=1, rgb_color=TARGET_POINTS_COLOR)\n    clear_output(wait=True)\n    images.append(PIL.Image.fromarray(im[::-1]))\n    \nHTML(animate_solution(images).to_jshtml())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Mask\n----\nAccording to the description of the data, \"mask.npz\" is only applicable to the test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"mask = np.load('../input/lyft-motion-prediction-autonomous-vehicles/scenes/mask.npz')\nmask.f.arr_0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import zarr\n\nz = zarr.open('../input/lyft-motion-prediction-autonomous-vehicles/scenes/test.zarr')\nz.info","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are four arrays contained in this data group but only agents can be masked."},{"metadata":{"trusted":true},"cell_type":"code","source":"agents = z.agents.get_mask_selection(mask.f.arr_0)\nagents","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"centroids = pd.DataFrame(agents['centroid'])\nextents = pd.DataFrame(agents['extent'])\nvelocities = pd.DataFrame(agents['velocity'])\nprob = pd.DataFrame(agents['label_probabilities'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## centroids"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,1,figsize=(8,8))\nplt.scatter(centroids[0], centroids[1])\nplt.xlabel('x', fontsize=11); plt.ylabel('y', fontsize=11)\nplt.title(\"Centroids distribution (sample.zarr)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## extent"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(10,10))\nax = fig.add_subplot(2, 2, 1, projection='3d')\nax.scatter3D(extents[0], extents[1], extents[2], color='yellow')\nax.set_title('Scatter Distribution of Extents')\n\nax = fig.add_subplot(2, 2, 2)\nsns.distplot(extents[0], color='red', ax=ax)\nax.set_title(\"Extent_0 Distribution\")\n\nax = fig.add_subplot(2, 2, 3)\nsns.distplot(extents[1], color='blue', ax=ax)\nax.set_title(\"Extent_1 Distribution\")\n\nax = fig.add_subplot(2, 2, 4)\nsns.distplot(extents[2], color='green', ax=ax)\nax.set_title(\"Extent_2 Distribution\")\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## velocity"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(velocities[0], velocities[1]).plot_joint(sns.kdeplot, zorder=0, n_levels=6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Label probabilities"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(prob.mean())","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}