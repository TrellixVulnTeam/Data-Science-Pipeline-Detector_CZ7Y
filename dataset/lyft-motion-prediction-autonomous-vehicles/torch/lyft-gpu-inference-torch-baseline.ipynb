{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install --no-index -f ../input/kaggle-l5kit pip==20.2.2 >/dev/nul\n!pip install --no-index -f ../input/kaggle-l5kit -U l5kit > /dev/nul","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport gc\ngc.enable()\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nfrom torch.utils.data import DataLoader\n\nfrom tqdm import tqdm \nfrom typing import Dict\nfrom pathlib import Path\nfrom prettytable import PrettyTable\n\nfrom l5kit.configs import load_config_data\nfrom l5kit.data import LocalDataManager, ChunkedDataset\nfrom l5kit.dataset import EgoDataset, AgentDataset\nfrom l5kit.rasterization import build_rasterizer\nfrom l5kit.evaluation import write_pred_csv, compute_metrics_csv, read_gt_csv, create_chopped_dataset\nfrom l5kit.evaluation.chop_dataset import MIN_FUTURE_STEPS\nfrom l5kit.evaluation.metrics import neg_multi_log_likelihood, time_displace\nfrom l5kit.geometry import transform_points\nfrom l5kit.visualization import PREDICTED_POINTS_COLOR, TARGET_POINTS_COLOR, draw_trajectory","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DIR_INPUT = '../input/lyft-motion-prediction-autonomous-vehicles/'\nDIR_INPUT_TRAIN = '../input/lyft-full-training-set/'\nSINGLE_MODE_SUBMISSION = f\"{DIR_INPUT}/single_mode_submission.csv\"\nMULTI_MODE_SUBMISSION = f\"{DIR_INPUT}/multi_mode_submission.csv\"\n\nDEBUG = False\n\nos.environ['L5KIT_DATA_FOLDER'] = DIR_INPUT\ndm = LocalDataManager(None)\n\ncfg = {\n    'format_version':4,\n    'model_params':{\n        'model_architecture':'resnet18',\n        'history_num_frames':15,\n        'history_step_size':1,\n        'history_delta_time':0.1,\n        'future_num_frames':50,\n        'future_step_size':1,\n        'future_delta_time':0.1\n    },\n    'raster_params':{\n        'raster_size':[331,331],\n        'pixel_size':[0.5,0.5],\n        'ego_center':[0.25,0.25],\n        'map_type':'py_semantic',\n        'satellite_map_key': 'aerial_map/aerial_map.png',\n        'semantic_map_key': 'semantic_map/semantic_map.pb',\n        'dataset_meta_key': 'meta.json',\n        'filter_agents_threshold': 0.5\n    },\n    'train_data_loader':{\n        'key':'scenes/train.zarr',\n        'batch_size':16,\n        'shuffle':True,\n        'num_workers':4\n    },\n    'train_params':{\n        'max_num_steps': 1000 if DEBUG else 20000,\n        'checkpoint_every_n_steps':5000\n    },\n    'test_data_loader': {\n        'key': 'scenes/test.zarr',\n        'batch_size': 8,\n        'shuffle': False,\n        'num_workers': 4\n    }\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ResNetModel(nn.Module):\n    def __init__(self, cfg):\n        super(ResNetModel, self).__init__()\n        # load pre-trained Conv2D model\n        self.backbone = torchvision.models.resnet18(pretrained=False, progress=False)\n        self.backbone.load_state_dict(\n            torch.load(\n                '../input/resnet18/resnet18.pth'\n            )\n        )\n        # change input channels number to match the rasterizer's output\n        num_history_channels = (cfg['model_params']['history_num_frames']+1) * 2\n        num_in_channels = 3 + num_history_channels\n        self.backbone.conv1 = nn.Conv2d(\n            num_in_channels,\n            self.backbone.conv1.out_channels,\n            kernel_size=self.backbone.conv1.kernel_size,\n            stride=self.backbone.conv1.stride,\n            padding=self.backbone.conv1.padding,\n            bias=False\n        )\n        \n        # change output size to (X, Y) * number of future states\n        num_targets = 2 * cfg[\"model_params\"][\"future_num_frames\"]\n        self.backbone.fc = nn.Linear(in_features=512, out_features=num_targets)        \n    \n    def forward(self, x):\n        # Forward pass\n        return self.backbone(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = ResNetModel(cfg).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.MSELoss(reduction=\"none\")\nprint(\"Device is {}.\".format(device))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"WEIGHT_FILE = '../input/gpubaseline/model_state_14999.pth'\n\ntest_cfg = cfg[\"test_data_loader\"]\n\n# Rasterizer\nrasterizer = build_rasterizer(cfg, dm)\n\n# Test dataset/dataloader\ntest_zarr = ChunkedDataset(dm.require(test_cfg[\"key\"])).open()\ntest_mask = np.load(f\"{DIR_INPUT}/scenes/mask.npz\")[\"arr_0\"]\ntest_dataset = AgentDataset(cfg, test_zarr, rasterizer, agents_mask=test_mask)\ntest_dataloader = DataLoader(test_dataset,\n                             shuffle=test_cfg[\"shuffle\"],\n                             batch_size=test_cfg[\"batch_size\"],\n                             num_workers=test_cfg[\"num_workers\"])\n\n\nprint(test_dataloader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ResNetModel(cfg=cfg).to(device)\nif WEIGHT_FILE is not None:\n    model.load_state_dict(\n        torch.load(WEIGHT_FILE, map_location=device),\n    )\nprint(f\"Running on {device}.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.eval()\n\nfuture_coords_offsets_pd = []\ntimestamps = []\nagent_ids = []\n\nwith torch.no_grad():\n    dataiter = tqdm(test_dataloader)\n    \n    for data in dataiter:\n\n        inputs = data[\"image\"].to(device)\n        target_availabilities = data[\"target_availabilities\"].unsqueeze(-1).to(device)\n        targets = data[\"target_positions\"].to(device)\n\n        outputs = model(inputs).reshape(targets.shape)\n        \n        future_coords_offsets_pd.append(outputs.cpu().numpy().copy())\n        timestamps.append(data[\"timestamp\"].numpy().copy())\n        agent_ids.append(data[\"track_id\"].numpy().copy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"write_pred_csv('submission.csv',\n               timestamps=np.concatenate(timestamps),\n               track_ids=np.concatenate(agent_ids),\n               coords=np.concatenate(future_coords_offsets_pd))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}