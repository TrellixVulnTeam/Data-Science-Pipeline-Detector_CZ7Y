{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this kernel, I am working on Lyft sample dataset. I have tried merging frame dataset with scene dataset, so that I could replicate the scenes both with and without using Rastiser library provided by L5Kit. Also using the curated dataset, I will try to do some feature engineering and train models on it. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\nimport os\n\n'''\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n'''\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import l5kit \n\nfrom l5kit.data import ChunkedDataset, LocalDataManager\nfrom l5kit.dataset import EgoDataset, AgentDataset\n\nfrom l5kit.rasterization import build_rasterizer\nfrom l5kit.configs import load_config_data\nfrom l5kit.visualization import draw_trajectory, TARGET_POINTS_COLOR\nfrom l5kit.geometry import transform_points\n\nfrom l5kit.data import PERCEPTION_LABELS\n\nimport zarr\nimport os\n\nl5kit.__version__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.environ[\"L5KIT_DATA_FOLDER\"] = \"/kaggle/input/lyft-motion-prediction-autonomous-vehicles\"\n# get config\ncfg = load_config_data(\"/kaggle/input/lyft-config-files/visualisation_config.yaml\")\ncfg1 = load_config_data(\"/kaggle/input/lyft-config-files/agent_motion_config.yaml\")\nprint(cfg1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = zarr.open(\"/kaggle/input/lyft-motion-prediction-autonomous-vehicles/scenes/train.zarr/\")\nprint(data.info)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dm = LocalDataManager()\n#dataset_path = dm.require(cfg[\"val_data_loader\"][\"key\"])\ndataset_path = '/kaggle/input/lyft-motion-prediction-autonomous-vehicles/scenes/sample.zarr'\nzarr_dataset = ChunkedDataset(dataset_path)\nzarr_dataset.open()\nprint(zarr_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cfg = cfg1[\"train_data_loader\"]\nrasterizer = build_rasterizer(cfg, dm)\nagent_zarr = ChunkedDataset(dm.require(train_cfg[\"key\"])).open()\nagent_dataset = AgentDataset(cfg, agent_zarr, rasterizer)\nprint(agent_dataset)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reading agents, frames, scenes and traffic light dataset into csv files"},{"metadata":{"trusted":true},"cell_type":"code","source":"agents = pd.DataFrame.from_records(zarr_dataset.agents, columns = ['centroid', 'extent', 'yaw', 'velocity', 'track_id', 'label_probabilities'])\nframes = pd.DataFrame.from_records(zarr_dataset.frames, columns = ['timestamp','agent_index_interval','traffic_light_faces_index_interval','ego_translation','ego_rotation'])\nscenes = pd.DataFrame.from_records(zarr_dataset.scenes, columns = ['frame_index_interval', 'host', 'start_time','end_time'])\ntraffic_light = pd.DataFrame.from_records(zarr_dataset.tl_faces, columns = ['face_id','traffic_light_id','traffic_light_face_status'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"agents[['centroid_x','centroid_y']] = agents.centroid.tolist()\nagents[['extent_x','extent_y','extent_z']] = agents.extent.tolist()\nagents[['velocity_x', 'velocity_y']] =  agents.velocity.tolist()\nagents.drop(['centroid','extent','velocity'],axis = 'columns',inplace = True)\nPERCEPTION_LABELS = [\n    \"PERCEPTION_LABEL_NOT_SET\",\n    \"PERCEPTION_LABEL_UNKNOWN\",\n    \"PERCEPTION_LABEL_DONTCARE\",\n    \"PERCEPTION_LABEL_CAR\",\n    \"PERCEPTION_LABEL_VAN\",\n    \"PERCEPTION_LABEL_TRAM\",\n    \"PERCEPTION_LABEL_BUS\",\n    \"PERCEPTION_LABEL_TRUCK\",\n    \"PERCEPTION_LABEL_EMERGENCY_VEHICLE\",\n    \"PERCEPTION_LABEL_OTHER_VEHICLE\",\n    \"PERCEPTION_LABEL_BICYCLE\",\n    \"PERCEPTION_LABEL_MOTORCYCLE\",\n    \"PERCEPTION_LABEL_CYCLIST\",\n    \"PERCEPTION_LABEL_MOTORCYCLIST\",\n    \"PERCEPTION_LABEL_PEDESTRIAN\",\n    \"PERCEPTION_LABEL_ANIMAL\",\n    \"AVRESEARCH_LABEL_DONTCARE\"]   #only these perception labels were true hence using only these values\nagents[PERCEPTION_LABELS]= agents['label_probabilities'].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"agents.drop(columns = ['PERCEPTION_LABEL_NOT_SET', 'AVRESEARCH_LABEL_DONTCARE', 'PERCEPTION_LABEL_ANIMAL', 'PERCEPTION_LABEL_MOTORCYCLE', 'PERCEPTION_LABEL_MOTORCYCLIST', 'PERCEPTION_LABEL_BICYCLE',  'PERCEPTION_LABEL_OTHER_VEHICLE', 'PERCEPTION_LABEL_EMERGENCY_VEHICLE', 'PERCEPTION_LABEL_DONTCARE', 'PERCEPTION_LABEL_VAN', 'PERCEPTION_LABEL_TRAM', 'PERCEPTION_LABEL_BUS', 'PERCEPTION_LABEL_TRUCK'], inplace = True)\nagents.drop(columns = ['label_probabilities'], inplace = True)\nagents.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frames[['frame_id']] = frames.index\nscenes['frame_index_interval'] = scenes['frame_index_interval'].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame( columns = ['host','start_time','end_time'])\nfor i in range(0,100):\n    for j in range(scenes['frame_index_interval'][i][0], scenes['frame_index_interval'][i][1]):\n        df = df.append(scenes.iloc[i,1:4], ignore_index=True)\ndf[['frame_id']] = df.index        \n        \n         ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Merging scene dataset and frame dataset using frame_id key"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.merge(frames, on = df.frame_id, how = 'left')\ndf[['ego_translation_x', 'ego_translation_y', 'ego_translation_z']] = df['ego_translation'].tolist()\ndf['ego_rotation'] = df['ego_rotation'].tolist()\ndf.drop(['frame_id_x','frame_id_y','ego_translation'], axis = 'columns', inplace = True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is the ego motion for the whole scenario."},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#sns.scatterplot(df['ego_translation_x'], df['ego_rotation_y'], df['ego_translation_z'])\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(df['ego_translation_x'], df['ego_translation_y'], df['ego_translation_z'], c='skyblue', s=60)\nax.view_init(10, 50)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the agents, encountered by the car during its motion."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(agents['extent_x'], agents['extent_y'], agents['extent_z'], c='green', s=60)\nax.view_init(10, 50)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Agent representation for the first scene."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(agents.iloc[38:86, 5], agents.iloc[38:86, 6], agents.iloc[38:86, 7], c='green', s=60)\nax.view_init(10, 50)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Agent Representation in 2-D graph"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x = agents.centroid_x, y = agents.centroid_y, color = 'blue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x = agents[agents.velocity_x != 0].velocity_x, y = agents[agents.velocity_y != 0].velocity_y, color = 'blue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#agents.head()\nsns.distplot(agents.yaw)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here the red dot represents the AV, while blue dots depicts the agents with which it is surrounded."},{"metadata":{"trusted":true},"cell_type":"code","source":"subsample = agents.iloc[38:86]\n\n#Create combo chart\nfig, ax1 = plt.subplots(figsize=(10,6))\n \nax1 = sns.scatterplot(x = subsample.centroid_x, y = subsample.centroid_y, color = 'blue')\n\n#specify we want to share the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:red'\n#line plot creation\n\nax2 = sns.scatterplot([df.iloc[1,8]], [df.iloc[1,9]], color=color)\nax2.tick_params(axis='y', color=color)\n#show plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_path = '/kaggle/input/lyft-motion-prediction-autonomous-vehicles/scenes/train.zarr'\nzarr_dataset = ChunkedDataset(dataset_path)\nzarr_dataset.open()\nrast = build_rasterizer(cfg, dm)\ntrain_agent_dataset = AgentDataset(cfg, zarr_dataset, rast)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the L5Kit's Rastesizer library."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = train_agent_dataset[1]\ncfg['raster_params']['map_type'] = \"py_satellite\"\n\nim = data[\"image\"].transpose(1, 2, 0)\nim = train_agent_dataset.rasterizer.to_rgb(im)\ntarget_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n#draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n\nplt.imshow(im[::-1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is alot of a work yet to be done wrt to feature engineering and model training."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}