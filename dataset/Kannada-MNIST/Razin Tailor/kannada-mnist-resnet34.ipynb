{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import sys\npt_models = '../input/pretrainedmodels/pretrainedmodels-0.7.4/'\nsys.path.insert(0, pt_models)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pretrainedmodels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import glob\nimport torch\nimport albumentations\nimport joblib\nimport pandas as pd\nimport numpy as np\nimport torch.nn as nn\n\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom torch.nn import functional as F","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TEST_BATCH_SIZE = 32\nMODEL_MEAN = (0.485, 0.456, 0.406)\nMODEL_STD = (0.229, 0.224, 0.225)\nIMG_HEIGHT = 28\nIMG_WIDTH = 28\nDEVICE=\"cuda\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class resnet34(nn.Module):\n    def __init__(self, pretrained):\n        super(resnet34, self).__init__()\n        if pretrained:\n            self.model = pretrainedmodels.__dict__[\"resnet34\"](pretrained=\"imagenet\")\n        else:\n            self.model = pretrainedmodels.__dict__[\"resnet34\"](pretrained=None)\n\n        self.out = nn.Linear(512, 10)\n\n    def forward(self, x):  # Takes a batch\n        bs, channels, height, width = x.shape\n        x = self.model.features(x)  # function of pretrainedmodels\n\n        x = F.adaptive_avg_pool2d(x, 1).reshape(bs, -1)\n        out = self.out(x)\n\n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class KannadaMNISTTest:\n    def __init__(self, df, img_width, img_height, mean, std):\n\n        self.image_ids = df.id.values\n        self.img_arr = df.iloc[:, 1:].values\n\n        # Augmentations\n\n        self.augment = albumentations.Compose(\n            [\n                albumentations.Resize(img_height, img_width, always_apply=True),\n                albumentations.Normalize(mean, std, always_apply=True),\n            ]\n        )\n\n    def __len__(self):\n        return len(self.image_ids)\n\n    def __getitem__(self, item):\n        image = self.img_arr[item, :]\n        image = image.reshape(28, 28).astype(float)\n        image = Image.fromarray(image).convert(\"RGB\")  # WHY?\n        id = self.image_ids[item]\n        # Because all the models that we would try:\n        # maybe from torchvision or pretrainedmodels they all work on rgb.\n        # So we don't want to spend time on making them work for single channel only\n\n        image = self.augment(image=np.array(image))[\"image\"]\n        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n\n        # Take a look at torchvision models to know why such dtype\n        return {\n            \"image\": torch.tensor(image, dtype=torch.float),\n            \"id\": torch.tensor(id, dtype=torch.long)\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = resnet34(pretrained=False)\nmodel = model.to(DEVICE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_state_dict(torch.load('../input/resnet34kannadamnist/model-resnet34-fold-4-epoch-7.bin'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv('../input/Kannada-MNIST/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = KannadaMNISTTest(df=test_df, img_width=28, img_height=28,\n                                mean=MODEL_MEAN,\n                                std=MODEL_STD)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataloader = torch.utils.data.DataLoader(\n    test_dataset, batch_size=64,\n    shuffle=False,\n    num_workers=6\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = []\nfor bt, d in tqdm(enumerate(dataloader), total = int(len(test_dataset) / dataloader.batch_size)):\n    image = d['image']\n    id = d['id']\n    image = image.to(DEVICE, dtype=torch.float)\n    out = model(image)\n    digit = np.argmax(out.cpu().detach().numpy(), axis=1)\n#     print(id, digit)\n    for ii, imid in enumerate(id):\n        predictions.append([int(imid.cpu().detach().numpy()), digit[ii]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub = pd.DataFrame(predictions, columns=['id', 'label'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub.to_csv(\"submission.csv\", index=False, columns=df_sub.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}