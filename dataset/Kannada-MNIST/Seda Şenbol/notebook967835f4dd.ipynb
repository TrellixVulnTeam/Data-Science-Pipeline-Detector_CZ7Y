{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nSpyder Editor\n\nThis is a temporary script file.\n\"\"\"\n\n\n#önce udemy datası üstünde train edip sonra kaggle da test edicez.\n\nimport numpy as np\nimport pandas as pd\n#import statsmodels.api as sm\n#import matplotlib.pyplot as plt\n#import mpl_toolkits\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport os\nos.chdir(r\"C:\\\\Users\\user\\Desktop\\Kannada\")\nfrom sklearn import preprocessing\n\n# Load the data\nraw_train_csv_data = np.loadtxt('train.csv',delimiter=',',skiprows=1)\nraw_test_csv_data = np.loadtxt('test.csv',delimiter=',',skiprows=1)\nraw_Dig_csv_data = np.loadtxt('Dig-MNIST.csv',delimiter=',',skiprows=1)\n# The inputs are all columns in the csv, except for the first one [:,0]\n# (which is just the arbitrary customer IDs that bear no useful information),\n# and the last one [:,-1] (which is our targets)\n\nunscaled_inputs_train = raw_train_csv_data[:,1:]\ntargets_train=raw_train_csv_data[:,0]\n\nunscaled_inputs_Dig=raw_Dig_csv_data[:,1:]\ntargets_Dig=raw_Dig_csv_data[:,0]\n\nraw_test_csv_data=raw_test_csv_data[:,1:]\nunscaled_inputs_submission = raw_test_csv_data\n\n# The targets are in the last column. That's how datasets are conventionally organized.\n\nscaled_inputs_train = preprocessing.scale(unscaled_inputs_train)\nscaled_inputs_submission=preprocessing.scale(unscaled_inputs_submission)\n\nscaled_inputs_Dig=preprocessing.scale(unscaled_inputs_Dig)\n\nsamples_count=unscaled_inputs_train.shape[0]\nsubmission_samples_count=unscaled_inputs_submission.shape[0]\n\n\n\nvalidation_samples_count = int(0.1 * samples_count)\ntrain_samples_count = int(0.8 * samples_count)\ntest_samples_count = samples_count - train_samples_count - validation_samples_count\n\n\ntrain_inputs = scaled_inputs_train[:train_samples_count]\ntrain_targets = targets_train[:train_samples_count]\n\n\nvalidation_inputs = scaled_inputs_train[train_samples_count:train_samples_count+validation_samples_count]\nvalidation_targets = targets_train[train_samples_count:train_samples_count+validation_samples_count]\n\ntest_inputs = scaled_inputs_train[train_samples_count+validation_samples_count:]\ntest_targets = targets_train[train_samples_count+validation_samples_count:]\n\n#print(np.sum(train_targets), train_samples_count, np.sum(train_targets) / train_samples_count)\n#print(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) / validation_samples_count)\n#print(np.sum(targets_test), test_samples_count, np.sum(targets_test) / test_samples_count)\n\nnp.savez('mnist_data_train', inputs=train_inputs, targets=train_targets)\nnp.savez('mnist_data_validation', inputs=validation_inputs, targets=validation_targets)\nnp.savez('mnist_data_test', inputs=test_inputs, targets=test_targets)\n\nnpz = np.load('mnist_data_train.npz')\n\n# we extract the inputs using the keyword under which we saved them\n# to ensure that they are all floats, let's also take care of that\ntrain_inputs = npz['inputs'].astype(np.float)\n# targets must be int because of sparse_categorical_crossentropy (we want to be able to smoothly one-hot encode them)\ntrain_targets = npz['targets'].astype(np.int)\n\n# we load the validation data in the temporary variable\nnpz = np.load('mnist_data_validation.npz')\n# we can load the inputs and the targets in the same line\nvalidation_inputs, validation_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n\n# we load the test data in the temporary variable\nnpz = np.load('mnist_data_test.npz')\n# we create 2 variables that will contain the test inputs and the test targets\ntest_inputs, test_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n\n\n\n\n\ninput_size = 784 \noutput_size = 10\n\nhidden_layer_size = 50\ncallback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n\nmodel = tf.keras.Sequential([\n    \n    # the first layer (the input layer)\n    # each observation is 28x28x1 pixels, therefore it is a tensor of rank 3\n    # since we don't know CNNs yet, we don't know how to feed such input into our net, so we must flatten the images\n    # there is a convenient method 'Flatten' that simply takes our 28x28x1 tensor and orders it into a (None,) \n    # or (28x28x1,) = (784,) vector\n    # this allows us to actually create a feed forward neural network\n    tf.keras.layers.Flatten(input_shape=(28, 28, 1)), # input layer\n    tf.keras.layers.BatchNormalization(),\n    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)\n    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function\n    #tf.keras.layers.LeakyReLU(alpha=0.3),\n    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n    tf.keras.layers.BatchNormalization(),\n    #tf.keras.layers.LeakyReLU(alpha=0.3),\n    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n    tf.keras.layers.BatchNormalization(),\n    # the final layer is no different, we just make sure to activate it with softmax\n    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n])\n\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nNUM_EPOCHS = 10\n\n\nmodel.fit(scaled_inputs_Dig,targets_Dig, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets),callbacks=[callback], verbose =2)\n\ntest_loss, test_accuracy = model.evaluate(scaled_inputs_Dig,targets_Dig)\nprint('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))\n\ndataframe=pd.DataFrame(columns=['ImageId','Label'])\ndataframe['ImageId']=range(1,5001)\n#submission=model.predict(unscaled_inputs_submission,batch_size=100,verbose=1)\nsubmission=model.predict(unscaled_inputs_submission,batch_size=100,verbose=1)\nerror = np.mean(test_targets!=np.argmax(submission, axis = 1))\nprint (error)\n\nprint(np.argmax(submission, axis = 1)[:100]) #[2 5 1 9 3 7 0 3 1 3 5 7 1 0 4 5 3 1 9 0 9 1 1 5 7 4 1 7 1 7 7 5 4 1 6 2 5\n #5 1 6 7 7 4 9 5 7 1 3 6 7 6 8 1 3 8 2 1 2 2 5 4 1 7 0 0 7 1 1 0 1 6 5 1 8\n #2 5 9 9 2 3 5 1 1 0 9 1 4 3 6 7 2 0 6 6 1 4 3 9 7 1]\nprint(submission.shape[0])\n\n\"\"\"\ndataframe['Label'] = np.argmax(submission, axis = 1)\n\nprint (dataframe)\ndf=pd.DataFrame(dataframe)\ndf.to_csv(index=False)\ncompression_opts = dict(method='zip',\n                        archive_name='out.csv')  \ndf.to_csv('out.zip', index=False,\n          compression=compression_opts)  \n\"\"\"\n\"\"\"\nprint(submission[0][:])\n\n\ndataframe=pd.DataFrame(columns=['ImageId','Label'])\ndataframe['ImageId']=range(1,28001)\nprint (dataframe)\ndf=np.array(dataframe)\n\nfor j in range(28000):\n    max=0\n    for i in range (10):\n        if submission[j][i]>max:\n            max=submission[j][i]\n            df[j][1]=i\n\nprint (submission[10000:11100])\n\"\"\"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}