{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import tensorflow as tf\n\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\ntf.keras.backend.set_session(tf.Session(config=config))\n\nfrom keras import backend as K\n\n# from keras.applications.vgg16 import VGG16\n# from keras.applications.vgg16 import preprocess_input\n\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, Dropout, MaxPooling2D, Flatten, Dense\n\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.optimizers import SGD, Adam, RMSprop\nfrom keras.callbacks import LearningRateScheduler\nfrom keras.utils.np_utils import to_categorical\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n# from sklearn.metrics import accuracy_score, classification_report\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import LeakyReLU\nfrom keras.layers.normalization import BatchNormalization","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train      = pd.read_csv('/kaggle/input/Kannada-MNIST/train.csv')\ntest       = pd.read_csv('/kaggle/input/Kannada-MNIST/test.csv')\nsample_sub = pd.read_csv('/kaggle/input/Kannada-MNIST/sample_submission.csv')\ndig        = pd.read_csv('/kaggle/input/Kannada-MNIST/Dig-MNIST.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train set shape = \" +str(train.shape))\nprint(\"Test set shape = \" +str(test.shape))\nprint(\"Sub set shape = \" +str(sample_sub.shape))\nprint(\"Dig set shape = \" +str(dig.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dados de TREINAMENTO  (todas as linhas, da segunda coluna em diante)\nx_train = train.values[:,1:].reshape(train.shape[0], 28, 28, 1)\n# Transformando os valores do CSV em imagem (float32)\nx_train = x_train.astype('float32')\n# Deixando os píxels na mesma escala, ie. deixando em uma escala de 0 a 1 para não gerar viés\nx_train = x_train / 255.0\n# Separando os labels de treinamento    \ny_train = train.values[:,0]\n# Separando dados de treinamento e validação\nx_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size = 0.1, random_state=2019)\n# Dados de TESTE (todas as linhas, da segunda coluna em diante)\nx_test = test.values[:,1:].reshape(test.shape[0], 28, 28, 1)\n# Transformando os valores do CSV em imagem (float32)\nx_test = x_test.astype('float32')\n# Deixando os píxels na mesma escala, ie. deixando em uma escala de 0 a 1 para não gerar viés\nx_test = x_test / 255.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transformando as classes numéricas (0,1,2,3,4,5,6,7,8,9) em binários ([1,0,0,0,0,0,0,0,0,0], [0,1,0,0,0,0,0,0,0,0],...,[0,0,0,0,0,0,0,0,0,1])\nlb = preprocessing.LabelBinarizer()\ny_train = lb.fit_transform(y_train)\ny_valid = lb.fit_transform(y_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelo = Sequential()\n\nmodelo.add(Conv2D(64,  (3,3), padding='same', input_shape=(28, 28, 1)))\nmodelo.add(BatchNormalization(momentum=0.1, epsilon=1e-5, gamma_initializer=\"uniform\"))\nmodelo.add(LeakyReLU(alpha=0.1))\n\nmodelo.add(MaxPooling2D(2, 2))\nmodelo.add(Dropout(0.2))\n\nmodelo.add(Conv2D(128, (3,3), padding='same'))\nmodelo.add(BatchNormalization(momentum=0.2, epsilon=1e-5, gamma_initializer=\"uniform\"))\nmodelo.add(LeakyReLU(alpha=0.1))\nmodelo.add(Conv2D(128, (3,3), padding='same'))\nmodelo.add(BatchNormalization(momentum=0.1, epsilon=1e-5, gamma_initializer=\"uniform\"))\nmodelo.add(LeakyReLU(alpha=0.1))\n\nmodelo.add(MaxPooling2D(2,2))\nmodelo.add(Dropout(0.2))\n\nmodelo.add(Conv2D(256, (3,3), padding='same'))\nmodelo.add(BatchNormalization(momentum=0.2, epsilon=1e-5, gamma_initializer=\"uniform\"))\nmodelo.add(LeakyReLU(alpha=0.1))\nmodelo.add(Conv2D(256, (3,3), padding='same'))\nmodelo.add(BatchNormalization(momentum=0.1, epsilon=1e-5, gamma_initializer=\"uniform\"))\nmodelo.add(LeakyReLU(alpha=0.1))\n\nmodelo.add(MaxPooling2D(2,2))\nmodelo.add(Dropout(0.2))\n\nmodelo.add(Flatten())\nmodelo.add(Dense(256))\nmodelo.add(LeakyReLU(alpha=0.1))\n\nmodelo.add(BatchNormalization())\nmodelo.add(Dense(10, activation='softmax'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"initial_learningrate=2e-3\ndef lr_decay(epoch):#lrv\n    return initial_learningrate * 0.99 ** epoch\nmodelo.compile(loss= 'categorical_crossentropy', optimizer= RMSprop(lr=initial_learningrate) , metrics=['accuracy'])\nmodelo.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs=100\nbatch_size=200\nhistory = modelo.fit(x_train, y_train,\n          epochs=epochs,\n          batch_size= batch_size,\n          callbacks=[LearningRateScheduler(lr_decay)],\n          validation_data=(x_valid, y_valid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"H = history\nplt.style.use(\"bmh\")\nplt.figure()\nplt.plot(np.arange(0, epochs), H.history[\"loss\"], label=\"train_loss\")\nplt.plot(np.arange(0, epochs), H.history[\"val_loss\"], label=\"val_loss\")\nplt.plot(np.arange(0, epochs), H.history[\"accuracy\"], label=\"train_acc\")\nplt.plot(np.arange(0, epochs), H.history[\"val_accuracy\"], label=\"val_acc\")\nplt.title(\"Training Loss and Accuracy on Dataset\")\nplt.xlabel(\"Epoch #\")\nplt.ylabel(\"Loss/Accuracy\")\nplt.legend(loc=\"best\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Val'], loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Val'], loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = modelo.predict(x_test)\nscore_classes = np.argmax(scores, axis = 1)\nx_dig=dig.drop('label',axis=1).iloc[:,:].values\nprint(x_dig.shape)\nx_dig = x_dig.reshape(x_dig.shape[0], 28, 28,1)\nprint(x_dig.shape)\ny_dig=dig.label\nprint(y_dig.shape)\npreds_dig=modelo.predict_classes(x_dig/255)\nmetrics.accuracy_score(preds_dig, y_dig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = pd.DataFrame({'id': sample_sub['id'],\n                       'label': score_classes})\noutput.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}