{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load train data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"../input/Kannada-MNIST/train.csv\")\ntrain_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualise train data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nshow_exmpl = train_data.values[:8, :-1]\nplt.figure(1, figsize=(14, 7))\nfor i in range(8):\n    plt.subplot(2, 4, i + 1)\n    plt.imshow(show_exmpl[i].reshape((28, 28)), cmap='gray')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Divide to train and validate data"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_test = train_data.values[:, 1:]\ny_train_test = train_data.label.values\n\nX_train, X_test, y_train, y_test = train_test_split(X_train_test, y_train_test, test_size = 0.02, random_state=42) \n\nprint('Train shapes: ', X_train.shape, y_train.shape)\nprint('Test shapes: ', X_test.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data normalization"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.min(X_train), np.max(X_train))\nX_train_max = np.max(X_train)\nX_train = X_train / (0.5 * X_train_max) - 1\nprint(np.min(X_train), np.max(X_train))\n\nprint(np.min(X_test), np.max(X_test))\nX_test = X_test / (0.5 * X_train_max) - 1 \nprint(np.min(X_test), np.max(X_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### CNN with RMSprop Optimizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import *\nfrom keras.models import Sequential\nfrom keras.optimizers import *\nfrom keras import regularizers\nfrom keras.utils import plot_model, model_to_dot\nfrom IPython.display import SVG","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nl2_reg_conv2d = 0\nl2_reg_dense = 0.01\nactivation_type = 'relu'\n\nmodel.add(Conv2D(64, kernel_size=3, activation=activation_type, input_shape=(28, 28, 1), padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64, kernel_size=3, activation=activation_type, padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64, kernel_size=5, activation=activation_type, padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\nmodel.add(Dropout(0.4))\n\nmodel.add(Conv2D(128, kernel_size=3, activation=activation_type, padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(128, kernel_size=3, activation=activation_type, padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(128, kernel_size=5, activation=activation_type, padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\nmodel.add(Dropout(0.3))\n\nmodel.add(Conv2D(256, kernel_size=3, activation=activation_type, padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(256, kernel_size=3, activation=activation_type, padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(256, kernel_size=5, activation=activation_type, padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\nmodel.add(Dropout(0.2))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation=activation_type, kernel_regularizer=regularizers.l2(l2_reg_dense)))\nmodel.add(BatchNormalization())\nmodel.add(Dense(128, activation=activation_type, kernel_regularizer=regularizers.l2(l2_reg_dense)))\nmodel.add(BatchNormalization())\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.compile(optimizer = SGD(lr=0.01),\n              loss = 'sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_SGD = Sequential()\n# l2_reg_conv2d = 0\n# l2_reg_dense = 0.01\n\n# model_SGD.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(28, 28, 1), padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_SGD.add(BatchNormalization())\n# model_SGD.add(Conv2D(64, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_SGD.add(BatchNormalization())\n# model_SGD.add(Conv2D(64, kernel_size=5, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_SGD.add(BatchNormalization())\n# model_SGD.add(MaxPooling2D(pool_size=(3, 3)))\n# model_SGD.add(Dropout(0.4))\n\n# model_SGD.add(Conv2D(128, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_SGD.add(BatchNormalization())\n# model_SGD.add(Conv2D(128, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_SGD.add(BatchNormalization())\n# model_SGD.add(Conv2D(128, kernel_size=5, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_SGD.add(BatchNormalization())\n# model_SGD.add(MaxPooling2D(pool_size=(3, 3)))\n# model_SGD.add(Dropout(0.3))\n\n# model_SGD.add(Conv2D(256, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_SGD.add(BatchNormalization())\n# model_SGD.add(Conv2D(256, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_SGD.add(BatchNormalization())\n# model_SGD.add(Conv2D(256, kernel_size=5, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_SGD.add(BatchNormalization())\n# model_SGD.add(MaxPooling2D(pool_size=(3, 3)))\n# model_SGD.add(Dropout(0.2))\n\n# model_SGD.add(Flatten())\n# model_SGD.add(Dense(256, kernel_regularizer=regularizers.l2(l2_reg_dense)))\n# model_SGD.add(BatchNormalization())\n# model_SGD.add(Dense(128, kernel_regularizer=regularizers.l2(l2_reg_dense)))\n# model_SGD.add(BatchNormalization())\n# model_SGD.add(Dense(10, activation='softmax'))\n\n# model_SGD.compile(optimizer = SGD(lr=0.01),\n#       loss = 'sparse_categorical_crossentropy',\n#       metrics=['accuracy'])\n\n# model_SGD.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_SGD_mom = Sequential()\n# l2_reg_conv2d = 0\n# l2_reg_dense = 0.01\n\n# model_SGD_mom.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(28, 28, 1), padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_SGD_mom.add(BatchNormalization())\n# model_SGD_mom.add(Conv2D(64, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_SGD_mom.add(BatchNormalization())\n# model_SGD_mom.add(Conv2D(64, kernel_size=5, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_SGD_mom.add(BatchNormalization())\n# model_SGD_mom.add(MaxPooling2D(pool_size=(3, 3)))\n# model_SGD_mom.add(Dropout(0.4))\n\n# model_SGD_mom.add(Conv2D(128, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_SGD_mom.add(BatchNormalization())\n# model_SGD_mom.add(Conv2D(128, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_SGD_mom.add(BatchNormalization())\n# model_SGD_mom.add(Conv2D(128, kernel_size=5, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_SGD_mom.add(BatchNormalization())\n# model_SGD_mom.add(MaxPooling2D(pool_size=(3, 3)))\n# model_SGD_mom.add(Dropout(0.3))\n\n# model_SGD_mom.add(Conv2D(256, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_SGD_mom.add(BatchNormalization())\n# model_SGD_mom.add(Conv2D(256, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_SGD_mom.add(BatchNormalization())\n# model_SGD_mom.add(Conv2D(256, kernel_size=5, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_SGD_mom.add(BatchNormalization())\n# model_SGD_mom.add(MaxPooling2D(pool_size=(3, 3)))\n# model_SGD_mom.add(Dropout(0.2))\n\n# model_SGD_mom.add(Flatten())\n# model_SGD_mom.add(Dense(256, kernel_regularizer=regularizers.l2(l2_reg_dense)))\n# model_SGD_mom.add(BatchNormalization())\n# model_SGD_mom.add(Dense(128, kernel_regularizer=regularizers.l2(l2_reg_dense)))\n# model_SGD_mom.add(BatchNormalization())\n# model_SGD_mom.add(Dense(10, activation='softmax'))\n\n# model_SGD_mom.compile(optimizer = SGD(lr=0.01, momentum=0.9),\n#       loss = 'sparse_categorical_crossentropy',\n#       metrics=['accuracy'])\n\n# model_SGD_mom.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_Adadelta = Sequential()\n# l2_reg_conv2d = 0\n# l2_reg_dense = 0.01\n\n# model_Adadelta.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(28, 28, 1), padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_Adadelta.add(BatchNormalization())\n# model_Adadelta.add(Conv2D(64, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_Adadelta.add(BatchNormalization())\n# model_Adadelta.add(Conv2D(64, kernel_size=5, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_Adadelta.add(BatchNormalization())\n# model_Adadelta.add(MaxPooling2D(pool_size=(3, 3)))\n# model_Adadelta.add(Dropout(0.4))\n\n# model_Adadelta.add(Conv2D(128, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_Adadelta.add(BatchNormalization())\n# model_Adadelta.add(Conv2D(128, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_Adadelta.add(BatchNormalization())\n# model_Adadelta.add(Conv2D(128, kernel_size=5, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_Adadelta.add(BatchNormalization())\n# model_Adadelta.add(MaxPooling2D(pool_size=(3, 3)))\n# model_Adadelta.add(Dropout(0.3))\n\n# model_Adadelta.add(Conv2D(256, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_Adadelta.add(BatchNormalization())\n# model_Adadelta.add(Conv2D(256, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_Adadelta.add(BatchNormalization())\n# model_Adadelta.add(Conv2D(256, kernel_size=5, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_Adadelta.add(BatchNormalization())\n# model_Adadelta.add(MaxPooling2D(pool_size=(3, 3)))\n# model_Adadelta.add(Dropout(0.2))\n\n# model_Adadelta.add(Flatten())\n# model_Adadelta.add(Dense(256, kernel_regularizer=regularizers.l2(l2_reg_dense)))\n# model_Adadelta.add(BatchNormalization())\n# model_Adadelta.add(Dense(128, kernel_regularizer=regularizers.l2(l2_reg_dense)))\n# model_Adadelta.add(BatchNormalization())\n# model_Adadelta.add(Dense(10, activation='softmax'))\n\n# model_Adadelta.compile(optimizer = Adadelta(learning_rate=1.0),\n#       loss = 'sparse_categorical_crossentropy',\n#       metrics=['accuracy'])\n\n# model_Adadelta.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_Adam = Sequential()\n# l2_reg_conv2d = 0\n# l2_reg_dense = 0.01\n\n# model_Adam.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(28, 28, 1), padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_Adam.add(BatchNormalization())\n# model_Adam.add(Conv2D(64, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_Adam.add(BatchNormalization())\n# model_Adam.add(Conv2D(64, kernel_size=5, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_Adam.add(BatchNormalization())\n# model_Adam.add(MaxPooling2D(pool_size=(3, 3)))\n# model_Adam.add(Dropout(0.4))\n\n# model_Adam.add(Conv2D(128, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_Adam.add(BatchNormalization())\n# model_Adam.add(Conv2D(128, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_Adam.add(BatchNormalization())\n# model_Adam.add(Conv2D(128, kernel_size=5, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_Adam.add(BatchNormalization())\n# model_Adam.add(MaxPooling2D(pool_size=(3, 3)))\n# model_Adam.add(Dropout(0.3))\n\n# model_Adam.add(Conv2D(256, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_Adam.add(BatchNormalization())\n# model_Adam.add(Conv2D(256, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_Adam.add(BatchNormalization())\n# model_Adam.add(Conv2D(256, kernel_size=5, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_reg_conv2d)))\n# model_Adam.add(BatchNormalization())\n# model_Adam.add(MaxPooling2D(pool_size=(3, 3)))\n# model_Adam.add(Dropout(0.2))\n\n# model_Adam.add(Flatten())\n# model_Adam.add(Dense(256, kernel_regularizer=regularizers.l2(l2_reg_dense)))\n# model_Adam.add(BatchNormalization())\n# model_Adam.add(Dense(128, kernel_regularizer=regularizers.l2(l2_reg_dense)))\n# model_Adam.add(BatchNormalization())\n# model_Adam.add(Dense(10, activation='softmax'))\n\n# model_Adam.compile(optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False),\n#       loss = 'sparse_categorical_crossentropy',\n#       metrics=['accuracy'])\n\n# model_Adam.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from keras.utils import plot_model\n# plot_model(model, to_file='model.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping,  ReduceLROnPlateau\n\ndatagen = ImageDataGenerator(\n    rotation_range = 20,\n    width_shift_range = 0.3,\n    height_shift_range = 0.3,\n    shear_range = 0.2,\n    zoom_range = 0.3,\n    horizontal_flip = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 75\nbatch_size = 128\n\nX_train = X_train.reshape(X_train.shape[0],28,28,1)\nX_test = X_test.reshape(X_test.shape[0],28,28,1)\n\ntrain_story = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\n                    epochs = epochs, \n                    steps_per_epoch = 100,\n                    validation_data = (X_test, y_test), \n                    callbacks=[\n                      ModelCheckpoint('/kaggle/working/best_kannada_model.h5', save_best_only=True),\n                      CSVLogger('/kaggle/working/learning_log_RMSprop_without_BN.csv'),\n#                       ReduceLROnPlateau(monitor='val_loss', patience=200, verbose=1, factor=0.2),\n                      ],\n                    verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt \n\nlog_batch_norm = np.array(pd.read_csv(\"/kaggle/working/learning_log_RMSprop_with_BN.csv\")['val_accuracy'])\nlog_no_batch_norm = np.array(pd.read_csv(\"/kaggle/working/learning_log_RMSprop_without_BN.csv\")['val_accuracy'])\n\nplt.figure(figsize=(20,10))\nplt.plot(range(1, 11), log_batch_norm, label='with BatchNorm')\nplt.plot(range(1, 11), log_no_batch_norm, label='without BatchNorm')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt \n\nlog_softmax = np.array(pd.read_csv(\"/kaggle/working/learning_log_RMSprop_softmax.csv\")['val_accuracy'])\nlog_elu = np.array(pd.read_csv(\"/kaggle/working/learning_log_RMSprop_elu.csv\")['val_accuracy'])\nlog_relu = np.array(pd.read_csv(\"/kaggle/working/learning_log_RMSprop_relu.csv\")['val_accuracy'])\nlog_tanh = np.array(pd.read_csv(\"/kaggle/working/learning_log_RMSprop_tanh.csv\")['val_accuracy'])\nlog_sigmoid = np.array(pd.read_csv(\"/kaggle/working/learning_log_RMSprop_sigmoid.csv\")['val_accuracy'])\nlog_exponential = np.array(pd.read_csv(\"/kaggle/working/learning_log_RMSprop_exponential.csv\")['val_accuracy'])\n\nplt.figure(figsize=(20,10))\nplt.plot(range(1, 11), log_softmax, label='softmax')\nplt.plot(range(1, 11), log_elu, label='elu')\nplt.plot(range(1, 11), log_relu, label='relu')\nplt.plot(range(1, 11), log_tanh, label='tanh')\nplt.plot(range(1, 11), log_sigmoid, label='sigmoid')\nplt.plot(range(1, 11), log_exponential, label='exponential')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# OPTIMIZATORS EXP DO NOT RUN\n\n# epochs = 10\n# batch_size = 128\n\n# X_train = X_train.reshape(X_train.shape[0],28,28,1)\n# X_test = X_test.reshape(X_test.shape[0],28,28,1)\n\n# train_story_RMSprop = model_RMSprop.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\n#                     epochs = epochs, \n#                     steps_per_epoch = 100,\n#                     validation_data = (X_test, y_test), \n#                     callbacks=[\n#                       ModelCheckpoint('/kaggle/working/best_kannada_model.h5', save_best_only=True),\n#                       CSVLogger('/kaggle/working/learning_log_RMSprop.csv'),\n# #                       ReduceLROnPlateau(monitor='val_loss', patience=200, verbose=1, factor=0.2),\n#                       ],\n#                     verbose=1)\n\n# train_story_SGD_mom = model_SGD_mom.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\n#                     epochs = epochs, \n#                     steps_per_epoch = 100,\n#                     validation_data = (X_test, y_test), \n#                     callbacks=[\n#                       ModelCheckpoint('/kaggle/working/best_kannada_model.h5', save_best_only=True),\n#                       CSVLogger('/kaggle/working/learning_log_SGD_mom.csv'),\n#                       ],\n#                     verbose=1)\n\n# train_story_SGD = model_SGD.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\n#                     epochs = epochs, \n#                     steps_per_epoch = 100,\n#                     validation_data = (X_test, y_test), \n#                     callbacks=[\n#                       ModelCheckpoint('/kaggle/working/best_kannada_model.h5', save_best_only=True),\n#                       CSVLogger('/kaggle/working/learning_log_SGD.csv'),\n#                       ],\n#                     verbose=1)\n\n# train_story_Adam = model_Adam.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\n#                     epochs = epochs, \n#                     steps_per_epoch = 100,\n#                     validation_data = (X_test, y_test), \n#                     callbacks=[\n#                       ModelCheckpoint('/kaggle/working/best_kannada_model.h5', save_best_only=True),\n#                       CSVLogger('/kaggle/working/learning_log_Adam.csv'),\n#                       ],\n#                     verbose=1)\n\n# train_story_Adadelta = model_Adadelta.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\n#                     epochs = epochs, \n#                     steps_per_epoch = 100,\n#                     validation_data = (X_test, y_test), \n#                     callbacks=[\n#                       ModelCheckpoint('/kaggle/working/best_kannada_model.h5', save_best_only=True),\n#                       CSVLogger('/kaggle/working/learning_log_Adadelta.csv'),\n#                       ],\n#                     verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_SGD = np.array(pd.read_csv(\"/kaggle/working/learning_log_SGD.csv\")['val_accuracy'])\nlog_SGD_mom = np.array(pd.read_csv(\"/kaggle/working/learning_log_SGD_mom.csv\")['val_accuracy'])\nlog_Adam = np.array(pd.read_csv(\"/kaggle/working/learning_log_Adam.csv\")['val_accuracy'])\nlog_Adadelta = np.array(pd.read_csv(\"/kaggle/working/learning_log_Adadelta.csv\")['val_accuracy'])\nlog_RMSprop = np.array(pd.read_csv(\"/kaggle/working/learning_log_RMSprop.csv\")['val_accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt \n\nplt.figure(figsize=(20,10))\nplt.plot(range(1, 11), log_SGD, label='SGD')\nplt.plot(range(1, 11), log_SGD_mom, label='SGD_mom')\nplt.plot(range(1, 11), log_Adam, label='Adam')\nplt.plot(range(1, 11), log_Adadelta, label='Adadelta')\nplt.plot(range(1, 11), log_RMSprop, label='RMSprop')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_csv = pd.read_csv(\"../input/Kannada-MNIST/test.csv\")\nX_val = np.array(test_csv.drop(\"id\",axis=1), dtype=np.float32)\nX_val.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_val_max = np.max(X_val)\nX_val = X_val / (0.5 * X_val_max) - 1\nX_val = np.reshape(X_val, (-1,28,28,1))\n\nprint(X_val.shape, np.min(X_val), np.max(X_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import load_model\n\nbest_model = load_model('/kaggle/working/best_kannada_model.h5')\nY_val = best_model.predict(X_val)\nY_val = np.argmax(Y_val, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(\"../input/Kannada-MNIST/sample_submission.csv\")\nsubmission['label'] = Y_val\nsubmission.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}