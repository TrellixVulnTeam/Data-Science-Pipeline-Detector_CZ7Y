{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n# ref taken from <https://www.kaggle.com/vikassingh1996/simple-cnn-modeling-kannanda-mnist>\n\n# network ref is taken from https://www.kaggle.com/yonminma/keras-easy-with-0-9892-score for optization\n# network is not able to converge after .9960 with augmentation\n# network is not able to converge after .9920 without  augmentation\n# most of the advance hyper parameter tuning cocept are taken from yonminma\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n'''Importing preprocessing libraries'''\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\n'''Seaborn and Matplotlib Visualization'''\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n'''Importing tensorflow libraries'''\nimport tensorflow as tf \nprint(tf.__version__)\n\nfrom tensorflow.keras import layers, models\n\nfrom keras.optimizers import RMSprop,Adam,Adadelta\nfrom keras.utils.np_utils import to_categorical\nfrom keras.layers import Conv2D, MaxPool2D, Dropout, Flatten, Dense, BatchNormalization,LeakyReLU\nfrom keras.models import Sequential\nfrom keras import backend as K\nfrom keras import regularizers\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train =pd.read_csv(os.path.join(dirname,'train.csv'))\ntest =pd.read_csv(os.path.join(dirname,'test.csv'))\nsample_submission =pd.read_csv(os.path.join(dirname,'sample_submission.csv'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#display(np.unique(train)) \ndisplay(train.head(1)) \ndisplay(np.unique(train.head(1)))\ndisplay(train.shape) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train=train.drop('label',axis=1)\nY_train=train.label\nX_test = test.drop('id', axis = 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train / 255.0\nX_test = X_test / 255.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(X_train.shape)\nX_train =X_train.values.reshape(-1,28,28,1)\ndisplay(X_train.shape)\nX_test=X_test.values.reshape(-1,28,28,1)\ndisplay(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_train = to_categorical(Y_train,num_classes=10)\ndisplay(Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_val,y_train,y_val=train_test_split(X_train,Y_train,random_state=42,test_size=0.10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kernel_size_3 = (3,3)\nkernel_size_5 = (5,5)\nfilters_32 = 32\nfilters_64 = 64\nfilters_128 = 128\nfilters_256 = 256\n\nmodel = Sequential()\nmodel.add(Conv2D(filters_64, kernel_size_3, activation='relu', input_shape=(28,28,1),padding='same' ))\nmodel.add(BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"))\nmodel.add(LeakyReLU(alpha=0.1))\nmodel.add(Conv2D(filters_64, kernel_size_3, activation='relu',padding='same'))\nmodel.add(BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"))\nmodel.add(LeakyReLU(alpha=0.1))\nmodel.add(Conv2D(filters_64, kernel_size_3, activation='relu',padding='same'))\nmodel.add(BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"))\nmodel.add(LeakyReLU(alpha=0.1))\nmodel.add(MaxPool2D((2, 2)))\nmodel.add(Dropout(0.3))\n#model.add(BatchNormalization())\n\n          \nmodel.add(Conv2D(filters_128, kernel_size_5, activation='relu',padding='same'))\nmodel.add(BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"))\nmodel.add(LeakyReLU(alpha=0.1))\nmodel.add(Conv2D(filters_128, kernel_size_5, activation='relu',padding='same'))\nmodel.add(BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"))\nmodel.add(LeakyReLU(alpha=0.1))\nmodel.add(Conv2D(filters_128, kernel_size_5, activation='relu',padding='same'))\nmodel.add(BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"))\nmodel.add(LeakyReLU(alpha=0.1))\nmodel.add(MaxPool2D((2, 2)))\n#model.add(BatchNormalization())\nmodel.add(Dropout(0.3))\n\nmodel.add(Conv2D(filters_256, kernel_size_5, activation='relu'))\nmodel.add(BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"))\nmodel.add(LeakyReLU(alpha=0.1))\n#model.add(BatchNormalization())\nmodel.add(MaxPool2D((2, 2)))\nmodel.add(Dropout(0.3))\n          \nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu',kernel_regularizer=regularizers.l2(0.02)))\nmodel.add(BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(256, activation='relu',kernel_regularizer=regularizers.l2(0.02)))\nmodel.add(BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"))\nmodel.add(Dense(512, activation='relu',kernel_regularizer=regularizers.l2(0.02)))\nmodel.add(BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(10, activation='softmax'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='categorical_crossentropy',optimizer=Adadelta(learning_rate=1.0, rho=0.95),metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = train.label\nplt.figure(figsize=(15,5))\nsns.countplot(target, color='crimson')\nplt.title('The distribution of the digits in the dataset', weight='bold', fontsize='18')\nplt.xticks(weight='bold', fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,8))\nfor i in range(60):\n    plt.subplot(6,10,i+1)\n    plt.imshow(X_train[i].reshape((28,28)),cmap='binary')\n    plt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, y_train, batch_size = 256, epochs = 10, validation_data = (X_val, y_val), verbose = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\ndatagen = ImageDataGenerator(\n    rotation_range=10,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range = 10,\n    horizontal_flip = False,\n    zoom_range = 0.15)\ndatagen.fit(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 512\nEPOCHS = 50","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# training\nhistory = model.fit_generator(datagen.flow(X_train,y_train, batch_size=BATCH_SIZE),\n                              epochs = EPOCHS,\n                              shuffle=True,\n                              validation_data = (X_val,y_val),\n                              verbose = 1,\n                              steps_per_epoch=X_train.shape[0] // BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs =50\nfig,ax=plt.subplots(2,1)\nfig.set\nx=range(1,1+epochs)\nax[0].plot(x,history.history['loss'],color='red')\nax[0].plot(x,history.history['val_loss'],color='blue')\nax[1].plot(x,history.history['accuracy'],color='red')\nax[1].plot(x,history.history['val_accuracy'],color='blue')\nax[0].legend(['trainng loss','validation loss'])\nax[1].legend(['trainng acc','validation acc'])\nplt.xlabel('Number of epochs')\nplt.ylabel('accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pre_test=model.predict(X_val)\ny_pre_test=np.argmax(y_pre_test,axis=1)\ny_test=np.argmax(y_val,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conf=confusion_matrix(y_test,y_pre_test)\nconf=pd.DataFrame(conf,index=range(0,10),columns=range(0,10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=(y_pre_test-y_test!=0).tolist()\nx=[i for i,l in enumerate(x) if l!=False]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax=plt.subplots(1,4,sharey=False,figsize=(15,15))\n\nfor i in range(4):\n    ax[i].imshow(X_test[x[i]][:,:,0])\n    ax[i].set_xlabel('Real {}, Predicted {}'.format(y_test[x[i]],y_pre_test[x[i]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''confusion matrix'''\nimport seaborn as sns\n# Predict the values from the validation dataset\nY_pred = model.predict(X_val)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(y_val,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nf,ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"Greens\",linecolor=\"gray\", fmt= '.1f',ax=ax)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''predict results'''\nresults = model.predict(X_test)\n'''select the indix with the maximum probability'''\nresults = np.argmax(results,axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub =pd.read_csv(os.path.join(dirname,'sample_submission.csv'))\nsample_sub['label'] = results\nsample_sub.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}