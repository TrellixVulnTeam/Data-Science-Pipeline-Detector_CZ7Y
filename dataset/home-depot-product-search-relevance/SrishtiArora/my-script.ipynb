{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n## Defining the environment\n\n#for windows and linux, change to your path\nimport sys\nsys.path.append('C:\\Python27\\lib\\site-packages')\nsys.path.append('/home/cleo/anaconda2/lib/python2.7/site-packages')\n\n## Packages to import and functions\n\n#part of this functions was credited to https://www.kaggle.com/the1owl\n\nimport sframe\nfrom nltk.stem.porter import *\nimport re\nimport Levenshtein as lv\n\n\n\nstemmer = PorterStemmer()\n\nstrNum = {'zero':0,'one':1,'two':2,'three':3,'four':4,'five':5,'six':6,'seven':7,'eight':8,'nine':9}\nstop_w = ['for', 'xbi', 'and', 'in', 'th','on','sku','with','what','from','that','less','er','ing']\n\nlen_home=74067\nlen_test=166693\n\n\ndef str_stem(s): \n    if isinstance(s, str):\n        s = re.sub(r\"(\\w)\\.([A-Z])\", r\"\\1 \\2\", s) #Split words with a.A\n        s = s.lower()\n        s = s.replace(\"  \",\" \")\n        s = s.replace(\",\",\"\") #could be number / segment later\n        s = s.replace(\"$\",\" \")\n        s = s.replace(\"?\",\" \")\n        s = s.replace(\"-\",\" \")\n        s = s.replace(\"//\",\"/\")\n        s = s.replace(\"..\",\".\")\n        s = s.replace(\" / \",\" \")\n        s = s.replace(\" \\\\ \",\" \")\n        s = s.replace(\".\",\" . \")\n        s = re.sub(r\"(^\\.|/)\", r\"\", s)\n        s = re.sub(r\"(\\.|/)$\", r\"\", s)\n        s = re.sub(r\"([0-9])([a-z])\", r\"\\1 \\2\", s)\n        s = re.sub(r\"([a-z])([0-9])\", r\"\\1 \\2\", s)\n        s = s.replace(\" x \",\" xbi \")\n        s = re.sub(r\"([a-z])( *)\\.( *)([a-z])\", r\"\\1 \\4\", s)\n        s = re.sub(r\"([a-z])( *)/( *)([a-z])\", r\"\\1 \\4\", s)\n        s = s.replace(\"*\",\" xbi \")\n        s = s.replace(\" by \",\" xbi \")\n        s = re.sub(r\"([0-9])( *)\\.( *)([0-9])\", r\"\\1.\\4\", s)\n        s = re.sub(r\"([0-9]+)( *)(inches|inch|in|')\\.?\", r\"\\1in. \", s)\n        s = re.sub(r\"([0-9]+)( *)(foot|feet|ft|'')\\.?\", r\"\\1ft. \", s)\n        s = re.sub(r\"([0-9]+)( *)(pounds|pound|lbs|lb)\\.?\", r\"\\1lb. \", s)\n        s = re.sub(r\"([0-9]+)( *)(square|sq) ?\\.?(feet|foot|ft)\\.?\", r\"\\1sq.ft. \", s)\n        s = re.sub(r\"([0-9]+)( *)(cubic|cu) ?\\.?(feet|foot|ft)\\.?\", r\"\\1cu.ft. \", s)\n        s = re.sub(r\"([0-9]+)( *)(gallons|gallon|gal)\\.?\", r\"\\1gal. \", s)\n        s = re.sub(r\"([0-9]+)( *)(ounces|ounce|oz)\\.?\", r\"\\1oz. \", s)\n        s = re.sub(r\"([0-9]+)( *)(centimeters|cm)\\.?\", r\"\\1cm. \", s)\n        s = re.sub(r\"([0-9]+)( *)(milimeters|mm)\\.?\", r\"\\1mm. \", s)\n        s = s.replace(\"Â°\",\" degrees \")\n        s = re.sub(r\"([0-9]+)( *)(degrees|degree)\\.?\", r\"\\1deg. \", s)\n        s = s.replace(\" v \",\" volts \")\n        s = re.sub(r\"([0-9]+)( *)(volts|volt)\\.?\", r\"\\1volt. \", s)\n        s = re.sub(r\"([0-9]+)( *)(watts|watt)\\.?\", r\"\\1watt. \", s)\n        s = re.sub(r\"([0-9]+)( *)(amperes|ampere|amps|amp)\\.?\", r\"\\1amp. \", s)\n        s = s.replace(\"  \",\" \")\n        s = s.replace(\" . \",\" \")\n        #s = (\" \").join([z for z in s.split(\" \") if z not in stop_w])\n        s = (\" \").join([str(strNum[z]) if z in strNum else z for z in s.split(\" \")])\n        #s = (\" \").join([stemmer.stem(z) for z in s.split(\" \")])\n        \n        s = s.lower()\n        s = s.replace(\"toliet\",\"toilet\")\n        s = s.replace(\"airconditioner\",\"air conditioner\")\n        s = s.replace(\"vinal\",\"vinyl\")\n        s = s.replace(\"vynal\",\"vinyl\")\n        s = s.replace(\"skill\",\"skil\")\n        s = s.replace(\"snowbl\",\"snow bl\")\n        s = s.replace(\"plexigla\",\"plexi gla\")\n        s = s.replace(\"rustoleum\",\"rust-oleum\")\n        s = s.replace(\"whirpool\",\"whirlpool\")\n        s = s.replace(\"whirlpoolga\", \"whirlpool ga\")\n        s = s.replace(\"whirlpoolstainless\",\"whirlpool stainless\")\n        return s\n    else:\n        return \"null\"\n\ndef seg_words(str1, str2):\n    str2 = str2.lower()\n    str2 = re.sub(\"[^a-z0-9./]\",\" \", str2)\n    str2 = [z for z in set(str2.split()) if len(z)>2]\n    words = str1.lower().split(\" \")\n    s = []\n    for word in words:\n        if len(word)>3:\n            s1 = []\n            s1 += segmentit(word,str2,True)\n            if len(s)>1:\n                s += [z for z in s1 if z not in ['er','ing','s','less'] and len(z)>1]\n            else:\n                s.append(word)\n        else:\n            s.append(word)\n    return (\" \".join(s))\n\ndef segmentit(s, txt_arr, t):\n    st = s\n    r = []\n    for j in range(len(s)):\n        for word in txt_arr:\n            if word == s[:-j]:\n                r.append(s[:-j])\n                #print(s[:-j],s[len(s)-j:])\n                s=s[len(s)-j:]\n                r += segmentit(s, txt_arr, False)\n    if t:\n        i = len((\"\").join(r))\n        if not i==len(st):\n            r.append(st[i:])\n    return r\n\ndef str_common_word(str1, str2):\n    words, cnt = str1.split(), 0\n    for word in words:\n        if str2.find(word)>=0:\n            cnt+=1\n    return cnt\n\ndef str_whole_word(str1, str2, i_):\n    cnt = 0\n    while i_ < len(str2):\n        i_ = str2.find(str1, i_)\n        if i_ == -1:\n            return cnt\n        else:\n            cnt += 1\n            i_ += len(str1)\n    return cnt\n\ndef map_similarity(arraya,arrayb,fn):\n    from nltk.corpus import wordnet as wn\n    arrayc=[]\n    #if arrayb==None:##this is for attributes, because not all rows has attributes\n     #   return None\n    for a in arraya:\n        for b in arrayb:\n            if fn=='path':\n                try:\n                    arrayc.append(wn.synset(wn.synsets(a)[0].name()).path_similarity(wn.synset(wn.synsets(b)[0].name())))\n                except:\n                    arrayc.append(None)\n            if fn=='lch':\n                try:\n                    arrayc.append(wn.synset(wn.synsets(a)[0].name()).lch_similarity(wn.synset(wn.synsets(b)[0].name())))\n                except:\n                    arrayc.append(None)\n            if fn=='wup':\n                try:\n                    arrayc.append(wn.synset(wn.synsets(a)[0].name()).wup_similarity(wn.synset(wn.synsets(b)[0].name())))\n                except:\n                    arrayc.append(None)\n    return arrayc\n\ndef sum_values(arrayx):\n    return sum(filter(None,arrayx))\n\ndef max_values(arrayx):\n    try:\n        return max(filter(None,arrayx))\n    except:\n        return 0\n    \ndef min_values(arrayx):\n    try:\n        return min(filter(None,arrayx))\n    except:\n        return 0\n\ndef count_values(arrayx):\n    return len(filter(None,arrayx))\n\n\ndef distance_lv(arraya,arrayb):\n    arrayc=[]\n    for a in arraya:\n        for b in arrayb:\n            arrayc.append(lv.distance(a,b))\n    return arrayc\n\ndef has_attribute(x):\n    if x is None:\n        return 0.0\n    elif x is not None: return 1.0\n    \n\n## reading raw data\n\nfrom sys import platform as _platform\nif _platform=='linux2' or _platform=='linux':\n    home=sframe.SFrame.read_csv('train.csv')\n    descriptions=sframe.SFrame.read_csv('product_descriptions.csv')\n    attributes=sframe.SFrame.read_csv('attributes.csv')\nelse:\n    home=sframe.SFrame.read_csv('c:/users/csbatista/Downloads/train_home_depot.csv/train.csv')\n    descriptions=sframe.SFrame.read_csv('c:/users/csbatista/Downloads/product_descriptions.csv')\n    attributes=sframe.SFrame.read_csv('c:/users/csbatista/Downloads/attributes.csv/attributes.csv')\n\n## refining some words in product title and search term\n\nhome['search_term']=home['search_term'].apply(lambda x:str_stem(x))\n\nhome['product_title']=home['product_title'].apply(lambda x:str_stem(x))\n\n##  Defining intervals to split and load data when it's necessary\n\nparts=5\nlen_data=len_home\npart=len_data/parts\nintervals=list()\nfor i in range(1,parts+1):\n    intervals.append([(i-1)*part,i*part-1])\nintervals[parts-1]=[(parts-1)*part,len_data]\n\n## to home refined\n\n### Product the home_join's files\n\n#This part is to produce the top 15 tfidf terms for product title.\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntf = TfidfVectorizer(analyzer='word', ngram_range=(1,1), min_df = 0, stop_words = 'english',encoding='utf-8',decode_error='ignore')\n\nhome_join=list()\nfor interval in intervals:#para cada intervalo\n    home_part=home[interval[0]:interval[1]+1]#the '+1' is for the stop index\n    tfidf_matrix =  tf.fit_transform(home_part['product_title'])\n    feature_names = tf.get_feature_names() \n    dense = tfidf_matrix.todense()\n    top_terms=list()\n    for i in range(0,len(home_part['product_title'])):\n        term = dense[i].tolist()[0]\n        phrase_scores = [pair for pair in zip(range(0, len(term)), term) if pair[1] > 0]\n        sorted_phrase_scores = sorted(phrase_scores, key=lambda t: t[1] * -1)\n        if len(sorted_phrase_scores )>15:\n            phrase_range=15\n        else: phrase_range = len(sorted_phrase_scores)\n        list_phrase=list()\n        #para cada conjunto de termos\n        for phrase, score in [(feature_names[word_id], score) for (word_id, score) in sorted_phrase_scores][:phrase_range]:\n            list_phrase.append(phrase)\n        top_terms.append(list_phrase)\n    home_part['top_terms_product_name']=top_terms\n    home_part.save('home_joins/home_join'+str(interval[0])+'.csv', format='csv')\n    #if len(home_join)==0:\n     #   home_join=home_part[0:0]\n    #home_join.append(home_part)\n\nhome=sframe.SFrame()\nfor i in intervals:\n    partial_home=sframe.SFrame.read_csv('home_joins/home_join'+str(i[0])+'.csv',column_type_hints ={'id':int,'top_terms_product_name':list})\n    home=home.append(partial_home)\n\n#Home_refined file has all the similarities, also those sums, max and count, and levenshtein with sum and min. All values are between search term and product title.\n\n#split search terms\nhome['search_term']=home['search_term'].apply(lambda x: x.split())\n\nhome['similarities_path']=home.apply(lambda x:map_similarity(x['search_term'],x['top_terms_product_name'],'path'))\n\nhome['similarities_lch']=home.apply(lambda x:map_similarity(x['search_term'],x['top_terms_product_name'],'lch'))\n\nhome['similarities_wup']=home.apply(lambda x:map_similarity(x['search_term'],x['top_terms_product_name'],'wup'))\n\nhome['sum_wup']=home['similarities_wup'].apply(lambda x:sum_values(x))\n\nhome['max_wup']=home['similarities_wup'].apply(lambda x:max_values(x))\n\nhome['count_wup']=home['similarities_wup'].apply(lambda x:count_values(x))\n\n#home['mean_wup']=home['sum_wup']/home['count_wup']\n\nhome['sum_lch']=home['similarities_lch'].apply(lambda x:sum_values(x))\n\nhome['max_lch']=home['similarities_lch'].apply(lambda x:max_values(x))\n\nhome['count_lch']=home['similarities_lch'].apply(lambda x:count_values(x))\n\n#home['mean_lch']=home['sum_lch']/home['count_lch']\n\nhome['sum_path']=home['similarities_path'].apply(lambda x:sum_values(x))\n\nhome['max_path']=home['similarities_path'].apply(lambda x:max_values(x))\n\nhome['count_path']=home['similarities_path'].apply(lambda x:count_values(x))\n\n#home['mean_path']=home['sum_path']/home['count_path']\n\n#to calculate levenshtein distance for each word in search term and top terms product name\nhome['levenshtein_dist_search_name']=home.apply(lambda x:distance_lv(x['search_term'],x['top_terms_product_name']))\n\nhome['min_lev_name']=home['levenshtein_dist_search_name'].apply(lambda x:min_values(x))\n\nhome['sum_lev_name']=home['levenshtein_dist_search_name'].apply(lambda x:sum_values(x))\n\nfrom sframe import aggregate as agg\nhome=home.join(home.groupby(key_columns='product_uid',operations={'count':agg.COUNT()}),on='product_uid')\n\nhome['count']=(home['count']-min(home['count']))/(max(home['count']-min(home['count'])))\n\nhome['len_search_term']=home['search_term'].apply(lambda x:len(x))\n\nhome['len_product_title']=home['product_title'].apply(lambda x:len(x))\n\nhome=home.remove_columns(['top_terms_product_name','similarities_wup','similarities_path','similarities_lch','levenshtein_dist_search_name'])\n\nhome.save('home_joins/home_refined.csv',format='csv')\n\n## to home refined 1\n\n#This part is to produce the top 15 tfidf terms for descriptions.\n\npart=len(descriptions)/20\nintervals=list()\nfor i in range(1,21):\n    intervals.append([(i-1)*part,i*part-1])\nintervals[19]=[19*part,len(descriptions)]\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntf = TfidfVectorizer(analyzer='word', ngram_range=(1,1), min_df = 0, stop_words = 'english',encoding='utf-8',decode_error='ignore')\n\ndesc_join=list()\nfor interval in intervals:#para cada intervalo\n    desc_part=descriptions[interval[0]:interval[1]+1]#the '+1' is for the stop index\n    tfidf_matrix =  tf.fit_transform(desc_part['product_description'])\n    feature_names = tf.get_feature_names() \n    dense = tfidf_matrix.todense()\n    top_terms=list()\n    for i in range(0,len(desc_part['product_description'])):\n        term = dense[i].tolist()[0]\n        phrase_scores = [pair for pair in zip(range(0, len(term)), term) if pair[1] > 0]\n        sorted_phrase_scores = sorted(phrase_scores, key=lambda t: t[1] * -1)\n        if len(sorted_phrase_scores )>15:\n            phrase_range=15\n        else: phrase_range = len(sorted_phrase_scores)\n        list_phrase=list()\n        #para cada conjunto de termos\n        for phrase, score in [(feature_names[word_id], score) for (word_id, score) in sorted_phrase_scores][:phrase_range]:\n            list_phrase.append(phrase)\n        top_terms.append(list_phrase)\n    desc_part['top_terms_product_description']=top_terms\n    desc_part.save('home_joins/home_join_description'+str(interval[0])+'.csv', format='csv')\n    #if len(home_join)==0:\n     #   home_join=home_part[0:0]\n    #home_join.append(home_part)\n\ndescriptions=sframe.SFrame.read_csv('home_joins/home_join_description0.csv',column_type_hints ={'product_uid':int,'top_terms_product_description':list})[['product_uid','top_terms_product_description']][0:0]\nfor i in intervals:\n    partial=sframe.SFrame.read_csv('home_joins/home_join_description'+str(i[0])+'.csv',column_type_hints ={'product_uid':int,'top_terms_product_description':list})[['product_uid','top_terms_product_description']]\n    descriptions=descriptions.append(partial)\n\ndescriptions.save('home_joins/descriptions_refined.csv',format='csv')\n\ndescriptions=sframe.SFrame.read_csv('home_joins/descriptions_refined.csv')\n\nhome=home.join(descriptions,on='product_uid')\n\nhome['similarities_path_description']=home.apply(lambda x:map_similarity(x['search_term'],x['top_terms_product_description'],'path'))\n\nhome['similarities_lch_description']=home.apply(lambda x:map_similarity(x['search_term'],x['top_terms_product_description'],'lch'))\n\nhome['similarities_wup_description']=home.apply(lambda x:map_similarity(x['search_term'],x['top_terms_product_description'],'wup'))\n\nhome['sum_wup_description']=home['similarities_wup_description'].apply(lambda x:sum_values(x))\n\nhome['max_wup_description']=home['similarities_wup_description'].apply(lambda x:max_values(x))\n\nhome['count_wup_description']=home['similarities_wup_description'].apply(lambda x:count_values(x))\n\n#home['mean_wup_description']=home['sum_wup_description']/home['count_wup_description']\n\nhome['sum_lch_description']=home['similarities_lch_description'].apply(lambda x:sum_values(x))\n\nhome['max_lch_description']=home['similarities_lch_description'].apply(lambda x:max_values(x))\n\nhome['count_lch_description']=home['similarities_lch_description'].apply(lambda x:count_values(x))\n\n#home['mean_lch_description']=home['sum_lch_description']/home['count_lch_description']\n\nhome['sum_path_description']=home['similarities_path_description'].apply(lambda x:sum_values(x))\n\nhome['max_path_description']=home['similarities_path_description'].apply(lambda x:max_values(x))\n\nhome['count_path_description']=home['similarities_path_description'].apply(lambda x:count_values(x))\n\n#home['mean_path_description']=home['sum_path_description']/home['count_path_description']\n\nhome['levenshtein_dist_search_description']=home.apply(lambda x:distance_lv(x['search_term'],x['top_terms_product_description']))\n\nhome['min_lev_description']=home['levenshtein_dist_search_description'].apply(lambda x:min_values(x))\n\nhome['sum_lev_description']=home['levenshtein_dist_search_description'].apply(lambda x:sum_values(x))\n\nhome=home.remove_columns(['top_terms_product_description','similarities_path_description','similarities_lch_description','similarities_wup_description','levenshtein_dist_search_description'])\n\nhome.save('home_joins/home_refined1.csv',format='csv')\n\n## to home refined 2\n\n#This part is to check attributes. I had a problem manipulating 'na' values generated in products without attributes, so I decided to use only some informations about attributes\n\n## to join attributes from the same product_uid\nfrom sframe import aggregate as agg\nattributes=attributes.groupby('product_uid',{'attribute':agg.CONCAT('name','value')})\nattributes=attributes.sort('product_uid')\n\nattributes['attribute']=attributes.apply(lambda x:str(x['attribute']))\n\nattributes.save('home_joins/attributes_refined.csv',format='csv')\n\nattributes=sframe.SFrame.read_csv('home_joins/attributes_refined.csv')\n\nhome=sframe.SFrame.read_csv('home_joins/home_refined1.csv')\n\nhome['has_attribute']=home['attribute'].apply(lambda x:has_attribute(x))\n\nhome=home.fillna('has_attribute',0)\n\nhome=home.join(attributes,on='product_uid',how='left')\n\nhome=home.remove_columns(['product_title','search_term','product_uid','attribute'])\n\nhome.save('home_joins/home_refined2.csv',format='csv')\n\nhome=sframe.SFrame.read_csv('home_joins/home_refined2.csv')\n\nhome.head()\n\n## Split the data in training and testing sets\n\nhome_train,home_test=home.random_split(0.8,seed=42)\n\nhome_train.save('home_joins/home_train_refined.csv')\nhome_test.save('home_joins/home_test_refined.csv')\n\nhome_train=sframe.SFrame.read_csv('home_joins/home_train_refined.csv')\n\nhome_test=sframe.SFrame.read_csv('home_joins/home_test_refined.csv')\n\nhome_train=home_train.remove_columns(['id','product_uid','search_term'])\n\nhome_test=home_test.remove_columns(['id','product_uid','search_term'])\n\nhome_train=home_train.to_dataframe()\n\nhome_test=home_test.to_dataframe()\n\n## creating a model\n\nfrom sklearn import linear_model\nclf = linear_model.BayesianRidge()\nclf.fit(home_train.drop('relevance',1), home_train['relevance'])\n\npred=clf.predict(home_test.drop('relevance',1))\n\n#to check rmse\nsum(abs(pred-home_test['relevance']))/len(home_test)\n\n## applying the model in the test set\n\ntest=sframe.SFrame.read_csv('test.csv')\n\ntest['search_term']=test['search_term'].apply(lambda x:str_stem(x))\n\ntest['product_title']=test['product_title'].apply(lambda x:str_stem(x))\n\nparts=10\nlen_data=len_test\npart=len_data/parts\nintervals=list()\nfor i in range(1,parts+1):\n    intervals.append([(i-1)*part,i*part-1])\nintervals[parts-1]=[(parts-1)*part,len_data]\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntf = TfidfVectorizer(analyzer='word', ngram_range=(1,1), min_df = 0, stop_words = 'english',encoding='utf-8',decode_error='ignore')\n\n#test_join=list()\nfor interval in intervals:#para cada intervalo\n    test_part=test[interval[0]:interval[1]+1]#the '+1' is for the stop index\n    tfidf_matrix =  tf.fit_transform(test_part['product_title'])\n    feature_names = tf.get_feature_names() \n    dense = tfidf_matrix.todense()\n    top_terms=list()\n    for i in range(0,len(test_part['product_title'])):\n        term = dense[i].tolist()[0]\n        phrase_scores = [pair for pair in zip(range(0, len(term)), term) if pair[1] > 0]\n        sorted_phrase_scores = sorted(phrase_scores, key=lambda t: t[1] * -1)\n        if len(sorted_phrase_scores )>15:\n            phrase_range=15\n        else: phrase_range = len(sorted_phrase_scores)\n        list_phrase=list()\n        #para cada conjunto de termos\n        for phrase, score in [(feature_names[word_id], score) for (word_id, score) in sorted_phrase_scores][:phrase_range]:\n            list_phrase.append(phrase)\n        top_terms.append(list_phrase)\n    test_part['top_terms_product_name']=top_terms\n    test_part.remove_column('product_title').save('home_joins/test_join'+str(interval[0])+'.csv', format='csv')\n    #if len(home_join)==0:\n     #   home_join=home_part[0:0]\n    #home_join.append(home_part)\n\n#complete_test=sframe.SFrame.read_csv('home_joins/test_join0.csv',column_type_hints ={'top_terms_product_name':list})[['product_uid','top_terms_product_name']][0:0]\ntest=sframe.SFrame()\nfor i in intervals:\n    partial_test=sframe.SFrame.read_csv('home_joins/test_join'+str(i[0])+'.csv',column_type_hints ={'top_terms_product_name':list})\n    test=test.append(partial_test)\n\ntest['search_term']=test['search_term'].apply(lambda x: x.split())\n\ntest.save('home_joins/test_refined.csv',format='csv')\n\ntest['similarities_path']=test.apply(lambda x:map_similarity(x['search_term'],x['top_terms_product_name'],'path'))\n\ntest['similarities_lch']=test.apply(lambda x:map_similarity(x['search_term'],x['top_terms_product_name'],'lch'))\n\ntest['similarities_wup']=test.apply(lambda x:map_similarity(x['search_term'],x['top_terms_product_name'],'wup'))\n\ndescriptions=sframe.SFrame.read_csv('home_joins/descriptions_refined.csv')\n\ntest=test.join(descriptions,on='product_uid')\n\ntest['similarities_path_description']=test.apply(lambda x:map_similarity(x['search_term'],x['top_terms_product_description'],'path'))\n\ntest['similarities_lch_description']=test.apply(lambda x:map_similarity(x['search_term'],x['top_terms_product_description'],'lch'))\n\ntest['similarities_wup_description']=test.apply(lambda x:map_similarity(x['search_term'],x['top_terms_product_description'],'wup'))\n\ntest['sum_wup']=test['similarities_wup'].apply(lambda x:sum_values(x))\n\ntest['max_wup']=test['similarities_wup'].apply(lambda x:max_values(x))\n\ntest['count_wup']=test['similarities_wup'].apply(lambda x:count_values(x))\n\n#test['mean_wup']=test['sum_wup']/test['count_wup']\n\ntest['sum_lch']=test['similarities_lch'].apply(lambda x:sum_values(x))\n\ntest['max_lch']=test['similarities_lch'].apply(lambda x:max_values(x))\n\ntest['count_lch']=test['similarities_lch'].apply(lambda x:count_values(x))\n\n#test['mean_lch']=test['sum_lch']/test['count_lch']\n\ntest['sum_path']=test['similarities_path'].apply(lambda x:sum_values(x))\n\ntest['max_path']=test['similarities_path'].apply(lambda x:max_values(x))\n\ntest['count_path']=test['similarities_path'].apply(lambda x:count_values(x))\n\n#test['mean_path']=test['sum_path']/test['count_path']\n\ntest['sum_wup_description']=test['similarities_wup_description'].apply(lambda x:sum_values(x))\n\ntest['max_wup_description']=test['similarities_wup_description'].apply(lambda x:max_values(x))\n\ntest['count_wup_description']=test['similarities_wup_description'].apply(lambda x:count_values(x))\n\n#test['mean_wup_description']=test['sum_wup_description']/test['count_wup_description']\n\ntest['sum_lch_description']=test['similarities_lch_description'].apply(lambda x:sum_values(x))\n\ntest['max_lch_description']=test['similarities_lch_description'].apply(lambda x:max_values(x))\n\ntest['count_lch_description']=test['similarities_lch_description'].apply(lambda x:count_values(x))\n\n#test['mean_lch_description']=test['sum_lch_description']/test['count_lch_description']\n\ntest['sum_path_description']=test['similarities_path_description'].apply(lambda x:sum_values(x))\n\ntest['max_path_description']=test['similarities_path_description'].apply(lambda x:max_values(x))\n\ntest['count_path_description']=test['similarities_path_description'].apply(lambda x:count_values(x))\n\n#test['mean_path_description']=test['sum_path_description']/test['count_path_description']\n\ntest['levenshtein_dist_search_description']=test.apply(lambda x:distance_lv(x['search_term'],x['top_terms_product_description']))\n\ntest['levenshtein_dist_search_name']=test.apply(lambda x:distance_lv(x['search_term'],x['top_terms_product_name']))\n\ntest['min_lev_description']=test['levenshtein_dist_search_description'].apply(lambda x:min_values(x))\n\ntest['min_lev_name']=test['levenshtein_dist_search_name'].apply(lambda x:min_values(x))\n\ntest['sum_lev_description']=test['levenshtein_dist_search_description'].apply(lambda x:sum_values(x))\n\ntest['sum_lev_name']=test['levenshtein_dist_search_name'].apply(lambda x:sum_values(x))\n\nfrom sframe import aggregate as agg\ntest=test.join(test.groupby(key_columns='product_uid',operations={'count':agg.COUNT()}),on='product_uid')\n\ntest['count']=(test['count']-min(test['count']))/(max(test['count']-min(test['count'])))\n\ntest['len_search_term']=test['search_term'].apply(lambda x:len(x))\n\ntest['len_product_title']=test['product_title'].apply(lambda x:len(x))\n\ntest=test.remove_columns(['product_title','search_term','product_uid','top_terms_product_name','top_terms_product_description','similarities_wup','similarities_path','similarities_lch','similarities_path_description','similarities_lch_description','similarities_wup_description','levenshtein_dist_search_description','levenshtein_dist_search_name'])\n\ntest.save('home_joins/test_refined1.csv')#falta por se tem attributos ou nao\n\n## Predicting in test set\n\ntest=sframe.SFrame.read_csv('home_joins/test_refined1.csv',column_type_hints={'levenshtein_dist_search_name':list})\n\nattributes=sframe.SFrame.read_csv('home_joins/attributes_refined.csv')\n\ntest=test.join(attributes,on='product_uid',how='left')\n\ntest['has_attribute']=test['top_terms_product_attributes'].apply(lambda x:has_attribute(x))\n\ntest=test.fillna('has_attribute',0)\n\ntest=test.remove_columns(['product_title','search_term','product_uid','top_terms_product_name','top_terms_product_attributes','top_terms_product_description','similarities_wup','similarities_path','similarities_lch','similarities_path_description','similarities_lch_description','similarities_wup_description','levenshtein_dist_search_description','levenshtein_dist_search_name'])\n\ntest.save('home_joins/test_refined_final.csv')\n\ntest=sframe.SFrame.read_csv('home_joins/test_refined_final.csv')\n\ntest=test.to_dataframe()\n\npred=clf.predict(test[home_train.columns[1:27]])\n\nresult_final=sframe.SFrame([sframe.SArray(test['id']),sframe.SArray(pred)]).rename({'X1':'id','X2':'relevance'})\n\nresult_final['relevance']=result_final['relevance'].apply(lambda x:3 if x>3 else 1 if x<1 else x)\n\nresult_final.save('home_joins/result_final2.csv',format='csv')\n\nresult_final"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}