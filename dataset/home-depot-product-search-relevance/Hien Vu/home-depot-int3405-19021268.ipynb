{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Report link\n Notion link: hvn2706.notion.site/INT3405E_20-B-o-c-o-b-i-t-p-cu-i-kh-a-8ddde04d9d7e400383ece7927e884036","metadata":{}},{"cell_type":"markdown","source":"# Initialize","metadata":{"id":"frYjWfnGmelJ"}},{"cell_type":"code","source":"%pip install nltk\n%pip install bs4\n%pip install textdistance\n%pip install catboost","metadata":{"id":"wDq0xfNfmelN","outputId":"690507ba-b2b0-4621-f56a-b55980eb6c7a","execution":{"iopub.status.busy":"2022-01-06T17:23:25.820364Z","iopub.execute_input":"2022-01-06T17:23:25.821471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#data visualize\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport numpy as np\nimport math\nimport time\n\n# preprocess\nimport string\nimport re\n\nfrom bs4 import BeautifulSoup\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import RegexpTokenizer\nimport ast\n\n# extract feature\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom scipy import spatial\nimport textdistance","metadata":{"id":"1Yv_iL3aAbYw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk.download('stopwords')","metadata":{"id":"I0ysITZ9melP","outputId":"096ed7e2-431a-4e78-ff11-dc63104ae6d2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH = '../input/home-depot-product-search-relevance/'\nattributes = pd.read_csv(f'{PATH}attributes.csv.zip')\ndescriptions = pd.read_csv(f'{PATH}product_descriptions.csv.zip')\nquestion_test = pd.read_csv(f'{PATH}test.csv.zip', encoding='latin-1')\ntrain = pd.read_csv(f'{PATH}train.csv.zip', encoding='latin-1')","metadata":{"id":"hyfWR_KCDAbe","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data exploration","metadata":{"id":"QYF-E16UYZm7"}},{"cell_type":"code","source":"train['relevance'].describe()","metadata":{"id":"41eTcxHBpw_g","outputId":"e96794ee-4770-45cb-9b95-b465d8455792","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(train['relevance'])","metadata":{"id":"sLOQYq1WqCIq","outputId":"e50a24d2-ac64-4827-ecc9-815c0b9f83cc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[train.isna().any(axis=1)]","metadata":{"id":"pEEy8peBxJOO","outputId":"a83f3e46-79c2-4989-b2e0-867c5abb109c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"descriptions[descriptions.isna().any(axis=1)]","metadata":{"id":"Xz3L9qC3sbWI","outputId":"1a3784bd-66dd-4245-abcd-35c5daba67a8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"attributes[attributes.isna().any(axis=1)]","metadata":{"id":"26gelskqvcBt","outputId":"36628ca1-6559-4722-ca3b-c28b63ca3af3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# number of unique products in total, in test dataset and in train dataset\nprint(len(descriptions['product_uid'].unique()), len(question_test['product_uid'].unique()), len(train['product_uid'].unique()))","metadata":{"id":"vpxqCx3-q1j4","outputId":"2c299b5e-3d9c-40ec-a970-f79f1b41bfc0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Longest title:', train['product_title'].apply(lambda x: len(str(x))).max())\nprint('Shortest title:', train['product_title'].apply(lambda x: len(str(x))).min())\nsns.countplot(train['product_title'].apply(lambda x: len(str(x))))","metadata":{"id":"6GzUlqRVxl_0","outputId":"fc808580-4a8b-436b-964a-89764d2f76ae","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('longest search_term:', train['search_term'].apply(lambda x: len(str(x))).max())\nprint('shortest search_term:', train['search_term'].apply(lambda x: len(str(x))).min())\nsns.countplot(train['search_term'].apply(lambda x: len(str(x))))","metadata":{"id":"tl-i94BQz79O","outputId":"f04ab952-414c-4d97-b1c0-b28a0e0fb283","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('longest attribute:', attributes['value'].apply(lambda x: len(str(x))).max())\nprint('shortest attribute:', attributes['value'].apply(lambda x: len(str(x))).min())\nplt.plot(attributes['value'].apply(lambda x: len(str(x))))","metadata":{"id":"N-KGBWj21FZM","outputId":"624d545d-1d5b-4535-f267-7d1de8235537","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('longest description:', descriptions['product_description'].apply(lambda x: len(str(x))).max())\nprint('shortest description:', descriptions['product_description'].apply(lambda x: len(str(x))).min())\nplt.plot(descriptions['product_description'].apply(lambda x: len(str(x))))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{"id":"I4HFfAN3melR"}},{"cell_type":"markdown","source":"Remove null value","metadata":{"id":"se_4_mgy-HjO"}},{"cell_type":"code","source":"train.fillna(' ')\ndescriptions.fillna(' ')\nattributes[['name', 'value']] = attributes[['name', 'value']].fillna(' ')\nattributes['product_uid'] = attributes['product_uid'].fillna(0)","metadata":{"id":"b1cT433FmelS","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fixing typos in search_term","metadata":{"id":"z6aUxiei-ajq"}},{"cell_type":"code","source":"# load typos file from https://www.kaggle.com/steubk/fixing-typos\nfile = open(\"../input/home-depot-typos/search_term_typo.txt\", \"r\")\ncontents = file.read()\ncorrect_typo = ast.literal_eval(contents)\n\nfile.close()","metadata":{"id":"yQflA9Up-cWp","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['search_term'] = train['search_term'].map(lambda x: correct_typo[x] if x in correct_typo.keys() else x)\nquestion_test['search_term'] = question_test['search_term'].map(lambda x: correct_typo[x] if x in correct_typo.keys() else x)","metadata":{"id":"eD__GkQv_CLh","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Merge the data into one data frame","metadata":{"id":"H41jN9ox-QkX"}},{"cell_type":"code","source":"attributes['product_uid'] = attributes['product_uid'].astype(np.int32)\nattributes['name_value'] = attributes['name'].map(str) + ' ' + attributes['value'].map(str)\natt_tmp = pd.pivot_table(attributes, index=['product_uid'], values=['name_value'], aggfunc=lambda x: ' '.join(x))","metadata":{"id":"3hSASc7vbc7j","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"att_tmp.head(5)","metadata":{"id":"PClNT4nepPsU","outputId":"2fdce89e-78c0-49bf-df84-eb402d4f5088","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.merge(train, descriptions, how='left', on='product_uid')\ntrain = pd.merge(train, att_tmp, how='left', on='product_uid')\n\nquestion_test = pd.merge(question_test, descriptions, how='left', on='product_uid')\nquestion_test = pd.merge(question_test, att_tmp, how='left', on='product_uid')","metadata":{"id":"KggmdJAtmelT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Turn sentences into tokens of words, remove html tags, stopwords and stemming","metadata":{"id":"LFUzR_hdN-KS"}},{"cell_type":"code","source":"tokenizer = RegexpTokenizer(r'\\w+')\nstop_words = set(stopwords.words('english'))\nps = PorterStemmer()\n\ndef tokenize(text: str):\n    text = str(text)\n    word_tokens = tokenizer.tokenize(text)\n    return word_tokens\n\n\ndef remove_html(text):\n    # reference from https://www.kaggle.com/yowtshjhj/hdp-search-relevant-from-doananh020418?scriptVersionId=82967241&cellId=44\n    text = str(text)\n    soup = BeautifulSoup(text, 'lxml')\n    text = soup.get_text().replace('Click here to review our return policy for additional information regarding returns', '')\n    return text\n\n\ndef remove_stopwords(token_list: list):\n    filtered_sentence = []\n    for token in token_list:\n        if token not in stop_words:\n            filtered_sentence.append(token)\n    return filtered_sentence\n\n\ndef stemming(token_list: list):\n    stemmed_sentence = []\n    for token in token_list:\n        stemmed_sentence.append(ps.stem(token))\n    return stemmed_sentence\n\n\ndef preprocess(df):\n    df.fillna('')\n    df['name_value'] = df['name_value'].apply(lambda x: remove_html(x))\n    df['token_name_value'] = df['name_value'].apply(lambda x: tokenize(x))\n    df['token_name_value'] = df['token_name_value'].apply(lambda x: remove_stopwords(x))\n    df['token_name_value'] = df['token_name_value'].apply(lambda x: stemming(x))\n    print('attribute done!')\n    \n    df['product_description'] = df['product_description'].apply(lambda x: remove_html(x))\n    df['token_description'] = df['product_description'].apply(lambda x: tokenize(x))\n    df['token_description'] = df['token_description'].apply(lambda x: remove_stopwords(x))\n    df['token_description'] = df['token_description'].apply(lambda x: stemming(x))\n    print('description done!')\n    \n    df['product_title'] = df['product_title'].apply(lambda x: remove_html(x))\n    df['token_title'] = df['product_title'].apply(lambda x: tokenize(x))\n    df['token_title'] = df['token_title'].apply(lambda x: remove_stopwords(x))\n    df['token_title'] = df['token_title'].apply(lambda x: stemming(x))\n    print('title done!')\n    \n    df['search_term'] = df['search_term'].apply(lambda x: remove_html(x))\n    df['token_search_term'] = df['search_term'].apply(lambda x: tokenize(x))\n    df['token_description'] = df['token_description'].apply(lambda x: remove_stopwords(x))\n    df['token_search_term'] = df['token_search_term'].apply(lambda x: stemming(x))\n    print('search term done!')\n    \n    return df","metadata":{"id":"v3r4QAggXx5m","scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = time.time()\ntrain = preprocess(train) #might take some minutes to run\nrun_time = time.time() - start_time\nprint(run_time)","metadata":{"id":"VHl9uwr1melU","outputId":"f544600a-1964-49c5-d4a9-7405d1c4f9a2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(5)","metadata":{"id":"USt-oAmK3Flu","outputId":"c60ebb66-99c7-47d2-b99f-1ed4dc4c8d5c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature extraction","metadata":{"id":"I6kgP6jYmelU"}},{"cell_type":"markdown","source":"## Get text and token length","metadata":{"id":"CvTPrbV34xau"}},{"cell_type":"code","source":"def cal_length_text(df):\n    # get the number of characters in name_value, product_description, product_title and search_term\n    df['len_name_value'] = df['name_value'].apply(lambda x: len(str(x)))\n    df['len_description'] = df['product_description'].apply(lambda x: len(x))\n    df['len_title'] = df['product_title'].apply(lambda x: len(x))\n    df['len_search_term'] = df['search_term'].apply(lambda x: len(x))\n    df['len_all'] = df['len_name_value'] + df['len_description'] + df['len_title']\n\n    return df\n\ndef cal_length_token(df):\n    # get the number of words in name_value, product_description, product_title and search_term\n    df['token_len_name_value'] = df['token_name_value'].apply(lambda x: len(x))\n    df['token_len_description'] = df['token_description'].apply(lambda x: len(x))\n    df['token_len_title'] = df['token_title'].apply(lambda x: len(x))\n    df['token_len_search_term'] = df['token_search_term'].apply(lambda x: len(x))\n    df['token_len_all'] = df['token_len_name_value'] + df['token_len_description'] + df['token_len_title']\n\n    return df","metadata":{"id":"NyKgjCMf4qeJ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = cal_length_text(train)\ntrain = cal_length_token(train)","metadata":{"id":"MFmBkXF4ZEIs","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Calculate tf-idf","metadata":{"id":"gWz5v8gumelV"}},{"cell_type":"code","source":"def count_words(term, docs: list):\n    # count how many times words in search_term appear in a document\n    # docs: list(list(token))\n    cnt = 0\n    for word in term:\n        for token in docs:\n            if word in token:\n                cnt += 1\n    return cnt","metadata":{"id":"6wyh6nGSbIKC","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calIDF(term, docs: list):\n    # calculate inverse document frequency\n    N = len(docs)\n    df = 0\n    \n    for doc in docs:\n        for token in doc:\n            check = False\n            for word in term:\n                if word in token:\n                    check = True\n                    df += 1\n                    break\n            if check:\n                break\n    \n    return math.log(N / (1 + df))\n\n# calIDF(['hello'], [['hello', 'i', 'am', 'good'], ['your', 'dog'], ['my', 'dog', 'say', 'hi']])","metadata":{"id":"aobwU2pqmelV","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cal_tfidf_features(df):\n    # extract number of words, tf, idf and tfidf in text columns\n    len_df = len(df.index)\n\n    cnt_name_value = []\n    cnt_description = []\n    cnt_title = []\n    cnt_all = []\n\n    tf_name_value = []\n    tf_description = []\n    tf_title = []\n    tf_all = []\n    \n    for i in range(len_df):\n        cnt_name_value.append(count_words(df['token_search_term'][i], df['token_name_value'][i]))\n        cnt_description.append(count_words(df['token_search_term'][i], df['token_description'][i]))\n        cnt_title.append(count_words(df['token_search_term'][i], df['token_title'][i]))\n        \n        docs = df['token_name_value'][i] + df['token_description'][i] + df['token_title'][i]\n        cnt_all.append(count_words(df['token_search_term'][i], docs))\n\n\n    df['cnt_name_value'] = cnt_name_value\n    df['cnt_description'] = cnt_description\n    df['cnt_title'] = cnt_title\n    df['cnt_all'] = cnt_all\n\n    df['tf_name_value'] = df['cnt_name_value'] / df['token_len_name_value']\n    df['tf_description'] = df['cnt_description'] / df['token_len_description']\n    df['tf_title'] = df['cnt_title'] / df['token_len_title']\n    df['tf_all'] = df['cnt_all'] / df['token_len_all']\n    \n    idf_all = []\n\n    for i in range(len_df):\n        docs = df['token_name_value'][i] + df['token_description'][i] + df['token_title'][i]\n        idf_all.append(calIDF(df['token_search_term'][i], docs))\n    \n    df['idf_all'] = idf_all\n    df['tf_idf'] = df['tf_all'] * df['idf_all']\n    \n    return df","metadata":{"id":"SjW9bU73melW","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = time.time()\ntrain = cal_tfidf_features(train)\nrun_time = time.time() - start_time\nprint(run_time)","metadata":{"id":"7TP1VfQYmelX","outputId":"921596d3-2dce-4cd9-a4a6-07bd7191daaf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Convert documents into vector and calculate similarity","metadata":{"id":"GpMAQHuU494N"}},{"cell_type":"markdown","source":"Merge the tokens into list of documents","metadata":{"id":"LL0rpfixmelW"}},{"cell_type":"code","source":"def demerge(df):\n    # merge back list of tokens into one string to vectorize strings\n    \n    doc_name_value = []\n    for doc in df['token_name_value']:\n        doc_name_value.append(' '.join(doc))\n    df['doc_name_value'] = doc_name_value\n    \n    doc_description = []\n    for doc in df['token_description']:\n        doc_description.append(' '.join(doc))\n    df['doc_description'] = doc_description\n    \n    doc_title = []\n    for doc in df['token_title']:\n        doc_title.append(' '.join(doc))\n    df['doc_title'] = doc_title\n    \n    doc_search_term = []\n    for doc in df['token_search_term']:\n        doc_search_term.append(' '.join(doc))\n    df['doc_search_term'] = doc_search_term\n    \n    return df","metadata":{"id":"TeHkXst6melW","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Convert documents into vector","metadata":{"id":"w0_UH_tlmelW"}},{"cell_type":"code","source":"tfidfvectorizer = TfidfVectorizer()\ncountvectorizer = CountVectorizer()\n\ndef vectorize(df, vectorizer):\n    # vectorizer: tfidfvectorizer or countvectorizer\n    \n    if vectorizer == tfidfvectorizer:\n        prefix = 't'\n    else:\n        prefix = 'c'\n\n    v_name_value = []\n    v_description = []\n    v_title = []\n    v_search_term = []\n    \n    for i in range(len(df.index)):\n        data = [df['doc_name_value'][i], df['doc_description'][i], df['doc_title'][i], df['doc_search_term'][i]]\n        count_wm = vectorizer.fit_transform(data)\n        vectors = count_wm.toarray()\n        \n        v_name_value.append(vectors[0])\n        v_description.append(vectors[1])\n        v_title.append(vectors[2])\n        v_search_term.append(vectors[3])\n    \n    df[f'{prefix}v_name_value'] = v_name_value\n    df[f'{prefix}v_description'] = v_description\n    df[f'{prefix}v_title'] = v_title\n    df[f'{prefix}v_search_term'] = v_search_term\n    \n    return df\n    \n\ndef cosine_similarity(data1, data2):\n    if all(np.array(data1) == 0) or all(np.array(data2) == 0):\n        return 0\n    return 1 - spatial.distance.cosine(data1, data2)\n\n    \ndef get_cosine_similarity(df, prefix = 't'):\n    \"\"\"\n    prefix = t: tfidf\n    prefix = c: count\n    \"\"\"\n    cosine_sl_name_value = []\n    cosine_sl_description = []\n    cosine_sl_title = []\n    \n    for i in range(len(df.index)):\n        # cosine similarity\n        cosine_sl_name_value.append(cosine_similarity(df[f'{prefix}v_name_value'][i], df[f'{prefix}v_search_term'][i]))\n        cosine_sl_description.append(cosine_similarity(df[f'{prefix}v_description'][i], df[f'{prefix}v_search_term'][i]))\n        cosine_sl_title.append(cosine_similarity(df[f'{prefix}v_title'][i], df[f'{prefix}v_search_term'][i]))\n\n    df[f'{prefix}cosine_sl_name_value'] = cosine_sl_name_value\n    df[f'{prefix}cosine_sl_description'] = cosine_sl_description\n    df[f'{prefix}cosine_sl_title'] = cosine_sl_title\n    \n    return df\n\n\ndef get_jaccard_similarity(df):\n    jaccard_sl_name_value = []\n    jaccard_sl_description = []\n    jaccard_sl_title = []\n\n    for i in range(len(df.index)):\n        # jaccard similarity\n        jaccard_sl_name_value.append(textdistance.jaccard.normalized_similarity(df['doc_search_term'][i], df['doc_name_value'][i]))\n        jaccard_sl_description.append(textdistance.jaccard.normalized_similarity(df['doc_search_term'][i], df['doc_description'][i]))\n        jaccard_sl_title.append(textdistance.jaccard.normalized_similarity(df['doc_search_term'][i], df['doc_title'][i]))\n\n    df['jaccard_sl_name_value'] = jaccard_sl_name_value\n    df['jaccard_sl_description'] = jaccard_sl_description\n    df['jaccard_sl_title'] = jaccard_sl_title\n\n    return df\n\n\ndef get_hamming_similarity(df):\n    hamming_sl_name_value = []\n    hamming_sl_description = []\n    hamming_sl_title = []\n\n    for i in range(len(df.index)):\n        # hamming similarity\n        hamming_sl_name_value.append(textdistance.hamming.normalized_similarity(df['doc_search_term'][i], df['doc_name_value'][i]))\n        hamming_sl_description.append(textdistance.hamming.normalized_similarity(df['doc_search_term'][i], df['doc_description'][i]))\n        hamming_sl_title.append(textdistance.hamming.normalized_similarity(df['doc_search_term'][i], df['doc_title'][i]))\n\n    df['hamming_sl_name_value'] = hamming_sl_name_value\n    df['hamming_sl_description'] = hamming_sl_description\n    df['hamming_sl_title'] = hamming_sl_title\n\n    return df","metadata":{"scrolled":true,"id":"cMqtoO2cmelX","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# merge all tokens in name_value, descriptions, title, search_term to vectorize\ntrain = demerge(train)","metadata":{"id":"vNj2KUSEmelX","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = time.time()\ntrain = vectorize(train, tfidfvectorizer)\ntrain = vectorize(train, countvectorizer)\nrun_time = time.time() - start_time\nprint(run_time)","metadata":{"id":"bbqyiN8OmelX","outputId":"412179d3-0da4-4854-d888-30ed65d4fa86","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = get_cosine_similarity(train, prefix='t')\ntrain = get_cosine_similarity(train, prefix='c')\ntrain = get_jaccard_similarity(train)\ntrain = get_hamming_similarity(train)","metadata":{"id":"q6akGzuImelY","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Apply to test data set","metadata":{"id":"deg3hp3s5gz1"}},{"cell_type":"markdown","source":"Apply to test set","metadata":{"id":"x8a-Hy-ymelY"}},{"cell_type":"code","source":"start_time = time.time()\n\nquestion_test = preprocess(question_test) #might take some minutes to run\n\nquestion_test = cal_length_text(question_test)\nquestion_test = cal_length_token(question_test)\nquestion_test = cal_tfidf_features(question_test)\nprint('tfidf done')\n\nquestion_test = demerge(question_test)\nprint('demerge done')\n\nquestion_test = vectorize(question_test, tfidfvectorizer)\nprint('tfidfvectorize done')\n\nquestion_test = vectorize(question_test, countvectorizer)\nprint('countvectorize done')\n\nquestion_test = get_cosine_similarity(question_test, prefix='t') # tfidf vectorizer\nquestion_test = get_cosine_similarity(question_test, prefix='c') # count vectorizer\nquestion_test = get_jaccard_similarity(question_test) # jaccard\nquestion_test = get_hamming_similarity(question_test) # hamming\nprint('similarity done')\n\nrun_time = time.time() - start_time\nprint(run_time)","metadata":{"id":"ISwNsQ9WZINH","outputId":"8e49ea0f-673d-400c-bdfe-4543e57cd618","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fit the data to the model","metadata":{"id":"ahk4YanUmelZ"}},{"cell_type":"markdown","source":"## Features","metadata":{"id":"buqeNqv5melZ"}},{"cell_type":"code","source":"features = ['len_name_value', 'len_description', 'len_title', 'len_search_term', 'len_all',\n            'token_len_name_value', 'token_len_description', 'token_len_title', 'token_len_search_term', 'token_len_all',\n            'cnt_name_value', 'cnt_description', 'cnt_title', 'cnt_all', \n            'tf_name_value', 'tf_description', 'tf_title', 'tf_all', 'idf_all', 'tf_idf', \n            'tcosine_sl_name_value', 'tcosine_sl_description', 'tcosine_sl_title', \n            'ccosine_sl_name_value', 'ccosine_sl_description', 'ccosine_sl_title',\n            'jaccard_sl_name_value', 'jaccard_sl_description', 'jaccard_sl_title',\n            'hamming_sl_name_value', 'hamming_sl_description', 'hamming_sl_title']","metadata":{"id":"GF9KkgPNmelZ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[['product_uid', 'relevance'] + features]","metadata":{"id":"Kno08FkM2Lu5","outputId":"b2d155ee-307d-4136-c285-2d02996309aa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split the data to evaluate\nfrom sklearn.model_selection import train_test_split\ny = train['relevance']\nfrom sklearn.metrics import mean_squared_error\n\nX = train[features]\n\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.1, random_state=1)","metadata":{"id":"mrXnX5S6melZ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random forest","metadata":{"id":"JZqalH6hmelZ"}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor, BaggingRegressor\n\nrf_model = RandomForestRegressor(random_state=1, max_depth=6)\nclf = BaggingRegressor(rf_model, random_state=1)\nclf.fit(train_X, train_y)\n\nrf_home_depot_preds = clf.predict(val_X)\nrf_val_mse = mean_squared_error(val_y, rf_home_depot_preds)\nrf_val_rmse = math.sqrt(rf_val_mse)\n\nprint(\"Validation RMSE for Random Forest Model: {}\".format(rf_val_rmse))","metadata":{"id":"L-NGzPW0melZ","outputId":"d98fbc21-a122-46e0-e640-2f0bfbd32da7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Gradient Boosting lightgbm","metadata":{"id":"_1zvZxSFmela"}},{"cell_type":"code","source":"import lightgbm as lgb\nfrom lightgbm import LGBMRegressor\n\nlgb_model = LGBMRegressor()\nlgb_model.fit(train_X, train_y)\n\nlgb_home_depot_preds = lgb_model.predict(val_X)\nlgb_val_mse = mean_squared_error(val_y, lgb_home_depot_preds)\nlgb_val_rmse = math.sqrt(lgb_val_mse)\n\nprint(\"Validation RMSE for Light GBM: {}\".format(lgb_val_rmse))","metadata":{"id":"Jkiuqy0Bmela","outputId":"6202a02d-35d4-4fff-f824-a072060bb7e5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Catboost","metadata":{"id":"T8Yy-BQvFdLQ"}},{"cell_type":"code","source":"import catboost as cb\n\ncb_model = cb.CatBoostRegressor(silent=True)\ncb_model.fit(train_X, train_y)\n\ncb_home_depot_preds = cb_model.predict(val_X)\ncb_val_mse = mean_squared_error(val_y, cb_home_depot_preds)\ncb_val_rmse = math.sqrt(cb_val_mse)\n\nprint(\"Validation RMSE for Cat Boost: {}\".format(cb_val_rmse))","metadata":{"id":"7nTMEtjAFfnk","outputId":"fb931fb8-a7de-432d-d2c1-2bc8250689d4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model evaluation","metadata":{"id":"dB70z3bN5Qey"}},{"cell_type":"markdown","source":"ploting the difference between predicted values and actual values","metadata":{}},{"cell_type":"code","source":"plt.plot(np.sort(np.array(rf_home_depot_preds - val_y)))","metadata":{"id":"L_O2zBB45_bN","outputId":"ddcd49ee-7016-43d6-b880-420bf85b1fd1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(np.sort(np.array(lgb_home_depot_preds - val_y)))","metadata":{"id":"9GD1BDLn9Afw","outputId":"bb45563b-8770-41c0-9b46-b7272f1933c7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cb_diff = np.sort(np.array(cb_home_depot_preds - val_y))\nplt.plot(cb_diff)","metadata":{"id":"YIqmMGrp9Gzt","outputId":"20f1ea81-01fc-4526-d8de-b5639eb6699b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save submission","metadata":{"id":"qjQco8bn6w1r"}},{"cell_type":"code","source":"final_model = cb_model # best result\n\nfinal_model.fit(train[features], train['relevance'])\ntestX = question_test[features]\ntest_predict = final_model.predict(testX)\n\nanswer = pd.DataFrame(data={'id': question_test['id'], 'relevance': test_predict})\nanswer['relevance'] = answer['relevance'].apply(lambda x: 3 if x > 3 else x) # some predictions exceeded 3 like 3.01 or 3.02\n\nanswer.to_csv('submission.csv', index=False)","metadata":{"id":"s5gx5KivmelZ","outputId":"f07851ae-0797-4f7f-da56-27ab4ec84141","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"0anl7tjz5Kgb"},"execution_count":null,"outputs":[]}]}