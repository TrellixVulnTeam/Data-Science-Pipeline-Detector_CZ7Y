{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"directory = \"../input/home-depot-product-search-relevance\"\npickle_directory = \"../input/btl-home-depot-product-search-relevance/\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib\nimport pickle\nimport keras as kr\nfrom keras.preprocessing import text, sequence\nfrom keras import layers, models, optimizers\nfrom keras.layers import *\nimport xgboost\nimport gensim\nfrom time import time\nfrom gensim.models import KeyedVectors\nfrom gensim.utils import simple_preprocess, tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nimport spacy\nimport re\nimport math \nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data exploration","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(directory + \"/train.csv.zip\", encoding = \"ISO-8859-1\", index_col= \"id\")\nprint(train.shape)\ntrain.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(directory + \"/test.csv.zip\", encoding = \"ISO-8859-1\", index_col= \"id\")\nprint(test.shape)\ntest.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"attributes=pd.read_csv(directory + \"/attributes.csv.zip\")\nprint(attributes.shape)\nattributes.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hight_relevance = train.loc[train[\"relevance\"] == 3]\nhight_relevance[['product_title', 'search_term']].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Join table 'product_descriptions' and 'train', 'test'","metadata":{}},{"cell_type":"code","source":"product_descriptions = pd.read_csv('../input/home-depot-product-search-relevance/product_descriptions.csv.zip')\ntrain = pd.merge(train, product_descriptions, on='product_uid')\ntest = pd.merge(test, product_descriptions, on='product_uid')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.iloc[0]['search_term'] + '/' + train.iloc[0]['product_title'] + '/'+ train.iloc[0]['product_description']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plotting","metadata":{}},{"cell_type":"code","source":"distribution = train['relevance'].value_counts().sort_index()\nplt.figure(figsize=(6, 4))\nplt.ylabel('count')\nplt.xlabel('value of relevance')\ndistribution.plot(kind = 'bar')\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"distribution_train = train.search_term.str.split().apply(len).value_counts().sort_index()\ndistribution_test = test.search_term.str.split().apply(len).value_counts().sort_index()\nfig, (ax1, ax2) = plt.subplots(2, sharex=True)\nfig.suptitle('Number of word in search term of train and test set')\n\ndistribution_train.plot(kind = 'bar', sharex = 1, figsize = (7,5), ax = ax1, title = 'train')\ndistribution_test.plot(kind = 'bar', colormap = 'jet', figsize = (7, 5), ax = ax2, title = 'test')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"distribution_train = train.product_description.str.split().apply(len).divmod(10)[0].value_counts().nlargest(40).sort_index()\ndistribution_test = test.product_description.str.split().apply(len).divmod(10)[0].value_counts().nlargest(40).sort_index()\nfig, (ax1, ax2) = plt.subplots(2, sharex=True)\nfig.suptitle('Number of word in product description of train and test set')\n\ndistribution_train.plot(kind = 'bar', sharex = 1, figsize = (10,5), ax = ax1, title = 'train')\ndistribution_test.plot(kind = 'bar', colormap = 'jet', figsize = (10, 5), ax = ax2, title = 'test')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Pre-processing","metadata":{}},{"cell_type":"markdown","source":"Extract brand name and material information from attributes file","metadata":{}},{"cell_type":"code","source":"print(len(train),' ',len(test))\ntest.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"brand_attributes = attributes[attributes.name == \"MFG Brand Name\"][[\"product_uid\", \"value\"]].rename(columns={\"value\": \"brand\"})\nmaterial_attributes = attributes[attributes.name == \"Material\"][[\"product_uid\", \"value\"]].rename(columns={\"value\": \"material\"})\nbrand_attributes.drop_duplicates(subset=['product_uid'], inplace = True)\nmaterial_attributes.drop_duplicates(subset=['product_uid'], inplace = True)\ntrain = pd.merge(train, brand_attributes, on='product_uid',  how='left').fillna('')\ntest = pd.merge(test, brand_attributes, on='product_uid',  how='left').fillna('')\ntrain = pd.merge(train, material_attributes, on='product_uid',  how='left').fillna('')\ntest = pd.merge(test, material_attributes, on='product_uid',  how='left').fillna('')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(train),' ',len(test))\ntest.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Open source : https://gist.github.com/susanli2016/b83d148de7394821509bd5172d2c96d3","metadata":{}},{"cell_type":"markdown","source":"## Processing the description","metadata":{}},{"cell_type":"code","source":"strNum = {'zero': 0, 'one': 1, 'two': 2, 'three': 3, 'four': 4, 'five': 5, 'six': 6, 'seven': 7, 'eight': 8, 'nine': 9}\ndef str_stem(s): \n    if isinstance(s, str):\n        #fix strong.Bullet strongBullet\n        s = re.sub(r\"(\\w)\\.([A-Z])\", r\"\\1 \\2\", s)\n        s = re.sub(r\"([a-z])([A-Z])\", r\"\\1 \\2\", s)\n        s = re.sub(r\"([0-9])( *)\\.( *)([0-9])\", r\"\\1.\\4\", s)\n        #stem unit of measurement\n        s = re.sub(r\"([0-9]+)( *)(inches|inch|in|')\\.?\", r\"\\1in. \", s)\n        s = re.sub(r\"([0-9]+)( *)(foot|feet|ft|'')\\.?\", r\"\\1ft. \", s)\n        s = re.sub(r\"([0-9]+)( *)(pounds|pound|lbs|lb)\\.?\", r\"\\1lb. \", s)\n        s = re.sub(r\"([0-9]+)( *)(square|sq) ?\\.?(feet|foot|ft)\\.?\", r\"\\1sq.ft. \", s)\n        s = re.sub(r\"([0-9]+)( *)(cubic|cu) ?\\.?(feet|foot|ft)\\.?\", r\"\\1cu.ft. \", s)\n        s = re.sub(r\"([0-9]+)( *)(gallons|gallon|gal)\\.?\", r\"\\1gal. \", s)\n        s = re.sub(r\"([0-9]+)( *)(ounces|ounce|oz)\\.?\", r\"\\1oz. \", s)\n        s = re.sub(r\"([0-9]+)( *)(centimeters|cm)\\.?\", r\"\\1cm. \", s)\n        s = re.sub(r\"([0-9]+)( *)(milimeters|mm)\\.?\", r\"\\1mm. \", s)\n        s = re.sub(r\"([0-9]+)( *)(Â°|degrees|degree)\\.?\", r\"\\1 deg. \", s)\n        s = re.sub(r\"([0-9]+)( *)(v|volts|volt)\\.?\", r\"\\1 volt. \", s)\n        s = re.sub(r\"([0-9]+)( *)(wattage|watts|watt)\\.?\", r\"\\1 watt. \", s)\n        s = re.sub(r\"([0-9]+)( *)(amperes|ampere|amps|amp)\\.?\", r\"\\1 amp. \", s)\n        s = re.sub(r\"([0-9]+)( *)(qquart|quart)\\.?\", r\"\\1 qt. \", s)\n        s = re.sub(r\"([0-9]+)( *)(hours|hour|hrs.)\\.?\", r\"\\1 hr \", s)\n        s = re.sub(r\"([0-9]+)( *)(gallons per minute|gallon per minute|gal per minute|gallons/min.|gallons/min)\\.?\", r\"\\1 gal. per min. \", s)\n        s = re.sub(r\"([0-9]+)( *)(gallons per hour|gallon per hour|gal per hour|gallons/hour|gallons/hr)\\.?\", r\"\\1 gal. per hr \", s)\n        # Deal with special characters\n        s = s.replace(\"$\",\" \")\n        s = s.replace(\"?\",\" \")\n        s = s.replace(\"&nbsp;\",\" \")\n        s = s.replace(\"&amp;\",\"&\")\n        s = s.replace(\"&#39;\",\"'\")\n        s = s.replace(\"/>/Agt/>\",\"\")\n        s = s.replace(\"</a<gt/\",\"\")\n        s = s.replace(\"gt/>\",\"\")\n        s = s.replace(\"/>\",\"\")\n        s = s.replace(\"<br\",\"\")\n        s = s.replace(\"<.+?>\",\"\")\n        s = s.replace(\"[ &<>)(_,;:!?\\+^~@#\\$]+\",\" \")\n        s = s.replace(\"'s\\\\b\",\"\")\n        s = s.replace(\"[']+\",\"\")\n        s = s.replace(\"[\\\"]+\",\"\")\n        s = s.replace(\"-\",\" \")\n        s = s.replace(\"+\",\" \")\n        s = s.replace(\"..\",\".\")\n        s = s.replace(\",\",\"\") #could be number / segment later\n        s = s.replace(\"-\",\" \")\n        s = s.replace(\"//\",\"/\")\n        s = s.replace(\"/\",\" \")\n        s = s.replace(\"..\",\".\")\n        s = s.replace(\" / \",\" \")\n        s = s.replace(\" \\\\ \",\" \")\n        s = s.replace(\".\",\" . \")\n        s = s.replace(\"  \",\" \")\n        s = re.sub(r\"(^\\.|/)\", r\"\", s)\n        s = re.sub(r\"(\\.|/)$\", r\"\", s)\n        s = re.sub(r\"([0-9])([a-z])\", r\"\\1 \\2\", s)\n        s = re.sub(r\"([a-z])([0-9])\", r\"\\1 \\2\", s)\n        s = re.sub(r\"([a-z])( *)\\.( *)([a-z])\", r\"\\1 \\4\", s)\n        s = re.sub(r\"([a-z])( *)/( *)([a-z])\", r\"\\1 \\4\", s)\n        s = s.replace(\"*\",\" xbi \")\n        s = s.replace(\" by \",\" xbi \")\n\n        s = (\" \").join([str(strNum[z]) if z in strNum else z for z in s.split(\" \")])\n        s = \" \".join([re.sub('[^A-Za-z0-9-./]', ' ', word) for word in s.lower().split()])\n        return s\n    else:\n        return \"null\"","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Spelling correctors","metadata":{}},{"cell_type":"code","source":"#TODO : test SymSpell\n!pip install pyspellchecker","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from spellchecker import SpellChecker\nspell = SpellChecker()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Remove stop words, tokenization, lemmatization. ","metadata":{}},{"cell_type":"code","source":"stop_words = set(stopwords.words('english')) \nwordnet_lemmatizer = WordNetLemmatizer()\ngensim.utils.tokenize\ndef preprocessing(doc, correction = 0):\n    words = []\n    doc  = str_stem(doc)\n    for word in doc.split(' '):\n        if word not in stop_words:\n            if correction == 1:\n                if len(spell.unknown([word])):\n                    word = spell.correction(word)\n            word1 = wordnet_lemmatizer.lemmatize(word, pos = \"n\")\n            word2 = wordnet_lemmatizer.lemmatize(word1, pos = \"v\")\n            word3 = wordnet_lemmatizer.lemmatize(word2, pos = (\"a\"))\n            words.append(word3)\n    return ' '.join(words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ntrain.search_term = [preprocessing(i, correction = 1) for i in train.search_term]\ntrain.product_description = [preprocessing(i) for i in train.product_description]\ntrain.brand = [preprocessing(i) for i in train.brand]\ntrain.material = [preprocessing(i) for i in train.material]\ntrain.product_title = [preprocessing(i) for i in train.product_title]\n\ntest.search_term = [preprocessing(i, correction = 1) for i in test.search_term]\ntest.product_description = [preprocessing(i) for i in test.product_description]\ntest.brand = [preprocessing(i) for i in test.brand]\ntest.material = [preprocessing(i) for i in test.material]\ntest.product_title = [preprocessing(i) for i in test.product_title]\n\n# train = pd.read_pickle(pickle_directory + \"proccessed_train\")\n# test = pd.read_pickle(pickle_directory + \"proccessed_test\")\n\ntrain.to_pickle(\"proccessed_train\")\ntest.to_pickle(\"proccessed_test\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature engineering","metadata":{}},{"cell_type":"code","source":"features = pd.concat((train, test), axis=0, ignore_index=True)\nprint(str(len(train)) + ' ' + str(len(test)) + ' ' + str(len(features)))\ndel train\ndel test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features['brand'] = features['brand'].replace(np.nan, '', regex=True)\nfeatures['material'] = features['material'].replace(np.nan, '', regex=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## String matching feature ","metadata":{}},{"cell_type":"code","source":"from fuzzywuzzy import fuzz\n#with description\nfeatures[\"partial_ratio_desc\"] = [fuzz.partial_ratio(features[\"search_term\"][i], features[\"product_description\"][i]) for i in range(0, len(features))]\nfeatures[\"token_set_ratio_desc\"] = [fuzz.token_set_ratio(features[\"search_term\"][i], features[\"product_description\"][i]) for i in range(0, len(features))]\nfeatures[\"token_sort_ratio_desc\"] = [fuzz.token_sort_ratio(features[\"search_term\"][i], features[\"product_description\"][i]) for i in range(0, len(features))]\n#with tittle\nfeatures[\"partial_ratio_tittle\"] = [fuzz.partial_ratio(features[\"search_term\"][i], features[\"product_title\"][i]) for i in range(0, len(features))]\nfeatures[\"token_set_ratio_tittle\"] = [fuzz.token_set_ratio(features[\"search_term\"][i], features[\"product_title\"][i]) for i in range(0, len(features))]\nfeatures[\"token_sort_ratio_tittle\"] = [fuzz.token_sort_ratio(features[\"search_term\"][i], features[\"product_title\"][i]) for i in range(0, len(features))]\n#with brand name\nfeatures[\"partial_ratio_brand\"] = [fuzz.partial_ratio(features[\"search_term\"][i], features[\"brand\"][i]) for i in range(0, len(features))]\nfeatures[\"token_set_ratio_brand\"] = [fuzz.token_set_ratio(features[\"search_term\"][i], features[\"brand\"][i]) for i in range(0, len(features))]\nfeatures[\"token_sort_ratio_brand\"] = [fuzz.token_sort_ratio(features[\"search_term\"][i], features[\"brand\"][i]) for i in range(0, len(features))]\n#with material\nfeatures[\"partial_ratio_material\"] = [fuzz.partial_ratio(features[\"search_term\"][i], features[\"material\"][i]) for i in range(0, len(features))]\nfeatures[\"token_set_ratio_material\"] = [fuzz.token_set_ratio(features[\"search_term\"][i], features[\"material\"][i]) for i in range(0, len(features))]\nfeatures[\"token_sort_ratio_material\"] = [fuzz.token_sort_ratio(features[\"search_term\"][i], features[\"material\"][i]) for i in range(0, len(features))]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## String similarity feature","metadata":{}},{"cell_type":"code","source":"pip install textdistance","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import textdistance\n#with description\nfeatures[\"jaccard_similar_desc\"] = [textdistance.jaccard(features[\"search_term\"][i], features[\"product_description\"][i]) for i in range(0, len(features))]\nfeatures[\"levenshtein_similarz_desc\"] = [textdistance.levenshtein(features[\"search_term\"][i], features[\"product_description\"][i]) for i in range(0, len(features))]\nfeatures[\"mra_similar_desc\"] = [textdistance.mra(features[\"search_term\"][i], features[\"product_description\"][i]) for i in range(0, len(features))]\n#with title\nfeatures[\"jaccard_similar_title\"] = [textdistance.jaccard(features[\"search_term\"][i], features[\"product_title\"][i]) for i in range(0, len(features))]\nfeatures[\"levenshtein_similarz_title\"] = [textdistance.levenshtein(features[\"search_term\"][i], features[\"product_title\"][i]) for i in range(0, len(features))]\nfeatures[\"mra_similar_title\"] = [textdistance.mra(features[\"search_term\"][i], features[\"product_title\"][i]) for i in range(0, len(features))]\n#with brand\nfeatures[\"jaccard_similar_brand\"] = [textdistance.jaccard(features[\"search_term\"][i], features[\"brand\"][i]) for i in range(0, len(features))]\nfeatures[\"levenshtein_similarz_brand\"] = [textdistance.levenshtein(features[\"search_term\"][i], features[\"brand\"][i]) for i in range(0, len(features))]\nfeatures[\"mra_similar_brand\"] = [textdistance.mra(features[\"search_term\"][i], features[\"brand\"][i]) for i in range(0, len(features))]\n#with material\nfeatures[\"jaccard_similar_material\"] = [textdistance.jaccard(features[\"search_term\"][i], features[\"material\"][i]) for i in range(0, len(features))]\nfeatures[\"levenshtein_similarz_material\"] = [textdistance.levenshtein(features[\"search_term\"][i], features[\"material\"][i]) for i in range(0, len(features))]\nfeatures[\"mra_similar_material\"] = [textdistance.mra(features[\"search_term\"][i], features[\"material\"][i]) for i in range(0, len(features))]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Word embedding feature","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import brown\nembed_model = gensim.models.Word2Vec(brown.sents())\nembed_model.save('brown.embedding')\nmodel = gensim.models.Word2Vec.load('brown.embedding')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model.wv.similarity(\"humanity\", \"angel\"), ' ', model.wv.similarity(\"humanity\", \"evil\"))\nprint(model.wv.similarity(\"cat\", \"alien\"), ' ', model.wv.similarity(\"cat\", \"human\"))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndef embeding_similarity_calculator(s, t, i):\n    _sum = 0\n    avg = 0\n    if len(s.split()) == 0 :\n        return 0\n    for s_word in s.split():\n        _max = 0\n        for t_word in t.split():\n            if ((s_word in model.wv) and (t_word in model.wv)):\n                _max = max(_max, model.wv.similarity(s_word, t_word))\n        _sum += _max\n    avg = _sum/ len(s.split())\n    return avg\nfeatures[\"word_ebed_similarity\"] = [embeding_similarity_calculator(features[\"search_term\"][i], features[\"product_description\"][i], i) for i in range(0, len(features))]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TF-IDF similarity measure feature at char level","metadata":{}},{"cell_type":"code","source":"\ntfidf_vect = TfidfVectorizer(analyzer='char_wb', ngram_range = (3,3), max_features = 1500)\ntfidf_des = tfidf_vect.fit_transform(features.product_description).toarray()\ntfidf_search = tfidf_vect.transform(features.search_term).toarray()\n\n# infile = open(pickle_directory + \"tfidf_des\",'rb')\n# tfidf_des = pickle.load(infile)\n# infile = open(pickle_directory + \"tfidf_search\",'rb')\n# tfidf_search = pickle.load(infile)\n\noutfile = open(\"tfidf_des\",'wb')\npickle.dump(tfidf_des, outfile)\noutfile = open(\"tfidf_search\",'wb')\npickle.dump(tfidf_search, outfile)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.spatial import distance\nfeatures[\"tfidf_cosine_distance\"] = [distance.cosine(tfidf_search[i], tfidf_des[i]) for i in range(0, len(tfidf_des))]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# infile = open(\"../input/btl-home-depot-product-search-relevance/features_18_4\",'rb')\n# features = pickle.load(infile)\noutfile = open(\"features_18_4\",'wb')\npickle.dump(features, outfile)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"features = pd.DataFrame(features).fillna(0)\ndf_train = features.iloc[:74067]\ndf_test = features.iloc[74067:]\ny_train = df_train['relevance']\ndf_train = df_train.drop(columns=['product_title','product_description','brand','material','search_term','relevance'])\ndf_test = df_test.drop(columns=['product_title','product_description','brand','material','search_term', 'relevance'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(df_train), ' ',len(df_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Metric to evaluate models : MAE, RMSE","metadata":{}},{"cell_type":"code","source":"df_train = df_train.sort_values('product_uid')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\n\ndef modelEvaluate(model, X_train, y_train, x_val, y_val, label_encode = None):\n    model.fit(X_train, y_train)\n    pred = model.predict(x_val)\n    _pred = pred\n    _y_val = y_val\n    if label_encode is not None:\n        _y_val = label_encode.inverse_transform(_y_val)\n        _pred = label_encode.inverse_transform(_pred)\n    mae, mse = (mean_absolute_error(_pred, _y_val),mean_squared_error(_pred, _y_val))\n    return (mae, mse)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\n\ndef crossValidate(model, label_encoder = None):\n    mae_score = []\n    mse_score = []\n    kf = KFold(n_splits=4)\n    kf.get_n_splits(df_train)\n    for train_index, test_index in kf.split(df_train):\n        X, X_test = df_train.iloc[train_index], df_train.iloc[test_index]\n        y, y_test = y_train.iloc[train_index], y_train.iloc[test_index]\n        (mae, mse) = modelEvaluate(model, X, y, X_test, y_test, label_encode = label_encoder)\n        mae_score.append(mae)\n        mse_score.append(mse)\n    return [sum(mae_score)/len(mae_score), math.sqrt(sum(mse_score)/len(mse_score))]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Regressor","metadata":{}},{"cell_type":"code","source":"%%time\nfrom xgboost import XGBRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import ExtraTreesRegressor, GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn import linear_model\n\nresult = []\nresult.append(('RandomForest',crossValidate(RandomForestRegressor(n_estimators=700, max_depth=6, random_state=42))))\nresult.append(('XGBoost',crossValidate(XGBRegressor(colsample_bytree=0.4,       \n                 learning_rate=0.1,\n                 max_depth=6,\n                 n_estimators=700,                                                                    \n                 reg_alpha=0.075,\n                 reg_lambda=0.045,\n                 subsample=0.6,\n                 seed=42))))\nresult.append(('GradientBoosting',crossValidate(GradientBoostingRegressor(n_estimators=700, max_depth=6, random_state=42))))\nresult.append(('ETR',crossValidate(ExtraTreesRegressor(n_estimators=700, max_depth=5, random_state=42))))\nresult.append(('Lasso',crossValidate(linear_model.Lasso(alpha=0.05))))\nresult.append(('ElasticNet',crossValidate(linear_model.ElasticNet(alpha=0.05))))\nresult.append(('Ridge',crossValidate(linear_model.Ridge(alpha=0.05))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfrom sklearn import linear_model\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.linear_model import BayesianRidge\nestimators = [  ('RandomForest', RandomForestRegressor(n_estimators=700, max_depth=6, random_state=42)),\n                ('XGBoost', XGBRegressor(colsample_bytree=0.4,learning_rate=0.1,max_depth=6,n_estimators=700,reg_alpha=0.075,reg_lambda=0.045,subsample=0.6,seed=42)),                                                                    \n                ('Lasso',linear_model.Lasso(alpha=0.05)),\n                ('ElasticNet',linear_model.ElasticNet(alpha=0.05)),\n                ('Ridge',linear_model.Ridge(alpha=0.05))\n             ]\nstacked_reg = StackingRegressor(\n        estimators=estimators,\n        final_estimator=BayesianRidge()\n    )\nresult.append(('StackedBayesianRidge',crossValidate(stacked_reg)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = []\nmae = []\nrmse = []\nfor _name, _score in result:\n    labels.append(_name)\n    mae.append(_score[0])\n    rmse.append(_score[1])\n\n#Plotting\nx = np.arange(len(labels))  # the label locations\nwidth = 0.3  # the width of the bars\n\nfig, ax = plt.subplots(figsize=(15,6))\nrects1 = ax.bar(x - width/2, mae, width, label='MAE')\nrects2 = ax.bar(x + width/2, rmse, width, label='RMSE')\n\nax.set_ylabel('Scores')\nax.set_title('Scores by models')\nax.set_xticks(x)\nax.set_xticklabels(labels)\nax.legend()\n\nax.bar_label(rects1, padding=3)\nax.bar_label(rects2, padding=3)\n\nfig.tight_layout()\n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Class weight +  classifier","metadata":{}},{"cell_type":"code","source":"%%time\nle = LabelEncoder()\ny_train = pd.Series(data = le.fit_transform(y_train))\ny_train = y_train.replace({1:2, 3:4, 5:6})\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.linear_model import Perceptron\nresult_classifier = []\nresult_classifier.append(('ETR_c',crossValidate(ExtraTreesClassifier(n_estimators=700,max_depth = 6, class_weight = 'balanced_subsample', random_state=42),le)))\nresult_classifier.append(('RF_c',crossValidate(RandomForestClassifier(n_estimators=700,max_depth = 6, class_weight = 'balanced_subsample', random_state=42),le)))\nresult_classifier.append(('PA_c',crossValidate(PassiveAggressiveClassifier(max_iter = 2000, early_stopping = True, class_weight = 'balanced'),le)))\nresult_classifier.append(('Perceptron_c',crossValidate(Perceptron(max_iter = 2000,penalty = 'l2', class_weight = 'balanced', early_stopping = True, random_state=42),le)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.replace({1:2, 3:4, 5:6, 7:8, 9:10, 11:12}).value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = []\nmae = []\nrmse = []\nfor _name, _score in result_classifier:\n    labels.append(_name)\n    mae.append(_score[0])\n    rmse.append(_score[1])\n\n#Plotting\nx = np.arange(len(labels))  # the label locations\nwidth = 0.3  # the width of the bars\n\nfig, ax = plt.subplots(figsize=(15,6))\nrects1 = ax.bar(x - width/2, mae, width, label='MAE')\nrects2 = ax.bar(x + width/2, rmse, width, label='RMSE')\n\nax.set_ylabel('Scores')\nax.set_title('Scores by models')\nax.set_xticks(x)\nax.set_xticklabels(labels)\nax.legend()\n\nax.bar_label(rects1, padding=3)\nax.bar_label(rects2, padding=3)\n\nfig.tight_layout()\n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = features.iloc[:74067]['relevance']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Make submission","metadata":{}},{"cell_type":"code","source":"stacked_reg.fit(df_train, y_train)\npred = stacked_reg.predict(df_test)\npred.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission =pd.read_csv(directory + \"/sample_submission.csv.zip\")\npred = [min(3,i) for i in pred]\npred = [max(1, i) for i in pred]\nsubmission['relevance'] = pred\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature importance","metadata":{}},{"cell_type":"code","source":"rf = RandomForestRegressor(n_estimators=500, max_depth=5, random_state=42)\nrf.fit(df_train, y_train)\nfi = pd.DataFrame({'feature': df_train.columns, 'importance': rf.feature_importances_}).sort_values(by='importance', ascending=False)\nfi = fi.reset_index()\nfi.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}