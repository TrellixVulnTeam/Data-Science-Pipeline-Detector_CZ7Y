{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"import time\nstart_time = time.time()\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\n#from sklearn import pipeline, model_selection\nfrom sklearn import pipeline, grid_search\n#from sklearn.feature_extraction import DictVectorizer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.decomposition import TruncatedSVD\n#from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import mean_squared_error, make_scorer\n#from nltk.metrics import edit_distance\nfrom nltk.stem.porter import *\nstemmer = PorterStemmer()\n#from nltk.stem.snowball import SnowballStemmer #0.003 improvement but takes twice as long as PorterStemmer\n#stemmer = SnowballStemmer('english')\nimport re\n#import enchant\nimport random\nrandom.seed(2016)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"df_train = pd.read_csv('../input/train.csv', encoding=\"ISO-8859-1\")[:1000] #update here\ndf_test = pd.read_csv('../input/test.csv', encoding=\"ISO-8859-1\")[:1000] #update here\ndf_pro_desc = pd.read_csv('../input/product_descriptions.csv')[:1000] #update here\ndf_attr = pd.read_csv('../input/attributes.csv')\ndf_brand = df_attr[df_attr.name == \"MFG Brand Name\"][[\"product_uid\", \"value\"]].rename(columns={\"value\": \"brand\"})\nnum_train = df_train.shape[0]\ndf_all = pd.concat((df_train, df_test), axis=0, ignore_index=True)\ndf_all = pd.merge(df_all, df_pro_desc, how='left', on='product_uid')\ndf_all = pd.merge(df_all, df_brand, how='left', on='product_uid')\nprint(\"--- Files Loaded: %s minutes ---\" % round(((time.time() - start_time)/60),2))"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"stop_w = ['for', 'xbi', 'and', 'in', 'th','on','sku','with','what','from','that','less','er','ing'] #'electr','paint','pipe','light','kitchen','wood','outdoor','door','bathroom'\nstrNum = {'zero':0,'one':1,'two':2,'three':3,'four':4,'five':5,'six':6,'seven':7,'eight':8,'nine':0}\n\ndef str_stem(s): \n    if isinstance(s, str):\n        s = re.sub(r\"(\\w)\\.([A-Z])\", r\"\\1 \\2\", s) #Split words with a.A\n        s = s.lower()\n        s = s.replace(\"  \",\" \")\n        s = s.replace(\",\",\"\") #could be number / segment later\n        s = s.replace(\"$\",\" \")\n        s = s.replace(\"?\",\" \")\n        s = s.replace(\"-\",\" \")\n        s = s.replace(\"//\",\"/\")\n        s = s.replace(\"..\",\".\")\n        s = s.replace(\" / \",\" \")\n        s = s.replace(\" \\\\ \",\" \")\n        s = s.replace(\".\",\" . \")\n        s = re.sub(r\"(^\\.|/)\", r\"\", s)\n        s = re.sub(r\"(\\.|/)$\", r\"\", s)\n        s = re.sub(r\"([0-9])([a-z])\", r\"\\1 \\2\", s)\n        s = re.sub(r\"([a-z])([0-9])\", r\"\\1 \\2\", s)\n        s = s.replace(\" x \",\" xbi \")\n        s = re.sub(r\"([a-z])( *)\\.( *)([a-z])\", r\"\\1 \\4\", s)\n        s = re.sub(r\"([a-z])( *)/( *)([a-z])\", r\"\\1 \\4\", s)\n        s = s.replace(\"*\",\" xbi \")\n        s = s.replace(\" by \",\" xbi \")\n        s = re.sub(r\"([0-9])( *)\\.( *)([0-9])\", r\"\\1.\\4\", s)\n        s = re.sub(r\"([0-9]+)( *)(inches|inch|in|')\\.?\", r\"\\1in. \", s)\n        s = re.sub(r\"([0-9]+)( *)(foot|feet|ft|'')\\.?\", r\"\\1ft. \", s)\n        s = re.sub(r\"([0-9]+)( *)(pounds|pound|lbs|lb)\\.?\", r\"\\1lb. \", s)\n        s = re.sub(r\"([0-9]+)( *)(square|sq) ?\\.?(feet|foot|ft)\\.?\", r\"\\1sq.ft. \", s)\n        s = re.sub(r\"([0-9]+)( *)(cubic|cu) ?\\.?(feet|foot|ft)\\.?\", r\"\\1cu.ft. \", s)\n        s = re.sub(r\"([0-9]+)( *)(gallons|gallon|gal)\\.?\", r\"\\1gal. \", s)\n        s = re.sub(r\"([0-9]+)( *)(ounces|ounce|oz)\\.?\", r\"\\1oz. \", s)\n        s = re.sub(r\"([0-9]+)( *)(centimeters|cm)\\.?\", r\"\\1cm. \", s)\n        s = re.sub(r\"([0-9]+)( *)(milimeters|mm)\\.?\", r\"\\1mm. \", s)\n        s = s.replace(\"Â°\",\" degrees \")\n        s = re.sub(r\"([0-9]+)( *)(degrees|degree)\\.?\", r\"\\1deg. \", s)\n        s = s.replace(\" v \",\" volts \")\n        s = re.sub(r\"([0-9]+)( *)(volts|volt)\\.?\", r\"\\1volt. \", s)\n        s = re.sub(r\"([0-9]+)( *)(watts|watt)\\.?\", r\"\\1watt. \", s)\n        s = re.sub(r\"([0-9]+)( *)(amperes|ampere|amps|amp)\\.?\", r\"\\1amp. \", s)\n        s = s.replace(\"  \",\" \")\n        s = s.replace(\" . \",\" \")\n        #s = (\" \").join([z for z in s.split(\" \") if z not in stop_w])\n        s = (\" \").join([str(strNum[z]) if z in strNum else z for z in s.split(\" \")])\n        s = (\" \").join([stemmer.stem(z) for z in s.split(\" \")])\n        \n        s = s.lower()\n        s = s.replace(\"toliet\",\"toilet\")\n        s = s.replace(\"airconditioner\",\"air conditioner\")\n        s = s.replace(\"vinal\",\"vinyl\")\n        s = s.replace(\"vynal\",\"vinyl\")\n        s = s.replace(\"skill\",\"skil\")\n        s = s.replace(\"snowbl\",\"snow bl\")\n        s = s.replace(\"plexigla\",\"plexi gla\")\n        s = s.replace(\"rustoleum\",\"rust-oleum\")\n        s = s.replace(\"whirpool\",\"whirlpool\")\n        s = s.replace(\"whirlpoolga\", \"whirlpool ga\")\n        s = s.replace(\"whirlpoolstainless\",\"whirlpool stainless\")\n        return s\n    else:\n        return \"null\"\n\ndef seg_words(str1, str2):\n    str2 = str2.lower()\n    str2 = re.sub(\"[^a-z0-9./]\",\" \", str2)\n    str2 = [z for z in set(str2.split()) if len(z)>2]\n    words = str1.lower().split(\" \")\n    s = []\n    for word in words:\n        if len(word)>3:\n            s1 = []\n            s1 += segmentit(word,str2,True)\n            if len(s)>1:\n                s += [z for z in s1 if z not in ['er','ing','s','less'] and len(z)>1]\n            else:\n                s.append(word)\n        else:\n            s.append(word)\n    return (\" \".join(s))\n\ndef segmentit(s, txt_arr, t):\n    st = s\n    r = []\n    for j in range(len(s)):\n        for word in txt_arr:\n            if word == s[:-j]:\n                r.append(s[:-j])\n                #print(s[:-j],s[len(s)-j:])\n                s=s[len(s)-j:]\n                r += segmentit(s, txt_arr, False)\n    if t:\n        i = len((\"\").join(r))\n        if not i==len(st):\n            r.append(st[i:])\n    return r\n\ndef str_common_word(str1, str2):\n    words, cnt = str1.split(), 0\n    for word in words:\n        if str2.find(word)>=0:\n            cnt+=1\n    return cnt\n\ndef str_whole_word(str1, str2, i_):\n    cnt = 0\n    while i_ < len(str2):\n        i_ = str2.find(str1, i_)\n        if i_ == -1:\n            return cnt\n        else:\n            cnt += 1\n            i_ += len(str1)\n    return cnt\n\ndef fmean_squared_error(ground_truth, predictions):\n    fmean_squared_error_ = mean_squared_error(ground_truth, predictions)**0.5\n    return fmean_squared_error_\n\nRMSE  = make_scorer(fmean_squared_error, greater_is_better=False)\n\nclass cust_regression_vals(BaseEstimator, TransformerMixin):\n    def fit(self, x, y=None):\n        return self\n    def transform(self, hd_searches):\n        d_col_drops=['id','relevance','search_term','product_title','product_description','product_info','attr','brand']\n        hd_searches = hd_searches.drop(d_col_drops,axis=1).values\n        return hd_searches\n\nclass cust_txt_col(BaseEstimator, TransformerMixin):\n    def __init__(self, key):\n        self.key = key\n    def fit(self, x, y=None):\n        return self\n    def transform(self, data_dict):\n        return data_dict[self.key].apply(str)\n\n#comment out the lines below use df_all.csv for further grid search testing\n#if adding features consider any drops on the 'cust_regression_vals' class\n#*** would be nice to have a file reuse option or script chaining option on Kaggle Scripts ***\ndf_all['search_term'] = df_all['search_term'].map(lambda x:str_stem(x))\ndf_all['product_title'] = df_all['product_title'].map(lambda x:str_stem(x))\ndf_all['product_description'] = df_all['product_description'].map(lambda x:str_stem(x))\ndf_all['brand'] = df_all['brand'].map(lambda x:str_stem(x))\nprint(\"--- Stemming: %s minutes ---\" % round(((time.time() - start_time)/60),2))\ndf_all['product_info'] = df_all['search_term']+\"\\t\"+df_all['product_title'] +\"\\t\"+df_all['product_description']\nprint(\"--- Prod Info: %s minutes ---\" % round(((time.time() - start_time)/60),2))\ndf_all['len_of_query'] = df_all['search_term'].map(lambda x:len(x.split())).astype(np.int64)\ndf_all['len_of_title'] = df_all['product_title'].map(lambda x:len(x.split())).astype(np.int64)\ndf_all['len_of_description'] = df_all['product_description'].map(lambda x:len(x.split())).astype(np.int64)\ndf_all['len_of_brand'] = df_all['brand'].map(lambda x:len(x.split())).astype(np.int64)\nprint(\"--- Len of: %s minutes ---\" % round(((time.time() - start_time)/60),2))\ndf_all['search_term'] = df_all['product_info'].map(lambda x:seg_words(x.split('\\t')[0],x.split('\\t')[1]))\n#print(\"--- Search Term Segment: %s minutes ---\" % round(((time.time() - start_time)/60),2))\ndf_all['query_in_title'] = df_all['product_info'].map(lambda x:str_whole_word(x.split('\\t')[0],x.split('\\t')[1],0))\ndf_all['query_in_description'] = df_all['product_info'].map(lambda x:str_whole_word(x.split('\\t')[0],x.split('\\t')[2],0))\nprint(\"--- Query In: %s minutes ---\" % round(((time.time() - start_time)/60),2))\ndf_all['query_last_word_in_title'] = df_all['product_info'].map(lambda x:str_common_word(x.split('\\t')[0].split(\" \")[-1],x.split('\\t')[1]))\ndf_all['query_last_word_in_description'] = df_all['product_info'].map(lambda x:str_common_word(x.split('\\t')[0].split(\" \")[-1],x.split('\\t')[2]))\nprint(\"--- Query Last Word In: %s minutes ---\" % round(((time.time() - start_time)/60),2))\ndf_all['word_in_title'] = df_all['product_info'].map(lambda x:str_common_word(x.split('\\t')[0],x.split('\\t')[1]))\ndf_all['word_in_description'] = df_all['product_info'].map(lambda x:str_common_word(x.split('\\t')[0],x.split('\\t')[2]))\ndf_all['ratio_title'] = df_all['word_in_title']/df_all['len_of_query']\ndf_all['ratio_description'] = df_all['word_in_description']/df_all['len_of_query']\ndf_all['attr'] = df_all['search_term']+\"\\t\"+df_all['brand']\ndf_all['word_in_brand'] = df_all['attr'].map(lambda x:str_common_word(x.split('\\t')[0],x.split('\\t')[1]))\ndf_all['ratio_brand'] = df_all['word_in_brand']/df_all['len_of_brand']\ndf_brand = pd.unique(df_all.brand.ravel())\nd={}\ni = 1000\nfor s in df_brand:\n    d[s]=i\n    i+=3\ndf_all['brand_feature'] = df_all['brand'].map(lambda x:d[x])\ndf_all['search_term_feature'] = df_all['search_term'].map(lambda x:len(x))\ndf_all.to_csv('df_all.csv')\n#df_all = pd.read_csv('df_all.csv', encoding=\"ISO-8859-1\", index_col=0)\ndf_train = df_all.iloc[:num_train]\ndf_test = df_all.iloc[num_train:]\nid_test = df_test['id']\ny_train = df_train['relevance'].values\nX_train =df_train[:]\nX_test = df_test[:]\nprint(\"--- Features Set: %s minutes ---\" % round(((time.time() - start_time)/60),2))\n\nrfr = RandomForestRegressor(n_estimators = 500, n_jobs = -1, random_state = 2016, verbose = 1)\ntfidf = TfidfVectorizer(ngram_range=(1, 1), stop_words='english')\ntsvd = TruncatedSVD(n_components=10, random_state = 2016)\nclf = pipeline.Pipeline([\n        ('union', FeatureUnion(\n                    transformer_list = [\n                        ('cst',  cust_regression_vals()),  \n                        ('txt1', pipeline.Pipeline([('s1', cust_txt_col(key='search_term')), ('tfidf1', tfidf), ('tsvd1', tsvd)])),\n                        ('txt2', pipeline.Pipeline([('s2', cust_txt_col(key='product_title')), ('tfidf2', tfidf), ('tsvd2', tsvd)])),\n                        ('txt3', pipeline.Pipeline([('s3', cust_txt_col(key='product_description')), ('tfidf3', tfidf), ('tsvd3', tsvd)])),\n                        ('txt4', pipeline.Pipeline([('s4', cust_txt_col(key='brand')), ('tfidf4', tfidf), ('tsvd4', tsvd)]))\n                        ],\n                    transformer_weights = {\n                        'cst': 1.0,\n                        'txt1': 0.5,\n                        'txt2': 0.25,\n                        'txt3': 0.0,\n                        'txt4': 0.5\n                        },\n                n_jobs = -1\n                )), \n        ('rfr', rfr)])\nparam_grid = {'rfr__max_features': [10], 'rfr__max_depth': [20]}\nmodel = grid_search.GridSearchCV(estimator = clf, param_grid = param_grid, n_jobs = -1, cv = 2, verbose = 20, scoring=RMSE)\nmodel.fit(X_train, y_train)\n\nprint(\"Best parameters found by grid search:\")\nprint(model.best_params_)\nprint(\"Best CV score:\")\nprint(model.best_score_)\nprint(model.best_score_ + 0.47003199274)\n\ny_pred = model.predict(X_test)\npd.DataFrame({\"id\": id_test, \"relevance\": y_pred}).to_csv('submission.csv',index=False)\nprint(\"--- Training & Testing: %s minutes ---\" % round(((time.time() - start_time)/60),2))\n\nfrom sklearn.feature_extraction import text\nimport nltk\n\ndf_outliers = pd.read_csv('df_all.csv', encoding=\"ISO-8859-1\", index_col=0)\n#stop_ = list(text.ENGLISH_STOP_WORDS)\nstop_ = []\nd={}\nfor i in range(len(df_outliers)):\n    s = str(df_outliers['search_term'][i]).lower()\n    #s = s.replace(\"\\n\",\" \")\n    #s = re.sub(\"[^a-z]\",\" \", s)\n    #s = s.replace(\"  \",\" \")\n    a = set(s.split(\" \"))\n    for b_ in a:\n        if b_ not in stop_ and len(b_)>0:\n            if b_ not in d:\n                d[b_] = [1,str_common_word(b_, df_outliers['product_title'][i]),str_common_word(b_, df_outliers['brand'][i]),str_common_word(b_, df_outliers['product_description'][i])]\n            else:\n                d[b_][0] += 1\n                d[b_][1] += str_common_word(b_, df_outliers['product_title'][i])\n                d[b_][2] += str_common_word(b_, df_outliers['brand'][i])\n                d[b_][3] += str_common_word(b_, df_outliers['product_description'][i])\nds2 = pd.DataFrame.from_dict(d,orient='index')\nds2.columns = ['count','in title','in brand','in prod']\nds2 = ds2.sort_values(by=['count'], ascending=[False])\n\nf = open(\"word_review.csv\", \"w\")\nf.write(\"word|count|in title|in brand|in description\\n\")\nfor i in range(len(ds2)):\n    f.write(ds2.index[i] + \"|\" + str(ds2[\"count\"][i]) + \"|\" + str(ds2[\"in title\"][i]) + \"|\" + str(ds2[\"in brand\"][i]) + \"|\" + str(ds2[\"in prod\"][i]) + \"\\n\")\nf.close()\nprint(\"--- Word List Created: %s minutes ---\" % round(((time.time() - start_time)/60),2))"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}