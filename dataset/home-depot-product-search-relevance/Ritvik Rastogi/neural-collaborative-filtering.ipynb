{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install vizard","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-11-22T10:52:57.597976Z","iopub.execute_input":"2021-11-22T10:52:57.598368Z","iopub.status.idle":"2021-11-22T10:53:07.457507Z","shell.execute_reply.started":"2021-11-22T10:52:57.598275Z","shell.execute_reply":"2021-11-22T10:53:07.456583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tqdm.auto import tqdm\n\nimport vizard\n\npd.options.display.max_colwidth = None\nsns.set_style('darkgrid')","metadata":{"execution":{"iopub.status.busy":"2021-11-22T10:53:31.653017Z","iopub.execute_input":"2021-11-22T10:53:31.653378Z","iopub.status.idle":"2021-11-22T10:53:39.525841Z","shell.execute_reply.started":"2021-11-22T10:53:31.653346Z","shell.execute_reply":"2021-11-22T10:53:39.525031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"code","source":"!unzip ../input/home-depot-product-search-relevance/product_descriptions.csv.zip\n!unzip ../input/home-depot-product-search-relevance/train.csv.zip\n!unzip ../input/home-depot-product-search-relevance/test.csv.zip","metadata":{"execution":{"iopub.status.busy":"2021-11-22T10:53:39.527706Z","iopub.execute_input":"2021-11-22T10:53:39.528046Z","iopub.status.idle":"2021-11-22T10:53:43.827608Z","shell.execute_reply.started":"2021-11-22T10:53:39.52801Z","shell.execute_reply":"2021-11-22T10:53:43.826748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"product_description = pd.read_csv('product_descriptions.csv')\nprint(product_description.shape)\nproduct_description.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T10:53:43.831032Z","iopub.execute_input":"2021-11-22T10:53:43.831332Z","iopub.status.idle":"2021-11-22T10:53:44.704215Z","shell.execute_reply.started":"2021-11-22T10:53:43.831296Z","shell.execute_reply":"2021-11-22T10:53:44.703329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dtrain = pd.read_csv('train.csv', encoding='latin-1').merge(product_description, on='product_uid')\nprint(dtrain.shape)\ndtrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T10:53:44.705911Z","iopub.execute_input":"2021-11-22T10:53:44.706259Z","iopub.status.idle":"2021-11-22T10:53:44.881601Z","shell.execute_reply.started":"2021-11-22T10:53:44.706223Z","shell.execute_reply":"2021-11-22T10:53:44.880647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dtest = pd.read_csv('test.csv', encoding='latin-1').merge(product_description, on='product_uid')\nprint(dtest.shape)\ndtest.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T10:53:44.884621Z","iopub.execute_input":"2021-11-22T10:53:44.885014Z","iopub.status.idle":"2021-11-22T10:53:45.170718Z","shell.execute_reply.started":"2021-11-22T10:53:44.884972Z","shell.execute_reply":"2021-11-22T10:53:45.169953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cleaning","metadata":{}},{"cell_type":"code","source":"from tqdm.auto import tqdm\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport re, spacy\n\n\nclass TextCleaner(BaseEstimator, TransformerMixin):\n    \"\"\"A general purpose text cleaning pipeline which utilizes `spacy` and regex to:\n        * lower cases the text\n        * removes urls and emails\n        * removes html css and js\n        * removes stop words\n        * performs lemmatization\n        * removes numbers, punctuations\n        * trims white spaces\n\n    Args:\n        model (str): spacy language model, default: en\n    \"\"\"\n\n    def __init__(self, model=\"en\"):\n        self.nlp = spacy.load(model, disable=[\"parser\", \"ner\"])\n\n    def fit(self, X=None):\n        return self\n\n    def transform(self, X):\n        transformed = []\n        for x in tqdm(X):\n            x = str(x).strip().lower()  # Lower case the data\n            x = re.sub(r\"\"\"((http[s]?://)[^ <>'\"{}|\\^`[\\]]*)\"\"\", r\" \", x)  # remove urls\n            x = re.sub(\n                r\"[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+\", r\" \", x\n            )  # remove emails\n            x = re.sub(r\"<style.*>[\\s\\S]+</style>\", \" \", x)  # remove css\n            x = re.sub(r\"<script.*>[\\s\\S]*</script>\", \" \", x)  # remove js\n            x = re.sub(\n                r\"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});\", \" \", x\n            )  # remove html\n\n            if len(x) != 0:\n                parsed = self.nlp(x)\n                lemmatized = \" \".join([w.lemma_ for w in parsed if not w.is_stop])\n\n                # Remove punct\n                punct_removed = re.sub(r\"\\W\", \" \", str(lemmatized))\n                punct_removed = re.sub(r\"\\d\", \" \", str(punct_removed))\n                punct_removed = re.sub(r\"\\s+\", \" \", str(punct_removed))\n            else:\n                punct_removed = x\n            transformed.append(punct_removed)\n        return transformed\n\n    def fit_transform(self, X):\n        return self.fit(X).transform(X)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T10:53:45.172224Z","iopub.execute_input":"2021-11-22T10:53:45.172504Z","iopub.status.idle":"2021-11-22T10:53:45.699254Z","shell.execute_reply.started":"2021-11-22T10:53:45.172476Z","shell.execute_reply":"2021-11-22T10:53:45.698491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# textcleaner = TextCleaner()\n# all_text = pd.DataFrame({\n#     'raw_text': dtrain['product_title'].values.tolist() + dtrain['search_term'].values.tolist() + \n#     dtest['product_title'].values.tolist() + dtest['search_term'].values.tolist() + \n#     product_description['product_description'].values.tolist()\n# }).drop_duplicates().reset_index(drop=True)\n# all_text['cleaned_text'] = textcleaner.fit_transform(all_text['raw_text'])\n\n# all_text.to_csv('home-depot-product-search-relevance.csv', index=False)\n\ntext_map = pd.read_csv('../input/qqp-cleaned/home-depot-product-search-relevance.csv')\ntext_map = {x:y for x, y in zip(text_map['raw_text'].values, text_map['cleaned_text'].values)}\n\ndtrain['product_title'] = dtrain['product_title'].apply(lambda x: text_map[x])\ndtrain['product_description'] = dtrain['product_description'].apply(lambda x: text_map[x])\ndtrain['search_term'] = dtrain['search_term'].apply(lambda x: text_map[x])\ndtest['product_title']  =  dtest['product_title'].apply(lambda x: text_map[x])\ndtest['product_description']  =  dtest['product_description'].apply(lambda x: text_map[x])\ndtest['search_term']  =  dtest['search_term'].apply(lambda x: text_map[x])\n\ndel text_map","metadata":{"execution":{"iopub.status.busy":"2021-11-22T10:53:45.700805Z","iopub.execute_input":"2021-11-22T10:53:45.701195Z","iopub.status.idle":"2021-11-22T10:53:49.567513Z","shell.execute_reply.started":"2021-11-22T10:53:45.701157Z","shell.execute_reply":"2021-11-22T10:53:49.566678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Vectorization","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntitle_vectorizer = TfidfVectorizer(sublinear_tf=True).fit(\n    pd.concat([dtrain['product_title'], dtest['product_title']], axis=0).drop_duplicates().dropna()\n)\ndescription_vectorizer = TfidfVectorizer(sublinear_tf=True).fit(\n    pd.concat([dtrain['product_description'], dtest['product_description']], axis=0).drop_duplicates().dropna()\n)\nsearch_vectorizer = TfidfVectorizer(sublinear_tf=True).fit(\n    pd.concat([dtrain['search_term'], dtest['search_term']], axis=0).drop_duplicates().dropna()\n)\n\n\ntrain_title = title_vectorizer.transform(dtrain['product_title'].fillna(''))\ntest_title = title_vectorizer.transform(dtest['product_title'].fillna(''))\ntrain_description = description_vectorizer.transform(dtrain['product_description'].fillna(''))\ntest_description = description_vectorizer.transform(dtest['product_description'].fillna(''))\ntrain_search = search_vectorizer.transform(dtrain['search_term'].fillna(''))\ntest_search = search_vectorizer.transform(dtest['search_term'].fillna(''))","metadata":{"execution":{"iopub.status.busy":"2021-11-22T10:53:49.568877Z","iopub.execute_input":"2021-11-22T10:53:49.569239Z","iopub.status.idle":"2021-11-22T10:54:27.957766Z","shell.execute_reply.started":"2021-11-22T10:53:49.569204Z","shell.execute_reply":"2021-11-22T10:54:27.956833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n\nclass ConvertToSparseTensor(BaseEstimator, TransformerMixin):\n    \"\"\"A utiliy to convert sparse vectors into sparse tensors\"\"\"\n\n    def __init__(self):\n        pass\n\n    def fit(self, X):\n        return self\n\n    def transform(self, X):\n        coo = X.tocoo()\n        indices = np.mat([coo.row, coo.col]).transpose()\n        return tf.sparse.reorder(tf.SparseTensor(indices, coo.data, coo.shape))\n","metadata":{"execution":{"iopub.status.busy":"2021-11-22T10:54:27.959548Z","iopub.execute_input":"2021-11-22T10:54:27.959887Z","iopub.status.idle":"2021-11-22T10:54:27.967671Z","shell.execute_reply.started":"2021-11-22T10:54:27.95985Z","shell.execute_reply":"2021-11-22T10:54:27.96674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cvt_to_tensors = ConvertToSparseTensor().fit(None)\ntrain_title = cvt_to_tensors.transform(train_title)\ntest_title = cvt_to_tensors.transform(test_title)\ntrain_description = cvt_to_tensors.transform(train_description)\ntest_description = cvt_to_tensors.transform(test_description)\ntrain_search = cvt_to_tensors.transform(train_search)\ntest_search = cvt_to_tensors.transform(test_search)\n\ny_train = dtrain['relevance'].values","metadata":{"execution":{"iopub.status.busy":"2021-11-22T10:54:28.294639Z","iopub.execute_input":"2021-11-22T10:54:28.295035Z","iopub.status.idle":"2021-11-22T10:54:32.533003Z","shell.execute_reply.started":"2021-11-22T10:54:28.294997Z","shell.execute_reply":"2021-11-22T10:54:32.532183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling","metadata":{}},{"cell_type":"code","source":"title_input = tf.keras.Input(shape=train_title.shape[1], name='title')\ndescription_input = tf.keras.Input(shape=train_description.shape[1], name='description')\nsearch_input = tf.keras.Input(shape=train_search.shape[1], name='search')\n\n\ntitle_x = tf.keras.layers.Dense(128, activation='relu')(title_input)\ntitle_x = tf.keras.layers.Dropout(0.3)(title_x)\ntitle_x = tf.keras.layers.BatchNormalization()(title_x)\ntitle_x = tf.keras.layers.Dense(32, activation='relu')(title_x)\ntitle_x = tf.keras.layers.Dropout(0.3)(title_x)\ntitle_x = tf.keras.layers.BatchNormalization()(title_x)\n\ndescription_x = tf.keras.layers.Dense(128, activation='relu')(description_input)\ndescription_x = tf.keras.layers.Dropout(0.3)(description_x)\ndescription_x = tf.keras.layers.BatchNormalization()(description_x)\ndescription_x = tf.keras.layers.Dense(32, activation='relu')(description_x)\ndescription_x = tf.keras.layers.Dropout(0.3)(description_x)\ndescription_x = tf.keras.layers.BatchNormalization()(description_x)\n\nsearch_x = tf.keras.layers.Dense(128, activation='relu')(search_input)\nsearch_x = tf.keras.layers.Dropout(0.3)(search_x)\nsearch_x = tf.keras.layers.BatchNormalization()(search_x)\nsearch_x = tf.keras.layers.Dense(32, activation='relu')(search_x)\nsearch_x = tf.keras.layers.Dropout(0.3)(search_x)\nsearch_x = tf.keras.layers.BatchNormalization()(search_x)\n\nx = tf.keras.layers.concatenate([title_x, description_x, search_x])\nx = tf.keras.layers.Dense(64, activation='relu')(x)\nx = tf.keras.layers.Dropout(0.3)(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.Dense(32, activation='relu')(x)\nx = tf.keras.layers.Dropout(0.3)(x)\nx = tf.keras.layers.BatchNormalization()(x)\n\noutput_layer = tf.keras.layers.Dense(1)(x)\n\nmodel = tf.keras.models.Model([title_input, description_input, search_input], output_layer)\nmodel.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(lr=1e-4))\ntf.keras.utils.plot_model(model, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T10:54:37.933628Z","iopub.execute_input":"2021-11-22T10:54:37.933991Z","iopub.status.idle":"2021-11-22T10:54:39.130683Z","shell.execute_reply.started":"2021-11-22T10:54:37.933954Z","shell.execute_reply":"2021-11-22T10:54:39.129666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T10:54:39.762109Z","iopub.execute_input":"2021-11-22T10:54:39.762454Z","iopub.status.idle":"2021-11-22T10:54:39.780446Z","shell.execute_reply.started":"2021-11-22T10:54:39.762419Z","shell.execute_reply":"2021-11-22T10:54:39.779729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"es = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, verbose=1, restore_best_weights=True)\nrlp = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', patience=2, verbose=1)\n\nhistory = model.fit(\n    [train_title, train_description, train_search], y_train, callbacks=[es, rlp], epochs=250, batch_size=64\n)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T10:54:45.100849Z","iopub.execute_input":"2021-11-22T10:54:45.101252Z","iopub.status.idle":"2021-11-22T11:15:23.156667Z","shell.execute_reply.started":"2021-11-22T10:54:45.101216Z","shell.execute_reply":"2021-11-22T11:15:23.155979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20,6))\npd.DataFrame(history.history)[['loss']].plot(ax=ax);","metadata":{"execution":{"iopub.status.busy":"2021-11-22T11:15:23.159226Z","iopub.execute_input":"2021-11-22T11:15:23.159548Z","iopub.status.idle":"2021-11-22T11:15:23.51463Z","shell.execute_reply.started":"2021-11-22T11:15:23.159518Z","shell.execute_reply":"2021-11-22T11:15:23.513797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame({\n    'id': dtest.id.values,\n    'relevance': np.clip(np.ravel(model.predict([test_title, test_description, test_search])), 1, 3)\n}).to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T11:15:23.515872Z","iopub.execute_input":"2021-11-22T11:15:23.51642Z","iopub.status.idle":"2021-11-22T11:15:36.399076Z","shell.execute_reply.started":"2021-11-22T11:15:23.516381Z","shell.execute_reply":"2021-11-22T11:15:36.398224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reference\n[Neural Collaborative Filtering](https://arxiv.org/pdf/1708.05031.pdf)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}