{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is a B1 version of [Alex Shonenkov](https://www.kaggle.com/shonenkov)'s best starter kernel whcih can be found here: https://www.kaggle.com/shonenkov/train-inference-gpu-baseline","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install -q efficientnet_pytorch > /dev/null\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from glob import glob\nfrom sklearn.model_selection import GroupKFold\nimport cv2\nfrom skimage import io\nimport torch\nfrom torch import nn\nimport os\nfrom datetime import datetime\nimport time\nimport random\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport albumentations as A\nimport matplotlib.pyplot as plt\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nimport sklearn\n\nSEED = 42\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from efficientnet_pytorch import EfficientNet\n\ndef get_net():\n    net = EfficientNet.from_pretrained('efficientnet-b1')\n    net._fc = nn.Linear(in_features=1280, out_features=4, bias=True)\n    return net\n\nnet = get_net().cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = torch.load('../input/eb1-weights/best-checkpoint-045epoch_dell.bin')\nnet.load_state_dict(checkpoint['model_state_dict']);\nnet.eval();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Testing Time Augmentation\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_test_transforms(mode):\n    if mode == 0:\n        return A.Compose([\n                A.Resize(height=512, width=512, p=1.0),\n                ToTensorV2(p=1.0),\n            ], p=1.0)\n    elif mode == 1:\n        return A.Compose([\n                A.HorizontalFlip(p=1),\n                A.Resize(height=512, width=512, p=1.0),\n                ToTensorV2(p=1.0),\n            ], p=1.0)    \n    elif mode == 2:\n        return A.Compose([\n                A.VerticalFlip(p=1),\n                A.Resize(height=512, width=512, p=1.0),\n                ToTensorV2(p=1.0),\n            ], p=1.0)\n    else:\n        return A.Compose([\n                A.HorizontalFlip(p=1),\n                A.VerticalFlip(p=1),\n                A.Resize(height=512, width=512, p=1.0),\n                ToTensorV2(p=1.0),\n            ], p=1.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DatasetSubmissionRetriever(Dataset):\n\n    def __init__(self, image_names, transforms=None):\n        super().__init__()\n        self.image_names = image_names\n        self.transforms = transforms\n\n    def __getitem__(self, index: int):\n        image_name = self.image_names[index]\n        image = cv2.imread(f'{DATA_ROOT_PATH}/Test/{image_name}', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image /= 255.0\n        if self.transforms:\n            sample = {'image': image}\n            sample = self.transforms(**sample)\n            image = sample['image']\n\n        return image_name, image\n\n    def __len__(self) -> int:\n        return self.image_names.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_ROOT_PATH = '../input/alaska2-image-steganalysis'\n\n\nresults = []\nfor mode in range(0, 4):\n    dataset = DatasetSubmissionRetriever(\n        image_names=np.array([path.split('/')[-1] for path in glob('../input/alaska2-image-steganalysis/Test/*.jpg')]),\n        transforms=get_test_transforms(mode),\n    )\n\n\n    data_loader = DataLoader(\n        dataset,\n        batch_size=8,\n        shuffle=False,\n        num_workers=2,\n        drop_last=False,\n    )\n    \n    result = {'Id': [], 'Label': []}\n    for step, (image_names, images) in enumerate(data_loader):\n        print(step, end='\\r')\n\n        y_pred = net(images.cuda())\n        y_pred = 1 - nn.functional.softmax(y_pred, dim=1).data.cpu().numpy()[:,0]\n\n        result['Id'].extend(image_names)\n        result['Label'].extend(y_pred)\n        \n    results.append(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = net(images.cuda())\ny_pred = 1 - nn.functional.softmax(y_pred, dim=1).data.cpu().numpy()[:,0]\n\nresult['Id'].extend(image_names)\nresult['Label'].extend(y_pred)\n\nsubmissions = []\nfor mode in range(0,4):\n    submission = pd.DataFrame(results[mode])\n    submissions.append(submission)\n    \nfor mode in range(0,4):\n    submissions[mode].to_csv(f'submission_{mode}.csv', index=False)\n    \nsubmissions[0]['Label'] = (submissions[0]['Label']*3 + submissions[1]['Label']*1 + submissions[2]['Label']*1 + submissions[3]['Label']*1) / 6\nsubmissions[0].to_csv(f'submission_B1_c.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}