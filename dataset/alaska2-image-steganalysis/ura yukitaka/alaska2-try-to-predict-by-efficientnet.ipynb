{"cells":[{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Alaska2,Try to predict by EfficientNet","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"As my first try for the ALSKA2 competition, I made predictions using tensorflow efficient net B7 model.<br>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install -q efficientnet","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Basic library\nimport numpy as np \nimport pandas as pd \nimport os\n\n# Data preprocessing\nfrom sklearn.model_selection import train_test_split\n\n# Visualization\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\n\n# tensorflow\nimport tensorflow as tf\nimport tensorflow.keras.layers as l\nimport efficientnet.tfkeras as efn\n\n# data set\nfrom kaggle_datasets import KaggleDatasets","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"TPU Setting","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# TPU setting\n# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tpu.master())\nprint(tpu_strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For tensorflow dataset\nAUTO = tf.data.experimental.AUTOTUNE\nignore_order = tf.data.Options()\nignore_order.experimental_deterministic = False\n\n# Pass\ngcs_path = KaggleDatasets().get_gcs_path()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data set loadng","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sample dataframe\nsample = pd.read_csv(\"/kaggle/input/alaska2-image-steganalysis/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# batch size in tpu\nBATCH_SIZE = 32 * tpu_strategy.num_replicas_in_sync","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Directrory and file name\n# Drop csv from dir_name\ndir_name = ['Test', 'JUNIWARD', 'JMiPOD', 'Cover', 'UERD']\ndf = pd.DataFrame({})\n\n# Create empty dataframe and list\nlists = []\ncate = []\n\n# get the filenames\nfor dir_ in dir_name:\n    # file name\n    list_ = os.listdir(\"/kaggle/input/alaska2-image-steganalysis/\"+dir_+\"/\")\n    lists = lists+list_\n    # category name\n    cate_ = np.tile(dir_,len(list_))\n    cate = np.concatenate([cate,cate_])\n    \n# insert dataframe\ndf[\"cate\"] = cate\ndf[\"name\"] = lists","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data and path preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# path line\ndf[\"path\"] = [str(os.path.join(gcs_path,cate,name)) for cate, name in zip(df[\"cate\"], df[\"name\"])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Labeling positive and negative\ndef cate_label(x):\n    if x[\"cate\"] == \"Cover\":\n        res = 0\n    else:\n        res = 1\n    return res\n\n# Test dataframe and Train dataframe\nTest_df = df.query(\"cate=='Test'\").sort_values(by=\"name\")\nTrain_df = df.query(\"cate!='Test'\")\n# Apply the function\nTrain_df[\"flg\"] = df.apply(cate_label, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# label_counts\nTrain_df[\"cate\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since I need to keeping memory and running time over, the number of samples was smalled 60000 data wset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_df = Train_df.sample(80000)\n# label_counts\nTrain_df[\"cate\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create train data and val data\nX = Train_df[\"path\"]\ny = Train_df[\"flg\"]\n\n# split train and val data\nX_train, X_val, y_train, y_val = train_test_split(X,y, test_size=0.2, random_state=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Change to numpy array\nX_train, X_val, y_train, y_val = np.array(X_train), np.array(X_val), np.array(y_train), np.array(y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create test data\nX_test = np.array(Test_df[\"path\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def decode_image(filename, label=None, image_size=(512,512)):\n    bits = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(bits, channels=3)\n    image = tf.cast(image, tf.float32)/255.0\n    image = tf.image.resize(image, image_size)\n    \n    if label is None:\n        return image\n    else:\n        return image, label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\nignore_order = tf.data.Options()\nignore_order.experimental_deterministic = False\n\n# Build input pipeline\ntrain_dataset = (tf.data.Dataset.from_tensor_slices((X_train, y_train)).prefetch(AUTO).with_options(ignore_order)\n                 .map(decode_image, num_parallel_calls=AUTO).shuffle(512).batch(BATCH_SIZE).repeat())\n\nvalid_dataset = (tf.data.Dataset.from_tensor_slices((X_val, y_val)).map(decode_image, num_parallel_calls=AUTO)\n                    .cache().batch(BATCH_SIZE).prefetch(AUTO))\n\ntest_dataset = (tf.data.Dataset.from_tensor_slices((X_test)).map(decode_image, num_parallel_calls=AUTO)\n                    .batch(BATCH_SIZE))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Model : EfficientNetB7<br>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"with tpu_strategy.scope():\n    model_b7 = tf.keras.Sequential([\n        efn.EfficientNetB7(input_shape=(512,512,3),weights='imagenet',include_top=False),\n        l.GlobalAveragePooling2D(),\n        l.Dense(1, activation=\"sigmoid\")\n    ])\n    \n    model_b7.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n    model_b7.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Calculation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"STEPS_PER_EPOCH = X_train.shape[0] // BATCH_SIZE\ncallbacks = [tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)]\n\nEPOCHS = 5\nhist_b7 = model_b7.fit(train_dataset, epochs=EPOCHS,\n                   steps_per_epoch=STEPS_PER_EPOCH, validation_data=valid_dataset, callbacks=callbacks, workers=4, use_multiprocessing=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prediction\npred_b7 = model_b7.predict(test_dataset, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# training history\ntrain_loss = hist_b7.history[\"loss\"]\nval_loss = hist_b7.history[\"val_loss\"]\ntrain_acc = hist_b7.history[\"accuracy\"]\nval_acc = hist_b7.history[\"val_accuracy\"]\n\nfig, ax = plt.subplots(1,2,figsize=(10,6))\nax[0].plot(range(len(train_loss)), train_loss, label=\"train_loss\")\nax[0].plot(range(len(val_loss)), val_loss, label=\"val_loss\")\nax[0].set_xlabel(\"epochs\")\nax[0].set_ylabel(\"loss\")\nax[0].set_title(\"EfficientNetB7 loss\")\nax[0].legend()\n\nax[1].plot(range(len(train_acc)), train_acc, label=\"train_accuracy\")\nax[1].plot(range(len(val_acc)), val_acc, label=\"val_accuracy\")\nax[1].set_xlabel(\"epochs\")\nax[1].set_ylabel(\"accuracy\")\nax[1].set_title(\"EfficientNetB7 accurary\")\nax[1].legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# EfficientNetB7\nsample_7 = sample.copy()\nsample_7[\"Label\"] = pred_b7\nsample_7.to_csv(\"submission_B7.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_7[\"Label\"].describe()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}