{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install -q efficientnet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install transformers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Load Dependencies","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\nimport tensorflow.keras.layers as l\nfrom keras.regularizers import l2\nfrom keras.optimizers import Adam\nimport efficientnet.tfkeras as efn\nfrom sklearn.model_selection import train_test_split\nfrom kaggle_datasets import KaggleDatasets\n\n\nfrom transformers import BertForSequenceClassification, AdamW, BertConfig\nfrom transformers import BertTokenizer\nfrom transformers import get_linear_schedule_with_warmup","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**TPU Setting:**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\nprint(tpu.master())\nprint(tpu_strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data access**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# For tensorflow dataset\nAUTO = tf.data.experimental.AUTOTUNE\nignore_order = tf.data.Options()\nignore_order.experimental_deterministic = False\n\n# Pass\ngcs_path = KaggleDatasets().get_gcs_path('alaska2-image-steganalysis')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Paths & Hyper-Parameters & filenames","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = pd.read_csv(\"/kaggle/input/alaska2-image-steganalysis/sample_submission.csv\")\nBATCH_SIZE = 32 * tpu_strategy.num_replicas_in_sync # batch size in tpu\nEPOCHS = 2\n\n#Variables\n\ndir_name = ['Test', 'JUNIWARD', 'JMiPOD', 'Cover', 'UERD']\ndf = pd.DataFrame({})\nlists = []\ncate = []\n\n#get filenames\nfor dir_ in dir_name:\n    # file name\n    list_ = os.listdir(\"/kaggle/input/alaska2-image-steganalysis/\"+dir_+\"/\")\n    lists = lists+list_\n    # category name\n    cate_ = np.tile(dir_,len(list_))\n    cate = np.concatenate([cate,cate_])\n    \n#to dataframe\ndf[\"cate\"] = cate\ndf[\"name\"] = lists","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data & path preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#add path to df\ndf[\"path\"] = [str(os.path.join(gcs_path,cate,name)) for cate, name in zip(df[\"cate\"], df[\"name\"])]\n\n#Labeling func\ndef cate_label(x):\n    if x[\"cate\"] == \"Cover\":\n        res = 0\n    else:\n        res = 1\n    return res\n\n#Training & test sets\nTest_df = df.query(\"cate=='Test'\").sort_values(by=\"name\")\nTrain_df = df.query(\"cate!='Test'\")\n\n#apply Labeling func\nTrain_df[\"labled\"] = df.apply(cate_label, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training set: \\n\",Train_df[\"cate\"].value_counts())\n\nprint('\\n', Train_df[\"path\"].head())\nprint('\\n',Train_df[\"labled\"].head())\nprint('\\n',Test_df[\"path\"].head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Validating set\n\n\nX = Train_df[\"path\"]\ny = Train_df[\"labled\"]\nz = Test_df[\"path\"]\n\n#Train & test split\nX_train, X_val, y_train, y_val = train_test_split(X,y, test_size=0.2, random_state=10)\n\n#convert to numpy array\nX_train, X_val, y_train, y_val = np.array(X_train), np.array(X_val), np.array(y_train), np.array(y_val)\n\n#test set\nX_test = np.array(Test_df[\"path\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_test[7])\nprint(X_train[7])\nprint(X_val[7])\nprint(y_train[7])\nprint(y_val[7])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dataset objects","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def decode_image(filename, label=None, image_size=(512,512)):\n    bits = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(bits, channels=3)\n    image = tf.cast(image, tf.float32)/255.0\n    image = tf.image.resize(image, image_size)\n    \n    if label is None:\n        return image\n    else:\n        return image, label","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Building the inputs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((X_train, y_train))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .repeat()\n    .shuffle(1024)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((X_val, y_val))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(X_test)\n    .map(decode_image, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Building the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"with tpu_strategy.scope():\n    model = tf.keras.Sequential([\n        efn.EfficientNetB3(\n            input_shape=(512, 512, 3),\n            weights='imagenet',\n            include_top=False\n        ),\n        #l.Dense(32, activation=\"relu\",kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001)),\n        #l.Dropout(0.4),\n       # l.BatchNormalization(),\n        l.GlobalAveragePooling2D(),\n        #l.BatchNormalization(),\n        #l.Activation('relu'),\n        l.Dropout(0.1),\n       # l.Dense(1),\n        l.Dense(1, activation='sigmoid')\n    ])\n    opt = Adam(lr=0.002, beta_1=0.9, beta_2=0.999, decay=0.01, amsgrad=False)    \n    model.compile(\n        optimizer=opt,\n        loss = 'binary_crossentropy',\n        metrics=['accuracy']\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"STEPS_PER_EPOCH = X_train.shape[0] // BATCH_SIZE\n#callbacks = [tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)] \n\nhistory = model.fit(train_dataset, steps_per_epoch=STEPS_PER_EPOCH, epochs=2, validation_data=valid_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save(\"Mymodel.h5\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plt.clf()\n#history_dict = history.history\n#loss_values = history_dict['loss']\n#val_loss_values = history_dict['val_loss']\n#epochs = range(1, (len(history_dict['loss']) + 1))\n#plt.plot(epochs, loss_values, 'bo', label='Training loss')\n#plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n#plt.title('Training and validation loss')\n#plt.xlabel('Epochs')\n#plt.ylabel('Loss')\n#plt.legend()\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plt.clf()\n#acc_values = history_dict['accuracy']\n#val_acc_values = history_dict['val_accuracy']\n#epochs = range(1, (len(history_dict['accuracy']) + 1))\n#plt.plot(epochs, acc_values, 'bo', label='Training acc')\n#plt.plot(epochs, val_acc_values, 'b', label='Validation acc')\n#plt.title('Training and validation accuracy')\n#plt.xlabel('Epochs')\n#plt.ylabel('Accuracy')\n#plt.legend()\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict(test_dataset, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_sample = sample.copy()\nmy_sample[\"Label\"] = pred\nmy_sample.to_csv(\"my_sample.csv\", index=False)\nmy_sample.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}