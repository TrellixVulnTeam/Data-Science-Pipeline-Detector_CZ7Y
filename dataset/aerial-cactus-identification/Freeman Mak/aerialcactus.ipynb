{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Aerial Cactus Identification\n\nThis notebook will document all of the steps I did for this kaggle competition. The goal is to create a classifier capable of predicting whether an image contains a cactus."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport cv2\nimport matplotlib.pyplot as plt\nimport os\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten, BatchNormalization\nfrom keras import regularizers\n\nfrom sklearn.utils import shuffle\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# First Look at the Data\nThe dataset contains a large number of thumbnail images of size 32x32. The labels for the pictures are provided within the **'train.csv'** file. Firstly, we will take a look at a couple sample images and also load the csv file into a pandas dataframe."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#Load CSV file into pandas\ntrain_df = pd.read_csv('../input/train.csv')\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Prepping Data...\")\nimage_directory = '../input/train/train/'\n\n#Loading images and labels\nX_train = [cv2.imread(image_directory + filename) for filename in os.listdir(image_directory)]\ny_train = [train_df[train_df['id'] == filename].has_cactus.values for filename in os.listdir(image_directory)]\n\nX_train = np.array(X_train)\ny_train = np.array(y_train)\ny_train = y_train.flatten()\n\nX_train, y_train = shuffle(X_train, y_train, random_state = 2019)\n\nprint(\"Complete!\")\nprint(\"X_train shape = {}\".format(X_train.shape))\nprint(\"y_train shape = {}\".format(y_train.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Show some random images\nlabels = ['No Cactus', 'Yes Cactus']\n\nplt.figure(figsize=(10,10))\nplt.subplot(1,3,1)\nplt.title(labels[y_train[50]])\nplt.imshow(X_train[50], interpolation='bilinear')\n\nplt.subplot(1,3,2)\nplt.title(labels[y_train[57]])\nplt.imshow(X_train[57], interpolation='bilinear')\n\nplt.subplot(1,3,3)\nplt.title(labels[y_train[60]])\nplt.imshow(X_train[60], interpolation='bilinear')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So just by looking at the images, it is pretty hard to tell if there is a cactus or not. That is because the images were heavily resized from the original dataset. Nevertheless, I think a simple convolutional neural network (CNN) will do the trick!"},{"metadata":{},"cell_type":"markdown","source":"# Image Preprocessing: Normalize Images\nFirst we are going to normalize the images (Zero Mean and Unit Variance). This trick has been proven to increase the performance of CNN's in general. As a rule of thumb, when you are dealing with computer vision tasks, you should normalize your images first."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = (X_train - X_train.mean()) / X_train.std()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Keras Model: Simple CNN\nFor our first model, we will use a single convolutional layer followed by two fully connected layers. We will be experimenting with the model architecture so it is generally a good idea to start as simple as possible and then work your way up."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(64, (3,3), input_shape = (32,32,3), activation='relu'))\nmodel.add(Flatten())\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train, epochs=5, validation_split = 0.25)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, just by using a simple CNN and normalizing the data, we were able to achieve amazing results (over 95%). Let's try to improve the performance with a couple of tricks."},{"metadata":{},"cell_type":"markdown","source":"# Batch Normalization\nBatch Normalization is a common trick used in today's deep learning networks. In simple terms, its purpose is to normalize the numbers found **WITHIN** the hidden layers. Remember how we first normalized our images before feeding them into the network? Well batch normalization pretty much does the same thing for the hidden layers inside of the network.\n\nHere is a youtube video that explains it a lot better than me:\n[Link](https://www.youtube.com/watch?time_continue=427&v=dXB-KQYkzNU)"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(64, (3,3), input_shape = (32,32,3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Flatten())\nmodel.add(Dense(64, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train, epochs=5, validation_split = 0.25)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Alright, so we can see that batch normalization did not really help the model. This is probably because the network is relatively small and error propogation due to non-normalized hidden layers are minimal."},{"metadata":{},"cell_type":"markdown","source":"# Submission\nAlright, so we are ready to make our submission. First we are going to retrain our model on all of the testing data (we previously reserved 25% for validation). Then, we are going to use the model to make predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(64, (3,3), input_shape = (32,32,3), activation='relu'))\nmodel.add(Flatten())\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train, epochs=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Prep the testing data\nprint(\"Prepping Testing Data...\")\ntesting_directory = '../input/test/test/'\nX_test = [cv2.imread(testing_directory + filename) for filename in os.listdir(testing_directory)]\nX_test = np.array(X_test)\nprint(\"Complete!\")\nprint(\"X_test shape = {}\".format(X_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Making Predictions\npredictions = model.predict_classes(X_test)\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['has_cactus'] = predictions\nsubmission.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('./submissions.csv', header=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}