{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](https://i.imgur.com/RBVgWdU.png)\n\nCurrently there are many DL frameworks which have their own strengths and advantages. Also there is a number of higher-level libraries, which provide an easier way to train models and make experiments.\n\n[Catalyst](https://github.com/catalyst-team/catalyst) provides high-level utils for PyTorch DL & RL research. It was developed with a focus on reproducibility, fast experimentation and code/ideas reusing. Being able to research/develop something new, rather then write another regular train loop.\n\nIt is supposed to be run from command line and customized with configuration files, but in this kernel I'll show how to use it in Jupyter Notebooks (or Kaggle Kernel)."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"!pip install albumentations > /dev/null 2>&1\n!pip install pretrainedmodels > /dev/null 2>&1\n!pip install catalyst > /dev/null 2>&1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader,Dataset\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nimport time \nfrom PIL import Image\ntrain_on_gpu = True\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\nfrom sklearn.metrics import accuracy_score\nimport cv2\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nimport albumentations\nimport pretrainedmodels\nfrom albumentations.pytorch import ToTensor\n\nimport collections","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Importing catalyst:\n- importing utils;\n- SupervisedRunner - a convenient wrapper for supervised models;\n- callbacks contain schedulers, metrics and many other useful things;"},{"metadata":{"trusted":true},"cell_type":"code","source":"from catalyst.dl.utils import UtilsFactory\nfrom catalyst.dl.experiments import SupervisedRunner\nfrom catalyst.dl.callbacks import EarlyStoppingCallback, OneCycleLR, InferCallback","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Defining tranformations using albumentations:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_transforms = albumentations.Compose([\n    albumentations.HorizontalFlip(),\n    albumentations.VerticalFlip(),\n    albumentations.RandomBrightness(),\n    albumentations.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225],\n    ),\n    ToTensor()\n    ])\ndata_transforms_test = albumentations.Compose([\n    albumentations.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225],\n    ),\n    ToTensor()\n    ])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reading and preparing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv')\ntrain, valid = train_test_split(train_df.has_cactus, stratify=train_df.has_cactus, test_size=0.1)\n# creating dict with image names and labels\nimg_class_dict = {k:v for k, v in zip(train_df.id, train_df.has_cactus)}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Writing custom dataset class\n\nOne interesting point worth noticing: Pytorch automatically converts numpy data types into Pytorch datatypes as a result sometimes the type of Pytorch tensor can be incompatible with the loss and we would need to convert datatypes. I define datatype for labels as `np.float32` to avoid such a problem."},{"metadata":{"trusted":true},"cell_type":"code","source":"class CactusDataset(Dataset):\n    def __init__(self, datafolder, datatype='train', transform = transforms.Compose([transforms.CenterCrop(32),transforms.ToTensor()]), labels_dict={}):\n        self.datafolder = datafolder\n        self.datatype = datatype\n        self.image_files_list = [s for s in os.listdir(datafolder)]\n        self.transform = transform\n        self.labels_dict = labels_dict\n        if self.datatype == 'train':\n            self.labels = [np.float32(labels_dict[i]) for i in self.image_files_list]\n        else:\n            self.labels = [np.float32(0.0) for _ in range(len(self.image_files_list))]\n\n    def __len__(self):\n        return len(self.image_files_list)\n\n    def __getitem__(self, idx):\n        img_name = os.path.join(self.datafolder, self.image_files_list[idx])\n        img = cv2.imread(img_name)[:,:,::-1]\n        image = self.transform(image=img)\n        image = image['image']\n        label = self.labels[idx]\n        \n        return image, label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = CactusDataset(datafolder='../input/train/train', datatype='train', transform=data_transforms, labels_dict=img_class_dict)\ntest_set = CactusDataset(datafolder='../input/test/test', datatype='test', transform=data_transforms_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating Catalyst data loaders\n\nWe crete data loaders in a usual way, but then we combine loaders into one dictionary for convenience."},{"metadata":{"trusted":true},"cell_type":"code","source":"loaders = collections.OrderedDict()\n\ntrain_sampler = SubsetRandomSampler(list(train.index))\nvalid_sampler = SubsetRandomSampler(list(valid.index))\nbatch_size = 512\nnum_workers = 0\ntrain_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\nvalid_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler, num_workers=num_workers)\ntest_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, num_workers=num_workers)\n\nloaders[\"train\"] = train_loader\nloaders[\"valid\"] = valid_loader\nloaders[\"test\"] = test_loader","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Neural net architecture"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Flatten(nn.Module):\n    def forward(self, input):\n        return input.view(input.size(0), -1)\n    \nclass Net(nn.Module):\n    def __init__(\n            self,\n            num_classes: int,\n            p: float = 0.2,\n            pooling_size: int = 2,\n            last_conv_size: int = 1664,\n            arch: str = \"densenet169\",\n            pretrained: str = \"imagenet\") -> None:\n        \"\"\"A simple model to finetune.\n        \n        Args:\n            num_classes: the number of target classes, the size of the last layer's output\n            p: dropout probability\n            pooling_size: the size of the result feature map after adaptive pooling layer\n            last_conv_size: size of the flatten last backbone conv layer\n            arch: the name of the architecture form pretrainedmodels\n            pretrained: the mode for pretrained model from pretrainedmodels\n        \"\"\"\n        super().__init__()\n        net = pretrainedmodels.__dict__[arch](pretrained=pretrained)\n        modules = list(net.children())[:-1]  # delete last layer\n        # add custom head\n        modules += [nn.Sequential(\n            Flatten(),\n            nn.BatchNorm1d(1664),\n            nn.Dropout(p),\n            nn.Linear(1664, num_classes)\n        )]\n        self.net = nn.Sequential(*modules)\n\n    def forward(self, x):\n        logits = self.net(x)\n        return torch.squeeze(logits)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Preparing for training"},{"metadata":{"trusted":true},"cell_type":"code","source":"# experiment setup\nnum_epochs = 10\nlogdir = \"./logs/simple\"\n\n# model, criterion, optimizer\nmodel = Net(num_classes=1)\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.SGD(model.parameters(), momentum=0.99, lr=1e-2)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training model with SupervisedRunner"},{"metadata":{},"cell_type":"markdown","source":"### Training with one cycle"},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"# model runner\nrunner = SupervisedRunner()\n\n# model training\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler=scheduler, \n    loaders=loaders,\n    logdir=logdir,\n    callbacks=[\n        OneCycleLR(\n            cycle_len=num_epochs, \n            div_factor=3,\n            increase_fraction=0.3,\n            momentum_range=(0.95, 0.85))\n    ],\n    num_epochs=num_epochs,\n    verbose=False\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting training progress\nUtilsFactory.plot_metrics(logdir=logdir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Training with early stopping"},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"model = Net(num_classes=1)\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.SGD(model.parameters(), momentum=0.99, lr=1e-2)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=2)\n\nnum_epochs = 20\nlogdir1 = \"./logs/simple1\"\n# model runner\nrunner1 = SupervisedRunner()\n\n# model training\nrunner1.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler=scheduler, \n    loaders=loaders,\n    callbacks=[\n        EarlyStoppingCallback(patience=4, min_delta=0.01)\n    ],\n    logdir=logdir1,\n    num_epochs=num_epochs,\n    verbose=False\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting training progress\nUtilsFactory.plot_metrics(logdir=logdir1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Inference\nMaking inference is easy - we simply need to use a special callback"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_loader = collections.OrderedDict([(\"infer\", loaders[\"test\"])])\nrunner.infer(\n    model=model,\n    loaders=test_loader,\n    callbacks=[InferCallback()],\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_img = os.listdir('../input/test/test')\ntest_df = pd.DataFrame(test_img, columns=['id'])\ntest_preds = pd.DataFrame({'imgs': test_df.id.values, 'preds': runner.callbacks[0].predictions[\"logits\"]})\ntest_preds.columns = ['id', 'has_cactus']\ntest_preds.to_csv('sub.csv', index=False)\ntest_preds.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"runner1.infer(\n    model=model,\n    loaders=test_loader,\n    callbacks=[InferCallback()],\n)\ntest_preds['has_cactus'] = runner1.callbacks[0].predictions[\"logits\"]\ntest_preds.to_csv('sub1.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}