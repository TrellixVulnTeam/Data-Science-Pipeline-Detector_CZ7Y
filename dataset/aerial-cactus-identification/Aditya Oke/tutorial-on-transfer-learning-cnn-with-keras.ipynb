{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os,cv2\nfrom IPython.display import Image\nfrom tqdm import tqdm, tqdm_notebook\nfrom keras.preprocessing import image\nfrom keras import optimizers\nfrom keras import layers\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom keras.applications.imagenet_utils import preprocess_input\nfrom keras.callbacks import ModelCheckpoint\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom keras import regularizers\nfrom keras.preprocessing.image import ImageDataGenerator\nprint(os.listdir(\"../input\"))\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Let us begin this task. We wll develop simple model. Beginner friendly code !!!! YAY !!!!\n* Doing the necessary imports.\n* We will be using Keras with Data Augementation\n* This time instead of building model from scratch we will do transfer learning.\n* You can check the CNN from scratch here. https://www.kaggle.com/okeaditya/aerial-cactus-keras-cnn\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_dir=\"../input/train/train\"\ntest_dir=\"../input/test/test\"\ntrain=pd.read_csv('../input/train.csv')\n\ndf_test=pd.read_csv('../input/sample_submission.csv')\ntrain.has_cactus=train.has_cactus.astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.tail(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['has_cactus'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*     Let us get a brief idea about the dataset that is given.\n*     We have total of 17500 cactii images here.\n*     We have a medium sized data-set.\n*     Looks like we have a biased data-set. We have has_cactus for more data equal to 1.\n*     This indicates that we have more images which have cactus."},{"metadata":{},"cell_type":"markdown","source":"## Reading the Images.\n\n\n*     The main part is to handle the image data.\n*     We need to take input of the image file\n*     Decode the JPEG data into RGB channels\n*     Then normalize it by dividing by 255.\n*     We can do data-augmentation on top of that.\n  \n*    We can rotate the images by certain angle. (Using rotation_range)\n*    We can also shift the images by a certain distance. (Using the width_shift_range and height_shift_range)\n*    We can also try to horizontally flip the images.\n*    I am not vertically flipping the image as it may mean an absurd view of aerial cactus.\n*    Also finally we are rescaling it by a factor of 1./255 to normalize the pixel values.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_datagen = ImageDataGenerator(featurewise_center=False, samplewise_center=False, \n                             featurewise_std_normalization=False, \n                             samplewise_std_normalization=False, zca_whitening=False, zca_epsilon=1e-06, \n                             rotation_range= 55, \n                             width_shift_range=0.2, \n                             height_shift_range=0.2, \n                             brightness_range=None, \n                             shear_range=0.0, zoom_range=0.0, \n                             channel_shift_range=0.0, \n                             fill_mode='nearest',\n                             cval=0.0, \n                             horizontal_flip=True, \n                             vertical_flip=False, \n                             rescale=1./255 , \n                             preprocessing_function=None, data_format=None, validation_split=0.0, dtype=None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*    Note while validating.\n*    We do not want to check on augemented data.\n*    We avoid checking on that by creating new validation generator and making use of it directly.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_datagen = ImageDataGenerator(rescale=1./255)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 128","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Now we have to read the images with their labels which are in the train dataframe\n* To do this Keras provides us the way of loading from dataframe using flow_from_dataframe\n* Check it here https://keras.io/preprocessing/image/\n* Also for a tutorial head here -> https://medium.com/@vijayabhaskar96/tutorial-on-keras-imagedatagenerator-with-flow-from-dataframe-8bd5776e45c1\n* This also splits the data into two parts.\n* We have 15000 images in train part.\n* We have 2500 images in validation part."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_generator = train_datagen.flow_from_dataframe(train[:15000], directory=train_dir, x_col='id', y_col='has_cactus', \n                    target_size=(64, 64), color_mode='rgb', classes=None, \n                    class_mode='binary', batch_size=batch_size, \n                    shuffle=True, seed=None, \n                    save_to_dir=None, save_prefix='', save_format='png', \n                    subset=None, interpolation='nearest', drop_duplicates=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_generator = valid_datagen.flow_from_dataframe(train[15000:], directory=train_dir, x_col='id', y_col='has_cactus', \n                    target_size=(64, 64), color_mode='rgb', classes=None, \n                    class_mode='binary', batch_size=batch_size, \n                    shuffle=True, seed=None, \n                    save_to_dir=None, save_prefix='', save_format='png', \n                    subset=None, interpolation='nearest', drop_duplicates=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## How does CNN work?"},{"metadata":{},"cell_type":"markdown","source":"* Bulding a CNN is just like building our brain to recognize patterns. We try to focus only on certain featurs of image to understand that it is an aerial cactus. E.g seeing the green colour and spikes, considering its thickness might mean that we are seeing an aerial cactus.\n\n* So we need to reduce the feature maps that are given in the image. This is done by Conv2D layer. We glide the 3x3 kernel over the entire image of size 256 x 256 multiple times. Thus we are reducing the dimensions of image while learning about them.\n\n* We are thus making an abstract identification and description of an image."},{"metadata":{},"cell_type":"markdown","source":"## Transfer Learning"},{"metadata":{},"cell_type":"markdown","source":"* Instead of building our own CNN architecture, we can try to use a pre-built CNN.\n* The pre-built CNN is already trained on datasets such as ImageNet.\n* So, it has certain ability to distinguish images quite well.\n* This gives it a generalizing chracter. Lowers the variance.\n* But we need to make it specialized on this dataset.\n* For that we need to add a few layers a train these parameters so that it specializes.\n* This ensures that bias is reduced, but variance is increased a bit, so we trade-off here.\n* Keras provides various pre-trainied models which are traioned on ImageNet dataset.\n* You can check them here -> https://keras.io/applications/"},{"metadata":{},"cell_type":"markdown","source":"* We will try two most popular models here VGG16 and then ResNet"},{"metadata":{},"cell_type":"markdown","source":"## Trying with VGG16"},{"metadata":{},"cell_type":"markdown","source":"* Turn on the internet feature in Kaggle kernels.\n* Also Turn on the GPU.\n* You may need to re-reun the code.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making imports for VGG16\nfrom keras.applications import VGG16","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We make include_top = False as we do not need the classifier of ImageNet.\n* As this is binary_classification problem we set classes = 2\n* Also our input size is 64 x 64 x 3. We need to specify that as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model = VGG16(include_top=False, weights='imagenet', input_tensor=None, input_shape=(64,64,3), pooling=None, classes=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Now we need to build a classifier on top of this.\n* We then train the classifier."},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model.summary()\n# This is summary of VGG16","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(base_model)\n# model.add(layers.Conv2D(32, kernel_size = (3,3), activation = 'relu')\n# model.add(layers.Dropout())\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(256, activation = 'relu' ))\nmodel.add(layers.Dense(1, activation = 'sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()\n# Summary of model after adding the extra features to the base_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us find the trainable weights that we have\nprint(\"Number of trainable parameters before freezing the model \", len(model.trainable_weights))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we freeze the base_model and train the extra two layers that we added\nbase_model.trainable = False\nprint(\"Number of trainable parameters after freezing the model \", len(model.trainable_weights))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us re-summarize the model once\n# We can see that some paramters are freezed and we have lot fewer models to train on\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* First we need to compile the model.\n* Compiling the model has three parts, loss_function, optimizer and then the metrics.\n* We want to minimize the binary_cross_entropy loss, since we are dealing with binary classification problem.\n* Also we are using Adam optimizer.\n* You could try different optimizer such as RMSProp or SGD also.\n* https://keras.io/optimizers/ for details of all optimizers available\n* To learn more http://ruder.io/optimizing-gradient-descent/\n* We bother about accuracy score so we are using the metrics to be equalt to accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us create an optimizer, complie the model and train it\noptim = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\nmodel.compile(optimizer = optim, loss='binary_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Also let us checkpoint the model to keep track of best model\nfilepath = \"best_model.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\ncall_backs_list = [checkpoint]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_epochs = 60\nhistory = model.fit_generator(\n    train_generator,\n    steps_per_epoch = 100,\n    epochs = max_epochs,\n    validation_data = valid_generator,\n    callbacks = call_backs_list,\n    validation_steps = 50.\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us check the plot once. \nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'g', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, loss, 'g', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reload the best model\nmodel.load_weights(\"best_model.hdf5\")\nmodel.compile(optimizer = optim, loss='binary_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Trying DenseNet121"},{"metadata":{},"cell_type":"markdown","source":"* Let us try one more model DenseNet121\n* We are trying to compare the two performances and the choose the better model of the two.\n* The proedure remains same, we will use pre-trained model of ResNet. \n* Then flatten and add a densely connected layer. On top of that we will have the output layer."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.applications.densenet import DenseNet121","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_layer = DenseNet121(include_top=False, weights='imagenet', input_tensor=None, input_shape=(64,64,3), pooling=None, classes=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Let us quickly summarize how DenseNet121 looks like"},{"metadata":{"trusted":true},"cell_type":"code","source":"base_layer.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2 = Sequential()\nmodel2.add(base_layer)\nmodel2.add(layers.Flatten())\nmodel2.add(layers.Dense(256, activation = 'relu'))\nmodel2.add(layers.Dense(1, activation = 'sigmoid'))\nmodel2.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we freeze the base_model and train the extra two layers that we added\nbase_layer.trainable = False\nmodel2.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optim = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\nmodel2.compile(optimizer = optim, loss='binary_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Also let us checkpoint the model to keep track of best model\nfilepath = \"best_model_DenseNet.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\ncall_backs_list = [checkpoint]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_epochs = 10\nhistory = model2.fit_generator(\n    train_generator,\n    steps_per_epoch = 100,\n    epochs = max_epochs,\n    validation_data = valid_generator,\n    callbacks = call_backs_list,\n    validation_steps = 50.\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us check the plot once. \nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'g', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, loss, 'g', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* So the DenseNet model does not genaralize well.\n* Let us use only the VGG16 for making final predicitions."},{"metadata":{"trusted":true},"cell_type":"code","source":"un_test_img=[]\ncount=0\nfor i in os.listdir(\"../input/test/test/\"):\n    un_test_img.append(i)\n    count+=1\nun_test_image=[]\nfor i in tqdm(range(count)):\n    img = image.load_img('../input/test/test/'+un_test_img[i], target_size=(64,64,3), grayscale=False)\n    img = image.img_to_array(img)\n    img = img/255\n    un_test_image.append(img)\nun_test_img_array = np.array(un_test_image)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = model.predict_classes(un_test_img_array)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_save = pd.DataFrame()\nsubmission_save['id'] = un_test_img\nsubmission_save['has_cactus'] = output\nsubmission_save.to_csv('submission.csv', header=True, index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.read_csv('submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}