{"cells":[{"metadata":{"_uuid":"2aba335f-205a-409c-af5b-aa6e27f8643c","_cell_guid":"4c0fb5cb-f906-4e1a-a663-317e71e579c5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n#import numpy as np # linear algebra\n#import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## This notebook has been written for my personal study.\n### I copied some codes from other sources. "},{"metadata":{"_uuid":"30eeaae9-72a3-47ea-8286-ff2240e1cff3","_cell_guid":"4f8019c3-984a-498a-b275-b640c97c6949","trusted":true},"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom torchvision import datasets, transforms\nfrom torch import optim, nn\nfrom torch.utils.data import Dataset, TensorDataset, DataLoader\nimport torch.nn.functional as F","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e8749a6-0c05-4ef2-a2fc-4f410c2be3e9","_cell_guid":"643f2537-8510-4162-b8f3-0438951b5174","trusted":true},"cell_type":"code","source":"# credit: intro to Deep Learning with Pytorch by Facebook\n# Udacity Course.\n# https://github.com/udacity/deep-learning-v2-pytorch/blob/master/intro-to-pytorch/helper.py\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef imshow(image, ax=None, title=None, normalize=True):\n    \"\"\"Imshow for Tensor.\"\"\"\n    if ax is None:\n        fig, ax = plt.subplots()\n    image = image.numpy().transpose((1, 2, 0))\n\n    if normalize:\n        mean = np.array([0.485, 0.456, 0.406])\n        std = np.array([0.229, 0.224, 0.225])\n        image = std * image + mean\n        image = np.clip(image, 0, 1)\n\n    ax.imshow(image)\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.tick_params(axis='both', length=0)\n    ax.set_xticklabels('')\n    ax.set_yticklabels('')\n\n    return ax\n\n# credit: I copied this method from \"https://www.kaggle.com/bonhart/simple-cnn-on-pytorch-for-beginers\"\n# NOTE: class is inherited from Dataset\nclass MyDataset(Dataset):\n    def __init__(self, df_data, data_dir = './', transform=None):\n        super().__init__()\n        self.df = df_data.values\n        self.data_dir = data_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        img_name,label = self.df[index]\n        img_path = os.path.join(self.data_dir, img_name)\n        image = cv2.imread(img_path)\n        if self.transform is not None:\n            image = self.transform(image)\n        return image, label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n!cp -r /kaggle/input/aerial-cactus-identification /kaggle/working/\n!unzip /kaggle/working/aerial-cactus-identification/train.zip -d /kaggle/working\n!unzip /kaggle/working/aerial-cactus-identification/test.zip -d /kaggle/working\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import zipfile\n\ntrain_path = zipfile.ZipFile(\"../input/aerial-cactus-identification/train.zip\", 'r') \ntrain_path.extractall(\"/kaggle/working\")\n\ntrain_path.close()\n\ntest_path = zipfile.ZipFile('../input/aerial-cactus-identification/test.zip', 'r')\ntest_path.extractall('/kaggle/working')\n\ntest_path.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport cv2\nimport PIL\nimport os\n#labels_data = pd.read_csv(data)\n\ndata = pd.read_csv(\"../input/aerial-cactus-identification/train.csv\")\ndata.sample(10)\n\ntrain, test = train_test_split(data,\n                              stratify=data.has_cactus,\n                              test_size=.2,\n                              random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['has_cactus'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['has_cactus'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['has_cactus'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f90c7d92-c253-4051-a6c7-6b5f3f394826","_cell_guid":"837174d7-9ac2-4801-8ebf-8a187d85353f","trusted":true},"cell_type":"code","source":"TRAIN_DIR = '/kaggle/working/train'\n\ntrain_transforms = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Pad(32, padding_mode='reflect'),\n    #transforms.RandomResizedCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean = [0.44, 0.44, 0.44], \n                         std = [0.24, 0.24, 0.24])])\n\ntest_transforms = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Pad(32, padding_mode='reflect'),\n    #transforms.RandomResizedCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean = [0.44, 0.44, 0.44], \n                         std = [0.24, 0.24, 0.24])])\n\ntrain_data = MyDataset(train, data_dir=TRAIN_DIR, transform=train_transforms)\ntest_data = MyDataset(test, data_dir=TRAIN_DIR, transform=test_transforms)\n\ntrainloader = torch.utils.data.DataLoader(train_data, batch_size = 128, shuffle = True)\ntestloader = torch.utils.data.DataLoader(test_data, batch_size = 64, shuffle = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5efb985f-12cc-473d-b7de-e26c42305eba","_cell_guid":"03f1c2ae-d4ec-4fc3-996b-fa2a0ca23a0c","trusted":true},"cell_type":"code","source":"trainimages, trainlabels = next(iter(trainloader))\n\nfig, axes = plt.subplots(figsize=(12, 12), ncols=5)\nprint('training images')\nfor i in range(5):\n    axe1 = axes[i] \n    imshow(trainimages[i], ax=axe1, normalize=False)\n\nprint(trainimages[0].size())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ebb4060b-43d4-46f9-9505-e5d71b2b5b0d","_cell_guid":"0693d02f-0bb8-42a6-8795-e7f465afa1d0","trusted":true},"cell_type":"code","source":"testimages, testlabels = next(iter(testloader))\n\nfig, axes = plt.subplots(figsize=(12, 12), ncols=5)\nprint('test images')\nfor i in range(5):\n    axe2 = axes[i]\n    imshow(testimages[i], ax=axe2, normalize=False)\nprint(testimages[0].size())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class cnn(nn.Module):\n    def __init__(self):\n        super(cnn, self).__init__()\n        \n                # Convolutional Neural Networks\n        self.conv1 = nn.Conv2d(3, 32, 3)\n        self.conv2 = nn.Conv2d(32, 64, 3)\n        self.conv3 = nn.Conv2d(64, 128, 3)\n        self.conv4 = nn.Conv2d(128, 256, 3)\n        self.conv5 = nn.Conv2d(256, 512, 3)\n        \n        self.bn1 = nn.BatchNorm2d(32)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.bn3 = nn.BatchNorm2d(128)\n        self.bn4 = nn.BatchNorm2d(256)\n        #self.bn5 = nn.BatchNorm2d(512)\n                  \n        self.fc1 = nn.Linear(4096, 2)\n        \n        # pooling and dropout layer\n        self.pool = nn.MaxPool2d(2, 2)\n        # self.dropout = nn.Dropout(0.2)\n        \n    def forward(self, x):\n        x = self.bn1(self.pool(F.leaky_relu(self.conv1(x))))\n        x = self.bn2(self.pool(F.leaky_relu(self.conv2(x))))\n        x = self.bn3(self.pool(F.leaky_relu(self.conv3(x))))\n        x = self.bn4(self.pool(F.leaky_relu(self.conv4(x))))\n        #x = self.bn5(self.pool(F.leaky_relu(self.conv5(x))))\n        \n        # reshape to fit into fully connected net\n        x = x.view(x.shape[0],-1)\n        x = self.fc1(x)\n        x = F.log_softmax(x, dim=1)\n        \n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = cnn()\nprint(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"criterion = nn.NLLLoss() #nn.NLLLoss() # \noptimizer = torch.optim.Adam(model.parameters(), lr=0.3)\nepochs = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ndevice\n\nmodel = cnn().to(device)\nprint(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_losses, test_losses = [], []\n\nfor e in range(epochs):\n    running_loss = 0\n    \n    for images, labels in trainloader:\n        \n        images = images.to(device)\n        labels = labels.to(device)\n        \n        #ititialize gradients\n        optimizer.zero_grad()\n        \n        # evaluate the loss\n        output = model(images)\n        loss = criterion(output, labels)\n        \n        # backpropagate the weights\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n    \n    # once complete the computation for one training batch size,\n    else: \n        test_loss = 0\n        accuracy = 0\n        \n        # turn off calculating gradients for validation\n        # to save memory and time\n        \n        with torch.no_grad():\n            model.eval() #turn off dropout to evaludate validation set\n            \n            for images, labels in testloader:\n                images = images.to(device)\n                labels = labels.to(device)\n\n                los_ps = model(images)\n                test_loss += criterion(los_ps, labels)\n                \n                ps = torch.exp(los_ps)\n                poss, top_class = ps.topk(1, dim=1)\n                \n                equals = top_class == labels.view(*top_class.shape)\n                accuracy += torch.mean(equals.type(torch.FloatTensor))\n        \n        # turn on dropout again for the next training.\n        model.train() \n        \n        train_losses.append(running_loss/len(trainloader))\n        test_losses.append(test_loss/len(testloader))\n        \n        print(\"epoch: {}/{}  \".format(e+1, epochs),\n             \"training loss: {:.3f} \".format(train_losses[-1]),\n             \"test loss: {:.3f} \".format(test_losses[-1]),\n             \"test accuracy: {:.3f} \".format(accuracy/len(testloader)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(train_losses, label = 'Training Loss')\nplt.plot(test_losses, label = 'Test Loss')\nplt.legend(frameon=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> I have tried many other networks on my own, but I couldn't figure out why the networks wouldn't work. Training loss, test loss, and accuracy used to remain constant. \n> So, I have brought new CNN model from \"https://www.kaggle.com/bonhart/simple-cnn-on-pytorch-for-beginers\""},{"metadata":{"trusted":true},"cell_type":"code","source":"## Parameters for model\n# Hyper parameters\nnum_epochs = 25\nnum_classes = 2\nbatch_size = 128\nlearning_rate = 0.002\n\n# Device configuration\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n# Image preprocessing\ntrans_train = transforms.Compose([transforms.ToPILImage(),\n                                  transforms.Pad(32, padding_mode='reflect'),\n                                  transforms.ToTensor(),\n                                  transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])])\n\ntrans_valid = transforms.Compose([transforms.ToPILImage(),\n                                  transforms.Pad(32, padding_mode='reflect'),\n                                  transforms.ToTensor(),\n                                  transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])])\n\n# Data generators\ndataset_train = MyDataset(df_data=train, data_dir=train_path, transform=trans_train)\ndataset_valid = MyDataset(df_data=val, data_dir=train_path, transform=trans_valid)\n\ntrainloader = DataLoader(dataset = dataset_train, batch_size=batch_size, shuffle=True, num_workers=0)\ntestloader = DataLoader(dataset = dataset_valid, batch_size=batch_size//2, shuffle=False, num_workers=0)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NOTE: class is inherited from nn.Module\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        # ancestor constructor call\n        super(SimpleCNN, self).__init__()\n        \n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=2) # match out_ch and next in_ch\n        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=2)\n        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=2)\n        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=2)\n        self.conv5 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=2)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.bn3 = nn.BatchNorm2d(128)\n        self.bn4 = nn.BatchNorm2d(256)\n        self.bn5 = nn.BatchNorm2d(512)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.avg = nn.AvgPool2d(4)\n        self.fc = nn.Linear(512 * 1 * 1, 2) # !!!\n   \n    def forward(self, x):\n        x = self.pool(F.leaky_relu(self.bn1(self.conv1(x)))) # first convolutional layer then batchnorm, then activation then pooling layer.\n        x = self.pool(F.leaky_relu(self.bn2(self.conv2(x))))\n        x = self.pool(F.leaky_relu(self.bn3(self.conv3(x))))\n        x = self.pool(F.leaky_relu(self.bn4(self.conv4(x))))\n        x = self.pool(F.leaky_relu(self.bn5(self.conv5(x))))\n        x = self.avg(x)\n        #print(x.shape) # lifehack to find out the correct dimension for the Linear Layer\n        x = x.view(-1, 512 * 1 * 1) # !!!\n        x = self.fc(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnnmodel = SimpleCNN().to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adamax(cnnmodel.parameters(), lr=learning_rate)\n\nepochs = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_losses, test_losses = [], []\n\nfor e in range(epochs):\n    running_loss = 0\n    \n    for images, labels in trainloader:\n        \n        images = images.to(device)\n        labels = labels.to(device)\n        \n        #ititialize gradients\n        optimizer.zero_grad()\n        \n        # evaluate the loss\n        output = cnnmodel(images)\n        loss = criterion(output, labels)\n        \n        # backpropagate the weights\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n    \n    # once complete the computation for one training batch size,\n    else: \n        test_loss = 0\n        accuracy = 0\n        \n        # turn off calculating gradients for validation\n        # to save memory and time\n        \n        with torch.no_grad():\n            cnnmodel.eval() #turn off dropout to evaludate validation set\n            \n            for images, labels in testloader:\n                images = images.to(device)\n                labels = labels.to(device)\n\n                los_ps = cnnmodel(images)\n                test_loss += criterion(los_ps, labels)\n                \n                ps = torch.exp(los_ps)\n                poss, top_class = ps.topk(1, dim=1)\n                \n                equals = top_class == labels.view(*top_class.shape)\n                accuracy += torch.mean(equals.type(torch.FloatTensor))\n        \n        # turn on dropout again for the next training.\n        cnnmodel.train() \n        \n        train_losses.append(running_loss/len(trainloader))\n        test_losses.append(test_loss/len(testloader))\n        \n        print(\"epoch: {}/{}  \".format(e+1, epochs),\n             \"training loss: {:.3f} \".format(train_losses[-1]),\n             \"test loss: {:.3f} \".format(test_losses[-1]),\n             \"test accuracy: {:.3f} \".format(accuracy/len(testloader)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(train_losses, label = 'Training Loss')\nplt.plot(test_losses, label = 'Test Loss')\nplt.legend(frameon=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}