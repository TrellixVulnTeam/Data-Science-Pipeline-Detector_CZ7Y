{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Aerial Cactus Identification\n\nGoal : classify whether aerial image has cactus or not\nDataset : [Aerial Cactus Image](https://www.kaggle.com/c/aerial-cactus-identification)\n\n## Table of Contents\n- [0. Prerequisites](#prerequisites)\n- [1. Data Preparation](#dataprep)\n- [2. Model Building](#modelbuild)\n- [3. Model Training and Evaluation](#modeltraineval)\n- [-1. Summary](#summary)\n\n\nauthor @otivedani | github.com/otivedani","metadata":{"_uuid":"5183f3ec-26f7-4f56-84b4-fde6a2680c58","_cell_guid":"28f27783-3531-4fb6-9fd0-8e5362a3ced5","trusted":true}},{"cell_type":"markdown","source":"---\n<a name='prerequisites'></a>\n## #0 :: Prerequisites \n\n*Here we define dependencies, environments and dataset download*","metadata":{}},{"cell_type":"code","source":"import random\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import transforms\nfrom typing import Callable, Optional, Tuple\nimport matplotlib.pyplot as plt\n","metadata":{"_uuid":"52b11381-beb6-440d-a4bf-3c77b91d77b1","_cell_guid":"df389847-eeb1-42e6-9aeb-4db53fc11766","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-11-07T13:59:06.268758Z","iopub.execute_input":"2021-11-07T13:59:06.269074Z","iopub.status.idle":"2021-11-07T13:59:07.847123Z","shell.execute_reply.started":"2021-11-07T13:59:06.26899Z","shell.execute_reply":"2021-11-07T13:59:07.84639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DEFINE random seed\nRANDOM_SEED = 86\n# RANDOM_SEED = random.randrange(sys.maxsize)\n#!DEFINE\n\nprint(f\"seed : {RANDOM_SEED}\")\n\n# initiate seeds\nrandom.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\n\n# use accelerator when available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"execution":{"iopub.status.busy":"2021-11-07T13:59:07.848795Z","iopub.execute_input":"2021-11-07T13:59:07.849121Z","iopub.status.idle":"2021-11-07T13:59:07.903001Z","shell.execute_reply.started":"2021-11-07T13:59:07.849083Z","shell.execute_reply":"2021-11-07T13:59:07.902279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Download and extract dataset from https://www.kaggle.com/c/aerial-cactus-identification\n\nDefine extracted dataset in the cell below and run.\nExpect directory listings something like this :\n```\n!tree /kaggle/working/aerial-cactus-identification\n.\n├── sample_submission.csv\n├── test.zip\n├── train.csv\n└── train.zip\n```","metadata":{"_uuid":"fa917dc8-0323-4453-b2c3-d6d9abd5cee1","_cell_guid":"88057438-dfe4-451d-9994-9baa81a0335a","trusted":true}},{"cell_type":"code","source":"# run this cell if notebook hosted on kaggle.com to resolve file permission problem\n!cp -r /kaggle/input/aerial-cactus-identification /kaggle/working/","metadata":{"_uuid":"8e617c87-ad96-4e6a-a0f5-321715742592","_cell_guid":"ce0656be-152c-4d45-b086-a2ba33a1a205","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-11-07T13:59:08.99368Z","iopub.execute_input":"2021-11-07T13:59:08.994362Z","iopub.status.idle":"2021-11-07T13:59:10.211152Z","shell.execute_reply.started":"2021-11-07T13:59:08.994321Z","shell.execute_reply":"2021-11-07T13:59:10.210281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DEFINE our dataset directory, e.g. /kaggle/working/aerial-cactus-identification\nACI_DATASET_PATH = '/kaggle/working/aerial-cactus-identification'\n#!DEFINE\n\n!tree -L 1 {ACI_DATASET_PATH}\n\n!test ! -d {ACI_DATASET_PATH}/train && unzip -q {ACI_DATASET_PATH}/train.zip -d {ACI_DATASET_PATH}\n!test ! -d {ACI_DATASET_PATH}/test && unzip -q {ACI_DATASET_PATH}/test.zip -d {ACI_DATASET_PATH}","metadata":{"_uuid":"f89ff8c8-ba51-4d27-9f59-5ad0ac7e3e65","_cell_guid":"8b9d7cb0-2da7-4800-a2cb-08a92f545956","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-11-07T13:59:10.213396Z","iopub.execute_input":"2021-11-07T13:59:10.213684Z","iopub.status.idle":"2021-11-07T13:59:13.274252Z","shell.execute_reply.started":"2021-11-07T13:59:10.213644Z","shell.execute_reply":"2021-11-07T13:59:13.273027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n<a name='dataprep'></a>\n## #1 :: Data Preparation\n\n*Dataset definition, and splitting training and validation*","metadata":{"_uuid":"7f1f2811-9572-4f8e-8df4-ec77dea2db15","_cell_guid":"df571fe8-b2e9-4ccf-9e11-cc336581a490","trusted":true}},{"cell_type":"markdown","source":"### Dataset Definition\n\nHow to define dataset and the transformations on images and labels are as follows :","metadata":{"_uuid":"34a3bdfa-e9de-44b6-b085-db57f2a3b285","_cell_guid":"e257e88a-2b50-402a-ab77-7c7ec27c0a80","trusted":true}},{"cell_type":"code","source":"# DEFINE how to load dataset, pairing labels and images here\n\nclass AerialCactus(torch.utils.data.Dataset):\n    \"\"\"\n    Aerial Cactus Identification Dataset \n    from https://www.kaggle.com/c/aerial-cactus-identification\n    \"\"\"\n    def __init__(self, \n                 root: str,\n                 train: bool = True,\n                 transform: Optional[Callable] = None, \n                 target_transform: Optional[Callable] = None,\n                ) -> None:\n        super().__init__()\n        self.root = Path(root)\n        self.transform = transform\n        self.target_transform = target_transform\n        \n        # assuming train.zip and test.zip were already unzipped\n        mode = 'train/' if train else 'test/'\n        self.images_path = self.root/mode\n        \n        # dataset are based on supplied csv\n        csv_dataframe = 'train.csv' if train else 'sample_submission.csv'\n        self.df = pd.read_csv(self.root/csv_dataframe)\n    \n    def __getitem__(self, idx:int) -> Tuple[torch.Tensor, int]:\n        \"\"\"Get each items from dataset by index\"\"\"\n        \n        # supplied csv data have 2 field\n        data = self.df.loc[idx]\n        \n        # `has_cactus` : 1 means the image has cactus, 0 means no cactus\n        label = data['has_cactus']\n        if self.target_transform:\n            label = self.target_transform(label)\n            \n        # `id` : the filename, which are inside train/test zip\n        image = Image.open(self.images_path/data['id'])\n        image = np.array(image)\n        if self.transform:\n            image = self.transform(image)\n        if not isinstance(image, torch.Tensor):\n            image = transforms.functional.to_tensor(image)\n        \n        return image, label\n    \n    def __len__(self) -> int:\n        return len(self.df)\n    \n#!DEFINE\n\n# preview dataset definition\npreview_dataset = AerialCactus(ACI_DATASET_PATH, train=True)\nlabel_counts = preview_dataset.df['has_cactus'].value_counts()\nlabel_counts.index = ['has_cactus', 'no_cactus']\nprint(label_counts)","metadata":{"_uuid":"29b6e3bd-1c2a-4e99-aa9b-b9ecfcacc0a6","_cell_guid":"f99c551e-c700-4167-a91f-22675e14a0c1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-11-07T13:59:19.134823Z","iopub.execute_input":"2021-11-07T13:59:19.135147Z","iopub.status.idle":"2021-11-07T13:59:19.185755Z","shell.execute_reply.started":"2021-11-07T13:59:19.135109Z","shell.execute_reply":"2021-11-07T13:59:19.184902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems the dataset is imbalanced, having ratio between class 1 (cactus image) and class 0 (no cactus image) was ~0.33. This could be resolved by adding more data, augmentation, or tweak class weight later, which will be done later.","metadata":{"_uuid":"0efc693b-491d-42e8-9f6d-80ee99018a1c","_cell_guid":"e9461ecb-6a17-475d-baef-a0effd177718","trusted":true}},{"cell_type":"code","source":"# DEFINE how to use the dataset, without transformations\nbase_dataset = AerialCactus(ACI_DATASET_PATH, train=True, transform=None)\n#!DEFINE\n\nprint(f\"Using total of {len(base_dataset)} data\")\n\n# preview how our dataset is loaded\nbase_dataloader = torch.utils.data.DataLoader(base_dataset, batch_size=8, shuffle=True)\nexample_data = next(iter(base_dataloader))\n\nf, axs = plt.subplots(1,8, figsize=(16,16))\nfor ax, img, label in zip(axs, *example_data,):\n    ax.imshow(np.moveaxis(img.numpy(), 0, -1))\n    ax.set_title(['no_cactus', 'has_cactus'][int(label)])","metadata":{"_uuid":"54e9dd7a-f52e-418f-93d6-bcf03849da6c","_cell_guid":"0f082c58-32e4-47e4-826f-92cfc7ecbc1a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-11-07T14:12:44.345024Z","iopub.execute_input":"2021-11-07T14:12:44.345315Z","iopub.status.idle":"2021-11-07T14:12:45.284136Z","shell.execute_reply.started":"2021-11-07T14:12:44.345284Z","shell.execute_reply":"2021-11-07T14:12:45.28345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train-Validation Split\n\nAssign dataset loader for training and validation in the cell below.","metadata":{"_uuid":"b8daae81-7066-4192-84d8-f830dcab8c51","_cell_guid":"8df39300-93c2-45e5-b450-c8ef879563ad","trusted":true}},{"cell_type":"code","source":"# DEFINE loader batch sizes, train-validation split ratio\nTRAIN_RATIO = 0.8\nBATCH_SIZE = 32\n#!DEFINE \n\n# split our base dataset to `train` and `validation` datasets\nn_train = int(len(base_dataset) * TRAIN_RATIO)\nn_valid = len(base_dataset) - n_train\ntrain_dataset, valid_dataset = torch.utils.data.random_split(base_dataset, [n_train, n_valid])\n\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=0, shuffle=True)\nvalid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, num_workers=0, shuffle=True)\nprint(f\"Each loader will load {BATCH_SIZE} each batch\")\n\n# count size of each target class from dataloader\ntrain_targets = np.concatenate([target.numpy() for _, target in train_dataloader])\nvalid_targets = np.concatenate([target.numpy() for _, target in valid_dataloader])\n\nprint(f\"Using {n_train} data for training and {n_valid} data for validation\")\nprint(     f\"Train set class\\n 1 : {train_targets.sum()}, 0 : {train_targets.size - train_targets.sum()}\")\nprint(f\"Validation set class\\n 1 : {valid_targets.sum()}, 0 : {valid_targets.size - valid_targets.sum()}\")","metadata":{"_uuid":"0abf81a4-c7c9-4587-86ea-7f0a49a52b5b","_cell_guid":"87fbe0e4-c4c4-49d3-98b2-c4afcbc9fa54","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-11-07T14:12:45.373141Z","iopub.execute_input":"2021-11-07T14:12:45.373867Z","iopub.status.idle":"2021-11-07T14:12:54.987696Z","shell.execute_reply.started":"2021-11-07T14:12:45.373827Z","shell.execute_reply":"2021-11-07T14:12:54.986927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n<a name='modelbuild'></a>\n## #2 :: Model Building\n\n*Model architecture, loss and optimizers definition*","metadata":{"_uuid":"4c2e466d-6e90-48f5-b1dd-044ee01f6f5a","_cell_guid":"3ce3cbde-e3c1-4057-8aff-e299109349b0","trusted":true}},{"cell_type":"markdown","source":"### Model Definition\n\ntensor size is (3, 32, 32) (channel, image width-height)\n\nthe network architecture we will using was inspired by VGG.\n\nsince it is one-class binary classification task, we use sigmoid-based activation (0,1)","metadata":{"_uuid":"9de585d1-0404-4b90-896d-b6a697cabb13","_cell_guid":"90717caf-f98c-405c-860b-08cd0b5d66ce","trusted":true}},{"cell_type":"code","source":"# DEFINE model architecture for our task\n\nclass ACIModel(nn.Module):\n    \"\"\"Model definition for Aerial Cactus Identification\"\"\"\n    def __init__(self) -> None:\n        super().__init__()\n        # inspired by VGG\n        self.features = nn.Sequential(\n                            # 1\n                            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=2),\n                            nn.ReLU(inplace=True),\n                            nn.MaxPool2d(kernel_size=2, stride=1),\n                            # 2\n                            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=2),\n                            nn.ReLU(inplace=True),\n                            nn.MaxPool2d(kernel_size=2, stride=1),\n                            # 3\n                            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=2),\n                            nn.ReLU(inplace=True),\n                            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=2),\n                            nn.ReLU(inplace=True),\n                            nn.MaxPool2d(kernel_size=2, stride=1),\n                            # 4\n                            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=2),\n                            nn.ReLU(inplace=True),\n                            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=2),\n                            nn.ReLU(inplace=True),\n                            nn.MaxPool2d(kernel_size=2, stride=1),\n                        )\n        self.avgpool = nn.AdaptiveAvgPool2d((7,7))\n        self.classifier = nn.Sequential(\n                            nn.Linear(256 * 7 * 7, 1024),\n                            nn.ReLU(inplace=True),\n                            nn.Dropout(p=0.5),\n                            nn.Linear(1024, 128),\n                            nn.ReLU(inplace=True),\n                            nn.Dropout(p=0.7),\n                            nn.Linear(128, 1),\n                            # usually the last will be BCELoss(). \n                            # but now are skipped since later will be using BCEWithLogitsLoss()\n                        )\n        \n    def forward(self, x:torch.Tensor) -> torch.Tensor:\n        \"\"\"Forward-pass input to tensors\"\"\"\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x,1)\n        x = self.classifier(x)\n        return x\n                \n#!DEFINE\n\n# model initiate and summary\nmodel = ACIModel().to(device)\nmodel, f\"running on {device}\"","metadata":{"_uuid":"36b100a0-d074-4cf4-b88d-1c17faf6396e","_cell_guid":"3564eed6-8950-4b61-a383-713993f1cee5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-11-07T14:12:54.989399Z","iopub.execute_input":"2021-11-07T14:12:54.989649Z","iopub.status.idle":"2021-11-07T14:12:55.137797Z","shell.execute_reply.started":"2021-11-07T14:12:54.989615Z","shell.execute_reply":"2021-11-07T14:12:55.137128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loss Functions and Optimizer\n\nInstead of using `nn.BCELoss()`, we use `nn.BCEWithLogitsLoss()`. It is said to be more stable, and class weight could be tweaked in order to deal with imbalanced data. \n\nAs the optimizer, we will using Stochastic Gradient Descent (`optim.SGD`)","metadata":{"_uuid":"ec795a1a-ddc8-4f7e-bfb9-85526918345c","_cell_guid":"cafd970b-6f88-4d30-834c-27b191e11648","trusted":true}},{"cell_type":"code","source":"# DEFINE loss function and optimizers\nWEIGHT_MOD = 12/3  # approx. neg/pos\nLEARNING_RATE = 0.001\n\nloss_function = nn.BCEWithLogitsLoss(reduction='mean', \n                                     pos_weight=torch.FloatTensor([WEIGHT_MOD]).to(device)\n                                    )\noptimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n#!DEFINE","metadata":{"_uuid":"80eb4aa2-dd97-4f6d-8c30-4ecc62ba2db2","_cell_guid":"ad06ec08-aa93-45cc-8fbc-6e7cade4aed5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-11-07T14:12:55.139188Z","iopub.execute_input":"2021-11-07T14:12:55.139466Z","iopub.status.idle":"2021-11-07T14:12:55.145131Z","shell.execute_reply.started":"2021-11-07T14:12:55.13943Z","shell.execute_reply":"2021-11-07T14:12:55.144156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='modeltraineval'></a>\n## #3 :: Model Training and Evaluation\n\n*Train, Validate and Measure*","metadata":{"_uuid":"bd254db4-a57a-4d41-a6d4-069cea033098","_cell_guid":"d5c3f940-a226-450e-b707-01466cc533b2","trusted":true}},{"cell_type":"markdown","source":"### Helpers\n\nSome helpers for training and validations are defined below.","metadata":{"_uuid":"8c03376b-8993-4696-914f-aac9c2b470d2","_cell_guid":"9bc7ee9f-0456-419f-bdbe-e8158765ecdc","trusted":true}},{"cell_type":"code","source":"def confusion_matrix_lin2sig(y_predict:torch.FloatTensor, y_target:torch.IntTensor) -> np.ndarray:\n    \"\"\" Calculate confusion matrix\n    Args:\n    y_predict - model output from linear layer (-inf,inf) / sigmoid unactivated,\n    y_target  - dataset label (0,1)\n        \n    Return:\n        confusion_matrix: np.ndarray - 2x2 confusion matrix of [[TN, FP]\n                                                                [FN, TP]]\n    \"\"\"\n    #  feed linear input to sigmoid function [(-inf,inf) -> (0,1)]\n    _y_predict = torch.sigmoid(y_predict.squeeze()).round()\n    _y_target = y_target.squeeze()\n    \n    assert(len(_y_predict) == len(_y_target))\n    \n    hit = _y_predict == _y_target\n    tn = ((_y_predict == 1) *  hit).sum()\n    tp = ((_y_predict == 0) *  hit).sum()\n    fn = ((_y_predict == 0) * ~hit).sum()\n    fp = ((_y_predict == 1) * ~hit).sum()\n    \n    return np.array([[tn,fp],[fn,tp]], dtype=np.intc)\n\n# test function\nconfusion_matrix_lin2sig(torch.FloatTensor([-1,-2,-3,-4, 1, 2, 3, 4, 5, 9, 8,-9,-8,-7]),\n                           torch.IntTensor([ 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1])\n                        ) # [[5, 4], [3, 2]]\n\n\ndef train(model: nn.Module, \n          train_dataloader,\n          loss_function,\n          optimizer,\n         ) -> Tuple[float, np.ndarray]:\n    \"\"\" \n    Training functions wrapper for one dataloader iteration\n    \n    Return:\n        total_loss: float              - sum of loss from loss function\n        confusion_matrix: np.ndarray   - 2x2 numpy array of confusion matrix\n    \"\"\"\n    total_loss = 0.0\n    confmat = np.zeros((2,2), dtype=np.intc)\n    \n    # train mode\n    model.train()\n    \n    # per-batches:\n    for images, labels in train_dataloader:\n        images, labels = images.to(device), labels.to(device)\n        \n        # training\n        optimizer.zero_grad()        \n        outputs = model(images).squeeze()        \n        loss = loss_function(outputs, labels.to(torch.float32))        \n        loss.backward()\n        optimizer.step()\n        \n        # store results\n        total_loss += loss.item()\n        confmat += confusion_matrix_lin2sig(outputs, labels)\n        \n    train_loss = total_loss/len(train_dataloader)\n    \n    return train_loss, confmat\n    \n\ndef validate(model: nn.Module, \n             valid_dataloader, \n             loss_function\n            ) -> Tuple[float, np.ndarray]:\n    \"\"\"\n    Validation functions wrapper for one dataloader iteration\n    \n    Return:\n        total_loss: float              - sum of loss from loss function\n        confusion_matrix: np.ndarray   - 2x2 numpy array of confusion matrix\n    \"\"\"\n    total_loss = 0.0\n    confmat = np.zeros((2,2), dtype=np.intc)\n    \n    # evaluation mode\n    model.eval()\n    \n    # per-batches:\n    for images, labels in valid_dataloader:\n        images, labels = images.to(device), labels.to(device)\n        \n        # validation\n        outputs = model(images).squeeze()\n        loss = loss_function(outputs, labels.to(torch.float32))\n\n        # store results\n        total_loss += loss.item()\n        confmat += confusion_matrix_lin2sig(outputs, labels)\n    \n    valid_loss = total_loss/len(valid_dataloader)\n    \n    return valid_loss, confmat","metadata":{"_uuid":"bdd64932-70eb-44ee-b138-5f12650681e2","_cell_guid":"e6173997-69c0-4a52-9ae4-6df0690b8c4e","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-11-07T14:12:57.714543Z","iopub.execute_input":"2021-11-07T14:12:57.715242Z","iopub.status.idle":"2021-11-07T14:12:57.749666Z","shell.execute_reply.started":"2021-11-07T14:12:57.715179Z","shell.execute_reply":"2021-11-07T14:12:57.748976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training Section\n\nDefine how much epoch we wanted, and we are ready to go.","metadata":{"_uuid":"4b640d84-e486-4d3c-b8c6-70099f92fae7","_cell_guid":"b2f2ebf6-5660-4e6f-844f-d1a882b62c2f","trusted":true}},{"cell_type":"code","source":"# DEFINE number of epochs\nEPOCHS = 100\n#!DEFINE\n\n\n# keep tracks of loss and accuracy each epochs\ntrain_losses = []\ntrain_accuracies = []\nvalid_losses = []\nvalid_accuracies = []\n\nprint(\"Running...\")\nfor epoch in range(1, EPOCHS+1):\n    train_loss, train_cm = train(model, train_dataloader, loss_function, optimizer)\n    valid_loss, valid_cm = validate(model, valid_dataloader, loss_function)\n    \n    train_accuracy = (train_cm[0,0] + train_cm[1,1]) / train_cm.sum()\n    valid_accuracy = (valid_cm[0,0] + valid_cm[1,1]) / valid_cm.sum()\n    \n    print(f\"Epoch [{epoch}/{EPOCHS}] :: \")\n    print(f\"\\tTrain Loss: {train_loss:.16f}, Accu: {train_accuracy}\")\n    print(f\"\\tValid Loss: {valid_loss:.16f}, Accu: {valid_accuracy}\")\n    \n    train_losses.append(train_loss)\n    train_accuracies.append(train_accuracy)\n    valid_losses.append(valid_loss)\n    valid_accuracies.append(valid_accuracy)\n    \nprint(\"End of training.\")","metadata":{"_uuid":"a6215f67-f55d-4fb2-9273-bffd14bf40c6","_cell_guid":"28759e6a-7b06-4b64-909d-1c927ae0af49","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-11-07T14:13:01.117821Z","iopub.execute_input":"2021-11-07T14:13:01.118279Z","iopub.status.idle":"2021-11-07T14:13:39.253682Z","shell.execute_reply.started":"2021-11-07T14:13:01.118237Z","shell.execute_reply":"2021-11-07T14:13:39.252897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Evaluation\n\nEvaluate latest trained model against validation dataset by calculating **Accuracy, Precision, Recall, F1 Score** on the following cell.","metadata":{"_uuid":"2bd40ef5-ae45-4590-a8f5-01212fe3c68f","_cell_guid":"3d37b0d0-a6a3-4d5e-a0d5-84bb29b2adef","trusted":true}},{"cell_type":"code","source":"valid_loss, valid_cm = validate(model, valid_dataloader, loss_function)\n\ntn, fp, fn, tp = tuple(valid_cm.ravel())\n\naccuracy = ( tn + tp ) / valid_cm.sum()\nprecision = tp / ( tp + fp )\nrecall = tp / ( tp + fn )\nf1_score = 2 * precision * recall / ( precision + recall )\n\nprint(f\"Confusion Matrix :\\n {valid_cm}\")\nprint(f\"Accuracy  : {accuracy}\")\nprint(f\"Precision : {precision}\")\nprint(f\"Recall    : {recall}\")\nprint(f\"F1 Score  : {f1_score}\")","metadata":{"_uuid":"4e7e8d53-cf64-4173-a1e5-ad4626c8f154","_cell_guid":"ff3720a0-cd2e-45be-81ab-7b264df121f4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-11-07T14:13:39.255431Z","iopub.execute_input":"2021-11-07T14:13:39.255689Z","iopub.status.idle":"2021-11-07T14:13:42.793695Z","shell.execute_reply.started":"2021-11-07T14:13:39.255654Z","shell.execute_reply":"2021-11-07T14:13:42.792901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualization\n\nLoss and Accuracy each epoch for both train and validation are shown below.","metadata":{"_uuid":"fda8617e-4470-441e-9737-11520572d522","_cell_guid":"04902430-4910-4d75-9089-0e3a20b703b7","trusted":true}},{"cell_type":"code","source":"plt.plot(train_losses,'-o')\nplt.plot(valid_losses,'-o')\nplt.xlabel('epoch')\nplt.ylabel('losses')\nplt.legend(['Train','Valid'])\nplt.title('Train vs Valid Losses')\n \nplt.show()","metadata":{"_uuid":"e163c20d-2ec0-4b0a-8963-e1bd8d806482","_cell_guid":"cec227de-96b7-4d37-a692-046b1ae2ec62","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-11-07T14:13:42.79508Z","iopub.execute_input":"2021-11-07T14:13:42.795563Z","iopub.status.idle":"2021-11-07T14:13:43.014792Z","shell.execute_reply.started":"2021-11-07T14:13:42.795521Z","shell.execute_reply":"2021-11-07T14:13:43.014044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(train_accuracies,'-o')\nplt.plot(valid_accuracies,'-o')\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.legend(['Train','Valid'])\nplt.title('Train vs Valid Accuracy')\n \nplt.show()","metadata":{"_uuid":"e2d0f994-5eaf-4d54-b944-281f23db2ac7","_cell_guid":"e22c5390-e8cc-4371-80c9-fabf0636ed10","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-11-07T14:13:43.016631Z","iopub.execute_input":"2021-11-07T14:13:43.017167Z","iopub.status.idle":"2021-11-07T14:13:43.231821Z","shell.execute_reply.started":"2021-11-07T14:13:43.017128Z","shell.execute_reply":"2021-11-07T14:13:43.231155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Save Trained Model","metadata":{}},{"cell_type":"code","source":"model.eval()\ntorch.save(model.state_dict(), 'aci_classifier.pth')","metadata":{"execution":{"iopub.status.busy":"2021-10-29T05:52:22.264516Z","iopub.execute_input":"2021-10-29T05:52:22.26596Z","iopub.status.idle":"2021-10-29T05:52:22.422067Z","shell.execute_reply.started":"2021-10-29T05:52:22.265921Z","shell.execute_reply":"2021-10-29T05:52:22.421308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Evaluation v2\n\nhttps://github.com/jakartaresearch/earth-vision library by Jakarta AI Research have test dataset derived from kaggle.com which have the images labelled, 3000 cactus images and 1000 no-cactus  images.\n\nWe will try to evaluate latest trained model against those dataset by calculating **Accuracy, Precision, Recall, F1 Score** below.","metadata":{"_uuid":"e90b0ecd-5d45-4aa6-ba3b-fcdde5cdefb5","_cell_guid":"68c95d35-1bb9-42fe-8e6d-1f46437b6900","trusted":true}},{"cell_type":"code","source":"# DEFINE uncomment lines below to use saved model\nSAVED_MODEL_PATH = 'aci_classifier.pth'\nmodel = ACIModel().to(device)\nmodel.load_state_dict(torch.load(SAVED_MODEL_PATH))\nmodel.eval()\nmodel","metadata":{"execution":{"iopub.status.busy":"2021-11-07T14:12:00.850654Z","iopub.execute_input":"2021-11-07T14:12:00.851119Z","iopub.status.idle":"2021-11-07T14:12:01.000286Z","shell.execute_reply.started":"2021-11-07T14:12:00.851082Z","shell.execute_reply":"2021-11-07T14:12:00.999108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q earth-vision\nimport earthvision\ntest_dataset = earthvision.datasets.AerialCactus(\n                   root='./', \n                   data_mode='validation_set',\n                    transform=transforms.Compose([\n                        transforms.Resize((32,32)),\n                        transforms.ToTensor(),\n                        lambda x: x.moveaxis(-1,1)\n                    ])\n               )\n\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=2, num_workers=0)\n\ntest_cm = np.zeros((2,2))\n\nfor image, label in test_dataloader:\n    image, label = image.to(device), label.to(device)\n    output = model(image).squeeze()\n\n    test_cm += confusion_matrix_lin2sig(output, label)\n\ntn, fp, fn, tp = tuple(test_cm.ravel())\n\naccuracy = ( tn + tp ) / test_cm.sum()\nprecision = tp / ( tp + fp )\nrecall = tp / ( tp + fn )\nf1_score = 2 * precision * recall / ( precision + recall )\n\nprint(f\"Confusion Matrix :\\n {test_cm}\")\nprint(f\"Accuracy  : {accuracy}\")\nprint(f\"Precision : {precision}\")\nprint(f\"Recall    : {recall}\")\nprint(f\"F1 Score  : {f1_score}\")","metadata":{"_uuid":"63d1a384-7d70-4a68-9355-1bb8efeb01fe","_cell_guid":"4beb9b1d-c626-4e93-a623-f79ca0ec6a4b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-11-07T14:33:30.165499Z","iopub.execute_input":"2021-11-07T14:33:30.166346Z","iopub.status.idle":"2021-11-07T14:34:13.000738Z","shell.execute_reply.started":"2021-11-07T14:33:30.166297Z","shell.execute_reply":"2021-11-07T14:34:12.999891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Submission Artifact","metadata":{}},{"cell_type":"code","source":"_submit_df = pd.read_csv(Path(ACI_DATASET_PATH) / 'sample_submission.csv')\nsubmit_df = _submit_df.copy()\n\n_submit_df = _submit_df.set_index('id')\n    \n\n# model.eval()\nwith torch.no_grad():\n    for idx in _submit_df.index:\n        img = Image.open(Path(ACI_DATASET_PATH) / 'test' / idx)\n        t_img = transforms.functional.to_tensor(img).unsqueeze(0).to(device)\n        prediction = model(t_img)\n        prediction = torch.sigmoid(prediction.squeeze()).round()\n        _submit_df.loc[idx]['has_cactus'] = prediction.cpu().numpy()\n        \n        \n_submit_df.to_csv('submission.csv')\n\n!head -10 submission.csv\n","metadata":{"execution":{"iopub.status.busy":"2021-11-07T14:39:42.45899Z","iopub.execute_input":"2021-11-07T14:39:42.459869Z","iopub.status.idle":"2021-11-07T14:39:52.913476Z","shell.execute_reply.started":"2021-11-07T14:39:42.459809Z","shell.execute_reply":"2021-11-07T14:39:52.912589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cleanups \n!rm -rf /kaggle/working/aerial-cactus-identification\n!rm -rf /kaggle/working/cactus-aerial-photos","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n<a name='summary'></a>\n## #-1 :: Summary\n\nsummary :\n\n:: Dataset \n- Train set class\n  1 : 10537, 0 : 3463, total 14000\n- Validation set class\n  1 : 2599, 0 : 901, total 3500\n- Batch size = 32\n\n:: Model\n```\nACIModel(\n   (features): Sequential(\n     (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n     (1): ReLU(inplace=True)\n     (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n     (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n     (4): ReLU(inplace=True)\n     (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n     (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n     (7): ReLU(inplace=True)\n     (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n     (9): ReLU(inplace=True)\n     (10): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n     (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n     (12): ReLU(inplace=True)\n     (13): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n     (14): ReLU(inplace=True)\n     (15): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n   )\n   (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n   (classifier): Sequential(\n     (0): Linear(in_features=12544, out_features=1024, bias=True)\n     (1): ReLU(inplace=True)\n     (2): Dropout(p=0.5, inplace=False)\n     (3): Linear(in_features=1024, out_features=128, bias=True)\n     (4): ReLU(inplace=True)\n     (5): Dropout(p=0.7, inplace=False)\n     (6): Linear(in_features=128, out_features=1, bias=True)\n   )\n```\n- Loss Function = BCEWithLogitsLoss, pos_weight = 4.0\n- Optimizer = SGD\n\n:: Result (seed = 86)\n\n- test.zip dataset\n```\nConfusion Matrix :\n [[2848.  157.]\n [ 152.  843.]]\nAccuracy  : 0.92275\nPrecision : 0.843\nRecall    : 0.8472361809045226\nF1 Score  : 0.8451127819548871\n```\n- Finish time : ~60 minutes on GPU\n\n\nnotes :\n\nWhile the documentation said the `pos_weight` value need to be negative/positive class ratio (ref [BCEWithLogitsLoss docs](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)) experiment results otherwise, positive/negative ratio.\n\n\nsome room of improvements : \n\n- use image augmentations\n```\ntransform = transforms.Compose([\n                # simulates drones from any rotation\n                # transforms.RandomRotation([360], expand=False), \n                # simulates out-of-focus \n                transforms.GaussianBlur(kernel_size=3), \n                # simulates lowlight and color shift\n                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0.3),\n                transforms.ToTensor(),\n            ])\n```\n\n- use mix of original and augmented image as dataset\n```\ntransformed_dataset = AerialCactus(ACI_DATASET_PATH, train=True, transform=transform)\nbase_dataset = torch.utils.data.ConcatDataset([base_dataset, transformed_dataset])\n```\n\n- use weight initialization on model declaration\n\n- use pretrained networks as feature layer e.g. VGG, ResNet","metadata":{"_uuid":"cd81e3f3-51f3-4aaa-8cd6-1c10d50df3e3","_cell_guid":"c2690c79-f7d0-4153-bcfe-1962f6a6a2bf","trusted":true}}]}