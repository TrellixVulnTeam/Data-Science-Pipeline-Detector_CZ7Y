{"cells":[{"metadata":{"_uuid":"17272b5234b2c59c5f5e113ca0c6f90f02326ebd"},"cell_type":"markdown","source":"### 1. Importing Modules"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#python specific modules\nimport os\nimport time\nimport copy\nimport pathlib\nfrom collections import defaultdict\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom PIL import Image","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#pytorch specific modules\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optmim\nfrom torch.optim import lr_scheduler\nfrom torchvision import models, datasets, transforms\nfrom torch.utils.data import Dataset\nfrom torch.utils.data.sampler import SubsetRandomSampler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"769f7380a9f3e5bf76eae0c0f8baf8d883501363"},"cell_type":"markdown","source":"### 2. Data Prep"},{"metadata":{"trusted":true,"_uuid":"5de02929cec58cd573c92911fe3a9a90e77a83a3"},"cell_type":"code","source":"base_path = pathlib.Path(\"../input/\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"142b6e81c2d00c194086cc90da6c960fa8fc9068"},"cell_type":"code","source":"df_data = pd.read_csv(base_path/'train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"82e77b0f67c307a3d4802cc9c33249a366744cc4"},"cell_type":"code","source":"#Custom data generator class\nclass CactusDataset(Dataset):\n    \"\"\"\n    Dataset to generate batches of multiple images and labels from a CSV file.\n    Purpose: To work with CSV files where the format is (file_name, cclass_label)\n    and generate batches of data(images, labels) on-the-fly.\n    \"\"\"\n    def __init__(self, df_data, image_path, image_size, transform=None):\n        self.data = df_data\n        self.image_path = image_path\n        self.transform = transform\n        \n    def __len__(self):\n        \"\"\"\n        Returns the no of datapoints in the dataset\n        \"\"\"\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        \"\"\"\n        Returns a batch of data given an index\n        \"\"\"\n        image_name = self.data.iloc[index, 0]\n        image = Image.open(str(self.image_path) + '/' +image_name)\n        image = image.convert('RGB')\n        image = image.resize(image_size, Image.ANTIALIAS) \n        if self.transform is not None:\n            image = self.transform(image)\n        label = self.data.iloc[index, 1]\n        label = torch.from_numpy(np.asarray(label))\n        \n        return image, label","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Defining Data Augmentation(For training)"},{"metadata":{"trusted":true,"_uuid":"cb5a5b432bcff8f81aeb9d7c5640b0710f8ee343"},"cell_type":"code","source":"train_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.ColorJitter(brightness=0.2, contrast=0.1, saturation=0.1),\n    transforms.RandomAffine(0.1),\n    transforms.RandomGrayscale(),\n    transforms.RandomRotation(10),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"62715e0aa9bbd7e3d6a098d11376e81c5f95ae37"},"cell_type":"code","source":"image_path = base_path/'train'/'train'\nimage_size = (224, 224)\nbs = 64","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Visualising Data"},{"metadata":{"trusted":true,"_uuid":"0b0b63ff3f7928846199d568377e08f52c6f5850"},"cell_type":"code","source":"cac_dataset = CactusDataset(df_data, image_path, image_size, transform=train_transform)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_loader = torch.utils.data.DataLoader(cac_dataset, batch_size=8, shuffle=True)\nimages, labels = next(iter(sample_loader))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c0ef87b1bb1276ffc29f69526d0f02339949065b"},"cell_type":"code","source":"def display_image(inp, title=None):\n    inp = inp.numpy()\n    inp = np.transpose(inp, (1,2,0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = inp * std + mean\n    inp = np.clip(inp,0,1)\n    if title is not None:\n        plt.title(title)\n    plt.figure(figsize=(32,6))\n    plt.imshow(inp)\n    plt.pause(0.001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b75406a50eb523b455156a97732bd4ff6a79b838"},"cell_type":"code","source":"out = torchvision.utils.make_grid(images, nrow=8, padding=0)\ndisplay_image(out, title=None)\nprint(labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting Data into Training and Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_data['id']\ny = df_data['has_cactus']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y,\n                                                stratify=y, \n                                                test_size=0.20, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.DataFrame({'id': X_train.values, 'has_cactus':y_train.values})\ndf_val = pd.DataFrame({'id':X_val.values, 'has_cactus':y_val.values})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cac_dataset_train = CactusDataset(df_train, image_path, image_size, transform=train_transform)\ncac_dataset_valid = CactusDataset(df_val, image_path, image_size, transform=valid_transform)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loader = torch.utils.data.DataLoader(cac_dataset_train, batch_size=32, shuffle=True)\nvalid_loader = torch.utils.data.DataLoader(cac_dataset_valid, batch_size=32, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Defining Model architecture and parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_base_model(model_name, num_classes, pretrained=True, unfreeze=True, to_gpu=True):\n    if model_name == \"resnet\":\n        model = models.resnet152(pretrained=pretrained)\n        input_features = model.fc.in_features\n        fc_custom = nn.Linear(input_features, num_classes)\n        model.fc = fc_custom\n    \n    elif model_name == \"densenet\":\n        model = models.densenet121(pretrained=pretrained)\n        input_features = model.classifier.in_features\n        fc_custom = nn.Linear(input_features, num_classes)\n        model.classifier = fc_custom\n    \n    if unfreeze:\n        for param in model.parameters():\n            param.requires_grad = True\n    \n    if to_gpu:\n        model = model.to(device)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = get_base_model(\"resnet\", 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Defining Optimizers and Loss functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optmim.SGD([\n            {'params': model.layer1.parameters(), 'lr': 1e-6},\n            {'params': model.layer2.parameters(), 'lr': 1e-5},\n            {'params': model.layer3.parameters(), 'lr': 1e-4},\n            {'params': model.layer4.parameters(), 'lr': 1e-4},\n            {'params': model.fc.parameters(), 'lr': 1e-3}\n        ], lr=1e-3)\n#optimizer = optmim.SGD(model.parameters(), lr=1e-7, momentum=0.9)\n#scheduler = lr_scheduler.StepLR(optimizer, 20, gamma=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"55a391205c531bfc143c3ac7faf1ec3d8e047a7b"},"cell_type":"code","source":"def get_auc_score(y_true, y_pred):\n    y_true = np.array([item for sublist in y_true for item in sublist])\n    y_pred = np.array([item for sublist in y_pred for item in sublist])\n    return roc_auc_score(y_true, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ### 5. Defining Training Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_epochs = 50","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"576ff6ebf4ab7b4ebb049427f78fe1a6a022277b"},"cell_type":"code","source":"def train_model(model, dataloaders, criterion, optimizer, epochs=50):\n    start_time = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    best_loss = np.Inf\n    metrics = defaultdict(list)\n    losses = defaultdict(list)\n    \n    for epoch_no in range(epochs):\n        print(\"*\" * 100)\n        print(f\"Starting epoch no {epoch_no+1}\")\n        for phase in ['train','valid']:\n            y_true = list()\n            y_pred = list()\n            \n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()\n            \n            running_loss = 0.0\n            running_corrects = 0\n            \n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                \n                optimizer.zero_grad()\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    idxs, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n                    \n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                \n                probs = torch.nn.functional.softmax(outputs, dim=1)[:, 1]\n                \n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n                \n                y_true.append(list(labels.data.cpu().numpy()))\n                y_pred.append(list(probs.detach().cpu().numpy()))\n            \n            auc_score = get_auc_score(y_true, y_pred)\n            \n            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n            epoch_acc = float(running_corrects) / len(dataloaders[phase].dataset)\n                \n            if phase == 'valid' and epoch_loss < best_loss:\n                print(f\"Validation loss decreased from {best_loss} to {epoch_loss}. Saving Model \")\n                #best_acc = epoch_acc\n                best_loss = epoch_loss\n                checkpoint = {'model': model,\n                              'state_dict': model.state_dict(),\n                              'optimizer' : optimizer.state_dict()\n                }\n                torch.save(checkpoint, 'model_resnet_50_v2.pth')\n                #torch.save(model.state_dict(), \"model_resnet_50_v1.0.pt\")\n                best_model_wts = copy.deepcopy(model.state_dict())\n                \n            print(f\"Ending epoch no {epoch_no+1} with below stats\")\n            print(f\"-------------Stats for {phase}-------------\")\n            print(f\"Loss: {epoch_loss}... Accuracy: {epoch_acc}\")\n            print(f\"AUC for {phase} is {auc_score}\")\n            \n            metrics[phase].append(epoch_acc)\n            losses[phase].append(epoch_loss)\n            \n    time_elapsed = time.time() - start_time\n    print(f\"Total time taken in training: {time_elapsed / 60} minutes\")\n    \n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model, metrics, losses\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7. Splitting Data and Doing cross validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_metrics_with_loss(metrics, losses):\n    train_metrics = metrics['train']\n    valid_metrics = metrics['valid']\n    \n    train_loss = losses['train']\n    valid_loss = losses['valid']\n    \n    x = list(range(1, n_epochs+1))\n    \n    fig = plt.figure(figsize=(15,10))\n    \n    plt.subplot(2, 2, 1)\n    plt.title('Training Loss Graph over multiple epochs')\n    plt.plot(x, train_loss)\n\n    plt.subplot(2, 2, 2)\n    plt.title('Validation Loss Graph over multiple epochs')\n    plt.plot(x, valid_loss)\n\n    plt.subplot(2, 2, 3)\n    plt.title('Training Metrics(Accuracy) Graph over multiple epochs')\n    plt.plot(x, train_metrics)\n\n    plt.subplot(2, 2, 4)\n    plt.title('Validation Metrics(Accuracy) Graph over multiple epochs')\n    plt.plot(x, valid_metrics)\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dbbb959afb14bd85dbad1762d85e30e128a7fcfd","scrolled":false},"cell_type":"code","source":"dataloaders = {\n        'train': train_loader,\n        'valid': valid_loader\n    }\nmodel, metrics, losses = train_model(model, dataloaders, criterion, optimizer)\nplot_metrics_with_loss(metrics, losses)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8. Submission"},{"metadata":{"trusted":true,"_uuid":"763adb2f428078571ce452be786f9730346d6d05"},"cell_type":"code","source":"df_submission = pd.read_csv(base_path/'sample_submission.csv')","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_model(checkpoint_path):\n    checkpoint = torch.load(checkpoint_path)\n    model = checkpoint['model']\n    model.load_state_dict(checkpoint['state_dict'])\n    model.cpu()\n    for parameter in model.parameters():\n        parameter.requires_grad = False\n\n    model.eval()\n    return model","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = load_model('model_resnet_50_v2.pth')","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_image_path = base_path / 'test' / 'test'","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_full_path(file_name):\n    full_path = str(test_image_path) + '/' + file_name\n    return full_path","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission['id'] = df_submission['id'].apply(add_full_path)","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_image(image_path):\n    image = Image.open(image_path)\n    image = image.convert('RGB')\n    image = image.resize(image_size, Image.ANTIALIAS)\n    return image","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n])","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_predictions(image_path):\n    image = read_image(image_path)\n    image = test_transform(image)\n    image = torch.unsqueeze(image, 0)\n    pred = model(image)\n    p_cactus = torch.nn.functional.softmax(pred, dim=1)[0][1].item()\n    return p_cactus\n","execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission['has_cactus'] = df_submission['id'].apply(get_predictions)","execution_count":37,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission.head()","execution_count":38,"outputs":[{"output_type":"execute_result","execution_count":38,"data":{"text/plain":"                                                  id  has_cactus\n0  ../input/test/test/000940378805c44108d287872b2...    0.990773\n1  ../input/test/test/0017242f54ececa4512b4d7937d...    0.999978\n2  ../input/test/test/001ee6d8564003107853118ab87...    0.056432\n3  ../input/test/test/002e175c3c1e060769475f52182...    0.000041\n4  ../input/test/test/0036e44a7e8f7218e9bc7bf8137...    0.990270","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>has_cactus</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>../input/test/test/000940378805c44108d287872b2...</td>\n      <td>0.990773</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>../input/test/test/0017242f54ececa4512b4d7937d...</td>\n      <td>0.999978</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>../input/test/test/001ee6d8564003107853118ab87...</td>\n      <td>0.056432</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>../input/test/test/002e175c3c1e060769475f52182...</td>\n      <td>0.000041</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>../input/test/test/0036e44a7e8f7218e9bc7bf8137...</td>\n      <td>0.990270</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_full_path(image_path):\n    paths = image_path.split(\"/\")\n    path = paths[4]\n    return path","execution_count":39,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission['id'] = df_submission['id'].apply(remove_full_path)","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission.head()","execution_count":41,"outputs":[{"output_type":"execute_result","execution_count":41,"data":{"text/plain":"                                     id  has_cactus\n0  000940378805c44108d287872b2f04ce.jpg    0.990773\n1  0017242f54ececa4512b4d7937d1e21e.jpg    0.999978\n2  001ee6d8564003107853118ab87df407.jpg    0.056432\n3  002e175c3c1e060769475f52182583d0.jpg    0.000041\n4  0036e44a7e8f7218e9bc7bf8137e4943.jpg    0.990270","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>has_cactus</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000940378805c44108d287872b2f04ce.jpg</td>\n      <td>0.990773</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0017242f54ececa4512b4d7937d1e21e.jpg</td>\n      <td>0.999978</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>001ee6d8564003107853118ab87df407.jpg</td>\n      <td>0.056432</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>002e175c3c1e060769475f52182583d0.jpg</td>\n      <td>0.000041</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0036e44a7e8f7218e9bc7bf8137e4943.jpg</td>\n      <td>0.990270</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission.to_csv('submission_resnet50_unfreeze_1.csv', index=None)","execution_count":42,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}