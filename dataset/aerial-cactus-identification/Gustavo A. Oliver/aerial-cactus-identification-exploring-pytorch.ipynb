{"cells":[{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Exploring PyTorch\nThis baseline approach comes from this `PyTorch` [tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py). I purposely did not follow some of the other winning approaches that use `fastai` because then I wouldn't learn anything. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport torch\nimport torchvision\nimport warnings\nimport shutil\n\n%matplotlib inline\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Split files into sub-directories with classes, to use `PyTorch` helper functions"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"image_cat = pd.read_csv(\"../input/train.csv\", low_memory=False, index_col=\"id\").to_dict()[\"has_cactus\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"PyTorch's helper methods expect the data directories to be broken into this structure:\n\n`> train`<br>\n&nbsp;&nbsp;&nbsp;&nbsp;`> cactus`<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`123.jpg`<br>\n&nbsp;&nbsp;&nbsp;&nbsp;`> noncactus`<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`456.jpg`<br>\n\nUnfortunately, `Kaggle` doesn't like this when you're submitting, so it's best to work around it somehow. \n\nTaking a look at PyTorch's `torchvision.datasets.ImageFolder` should give us an idea of what to do:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ??torchvision.datasets.ImageFolder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchvision.datasets import DatasetFolder, ImageFolder\nfrom torchvision.datasets.folder import IMG_EXTENSIONS, default_loader\n\ndef make_dataset(dir, class_to_idx, extensions=None, is_valid_file=None):\n    images = []\n    dir = os.path.expanduser(dir)\n    \n    for filename in os.listdir(dir):\n        path = os.path.join(dir, filename)\n        item = (path, image_cat[filename])\n        images.append(item)\n    return images\n\nclass CactusImageFolder(DatasetFolder):\n    def __init__(self, root, transform=None, target_transform=None, loader=default_loader, is_valid_file=None):                                                                                              \n        self.root = root\n        self.transform = transform\n        self.target_transform = target_transform\n        self.classes, self.class_to_idx = self._find_classes(self.root)\n        self.samples = make_dataset(self.root, self.class_to_idx)\n        self.loader = loader\n        self.targets = [s[1] for s in self.samples]\n        \n    def _find_classes(self, dir):\n        def f(x):\n            if x in image_cat.keys():\n                return \"cactus\" if image_cat[x] == 1 else \"noncactus\"\n            else:\n                return \"\"\n        classes = list(set([ f(filename) for filename in os.listdir(dir)]))\n        class_to_idx = {classes[i]: i for i in range(len(classes))}\n        return classes, class_to_idx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchvision import transforms, datasets\nfrom torch.utils.data.sampler import SubsetRandomSampler\n\nBATCH = 10\ndata_transorm = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ntrain_set = CactusImageFolder(root=\"../input/train/train\", transform=data_transorm)\nindices = list(range(0, len(image_cat)))\nsplit = int(len(indices) * 0.2)\nval_idx, train_idx = indices[:split], indices[split:]\nval_sampler, train_sampler = SubsetRandomSampler(val_idx), SubsetRandomSampler(train_idx)\n\ntrain_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH, sampler=train_sampler)\nval_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH, sampler=val_sampler)\n\ntest_set = datasets.ImageFolder(root=\"../input/test\", transform=data_transorm)\ntest_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH)\n\nclasses = train_set.classes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def imshow(img):\n    img = img / 2 + 0.5\n    npimg = img.numpy()\n    plt.figure(figsize=(5, 5))\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n    \ndata_iter = iter(train_loader)\nimages, labels = data_iter.next()\nimshow(torchvision.utils.make_grid(images, nrow=int(BATCH/2)))\nprint(' '.join('%5s' % classes[labels[j]] for j in range(BATCH)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\nnet = Net()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(net.parameters(), lr=0.001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for epoch in range(3):  # loop over the dataset multiple times\n\n    running_loss = 0.0\n    for i, data in enumerate(train_loader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n        if i % 1000 == 0 and i != 0:    # print every 1000 mini-batches\n            print('[%d, %5d] loss: %.3f' %\n                  (epoch + 1, i + 1, running_loss / 1000))\n            running_loss = 0.0\n\nprint('Finished Training')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Take a few of examples to see how it was classified. "},{"metadata":{"trusted":true},"cell_type":"code","source":"test_iter = iter(test_loader)\nimages, _ = test_iter.next()\nimshow(torchvision.utils.make_grid(images, nrow=int(BATCH/2)))\n\noutputs = net(images)\n_, predicted = torch.max(outputs, 1)\n\nprint('Predicted: ', ' '.join('%5s' % classes[predicted[j]] for j in range(BATCH)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\nnet.to(device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How does this perform on the validation set? "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\npreds = []\nscore = 0\n\nwith torch.no_grad():\n    for i, data in enumerate(val_loader):\n        file_names = [os.path.basename(name) for name, _ in test_set.imgs[i*BATCH:i*BATCH+BATCH]]\n        images, labels = data\n        outputs = net(images)\n        _, predicted = torch.max(outputs.data, 1)\n        preds += list(zip(file_names, predicted.numpy()))\n        score += (labels == predicted).sum()\nprint((score.item() * 1.0) / len(preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.max(outputs.data, 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have a model, we can start making predictions!"},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = []\n\nwith torch.no_grad():\n    for i, data in enumerate(test_loader):\n        file_names = [os.path.basename(name) for name, _ in test_loader.dataset.imgs[i*BATCH:i*BATCH+BATCH]]\n        images, _ = data\n        outputs = net(images)\n        _, predicted = torch.max(outputs.data, 1)\n        preds += list(zip(file_names, predicted.numpy()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!head ../input/sample_submission.csv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = pd.DataFrame(preds, columns=[\"id\", \"has_cactus\"])\noutput.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output[\"has_cactus\"].sum() / len(output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rows, cols = 10, 5\ni = 0\nfig, ax = plt.subplots(nrows=rows, ncols=cols, squeeze=False, figsize=(40, 40))\nfig.subplots_adjust(hspace=0.5, wspace=0.5)\nfor name, label in output.iloc[:50].values:    \n    img = plt.imread(f\"../input/test/test/{name}\")\n    row = int(i/cols)\n    col = i%cols\n    ax[row][col].imshow(img)\n    ax[row][col].title.set_text(f\"Cactus? {label == 1}\")\n    i += 1","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}