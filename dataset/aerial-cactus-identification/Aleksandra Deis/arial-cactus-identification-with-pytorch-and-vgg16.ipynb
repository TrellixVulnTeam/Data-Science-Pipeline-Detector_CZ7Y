{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# General libraries imports\nimport pandas as pd\nimport numpy as np\nimport os\nimport time\n\n# import image manipulation\nfrom PIL import Image\n\n# import matplotlib for visualization\nfrom matplotlib.pyplot import imshow\nimport matplotlib.pyplot as plt\n\n# Import PyTorch\nimport torch\nfrom torch import nn\nfrom torch import optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import Dataset, DataLoader","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Arial Cactus Identification with PyTorch and VGG16"},{"metadata":{},"cell_type":"markdown","source":"## Introduction\n\nThe goal of the [Aerial Cactus Identification competition](https://www.kaggle.com/c/aerial-cactus-identification) is to create an algorithm that can identify a specific type of cactus in aerial imagery to advance a system for autonomous surveillance of protected areas.\n\n<br>In this competition, we are given a dataset with labeled 32 x 32 images, which contain aerial photos of a columnar cactus. The task is to build an algorithm, which predicts whether there is a cactus on the image. This is a classification problem. Since we have a lot of data, deep learning models will suit well for this problem.\n\n<br>In this analysis I used the code from this [Kaggle kernel](https://www.kaggle.com/atrisaxena/pytorch-simple-model-iscactus-classification)."},{"metadata":{},"cell_type":"markdown","source":"## Explore Data\n\nLet's explore the dataset given:\n* How many records are there in train and test datasets?\n* How many images are there with/without cactus?\n* Visualize images with and without cactus."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Set path to folders containing the data\nTRAIN_IMG_PATH = \"../input/train/train/\"\nTEST_IMG_PATH = \"../input/test/test/\"\nLABELS_CSV_PATH = \"../input/train.csv\"\nSAMPLE_SUB_PATH = \"../input/sample_submission.csv\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`1` Number of images in train and test datasets:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# read the csv with labels for train dataset\npd_train = pd.read_csv(LABELS_CSV_PATH)\n\n# count the number or rows\ntrain_images = len(pd_train)\n\nprint(\"The number of images in train dataset is {}.\".format(train_images))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# count the number of images in test dataset\ntest_images = len([f for f in os.listdir(TEST_IMG_PATH) if os.path.isfile(os.path.join(TEST_IMG_PATH, f))])\n\nprint(\"The number of images in test dataset is {}.\".format(test_images))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Plot pie chart\nlabels = 'Train', 'Test'\nsizes = [train_images, test_images]\nexplode = (0, 0.1)  # \"explode\" the 2nd slice\n\nfig, ax = plt.subplots()\nax.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\nax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\nplt.title(\"Number of images in training dataset\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`2` Number of images with and without cactus in train dataset:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate the number of images with and without cactus\nwith_cactus_num = pd_train[pd_train['has_cactus'] == 1].has_cactus.count()\nno_cactus_num = pd_train[pd_train['has_cactus'] == 0].has_cactus.count()\n\nprint(\"The number of images with cactus in train dataset is {}.\".format(with_cactus_num))\nprint(\"The number of images without cactus in train dataset is {}.\".format(no_cactus_num))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Plot pie chart\nlabels = 'Cactus', 'No cactus'\nsizes = [with_cactus_num, no_cactus_num]\nexplode = (0, 0.1)  # \"explode\" the 2nd slice\n\nfig, ax = plt.subplots()\nax.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\nax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\nplt.title(\"Number of images with/without cactus\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`3` View examples of images with cactus and without cactus:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# get array of image filenames with and without cactus\nhas_cactus = pd_train[pd_train['has_cactus'] == 1][:9].id.values\nno_cactus = pd_train[pd_train['has_cactus'] == 0][:9].id.values","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def view_cactus(cactus_images, title = ''):\n    \"\"\"\n    Function to plot grid with several examples of images.\n    INPUT:\n        cactus_images - array with filenames for images\n\n    OUTPUT: None\n    \"\"\"\n    fig, axs = plt.subplots(3, 3, figsize=(7,7))\n    \n    for im in range(0,9):\n        # open image\n        image = Image.open(os.path.join(TRAIN_IMG_PATH,cactus_images[im]))\n        i = im // 3\n        j = im % 3\n        axs[i,j].imshow(image) #plot the data\n        axs[i,j].axis('off')\n\n    # set suptitle\n    plt.suptitle(title)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"view_cactus(has_cactus, title = 'Images with cactus')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"view_cactus(no_cactus, title = 'Images without cactus')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelling"},{"metadata":{},"cell_type":"markdown","source":"I decided to use VGG-16 pretrained model (see [this article](https://hackernoon.com/learning-keras-by-implementing-vgg16-from-scratch-d036733f2d5) for further reading) to classify the images."},{"metadata":{},"cell_type":"markdown","source":"`1` Load the dataset:"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Define the dataset\nclass CactusDataset(Dataset):\n    \"\"\"Cactus identification dataset.\"\"\"\n\n    def __init__(self, img_dir, dataframe, transform=None):\n        \"\"\"\n        Args:\n            img_dir (string): Directory with all the images.        \n            dataframe (pandas.core.frame.DataFrame): Pandas dataframe obtained\n                by read_csv().\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.labels_frame = dataframe\n        self.img_dir = img_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.labels_frame)\n\n    def __getitem__(self, idx):\n        img_name = os.path.join(self.img_dir, self.labels_frame.id[idx]) \n        image = Image.open(img_name)\n        label = self.labels_frame.has_cactus[idx]\n\n        if self.transform:\n            image = self.transform(image)\n\n        return [image, label] \n    \n    \n# define train transformations \ntrain_transforms = transforms.Compose([transforms.RandomRotation(30),\n                                       transforms.RandomResizedCrop(224),\n                                       transforms.RandomHorizontalFlip(),\n                                       transforms.ToTensor(),\n                                       transforms.Normalize([0.485, 0.456, 0.406], \n                                                            [0.229, 0.224, 0.225])])\n# define test transformations \ntest_transforms = transforms.Compose([transforms.Resize(256),\n                                      transforms.CenterCrop(224),\n                                      transforms.ToTensor(),\n                                      transforms.Normalize([0.485, 0.456, 0.406], \n                                                           [0.229, 0.224, 0.225])])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"dframe = pd.read_csv(LABELS_CSV_PATH)\ncut = int(len(dframe)*0.95)\ntrain, test = np.split(dframe, [cut], axis=0)\ntest = test.reset_index(drop=True)\n\ntrain_ds = CactusDataset(TRAIN_IMG_PATH, train, train_transforms)\ntest_ds = CactusDataset(TRAIN_IMG_PATH, test, test_transforms)\ndatasets = {\"train\": train_ds, \"val\": test_ds}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainloader = DataLoader(train_ds, batch_size=32,\n                        shuffle=True, num_workers=0)\n\ntestloader = DataLoader(test_ds, batch_size=4,\n                        shuffle=True, num_workers=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`2` Setup hyperparameters:"},{"metadata":{},"cell_type":"markdown","source":"_Don't forget to enable GPU in kernel settings to run the model on GPU._"},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 20\nbatch_size = 128\nlearning_rate = 0.003\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\ndevice","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`3` Load pretrained VGG-16 model:"},{"metadata":{},"cell_type":"markdown","source":"_Don't forget to enable Internet in kernel settings to download the model._"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"model = models.vgg16(pretrained=True)\nmodel","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`4` Train the model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# freeze parameters\nfor param in model.parameters():\n    param.requires_grad = False\n\n# add layers to train\nfrom collections import OrderedDict\nclassifier = nn.Sequential(OrderedDict([\n                          ('fc1', nn.Linear(25088, 12000)),\n                          ('dr1', nn.Dropout(p = 0.3)),\n                          ('bn1', nn.BatchNorm1d(num_features=12000)),\n                          ('relu1', nn.ReLU()),\n                          ('fc2', nn.Linear(12000, 1000)),\n                          ('dr2', nn.Dropout(p = 0.3)),\n                          ('bn2', nn.BatchNorm1d(num_features=1000)),\n                          ('relu2', nn.ReLU()),\n                          ('fc3', nn.Linear(1000, 102)),\n                          ('output', nn.LogSoftmax(dim = 1))\n                          ]))\n    \nmodel.classifier = classifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set loss function\ncriterion = nn.NLLLoss()\n\n# set optimizer, only train the classifier parameters, feature parameters are frozen\noptimizer = optim.Adam(model.classifier.parameters(), lr=learning_rate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# train the model\nmodel.to(device)\n\nsteps = 0\nrunning_loss = 0\nprint_every = 5\nfor epoch in range(epochs):\n    for inputs, labels in trainloader:\n        steps += 1\n        # Move input and label tensors to the default device\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        \n        logps = model.forward(inputs)\n        loss = criterion(logps, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        \n        if steps % print_every == 0:\n            test_loss = 0\n            accuracy = 0\n            model.eval()\n            with torch.no_grad():\n                for inputs, labels in testloader:\n                    inputs, labels = inputs.to(device), labels.to(device)\n                    logps = model.forward(inputs)\n                    batch_loss = criterion(logps, labels)\n                    \n                    test_loss += batch_loss.item()\n                    \n                    # Calculate accuracy\n                    ps = torch.exp(logps)\n                    top_p, top_class = ps.topk(1, dim=1)\n                    equals = top_class == labels.view(*top_class.shape)\n                    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n                    \n            print(f\"Epoch {epoch+1}/{epochs}.. \"\n                  f\"Train loss: {running_loss/print_every:.3f}.. \"\n                  f\"Test loss: {test_loss/len(testloader):.3f}.. \"\n                  f\"Test accuracy: {accuracy/len(testloader):.3f}\")\n            running_loss = 0\n            model.train()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`5` Create submission file:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"submission_df = pd.read_csv(SAMPLE_SUB_PATH)\noutput_df = pd.DataFrame(index=submission_df.index, columns=submission_df.keys() )\noutput_df['id'] = submission_df['id']\nsubmission_df['target'] =  [0] * len(submission_df)\n\ntdata_transform = transforms.Compose([transforms.Resize(256),\n                    transforms.CenterCrop(224),\n                    transforms.ToTensor(),\n                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n\nsubmission_ds = CactusDataset(TEST_IMG_PATH, submission_df, tdata_transform)\n\nsub_loader = DataLoader(submission_ds, batch_size=1,\n                        shuffle=False, num_workers=0)\n\n\ndef test_sumission(model):\n    since = time.time()\n    sub_outputs = []\n    model.train(False)  # Set model to evaluate mode\n    # Iterate over data.\n    prediction = []\n    for data in sub_loader:\n        # get the inputs\n        inputs, labels = data\n\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        # forward\n        outputs = model(inputs)\n        _, pred = torch.max(outputs.data, 1)\n        prediction.append(int(pred))\n      \n    time_elapsed = time.time() - since\n    print('Run complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed // 60, time_elapsed % 60))\n\n    return prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('../input/sample_submission.csv')\nsub['has_cactus'] = test_sumission(model)\nsub.to_csv('submission1.csv', index= False)\n\nsub.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}