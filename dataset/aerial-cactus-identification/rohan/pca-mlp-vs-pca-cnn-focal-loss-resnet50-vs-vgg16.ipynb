{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":30,"outputs":[{"output_type":"stream","text":"['test', 'sample_submission.csv', 'train.csv', 'train']\n","name":"stdout"}]},{"metadata":{"_uuid":"9e164b140a63b2633c87dd13726e572eda474a5c"},"cell_type":"markdown","source":"# Introduction\nIn this short notebook I wanted to experiment with PCA and neural network implemented with Keras library.\nAlso I would like to ilustrate how PCA decomposition works on data from this dataset. Most important thing, I would like hopefully to get some feedback to improve this method or find completely different approach.\n\nThanks to other authors for publishing their notebooks, I reused some parts of the code when I was looking for fiding nice solutions for problems I had on a way. "},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# let's first start analyzing the data how much classes we are dealing , data is imbalance or not "},{"metadata":{"trusted":true,"_uuid":"1949163e1275ba6307e9674fe1d0461526d9801c"},"cell_type":"code","source":"train_df=pd.read_csv(\"../input/train.csv\")","execution_count":31,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6722aa88cb89bc0ca41244455ba13d746b2d1810"},"cell_type":"code","source":"train_df.shape","execution_count":32,"outputs":[{"output_type":"execute_result","execution_count":32,"data":{"text/plain":"(17500, 2)"},"metadata":{}}]},{"metadata":{"trusted":true,"_uuid":"a42f32458b5d060ccaf9d17d142155870566cb4e"},"cell_type":"code","source":"train_df.head()","execution_count":33,"outputs":[{"output_type":"execute_result","execution_count":33,"data":{"text/plain":"                                     id  has_cactus\n0  0004be2cfeaba1c0361d39e2b000257b.jpg           1\n1  000c8a36845c0208e833c79c1bffedd1.jpg           1\n2  000d1e9a533f62e55c289303b072733d.jpg           1\n3  0011485b40695e9138e92d0b3fb55128.jpg           1\n4  0014d7a11e90b62848904c1418fc8cf2.jpg           1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>has_cactus</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0004be2cfeaba1c0361d39e2b000257b.jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000c8a36845c0208e833c79c1bffedd1.jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000d1e9a533f62e55c289303b072733d.jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0011485b40695e9138e92d0b3fb55128.jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0014d7a11e90b62848904c1418fc8cf2.jpg</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true,"_uuid":"869dd6221820f598ff70c6e3adf9ae586e3d32f4"},"cell_type":"code","source":"import seaborn as sns \nimport matplotlib.pyplot as plt","execution_count":34,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5fca364d05cd840560c6d8701812c5976b72bf26"},"cell_type":"code","source":"sns.countplot(train_df[\"has_cactus\"])","execution_count":35,"outputs":[{"output_type":"execute_result","execution_count":35,"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x7fdf25661470>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"metadata":{"_uuid":"9c1d6072964ca484e1a32696bb05665f287de615"},"cell_type":"markdown","source":"# so again dealing with imbalance classifacition we have to use some dice loss or focal loss or in keras we have to use logits_v2 with categorical cross entropy "},{"metadata":{"trusted":true,"_uuid":"84e59d0a2d447280e9fa188d561bf99384851da4"},"cell_type":"code","source":"import cv2\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport json\nimport os\nfrom tqdm import tqdm, tqdm_notebook\nfrom keras.models import Sequential\nfrom keras.layers import Activation, Dropout, Flatten, Dense\nfrom keras.applications import VGG16\nfrom keras.optimizers import Adam","execution_count":36,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc48178e40011cb0eedef3d0a077780762025d3c"},"cell_type":"code","source":"train_dir = \"../input/train/train/\"\ntest_dir = \"../input/test/test/\"","execution_count":37,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7f340555c9dd0b55bf19d3af39763d787306c93"},"cell_type":"code","source":"# X_tr = []\n# Y_tr = []\n# imges = train_df['id'].values\n# for img_id in tqdm_notebook(imges):\n#     X_tr.append(cv2.imread(train_dir + img_id,0))    \n#     Y_tr.append(train_df[train_df['id'] == img_id]['has_cactus'].values[0])  \n# X_tr = np.asarray(X_tr)\n# X_tr = X_tr.astype('float32')\n# X_tr /= 255\n# Y_tr = np.asarray(Y_tr)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1affc026c37b3e218195d7a6fe04f48ac2a65d10"},"cell_type":"code","source":"# X_tr=X_tr.reshape(-1,32,32,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c8407daee8b1762ff7fadbad7284338632234001"},"cell_type":"code","source":"# Y_tr.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f574413a03bcec1a3b6c8bd503a317300f112d3"},"cell_type":"markdown","source":"# First for applying PCA we need to convert rgb images to grayscale , same as well for test images we have to do "},{"metadata":{"_uuid":"9bfa0693a986207abb9229500adccf9f2c624e9a"},"cell_type":"markdown","source":"Image has 32x32 pixels, so data has 1024 features. Now for computers such amount of data isn't that big, but there can be cases, when dimensional reduction can be important. It is better for further processing to have 625 features containing most of the data than 1000 features.\n\nBefore I use PCA to reduce dimensionality of the data I will standardize it using sklearn StandartScaler. It is fitted to train data, because I assume that I know nothing about test data. Both datasets are transformed. "},{"metadata":{"trusted":true,"_uuid":"e5e6698f105f715e56bb511b9e72bb333358f90b"},"cell_type":"code","source":"# X_tr.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a77f58ee3f4a888f516765af13fe4bb7b124b92"},"cell_type":"markdown","source":"# so now we need pixel wise values after that only we can apply PCA to see that with less features can we get same accuracy"},{"metadata":{"trusted":true,"_uuid":"ff3f1918e6efbe67c342eae661280c85f003ee80"},"cell_type":"code","source":"# target=train_df[\"has_cactus\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"91e1250b320e448a8fe12940224e8cc3a5c0bab4"},"cell_type":"code","source":"# train_df=train_df.drop(\"has_cactus\",axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6576d68bce7b44ab0214c348d4487653cd18b77e"},"cell_type":"code","source":"# import os,array\n# import pandas as pd\n# import time\n# import dask as dd\n\n# from PIL import Image\n# def pixelconv(file_list,img_height,img_width,pixels):  \n#     columnNames = list()\n\n#     for i in range(pixels):\n#         pixel = 'pixel'\n#         pixel += str(i)\n#         columnNames.append(pixel)\n\n\n#     train_data = pd.DataFrame(columns = columnNames)\n#     start_time = time.time()\n#     for i in tqdm_notebook(file_list):\n#         t = i\n#         img_name = t\n#         img = Image.open('../input/train/train/'+img_name)\n#         rawData = img.load()\n#         #print rawData\n#         data = []\n#         for y in range(img_height):\n#             for x in range(img_width):\n#                 data.append(rawData[x,y][0])\n#         print (i)\n#         k = 0\n#         #print data\n#         train_data.loc[i] = [data[k] for k in range(pixels)]\n#     #print train_data.loc[0]\n\n#     print (\"Done pixel values conversion\")\n#     print  (time.time()-start_time)\n#     print (train_data)\n#     train_data.to_csv(\"train_converted_new.csv\",index = False)\n#     print (\"Done data frame conversion\")\n#     print  (time.time()-start_time)\n# pixelconv(train_df.id,32,32,1024) # pass pandas dataframe in which path of images only as column\n#                                     # in return csv file will save in working directory ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"acbba43031d7a254bc5b6ab5a3958db3ea9592b3"},"cell_type":"code","source":"# new_data=pd.read_csv(\"../working/train_converted_new.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c449649a1722bf9ce2ff3448dbdff37e94cd97c6"},"cell_type":"code","source":"# new_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eca1940b0cfa8d6e72c13248e681e62317272ab9"},"cell_type":"markdown","source":"# so finally we have dataset of pixel values now we can apply PCA with pixeld data "},{"metadata":{"trusted":true,"_uuid":"d11ce702355110bcde7240582936f90db188d974"},"cell_type":"code","source":"# import numpy as np\n# import pandas as pd\n\n# from sklearn.preprocessing import StandardScaler\n# from sklearn.decomposition import PCA\n\n# from keras.models import Sequential\n# from keras.utils import np_utils\n# from keras.layers import Dense, Dropout, GaussianNoise, Conv1D\n# from keras.preprocessing.image import ImageDataGenerator\n\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n# %matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e68368e47433683e582215f87deb2b1fbe5cc33"},"cell_type":"code","source":"\n# pca = PCA(n_components=500)\n# pca.fit(new_data)\n\n# plt.plot(np.cumsum(pca.explained_variance_ratio_))\n# plt.xlabel('Number of components')\n# plt.ylabel('Cumulative explained variance')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"70e1d503513645df1694b11d37a5ee238e188ee4"},"cell_type":"markdown","source":"In plot above we can see that cumulative explained variance is very high near 500 and then it increases very slowly. That means that data describing changes is mostly contained in i guess 625  components. We need to evaluate trade-offs before we choose number of components we use further. I choose 625 to check how it will work as it seems to have most of the data."},{"metadata":{"_uuid":"85b713e16a703f5982bd15656f5b4c6daba96283"},"cell_type":"markdown","source":"# so we come to know that with less number features we can explain the dataset "},{"metadata":{"trusted":true,"_uuid":"18aca15db6d9e04c90495ffee0b8b96267b3c154"},"cell_type":"code","source":"# NCOMPONENTS = 625\n\n# pca = PCA(n_components=NCOMPONENTS)\n# X_pca_train = pca.fit_transform(new_data)\n# pca_std = np.std(X_pca_train)\n# print(X_pca_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16b0fa7350bb168574ff2e8b94683f5e1df647ee"},"cell_type":"code","source":"# inv_pca = pca.inverse_transform(X_pca_train)\n# #inv_sc = scaler.inverse_transform(inv_pca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bbcf1668b6225e1085cda94afa741d20e38a93c9"},"cell_type":"code","source":"# X_pca_train_new=X_pca_train.reshape(X_pca_train.shape[0],25,25,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db97fc9724dc08e416e4674b8ab87be400acc0de"},"cell_type":"code","source":"# X_pca_train_new.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5039f40304013584ec7fd30c6aa2dd6b0994da70"},"cell_type":"markdown","source":"# Apply MLP With PCA"},{"metadata":{"trusted":true,"_uuid":"48a2ed75021c816ab61f5e04acd32c687df9693e"},"cell_type":"code","source":"# X_pca_train.shape ### this shape will be used in MLP","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"70eaa137abb48fdbfaa5de64eee19d63afb956e3"},"cell_type":"markdown","source":"# Neural network with Keras\n\nI implemened simple model of multilayer perceptron (MLP) neural network using Keras and experimented with it.\n\nUsing library is simple. First you need to create model instance and then add layers using models.add() method. First layer need to be set up for proper input dimension. Output layer needs to have proper output dimension and activation function. In between hidden layers can be added.\n\nDuring compilation parameters of loss function, optimizer and metrics need to be set depanding on problem."},{"metadata":{"trusted":true,"_uuid":"6a822322c9e8b68f9eb46c4beadbb8eefb421453"},"cell_type":"code","source":"# import keras\n# model = Sequential()\n# layers = 1\n# units = 128\n\n# model.add(Dense(units, input_dim=NCOMPONENTS, activation='relu'))\n# model.add(GaussianNoise(pca_std))\n# model.add(Dense(units, activation='relu'))\n# model.add(GaussianNoise(pca_std))\n# model.add(Dropout(0.1))\n# model.add(Dense(1, activation='sigmoid'))\n\n# model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr=1e-5), metrics=['acc'])\n# history = model.fit(X_pca_train,target,\n#           batch_size=32,\n#           epochs=10,\n#           verbose=1,\n#           validation_split=0.15)\n\n# #model.fit(X_pca_train, Y_train, epochs=100, batch_size=256, validation_split=0.15, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9252239d44aab17bdac7b23facc9583da943cfa6"},"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# %matplotlib inline\n# accuracy = history.history['acc']\n# val_accuracy = history.history['val_acc']\n# loss = history.history['loss']\n# val_loss = history.history['val_loss']\n# epochs = range(len(accuracy))\n# plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n# plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n# plt.title('Training and validation accuracy')\n# plt.legend()\n# plt.figure()\n# plt.plot(epochs, loss, 'bo', label='Training loss')\n# plt.plot(epochs, val_loss, 'b', label='Validation loss')\n# plt.title('Training and validation loss')\n# plt.legend()\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d96f99686e9aae16b3d782ff1aa3924794f6614f"},"cell_type":"markdown","source":"# Applied CNN with PCA"},{"metadata":{"trusted":true,"_uuid":"7d9daeea460bdec2f813c76cf67534cea9027dca"},"cell_type":"code","source":"# import keras\n# from keras.models import Sequential\n# from keras.layers import Dense, Dropout, Flatten\n# from keras.layers import Conv2D, MaxPooling2D\n# from keras.layers.normalization import BatchNormalization\n\n# batch_size = 256\n# num_classes = 1\n# epochs = 200\n\n# #input image dimensions\n# img_rows, img_cols = 25, 25\n\n# model = Sequential()\n# model.add(Conv2D(64, kernel_size=(3, 3),\n#                  activation='relu',\n#                  input_shape=(25,25,1)))\n# model.add(MaxPooling2D((2, 2)))\n# model.add(Dropout(0.1))\n# model.add(Conv2D(128, (3, 3), activation='relu'))\n# model.add(Dropout(0.2))\n# model.add(Flatten())\n# model.add(Dense(128, activation='relu'))\n# model.add(Dropout(0.1))\n# model.add(Dense(num_classes, activation='sigmoid'))\n\n# model.compile(loss=keras.losses.binary_crossentropy,\n#               optimizer=keras.optimizers.Adam(lr=1e-5),\n#               metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fce8e576e2109e0b44bcc576f9d7eb2438d6c2e9","scrolled":false},"cell_type":"code","source":"# history1 = model.fit(X_pca_train_new,target,\n#           batch_size=batch_size,\n#           epochs=200,\n# #           verbose=1,\n#           validation_split=0.15)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"76606df06b68f9a43e6e4dee019be36ab95430ef"},"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# %matplotlib inline\n# accuracy = history1.history['acc']\n# val_accuracy = history1.history['val_acc']\n# loss = history1.history['loss']\n# val_loss = history1.history['val_loss']\n# epochs = range(len(accuracy))\n# plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n# plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n# plt.title('CNN result Training and validation accuracy')\n# plt.legend()\n# plt.figure()\n# plt.plot(epochs, loss, 'bo', label='Training loss')\n# plt.plot(epochs, val_loss, 'b', label='Validation loss')\n# plt.title('cnn Training and validation loss')\n# plt.legend()\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea9234a87688b07f134e1061fff2a54916a94ef4"},"cell_type":"markdown","source":"# so we have pca with MLP as well as with CNN"},{"metadata":{"trusted":true,"_uuid":"f2c8db8a896ae7b3787e8747c961bd74673adbd3"},"cell_type":"code","source":"# %%time\n# X_tst = []\n# Test_imgs = []\n# for img_id in tqdm_notebook(os.listdir(test_dir)):\n#     X_tst.append(cv2.imread(test_dir + img_id,0))     \n#     Test_imgs.append(img_id)\n# X_tst = np.asarray(X_tst)\n# X_tst = X_tst.astype('float32')\n# X_tst /= 255","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba01aadf8b4d8ff4134a70df68c5720767f92091"},"cell_type":"markdown","source":"# so now we will be again applyig PCA on test set and predict with cnn model let's how much accuracy we can bring "},{"metadata":{"trusted":true,"_uuid":"36d27382a5f30c307a49a7db2c5877e6eb085310"},"cell_type":"code","source":"# X_tst.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85b580cccbd60c575f968bc889a6d8ff59280e66"},"cell_type":"code","source":"# X_tst=X_tst.reshape(-1,32,32,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e5ecb680ac5381767b2357be302c458538a98c28"},"cell_type":"code","source":"# test_path=[]\n# for i in os.listdir(test_dir):\n#     test_path.append(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8c9e49951cb920d3a8437dbcc0bee43efd3a29f"},"cell_type":"code","source":"# test_dataframe=pd.DataFrame(data=test_path,columns=[\"id\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0218d009a7446b56db046c4182aa4e08aa0aa66a"},"cell_type":"code","source":"# test_dataframe.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb73eb76a70e0e61bd97fb20cb6d6d031188eb5f"},"cell_type":"code","source":"# import os,array\n# import pandas as pd\n# import time\n# import dask as dd\n\n# from PIL import Image\n# def pixelconv(file_list,img_height,img_width,pixels):  \n#     columnNames = list()\n\n#     for i in range(pixels):\n#         pixel = 'pixel'\n#         pixel += str(i)\n#         columnNames.append(pixel)\n\n\n#     train_data = pd.DataFrame(columns = columnNames)\n#     start_time = time.time()\n#     for i in file_list:\n#         t = i\n#         img_name = t\n#         img = Image.open('../input/test/test/'+img_name)\n#         rawData = img.load()\n#         #print rawData\n#         data = []\n#         for y in range(img_height):\n#             for x in range(img_width):\n#                 data.append(rawData[x,y][0])\n#         print (i)\n#         k = 0\n#         #print data\n#         train_data.loc[i] = [data[k] for k in range(pixels)]\n#     #print train_data.loc[0]\n\n#     print (\"Done pixel values conversion\")\n#     print  (time.time()-start_time)\n#     print (train_data)\n#     train_data.to_csv(\"test_converted_new.csv\",index = False)\n#     print (\"Done data frame conversion\")\n#     print  (time.time()-start_time)\n# pixelconv(test_dataframe.id,32,32,1024) # pass pandas dataframe in which path of images only as column\n#                                     # in return csv file will save in working directory ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8573e175ad982f18ef901479bb458842411ed6ab"},"cell_type":"code","source":"# new_test=pd.read_csv(\"../working/test_converted_new.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d99ca06aed0d57c0b8eb25d51f00fd9c0cc7c30"},"cell_type":"code","source":"# X_tst=pca.transform(new_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"989196ca18f2258618fbf585d3e82a2ae1fb0409"},"cell_type":"code","source":"# X_tst.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5bddd12bef0a778cf7c6c18b4f04b8feda5e62d2"},"cell_type":"code","source":"# X_tst=X_tst.reshape(-1,25,25,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd0ca35089c3772959f4b26e8c3865691a4e7121"},"cell_type":"code","source":"# X_tst.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"af3f3134d54874887aa13f3aa82ae68f9e71327e"},"cell_type":"markdown","source":"# CNN with Focal Loss , hey man i should foucs tuff example to classify"},{"metadata":{"_uuid":"25a17e89302accc928ec3088c765dce1187359f0"},"cell_type":"markdown","source":"The focal loss was proposed for dense object detection task early this year. It enables training highly accurate dense object detectors with an imbalance between foreground and background classes at 1:1000 scale. This tutorial will show you how to apply focal loss to train a multi-class classifier model given highly imbalanced datasets.\n\nBackground\nLet's first take a look at other treatments for imbalanced datasets, and how focal loss comes to solve the issue.\n\nIn multi-class classification, a balanced dataset has target labels that are evenly distributed. If one class has overwhelmingly more samples than another, it can be seen as an imbalanced dataset. This imbalance causes two problems:\n\nTraining is inefficient as most samples are easy examples that contribute no useful learning signal;\nThe easy examples can overwhelm training and lead to degenerate models.\nA common solution is to perform some form of hard negative mining that samples hard examples during training or more complex sampling/reweighing schemes.\n\nFor image classification specific, data augmentation techniques are also variable to create synthetic data for under-represented classes.\n\nThe focal loss is designed to address class imbalance by down-weighting inliers (easy examples) such that their contribution to the total loss is small even if their number is large. It focuses on training a sparse set of hard examples.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"image_location\"]=train_dir+train_df[\"id\"]","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":39,"outputs":[{"output_type":"execute_result","execution_count":39,"data":{"text/plain":"                                     id                        ...                                                             image_location\n0  0004be2cfeaba1c0361d39e2b000257b.jpg                        ...                          ../input/train/train/0004be2cfeaba1c0361d39e2b...\n1  000c8a36845c0208e833c79c1bffedd1.jpg                        ...                          ../input/train/train/000c8a36845c0208e833c79c1...\n2  000d1e9a533f62e55c289303b072733d.jpg                        ...                          ../input/train/train/000d1e9a533f62e55c289303b...\n3  0011485b40695e9138e92d0b3fb55128.jpg                        ...                          ../input/train/train/0011485b40695e9138e92d0b3...\n4  0014d7a11e90b62848904c1418fc8cf2.jpg                        ...                          ../input/train/train/0014d7a11e90b62848904c141...\n\n[5 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>has_cactus</th>\n      <th>image_location</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0004be2cfeaba1c0361d39e2b000257b.jpg</td>\n      <td>1</td>\n      <td>../input/train/train/0004be2cfeaba1c0361d39e2b...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000c8a36845c0208e833c79c1bffedd1.jpg</td>\n      <td>1</td>\n      <td>../input/train/train/000c8a36845c0208e833c79c1...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000d1e9a533f62e55c289303b072733d.jpg</td>\n      <td>1</td>\n      <td>../input/train/train/000d1e9a533f62e55c289303b...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0011485b40695e9138e92d0b3fb55128.jpg</td>\n      <td>1</td>\n      <td>../input/train/train/0011485b40695e9138e92d0b3...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0014d7a11e90b62848904c1418fc8cf2.jpg</td>\n      <td>1</td>\n      <td>../input/train/train/0014d7a11e90b62848904c141...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true,"_uuid":"894d42eafbf93f16da8ad4aa14540eb840bd7161"},"cell_type":"code","source":"import tensorflow as tf\nfrom keras import backend as K","execution_count":40,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1bcccdfbfcccb2933f76041cf361c7b81c979ec9"},"cell_type":"code","source":"def binary_focal_loss(gamma=2., alpha=.25):\n    \"\"\"\n    Binary form of focal loss.\n      FL(p_t) = -alpha * (1 - p_t)**gamma * log(p_t)\n      where p = sigmoid(x), p_t = p or 1 - p depending on if the label is 1 or 0, respectively.\n    References:\n        https://arxiv.org/pdf/1708.02002.pdf\n    Usage:\n     model.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n    \"\"\"\n    def binary_focal_loss_fixed(y_true, y_pred):\n        \"\"\"\n        :param y_true: A tensor of the same shape as `y_pred`\n        :param y_pred:  A tensor resulting from a sigmoid\n        :return: Output tensor.\n        \"\"\"\n        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n\n        epsilon = K.epsilon()\n        # clip to prevent NaN's and Inf's\n        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n\n        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) \\\n               -K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n\n    return binary_focal_loss_fixed","execution_count":41,"outputs":[]},{"metadata":{"_uuid":"5cd23537588b853f5dd9f6d776190464a982a090"},"cell_type":"markdown","source":"# image augmentation we are using at runtime but to do that we need few things , i'm using a script to pass respective images in their respective label folder "},{"metadata":{"_uuid":"6baac4c0dc2d3dae610c35994237bbd9b80045f8"},"cell_type":"markdown","source":"# so that script will work in smooth process , make unique set of labels folder first then image mapping we will using and pass every image in their respective folder "},{"metadata":{"trusted":true,"_uuid":"4f6f4a9982604dcee965cab3d85d128ee34699d0"},"cell_type":"code","source":"train_df.head()","execution_count":42,"outputs":[{"output_type":"execute_result","execution_count":42,"data":{"text/plain":"                                     id                        ...                                                             image_location\n0  0004be2cfeaba1c0361d39e2b000257b.jpg                        ...                          ../input/train/train/0004be2cfeaba1c0361d39e2b...\n1  000c8a36845c0208e833c79c1bffedd1.jpg                        ...                          ../input/train/train/000c8a36845c0208e833c79c1...\n2  000d1e9a533f62e55c289303b072733d.jpg                        ...                          ../input/train/train/000d1e9a533f62e55c289303b...\n3  0011485b40695e9138e92d0b3fb55128.jpg                        ...                          ../input/train/train/0011485b40695e9138e92d0b3...\n4  0014d7a11e90b62848904c1418fc8cf2.jpg                        ...                          ../input/train/train/0014d7a11e90b62848904c141...\n\n[5 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>has_cactus</th>\n      <th>image_location</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0004be2cfeaba1c0361d39e2b000257b.jpg</td>\n      <td>1</td>\n      <td>../input/train/train/0004be2cfeaba1c0361d39e2b...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000c8a36845c0208e833c79c1bffedd1.jpg</td>\n      <td>1</td>\n      <td>../input/train/train/000c8a36845c0208e833c79c1...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000d1e9a533f62e55c289303b072733d.jpg</td>\n      <td>1</td>\n      <td>../input/train/train/000d1e9a533f62e55c289303b...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0011485b40695e9138e92d0b3fb55128.jpg</td>\n      <td>1</td>\n      <td>../input/train/train/0011485b40695e9138e92d0b3...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0014d7a11e90b62848904c1418fc8cf2.jpg</td>\n      <td>1</td>\n      <td>../input/train/train/0014d7a11e90b62848904c141...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true,"_uuid":"547fd118d0c0551920f7d04a7c92323a6b0ebdfd"},"cell_type":"code","source":"#from console_progressbar import ProgressBar\nimport shutil\nimport tqdm\nimport os\nfilenames=list(train_df[\"image_location\"].values)\nlabels=list(train_df[\"has_cactus\"].values)\nfolders_to_be_created = np.unique(list(train_df['has_cactus'].values))\nfiles=[]\npath=\"../working/trainset/\"\nfor i in folders_to_be_created:\n    if not os.path.exists(path+str(i)):\n        os.makedirs(path+str(i)) \n#pb = ProgressBar(total=100, prefix='Save valid data', suffix='', decimals=3, length=50, fill='=')\nfor f in tqdm_notebook(range(len(filenames))):\n    \n    current_image=filenames[f]\n    current_label=labels[f]\n    src_path=current_image\n   \n    dst_path =path+str(current_label) \n    \n    try :\n        shutil.copy(src_path, dst_path)\n        #pb.print_progress_bar((f + 1) * 100 / 4000)\n    except Exception as e :\n        files.append(src_path)","execution_count":43,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, max=17500), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3c8348d1244476394da0e33ce5202c8"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"_uuid":"54bf3ebcde2e0e489eaca48da5bfff48e88855fe"},"cell_type":"markdown","source":"# image augumentation begins"},{"metadata":{"_uuid":"d1cdc310077e182a29af5b4eaae0b9d619f292dd"},"cell_type":"markdown","source":"# we are using subset in imagedatagenerator therefore whatever augumentation we are applying will not get introduced in validation "},{"metadata":{"trusted":true,"_uuid":"b89566d8e3e76a126a033399fcad4fdb435a428a"},"cell_type":"code","source":"import keras\nfrom keras_preprocessing.image import ImageDataGenerator\nfrom keras.applications.vgg16 import preprocess_input\ntrain_datagen = ImageDataGenerator(\n    rescale = 1./255,\n#     preprocessing_function= preprocess_input,\n    #shear_range=0.2,\n    zoom_range=0.2,\n    fill_mode = 'reflect',\n    #cval = 1,\n    rotation_range = 30,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    horizontal_flip=True,validation_split=.20)\n\nvalid_datagen = ImageDataGenerator(rescale=1./255)#,preprocessing_function=preprocess_input)\n\ntrain_generator = train_datagen.flow_from_directory(\n    directory='../working/trainset/',\n    target_size=(32, 32),\n    batch_size=32,\n    class_mode='binary',subset=\"training\")\n\nvalidation_generator = train_datagen.flow_from_directory(\n    directory='../working/trainset/',\n    target_size=(32,32),\n    batch_size=32,\n    class_mode='binary',subset=\"validation\")","execution_count":44,"outputs":[{"output_type":"stream","text":"Found 14001 images belonging to 2 classes.\nFound 3499 images belonging to 2 classes.\n","name":"stdout"}]},{"metadata":{"_uuid":"9240b537631cd30d63f53b0d4b179ae7218833c4"},"cell_type":"markdown","source":"# Applying VGG-16 First "},{"metadata":{"trusted":true,"_uuid":"e7e9193d0b49119bcd048374cf11bca89c5c2fd4"},"cell_type":"code","source":"import cv2\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport json\nimport os\nfrom tqdm import tqdm, tqdm_notebook\nfrom keras.models import Sequential\nfrom keras.layers import Activation, Dropout, Flatten, Dense\nfrom keras.applications import VGG16\nfrom keras.optimizers import Adam\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd88dcbf2cca010d8c6d15b00f9addbd9a3582d0"},"cell_type":"code","source":"vgg16_net = VGG16(weights='imagenet', \n                  include_top=False, \n                  input_shape=(32, 32, 3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4fa68a9d6e8d252190bb287eab4e2d577a5cbf0"},"cell_type":"code","source":"vgg16_net.trainable = False\nvgg16_net.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5fc2f1d19dcc476daca9fff33b4e108f4169d6b0"},"cell_type":"code","source":"model = Sequential()\nmodel.add(vgg16_net)\nmodel.add(Flatten())\nmodel.add(Dense(256))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf85a73182715c09c77d3daa81ae838220a1dfb7"},"cell_type":"code","source":"model.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=Adam(lr=1e-5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72198d9186d00603786eb3c0ee79c173a464d03a"},"cell_type":"code","source":"history=model.fit_generator(train_generator,\n                    steps_per_epoch = 14001//32,\n                    epochs=50,\n                    validation_data = validation_generator,validation_steps=3499//32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee9e58a74a4bc6e5313c31f8d4b6627467b9e1b8"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n#plot of model epochs what has been happened within 100 epochs baseline with vgg16 training 135 million params\n# freezing first layer\nfig = plt.figure(figsize=(12,8))\nplt.plot(history.history['acc'],'blue')\nplt.plot(history.history['val_acc'],'orange')\nplt.xticks(np.arange(0, 50, 1))\nplt.yticks(np.arange(0,1,.1))\nplt.rcParams['figure.figsize'] = (10, 10)\nplt.xlabel(\"Num of Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Training Accuracy vs Validation Accuracy\")\nplt.grid(True)\nplt.gray()\nplt.legend(['train','validation'])\nplt.show()\n \nplt.figure(1)\nplt.plot(history.history['loss'],'blue')\nplt.plot(history.history['val_loss'],'orange')\nplt.xticks(np.arange(0, 50, 1))\nplt.rcParams['figure.figsize'] = (10, 10)\nplt.xlabel(\"Num of Epochs\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Loss vs Validation Loss\")\nplt.grid(True)\nplt.gray()\nplt.legend(['train','validation'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c2c3607b2a6f41e2c96d365d5e1d3a2e9412e9b"},"cell_type":"code","source":"%%time\nX_tst = []\nTest_imgs = []\nfor img_id in tqdm_notebook(os.listdir(test_dir)):\n    X_tst.append(cv2.imread(test_dir + img_id))     \n    Test_imgs.append(img_id)\nX_tst = np.asarray(X_tst)\nX_tst = X_tst.astype('float32')\nX_tst /= 255","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1749b2a2d0c2a299c5fc232efae4c6695da64802"},"cell_type":"code","source":"# Prediction\ntest_predictions = model.predict(X_tst)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7736e692b2944cc227e0318eeddb2d84606840d7"},"cell_type":"markdown","source":"# due focal loss we are taking 0.5 as threshold , but let's see on scoreboard"},{"metadata":{"trusted":true,"_uuid":"02afb21450811376ce59212bad901bf0ac860df2"},"cell_type":"code","source":"sub_df = pd.DataFrame(test_predictions, columns=['has_cactus'])\nsub_df['has_cactus'] = sub_df['has_cactus'].apply(lambda x: 1 if x > 0.5 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed9bc43e2374c5ec654decdc4640b3dacc01b7d5"},"cell_type":"code","source":"sub_df['id'] = ''\ncols = sub_df.columns.tolist()\ncols = cols[-1:] + cols[:-1]\nsub_df=sub_df[cols]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0987a4263a3093ee9fe9abe4e492547c7b5849f5"},"cell_type":"code","source":"for i, img in enumerate(Test_imgs):\n    sub_df.set_value(i,'id',img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b3918b178e7adb0c0cdb4f52940dcae73adcb85"},"cell_type":"code","source":"sub_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6898bbc50ec5e0c5ce5d2bd2698e7d8d26e27833"},"cell_type":"code","source":"sub_df.to_csv('submission_focal_loss_vgg-16.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e308fe875602b9d94ca51c3e2477f6850f0d5e0"},"cell_type":"markdown","source":"# Applying Resnet-50 and then after that we can compare results what we get from these models"},{"metadata":{"trusted":true,"_uuid":"9f4caccae85aa9e04cdc5c03c1a7291fb1473015"},"cell_type":"code","source":"from keras.applications import ResNet50\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, GlobalAveragePooling2D, BatchNormalization\nfrom keras.applications.resnet50 import preprocess_input\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.preprocessing.image import load_img, img_to_array\nfrom keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ac44c232800d9cec7165c120cca9441b3b8c165"},"cell_type":"code","source":"base_model = ResNet50(weights='imagenet',include_top=False,input_shape=(32,32,3))\n\nbase_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a1846c2fe5f3ee247e485ab40411e14fec308607"},"cell_type":"code","source":"model = Sequential()\nmodel.add(base_model)\nmodel.add(Flatten())\n# # let's add a fully-connected layer\nmodel.add(Dense(500, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(500, activation='relu'))\nmodel.add(Dropout(0.2))\n# # and a logistic layer -- let's say we have 200 classes\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss=binary_focal_loss(gamma=2,alpha=0.28), metrics=[\"accuracy\"], optimizer=Adam(lr=1e-5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d8f2692dd3f7b64adafe193eb991a008686adbc"},"cell_type":"code","source":"history1=model.fit_generator(train_generator,\n                    steps_per_epoch = 14001//32,\n                    epochs=85,\n                    validation_data = validation_generator,validation_steps=3499//32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prediction\ntest_predictions = model.predict(X_tst)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df = pd.DataFrame(test_predictions, columns=['has_cactus'])\nsub_df['has_cactus'] = sub_df['has_cactus'].apply(lambda x: 1 if x > 0.5 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df['id'] = ''\ncols = sub_df.columns.tolist()\ncols = cols[-1:] + cols[:-1]\nsub_df=sub_df[cols]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, img in enumerate(Test_imgs):\n    sub_df.set_value(i,'id',img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.to_csv('submission_focal_loss_resnet_50.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6cb3c689664ceae5211e71b24a1ee8d06e6d4fd0"},"cell_type":"markdown","source":"# AS we can see the effects of focal loss at early stages it need sometime to adjust itself even i have same focal loss method for categorical cross entropy to work on imbalance classification it will take some time about 20 epochs adjust it's loss on validation , so still we can see validation at 98.92 and training is 98.30"},{"metadata":{"trusted":true,"_uuid":"285e9fc914c69f1e66ae617bac894f31ac71ce5c"},"cell_type":"markdown","source":"# Conclusion\n\nAfter experiments with parameters of the used models I came to the result of 0.89+ accuracy on the part of the test set, but I am unable to improve it.\n\nBecause dataset is build with images of handwritten digits getting bigger train set could help. Maybe I need to change approach, ignore PCA decomposition at all and use ImageDataGenerator to generate more images or use convolution layers. \n\nIf you know how above result can be (easily?) improved, please leave comment with suggestion. As data science newbie I would be grateful for any suggestions :)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}