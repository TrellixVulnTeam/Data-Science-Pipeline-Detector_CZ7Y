{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert to str to use class mode binary( categories )\ntrain_df['has_cactus'] = train_df['has_cactus'].apply(lambda x: str(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split train-validation \ntrain_split = 0.4 \nvalidation_df = train_df.sample(n=int(train_split*len(train_df)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(validation_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#trim validation set data points\ntrain_df = train_df[~train_df['id'].isin(validation_df['id'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check how much sample size we have for each category\nprint(validation_df['has_cactus'].value_counts())\nprint(train_df['has_cactus'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import models\nfrom keras import layers\n\n# this architecture was taken from the deep-learning with keras book.\n# conv2d+MaxPool layer -> flatten -> dense layer(relu) -> dense_layer(sigmoid)\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\n#another conv layer can be added if needed\n#model.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dense(1,activation='sigmoid'))\nmodel.build()\nmodel.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\n\n# All images will be rescaled by 1./255\ntrain_datagen = ImageDataGenerator(rescale=1./255)\ntrain_generator = train_datagen.flow_from_dataframe(dataframe=train_df,directory='/kaggle/input/train/train/',x_col='id',y_col='has_cactus',target_size=(32,32),class_mode='binary',batch_size=20)\n\n\n# All images will be rescaled by 1./255\nvalidator_datagen = ImageDataGenerator(rescale=1./255)\nvalidator_generator = validator_datagen.flow_from_dataframe(dataframe=validation_df,directory='/kaggle/input/train/train/',x_col='id',y_col='has_cactus',target_size=(32,32),class_mode='binary',batch_size=20)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(set(train_df['id']) & set(validation_df['id']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#binary_crossentropy works well comparing probability distributions\nmodel.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit_generator(train_generator,steps_per_epoch=len(train_df)//20,epochs=20,validation_data=validator_generator,validation_steps=len(validation_df)//20)\nimport matplotlib.pyplot as plt\n\ndef plot_history(history):\n    acc = history.history['acc']\n    val_acc = history.history['val_acc']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    epochs = range(len(acc))\n\n    plt.plot(epochs, acc, 'bo', label='Training acc')\n    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n\n    plt.figure()\n\n    plt.plot(epochs, loss, 'bo', label='Training loss')\n    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()\n\n    plt.show()\n\n\n    \nplot_history(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# run with test set\ntest_datagen = ImageDataGenerator(rescale=1./255)\ntest_generator = test_datagen.flow_from_directory('/kaggle/input/test', target_size=(32, 32),class_mode=None, batch_size=1,shuffle=False)\n\nY_pred = model.predict_generator(test_generator,steps=len(filenames))\nfilenames = list(map(lambda x: x.replace('test/',''),test_generator.filenames))\ntest_df = pd.DataFrame()\ntest_df['id']=filenames\ntest_df['has_cactus']=Y_pred\ntest_df[['id','has_cactus']].to_csv('submission_baseline.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}