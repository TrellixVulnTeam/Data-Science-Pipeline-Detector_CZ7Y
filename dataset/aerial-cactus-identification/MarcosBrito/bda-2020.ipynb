{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introdução\n\nNeste caderno, examinaremos a definição, o treinamento e os testes, bem como as etapas de pré-processamento necessárias para alimentar uma imagem nas CNNs para obter resultados de última geração. Usaremos o PyTorch para este tutorial. O PyTorch é uma poderosa estrutura de aprendizado profundo que está crescendo em popularidade e é completamente à vontade no Python, o que facilita muito o aprendizado e o uso. Este tutorial não assume muito em relação ao conhecimento prévio do PyTorch, mas pode ser útil fazer o checkout [meu tutorial introdutório anterior de CV] (https://www.kaggle.com/abhinand05/mnist-introduction-to-computervision- com pytorch).\n\n\n![FeaturedImage](https://i.ibb.co/ws3htpn/2088474-6a86-3.jpg)\n\n\n\nNeste caderno, treinaremos a CNN para classificar imagens com base no fato de terem ou não um cacto colunar. Usaremos o Conjunto de dados aéreos dos cactos [desta competição atualmente em execução no Kaggle] (https://www.kaggle.com/c/aerial-cactus-identification/overview). Para obter mais informações sobre o conjunto de dados, visite [esta página] (https://www.kaggle.com/c/aerial-cactus-identification/data). Eu escolhi essa competição porque achava que é o melhor lugar para os iniciantes praticarem suas novas habilidades encontradas com as CNNs, já que o MNIST é muito simples para colocar as CNNs em jogo. Um perceptron regular de várias camadas pode muito bem fazer o trabalho. Portanto, esta é uma competição perfeita para iniciantes, como alguém disse corretamente nos fóruns de discussão.\n\n### ** Se você gosta deste kernel ou deseja bifurcá-lo, por favor, faça um UPVOTE para mostrar sua apreciação. **"},{"metadata":{},"cell_type":"markdown","source":"# Importando bibliotecas"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir('../input/'))\n\nimport matplotlib.pyplot as plt\nplt.style.use(\"ggplot\")\n\n# OpenCV Image Library\nimport cv2\n\n# Import PyTorch\nimport torchvision.transforms as transforms\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader, Dataset\nimport torchvision\nimport torch.optim as optim","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Carregamento de dados de treinamento + EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/aerial-cactus-identification/train.csv\")\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Os dados do trem contêm 17500 imagens que podem ser encontradas em um diretório separado e também temos um arquivo csv, mas não podemos visualizá-las diretamente, como veremos mais adiante.\n\nOs dados de teste têm 4000 imagens e são armazenados em um diretório separado. Observe que ele não possui um arquivo CSV como vimos para os dados do traino"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Train Size: {len(os.listdir('../input/aerial-cactus-identification/train/train'))}\")\nprint(f\"Test Size: {len(os.listdir('../input/aerial-cactus-identification/test/test'))}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Aqui, inspecionamos os dados para ver a quantidade de amostras de treinamento para cada classe. Como podemos ver, cerca de 2/3 dos dados de treinamento pertencem a uma classe. Se o conjunto de dados que você estiver usando contiver quase ou acima de 90% dos dados de treinamento pertencentes a uma única classe, isso afetará bastante seus resultados.\nIsso é chamado de classes distorcidas, podemos usar aumento de dados, amostragem e várias maneiras de superar isso. No entanto, não acredito que seja um problema aqui, pois temos dados suficientes para que as CNNs obtenham ótimos resultados."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Contando o número de dados de amostra para cada classe\nvalue_counts = train_df.has_cactus.value_counts()\n%matplotlib inline\nplt.pie(value_counts, labels=['Has Cactus', 'No Cactus'], autopct='%1.1f', colors=['green', 'red'], shadow=True)\nplt.figure(figsize=(5,5))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Configurando dados de trem para Pytorch\nNão podemos simplesmente usar os dados brutos da imagem para fazer previsões usando o PyTorch. Existem várias etapas de pré-processamento envolvidas que discutiremos em detalhes nesta seção.\n\n**Passo 1:**\n\nPrimeiro, definimos uma classe personalizada que estende a classe `torch.utils.data.Dataset` do PyTorch. Eu acho que tudo lá é bem direto. Definimos nossos construtores e adicionamos dois métodos diferentes, `len` e` getitem`, que substituem essencialmente as definições dos pais."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Caminhos de dados\ntrain_path = '../input/aerial-cactus-identification/train/train/'\ntest_path = '../input/aerial-cactus-identification/test/test/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Nossa própria classe personalizada para conjuntos de dados\nclass CreateDataset(Dataset):\n    def __init__(self, df_data, data_dir = './', transform=None):\n        super().__init__()\n        self.df = df_data.values\n        self.data_dir = data_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        img_name,label = self.df[index]\n        img_path = os.path.join(self.data_dir, img_name)\n        image = cv2.imread(img_path)\n        if self.transform is not None:\n            image = self.transform(image)\n        return image, label","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Passo 2:**\n\nAgora que definimos nossa classe, é hora de passar os dados brutos e convertê-los para o formato compreensível do PyTorch.\n\n** Transformações ** - podemos usar o recurso de transformações no PyTorch para aplicar aumentos de dados que nos ajudam a melhorar a precisão do nosso modelo quando bem feito. Existem transformações sevarais que podem ser aplicadas e você pode dar uma olhada na [documentação aqui] (https://pytorch.org/docs/stable/torchvision/transforms.html). Aqui, nós a inserimos primeiro em uma imagem PIL. A inversão horizontal aleatória de imagens de amostra é aplicada juntamente com rotação aleatória de 10 graus para exemplos de treinamento aleatório. Em seguida, convertemos as imagens em um tensor PyTorch e normalizamos as imagens.\n\n** Criando nosso conjunto de dados - ** Em seguida, usamos nossa classe `CreateDataset` para ocultar os dados brutos da maneira que o PyTorch espera. Também aplicamos as transformações lá."},{"metadata":{},"cell_type":"markdown","source":"Aplicando o Data Augmentation"},{"metadata":{"trusted":true},"cell_type":"code","source":"transforms_train = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ntrain_data = CreateDataset(df_data=train_df, data_dir=train_path, transform=transforms_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Etapa 3:**\n\nO tamanho do lote está definido. O tamanho do lote geralmente é definido entre 64 e 256. O tamanho do lote afeta a precisão do teste final. Uma maneira de pensar sobre isso é que lotes menores significam que o número de atualizações de parâmetros por época é maior.\n\nEm seguida, a porcentagem de dados necessários para a validação é definida em 20%, o que quase sempre parece funcionar para mim. Mas no final, é apenas outro hiperparâmetro que você pode ajustar.\n\nNas próximas etapas, usamos a função `torch.utils.data.samplerSubsetRandomSampler` para dividir nossos dados em conjuntos de treinamento e validação, que é semelhante à função` train_test_split` do scikit-learn.\n\nTemos os dados de treinamento passados para o carregador de trem. Podemos fazer um iterador com o iter (trainloader) que pode nos ajudar a coletar dados. Mais tarde, usaremos isso para percorrer o conjunto de dados para treinamento. Cada vez que podemos extrair dados do tamanho do lote definido."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Definir tamanho do lote\nbatch_size = 64\n\n# Porcentagem de treinamento definida para uso como validação\nvalid_size = 0.2\n\n# obter índices de treinamento que serão usados para validação\nnum_train = len(train_data)\nindices = list(range(num_train))\nnp.random.shuffle(indices)\nsplit = int(np.floor(valid_size * num_train))\ntrain_idx, valid_idx = indices[split:], indices[:split]\n\n# Criar Samplers\ntrain_sampler = SubsetRandomSampler(train_idx)\nvalid_sampler = SubsetRandomSampler(valid_idx)\n\n# preparar carregadores de dados (combinar conjunto de dados e amostrador)\ntrain_loader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\nvalid_loader = DataLoader(train_data, batch_size=batch_size, sampler=valid_sampler)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Configurando dados de teste\nEsta seção do código deve fazer sentido para você agora. As mesmas etapas usadas acima são repetidas para os dados de teste também.\n\nObserve que não aumentamos nossos dados no conjunto de treinamento. Isso ocorre porque o aprimoramento é feito apenas no conjunto de treinamento para melhorar o desempenho, fornecendo ao nosso modelo variações complexas que podem torná-lo generalizado para novas amostras no conjunto de testes, portanto, não faz sentido fazer o aprimoramento de dados nos dados de teste também. No entanto, ainda precisamos convertê-lo em um tensor e normalizá-lo."},{"metadata":{"trusted":true},"cell_type":"code","source":"transforms_test = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\n# criando dados de teste\nsample_sub = pd.read_csv(\"../input/aerial-cactus-identification/sample_submission.csv\")\ntest_data = CreateDataset(df_data=sample_sub, data_dir=test_path, transform=transforms_test)\n\n# prepare o carregador de teste\ntest_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualizar imagens"},{"metadata":{},"cell_type":"markdown","source":"Visualizar as imagens e inspecioná-las para ter uma idéia melhor do que são são sempre úteis antes de construir o modelo para fazer previsões.\n\nPodemos ver imagens aéreas de cactos. As imagens contêm canais de cores, devemos ter em mente e são imagens de 32x32. Observe que essas imagens são de baixa resolução e, como seres humanos, evoluímos de maneira a dar sentido a esse tipo de imagem. Vamos construir uma CNN nas próximas seções para, no final das contas, alcançar exatamente esse ou um desempenho ainda melhor."},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = [ 'No Cactus','Cactus']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def imshow(img):\n    '''Helper function to un-normalize and display an image'''\n    # anormalizar\n    img = img / 2 + 0.5\n    # converter da imagem Tensor e exibir\n    plt.imshow(np.transpose(img, (1, 2, 0)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# obter um lote de imagens de treinamento\ndataiter = iter(train_loader)\nimages, labels = dataiter.next()\nimages = images.numpy() # converter imagens em numpy para exibição\n\n# plota as imagens no lote, juntamente com os rótulos correspondentes\nfig = plt.figure(figsize=(25, 4))\n# exibir 20 imagens\nfor idx in np.arange(20):\n    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n    imshow(images[idx])\n    ax.set_title(classes[labels[idx]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualizando uma imagem com mais detalhes\nAqui, examinamos os canais de cores normalizados de vermelho, verde e azul (RGB) como três imagens separadas de intensidade de escala de cinza para fins de ilustração. Isso me ajuda a explicar as CNNs mais tarde.\n\nCada pequeno quadrado que você pode ver são pixels com seus valores normalizados. Valor maior significa mais brilhante, mais baixo significa mais escuro. É assim que as imagens são representadas em nossos computadores. Quando combinamos os três canais, obtemos uma imagem colorida. Esse esquema de cores RGB pode representar cerca de 16,77 milhões de cores, o que é incrível.\n\nComo você já deve ter imaginado, as imagens coloridas são na verdade tridimensionais, onde as imagens em escala de cinza, por exemplo, são 1D."},{"metadata":{"trusted":true},"cell_type":"code","source":"rgb_img = np.squeeze(images[3])\nchannels = ['red channel', 'green channel', 'blue channel']\n\nfig = plt.figure(figsize = (36, 36)) \nfor idx in np.arange(rgb_img.shape[0]):\n    ax = fig.add_subplot(3, 1, idx + 1)\n    img = rgb_img[idx]\n    ax.imshow(img, cmap='gray')\n    ax.set_title(channels[idx])\n    width, height = img.shape\n    thresh = img.max()/2.5\n    for x in range(width):\n        for y in range(height):\n            val = round(img[x][y],2) if img[x][y] !=0 else 0\n            ax.annotate(str(val), xy=(y,x),\n                    horizontalalignment='center',\n                    verticalalignment='center', size=8,\n                    color='white' if img[x][y]<thresh else 'black')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Perceptrons de várias camadas (MLP) versus redes neurais convolucionais (CNN)\nComo você deve ter visto no [meu tutorial anterior] (https://www.kaggle.com/abhinand05/mnist-introduction-to-computervision-with-pytorch) no conjunto de dados MNIST, os MLPs foram bons o suficiente para marcar mais de 90% precisão. Acontece que eles não são nem bons o suficiente para obter resultados semelhantes em conjuntos de dados complexos, principalmente porque esperam um vetor achatado da imagem original como o abaixo. O conjunto de dados MNIST é uma exceção, pois já vem bem processado.\n\n![Example](https://github.com/abhinand5/CNNs-in-PyTorch/blob/master/cifar-cnn/FireShot%20Capture%20084%20-%20Intro%20to%20Deep%20Learning%20with%20PyTorch%20-%20Udacity%20-%20classroom.udacity.com.png?raw=true)\n\nEntão, tudo o que vê é um vetor e o trata apenas como um vetor sem estrutura especial. Não tem conhecimento algum do fato de ter sido previamente organizado em uma grade, perdendo informações importantes. Se fazemos o mesmo com imagens coloridas, descartamos informações vitais, achatando-as, o que quase nunca funciona para imagens do mundo real. Por outro lado, as CNNs funcionam exatamente do contrário, capazes de trabalhar e elucidar padrões a partir de dados multidimensionais, por isso são tão poderosos. Diferentemente dos MLPs, as CNNs realmente entendem as informações dos pixels que estão próximos um do outro e mais relacionados entre si do que os pixels que estão distantes e não relacionados.\n"},{"metadata":{},"cell_type":"markdown","source":"## Noções básicas sobre CNNs\n\nAs Redes Neurais Conolucionais usam três classes diferentes de camadas que diferem distintamente entre si. Mas quando combinados, eles dão resultados extraordinários.\n\n* Camadas Convolucionais\n* Camadas de pool\n* Camadas totalmente conectadas\n\nAnalisaremos cada uma das camadas em grande detalhe.\n\nVou usar um exemplo de classificação de um carro aqui para explicar as diferentes camadas de uma Rede Neural Convolucional.\n\n![Layers](https://s3.amazonaws.com/video.udacity-data.com/topher/2018/May/5b1070e4_screen-shot-2018-05-31-at-2.59.36-pm/screen-shot-2018-05-31-at-2.59.36-pm.png)\n\n** Camada Convolucional: **\n\nAcontece que, para entender primeiro a camada Convolucional, precisamos entender um conceito chamado Filtros. Filtros especialmente passa-alto.\n\nPara detectar alterações na intensidade de uma imagem, usaremos e criaremos filtros de imagem específicos que analisam grupos de pixels e reagem a padrões alternados de pixels escuros / claros. Esses filtros produzem uma saída que mostra bordas dos objetos e texturas diferentes.\n\nPara isso, primeiro construímos um filtro de acordo com um tamanho chamado tamanho da janela, que nada mais é do que uma matriz como essa.\n\nAqui em um exemplo gif.\n\n![Example](https://media.giphy.com/media/jrzu0JxxZydz0valeu/giphy.gif)\n\nEssas janelas são geralmente do tamanho 3x3, o que ajuda as CNNs a identificar os padrões em uma imagem. Esses filtros podem ser modificados para obter filtros diferentes como saída. Pode haver vários filtros em uma camada convolucional.\n\nA camada convolucional é produzida aplicando uma série de muitos filtros de imagem diferentes, também conhecidos como kernels convolucionais, a uma imagem de entrada.\n\n![Example](https://s3.amazonaws.com/video.udacity-data.com/topher/2018/May/5b10723a_screen-shot-2018-05-31-at-3.06.07-pm/screen-shot-2018-05-31-at-3.06.07-pm.png)\n\nNo exemplo mostrado, 4 filtros diferentes produzem 4 imagens de saída filtradas de maneira diferente. Quando empilhamos essas imagens, formamos uma camada convolucional completa com profundidade de 4!\n\n![Example](https://s3.amazonaws.com/video.udacity-data.com/topher/2018/May/5b10729b_screen-shot-2018-05-31-at-3.07.03-pm/screen-shot-2018-05-31-at-3.07.03-pm.png)\n\nA profundidade de cada camada convolucional consecutiva pode aumentar, resultando na captura de redes de características / padrões incrivelmente complexos das imagens. De fato, isso é muito parecido com o modo como nossos cérebros interpretam imagens em um instante.\n\n** Camada de pool: **\n\nO pool é usado em redes neurais convolucionais para tornar a detecção de certos recursos um tanto invariável às mudanças de escala e orientação. Outra maneira de pensar sobre o que o pool faz é que ele generaliza em informações mais complexas e de nível inferior.\n\nComo isso é feito é uma janela de tamanho fixo feita para passear sobre a imagem e, dentro da janela, um valor específico de acordo com uma métrica é calculado e um novo tensor é formado. Quando tiramos o elemento máximo da janela, ele é conhecido como Max-Pooling, que é a técnica de pool mais comum. Também existe um pool médio que você pode ver na natureza. Aqui está uma ilustração que você pode achar útil. Existem também alternativas ao Pooling, como a Capsule Networks, que está fora da pontuação deste kernel.\n\nAqui em um exemplo de gif de Max Pooling.\n\n![Example](https://media.giphy.com/media/U7PsR7cv9oIcB6eEAd/giphy.gif)\n\nQuando combinamos camadas de pool com camadas convolucionais, reduzimos a dimensão das camadas, o que ajuda na computação, mas mais do que isso seleciona os valores de pixel de maior significado. Até agora, nosso modelo se parece com isso ...\n\n![Example](https://i.ibb.co/V06mcY0/Intro-to-Deep-Learning-with-Py-Torch-Udacity-classroom-udacity-com.png)\n\n** Camadas totalmente conectadas: **\n\nAs camadas totalmente conectadas não são diferentes daquelas que você já conhece dos MLPs. É a peça final do quebra-cabeça que o torna especial e poderoso. Uma forma ainda mais refinada é passada para as camadas FC para fazer a previsão final.\n\nAí vem o nosso modelo final ...\n\n![Example](http://cs231n.github.io/assets/cnn/convnet.jpeg)\n\n\nEu sei que essa pode não ser a melhor das definições que você veria para as CNNs, mas o objetivo aqui é torná-la pelo menos vagamente compreensível. Há muito mais acontecendo nos bastidores que eu pulei. Encorajo-vos a sair e explorar por conta própria para encontrar essas coisas impressionantes. Espero que isso faça sentido para você. Vamos continuar.\n\n### ** Se você gosta deste kernel ou deseja bifurcá-lo, por favor, faça um UPVOTE para mostrar sua apreciação. **\n\nCréditos da imagem: Stanford CS231n, Udacity"},{"metadata":{},"cell_type":"markdown","source":"## Definir a arquitetura de rede\n\nAí vem a parte importante da definição de uma CNN que pode ser feita usando o módulo `torch.nn`. Primeiro, você deve definir uma classe Model e preencher duas funções `__init__` e` __forward__`. Agora que você entende como a CNN funciona, tudo é praticamente auto-explicativo. Leia os documentos para `nn.Conv2d` para saber mais sobre os parâmetros.\n\n* Definimos primeiro as camadas convolucionais. Os detalhes podem ser principalmente interpretados a partir das próprias linhas de comentário. Mas deixe-me explicar um. Nossa imagem (RGB) tem 3 canais com profundidade = 3, é por isso que a nossa primeira camada de conv tem 3 canais de entrada. Decidi ter 16 filtros para a primeira camada de conv, então out_channels = 16, pois produz 16 filtros diferentes, o kernel tamanho aka tamanho da janela é definido como 3 com padding = 1, que cria espaços extras nas bordas da imagem para ajudar o kernel 3x3 a deslizar sobre todos os pixels das bordas da imagem em caso de incompatibilidade de tamanho.\n\n* Em seguida, definimos as camadas de pool onde um tamanho de janela e passo de 2 são definidos. Exatamente o mesmo que o exemplo de gif que você viu anteriormente.\n\n* As saídas são então conectadas a uma camada FC.\n\n* O abandono é uma técnica de regularização para evitar ajustes excessivos, que também é adicionada. Tudo é montado em uma função de propagação direta posteriormente.\n\n* Você também pode usar a Normalização em lote, mas decidi não, porque atingi mais de 99% de precisão sem ela.\n\n* As saídas aqui não precisam ser 0 ou 1, o que o Kaggle espera são as probabilidades, portanto não precisamos de uma função de ativação como sigmóide na camada de saída."},{"metadata":{"trusted":true},"cell_type":"code","source":"class CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        # Camada Convolucional (veja o tensor de imagem 32x32x3)\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n        # Camada convolucional (veja o tensor da imagem 16x16x16)\n        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n        # Camada convolucional (consulte o tensor de imagem 8x8x32)\n        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n        # Camada Convolucional (veja o tensor de imagem 4 * 4 * 64)\n        self.conv4 = nn.Conv2d(64, 128, 3, padding=1)\n        # Camada Maxpooling\n        self.pool = nn.MaxPool2d(2, 2)\n        # Camada 1 linear totalmente conectada (consulte o tensor de imagem 2 * 2 * 128)\n        self.fc1 = nn.Linear(128*2*2, 512)\n        # Camada FC linear 2\n        self.fc2 = nn.Linear(512, 2)\n        # Definir desistência\n        self.dropout = nn.Dropout(0.2)\n        \n    def forward(self, x):\n        # adicionar sequência de camadas convolucionais e de max pooling\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = self.pool(F.relu(self.conv3(x)))\n        x = self.pool(F.relu(self.conv4(x)))\n        # achatar a entrada da imagem\n        x = x.view(-1, 128 * 2 * 2)\n        # adicionar camada de desistência\n        x = self.dropout(x)\n        # adicione a 1ª camada oculta, com função de ativação relu\n        x = F.relu(self.fc1(x))\n        # adicionar camada de desistência\n        x = self.dropout(x)\n        # adicione a segunda camada oculta, com função de ativação relu\n        x = self.fc2(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# verifique se CUDA está disponível\ntrain_on_gpu = torch.cuda.is_available()\n\nif not train_on_gpu:\n    print('CUDA is not available.  Training on CPU ...')\nelse:\n    print('CUDA is available!  Training on GPU ...')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# crie uma CNN completa\nmodel = CNN()\nprint(model)\n\n# Mover modelo para GPU, se disponível\nif train_on_gpu: model.cuda()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Treinando nossa CNN\n\nAgora, as etapas de treinamento são as mesmas do treinamento de um MLP, que expliquei em um kernel anterior. A única diferença aqui é que eu estou salvando o modelo toda vez que a perda de validação diminui. Finalmente, obteremos os melhores parâmetros de modelo aprendidos. Este é um tipo de parada antecipada."},{"metadata":{"trusted":true},"cell_type":"code","source":"# especificar função de perda (perda de entropia cruzada categórica)\ncriterion = nn.CrossEntropyLoss()\n\n# especificar otimizador\noptimizer = optim.Adamax(model.parameters(), lr=0.001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# número de épocas para treinar o modelo\nn_epochs = 40\n\nvalid_loss_min = np.Inf # rastrear alteração na perda de validação\n\n# acompanhando as perdas assim que acontecem\ntrain_losses = []\nvalid_losses = []\n\nfor epoch in range(1, n_epochs+1):\n\n    # acompanhar as perdas de treinamento e validação\n    train_loss = 0.0\n    valid_loss = 0.0\n    \n    ###################\n    # treinar o modelo#\n    ###################\n    model.train()\n    for data, target in train_loader:\n        # mover tensores para GPU se CUDA estiver disponível\n        if train_on_gpu:\n            data, target = data.cuda(), target.cuda()\n        # limpe os gradientes de todas as variáveis otimizadas\n        optimizer.zero_grad()\n        # forward forward: calcula as saídas previstas passando entradas para o modelo\n        output = model(data)\n        # calcular a perda de lote\n        loss = criterion(output, target)\n        #retrocesso: calcular o gradiente da perda em relação aos parâmetros do modelo\n        loss.backward()\n        # execute uma única etapa de otimização (atualização de parâmetro)\n        optimizer.step()\n        # atualizar perda de treinamento\n        train_loss += loss.item()*data.size(0)\n        \n    ######################    \n   # validar o modelo #\n    ######################\n    model.eval()\n    for data, target in valid_loader:\n        # mova tensores para a GPU se CUDA estiver disponível\n        if train_on_gpu:\n            data, target = data.cuda(), target.cuda()\n       # forward pass: calcula as saídas previstas passando entradas para o modelo\n        output = model(data)\n        # calcular a perda de lote\n        loss = criterion(output, target)\n       # atualizar perda média de validação\n        valid_loss += loss.item()*data.size(0)\n    \n   # calcular perdas médias\n    train_loss = train_loss/len(train_loader.sampler)\n    valid_loss = valid_loss/len(valid_loader.sampler)\n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n        \n   # imprimir estatísticas de treinamento / validação\n    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n        epoch, train_loss, valid_loss))\n    \n    # salvar modelo se a perda de validação diminuiu\n    if valid_loss <= valid_loss_min:\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        valid_loss_min,\n        valid_loss))\n        torch.save(model.state_dict(), 'best_model.pt')\n        valid_loss_min = valid_loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Gráfico de desempenho\nEu lhe disse o quão poderosas as CNNs são agora olhem para esse gráfico. Atingimos resultados avançados, como prometido."},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nplt.plot(train_losses, label='Training loss')\nplt.plot(valid_losses, label='Validation loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend(frameon=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Carregar Os melhores parâmetros aprendidos com o treinamento em nosso modelo para fazer previsões mais tarde\nmodel.load_state_dict(torch.load('best_model.pt'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fazer previsões no conjunto de testes"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Desativar gradientes\nmodel.eval()\n\npreds = []\nfor batch_i, (data, target) in enumerate(test_loader):\n    data, target = data.cuda(), target.cuda()\n    output = model(data)\n\n    pr = output[:,1].detach().cpu().numpy()\n    for i in pr:\n        preds.append(i)\n\n# Criar arquivo de envio     \nsample_sub['has_cactus'] = preds\nsample_sub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ** Se você gosta deste kernel ou deseja bifurcá-lo, por favor, faça um UPVOTE para mostrar sua apreciação. **"},{"metadata":{},"cell_type":"markdown","source":"** Autor: **\n\n[Abhinand](https://www.kaggle.com/abhinand05)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}