{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this kernel, we use efficientnet to complete the binary classification task. **This kernel is especially helpful if you are making an introduction to computer vision and deep learning in general**. In order to solve this challenge, the steps I take are the following:\n1. Specify where the training and test folders are\n2. Visualize a few images to know what data we're dealing with\n3. Use Keras's ImageDataGenerator to augment the training data. If you haven't used this library before, or are new to data augmentation, take a look at this link: [http://keras.io/preprocessing/image/](http://)\n4. We use a pre-trained model called EfficientNet. You don't need to know how this works. We just feed the data to the model we obtain online, and it gives us a good accuracy.\n5. We finally make our predictions on the test images in the test zip file and format the submission.csv file to hold our own submissions!\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Installing pre-trained model - EfficientNet"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"!pip install -U efficientnet","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. Specifying Train and Test directories"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dir = '../input/train/train'\ntest_dir = '../input/test'\ntrain_df = pd.read_csv('../input/train.csv')\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. Visualising a few cactuses and non-cactuses"},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2\ncactus = []\ncactus.append(cv2.imread(train_dir + '/' + train_df['id'][0]))\ncactus.append(cv2.imread(train_dir + '/' + train_df['id'][1]))\ncactus.append(cv2.imread(train_dir + '/' + train_df['id'][2]))\n#now reading no cactus images\ncactus.append(cv2.imread(train_dir + '/' + train_df['id'][6]))\ncactus.append(cv2.imread(train_dir + '/' + train_df['id'][7]))\ncactus.append(cv2.imread(train_dir + '/' + train_df['id'][11]))\n\n\nlabels = ['cactus','cactus','cactus','no cactus','no cactus',' no cactus']\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=[10,10])\nfor x in range(0,6):\n    plt.subplot(3, 3,x+1)\n    plt.imshow(cactus[x])\n    plt.title(labels[x])\n    x += 1\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Takeout: Might be a little difficult to recognise between the two categories. Each image is 32x32 pixels. Now that we know the data we are dealing with, lets try to augment the data. Before that, Let's import a few libraries we need for our EfficientNet"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import applications\nfrom efficientnet import EfficientNetB3\nfrom keras import callbacks\nfrom keras.models import Sequential","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['has_cactus'] = train_df['has_cactus'].astype('str')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before we move on, a little explanation on ImageDataGenerator:\n* we generate two generators - one for training, and another for validation. These are stored in train_generator and val_generator. For both, we apply a series of distortions. \n* However, instead of storing all these new images in a directory, we use the method **flow_from_dataframe** to dynamically load these images as we train the model\n* However, all the distortions we made for train_gen are not applied to test_gen. This is because we don't want to augment the data in the test directory."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1/255,\n    validation_split=0.10,\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\ntrain_generator = train_datagen.flow_from_dataframe(\n    dataframe = train_df,\n    directory = train_dir,\n    x_col=\"id\",\n    y_col=\"has_cactus\",\n    target_size=(32,32),\n    subset=\"training\",\n    batch_size=1024,\n    shuffle=True,\n    class_mode=\"binary\"\n)\n\nval_generator = train_datagen.flow_from_dataframe(\n    dataframe = train_df,\n    directory = train_dir,\n    x_col=\"id\",\n    y_col=\"has_cactus\",\n    target_size=(32,32),\n    subset=\"validation\",\n    batch_size=256,\n    shuffle=True,\n    class_mode=\"binary\"\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_datagen = ImageDataGenerator(\n    rescale=1/255\n)\n\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=(32,32),\n    batch_size=1,\n    shuffle=False,\n    class_mode=None\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we used the pre-trained EfficientNet Model. The weights we used are the 'imagenet' weights. These weights are trained for object classification. This can really help our model because we don't have to train from scratch. \n\nThis is the reason we set the include_top parameter of the EfficientNet to False. Essentially, we use the already trained initial layers - which detects lower-level features like edges, etc. Our EfficientNet is very good at this. We then replace the top layers with our own dense layers to recognise higher level features : like cactus or not. This is called **Transfer Learning**\n\nFor more on Transfer learning, see [https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html](http://)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Dense\nfrom keras.optimizers import Adam\n\nefficient_net = EfficientNetB3(\n    weights='imagenet',\n    input_shape=(32,32,3),\n    include_top=False,\n    pooling='max'\n)\n\nmodel = Sequential()\nmodel.add(efficient_net)\nmodel.add(Dense(units = 120, activation='relu'))\nmodel.add(Dense(units = 120, activation = 'relu'))\nmodel.add(Dense(units = 1, activation='sigmoid'))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After we build or model, we compile it. We use Adam as the optimizer (safest choice)"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer=Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we train the network with 51 epochs!"},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit_generator(\n    train_generator,\n    epochs = 50,\n    steps_per_epoch = 15,\n    validation_data = val_generator,\n    validation_steps = 7\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have trained the network, let's plot the training vs validation accuracy and loss"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1,len(acc) + 1)\n\nplt.plot(epochs,acc,'bo',label = 'Training Accuracy')\nplt.plot(epochs,val_acc,'b',label = 'Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs,loss,'bo',label = 'Training loss')\nplt.plot(epochs,val_loss,'b',label = 'Validation Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\n\nplt.show()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the plots, we see that the model has not overfit. Training and validation accuracy are almost the same. Now, lets generate our predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model.predict_generator(\n    test_generator,\n    steps=len(test_generator.filenames)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_ids = [name.split('/')[-1] for name in test_generator.filenames]\npredictions = preds.flatten()\ndata = {'id': image_ids, 'has_cactus':predictions} \nsubmission = pd.DataFrame(data)\nprint(submission.head())","execution_count":68,"outputs":[{"output_type":"stream","text":"                                     id  has_cactus\n0  000940378805c44108d287872b2f04ce.jpg    0.501862\n1  0017242f54ececa4512b4d7937d1e21e.jpg    0.500876\n2  001ee6d8564003107853118ab87df407.jpg    0.511697\n3  002e175c3c1e060769475f52182583d0.jpg    0.510563\n4  0036e44a7e8f7218e9bc7bf8137e4943.jpg    0.497998\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Finally, we generate the submission file! After this, go ahead and commit!"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}