{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np # math (array ; .expand_dims ; .squeeze Remove 1-dimensional entries of the shape ; )\nimport pandas as pd # import dataset (.read_csv ; )\nimport cv2\n#from keras.preprocessing.image import ImageDataGenerator\n\nimport matplotlib.pyplot as plt # plotting (.imshow to render images ; )\nimport matplotlib.image as mpimg\n%matplotlib inline\n\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, Dense, Conv2D, Dropout, MaxPooling2D, LSTM, CuDNNLSTM, Flatten, Reshape, ZeroPadding2D, Convolution2D, BatchNormalization, Activation  # CuDNNLSTM only on GPU\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nfrom tqdm import tqdm, tqdm_notebook\nimport random as rn","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!ls ../input/aerial-cactus-identification","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = '../input/aerial-cactus-identification/'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading Dataframes\n\nsubmissionDf = pd.read_csv(PATH + 'sample_submission.csv')\nsubmissionDf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainDf = pd.read_csv(PATH + 'train.csv')\ntrainDf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAINPATH = '../input/aerial-cactus-identification/train/train/'\nTESTPATH = '../input/aerial-cactus-identification/test/test/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load Images\n\nX_tr = []\nY_tr = []\n\nimgs = trainDf['id'].values\n\nfor img_id in tqdm_notebook(imgs):\n    X_tr.append(cv2.imread(TRAINPATH + img_id))    \n    Y_tr.append(trainDf[trainDf['id'] == img_id]['has_cactus'].values[0])  \n    \nX_tr = np.asarray(X_tr)\nX_tr = X_tr.astype('float32')\nX_tr /= 255\nY_tr = np.asarray(Y_tr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data separation :\n### * 12000 imgs for training\n### * 3000 imgs for validation (split made in model.fit())\n### * 2500 imgs for testing\n### * Submission set of 4000 images for competition scoring"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separation train set and validation set\n\nx_tr = X_tr[:15000]\ny_tr = Y_tr[:15000]\n\nx_te = X_tr[15000:]\nx_te = X_tr[15000:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build Model"},{"metadata":{},"cell_type":"markdown","source":"## Model 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=40)\nmc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Conv2D(32, (2,2), strides=(1,1), padding='same', activation='relu', input_shape=(32,32,3)))\nmodel.add(Conv2D(32, (2,2), strides=(1,1), padding='same', activation='relu'))\nmodel.add(Conv2D(32, (2,2), strides=(1,1), padding='same', activation='relu'))\nmodel.add(Conv2D(32, (2,2), strides=(1,1), padding='same', activation='relu'))\nmodel.add(Conv2D(32, (2,2), strides=(1,1), padding='same', activation='relu'))\nmodel.add(Flatten())\nmodel.add(Dense(1, activation= 'sigmoid'))\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n \nhistory = model.fit(x_tr,y_tr,epochs=100,batch_size=20, validation_split=0.2, callbacks=[es, mc]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (history.history.keys())\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = np.vectorize(lambda x: 1 if x > 0.75 else 0)(model.predict(x_te))\npreds = np.resize(preds,(2500))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading ground truth for the test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"trues = trainDf.iloc[-2500:]['has_cactus'].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Score : Model 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.sum(preds == trues)/2500","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"model2 = Sequential()\n\nmodel2.add(Conv2D(50,kernel_size=(3,3),strides=(1,1),padding='same', activation='relu', input_shape=x_tr.shape[1:]))\nmodel2.add(MaxPooling2D(pool_size=(2,2)))\nmodel2.add(Dropout(0.2))\nmodel2.add(Conv2D(50,kernel_size=(3,3),strides=(1,1),padding='same', activation='relu'))\nmodel2.add(MaxPooling2D(pool_size=(2,2)))\nmodel2.add(Dropout(0.2))\nmodel2.add(Conv2D(50,kernel_size=(3,3),strides=(1,1),padding='same', activation='relu'))\nmodel2.add(MaxPooling2D(pool_size=(2,2)))\nmodel2.add(Dropout(0.2))\nmodel2.add(Conv2D(50,kernel_size=(3,3),strides=(1,1),padding='same', activation='relu'))\nmodel2.add(MaxPooling2D(pool_size=(2,2)))\nmodel2.add(Flatten())\nmodel2.add(Dense(1, activation= 'sigmoid'))\n\nmodel2.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"es2 = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=40)\nmc2 = ModelCheckpoint('best_model2.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n \nhistory2 = model2.fit(x_tr,y_tr,epochs=100,batch_size=20, validation_split=0.2, callbacks=[es2, mc2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (history2.history.keys())\n\nplt.plot(history2.history['loss'])\nplt.plot(history2.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds2 = np.vectorize(lambda x: 1 if x > 0.75 else 0)(model2.predict(x_te))\npreds2 = np.resize(preds2,(2500))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Score : Model 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.sum(preds2 == trues)/2500","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import load_model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Score : Saved Model 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"saved_model = load_model('best_model.h5')\n\npreds11 = np.vectorize(lambda x: 1 if x > 0.75 else 0)(saved_model.predict(x_te))\n\npreds11 = np.resize(preds11,(2500))\n\nnp.sum(preds11 == trues)/2500","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Score : Saved Model 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"saved_model2 = load_model('best_model2.h5')\n\npreds22 = np.vectorize(lambda x: 1 if x > 0.75 else 0)(saved_model2.predict(x_te))\n\npreds22 = np.resize(preds22,(2500))\n\nnp.sum(preds22 == trues)/2500","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model with batch normalization"},{"metadata":{"trusted":true},"cell_type":"code","source":"model3 = Sequential()\n\nmodel3.add(Conv2D(32, (3, 3), input_shape=x_tr.shape[1:]))\nmodel3.add(BatchNormalization())\nmodel3.add(Activation('relu'))\nmodel3.add(Conv2D(32, (3, 3)))\nmodel3.add(BatchNormalization())\nmodel3.add(Activation('relu'))\nmodel3.add(Conv2D(32, (3, 3)))\nmodel3.add(BatchNormalization())\nmodel3.add(Activation('relu'))\nmodel3.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel3.add(Conv2D(64, (3, 3)))\nmodel3.add(BatchNormalization())\nmodel3.add(Activation('relu'))\nmodel3.add(Conv2D(64, (3, 3)))\nmodel3.add(BatchNormalization())\nmodel3.add(Activation('relu'))\nmodel3.add(Conv2D(64, (3, 3)))\nmodel3.add(BatchNormalization())\nmodel3.add(Activation('relu'))\nmodel3.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel3.add(Conv2D(128, (3, 3)))\nmodel3.add(BatchNormalization())\nmodel3.add(Activation('relu'))\n\nmodel3.add(Flatten())\nmodel3.add(Dense(1024))\nmodel3.add(Activation('relu'))\nmodel3.add(Dropout(0.6))\n\nmodel3.add(Dense(256))\nmodel3.add(Activation('relu'))\nmodel3.add(Dropout(0.6))\n\nmodel3.add(Dense(1))\nmodel3.add(Activation('sigmoid'))\n\nmodel3.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"es3 = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=40)\nmc3 = ModelCheckpoint('best_model3.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n \nhistory3 = model3.fit(x_tr,y_tr,epochs=100,batch_size=20, validation_split=0.2, callbacks=[es3, mc3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (history3.history.keys())\n\nplt.plot(history3.history['loss'])\nplt.plot(history3.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Score : Model 3"},{"metadata":{"trusted":true},"cell_type":"code","source":"preds3 = np.vectorize(lambda x: 1 if x > 0.75 else 0)(model3.predict(x_te))\n\npreds3 = np.resize(preds3,(2500))\n\nnp.sum(preds3 == trues)/2500","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls -l","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"saved_model3 = load_model('best_model3.h5')\n\npreds33 = np.vectorize(lambda x: 1 if x > 0.75 else 0)(saved_model3.predict(x_te))\n\npreds33 = np.resize(preds33,(2500))\n\nnp.sum(preds33 == trues)/2500","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bestModel = saved_model3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nX_tst = []\nTest_imgs = []\n\nimgs = submissionDf['id'].values\n\nfor img_id in tqdm_notebook(imgs):\n    X_tst.append(cv2.imread(TESTPATH + img_id))     \n    Test_imgs.append(img_id)\n    \nX_tst = np.asarray(X_tst)\nX_tst = X_tst.astype('float32')\nX_tst /= 255\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predictions = bestModel.predict(X_tst)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df = pd.DataFrame(test_predictions, columns=['has_cactus'])\nsub_df['has_cactus'] = sub_df['has_cactus'].apply(lambda x: 1 if x > 0.75 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df['id'] = ''\ncols = sub_df.columns.tolist()\ncols = cols[-1:] + cols[:-1]\nsub_df=sub_df[cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, img in enumerate(Test_imgs):\n    sub_df.set_value(i,'id',img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}