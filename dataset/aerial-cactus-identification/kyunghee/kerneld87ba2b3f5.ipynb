{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\n\n%matplotlib inline\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"os.listdir('../input/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/train.csv')\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir('../input/test/test')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir('../input/train/train')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dir = '../input/train/train/'\nfilename = df['id'][0]\npath = os.path.join(data_dir, filename)\npath\n\ntest_dir = '../input/test/test/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_pil = Image.open(path)\nimage_pil","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image = np.array(image_pil)\nplt.imshow(image)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"has_cactus = df['has_cactus'][0]\nhas_cactus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image = np.array(image_pil)\nplt.title(has_cactus)\nplt.imshow(image)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(df['has_cactus']) # cactus가 포함될 비율","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.sum(df['has_cactus']), len(df['has_cactus'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.min(image), np.max(image)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#def get_label(path):\n#    if df['id'] == path.split('/')[-1]\n    \n\ndef get_data(pathtuple):\n    path, label = pathtuple\n    image_pil = Image.open(path)\n    image = np.array(image_pil)\n    image = image/255.0\n    label=tf.keras.utils.to_categorical(label,2)\n\n    #return image, label\n    return image.astype(np.float32), label.astype(np.float32)\n\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.utils.to_categorical?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heights = []\nwidths = []\ntrain_arr = []\ntest_arr = []\n\ntrain_filenames = []\nindex = 0\n\nfor filename in df['id']:\n    path = os.path.join(data_dir, filename)\n    train_filenames.append((path, df['has_cactus'][index]))\n    index = index + 1\n    \n#trainimage, trainlabel=get_data(train_filenames[0])\n#plt.imshow(trainimage)\n#print(trainlabel)\n    \nfor testfilename in os.listdir(test_dir):\n    #print(testfilename)\n    path = os.path.join(test_dir, testfilename)\n    image_pil = Image.open(path)\n    image = np.array(image_pil)\n    image = image/255.0\n    test_arr.append(image.astype(np.float32))\n    \ntest_data = np.array(test_arr)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# batch dataset\n\n#batch_paths = train_filenames[:8]\n\ndef make_batch(batch_paths):\n\n    batch_images = []\n    batch_labels = []\n\n    for pathtuple in batch_paths:\n        path, label = pathtuple\n        image, label = get_data(pathtuple)\n        batch_images.append(image)\n        batch_labels.append(label)\n\n    batch_images = np.array(batch_images)\n    batch_labels = np.array(batch_labels)\n\n    return batch_images, batch_labels\n\n#images, labels = make_batch(batch_paths)\n#images.shape, labels.shape ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_gen(data_paths, is_training=True):\n    global_step = 0\n    steps_per_epoch = len(data_paths) // batch_size\n    while True:\n        step = global_step % steps_per_epoch\n        if step == 0:\n            np.random.shuffle(data_paths)\n        images, labels = make_batch(data_paths[step*batch_size:(step+1)*batch_size])\n        global_step += 1\n        yield images, labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#generator = data_gen(train_paths)\ngenerator = data_gen(train_filenames)\n\nfor i, (img, lbl) in enumerate(generator):\n    if i < 5:\n        plt.title(i + lbl[0])\n        plt.imshow(img[0])\n        plt.show()\n        #print(lbl)\n    else:\n        break\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport os\nimport time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom IPython.display import clear_output\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\ntf.enable_eager_execution()\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\nnum_epochs = 20\nlearning_rate = 0.001\n\nnum_classes = 2\ninput_shape = (32, 32, 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# VGG16\ninputs = layers.Input(input_shape)\nnet = layers.Conv2D(64, (3, 3), padding='same')(inputs)\nnet = layers.Conv2D(64, (3, 3), padding='same')(net)\nnet = layers.Conv2D(64, (3, 3), padding='same')(net)\nnet = layers.BatchNormalization()(net)\nnet = layers.Activation('relu')(net)\nnet = layers.MaxPooling2D(pool_size=(2, 2))(net)\n\nnet = layers.Conv2D(128, (3, 3), padding='same')(net)\nnet = layers.Conv2D(128, (3, 3), padding='same')(net)\nnet = layers.Conv2D(128, (3, 3), padding='same')(net)\nnet = layers.BatchNormalization()(net)\nnet = layers.Activation('relu')(net)\nnet = layers.MaxPooling2D(pool_size=(2, 2))(net)\nnet = layers.Dropout(0.25)(net)\n\nnet = layers.Conv2D(256, (3, 3), padding='same')(net)\nnet = layers.Conv2D(256, (3, 3), padding='same')(net)\nnet = layers.Conv2D(256, (3, 3), padding='same')(net)\nnet = layers.BatchNormalization()(net)\nnet = layers.Activation('relu')(net)\nnet = layers.MaxPooling2D(pool_size=(2, 2))(net)\nnet = layers.Dropout(0.25)(net)\n\nnet = layers.Conv2D(512, (3, 3), padding='same')(net)\nnet = layers.Conv2D(512, (3, 3), padding='same')(net)\nnet = layers.Conv2D(512, (3, 3), padding='same')(net)\nnet = layers.BatchNormalization()(net)\nnet = layers.Activation('relu')(net)\nnet = layers.MaxPooling2D(pool_size=(2, 2))(net)\nnet = layers.Dropout(0.25)(net)\n\nnet = layers.Conv2D(512, (3, 3), padding='same')(net)\nnet = layers.Conv2D(512, (3, 3), padding='same')(net)\nnet = layers.Conv2D(512, (3, 3), padding='same')(net)\nnet = layers.BatchNormalization()(net)\nnet = layers.Activation('relu')(net)\nnet = layers.MaxPooling2D(pool_size=(2, 2))(net)\nnet = layers.Dropout(0.25)(net)\n\n# net = layers.GlobalAveragePooling2D()(net)\nnet = layers.Flatten()(net)\nnet = layers.Dense(512)(net)\nnet = layers.Activation('relu')(net)\nnet = layers.Dropout(0.5)(net)\nnet = layers.Dense(num_classes)(net)\nnet = layers.Activation('softmax')(net)\n\nmodel = tf.keras.Model(inputs=inputs, outputs=net)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', \n             optimizer=tf.keras.optimizers.Adam(learning_rate),\n             metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#callbacks = [\n#    tf.keras.callbacks.TensorBoard(log_dir=os.path.join(os.getcwd(),'logs'))\n#]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit_generator?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"steps_per_epoch = len(train_filenames) // batch_size\n\nhistory=model.fit_generator(generator=data_gen(train_filenames),\n                    steps_per_epoch=steps_per_epoch,\n                    epochs=num_epochs,\n                    verbose=1)\n                    #callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(history.history.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predictions = []\n\nfor i in range(test_data.shape[0]):\n#for i in range(1):\n    predictions = model.predict(np.expand_dims(test_data[i],0))\n    test_predictions.append(np.argmax(tf.squeeze(predictions)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(test_predictions[0])\n#len(test_predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_filenames=os.listdir(test_dir)\n\ntest_df = pd.DataFrame({'id': test_filenames, 'has_cactus': test_predictions },columns=['id','has_cactus'])\n\ntest_df.to_csv('test_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_batches_per_epoch = len(test_paths) // batch_size\n#model.evaluate_generator(data_gen(test_paths, False),\n#                        steps = test_batches_per_epoch,\n#                        verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model.predict(np.expand_dims(image,0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model.save(model.name+'_cactus.h5')\n\n#load model\n#model = tf.keras.models.load_model(model.name + '_cactus.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!tensorboard --logdir=./logs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#t = [('a',1),('b',2),('c',3)]\n#np.random.shuffle(t)\n#print(t)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#(df['id']==path.split('/')[-1])==1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from tqdm import tqdm_notebook","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}