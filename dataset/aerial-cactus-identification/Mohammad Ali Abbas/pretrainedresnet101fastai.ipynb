{"cells":[{"metadata":{},"cell_type":"markdown","source":"Loading All Libraries to get desire functionality. \n* **fastaI** for overall model fitting , training and testing\n* **pathlib** for making string to Path so that can work with Python OS Module\n* **os** for getting working of Operating System like creating directory , getting files etc\n* **pandas** to open CSV as Data Frames and Data Table Operation\n* **seaborn and matplotlib** for ploting graphs\n* **PIL** *Python Image Library* for opening images and some image operation\n\n\n**matplotlib inline is method to work with graph inside notebook**\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from fastai.utils import *\nfrom fastai.vision import *\nfrom fastai.callbacks import *\nfrom pathlib import Path\nimport os\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport PIL\nfrom torch.utils import model_zoo\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking our input directory using ****ls**** Command\n\n*CMD Commands / DOS Command start with ! (marks of exclamiation). *"},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Setting Path Input path , from which our data loader will open images . data_path is Python pathlib object which is reciving input path as a string. **train_df** is a pandas data frame created with train.csv files. Previous mentioned files have training images id and lables. **test_df** pandas frame have label of test image dataset from sample_submission.csv ."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_path = Path(\"../input/\")\ntrain_df = pd.read_csv(data_path/'train.csv')\ntest_df = pd.read_csv(data_path/'sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To get Visualization about clases are balanced or not."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sns.countplot('has_cactus', data=train_df)\nplt.title('Classes', fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As clases are unblanced one class have 3x more images than other so we have to equalize both classes. We can perform two apporches \n1. Decrease the size of larger class.\n2. Increase the size of shorter class.\nWe will perform 2nd approch in later cell to balance classes."},{"metadata":{},"cell_type":"markdown","source":"Let plot some images from both classes. Images are open by **Python Image Library** , you can use **OpenCV** also which is comprehensive library having Computer Vision and Image Processing operation. We do not have require any rich image processing operation till now so we think PIL is enough till now."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(8,6))\n\ni = 0\nsample = train_df.sample(12)\nfor row in sample.iterrows():\n    img_name = row[1][0]\n    img = PIL.Image.open(data_path/'train'/'train'/img_name)\n    i += 1\n    plt.subplot(3,4,i)\n    title = 'Not Cactus (0)'\n    if row[1][1] == 1:\n        title = 'Cactus (1)'\n    plt.title(title, fontsize=10)\n    plt.imshow(img)\n    plt.axis('off')\n\nplt.subplots_adjust(top=0.90)\nplt.suptitle('Sample of Images', fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = train_df[train_df.has_cactus==0].copy()\ndf2 = df1.copy()\ntrain_df = train_df.append([df1, df2], ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A quick plot verifies that the dataset is now very close to being balanced."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sns.countplot('has_cactus', data=train_df)\nplt.title('Oversampled Classes', fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Augmentation\nFast.ai has a powerful set of transformations [built into the library](https://docs.fast.ai/vision.transform.html). The has default setting which have experimentation been shown to be a good starting point for regular photos. These default augmentations include flipping on the horizontal axis, rotating, zooming, changing the lighting, and warping and are applied randomly on each photo during a training epoch.\n\nSince this kernel uses aerial images, I enable flipping on the vertical axis by setting `flip_vert=True`. A trick I picked up from looking at some of the other fast.ai kernels (like this one by [Alexander Milekhin](https://www.kaggle.com/kenseitrg/simple-fastai-exercise)) is to upscale the original 32x32 images to 128x128, so the images are large enough for the fast.ai rotation, zooming, and warping transformations to be useful."},{"metadata":{"trusted":true},"cell_type":"code","source":"tfms = get_transforms(do_flip=True, flip_vert=True, max_rotate=20, max_lighting=0.3, max_warp=0.2, max_zoom=1.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I then use the [data block api](https://docs.fast.ai/data_block.html) to finish constructing the dataset, holding back 20 percent of the training images as a validation set, labeling the images from the `train_df` dataframe, and adding the test images for ease of use later."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_images = ImageList.from_df(test_df, path=data_path/'test', folder='test')\n\nsrc = (ImageList.from_df(train_df, path=data_path/'train', folder='train')\n       .split_by_rand_pct(0.2)\n       .label_from_df()\n       .add_test(test_images))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When performing 1cycle learning, its recommended to use a large batch size. However, for this dataset using large batch sizes of 1024 or 2048 performed worse than smaller batch sizes. After some experimentation, I concluded that a batch size of 256 works well for training the model. \n\nWhen applying transformations that changes the image size, such as perspective warping, the best performing method of dealing with squaring the now warped image is to reflect the image data to fill the edges. There is a bug in the underlying pytorch reflection method which prevents it from being used on all datasets but given the regularity of these images reflection works fine.\n\nSince I am using transfer learning via a ResNet-34 trained on ImageNet, I need to normalize the Aerial Cactus images with `imagenet_stats` to match the pre-trained model."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = (src.transform(tfms, \n                     size=128,\n                     resize_method=ResizeMethod.PAD, \n                     padding_mode='reflection')\n        .databunch(bs=256)\n        .normalize(imagenet_stats))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A quick sanity check shows that both classes have loaded correctly."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.classes, data.c","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting `show_batch` gives a visual example of the images with the data augmentation from the fast.ai transformer applied."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.show_batch(rows=3, figsize=(9,9))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training the Model\nBehind the scenes, the fast.ai `cnn_learner` has stripped out the last few layers of the ResNet-34 and replaced them with a few untrained layers which ends in a linear layer to predict the two classes. The pretrained ResNet-34 model is frozen and is not allowed to change while the newly created layers will be trained to predict cactus or not cactus. After training the new layers, the pretrained ResNet-34 layers can be unfrozen and the whole model trained on the dataset.\n\nThe `cnn_learner` has sensible defaults for hyperparameters such as dropout, weight decay, and momentum. After some experimentation I concluded that the fast.ai defaults perform pretty well on this dataset and have left them be.\n\nSince this kernel is being evaluated on the [area under the ROC curve](http://en.wikipedia.org/wiki/Receiver_operating_characteristic) (AUROC) between the predicted probabilities and the observed targets, I will include `AUROC` as a metric which the learner will output while training."},{"metadata":{"trusted":true},"cell_type":"code","source":"\nlearn = cnn_learner(data,\n                    models.resnet101,pretrained=True,\n                    metrics=[accuracy, AUROC()],path='',model_dir='work')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The learning rate hyperparameter is one of the most important hyperparameters to set, and the `lr_find` method provides a graphical way to set a good learning rate. After plotting the losses, one looks for the steepest slope where the loss is decreasing the fastest. There are multiple selections that could be made, and I have chosen the steepest looking slope right before the incline decreases at 1e-3."},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.lr_find(stop_div=True)\nlearn.recorder.plot(suggestion=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will train the frozen layers of the model using `fit_one_cycle` for five epochs."},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = 3.98E-04\nlearn.fit_one_cycle(5, lr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The learner has a built in method for plotting the training and validation loss."},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.recorder.plot_losses()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## A Digression on Fit One Cycle\n`fit_one-cycle` works by taking the learning rate, dividing it by ten, and then gradually increasing and then decreasing the learning rate as training progresses. A simplified version of the idea behind 1cycle learning is increasing the learning rate allows the model to escape any suboptimal local minima. Then decreasing the learning rate assists the model in selecting a good minimum. The momentum of the model is adjusted in the opposite direction so the model does not overshoot while at the highest learning rate, but still has enough momentum to find a new minima at the lower learning rates.\n\nThe charts below show the cyclical learning rate and momentum during the training of the frozen ResNet-34 model. \n\nFor more details on 1cycle learning, check out [Sylvain Gugger's post](https://sgugger.github.io/the-1cycle-policy.html) on the subject."},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.recorder.plot_lr(show_moms=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training the Unfrozen Model\nFor many tasks, the output of the ResNet-34 model after five epochs would be satisfactory. But should the initial training not result in such highly accurate model, you can unfreeze the frozen layers using the `unfreeze` method of `learner` and then run `lr_find` to select a discriminative learning rate. The learning rate will be lower for the first layers of the model and then increase for the last layers of the model. A rule of thumb for selecting a good learning rate for the whole model is to pick the higher slice to be ten times before the loss jumps and then set the lower slice to be the original learning rate divided by five or ten.\n\nIn this case, I won't be using the normal procedure and instead will be using `freeze_to(1)`. This keeps the first half of the pre-trained ResNet-34 model frozen while allowing the second half to train on this dataset. The rational is the first layers are already good at picking out lines, shapes, combinations of shapes, and objects of increasing complexity. While the second half is good at identifying the wide variety of objects in ImageNet, so training the second half on this dataset will allow the model to specialize on this competition's desert fauna. "},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save('step-1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls work","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"While the default fast.ai settings worked well for the initial frozen training, I will set the weight decay `wd` to 0.1 and a range of dropout `ps` to be 0.6 and 0.4 for the unfrozen portions of the model. For this training, the model does not steadily progress in accuracy, AUROC, or training and validation loss. I will use the `SaveModelCallback` to keep track of the best performing model as determined by validation loss and use it at the end of training.\n\nI will create a new learner with these settings and then load the weights trained in the previous step."},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = cnn_learner(data,\n                    models.resnet101, pretrained=True,\n                    metrics=[accuracy, AUROC()], \n                    callback_fns=[partial(SaveModelCallback)],\n                    wd=0.1,\n                    ps=[0.9, 0.6, 0.4],\n                    path = '')\nlearn = learn.load('../work/step-1')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next use `lr_find` again to to select a discriminative learning rate."},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.freeze_to(4)\nlearn.lr_find()\nlearn.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this case the optimal learning rates will not differ too much: 4e-4 and 2e-4."},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.recorder.plot_losses()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interp = ClassificationInterpretation.from_learner(learn)\n\ninterp.plot_confusion_matrix(figsize=(2,2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interp.plot_top_losses(4, figsize=(6,6))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probability, classification = learn.get_preds(ds_type=DatasetType.Test)\ntest_df.has_cactus = probability.numpy()[:, 0]\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}