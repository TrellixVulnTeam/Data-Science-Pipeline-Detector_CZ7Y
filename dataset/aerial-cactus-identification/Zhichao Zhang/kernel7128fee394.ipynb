{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport cv2\nfrom matplotlib import pyplot as plt\nimport gc\nprint(os.listdir(\"../input\"))\ngc.collect()\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os,cv2\nfrom IPython.display import Image\nfrom keras.preprocessing import image\nfrom keras import optimizers\nfrom keras import layers,models\nfrom keras.applications.imagenet_utils import preprocess_input\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom keras import regularizers\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.vgg16 import VGG16\nimport tensorflow as tf\nimport numpy as np\nimport time\nimport albumentations","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.getcwd()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_y=pd.read_csv('../input/train.csv')\ntest_y=pd.read_csv('../input/sample_submission.csv')\ntrain_y['has_cactus']=train_y['has_cactus'].astype(str)\n\ntrain_y['has_cactus'][16500:].value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_data(x):\n    train_dir=\"../input/train/train\"\n    test_dir=\"../input/test/test\"\n    train_x=[]\n    test_x=[]\n    filename=[]\n    filenamey=[]\n    for datefile in os.listdir('../input/train/train'):\n                                \n        img=cv2.imread(os.path.join(train_dir,datefile))\n        filename.append(datefile)\n        train_x.append(img)\n\n\n    for datefile in os.listdir('../input/test/test'):\n                                \n        img=cv2.imread(os.path.join(test_dir,datefile))\n        filenamey.append(datefile)\n        test_x.append(img)\n    \n    \n    label=pd.DataFrame({'id':filename})\n    train_y=label.merge(x,how='left',on='id')\n\n    train_Y=train_y['has_cactus']\n    train_Y=np.array(pd.get_dummies(train_Y))\n    \n    trm_x=train_x.copy()\n    trm_y=train_Y\n    for r in range(8):\n        for i in train_y[train_y['has_cactus']=='0'].index:\n            img=augs()(image=train_x[i])[\"image\"]\n            trm_x.append(img)\n            trm_y=np.append(trm_y,[train_Y[i]],axis=0)   \n         \n    for r in range(2):    \n        for i in train_y[train_y['has_cactus']=='1'].index:\n            img=augs()(image=train_x[i])[\"image\"]\n            trm_x.append(img)\n            trm_y=np.append(trm_y,[train_Y[i]],axis=0)  \n    return trm_x,trm_y,test_x,filenamey\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def augs(p=0.5):\n    return albumentations.Compose([\n        albumentations.HorizontalFlip(),\n        albumentations.VerticalFlip(),\n        albumentations.RandomBrightness(),\n    ], p=p)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trm_x,trm_y,test_x,filenamey=load_data(train_y)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sum(train_y['has_cactus']=='0')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(trm_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.sum(trm_y,axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trm_x=[i/255 for i in trm_x]\ntrm_x=np.array(trm_x)\ntest_x=[i/255 for i in test_x]\ntest=np.array(test_x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trm_x.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"keepprob=0.8\ntf.reset_default_graph()\n\n\ndef max_pool_2x2(x):\n    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n\n\nx=tf.placeholder(tf.float32,[None,32,32,3],name='x_image')\ny=tf.placeholder(tf.float32,[None,2],name='y')\n\nW_conv1=tf.get_variable('W1',shape=[3,3,3,16],initializer=tf.truncated_normal_initializer(mean=0.0,stddev=0.1))\nb_conv1=tf.get_variable('b1',shape=[16],initializer=tf.constant_initializer(0.1))\n\nh_conv1=tf.nn.relu(tf.nn.conv2d(x,W_conv1,strides=[1,1,1,1],padding='SAME')+b_conv1)\n\n\nW_conv12=tf.get_variable('W12',shape=[3,3,16,16],initializer=tf.truncated_normal_initializer(mean=0.0,stddev=0.1))\nb_conv12=tf.get_variable('b12',shape=[16],initializer=tf.constant_initializer(0.1))\n\nh_conv12=tf.nn.relu(tf.nn.conv2d(h_conv1,W_conv12,strides=[1,1,1,1],padding='SAME')+b_conv12)\n\nh_pool1=max_pool_2x2(h_conv12)\n\nW_conv2=tf.get_variable('W2_1',shape=[3,3,16,32],initializer=tf.truncated_normal_initializer(mean=0.0,stddev=0.1))\nb_conv2=tf.get_variable('b2_1',shape=[32],initializer=tf.constant_initializer(0.1))\n\nh_conv2=tf.nn.relu(tf.nn.conv2d(h_pool1,W_conv2,strides=[1,1,1,1],padding='SAME')+b_conv2)\n\nW_conv22=tf.get_variable('W2_2',shape=[3,3,32,32],initializer=tf.truncated_normal_initializer(mean=0.0,stddev=0.1))\nb_conv22=tf.get_variable('b2_2',shape=[32],initializer=tf.constant_initializer(0.1))\n\nh_conv22=tf.nn.relu(tf.nn.conv2d(h_conv2,W_conv22,strides=[1,1,1,1],padding='SAME')+b_conv22)\nh_pool2=max_pool_2x2(h_conv22)\n\nW_conv3=tf.get_variable('W3',shape=[3,3,32,64],initializer=tf.truncated_normal_initializer(mean=0.0,stddev=0.1))\nb_conv3=tf.get_variable('b3',shape=[64],initializer=tf.constant_initializer(0.1))\nh_conv3=tf.nn.relu(tf.nn.conv2d(h_pool2,W_conv3,strides=[1,1,1,1],padding='SAME')+b_conv3)\n\nW_conv32=tf.get_variable('W32',shape=[3,3,64,64],initializer=tf.truncated_normal_initializer(mean=0.0,stddev=0.1))\nb_conv32=tf.get_variable('b32',shape=[64],initializer=tf.constant_initializer(0.1))\nh_conv32=tf.nn.relu(tf.nn.conv2d(h_conv3,W_conv32,strides=[1,1,1,1],padding='SAME')+b_conv32)\n\nW_conv33=tf.get_variable('W33',shape=[3,3,64,64],initializer=tf.truncated_normal_initializer(mean=0.0,stddev=0.1))\nb_conv33=tf.get_variable('b33',shape=[64],initializer=tf.constant_initializer(0.1))\nh_conv33=tf.nn.relu(tf.nn.conv2d(h_conv32,W_conv33,strides=[1,1,1,1],padding='SAME')+b_conv33)\n\nh_pool3=max_pool_2x2(h_conv33)\n\n\nW_conv4=tf.get_variable('W4',shape=[3,3,64,128],initializer=tf.truncated_normal_initializer(mean=0.0,stddev=0.1))\nb_conv4=tf.get_variable('b4',shape=[128],initializer=tf.constant_initializer(0.1))\nh_conv4=tf.nn.relu(tf.nn.conv2d(h_pool3,W_conv4,strides=[1,1,1,1],padding='SAME')+b_conv4)\n\nW_conv42=tf.get_variable('W42',shape=[3,3,128,128],initializer=tf.truncated_normal_initializer(mean=0.0,stddev=0.1))\nb_conv42=tf.get_variable('b42',shape=[128],initializer=tf.constant_initializer(0.1))\nh_conv42=tf.nn.relu(tf.nn.conv2d(h_conv4,W_conv42,strides=[1,1,1,1],padding='SAME')+b_conv42)\n\n\nh_pool4=max_pool_2x2(h_conv42)\n\n\n\n\nW_fc1=tf.get_variable(\"Wf1_2\",shape=[2*2*128,512],initializer=tf.truncated_normal_initializer(mean=0.0,stddev=0.1))\nb_fc1=tf.get_variable('bf2',shape=[512],initializer=tf.constant_initializer(0.1))\nh_pool_flat=tf.reshape(h_pool4,[-1,2*2*128])\nh_fc1=tf.nn.relu(tf.matmul(h_pool_flat,W_fc1)+b_fc1)\nkeep_prob=tf.placeholder(tf.float32,name='keep_prob')\nh_fc1_drop=tf.nn.dropout(h_fc1,keep_prob)\nW_fc2=tf.get_variable('Wf3_2',shape=[512,256],initializer=tf.truncated_normal_initializer(mean=0.0,stddev=0.1))\nb_fc2=tf.get_variable('bf3',shape=[256],initializer=tf.constant_initializer(0.1))\nh_fc2=tf.nn.relu(tf.matmul(h_fc1_drop,W_fc2)+b_fc2)\nh_fc2_drop=tf.nn.dropout(h_fc2,keep_prob)\n\nW_fc3=tf.get_variable('Wf4_2',shape=[256,2],initializer=tf.truncated_normal_initializer(mean=0.0,stddev=0.1))\nb_fc3=tf.get_variable('bf4',shape=[2],initializer=tf.constant_initializer(0.1))\n\ny_conv=tf.matmul(h_fc2_drop,W_fc3)+b_fc3\n\n\ncross_entropy=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=y_conv))\ntrain_step=tf.train.AdamOptimizer(learning_rate=0.001,beta1=0.9).minimize(cross_entropy)\ncorrect_prediction=tf.equal(tf.argmax(y_conv,1),tf.argmax(y,1))\naccuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n\ntf.add_to_collection('y_conv',y_conv)\ntf.add_to_collection('cross_entropy',cross_entropy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mybatch=50\niterations=20\n\ndef train_model(mybatch,iterations,m,n,keepprob):\n    \n    init = tf.global_variables_initializer()\n    \n    #saver=tf.train.Saver(max_to_keep=1)\n\n    with tf.Session() as sess:      \n        sess.run(init)\n        a=0.8\n        indice=list(range(len(m)))\n        np.random.shuffle(indice)\n        m=m[indice]\n        n=n[indice]\n        x1=m[15000:]\n        y1=n[15000:]\n        x2=m[:15000]\n        y2=n[:15000]\n        for s in range(iterations):\n            t0=time.time()\n            myindice=list(range(len(x1)))\n            np.random.shuffle(myindice)\n           \n            x_=x1[myindice]\n            y_=y1[myindice]\n\n            for i in range(len(x1)//mybatch):\n                sess.run(train_step,feed_dict={x:x_[i*mybatch:i*mybatch+mybatch],y:y_[i*mybatch:i*mybatch+mybatch],keep_prob:keepprob})\n      \n            t1=time.time()\n            print(\"traintime:%.2f s\"%(t1-t0))\n            print(sess.run(cross_entropy,feed_dict={x:x_[:1000],y:y_[:1000],keep_prob:1.0}))  \n            \n            #train_acc=sess.run(accuracy,feed_dict={x:x2,y:y2,keep_prob:1.0})\n            t2=time.time()\n            print(\"evaltime:%.2f s\"%(t2-t0))\n            #print(\"eval:%1.4f\"%(train_acc))\n            cross_val=sess.run(cross_entropy,feed_dict={x:x2,y:y2,keep_prob:1.0})\n            print(\"evalcross:%1.4f\"%(cross_val))\n            if cross_val<a:\n                #saver.save(sess,'basemode.ckpt')               \n                a=cross_val\n                pre_plate=sess.run(y_conv,feed_dict={x:test,keep_prob:1.0})\n                predict=np.argmax(pre_plate,axis=1)\n        return predict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict=train_model(mybatch,iterations,trm_x,trm_y,keepprob)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub=pd.DataFrame({\"id\":filenamey,'has_cactus':predict})\nrealsub=test_y[['id']]\nsub=realsub.merge(sub,how='left',on='id')\n\nsub.to_csv(\"sample_submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}