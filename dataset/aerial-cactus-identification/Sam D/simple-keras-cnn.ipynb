{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Dense,Conv2D,Flatten,Dropout,MaxPooling2D\nfrom tensorflow.keras.optimizers import Adam\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pathlib\n\nDEV_MODE = False\ndata_root_path = pathlib.Path(\"../input\")","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# will feed through data set map\ndef load_and_preprocess_image(path):\n    image = tf.io.read_file(path)\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.resize(image, [32, 32])\n    \n    image /= 255.0  # normalize to [0,1] range\n    return image","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# augmentations will feed through training data set\ndef random_bright(image):\n    return tf.image.random_brightness(image, 0.12)\n            \ndef random_contrast(image):\n    return tf.image.random_contrast(image, 0.9, 1.1)\n\ndef augment_image(image,label):\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    image = random_contrast(image)\n    image = random_bright(image)\n    return image,label","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read training csv\ntrain_df = pd.read_csv(data_root_path/'train.csv', dtype={'id': 'str', 'has_cactus': np.int32})\nDATASET_SIZE = len(train_df)\nprint(\"n =\", DATASET_SIZE)","execution_count":6,"outputs":[{"output_type":"stream","text":"n = 17500\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fix distribution in training data since there are more 1's than 0's\n# if there are 75% 1's then by guessing 1 we are already at 75% accuracy\nno_cactus, yes_cactus = train_df.has_cactus.value_counts().sort_values().values\nno_multiplier = int(yes_cactus/no_cactus)-1\nno_cactus_rows = train_df[train_df.has_cactus == 0]\nfor i in range(no_multiplier):\n    train_df = train_df.append(no_cactus_rows)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ROOT_TRAIN_PATH = '../input/train/train/'\ntrain_paths = ROOT_TRAIN_PATH + train_df['id']","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dataset\npath_ds = tf.data.Dataset.from_tensor_slices(train_paths)\nimage_ds = path_ds.map(load_and_preprocess_image)\nlabel_ds = tf.data.Dataset.from_tensor_slices(tf.cast(train_df['has_cactus'].values, tf.int32))","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IMAGE_SHAPE = (32, 32, 3)\n\n# batching\nimage_label_ds = tf.data.Dataset.zip((image_ds, label_ds)).shuffle(DATASET_SIZE)\n\nBATCH_SIZE = 16\n\nif DEV_MODE:\n    train_size, val_size = int(0.7 * DATASET_SIZE), int(0.3 * DATASET_SIZE)\nelse:\n    train_size, val_size = DATASET_SIZE, 0\n\ntrain_ds = image_label_ds.map(augment_image).take(train_size).batch(BATCH_SIZE).repeat()\nval_ds = image_label_ds.skip(train_size).batch(BATCH_SIZE).repeat()","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model():\n    model = Sequential()\n    \n    model.add(Conv2D(32, (2, 2),  input_shape=IMAGE_SHAPE,  activation='relu'))\n    model.add(Conv2D(64, (3, 3),  activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    \n    model.add(Conv2D(128, (4, 4),  activation='relu'))\n    model.add(Conv2D(64, (5, 5), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n\n    \n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(16, activation='relu'))\n    model.add(Dense(1, activation=\"sigmoid\"))\n    \n    return model\n\nmodel = get_model()\n\nmodel.compile(optimizer='adam', \n              loss='binary_crossentropy',\n              metrics=['acc'])\n\nmodel.summary()","execution_count":17,"outputs":[{"output_type":"stream","text":"_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_4 (Conv2D)            (None, 31, 31, 32)        416       \n_________________________________________________________________\nconv2d_5 (Conv2D)            (None, 29, 29, 64)        18496     \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 14, 14, 64)        0         \n_________________________________________________________________\nconv2d_6 (Conv2D)            (None, 11, 11, 128)       131200    \n_________________________________________________________________\nconv2d_7 (Conv2D)            (None, 7, 7, 64)          204864    \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 3, 3, 64)          0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 576)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 128)               73856     \n_________________________________________________________________\ndense_4 (Dense)              (None, 16)                2064      \n_________________________________________________________________\ndense_5 (Dense)              (None, 1)                 17        \n=================================================================\nTotal params: 430,913\nTrainable params: 430,913\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# lets training re-augment images multiple times per epoch\ntrain_steps = 2*(train_size//BATCH_SIZE)\nval_steps = val_size//BATCH_SIZE\n\nif val_steps == 0:\n    val_steps = None\n    val_ds = None\n\nhistory = model.fit(train_ds,\n                    steps_per_epoch=train_steps, \n                    epochs=50,\n                    validation_data=val_ds, \n                    validation_steps=val_steps,\n                    callbacks=None)","execution_count":19,"outputs":[{"output_type":"stream","text":"2186/2186 [==============================] - 37s 17ms/step - loss: 0.1338 - acc: 0.9475\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot accuracy\nplt.plot(history.history['acc'], label=\"train acc\")\nif DEV_MODE:\n    plt.plot(history.history['val_acc'], label=\"val acc\")\nplt.legend()\nplt.show()","execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#evaluate on non augmented images, should not be worse\ntraining_eval = tf.data.Dataset.zip((image_ds, label_ds)).shuffle(DATASET_SIZE)\ntraining_eval = training_eval.take(DATASET_SIZE).batch(DATASET_SIZE)\nmodel.evaluate(training_eval, steps=1)","execution_count":21,"outputs":[{"output_type":"stream","text":"\r1/1 [==============================] - 10s 10s/step - loss: 0.0734 - acc: 0.9736\n","name":"stdout"},{"output_type":"execute_result","execution_count":21,"data":{"text/plain":"[0.07337398082017899, 0.9736]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test dataset setup\ntest_paths = [path for path in sorted(pathlib.Path('../input/test/test/').glob('*.jpg'))]\nTEST_SIZE = len(test_paths)\ntest_path_ds = tf.data.Dataset.from_tensor_slices([str(path)for path in test_paths])\ntest_image_ds = test_path_ds.map(load_and_preprocess_image)\ntest_image_ds = test_image_ds.take(TEST_SIZE).batch(TEST_SIZE)","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predictions\npreds=model.predict(test_image_ds, steps = 1)","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#write submission csv\ntest_df=pd.DataFrame({'id': [path.name for path in test_paths] })\ntest_df['has_cactus']=preds\ntest_df.to_csv(\"submission.csv\",index=False)","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":1}