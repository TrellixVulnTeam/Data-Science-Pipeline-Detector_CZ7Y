{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **This was created by a beginner for a beginner.**\n\nIt's common to want to learn about Transfer Learning and Fine-Tuning, but stumble over the process of getting the image files into a form that can be trained on a model. Maybe it's just me...\n\nYou can create a simple model and submission file in this notebook.ã€€So you can focus on studying data augmentation and Transfer Learning.\n\nIt's a newbie's notebook, so I'm sorry if the code is messy and the processing isn't as smart as it should be."},{"metadata":{"trusted":true,"collapsed":true,"_kg_hide-output":true,"_kg_hide-input":false},"cell_type":"code","source":"!unzip /kaggle/input/aerial-cactus-identification/train.zip\n!unzip /kaggle/input/aerial-cactus-identification/test.zip","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport sys\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib.image as pimg\nimport seaborn as sns\nimport math\nfrom tqdm import tqdm\nfrom PIL import Image\n\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import load_img, ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, GlobalMaxPooling2D\nfrom tensorflow.keras import optimizers, regularizers\nfrom tensorflow.keras.callbacks import Callback, EarlyStopping, LearningRateScheduler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sys.version)\nprint('tensorflow -> ', tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# When implementing machine learning code in TensorFlow, \n# TensorFlow often uses pseudo-random seed for things like weight initialization.\n# As a result, the results change each time the code is re-executed, \n# and it's impossible to tell whether the result is due to a change in data or parameters or a random seed.\n# Therefore, The random seed needs to be fixed.\n\nnp.random.seed(12)\ntf.random.set_seed(12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"main_df = pd.read_csv('/kaggle/input/aerial-cactus-identification/train.csv')\nsub_df = pd.read_csv('/kaggle/input/aerial-cactus-identification/sample_submission.csv')\n\ntrain_dir = '/kaggle/working/train/'\ntest_dir = '/kaggle/working/test/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"main_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('shape: ', main_df.shape)\nprint('===================================')\nprint(main_df['has_cactus'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('default')\nsns.set()\nsns.set_style('whitegrid')\nsns.set_palette('Pastel2')\n\nx = ['has cactus', 'hasn\\'t cactus']\ny = main_df.groupby('has_cactus').size()\n\nfig = plt.figure()\nax = fig.add_subplot(1, 1, 1)\nax.pie(y, labels=x, autopct=\"%1.1f%%\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(2, 5, figsize = (12,6))\n\nfor i, idx in enumerate(main_df[main_df['has_cactus'] == 1]['id'][-5:]):\n    path = os.path.join(train_dir, idx)\n    img = load_img(path)\n    ax[0, i].axis('off')\n    ax[0, i].set_title('has cactus')\n    ax[0, i].imshow(img)\n    \nfor i, idx in enumerate(main_df[main_df['has_cactus'] == 0]['id'][-5:]):\n    path = os.path.join(train_dir, idx)\n    img = load_img(path)\n    ax[1, i].axis('off')\n    ax[1, i].set_title('hasn\\'t cactus')\n    ax[1, i].imshow(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df, val_df = train_test_split(main_df, test_size=0.25, stratify=main_df['has_cactus'], shuffle=True, random_state=12)\n\ntrain_df = train_df.reset_index()\nval_df = val_df.reset_index()\n\ntotal_train = train_df.shape[0]\ntotal_val = val_df.shape[0]\n\nprint('total_train: {}, total_val: {}'.format(total_train, total_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_width, img_height = 32, 32\ntarget_size = (img_width, img_height)\n\n# Define Data Augmentation\ntrain_datagen = ImageDataGenerator(rescale=1./255)\nval_datagen = ImageDataGenerator(rescale=1./255)\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\n\n# Convert the data type of 'has_cactus' to str to allow the model to be trained.\ntrain_df['has_cactus'] = train_df['has_cactus'].astype(str)\nval_df['has_cactus'] = val_df['has_cactus'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\nx_col, y_col = 'id', 'has_cactus'\nclass_mode = 'binary'\n\n\ntrain_gen = train_datagen.flow_from_dataframe(train_df,\n                                            train_dir,\n                                            x_col=x_col,\n                                            y_col=y_col,\n                                            class_mode=class_mode,\n                                            target_size=target_size,\n                                            batch_size=batch_size,\n                                            )\n\nval_gen = val_datagen.flow_from_dataframe(val_df,\n                                        train_dir,\n                                        x_col=x_col,\n                                        y_col=y_col,\n                                        class_mode=class_mode,\n                                        target_size=target_size,\n                                        batch_size=batch_size,\n                                        )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_shape = (img_width, img_height, 3)\noptimizer = optimizers.Adam(lr=1e-3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu', input_shape=input_shape))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(64, kernel_size=(3,3), padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(128, kernel_size=(3,3), padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(GlobalMaxPooling2D())\n\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(1, activation='sigmoid'))\n\n\nmodel.compile(loss='binary_crossentropy', metrics=['acc'], optimizer=optimizer)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Vary the learning rate according to the number of epochs.\ndef step_decay(epoch):\n    initial_rate = 0.001\n    drop = 0.5\n    epochs_drop = 10.0\n    lrate = initial_rate * math.pow(drop, math.floor((epoch) / epochs_drop))\n    \n    return lrate","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lrate = LearningRateScheduler(step_decay)\nes = EarlyStopping(monitor='val_loss', min_delta=0, patience=5)\n\ncallbacks = [lrate, es]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"epochs = 30\n\nhistory = model.fit(\n    train_gen,\n    epochs=epochs,\n    steps_per_epoch=total_train//batch_size,\n    validation_data=val_gen,\n    validation_steps=total_val//batch_size,\n    callbacks=callbacks,\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_palette('Dark2')\nfig,ax = plt.subplots(2, 1)\n\nplot_acc = pd.DataFrame({'acc': history.history['acc'],\n                         'val_acc': history.history['val_acc']})\n\nplot_loss = pd.DataFrame({'loss': history.history['loss'],\n                          'val_loss': history.history['val_loss']})\n\nplot_acc.plot(ax=ax[0])\nplot_loss.plot(ax=ax[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(model, sub_df):\n    pred = np.empty((sub_df.shape[0],))\n    \n    for n in tqdm(range(sub_df.shape[0])):\n        image = np.array(Image.open(test_dir + sub_df.id[n]))\n        pred[n] = model.predict(image.reshape((1, 32, 32, 3))/255.0)[0]\n    \n    sub_df['has_cactus'] = pred\n    return sub_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = predict(model, sub_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# If you ignore this process, you can't submit file.\n# Maybe it's because there's more than just a file to submit in the working directory.\n# Please let me know if you have any other solution to this.\n\n!rm -r *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions.to_csv('submission.csv', header=True, index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}