{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":24,"outputs":[{"output_type":"stream","text":"['test', 'sample_submission.csv', 'train.csv', 'train']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport imageio as im\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n\nimport gc\nimport glob\nimport os\nimport cv2\nimport random\n","execution_count":25,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os,cv2\nfrom IPython.display import Image\nfrom keras.preprocessing import image\nfrom keras import optimizers\nfrom keras import layers,models\nfrom keras.applications.imagenet_utils import preprocess_input\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nprint(os.listdir(\"../input\"))\n\nimport numpy as np","execution_count":26,"outputs":[{"output_type":"stream","text":"['test', 'sample_submission.csv', 'train.csv', 'train']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dir=\"../input/train/train\"\ntest_dir=\"../input/test/test\"\ntrain=pd.read_csv('../input/train.csv')\n\ndf_test=pd.read_csv('../input/sample_submission.csv')","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(5)","execution_count":28,"outputs":[{"output_type":"execute_result","execution_count":28,"data":{"text/plain":"                                     id  has_cactus\n0  0004be2cfeaba1c0361d39e2b000257b.jpg           1\n1  000c8a36845c0208e833c79c1bffedd1.jpg           1\n2  000d1e9a533f62e55c289303b072733d.jpg           1\n3  0011485b40695e9138e92d0b3fb55128.jpg           1\n4  0014d7a11e90b62848904c1418fc8cf2.jpg           1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>has_cactus</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0004be2cfeaba1c0361d39e2b000257b.jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000c8a36845c0208e833c79c1bffedd1.jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000d1e9a533f62e55c289303b072733d.jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0011485b40695e9138e92d0b3fb55128.jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0014d7a11e90b62848904c1418fc8cf2.jpg</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('out dataset has {} rows and {} columns'.format(train.shape[0],train.shape[1]))","execution_count":29,"outputs":[{"output_type":"stream","text":"out dataset has 17500 rows and 2 columns\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['has_cactus'].value_counts(normalize=True)","execution_count":30,"outputs":[{"output_type":"execute_result","execution_count":30,"data":{"text/plain":"1    0.750629\n0    0.249371\nName: has_cactus, dtype: float64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The number of rows in test set is %d\"%(len(os.listdir('../input/test/test'))))","execution_count":31,"outputs":[{"output_type":"stream","text":"The number of rows in test set is 4000\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(os.path.join(\"../input/train/train\",train.iloc[0,0]),width=250,height=250)","execution_count":32,"outputs":[{"output_type":"execute_result","execution_count":32,"data":{"image/jpeg":"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAMCAgMCAgMDAwMEAwMEBQgFBQQEBQoHBwYIDAoMDAsKCwsNDhIQDQ4RDgsLEBYQERMUFRUVDA8XGBYUGBIUFRT/2wBDAQMEBAUEBQkFBQkUDQsNFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBT/wAARCAAgACADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDp59C8G648Fvr3iB9KkSHh44YIgvIA/wBZdHByT82MncOuAa6Kz+Evw6trWKU+Ob9VRQpIuNPXdwuDgsPT1xyeK8YtvEMV9eP5MiTCObyy2yRkUZypykbbTyDjjv0Fa1/8LvGGtys1rqllaOrgwibS9UEWcsCSBanPGCMHBJPLDmlSbXunXGLi7HpV/wCF/Afh2I38GuX0ka5VGUWLYZ3/AOWeJSdxbHTJOO9cxrdxbwXkaRR3EdvLxDPMg8112xZJwSCwZjjkcDoeSbA+GniC1t7zzLI6vsRLiOZIxbxxHeQ/F0I9uCYgM7cliQ3y4PNLZ6hdXc1rqqRW96iISsUqSQDCsCN8TMhzlWJ+8DxjAINuJ1xdkaNrZ6NYT6deWGm6Q8V1D5wlhSRWmbAd2GyQDcyssufvFZBJ8y5NGpa09i1s2meFLTTmkdwXt4L6FGyzFuEnQtyHOQMMDkNxT9P0GSXxVq95BeTQ295PHItvN5aQQTi63wrG7FiXVGktmTZypIyAimutls2SWfa8N9cQQxW+0HKp8/mSksfmdmcvjqFDepwJd1qOUdTA0TwLfapcyahrw3XE2yWK3vZ5wI2RGVvknlIUF3lCnJJCK5fCqBqP8PLh5oXi1DSZHjQxKr6naxAAnPDeYVyP89TSx2ItzvXdaBCSyMuFkOWz0OedxORgnNRtqV5YW0hMjCGBwSGOAGJGWAydvPvnpSg3MSg2f//Z\n","text/plain":"<IPython.core.display.Image object>"},"metadata":{"image/jpeg":{"width":250,"height":250}}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_data(data,m,direc):\n    print('preparing data')\n    X_train=np.zeros((m,32,32,3))\n    count=0\n    for fig in data['id']:\n        img=image.load_img(os.path.join(direc,fig),target_size=(32,32,3))\n        x=image.img_to_array(img)\n        x=preprocess_input(x)\n        X_train[count]=x/255\n        count+=1\n        \n        \n    print(\"Done\")\n    return X_train","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=prepare_data(train,train.shape[0],train_dir)\ntest=prepare_data(df_test,len(df_test),test_dir)","execution_count":34,"outputs":[{"output_type":"stream","text":"preparing data\nDone\npreparing data\nDone\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x=data[:15001]\ntrain_y=train['has_cactus'][:15001]\ntest_x=data[15001:]\ntest_y=train['has_cactus'][15001:]","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head(1)","execution_count":40,"outputs":[{"output_type":"execute_result","execution_count":40,"data":{"text/plain":"                                     id  has_cactus\n0  000940378805c44108d287872b2f04ce.jpg         0.5","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>has_cactus</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000940378805c44108d287872b2f04ce.jpg</td>\n      <td>0.5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Input\nheight = 32\nwidth = 32\nchannels = 3\nn_inputs = height * width * channels\n\n# Parameters for TWO convolutional layers: \n\nconv3_fmaps = 32\nconv3_ksize = 3\nconv3_stride = 1\nconv3_pad = \"SAME\"\n\nconv1_fmaps = 128\nconv1_ksize = 3\nconv1_stride = 1\nconv1_pad = \"SAME\"\n\nconv2_fmaps = 64\nconv2_ksize = 3\nconv2_stride = 1\nconv2_pad = \"SAME\"\n\n# Define a pooling layer\npool3_dropout_rate = 0.25\npool3_fmaps = conv2_fmaps\n\n# Define a fully connected layer \nn_fc1 = 128\nfc1_dropout_rate = 0.5\n\n# Output\nn_outputs = 2\n\n\ntf.reset_default_graph() \n\n# Step 2: Set up placeholders for input data\nwith tf.name_scope(\"inputs\"):\n    X = tf.placeholder(tf.float32, shape=[None, 32,32,3], name=\"X\")\n    y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n    y_true=tf.argmax(y, dimension=1)\n    training = tf.placeholder_with_default(False, shape=[], name='training')\n\n    \n# Step 3: Set up the two convolutional layers using tf.layers.conv2d\nconv3 = tf.layers.conv2d(X, filters=conv3_fmaps, kernel_size=conv3_ksize,\n                         strides=conv3_stride, padding=conv3_pad,\n                         activation=tf.nn.relu, name=\"conv1\")\nconv1 = tf.layers.conv2d(conv3, filters=conv1_fmaps, kernel_size=conv1_ksize,\n                         strides=conv1_stride, padding=conv1_pad,\n                         activation=tf.nn.relu, name=\"conv2\")\nconv2 = tf.layers.conv2d(conv1, filters=conv2_fmaps, kernel_size=conv2_ksize,\n                         strides=conv2_stride, padding=conv2_pad,\n                         activation=tf.nn.relu, name=\"conv3\")\n\n# Step 4: Set up the pooling layer with dropout using tf.nn.max_pool \nwith tf.name_scope(\"pool3\"):\n    pool3 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n    pool3_flat = tf.reshape(pool3, shape=[-1, pool3_fmaps * 16 * 16])\n    pool3_flat_drop = tf.layers.dropout(pool3_flat, pool3_dropout_rate, training=training)\n\n# Step 5: Set up the fully connected layer using tf.layers.dense\nwith tf.name_scope(\"fc1\"):\n    fc1 = tf.layers.dense(pool3_flat_drop, n_fc1, activation=tf.nn.relu, name=\"fc1\")\n    fc1_drop = tf.layers.dropout(fc1, fc1_dropout_rate, training=training)\n\n# Step 6: Calculate final output from the output of the fully connected layer\nwith tf.name_scope(\"output\"):\n    logits = tf.layers.dense(fc1, n_outputs, name=\"output\")\n    Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n    y_pred=tf.argmax(Y_proba, dimension=1)\n\n# Step 5: Define the optimizer; taking as input (learning_rate) and (loss)\nwith tf.name_scope(\"train\"):\n    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n    loss = tf.reduce_mean(xentropy)\n    optimizer = tf.train.AdamOptimizer()\n    training_op = optimizer.minimize(loss)\n\n# Step 6: Define the evaluation metric\nwith tf.name_scope(\"eval\"):\n    correct = tf.nn.in_top_k(logits, y, 1)\n    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n\n# Step 7: Initiate    \nwith tf.name_scope(\"init_and_save\"):\n    init = tf.global_variables_initializer()\n    saver = tf.train.Saver()\n    \n# Step 8: Read in data\n\n\n# Step 9: Define some necessary functions\ndef get_model_params():\n    gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n    return {gvar.op.name: value for gvar, value in zip(gvars, tf.get_default_session().run(gvars))}\n\ndef restore_model_params(model_params):\n    gvar_names = list(model_params.keys())\n    assign_ops = {gvar_name: tf.get_default_graph().get_operation_by_name(gvar_name + \"/Assign\")\n                  for gvar_name in gvar_names}\n    init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n    feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n    tf.get_default_session().run(assign_ops, feed_dict=feed_dict)\n\ndef shuffle_batch(X, y, batch_size):\n    rnd_idx = np.random.permutation(len(X))\n    n_batches = len(X) // batch_size\n    for batch_idx in np.array_split(rnd_idx, n_batches):\n        X_batch, y_batch = X[batch_idx], y[batch_idx]\n        yield X_batch, y_batch\n\n# Step 10: Define training and evaluation parameters\nn_epochs = 20\nbatch_size = 100\niteration = 0\n\nbest_loss_val = np.infty\ncheck_interval = 500\nchecks_since_last_progress = 0\nmax_checks_without_progress = 20\nbest_model_params = None \n\n# Step 11: Train and evaluate CNN with Early Stopping procedure defined at the very top\nwith tf.Session() as sess:\n    init.run()\n    for epoch in range(n_epochs):\n        for X_batch, y_batch in shuffle_batch(train_x, train_y, batch_size):\n            iteration += 1\n            sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})\n            if iteration % check_interval == 0:\n                loss_val = loss.eval(feed_dict={X: test_x, y: test_y})\n                if loss_val < best_loss_val:\n                    best_loss_val = loss_val\n                    checks_since_last_progress = 0\n                    best_model_params = get_model_params()\n                else:\n                    checks_since_last_progress += 1\n        acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n        acc_val = accuracy.eval(feed_dict={X: test_x, y: test_y})\n        print(\"Epoch {}, last batch accuracy: {:.4f}%, valid. accuracy: {:.4f}%, valid. best loss: {:.6f}\".format(\n                  epoch, acc_batch * 100, acc_val * 100, best_loss_val))\n        if checks_since_last_progress > max_checks_without_progress:\n            print(\"Early stopping!\")\n            break\n    if best_model_params:\n        restore_model_params(best_model_params)\n    prediction= y_pred.eval(feed_dict={X: test})\n    save_path = saver.save(sess, \"./my_mnist_model\")\n\n\n    \n","execution_count":41,"outputs":[{"output_type":"stream","text":"Epoch 0, last batch accuracy: 100.0000%, valid. accuracy: 98.1192%, valid. best loss: inf\nEpoch 1, last batch accuracy: 99.0000%, valid. accuracy: 99.1597%, valid. best loss: inf\nEpoch 2, last batch accuracy: 99.0000%, valid. accuracy: 99.1997%, valid. best loss: inf\nEpoch 3, last batch accuracy: 100.0000%, valid. accuracy: 97.3589%, valid. best loss: 0.021854\nEpoch 4, last batch accuracy: 100.0000%, valid. accuracy: 99.5198%, valid. best loss: 0.021854\nEpoch 5, last batch accuracy: 100.0000%, valid. accuracy: 99.7199%, valid. best loss: 0.021854\nEpoch 6, last batch accuracy: 100.0000%, valid. accuracy: 98.9596%, valid. best loss: 0.020820\nEpoch 7, last batch accuracy: 100.0000%, valid. accuracy: 99.5998%, valid. best loss: 0.020820\nEpoch 8, last batch accuracy: 100.0000%, valid. accuracy: 99.6799%, valid. best loss: 0.020820\nEpoch 9, last batch accuracy: 100.0000%, valid. accuracy: 99.7199%, valid. best loss: 0.020820\nEpoch 10, last batch accuracy: 100.0000%, valid. accuracy: 99.3998%, valid. best loss: 0.020820\nEpoch 11, last batch accuracy: 98.0000%, valid. accuracy: 97.6791%, valid. best loss: 0.020820\nEpoch 12, last batch accuracy: 100.0000%, valid. accuracy: 99.6799%, valid. best loss: 0.020820\nEpoch 13, last batch accuracy: 100.0000%, valid. accuracy: 99.7999%, valid. best loss: 0.016487\nEpoch 14, last batch accuracy: 100.0000%, valid. accuracy: 99.7599%, valid. best loss: 0.016487\nEpoch 15, last batch accuracy: 100.0000%, valid. accuracy: 99.8399%, valid. best loss: 0.016487\nEpoch 16, last batch accuracy: 100.0000%, valid. accuracy: 99.8399%, valid. best loss: 0.016487\nEpoch 17, last batch accuracy: 100.0000%, valid. accuracy: 99.7999%, valid. best loss: 0.016487\nEpoch 18, last batch accuracy: 100.0000%, valid. accuracy: 99.7999%, valid. best loss: 0.016487\nEpoch 19, last batch accuracy: 100.0000%, valid. accuracy: 99.8399%, valid. best loss: 0.016487\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.DataFrame({'id':df_test['id'] })\ndf['has_cactus']=prediction\ndf.to_csv(\"samplesubmission.csv\",index=False)","execution_count":44,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}