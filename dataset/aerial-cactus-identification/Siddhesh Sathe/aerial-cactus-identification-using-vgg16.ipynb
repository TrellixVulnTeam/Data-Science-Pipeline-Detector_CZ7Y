{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input/\"))\n\n# Any results you write to the current directory are saved as output.\n!unzip ../input/test.zip\n!unzip ../input/train.zip","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"## Data preparation"},{"metadata":{},"cell_type":"markdown","source":"To curate the data and make it easier for training, will follow below steps.\n1. Read the `train.csv` file\n2. Read the labels from `train.csv` which are the `positive` (containing cactus) and `negative` (not containing cactus) images\n3. Create two different directories for positive and negative images\n4. Move positive images to positive dir and negative images to negative dir\n5. Train with `ImageDataGenerator`"},{"metadata":{},"cell_type":"markdown","source":"### Making directories\n1. `has_cactus`: Positive samples\n2. `has_no_cactus`: Negative samples"},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir has_cactus has_no_cactus ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading the `train.csv`"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import shutil\nimages_having_cactus = []\nimages_having_no_cactus = []\n\nfor i in df[df['has_cactus'] == 1]['id']:\n    p = os.path.join('./train/', i)\n    images_having_cactus.append(p)\n\nfor i in df[df['has_cactus'] == 0]['id']:\n    p = os.path.join('./train/', i)\n    images_having_no_cactus.append(p)\n\n# Copying the images actually\nfor i in images_having_cactus:\n    shutil.copy(i, './has_cactus/')\nfor i in images_having_no_cactus:\n    shutil.copy(i, './has_no_cactus/')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analysis"},{"metadata":{},"cell_type":"markdown","source":"Now, let's see how many training images are present per class"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Has Cactus: {}'.format(df[df['has_cactus'] == 1]['id'].count()))\nprint('Has No Cactus: {}'.format(df[df['has_cactus'] == 0]['id'].count()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This means, we need `8772` more samples in training for `has_no_cactus` category.\nThe solution is [Data Augumentation](https://towardsdatascience.com/data-augmentation-experimentation-3e274504f04b)"},{"metadata":{},"cell_type":"markdown","source":"## Data Augumentation\n1. We'll be adding the more images to the existing dataset\n2. After iterating over the images under no_cactus, we'll\n* Flip the image horizontally and save\n* Flip the image vertically and save\n3. We won't add it to the data frame since we are going to use ImageDataGenerator from keras"},{"metadata":{"trusted":true},"cell_type":"code","source":"def augument_data(\n    directory,              # Directory where augumentation is needed. Same dir will have sample images\n    number_of_images_to_add # Image count to add \n):\n    print('Images to add: {}'.format(number_of_images_to_add))\n    import cv2\n    from glob import glob\n    l = glob(directory + '/*.jpg')\n    for image in l:\n        if number_of_images_to_add == 0:\n            break\n        img = cv2.imread(image)\n        h_img = cv2.flip(img, 0)\n        v_img = cv2.flip(img, 1)\n        cv2.imwrite(directory + '/h_img_{}.jpg'.format(number_of_images_to_add), h_img)\n        number_of_images_to_add -= 1\n        cv2.imwrite(directory + '/v_img_{}.jpg'.format(number_of_images_to_add), v_img)\n        number_of_images_to_add -= 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"augument_data('./has_no_cactus/', df[df['has_cactus'] == 1]['id'].count() - df[df['has_cactus'] == 0]['id'].count())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Moved the curated data to `curated_data` directory\n* Created `validation_data` and `test_data` under curated data along with `train_data`"},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir -p curated_data/train_data curated_data/validation_data/has_cactus\n!mkdir -p curated_data/validation_data/has_no_cactus\n!mv has_cactus has_no_cactus curated_data/train_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from glob import glob\nimport shutil\nl = glob('curated_data/train_data/has_cactus/*.jpg')\nfor i in range(300):\n    shutil.move(l[i], 'curated_data/validation_data/has_cactus')\n\nl = glob('curated_data/train_data/has_no_cactus/*.jpg')\nfor i in range(300):\n    shutil.move(l[i], 'curated_data/validation_data/has_no_cactus')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, the number of images in both the directories is equal.\n\n\n| Class  | Number of images   |\n|---|---|\n|  `has_cactus` | 13136  |\n|  `has_no_cactus` | 13136  |"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Creating Data Generator"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Input, Flatten, Dense, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.callbacks import EarlyStopping","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datagen = ImageDataGenerator(\n    featurewise_std_normalization=True,\n    samplewise_std_normalization=True,\n    horizontal_flip=True,\n    vertical_flip=True\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = datagen.flow_from_directory(\n    'curated_data/train_data/',\n    class_mode='categorical'\n)\n\nvalidation_data = datagen.flow_from_directory(\n    'curated_data/validation_data/',\n    class_mode='categorical'\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Definition"},{"metadata":{},"cell_type":"markdown","source":"We'll try to use pre-trained model named `VGG16` with `imagenet` weights."},{"metadata":{"trusted":true},"cell_type":"code","source":"vgg16_model = VGG16(\n    include_top=False,\n    weights='imagenet',\n    input_shape=(256, 256, 3)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vgg16_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for layer in vgg16_model.layers[:5]:\n    layer.trainable = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = vgg16_model.output\nx = Flatten()(x)\nx = Dense(1024)(x)\nx = Dropout(0.5)(x)\nx = Dense(1024, activation=\"relu\")(x)\npredictions = Dense(2, activation=\"softmax\")(x)\n\n# creating the final model \nmodel = Model(inputs= vgg16_model.input, outputs= predictions)\n\nearly_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n\n# compile the model \nmodel.compile(loss = \"binary_crossentropy\", optimizer = SGD(lr=0.0001, momentum=0.9), metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit_generator(\n    train_data,\n    epochs=20,\n    validation_data=validation_data,\n    callbacks=[early_stop]\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = pd.DataFrame(model.history.history)\nhist.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2\nfrom glob import glob\ntest_images = glob('../input/test/test/*.jpg')\ndf = pd.DataFrame(columns=['id', 'has_cactus'])\ndf.index.name = 'id'\nfor img in test_images:\n    i = cv2.imread(img)\n    i.resize(256, 256, 3)\n    pred = model.predict(i.reshape(1, 256, 256, 3))\n    tempDf = pd.DataFrame({\n        'id': [img.split('/')[-1]],\n        'has_cactus': [pred[0][0]]\n    })\n    df = df.append(tempDf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Removing the new directories created locally\n!rm -rf curated_data/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.set_index('id')\ndf.to_csv('submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Download `submission.csv` from here\n<a href='submission.csv'>Download Submission</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}