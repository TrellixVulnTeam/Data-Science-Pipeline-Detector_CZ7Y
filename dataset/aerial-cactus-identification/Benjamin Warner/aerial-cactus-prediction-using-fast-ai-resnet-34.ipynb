{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Aerial Cactus Prediction using fast.ai & ResNet-34\n# Introduction\n> To assess the impact of climate change on Earth's flora and fauna, it is vital to quantify how human activities such as logging, mining, and agriculture are impacting our protected natural areas. Researchers in Mexico have created the [VIGIA project](https://jivg.org/research-projects/vigia/), which aims to build a system for autonomous surveillance of protected areas. A first step in such an effort is the ability to recognize the vegetation inside the protected areas. In this competition, you are tasked with creation of an algorithm that can identify a specific type of cactus in aerial imagery.\n>\n> The original version of this data can be found [here](https://www.kaggle.com/irvingvasquez/cactus-aerial-photos), with full details in López-Jiménez's master thesis:\n>\n>Efren López-Jiménez, Sistema embebido para la supervisión inteligente de terrenos con vehı́culos aéreos no tripulados. Master Thesis, Instituto Politécnico Nacional, 2018. DOI: 10.13140/RG.2.2.19455.46246.\n\nIn this kernel I will attempt to identify a specific type of cactus in aerial imagery using [fast.ai](https://www.fast.ai/) and a ImageNet pre-trained ResNet-34. I will explain my process throughout the kernel, which should allow anyone familiar with the fast.ai library to follow along.\n\nWhy train a smaller model and not a larger model such as DenseNet-161? While I could train a DenseNet-161 model using the fast.ai defaults and match the top results in this competition, a smaller model that can match the accuracy of a larger model is more flexible. It could more easily be deployed as part of a smartphone app, for example. Plus training a smaller model requires different approaches and hyperparameter tuning."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from fastai.utils import *\nfrom fastai.vision import *\nfrom fastai.callbacks import *\nfrom pathlib import Path\nimport os\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport PIL\nfrom torch.utils import model_zoo\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# from https://www.kaggle.com/bminixhofer/deterministic-neural-networks-using-pytorch#437938\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Load the pretrained weights work without needing to find the expected filename\nPath('models').mkdir(exist_ok=True)\n!cp '../input/resnet34/resnet34.pth' 'models/'\ndef load_url(*args, **kwargs):\n    model_dir = Path('models')\n    filename  = 'resnet34.pth'\n    if not (model_dir/filename).is_file(): raise FileNotFoundError\n    return torch.load(model_dir/filename)\nmodel_zoo.load_url = load_url","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After loading all the libraries, setting seeds for better reproducibility when the kernel is submitted, and pointing the fast.ai learner to the pretrained ResNet-34 weights, the data can be loaded."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_path = Path(\"../input/aerial-cactus-identification\")\ntrain_df = pd.read_csv(data_path/'train.csv')\ntest_df = pd.read_csv(data_path/'sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Exploration\nFirst, I want to check balance of the dataset between cacti and non-cacti. The plot below shows a disparity between the two classes, however after some quick experimentation it was apparent that this did not cause an issue with training or predictions. This Kernel had a [area under the ROC curve](http://en.wikipedia.org/wiki/Receiver_operating_characteristic) (AUROC) of 0.9999 using ResNet-50 without accounting for the class imbalance. But to score higher with a ResNet-34 model, it will probably need to be addressed."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sns.countplot('has_cactus', data=train_df)\nplt.title('Classes', fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As seen below, the cactus images are small, only 32x32 pixels. Because there is not a lot of information to work with, I don't anticipate that the fast.ai warping and rotational transformations will be that useful on the original images."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(8,6))\n\ni = 0\nsample = train_df.sample(12)\nfor row in sample.iterrows():\n    img_name = row[1][0]\n    img = PIL.Image.open(data_path/'train'/'train'/img_name)\n    i += 1\n    plt.subplot(3,4,i)\n    title = 'Not Cactus (0)'\n    if row[1][1] == 1:\n        title = 'Cactus (1)'\n    plt.title(title, fontsize=10)\n    plt.imshow(img)\n    plt.axis('off')\n\nplt.subplots_adjust(top=0.90)\nplt.suptitle('Sample of Images', fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Oversampling"},{"metadata":{},"cell_type":"markdown","source":"As I mentioned earlier, this Kernel scored 0.9999 AUROC leaving the class imbalance alone with ResNet-50. To see if ResNet-34 can match that score or hit 1.000 AUROC, I will correct the class imbalance by oversampling the non-cactus images. Assuming the non-cactus images are a good representation of the images seen in the wild, each image appearing three times in the dataset should not cause any issues during training thanks to the powerful transformations built into fast.ai. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = train_df[train_df.has_cactus==0].copy()\ndf2 = df1.copy()\ntrain_df = train_df.append([df1, df2], ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A quick plot verifies that the dataset is now very close to being balanced."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sns.countplot('has_cactus', data=train_df)\nplt.title('Oversampled Classes', fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Augmentation\nFast.ai has a powerful set of transformations [built into the library](https://docs.fast.ai/vision.transform.html). The has default setting which have experimentation been shown to be a good starting point for regular photos. These default augmentations include flipping on the horizontal axis, rotating, zooming, changing the lighting, and warping and are applied randomly on each photo during a training epoch.\n\nSince this kernel uses aerial images, I enable flipping on the vertical axis by setting `flip_vert=True`. A trick I picked up from looking at some of the other fast.ai kernels (like this one by [Alexander Milekhin](https://www.kaggle.com/kenseitrg/simple-fastai-exercise)) is to upscale the original 32x32 images to 128x128, so the images are large enough for the fast.ai rotation, zooming, and warping transformations to be useful."},{"metadata":{"trusted":true},"cell_type":"code","source":"tfms = get_transforms(do_flip=True, flip_vert=True, max_rotate=20, max_lighting=0.3, max_warp=0.2, max_zoom=1.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I then use the [data block api](https://docs.fast.ai/data_block.html) to finish constructing the dataset, holding back 20 percent of the training images as a validation set, labeling the images from the `train_df` dataframe, and adding the test images for ease of use later."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_images = ImageList.from_df(test_df, path=data_path/'test', folder='test')\n\nsrc = (ImageList.from_df(train_df, path=data_path/'train', folder='train')\n       .split_by_rand_pct(0.2)\n       .label_from_df()\n       .add_test(test_images))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When performing 1cycle learning, its recommended to use a large batch size. However, for this dataset using large batch sizes of 1024 or 2048 performed worse than smaller batch sizes. After some experimentation, I concluded that a batch size of 256 works well for training the model. \n\nWhen applying transformations that changes the image size, such as perspective warping, the best performing method of dealing with squaring the now warped image is to reflect the image data to fill the edges. There is a bug in the underlying pytorch reflection method which prevents it from being used on all datasets but given the regularity of these images reflection works fine.\n\nSince I am using transfer learning via a ResNet-34 trained on ImageNet, I need to normalize the Aerial Cactus images with `imagenet_stats` to match the pre-trained model."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = (src.transform(tfms, \n                     size=128,\n                     resize_method=ResizeMethod.PAD, \n                     padding_mode='reflection')\n        .databunch(bs=256)\n        .normalize(imagenet_stats))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A quick sanity check shows that both classes have loaded correctly."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.classes, data.c","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting `show_batch` gives a visual example of the images with the data augmentation from the fast.ai transformer applied."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.show_batch(rows=3, figsize=(9,9))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training the Model\nBehind the scenes, the fast.ai `cnn_learner` has stripped out the last few layers of the ResNet-34 and replaced them with a few untrained layers which ends in a linear layer to predict the two classes. The pretrained ResNet-34 model is frozen and is not allowed to change while the newly created layers will be trained to predict cactus or not cactus. After training the new layers, the pretrained ResNet-34 layers can be unfrozen and the whole model trained on the dataset.\n\nThe `cnn_learner` has sensible defaults for hyperparameters such as dropout, weight decay, and momentum. After some experimentation I concluded that the fast.ai defaults perform pretty well on this dataset and have left them be.\n\nSince this kernel is being evaluated on the [area under the ROC curve](http://en.wikipedia.org/wiki/Receiver_operating_characteristic) (AUROC) between the predicted probabilities and the observed targets, I will include `AUROC` as a metric which the learner will output while training."},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = cnn_learner(data,\n                    models.resnet34, \n                    metrics=[accuracy, AUROC()], \n                    path = '.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The learning rate hyperparameter is one of the most important hyperparameters to set, and the `lr_find` method provides a graphical way to set a good learning rate. After plotting the losses, one looks for the steepest slope where the loss is decreasing the fastest. There are multiple selections that could be made, and I have chosen the steepest looking slope right before the incline decreases at 1e-3."},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will train the frozen layers of the model using `fit_one_cycle` for five epochs."},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = 1e-3\nlearn.fit_one_cycle(5, lr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The learner has a built in method for plotting the training and validation loss."},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.recorder.plot_losses()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## A Digression on Fit One Cycle\n`fit_one-cycle` works by taking the learning rate, dividing it by ten, and then gradually increasing and then decreasing the learning rate as training progresses. A simplified version of the idea behind 1cycle learning is increasing the learning rate allows the model to escape any suboptimal local minima. Then decreasing the learning rate assists the model in selecting a good minimum. The momentum of the model is adjusted in the opposite direction so the model does not overshoot while at the highest learning rate, but still has enough momentum to find a new minima at the lower learning rates.\n\nThe charts below show the cyclical learning rate and momentum during the training of the frozen ResNet-34 model. \n\nFor more details on 1cycle learning, check out [Sylvain Gugger's post](https://sgugger.github.io/the-1cycle-policy.html) on the subject."},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.recorder.plot_lr(show_moms=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training the Unfrozen Model\nFor many tasks, the output of the ResNet-34 model after five epochs would be satisfactory. But should the initial training not result in such highly accurate model, you can unfreeze the frozen layers using the `unfreeze` method of `learner` and then run `lr_find` to select a discriminative learning rate. The learning rate will be lower for the first layers of the model and then increase for the last layers of the model. A rule of thumb for selecting a good learning rate for the whole model is to pick the higher slice to be ten times before the loss jumps and then set the lower slice to be the original learning rate divided by five or ten.\n\nIn this case, I won't be using the normal procedure and instead will be using `freeze_to(1)`. This keeps the first half of the pre-trained ResNet-34 model frozen while allowing the second half to train on this dataset. The rational is the first layers are already good at picking out lines, shapes, combinations of shapes, and objects of increasing complexity. While the second half is good at identifying the wide variety of objects in ImageNet, so training the second half on this dataset will allow the model to specialize on this competition's desert fauna. "},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save('step-1')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"While the default fast.ai settings worked well for the initial frozen training, I will set the weight decay `wd` to 0.1 and a range of dropout `ps` to be 0.6 and 0.4 for the unfrozen portions of the model. For this training, the model does not steadily progress in accuracy, AUROC, or training and validation loss. I will use the `SaveModelCallback` to keep track of the best performing model as determined by validation loss and use it at the end of training.\n\nI will create a new learner with these settings and then load the weights trained in the previous step."},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = cnn_learner(data,\n                    models.resnet34, \n                    metrics=[accuracy, AUROC()], \n                    callback_fns=[partial(SaveModelCallback)],\n                    wd=0.1,\n                    ps=[0.9, 0.6, 0.4],\n                    path = '.')\nlearn = learn.load('step-1')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next use `lr_find` again to to select a discriminative learning rate."},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.freeze_to(1)\nlearn.lr_find()\nlearn.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this case the optimal learning rates will not differ too much: 4e-4 and 2e-4."},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(20, slice(4e-4, lr/5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And once again plot the losses of the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.recorder.plot_losses()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unfrozen_validation = learn.validate()\nprint(\"Final model validation loss: {0}\".format(unfrozen_validation[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploring the Results\nAfter training the model on the upscaled images, it scores quite well on all metrics. Usually the model's accuracy approaches 0.999 and it's AUROC approaches 1.000 when testing against the validation set. Plotting the Confusion Matrix confirms the model's performance on the validation set. Of the roughly 5,000 images held back for validation the model misidentifies less than a few images."},{"metadata":{"trusted":true},"cell_type":"code","source":"interp = ClassificationInterpretation.from_learner(learn)\n\ninterp.plot_confusion_matrix(figsize=(2,2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting the top losses shows the images which the fully trained ResNet-34 had the most issues with along with its prediction, loss per image, and probability it assigned to each image. It also shows a heatmap of where the model was focusing on when making its prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"interp.plot_top_losses(4, figsize=(6,6))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The heatmap can be turned off for better visiual inspection of the original images."},{"metadata":{"trusted":true},"cell_type":"code","source":"interp.plot_top_losses(4, figsize=(6,6), heatmap=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predictions"},{"metadata":{},"cell_type":"markdown","source":"Now I will use the trained ResNet-34 model make predictions on the test set. The evaluation calls for the probability that each image is a cactus."},{"metadata":{"trusted":true},"cell_type":"code","source":"probability, classification = learn.get_preds(ds_type=DatasetType.Test)\ntest_df.has_cactus = probability.numpy()[:, 0]\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And after verifying that the predictions look right, I will submit the results."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"My previous ResNet-50 submission resulted in an AUROC of 0.9999, which is good but falls short of the top kernels' AUROC of 1.000. Will the ResNet-34 model match or beat that AUROC score? Let's submit and find out."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}