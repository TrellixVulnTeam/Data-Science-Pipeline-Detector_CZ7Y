{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import print_function, division\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport pandas as pd\nfrom skimage import io\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import transforms\n\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Seting up the dataloader"},{"metadata":{},"cell_type":"markdown","source":"Implementing (Inheriting) the dataset class"},{"metadata":{"trusted":true},"cell_type":"code","source":"class CactusImageDataset(Dataset):\n    \"Aerial Cactus Classification Dataset\"\n    \n    def __init__(self, csv_file, root_dir, transform = None):\n        \"\"\"\n        Args:\n            csv_file (string): Path to csv file with annotations\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied on a sample.\n        \"\"\"\n        self.cactus_annotations = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.transform = transform\n    \n    def __len__(self):\n        return self.cactus_annotations.shape[0]\n    \n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n        \n        img_name = os.path.join(self.root_dir,\n                                self.cactus_annotations.iloc[idx, 0])\n        image = io.imread(img_name)\n        has_cactus = self.cactus_annotations.iloc[idx, 1]\n        \n        if self.transform:\n            image = self.transform(image)\n            \n        sample = {\"image\":image, 'label':has_cactus}\n        return sample\n    \n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Setting Up the Transforms, Only using transforms available. No custom transforms. "},{"metadata":{"trusted":true},"cell_type":"code","source":"image_transforms = {\n    'train': transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomVerticalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n    ]),\n    'test':transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n    ])\n}\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Initilaizing the Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = CactusImageDataset('../input/aerial-cactus-identification/train.csv','../input/aerial-cactus-identification/train/train/', image_transforms['train'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here it would be nice to see the distrbution of the input. Let's make a histogram"},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nplt.figure(dpi=128, figsize=(3,3))\nplt.title('Distribution of Cacti Images')\nplt.xticks([0,1])\nplt.xlabel('Has Cactus')\nplt.ylabel('Number of Images')\ndataset.cactus_annotations['has_cactus'].hist(bins=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> **So about 75% of the images contains cacti. A very unbalanced set. But we'll split the dataset into training and dev set which will help us generalize the model somewhat better.**"},{"metadata":{},"cell_type":"markdown","source":"#### Splitting into train (80%) and dev set (20%)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_size = int((0.80 * dataset.__len__()))\ndev_size = dataset.__len__() - train_size\ntrain_set, dev_set = random_split(dataset, [train_size, dev_size])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 32\ntrain_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\ndev_loader = DataLoader(dev_set, batch_size=BATCH_SIZE, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Defining the Model"},{"metadata":{},"cell_type":"markdown","source":"> **Here, let me just show how do the dimensions work in the model. This is something that I used to get confused about a lot.** (Still do sometimes)"},{"metadata":{},"cell_type":"markdown","source":"* **Input** Dimension: **32x32x3** Here **3** is the number if channels for our image (RGB). Here the number of output channels refers to the number of **filters** which we define to be **3x3**\n\n* The first layer will make an output of dim $$\\lfloor \\frac{INPUTDIM + 2*PAD - (KERNEL(FILTER) - 1) - 1}{STRIDE}  + 1 \\rfloor$$ The complete one you can see on PyTorch docs. Which in my model gives us : $$\\lfloor \\frac{32 + 2*1 - (3 - 1) - 1}{1} +1 \\rfloor = 32$$  \nAnd then we pass this to our max pooling layer of kernel size 2 and stride 2 as well which gives us: $$\\lfloor \\frac{32 + 2*0 - (2 - 1) - 1}{2} +1 \\rfloor = 16$$ \n\n\n* The second layer similarly gives us $$\\lfloor \\frac{16 + 2*1 - (3 - 1) - 1}{1} + 1\\rfloor = 16$$ and then the max pooling layer with kernel size 2 and stride 2 gives us: $$\\lfloor \\frac{16 + 2*0 - (2 - 1) - 1}{2} +1 \\rfloor = 8$$\n\n* The third layer $$\\lfloor \\frac{8 + 2*1 - (3 - 1) - 1}{1} + 1 \\rfloor = 8$$ and then the max pooling layer with kernel size 2 and stride 2 gives us: $$\\lfloor \\frac{8 + 2*0 - (2 - 1) - 1}{2} +1 \\rfloor = 4$$\n\n* The fourth layer $$\\lfloor \\frac{4 + 2*1 - (3 - 1) - 1}{1}  + 1\\rfloor = 4$$ and then the max pooling layer with kernel size 2 and stride 2 gives us: $$\\lfloor \\frac{4 + 2*0 - (2 - 1) - 1}{2} +1 \\rfloor = 2$$\n\nAt this point, I stopped adding any convolution layers, but we can add if we want to. Now we take the out channels and line them them up as a linear vector of size $2*2*128$ because $2*2$ in the size of image at this point and there are 128 filters as we defined.  After which the linear layer follows. \n\nI hope this helps anyone who reads it. Cheers\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"class CactusIdentifier(nn.Module):\n    \"\"\"\n        Aerial Cactus Identifier Model\n    \"\"\"\n    def __init__(self):\n        super(CactusIdentifier, self).__init__()\n        self.conv1 = nn.Conv2d(3,16,3, padding = 1)\n        self.conv2 = nn.Conv2d(16,32,3, padding = 1)\n        self.conv3 = nn.Conv2d(32,64,3, padding = 1)\n        self.conv4 = nn.Conv2d(64,128,3, padding = 1)\n        self.pool = nn.MaxPool2d(2,2)\n        \n        self.fc1 = nn.Linear(128*2*2, 256)\n        self.fc2 = nn.Linear(256, 64)\n        self.fc3 = nn.Linear(64,2)\n        self.act = nn.ReLU()\n        self.drop = nn.Dropout()\n    \n    def forward(self, inp_image):\n        out = self.pool(self.act(self.conv1(inp_image)))\n        out = self.pool(self.act(self.conv2(out)))\n        out = self.pool(self.act(self.conv3(out)))\n        out = self.pool(self.act(self.conv4(out)))\n        \n        out = out.view(-1, 128*2*2)\n        out = self.drop(out)\n        out = self.act(self.fc1(out))\n        out = self.drop(out)\n        out = self.act(self.fc2(out))\n        out = self.drop(out)\n        out = self.fc3(out)\n        \n        return out\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training Module"},{"metadata":{},"cell_type":"markdown","source":"In the training module we implement early stopping in the function train_model to reduce overfitting. We after each epoch check the loss on the dev set, if the loss reduces on an epoch we save the model as a dict. and keep on going for the number of epocs in the same manner.\nOnce our training finishes we load the dict and use that model for the results"},{"metadata":{"trusted":true},"cell_type":"code","source":"class TrainingModule():\n    \"\"\"\n    Training Module to train the model\n    \"\"\"\n    \n    def __init__(self, model):\n        self.model = model\n        self.loss_fn = nn.CrossEntropyLoss()\n        self.optimizer = optim.Adam(self.model.parameters())\n        self.is_cuda = False\n        if torch.cuda.is_available():\n            self.is_cuda = True\n            self.model = model.cuda()\n    \n    def train_epoch(self, epoch, train_iterator):\n        total_loss = 0\n        stats_string = \"Epoch : {:3d} | Iteration : {:4d} | Loss : {:4.4f}\"\n        for i, data in enumerate(train_iterator):\n            inputs = data['image']\n            labels = data['label'].long()\n            \n            if self.is_cuda:\n                inputs = inputs.cuda()\n                labels = labels.cuda()\n                    \n            self.optimizer.zero_grad()\n        \n            outputs = self.model(inputs)\n            loss = self.loss_fn(outputs, labels)\n            loss.backward()\n            self.optimizer.step()\n            \n            total_loss += loss.item()\n            \n            if i % 100 == 0:\n                print(stats_string.format(epoch+1, i, total_loss/(i+1)))\n                total_loss = 0\n    \n    def train_model(self, train_iterator, dev_iterator, num_epocs = 30):\n        self.model.train()\n        min_loss = 1000000\n        stats_string = \"Epoch : {:2d} | Train Loss : {:4.4f} | Dev Loss : {:4.4f}\"\n        for i in range(num_epocs):\n            self.train_epoch(i, train_iterator)\n            dev_loss = self.evaluate(dev_iterator)\n            train_loss = self.evaluate(train_iterator)\n            print(stats_string.format(i+1, train_loss, dev_loss))\n            if dev_loss <= min_loss:\n                torch.save(self.model.state_dict(), 'best_model')\n                min_loss = dev_loss\n    \n        print(\"Finished Training\")\n        \n    def evaluate(self, iterator):\n        self.model.eval()\n        loss_total = 0\n        \n        for i, data in enumerate(iterator):\n            inputs = data['image']\n            labels = data['label'].long()\n            \n            if self.is_cuda:\n                inputs = inputs.cuda()\n                labels = labels.cuda()\n            \n            preds = self.model(inputs)\n            loss = self.loss_fn(preds, labels)\n            loss_total += loss.item()\n        \n        return loss_total\n            ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Initializing the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = CactusIdentifier()\nprint(model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training the Model"},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"trainer = TrainingModule(model)\ntrainer.train_model(train_loader, dev_loader, num_epocs = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Setting Up the Test Set"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set = CactusImageDataset(\"../input/aerial-cactus-identification/sample_submission.csv\", \"../input/aerial-cactus-identification/test/test\", image_transforms['test'])\ntest_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Generating Predictions using the best model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_state_dict(torch.load('best_model'))\nfinal_preds = []\n\nfor i, data in enumerate(test_loader):\n    inputs = data['image']\n    labels = data['label'].long()\n\n    if torch.cuda.is_available():\n        inputs = inputs.cuda()\n        labels = labels.cuda()\n\n    preds = model(inputs)\n    final_preds += preds[:,1].tolist()\n\ntest_set.cactus_annotations['has_cactus'] = final_preds\ntest_set.cactus_annotations.to_csv(\"samplesubmission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set.cactus_annotations.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}