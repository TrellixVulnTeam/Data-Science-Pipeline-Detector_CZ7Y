{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom keras import backend as K\nfrom keras.utils import Sequence, to_categorical\nfrom keras.applications.densenet import DenseNet121\nfrom keras.callbacks import TensorBoard, EarlyStopping\nfrom keras.optimizers import Adam\nfrom keras import layers, Model, metrics\nfrom skimage.io import imread, imshow\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport types\nimport os","execution_count":1,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Sequence generator, based on https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\nclass CactusSequence(Sequence):\n    def __init__(self, list_IDs, labels=None, batch_size=128, dim=(32,32,3),\n                 n_classes=2, shuffle=True, is_test=False, \n                 data_dir=\"data/train\"):\n        \"\"\"\n        Initialize sequence generator\n        @param list_IDs: array of filenames to load\n        @param labels: array or dict of labels corresponding to items in list_IDs. \n                       In case of list we suppose that labels are aligned with IDs.\n                       Ignored if is_test=True.\n        @param batch_size: size of the batch. Last batch can be smaller that this value.\n        @param dim: dimensions of the single sample (i.e. 32x32 RGB image should have dimensions (32,32,3)\n        @param n_classes: number of classes for classification\n                          Ignored if is_test=True.\n        @param shuffle: should we shuffle our data before producing the next batch\n        @param is_test: should we yield label for each sample (i.e. test sequence doesn't have label)\n        \"\"\"\n        self.dim = dim\n        self.batch_size = batch_size\n        self.list_IDs = list_IDs\n        self.indexes = np.arange(len(self.list_IDs))\n        self.is_test = is_test\n        self.shuffle = shuffle\n        self.data_dir = data_dir\n        \n        if not is_test:    \n            self.n_classes = n_classes    \n            self.labels = labels\n            if not isinstance(labels, dict):\n                self.labels = dict(zip(list_IDs, labels))\n        \n            self.on_epoch_end()        \n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        l = int(np.floor(len(self.list_IDs) / self.batch_size))+1\n        return l\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        # Find list of IDs\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n\n        # Generate data\n        # Note that we don't include label for test sequence\n        if not self.is_test:\n            X, y = self.__data_generation_train(list_IDs_temp)\n            return X, y\n        else:\n            X = self.__data_generation_test(list_IDs_temp)\n            return X\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        if self.shuffle == True and not self.is_test:\n            np.random.shuffle(self.indexes)\n\n    def __data_generation_train(self, list_IDs_temp):\n        'Generates data containing not more than batch_size samples with labels' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        X = np.empty((len(list_IDs_temp), *self.dim))\n        y = np.empty((len(list_IDs_temp)), dtype=np.int32)\n\n        # Generate data\n        for i, ID in enumerate(list_IDs_temp):\n            # Store sample\n            X[i,] = imread(self.data_dir+\"/\"+ID) / 255\n\n            # Store class\n            y[i] = self.labels[ID]\n            \n        return X, to_categorical(y, num_classes=self.n_classes)\n    \n    def __data_generation_test(self, list_IDs_temp):\n        'Generates data containing not more than batch_size samples without labels' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        X = np.empty((len(list_IDs_temp), *self.dim))\n\n        # Generate data\n        for i, ID in enumerate(list_IDs_temp):\n            # Store sample\n            X[i,] = imread(self.data_dir+\"/\"+ID) / 255\n            \n        return X","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#AUC-score function\ndef auc(y_true, y_pred):\n    auc = tf.py_func(lambda y_true, y_pred : roc_auc_score( y_true, y_pred, average='macro', sample_weight=None).astype('float32'),\n                     [y_true, y_pred],\n                     'float32',\n                     stateful=False,\n                     name='sklearnAUC')\n    return auc","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"../input/train.csv\", dtype={\"id\": str, \"has_cactus\": np.int32})\ntrain_data = train_data.sample(frac=1).reset_index(drop=True)\nprint(\"Total samples in train set:\", len(train_data))","execution_count":4,"outputs":[{"output_type":"stream","text":"Total samples in train set: 17500\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#note that our classes are disbalanced\ntrain_data.describe()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"         has_cactus\ncount  17500.000000\nmean       0.750629\nstd        0.432662\nmin        0.000000\n25%        1.000000\n50%        1.000000\n75%        1.000000\nmax        1.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>has_cactus</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>17500.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.750629</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.432662</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = os.listdir(\"../input/test/test\")\nprint(\"Total samples in test set:\", len(test_data))","execution_count":6,"outputs":[{"output_type":"stream","text":"Total samples in test set: 4000\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we'll divide train set into train and validation parts \ntrain_data, val_data = train_data[:12000], train_data[12000:]","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Instantiating our generators\ntrain_generator = CactusSequence(train_data.id.values, train_data.has_cactus.values, data_dir=\"../input/train/train\")\nval_generator = CactusSequence(val_data.id.values, val_data.has_cactus.values, data_dir=\"../input/train/train\", shuffle=False)\ntest_generator = CactusSequence(test_data, is_test=True, data_dir=\"../input/test/test\")","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We'll use Densenet architecture and train it from scratch. We exclude last year so we use our own output shape.\ndensenet_model = DenseNet121(include_top=False, input_shape=(32,32,3), weights=None)","execution_count":10,"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We flatten the last layer of cutted densenet model and use sigmoid activation to produce probabilities for 2 classes.\nx = layers.Flatten()(densenet_model.output)\npredictions = layers.Dense(2, activation=\"sigmoid\")(x)","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#final model\nmodel = Model(inputs=densenet_model.input, outputs=predictions)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#using binary_crossentropy as a loss function and binary_accuracy as a metric (along with ROC AUC on validation)\nmodel.compile(optimizer=Adam(lr=0.000005), loss='binary_crossentropy', metrics=[metrics.binary_accuracy, auc])","execution_count":13,"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From <ipython-input-3-d41c4c61cbad>:7: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\ntf.py_func is deprecated in TF V2. Instead, use\n    tf.py_function, which takes a python function which manipulates tf eager\n    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n    an ndarray (just call tensor.numpy()) but having access to eager tensors\n    means `tf.py_function`s can use accelerators such as GPUs as well as\n    being differentiable using a gradient tape.\n    \n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#adding earlystop callback. Optionaly adding tensorboard callback.\nearlystop_callback = EarlyStopping(monitor='val_loss', min_delta=0.005, patience=10, restore_best_weights=True)","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fitting our model. Note that we set class_weight according to disbalance factor.\nmodel.fit_generator(generator=train_generator, validation_data=val_generator, epochs=50, callbacks=[earlystop_callback], class_weight={0: 3.0, 1:1.0})","execution_count":17,"outputs":[{"output_type":"stream","text":"Epoch 1/1\n94/94 [==============================] - 24s 252ms/step - loss: 0.6025 - binary_accuracy: 0.7942 - auc: 0.9215 - val_loss: 0.4784 - val_binary_accuracy: 0.7757 - val_auc: 0.9149\n","name":"stdout"},{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"<keras.callbacks.History at 0x7f7ee7a5a710>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict_generator(test_generator)\nresult = pd.DataFrame(data={\"id\":test_data, \"has_cactus\":y_pred[:,1]})\nresult.to_csv(\"densenet_30epochs.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":1}