{"cells":[{"metadata":{"_uuid":"a4e95c0ce8db5153f34a62a719411b00eeef33db"},"cell_type":"markdown","source":"This competition focuses on determining whether a 32x32px image contains a cactus or not. The dataset comes from Efren López-Jiménez master's thesis titled _Sistema embebido para la supervisión inteligente de terrenos con vehı́culos aéreos no tripulados_ which translates to _Embedded system for intelligent monitoring of land with unmanned aerial vehicles_.\n\nThe abstract of the paper reads as follows:\n\n> The terrain survillance by an aerial vehicle provides the view of an area of interest represented by a polygon through an on-board sensor. In the present work we propose an integral system, it has three mainly approaches: coverage path planning, object recognize and its processing on an embedded system. For solve the coverage path planning problem, we used the rotating caliper algorithm proposed by Vasquez. For the classification and recognition object we used Lenet-5 convolutional neural network proposed by Lecun. The dataset was obtained from the cactus plants images, captured by an unmanned aerial vehicle (UAV) at San Antonio Nanahuatipan, Oax, biosphere reserve. Finally we performed the couple between the embedded system and the UAV. \n\n[Source](https://www.researchgate.net/publication/329453166_Sistema_embebido_para_la_supervision_inteligente_de_terrenos_con_vehiculos_aereos_no_tripulados)\n\nThis is an interesting application and integration of machine learning where it is used as a step in the process of mapping an area through a UAV. The difficulty of the challenge is not in predicting the class of the image - that is trivial with today's state of the art techniques. However, the challenge seems to be able to integrate this into the UAV's embedded processing system. Thus, size and processing power for the network are highly constrained."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from fastai import *\nfrom fastai.vision import *\nimport fastai\nfastai.__version__","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"path = Path('../input')\npath.ls()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2535226429b3940202fbcd1112e090345813dc35"},"cell_type":"code","source":"labs = pd.read_csv(path/'train.csv')\nlabs.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ff20e21489f727f4e2c8734fc008c684b7d65f03"},"cell_type":"markdown","source":"## Let's Look at Some of the Data"},{"metadata":{"_uuid":"896f3472a8766dd315ea1764954e6a609753e68b"},"cell_type":"markdown","source":"There are 17500 training images."},{"metadata":{"trusted":true,"_uuid":"2fbcb3b7eaa29dc0955c254c94b2cdb6f8f66980"},"cell_type":"code","source":"labs.has_cactus.count()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1d4c084e6194579028e59f504820d34af4abc1c8"},"cell_type":"markdown","source":"With about 13000 containing a cactus and about 5000 without a cactus."},{"metadata":{"trusted":true,"_uuid":"c0ee4a78345ad4467e47cfa40f33fd13185888f1"},"cell_type":"code","source":"labs['has_cactus'].value_counts().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f1299730e55d6b733d2d377e4e718ed699a8307"},"cell_type":"markdown","source":"The augmentations we will use for this data are probably limited since changing the few pixel values that we have can drastically change the image. For example, rotations introduce pixelations in the output. However, since our application is on a drone, we probably want to use vertical flips."},{"metadata":{"trusted":true,"_uuid":"f6064140c24ba85a6d43ea332a2ff9a3b5581bb4"},"cell_type":"code","source":"data = (ImageList.from_csv(path, 'train.csv', folder='train/train')\n        .random_split_by_pct()\n        .label_from_df()\n        .add_test_folder('test/test')\n        .transform(get_transforms(max_rotate=0, max_lighting=0.1, max_zoom=1, flip_vert=True), size=32)\n        .databunch(bs=256))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e6c4e8e4ca3a302755ba3120f29afcf088ba82bc"},"cell_type":"code","source":"data.show_batch(5, figsize=(6,6))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c73c77038e846d71a6f3960d87deb4f885ee6e43"},"cell_type":"markdown","source":"## Let's create a Good Model"},{"metadata":{"_uuid":"27301235130f834a00786229cf4a5963fe4ce54d"},"cell_type":"markdown","source":"Define the AUC metric that Kaggle uses to score the competition."},{"metadata":{"trusted":true,"_uuid":"d42e127a66d3df6b120862263724b6a6d49ae384"},"cell_type":"code","source":"# https://www.kaggle.com/guntherthepenguin/fastai-v1-densenet169\nfrom sklearn.metrics import roc_auc_score\n\ndef auc_score(y_pred,y_true,tens=True):\n    score=roc_auc_score(y_true,torch.sigmoid(y_pred)[:,1])\n    if tens:\n        score=tensor(score)\n    else:\n        score=score\n    return score","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fdfa2882b60f03969f1575b91960cbcaaeab1a61"},"cell_type":"markdown","source":"Let's use a resnet50 to prove that we can get good results on this task."},{"metadata":{"trusted":true,"_uuid":"005daa661afce317b6f35c6f81bf2c191f8e8fac"},"cell_type":"code","source":"learn = create_cnn(data, models.resnet50, path=\".\", metrics=[accuracy, auc_score])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5cf19070a2acdb599bcfc5514dc15a9aeb88184a"},"cell_type":"code","source":"learn.lr_find()\nlearn.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f4bc568931dba025df656dd1148da36e09cb9b2"},"cell_type":"code","source":"learn.fit_one_cycle(5, 1e-2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"02b17b5d25c06936b3a696963086c8808c12e63d"},"cell_type":"markdown","source":"After 5 epochs of training we attain a validation accuracy of 99%, showing that this isn't very difficult for powerful models. Let's learn a bit more about the `learn` object. "},{"metadata":{"_uuid":"0cfc870f3dca672dd54b05cf599884c76eacf38a"},"cell_type":"markdown","source":"## Computational Size of Model\n\nWe can use the objgraph package to visualise the connections of the `learn` object."},{"metadata":{"trusted":true,"_uuid":"2903822106ba478a5c8c875733e4ecae1f23fbc8"},"cell_type":"code","source":"! pip install objgraph xdot -q\nimport objgraph\nobjgraph.show_refs([learn])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aee32b70b36df8a24b70478599d4e19851db5b43"},"cell_type":"markdown","source":"Theres clearly a lot going on in the model. Additionally, the model weights are 114MB which is fairly large!"},{"metadata":{"trusted":true,"_uuid":"90471606df3a111ede7bdfe3a102ace1db3c7ca4"},"cell_type":"code","source":"learn.save('model-resnet')\nprint(Path('./models/model-resnet.pth').stat().st_size//(1024*1024), 'MB')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2734f29460e450f6b0bf45fbfe9cb2664640613"},"cell_type":"markdown","source":"Let's see the RAM usage when we're using the model for inference."},{"metadata":{"trusted":true,"_uuid":"38ea4337df0bc38fbf03be39efedfbee6f6da1e8"},"cell_type":"code","source":"# Export the model for inference\nlearn.export()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"449e17270a3a7ef7e41ec18e5400acbb72aa641b"},"cell_type":"markdown","source":"We can use the [IPython Memwatcher](https://github.com/FrancescAlted/ipython_memwatcher) package to monitor RAM usage when predicting."},{"metadata":{"trusted":true,"_uuid":"adbbb0990169f564297cf63f1d89287de5c375af"},"cell_type":"code","source":"!pip install git+https://github.com/FrancescAlted/ipython_memwatcher -q","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11bbe93302f1bb32f07d4cffca035288f9d8f288"},"cell_type":"code","source":"from ipython_memwatcher import MemWatcher\nmw = MemWatcher()\nmw.start_watching_memory()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e3be90f5bf7b258ccf54f2aaa4ba854e5ca3a620"},"cell_type":"code","source":"learn = load_learner(\"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d8791e791e5327987a5fb6e7393f1088c04e0e5"},"cell_type":"code","source":"img = open_image(path/'train/train'/labs.id.iloc[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d8791e791e5327987a5fb6e7393f1088c04e0e5"},"cell_type":"code","source":"learn.predict(img)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0a9a05e1eee40021c2ab8478e6f1c32c690e0f84"},"cell_type":"markdown","source":"Similarly the RAM usage is fairly high for classification of a single image."},{"metadata":{"trusted":true,"_uuid":"02cb7834c8f5268ba7a3664fbe62f7534afbc399"},"cell_type":"code","source":"print(mw.measurements)\nmw.stop_watching_memory()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"358fde6a72659141352393508e9e6ef2b224a427"},"cell_type":"markdown","source":"Finally, let's look at the number of parameters and FLOPs the model uses by using this [simple library](https://github.com/Lyken17/pytorch-OpCounter). The library however had not implemented some of the nn Modules like `nn.AdaptiveAvgPool2d` so I implemented them and have made a pull request to the repo. In the mean time, you can use my branch to install the repo."},{"metadata":{"trusted":true,"_uuid":"9d4b1df1088a471390ad995c2c61a45903ee54c4"},"cell_type":"code","source":"!pip install git+https://github.com/Tom2718/pytorch-OpCounter -q --upgrade","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb5837b8c271eb02b0adc77afc3db01975cfd7f7"},"cell_type":"code","source":"from thop import profile","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fbcada87226802a74563ef6ce391145523d714e7"},"cell_type":"code","source":"model = learn.model\nflops, params = profile(model, input_size=(1, 3, 32,32), \n                        custom_ops={Flatten: None})\n\nprint('FLOPs:', flops//1e6, 'M')\nprint('Params:', params//1e6, 'M')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"74dec03d069916e1a128f21fbbd3f25f0cda0d2e"},"cell_type":"markdown","source":"## Reduce Model Size\n\nNow we will use a different architecture more optimized to have a small footprint: squeezenet."},{"metadata":{"trusted":true,"_uuid":"49ade631fdf1b643fddaf042e75456da6640759b"},"cell_type":"code","source":"learn = create_cnn(data, models.squeezenet1_1, path=\".\", metrics=[accuracy, auc_score])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ee5c3d771f453c3e6c610f8aad66e4e23b97212"},"cell_type":"markdown","source":"With a few more training epochs, we can reach similar accuracy to the Resnet model:"},{"metadata":{"trusted":true,"_uuid":"6db0bb2252e794e8c47833b0c3fd6808ef94c127"},"cell_type":"code","source":"learn.lr_find()\nlearn.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f23c0bb0997c8b251b049a83b0816492e85426a7"},"cell_type":"code","source":"learn.fit_one_cycle(10, 5e-2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"40a1dca6f0ce22502d5799151dff35dbcc4cbdb4"},"cell_type":"code","source":"learn.unfreeze()\nlearn.lr_find()\nlearn.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a7c359d0a49790bb2c944a685c8265dfb3eb2722"},"cell_type":"code","source":"learn.fit_one_cycle(7, slice(5e-4, 5e-3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1604a55378955918dbc44e02b60d6ed192dd290"},"cell_type":"markdown","source":"Repeating the above steps, we can see the computational impact that this model has."},{"metadata":{"trusted":true,"_uuid":"d67cf6d27af3b73c5ce9d94ad831acf4d1258b49"},"cell_type":"code","source":"learn.save('model-squeezenet')\nprint(Path('./models/model-squeezenet.pth').stat().st_size//(1024*1024), 'MB')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed81c8d2f09923d8da948be56eca68515aa267c5"},"cell_type":"code","source":"learn.export()\nmw.start_watching_memory()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2269f90c8dd106950943d09402597d3cfbef92b5"},"cell_type":"code","source":"learn = load_learner(\"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d81e3f6749530b2de280e3a2b36ca1cc4a3a48ac"},"cell_type":"code","source":"img = open_image(path/'train/train'/labs.id.iloc[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a7a5cc0f2506145e659702ccab70667a9c8fe8a"},"cell_type":"code","source":"learn.predict(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"345411eba0a718c8850cc85b4b8e070a94699bc2"},"cell_type":"code","source":"print(mw.measurements)\nmw.stop_watching_memory()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e8136b6ed25c17f2d8759fc001fe091f495c6dd"},"cell_type":"code","source":"model = learn.model\nflops, params = profile(model, input_size=(1, 3, 32,32), custom_ops={Flatten: None})\n\nprint('FLOPs:', flops//1e6, 'M')\nprint('Params:', params//1e6, 'M')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9da6cb6f67b2ea507619d938a6a89ba135e91040"},"cell_type":"markdown","source":"This is clearly less computationally expensive!"},{"metadata":{"trusted":true,"_uuid":"f42aab19823733f4ac2ae9e655dfb3e4609a2e7a"},"cell_type":"markdown","source":"## A Smaller Resnet\n\nLet's wrap this up by comparing these results with a Resnet18."},{"metadata":{"trusted":true,"_uuid":"d8c14b1f4523b7dfe91bbd78f03b1ba3bb206b90"},"cell_type":"code","source":"learn = create_cnn(data, models.resnet18, path=\".\", metrics=[accuracy, auc_score])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"206a518334fd072f034054cf51aa6d656eb0adf5"},"cell_type":"code","source":"learn.fit_one_cycle(10, 5e-2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73bdeed2d1f127ef9465e517115bd3c63f265f21"},"cell_type":"code","source":"learn.save('model-resnet18')\nprint(Path('./models/model-resnet18.pth').stat().st_size//(1024*1024), 'MB')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b11ed54a4fec9b1273db0d57b55e02ae3fb74e4"},"cell_type":"code","source":"learn.export()\nmw.start_watching_memory()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df9fb21a59affe60f1a921a8f53e54a7c0844798"},"cell_type":"code","source":"learn = load_learner(\"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1e49e717a0b317a10f5bc13763f9acda8befaae"},"cell_type":"code","source":"img = open_image(path/'train/train'/labs.id.iloc[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6483994525295e86113e297d46b799aed5d52dd"},"cell_type":"code","source":"learn.predict(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"50b67e3cd258f179833ab6429db2a13ccbeb9e21"},"cell_type":"code","source":"print(mw.measurements)\nmw.stop_watching_memory()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d22f38c400101364a93a653f3e47d9f1f228ff87"},"cell_type":"code","source":"model = learn.model\nflops, params = profile(model, input_size=(1, 3, 32,32), custom_ops={Flatten: None})\n\nprint('FLOPs:', flops//1e6, 'M')\nprint('Params:', params//1e6, 'M')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd74bf89cbe1009da359c1182165467c0e09c1d7"},"cell_type":"markdown","source":"## Final Thoughts"},{"metadata":{"_uuid":"dd1b314e19f59334e87136a14eb67f6fb021da6a"},"cell_type":"markdown","source":"Let's tabulate our results:\n\n|               | Model Size (MB) | Loading Model RAM (MB) | Predict RAM (MB) | FLOPs (M) | Params (M) |\n|---------------|-----------------|------------------------|------------------|-----------|------------|\n|      ResNet50 |       114       |          0.008         |       0.488      |     88    |     25     |\n|      ResNet18 |        48       |          0.254         |       0.566      |     38    |     11     |\n| SqueezeNet1_1 |        14       |          0.004         |       0.191      |     4     |      1     |\n\n\nAs we observed, the smaller our model was, the fewer computational resources it required. These results however, should be taken with a pinch of salt - the computer that they are running on is far more powerful than the embedded computer that might be found on a UAV. They are relevant however when looking at relative differences in compute.\n\nMemWatcher does have the limitation of showing the combined RAM usage so the more variables the notebook has defined above, the more RAM will be used anyway. The most demonstrative runs were done by restarting the kernel and running just that model and these are the results you see in the table. They will certainly be different when the kernel is committed. If I get more time, I will try do multiple runs of the same model so we can get a better result.\n\nThanks for reading!"},{"metadata":{"trusted":true,"_uuid":"22210740b1b9d697767778459f624bc4a9a34d1d"},"cell_type":"code","source":"plt.plot([14,48,114], [4,38,88])\nplt.xlabel('Model Size (MB)')\nplt.ylabel('FLOPs (M)')\nplt.title('Model Size vs FLOPs')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}