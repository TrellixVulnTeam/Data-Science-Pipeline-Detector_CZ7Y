{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Библиотеки"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport torch\nimport torchvision\nimport torch.nn.functional as F\nfrom torch import nn, optim\n# from torch.optim.lr_scheduler import StepLR\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models, utils\nfrom torch.optim.lr_scheduler import MultiplicativeLR\n# from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n# from pytorch_lightning import Trainer\n# from ignite.handlers import EarlyStopping\n# from pytorchtools import EarlyStopping\n\n# import matplotlib.image as Image\nfrom skimage import io\nfrom PIL import Image\nfrom os import listdir\nfrom os.path import isfile, join\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import balanced_accuracy_score, accuracy_score, roc_curve, auc, roc_auc_score, precision_score, recall_score\nfrom sklearn.utils import class_weight\n\nimport matplotlib.pyplot as plt\n\nfrom collections import defaultdict \n\nfrom shutil import copyfile\nimport zipfile\n\nfrom tqdm import tqdm\n# from tqdm.notebook import tqdm\nfrom time import sleep\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GPU"},{"metadata":{"trusted":true},"cell_type":"code","source":"GPU = True\ndevice = \"cuda\" if GPU and torch.cuda.is_available() else \"cpu\"\n\nprint(f'Using device {device}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## DataFrame"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/aerial-cactus-identification/train.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Распределение данных"},{"metadata":{"trusted":true},"cell_type":"code","source":"cmap = plt.get_cmap('Blues')\ncolors = [cmap(i) for i in np.linspace(0, 0.7, df['has_cactus'].unique().shape[0])]\n\nplt.title('Сlass distribution')\ndf['has_cactus'].value_counts().plot(kind='pie', figsize=(6, 6), autopct='%1.2f%%', shadow=True, colors=colors)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Тестовая выборка"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/aerial-cactus-identification/sample_submission.csv')\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Создание тренировочной и валидационных частей"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_link, test_link = '../input/aerial-cactus-identification/train.zip', '../input/aerial-cactus-identification/test.zip'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with zipfile.ZipFile(train_link, \"r\") as z:\n    z.extractall(\".\")\n    \nwith zipfile.ZipFile(test_link, \"r\") as z:\n    z.extractall(\".\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_train, Y_valid = train_test_split(df, stratify=df['has_cactus'], random_state=42, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_valid.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"current_path = './train/'\nneed_path = './valid/'\ntest_path = './test/'\n\ntry:\n    os.makedirs(need_path)\nexcept FileExistsError:\n    pass\n\nfor link in Y_valid['id']:\n    copyfile(current_path + link, need_path + link)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(os.listdir(current_path)), len(os.listdir(need_path)), len(os.listdir(test_path))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataset Class"},{"metadata":{"trusted":true},"cell_type":"code","source":"class CactusDataset(Dataset):\n    # папка, метки и преобразования\n    def __init__(self, folder, labels, transform=None):\n        self.transform = transform\n        self.folder = folder\n        self.labels = labels\n    \n    # размерность датасета\n    def __len__(self):\n#         return len(self.data)\n        return self.labels.shape[0]\n    \n    # получить образец\n    def __getitem__(self, index):\n        image_path = os.path.join(self.folder, self.labels['id'].iloc[index])\n        label =  self.labels['has_cactus'].iloc[index]\n        \n        image = Image.open(image_path).convert('RGB')\n#         image = cv2.imread(image_path)\n#         image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n        \n        if self.transform is not None:\n            image = self.transform(image)\n\n        return image, label\n    \n    # аолучить среднее и отклонение\n    def get_mean_std(self):\n        image_all = np.array([np.array(self[ind][0]) for ind in tqdm(range(self.__len__()))])\n        \n        return image_all.mean(axis=(0, 1, 2)) / 255, image_all.std(axis=(0, 1, 2)) / 255\n    \n    # просмотреть некоторый набор, индексы которого будут переданы                         \n    def view_sample(self, indices, mean, std, count=8):\n        plt.figure(figsize=(count * 3, 3))\n\n        for i, ind in enumerate(indices):\n            image, label = self[ind]\n            \n            if self.transform is not None:\n                image = image.squeeze().permute(1, 2, 0).numpy()\n                image = std * image + mean\n#                 image = np.clip(image, 0, 1)\n            \n            plt.subplot(1, count, i + 1)\n            plt.imshow(image)\n            plt.axis('off')\n            plt.title(f'Cactus: {label == 1}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = CactusDataset(folder=current_path, labels=Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean, std = train_dataset.get_mean_std()\nmean, std","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# indices = np.random.choice(np.arange(self.__len__()), count, replace=False)\nindices = list(range(8))\n\ntrain_dataset.view_sample(indices=indices, mean=mean, std=std)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Преобразования и создание нового набора"},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(),\n#     transforms.Resize(50),\n#     transforms.RandomAffine(5, shear=(2, 2)),\n#     transforms.Resize(30),\n    \n    transforms.RandomRotation(10),\n    transforms.CenterCrop(28),\n    \n#     transforms.RandomCrop(28),\n    \n    transforms.Pad(padding=2, padding_mode='symmetric'),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=mean, std=std)\n])\n\ntrain_dataset = CactusDataset(folder=current_path, labels=Y_train, transform=transform)\nvalid_dataset = CactusDataset(folder=need_path, labels=Y_valid, transform=transform)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset.view_sample(indices=indices, mean=mean, std=std)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Class"},{"metadata":{},"cell_type":"markdown","source":"https://www.aiworkbox.com/lessons/how-to-define-a-convolutional-layer-in-pytorch\n\nhttps://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n\nhttps://wandb.ai/authors/ayusht/reports/Implementing-and-Tracking-the-Performance-of-a-CNN-in-Pytorch-An-Example--VmlldzoxNjEyMDU\n\nhttps://www.aiworkbox.com/lessons/batchnorm2d-how-to-use-the-batchnorm2d-module-in-pytorch\n\nhttps://stackoverflow.com/questions/53419474/using-dropout-in-pytorch-nn-dropout-vs-f-dropout\n\nhttps://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html#torch.nn.LeakyReLU\n\nпочему leaky_relu\n\nhttps://www.quora.com/What-are-the-advantages-of-using-Leaky-Rectified-Linear-Units-Leaky-ReLU-over-normal-ReLU-in-deep-learning"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # свёрточные\n        # torch.Size([16, 3, 3, 3]) = torch.Size([out_channels, in_channels, kernel_size[0], kernel_size[1]])\n        # in_channels, out_channels: кол-во входных и выходных каналов соответственно\n        # kernel_size: размер ядра\n        # padding: сколько пустых (0) пикселей добавится\n        # stride: отступ\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n        \n        # избежать ковариантного сдвига и не насыщать функцию активации\n        # num_features: кол-во признаков в пред. слое\n        self.dense_1 = nn.BatchNorm2d(16)\n        \n        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n        self.dense_2 = nn.BatchNorm2d(32)\n        \n        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n        self.dense_3 = nn.BatchNorm2d(64)\n        \n        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n        self.dense_4 = nn.BatchNorm2d(128)\n        \n        # полносвязные\n        # принимает на вход (batch_size, 512) и возвращает тензор (batch_size, 128). y = x * W^T + b, W.shape=(out_features, in_features), b.shape=(out_features)\n        self.fc1 = nn.Linear(in_features=512, out_features=128)\n        self.fc_dense1 = nn.BatchNorm1d(128)\n        self.out = nn.Linear(in_features=128, out_features=2)\n         \n        # прореживание\n        # p: вер-сть отключения нейрона\n        self.d1 = nn.Dropout(0.5)\n        # сигмоидная функция (логит)\n        self.f = nn.Sigmoid()\n\n        \n    def forward(self, t):\n        x = t\n        x = self.conv1(x)\n        x = self.dense_1(x)\n        # LRELU = max{0, x} + neg * min{0, x}, neg == 0.01\n        x = F.leaky_relu(x)\n        # взятие максимума из прямоугольников (kernel_size[0], kernel_size[1])\n        # stride: отступ пикселей при соседних взятиях максимума\n        x = F.max_pool2d(x, kernel_size=2, stride=2)\n\n\n        x = self.conv2(x)\n        x = self.dense_2(x)\n        x = F.leaky_relu(x)\n        x = F.max_pool2d(x, kernel_size=2, stride=2)\n\n        x = self.conv3(x)\n        x = self.dense_3(x)\n        x = F.leaky_relu(x)\n        x = F.max_pool2d(x, kernel_size=2, stride=2)\n\n        x = self.conv4(x)\n        x = self.dense_4(x)\n        x = F.leaky_relu(x)\n        x = F.max_pool2d(x, kernel_size=2, stride=2)\n\n        # keras -> flatten(): сделать слой плотным (вытянуть)\n        x = x.reshape(-1, 512)\n\n        x = self.fc1(x)\n        x = self.fc_dense1(x)\n        x = F.leaky_relu(x)\n#         x = self.d1(x)\n        \n        x = self.out(x)\n        x = self.f(x)\n\n        return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Параметры"},{"metadata":{"trusted":true},"cell_type":"code","source":"# норма обучения\nlr = 0.02\n\n# максимальное количество эпох \nepochs = 100\n\n# размер батча\nbatch_size = 64\n\n# всё в основном процессе\nnum_workers = 0\n\n# словарь метрик\nmetrics = defaultdict(list)\n\n# сама модель\nmodel = Model().to(device)\n\n# оптимизатор\noptimizer = optim.Adam(model.parameters(), lr=lr)\n\n# стратегия изменения нормы обучения\nscheduler = MultiplicativeLR(optimizer, lr_lambda=lambda epoch: 0.9)\n\n# загрузчики\ntrain_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\nvalid_data_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_loader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# итератор по загрузчику\ndataiter = iter(train_data_loader)\n\n# один батч\nimages, labels = dataiter.next()\n\nplt.figure(figsize=(8, 8))\nplt.axis('off')\nplt.title(\"Изображения тренировочного набора\")\nplt.imshow(np.transpose(utils.make_grid(images.to(device), padding=2, normalize=True).cpu(), (1, 2, 0)))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Несбалансированность классов"},{"metadata":{"trusted":true},"cell_type":"code","source":"weight = class_weight.compute_class_weight('balanced', np.unique(Y_train['has_cactus']), Y_train['has_cactus'])\nweight = {i : weight[i] for i in np.unique(Y_train['has_cactus'])}\nweight","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Дополнительные метрики"},{"metadata":{"trusted":true},"cell_type":"code","source":"def other_metrics(y_true, y_pred, weights):\n    balanced_accuracy = balanced_accuracy_score(y_true, y_pred, sample_weight=[weights[i] for i in y_true])\n    accuracy = accuracy_score(y_true, y_pred)\n    precision= precision_score(y_true, y_pred)\n    recall = recall_score(y_true, y_pred)\n    roc_auc = roc_auc_score(y_true, y_pred)\n    \n    return np.array([balanced_accuracy, accuracy, precision, recall, roc_auc])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Тренировка сети"},{"metadata":{"trusted":true},"cell_type":"code","source":"# количество эпох без улучшения функции ошибки (на валидации)\nearly_stopping = 25\n\n# счётчик\nearly_count = 0\n\n# папка для сохранения модели\ncheckpoint_dirr = './checkpoint/'\ntry:\n    os.makedirs(checkpoint_dirr)\nexcept FileExistsError:\n    pass\n\n# начальная лучшая потеря\nbest_loss = np.inf\n\n# матрики\nbalanced_accuracy, accuracy, precision, recall, roc_auc = [0] * 5\n\n# по каждой эпохе\nfor epoch in range(epochs):\n    # шаг изменения learning rate\n    scheduler.step()\n    \n    # обнулить loss ошибку и метрики\n    train_loss = 0\n    metric = 0\n    model.train()\n    \n    with tqdm(train_data_loader, unit=\"batch\") as tepoch:\n        tepoch.set_description(f\"Epoch {epoch}\")\n        # по каждому батчу\n        for i, batch in enumerate(tepoch):\n\n            # получить изо и метки из батча\n            images, labels = batch\n            \n            # оптимизация под устройство\n            images = images.to(device)\n            labels = labels.to(device) \n\n            # в 64 бита\n            labels = labels.long()\n            \n            # прогнозы\n            preds = model(images)\n            pred = preds.argmax(dim=1)\n            \n            # функция потерь: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n            loss = F.cross_entropy(preds, labels)\n\n            # обнулить градиент перед обратным распространением\n            optimizer.zero_grad()\n            \n            # вычислить градиент потерь\n            loss.backward()\n            \n            # обновить параметры\n            optimizer.step()\n            \n            # значение ошибки\n            train_loss += loss.item()\n            \n            # другие метрики\n            metric += other_metrics(labels.tolist(), pred.tolist(), weight)\n            balanced_accuracy, accuracy, precision, recall, roc_auc = metric\n            \n            tepoch.set_postfix(loss=train_loss / (i + 1), balanced_accuracy=balanced_accuracy / (i + 1), accuracy=accuracy / (i + 1), precision=precision / (i + 1), \n                               recall=recall / (i + 1), roc_auc=roc_auc / (i + 1), lr=scheduler.get_lr()[0])\n        \n        # в словарь\n        metrics['loss'].append(train_loss / (i + 1))\n        metrics['balanced_accuracy'].append(balanced_accuracy / (i + 1))\n        metrics['accuracy'].append(accuracy / (i + 1))\n        metrics['precision'].append(precision / (i + 1))\n        metrics['recall'].append(recall / (i + 1))\n        metrics['roc_auc'].append(roc_auc / (i + 1))\n    \n    sleep(0.1)\n    \n    model.eval()\n    \n    # тоже самое, но без градиентов (т.к. валидация)\n    with torch.no_grad():\n        valid_loss = 0\n        metric = 0\n        with tqdm(valid_data_loader, unit=\"batch\") as tepoch:\n            tepoch.set_description(f\"Epoch {epoch}\")\n        \n            for i, valid_batch in enumerate(tepoch):\n                valid_images, valid_labels = valid_batch\n                valid_images = valid_images.to(device)\n                valid_labels = valid_labels.to(device)\n                valid_labels = valid_labels.long()\n\n                valid_preds = model(valid_images)\n                valid_pred = valid_preds.argmax(dim=1)\n\n                loss_valid = F.cross_entropy(valid_preds ,valid_labels)\n                \n                valid_loss += loss_valid.item()\n                \n                metric += other_metrics(valid_labels.tolist(), valid_pred.tolist(), weight)\n                balanced_accuracy, accuracy, precision, recall, roc_auc = metric\n                \n                tepoch.set_postfix(loss=valid_loss / (i + 1), balanced_accuracy=balanced_accuracy / (i + 1), accuracy=accuracy / (i + 1), precision=precision / (i + 1), \n                               recall=recall / (i + 1), roc_auc=roc_auc / (i + 1))\n\n            metrics['val_loss'].append(valid_loss / (i + 1))\n            metrics['val_balanced_accuracy'].append(balanced_accuracy / (i + 1))\n            metrics['val_accuracy'].append(accuracy / (i + 1))\n            metrics['val_precision'].append(precision / (i + 1))\n            metrics['val_recall'].append(recall / (i + 1))\n            metrics['val_roc_auc'].append(roc_auc / (i + 1))\n\n    sleep(0.1)\n    \n    # если ошибка улучшилась\n    if valid_loss < best_loss:\n        # пересохранить\n        best_loss = valid_loss\n        \n        # занулить счётчик\n        early_count = 0\n        \n        # записать \n        torch.save(model.state_dict(), f'{checkpoint_dirr}epoch:{epoch}.pt')\n    else:\n        early_count += 1\n\n        if early_count > early_stopping:\n            print(f\"Loss did not improve over {early_stopping} epochs => early stopping\")\n            break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Графики"},{"metadata":{"trusted":true},"cell_type":"code","source":"def graph_plot(history, typ=False):\n    if typ:\n        for i in history.keys():\n            print(f'{i} = [{min(history[i])}; {max(history[i])}]\\n')\n    \n    epoch = len(history['loss'])\n    # на каждую: (train, val) + lr\n    size = len(history.keys()) // 2 + 1\n    \n    ncols = 3\n    nrows = np.ceil(size / ncols)\n    \n    fig = plt.figure(figsize=(30, 20))\n    i = 1\n    for k in list(history.keys()):\n        if 'val' not in k:\n            fig.add_subplot(nrows, ncols, i)\n            plt.plot(history[k][2:], marker='o', markersize=5)\n            if k != 'lr':\n                plt.plot(history['val_' + k][2:], marker='o', markersize=5)\n            plt.title(k, fontsize=10)\n\n            plt.ylabel(k)\n            plt.xlabel('epoch')\n            plt.grid()\n\n            plt.yticks(fontsize=10, rotation=30)\n            plt.xticks(fontsize=10, rotation=30)\n            plt.legend(['train', 'valid'], loc='upper left', fontsize=10, title_fontsize=15)\n            i += 1\n#         plt.show()\n\ngraph_plot(metrics)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.argmax(-np.array(metrics['val_loss']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[name for name in sorted(os.listdir(checkpoint_dirr)) if str(np.argmax(-np.array(metrics['val_loss']))) in name][0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Загрузка лучшей"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Model().to(device)\nmodel.load_state_dict(torch.load(f\"{checkpoint_dirr}{[name for name in sorted(os.listdir(checkpoint_dirr)) if str(np.argmax(-np.array(metrics['val_loss']))) in name][0]}\"))\nmodel.eval()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Прогноз"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = CactusDataset(folder=test_path, labels=submission, \n                             transform=transforms.Compose([transforms.ToTensor(), \n                                                           transforms.Normalize(mean=mean, std=std)]))\n\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict = []\n\nfor test_batch in test_loader:\n    data, target = test_batch\n    data, target = data.to(device), target.to(device)\n    output = model(data)\n\n    predict += output.argmax(dim=1).tolist()\n\nsubmission['has_cactus'] = predict\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}