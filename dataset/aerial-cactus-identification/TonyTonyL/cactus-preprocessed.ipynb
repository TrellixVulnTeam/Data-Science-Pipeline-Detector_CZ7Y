{"cells":[{"metadata":{},"cell_type":"markdown","source":"**1. Import des packages de base**"},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\n\nimport pandas as pd\nimport numpy as np\nimport pylab as plt\nimport seaborn as sns; sns.set()\n\nimport random # Utile pour creer un jeu de validation\nrandom.seed(0)\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nimport sys","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"python\", sys.version)\nfor module in np, pd, tf, keras:\n    print(module.__name__, module.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assert sys.version_info >= (3, 5) # Python ≥3.5 required\nassert tf.__version__ >= \"2.0\"    # TensorFlow ≥2.0 required","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2. Import du csv d'apprentissage et observations des données**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dir = \"../input/cactus-dataset/cactus/train.csv\"\ntrain_data = pd.read_csv(train_dir)\ntrain_data.has_cactus = train_data.has_cactus.astype(str) # Pour appliquer le preprocessing il faut transformer les variables has_cactus en str","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.has_cactus.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On peut voir qu'il y a 13136 images avec cactus et 4364 images sans cactus. Le jeu de données n'est pas équilibré."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as mlp\nimport matplotlib.image as mpimg\n\nimg = mpimg.imread(\"../input/cactus-dataset/cactus/train/000c8a36845c0208e833c79c1bffedd1.jpg\")\nplt.axis(\"off\")\nimgplot = mlp.imshow(img)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**3. Data augmentation des images train**"},{"metadata":{},"cell_type":"markdown","source":"> Dans une précédente version du programme nous n'avions pas fait de data augmentation sur les images. Cela nous donnait un score beaucoup plus faible. <br>\nDe plus nous n'utilisions pas le parametre pour separer les données dans la fonction ImageDataGenerator, nous le faisions a la main. <br>\nNous avons ensuite tenté de faire de la data augmentation en tatonnant pour ajuster les parametres."},{"metadata":{"trusted":true},"cell_type":"code","source":"#nb_train = np.random.rand(len(train_data)) < 0.8\n#train_set = train_data[nb_train]\n#valid_set = train_data[~nb_train]\n\n#datagen = keras.preprocessing.image.ImageDataGenerator(\n#    rescale=1./255,\n#    shear_range = 0.2,\n#    zoom_range = 0.2,\n#    horizontal_flip = True,\n#    vertical_flip = True,\n#    preprocessing_function = keras.applications.xception.preprocess_input)\n\n#train_generator = datagen.flow_from_dataframe(\n#    dataframe = train_set,\n#    directory = train_dir,\n#    x_col = 'id',\n#    y_col = 'has_cactus',\n#    target_size = (32,32),\n#    batch_size = 64,\n#    class_mode = 'binary')\n\n#valid_generator = datagen.flow_from_dataframe(\n#    dataframe = valid_set,\n#    directory = train_dir,\n#    x_col = 'id',\n#    y_col = 'has_cactus',\n#    target_size = (32,32),\n#    batch_size = 64,\n#    class_mode = 'binary')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ndef generator(train_data, directory, batch_size, target_size, class_mode):\n    \n    x_col = 'id'\n    y_col = 'has_cactus'\n    \n    train_datagen = ImageDataGenerator(\n        rescale = 1./255, \n        horizontal_flip = True, \n        vertical_flip = True, \n        validation_split = 0.2)\n\n    train_generator = train_datagen.flow_from_dataframe(\n        train_data, \n        directory = directory, \n        x_col = x_col, \n        y_col = y_col, \n        target_size = target_size, \n        class_mode = class_mode, \n        batch_size = batch_size, \n        shuffle = True, \n        subset = 'training')\n\n    valid_generator = train_datagen.flow_from_dataframe(\n        train_data, \n        directory = directory, \n        x_col = x_col, \n        y_col = y_col, \n        target_size = target_size, \n        class_mode = class_mode, \n        batch_size = batch_size, \n        shuffle = True, \n        subset = 'validation')\n    \n    return train_generator, valid_generator","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On peut directement normaliser les images dans cette fonctions (rescale) et également separer nos données en deux pour en faire un jeu d'apprentissage et de validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"directory = \"../input/cactus-dataset/cactus/train\"\nbatch_size = 64\ntarget_size = (32,32) # On a des images 32x32\nclass_mode = 'binary' # Binary puisque qu'on a un vecteur contenant des 0 ou des 1\n\ntrain_generator, valid_generator = generator(train_data, directory, batch_size, target_size, class_mode)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**4. Construction et entrainement du reseau de neurones**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, Dense, Flatten, Dropout, Activation\nfrom tensorflow.keras.layers import BatchNormalization, MaxPooling2D, GlobalAveragePooling2D","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential([\n    Conv2D(32, (3, 3), padding = 'same', activation = 'relu', input_shape = (32,32,3)),\n    BatchNormalization(),\n    Conv2D(32, (3, 3), padding = 'same', activation = 'relu', input_shape = (32,32,3)),\n    BatchNormalization(),\n    MaxPooling2D(),\n    \n    Conv2D(64, (3, 3), padding = 'same', activation = 'relu'),\n    BatchNormalization(),\n    MaxPooling2D(),\n    \n    Conv2D(128, (3, 3), padding = 'same', activation = 'relu'),\n    BatchNormalization(),\n    MaxPooling2D(),\n    \n    Conv2D(256, (3, 3), padding = 'same', activation = 'relu'),\n    BatchNormalization(),\n    MaxPooling2D(),\n    \n    GlobalAveragePooling2D(),\n    \n    Dense(256, activation = 'relu'),\n    Dropout(0.5),\n    \n    Dense(1, activation = 'sigmoid')\n]) # padding = 'same' permet de ne pas diminuer la taille des images\n   # sigmoid au lieu de softmax puisqu'il est utilisé pour une regression logistique a 2 classes (classification), de plus la somme des proba ne doit pas etre égale a 1\n\nmodel.compile(loss = 'binary_crossentropy',\n              optimizer = 'adam',\n              metrics = ['accuracy']) # On utilise une crossentropy binaire et non pas une sparse_categorical_crossentropy\n                                      # On utilise l'optimizer Adam, plus rapide que SGD : en ayant effectuer plusieurs test on peut supposer qu'il converge correctement\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dans un premier temps nous avions fait le reseau de neurone suivant mais les resultats n'etaient pas concluant, nous avions tenter de remplacer les Conv2D par des SeparableConv2D :"},{"metadata":{"trusted":true},"cell_type":"code","source":"#model = keras.models.Sequential([\n    \n#    keras.layers.Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\", input_shape=[32, 32, 3]),\n#    keras.layers.BatchNormalization(),\n#    keras.layers.SeparableConv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\"),\n#    keras.layers.BatchNormalization(),\n#    keras.layers.MaxPool2D(pool_size=2),\n#    keras.layers.Dropout(rate=0.4),\n    \n#    keras.layers.SeparableConv2D(filters=64, kernel_size=3, padding=\"same\", activation=\"relu\"),\n#    keras.layers.BatchNormalization(),\n#    keras.layers.SeparableConv2D(filters=64, kernel_size=3, padding=\"same\", activation=\"relu\"),\n#    keras.layers.BatchNormalization(),\n#    keras.layers.MaxPool2D(pool_size=2),\n#    keras.layers.Dropout(rate=0.4),\n    \n#    keras.layers.Flatten(),\n#    keras.layers.Dense(128, activation=\"relu\"),\n#    keras.layers.Dense(1, activation=\"sigmoid\")\n#])\n\n#model.compile(optimizer= keras.optimizers.SGD(lr=1e-4, momentum=0.9), loss='binary_crossentropy', \n#              metrics=['accuracy'])\n\n#epochs = 40\n#history = model.fit_generator(train_generator,\n#          validation_data=valid_generator,\n#          epochs=epochs,\n#          callbacks = [EarlyStopping(patience=10)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom sklearn.utils import class_weight\n\nclass_weights = class_weight.compute_class_weight('balanced', np.unique(train_generator.classes), train_generator.classes)\n\ncallbacks = [EarlyStopping(monitor = 'val_loss', patience = 20),\n             ReduceLROnPlateau(patience = 10, verbose = 1),\n             ModelCheckpoint(filepath = 'best_model.h5', monitor = 'val_loss', verbose = 0, save_best_only = True)]\n\nhistory = model.fit_generator(train_generator,\n          validation_data = valid_generator,\n          epochs = 100,\n          verbose = 1,\n          shuffle = True,\n          callbacks = callbacks,\n          class_weight = class_weights)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dans notre programme précédent nous n'avions pas utiliser de callbacks : <br>\n- Early stopping pour eviter un surajustement du jeu d'entrainement\n- ReduceLROnPlateau nous permet de réduire le learning rate lorsque la val_accuracy ne s'ameliore plus\n- ModelCheckpoint va nous permettre de sauvegarder le modele apres chaque epoch, notamment en observant les val_loss <br> <br>\n\nNotre jeu d'image n'est pas équilibré, on utlise la fonction compute_class_weight pour estimer le poids des classes du jeu d'image : <br>\n- 'balanced' va permettre de \"répliquer\" la classe inférieure (0 : pas de cactus) jusqu'a obtenir autant d'echantillions que la classe supérieure (1 : cactus) et donc de réequilibrer les poids"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(history.history).plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights(\"best_model.h5\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**5. Apprentissage sur le jeu test et soumisson**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_gen(test_dir, target_size, batch_size, class_mode):\n    test_datagen = ImageDataGenerator(\n        rescale = 1./255)\n\n    test_generator = test_datagen.flow_from_directory(\n        directory = test_dir,\n        target_size = target_size, \n        batch_size = batch_size,\n        class_mode = class_mode,\n        shuffle = False)  \n    return test_generator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dir = \"../input//cactus-dataset/cactus/test/\"\ntarget_size = (32,32)\nbatch_size = 1\nclass_mode = None\n\ntest_generator = test_gen(test_dir, target_size, batch_size, class_mode)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Avant de prédire des probabilités, nous voulions afficher uniquement des 0 ou des 1, mais les resultats n'etaient pas concluant."},{"metadata":{"trusted":true},"cell_type":"code","source":"#pred = model.predict_generator(test_generator,verbose=1)\n#pred_binary = [0 if value < 0.50 else 1 for value in pred]  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def submission():\n    sample_submission = pd.read_csv(\"../input/cactus-dataset/cactus/sample_submission.csv\")\n\n    filenames = [path.split('/')[-1] for path in test_generator.filenames] # On récupere les noms des images pour en faire une colonne sur notre csv final\n    proba = list(model.predict_generator(test_generator)[:,0]) # On recupere les probabilités prédites par notre modele sur le jeu test (sur lequel on a fait de la data augmentation)\n\n    sample_submission.id = filenames\n    sample_submission.has_cactus = proba\n\n    sample_submission.to_csv('submission.csv', index=False)\n    return sample_submission\n\nsample_submission = submission()\nsample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Par manque de temps nous n'avons pas pu tester le preprocess avec xception (en supposant que les images soient préentrainées)."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}