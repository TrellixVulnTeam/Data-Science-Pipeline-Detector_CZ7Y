{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"/kaggle/input/test\"))\n!pip install efficientnet_pytorch\n!pip install torchsummary\nfrom efficientnet_pytorch import EfficientNet\nimport torchvision\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport torchvision.models as models\nfrom torchsummary import summary\nimport torch.optim as optim\nimport copy\nimport os\nimport torch\nfrom tqdm.autonotebook import tqdm\nfrom torch.optim.lr_scheduler import _LRScheduler\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom torch.utils.data import Dataset\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install googledrivedownloader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''from google_drive_downloader import GoogleDriveDownloader as gdd\n\ngdd.download_file_from_google_drive(file_id='1DSTHrFxJF4Xq7Jyu1ZdTZMs1NuRLelQ2',\n                                    dest_path='/kaggle/working/cactuseff_2.h5')'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from google_drive_downloader import GoogleDriveDownloader as gdd\ngdd.download_file_from_google_drive(file_id='1_O-7ypeXY381lP6vaSQBeGDr6x5Y6VCN',\n                                    dest_path='/kaggle/working/cactusdense_3.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_csv = pd.read_csv('/kaggle/input/train.csv')\n#train_csv.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''from sklearn.model_selection import train_test_split\nclass cactus_dataset(Dataset):\n  def __init__(self,image_dir,train_csv,transform = None):\n    self.img_dir = image_dir\n    self.transform = transform\n    self.id = train_csv.iloc[:,0]\n    self.classes =  train_csv.iloc[:,1]\n  def __len__(self):\n    return len(self.id)\n  def __getitem__(self,idx):\n    img_name = os.path.join(self.img_dir, self.id[idx])\n    image = cv2.imread(img_name)\n    if self.transform:\n        image = self.transform(image)\n    label = self.classes[idx]\n    return image,label\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''batch_size = 8\nimport cv2\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\ntrain_transforms = transforms.Compose([\n                                        transforms.ToPILImage(),\n                                    \n                                        transforms.RandomResizedCrop(224),                                    \n                                        transforms.RandomHorizontalFlip(),\n                                        #transforms.RandomRotation(30),\n                                        transforms.ToTensor(),\n                                        transforms.Normalize([0.485, 0.456, 0.406], \n                                                            [0.229, 0.224, 0.225])])\ntest_transforms = transforms.Compose([\n                                        transforms.ToPILImage(),\n                                        transforms.Resize(256),\n                                          transforms.CenterCrop(224),\n                                        transforms.ToTensor(),\n                                        transforms.Normalize([0.485, 0.456, 0.406], \n                                                            [0.229, 0.224, 0.225])])\n\n#inverse normalization for image plot\ntrain_data = cactus_dataset('/kaggle/input/train/train',train_csv,transform = train_transforms)\n#val_data = cactus_dataset('/kaggle/input/train/train',val_df,transform = test_transforms)\ntrain_loader = DataLoader(train_data, batch_size=8,\n                        shuffle=True, num_workers=0)\n\n#val_loader = DataLoader(val_data, batch_size=4,shuffle=True, num_workers=0)\ndataloaders = {'train':train_loader}\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torchvision\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport torchvision.models as models\n#from torchsummary import summary\nimport torch.optim as optim\nimport copy\nimport os\nimport torch\nfrom tqdm.autonotebook import tqdm\n\nimport matplotlib.pyplot as plt\n\nclass classifie(nn.Module):\n    def __init__(self):\n        super(classifie, self).__init__()\n        model = models.densenet201(pretrained = True)\n        model = model.features\n        #model = EfficientNet.from_pretrained('efficientnet-b3')\n        #model =  nn.Sequential(*list(model.children())[:-3])\n        \n        self.model = model\n        self.linear = nn.Linear(3840, 512)\n        self.bn = nn.BatchNorm1d(512)\n        self.dropout = nn.Dropout(0.5)\n        self.elu = nn.ELU()\n        self.out = nn.Linear(512, 2)\n        self.bn1 = nn.BatchNorm1d(3840)\n        self.dropout2 = nn.Dropout(0.2)\n    def forward(self, x):\n        out = self.model(x)\n        avg_pool = nn.functional.adaptive_avg_pool2d(out, output_size = 1)\n        max_pool = nn.functional.adaptive_max_pool2d(out, output_size = 1)\n        out = torch.cat((avg_pool,max_pool),1)\n        batch = out.shape[0]\n        out = out.view(batch, -1)\n        conc = self.linear(self.dropout2(self.bn1(out)))\n        conc = self.elu(conc)\n        conc = self.bn(conc)\n        conc = self.dropout(conc)\n        res = self.out(conc)\n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = classifie().to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_state_dict(torch.load('/kaggle/working/cactusdense_3.h5'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''import torch.optim as optim\nimport matplotlib.pyplot as plt\nimport random\nfrom torch.autograd import Variable\nimport numpy as np\nimport torch\nfrom torch import nn\nimport sys\ndef train(model,dataloaders,device,num_epochs,lr,batch_size,patience):\n    phase1 = dataloaders.keys()\n    losses = list()\n    criterion = nn.CrossEntropyLoss()\n    acc = list()\n    for epoch in range(num_epochs):\n        print('Epoch:',epoch)\n        optimizer = optim.Adam(model.parameters(), lr=lr,weight_decay = 1e-6)\n        lr = lr*0.9\n        for phase in phase1:\n            epoch_metrics = {\"loss\": [], \"acc\": []}\n            if phase == ' train':\n                model.train()\n            else:\n                model.eval()\n            for  batch_idx, (data, target) in enumerate(dataloaders[phase]):\n                data, target = Variable(data), Variable(target)\n                data = data.type(torch.FloatTensor).to(device)\n                target = target.type(torch.LongTensor).to(device)\n\n                optimizer.zero_grad()\n                output = model(data)\n                loss = criterion(output, target)\n                target = target.type(torch.LongTensor).to(device)\n\n                acc = 100 * (output.detach().argmax(1) == target).cpu().numpy().mean()\n                epoch_metrics[\"loss\"].append(loss.item())\n                epoch_metrics[\"acc\"].append(acc)\n                if(phase =='train'):\n                    loss.backward()\n                    optimizer.step()\n                sys.stdout.write(\n                \"\\r[Epoch %d/%d] [Batch %d/%d] [Loss: %f (%f), Acc: %.2f%% (%.2f%%)]\"\n                % (\n                    epoch,\n                    num_epochs,\n                    batch_idx,\n                    len(dataloaders[phase]),\n                    loss.item(),\n                    np.mean(epoch_metrics[\"loss\"]),\n                    acc,\n                    np.mean(epoch_metrics[\"acc\"]),\n                    )\n                )\n               \n            epoch_acc = np.mean(epoch_metrics[\"acc\"])\n            epoch_loss = np.mean(epoch_metrics[\"loss\"])\n        print('')  \n        print('{} Accuracy: {}'.format(phase,epoch_acc.item()))\n    return losses,acc\n\ndef train_model(model,dataloaders,encoder,lr_scheduler = None,inv_normalize = None,num_epochs=10,lr=0.0001,batch_size=8,patience = None,classes = None):\n    dataloader_train = {}\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    losses = list()\n    accuracy = list()\n    key = dataloaders.keys()\n    perform_test = False\n    for phase in key:\n        if(phase == 'test'):\n            perform_test = True\n        else:\n            dataloader_train.update([(phase,dataloaders[phase])])\n    losses,accuracy = train(model,dataloader_train,device,num_epochs,lr,batch_size,patience)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import cv2\n#lr = 0.001\n#train_model(classifier,dataloaders,encoder,inv_normalize = None,num_epochs=4,lr = lr,batch_size = batch_size,patience = None,classes = classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class cactus_dataset_test(Dataset):\n  def __init__(self,image_dir,transform = None):\n    self.img_dir = image_dir\n    self.transform = transform\n    self.id = os.listdir(image_dir)\n  def __len__(self):\n    return len(self.id)\n  def __getitem__(self,idx):\n    img_name = os.path.join(self.img_dir, self.id[idx])\n    image = cv2.imread(img_name)\n    if self.transform:\n        image = self.transform(image)\n    return (self.id[idx],image)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\ntest_transforms = transforms.Compose([\n                                        transforms.ToPILImage(),\n                                        transforms.ToTensor(),\n                                        transforms.Normalize([0.485, 0.456, 0.406], \n                                                            [0.229, 0.224, 0.225])])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test1 = cactus_dataset_test('/kaggle/input/test/test',test_transforms)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_loader = DataLoader(test1, batch_size =32, shuffle = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test(model,dataloader,device,batch_size):\n    running_corrects = 0\n    running_loss=0\n    pred = []\n    id = list()\n    sm = nn.Softmax(dim = 1)\n    criterion = nn.CrossEntropyLoss()\n    for batch_idx, (id_1,data) in enumerate(dataloader):\n        data = Variable(data)\n        data = data.type(torch.FloatTensor).to(device)\n        model.eval()\n        output = model(data)\n        #output = sm(output)\n        _, preds = torch.max(output, 1)\n        preds = preds.cpu().numpy()\n        preds = np.reshape(preds,(len(preds),1))\n        \n        for i in range(len(preds)):\n            pred.append(preds[i])\n            id.append(id_1[i])\n    return id,pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.optim as optim\nimport matplotlib.pyplot as plt\nfrom torch.autograd import Variable\nimport numpy as np\nimport torch\nfrom torch import nn\nid,pred = test(model,test_loader,'cuda',32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = list()\nfor i in range(len(pred)):\n    a.append(pred[i][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = np.asarray(a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = np.reshape(a,(-1,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"b = np.asarray(id)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"b = np.reshape(b,(-1,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = np.concatenate((b,a),axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df = pd.DataFrame(sub)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.columns = ['id','has_cactus']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.to_csv(\"/kaggle/working/submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}