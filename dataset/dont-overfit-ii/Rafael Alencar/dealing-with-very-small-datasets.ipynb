{"cells":[{"metadata":{"_uuid":"86c7b637c8f0b16fdd573ac8ff6576ab82f47cdd"},"cell_type":"markdown","source":"![](https://raw.githubusercontent.com/rafjaa/curso-mineracao-de-dados-aplicada/master/img/kernel_overfitting/creativity.jpg)\n\n# Dealing with very small datasets\n\nIn this kernel we will see some techniques to handle very small datasets, where the main challenge is to avoid overfitting.\n\n1. <a href=\"#t1\">Why small datasets lead to overfitting?</a>\n2. <a href=\"#t2\">Use simple models</a>\n3. <a href=\"#t3\">Beware the outliers</a>\n4. <a href=\"#t4\">Select the features</a>\n5. <a href=\"#t5\">Balance the dataset with synthetic samples (SMOTE)</a>\n6. <a href=\"#t6\">Combine models for the final submission</a>\n7. <a href=\"#t7\">References</a>"},{"metadata":{"_uuid":"c26cdecd580c5993bef9df0d694918a7151cd6e5"},"cell_type":"markdown","source":"Let's load the data from the **Don't Overfit! II** competition:"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\n\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\nlabels = train.columns.drop(['id', 'target'])\ntarget = train['target']\n\nprint('Train rows:', train.shape[0])\nprint('Test rows:', test.shape[0])\nprint('Data columns:', test.columns.drop(['id']).shape[0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d051facc3f1f4492636a0a8fab86b395b593c146"},"cell_type":"markdown","source":"A visual inspection of the number of rows:"},{"metadata":{"trusted":true,"_uuid":"6fd9e2a4a61f803057c2bc58c217a51968e56165"},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nd_names = ('train.csv', 'test.csv')\ny_pos = range(len(d_names))\n \nplt.bar(\n    y_pos, \n    (train.shape[0], test.shape[0]), \n    align='center', \n    alpha=0.8\n)\nplt.xticks(y_pos, d_names)\nplt.ylabel('Number of rows') \nplt.title('😱 Wow!')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a45eede83e544b588fe660c3cf8b77e23ef3ed75"},"cell_type":"markdown","source":"<h1 id=\"t1\">1. Why small datasets lead to overfitting?</h1>\n\nThe goal of a machine learning model is to **generalize** patterns in training data so that you can correctly predict new data that has never been presented to the model. Overfitting occurs when a model adjusts excessively to the training data, seeing patterns that do not exist, and consequently performing poorly in predicting new data:\n\n![](https://raw.githubusercontent.com/rafjaa/curso-mineracao-de-dados-aplicada/master/img/kernel_overfitting/under_over.png)\n<center><strong>Source:</strong> <a href=\"https://medium.com/@shubhapatnim86/generalisation-training-validation-test-data-machine-learning-part-6-1de9dbb7d3d5\">https://medium.com/@shubhapatnim86/generalisation-training-validation-test-data-machine-learning-part-6-1de9dbb7d3d5</a></center>\n\n<br>\n\nThe fewer samples for training, the more models can fit our data. In an extreme example (a), for just one training point, any model will be able to \"explain\" it, however simple or complex the model may be. As we get to have more samples (b, c), fewer models are able to explain them:\n\n![](https://raw.githubusercontent.com/rafjaa/curso-mineracao-de-dados-aplicada/master/img/kernel_overfitting/few_samples.png)\n<center><strong>Source:</strong> <a href=\"https://towardsdatascience.com/breaking-the-curse-of-small-datasets-in-machine-learning-part-1-36f28b0c044d\">https://towardsdatascience.com/breaking-the-curse-of-small-datasets-in-machine-learning-part-1-36f28b0c044d</a></center>\n\n<br>\n\nThat way, for a dataset with only 250 samples, we need to be very careful not to be fooled by overfitting. In this kernel we will see some tips that can help.\n\n![](https://raw.githubusercontent.com/rafjaa/curso-mineracao-de-dados-aplicada/master/img/kernel_overfitting/meme1.jpeg)"},{"metadata":{"_uuid":"4c86e4397232fa5e9ca6033629233a669dd4ca47"},"cell_type":"markdown","source":"<h1 id=\"t2\">2. Use simple models</h1>\n\nAs we saw, few samples allow several models to \"explain\" the data. By thinking graphically, complex models can make crazy curves that will almost perfectly explain the training data, but possibly will perform poorly over the test data.\n\n![](https://raw.githubusercontent.com/rafjaa/curso-mineracao-de-dados-aplicada/master/img/kernel_overfitting/overfitting_curve.png)\n<center><strong>Source:</strong> <a href=\"https://bioinfo.iric.ca/overfitting-and-regularization/\">https://bioinfo.iric.ca/overfitting-and-regularization/</a></center>\n\n<br>\n\nAvoid complex models with many parameters, thus limiting their generalization and possibility of overfitting. Regularization techniques like L1 and L2 also help make the models more conservative. For tree-based models, reducing their maximum depth also limits the model's ability to see patterns and non-existent relationships.\n\nA good model to get started is Logistic Regression, a linear model used when the dependent variable (target) is categorical (classification tasks).\n\n![](https://raw.githubusercontent.com/rafjaa/curso-mineracao-de-dados-aplicada/master/img/kernel_overfitting/logisticregression.png)\n<center><strong>Source:</strong> <a href=\"https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc\">https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc</a></center>\n\n<br>\n\nIn this model, we can control the regularization by means of the <code>penalty</code> and <code>C</code> parameters (inverse of regularization strength - smaller values specify stronger regularization) to deal with the overfitting:"},{"metadata":{"trusted":true,"_uuid":"af6bee68c72fc1f24980cdf910b4af276523cab5"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nm = LogisticRegression(\n    penalty='l1',\n    C=0.1\n)\nm.fit(train[labels], target)\nm.predict_proba(test[labels])[:,1]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0cc6dd4980e4aacbd3e42ea806bab09839175851"},"cell_type":"markdown","source":"For tree-based models like XGBoost, we can control the overfitting by tuning a series of parameters:\n\n- Restricting the maximum depth of trees via <code>max_depth</code> (low values)\n- Making the model more conservative via <code>gamma</code> and <code>eta</code> (high values)\n- L1 and L2 regularization via <code>reg_alpha</code> and <code>reg_lambda</code> (high values)"},{"metadata":{"trusted":true,"_uuid":"552cc02db45c39cc57cc674e6abb4495c55c8a5c"},"cell_type":"code","source":"from xgboost import XGBClassifier\n\nm = XGBClassifier(\n    max_depth=2,\n    gamma=2,\n    eta=0.8,\n    reg_alpha=0.5,\n    reg_lambda=0.5\n)\nm.fit(train[labels], target)\nm.predict_proba(test[labels])[:,1]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"371cee8539913c6d7e473640f136965ea53383ea"},"cell_type":"markdown","source":"**Note that because it is a nonlinear model with several parameters, it tends to be more prone to overfitting than a simple linear model such as logistic regression.**"},{"metadata":{"_uuid":"52a6bc0d76ea6787dca7f5b0a79a5c5957c8d2c3"},"cell_type":"markdown","source":"<h1 id=\"t3\">3. Beware the outliers</h1>\n\nOutliers are extreme values that fall a long way outside of the other observations. In a small dataset, the impact of an outlier can be much greater, since it will have a heavy weight for the model:\n\n![](https://raw.githubusercontent.com/rafjaa/curso-mineracao-de-dados-aplicada/master/img/kernel_overfitting/outlier.png)\n<center><strong>Source:</strong> <a href=\"https://becominghuman.ai/day-7-data-cleaning-all-that-you-need-to-know-about-it-23b05738abe7\">https://becominghuman.ai/day-7-data-cleaning-all-that-you-need-to-know-about-it-23b05738abe7</a></center>\n\n<br>\n\nThe scikit-learn library has several implementations of outliers detection techniques:\n\n![](https://raw.githubusercontent.com/rafjaa/curso-mineracao-de-dados-aplicada/master/img/kernel_overfitting/outl_detection.png)\n<center><strong>Source:</strong> <a href=\"https://scikit-learn.org/stable/auto_examples/plot_anomaly_comparison.html\">https://scikit-learn.org/stable/auto_examples/plot_anomaly_comparison.html</a></center>\n\n<br>\n\nLet's do an experiment with the <code>IsolationForest</code> technique, which uses random forests for efficient detection of outliers in high-dimensional datasets. From scikit-learn documentation:\n\n> The IsolationForest ‘isolates’ observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.\n> \n> Since recursive partitioning can be represented by a tree structure, the number of splittings required to isolate a sample is equivalent to the path length from the root node to the terminating node.\n> \n> This path length, averaged over a forest of such random trees, is a measure of normality and our decision function.\n\n![](https://raw.githubusercontent.com/rafjaa/curso-mineracao-de-dados-aplicada/master/img/kernel_overfitting/isolation_forest.jpg)\n<center><strong>Source:</strong> <a href=\"https://www.slideshare.net/mlvlc/l14-anomaly-detection\">https://www.slideshare.net/mlvlc/l14-anomaly-detection</a></center>\n\n<br>\n\nLet's score each sample of our dataset using a isolation forest (the lower, the more abnormal):"},{"metadata":{"trusted":true,"_uuid":"fa4e76a07168b3638050b762aa5cb9f5d9ebfe28"},"cell_type":"code","source":"from sklearn.ensemble import IsolationForest\n\nisf = IsolationForest(n_jobs=-1, random_state=1)\nisf.fit(train[labels], train['target'])\n\nprint(isf.score_samples(train[labels]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ddba95cb7e564206474a39ce5889c93159adaa38"},"cell_type":"markdown","source":"Now, let's predict the outliers (1 for inliers, -1 for outliers):"},{"metadata":{"trusted":true,"_uuid":"9f2055a4e158a9d3031b8ba92b1b8536862c50c4"},"cell_type":"code","source":"isf.predict(train[labels])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d42c39fec2ae9aae3c27aa0035ea0061c74bfbb1"},"cell_type":"markdown","source":"**Beware, with few samples, it becomes a challenge to adjust the algorithms to correctly identify the outliers.**"},{"metadata":{"_uuid":"f7a275c6a941a7b20b908f5cbd4b18d2d6766870"},"cell_type":"markdown","source":"<h1 id=\"t4\">4. Select the features</h1>\n\nWhile removing outliers consists of deleting rows from the dataset, feature selection consists of deleting columns that do not contribute to the prediction. There is a wide variety of methods, such as analysis of its correlation with the target, importance analysis and recursive elimination.\n\nLet's see an example of how to identify the most relevant features for classification using a tree model:"},{"metadata":{"trusted":true,"_uuid":"f721b3aca3bd586586f6ef3956a2550bf24be06c"},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\n\nTOP_FEATURES = 15\n\nforest = ExtraTreesClassifier(n_estimators=250, max_depth=5, random_state=1)\nforest.fit(train[labels], train['target'])\n\nimportances = forest.feature_importances_\nstd = np.std(\n    [tree.feature_importances_ for tree in forest.estimators_],\n    axis=0\n)\nindices = np.argsort(importances)[::-1]\nindices = indices[:TOP_FEATURES]\n\nprint('Top features:')\nfor f in range(TOP_FEATURES):\n    print('%d. feature %d (%f)' % (f + 1, indices[f], importances[indices[f]]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ecc50c16f2cf1c39bf6d5c1e198c59fb8e99b74"},"cell_type":"markdown","source":"We can plot a chart of the importances:"},{"metadata":{"trusted":true,"_uuid":"ac55b127995b43ed7347e59d9b8e9e07484cf802"},"cell_type":"code","source":"plt.figure()\nplt.title('Top feature importances')\nplt.bar(\n    range(TOP_FEATURES), \n    importances[indices],\n    yerr=std[indices], \n)\nplt.xticks(range(TOP_FEATURES), indices)\nplt.show()","execution_count":36,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"metadata":{"_uuid":"5ea7fbeb73ee8e742bd5671d9050a295130d063e"},"cell_type":"markdown","source":"In the next example, we will try to identify the most important features by successively training a model and recursively eliminating those that do not contribute to a good final solution (according to the selected model).\n\nWe will use the recursive feature elimination <code>RFE</code> from the scikit-learn library and the <code>XGBClassifier</code> model:"},{"metadata":{"trusted":true,"_uuid":"adb2e9aa37a3064ef42fdd2bf8a3d6a6933be3a9"},"cell_type":"code","source":"from sklearn.feature_selection import RFE\n\nrfe = RFE(XGBClassifier(n_jobs=-1, random_state=1))\n\nrfe.fit(train[labels], train['target'])\n\nprint('Selected features:')\nprint(labels[rfe.support_].tolist())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"627acfe6b68e64db435b37ebb9bfe45e412c2df1"},"cell_type":"markdown","source":"**Caution: for a very small dataset, very robust local validation is required to determine whether or not a feature contributes to the final solution. **"},{"metadata":{"_uuid":"deb0972159b6229a555db03f3efdc6d8cddd8e95"},"cell_type":"markdown","source":"<h1 id=\"t5\">5. Balance the dataset with synthetic samples (SMOTE)</h1>\n\n<br>\n\nLet's look at the distribution of target values:"},{"metadata":{"trusted":true,"_uuid":"5a2736f9c0d29dd6514c89d82c45be3e9a338ae9"},"cell_type":"code","source":"train['target'].value_counts().plot(kind='bar', title='Count (target)');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e9632368a131cac585107c69662f8bdc9cd9b4c"},"cell_type":"markdown","source":"In addition to being extremely small, our training dataset has the unbalanced <code>target</code> binary variable, which can undermine some models' predictability. We will perform an oversampling, which consists of creating new samples to increase the <code>0</code> minority class. For this we will use the SMOTE technique.\n\nSMOTE (Synthetic Minority Oversampling TEchnique) consists of synthesizing elements for the minority class, based on those that already exist. It works randomly picingk a point from the minority class and computing the k-nearest neighbors for this point. The synthetic points are added between the chosen point and its neighbors.\n\n![](https://raw.githubusercontent.com/rafjaa/machine_learning_fecib/master/src/static/img/smote.png)\n<center><strong>Source:</strong> <a href=\"https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets\">https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets</a></center>\n\n<br>\n\nWe'll use the SMOTE implementation from the library <code>imbalanced-learn</code>, with the parameter <code>ratio='minority'</code> to resample the minority class:"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"bf36c7a7d6184b5ec40bfda5e3e11c2bdc7d0433"},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(ratio='minority', n_jobs=-1)\nX_sm, y_sm = smote.fit_resample(train[labels], train['target'])\n\ndf = pd.DataFrame(X_sm, columns=labels)\ndf['target'] = y_sm\n\ndf['target'].value_counts().plot(kind='bar', title='Count (target)');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8fb6548d56956e994069d0641283fc17b534a910"},"cell_type":"markdown","source":"<h1 id=\"t6\">6. Combine models for the final submission</h1>\n\nCombine the prediction of several models or the same model with different values of hyperparameters reduces variance and enhances generalization.\n\n![](https://raw.githubusercontent.com/rafjaa/curso-mineracao-de-dados-aplicada/master/img/kernel_overfitting/combine.jpg)\n<center><strong>Source:</strong> <a href=\"https://medium.com/rants-on-machine-learning/what-to-do-with-small-data-d253254d1a89\">https://medium.com/rants-on-machine-learning/what-to-do-with-small-data-d253254d1a89</a></center>\n\n<br>\n\nOften, combining weak models that are poorly correlated with each other can lead to superior results than a strong individual model. There are several ways to do this. The simplest is to perform a weighted average of the various predictions:"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"6c94a9467b0cbbfdb2c65d1690233efd2bce1cf9"},"cell_type":"code","source":"models = [\n    LogisticRegression(),\n    XGBClassifier(max_depth=2)\n]\n\npreds = pd.DataFrame()\nfor i, m in enumerate(models):\n    m.fit(train[labels], target),\n    preds[i] = m.predict_proba(test[labels])[:,1]\n\nweights = [1, 0.3]\npreds['weighted_pred'] = (preds * weights).sum(axis=1) / sum(weights)\npreds.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e0ebfeab0afddd15ebcb263878c6d8110db2d22"},"cell_type":"markdown","source":"Another more sophisticated way of combining predictions is the use of a meta-classifier, which receives as input the prediction of other classifiers, and performs the final predict. From the <code>mlxtend</code> library documentation:\n\n> Stacking is an ensemble learning technique to combine multiple classification models via a meta-classifier. The individual classification models are trained based on the complete training set; then, the meta-classifier is fitted based on the outputs -- meta-features -- of the individual classification models in the ensemble. The meta-classifier can either be trained on the predicted class labels or probabilities from the ensemble.\n\n![](https://raw.githubusercontent.com/rafjaa/curso-mineracao-de-dados-aplicada/master/img/kernel_overfitting/stack.png)\n<center><strong>Source:</strong> <a href=\"http://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/\">http://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/</a></center>\n\n<br>\n\nAs an example, we will use the <code>StackingClassifier</code> of the <code>mlxtend</code> library:"},{"metadata":{"trusted":true,"_uuid":"90a2afb30f624565faa6d2f7a6e04220003b0938"},"cell_type":"code","source":"from mlxtend.classifier import StackingClassifier\n\nm = StackingClassifier(\n    classifiers=[\n        LogisticRegression(),\n        XGBClassifier(max_depth=2)\n    ],\n    use_probas=True,\n    meta_classifier=LogisticRegression()\n)\n\nm.fit(train[labels], target),\npreds['stack_pred'] = m.predict_proba(test[labels])[:,1]\npreds.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"04c653c3e5b8380b2e297be67865b516022dc6b5"},"cell_type":"markdown","source":"![](https://raw.githubusercontent.com/rafjaa/curso-mineracao-de-dados-aplicada/master/img/kernel_overfitting/mem2.jpg)"},{"metadata":{"trusted":true,"_uuid":"f8a97a205f557f6cd475e067a8ee953ec5f52e87"},"cell_type":"markdown","source":"<h1 id=\"t7\">References</h1>\n\n**Overfitting:**\n* IRIC's Bioinformatics Platform. Overfitting and Regularization. https://bioinfo.iric.ca/overfitting-and-regularization/\n* PATNI, Shubham. Generalisation, Training-Validation & Test data. Machine Learning- Part 6. https://medium.com/@shubhapatnim86/generalisation-training-validation-test-data-machine-learning-part-6-1de9dbb7d3d5\n* Rants on Machine Learning. What to do with “small” data? https://medium.com/rants-on-machine-learning/what-to-do-with-small-data-d253254d1a89\n* Towards Data Science. Breaking the curse of small datasets in Machine Learning: Part 1. https://towardsdatascience.com/breaking-the-curse-of-small-datasets-in-machine-learning-part-1-36f28b0c044d\n\n**Models:**\n* scikit-learn. LogisticRegression. https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n* Towards Data Science. Logistic Regression — Detailed Overview. https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc\n* XGBoost. XGBoost Parameters. https://xgboost.readthedocs.io/en/latest/parameter.html\n\n**Outlier detection:**\n* Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. “Isolation forest.” Data Mining, 2008. ICDM‘08. Eighth IEEE International Conference on.\n* Machine Learning Valencia. Anomaly Detection. Valencian Summer School 2015. https://www.slideshare.net/mlvlc/l14-anomaly-detection\n* scikit-learn. Novelty and Outlier Detection. https://scikit-learn.org/stable/modules/outlier_detection.html\n* VADALI, SaiGayatri.Day 7: Data cleaning — All you need to know about it. https://becominghuman.ai/day-7-data-cleaning-all-that-you-need-to-know-about-it-23b05738abe7\n\n**Feature selection:**\n* scikit-learn. Feature selection. https://scikit-learn.org/stable/modules/feature_selection.html\n\n**SMOTE**\n* ALENCAR, Rafael. Resampling strategies for imbalanced datasets. https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets\n* Chawla, Nitesh V., et al. \"SMOTE: synthetic minority over-sampling technique.\" Journal of artificial intelligence research 16 (2002)\n* imbalanced-learn. https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html\n\n**Stacking**\n* mlxtend. StackingClassifier. http://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/\n* Tang, J., S. Alelyani, and H. Liu. \"Data Classification: Algorithms and Applications.\" Data Mining and Knowledge Discovery Series, CRC Press (2015): pp. 498-500.\n* Wolpert, David H. \"Stacked generalization.\" Neural networks 5.2 (1992): 241-259."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}