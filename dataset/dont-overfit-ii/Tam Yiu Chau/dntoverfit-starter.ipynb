{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Code for beginners to easily start participating in this competition"},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/300-probed-aucs-from-dont-overfit-ii\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[link](http://google.com)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport random\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression,RidgeClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prob_auc_df = pd.read_csv(\"../input/300-probed-aucs-from-dont-overfit-ii/probed_aucs.csv\")\nprob_auc_df.loc[:,\"variable\"]=prob_auc_df.loc[:,\"variable\"].transform(lambda x: x[1:]).astype(int)\nprob_auc_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prob_auc_df.sort_values(\"variable\",inplace=True)\nprob_auc_df.reset_index(drop=True,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prob_auc_df.loc[:,\"public_auc\"] = prob_auc_df.loc[:,\"public_auc\"]-0.5\nprob_auc_df.loc[:,\"public_auc\"]=prob_auc_df.loc[:,\"public_auc\"].transform(lambda x: x if np.abs(x)>0.04 else 0)\nprob_auc_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = np.array(prob_auc_df.loc[:,\"public_auc\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/dont-overfit-ii/train.csv\").drop('id', axis=1)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv('../input/dont-overfit-ii/test.csv').drop('id', axis = 1)\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.special import expit\nprob_df = test_df.copy()\nprob_df.loc[:,\"target\"]= expit(np.dot(np.array(prob_df),score))>0.5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prob_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_df,prob_df = prob_df,train_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's visualize difference between train and test data"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nplt.figure(figsize=(12, 12))\nimport seaborn as sns #sns.set(style=\"whitegrid\")\nsns.violinplot(data=test_df.dropna()[\"0\"],orient=\"h\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy\n#test_df = test_df.transpose()\nfor col in test_df.columns:\n    z,p= scipy.stats.normaltest(test_df[col])\n    print(p<0.005,z,p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_corr = test_df.corr().sort_values(\"0\").sort_values(\"0\",axis=1)#.sortlevel(level=0, inplace=True)#.iloc[0:20,0:20]\nx_corr.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#x_corr.sort_values(\"0\").sort_values(\"0\",axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 30))\nmask = np.zeros_like(x_corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\ncmap = sns.diverging_palette(10, 250, as_cmap=True)\nsns.heatmap(x_corr,mask=mask,robust=True, vmin=-0.05,vmax=0.05, #cmap=cmap, \n        square=True,\n        cbar_kws={\"shrink\": .5})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"plt.bar(range(2), (train_df.shape[0], test_df.shape[0])) \nplt.xticks(range(2), ('Train', 'Test'))\nplt.ylabel('Count') \nplt.show()\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nkbest = TSNE(3)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest,RFE\nfrom sklearn.feature_selection import f_regression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.svm import LinearSVC\nfrom sklearn.preprocessing import QuantileTransformer,PowerTransformer,normalize,RobustScaler\nfrom sklearn.model_selection import StratifiedShuffleSplit,LeavePOut,LeaveOneGroupOut\nrobust = RobustScaler().fit(np.concatenate((train_df.drop('target', axis=1), test_df), axis=0))\n#kbest = PowerTransformer(method='yeo-johnson', standardize=True)#SelectKBest(f_regression,24)\n#kbest.fit(test_df)\ny = train_df['target']\nX = SimpleImputer(strategy='mean').fit_transform(train_df.drop('target', axis=1))#,y)#robust.transform(\n#kbest.transform(\n#X = train_df.drop('target', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42,train_size=0.8)#\n#X_train = X_test = X\n#y_train = y_test = y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"X_ = X_train\ny_ = y_train\nX = np.concatenate([X_train]*10)\ny = np.concatenate([y_train]*10)\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nlogreg = GaussianNB()#solver='liblinear',\nlogreg.fit(X_train, y_train)\ntest_score = logreg.score(X_test, y_test)\ntest_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dir(logreg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_score = 0\nbest_std = 0\nfor penalty in ['l1']:\n    for C in [2.**(i/2) for i in range(-9*2,6*2)]:#[0.001, 0.01, 0.1, 1, 10, 100]:\n        score_ = []\n        for train_index, val_index in StratifiedShuffleSplit(n_splits=12, test_size=0.1, random_state=42).split(X, y):\n            X_train = X[train_index]\n            y_train = y[train_index]\n            X_test = X[val_index]\n            y_test = y[val_index]\n            seed = random.randint(0,2<<31)\n            np.random.seed(seed)\n            logreg = LogisticRegression(dual=False,max_iter=10**5,penalty=penalty, C=C)#, solver='liblinear')\n            logreg.fit(X_train, y_train)\n            score_ += [roc_auc_score(y_train,logreg.predict(X_train))]\n        score = np.mean(score_)\n        if score > best_score:\n                best_std = np.std(score_)\n                print(best_score,best_std)\n                best_score = score\n                best_parameters = {'C': C, 'penalty': penalty}\n                s = seed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_parameters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#best_parameters = {'C': 0.1, 'penalty': 'l1'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg_list = []\nfor train_index, val_index in StratifiedShuffleSplit(n_splits=30, test_size=0.1, random_state=42).split(X, y):\n    X_train = X[train_index]\n    y_train = y[train_index]\n    X_test = X[val_index]\n    y_test = y[val_index]\n    seed = random.randint(0,2<<31)\n    np.random.seed(seed)\n    logreg = LogisticRegression(dual=False,max_iter=10**5,**best_parameters)#solver='liblinear',\n    logreg.fit(X_train, y_train)\n    test_score = logreg.score(X_test, y_test)\n    reg_list.append(logreg)\n#flattern)weight\ndef auc_reg():\n    return sum(map(lambda i:i.score(X_test, y_test), reg_list))/len(reg_list)\ndef score_reg():\n    return sum(map(lambda i:roc_auc_score(y_test,i.predict(X_test)), reg_list))/len(reg_list)\ndef sum_reg(x):\n    return sum(map(lambda i:i.predict_proba(x), reg_list))/len(reg_list)\ntest_score = score_reg()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_prob = prob_df.drop(\"target\",axis=1)\nY_prob = prob_df[\"target\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sum(map(lambda i:roc_auc_score(Y_prob,i.predict(X_prob)), reg_list))/len(reg_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.average([i.coef_ for i in reg_list],axis = 0)\nprint(x,np.std(x),np.median(np.std([i.coef_ for i in reg_list],axis = 0)))\nlogreg.coef_ = x\nlogreg.intercept_ = np.average([i.intercept_ for i in reg_list],axis = 0)\nprint(logreg.score(X_test,y_test))\nprint(roc_auc_score(y_test,logreg.predict(X_test)))\nprint(roc_auc_score(y_train,logreg.predict(X_train)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(roc_auc_score(Y_prob,logreg.predict(X_prob)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"print(\"Best score: {:.3f} {:.3f}\".format(best_score,best_std))\nprint(\"Best parameters: {}\".format(best_parameters))\nprint(\"Best score on test data: {:.3f}\".format(test_score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('../input/dont-overfit-ii/sample_submission.csv')\nsub['target'] = sum_reg(robust.transform(test_df))[:,1]#logreg.predict_proba(test_df)[:,1]\n#sub['target'] = sum_reg(test_df)[:,1]#logreg.predict_proba(test_df)[:,1]\n#sub['target'] = logreg.predict_proba(robust.transform(test_df))#[:,1]#kbest.transform(\n#sub['target'] = logreg.predict(test_df)#[:,1]\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head(100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#logreg.score(test_df)\n#print(test_df)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}