{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Import general useful packages\nimport numpy as np\nimport pandas as pd\n\n# Import matplotlib for visualisations\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport seaborn as sns\nimport scikitplot as skplt\n\n# Import all machine learning algorithms\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport xgboost as xgb\n\n# Import other useful subpackage\nfrom sklearn import model_selection\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.model_selection import GridSearchCV , train_test_split , cross_val_score\n\n#ignore warning messages \nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Loadning trainning and testing data\ntraining_data = pd.read_csv('/kaggle/input/dont-overfit-ii/train.csv')\ntesting_data = pd.read_csv('/kaggle/input/dont-overfit-ii/test.csv')\n\n#/kaggle/input/dont-overfit-ii/train.csv\n#/kaggle/input/dont-overfit-ii/test.csv\nprint(training_data.info())\nprint(testing_data.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For training data\nprint(\"Training Data: {}\".format(training_data.shape))\nprint(\"Null values present in training data: {}\".format(training_data.isnull().values.any()))\n  \n# For testing data\nprint(\"Testing Data: {}\".format(testing_data.shape))\nprint(\"Null values present in testing data: {}\".format(testing_data.isnull().values.any()))\n\n##Its showing NULL value presents in testing data only due to Target column. So we can ignore.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#training_data.head(5)\ntraining_data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Distribution of Labels on train data\nax = sns.countplot(y=training_data['target'])\nxx=training_data['target'].value_counts()\nprint(xx)\nax.set_title('Distribution of Labels on train data')\nplt.show()\n\n##the target in data set is imbalance. target is binary and has some disbalance: 36% of samples belong to 0 class;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Univariate Analysis\ntraining_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Distribution of means of all columns\ntraining_data[training_data.columns[2:]].mean().plot('hist');\nplt.title('Distribution of means of all columns');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Correlation Check (Multicolinearity)\ncorrs = training_data.corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\ncorrs = corrs[corrs['level_0'] != corrs['level_1']]\ncorrs.tail(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We can see that correlations between features are lower that 0.3 and the most correlated feature with target \n#has correlation of 0.33. So we have no highly correlated features which we could drop, \n#on the other hand we could drop some columns with have little correlation with the target.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#To split the Features and Target values of both Training and Test dataset\n# Get X and y for training data\ny_train = training_data['target']\nX_train = training_data.drop(columns = ['target', 'id'])\n\n# Get X and y for testing data\n#y_test = testing_data['target'] # it is blank, but have to remove from X_test\nX_test = testing_data.drop(columns = ['id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#K-fold stratified cross validation on Dataset1 to check over fitting...\nn_fold = 10\nfolds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\nrepeated_folds = RepeatedStratifiedKFold(n_splits=20, n_repeats=20, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Standardize features by removing the mean and scaling to unit variance\nscaler=StandardScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.fit_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cross Validation to reduce overfitting of the data\n#Appling Logistic regression for binary classification\n#for small samples set, So better to limited yourself to linear models.\n\nrandom_state = 42\nlr_clf = LogisticRegression(random_state = random_state, solver='liblinear', max_iter=1000)\nparam_grid = {'class_weight' : ['balanced', None], \n                'penalty' : ['l2','l1'],  \n                'C' : [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\n#Find best hyperparameters using GridSearchCV (roc_auc)\ngrid_lr = GridSearchCV(estimator = lr_clf, param_grid = param_grid , cv=folds, scoring = 'roc_auc', verbose = 1, n_jobs = -1)\ngrid_lr.fit(X_train,y_train)\n\nprint(\"Best Score:\" + str(grid_lr.best_score_))\nprint(\"Best Parameters: \" + str(grid_lr.best_params_))\n\n#model with best parameters\nmodel_lr=LogisticRegression(class_weight='balanced', solver='liblinear', penalty ='l1', C= 0.1, max_iter=10000)\nmodel_lr.fit(X_train,y_train)\n# To Cross validate and remodel it with less features\n# cv - number of runs to find cross validated model\n\n#Score of each cross validation score\nscores_lr = model_selection.cross_val_score(model_lr,X_train,y_train,scoring=\"roc_auc\",cv=10)\nprint(scores_lr) # moreover consistance score for flods\nscores_lr = np.mean(scores_lr)\nprint('Mean cv Score',scores_lr)\nprint(\"Score on training data: \" + str(model_lr.score(X_train,y_train)*100) + \"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tring to apply various classification algorithmns to check wheather gerring consistance CV scores\nfrom sklearn.svm import SVC\nsvc = SVC(probability=True, gamma='scale')\n\nparameter_grid = {'C': [0.001, 0.01, 0.1, 1.0, 10.0],\n                  'kernel': ['linear', 'poly', 'rbf'],\n                 }\n\ngrid_search_svc = GridSearchCV(svc, param_grid=parameter_grid, cv=folds, scoring='roc_auc', n_jobs=-1)\ngrid_search_svc.fit(X_train, y_train)\nprint('Best score: {}'.format(grid_search_svc.best_score_))\nprint('Best parameters: {}'.format(grid_search_svc.best_params_))\n\nmodel_svc = SVC(probability=True, gamma='scale', **grid_search_svc.best_params_)\nmodel_svc.fit(X_train,y_train)\n\n#Cross Validation to reduce overfitting of the data\nscores_svc = model_selection.cross_val_score(model_svc,X_train,y_train,scoring=\"roc_auc\",cv=10)\nprint(scores_svc)   #less consistance than LR\nscores_svc = np.mean(scores_svc)\nprint('Mean cv Score',scores_svc)\nprint(\"Score on training data: \" + str(model_svc.score(X_train,y_train)*100) + \"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets check another one\nfrom sklearn.neighbors import KNeighborsClassifier\nknc = KNeighborsClassifier()\n\nparameter_grid = {'n_neighbors': [2, 3, 5, 10, 20],\n                  'weights': ['uniform', 'distance'],\n                  'leaf_size': [5, 10, 30]\n                 }\n\ngrid_search_knc = GridSearchCV(knc, param_grid=parameter_grid, cv=folds, scoring='roc_auc', n_jobs=-1)\ngrid_search_knc.fit(X_train, y_train)\nprint('Best score: {}'.format(grid_search_knc.best_score_))\nprint('Best parameters: {}'.format(grid_search_knc.best_params_))\n\nmodel_knn = KNeighborsClassifier(**grid_search_knc.best_params_)\nmodel_knn.fit(X_train,y_train)\n\n#Cross Validation to reduce overfitting of the data\nscores_knn = model_selection.cross_val_score(model_knn,X_train,y_train,scoring=\"roc_auc\",cv=10)\nprint(scores_knn) #scores are less compare to above two algorithms\nscores_knn = np.mean(scores_knn)\nprint('Mean cv Score',scores_knn)\nprint(\"Score on training data: \" + str(model_knn.score(X_train,y_train)*100) + \"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## for small samples set, using non linear models that RF, GBM or XGB, thats not gonna work. \n## But still appling ensemble learning algorithm (Bagging and Boosting)\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\n\nparameter_grid = {'n_estimators': [10, 50, 100, 1000],\n                  'max_depth': [None, 3, 5, 15]\n                 }\n\ngrid_rf = GridSearchCV(rfc, param_grid=parameter_grid, cv=folds, scoring='roc_auc', n_jobs=-1)\ngrid_rf.fit(X_train, y_train)\nprint('Best score: {}'.format(grid_rf.best_score_))\nprint('Best parameters: {}'.format(grid_rf.best_params_))\n\nmodel_rfc = RandomForestClassifier(**grid_rf.best_params_)\nmodel_rfc.fit(X_train,y_train)\n\n#Cross Validation to reduce overfitting of the data\nscores_rf = model_selection.cross_val_score(model_rfc,X_train,y_train,scoring=\"roc_auc\",cv=10)\nprint(scores_rf)\nscores_rf = np.mean(scores_rf)\nprint('Mean cv Score',scores_rf)\nprint(\"Score on training data: \" + str(model_rfc.score(X_train,y_train)*100) + \"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nabc = AdaBoostClassifier()\n\nparameter_grid = {'n_estimators': [5, 10, 20, 50, 100],\n                  'learning_rate': [0.001, 0.01, 0.1, 1.0, 10.0]\n                 }\n\ngrid_search_abc = GridSearchCV(abc, param_grid=parameter_grid, cv=folds, scoring='roc_auc')\ngrid_search_abc.fit(X_train, y_train)\nprint('Best score: {}'.format(grid_search_abc.best_score_))\nprint('Best parameters: {}'.format(grid_search_abc.best_params_))\n\nmodel_abc = AdaBoostClassifier(**grid_search_abc.best_params_)\nmodel_abc.fit(X_train,y_train)\n\n#Cross Validation to reduce overfitting of the data\nscores_abc = model_selection.cross_val_score(model_abc,X_train,y_train,scoring=\"roc_auc\",cv=10)   \nprint(scores_abc)\nscores_abc = np.mean(scores_abc)\nprint('Mean cv Score',scores_abc)\nprint(\"Score on training data: \" + str(model_abc.score(X_train,y_train)*100) + \"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## CV_Score of various algorithms (Bar plot)\ndf = pd.DataFrame({'Algorithm':['LR','SVC','KNC','RFC', 'ABC'],\n                   'Score':[scores_lr,scores_svc,scores_knn,scores_rf,scores_abc]})\nprint(df)\nCV_Scores  = df['Score']\ncolors = cm.rainbow(np.linspace(0, 2, 9))\n#labels = ['LogisticRegression','SVC','KNeighborsClassifier','RandomForestClassifier', 'AdaBoostClassifier']\nlabels = df['Algorithm']\n\nplt.bar(labels,\n        CV_Scores,\n        color = colors)\nplt.xlabel('Classifiers')\nplt.ylabel('CV_Scores')\nplt.title('CV_Score of various algorithms')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## CV_Score of various algorithms (Box-Plot)\nplt.figure(figsize=(10, 5));\nscores_df1 = pd.DataFrame({'LogisticRegression': [scores_lr]})\nscores_df2 = pd.DataFrame({'AdaBoostClassifier': [scores_abc]})\nscores_df3 = pd.DataFrame({'SVC': [scores_svc]})\nscores_df4 = pd.DataFrame({'KNeighborsClassifier': [scores_knn]})\nscores_df5 = pd.DataFrame({'RandomForestClassifier': [scores_rf]})\ndf = scores_df1.append([scores_df2, scores_df3, scores_df4, scores_df5],  ignore_index=True)\nsns.boxplot(data=df);\nplt.xticks(rotation=25);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We can see that logistic regression is superior to most other models. \n#It seems that other models either overfit or can't work on this small dataset.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Furure scope::\n#  1. can check with other algorithms like Naive Bayes, extratree classifier, SGDClassifier etc  and compare CV scores\n#  2. feature selection (variable reduction) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Now I am going to split trainning data set (250 observations) by 80/20 set \n#and training model on 80 % randon sample and predicting on 20% sample to check model accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Spliting data (80/20)\ny = training_data['target']\nX = training_data.drop(columns = ['target', 'id'])\nfrom sklearn.model_selection import train_test_split\nX_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X, y, test_size = 0.2, random_state = 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying various Classification algorithms without doing variable reductions\naccuracy_scores = np.zeros(6)\n\n# Support Vector Classifier\nclf_svc = SVC().fit(X_train_new, y_train_new)\nprediction1 = clf_svc.predict(X_test_new)\naccuracy_scores[0] = accuracy_score(y_test_new, prediction1)*100\nprint('Support Vector Classifier accuracy: {}%'.format(accuracy_scores[0]))\n\n# Logistic Regression\nclf_lr = LogisticRegression(class_weight = 'balanced').fit(X_train_new, y_train_new)\nprediction2 = clf_lr.predict(X_test_new)\naccuracy_scores[1] = accuracy_score(y_test_new, prediction2)*100\nprint('Logistic Regression accuracy: {}%'.format(accuracy_scores[1]))\n\n# K Nearest Neighbors\nclf_knn = KNeighborsClassifier( ).fit(X_train_new, y_train_new)\nprediction3 = clf_knn.predict(X_test_new)\naccuracy_scores[2] = accuracy_score(y_test_new, prediction3)*100\nprint('K Nearest Neighbors Classifier accuracy: {}%'.format(accuracy_scores[2]))\n\n# Random Forest\nclf_rf = RandomForestClassifier(class_weight = 'balanced').fit(X_train_new, y_train_new)\nprediction4 = clf_rf.predict(X_test_new)\naccuracy_scores[3] = accuracy_score(y_test_new, prediction4)*100\nprint('Random Forest Classifier accuracy: {}%'.format(accuracy_scores[3]))\n\n# Gradient Boosting\nclf_gb = GradientBoostingClassifier().fit(X_train_new, y_train_new)\nprediction5 = clf_gb.predict(X_test_new)\naccuracy_scores[4] = accuracy_score(y_test_new, prediction5)*100\nprint('Gradient Boosting Classifier accuracy: {}%'.format(accuracy_scores[4]))\n\n#XGBoosting\nxgb_model = xgb.XGBClassifier() # 160/90 = 1.88\nxgb_model.fit(X_train_new, y_train_new)\nprediction6 = xgb_model.predict(X_test_new)\naccuracy_scores[5] = accuracy_score(y_test_new, prediction6)*100\nprint('XGBoost Classifier accuracy: {}%'.format(accuracy_scores[5]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Here accuracy is higher for Gradient Boosting and XGB classifier \n#but based on bellow confusion matrix, Misclassification error is less for Logistic Regression\n# So, Finally selecting Logistic Regression as a final model and going to use LR for prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion Matrix for above all models\nfrom sklearn.metrics import confusion_matrix\nconf1 = confusion_matrix(y_test_new, prediction1)\nprint(conf1)\nconf2 = confusion_matrix(y_test_new, prediction2)\nprint(conf2)\nconf3 = confusion_matrix(y_test_new, prediction3)\nprint(conf3)\nconf4 = confusion_matrix(y_test_new, prediction4)\nprint(conf4)\nconf5 = confusion_matrix(y_test_new, prediction5)\nprint(conf5)\nconf6 = confusion_matrix(y_test_new, prediction6)\nprint(conf6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check validation statistics (Classification Summary)\nprint(classification_report(y_test_new, prediction2)) # from confusion matrix Logistic perform well without variable reduction\n# Plot confusion Matrix\nskplt.metrics.plot_confusion_matrix(y_test_new, prediction2, figsize=(10, 8))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ROC Curves\ny_probas = clf_lr.predict_proba(X_test_new)\nskplt.metrics.plot_roc(y_test_new, y_probas, figsize=(10, 8))   # Plot ROC Curve\nplt.show()\n\n## test sample is too small so ROC curve is little bit as expected.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Prediction on test data\nprediction = model_lr.predict(X_test)\nprint(prediction)\nprediction_prob = model_lr.predict_proba(X_test)\n#print(prediction_prob)\n\npredictions_df = pd.DataFrame(prediction)\npredictions_df.rename(columns={0:'target'}, inplace=True)\nresult = pd.concat([testing_data['id'], predictions_df], axis=1)\nprint(result)\n\npredictions_df_prob = pd.DataFrame(prediction_prob)\npredictions_df_prob.rename(columns={0:'Probability for 0', 1: 'Probability for 1'}, inplace=True)\nprint(predictions_df_prob)\nresult3 = pd.concat([testing_data['id'], predictions_df_prob['Probability for 1']], axis=1)\nresult3.rename(columns={'Probability for 1':'target'}, inplace=True)\nprint(result3)\nresult.to_csv('sample submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## End of code","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}