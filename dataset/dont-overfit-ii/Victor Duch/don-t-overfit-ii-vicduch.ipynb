{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.decomposition import PCA\nfrom sklearn import linear_model\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom matplotlib import pyplot as plt\nfrom sklearn import datasets, linear_model\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/dont-overfit-ii/test.csv')\ntrain = pd.read_csv('/kaggle/input/dont-overfit-ii/train.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ici nous donnons à nos tableaux de données un nom, \"test\" pour l'echantillon de test et \"train\" pour l'échantillon d'entrainement","metadata":{}},{"cell_type":"code","source":"x_train = train.drop([\"target\",\"id\"], axis = 1)\ny_train = train[\"target\"]\nx_test = test.drop([\"id\"], axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nous associons à la variable x_train/x_test le tableau de données d'entrainement/test en enlevant les id et les données cibles.\nNous associons à la variable y_train/y_test les données cibles \"target\" d'entrainement/test","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Les données sont centrées et réduites","metadata":{}},{"cell_type":"code","source":"pca = PCA(n_components = 75)\npca.fit(x_train)\nx_train = pd.DataFrame(pca.transform(x_train))\nx_test = pd.DataFrame(pca.transform(x_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ici nous appliquons la PCA à notre ensemble de données d'entrainement. On peut choisir la dimension que l'on souhaite atteindre grace à (n_components = K) ou bien on peut simplement préciser la variance que l'on souhaite conserver avec notre réduction. (ici 60% de variance expliquée -> 0.6 ou 75 colonnes -> n_components = 75)","metadata":{}},{"cell_type":"code","source":"ridge = linear_model.Ridge()\nlasso = linear_model.Lasso()\nelastic = linear_model.ElasticNet()\nlasso_lars = linear_model.LassoLars()\nbayesian_ridge = linear_model.BayesianRidge()\nlogreg = linear_model.LogisticRegression(solver='liblinear')\nsgd = linear_model.SGDClassifier()\nknc = KNeighborsClassifier(n_neighbors=20)\nforest_reg = RandomForestRegressor(n_estimators = 10, random_state = 0)\nmodels = [ridge, lasso, elastic, lasso_lars, bayesian_ridge, logreg, sgd, knc,forest_reg]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ici nous déclarons tous les modèles ","metadata":{}},{"cell_type":"code","source":"def cv_scores(model):\n    scores = cross_val_score(model, x_train, y_train, cv=5, scoring='roc_auc')\n    print('Moyenne des scores: ', np.mean(scores))\n    print('Ecart-type des scores: ', np.std(scores))\n    print('\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On crée une fonction qui va faire tourner tous les modèles un à un avec validation croisée (10 groupes), et qui va nous montrer la moyenne des scores et l'ecart type des scores de chaque modèle (inspiré du code d'un autre participant)","metadata":{}},{"cell_type":"code","source":"for model in models:\n    print(model)\n    cv_scores(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Après avoir fait tourner la fonction, on remarque que le modèle ls plus performant est la regression logistique (moyenne des scores la plus élevée)","metadata":{}},{"cell_type":"markdown","source":"Maintenant nous allons chercher les paramètres qui permettent de maximiser les performances de la regression logistique ","metadata":{}},{"cell_type":"code","source":"penalite = ['l1', 'l2']\nC = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\nclass_weight = [{1:0.5, 0:0.5}, {1:0.4, 0:0.6}, {1:0.6, 0:0.4}, {1:0.7, 0:0.3}]\nsolver = ['liblinear', 'saga']\n\nparam_grid = dict(penalty=penalite,\n                  C=C,\n                  class_weight=class_weight,\n                  solver=solver)\n\ngrid = GridSearchCV(estimator=logreg, param_grid=param_grid, scoring='roc_auc', verbose=1, n_jobs=-1)\ngrid_result = grid.fit(x_train, y_train)\n\nprint('Meilleur Score: ', grid_result.best_score_)\nprint('Meilleurs Paramètres: ', grid_result.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ici on a obtenu les meilleurs paramètres pour la regression logistique","metadata":{}},{"cell_type":"code","source":"logreg = linear_model.LogisticRegression(C=0.001, class_weight={1:0.4, 0:0.6}, penalty='l2', solver='liblinear')\ncv_scores(logreg)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test = logreg.fit(x_train, y_train).predict_proba(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}