{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Problem objective\n\n### To fit a model to the data given such that it does not overfit. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35263e74ae0a0472bad8c617826d69c669f30ab4"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04a94dd7c8d3bf405b00d6ba0a6b79b3ef71a02c"},"cell_type":"code","source":"df[\"target\"].value_counts(), df.info(), df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df[\"target\"]\nX = df.drop([\"target\", \"id\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preliminary hypothesis \n* Fit a base classification model without any feature engineering. Choosing logistic regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"def logisticRegression(X,y):\n    train_x, test_x, train_y, test_y = train_test_split(X, y, test_size = 0.2, random_state = 42)\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.metrics import accuracy_score, log_loss\n    lrModel = LogisticRegression()\n    lrModel.fit(train_x, train_y)\n    print(accuracy_score(lrModel.predict(test_x), test_y))\n    print(log_loss(lrModel.predict(test_x), test_y))\n    return lrModel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logisticRegression(X,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Feature engineering for figuring out the kind of data available to us and building pipeline for it."},{"metadata":{"trusted":true},"cell_type":"code","source":"y.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Higly imbalanced data. Applying SMOTE to even out the target data."},{"metadata":{"trusted":true,"_uuid":"0fc0d9126d3ce006952f212c798c6d1e377b1a45"},"cell_type":"code","source":"# from imblearn.over_sampling import SMOTE\n# sm = SMOTE(random_state=42, k_neighbors=3, n_jobs = 5)\n# X,y = sm.fit_resample(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b8a26fa765fa2f802a8880f6980f25cea885a187"},"cell_type":"code","source":"logisticRegression(X,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Significant improvement from just using SMOTE on our highly imbalanced target. We will try to apply other set of normalizations based on data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pd.DataFrame(X)\nX.std().plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85a8d749ed6dc2cd325a353a87443cb35c83de14"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nX = ss.fit_transform(X)\nlrModel = logisticRegression(X,y)\ntest_df_pred = lrModel.predict_proba(test_df.iloc[:,1:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pd.DataFrame(X)\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit_df = pd.read_csv(\"../input/sample_submission.csv\")\nsubmit_df.head()\nsubmit_df[\"target\"] = test_df_pred\n# submit_df.to_csv(\"lrModel.csv\", index = False) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### LB score 0.504. Let's create another base model with the same X and y. "},{"metadata":{"trusted":true,"_uuid":"548d33c1376a6fa144a8fd563ac2849a4a08a17a"},"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\n\nclassifiers = [\n    KNeighborsClassifier(3),\n    SVC(probability=True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    GaussianNB(),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis(),\n    LogisticRegression(),\n    XGBClassifier()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d2d26bfaa2f663ecc2adba88062189bad4c2c619"},"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4ebbad581a13224e4780b7140cd71afcff8f930"},"cell_type":"code","source":"for clf in classifiers:\n    name = clf.__class__.__name__\n    clf.fit(X_train, y_train)\n    acc = clf.score(X_test,y_test)\n    print(\"{0}: {1}\".format(name,acc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's apply QuadraticDiscriminantAnalysis and figure out our LB score. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def applyModel(model, X,y):\n    train_x, test_x, train_y, test_y = train_test_split(X, y, test_size = 0.2, random_state = 42)\n    from sklearn.metrics import accuracy_score, log_loss\n    model.fit(train_x, train_y)\n#     print(accuracy_score(model.predict(test_x), test_y))\n#     print(log_loss(model.predict(test_x), test_y))\n    return (accuracy_score(model.predict(test_x), test_y), log_loss(model.predict(test_x), test_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"qdaModel = QuadraticDiscriminantAnalysis()\napplyModel(qdaModel, X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit_df = pd.read_csv(\"../input/sample_submission.csv\")\nsubmit_df.head()\ntest_df_pred = qdaModel.predict_proba(test_df.iloc[:,1:])\nsubmit_df[\"target\"] = test_df_pred\n# submit_df.to_csv(\"qdaModel Base 2.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Worse off compared to our logistic regression with 0.500 score. "},{"metadata":{},"cell_type":"markdown","source":"#### RFE did not work well with the data. Other feature extraction techniques that can be tried are PCA, SelectKBest, ICA, LLE. Let's create a function to iteratively try all the techniques"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\nStratifiedShuffleSplit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def featureExtractionAndModel(stat_model, feature_ext_model, X, y):\n    if(clf.__class__.__name__ == \"LinearDiscriminantAnalysis\") or (clf.__class__.__name__ == \"RFE\") or (clf.__class__.__name__ == \"SelectKBest\"):\n        train_x, test_x, train_y, test_y = train_test_split(feature_ext_model.fit_transform(X, y), y, test_size = 0.2, random_state = 42)\n    else:\n        train_x, test_x, train_y, test_y = train_test_split(feature_ext_model.fit_transform(X), y, test_size = 0.2, random_state = 42)\n    from sklearn.metrics import accuracy_score, log_loss\n    stat_model.fit(train_x, train_y)\n#     print(accuracy_score(model.predict(test_x), test_y))\n#     print(log_loss(model.predict(test_x), test_y))\n    \n    return (accuracy_score(stat_model.predict(test_x), test_y), log_loss(stat_model.predict(test_x), test_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lrModel_FE = LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import LocallyLinearEmbedding\nfrom sklearn.manifold import SpectralEmbedding\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.decomposition import FastICA\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.feature_selection import RFE\nfrom sklearn.feature_selection import SelectKBest\n\nextractors = [\n    LocallyLinearEmbedding(n_components = 150),\n    SpectralEmbedding(n_components = 150),\n    PCA(n_components=150,svd_solver='full'),\n    LinearDiscriminantAnalysis(n_components = 150),\n    FastICA(n_components = 150),\n    TSNE(n_components = 3),\n    RFE(lrModel_FE, n_features_to_select = 100),\n    SelectKBest(k = 150)\n    ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for clf in extractors:\n    name = clf.__class__.__name__\n#     print(name)\n    acc = featureExtractionAndModel(lrModel_FE, clf, X, y)\n    print(\"{0}: {1:.4f} {2:.4f}\".format(name,acc[0], acc[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### RFE and SelectKBest seem promising let's delve into it further"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_lst = []\nlog_loss_lst = []\nfor i in range(25, 150, 5):\n    acc = applyModel(lrModel_FE, RFE(lrModel_FE, n_features_to_select = i).fit_transform(X, y), y)\n#     print(\"{0}: {1:.4f} {2:.4f}\".format(i, acc[0], acc[1]))\n    acc_lst.append(acc[0])\n    log_loss_lst.append(acc[1])\nplt.plot(range(25, 150, 5), acc_lst)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_lst = []\nlog_loss_lst = []\nfor i in range(155, 250, 1):\n    acc = applyModel(lrModel_FE, SelectKBest(k = i).fit_transform(X, y), y)\n    print(\"{0}: {1:.4f} {2:.4f}\".format(i, acc[0], acc[1]))\n    acc_lst.append(acc[0])\n    log_loss_lst.append(acc[1])\nplt.plot(range(155, 250, 1), acc_lst)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfe = RFE(lrModel_FE, n_features_to_select = 50)\napplyModel(lrModel_FE, rfe.fit_transform(X, y), y)\nsubmit_df = pd.read_csv(\"../input/sample_submission.csv\")\nsubmit_df.head()\ntest_df_pred = lrModel_FE.predict(rfe.transform(test_df.iloc[:,1:]))\nsubmit_df[\"target\"] = test_df_pred\n# submit_df.to_csv(\"lrModel RFE.csv\", index = False) #0.519","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kbest = SelectKBest(k = 173)\napplyModel(lrModel_FE, kbest.fit_transform(X, y), y)\nsubmit_df = pd.read_csv(\"../input/sample_submission.csv\")\nsubmit_df.head()\ntest_df_pred = lrModel_FE.predict(kbest.transform(test_df.iloc[:,1:]))\nsubmit_df[\"target\"] = test_df_pred\n# submit_df.to_csv(\"lrModel SelectKBest.csv\", index = False) 0.500","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_lst = []\nlog_loss_lst = []\nfor i in range(70, 299, 1):\n    acc = applyModel(tuned_lr_model, PCA(n_components= i).fit_transform(X), y)\n    print(\"{0}: {1:.4f} {2:.4f}\".format(i, acc[0], acc[1]))\n    acc_lst.append(acc[0])\n    log_loss_lst.append(acc[1])\nplt.plot(range(70, 299, 1), acc_lst)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tuned_lr_model = LogisticRegression(C=2.4, class_weight=None, \n                                                      dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=8, multi_class='warn',\n          n_jobs=None, penalty='l2', random_state=42, solver='warn',\n          tol=0.0001, warm_start=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components = 73)\napplyModel(tuned_lr_model, pca.fit_transform(X), y)\nsubmit_df = pd.read_csv(\"../input/sample_submission.csv\")\nsubmit_df[\"target\"] = tuned_lr_model.predict(pca.transform(test_df.iloc[:,1:]))\n# submit_df.to_csv(\"lrModel tuned and PCA.csv\", index = False) #0.499","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* #### Below code is for practice. check out original by [Andrew Lukyanenko](https://www.kaggle.com/artgor/how-to-not-overfit/notebook)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import eli5\nweights_df = eli5.formatters.as_dataframe.explain_weights_df(tuned_lr_model, top=200)\nweights_df[\"weight\"] = weights_df[\"weight\"].abs()\nweights_df = weights_df.sort_values(by=\"weight\", ascending=False)\nweights_df.iloc[:20,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_features = [int(feature[1:]) for feature in weights_df[\"feature\"] if feature!=\"<BIAS>\"][:10]\n# X.loc[:, best_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"acc_lst = []\nlog_loss_lst = []\nfor i in range(5, 150, 1):\n    best_features = [int(feature[1:]) for feature in weights_df[\"feature\"] if feature!=\"<BIAS>\"][:i]\n    acc = applyModel(lrModel_FE, X.loc[:, best_features], y)\n    print(\"{0}: {1:.4f} {2:.4f}\".format(i, acc[0], acc[1]))\n    acc_lst.append(acc[0])\n    log_loss_lst.append(acc[1])\nplt.plot(range(5, 150, 1), acc_lst)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_features = [int(feature[1:]) for feature in weights_df[\"feature\"] if feature!=\"<BIAS>\"][:10]\napplyModel(tuned_lr_model, X.loc[:, best_features], y)\nsubmit_df = pd.read_csv(\"../input/sample_submission.csv\")\nsubmit_df[\"target\"] = tuned_lr_model.predict(test_df.iloc[:,best_features])\nsubmit_df.to_csv(\"lrModel tuned and ELI5.csv\", index = False) #0.499","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_features = [int(feature[1:]) for feature in weights_df[\"feature\"] if feature!=\"<BIAS>\"][:10]\napplyModel(lrModel_FE, X.loc[:, best_features], y)\nsubmit_df = pd.read_csv(\"../input/sample_submission.csv\")\nsubmit_df[\"target\"] = lrModel_FE.predict(test_df.iloc[:,best_features])\nsubmit_df.to_csv(\"lrModel_FE and ELI5.csv\", index = False) #0.499","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1211de9408491b46abbf012b2da21b91f77ad2f3","trusted":true},"cell_type":"code","source":"### Lets create a model on GaussianNB and use it on test data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d4e5751ebb33a327bf65deb4850372f5f576973"},"cell_type":"code","source":"# gnb = GaussianNB()\n# gnb.fit(X,y)\n# test_df_pred = pd.DataFrame({\"target\": gnb.predict_proba(test_df.iloc[:,1:])})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74d1ddbd2a95adcb6993c5a4a40bbab1028241b0"},"cell_type":"code","source":"# pd.read_csv(\"../input/sample_submission.csv\").head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"54b6dab17865c3d80a67a3900ef8eb59efed1464"},"cell_type":"code","source":"# test_df_pred[\"id\"] = test_df[\"id\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5746198a08238363bf8cffe05a26502d192c7ddb"},"cell_type":"code","source":"# test_df_pred.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47706405049b9f783384abf001015c6ee19bf6d0"},"cell_type":"code","source":"# test_df_pred.to_csv(\"Submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"674c7d4867fc7c7efdd69bae494bc07636e8597a"},"cell_type":"markdown","source":"### So we have 0.611 as our score. We can improve this with a bit of a feature engineering.  Lets apply decomposition on the dataset and see what happens."},{"metadata":{"trusted":true,"_uuid":"01da944c0eea1ed06164117b20d6dfcdd5cc7474"},"cell_type":"code","source":"# import matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0377193956c19d169ceba4f4f9d426d8607588c"},"cell_type":"code","source":"# from sklearn.decomposition import PCA\n# pca = PCA().fit(X)\n# plt.plot(np.cumsum(pca.explained_variance_ratio_))\n# plt.xlim(0,250,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"035e6c408bb437387dfe13b6612a4f8ad1b2dd58"},"cell_type":"code","source":"# sklearn_pca = PCA(n_components=250)\n# print(sklearn_pca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5fffe55ca95d1edd1b2cc37aceba3c0fec82fa6f","scrolled":true},"cell_type":"code","source":"# X_train_pca = sklearn_pca.fit_transform(X_train)\n# print(X_train_pca.shape)\n\n# X_test_pca = sklearn_pca.transform(X_test)\n# print(X_test_pca.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"48de6e33696f2be8d5c8001e1555fc18ba96ef02"},"cell_type":"code","source":"# classifiers = [\n#     KNeighborsClassifier(3),\n#     SVC(probability=True),\n#     DecisionTreeClassifier(),\n#     RandomForestClassifier(),\n#     AdaBoostClassifier(),\n#     GradientBoostingClassifier(),\n#     GaussianNB(),\n#     LinearDiscriminantAnalysis(),\n#     QuadraticDiscriminantAnalysis(),\n#     LogisticRegression(),\n#     XGBClassifier()]\n# for clf in classifiers:\n#     name = clf.__class__.__name__\n#     clf.fit(X_train_pca, y_train)\n#     acc = clf.score(X_test_pca,y_test)\n#     print(\"{0}: {1}\".format(name,acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"930eb5d36a85394b5d6e012179fdaca4aba0a348"},"cell_type":"code","source":"# sklearn_pca = PCA(n_components=200)\n# print(sklearn_pca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9460822e4cc2ad62dab9b326ccfc1db53788933"},"cell_type":"code","source":"# X_pca = sklearn_pca.fit_transform(X)\n# print(X_pca.shape)\n\n# test_pca = sklearn_pca.transform(test_df.iloc[:,1:])\n# print(test_pca.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b534d284c65b88fefab2a23996905cb91bb4af98"},"cell_type":"code","source":"# qda = QuadraticDiscriminantAnalysis()\n# qda.fit(X_pca,y)\n# test_df_pred = pd.DataFrame({\"target\": qda.predict_proba(test_pca)})\n# test_df_pred[\"id\"] = test_df[\"id\"]\n# test_df_pred.to_csv(\"Submission with pca and qda.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3003479504429296a2b1efdb3c0dcfe3b15ed0fe"},"cell_type":"code","source":"# qda = QuadraticDiscriminantAnalysis()\n# qda.fit(X,y)\n# test_df_pred = pd.DataFrame({\"target\": qda.predict_proba(test_df)})\n# test_df_pred[\"id\"] = test_df[\"id\"]\n# test_df_pred.to_csv(\"Submission with qda.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cf7377bb14f5148ebe1782074365485efe97b416"},"cell_type":"markdown","source":"## GridSearchCV on QDA"},{"metadata":{"trusted":true,"_uuid":"bb5027772477a5f5df8bc9b8198dd78c944a991b"},"cell_type":"code","source":"# from sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10e0798195a8d1e11effe386df0d1ec8cbcaa2d3"},"cell_type":"code","source":"# xgb_tuning = XGBClassifier(learning_rate =0.1,\n#  n_estimators=1000,\n#  max_depth=5,\n#  min_child_weight=1,\n#  gamma=0,\n#  subsample=0.8,\n#  colsample_bytree=0.8,\n#  objective= 'binary:logistic',\n#  nthread=4,\n#  scale_pos_weight=1,\n#  seed=27,\n# n_gpus=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"45c725c0b40671d9de7dfd1edb6fb7036ee37bc3"},"cell_type":"code","source":"# xgb_tuning.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4757f216b84f75913e10d08bbdac52b9cb1ab5ef"},"cell_type":"code","source":"# xgb_tuning.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9cfc64eaa625a0862d38373c100d392805bf5642"},"cell_type":"code","source":"# param_test1 = {\n#  'max_depth':[7,8,9,10],\n#  'min_child_weight':[0,1,2]\n# }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"047574a33b65dbdcc57c51c2469a2cc7762a92c3"},"cell_type":"code","source":"# gsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,\n#                                                  min_child_weight=1, gamma=0, \n#                                                   subsample=0.8, colsample_bytree=0.8,\n#                                                   objective= 'binary:logistic',\n#                                                   nthread=32, scale_pos_weight=1, seed=27,n_gpus=1), \n#                         param_grid = param_test1, scoring='roc_auc',n_jobs=32,iid=False, cv=5,verbose=4)\n# gsearch1.fit(X_train,y_train)\n# gsearch1.best_params_, gsearch1.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"23d5747ae8d048d83f7837373da58802e00b4f34"},"cell_type":"code","source":"# xgboost_params = { \"n_estimators\": 400, 'tree_method':'gpu_hist', 'predictor':'gpu_predictor' }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"304cc2c5fadf57ef25568f79167f430bf0961aec","scrolled":true},"cell_type":"code","source":"# param_test3 = {\n#  'gamma':[i/10.0 for i in range(0,5)]\n# }\n# gsearch3 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1, max_depth=9,\n#  min_child_weight=0, gamma=0, subsample=0.8, colsample_bytree=0.8,\n#  objective= 'binary:logistic', nthread=32, scale_pos_weight=1,seed=27,n_gpus=1,**xgboost_params), \n#  param_grid = param_test3, scoring='roc_auc',n_jobs=32,iid=False,verbose=4 ,cv=5)\n# gsearch3.fit(X,y)\n# gsearch3.best_params_, gsearch3.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d57f9d06270b32df509c757adafaa03dc397aa78"},"cell_type":"code","source":"# param_test4 = {\n#  'colsample_bytree':[i/10.0 for i in range(7,10)],\n#     'subsample': [i/10.0 for i in range(7,10)]\n# }\n# gsearch4 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1, max_depth=9,\n#  min_child_weight=0, gamma=0.0, subsample=0.8, colsample_bytree=0.8,\n#  objective= 'binary:logistic', nthread=32, scale_pos_weight=1,seed=27,n_gpus=1,**xgboost_params), \n#  param_grid = param_test4, scoring='roc_auc',n_jobs=32,iid=False,verbose=5 ,cv=5)\n# gsearch4.fit(X,y)\n# gsearch4.best_params_, gsearch4.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39cc402ed2e2982c14c836fe967d628ec4af143f"},"cell_type":"code","source":"# param_test5 = {\n#     'subsample': [i/10.0 for i in range(5,8)]\n# }\n# gsearch5 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1, max_depth=9,\n#  min_child_weight=0, gamma=0.0, subsample=0.8, colsample_bytree=0.8,\n#  objective= 'binary:logistic', nthread=32, scale_pos_weight=1,seed=27,n_gpus=1,**xgboost_params), \n#  param_grid = param_test5, scoring='roc_auc',n_jobs=32,iid=False,verbose=5 ,cv=5)\n# gsearch5.fit(X,y)\n# gsearch5.best_params_, gsearch5.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5724fcdb34c41793c355d413e5c4f49ae6c0d2a"},"cell_type":"code","source":"# param_test7 = {\n#  'reg_alpha':[i/10.0 for i in range(0,3)],\n#     'reg_lambda':[i/10.0 for i in range(9,11)]\n# }\n# gsearch7 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.11, max_depth=9,\n#  min_child_weight=0, gamma=0.0, subsample=0.7, colsample_bytree=0.8,\n#  objective= 'binary:logistic', nthread=5, scale_pos_weight=1,seed=27,n_gpus=1,**xgboost_params), \n#  param_grid = param_test7, scoring='roc_auc',n_jobs=32,iid=False,verbose=5 ,cv=5)\n# gsearch7.fit(X,y)\n# gsearch7.best_params_, gsearch7.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de0ef8fed02617f675a81a987ab6ee281bdc3f65"},"cell_type":"code","source":"# param_test8 = {\n#  'learning_rate' :[0.08,0.09,0.1,0.11,0.12]\n# }\n# gsearch8 = GridSearchCV(estimator = XGBClassifier(\n#                                 learning_rate =0.1,max_depth=9,min_child_weight=0, gamma=0.0, \n#                                 subsample=0.7, colsample_bytree=0.8,objective= 'binary:logistic', \n#                                 nthread=5,reg_alpha= 0.1, reg_lambda= 0.9, \n#                                 scale_pos_weight=1,seed=27,n_gpus=1,**xgboost_params), \n#                         param_grid = param_test8, scoring='roc_auc',\n#                         n_jobs=32,iid=False,verbose=5 ,cv=5\n#                        )\n# gsearch8.fit(X,y)\n# gsearch8.best_params_, gsearch8.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10e0798195a8d1e11effe386df0d1ec8cbcaa2d3"},"cell_type":"code","source":"# xgb_tuning = XGBClassifier(learning_rate =0.11,\n#  n_estimators=400,\n#  max_depth=9,\n#  min_child_weight=0,\n#  gamma=0.0,\n#  subsample=0.7,\n#  colsample_bytree=0.8,\n#  objective= 'binary:logistic',\n#  nthread=4,\n#  scale_pos_weight=1,\n#  seed=27,\n# reg_alpha= 0.1, reg_lambda= 0.9,\n# n_gpus=1,\n# tree_method='gpu_hist',\n# predictor='gpu_predictor')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"45c725c0b40671d9de7dfd1edb6fb7036ee37bc3"},"cell_type":"code","source":"# xgb_tuning.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4757f216b84f75913e10d08bbdac52b9cb1ab5ef"},"cell_type":"code","source":"# xgb_tuning.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a4c611c44560743ee7a969e745bb8ceff0bb553"},"cell_type":"code","source":"# X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67c8b1201e43045b52afa4f83ecf827e6f6c8195"},"cell_type":"code","source":"# test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c12547392de6935e0e9e203772e6817529d5c56"},"cell_type":"code","source":"# xgb_tuning.fit(X,y)\n# test_df_pred = pd.DataFrame({\"target\": xgb_tuning.predict_proba(np.array(test_df.drop(\"id\",axis=1)))})\n# test_df_pred[\"id\"] = test_df[\"id\"]\n# test_df_pred.to_csv(\"Submission xgb tuning.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd67a866e3fb5a244af05035db10ef14c8f060bf"},"cell_type":"markdown","source":"### XGBoostClassifier is not getting us anywhere. It needs more data to perform better. Lets try logistic regression"},{"metadata":{"trusted":true,"_uuid":"cc898df0abce1266c1131a89a69897f98d0758be"},"cell_type":"code","source":"# from sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"620297751c9d65451971954a0d2e2dadf637c9cf"},"cell_type":"code","source":"# lrcv = LogisticRegression(C=50000,penalty=\"l2\")\n# lrcv.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8efa834e0e8c51cf9b2a6cdb42a5863bc9ff44ae"},"cell_type":"code","source":"# lrcv.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4bf76a4e48360b26d9879650de152cd156ffbec"},"cell_type":"code","source":"# param1 = {\n#  'penalty':[\"l1\",\"l2\"]\n# }\n# gsearch = GridSearchCV(estimator = LogisticRegression(C=5, class_weight=None, dual=False,\n#                                                       fit_intercept=True,\n#           intercept_scaling=1, max_iter=100, multi_class='warn',\n#           n_jobs=None, penalty='l1', random_state=42, solver='warn',\n#           tol=0.0001, verbose=0, warm_start=False), \n#  param_grid = param1, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n# gsearch.fit(X,y)\n# gsearch.best_params_,gsearch.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af56de7527210a863509209d89858bdc11de8f81"},"cell_type":"code","source":"# param2 = {\n#  'C':[i/1000 if i!=0 else 1 for i in range(0,100000,10)]\n# }\n# gsearch2 = GridSearchCV(estimator = LogisticRegression(C=5, class_weight=None, dual=False,\n#                                                        fit_intercept=True,\n#           intercept_scaling=1, max_iter=100, multi_class='warn',\n#           n_jobs=None, penalty='l2', random_state=42, solver='warn',\n#           tol=0.0001, verbose=0, warm_start=False), \n#  param_grid = param2, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n# gsearch2.fit(X,y)\n# gsearch2.best_params_, gsearch2.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9240e21907188b045b81acaa2f6c245ccfbc840e"},"cell_type":"code","source":"# param3 = {\n#  'tol':[i/10000 if i!=0 else 1 for i in range(0,1000,1)]\n# }\n# gsearch3 = GridSearchCV(estimator = LogisticRegression(C=2.4, class_weight=None, \n#                                                       dual=False, fit_intercept=True,\n#           intercept_scaling=1, max_iter=100, multi_class='warn',\n#           n_jobs=None, penalty='l2', random_state=42, solver='warn',\n#           tol=0.0001, verbose=0, warm_start=False), \n#  param_grid = param3, scoring='roc_auc',verbose=4,n_jobs=32,iid=False, cv=5)\n# gsearch3.fit(X,y)\n# gsearch3.best_params_, gsearch3.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92aa28c740a78a4db489b02b61926eefebfe3edb"},"cell_type":"code","source":"# param4 = {\n#  'max_iter':[i for i in range(8,100,1)]\n# }\n# gsearch4 = GridSearchCV(estimator = LogisticRegression(C=2.4, class_weight=None, \n#                                                       dual=False, fit_intercept=True,\n#           intercept_scaling=1, max_iter=100, multi_class='warn',\n#           n_jobs=None, penalty='l2', random_state=42, solver='warn',\n#           tol=0.0001, verbose=0, warm_start=False), \n#  param_grid = param4, scoring='roc_auc',verbose=4,n_jobs=32,iid=False, cv=5)\n# gsearch4.fit(X,y)\n# gsearch4.best_params_, gsearch4.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c26ff9fe1b960ed59db0d4e1eaaf6205b4004152"},"cell_type":"code","source":"# param5 = {\n#  'class_weight':[\"balanced\",None],\n# }\n# gsearch5 = GridSearchCV(estimator = LogisticRegression(C=2.4, class_weight=None, \n#                                                       dual=False, fit_intercept=True,\n#           intercept_scaling=1, max_iter=8, multi_class='warn',\n#           n_jobs=None, penalty='l2', random_state=42, solver='warn',\n#           tol=0.0001, verbose=0, warm_start=False), \n#  param_grid = param5, scoring='roc_auc',verbose=4,n_jobs=-1,iid=False, cv=5)\n# gsearch5.fit(X,y)\n# gsearch5.best_params_, gsearch5.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"06668e8ccb5be0619ea8d269f05e3ec0fedd35d7"},"cell_type":"code","source":"# from sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"d41f9a5eecce660bb402f5156d61fc8cedc6eb2d"},"cell_type":"code","source":"# lr = LogisticRegression(C=2.4, class_weight=\"balanced\", \n#                                                       dual=False, fit_intercept=True,\n#           intercept_scaling=1, max_iter=8, multi_class='warn',\n#           n_jobs=None, penalty='l2', random_state=42, solver='warn',\n#           tol=0.0001, warm_start=False)\n# lr.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d6f6139ddf3d9c3a2ed42ff68d86ea2ac5b39e88"},"cell_type":"code","source":"# lr.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5264fa30e0c6448dbb6a273ce60fe401bdd6cab2"},"cell_type":"code","source":"# lr.fit(X,y)\n# y_pred = pd.DataFrame({\"target\":lr.predict_proba(test_df.iloc[:,1:]).flatten()}, index=test_df.index)\n# y_pred[\"id\"] = test_df[\"id\"] \n# y_pred.to_csv(\"submission with tuned logistic regression.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb5efef170b5bf840bad5edb2028686037203fa8"},"cell_type":"code","source":"# from sklearn.ensemble import RandomForestClassifier ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"03cddfe8d8b471fcb5da021b5e21dec5766e8bb2"},"cell_type":"code","source":"# rf = RandomForestClassifier(n_estimators=200, n_jobs=4, class_weight='balanced', max_depth=6)\n# rf.fit(X_train,y_train)\n# rf.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d587bbc426868de6fbbac34a4c08decb64552e3"},"cell_type":"markdown","source":"### We will try Recursive feature elimination with our lr object"},{"metadata":{"trusted":true,"_uuid":"6e8a704f5e93526888d81764aacc4ee814cedfbf"},"cell_type":"code","source":"# from sklearn.feature_selection import RFE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d1f000da3534042b07eb334291ca30e195c7c67"},"cell_type":"code","source":"# lr_rfe = RFE(lr, 75, step=1)\n# lr_rfe.fit(X_train,y_train)\n# # scores_table(selector, 'selector_clf')\n# lr_rfe.score(X_test, y_test) \n# y_pred = lr_rfe.predict_proba(test_df.iloc[:,1:])\n# s = pd.read_csv('../input/sample_submission.csv')\n# s[\"target\"] = y_pred\n# s.to_csv(\"submission with RFE.csv\", index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}