{"cells":[{"metadata":{"_uuid":"4ae92511bd0f30ad614066b1528111c12165f58f"},"cell_type":"markdown","source":""},{"metadata":{"trusted":true,"_uuid":"9623a6dc66907adbc5df9a096d79d1ec6d86aa88"},"cell_type":"code","source":"# !pip install bayesian-optimization --user\n\nimport pandas as pd , numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom bayes_opt import BayesianOptimization\nfrom sklearn.model_selection  import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom imblearn.combine import SMOTEENN\nfrom sklearn.metrics import balanced_accuracy_score\nfrom collections import Counter\n#from MulticoreTSNE import MulticoreTSNE as TSNE\nfrom IPython.display import clear_output\nimport collections, os\nimport pickle\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aca1c540d5773b1910205ec7178d185cbe505a5e"},"cell_type":"code","source":"df = pd.read_csv(\"../input/train.csv\")\ndf.shape\ntest_df = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"454edaba5c9a311d427e1e41b3ec2f384dd25185"},"cell_type":"code","source":"test_df = test_df.drop(\"id\", axis = 1)\nlabel = df[\"target\"]\nInput = df.drop(columns=['id', 'target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"245cdea9985bad1ec84fdffa9ddf4d44ca1732ac"},"cell_type":"code","source":"test_df = test_df.values.astype(np.float32)\n\nx_data = Input.values.astype(np.float32)\ny_datalabel = label\ny_data = LabelEncoder().fit_transform(label)\n\nonehot = np.zeros((y_data.shape[0], np.unique(y_data).shape[0]))\nfor i in range(y_data.shape[0]):\n    onehot[i, y_data[i]] = 1.0\nx_train, x_test, y_train, y_test, y_train_label, y_test_label = train_test_split(x_data, onehot, y_data, test_size = 0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"83f174d7cc5bd5d01ab7bf7d35d1011ed93d9351"},"cell_type":"code","source":"print(x_train.shape)\nprint(sorted(Counter(y_train_label).items()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9944476440ab02de250cb56ebdbff4fc87ae4a44"},"cell_type":"code","source":"def focal_loss_sigmoid(labels,logits,alpha=0.25 , gamma=2):\n    y_pred=tf.nn.sigmoid(logits)\n    labels=tf.to_float(labels)\n    L=-labels*(1-alpha)*((1-y_pred)*gamma)*tf.log(  tf.maximum(y_pred , 1e-14 )   )-\\\n      (1-labels)*alpha*(y_pred**gamma)*tf.log( tf.maximum( 1-y_pred ,  1e-14 ) ) \n    return L\n\ndef spectral_norm(w, iteration= 2 , name = None):\n    w_shape = w.shape.as_list()\n    w = tf.reshape(w, [-1, w_shape[-1]])\n    \n    u = tf.get_variable(name , [1, w_shape[-1]], initializer=tf.random_normal_initializer(), trainable=False)\n\n    u_hat = u\n    v_hat = None\n    for i in range(iteration):\n       \n        \"\"\"\n       power iteration\n       Usually iteration = 1 will be enough\n       \"\"\"\n        \n        v_ = tf.matmul(u_hat, tf.transpose(w))\n        v_hat = tf.nn.l2_normalize(v_)\n\n        u_ = tf.matmul(v_hat, w)\n        u_hat = tf.nn.l2_normalize(u_)\n\n    u_hat = tf.stop_gradient(u_hat)\n    v_hat = tf.stop_gradient(v_hat)\n\n    sigma = tf.matmul(tf.matmul(v_hat, w), tf.transpose(u_hat))\n\n    with tf.control_dependencies([u.assign(u_hat)]):\n        w_norm = w / sigma\n        w_norm = tf.reshape(w_norm, w_shape)\n\n    return w_norm \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"377dd794aa948bf8cf7ea2ffb899c4256cd7f7db"},"cell_type":"code","source":"l1_regularizer = tf.contrib.layers.l1_regularizer(scale=0.005, scope=None)\n\ndef neural_network(num_hidden, size_layer, learning_rate , dropout_rate , beta ,\n                   activation , focal_weight , reduction_node , batch_size = 20 ,\n                   x_train = x_train , y_train = y_train  ):\n    \n    def activate(activation, first_layer, second_layer, bias):\n        if activation == 0:\n            activation = tf.nn.leaky_relu\n#         elif activation == 1:\n#             activation = tf.nn.tanh\n        elif activation == 1:\n            activation = tf.nn.relu\n        elif activation == 2:\n            activation = tf.nn.elu\n        elif activation == 3:\n            activation = tf.nn.relu6\n        else :\n            activation = tf.nn.selu\n            \n        layer = activation(tf.matmul(first_layer, second_layer) + bias)\n        return tf.contrib.nn.alpha_dropout(layer, dropout_rate)\n\n    tf.reset_default_graph()\n    X = tf.placeholder(tf.float32, (None, x_data.shape[1]))\n    Y = tf.placeholder(tf.float32, (None, onehot.shape[1]))\n    ## W = tf.Variable(tf.contrib.layers.xavier_initializer()((n_prev, n)))\n    \n    \n    \n    input_layer = tf.Variable(tf.contrib.layers.xavier_initializer()((x_data.shape[1], size_layer)))\n    biased_layer = tf.Variable(tf.random_normal([size_layer], stddev = 0.1))\n    output_layer = tf.Variable(tf.contrib.layers.xavier_initializer()((size_layer - reduction_node * (num_hidden - 1), onehot.shape[1])))\n    #biased_output = tf.Variable(tf.random_normal([onehot.shape[1]], stddev = 0.1))\n    \n    \n    layers, biased = [], []\n    \n#     for i in range(num_hidden - 1):\n#         layers.append(tf.Variable(tf.contrib.layers.xavier_initializer()((size_layer, size_layer))))\n#         biased.append(tf.Variable(tf.random_normal([size_layer])))\n     \n    for i in range(num_hidden - 1):\n        size_layer2 = size_layer - reduction_node \n        layers.append( spectral_norm(tf.Variable(tf.contrib.layers.xavier_initializer()((size_layer, size_layer2))) , name = \"SN\" + str(i)) )        \n        biased.append(tf.Variable(tf.random_normal([size_layer2])))\n        size_layer = size_layer2\n    \n    \n    \n    first_l = activate(activation, X, input_layer, biased_layer)\n    next_l = activate(activation, first_l, layers[0], biased[0])\n    \n    for i in range(1, num_hidden - 1):\n        next_l = activate(activation, next_l, layers[i], biased[i])\n    \n    last_l = tf.matmul(next_l, output_layer) # + biased_output\n    cost2 = tf.reduce_sum( focal_loss_sigmoid(logits = last_l, labels = Y , alpha = focal_weight ))\n    beta_Factor = 0.99\n    cost2 = (1- beta_Factor) / (1- beta_Factor**batch_size) * cost2\n    # tf.nn.softmax_cross_entropy_with_logits_v2(logits = last_l, labels = Y)\n      \n    regularizers = tf.nn.l2_loss(input_layer) +  sum(map(lambda x: tf.nn.l2_loss(x), layers)) + tf.nn.l2_loss(output_layer)\n    #+ \n    #regularizers = tf.contrib.layers.l1_l2_regularizer(0.5, 0.5)(input_layer) + \\\n    #sum(map(lambda x: tf.contrib.layers.l1_l2_regularizer(0.5, 0.5)(x), layers)) \n    \n    #tf.contrib.layers.l1_l2_regularizer(1.0, 1.0)(input_layer) + \n    \n    \n    cost = cost2\n    #+ beta * regularizers\n    \n    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n    last_l = tf.nn.sigmoid(last_l)\n    last_l = tf.argmax( last_l, 1)\n    last_y = tf.argmax(Y, 1)\n    #correct_prediction = tf.equal(, )\n    #accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n    \n    #sess = tf.InteractiveSession()\n    #sess.run(tf.global_variables_initializer())\n    \n    config=tf.ConfigProto( log_device_placement=True)\n    config.gpu_options.allow_growth = True\n    sess = tf.Session(config = config)\n    sess.run(tf.global_variables_initializer())\n    \n    \n    COST, TEST_COST, ACC, TEST_ACC = [], [], [], []\n    \n    for i in range(100):\n        train_acc, train_loss = 0, 0\n        train_dt = np.concatenate((x_train , y_train) , axis = 1)\n        np.random.shuffle(train_dt)\n        x_train = train_dt[:,:-2]\n        y_train = train_dt[:,-2:]\n        for n in range(0, (x_train.shape[0] // batch_size) * batch_size, batch_size):\n            _, loss , loss2 , regu2 = sess.run([optimizer, cost , cost2 , regularizers ], \n                                                 feed_dict = {X: x_train[n: n + batch_size, :], Y: y_train[n: n + batch_size, :]})\n            TRUE , PRED = sess.run([last_l , last_y], feed_dict = {X: x_train[n: n + batch_size, :], Y: y_train[n: n + batch_size, :]})\n            train_acc += balanced_accuracy_score(TRUE , PRED)\n            train_loss += loss\n            \n        \n        if i % 100 == 0 :\n            print(\"Epoch : {} , Train Loss : {} , Regularizer : {} , Total Loss : {} \".format(i ,loss2 , regu2 , loss))\n\n        \n        train_loss /= (x_train.shape[0] // batch_size)\n        train_acc /= (x_train.shape[0] // batch_size)\n        ACC.append(train_acc)\n        COST.append(train_loss)\n    ## test는 학습 다하고 딱 한번만 하는 것이 맞지 않을까? \n    \n    TEST_COST.append(sess.run(cost, feed_dict = {X: x_test, Y: y_test}))\n    TRUE , PRED = sess.run([last_l , last_y], feed_dict = {X: x_test, Y: y_test})\n    TEST_ACC.append(balanced_accuracy_score(TRUE , PRED))\n\n    clear_output(wait=True)\n    COST = np.array(COST).mean()\n    ACC = np.array(ACC).mean()\n    \n    TEST_COST = np.array(TEST_COST).mean()\n    TEST_ACC = np.array(TEST_ACC).mean()\n    \n    Test_TRUE = sess.run([last_l], feed_dict = { X: test_df })\n    test_pred_n = collections.Counter(Test_TRUE[0])\n    \n    if TEST_ACC > 0.8 : \n        TEST_ACC = np.round(TEST_ACC , 3)\n        with open('test_predict_{}.pkl'.format(TEST_ACC), 'wb') as f:\n            pickle.dump(Test_TRUE, f)\n    \n    return COST, TEST_COST, ACC, TEST_ACC , test_pred_n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a71030e1dbef899d4effcc75b0a543a44a5fc47a"},"cell_type":"code","source":"def generate_nn(num_hidden, size_layer, learning_rate, dropout_rate, beta, activation , focal_weight , reduction_node ):\n    global accbest\n    param = {\n        'num_hidden' : int(np.around(num_hidden)),\n        'size_layer' : int(np.around(size_layer)),\n        'learning_rate' : max(min(learning_rate, 1), 0.0001),\n        'dropout_rate' : max(min(dropout_rate, 0.7), 0.2),\n        'beta' : max(min(beta, 0.5), 0.000001),\n        'activation': int(np.around(activation)) , \n        \"focal_weight\" : max( min(focal_weight , 0.5) , 0.01) ,\n        \"reduction_node\" : min( int(reduction_node) , int(np.around(size_layer) / np.around(num_hidden) )- 10  ) ,\n    }\n    print(\"\\n Search parameters \\n %s\" % (param), file = log_file)\n    \n    log_file.flush()\n    learning_cost, valid_cost, learning_acc, valid_acc , test_pred_n = neural_network(**param)\n    print(\"stop after 5000 iteration with train cost %f, test cost %f, train acc %f, test acc %f\" % (learning_cost, valid_cost, learning_acc, valid_acc))\n    \n    f = open(\"nn-bayesian_acc.txt\",'a')\n    result_ = \"stop after 5000 iteration with train cost {:.3f}, test cost {:.3f}, train acc {:.3f}, test acc {:.3f} , True Test: {} \\n\".format(learning_cost, valid_cost, learning_acc, valid_acc , test_pred_n)\n    f.write(result_)\n    if (valid_acc > accbest):\n        costbest = valid_acc\n    return valid_acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"81cf28126722c5da2b240fb97ac28264e4e7171c"},"cell_type":"code","source":"log_file = open('nn-bayesian.log', 'a')\naccbest = 0.8\nNN_BAYESIAN = BayesianOptimization(generate_nn, \n                              {'num_hidden': (3, 6),\n                               'size_layer': (df.shape[1]-30 , df.shape[1] + 10 ),\n                               'learning_rate': (0.001, 0.0001),\n                               'dropout_rate': (0.2, 0.8),\n                               'beta': (0.01, 0.001),\n                               'activation': (0, 4),\n                               \"focal_weight\" : (0.2 , 0.6),\n                               \"reduction_node\" : (50 , 90 )\n                              })\n\n#NN_BAYESIAN.maximize(init_points = 10 , n_iter = 1 , acq=\"ucb\", kappa= 10.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5769c898bdac6d9d172b7b5b26ccaec9b89adee"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"dbe94b0c61bf04aea99ddcfa73efad1b61aef2fa"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"55280389020887bcb3158d7e4e6c442a7b21215c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}