{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-19T20:03:37.066537Z","iopub.execute_input":"2021-08-19T20:03:37.066894Z","iopub.status.idle":"2021-08-19T20:03:37.077172Z","shell.execute_reply.started":"2021-08-19T20:03:37.066865Z","shell.execute_reply":"2021-08-19T20:03:37.076262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## don't overfit we have a very small training data set with more features than row\n### our goal is develop a model that doesn\\t over fit ","metadata":{"execution":{"iopub.status.busy":"2021-08-19T18:53:26.497213Z","iopub.execute_input":"2021-08-19T18:53:26.497654Z","iopub.status.idle":"2021-08-19T18:53:26.50142Z","shell.execute_reply.started":"2021-08-19T18:53:26.49762Z","shell.execute_reply":"2021-08-19T18:53:26.500287Z"}}},{"cell_type":"code","source":"# loading train and test data\ntrain_df=pd.read_csv('/kaggle/input/dont-overfit-ii/train.csv')\ntest_df=pd.read_csv('/kaggle/input/dont-overfit-ii/test.csv')\nprint('train data shape ', train_df.shape)\nprint('test data shape ', test_df.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T20:03:39.627854Z","iopub.execute_input":"2021-08-19T20:03:39.628435Z","iopub.status.idle":"2021-08-19T20:03:40.760785Z","shell.execute_reply.started":"2021-08-19T20:03:39.628401Z","shell.execute_reply":"2021-08-19T20:03:40.759679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T20:03:41.922422Z","iopub.execute_input":"2021-08-19T20:03:41.92275Z","iopub.status.idle":"2021-08-19T20:03:41.956857Z","shell.execute_reply.started":"2021-08-19T20:03:41.922723Z","shell.execute_reply":"2021-08-19T20:03:41.955882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T20:03:44.008474Z","iopub.execute_input":"2021-08-19T20:03:44.008824Z","iopub.status.idle":"2021-08-19T20:03:44.045973Z","shell.execute_reply.started":"2021-08-19T20:03:44.008797Z","shell.execute_reply":"2021-08-19T20:03:44.045019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking for missing values \nprint ('missing values in train data',train_df.isna().sum().sum())\nprint ('missing values in test data',test_df.isna().sum().sum())","metadata":{"execution":{"iopub.status.busy":"2021-08-19T20:03:45.575478Z","iopub.execute_input":"2021-08-19T20:03:45.576013Z","iopub.status.idle":"2021-08-19T20:03:45.601018Z","shell.execute_reply.started":"2021-08-19T20:03:45.575963Z","shell.execute_reply":"2021-08-19T20:03:45.59998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we can see that there is class imbalance \ntrain_df.target.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T20:03:47.027473Z","iopub.execute_input":"2021-08-19T20:03:47.028087Z","iopub.status.idle":"2021-08-19T20:03:47.036999Z","shell.execute_reply.started":"2021-08-19T20:03:47.028052Z","shell.execute_reply":"2021-08-19T20:03:47.035812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# viewwing vairnace in train features \n\nimport matplotlib.pyplot as plt\ntrain_df[train_df.columns[2:]].std().plot(kind='hist');\nplt.title('Distribution of stds of all columns');","metadata":{"execution":{"iopub.status.busy":"2021-08-19T20:03:48.537148Z","iopub.execute_input":"2021-08-19T20:03:48.537745Z","iopub.status.idle":"2021-08-19T20:03:48.791308Z","shell.execute_reply.started":"2021-08-19T20:03:48.537711Z","shell.execute_reply":"2021-08-19T20:03:48.790614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# look at distruibution of means \ntrain_df[train_df.columns[2:]].mean().plot(kind='hist');\nplt.title('Distribution of stds of all columns');","metadata":{"execution":{"iopub.status.busy":"2021-08-19T20:03:50.717317Z","iopub.execute_input":"2021-08-19T20:03:50.717985Z","iopub.status.idle":"2021-08-19T20:03:50.976837Z","shell.execute_reply.started":"2021-08-19T20:03:50.717931Z","shell.execute_reply":"2021-08-19T20:03:50.976026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# looks at distribution of median \n# it seems the distribution is little skewed \n\ntrain_df[train_df.columns[2:]].median().plot(kind='hist');\nplt.title('Distribution of stds of all columns');","metadata":{"execution":{"iopub.status.busy":"2021-08-19T20:03:52.906986Z","iopub.execute_input":"2021-08-19T20:03:52.907499Z","iopub.status.idle":"2021-08-19T20:03:53.111735Z","shell.execute_reply.started":"2021-08-19T20:03:52.907465Z","shell.execute_reply":"2021-08-19T20:03:53.110971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# grouping together features with high correlation\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# it seems that features has low correlation to each other and the target \n\n# so it is difficult to drop based on correlation\n\n\ncorrs = train_df.corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\ncorrs = corrs[corrs['level_0'] != corrs['level_1']]\ncorrs.tail(10)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-19T20:03:56.407678Z","iopub.execute_input":"2021-08-19T20:03:56.408344Z","iopub.status.idle":"2021-08-19T20:03:56.531898Z","shell.execute_reply.started":"2021-08-19T20:03:56.408296Z","shell.execute_reply":"2021-08-19T20:03:56.530664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# as a first approach we will choose 22 features as random and see the performance of our model\nt = train_df.drop(['id', 'target'], axis=1)\ncolums= np.random.choice(t.columns, 22,replace=False, )\ncolums","metadata":{"execution":{"iopub.status.busy":"2021-08-19T20:07:31.5802Z","iopub.execute_input":"2021-08-19T20:07:31.580579Z","iopub.status.idle":"2021-08-19T20:07:31.589511Z","shell.execute_reply.started":"2021-08-19T20:07:31.580547Z","shell.execute_reply":"2021-08-19T20:07:31.588433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# getting selected features \n\nXtrain=train_df[colums]\nXtest=test_df[colums]","metadata":{"execution":{"iopub.status.busy":"2021-08-19T20:08:00.553109Z","iopub.execute_input":"2021-08-19T20:08:00.553465Z","iopub.status.idle":"2021-08-19T20:08:00.560045Z","shell.execute_reply.started":"2021-08-19T20:08:00.553436Z","shell.execute_reply":"2021-08-19T20:08:00.55931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## apply standard scaler to train and test data \n\n\n\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\n#train = train_df.drop(['id', 'target'], axis=1)\ny = train_df['target']\n#test = test_df.drop(['id'], axis=1)\n\nscaler = StandardScaler()\ntrain = scaler.fit_transform(Xtrain)\ntest = scaler.transform(Xtest)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-19T20:09:30.709202Z","iopub.execute_input":"2021-08-19T20:09:30.70989Z","iopub.status.idle":"2021-08-19T20:09:30.724684Z","shell.execute_reply.started":"2021-08-19T20:09:30.709851Z","shell.execute_reply":"2021-08-19T20:09:30.72379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-08-19T20:04:03.528694Z","iopub.execute_input":"2021-08-19T20:04:03.529182Z","iopub.status.idle":"2021-08-19T20:04:03.536582Z","shell.execute_reply.started":"2021-08-19T20:04:03.529153Z","shell.execute_reply":"2021-08-19T20:04:03.535862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-08-19T20:04:18.713302Z","iopub.execute_input":"2021-08-19T20:04:18.713669Z","iopub.status.idle":"2021-08-19T20:04:18.72053Z","shell.execute_reply.started":"2021-08-19T20:04:18.71364Z","shell.execute_reply":"2021-08-19T20:04:18.719564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# train a logistic regression model on the training set\nfrom sklearn.linear_model import LogisticRegression\n\n\n# instantiate the model\nlogreg = LogisticRegression(solver='liblinear', random_state=0)\n\n\n# fit the model\nlogreg.fit(Xtrain, y)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-19T20:09:40.774078Z","iopub.execute_input":"2021-08-19T20:09:40.774762Z","iopub.status.idle":"2021-08-19T20:09:40.837319Z","shell.execute_reply.started":"2021-08-19T20:09:40.774719Z","shell.execute_reply":"2021-08-19T20:09:40.836032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# making prediction \ny_pred_test = logreg.predict(Xtest)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-19T20:09:43.535912Z","iopub.execute_input":"2021-08-19T20:09:43.536274Z","iopub.status.idle":"2021-08-19T20:09:43.549518Z","shell.execute_reply.started":"2021-08-19T20:09:43.53624Z","shell.execute_reply":"2021-08-19T20:09:43.548156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({'id': test_df.id, 'target': y_pred_test})                                                     \nsubmission.to_csv('submission.csv', index=False) ","metadata":{"execution":{"iopub.status.busy":"2021-08-19T20:09:45.44195Z","iopub.execute_input":"2021-08-19T20:09:45.44231Z","iopub.status.idle":"2021-08-19T20:09:45.498842Z","shell.execute_reply.started":"2021-08-19T20:09:45.442281Z","shell.execute_reply":"2021-08-19T20:09:45.497677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}