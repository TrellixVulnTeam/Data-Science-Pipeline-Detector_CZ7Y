{"cells":[{"metadata":{},"cell_type":"markdown","source":"Sources https://towardsdatascience.com/metrics-and-python-850b60710e0c","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"path = \"../input/dont-overfit-ii/train.csv\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# As above we can see most of the series data is in right skewed","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n\n# to find the correlation among variables (Multicollinearity)\n\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.corr()\ncor = data.iloc[:,0:5].corr()\nprint(cor)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# >  LINEAR REGRESSION","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = train_test_split(data, test_size = 0.3)\nprint(train.shape)\nprint(test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# split the train and test into X and Y variables\n# ------------------------------------------------\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x = train.iloc[:,0:120]; train_y = train.iloc[:,120]\ntest_x  = test.iloc[:,0:120];  test_y = test.iloc[:,120]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_x.shape)\nprint(train_y.shape)\nprint(test_x.shape)\nprint(test_y.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x.head()\ntrain_y.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_y.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ensure that the X variables are all numeric for regression\n# ---------------------------------------------","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# To add the constant term A (Y = A + B1X1 + B2X2 + ... + BnXn)\n# Xn = ccomp,slag,flyash.....\n# ----------------------------------------------------------\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lm1 = sm.OLS(train_y, train_x).fit()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n# Prediction\n# -----------------","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# store the actual and predicted values in a dataframe for comparison\n# --------------------------------------------","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pdct1 = lm1.predict(test_x)\nprint(pdct1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"actual = list(test_y.head(50))\ntype(actual)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted = np.round(np.array(list(pdct1.head(50))),2)\nprint(predicted)\ntype(predicted)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Actual vs Predicted\n#-----------------------","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_results = pd.DataFrame({'actual':actual, 'predicted':predicted})\nprint(df_results.head(115))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ##To Check the Accuracy:\n#-----------------------------","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics  \nprint('Mean Absolute Error:', metrics.mean_absolute_error(test_y, pdct1))  \nprint('Mean Squared Error:', metrics.mean_squared_error(test_y, pdct1))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(test_y, pdct1))) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MAE : MEAN ABSOLUTE ERROR\n\n# MAE measures the average magnitude of the errors in a set of predictions, without considering their direction. It’s the average over the test sample of the absolute differences between prediction and actual observation where all individual differences have equal weight.\n\n# The mean absolute error uses the same scale as the data. This is known as a scale-dependent accuracy measure and, therefore, cannot be used to make comparisons between series using different scales.\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# MSE: MEAN SQUARE ERROR\n\n# MSE is a risk function, corresponding to the expected value of the squared error loss. The fact that MSE is almost always strictly positive (and not zero) is because of randomness or because the estimator does not account for information that could produce a more accurate estimate. The MSE is a measure of the quality of an estimator — it is always non-negative, and values closer to zero are better.\n\n# “Minimizing MSE is a key criterion in selecting estimators: see minimum mean-square error. Among unbiased estimators, minimizing the MSE is equivalent to minimizing the variance, and the estimator that does this is the minimum variance unbiased estimator. However, a biased estimator may have lower MSE; see estimator bias.\n\n\n# In statistical modelling the MSE can represent the difference between the actual observations and the observation values predicted by the model. In this context, it is used to determine the extent to which the model fits the data as well as whether removing some explanatory variables is possible without significantly harming the model’s predictive ability.”\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# RMSE: Root mean square error\n \n# RMSE is a quadratic scoring rule that also measures the average magnitude of the error. It’s the square root of the average of squared differences between prediction and actual observation.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Comparision Root mean square error vs Mean square error\n\n# Similarities:\n\n# Express average model prediction error in same units of the variable of interest.\n\n# Can range from 0 to ∞ and are indifferent to the direction of errors.\n\n# Lower values are better.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# RMSE VS MSE:\n\n# Difference:\n\n# Taking the square root before they are averaged, RMSE gives a relatively high weight to large errors, so RMSE should be useful when large errors are undesirable.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def RMSE(predict, target):\n    return np.sqrt(((predict - target) ** 2).mean())\nprint ('My RMSE: ' + str(RMSE(test_y,pdct1)) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MAPE: Mean absolute percentage error\n\n# Measure of prediction accuracy of a forecasting method in statistics, for example in trend estimation, also used as a loss function for regression problems in machine learning. It usually expresses accuracy as a percentage.\n\n# Although the concept of MAPE sounds very simple and convincing, it has major drawbacks in practical application, and there are many studies on shortcomings and misleading results from MAPE","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def MAPE(predict,target):\n    return ( abs((target - predict) / target).mean()) * 100\nprint ('My MAPE: ' + str(MAPE(test_y,pdct1)) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# here we go ahead with 60:40 train test split and that mean absoulate percentage error is more than 51% that is inaccurate forcasting.An inaccurate forecast of flurries might not cause a problem if they don't materialize, but an inaccurate forecast of a foot of snow – in either direction – has serious consequences. Inaccurate sales forecasts are legendary. ... They lack a functional, sales-specific CRM or Pipeline Management application.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# RMSLE: Root Mean Squared Logarithmic Error\n\n# In case of RMSLE, you take the log of the predictions and actual values. So basically, what changes is the variance that you are measuring. I believe RMSLE is usually used when you don’t want to penalize huge differences in the predicted and the actual values when both predicted and true values are huge numbers.\n\n# RMSLE measures the ratio between actual and predicted.\n\n# log(pi+1)−log(ai+1)log(pi+1)−log(ai+1)\n\n# can be written as log((pi+1)/(ai+1))log((pi+1)/(ai+1))\n\n# It can be used when you don’t want to penalize huge differences when the values are huge numbers.\n\n# Also, this can be used when you want to penalize under estimates more than over estimates.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\n\n#A function to calculate Root Mean Squared Logarithmic Error (RMSLE)\ndef rmsle(pdct1, test_y):\n    assert len(pdct1) == len(test_y)\n    terms_to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(test_y)]\n    return (sum(terms_to_sum) * (1.0/len(pdct1))) ** 0.5\nprint ('My RMSLE: ' + str(RMSE(test_y,pdct1)) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# R² and R-Squared: Coefficient of determination\n\n# R² and R-Squared help us to know how good our regression model as compared to a very simple model that just predicts the mean value of target from the train set as predictions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\nprint ('Sk MAE: ' + str(mean_absolute_error(pdct1,test_y)) )\ndef MAE(predict,target):\n    return (abs(predict-target)).mean()\nprint ('My MAE: ' + str(MAE(test_y,pdct1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def R2(predict, target):\n    return 1 - (MAE(predict,target) / MAE(target.mean(),target))\ndef R_SQR(predict, target):\n    r2 = R2(predict,target)\n    return np.sqrt(r2)\nprint ('My R2         : ' + str(R2(test_y,pdct1)) )\nprint ('My R          : ' + str(R_SQR(test_y,pdct1)) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Adjusted R²\n\n# A model performing equal to baseline would give R-Squared as 0. Better the model, higher the r2 value. The best model with all correct predictions would give R-Squared as 1. However, on adding new features to the model, the R-Squared value either increases or remains the same. R-Squared does not penalize for adding features that add no value to the model. So an improved version over the R-Squared is the adjusted R-Squared","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def R2_ADJ(predict, target, k):\n    r2 = R2(predict,target)\n    n = len(target)\n    return (1 -  ( (1-r2) *  ( (n-1) / (n-(k+1)) ) ) )\nk= len(data.columns)\nprint ('My R2 adjusted: ' + str(R2_ADJ(test_y,pdct1,k)) )","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}