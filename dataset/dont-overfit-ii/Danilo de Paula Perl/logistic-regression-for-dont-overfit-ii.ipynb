{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import numpy as np\nimport os\nimport pandas as pd\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.pipeline import make_pipeline, FeatureUnion, Pipeline\nfrom sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score, GridSearchCV, cross_validate, RepeatedStratifiedKFold\nfrom sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, make_scorer, mean_squared_error, mean_absolute_error \nfrom sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, roc_auc_score, auc, log_loss, r2_score\nfrom sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import Lasso\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom scipy.stats import ks_2samp\nfrom imblearn.over_sampling import SMOTE","metadata":{"execution":{"iopub.status.busy":"2021-07-24T22:48:59.749616Z","iopub.execute_input":"2021-07-24T22:48:59.750375Z","iopub.status.idle":"2021-07-24T22:48:59.762687Z","shell.execute_reply.started":"2021-07-24T22:48:59.750313Z","shell.execute_reply":"2021-07-24T22:48:59.761822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading the data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/dont-overfit-ii/train.csv')\ntest = pd.read_csv('../input/dont-overfit-ii/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-24T22:49:01.522974Z","iopub.execute_input":"2021-07-24T22:49:01.523372Z","iopub.status.idle":"2021-07-24T22:49:02.735229Z","shell.execute_reply.started":"2021-07-24T22:49:01.523336Z","shell.execute_reply":"2021-07-24T22:49:02.734226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-24T22:49:03.486298Z","iopub.execute_input":"2021-07-24T22:49:03.486683Z","iopub.status.idle":"2021-07-24T22:49:03.522846Z","shell.execute_reply.started":"2021-07-24T22:49:03.486649Z","shell.execute_reply":"2021-07-24T22:49:03.52178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-24T22:49:04.260617Z","iopub.execute_input":"2021-07-24T22:49:04.261002Z","iopub.status.idle":"2021-07-24T22:49:04.267763Z","shell.execute_reply.started":"2021-07-24T22:49:04.260969Z","shell.execute_reply":"2021-07-24T22:49:04.266719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isnull().sum()[train.isnull().sum() > 0]","metadata":{"execution":{"iopub.status.busy":"2021-07-24T22:49:05.487868Z","iopub.execute_input":"2021-07-24T22:49:05.488329Z","iopub.status.idle":"2021-07-24T22:49:05.5055Z","shell.execute_reply.started":"2021-07-24T22:49:05.488283Z","shell.execute_reply":"2021-07-24T22:49:05.504084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-24T22:49:06.124284Z","iopub.execute_input":"2021-07-24T22:49:06.124789Z","iopub.status.idle":"2021-07-24T22:49:06.151696Z","shell.execute_reply.started":"2021-07-24T22:49:06.124758Z","shell.execute_reply":"2021-07-24T22:49:06.150785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"#from kernel  \"https://www.kaggle.com/nanomathias/distribution-of-test-vs-training-data\"\ndef get_diff_columns(train_df, test_df, show_plots=True, show_all=False, threshold=0.1):\n    \"\"\"Use KS to estimate columns where distributions differ a lot from each other\"\"\"\n\n    # Find the columns where the distributions are very different\n    diff_data = []\n    for col in tqdm(train_df.columns):\n        statistic, pvalue = ks_2samp(\n            train_df[col].values, \n            test_df[col].values\n        )\n        if pvalue > 0.05 and np.abs(statistic) < threshold:\n            diff_data.append({'feature': col, 'p': np.round(pvalue, 5), 'statistic': np.round(np.abs(statistic), 2)})\n\n    # Put the differences into a dataframe\n    diff_df = pd.DataFrame(diff_data).sort_values(by='statistic', ascending=False)\n    print(f\"number of features with diff distribution : {len(diff_df)}\")\n    if show_plots:\n        # Let us see the distributions of these columns to confirm they are indeed different\n        n_cols = 5\n        n_rows = 5\n        _, axes = plt.subplots(n_rows, n_cols, figsize=(20, 3*n_rows))\n        axes = [x for l in axes for x in l]\n\n        # Create plots\n        for i, (_, row) in enumerate(diff_df.iterrows()):\n            if i >= len(axes):\n                break\n            extreme = np.max(np.abs(train_df[row.feature].tolist() + test_df[row.feature].tolist()))\n            train_df.loc[:, row.feature].apply(np.log1p).hist(\n                ax=axes[i], alpha=0.5, label='Train', density=True,\n                bins=np.arange(-extreme, extreme, 0.25)\n            )\n            test_df.loc[:, row.feature].apply(np.log1p).hist(\n                ax=axes[i], alpha=0.5, label='Test', density=True,\n                bins=np.arange(-extreme, extreme, 0.25)\n            )\n            axes[i].set_title(f\"Statistic = {row.statistic}, p = {row.p}\")\n            axes[i].set_xlabel(f'Log({row.feature})')\n            axes[i].legend()\n\n        plt.tight_layout()\n        plt.show()\n        \n    return diff_df\n\n# Get the columns which differ a lot between test and train\ndiff_df = get_diff_columns(train.drop(['id','target'], axis=1), test.drop(['id'], axis=1))","metadata":{"execution":{"iopub.status.busy":"2021-07-24T22:49:07.956411Z","iopub.execute_input":"2021-07-24T22:49:07.956775Z","iopub.status.idle":"2021-07-24T22:49:17.978699Z","shell.execute_reply.started":"2021-07-24T22:49:07.956745Z","shell.execute_reply":"2021-07-24T22:49:17.977399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_with_y = pd.DataFrame(train.drop(['id','target'], axis=1).corrwith(train[\"target\"]).abs()).reset_index()\ncorr_with_y.columns = [\"Feature\", \"Correlation with Target\"]\ncorr_with_y = corr_with_y.sort_values(by=\"Correlation with Target\", ascending=False)\ncorr_with_y.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T22:49:17.981894Z","iopub.execute_input":"2021-07-24T22:49:17.982214Z","iopub.status.idle":"2021-07-24T22:49:18.08198Z","shell.execute_reply.started":"2021-07-24T22:49:17.982184Z","shell.execute_reply":"2021-07-24T22:49:18.080817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop identity and target columns\nvariables_train = train.drop(['id','target'], axis=1).values\nvar_resp = train[\"target\"].copy()\nvariables_test = test.drop(['id'], axis=1).values","metadata":{"execution":{"iopub.status.busy":"2021-07-25T00:22:23.479508Z","iopub.execute_input":"2021-07-25T00:22:23.479932Z","iopub.status.idle":"2021-07-25T00:22:23.61378Z","shell.execute_reply.started":"2021-07-25T00:22:23.479898Z","shell.execute_reply":"2021-07-25T00:22:23.612829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(var_resp.value_counts()/var_resp.count())*100","metadata":{"execution":{"iopub.status.busy":"2021-07-25T00:22:24.562807Z","iopub.execute_input":"2021-07-25T00:22:24.56356Z","iopub.status.idle":"2021-07-25T00:22:24.582455Z","shell.execute_reply.started":"2021-07-25T00:22:24.563496Z","shell.execute_reply":"2021-07-25T00:22:24.58139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature engineering","metadata":{}},{"cell_type":"code","source":"def with_statistics(X):\n    statistics = pd.DataFrame()\n    statistics['mean']   = X.mean(axis=1)\n    statistics['std']    = X.std(axis=1)\n    statistics['kurt']   = X.kurt(axis=1)\n    statistics['mad']    = X.mad(axis=1)\n    statistics['median'] = X.median(axis=1)\n    statistics['max']    = X.max(axis=1)\n    statistics['min']    = X.min(axis=1)\n    statistics['skew']   = X.skew(axis=1)\n    statistics['sem']    = X.sem(axis=1)\n    \n    from sklearn.neighbors import NearestNeighbors\n    neigh = NearestNeighbors(5, n_jobs=-1)\n    neigh.fit(X)\n\n    dists, _ = neigh.kneighbors(X, n_neighbors=5)\n    dists = np.delete(dists, 0, 1)\n    statistics['minDist'] = dists.mean(axis=1)\n    statistics['maxDist'] = dists.max(axis=1)\n    statistics['meanDist'] = dists.min(axis=1)\n\n# Trigometric FE\n    sin_temp = np.sin(X)\n    cos_temp = np.cos(X)\n    tan_temp = np.tan(X)\n    statistics['mean_sin'] = np.mean(sin_temp, axis=1)\n    statistics['mean_cos'] = np.mean(cos_temp, axis=1)\n    statistics['mean_tan'] = np.mean(tan_temp, axis=1)\n# Hyperbolic FE\n    sinh_temp = np.sinh(X)\n    cosh_temp = np.cosh(X)\n    tanh_temp = np.tanh(X)\n    statistics['mean_sinh'] = np.mean(sinh_temp, axis=1)\n    statistics['mean_cosh'] = np.mean(cosh_temp, axis=1)\n    statistics['mean_tanh'] = np.mean(tanh_temp, axis=1)\n# Exponents FE\n    exp_temp = np.exp(X)\n    expm1_temp = np.expm1(X)\n    exp2_temp = np.exp2(X)\n    statistics['mean_exp'] = np.mean(exp_temp, axis=1)\n    statistics['mean_expm1'] = np.mean(expm1_temp, axis=1)\n    statistics['mean_exp2'] = np.mean(exp2_temp, axis=1)\n# Polynomial FE\n    # X**2\n    statistics['mean_x2'] = np.mean(np.power(X, 2), axis=1)\n    # X**3\n    statistics['mean_x3'] = np.mean(np.power(X, 3), axis=1)\n    # X**4\n    statistics['mean_x4'] = np.mean(np.power(X, 4), axis=1)\n    \n    X = pd.concat([X, statistics], axis=1)\n    return X","metadata":{"execution":{"iopub.status.busy":"2021-07-25T00:22:26.591828Z","iopub.execute_input":"2021-07-25T00:22:26.592249Z","iopub.status.idle":"2021-07-25T00:22:26.614912Z","shell.execute_reply.started":"2021-07-25T00:22:26.592214Z","shell.execute_reply":"2021-07-25T00:22:26.613299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here I apply a Pipeline to standardize the scale on numerical data\n# As we don't have missings and categorical data, I don't need to worry about this part\n# As we have 282/301 variables with different distribution on training and test basis, we will standardize with RobustScaler\n\npreprocessor = Pipeline([\n        ('selector', VarianceThreshold()),\n        ('std_scaler', RobustScaler())\n    ])","metadata":{"execution":{"iopub.status.busy":"2021-07-25T00:22:28.510331Z","iopub.execute_input":"2021-07-25T00:22:28.510756Z","iopub.status.idle":"2021-07-25T00:22:28.518074Z","shell.execute_reply.started":"2021-07-25T00:22:28.510723Z","shell.execute_reply":"2021-07-25T00:22:28.516798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = preprocessor.fit_transform(np.concatenate((variables_train, variables_test), axis=0))\nvariables_train = data[:variables_train.shape[0]]\nvariables_test = data[variables_train.shape[0]:]","metadata":{"execution":{"iopub.status.busy":"2021-07-25T00:22:30.024807Z","iopub.execute_input":"2021-07-25T00:22:30.025187Z","iopub.status.idle":"2021-07-25T00:22:30.670879Z","shell.execute_reply.started":"2021-07-25T00:22:30.025155Z","shell.execute_reply":"2021-07-25T00:22:30.669845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"variables_train = with_statistics(pd.DataFrame(variables_train)).values\nvariables_test = with_statistics(pd.DataFrame(variables_test)).values","metadata":{"execution":{"iopub.status.busy":"2021-07-25T00:22:32.200794Z","iopub.execute_input":"2021-07-25T00:22:32.20128Z","iopub.status.idle":"2021-07-25T00:24:30.580453Z","shell.execute_reply.started":"2021-07-25T00:22:32.201227Z","shell.execute_reply":"2021-07-25T00:24:30.579414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling with hyperparameter tuning","metadata":{}},{"cell_type":"code","source":"# define roc_auc_metric robust to only one class in y_pred\ndef scoring_roc_auc(y, y_pred):\n    try:\n        return roc_auc_score(y, y_pred)\n    except:\n        return 0.5\n\nrobust_roc_auc = make_scorer(scoring_roc_auc)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T00:24:30.582045Z","iopub.execute_input":"2021-07-25T00:24:30.582347Z","iopub.status.idle":"2021-07-25T00:24:30.58863Z","shell.execute_reply.started":"2021-07-25T00:24:30.582318Z","shell.execute_reply":"2021-07-25T00:24:30.587524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid = [\n    {\n        'C': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1],\n        'tol': [0.00009, 0.0001, 0.00011],\n        'max_iter': [int(x) for x in np.linspace(start = 100, stop = 10000, num = 32)],\n        'penalty': ['l1', 'l2', 'elasticnet'],\n        'solver': ['liblinear','sag']\n    }\n]\n\nmodel = LogisticRegression(random_state=42, class_weight='balanced')\n\ngrid_search = GridSearchCV(model, param_grid=param_grid, verbose=0, n_jobs=-1, scoring=robust_roc_auc, cv=20)\ngrid_search.fit(variables_train, var_resp)\n\nfeature_selector = RFECV(grid_search.best_estimator_, verbose=0, min_features_to_select=10, scoring=robust_roc_auc, step=15, cv=20, n_jobs=-1)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T00:25:27.760432Z","iopub.execute_input":"2021-07-25T00:25:27.761023Z","iopub.status.idle":"2021-07-25T00:47:31.943271Z","shell.execute_reply.started":"2021-07-25T00:25:27.760976Z","shell.execute_reply":"2021-07-25T00:47:31.942065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"counter | val_mse  |  val_mae  |  val_roc  |  val_cos  |  val_dist  |  val_r2    | feature_count \")\nprint(\"-------------------------------------------------------------------------------------------------\")\n\npredictions = pd.DataFrame()\ncounter = 0\n# split training data to build one model on each traing-data-subset\nfor train_index, val_index in StratifiedKFold(n_splits=20, shuffle=True).split(variables_train, var_resp):\n    X, val_X = variables_train[train_index], variables_train[val_index]\n    y, val_y = var_resp[train_index], var_resp[val_index]\n\n    # get the best features for this data set\n    feature_selector.fit(X, y)\n    \n    # remove irrelevant features from X, val_X and test\n    X_important_features        = feature_selector.transform(X)\n    val_X_important_features    = feature_selector.transform(val_X)\n    test_important_features     = feature_selector.transform(variables_test)\n\n    # run grid search to find the best model parameters for this subset of training data and subset of features \n    grid_search = GridSearchCV(feature_selector.estimator_, param_grid=param_grid, verbose=0, n_jobs=-1, scoring=robust_roc_auc, cv=20)\n    grid_search.fit(X_important_features, y)\n\n    # score our fitted model on validation data\n    val_y_pred = grid_search.best_estimator_.predict_proba(val_X_important_features)[:,1]\n    val_mse = mean_squared_error(val_y, val_y_pred)\n    val_mae = mean_absolute_error(val_y, val_y_pred)\n    val_roc = roc_auc_score(val_y, val_y_pred)\n    val_cos = cosine_similarity(val_y.values.reshape(1, -1), val_y_pred.reshape(1, -1))[0][0]\n    val_dst = euclidean_distances(val_y.values.reshape(1, -1), val_y_pred.reshape(1, -1))[0][0]\n    val_r2  = r2_score(val_y, val_y_pred)\n\n    # if model did well on validation, save its prediction on test data, using only important features\n    # r2_threshold (0.2) is a heuristic threshold for r2 error\n    # you can use any other metric/metric combination that works for you\n    if val_r2 > 0.185:\n        message = '<-- OK'\n        prediction = grid_search.best_estimator_.predict_proba(test_important_features)[:,1]\n        predictions = pd.concat([predictions, pd.DataFrame(prediction)], axis=1)\n    else:\n        message = '<-- skipping'\n\n\n    print(\"{0:2}      | {1:.4f}   |  {2:.4f}   |  {3:.4f}   |  {4:.4f}   |  {5:.4f}    |  {6:.4f}    |  {7:3}         {8}  \".format(counter, val_mse, val_mae, val_roc, val_cos, val_dst, val_r2, feature_selector.n_features_, message))\n    \n    counter += 1","metadata":{"execution":{"iopub.status.busy":"2021-07-25T00:51:17.188606Z","iopub.execute_input":"2021-07-25T00:51:17.188993Z","iopub.status.idle":"2021-07-25T01:37:04.647747Z","shell.execute_reply.started":"2021-07-25T00:51:17.188962Z","shell.execute_reply":"2021-07-25T01:37:04.646578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_pred = pd.DataFrame(predictions.mean(axis=1))\nmean_pred.index += 250\nmean_pred.columns = ['target']\nmean_pred.to_csv('submission.csv', index_label='id', index=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T01:40:55.917295Z","iopub.execute_input":"2021-07-25T01:40:55.917722Z","iopub.status.idle":"2021-07-25T01:40:56.010851Z","shell.execute_reply.started":"2021-07-25T01:40:55.917686Z","shell.execute_reply":"2021-07-25T01:40:56.009745Z"},"trusted":true},"execution_count":null,"outputs":[]}]}