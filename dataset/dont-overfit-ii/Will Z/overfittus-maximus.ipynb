{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"cd '/kaggle/input/dont-overfit-ii/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('train.csv')\ny = df['target']\nx = df.drop(['target','id'],axis=1)\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.1)\nprint(x_train.shape,x_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's first try with a very simple linear model.\nfrom keras import models, layers\nnet_input = layers.Input((300,))\noutput = layers.Dense(1,activation='sigmoid')(net_input)\nmodel = models.Model(net_input, output)\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['acc'])\nhistory = model.fit(x_train, y_train, epochs=10,validation_data=(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Alright, it seems to work. However, it's overfitting extremely heavily. Let's do some feature analysis to see which\n# features affect the output the most.\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression\n\nbestfeatures = SelectKBest(score_func=f_regression, k=10)\nfit = bestfeatures.fit(x.values,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(x.columns)\nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Feature','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(10,'Score'))  #print 10 best features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\ntop10 = featureScores.nlargest(10,'Score')\nplt.bar(top10['Feature'],top10['Score'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Seems as if features 33 and 65 are the most important ones. Let's single those out:\nvalues = (33,65)\nx_train, x_test, y_train, y_test = train_test_split(x.values[:,tuple(values)],y,test_size=0.1)\nprint(x_train.shape,x_test.shape)\nlosses = []\nfor i in range(5):\n    net_input = layers.Input((len(values),))\n    output = layers.Dense(1,activation='sigmoid')(net_input)\n    model = models.Model(net_input, output)\n    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['acc'])\n    history = model.fit(x_train, y_train, epochs=10,validation_data=(x_test,y_test),verbose=0)\n    losses.extend(history.history['val_loss'][-1:])\n    print(\"Attempt\",i+1,':',history.history['val_loss'][-1:])\nprint(\"Average loss: \",sum(losses)/len(losses))\n\n# Try playing around with the values a bit - see which combination gives you the lowest loss.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# It's pretty clear that only the five most important features help us.\n# Let's see if we can enhance our model.\nfrom keras import optimizers\nfrom keras import callbacks\nvalues = (33,65,217,117,91)\n\nx_train, y_train = x.values[:,tuple(values)],y\nopt = optimizers.Adam(lr=0.03)\nnet_input = layers.Input((len(values),))\noutput = layers.Dense(1,activation='sigmoid')(net_input)\nmodel = models.Model(net_input, output)\ncallback = callbacks.EarlyStopping(monitor='val_loss',\n                              min_delta=0,\n                              patience=2,\n                              verbose=0, mode='auto',\n                              restore_best_weights=True)\nmodel.compile(loss='binary_crossentropy',optimizer=opt,metrics=['acc'])\nhistory = model.fit(x_train, y_train, epochs=20,validation_split=0.1,callbacks=[callback])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After some testing, it seems like 7-8 epochs is the optimal number.\nLet's see what results we can get with this model - time to train with all the data!"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import optimizers\nx_train, y_train = x.values[:,tuple(values)],y\nopt = optimizers.Adam(lr=0.03)\nnet_input = layers.Input((len(values),))\noutput = layers.Dense(1,activation='sigmoid')(net_input)\nmodel = models.Model(net_input, output)\ncallback = callbacks.EarlyStopping(monitor='val_loss',\n                              min_delta=0,\n                              patience=2,\n                              verbose=0, mode='auto',\n                              restore_best_weights=True)\nmodel.compile(loss='binary_crossentropy',optimizer=opt,metrics=['acc'])\nhistory = model.fit(x_train, y_train, epochs=8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_val = pd.read_csv('test.csv')\nx_val.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"values = (33,65,217,117,91)\nx = x_val.drop(['id'],axis=1)\nids = x_val['id']\npredictions = model.predict(x.values[:,tuple(values)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = ['id,target\\n']\nfor index,prediction in enumerate(np.around(predictions)[:,0].astype(np.int)):\n    txt = str(ids[index])+','+str(prediction)\n    if ids[index] != 19999: txt += '\\n'\n    submission.append(txt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cd ../../working","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('submission.csv','w+') as writer:\n    writer.writelines(submission)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('submission.csv')\nsubmission.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Aaaaand we're done!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}