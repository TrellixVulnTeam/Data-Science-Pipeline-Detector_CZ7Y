{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\n#!pip install xgboost \n#from xgboost import XGBClassifier\n#Random Forest\n# Import the model we are using\nfrom sklearn.ensemble import RandomForestClassifier\n#Score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#Loading Data into DF\ndf_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"53de0b0b3e017efb28e43dfe4479435636d5461b"},"cell_type":"code","source":"#PREPARE TEST DATA\nx_train,x_test,y_train,y_test = train_test_split(df_train.drop(columns=['target']),df_train['target'],test_size=0.20, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28a4f4c056ad84bf6b66368851dfc9ef8c82cbab"},"cell_type":"code","source":"\n# model XGBC Classifier\nmodel = XGBClassifier(max_depth=4,\n                           min_child_weight=1,\n                           learning_rate=0.1,\n                           n_estimators=1000,\n                           silent=True,\n                           objective='binary:logistic',\n                           gamma=0,\n                           max_delta_step=0,\n                           subsample=1,\n                           colsample_bytree=1,\n                           colsample_bylevel=1,\n                           reg_alpha=0,\n                           reg_lambda=0,\n                           scale_pos_weight=1,\n                           seed=1,\n                           missing=None)\nmodel.fit(x_train,\n          y_train)\n#predict test data\npredictions = model.predict(x_test)\npredictions = [round(p) for p in predictions]\n\n#Metrics\naccuracy = accuracy_score(y_test,predictions)\nprint(\"X_Test Data Accuracy: %2f\" % (accuracy * 100))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca2971aedf0cc1a1eb947e06ec50026819fee118"},"cell_type":"code","source":"#RANDOM FOREST MODEL\n# Instantiate model with 1000 decision trees\nrf = RandomForestClassifier(n_estimators = 1000, \n                            random_state = 42,\n                            bootstrap=True,\n                            min_samples_leaf=4,\n                            min_samples_split=5,\n                            max_depth=100,\n                            max_features='auto')\n# Train the model on training data\nrf.fit(x_train, y_train);\nprint('Finish to create the random forest!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ffcc074d87acdfffdd3b877d3584d75b6918c740"},"cell_type":"code","source":"#Hyperparameters tuned for random forest\n#k = 3\n{'bootstrap': True,\n 'max_depth': 100,\n 'max_features': 'auto',\n 'min_samples_leaf': 4,\n 'min_samples_split': 5,\n 'n_estimators': 200}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"180f479eee30ce22e394dfdf3f86db4acfb0cf64"},"cell_type":"code","source":"#SCORE Random Forest\npredictions = rf.predict(x_test)\nprint(predictions)\npredictions = [round(p) for p in predictions]\n#Metrics\naccuracy = accuracy_score(y_test,predictions)\nprint(\"X_Test Data Accuracy: %2f\" % (accuracy * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f4ad782f3dd05831bfc2bf463dd4d92518e1168"},"cell_type":"code","source":"#feature importance\nfeature_importances = pd.DataFrame(rf.feature_importances_,\n                                   index = x_train.columns,\n                                    columns=['importance']).sort_values('importance',ascending=False)\nfeature_importances.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"890c9085bf5cfe3ef68bd6595104d1a1c0e05430"},"cell_type":"code","source":"#LOGISTIC REGRESSION\nfrom sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear').fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"100f3d5c09a32763161a504c4ca07462d6b24ef2"},"cell_type":"code","source":"#SCORE Logistic Regression\npredictions = clf.predict(x_test)\npredictions = [round(p) for p in predictions]\n#Metrics\naccuracy = accuracy_score(y_test,predictions)\nprint(\"X_Test Data Accuracy: %2f\" % (accuracy * 100))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4109fac3877e9f74b8d6306bc412d2eafcb4a54"},"cell_type":"code","source":"#Comparing Classifier on this dataset\n\nimport numpy as np\nnp.random.seed(0)\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.calibration import calibration_curve\n\nX, y = datasets.make_classification(n_samples=100000, n_features=20,\n                                    n_informative=2, n_redundant=2)\n\ntrain_samples = 100  # Samples used for training the models\n\n# Create classifiers\nlr = LogisticRegression(solver='liblinear')\ngnb = GaussianNB()\nsvc = LinearSVC(C=1.0)\nrfc = RandomForestClassifier(n_estimators=100)\n\n\n# #############################################################################\n# Plot calibration plots\n\nplt.figure(figsize=(10, 10))\nax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\nax2 = plt.subplot2grid((3, 1), (2, 0))\n\nax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\nfor clf, name in [(lr, 'Logistic'),\n                  (gnb, 'Naive Bayes'),\n                  (svc, 'Support Vector Classification'),\n                  (rfc, 'Random Forest')]:\n    clf.fit(x_train, y_train)\n    if hasattr(clf, \"predict_proba\"):\n        prob_pos = clf.predict_proba(x_test)[:, 1]\n    else:  # use decision function\n        prob_pos = clf.decision_function(x_test)\n        prob_pos = \\\n            (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())\n    fraction_of_positives, mean_predicted_value = \\\n        calibration_curve(y_test, prob_pos, n_bins=10)\n\n    ax1.plot(mean_predicted_value, fraction_of_positives, \"s-\",\n             label=\"%s\" % (name, ))\n\n    ax2.hist(prob_pos, range=(0, 1), bins=10, label=name,\n             histtype=\"step\", lw=2)\n\nax1.set_ylabel(\"Fraction of positives\")\nax1.set_ylim([-0.05, 1.05])\nax1.legend(loc=\"lower right\")\nax1.set_title('Calibration plots  (reliability curve)')\n\nax2.set_xlabel(\"Mean predicted value\")\nax2.set_ylabel(\"Count\")\nax2.legend(loc=\"upper center\", ncol=2)\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"82a90f9174b88f00f4797e47f88f8ef1ae6aa5d9"},"cell_type":"code","source":"#Predict Validation data\n#predictions = model.predict(df_test)\npredictions = clf.predict(df_test)\n# Submission\ndf_submission = pd.read_csv('../input/sample_submission.csv')\ndf_submission['target'] = predictions\ndf_submission.to_csv('submission.csv',index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1100d7cfe8a7c7084f708f5c1161a494d2f48e11"},"cell_type":"code","source":"#Gaussian Naive bayes with parameter inference\n%matplotlib inline\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\")\nimport os\nos.environ['MKL_THREADING_LAYER'] = 'GNU'\nos.environ['THEANO_FLAGS'] = 'device=cpu'\n\nimport pymc3 as pm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nnp.random.seed(12345)\nrc = {'xtick.labelsize': 20, 'ytick.labelsize': 20, 'axes.labelsize': 20, 'font.size': 20,\n      'legend.fontsize': 12.0, 'axes.titlesize': 10, \"figure.figsize\": [12, 6]}\nsns.set(rc = rc)\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bffa8117fbf2a3205455ee07c6d587688394fe04"},"cell_type":"code","source":"!pip install git+https://github.com/pymc-learn/pymc-learn\nimport pmlearn\nfrom pmlearn.naive_bayes import GaussianNB\nprint('Running on pymc-learn v{}'.format(pmlearn.__version__))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1facb8447f38ccf3fd9378b66c2b34dff03f512"},"cell_type":"code","source":"#Instantiate Gaussian Naive Bayesian Classifier based on probabilistic ML\nmodel = GaussianNB()\nmodel.fit(x_train,y_train)\nprint(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b2a09d0caab427bf6c0375058542c92e80df2772"},"cell_type":"code","source":"#Use for predicting\npredictions = model.predict_proba(X_test)\nmodel.score(X_test, y_test)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}