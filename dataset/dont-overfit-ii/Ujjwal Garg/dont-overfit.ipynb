{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport seaborn as sns\nsns.set(color_codes=True)\n\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n%precision 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df = pd.read_csv('../input/dont-overfit-ii/train.csv')\n\ndata_df.sample(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df['target'].value_counts().plot(kind='bar')\nplt.xlabel('label')\nplt.title('Distribution of classes among the training data')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Distribution of first 24 columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(26, 24))\nfor i, col in enumerate(data_df.columns[2:26]):\n    plt.subplot(6, 4, i + 1)\n    sns.distplot(data_df[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df[data_df.columns[2:]].mean().plot('hist')\nplt.title('Distribution of Mean of each column');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 5))\nfor i, col in enumerate(data_df.columns[2:6]):\n    plt.subplot(2, 2, i + 1)\n    sns.violinplot(data=data_df, y=col, x='target')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data_df.drop(columns=[\"target\", \"id\"])\ny = data_df[\"target\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest\nFits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n\nThe sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default)."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier(max_depth=2, bootstrap=True)\nrfc.fit(X_train, y_train)\n# make predictions using the trained model\ny_pred = rfc.predict(X_test)\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_pred, y_test))\n\nfrom sklearn.metrics import confusion_matrix\nprint(confusion_matrix(y_pred, y_test))\n\nfrom sklearn.metrics import accuracy_score\nprint('accuracy is ',accuracy_score(y_pred, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Meta Analysis\nA meta-analysis is a statistical analysis that combines the results of multiple scientific studies. Meta-analysis can be performed when there are multiple scientific studies addressing the same question, with each individual study reporting measurements that are expected to have some degree of error. \n\n### Bagging Classifier\n\nAn ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\n\nbag_model = BaggingClassifier()\nbag_model.fit(X_train, y_train)\ny_pred=bag_model.predict(X_test)\n\nprint(classification_report(y_pred, y_test))\nprint(confusion_matrix(y_pred, y_test))\nprint('accuracy is ',accuracy_score(y_pred, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### AdaBoost\nAn AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\n\nabc = AdaBoostClassifier()\nabc.fit(X_train, y_train)\ny_pred = abc.predict(X_test)\n\nprint(classification_report(y_pred, y_test))\nprint(confusion_matrix(y_pred, y_test))\nprint('accuracy is ',accuracy_score(y_pred, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Gradient Boosting Classifiers\nGB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\n\ngbc = GradientBoostingClassifier()\ngbc.fit(X_train, y_train)\ny_pred = gbc.predict(X_test)\n\nprint(classification_report(y_pred, y_test))\nprint(confusion_matrix(y_pred, y_test))\nprint('accuracy is ',accuracy_score(y_pred, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LDA/QDA\n- Linear Discriminant Analysis -  a method used in statistics, pattern recognition, and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events.\n\n- Qudratic Discriminatn Analysis - Here the combination is not linear in nature."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nlda = LinearDiscriminantAnalysis()\nlda.fit(X_train, y_train)\ny_pred = lda.predict(X_test)\n\nprint(classification_report(y_pred, y_test))\nprint(confusion_matrix(y_pred, y_test))\nprint('accuracy is ',accuracy_score(y_pred, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nqda = QuadraticDiscriminantAnalysis()\nqda.fit(X_train, y_train)\ny_pred = qda.predict(X_test)\n\nprint(classification_report(y_pred, y_test))\nprint(confusion_matrix(y_pred, y_test))\nprint('accuracy is ',accuracy_score(y_pred, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Permutation Importance of each feautre\nPermutation importance is calculated after a model has been fitted. It shows how much does a feature effect the predictions. It answers the following question.\n_If we randomly shuffle a single column of the validation data, leaving the target and all other columns in place, how would that affect the accuracy of predictions in that now-shuffled data?_"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X, test_X, train_y, test_y = train_test_split(X, y, random_state=1)\n\nrfc_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)\n\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(rfc_model, random_state=1).fit(test_X, test_y)\neli5.show_weights(perm, feature_names=test_X.columns.tolist(), top=300)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}