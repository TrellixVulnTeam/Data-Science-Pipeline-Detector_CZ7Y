{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\nimport datetime\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable","execution_count":1,"outputs":[{"output_type":"stream","text":"['train.csv', 'sample_submission.csv', 'test.csv']\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"   id  target      0      1      2  ...      295    296    297    298    299\n0   0     1.0 -0.098  2.165  0.681  ...   -2.097  1.051 -0.414  1.038 -1.065\n1   1     0.0  1.081 -0.973 -0.383  ...   -1.624 -0.458 -1.099 -0.936  0.973\n2   2     1.0 -0.523 -0.089 -0.348  ...   -1.165 -1.544  0.004  0.800 -1.211\n3   3     1.0  0.067 -0.021  0.392  ...    0.467 -0.562 -0.254 -0.533  0.238\n4   4     1.0  2.347 -0.831  0.511  ...    1.378  1.246  1.478  0.428  0.253\n\n[5 rows x 302 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n      <th>21</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n      <th>25</th>\n      <th>26</th>\n      <th>27</th>\n      <th>28</th>\n      <th>29</th>\n      <th>30</th>\n      <th>31</th>\n      <th>32</th>\n      <th>33</th>\n      <th>34</th>\n      <th>35</th>\n      <th>36</th>\n      <th>37</th>\n      <th>...</th>\n      <th>260</th>\n      <th>261</th>\n      <th>262</th>\n      <th>263</th>\n      <th>264</th>\n      <th>265</th>\n      <th>266</th>\n      <th>267</th>\n      <th>268</th>\n      <th>269</th>\n      <th>270</th>\n      <th>271</th>\n      <th>272</th>\n      <th>273</th>\n      <th>274</th>\n      <th>275</th>\n      <th>276</th>\n      <th>277</th>\n      <th>278</th>\n      <th>279</th>\n      <th>280</th>\n      <th>281</th>\n      <th>282</th>\n      <th>283</th>\n      <th>284</th>\n      <th>285</th>\n      <th>286</th>\n      <th>287</th>\n      <th>288</th>\n      <th>289</th>\n      <th>290</th>\n      <th>291</th>\n      <th>292</th>\n      <th>293</th>\n      <th>294</th>\n      <th>295</th>\n      <th>296</th>\n      <th>297</th>\n      <th>298</th>\n      <th>299</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.098</td>\n      <td>2.165</td>\n      <td>0.681</td>\n      <td>-0.614</td>\n      <td>1.309</td>\n      <td>-0.455</td>\n      <td>-0.236</td>\n      <td>0.276</td>\n      <td>-2.246</td>\n      <td>1.825</td>\n      <td>-0.912</td>\n      <td>-0.107</td>\n      <td>0.305</td>\n      <td>0.102</td>\n      <td>0.826</td>\n      <td>0.417</td>\n      <td>0.177</td>\n      <td>-0.673</td>\n      <td>-0.503</td>\n      <td>1.864</td>\n      <td>0.410</td>\n      <td>-1.927</td>\n      <td>0.102</td>\n      <td>-0.931</td>\n      <td>1.763</td>\n      <td>1.449</td>\n      <td>-1.097</td>\n      <td>-0.686</td>\n      <td>-0.250</td>\n      <td>-1.859</td>\n      <td>1.125</td>\n      <td>1.009</td>\n      <td>-2.296</td>\n      <td>0.385</td>\n      <td>-0.876</td>\n      <td>1.528</td>\n      <td>-0.144</td>\n      <td>-1.078</td>\n      <td>...</td>\n      <td>-0.681</td>\n      <td>1.250</td>\n      <td>-0.565</td>\n      <td>-1.318</td>\n      <td>-0.923</td>\n      <td>0.075</td>\n      <td>-0.704</td>\n      <td>2.457</td>\n      <td>0.771</td>\n      <td>-0.460</td>\n      <td>0.569</td>\n      <td>-1.320</td>\n      <td>-1.516</td>\n      <td>-2.145</td>\n      <td>-1.120</td>\n      <td>0.156</td>\n      <td>0.820</td>\n      <td>-1.049</td>\n      <td>-1.125</td>\n      <td>0.484</td>\n      <td>0.617</td>\n      <td>1.253</td>\n      <td>1.248</td>\n      <td>0.504</td>\n      <td>-0.802</td>\n      <td>-0.896</td>\n      <td>-1.793</td>\n      <td>-0.284</td>\n      <td>-0.601</td>\n      <td>0.569</td>\n      <td>0.867</td>\n      <td>1.347</td>\n      <td>0.504</td>\n      <td>-0.649</td>\n      <td>0.672</td>\n      <td>-2.097</td>\n      <td>1.051</td>\n      <td>-0.414</td>\n      <td>1.038</td>\n      <td>-1.065</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.0</td>\n      <td>1.081</td>\n      <td>-0.973</td>\n      <td>-0.383</td>\n      <td>0.326</td>\n      <td>-0.428</td>\n      <td>0.317</td>\n      <td>1.172</td>\n      <td>0.352</td>\n      <td>0.004</td>\n      <td>-0.291</td>\n      <td>2.907</td>\n      <td>1.085</td>\n      <td>2.144</td>\n      <td>1.540</td>\n      <td>0.584</td>\n      <td>1.133</td>\n      <td>1.098</td>\n      <td>-0.237</td>\n      <td>-0.498</td>\n      <td>0.283</td>\n      <td>-1.100</td>\n      <td>-0.417</td>\n      <td>1.382</td>\n      <td>-0.515</td>\n      <td>-1.519</td>\n      <td>0.619</td>\n      <td>-0.128</td>\n      <td>0.866</td>\n      <td>-0.540</td>\n      <td>1.238</td>\n      <td>-0.227</td>\n      <td>0.269</td>\n      <td>-0.390</td>\n      <td>-2.721</td>\n      <td>1.659</td>\n      <td>0.106</td>\n      <td>-0.121</td>\n      <td>1.719</td>\n      <td>...</td>\n      <td>0.971</td>\n      <td>-1.489</td>\n      <td>0.530</td>\n      <td>0.917</td>\n      <td>-0.094</td>\n      <td>-1.407</td>\n      <td>0.887</td>\n      <td>-0.104</td>\n      <td>-0.583</td>\n      <td>1.267</td>\n      <td>-1.667</td>\n      <td>-2.771</td>\n      <td>-0.516</td>\n      <td>1.312</td>\n      <td>0.491</td>\n      <td>0.932</td>\n      <td>2.064</td>\n      <td>0.422</td>\n      <td>1.215</td>\n      <td>2.012</td>\n      <td>0.043</td>\n      <td>-0.307</td>\n      <td>-0.059</td>\n      <td>1.121</td>\n      <td>1.333</td>\n      <td>0.211</td>\n      <td>1.753</td>\n      <td>0.053</td>\n      <td>1.274</td>\n      <td>-0.612</td>\n      <td>-0.165</td>\n      <td>-1.695</td>\n      <td>-1.257</td>\n      <td>1.359</td>\n      <td>-0.808</td>\n      <td>-1.624</td>\n      <td>-0.458</td>\n      <td>-1.099</td>\n      <td>-0.936</td>\n      <td>0.973</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>1.0</td>\n      <td>-0.523</td>\n      <td>-0.089</td>\n      <td>-0.348</td>\n      <td>0.148</td>\n      <td>-0.022</td>\n      <td>0.404</td>\n      <td>-0.023</td>\n      <td>-0.172</td>\n      <td>0.137</td>\n      <td>0.183</td>\n      <td>0.459</td>\n      <td>0.478</td>\n      <td>-0.425</td>\n      <td>0.352</td>\n      <td>1.095</td>\n      <td>0.300</td>\n      <td>-1.044</td>\n      <td>0.270</td>\n      <td>-1.038</td>\n      <td>0.144</td>\n      <td>-1.658</td>\n      <td>-0.946</td>\n      <td>0.633</td>\n      <td>-0.772</td>\n      <td>1.786</td>\n      <td>0.136</td>\n      <td>-0.103</td>\n      <td>-1.223</td>\n      <td>2.273</td>\n      <td>0.055</td>\n      <td>-2.032</td>\n      <td>-0.452</td>\n      <td>0.064</td>\n      <td>0.924</td>\n      <td>-0.692</td>\n      <td>-0.067</td>\n      <td>-0.917</td>\n      <td>1.896</td>\n      <td>...</td>\n      <td>-0.540</td>\n      <td>-0.299</td>\n      <td>1.074</td>\n      <td>-0.748</td>\n      <td>1.086</td>\n      <td>-0.766</td>\n      <td>-0.931</td>\n      <td>0.432</td>\n      <td>1.345</td>\n      <td>-0.491</td>\n      <td>-1.602</td>\n      <td>-0.727</td>\n      <td>0.346</td>\n      <td>0.780</td>\n      <td>-0.527</td>\n      <td>-1.122</td>\n      <td>-0.208</td>\n      <td>-0.730</td>\n      <td>-0.302</td>\n      <td>2.535</td>\n      <td>-1.045</td>\n      <td>0.037</td>\n      <td>0.020</td>\n      <td>1.373</td>\n      <td>0.456</td>\n      <td>-0.277</td>\n      <td>1.381</td>\n      <td>1.843</td>\n      <td>0.749</td>\n      <td>0.202</td>\n      <td>0.013</td>\n      <td>0.263</td>\n      <td>-1.222</td>\n      <td>0.726</td>\n      <td>1.444</td>\n      <td>-1.165</td>\n      <td>-1.544</td>\n      <td>0.004</td>\n      <td>0.800</td>\n      <td>-1.211</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>1.0</td>\n      <td>0.067</td>\n      <td>-0.021</td>\n      <td>0.392</td>\n      <td>-1.637</td>\n      <td>-0.446</td>\n      <td>-0.725</td>\n      <td>-1.035</td>\n      <td>0.834</td>\n      <td>0.503</td>\n      <td>0.274</td>\n      <td>0.335</td>\n      <td>-1.148</td>\n      <td>0.067</td>\n      <td>-1.010</td>\n      <td>1.048</td>\n      <td>-1.442</td>\n      <td>0.210</td>\n      <td>0.836</td>\n      <td>-0.326</td>\n      <td>0.716</td>\n      <td>-0.764</td>\n      <td>0.248</td>\n      <td>-1.308</td>\n      <td>2.127</td>\n      <td>0.365</td>\n      <td>0.296</td>\n      <td>-0.808</td>\n      <td>1.854</td>\n      <td>0.118</td>\n      <td>0.380</td>\n      <td>0.999</td>\n      <td>-1.171</td>\n      <td>2.798</td>\n      <td>0.394</td>\n      <td>-1.048</td>\n      <td>1.078</td>\n      <td>0.401</td>\n      <td>-0.486</td>\n      <td>...</td>\n      <td>-0.083</td>\n      <td>-0.831</td>\n      <td>1.251</td>\n      <td>-0.206</td>\n      <td>-0.933</td>\n      <td>-1.215</td>\n      <td>0.281</td>\n      <td>0.512</td>\n      <td>-0.424</td>\n      <td>0.769</td>\n      <td>0.223</td>\n      <td>-0.710</td>\n      <td>2.725</td>\n      <td>0.176</td>\n      <td>0.845</td>\n      <td>-1.226</td>\n      <td>1.527</td>\n      <td>-1.701</td>\n      <td>0.597</td>\n      <td>0.150</td>\n      <td>1.864</td>\n      <td>0.322</td>\n      <td>-0.214</td>\n      <td>1.282</td>\n      <td>0.408</td>\n      <td>-0.910</td>\n      <td>1.020</td>\n      <td>-0.299</td>\n      <td>-1.574</td>\n      <td>-1.618</td>\n      <td>-0.404</td>\n      <td>0.640</td>\n      <td>-0.595</td>\n      <td>-0.966</td>\n      <td>0.900</td>\n      <td>0.467</td>\n      <td>-0.562</td>\n      <td>-0.254</td>\n      <td>-0.533</td>\n      <td>0.238</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>1.0</td>\n      <td>2.347</td>\n      <td>-0.831</td>\n      <td>0.511</td>\n      <td>-0.021</td>\n      <td>1.225</td>\n      <td>1.594</td>\n      <td>0.585</td>\n      <td>1.509</td>\n      <td>-0.012</td>\n      <td>2.198</td>\n      <td>0.190</td>\n      <td>0.453</td>\n      <td>0.494</td>\n      <td>1.478</td>\n      <td>-1.412</td>\n      <td>0.270</td>\n      <td>-1.312</td>\n      <td>-0.322</td>\n      <td>-0.688</td>\n      <td>-0.198</td>\n      <td>-0.285</td>\n      <td>1.042</td>\n      <td>-0.315</td>\n      <td>-0.478</td>\n      <td>0.024</td>\n      <td>-0.190</td>\n      <td>1.656</td>\n      <td>-0.469</td>\n      <td>-1.437</td>\n      <td>-0.581</td>\n      <td>-0.308</td>\n      <td>-0.837</td>\n      <td>-1.739</td>\n      <td>0.037</td>\n      <td>0.336</td>\n      <td>-1.102</td>\n      <td>2.371</td>\n      <td>0.554</td>\n      <td>...</td>\n      <td>-1.050</td>\n      <td>-0.347</td>\n      <td>0.904</td>\n      <td>-1.324</td>\n      <td>-0.849</td>\n      <td>3.432</td>\n      <td>0.222</td>\n      <td>0.416</td>\n      <td>0.174</td>\n      <td>-1.517</td>\n      <td>-0.337</td>\n      <td>0.055</td>\n      <td>-0.464</td>\n      <td>0.014</td>\n      <td>-1.073</td>\n      <td>0.325</td>\n      <td>-0.523</td>\n      <td>-0.692</td>\n      <td>0.190</td>\n      <td>-0.883</td>\n      <td>-1.830</td>\n      <td>1.408</td>\n      <td>2.319</td>\n      <td>1.704</td>\n      <td>-0.723</td>\n      <td>1.014</td>\n      <td>0.064</td>\n      <td>0.096</td>\n      <td>-0.775</td>\n      <td>1.845</td>\n      <td>0.898</td>\n      <td>0.134</td>\n      <td>2.415</td>\n      <td>-0.996</td>\n      <td>-1.006</td>\n      <td>1.378</td>\n      <td>1.246</td>\n      <td>1.478</td>\n      <td>0.428</td>\n      <td>0.253</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df_train['299'])","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x7fcfe8cd5828>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential  \nfrom keras.layers.core import Dense, Activation  \nfrom keras.layers.recurrent import LSTM","execution_count":5,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x = np.array(df_train.iloc[:,2:])\ntrain_t = np.array(df_train['target'])\n\nlength_of_sequences = 300\nin_out_neurons = 1\nhidden_neurons = 300\n\nmodel = Sequential()  \nmodel.add(LSTM(hidden_neurons, batch_input_shape=(None, length_of_sequences, in_out_neurons), return_sequences=False))  \nmodel.add(Dense(in_out_neurons))  \nmodel.add(Activation(\"linear\"))  \nmodel.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\")\nmodel.fit(train_x, train_t, batch_size=600, nb_epoch=15, validation_split=0.05) \n","execution_count":10,"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n","name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"Error when checking input: expected lstm_1_input to have 3 dimensions, but got array with shape (250, 300)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-478fac5280eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"linear\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mean_squared_error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rmsprop\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Error when checking input: expected lstm_1_input to have 3 dimensions, but got array with shape (250, 300)"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.ensemble import RandomForestClassifier\n# rfc = RandomForestClassifier(random_state=0)\n\n# rfc.fit(df_train.iloc[:,2:], df_train['target'])\n# pred = rfc.predict(df_test.iloc[:,1:])\n\n# submit_df = pd.DataFrame()\n# submit_df['id'] = df_test['id']\n# submit_df['target'] = pred\n# submit_df.to_csv(\"submission.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LSTM(nn.Module):\n    def __init__(self, seq_size, hidden_size, out_size):\n        super(LSTM, self).__init__()\n        self.xh = torch.nn.LSTM(seq_size, hidden_size)\n        self.hy = torch.nn.Linear(hidden_size, out_size)\n        self.hidden_size = hidden_size\n    \n    def __call__(self, xs):\n        h, self.hidden = self.xh(xs, self.hidden)\n        y = self.hy(h)\n        return y\n    \n    def reset(self):\n        self.hidden = (Variable(torch.zeros(1, 1, self.hidden_size)), Variable(torch.zeros(1, 1, self.hidden_size)))\n\nEPOCH_NUM = 300\nHIDDEN_SIZE = 5\nBATCH_ROW_SIZE = 100\nBATCH_COL_SIZE = 300\nN = len(df_train)\n\ntrain_x = np.array(df_train.iloc[:,2:])\ntrain_t = np.array(df_train['target'])\n\n# train_data = np.array([np.sin(i*2*np.pi/50) for i in range(50)]*10)\n# train_x, train_t = [], []\n# for i in range(0, len(train_data) - BATCH_COL_SIZE):\n#     train_x.append(train_data[i:i+BATCH_COL_SIZE])\n#     train_t.append(train_data[i+BATCH_COL_SIZE])\n# train_x = np.array(train_x, dtype=\"float32\")\n# train_t = np.array(train_t, dtype=\"float32\")\n\nmodel = LSTM(seq_size=BATCH_COL_SIZE, hidden_size=HIDDEN_SIZE, out_size=1)\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters())\n\nprint(\"Training ...\")\nst = datetime.datetime.now()\nfor epoch in range(EPOCH_NUM):\n    x, t = [], []\n    for i in range(BATCH_ROW_SIZE):\n        index = np.random.randint(0, N)\n        x.append(train_x[index])\n        t.append(train_t[index])\n    x = np.array(x, dtype=\"float64\")\n    t = np.array(t, dtype=\"float64\")\n    x = Variable(torch.from_numpy(x))\n    t = Variable(torch.from_numpy(t))\n    total_loss = 0\n    model.reset()\n    y = model(x)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}