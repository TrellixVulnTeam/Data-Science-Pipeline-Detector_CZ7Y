{"cells":[{"metadata":{"trusted":true},"cell_type":"markdown","source":"First of all, I have used ideas from this website:\n\nhttps://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/\n\nIn this kernel we are going to use lasso regression.\n\nhttps://en.wikipedia.org/wiki/Lasso_(statistics)"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Loading the packages\nimport numpy as np\nimport pandas as pd \nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import make_scorer \nfrom sklearn.model_selection import GridSearchCV\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading the training dataset\ndf_train = pd.read_csv(\"../input/train.csv\")","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df_train[\"target\"]\n# We exclude the target and id columns from the training dataset\ndf_train.pop(\"target\");\ndf_train.pop(\"id\")\ncolnames1 = df_train.columns","execution_count":19,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are going to standardize the explanatory variables by removing the mean and scaling to unit variance, this is mandatory for logistic regression. The standard score for the variable X is calculated as follows:\n\n$$ z= \\frac{X−\\mu}{s} $$\n \nWhere  μ  is the mean and s is the standard deviation."},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nscaler = StandardScaler()\nscaler.fit(df_train)\nX = scaler.transform(df_train)\ndf_train = pd.DataFrame(data = X, columns=colnames1)   # df_train is standardized ","execution_count":20,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this kernel:\n\nhttps://www.kaggle.com/ricardorios/random-forests-don-t-overfit\n\nWe have found the following variables that are related with the target variable: 33, 279, 272, 83, 237, 241, 91, 199, 216, 19, 65, 141, 70, 243, 137, 26, 90. We are going to use these variables to fit the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest_predictors = [\"33\", \"279\", \"272\", \n                           \"83\", \"237\", \"241\", \n                           \"91\", \"199\", \"216\", \n                           \"19\", \"65\", \"141\", \"70\", \"243\", \"137\", \"26\", \"90\"]\n\npredictors = random_forest_predictors\n\ndf_train = df_train[predictors]\n","execution_count":21,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to regularize the model fitted in: \n\nhttps://www.kaggle.com/ricardorios/logistic-regression-model-don-t-overfit\n\nWe are going to use [Lasso Regression](https://www.statisticshowto.datasciencecentral.com/lasso-regression/), one of the advantages of using this approach is that the model is sparse and we get the best predictors. Next, we are going to perform a grid search over the parameter C.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# We adapt code from this kernel: \n# https://www.kaggle.com/vincentlugat/logistic-regression-rfe\n\n# Find best hyperparameters (roc_auc)\nrandom_state = 0\nclf = LogisticRegression(random_state = random_state)\nparam_grid = {'class_weight' : ['balanced'], \n              'penalty' : ['l1'],  \n              'C' : [0.0001, 0.0005, 0.001, \n                     0.005, 0.01, 0.05, 0.1, 0.5, 1, \n                     10, 100, 1000, 1500, 2000, 2500, \n                     2600, 2700, 2800, 2900, 3000, 3100, 3200  \n                     ], \n              'max_iter' : [100, 1000, 2000, 5000, 10000] }\n\n# Make an roc_auc scoring object using make_scorer()\nscorer = make_scorer(roc_auc_score)\n\ngrid = GridSearchCV(estimator = clf, param_grid = param_grid , \n                    scoring = scorer, verbose = 1, cv=20,\n                    n_jobs = -1)\n\nX = df_train.values\n\ngrid.fit(X,y)\n\nprint(\"Best Score:\" + str(grid.best_score_))\nprint(\"Best Parameters: \" + str(grid.best_params_))\n\nbest_parameters = grid.best_params_","execution_count":24,"outputs":[{"output_type":"stream","text":"Fitting 20 folds for each of 110 candidates, totalling 2200 fits\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n","name":"stderr"},{"output_type":"stream","text":"Best Score:0.7501500000000001\nBest Parameters: {'C': 0.1, 'class_weight': 'balanced', 'max_iter': 100, 'penalty': 'l1'}\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=-1)]: Done 2200 out of 2200 | elapsed:    3.0s finished\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We get the best model \nbest_clf = grid.best_estimator_\nprint(best_clf)","execution_count":25,"outputs":[{"output_type":"stream","text":"LogisticRegression(C=0.1, class_weight='balanced', dual=False,\n          fit_intercept=True, intercept_scaling=1, max_iter=100,\n          multi_class='warn', n_jobs=None, penalty='l1', random_state=0,\n          solver='warn', tol=0.0001, verbose=0, warm_start=False)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"The best model is obtained with C=0.1, next we are going to fit the model with the whole training dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LogisticRegression(C=0.1, class_weight='balanced', dual=False,\n          fit_intercept=True, intercept_scaling=1, max_iter=100,\n          multi_class='warn', n_jobs=None, penalty='l1', random_state=0,\n          solver='warn', tol=0.0001, verbose=0, warm_start=False);\n\nmodel.fit(X, y);\n","execution_count":26,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The coefficients of the model are shown as follows. "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model.coef_)","execution_count":27,"outputs":[{"output_type":"stream","text":"[[ 0.67397473  0.          0.04381562  0.         -0.13063115  0.\n  -0.22047058  0.2367313   0.          0.          0.48645642  0.\n   0.          0.          0.          0.         -0.08626306]]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"There are only 7 variables with coefficients different than zero, the resulting model is more parsimonious and, in consequence less prone to overfitting. Finally, we will generate the file submission.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv(\"../input/test.csv\")\ndf_test.pop(\"id\");\nX = df_test \nX = scaler.transform(X)\ndf_test = pd.DataFrame(data = X, columns=colnames1)   # df_train is standardized \ndf_test = df_test[predictors]\n\n\nX = df_test.values\ny_pred = model.predict_proba(X)\ny_pred = y_pred[:,1]    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submit prediction\nsmpsb_df = pd.read_csv(\"../input/sample_submission.csv\")\nsmpsb_df[\"target\"] = y_pred\nsmpsb_df.to_csv(\"logistic_regression_l2_v1.csv\", index=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}