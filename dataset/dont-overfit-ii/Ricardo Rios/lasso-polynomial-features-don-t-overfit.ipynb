{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"First of all, I have used ideas from this website:\n\nhttps://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/\n\nIn this kernel we are going to use lasso regression.\n\nhttps://en.wikipedia.org/wiki/Lasso_(statistics)"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Loading the packages\nimport numpy as np\nimport pandas as pd \nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import make_scorer \nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import PolynomialFeatures\n\nimport warnings\nwarnings.filterwarnings('ignore')\n","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading the training dataset\ndf_train = pd.read_csv(\"../input/train.csv\")","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df_train[\"target\"]\n# We exclude the target and id columns from the training dataset\ndf_train.pop(\"target\");\ndf_train.pop(\"id\")\ncolnames1 = df_train.columns","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are going to standardize the explanatory variables by removing the mean and scaling to unit variance, this is mandatory for logistic regression. The standard score for the variable X is calculated as follows:\n\n$$ z=\\frac{Xâˆ’\\mu}{s} $$\n \nWhere $\\mu$ is the mean and s is the standard deviation."},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(df_train)\nX = scaler.transform(df_train)\ndf_train = pd.DataFrame(data = X, columns=colnames1)   # df_train is standardized ","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this kernel:\n\nhttps://www.kaggle.com/ricardorios/random-forests-don-t-overfit\n\nWe have found the following variables that are related with the target variable: 33, 279, 272, 83, 237, 241, 91, 199, 216, 19, 65, 141, 70, 243, 137, 26, 90 but from this kernel: \n\nhttps://www.kaggle.com/ricardorios/logistic-regression-with-lasso-don-t-overfit\n\nThe best predictors are 33, 272, 237, 91, 199, 65, and 90, this result was obtained using Lasso Regression which can be seen as a method of variable selection.\n\nhttps://courses.cs.washington.edu/courses/cse446/13sp/slides/lasso-annotated.pdf"},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest_predictors = [\"33\", \"279\", \"272\", \n                           \"83\", \"237\", \"241\", \n                           \"91\", \"199\", \"216\", \n                           \"19\", \"65\", \"141\", \"70\", \"243\", \"137\", \"26\", \"90\"]\n\nselected_predictors = [0, 2, 4, 6, 7, 10, 16]\nnew_predictors = []\n\nfor i in selected_predictors: \n    new_predictors.append(random_forest_predictors[i])\n\ndf_train = df_train[new_predictors]\n","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"We are going to perform polynomial features, this is a kind of feature engineering. We suggest you to read the following: \n\nhttps://towardsdatascience.com/feature-engineering-what-powers-machine-learning-93ab191bcc2d"},{"metadata":{"trusted":true},"cell_type":"code","source":"poly = PolynomialFeatures(2, interaction_only=True)\npoly.fit(df_train)\nX = poly.transform(df_train)","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to regularize the model we are going to use [Lasso Regression](https://www.statisticshowto.datasciencecentral.com/lasso-regression/), one of the advantages of using this approach is that the model is sparse and we get the best predictors. Next, we are going to perform a grid search over the parameter C.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# We adapt code from this kernel: \n# https://www.kaggle.com/vincentlugat/logistic-regression-rfe\n\n# Find best hyperparameters (roc_auc)\nrandom_state = 0\nclf = LogisticRegression(random_state = random_state)\nparam_grid = {'class_weight' : ['balanced'], \n              'penalty' : ['l1'],  \n              'C' : [0.0001, 0.0005, 0.001, \n                     0.005, 0.01, 0.05, 0.1, 0.5, 1, \n                     10, 100, 1000, 1500, 2000 \n                     ], \n              'max_iter' : [100, 1000] }\n\n# Make an roc_auc scoring object using make_scorer()\nscorer = make_scorer(roc_auc_score)\n\ngrid = GridSearchCV(estimator = clf, param_grid = param_grid , \n                    scoring = scorer, verbose = 10, cv=20,\n                    n_jobs = -1)\n\n\n\ngrid.fit(X,y)\n\nprint(\"Best Score:\" + str(grid.best_score_))\nprint(\"Best Parameters: \" + str(grid.best_params_))\n\nbest_parameters = grid.best_params_","execution_count":7,"outputs":[{"output_type":"stream","text":"Fitting 20 folds for each of 28 candidates, totalling 560 fits\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    2.5s\n[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    2.6s\n[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    2.7s\n[Parallel(n_jobs=-1)]: Batch computation too fast (0.1864s.) Setting batch_size=2.\n[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    2.8s\n[Parallel(n_jobs=-1)]: Batch computation too fast (0.0701s.) Setting batch_size=10.\n[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:    3.0s\n[Parallel(n_jobs=-1)]: Done 129 tasks      | elapsed:    3.4s\n[Parallel(n_jobs=-1)]: Done 239 tasks      | elapsed:    3.9s\n[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:    4.3s\n","name":"stderr"},{"output_type":"stream","text":"Best Score:0.7441500000000001\nBest Parameters: {'C': 0.1, 'class_weight': 'balanced', 'max_iter': 100, 'penalty': 'l1'}\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=-1)]: Done 479 tasks      | elapsed:    4.9s\n[Parallel(n_jobs=-1)]: Done 560 out of 560 | elapsed:    5.1s finished\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We get the best model \nbest_clf = grid.best_estimator_\nprint(best_clf)","execution_count":8,"outputs":[{"output_type":"stream","text":"LogisticRegression(C=0.1, class_weight='balanced', dual=False,\n          fit_intercept=True, intercept_scaling=1, max_iter=100,\n          multi_class='warn', n_jobs=None, penalty='l1', random_state=0,\n          solver='warn', tol=0.0001, verbose=0, warm_start=False)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"The best model is obtained with C=0.1, next we are going to fit the model with the whole training dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LogisticRegression(C=0.1, class_weight='balanced', dual=False,\n          fit_intercept=True, intercept_scaling=1, max_iter=100,\n          multi_class='warn', n_jobs=None, penalty='l1', random_state=0,\n          solver='warn', tol=0.0001, verbose=0, warm_start=False);\n\nmodel.fit(X, y);","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"\n\nThe coefficients of the model are shown as follows.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model.coef_)","execution_count":10,"outputs":[{"output_type":"stream","text":"[[ 0.          0.66988478  0.04135936 -0.1308607  -0.21900114  0.23835914\n   0.47937183 -0.07259055  0.          0.          0.          0.\n   0.          0.          0.          0.          0.          0.\n   0.          0.          0.          0.          0.          0.\n   0.00232618  0.          0.11266933 -0.0468331   0.        ]]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Finally, we will generate the file submission."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv(\"../input/test.csv\")\ndf_test.pop(\"id\");\nX = df_test \nX = scaler.transform(X)\ndf_test = pd.DataFrame(data = X, columns=colnames1)   # df_train is standardized \ndf_test = df_test[new_predictors]\n\nX = poly.transform(df_test)\ny_pred = model.predict_proba(X)\ny_pred = y_pred[:,1]    ","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submit prediction\nsmpsb_df = pd.read_csv(\"../input/sample_submission.csv\")\nsmpsb_df[\"target\"] = y_pred\nsmpsb_df.to_csv(\"logistic_regression_l2_v2.csv\", index=None)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}