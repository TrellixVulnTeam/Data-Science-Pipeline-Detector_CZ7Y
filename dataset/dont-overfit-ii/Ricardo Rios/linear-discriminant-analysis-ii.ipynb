{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"First of all, we are going to use the statistical model called Fisher's linear discriminant analysis  which finds linear combination of features that separates two or more classes. One advantage of using statistical models is that are less prone to overfit small dataset. The following web page is a good reference in order to understand how linear discriminant analysis works internally. \n\nhttps://web.stanford.edu/class/stats202/content/lec9.pdf"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Loading the packages\nimport numpy as np\nimport pandas as pd \nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nimport seaborn as sns \nfrom sklearn.cluster import KMeans\nfrom sklearn.feature_selection import RFE\nsns.set(style=\"ticks\", color_codes=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading the training dataset\ndf_train = pd.read_csv(\"../input/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df_train[\"target\"]\n# We exclude the target and id columns from the training dataset\ndf_train.pop(\"target\");\ndf_train.pop(\"id\")\ncolnames1 = df_train.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Linear discriminat analysis is sensitive to variables that are not standardized, that is why we are going to standardize the explanatory variables by removing the mean and scaling to unit variance. The standard score for the variable X is calculated as follows: \n\n$$ z = (X-\\mu) / s $$\n\nWhere $\\mu$ is the mean and s is the standard deviation. Moreover, one of the hypothesis of linear discriminant analysis is that the predictors must have the same variance.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(df_train)\nX = scaler.transform(df_train)\ndf_train = pd.DataFrame(data = X, columns=colnames1)   # df_train is standardized ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another hyphothesis of linear discriminant analysis is that the explanatory variables must follow a normal distribution on condition of the classes, hence we will find the variables that meet this criterion with the following code: "},{"metadata":{"trusted":true},"cell_type":"code","source":"normal_vars = [] # This list will contain the explanatory variables \n                 # that follow a normal distribution\n\ncounter = 0 \nfor c in df_train.columns:\n    \n    df = df_train[c]\n    index_0 = y == 0.0\n    index_1 = y == 1.0 \n    df_0 = df[index_0]\n    df_1 = df[index_1]\n    \n    if stats.kstest(df_0, 'norm')[1] >= 0.5 and stats.kstest(df_1, 'norm')[1] >= 0.5:        \n        print(\"Variable: {} follows a normal distribution on condition of the classes\".format(c))                \n        counter += 1\n        normal_vars.append(c)\n        \n        \nprint(\"Number of variables that follow a normal distribution {}\".format(counter))\n        \n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\"In the cell above we have used the kolmogorov-smirnov test which is a nonparametric test that can be used to compare a sample with a reference probability distribution\" [1].\n\nIn our case we have used the kolmogorov-smirnov test to prove the following  hypothesis: \n\n$H_o$ : The distribution of the explanatory variable follows a normal distribution on condition of the classes\n\n$H_1$ : The distribution of the explanatory variable does not follow a normal distribution on condition of the classes \n\nIf you do not know about hypothesis testing we suggest you to read the following resources from Khan Academy:\n\n[Sampling distribution](https://www.khanacademy.org/math/statistics-probability/sampling-distributions-library)\n\n[Confidence interval](https://www.khanacademy.org/math/statistics-probability/confidence-intervals-one-sample)\n\n[Hypothesis testing](https://www.khanacademy.org/math/statistics-probability/significance-tests-one-sample)\n\nIf we get a p-value greater than or equal to 0.5 in both distributions (1's and 0's) we do not reject the $H_o$ hypothesis, from the cell above there are 129 explanatory variables that meet this criterion and therefore follow a normal distribution. In the next cell we are going to plot some of these variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_hist(list_var):\n    \n    for l in list_var:\n        df = df_train[l]\n        index_0 = y == 0.0\n        index_1 = y == 1.0 \n        df_0 = df[index_0]\n        df_1 = df[index_1]\n\n        \n        n, bins, patches = plt.hist(x=df_0, bins='auto', color='#0504aa',\n                            alpha=0.7, rwidth=0.85); \n        plt.grid(axis='y', alpha=0.75)\n        plt.xlabel('Value')\n        plt.title('Histogram of the variable ' + l + ' on condition of the class 0 ')\n        plt.show()\n        \n        \n        n, bins, patches = plt.hist(x=df_1, bins='auto', color='#0504aa',\n                            alpha=0.7, rwidth=0.85); \n        plt.grid(axis='y', alpha=0.75)\n        plt.xlabel('Value')\n        plt.title('Histogram of the variable ' + l + ' on condition of the class 1 ')\n        plt.show()\n        \n        \nlist_var = [\"3\", \"5\", \"6\"]     \n\nplot_hist(list_var)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"From the results above, we have 129 possible predictors and 250 observations, with this amount of data it is highly likely to include irrelevant variables in the model, moreover linear discriminant analysis will be affected by the curse of dimensionality. We suggest you read the following article in order to understand the curse of dimensionality. \n\n[The curse of dimensionality](https://medium.com/@paritosh_30025/curse-of-dimensionality-f4edb3efa6ec)\n\nThere are two ways to prevent the curse of dimensionality which are shown as follows: \n\n* Feature selection: Selecting important features which are relevant to model (it avoids the curse of dimensionality)\n* Feature extraction: Transformation of high dimensional space into lower dimensional space by using various methods such as PCA, TSVD, T-SNE etc\n\nIn this kernel we will use feature selection but before to do that we are going to explore the explanatory variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_train[normal_vars]\nX = df_train.values\ncolnames2 = df_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictors = colnames2\n\ndef calculate_cor_mat(predictors, X):\n    X = X[predictors]\n    X = X.values\n    correlation_matrix = np.corrcoef(X.T)\n    print(correlation_matrix)\n        \ncalculate_cor_mat(predictors, df_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_pair_plot(predictors, X):\n    X = X.loc[:, predictors]\n    g = sns.pairplot(X) \n\nplot_pair_plot(predictors[0:10], df_train) # we select only a \n                                           # subset of the predictors\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above it seems that the predictors are uncorrelated each other. In this kernel: \n\nhttps://www.kaggle.com/ricardorios/random-forests-don-t-overfit\n\nWe have found the following variables that are related with the target variable: 33, 279, 272, 83, 237, 241, 91, 199, 216, 19, 65, 141, 70, 243, 137, 26, 90. From these variables we will find the ones that meet the previously mentioned criterion: the explanatory variables must follow a normal distribution on condition of the classes.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nrandom_forest_predictos = [\"33\", \"279\", \"272\", \n                           \"83\", \"237\", \"241\", \n                           \"91\", \"199\", \"216\", \n                           \"19\", \"65\", \"141\", \"70\", \"243\", \"137\", \"26\", \"90\"]\n\npredictors = list(set(predictors) & set(random_forest_predictos))\nprint(predictors)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Only one variable meets the criterion, with one variable it is possible that the model will be too simple."},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndef five_num(X):\n    \n    quartiles = np.percentile(X, [25, 50, 75])\n    data_min, data_max = X.min(), X.max()\n    print(\"Minimum: {}\".format(data_min))\n    print(\"Q1: {}\".format(quartiles[0]))\n    print(\"Median: {}\".format(quartiles[1]))\n    print(\"Q3: {}\".format(quartiles[2]))\n    print(\"Maximum: {}\".format(data_max))    \n\n\ndef fit_discriminant(predictors, X):\n    \n    X = X[predictors]\n    X = X.values \n    \n    skf = StratifiedKFold(n_splits=10)\n    skf.get_n_splits(X, y)\n    \n    \n    train_auc = []\n    valid_auc = []\n    \n    for train_index, test_index in skf.split(X, y):\n        \n        model = LinearDiscriminantAnalysis(solver='eigen', shrinkage='auto')\n        model.fit(X[train_index], y[train_index])    \n        \n        y_train = y[train_index]\n        y_test = y[test_index]\n    \n        y_train_predict = model.predict_proba(X[train_index])\n        y_train_predict = y_train_predict[:,1]\n        y_test_predict = model.predict_proba(X[test_index], )\n        y_test_predict = y_test_predict[:,1]           \n        \n        train_auc.append(roc_auc_score(y_train, y_train_predict))\n        valid_auc.append(roc_auc_score(y_test, y_test_predict))\n        \n    n_bins = 5\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, tight_layout=True);\n    ax1.hist(train_auc, bins=n_bins);\n    ax1.set_title(\"Histogram of AUC training\")\n    ax2.hist(valid_auc, bins=n_bins);\n    ax2.set_title(\"Histogram of AUC validation\")  \n    \n    print(\"Five numbers Training AUC\\n\")\n    five_num(np.array(train_auc))\n    print(\"\\nFive numbers Valid AUC\\n\")\n    five_num(np.array(valid_auc))\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fit_discriminant(predictors, df_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We fit the model with the whole training dataset\nmodel = LinearDiscriminantAnalysis(solver='eigen', shrinkage='auto')\nmodel.fit(df_train[predictors], y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we will send the submission."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv(\"../input/test.csv\")\ndf_test.pop(\"id\");\nX = df_test \nX = scaler.transform(X)\ndf_test = pd.DataFrame(data = X, columns=colnames1)   # df_train is standardized \nX = df_test[predictors]\ndel df_test\ny_pred = model.predict_proba(X)\ny_pred = y_pred[:,1]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submit prediction\nsmpsb_df = pd.read_csv(\"../input/sample_submission.csv\")\nsmpsb_df[\"target\"] = y_pred\nsmpsb_df.to_csv(\"discrimant_analysisii.csv\", index=None)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This approach is not suitable, in next kernels we are going to explore quadratic discrimant analysis."},{"metadata":{},"cell_type":"markdown","source":"## References \n\n[1] https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}