{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"First of all, we are going to use the statistical model called quadratic discriminant analysis  which finds linear combination of features that separates two or more classes. This method is more general than linear discriminant analysis. The following web page is a good reference in order to understand how quadratic discriminant analysis works internally. \n\nhttps://web.stanford.edu/class/stats202/content/lec9.pdf"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Loading the packages\nimport numpy as np\nimport pandas as pd \nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nimport seaborn as sns \nfrom sklearn.cluster import KMeans\nfrom sklearn.feature_selection import RFE\nsns.set(style=\"ticks\", color_codes=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading the training dataset\ndf_train = pd.read_csv(\"../input/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df_train[\"target\"]\n# We exclude the target and id columns from the training dataset\ndf_train.pop(\"target\");\ndf_train.pop(\"id\")\ncolnames1 = df_train.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Quadratic discriminant analysis is sensitive to variables that are not standardized, that is why we are going to standardize the explanatory variables by removing the mean and scaling to unit variance. The standard score for the variable X is calculated as follows: \n\n$$ z = (X-\\mu) / s $$\n\nWhere $\\mu$ is the mean and s is the standard deviation. Moreover, one of the hypothesis of linear discriminant analysis is that the predictors must have the same variance.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(df_train)\nX = scaler.transform(df_train)\ndf_train = pd.DataFrame(data = X, columns=colnames1)   # df_train is standardized ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this kernel: \n\nhttps://www.kaggle.com/ricardorios/random-forests-don-t-overfit\n\nWe have found the following variables that are related with the target variable: 33, 279, 272, 83, 237, 241, 91, 199, 216, 19, 65, 141, 70, 243, 137, 26, 90. We are going to use these variables to fit the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest_predictors = [\"33\", \"279\", \"272\", \n                           \"83\", \"237\", \"241\", \n                           \"91\", \"199\", \"216\", \n                           \"19\", \"65\", \"141\", \"70\", \"243\", \"137\", \"26\", \"90\"]\n\npredictors = random_forest_predictors\nprint(predictors)\n\n\ndef five_num(X):\n    \n    quartiles = np.percentile(X, [25, 50, 75])\n    data_min, data_max = X.min(), X.max()\n    print(\"Minimum: {}\".format(data_min))\n    print(\"Q1: {}\".format(quartiles[0]))\n    print(\"Median: {}\".format(quartiles[1]))\n    print(\"Q3: {}\".format(quartiles[2]))\n    print(\"Maximum: {}\".format(data_max))    \n\n\ndef fit_discriminant(predictors, X):\n    \n    X = X[predictors]\n    X = X.values \n    \n    skf = StratifiedKFold(n_splits=10)\n    skf.get_n_splits(X, y)\n    \n    \n    train_auc = []\n    valid_auc = []\n    \n    for train_index, test_index in skf.split(X, y):\n        \n        model = QuadraticDiscriminantAnalysis()\n        model.fit(X[train_index], y[train_index])    \n        \n        y_train = y[train_index]\n        y_test = y[test_index]\n    \n        y_train_predict = model.predict_proba(X[train_index])\n        y_train_predict = y_train_predict[:,1]\n        y_test_predict = model.predict_proba(X[test_index], )\n        y_test_predict = y_test_predict[:,1]           \n        \n        train_auc.append(roc_auc_score(y_train, y_train_predict))\n        valid_auc.append(roc_auc_score(y_test, y_test_predict))\n        \n    n_bins = 5\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, tight_layout=True);\n    ax1.hist(train_auc, bins=n_bins);\n    ax1.set_title(\"Histogram of AUC training\")\n    ax2.hist(valid_auc, bins=n_bins);\n    ax2.set_title(\"Histogram of AUC validation\")  \n    \n    print(\"Five numbers Training AUC\\n\")\n    five_num(np.array(train_auc))\n    print(\"\\nFive numbers Valid AUC\\n\")\n    five_num(np.array(valid_auc))\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fit_discriminant(predictors, df_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that this model exhibits overfitting. In order to solve the overfitting problem we are going to start with the two most important variables and then we will continue to increase progressively until we find a good model."},{"metadata":{"trusted":true},"cell_type":"code","source":"predictors = ['33', '279']\nfit_discriminant(predictors, df_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictors = ['33', '279',  '272']\nfit_discriminant(predictors, df_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictors = ['33', '279',  '272', '83']\nfit_discriminant(predictors, df_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictors = ['33', '279',  '272', '83', '237']\nfit_discriminant(predictors, df_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictors = ['33', '279',  '272', '83', '237', '241']\nfit_discriminant(predictors, df_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best model has the following explanatory variables: 33, 279, and 272."},{"metadata":{"trusted":true},"cell_type":"code","source":"predictors = ['33', '279',  '272']\n\n# We fit the model with the whole training dataset\nmodel = QuadraticDiscriminantAnalysis()\nmodel.fit(df_train[predictors], y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we will send the submission."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv(\"../input/test.csv\")\ndf_test.pop(\"id\");\nX = df_test \nX = scaler.transform(X)\ndf_test = pd.DataFrame(data = X, columns=colnames1)   # df_train is standardized \nX = df_test[predictors]\ndel df_test\ny_pred = model.predict_proba(X)\ny_pred = y_pred[:,1]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submit prediction\nsmpsb_df = pd.read_csv(\"../input/sample_submission.csv\")\nsmpsb_df[\"target\"] = y_pred\nsmpsb_df.to_csv(\"quadratic_discrimant_analysis.csv\", index=None)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## References \n\n[1] https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}