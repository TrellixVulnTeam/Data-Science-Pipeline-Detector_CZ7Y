{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"We are going to improve the result from the decision tree proposed in the following kaggle kernel: \n\nhttps://www.kaggle.com/ricardorios/decision-trees-don-t-overfit\n\nIn this kernel the decision tree was built with default parameters and it turned out to be an overfitting model, one important aspect is the way how the decision tree was built which uses [mathematical optimization](https://en.wikipedia.org/wiki/Mathematical_optimization). If you have time you can review the following web page:\n\n[How decision trees work internally](https://medium.com/cracking-the-data-science-interview/decision-trees-how-to-optimize-my-decision-making-process-e1f327999c7a)\n\n\nIn order to improve the model, we are going to consider the parameter max_depth which controls the complexity of the model, we start with max_depth = 1 (a simpler model) and increase this parameter until find a good model. "},{"metadata":{"trusted":true,"_uuid":"f662a7dd5032e493c65fb4717c5dcba1ceab0984"},"cell_type":"code","source":"# Loading the packages\nimport numpy as np\nimport pandas as pd \nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.tree import DecisionTreeClassifier\n#from sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import StratifiedKFold\nimport matplotlib.pyplot as plt\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Loading the training dataset\ndf_train = pd.read_csv(\"../input/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"81217ca2ddb56711fe02dd1826eb84b80b2055c7"},"cell_type":"code","source":"y = df_train[\"target\"]\n# We exclude the target and id columns from the training dataset\ndf_train.pop(\"target\");\ndf_train.pop(\"id\")\nX = df_train \ndel df_train\nX = X.values # Converting pandas dataframe to numpy array \ny = y.values # Converting pandas series to numpy array \n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa7bb3dd8927afbb78ab314d12fbaac8a5b56cea"},"cell_type":"markdown","source":"In order to perform our analysis, we take the following facts into account. \n\n\"Typically, given these considerations, one performs k-fold cross-validation using k = 5 or k = 10, as these values have been shown empirically to yield test error rate estimates that suffer neither from excessively high bias nor from very high variance\".[1]\n\nhttps://machinelearningmastery.com/k-fold-cross-validation/\n\nWe are going to use stratified cross validation the reason for that is \"Stratification is a technique where we rearrange the data in a way that each fold has a good representation of the whole dataset. It forces each fold to have at least m instances of each class. This approach ensures that one class of data is not overrepresented especially when the target variable is unbalanced\". [2]\n\nhttps://medium.com/datadriveninvestor/k-fold-and-other-cross-validation-techniques-6c03a2563f1e\n"},{"metadata":{"trusted":true,"_uuid":"31bcc96cdadfd47e20877e807ebba4412fab4598"},"cell_type":"code","source":"skf = StratifiedKFold(n_splits=10)\nskf.get_n_splits(X, y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c3fe6c2ba4c42ccf17be5a077b677a4e0f8b1536"},"cell_type":"markdown","source":"We are going to use the following values for the max_depth argument: 1, 2, and 3."},{"metadata":{"trusted":true,"_uuid":"062bd88641f7f3470436704c9f14515557395f7a"},"cell_type":"code","source":"def fit_decision_tree(max_depth=1, nbins=5):\n    train_auc = []\n    test_auc = []\n    \n    for train_index, test_index in skf.split(X, y):\n        model = DecisionTreeClassifier(max_depth=max_depth)\n        model.fit(X[train_index], y[train_index])\n        y_train = y[train_index]\n        y_test = y[test_index]\n    \n        y_train_predict = model.predict_proba(X[train_index])\n        y_train_predict = y_train_predict[:,1]\n        y_test_predict = model.predict_proba(X[test_index], )\n        y_test_predict = y_test_predict[:,1]        \n        train_auc.append(roc_auc_score(y_train, y_train_predict))\n        test_auc.append(roc_auc_score(y_test, y_test_predict))\n        \n    n_bins = 5\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, tight_layout=True);\n\n    ax1.hist(train_auc, bins=n_bins);\n    ax1.set_title(\"Histogram of AUC training\")\n    ax2.hist(test_auc, bins=n_bins);\n    ax2.set_title(\"Histogram of AUC validation\")        \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc8297004b2504dd356a19ec1b8c1a761f797f03"},"cell_type":"code","source":"fit_decision_tree(1, 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f3ed81c8200c8ef8ff9592f2ffff0028866b98bf"},"cell_type":"code","source":"fit_decision_tree(2, 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e9d182f15eeef83b32bb85d93870e99e7309fca"},"cell_type":"code","source":"fit_decision_tree(3, 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b4a2a4dd1a51d7a8b5d592a64c0eaf832dcfb75"},"cell_type":"markdown","source":"From the plots above, it seems that with a value of max_depth equals to 3 or 2 it turns out overfitting models. On the other hand with a value of max_depth  equals to 1 we obtain a similar variation in the distribution of AUC values in the training and validation dataset that is why we are going to choose this model. Next, we are going to fit this model with the whole training dataset."},{"metadata":{"trusted":true,"_uuid":"153756b85510cfd78176733f9e56d2c294df15f2"},"cell_type":"code","source":"model = DecisionTreeClassifier(max_depth=1, class_weight='balanced')\nmodel.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"81a8a9aabacd3f2ef37894c694f4fec1a68d16a6"},"cell_type":"code","source":"df_test = pd.read_csv(\"../input/test.csv\")\ndf_test.pop(\"id\");\nX = df_test \ndel df_test\ny_pred = model.predict_proba(X)\ny_pred = y_pred[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56a35b228f58f93c3dd3d181d900b715c8d27b3d"},"cell_type":"code","source":"# submit prediction\nsmpsb_df = pd.read_csv(\"../input/sample_submission.csv\")\nsmpsb_df[\"target\"] = y_pred\nsmpsb_df.to_csv(\"decision_tree_improved.csv\", index=None)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9dfc6558e8570f0d935611e88bd62227ea1e7c2e"},"cell_type":"markdown","source":"## References: \n\n[1] https://machinelearningmastery.com/k-fold-cross-validation/\n\n[2] https://medium.com/datadriveninvestor/k-fold-and-other-cross-validation-techniques-6c03a2563f1e"},{"metadata":{"trusted":true,"_uuid":"229e330a46adebc1ac0f3fab36835b1dc665cecd"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}