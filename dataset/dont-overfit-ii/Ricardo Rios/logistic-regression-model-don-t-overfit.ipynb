{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"First of all, we are going to use the statistical model called logistic regression model. This model does not have many assumptions like linear discriminant analysis or quadratic discriminant analysis. \n\nhttps://towardsdatascience.com/building-a-logistic-regression-in-python-301d27367c24"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Loading the packages\nimport numpy as np\nimport pandas as pd \nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nimport seaborn as sns \nsns.set(style=\"ticks\", color_codes=True)\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading the training dataset\ndf_train = pd.read_csv(\"../input/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df_train[\"target\"]\n# We exclude the target and id columns from the training dataset\ndf_train.pop(\"target\");\ndf_train.pop(\"id\")\ncolnames1 = df_train.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are going to standardize the explanatory variables by removing the mean and scaling to unit variance. The standard score for the variable X is calculated as follows: \n\n$$ z = (X-\\mu) / s $$\n\nWhere $\\mu$ is the mean and s is the standard deviation. Moreover, one of the hypothesis of linear discriminant analysis is that the predictors must have the same variance.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(df_train)\nX = scaler.transform(df_train)\ndf_train = pd.DataFrame(data = X, columns=colnames1)   # df_train is standardized ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this kernel: \n\nhttps://www.kaggle.com/ricardorios/random-forests-don-t-overfit\n\nWe have found the following variables that are related with the target variable: 33, 279, 272, 83, 237, 241, 91, 199, 216, 19, 65, 141, 70, 243, 137, 26, 90. We are going to use these variables to fit the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest_predictors = [\"33\", \"279\", \"272\", \n                           \"83\", \"237\", \"241\", \n                           \"91\", \"199\", \"216\", \n                           \"19\", \"65\", \"141\", \"70\", \"243\", \"137\", \"26\", \"90\"]\n\npredictors = random_forest_predictors\nprint(predictors)\n\n\ndef five_num(X):\n    \n    quartiles = np.percentile(X, [25, 50, 75])\n    average = np.mean(X)\n    data_min, data_max = X.min(), X.max()\n    print(\"Minimum: {}\".format(data_min))\n    print(\"Q1: {}\".format(quartiles[0]))\n    print(\"Median: {}\".format(quartiles[1]))\n    print(\"Average: {}\".format(average))\n    print(\"Q3: {}\".format(quartiles[2]))\n    print(\"Maximum: {}\".format(data_max))    \n\n\ndef fit_logistic(predictors, X):\n    \n    X = X[predictors]\n    X = X.values \n    \n    skf = StratifiedKFold(n_splits=10)\n    skf.get_n_splits(X, y)\n    \n    \n    train_auc = []\n    valid_auc = []\n    \n    for train_index, test_index in skf.split(X, y):\n        \n        model = LogisticRegression(random_state=0, class_weight='balanced')\n        model.fit(X[train_index], y[train_index])    \n        \n        y_train = y[train_index]\n        y_test = y[test_index]\n    \n        y_train_predict = model.predict_proba(X[train_index])\n        y_train_predict = y_train_predict[:,1]\n        y_test_predict = model.predict_proba(X[test_index], )\n        y_test_predict = y_test_predict[:,1]           \n        \n        train_auc.append(roc_auc_score(y_train, y_train_predict))\n        valid_auc.append(roc_auc_score(y_test, y_test_predict))\n        \n    n_bins = 5\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, tight_layout=True);\n    ax1.hist(train_auc, bins=n_bins);\n    ax1.set_title(\"Histogram of AUC training\")\n    ax2.hist(valid_auc, bins=n_bins);\n    ax2.set_title(\"Histogram of AUC validation\")  \n    \n    print(\"Five numbers Training AUC\\n\")\n    five_num(np.array(train_auc))\n    print(\"\\nFive numbers Valid AUC\\n\")\n    five_num(np.array(valid_auc))    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fit_logistic(predictors, df_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that this model exhibits a greater variance in the validation dataset. In order to solve this problem we are going to start with the two most important variables and then we will continue to increase progressively until we find a good model, it is possible that this strategy will be not optimal (it is a [greedy algorithm](https://en.wikipedia.org/wiki/Greedy_algorithm))."},{"metadata":{"trusted":true},"cell_type":"code","source":"predictors = ['33', '279']\nfit_logistic(predictors, df_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictors = ['33', '279',  '272']\nfit_logistic(predictors, df_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictors = ['33', '279',  '272', '83']\nfit_logistic(predictors, df_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictors = ['33', '279',  '272', '83', '237']\nfit_logistic(predictors, df_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictors = ['33', '279',  '272', '83', '237', '241']\nfit_logistic(predictors, df_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictors = ['33', '279',  '272', '83', '237', '241', '91']\nfit_logistic(predictors, df_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictors = ['33', '279',  '272', '83', '237', '241', '91', '199']\nfit_logistic(predictors, df_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictors = ['33', '279',  '272', '83', '237', '241', '91', '199', '216']\nfit_logistic(predictors, df_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictors = ['33', '279',  '272', '83', '237', '241', '91', '199', '216', '19']\nfit_logistic(predictors, df_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictors = ['33', '279',  '272', '83', '237', '241', '91', '199', '216', '19', '65']\nfit_logistic(predictors, df_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictors = ['33', '279',  '272', '83', '237', '241', '91', '199', '216', '19', '65', '141']\nfit_logistic(predictors, df_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are two possible models: \n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Model1\")\npredictors = ['33', '279',  '272', '83', '237', '241', '91', '199', '216', '19', '65']\nprint(predictors)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Model2\")\npredictors = ['33', '279',  '272', '83', '237', '241', '91', '199', '216', '19', '65', '141']\nprint(predictors)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The performance of the above models are similar, for the principle of [Occam's Razor](https://en.wikipedia.org/wiki/Occam%27s_razor) we choose the model with fewer parameters:\n\n['33', '279', '272', '83', '237', '241', '91', '199', '216', '19', '65']"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictors = ['33', '279', '272', '83', '237', '241', '91', '199', '216', '19', '65']\nprint(predictors)\n# We fit the model with the whole training dataset\nmodel = LogisticRegression(random_state=0, class_weight='balanced')\nmodel.fit(df_train[predictors], y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we will send the submission."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv(\"../input/test.csv\")\ndf_test.pop(\"id\");\nX = df_test \nX = scaler.transform(X)\ndf_test = pd.DataFrame(data = X, columns=colnames1)   # df_train is standardized \nX = df_test[predictors]\ndel df_test\ny_pred = model.predict_proba(X)\ny_pred = y_pred[:,1]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submit prediction\nsmpsb_df = pd.read_csv(\"../input/sample_submission.csv\")\nsmpsb_df[\"target\"] = y_pred\nsmpsb_df.to_csv(\"logistic_regression.csv\", index=None)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}