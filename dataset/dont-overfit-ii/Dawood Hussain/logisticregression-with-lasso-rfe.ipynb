{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Final ROC_AUC SCORE (Public score) = 0.841**\n\n* Basic EDA\n* RandomForestClassifer\n* LogisticRegression (+lasso)\n* Feature selection - RFE, PCA"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":1,"outputs":[{"output_type":"stream","text":"['test.csv', 'train.csv', 'sample_submission.csv']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Load the data into training set & test set"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')\nsamplesub_df = pd.read_csv('../input/sample_submission.csv')","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**BASIC EDA**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"   id  target      0      1      2  ...      295    296    297    298    299\n0   0     1.0 -0.098  2.165  0.681  ...   -2.097  1.051 -0.414  1.038 -1.065\n1   1     0.0  1.081 -0.973 -0.383  ...   -1.624 -0.458 -1.099 -0.936  0.973\n2   2     1.0 -0.523 -0.089 -0.348  ...   -1.165 -1.544  0.004  0.800 -1.211\n3   3     1.0  0.067 -0.021  0.392  ...    0.467 -0.562 -0.254 -0.533  0.238\n4   4     1.0  2.347 -0.831  0.511  ...    1.378  1.246  1.478  0.428  0.253\n\n[5 rows x 302 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n      <th>21</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n      <th>25</th>\n      <th>26</th>\n      <th>27</th>\n      <th>28</th>\n      <th>29</th>\n      <th>30</th>\n      <th>31</th>\n      <th>32</th>\n      <th>33</th>\n      <th>34</th>\n      <th>35</th>\n      <th>36</th>\n      <th>37</th>\n      <th>...</th>\n      <th>260</th>\n      <th>261</th>\n      <th>262</th>\n      <th>263</th>\n      <th>264</th>\n      <th>265</th>\n      <th>266</th>\n      <th>267</th>\n      <th>268</th>\n      <th>269</th>\n      <th>270</th>\n      <th>271</th>\n      <th>272</th>\n      <th>273</th>\n      <th>274</th>\n      <th>275</th>\n      <th>276</th>\n      <th>277</th>\n      <th>278</th>\n      <th>279</th>\n      <th>280</th>\n      <th>281</th>\n      <th>282</th>\n      <th>283</th>\n      <th>284</th>\n      <th>285</th>\n      <th>286</th>\n      <th>287</th>\n      <th>288</th>\n      <th>289</th>\n      <th>290</th>\n      <th>291</th>\n      <th>292</th>\n      <th>293</th>\n      <th>294</th>\n      <th>295</th>\n      <th>296</th>\n      <th>297</th>\n      <th>298</th>\n      <th>299</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.098</td>\n      <td>2.165</td>\n      <td>0.681</td>\n      <td>-0.614</td>\n      <td>1.309</td>\n      <td>-0.455</td>\n      <td>-0.236</td>\n      <td>0.276</td>\n      <td>-2.246</td>\n      <td>1.825</td>\n      <td>-0.912</td>\n      <td>-0.107</td>\n      <td>0.305</td>\n      <td>0.102</td>\n      <td>0.826</td>\n      <td>0.417</td>\n      <td>0.177</td>\n      <td>-0.673</td>\n      <td>-0.503</td>\n      <td>1.864</td>\n      <td>0.410</td>\n      <td>-1.927</td>\n      <td>0.102</td>\n      <td>-0.931</td>\n      <td>1.763</td>\n      <td>1.449</td>\n      <td>-1.097</td>\n      <td>-0.686</td>\n      <td>-0.250</td>\n      <td>-1.859</td>\n      <td>1.125</td>\n      <td>1.009</td>\n      <td>-2.296</td>\n      <td>0.385</td>\n      <td>-0.876</td>\n      <td>1.528</td>\n      <td>-0.144</td>\n      <td>-1.078</td>\n      <td>...</td>\n      <td>-0.681</td>\n      <td>1.250</td>\n      <td>-0.565</td>\n      <td>-1.318</td>\n      <td>-0.923</td>\n      <td>0.075</td>\n      <td>-0.704</td>\n      <td>2.457</td>\n      <td>0.771</td>\n      <td>-0.460</td>\n      <td>0.569</td>\n      <td>-1.320</td>\n      <td>-1.516</td>\n      <td>-2.145</td>\n      <td>-1.120</td>\n      <td>0.156</td>\n      <td>0.820</td>\n      <td>-1.049</td>\n      <td>-1.125</td>\n      <td>0.484</td>\n      <td>0.617</td>\n      <td>1.253</td>\n      <td>1.248</td>\n      <td>0.504</td>\n      <td>-0.802</td>\n      <td>-0.896</td>\n      <td>-1.793</td>\n      <td>-0.284</td>\n      <td>-0.601</td>\n      <td>0.569</td>\n      <td>0.867</td>\n      <td>1.347</td>\n      <td>0.504</td>\n      <td>-0.649</td>\n      <td>0.672</td>\n      <td>-2.097</td>\n      <td>1.051</td>\n      <td>-0.414</td>\n      <td>1.038</td>\n      <td>-1.065</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.0</td>\n      <td>1.081</td>\n      <td>-0.973</td>\n      <td>-0.383</td>\n      <td>0.326</td>\n      <td>-0.428</td>\n      <td>0.317</td>\n      <td>1.172</td>\n      <td>0.352</td>\n      <td>0.004</td>\n      <td>-0.291</td>\n      <td>2.907</td>\n      <td>1.085</td>\n      <td>2.144</td>\n      <td>1.540</td>\n      <td>0.584</td>\n      <td>1.133</td>\n      <td>1.098</td>\n      <td>-0.237</td>\n      <td>-0.498</td>\n      <td>0.283</td>\n      <td>-1.100</td>\n      <td>-0.417</td>\n      <td>1.382</td>\n      <td>-0.515</td>\n      <td>-1.519</td>\n      <td>0.619</td>\n      <td>-0.128</td>\n      <td>0.866</td>\n      <td>-0.540</td>\n      <td>1.238</td>\n      <td>-0.227</td>\n      <td>0.269</td>\n      <td>-0.390</td>\n      <td>-2.721</td>\n      <td>1.659</td>\n      <td>0.106</td>\n      <td>-0.121</td>\n      <td>1.719</td>\n      <td>...</td>\n      <td>0.971</td>\n      <td>-1.489</td>\n      <td>0.530</td>\n      <td>0.917</td>\n      <td>-0.094</td>\n      <td>-1.407</td>\n      <td>0.887</td>\n      <td>-0.104</td>\n      <td>-0.583</td>\n      <td>1.267</td>\n      <td>-1.667</td>\n      <td>-2.771</td>\n      <td>-0.516</td>\n      <td>1.312</td>\n      <td>0.491</td>\n      <td>0.932</td>\n      <td>2.064</td>\n      <td>0.422</td>\n      <td>1.215</td>\n      <td>2.012</td>\n      <td>0.043</td>\n      <td>-0.307</td>\n      <td>-0.059</td>\n      <td>1.121</td>\n      <td>1.333</td>\n      <td>0.211</td>\n      <td>1.753</td>\n      <td>0.053</td>\n      <td>1.274</td>\n      <td>-0.612</td>\n      <td>-0.165</td>\n      <td>-1.695</td>\n      <td>-1.257</td>\n      <td>1.359</td>\n      <td>-0.808</td>\n      <td>-1.624</td>\n      <td>-0.458</td>\n      <td>-1.099</td>\n      <td>-0.936</td>\n      <td>0.973</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>1.0</td>\n      <td>-0.523</td>\n      <td>-0.089</td>\n      <td>-0.348</td>\n      <td>0.148</td>\n      <td>-0.022</td>\n      <td>0.404</td>\n      <td>-0.023</td>\n      <td>-0.172</td>\n      <td>0.137</td>\n      <td>0.183</td>\n      <td>0.459</td>\n      <td>0.478</td>\n      <td>-0.425</td>\n      <td>0.352</td>\n      <td>1.095</td>\n      <td>0.300</td>\n      <td>-1.044</td>\n      <td>0.270</td>\n      <td>-1.038</td>\n      <td>0.144</td>\n      <td>-1.658</td>\n      <td>-0.946</td>\n      <td>0.633</td>\n      <td>-0.772</td>\n      <td>1.786</td>\n      <td>0.136</td>\n      <td>-0.103</td>\n      <td>-1.223</td>\n      <td>2.273</td>\n      <td>0.055</td>\n      <td>-2.032</td>\n      <td>-0.452</td>\n      <td>0.064</td>\n      <td>0.924</td>\n      <td>-0.692</td>\n      <td>-0.067</td>\n      <td>-0.917</td>\n      <td>1.896</td>\n      <td>...</td>\n      <td>-0.540</td>\n      <td>-0.299</td>\n      <td>1.074</td>\n      <td>-0.748</td>\n      <td>1.086</td>\n      <td>-0.766</td>\n      <td>-0.931</td>\n      <td>0.432</td>\n      <td>1.345</td>\n      <td>-0.491</td>\n      <td>-1.602</td>\n      <td>-0.727</td>\n      <td>0.346</td>\n      <td>0.780</td>\n      <td>-0.527</td>\n      <td>-1.122</td>\n      <td>-0.208</td>\n      <td>-0.730</td>\n      <td>-0.302</td>\n      <td>2.535</td>\n      <td>-1.045</td>\n      <td>0.037</td>\n      <td>0.020</td>\n      <td>1.373</td>\n      <td>0.456</td>\n      <td>-0.277</td>\n      <td>1.381</td>\n      <td>1.843</td>\n      <td>0.749</td>\n      <td>0.202</td>\n      <td>0.013</td>\n      <td>0.263</td>\n      <td>-1.222</td>\n      <td>0.726</td>\n      <td>1.444</td>\n      <td>-1.165</td>\n      <td>-1.544</td>\n      <td>0.004</td>\n      <td>0.800</td>\n      <td>-1.211</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>1.0</td>\n      <td>0.067</td>\n      <td>-0.021</td>\n      <td>0.392</td>\n      <td>-1.637</td>\n      <td>-0.446</td>\n      <td>-0.725</td>\n      <td>-1.035</td>\n      <td>0.834</td>\n      <td>0.503</td>\n      <td>0.274</td>\n      <td>0.335</td>\n      <td>-1.148</td>\n      <td>0.067</td>\n      <td>-1.010</td>\n      <td>1.048</td>\n      <td>-1.442</td>\n      <td>0.210</td>\n      <td>0.836</td>\n      <td>-0.326</td>\n      <td>0.716</td>\n      <td>-0.764</td>\n      <td>0.248</td>\n      <td>-1.308</td>\n      <td>2.127</td>\n      <td>0.365</td>\n      <td>0.296</td>\n      <td>-0.808</td>\n      <td>1.854</td>\n      <td>0.118</td>\n      <td>0.380</td>\n      <td>0.999</td>\n      <td>-1.171</td>\n      <td>2.798</td>\n      <td>0.394</td>\n      <td>-1.048</td>\n      <td>1.078</td>\n      <td>0.401</td>\n      <td>-0.486</td>\n      <td>...</td>\n      <td>-0.083</td>\n      <td>-0.831</td>\n      <td>1.251</td>\n      <td>-0.206</td>\n      <td>-0.933</td>\n      <td>-1.215</td>\n      <td>0.281</td>\n      <td>0.512</td>\n      <td>-0.424</td>\n      <td>0.769</td>\n      <td>0.223</td>\n      <td>-0.710</td>\n      <td>2.725</td>\n      <td>0.176</td>\n      <td>0.845</td>\n      <td>-1.226</td>\n      <td>1.527</td>\n      <td>-1.701</td>\n      <td>0.597</td>\n      <td>0.150</td>\n      <td>1.864</td>\n      <td>0.322</td>\n      <td>-0.214</td>\n      <td>1.282</td>\n      <td>0.408</td>\n      <td>-0.910</td>\n      <td>1.020</td>\n      <td>-0.299</td>\n      <td>-1.574</td>\n      <td>-1.618</td>\n      <td>-0.404</td>\n      <td>0.640</td>\n      <td>-0.595</td>\n      <td>-0.966</td>\n      <td>0.900</td>\n      <td>0.467</td>\n      <td>-0.562</td>\n      <td>-0.254</td>\n      <td>-0.533</td>\n      <td>0.238</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>1.0</td>\n      <td>2.347</td>\n      <td>-0.831</td>\n      <td>0.511</td>\n      <td>-0.021</td>\n      <td>1.225</td>\n      <td>1.594</td>\n      <td>0.585</td>\n      <td>1.509</td>\n      <td>-0.012</td>\n      <td>2.198</td>\n      <td>0.190</td>\n      <td>0.453</td>\n      <td>0.494</td>\n      <td>1.478</td>\n      <td>-1.412</td>\n      <td>0.270</td>\n      <td>-1.312</td>\n      <td>-0.322</td>\n      <td>-0.688</td>\n      <td>-0.198</td>\n      <td>-0.285</td>\n      <td>1.042</td>\n      <td>-0.315</td>\n      <td>-0.478</td>\n      <td>0.024</td>\n      <td>-0.190</td>\n      <td>1.656</td>\n      <td>-0.469</td>\n      <td>-1.437</td>\n      <td>-0.581</td>\n      <td>-0.308</td>\n      <td>-0.837</td>\n      <td>-1.739</td>\n      <td>0.037</td>\n      <td>0.336</td>\n      <td>-1.102</td>\n      <td>2.371</td>\n      <td>0.554</td>\n      <td>...</td>\n      <td>-1.050</td>\n      <td>-0.347</td>\n      <td>0.904</td>\n      <td>-1.324</td>\n      <td>-0.849</td>\n      <td>3.432</td>\n      <td>0.222</td>\n      <td>0.416</td>\n      <td>0.174</td>\n      <td>-1.517</td>\n      <td>-0.337</td>\n      <td>0.055</td>\n      <td>-0.464</td>\n      <td>0.014</td>\n      <td>-1.073</td>\n      <td>0.325</td>\n      <td>-0.523</td>\n      <td>-0.692</td>\n      <td>0.190</td>\n      <td>-0.883</td>\n      <td>-1.830</td>\n      <td>1.408</td>\n      <td>2.319</td>\n      <td>1.704</td>\n      <td>-0.723</td>\n      <td>1.014</td>\n      <td>0.064</td>\n      <td>0.096</td>\n      <td>-0.775</td>\n      <td>1.845</td>\n      <td>0.898</td>\n      <td>0.134</td>\n      <td>2.415</td>\n      <td>-0.996</td>\n      <td>-1.006</td>\n      <td>1.378</td>\n      <td>1.246</td>\n      <td>1.478</td>\n      <td>0.428</td>\n      <td>0.253</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"    id      0      1      2      3  ...      295    296    297    298    299\n0  250  0.500 -1.033 -1.595  0.309  ...    2.132  0.609 -0.104  0.312  0.979\n1  251  0.776  0.914 -0.494  1.347  ...   -1.133 -3.138  0.281 -0.625 -0.761\n2  252  1.750  0.509 -0.057  0.835  ...    0.701  0.976  0.135 -1.327  2.463\n3  253 -0.556 -1.855 -0.682  0.578  ...    0.916  2.411  1.053 -1.601 -1.529\n4  254  0.754 -0.245  1.173 -1.623  ...    0.322 -0.068 -0.156 -1.153  0.825\n\n[5 rows x 301 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n      <th>21</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n      <th>25</th>\n      <th>26</th>\n      <th>27</th>\n      <th>28</th>\n      <th>29</th>\n      <th>30</th>\n      <th>31</th>\n      <th>32</th>\n      <th>33</th>\n      <th>34</th>\n      <th>35</th>\n      <th>36</th>\n      <th>37</th>\n      <th>38</th>\n      <th>...</th>\n      <th>260</th>\n      <th>261</th>\n      <th>262</th>\n      <th>263</th>\n      <th>264</th>\n      <th>265</th>\n      <th>266</th>\n      <th>267</th>\n      <th>268</th>\n      <th>269</th>\n      <th>270</th>\n      <th>271</th>\n      <th>272</th>\n      <th>273</th>\n      <th>274</th>\n      <th>275</th>\n      <th>276</th>\n      <th>277</th>\n      <th>278</th>\n      <th>279</th>\n      <th>280</th>\n      <th>281</th>\n      <th>282</th>\n      <th>283</th>\n      <th>284</th>\n      <th>285</th>\n      <th>286</th>\n      <th>287</th>\n      <th>288</th>\n      <th>289</th>\n      <th>290</th>\n      <th>291</th>\n      <th>292</th>\n      <th>293</th>\n      <th>294</th>\n      <th>295</th>\n      <th>296</th>\n      <th>297</th>\n      <th>298</th>\n      <th>299</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>250</td>\n      <td>0.500</td>\n      <td>-1.033</td>\n      <td>-1.595</td>\n      <td>0.309</td>\n      <td>-0.714</td>\n      <td>0.502</td>\n      <td>0.535</td>\n      <td>-0.129</td>\n      <td>-0.687</td>\n      <td>1.291</td>\n      <td>0.507</td>\n      <td>-0.317</td>\n      <td>1.848</td>\n      <td>-0.232</td>\n      <td>-0.340</td>\n      <td>-0.051</td>\n      <td>0.804</td>\n      <td>0.764</td>\n      <td>1.860</td>\n      <td>0.262</td>\n      <td>1.112</td>\n      <td>-0.491</td>\n      <td>-1.039</td>\n      <td>-0.492</td>\n      <td>0.183</td>\n      <td>-0.671</td>\n      <td>-1.313</td>\n      <td>0.149</td>\n      <td>0.244</td>\n      <td>1.072</td>\n      <td>-1.003</td>\n      <td>0.832</td>\n      <td>-1.075</td>\n      <td>1.988</td>\n      <td>1.201</td>\n      <td>-2.065</td>\n      <td>-0.826</td>\n      <td>-0.016</td>\n      <td>0.490</td>\n      <td>...</td>\n      <td>0.824</td>\n      <td>0.928</td>\n      <td>1.372</td>\n      <td>1.505</td>\n      <td>0.645</td>\n      <td>0.641</td>\n      <td>-1.132</td>\n      <td>1.009</td>\n      <td>0.998</td>\n      <td>0.210</td>\n      <td>-1.634</td>\n      <td>1.046</td>\n      <td>0.114</td>\n      <td>-0.806</td>\n      <td>0.301</td>\n      <td>0.145</td>\n      <td>-0.684</td>\n      <td>0.794</td>\n      <td>-0.290</td>\n      <td>-1.688</td>\n      <td>0.313</td>\n      <td>1.140</td>\n      <td>0.447</td>\n      <td>-0.616</td>\n      <td>1.294</td>\n      <td>0.785</td>\n      <td>0.453</td>\n      <td>1.550</td>\n      <td>-0.866</td>\n      <td>1.007</td>\n      <td>-0.088</td>\n      <td>-2.628</td>\n      <td>-0.845</td>\n      <td>2.078</td>\n      <td>-0.277</td>\n      <td>2.132</td>\n      <td>0.609</td>\n      <td>-0.104</td>\n      <td>0.312</td>\n      <td>0.979</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>251</td>\n      <td>0.776</td>\n      <td>0.914</td>\n      <td>-0.494</td>\n      <td>1.347</td>\n      <td>-0.867</td>\n      <td>0.480</td>\n      <td>0.578</td>\n      <td>-0.313</td>\n      <td>0.203</td>\n      <td>1.356</td>\n      <td>-1.086</td>\n      <td>0.322</td>\n      <td>0.876</td>\n      <td>-0.563</td>\n      <td>-1.394</td>\n      <td>0.385</td>\n      <td>1.891</td>\n      <td>-2.107</td>\n      <td>-0.636</td>\n      <td>-0.055</td>\n      <td>-0.843</td>\n      <td>0.041</td>\n      <td>0.253</td>\n      <td>0.557</td>\n      <td>0.475</td>\n      <td>-0.839</td>\n      <td>-1.146</td>\n      <td>1.210</td>\n      <td>1.427</td>\n      <td>0.347</td>\n      <td>1.077</td>\n      <td>-0.194</td>\n      <td>0.323</td>\n      <td>0.543</td>\n      <td>0.894</td>\n      <td>1.190</td>\n      <td>0.342</td>\n      <td>-0.858</td>\n      <td>0.756</td>\n      <td>...</td>\n      <td>-1.791</td>\n      <td>0.122</td>\n      <td>-0.669</td>\n      <td>-1.558</td>\n      <td>-0.244</td>\n      <td>2.583</td>\n      <td>-0.829</td>\n      <td>0.133</td>\n      <td>-2.746</td>\n      <td>0.341</td>\n      <td>-1.145</td>\n      <td>0.492</td>\n      <td>0.437</td>\n      <td>-0.628</td>\n      <td>0.271</td>\n      <td>2.639</td>\n      <td>0.481</td>\n      <td>-0.687</td>\n      <td>1.017</td>\n      <td>1.648</td>\n      <td>-1.272</td>\n      <td>-0.797</td>\n      <td>-0.870</td>\n      <td>-1.582</td>\n      <td>-1.987</td>\n      <td>-0.052</td>\n      <td>-0.194</td>\n      <td>0.539</td>\n      <td>-1.788</td>\n      <td>-0.433</td>\n      <td>-0.683</td>\n      <td>-0.066</td>\n      <td>0.025</td>\n      <td>0.606</td>\n      <td>-0.353</td>\n      <td>-1.133</td>\n      <td>-3.138</td>\n      <td>0.281</td>\n      <td>-0.625</td>\n      <td>-0.761</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>252</td>\n      <td>1.750</td>\n      <td>0.509</td>\n      <td>-0.057</td>\n      <td>0.835</td>\n      <td>-0.476</td>\n      <td>1.428</td>\n      <td>-0.701</td>\n      <td>-2.009</td>\n      <td>-1.378</td>\n      <td>0.167</td>\n      <td>-0.132</td>\n      <td>0.459</td>\n      <td>-0.341</td>\n      <td>0.014</td>\n      <td>0.184</td>\n      <td>-0.460</td>\n      <td>-0.991</td>\n      <td>-1.039</td>\n      <td>0.992</td>\n      <td>1.036</td>\n      <td>1.552</td>\n      <td>-0.830</td>\n      <td>1.374</td>\n      <td>-0.914</td>\n      <td>0.427</td>\n      <td>0.027</td>\n      <td>0.327</td>\n      <td>1.117</td>\n      <td>0.871</td>\n      <td>-2.556</td>\n      <td>-0.036</td>\n      <td>-0.081</td>\n      <td>0.744</td>\n      <td>-1.191</td>\n      <td>-1.784</td>\n      <td>0.239</td>\n      <td>0.500</td>\n      <td>0.437</td>\n      <td>0.746</td>\n      <td>...</td>\n      <td>-1.167</td>\n      <td>1.009</td>\n      <td>-0.180</td>\n      <td>-0.683</td>\n      <td>-1.383</td>\n      <td>1.020</td>\n      <td>0.268</td>\n      <td>-1.558</td>\n      <td>0.620</td>\n      <td>-0.489</td>\n      <td>-2.090</td>\n      <td>-0.977</td>\n      <td>1.672</td>\n      <td>-0.655</td>\n      <td>-0.801</td>\n      <td>-1.846</td>\n      <td>0.761</td>\n      <td>-0.846</td>\n      <td>0.181</td>\n      <td>0.962</td>\n      <td>-0.611</td>\n      <td>1.450</td>\n      <td>0.021</td>\n      <td>0.320</td>\n      <td>-0.951</td>\n      <td>-2.662</td>\n      <td>0.761</td>\n      <td>-0.665</td>\n      <td>-0.619</td>\n      <td>-0.645</td>\n      <td>-0.094</td>\n      <td>0.351</td>\n      <td>-0.607</td>\n      <td>-0.737</td>\n      <td>-0.031</td>\n      <td>0.701</td>\n      <td>0.976</td>\n      <td>0.135</td>\n      <td>-1.327</td>\n      <td>2.463</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>253</td>\n      <td>-0.556</td>\n      <td>-1.855</td>\n      <td>-0.682</td>\n      <td>0.578</td>\n      <td>1.592</td>\n      <td>0.512</td>\n      <td>-1.419</td>\n      <td>0.722</td>\n      <td>0.511</td>\n      <td>0.567</td>\n      <td>0.356</td>\n      <td>-0.060</td>\n      <td>0.767</td>\n      <td>-0.196</td>\n      <td>0.359</td>\n      <td>0.080</td>\n      <td>-0.956</td>\n      <td>0.857</td>\n      <td>-0.655</td>\n      <td>-0.090</td>\n      <td>-0.008</td>\n      <td>-0.596</td>\n      <td>-0.413</td>\n      <td>-1.030</td>\n      <td>0.173</td>\n      <td>-0.969</td>\n      <td>0.998</td>\n      <td>0.079</td>\n      <td>0.790</td>\n      <td>-0.776</td>\n      <td>-0.374</td>\n      <td>-1.995</td>\n      <td>0.572</td>\n      <td>0.542</td>\n      <td>0.547</td>\n      <td>0.307</td>\n      <td>-0.074</td>\n      <td>1.703</td>\n      <td>-0.003</td>\n      <td>...</td>\n      <td>-1.029</td>\n      <td>-0.340</td>\n      <td>0.052</td>\n      <td>2.122</td>\n      <td>-0.136</td>\n      <td>-1.799</td>\n      <td>1.450</td>\n      <td>1.866</td>\n      <td>-0.273</td>\n      <td>-0.237</td>\n      <td>-0.207</td>\n      <td>-0.196</td>\n      <td>-1.106</td>\n      <td>-1.560</td>\n      <td>-0.934</td>\n      <td>2.167</td>\n      <td>0.323</td>\n      <td>0.583</td>\n      <td>1.480</td>\n      <td>-0.685</td>\n      <td>-0.473</td>\n      <td>-1.066</td>\n      <td>-0.271</td>\n      <td>0.506</td>\n      <td>-0.753</td>\n      <td>1.048</td>\n      <td>-0.450</td>\n      <td>-0.300</td>\n      <td>-1.221</td>\n      <td>0.235</td>\n      <td>-0.336</td>\n      <td>-0.787</td>\n      <td>0.255</td>\n      <td>-0.031</td>\n      <td>-0.836</td>\n      <td>0.916</td>\n      <td>2.411</td>\n      <td>1.053</td>\n      <td>-1.601</td>\n      <td>-1.529</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>254</td>\n      <td>0.754</td>\n      <td>-0.245</td>\n      <td>1.173</td>\n      <td>-1.623</td>\n      <td>0.009</td>\n      <td>0.370</td>\n      <td>0.781</td>\n      <td>-1.763</td>\n      <td>-1.432</td>\n      <td>-0.930</td>\n      <td>-0.098</td>\n      <td>0.896</td>\n      <td>0.293</td>\n      <td>-0.259</td>\n      <td>0.030</td>\n      <td>-0.661</td>\n      <td>0.921</td>\n      <td>0.006</td>\n      <td>-0.631</td>\n      <td>1.284</td>\n      <td>-1.167</td>\n      <td>-0.744</td>\n      <td>-2.184</td>\n      <td>2.146</td>\n      <td>1.130</td>\n      <td>0.017</td>\n      <td>1.421</td>\n      <td>-0.590</td>\n      <td>1.938</td>\n      <td>-0.194</td>\n      <td>0.794</td>\n      <td>0.579</td>\n      <td>0.521</td>\n      <td>0.635</td>\n      <td>-0.023</td>\n      <td>-0.892</td>\n      <td>-0.363</td>\n      <td>-0.360</td>\n      <td>0.405</td>\n      <td>...</td>\n      <td>-0.486</td>\n      <td>-0.068</td>\n      <td>-0.534</td>\n      <td>-1.322</td>\n      <td>0.500</td>\n      <td>0.263</td>\n      <td>-0.745</td>\n      <td>0.578</td>\n      <td>-0.064</td>\n      <td>0.738</td>\n      <td>-0.280</td>\n      <td>0.745</td>\n      <td>-0.588</td>\n      <td>-0.429</td>\n      <td>-0.588</td>\n      <td>0.154</td>\n      <td>-1.187</td>\n      <td>1.681</td>\n      <td>-0.832</td>\n      <td>-0.437</td>\n      <td>-0.038</td>\n      <td>-1.096</td>\n      <td>-0.156</td>\n      <td>3.565</td>\n      <td>-0.428</td>\n      <td>-0.384</td>\n      <td>1.243</td>\n      <td>-0.966</td>\n      <td>1.525</td>\n      <td>0.458</td>\n      <td>2.184</td>\n      <td>-1.090</td>\n      <td>0.216</td>\n      <td>1.186</td>\n      <td>-0.143</td>\n      <td>0.322</td>\n      <td>-0.068</td>\n      <td>-0.156</td>\n      <td>-1.153</td>\n      <td>0.825</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"samplesub_df.head()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"    id  target\n0  250       0\n1  251       0\n2  252       0\n3  253       0\n4  254       0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>250</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>251</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>252</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>253</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>254</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df.shape, test_df.shape,samplesub_df.shape)","execution_count":6,"outputs":[{"output_type":"stream","text":"(250, 302) (19750, 301) (19750, 2)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"training set has more features than observations. It is a small dataset which leads to overfitting easily for many algorithms with default hyperparameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df['target'].unique())","execution_count":7,"outputs":[{"output_type":"stream","text":"[1. 0.]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Just two classes we need to predict. either 1 or 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.countplot(train_df['target'])","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x7f7de8129a90>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Visualizing how balanced the training set is,"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[8,8])\ntrain_df['target'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',shadow=True)\nplt.show()","execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 576x576 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAdIAAAHICAYAAAD6LWvLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3Xl8VNXBPvDn3Jk7k0x29p0BCSMIggiEfXWPjlr3Wm1rW5fWnarTVm3s4o9aa321ra193XettuXtuOAuYokLoCgagySs2UkmyySZ7fz+mAFCCCRk7syZ5fl+PvlIZjKHJzHkyb333HOElBJERETUP5rqAERERMmMRUpERBQFFikREVEUWKRERERRYJESERFFgUVKREQUBRYpERFRFFikREREUWCREhERRYFFSkREFAUWKRERURRYpERERFFgkRIREUWBRUpERBQFFikREVEUWKRERERRYJESERFFgUVKREQUBRYpERFRFFikREREUWCREhERRYFFSkREFAUWKRERURRYpERERFFgkRIREUWBRUpERBQFFikREVEUWKRERERRYJESERFFgUVKREQUBRYpERFRFFikREREUWCREhERRYFFSkREFAUWKRERURRYpERERFFgkRIREUWBRUpERBQFFikREVEUWKRERERRYJESERFFgUVKREQUBRYpERFRFFikREREUWCREhERRYFFSkREFAUWKRERURRYpERERFFgkRIREUWBRUr9IoR4WAhRK4T4/BDPCyHEfUKILUKIz4QQM+KdkYgoHlik1F+PAjjlMM+fCqAw8nY5gAfikImIKO5YpNQvUsr3AOw5zIecCeBxGbYOQL4QYnh80hERxY9ZdQBKWSMB7Ojy/s7IY1Vq4iQnu8stAFgBiD6+xFe5sjgYw0hE1A2LlChO7C63CcBoAOMQ/qWiAEB+5K3rn7u+n4sjPHNkd7nbAbRE3pojb3sibw1d3qoAVAKorFxZ7I3qkyNKYyxSipVdCJfGXqMij6Usu8utIfx52iNv47r9eSTi828uM/I2pK8vsLvctQiXakW3/1YiXLSdBmckShlCSqk6AyUpIYQdwH+klFN6eK4YwNUATgNQBOA+KeXsuAaMIbvLnQ1gGoDjurxNRvg0bKqRALYC2ND1rXJlcbXSVEQJgkVK/SKEeAbAEgCDANQA+CUAHQCklH8VQggAf0J4Zq8XwPellB+rSRsdu8s9BAcW5nEAjgIn61WjW7kC2Fq5spg/VCitsEiJurG73KMBLIu8LQEwRmmg5OIBsAbAW5G3z1islOpYpJT2Ikecy7q8HaU2UUqpB/AuIsVaubL4K8V5iAzHIqW0E7m+eQKA5QgX52S1idJKFYC3ES7W1ypXFu9UnIcoaixSSgt2lzsfgBPAOQBOApChNhEhPInpQwAvAnixcmXxVsV5iPqFRUopy+5yDwZwFsLluQyRyVCUsDYiXKr/4ClgSiYsUkopdpd7BIBvIVyeCwGY1CaiftqM/Ueqn6oOQ3Q4LFJKenaXOwPAuQB+CGAR+r6cHiWHcgAPA3ikcmVxjeowRN2xSClp2V3uKQB+BOAShJfUo9TmB/B/AP4OYHXlyuKQ4jxEAFiklGTsLrcNwAUIF+hcxXFInW0AHgLwcOXK4pReepISH4uUkoLd5T4O4fK8GOGF3IkAIAjgZYSPUl/mzjekAouUElZkt5RvAfgpgJRZp5diZieAewH8rXJlcavqMJQ+WKSUcCKnb78P4EYA4xXHoeTTiPA6z/dVriyuVx2GUh+LlBKG3eUuAHAtgGsADFQch5KfF+HrqHdXrizerjoMpS4WKSkXWet2BYCrAOQojkOpJwDgaQC/q1xZvFl1GEo9LFJSxu5yjwRwi5Tyh0KITNV5KOVJhG+fubNyZXGp6jCUOlikFHd2lzsXwM+klNcLIbjmLanwEgBX5crictVBKPmxSClu7C63DuBKKeXtQohBqvNQ2gsA+BuAOypXFtepDkPJi0VKcWF3uc+VUt4phChUnYWom2YAdwG4p3JlcbvqMJR8WKQUU3aXe56U8m4hBFchokS3C8BtAB7j8oN0JFikFBN2l7sQwEqEF1QgSiabANxcubL4VdVBKDmwSMlQdpc7E8AvpZQ3CiG4/ycls5cB/KRyZXGl6iCU2FikZBi7y71cSvmgEIKrEVGq8AL4NYA/VK4s9qsOQ4mJRUpRs7vcA6UM3SOEdqnqLEQx8jmAKytXFq9VHYQSD4uUomJ3uS+WodB9QtMGqM5CFGMS4dtlbq5cWdyiOgwlDhYp9Yvd5R4nQ8EHhWY6QXUWojjbgfDR6cuqg1BiYJHSEbG73CYp5fWA/I0QGlclonT2FIDrucMMsUipz+wu91gZCjwvNDP3BiUKqwZwSeXK4jdUByF1WKTUJ2Nv+vcFEOJ/hWbKVp2FKMFIAL8HcCtn9qYnFikdlt3ltoU6vQ9pVtuFqrMQJbgPAVxUubJ4q+ogFF8sUjqkMStemg5glaZbR6vOQpQkmgFcVbmy+GnVQSh+WKTUo9HXP3eLZrH9RmiaWXUWoiT0OMKrIrWqDkKxxyKlA9hd7oGhTu8/NKttieosREmuHMCFlSuL16sOQrGlqQ5AiWPM9c8tlAHf1yxRIkMUAviv3eW+WnUQii0ekRIAYNRPHrvOlFXwB6GZTKqzEKWgBwFczVm9qYlFmuaGfef3mjln0JPmvCEXqc5ClOLeBXBO5criBtVByFgs0jQ25NxfDrEMPepNc87AKaqzEKWJrQCclSuLv1AdhIzDIk1Twy7+3Ux98NiXTRk5g1VnIUozLQC+Xbmy+D+qg5AxONkoDQ2/9J6LLcMnrmGJEimRA+Dfdpf7ZtVByBg8Ik0jtsIikb/wkrv0QWNvFJrGX6KI1HscwOWVK4s7VQeh/mORponcWWeZc4479d/6gFGnqc5CRAf4AMDplSuLG1UHof5hkaaB3Fln5eYcd9ob+oCRs1RnIaIebQJwUuXK4mrVQejI8fReisuddeaYnBnFH7FEiRLaVABr7C73WNVB6MixSFNY7uyzp+Uc73xfLxgxUXUWIurVBADv211uh+ogdGR4ajdF5c27YF7O9NNeMucOGqo6CxEdkToAJ1euLN6gOgj1DYs0BeUv/M5JOdNPecaUVTBAdRYi6hcPgOLKlcVrVQeh3vHUboopWPr983JmFL/AEiVKankAVttd7pNVB6He8Yg0RdgKi0TGmKnfzz72pPs0a1aW6jxEZAgfwqsgvag6CB0aj0hTgK2wSLMMm3B59rEns0SJUosFwLN2l/sM1UHo0FikSc5WWGQyDxh5Rc6MM36vWW0sUaLUYwbwvN3lXqo6CPWMRZrEbIVFuil74NV5s8/5rSkzJ0d1HiKKmQwAq+wu92zVQehgLNIkZSssMmkZ2ZfnzbvgF6as/ALVeYgo5rIBvGJ3ubntYYJhkSYhW2GRJvTMS/Pmf/tWc84g7uBClD4GAHjd7nIfpToI7cciTTK2wiIhTPo5+fMvvEPPHzZMdR4iirthAN6wu9wjVQehMBZpErEVFgkIcVre3AtW6gNHj1adh4iUsSN8ZDpIdRBikSabJXlF5/3BMnT8eNVBiEi5SQBes7vcuaqDpDsWaZKwFRbNzT72pN9ZRx7NBa2JaK8ZAJ6xu9z8Wa4Qv/hJwFZYNC1j7PRfZU6YPVN1FiJKOKcBuEt1iHTGIk1wtsKiSfrA0bfnHHfqQiE0oToPESWkFXaX+3uqQ6QrFmkCsxUWjdQycm7Jm3P+MmHSrarzEFFC+5vd5Z6nOkQ6YpEmKFthUS400/X5C769TMvIyledh4gSngXAP+0u9xjVQdINizQB2QqLdABX5hWde5o5byhvcyGivhqC8FKCXHc7jlikCcZWWCQAXJh1zNKzrSMck1XnIaKkMw3A43aXm3Mq4oRFmniWWEdP+Y7NMZ+LUxNRf30LwK9Uh0gXLNIEYissmmTKGfTj3BmnLxRC4/8bIorGrXaX+0zVIdIBf1gnCFth0TAI7bq8uRfMFWZLpuo8RJQSHra73KNUh0h1LNIEYCssygZwXe6ss2aacwZyIWoiMsoAAE9y5aPY4hdXMVthkQnADzPGTp9uHXXM8arzEFHKWQzgF6pDpDIWqXonmrIK5mZPP2WhEJxkR0Qx8Usu1hA7LFKFbIVFEwBcmDf3gpma2cL7vogoVkwAnra73FzcJQZYpIrYCotyAPwke9opdnPeEG6LRkSxNhbA31WHSEUsUgVshUUagO/pg+2jM8cfP191HiJKG+faXe7LVYdINSxSNRbBpBflzjp7odBMZtVhUkmooxV1/7wTu/5+JXb9/Up07vpy33PNH76Ebb87HUGvp8fXtm56E7se/BF2PfgjtG56EwAgA37UPH87dj/0Y7Ssd+/72IZX70dn9ZbYfjJEsXGv3eXmqmkGYpHGma2waCSA7+TOOmu8KTNniOo8qWbPmw8iY/zxGPmjv2LEZfdDHxheqjjQXIf2ig0w5Q7u8XXB9hZ41j6NYZfcg2GX/hGetU8j2NGK9or1sI6ajOGX/QmtX7wFAPDVboUMhWAdNiFunxeRgTIBPGZ3uU2qg6QKFmkc2QqLLACusAw9Kts6wlGkOk+qCXW2oWPHF8g+9iQAgDDp0DKyAQCNb/4dBUu/D6DnmdEdFeuRYT8OpswcmDKykWE/Dh1bP4HQTJD+TiAYBGT4Y5vWPIn8hd+Jx6dEFCszAVyrOkSqYJHG15kQYnQOlwCMiUBTDUy2XDS8fC92P3ItGl65DyFfB7zl62DKGQjLYeZ0BVoaYModtO99U85ABFoakDHuOAQ8tah6YgVyZ54Bb3kpLEOPgjlnYDw+JaJY+rXd5R6rOkQq4PW5OLEVFjkAnJ4z/bRhJlveCNV5UpEMBeGr/gYDTrgS1hEO7Hnjb/CsfRodOz7H0At+3a8xhWbCYOdN4fGDAdQ8fzuGfOtW7Hnz7wg21yFrynLYCnlygZJSFoAHAJymOkiy41FRHNgKi6wALjPnDQtkjJ22RHWeVGXOGQRTziBYRzgAADbHfPhqtiDgqcHuh6/BzgcuQ7ClHlWPXo9ga2O31w5EsLl+3/vBloaDjjpbNriRPWUZOneXQbNmYdCZt6D5o3/G/hMjip1T7S73hapDJDsWaXycAmBI7qyzFgqT2aI6TKoyZRfAnDsI/oadAICObZ/CMnQCRl/zFEZd9TBGXfUwTDmDMPx798KUXXDAazPGzUB75QYEO1rDk4wqNyBj3Ix9zwc7WtG+5SNkTVkGGegEhACECP+ZKLn9j93lHqA6RDLjqd0YsxUWjQJwlm3Sojxz3pBC1XlS3YATrkT9f+6GDAZgzh+Ggaddf8iP7awqR+vGVzDw1GthysxB/rwLUP3YDQCA/HkXwpSZs+9jPWufQd688yGEhsxxM9Cy3o2qh65G9nGnxvxzIoqxIQDuBnCZ6iDJSkgpVWdIWZEF6V1aRva4ASf95EJNt3IZQCJKVMsqVxa/rTpEMuKp3dhaCGBi7uxzjmOJElGC+5vd5c5QHSIZsUhjxFZYNBDARdbRUzR90JgZvb6AiEitQgAu1SGSEYs0BmyFRQLARQBE9jFLl3B7NCJKEjfZXe6RqkMkGxZpbEwDMMt29KJ8U1bBKNVhiIj6yAbgTtUhkg2L1GC2wiIbgO9DMzfYJsxerjoPEdERusTuch+vOkQyYZEabzmAnJxpJx+tWW28N4uIko0AcI/qEMmERWogW2HRAABOLSO7IWPM1MWq8xAR9dMiu8t9puoQyYJFaqwzACDnuNPmCLPFpjoMEVEU7uRWa33DIjVIZAWjJeb8Ya2WYYVzVOchIorSZACXqg6RDFikBojc7nIegI7saacsEZqJSy8SUSq4g4s09I5FagwHgOmW4ROhDxw9TXUYIiKDjAbwE9UhEh2LNEqR9XS/DcCTNWnxYsHVF4gotdxsd7kzVYdIZCzS6B0PYKw+eJww5w87WnUYIiKDDQF3hjksFmkUIht2XwSgLmvyovk8GiWiFPVTu8vNuR+HwCKNzjwABeb8YZo+YPSxqsMQEcWIHcCFqkMkKhZpP9kKi3QAZwKozTpm6VyhabzfiohS2S12l5tn3XrAIu2/GQDyNVu+tAwZx3UpiSjVTQFwuuoQiYhF2g+RmbpnA9iTPXX5bKGZddWZiIjigPuV9oBF2j9TAAwXFlu7dVhhkeowRERxMs/uci9SHSLRsEiPUGQVo7MANGVPWXa8MFt4fxURpRMelXbDIj1yEwGMg2byWEdOnqs6DBFRnJ1qd7m5glsXLNIjEDkadQJos02c59AsGbmqMxERKcBlA7tgkR6ZsQCOAVCfMXoKZ+oSUbq6yO5yZ6sOkShYpEfmVAAd5oKRuaacQeNVhyEiUiQbXKBhHxZpH9kKiwYBmAWg1uaYN4PLARJRmrtcdYBEwSLtuzkAJIQmLUPGHac6DBGRYrM46SiMRdoHtsIiM4ATAdTbCudM1PSMHNWZiIgSwI9UB0gELNK+ORpALoD2jDFTZ6gOQ0SUIL7DvUpZpH21DEC7OW9Yjil3SKHqMERECSIPwPmqQ6jGIu2FrbBoAIDpAOptjvmcZEREdKC0P73LIu3dbACAENIydDwnGRERHWi+3eWerDqESizSw4js8nISgPqMsdPHaJbMPNWZiIgS0MWqA6jEIj28iQAKAHitoyZPUh2GiChBnaM6gEos0sNbCqADAPQBI1mkREQ9c9hd7mNUh1CFRXoItsKiHAAzANRZR08ZqelcoJ6I6DDS9qiURXpokxD++oQyRk9J6wvpRER9wCKlg8wD4AUAfeBoFikR0eEda3e5J6gOoQKLtAe2wqIsAFMA7LGOOHqYZsnMV52JiCgJpOVRKYu0Z0cDMAEIWcdM5dEoEVHfsEhpn7mInNa1DBzDIiUi6ptZdpd7jOoQ8cYi7cZWWJQJYBqAPZahEwZrGVkDVWciIkoiaXdUyiI92ESET+sGM8ZMcagOQ0SUZM5UHSDeWKQHKwLQCQDmghHjFWchIko2c9NtazUWaRe2wiIrgJkAGoTZYjJlFYxWnYmIKMlYACxQHSKeWKQHKgRgBhCwjp4yWmgms+pARERJaKnqAPHEIj3QZABBALAMGT9OcRYiomS1THWAeGKRHmgGgCYA0POHs0iJiPpnpt3lTpv1yVmkEbbCogIAQwG0CYtN17LyRqrORESUpEwAFqkOES8s0v3sACQAZIyZMkYIjV8bIqL+S5vrpCyL/Y4B4AcAy+BxPK1LRBSdtLlOyiIFYCssEgCOQ+T6qDl/GIuUiCg60+wu9wDVIeKBRRo2EEABgHYtM8eqZeYOVx2IiCjJCQBLVIeIBxZp2L4jUOtwxwghhFAZhogoRcxRHSAeWKRhU7F/WUAejRIRGeM41QHiIe2LNHJ9dDr2Xh/NHTRMbSIiopQxXXWAeEj7IgUwCEA2IkekJls+j0iJiIwxyO5yj1IdItZYpMAwRO4fFZZMXVi5/ygRkYFS/vQuixTYt8OLZeiEoZxoRERkKBZpGpgIoA0A9IGjeH2UiMhYLNJUFplodBSAVgAw5w7m9VEiImOxSFNcLoAsAD4AMGUV8IiUiMhYY1N9haN0L9LhiEw0gmbStIzsIWrjEBGlpJS+DYZFGl7GCpbB4wYKzWRWnIeIKBWxSFPYRADtAGDOH8bbXoiIYuMo1QFiKd2LdAIiE41M2QNS+hw+EZFCKb2jVtoWqa2wyIbwri/tAGCy5RWoTURElLLsqgPEUtoWKcJLA8q972gZOTwiJSKKjbGqA8RSOhfpAcWpWW08IiUiig2b3eUeqjpErKRzkQ7C3s9fCCEsGXlq4xARpTS76gCxks5FOhJABwCY84bmCKGl89eCiCjWUnbCUTqXx/4izR2cqzgLEVGqs6sOECvpXKTDsHfGbvYAntYlIootu+oAsZKWRWorLNIB5CCyxq6WmcciJSKKLZ7aTTF5AEJ739Eysnlql4gotsaoDhAr6Vyk++8h1a2ZCrMQEaWDlL1XP12LNBeRxeoBQJgtGQqzEBGlg5S9hNZrkQoh5vflsSSTjy6fuzDpLFIiotiy2l3ulDz715cj0vv7+FgyKQAQ2PeemUVKRBQH+aoDxMIh998UQswFMA/AYCHEjV2eygVginWwGMtBlyLlESkRUVzkA6hSHcJoh9vI2gIgO/IxOV0ebwZwbixDxcGBRaqZWaRERLGXkmuaH7JIpZTvAnhXCPGolHKbEMImpfTGMVssZWNvkQpNCJPZojYOEVFaSMlTu325RjpCCLEZwFcAIISYJoT4S2xjxdy+IjXZ8ng0SkQUH2lbpPcCOBlAAwBIKT8FsCiWoeIgC5Ei1TJyWKRERPGRtkUKKeWObg8FY5AlnmzYV6RZLFIiovhIySI93GSjvXYIIeYBkEIIHcB1AL6MbazYsRUWmQFYEfllQFgyrWoTERGljZRcjrUvR6RXAvgJwtuO7QIwPfJ+srKiyzq7QghxmI8lIiLjJPutkz3q9YhUSlkP4OI4ZImXDHRZZ5eIiOImPYtUCHFfDw97AHwspfy38ZFijkVKRKRGShZpX07tZiB8Orc88nYsgFEAfiCEuDeG2WIlJf9HEhElgZTcKKUvk42OBTBfShmenCPEAwDWAFgAYFMMsxHRIcigv1MG/KmyQAqlKhkywax7NbO1LfLIHqV5YqQvRVqA8AIGnsj7WQAGSCmDQojOmCUjoh7JYKDT89/nH/LVfFOnOgtRL0YCeMVbXvqS6iCx1JcivQvARiHEOwjv4bkIwJ1CiCwAb8QwGxF1I6WUrV+89SJLlJKERIqezu3qsEUauTVkNYCXAcyOPPxzKeXuyJ9vimE2Iuqmo3LDu+3l63YhvKgIUaKzIA3mpRy2SKWUUgjxspRyKoBknKFLlDJ8ddu+aFn/nw9U5yA6ArsAVKgOEWt9ObW7XggxS0r5UczTEFGPgm1N2/yNu+Z4y0tbVWchogP1pUiLAFwshNgGoA3h66RSSnlsTJPFiQwGk33dYEpxIV97i69+2wl1L/2WJUqUgPpSpCfHPIVCIV87Zx5TwpLBQMBXW3FezTM/36I6CxH1rC9LBG4DACHEEIQXZ0gpoY7WDtUZiHoipYSvrvLm6idvek11FiI6tF6nJQshnEKIcoQvGL8LoBLAKzHOFTehjmYWKSUkf8OOJ6oeve6PqnMQ0eH15f6eXwOYA+BrKeU4AMsBrItpqtjyd30n1N7SKaXk2ruUUAKe2o+b3n30e6pzEFHv+lKkfillAwBNCKFJKd8GMDPGuWKpA+EJU/sFA7xOSgkj6G3e7aurOMFbXhrq/aOJSLW+TDZqEkJkA3gPwFNCiFoAyTx78KAilaFAh4Cectd/KfmE/B1t/vrKE2v/8StP7x9NRImgL0X6KQAvgBsQ3pc0D+G1d5NVJ7oXadDfAWQqikMUJkPBoK+24uLqp3+2WXUWIuq7vhTpUillCEAIwGMAIIT4LKapYshbXhqyFRZ1IPy5BwBABvyccETK+eoqS6qf+ClXECNKMocsUiHEVQB+DOCobsWZA2BtrIPFWCu6FmnQxyIlpfwNO1+seuTa36jOQURH7nBHpE8jfJvL/wPg6vJ4i5Qy2feUa0X4FwIAgAz42hVmoTQXaKn/zFu+7nzgCmMGLMnLBnARuk+qI0psEiWev6sO0R+HLFIppQfhPUgvil+cuGlDeJ9VAID0dXCDZFIi2N5S66utWN74ziPGzNAtydMAPAXAach4RPETBJCURZry+8QdQgu6/BIRbG9uUpiF0lTI39nur6s8pfaFknoDh/0NWKKUnJJ23fO+TDZKRXuvkQIAgq17WKQUVzIUCvnrKi6rftq1IdqxnA5dAJhcssT6rRnDTT8zIB6RCizSJNOMLp97oLmORUpx5a+v/F3V4yueNWi4ObNGaLceO1RL6Q0mKOUlbZGm66ndPeiya3ugcTeLlOLGv2f3y01rnvyFEWM5HfpRo3LF9TfOtS4ya8LU+yuIEhaLNMk0InxfLABA+jsCIX9nMq/WREki0NLwla+6/CxveWnU6zs7Hfogm44Vv1xsXZJlEcm8SAoRwCJNOgcdgUpfO49KKaZCHa0N/rqKZXWr7vL3/tGH53TomQCuuX2x9aSh2doQA+IRqdasOkB/pXORHnCPXaizjUVKMSMDvk5fXeXpNc//sirasZwOXQPwvatnW06fPNh0lAHxiBJB0q5PkK5F2o7wmrv7JhyFOlpYpBQTMhSSvtqKq6qfusWo7QfPOH2i+fwTx5tmGDQeUSJgkSaTyPWpOgD7dnwJej0sUooJf8P2+6sev/ERI8ZyOvSZ04ZqP/z+dH2+EFy4iFIKizQJVaNrkbY2skjJcP7Gqjeb3nv8eiPGcjp0+7BsccPN862LdJPQjRiTKIE0qA7QX+lcpLvQpUj9e3YauboMEYJtjVt9VV+fYdAM3QKrCTeWLLEuzbGKXCPyESUYHpEmoVp0+fwDjbs9MuDn4vVkiFCn1+OrrVxWt+quqL+nnA7dCuAnty6ynjwiRxtuQDyiRMQiTUJN6HIvKQAE2z3VirJQCpFBv99XV3FWzXO3bot2rMgM3UsvP153ThtmmmhAPKJExSJNQo3odgtMsLWRRUpRkVJKX23l9dVP3vyOQUOefOJ404WnFZpnGjQeUaJikSahvddE95/e9dREfY8fpTd//faHqh67/i9GjOV06MdOHqxdccVMy3yNU3Qp9XGyUbLxlpcGAOwAkLX3MX/9Nh6RUr8FPDUfNL332OVGjOV06KMGZooVP1tgXWwxCasRYxIlOB6RJqlyAPvWKPXVVtTLUDCgMA8lqaC3aXtnzTcnGzRDN0/XcMMdS63L8jJEvhH5iJIAizRJfQPAsu89GZKhjpYadXEoGYV87S2+mooT6l76bdQbHzgdugXAVa4F1lPH5GmjDIhHlAx82H+5Lemke5FWo9uOA5xwREdCBgMBX23FBTXP3Voe7ViRDbov+u40/axZI02TDIhHlCwqUOLh7i9Jqhrhr8G+iRyB5jpOOKI+kVLCV1f5s+onb3rFoCGXLh5ruvjsSeZZBo1HlCy2qA4QjbQuUm95aQfCCzNk7n3M37CDR6TUJ4E9O5+uevS6u40Yy+nb/PZVAAAgAElEQVTQJ08YoF39k9mWBZoQaf3vktJS1Gd0VOI/2PBvQvsmHHVWlVXJUCDq/SIptQWaaz/xlpdeYsRYToc+PD8DN922yLo4wywye38FUcrhEWmSK0eXNXcRDISCrY3b1cWhRBf0Nlf5aipObHznkVDvH314ToeebRK4oWRJxrKCTDHAiHxESYhHpEmuCt2WCvQ37q5QlIUSXMjf4fXVVZ5U++KvGqMdy+nQzQCu+Ok8y6njC7QxBsQjSlY8Ik1yu9FtwpGveguLlA4iQ8Ggr7bi4ppnfvZ5tGNFZuied8Ex5rPnjzFPMSAeUbLyA4h6XWqV0r5IveWlrQB2ossKR527vqySQX+HulSUiPx1lb+ufuKn/zJouIVzR5kuvXCKPseg8YiSVVLf+gKwSPdaD2D/CjIyJAMte5L6NyQyln/Pzn/ufuTaO4wYy+nQJ9rzxTXXz7EsMmnCZMSYREksqa+PAizSvb5Gt51gAo27KtVEoUQTaKnf1F658XwjxnI69CHZFqy4fbF1SaYubEaMSZTkkvr6KMAi3asy8t99ZdpZ9TWvkxKC7S11vpqtJ+xZ/UDUazA7HbpNANeVLLGeOMimDTIiH1EK4BFpKvCWl3oRvtids/cxX9XXNTLg86pLRaqF/J0d/rrKU2v/cUdttGM5HboJwA+un2M5beJA0zgD4hGlis2qA0SLRbrfegC5XR8ItNRXqolCqslQKOSvq/xB9dOuT6IdKzJD96yzjzaft3SceboB8YhSRQjAx6pDRItFut9B10n99Tu+UZSFFPPXV95d9fiNTxs0XNHxw7XLLpmmzzVoPKJUsRklnhbVIaLFIt1v7yzdfV+T9oqPy6SUUe8vScnFv2f3q01rnnQZMZbToY8fmSOu++k86yKzJsxGjEmUQkpVBzACizTCW17aifDssX2nd4MtDW3BNi4XmE4CrXvKOrZ/dqZBG3QPzDRjxS+XWJdmWUR2768gSjss0hS0Hl0mHAGAv7Yi6S+EU9+EOtr2+Gq+Wd7w6v2+aMdyOvQMANfctth60rBsbagB8YhSEYs0BX2JbtdJ27d+/CXP7qY+GfD5fHUVztoXSnZFO5bToWsAvvfjWfoZU4aYJhgQjygVtQKIernNRMAiPdAOAE3osj9pwFPTEvI27VQXiWJNypD01VX+uPqpW9YaNGTxaYXm808+yjzDoPGIUtHHKPFEvYNSImCRdhG5LvY+gIFdH/fVbePp3RTmr9/+56rHbnjIiLGcDv34Y4dql//gOH2BEKL3FxClr5Q4rQuwSHuyEd1P71as/1JRFooxf1PVO03vPX6tEWM5HfrYIVnihlvmWxfpJqEbMSZRCmORprBKAG3ostl3YM/OpqC3uUpZIoqJYFtjhW/316cZNEM332rCjXcssS7NsYrc3l9BlPbWqQ5gFBZpN97y0hCAteh2etdfv41HpSkk1On1+GorltWtuqs92rGcDt0K4Cc/X2g9eWSuNsKAeESpbgdKPClzcMIi7dl6AAdsb9VeuYHXSVOEDPr9vrqKb9U8d1tltGNFlv+7+Icz9DOPG25yRJ+OKC2sUR3ASCzSnlUAaAdg2fuAv66yIehtivrWCFJLSil9dZUrqp+8+S2Dhjz5hPGmb58+0TzToPGI0sErqgMYiUXaA295aQDAf9Ht9G7n7q83qElERvE37Hi06tHr7zdiLKdDn3r0IO2KK2daFmicokvUVyEAr6oOYSQW6aF9AuCAmZfesrWfy1DQrygPRSngqVnX9O6jPzBiLKdDHzkwU6z4xULrEotJWI0YkyhNfIgST73qEEZikR7aFoRP7+77IRnqaOkMNO7mpKMkFPR6dvjqt51k0AzdXF3DDSVLrMvzMkS+EfmI0sjLqgMYjUV6CN7yUj+AtwEM7vp4e+XG9WoSUX+FfO2tvtqKE2pfuCPq7ZqcDl0HcOUtCyynjs3XRhkQjyjduFUHMBqL9PDWodvs3Y7KDdtCHW0NivLQEZLBQNBXW3FhzbO/+DrasSIzdC+85Fj97NkjzZMNiEeUbqoApNxcExbp4e0CsB1AXtcHO6u//kRNHDpSvrrKX1Q/eZNRvwEvWTjGdMk5k82zDRqPKN28ihJPyu0CwiI9jMj1tNUADrgO1vblmo0yFAyoSUV95W/Y8WzVo9f9zoixnA590lEF2tXXFFkWaELw3w1R/6TcaV2ARdoXGwEEAJj3PhDyNrX7G3d/oS4S9SbQXLfBW156sRFjOR36sDwrVty22LI4wywye38FEfXAD+B11SFigUXaC295aRvCO8IM6fp4+zcff6wmEfUm2N5c7avZurzxnUei3qLJ6dCzNYHrSpZknDAgUxvY+yuI6BDWosTTrDpELLBI++ZddLuntHPHpp3BNu5TmmhC/g6vr7bipNoXf9UY7VhOh24G8KMVcy3FRw3QxhoQjyidpeRpXYBF2lfbEN70+4BJR+0Vn7yvJg71RIaCQX9t5aU1z/x8U7RjRWbonnv+MeZzFo41TzUgHlE6kwBeVB0iVlikfRCZdPQyuk068patLQt2tNapSUXd+esq76x6YoVR/1gXzBll+u5FU/Q5Bo1HlM7WoMRToTpErLBI++5TAJ3ospA9AHRs+3StmjjUlX/Pzn83vf/UL40Yy+nQC8fmiWuun2NZaNKEqfdXEFEvHlcdIJZYpH3kLS9tR/iodGjXx9s2v7Mp5Gv3qElFABBoqf+ivXLjuQYt/zc424Kf3r7YusSmiywj8hGluXYAL6gOEUss0iPzLsI7F+y7FQahYKhz5+YPlCVKc8GO1np/3bble1Y/EPV9vU6HbhPAtbcvtp4wOEsb3PsriKgP/pWqs3X3YpEeAW95qQfAm+h2VNr6+ZvrZcDnVZMqfYUCvg5/bcVpNc/fXhPtWE6HbgJw2bVFltOPHmQab0A8IgpL6dO6AIu0P95EeP3dfV876e8IdFZ9XaouUvqRoVDIX1vxo+qnXR8ZNOSZZx1tPm/5ePN0g8YjovDauim5CENXLNIj5C0vrQOwFt2PSje98aEMBnxqUqUff33lH6sev/FJI8ZyOvSiGcO1yy6dps8zYjwi2ucplHiCqkPEGou0f15DePau2PtAqL25w1e7lasdxYG/cffqpjVP3mTEWE6HPm5EjrjupnnWRWZNmHt/BREdgcdUB4gHFmk/eMtLdyK8FdABywa2frZ6rQwGOtWkSg+B1j3lvqpyp0EzdAdkmvHTkiXWpVkWkWNEPiLaZwNKPJ+rDhEPLNL+cwM4YAHzYOseb+euzVztKEZCnW2N/rqKZXWr7or6lxWnQ88AcM2ti6wnDsvWhhkQj4gOlPKTjPZikfbfVgBlAAZ1fbBl46vrQv6OlJ7qrYIM+H2+2ooza567Per1jZ0OXQPw3atm6mdMHWoqNCAeER3IB+Bp1SHihUXaT5FTi/8CkN31cenvCLRvXf+WmlSpScqQ9NVVXFP91C1rDBrytFMmmC84ZYL5eIPGI6IDPYUST63qEPHCIo3OVwC+QPfVjr5487Nge3PU9zZSmL9+xwNVj93woBFjOR36jClDtMt/NEOfL4To/QVE1B9/UB0gnlikUYgclT6H8LXS/V9LKaW3bO1qVblSib+p+r2m9x672oixnA59zGCbuN61wLpINwlL768gon54FSWeL1SHiCcWaZS85aXbAawBMLzr4+3ffLQ10Fz3jZpUqSHY1lTpr6041aAZuvkWE264Y6l1ea5V5PX+CiLqp7tVB4g3Fqkx/o3wPaUHbP7duumN1VLKqEsgHYU6vc2+uspltS/9JuqlF50O3QLgxz9faD11VK42woB4RNSzT1HieVN1iHhjkRrAW15aj/DOMAcclfqqy2v9DTs+VZMqeclgwO+rqzyn5tlfRL1/YWSD7osvO04/c8Zwk8OAeER0aGl1bXQvFqlxViO8XdAB95a2bnz1LS4d2HdSSvjqKm+qfvKmNwwa8sRl40zfdjrMswwaj4h6tgvAs6pDqMAiNYi3vLQVwD/QbQZvwFPd0rHtU94O00f+hh2PVT163f8YMZbToU91DNSuumqmZYHGKbpEsXYfSjx+1SFUYJEaay2AegC5XR9s2fjyh0Fv0y41kZJHwFPzYdO7j15mxFhOhz5yQKa48ReLrIutZpFhxJhEdEgtAP6mOoQqLFIDectL967mMfCAJ6SULRtf/T8pQyElwZJA0OvZ1Vn9zYne8tKov0ZOh56ja7i+ZIl1eX6GKDAiHxEd1kMo8XhUh1CFRWq8jQA2o9spXl/V1zW+qvIP1ERKbCFfR5uvrvLEun/+NuqlFZ0OXQdw5U3zLafa87XRBsQjosMLALhXdQiVWKQGixxRPY7wNmsH3PTf/PG/3g11evcoCZagZDAQ9NdVXlTzzM+/jHasyAzd8y+eqp89Z5T5GAPiEVHv/o4SzzbVIVRikcaAt7y0CsCLAA64Z1H6OwNtm9/+j5pUiclXV3l71RMr/s+g4RYvGGO69NzJ5iKDxiOiw5BStgG4Q3UO1ViksfM6wtPBB3R9sH3rJxW++u0b1URKLP6GHc9XPXrdnUaM5XTok8YXiKuvLbIsNGmC39dEcSCEuAclnrRfV5w/cGLEW17qB/AwwjN4TV2fa/7oX6tDAV+bkmAJItBct9FbXnqREWM5HfrQXCtW3LbIuiTDLDJ7fwURRUtKWQfg96pzJAIWaQx5y0u/QXihhgNO8Ya8Te3t5eteVZNKvWB7c42v5psTGt95xIgZulmaCM/QHWjTBvb+CiIyghDiNyjxtKjOkQhYpLH3b4Tvscrp+mDb5nc+9zfs/ExNJHVC/s52X23FybUv/roh2rGcDt0M4Ec3zrWcNmGAyR59OiLqCynlVgB/VZ0jUbBIY8xbXtoG4BEAgxFe2H4fz7rn3aFOb6OSYArIUDDkr634bs0zP496/eHIDN1vnTvZfM6iseZjDYhHRH0khLgNJR4ufRrBIo2PzwCsQ/dTvB2tvpaNr/wjXRZq8Ndt+39VT6x4waDh5s0eafrexVP1uQaNR0R9IKXcAOAZ1TkSCYs0DiL7aT4DwA8gu+tznTu/2N2xfVPKr8Xr37Pr/5ref/I2I8ZyOvQJY/LEtTfOtSwyacLU+yuIyChCCBdKPNwesgsWaZx4y0sbEV6Lcgi6zeJt+fjfawPN9VuVBIuDQEvDl77qLecYtEH3oCwdK25fbF1i00WWEfmIqM/eQIlnteoQiYZFGkfe8tJPAbwKYFT35zylL/wzFW+JCXa01vvrty+rW3VX1LtCOB16pgCuu32x9cQhWdoQI/IRUd9IKUMAblGdIxGxSOPvRQA7AQzq+mCwua617Yu3/yVl6pwxkQFfp7+24vSa526tjnYsp0M3AbjsmiJL8aTBpqMMiEdER0AI8QBKPOtV50hELNI485aXdgJ4AEBG5G2f9i2lW3w1W9YpCWYwGQpJX23F5dVPu0oNGvIMp8N83vJxpuMMGo8M1hGQmP33Vkz7ayuO+Usrfvl2B4DwZu2/eLMDE+9vxaQ/t+K+0s4eX//YRh8K729F4f2teGxjeEJoZ0DilCfbMOUvrfjLR/sniV7+f+1YXxWM/SdFAICQlNUAfq46R6Iyqw6QjrzlpbtthUWPArgcQAWAfYehzev+8caAk348xmTLG3Go1ycDf/22e6sev/FxI8ZyOvRZ04dpP/juNH0e9+dOXFYT8NZ3s5BtEfAHJRY80oZTCwP4si6EHc0SX12dBU0I1LYdPEl9T7vEHe924uPLsyEAHP9gK5wOHWu2B7BgjBk/X2jB/Ie9+PEsCz6tDiIYAmYM5zyzeNGEuAElnqh3Z0pVPCJVZy2A/wIY2fVBGfQHPR88+2zI35G0K4b4G3e/0bTmiRVGjOV06OOGZ4sbbp5vXaybhG7EmBQbQghkW8K/6PhDgD8YvnH6gY99uH2xFVrkl6AhWQf/2HltSwAnjjdjQKZAQabAiePNeHVLALoGeP0S/iCw96rHbW934tfLrPH6tNJeMCTfQInnWdU5EhmLVJHIDNYnAHgA5Hd9LuCpaWlZ735WhoIBJeGiEGzds8VXVX66QTN0B2SYcWPJEuvSbIvI6f0VpFowJDH9r60Y8vsWnDjejKJRZnzTKPHc537MfLAVpz7VhvKGg0/J7moJYXTe/h9Ho3I17GoJ4cSjzKhsCmHOQ224tsiCVWV+zBiuYUQOf3TFQ0jKTpMmrlKdI9Hxu1Ehb3lpK8LXS/MBHHC01bnzi93esg/+rSRYP4U625p8tRXL61bd1fNFsCPgdOhWAFffush60vAcbZgB8SgOTJrAxiuzsfPGHHy4O4jPa4PoDEhkmIGPL8/Gj2ZYcNmqjj6PZ9YEnj7Hhg1XZOO8yWbcu86HFXOtuPG1Dpz7vBeryqKeDE6HIYBfosSzRXWORMciVcxbXroF4cUaRqPbEoJtm9/+vHPXV2uUBDtCMuj3+Worzqx5/vbt0Y7ldOgagO9ecbzuPHaoaaIB8SjO8jMEltrDp2dH5Wr41qTw74lnH23GZzUHH5GOzNGww7P/2unO5hBGdjvq/MtHPlw6Tce6nUHkWQWeOzcTf/gvV6mLlUBIfiqEuFt1jmTAIk0MqwG8C2Bs9yc8655/y99U/VX8I/WdlFL6aiuurX7qlvcMGvKUk48yX3BqoXmGQeNRHNS1hdDUET6j3+6XeH1rAEcP0nDW0Wa8XRm+SvHutiAmDjz4x87JE8xYvTWAxnaJxnaJ1VsDOHnC/rmQje0S/ykP4NJpOrx+CU0AQoT/HjJeSMqAWROXosTDqdF9IFLpvsVkZisssgK4CeEj06quzwlrlmXA8ssvM2XmDFUSrhe+um0P7n7ox1cYMZbToU8/ZrBWcsdS66kWk7AYMSbFx2c1QXz3X+0IhoCQBM4/Rsfti61o6pC4+KV2bPeEkG0R+GtxBqYNM+Hj3UH89WMf/tcZ3kL24Q0+3LkmfFXgFwut+P5x+//33/BqB8482owldjM6AhLOZ7zY1SJx5fEWXFPEbxOjBULyt+ZfNd+qOkeyYJEmEFthUQGA2xE+U3DArjDmgpF5BYsuuVyYLTYl4Q7B31S9pvHNBxcbNLlo9GCb+O0fT8lw5lpFnhH5iOjI+IOyXDeJKdzdpe94ajeBRNbjvRdAFoADCjPQuMvTsvGV52QolDCnWoJtTdt8u8tONahE8ywm3FiyxLqMJUqkRkhKv24S57NEjwyLNMF4y0u3ITyTdxi6LZjRse3T7W1fvfeSTIDTCCFfe4uvrmJ53aq7ol4f2OnQLQB+/LMF1lNG52kje30BEcWEL4ibUeLZqDpHsmGRJiBveeknAF4AMAbdZvJ6v3xvc3v5ulUqu1QGAwFf7dZza5699Ztox4ps0H3R96brZx4/wnS0AfGIqB9affL1jN8036s6RzJikSYuN8IrH43u/kTrptc3dlRueC3+kcLrpvrqKm+pfvJmo7ZSOmGp3fSds442zzZoPCI6Qu1+WZ9tEReozpGsWKQJylteGgLwCIBtAA5ad7dl/X/Wdezc/G68c/kbdjxR9eh19xgxltOhT5k4ULvqx7MsCzQuokukREjKUHsA56LE09j7R1NPWKQJzFte2oHw5KMGAAfd+tJc+o93Oqvjt1tMwFP7UdO7j37PiLGcDn1EQYZY8YuF1sVWs8jo/RVEFAt72uXKAb9rjvsv5amERZrgvOWlHgB3A/Ci2x6mAOBZ+/RrvvrtMZ8cEPR6dvvqKk6MHClHxenQc0wCN5QssS4vyBQDjMhHREfO0yFLB9k03i8aJRZpEvCWlzYAuAtACMBBxdP03uOr/I27N8fq7w/5O9r89dtOrP3HrzzRjuV06DqAK26ebzllXIF20PVfIoqPjoD0WM1wosSj/C6AZMciTRLe8tIaAL9HeHH7A++zlCHZ9O5jLwWa6wxfXFqGgkFfTcV3qp/+WdRFHZmhe963p+pnzx1tnmJAPCLqh5CU0uuXF2X8prlWdZZUwCJNIt7y0h0In+bNjrztI4P+YOPbDz3rb6opM/Lv9NVVllQ/+dN/GTTcovmjTZeeN9k8x6DxiKgfGtvl/QN+1/KK6hypgkWaZLzlpd8AuAfhU7wHrH4kA75g49v/+7y/YecmI/4uf8POF6seufY3RozldOiOcfnimmuLLItMmuD3HZEiDd7QBwNt2g2qc6QS/kBLQt7y0i8B/A/CM3kPnPEaCoYa33n4JV9txcfR/B2B5vrPvOXrzo9mjL2cDn1ojgUrbltsXZypi0wjxiSiI9fYLnd0BHAySjxRTxqk/VikScpbXvopwksJDgdwUDk1rXnC3bm77P3+jB1sb6n11VUsb3znESNm6GZpAteWLLGeMMimHTTrmIjio9UnW8sagstH3tPSqjpLqmGRJjFveek6APcBGILwQvcH8Pz3uTc7tn325pGMGfJ3tvvrKk+pfaGkPtp8ToduAvDD6+dYTi8caBoX7XhE1D++oAz8d0fwojn/21auOksqYpEmuci6vH9A+Jppbvfnmz/+1/veLR+6+7LQvQwFQ/66isuqn3ZtiDZXZIbut86ZZD5nid18bLTjEVH/hKSUH+wI/uzEJ9r+ozpLqmKRpgBveennAFYifFRa0P351k9f/dhbtvafUoYOe6rWX7dtZdXjK541KNacWSNM37/4WH2uQeMRUT98sjv08D3/9f1BdY5UxiJNEd7y0nIAdyK89dpB1yLbvnhrU+umN5+VwUCP+wz69+xyN73/pCErnDgd+lGjcsV1K+ZZFpk1Ye79FUQUC5vrgu/8+r3Oy1eV+bnoQgyxSFNIZC/T3wIIIHzd9ADt5f8t95S++HDI19Hc9fFAS8NXHds3fcugDboH2XSs+OVi61KbLg66bktE8bHdE/r6xc2B01aV+TlDN8ZYpCnGW166G+EybUV4c/AD+KrKahrffeTvQa9nNwCEOlob/HUVyxpevb/HI9Uj4XTomQCuuX2x9aSh2dpBRU5E8VHvDdW9uTWw9La3O9pVZ0kHQuUG0RQ7tsKiAgArEL7XdFf354VuLcgtOvdE6Wu/rPqpW6LeQcbp0DUAV1492/LDk44yHxfteETUP00dsvnNrYGl5zzvXa86S7pgkaYwW2FRDoCrAEwGsB3hRe8BwIrw/acrveWlXxnxdzkd+pmnTzRf/6MZ+hJuLUqkRlOHbH3pS/+Zl/27/S3VWdIJT+2mMG95aQuAPwJ4B4Ad4QXvNQAjATxqYInOnDZU++H3p+vzWaJEang6ZNujG30/YInGH4s0xXnLS/0AHgPwFIBRAMYDeA2AIRv5Oh26fVi2uOHm+dZFuknoRoxJREemuVN6//KR76a3KoIvqM6SjnhqN43YCoumA5gK4NlIwUbF6dALMsz41b2nZJwzIkcbHn1CIjpSLZ3S+z+lvpIPdwXvWVXmD6rOk45YpNQvToduBXDTr5daL5k2zDRRdR6idNTqk+33lfp+tW5n8PcsUXV4apeOWGSG7qWXH687WaJEarT5ZMefPvTdyRJVj0VK/XHyieNNF55WaJ6pOghROvL6ZcefPvSt/GBHcCVLVD0WKR0Rp0OfNnmwdsUVMy3zNU7RJYq7dr/s/POHvt+v3RH87aoyf0B1Hgqvy0rUJ06HPmqQTdz4swXWxRaTsKrOQ5RuPB2y7c8f+e5ZtzP4a5Zo4mCRUp84HXqexYQbS5ZYl+dliHzVeYjSTXVrqOF373f+4ZtGefeqMn/Us+7JOCxS6pXToVsAXHnLfOspY/K0karzEKWbLXtCu37zXucf97TL+1miiYdFSocV2aD7ou9O08+eNdI0SXUeonTzye5g+f97v/OvviD+tKrMH/XmEmQ8Fin1ZtnisaaLz55knqU6CFG6Wf1N4NM/fei7D8ATPBJNXCxSOiSnQz9mwgDtJ1fPtizUhOAMb6I4CUkZevZz/7pnPw/8HsAq7ima2Fik1COnQx+en4EVty2yLraaRYbqPETpwheU/gc+8r3zZkXwNwDWrCrzc/m5BMcipYM4HXo2gOuvLbLOKsgUA1TnIUoXbT7Z9vsPOlevrwrdvqrM/7nqPNQ3PF1HB3A6dDOAKwAM+POHvv9UNoXKVWciSge7mkPVrjc6nl9fFbqRJZpcuGg97ROZoXshgFMAVACAJiBumW9ZOmeUaSEXMiKKjbXbA5//cZ3vFV8Qd68q89eqzkNHhkVK+zgd+gIAlwOoBHDA5IYLjjFPOv8Y/WzuOUpkHF9Qdj660f/Bf74OvA7gT6vK/C2qM9GR4zVS6ioz8l8rgPauTzz3ReDLrY2hhmuKrOfnZ4iB8Y9GlFrqvaGaO9f4PtiyJ/Q6gIdXlfk7VWei/uERKe0TObU7C+FrpK0AGrt/TI4FumuB9ZSpQ00z4p2PKFV8Wh38/LdrOjd2BPAMgFd5e0tyY5HSQZwOfSyA6wBkAajq6WPOP8Y86bzJ+hlWs8js6XkiOlggJAMvfBF4/5nP/Z8jfCq3THUmih6LlHrkdOh5AK4EMAnADgAH7Xk4Ll/k3DzfevbIXG1cvPMRJRtPh2y4a23n2k21oQ8APLiqzH/QGR9KTixSOiSnQ9cBnA2gGEA9gIMmQmgC4toiy7zFY03LTBpXPyLqyea64Obfvte5scWHlwD8m1ugpRYWKfXK6dAnI3x0mglgN4CDvmnmjzYNv2Km5RxORCLaz+uXLU986n/fXR7YBuCBVWX+z1RnIuOxSKlPIqd6LwUwE+EyPWiGISciEe33RW1w411rOzc3dqAM4RKtU52JYoNFSn3mdOgagIUALgHQgfDp3oOcf4x50rmT9dMzzMIWz3xEiaDVJz1PfuZf/XJ5oAPAawBe4PZnqY1FSkfM6dBHIrxwwxgAO9HDRKQhWSLj2iLLCVOGaDM0LolEaUBKic9qQh///oPOL5o74QXwMIBPuOh86mORUr84HboFwFk4zEQkAFg01jTye9P10wfZtGHxzEcUT82dshq++9UAAAtnSURBVPHRjb7X39ga7ATwXwDPrCrzN6nORfHBIqWo9GUiklmDuPx4y+xl40xLLSZhjXdGolgJSSk3VIU+vPuDzi/b/GgD8BCAjTwKTS8sUopaZCLStwHMAdAAoLmnjxubJ7KvKbKcPHGgaUo88xHFQm1baNejG/3vvb896AfwPoDnVpX5e/zep9TGIiVDRJYXPAbA9wAMRPjo1N/Tx55WaB5/0RT9tDzeKkNJqNUnPe6vA28+vcnfKLHvKHQTj0LTF4uUDOV06FYAJyF8/dQHoKanj8s0w/ST2Zb580abFpo1wc0TKOH5grJz7fbg+3/92PdFewD5AN4C8OKqMn+r6mykFouUYsLp0IcDuBjAVAC1CP/mfpCJA7W870/Xl0warE3j7F5KRCEp5ee1oU8e+Mj3/q4WmQegCcD/rirzb1adjRIDi5RiJnK6dybC951mA9iFHm6VAYBpQ7WBl07Tl04YoB3DPqVEsa0ptOXhDb7XN1SHzAA0AP8BsHpVmd+rOBolEBYpxZzToWcBOAPAKQgfmR5yhZeikaahFx+rL7PnaxPjlY+ouz3tsvaFL/yvucsDLQjvgvQ+gH+tKvP3uAgJpTcWKcVNZHu27wI4CoeZ3QsAi8eaRl0wRV82ijvLUBx5OuSetyoCax771L81JDEQQBnC94RWqM5GiYtFSnHldOgmANMBXABgCMKLORxyssYpE8z2cyaZlw/N1kbFKSKlocZ2Wff61sCaZzb5twTlvu/LpwB8xk23qTcsUlLC6dDNAGYBOB9AAcITkg553emso82FZ0w0Lx2cpQ2PU0RKA/XeUNWrWwJrXvgiUC6B4QjfsvUCgDVcH5f6ikVKSkWWGpwL4FyEJyTVILwgfo+WjzONKZ6oF40vEJM4y5f6a4cntPXl8sDayPZmQwGYEV5g/uVVZf4el7skOhQWKSUEp0PPQHhnmbMBZACoRvg+1B5NGKDlXnCMefb0YaYZVrPIjFNMSmIhKUPlDaEvXvwy8MG6ncF6AMMQnom7BsArq8r81WoTUrJikVJCiczwXYrwLF8zwoXa4wpJQHgP1Aun6McuGGMuKsgUg+MUk5JIR0B6N9WENj7zub90y56QF+ECFQgvqLCa+4RStFiklJCcDj0XwAkIr5JkRXiW72FXkDl1gnncqYXmOWPyRCFP+6a3kJRyh0d+8/72wPp/fhUo8wVhQXhyWxDA6wDeWFXm36M2JaUKFiklNKdDtyG8qMPpAAYjPCGpHj3sMrPX5MFawXmT9dmTB2tTM3WRFZ+klAhaOmXThurghn9+6d/4TaNsRvi6+0CEv2/cAN7nwvJkNBYpJYXIbTNHI7yowxQAAYQnJgUO9RqzBnHKBPP4BWNMUycM0I7mFm6pKRCSgW/2hL58syK44bUtgQoZPm07AOESrQfwL4Q32D7kJDaiaLBIKek4HfoIAIsBLANgQvi0b49r+e5l02E+Y6J54tzR5qlj80ShSROmOESlGKr3hqo/3BVc/4/NgU31XtmB8J64gxCeQLQZwGoAn68q8/e4LCWRUViklLScDj0bQBGAYoTvRW0DsAfAYW+gH5gprGcebZ48c4Rp6ogcYef11OQQklLWtsmdm+tCZW9XBL76tCbUgPAvUoMRvo7uQfj654ecQETxxCKlpBdZ3OEYhI9Qp0Yebkb4B+thv8HH5onsMxz6lKlDtElDs8UoTQgttmnpSARC0r/DI7d+WhMsW/1N4OudzXLvmYcchH95CgH4GMC7AL7m0SepwCKllBKZ7TsVwBIAExAu0kYAvd5kPyBTWJeNM42bNtR01LgC7ajc/9/e3fzGdZVxHP+deY/j98ZxPS2kJURWKwGiLUIsukIIxCISLGjLX4Dav6A7hIRgSfcVCxD/gDeILlO1QWqgCEplR+TFHr8EZxw7icdjZ+beh8Vzp74yjhNxsMeyvx/p6M7LtXUdKfc355znnKmGsUO9WOyr3bHWjfX0+rXlZPaDG92bm4++mAcvyytvS/Ivjv9A0qcUD6HfCFKcWJeny+ckfUPeU52Sh+qaDtiKMO+lc4Wx1y8UL748Ubz4/HB4kWKlw5Gklq61beXWenr7o0Yye2U+WUrti5GEmrxwqCRfT3xF0keS5mfmOty8cCwQpDjxsu9FrUt6Rb7Zw6h8PeE9Se2n+R3lggqvXyg+96168eJXxwsXJ86GOsPA/5udrm3f2bTGrY208dlqsvBxI1nK9Tolr7Ydk1ffbkq6Kulvkm70Y//bEMIPJL0nn49938x+vef9qqTfSXpV/kHtDTO7fdTXif4hSHGqXJ4uFyS9IF+b+m35DVvyQF3XActp8gbKKr1WL06+PFGoXxgpTD07GOpjZ8IE4frfHu7YxuKDdOHGerrw15Wk8ZfldHXPXSfIP9wMZ8+XJX0s6TNJjX5++0oIoSjpuqTvSVqU9Imkt8zs89w5b0v6upn9LITwpqQfmdkbfblg9AVBilMr66lOyL8f9VX53Gope/u+fF71qf+D9ML1pXOFqQujhfrUYJgarYXzxcLpCNduat0HO7q3tpXeXW1Z8/ZGuvrnxWRh/r7ttyNVTdJIdjRJs/Ke56yk5nEZtg0hfEfSz83s+9nzdyXJzH6VO+dP2TlXQwi9bS0njJvrqVF68inAyZTdrFezdjWr/v2ypGl5j/VF+U2+Nwx84IL+rY66V+aTpSvzyVLvtTMlFV+rFycvPVOYnDwbxs4NhPHRWhgfroaxainUDulPO1Q7Xdte37a7a1vWvLNpzfn76d3ra2lztplu5OY284J8uHZEvsZT8t7/NUmfyqttD9z+sY+ek9TIPV+Uj2Tse46ZdUMI9+W7KTWP5ArRdwQpkJmZ63Ql3czaH7N1ql+R91RfkVeMmjwY2vIe64Hh2u4q+XAhWf5wIVne+96zg+HMpfHC2NRQGJkYKAw/MxBGRmsaGaqE4bOVMFwpqloqqHJU61y7qXW3u2q1O9ZqddRqPbLWw0e2+XBHrY1ta/27lT74/G7azC1BeZySfJh2ULv/Xg35cO2/JC1I2jguvU4gFkEKPEbWS/p71v5webo8JC9aqsu3K7yk3XAtyKuBnxiuPXc2rX1nM2nL5wT3FSSN1lQZrYXqaC1Uh6qhOlQJ1cGKqgPlUB0oh+qZsqq1klcUp2ZpkipNTWli2THtPfb3uqmSxJRudexRc8s2V1vWWnmYtta3H/+1dQeoSRrIWq+32ZE0J+kfkm5LWpyZ6zxVUdcxtCTpS7nnz2ev7XfOYja0OyIvOsIpwRwpECEL1yn58N60PFzH5RsFhOzYztq2nrDr0jFVkG+/NyAPzp4gH6JtSLol/0CwImnlpGyMkAXjdUnflQfmJ5J+amb/zJ3zjqSv5YqNfmxmP+nLBaMvCFLg/ywbEp6Sz5NNynsrdXlhU0G7w50FeZXwtqQdeU+uK5+TPQpBvslBJXesZNeV5q7T5AU0C/LAXJXP/zVn5jo7R3StfRNC+KGk38iXv/zWzH4ZQviFpGtmNhNCqEn6vaRvyufS3zSzm/27Yhw1ghQ4ItnSm5GsDWfH8/KwPS/f9q7X67Ncywu5ZrnH2nN+/ufygag9P9/bSnFDHgL3ssct7e5dvH5SepjAYSBIgWMmW5ZTkW/EXsuO+7WKvPeayHuQ+fa417a0G5JtCn6AeAQpAAARTsVCcQAADgtBCgBABIIUAIAIBCkAABEIUgAAIhCkAABEIEgBAIhAkAIAEIEgBQAgAkEKAEAEghQAgAgEKQAAEQhSAAAiEKQAAEQgSAEAiECQAgAQgSAFACACQQoAQASCFACACAQpAAARCFIAACIQpAAARCBIAQCIQJACABCBIAUAIAJBCgBABIIUAIAIBCkAABEIUgAAIhCkAABEIEgBAIhAkAIAEIEgBQAgwn8AekH69OrgIhcAAAAASUVORK5CYII=\n"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Just the mathematical way of above graph,"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df['target'].value_counts())\nTotal = np.add(train_df['target'].where(train_df['target']==1).value_counts(), \n               train_df['target'].where(train_df['target']==0).value_counts())\nprint('target 1 : ',(train_df['target'].where(train_df['target']==1).value_counts()/Total)*100)\nprint('target 0 : ',100 - (train_df['target'].where(train_df['target']==1).value_counts()/Total)*100)","execution_count":10,"outputs":[{"output_type":"stream","text":"1.0    160\n0.0     90\nName: target, dtype: int64\ntarget 1 :  1.0    64.0\nName: target, dtype: float64\ntarget 0 :  1.0    36.0\nName: target, dtype: float64\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df.isnull().any().sum())\nprint(test_df.isnull().any().sum())","execution_count":22,"outputs":[{"output_type":"stream","text":"0\n0\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Luckily we don't have any missing values in both training set & test set.\nnothing to do more here.."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.describe()","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"               id      target     ...             298         299\ncount  250.000000  250.000000     ...      250.000000  250.000000\nmean   124.500000    0.640000     ...        0.009372   -0.128952\nstd     72.312977    0.480963     ...        1.008099    0.971219\nmin      0.000000    0.000000     ...       -3.211000   -3.500000\n25%     62.250000    0.000000     ...       -0.550000   -0.754250\n50%    124.500000    1.000000     ...       -0.009000   -0.132500\n75%    186.750000    1.000000     ...        0.654250    0.503250\nmax    249.000000    1.000000     ...        3.530000    2.771000\n\n[8 rows x 302 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n      <th>21</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n      <th>25</th>\n      <th>26</th>\n      <th>27</th>\n      <th>28</th>\n      <th>29</th>\n      <th>30</th>\n      <th>31</th>\n      <th>32</th>\n      <th>33</th>\n      <th>34</th>\n      <th>35</th>\n      <th>36</th>\n      <th>37</th>\n      <th>...</th>\n      <th>260</th>\n      <th>261</th>\n      <th>262</th>\n      <th>263</th>\n      <th>264</th>\n      <th>265</th>\n      <th>266</th>\n      <th>267</th>\n      <th>268</th>\n      <th>269</th>\n      <th>270</th>\n      <th>271</th>\n      <th>272</th>\n      <th>273</th>\n      <th>274</th>\n      <th>275</th>\n      <th>276</th>\n      <th>277</th>\n      <th>278</th>\n      <th>279</th>\n      <th>280</th>\n      <th>281</th>\n      <th>282</th>\n      <th>283</th>\n      <th>284</th>\n      <th>285</th>\n      <th>286</th>\n      <th>287</th>\n      <th>288</th>\n      <th>289</th>\n      <th>290</th>\n      <th>291</th>\n      <th>292</th>\n      <th>293</th>\n      <th>294</th>\n      <th>295</th>\n      <th>296</th>\n      <th>297</th>\n      <th>298</th>\n      <th>299</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>...</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>124.500000</td>\n      <td>0.640000</td>\n      <td>0.023292</td>\n      <td>-0.026872</td>\n      <td>0.167404</td>\n      <td>0.001904</td>\n      <td>0.001588</td>\n      <td>-0.007304</td>\n      <td>0.032052</td>\n      <td>0.078412</td>\n      <td>-0.036920</td>\n      <td>0.035448</td>\n      <td>-0.005032</td>\n      <td>0.110248</td>\n      <td>0.019808</td>\n      <td>-0.001108</td>\n      <td>-0.016280</td>\n      <td>-0.039644</td>\n      <td>0.017260</td>\n      <td>-0.106856</td>\n      <td>0.036184</td>\n      <td>-0.043296</td>\n      <td>-0.110832</td>\n      <td>0.072680</td>\n      <td>0.017296</td>\n      <td>-0.030728</td>\n      <td>-0.128252</td>\n      <td>0.154736</td>\n      <td>0.083408</td>\n      <td>0.039552</td>\n      <td>-0.091784</td>\n      <td>0.054636</td>\n      <td>-0.048288</td>\n      <td>-0.017296</td>\n      <td>0.007708</td>\n      <td>-0.134460</td>\n      <td>0.093852</td>\n      <td>-0.020588</td>\n      <td>-0.002492</td>\n      <td>-0.141400</td>\n      <td>...</td>\n      <td>0.005780</td>\n      <td>-0.102304</td>\n      <td>-0.013796</td>\n      <td>0.089384</td>\n      <td>0.036368</td>\n      <td>0.016276</td>\n      <td>-0.069448</td>\n      <td>-0.113236</td>\n      <td>0.035696</td>\n      <td>0.034484</td>\n      <td>-0.066236</td>\n      <td>-0.057988</td>\n      <td>0.091556</td>\n      <td>-0.029896</td>\n      <td>0.115648</td>\n      <td>0.007372</td>\n      <td>0.033552</td>\n      <td>0.090524</td>\n      <td>0.001576</td>\n      <td>-0.007784</td>\n      <td>0.043184</td>\n      <td>0.082696</td>\n      <td>0.098476</td>\n      <td>0.055356</td>\n      <td>0.111708</td>\n      <td>-0.015688</td>\n      <td>0.035992</td>\n      <td>0.026452</td>\n      <td>-0.059152</td>\n      <td>0.077272</td>\n      <td>0.044652</td>\n      <td>0.126344</td>\n      <td>0.018436</td>\n      <td>-0.012092</td>\n      <td>-0.065720</td>\n      <td>-0.106112</td>\n      <td>0.046472</td>\n      <td>0.006452</td>\n      <td>0.009372</td>\n      <td>-0.128952</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>72.312977</td>\n      <td>0.480963</td>\n      <td>0.998354</td>\n      <td>1.009314</td>\n      <td>1.021709</td>\n      <td>1.011751</td>\n      <td>1.035411</td>\n      <td>0.955700</td>\n      <td>1.006657</td>\n      <td>0.939731</td>\n      <td>0.963688</td>\n      <td>1.019689</td>\n      <td>1.085089</td>\n      <td>1.036265</td>\n      <td>1.050041</td>\n      <td>1.024305</td>\n      <td>0.926789</td>\n      <td>0.955915</td>\n      <td>1.025655</td>\n      <td>1.012777</td>\n      <td>0.945099</td>\n      <td>1.055935</td>\n      <td>1.003178</td>\n      <td>1.039556</td>\n      <td>0.988482</td>\n      <td>0.945902</td>\n      <td>0.997026</td>\n      <td>0.997894</td>\n      <td>1.040371</td>\n      <td>0.922270</td>\n      <td>1.047282</td>\n      <td>1.041432</td>\n      <td>1.010971</td>\n      <td>0.992464</td>\n      <td>0.986350</td>\n      <td>1.015563</td>\n      <td>1.117898</td>\n      <td>0.958191</td>\n      <td>0.948855</td>\n      <td>1.042429</td>\n      <td>...</td>\n      <td>0.994761</td>\n      <td>1.094494</td>\n      <td>1.026025</td>\n      <td>0.963489</td>\n      <td>1.026373</td>\n      <td>1.008207</td>\n      <td>0.989451</td>\n      <td>1.002857</td>\n      <td>0.944743</td>\n      <td>1.023709</td>\n      <td>0.985451</td>\n      <td>0.951879</td>\n      <td>1.027877</td>\n      <td>0.966882</td>\n      <td>1.037173</td>\n      <td>1.004543</td>\n      <td>1.006219</td>\n      <td>1.037119</td>\n      <td>1.024067</td>\n      <td>1.056086</td>\n      <td>1.012516</td>\n      <td>1.068741</td>\n      <td>0.934163</td>\n      <td>0.988100</td>\n      <td>1.043230</td>\n      <td>1.010720</td>\n      <td>1.058982</td>\n      <td>0.896318</td>\n      <td>1.113760</td>\n      <td>0.972530</td>\n      <td>1.011416</td>\n      <td>0.972567</td>\n      <td>0.954229</td>\n      <td>0.960630</td>\n      <td>1.057414</td>\n      <td>1.038389</td>\n      <td>0.967661</td>\n      <td>0.998984</td>\n      <td>1.008099</td>\n      <td>0.971219</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-2.319000</td>\n      <td>-2.931000</td>\n      <td>-2.477000</td>\n      <td>-2.359000</td>\n      <td>-2.566000</td>\n      <td>-2.845000</td>\n      <td>-2.976000</td>\n      <td>-3.444000</td>\n      <td>-2.768000</td>\n      <td>-2.361000</td>\n      <td>-3.302000</td>\n      <td>-2.851000</td>\n      <td>-2.681000</td>\n      <td>-2.596000</td>\n      <td>-3.275000</td>\n      <td>-3.512000</td>\n      <td>-2.476000</td>\n      <td>-3.619000</td>\n      <td>-2.428000</td>\n      <td>-3.229000</td>\n      <td>-3.024000</td>\n      <td>-2.775000</td>\n      <td>-2.962000</td>\n      <td>-2.490000</td>\n      <td>-3.107000</td>\n      <td>-2.943000</td>\n      <td>-2.933000</td>\n      <td>-2.942000</td>\n      <td>-2.957000</td>\n      <td>-2.911000</td>\n      <td>-2.568000</td>\n      <td>-2.649000</td>\n      <td>-3.031000</td>\n      <td>-2.913000</td>\n      <td>-3.265000</td>\n      <td>-2.372000</td>\n      <td>-3.037000</td>\n      <td>-3.340000</td>\n      <td>...</td>\n      <td>-2.512000</td>\n      <td>-2.873000</td>\n      <td>-2.549000</td>\n      <td>-2.721000</td>\n      <td>-2.578000</td>\n      <td>-2.239000</td>\n      <td>-3.046000</td>\n      <td>-2.755000</td>\n      <td>-2.507000</td>\n      <td>-3.369000</td>\n      <td>-2.448000</td>\n      <td>-2.771000</td>\n      <td>-2.903000</td>\n      <td>-2.522000</td>\n      <td>-2.759000</td>\n      <td>-2.915000</td>\n      <td>-2.618000</td>\n      <td>-3.623000</td>\n      <td>-2.673000</td>\n      <td>-3.229000</td>\n      <td>-2.537000</td>\n      <td>-2.748000</td>\n      <td>-2.850000</td>\n      <td>-2.577000</td>\n      <td>-2.973000</td>\n      <td>-2.709000</td>\n      <td>-3.605000</td>\n      <td>-2.357000</td>\n      <td>-2.904000</td>\n      <td>-2.734000</td>\n      <td>-2.804000</td>\n      <td>-2.443000</td>\n      <td>-2.757000</td>\n      <td>-2.466000</td>\n      <td>-3.287000</td>\n      <td>-3.072000</td>\n      <td>-2.634000</td>\n      <td>-2.776000</td>\n      <td>-3.211000</td>\n      <td>-3.500000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>62.250000</td>\n      <td>0.000000</td>\n      <td>-0.644750</td>\n      <td>-0.739750</td>\n      <td>-0.425250</td>\n      <td>-0.686500</td>\n      <td>-0.659000</td>\n      <td>-0.643750</td>\n      <td>-0.675000</td>\n      <td>-0.550750</td>\n      <td>-0.689500</td>\n      <td>-0.643500</td>\n      <td>-0.693500</td>\n      <td>-0.524000</td>\n      <td>-0.708500</td>\n      <td>-0.692000</td>\n      <td>-0.677000</td>\n      <td>-0.634500</td>\n      <td>-0.683500</td>\n      <td>-0.801500</td>\n      <td>-0.574250</td>\n      <td>-0.758000</td>\n      <td>-0.870500</td>\n      <td>-0.596000</td>\n      <td>-0.725750</td>\n      <td>-0.652000</td>\n      <td>-0.779500</td>\n      <td>-0.424250</td>\n      <td>-0.585750</td>\n      <td>-0.625000</td>\n      <td>-0.751250</td>\n      <td>-0.582500</td>\n      <td>-0.713500</td>\n      <td>-0.750000</td>\n      <td>-0.588000</td>\n      <td>-0.829000</td>\n      <td>-0.648500</td>\n      <td>-0.659750</td>\n      <td>-0.614000</td>\n      <td>-0.816750</td>\n      <td>...</td>\n      <td>-0.622750</td>\n      <td>-1.009250</td>\n      <td>-0.693250</td>\n      <td>-0.567750</td>\n      <td>-0.696500</td>\n      <td>-0.684000</td>\n      <td>-0.703750</td>\n      <td>-0.771250</td>\n      <td>-0.624500</td>\n      <td>-0.653000</td>\n      <td>-0.786750</td>\n      <td>-0.701000</td>\n      <td>-0.543250</td>\n      <td>-0.672750</td>\n      <td>-0.626750</td>\n      <td>-0.730250</td>\n      <td>-0.649750</td>\n      <td>-0.589500</td>\n      <td>-0.725750</td>\n      <td>-0.667750</td>\n      <td>-0.605000</td>\n      <td>-0.637750</td>\n      <td>-0.458250</td>\n      <td>-0.553500</td>\n      <td>-0.566750</td>\n      <td>-0.778250</td>\n      <td>-0.693250</td>\n      <td>-0.596750</td>\n      <td>-0.789000</td>\n      <td>-0.671250</td>\n      <td>-0.617000</td>\n      <td>-0.510500</td>\n      <td>-0.535750</td>\n      <td>-0.657000</td>\n      <td>-0.818500</td>\n      <td>-0.821000</td>\n      <td>-0.605500</td>\n      <td>-0.751250</td>\n      <td>-0.550000</td>\n      <td>-0.754250</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>124.500000</td>\n      <td>1.000000</td>\n      <td>-0.015500</td>\n      <td>0.057000</td>\n      <td>0.184000</td>\n      <td>-0.016500</td>\n      <td>-0.023000</td>\n      <td>0.037500</td>\n      <td>0.060500</td>\n      <td>0.183500</td>\n      <td>-0.012500</td>\n      <td>0.052000</td>\n      <td>0.066000</td>\n      <td>0.115500</td>\n      <td>0.090000</td>\n      <td>0.016000</td>\n      <td>0.009500</td>\n      <td>0.010000</td>\n      <td>-0.119000</td>\n      <td>-0.164500</td>\n      <td>-0.009500</td>\n      <td>-0.018000</td>\n      <td>-0.161500</td>\n      <td>0.048000</td>\n      <td>0.135000</td>\n      <td>-0.016000</td>\n      <td>-0.165500</td>\n      <td>0.125500</td>\n      <td>0.036500</td>\n      <td>0.045000</td>\n      <td>-0.026000</td>\n      <td>0.045000</td>\n      <td>-0.130000</td>\n      <td>0.016000</td>\n      <td>0.023500</td>\n      <td>-0.216500</td>\n      <td>0.231500</td>\n      <td>0.014000</td>\n      <td>-0.012500</td>\n      <td>-0.234000</td>\n      <td>...</td>\n      <td>-0.048500</td>\n      <td>-0.134000</td>\n      <td>-0.065500</td>\n      <td>0.097000</td>\n      <td>-0.088000</td>\n      <td>0.019500</td>\n      <td>0.001500</td>\n      <td>-0.107500</td>\n      <td>0.045500</td>\n      <td>-0.023500</td>\n      <td>-0.101000</td>\n      <td>-0.109000</td>\n      <td>0.050500</td>\n      <td>-0.081500</td>\n      <td>0.087500</td>\n      <td>0.035000</td>\n      <td>0.086500</td>\n      <td>0.126500</td>\n      <td>0.053500</td>\n      <td>0.063500</td>\n      <td>0.052500</td>\n      <td>0.103000</td>\n      <td>0.135500</td>\n      <td>0.039000</td>\n      <td>0.093500</td>\n      <td>0.014500</td>\n      <td>-0.007500</td>\n      <td>0.000500</td>\n      <td>-0.122500</td>\n      <td>0.057500</td>\n      <td>0.067500</td>\n      <td>0.091000</td>\n      <td>0.057500</td>\n      <td>-0.021000</td>\n      <td>-0.009000</td>\n      <td>-0.079500</td>\n      <td>0.009500</td>\n      <td>0.005500</td>\n      <td>-0.009000</td>\n      <td>-0.132500</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>186.750000</td>\n      <td>1.000000</td>\n      <td>0.677000</td>\n      <td>0.620750</td>\n      <td>0.805000</td>\n      <td>0.720000</td>\n      <td>0.735000</td>\n      <td>0.660500</td>\n      <td>0.783250</td>\n      <td>0.766250</td>\n      <td>0.635000</td>\n      <td>0.733000</td>\n      <td>0.694250</td>\n      <td>0.786250</td>\n      <td>0.805250</td>\n      <td>0.654000</td>\n      <td>0.611000</td>\n      <td>0.578000</td>\n      <td>0.699000</td>\n      <td>0.496500</td>\n      <td>0.686000</td>\n      <td>0.698000</td>\n      <td>0.560500</td>\n      <td>0.797000</td>\n      <td>0.631500</td>\n      <td>0.619250</td>\n      <td>0.579250</td>\n      <td>0.719500</td>\n      <td>0.798250</td>\n      <td>0.721750</td>\n      <td>0.678500</td>\n      <td>0.728750</td>\n      <td>0.670500</td>\n      <td>0.698750</td>\n      <td>0.602750</td>\n      <td>0.557750</td>\n      <td>0.736250</td>\n      <td>0.599250</td>\n      <td>0.612000</td>\n      <td>0.557000</td>\n      <td>...</td>\n      <td>0.675750</td>\n      <td>0.708750</td>\n      <td>0.726000</td>\n      <td>0.739250</td>\n      <td>0.856750</td>\n      <td>0.709250</td>\n      <td>0.616000</td>\n      <td>0.569500</td>\n      <td>0.655750</td>\n      <td>0.634750</td>\n      <td>0.525750</td>\n      <td>0.612000</td>\n      <td>0.864000</td>\n      <td>0.630250</td>\n      <td>0.836750</td>\n      <td>0.718000</td>\n      <td>0.791500</td>\n      <td>0.726000</td>\n      <td>0.688000</td>\n      <td>0.665750</td>\n      <td>0.603750</td>\n      <td>0.705500</td>\n      <td>0.688250</td>\n      <td>0.759000</td>\n      <td>0.704750</td>\n      <td>0.673750</td>\n      <td>0.748750</td>\n      <td>0.604250</td>\n      <td>0.650500</td>\n      <td>0.772500</td>\n      <td>0.797250</td>\n      <td>0.804250</td>\n      <td>0.631500</td>\n      <td>0.650250</td>\n      <td>0.739500</td>\n      <td>0.493000</td>\n      <td>0.683000</td>\n      <td>0.794250</td>\n      <td>0.654250</td>\n      <td>0.503250</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>249.000000</td>\n      <td>1.000000</td>\n      <td>2.567000</td>\n      <td>2.419000</td>\n      <td>3.392000</td>\n      <td>2.771000</td>\n      <td>2.901000</td>\n      <td>2.793000</td>\n      <td>2.546000</td>\n      <td>2.846000</td>\n      <td>2.512000</td>\n      <td>2.959000</td>\n      <td>3.271000</td>\n      <td>2.998000</td>\n      <td>2.729000</td>\n      <td>2.651000</td>\n      <td>2.913000</td>\n      <td>2.508000</td>\n      <td>3.286000</td>\n      <td>2.430000</td>\n      <td>2.557000</td>\n      <td>2.868000</td>\n      <td>2.703000</td>\n      <td>2.691000</td>\n      <td>2.604000</td>\n      <td>2.362000</td>\n      <td>2.927000</td>\n      <td>2.976000</td>\n      <td>2.581000</td>\n      <td>2.305000</td>\n      <td>2.489000</td>\n      <td>2.895000</td>\n      <td>2.457000</td>\n      <td>2.407000</td>\n      <td>2.882000</td>\n      <td>2.649000</td>\n      <td>2.914000</td>\n      <td>2.995000</td>\n      <td>2.382000</td>\n      <td>2.481000</td>\n      <td>...</td>\n      <td>2.612000</td>\n      <td>2.680000</td>\n      <td>2.964000</td>\n      <td>2.663000</td>\n      <td>2.406000</td>\n      <td>3.457000</td>\n      <td>2.496000</td>\n      <td>2.501000</td>\n      <td>2.832000</td>\n      <td>2.897000</td>\n      <td>3.753000</td>\n      <td>2.498000</td>\n      <td>2.725000</td>\n      <td>2.680000</td>\n      <td>3.445000</td>\n      <td>2.846000</td>\n      <td>2.315000</td>\n      <td>2.780000</td>\n      <td>2.364000</td>\n      <td>2.908000</td>\n      <td>2.926000</td>\n      <td>3.441000</td>\n      <td>2.319000</td>\n      <td>2.842000</td>\n      <td>3.343000</td>\n      <td>3.266000</td>\n      <td>3.061000</td>\n      <td>2.146000</td>\n      <td>2.853000</td>\n      <td>3.026000</td>\n      <td>2.865000</td>\n      <td>2.801000</td>\n      <td>2.736000</td>\n      <td>2.596000</td>\n      <td>2.226000</td>\n      <td>3.131000</td>\n      <td>3.236000</td>\n      <td>2.626000</td>\n      <td>3.530000</td>\n      <td>2.771000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"we dont need 'Id' column as it is unnecessary for model training,"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['target','id']\nX = train_df.drop(cols,axis=1)\ny = train_df['target']\nX_test = test_df.drop('id',axis=1)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\n#defining a generic Function to give ROC_AUC Scores in table format for better readability\ndef crossvalscore(model):\n    scores = cross_val_score(model,X,y,cv=5,scoring='roc_auc',n_jobs=-1)\n    acc = cross_val_score(model,X,y,cv=5,scoring='accuracy',n_jobs=-1)\n    rand_scores = pd.DataFrame({\n    'cv':range(1,6),\n    'roc_auc score':scores,\n    'accuracy score':acc\n    })\n    print('AUC :',rand_scores['roc_auc score'].mean())\n    print('accuracy :',rand_scores['accuracy score'].mean())\n    return rand_scores.sort_values(by='roc_auc score',ascending=False)","execution_count":25,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**1. RandomForest Classifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrand_clf = RandomForestClassifier(max_depth=2,random_state=42)\ncrossvalscore(rand_clf)","execution_count":26,"outputs":[{"output_type":"stream","text":"AUC : 0.6305555555555555\naccuracy : 0.64\n","name":"stdout"},{"output_type":"execute_result","execution_count":26,"data":{"text/plain":"   cv  roc_auc score  accuracy score\n1   2       0.720486            0.70\n0   1       0.635417            0.66\n4   5       0.623264            0.64\n2   3       0.612847            0.60\n3   4       0.560764            0.60","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cv</th>\n      <th>roc_auc score</th>\n      <th>accuracy score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0.720486</td>\n      <td>0.70</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.635417</td>\n      <td>0.66</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0.623264</td>\n      <td>0.64</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0.612847</td>\n      <td>0.60</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0.560764</td>\n      <td>0.60</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"* RandomForest Classifier has just avg 0.63 roc_auc score with an accuracy of 64%.\nlet's try some other algorithm"},{"metadata":{},"cell_type":"markdown","source":"**LogisticRegression using PCA data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#logistic regression using PCA\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\npca = PCA(n_components=42)\nX_new = pca.fit_transform(X)\n\nlog_clf = LogisticRegression(C = 0.1, class_weight= 'balanced', penalty= 'l1', solver= 'liblinear',random_state=42)\nscores = cross_val_score(log_clf,X_new,y,cv=5,scoring='roc_auc',n_jobs=-1)\nacc = cross_val_score(log_clf,X_new,y,cv=5,scoring='accuracy',n_jobs=-1)\nrand_scores = pd.DataFrame({\n'cv':range(1,6),\n'roc_auc score':scores,\n'accuracy score':acc\n})\nprint('AUC :',rand_scores['roc_auc score'].mean())\nprint('accuracy :',rand_scores['accuracy score'].mean())\nrand_scores.sort_values(by='roc_auc score',ascending=False)","execution_count":29,"outputs":[{"output_type":"stream","text":"AUC : 0.7447916666666666\naccuracy : 0.6519999999999999\n","name":"stdout"},{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"   cv  roc_auc score  accuracy score\n4   5       0.805556            0.72\n3   4       0.765625            0.64\n2   3       0.744792            0.62\n0   1       0.730903            0.70\n1   2       0.677083            0.58","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cv</th>\n      <th>roc_auc score</th>\n      <th>accuracy score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0.805556</td>\n      <td>0.72</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0.765625</td>\n      <td>0.64</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0.744792</td>\n      <td>0.62</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.730903</td>\n      <td>0.70</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0.677083</td>\n      <td>0.58</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"avg roc_auc score is now 0.74 with 65% accuracy.\n\nThis is better than RandomForestClassifier scores"},{"metadata":{},"cell_type":"markdown","source":"**2. Logistic Regression with Regularization**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n#simple logistic regression with lasso regularization\nlog_clf = LogisticRegression(C = 0.1, class_weight= 'balanced', penalty= 'l1', solver= 'liblinear',random_state=42)\ncrossvalscore(log_clf)","execution_count":30,"outputs":[{"output_type":"stream","text":"AUC : 0.8138888888888889\naccuracy : 0.6880000000000001\n","name":"stdout"},{"output_type":"execute_result","execution_count":30,"data":{"text/plain":"   cv  roc_auc score  accuracy score\n2   3       0.850694            0.70\n3   4       0.848958            0.68\n4   5       0.810764            0.72\n1   2       0.809028            0.68\n0   1       0.750000            0.66","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cv</th>\n      <th>roc_auc score</th>\n      <th>accuracy score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0.850694</td>\n      <td>0.70</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0.848958</td>\n      <td>0.68</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0.810764</td>\n      <td>0.72</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0.809028</td>\n      <td>0.68</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.750000</td>\n      <td>0.66</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"avg roc_auc score is improved to 0.813 now with an accuracy of 68%."},{"metadata":{},"cell_type":"markdown","source":"**3. logistic regression with lasso regularization and recursive feature elimination**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFE\nlog_clf = LogisticRegression(C = 0.1, class_weight= 'balanced', penalty= 'l1', solver= 'liblinear',random_state=42)\nselector = RFE(log_clf,21)\nselector.fit(X,y)\ncrossvalscore(selector)","execution_count":31,"outputs":[{"output_type":"stream","text":"AUC : 0.8145833333333332\naccuracy : 0.7\n","name":"stdout"},{"output_type":"execute_result","execution_count":31,"data":{"text/plain":"   cv  roc_auc score  accuracy score\n2   3       0.857639            0.72\n3   4       0.843750            0.66\n4   5       0.810764            0.74\n1   2       0.809028            0.68\n0   1       0.751736            0.70","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cv</th>\n      <th>roc_auc score</th>\n      <th>accuracy score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0.857639</td>\n      <td>0.72</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0.843750</td>\n      <td>0.66</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0.810764</td>\n      <td>0.74</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0.809028</td>\n      <td>0.68</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.751736</td>\n      <td>0.70</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"***avg roc_auc score is now 0.814 with 70% accuracy.***\n\nThis is the best score i got compared to all above algorithms"},{"metadata":{},"cell_type":"markdown","source":"**Just trying Logistic regression with elastic net**\n\nimplements logistic regression with elastic net penalty (SGDClassifier(loss=\"log\", penalty=\"elasticnet\"))."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\nsgd_clf = SGDClassifier(loss='log',penalty='elasticnet')\ncrossvalscore(sgd_clf)","execution_count":32,"outputs":[{"output_type":"stream","text":"AUC : 0.7020833333333334\naccuracy : 0.6679999999999999\n","name":"stdout"},{"output_type":"execute_result","execution_count":32,"data":{"text/plain":"   cv  roc_auc score  accuracy score\n0   1       0.736111            0.60\n3   4       0.717014            0.74\n4   5       0.713542            0.66\n1   2       0.703125            0.66\n2   3       0.640625            0.68","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cv</th>\n      <th>roc_auc score</th>\n      <th>accuracy score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.736111</td>\n      <td>0.60</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0.717014</td>\n      <td>0.74</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0.713542</td>\n      <td>0.66</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0.703125</td>\n      <td>0.66</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0.640625</td>\n      <td>0.68</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"**4. Soft Voting Classifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import VotingClassifier,BaggingClassifier, ExtraTreesClassifier, AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nlog_clf = LogisticRegression(C = 0.1, class_weight= 'balanced', penalty= 'l1', solver= 'liblinear',random_state=42)\nlog_selector = RFE(log_clf,21)\nlog_selector.fit(X,y)\nrand_clf = RandomForestClassifier(max_depth=2,random_state=42)\nsvm_clf = SVC(gamma='auto',probability=True, random_state=42)\nextra_clf = ExtraTreesClassifier(max_depth=2,random_state=42)\n#ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=200,algorithm='SAMME.R', learning_rate=0.5, random_state=42)\n\nvoting_clf = VotingClassifier(\n    estimators = [('lr',log_selector),('rf',rand_clf),('ex',extra_clf),('sv',svm_clf)],\n    voting='soft')\ncrossvalscore(selector)","execution_count":33,"outputs":[{"output_type":"stream","text":"AUC : 0.8145833333333332\naccuracy : 0.7\n","name":"stdout"},{"output_type":"execute_result","execution_count":33,"data":{"text/plain":"   cv  roc_auc score  accuracy score\n2   3       0.857639            0.72\n3   4       0.843750            0.66\n4   5       0.810764            0.74\n1   2       0.809028            0.68\n0   1       0.751736            0.70","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cv</th>\n      <th>roc_auc score</th>\n      <th>accuracy score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0.857639</td>\n      <td>0.72</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0.843750</td>\n      <td>0.66</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0.810764</td>\n      <td>0.74</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0.809028</td>\n      <td>0.68</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.751736</td>\n      <td>0.70</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Though soft voting classifier gave me better ROC_AUC socre on training set,i got just 0.822 Public score for some reason.\n\nso i had to settle with Logistic Regression with lasso + RFE model which gave 0.841 public score"},{"metadata":{"trusted":true},"cell_type":"code","source":"voting_clf.fit(X,y)\n\n#submission = pd.read_csv('../input/sample_submission.csv')\n#submission['target'] = voting_clf.predict_proba(X_test)\n#submission.to_csv('submission1.csv', index=False)","execution_count":21,"outputs":[{"output_type":"execute_result","execution_count":21,"data":{"text/plain":"VotingClassifier(estimators=[('lr', RFE(estimator=LogisticRegression(C=0.1, class_weight='balanced', dual=False,\n          fit_intercept=True, intercept_scaling=1, max_iter=100,\n          multi_class='warn', n_jobs=None, penalty='l1', random_state=42,\n          solver='liblinear', tol=0.0001, verbose=0, warm_start=F...bf',\n  max_iter=-1, probability=True, random_state=42, shrinking=True,\n  tol=0.001, verbose=False))],\n         flatten_transform=None, n_jobs=None, voting='soft', weights=None)"},"metadata":{}}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}