{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Don't Overfit! II"},{"metadata":{},"cell_type":"markdown","source":"## Distribution of points in 300D space"},{"metadata":{},"cell_type":"markdown","source":"I was interested in ellipses on this kernel https://www.kaggle.com/cyones77/t-sne-projection. –ênd I asked myself - is there some kind of second-order logic in the data, if I present a data set as the coordinates of points in 300D space?\n\n__Spoiler - there is!__\n\nLet's start the research..."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pylab as plt","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\nimport time\nfrom datetime import datetime\nimport warnings\nwarnings.simplefilter(action = 'ignore')","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score, log_loss, accuracy_score, confusion_matrix\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.linear_model import LogisticRegression","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import mannwhitneyu","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's prepare everything we need"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv', index_col = 'id')\ntrain.shape","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"(250, 301)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = train['target']\ntrain.drop('target', axis = 1, inplace = True)\ntarget.value_counts()","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"1.0    160\n0.0     90\nName: target, dtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/test.csv', index_col = 'id')\ntest.shape","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"(19750, 300)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"It will be more convenient to use the combined data set:"},{"metadata":{"trusted":true},"cell_type":"code","source":"index_train = train.index\nindex_test = test.index\nprint(len(index_train), len(index_test))","execution_count":8,"outputs":[{"output_type":"stream","text":"250 19750\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full = pd.concat([train, test], axis = 0)\n\ndel train, test\ngc.collect()","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"14"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Data set for research with some basic source statistics:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_stats = df_full.T.describe().T.drop('count', axis = 1)\ndf_stats.columns = ['source_' + c for c in df_stats.columns]\ndf_stats.head()","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"    source_mean  source_std     ...      source_75%  source_max\nid                              ...                            \n0     -0.009223    1.089171     ...         0.79625       2.929\n1      0.086130    0.985838     ...         0.74275       2.907\n2      0.027657    1.012757     ...         0.66075       2.895\n3      0.088357    0.939743     ...         0.74525       3.270\n4      0.134413    0.941277     ...         0.69475       3.432\n\n[5 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source_mean</th>\n      <th>source_std</th>\n      <th>source_min</th>\n      <th>source_25%</th>\n      <th>source_50%</th>\n      <th>source_75%</th>\n      <th>source_max</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.009223</td>\n      <td>1.089171</td>\n      <td>-2.851</td>\n      <td>-0.77575</td>\n      <td>-0.0505</td>\n      <td>0.79625</td>\n      <td>2.929</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.086130</td>\n      <td>0.985838</td>\n      <td>-2.771</td>\n      <td>-0.55150</td>\n      <td>0.0745</td>\n      <td>0.74275</td>\n      <td>2.907</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.027657</td>\n      <td>1.012757</td>\n      <td>-2.788</td>\n      <td>-0.70875</td>\n      <td>0.0285</td>\n      <td>0.66075</td>\n      <td>2.895</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.088357</td>\n      <td>0.939743</td>\n      <td>-2.404</td>\n      <td>-0.61050</td>\n      <td>0.1525</td>\n      <td>0.74525</td>\n      <td>3.270</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.134413</td>\n      <td>0.941277</td>\n      <td>-2.087</td>\n      <td>-0.47425</td>\n      <td>0.1120</td>\n      <td>0.69475</td>\n      <td>3.432</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_stats.shape","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"(20000, 7)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_stats.loc[index_train].corrwith(target)","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"source_mean   -0.179397\nsource_std     0.065762\nsource_min    -0.061177\nsource_25%    -0.154892\nsource_50%    -0.166062\nsource_75%    -0.145751\nsource_max    -0.117318\ndtype: float64"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Functions and table for comparing scores of logistic regression and make a submittion:"},{"metadata":{"trusted":true},"cell_type":"code","source":"PARAMS = {}\nPARAMS['random_state'] = 0\nPARAMS['n_jobs'] = -1\nPARAMS['C'] = .2\nPARAMS['penalty'] = 'l1'\nPARAMS['class_weight'] = 'balanced'\nPARAMS['solver'] = 'saga'","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg_scores = pd.DataFrame(columns = ['auc', 'acc', 'loss', 'tn', 'fn', 'fp', 'tp'])\n\ndef get_logreg_score(train_, target_):\n    folds = RepeatedStratifiedKFold(n_splits = 5, n_repeats = 20, random_state = 0)\n    predict = pd.DataFrame(index = train_.index)\n    \n    # Cross-validation cycle\n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(target_, target_)):\n        train_x, train_y = train_.iloc[train_idx], target_.iloc[train_idx]\n        valid_x, valid_y = train_.iloc[valid_idx], target_.iloc[valid_idx]\n        \n        clf = LogisticRegression(**PARAMS)\n        clf.fit(train_x, train_y)\n        predict[n_fold] = pd.Series(clf.predict_proba(valid_x)[:, 1], index = valid_x.index)\n\n        del train_x, train_y, valid_x, valid_y\n        gc.collect()\n        \n    predict = predict.mean(axis = 1)\n    tn, fp, fn, tp = confusion_matrix(target_, (predict >= .5) * 1).ravel()\n    return [\n                 roc_auc_score(target_, predict), \n                 accuracy_score(target_, (predict >= .5) * 1), \n                 log_loss(target_, predict),\n                 tn, fn, fp, tp\n            ]","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_submit(train_, test_, target_):\n    predict = pd.DataFrame(index = test_.index)\n    \n    clf = LogisticRegression(**PARAMS)\n    clf.fit(train_, target_)\n    \n    predict = pd.Series(clf.predict_proba(test_)[:, 1], index = test_.index).reset_index()\n    predict.columns = ['id', 'target']\n    \n    return predict","execution_count":15,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's start..."},{"metadata":{},"cell_type":"markdown","source":"First of all, let's calculate the score for the source data."},{"metadata":{"trusted":true},"cell_type":"code","source":"step = 'source dataset'\nlogreg_scores = logreg_scores.T\nlogreg_scores[step] = get_logreg_score(df_full.loc[index_train], target)\nlogreg_scores = logreg_scores.T\nlogreg_scores","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"                     auc    acc      loss    tn    fn    fp     tp\nsource dataset  0.815417  0.732  0.506695  55.0  32.0  35.0  128.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>auc</th>\n      <th>acc</th>\n      <th>loss</th>\n      <th>tn</th>\n      <th>fn</th>\n      <th>fp</th>\n      <th>tp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>source dataset</th>\n      <td>0.815417</td>\n      <td>0.732</td>\n      <td>0.506695</td>\n      <td>55.0</td>\n      <td>32.0</td>\n      <td>35.0</td>\n      <td>128.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit = get_submit(df_full.loc[index_train], df_full.loc[index_test], target)\n\nscore_auc = logreg_scores.loc[step, 'auc']\nscore_acc = logreg_scores.loc[step, 'acc']\nscore_loss = logreg_scores.loc[step, 'loss']\nfilename = 'subm_{}_{:.4f}_{:.4f}_{:.4f}_{}.csv'.format('source', score_auc, score_acc, score_loss,\n                                                        datetime.now().strftime('%Y-%m-%d'))\nprint(filename)\nsubmit.to_csv(filename, index = False)","execution_count":17,"outputs":[{"output_type":"stream","text":"subm_source_0.8154_0.7320_0.5067_2019-04-24.csv\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"__LB = 0.845__"},{"metadata":{},"cell_type":"markdown","source":"If we are looking for second-order logic, let's first check the distance from the points to the origin."},{"metadata":{"trusted":true},"cell_type":"code","source":"dist_to_origin_sqr = (df_full**2).sum(axis = 1)\ndist_to_origin_sqr.describe()","execution_count":18,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"count    20000.000000\nmean       300.267708\nstd         24.626507\nmin        207.933984\n25%        283.360689\n50%        299.423155\n75%        316.498468\nmax        408.301493\ndtype: float64"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Wow! It looks like a sphere centered at the origin with a radius of sqrt(300)! All ponts are located near it.\n\nHmm... The square of the radius is equal to the dimension of space... What does this mean? For example, for such a sphere, the coordinates of the \"bisectors\" of quadrants are 1 or -1. Or may be initial coordinates were 1 and -1, and then some kind of transformation was applied. For synthetic set this is well likely assumption.\n\nNow let's try to project the points onto the sphere and analyze the distance to it. Does it have any useful information?"},{"metadata":{"trusted":true},"cell_type":"code","source":"rad_sphere_sqr = 300\nrad_sphere = np.sqrt(rad_sphere_sqr)\nrad_sphere","execution_count":19,"outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"17.320508075688775"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_stats['dist_to_sphere'] = np.sqrt(dist_to_origin_sqr) - rad_sphere\ndf_stats['dist_to_sphere'].describe()","execution_count":20,"outputs":[{"output_type":"execute_result","execution_count":20,"data":{"text/plain":"count    20000.000000\nmean        -0.006832\nstd          0.710176\nmin         -2.900592\n25%         -0.487187\n50%         -0.016660\n75%          0.469896\nmax          2.885963\nName: dist_to_sphere, dtype: float64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.corrcoef(df_stats['dist_to_sphere'].loc[index_train], target)[0, 1]","execution_count":23,"outputs":[{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"0.06722549873039244"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.corrcoef(abs(df_stats['dist_to_sphere'].loc[index_train]), target)[0, 1]","execution_count":24,"outputs":[{"output_type":"execute_result","execution_count":24,"data":{"text/plain":"0.08679370204297157"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"mannwhitneyu(df_stats['dist_to_sphere'].loc[index_train], df_stats['dist_to_sphere'].loc[index_test])","execution_count":25,"outputs":[{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"MannwhitneyuResult(statistic=2427657.0, pvalue=0.32528354304664864)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"It looks like the distanse to the sphere has no useful information.\n\nAnd it has no difference between train and test sets."},{"metadata":{},"cell_type":"markdown","source":"Now let's project the points onto the sphere..."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full_sphere = (df_full * rad_sphere).divide(np.sqrt(dist_to_origin_sqr), axis = 'rows')\n(df_full_sphere**2).sum(axis = 1).describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp = df_full_sphere.T.describe().T.drop('count', axis = 1)\ntmp.columns = ['sphere_' + c for c in tmp.columns]\ntmp.loc[index_train].corrwith(target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_stats = pd.concat([df_stats, tmp], axis = 1)\n\ndel tmp\ngc.collect()\n\ndf_stats.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"...and check the score change."},{"metadata":{"trusted":true},"cell_type":"code","source":"step = 'projection onto sphere'\nlogreg_scores = logreg_scores.T\nlogreg_scores[step] = get_logreg_score(df_full_sphere.loc[index_train], target)\nlogreg_scores = logreg_scores.T\nlogreg_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit = get_submit(df_full_sphere.loc[index_train], df_full_sphere.loc[index_test], target)\n\nscore_auc = logreg_scores.loc[step, 'auc']\nscore_acc = logreg_scores.loc[step, 'acc']\nscore_loss = logreg_scores.loc[step, 'loss']\nfilename = 'subm_{}_{:.4f}_{:.4f}_{:.4f}_{}.csv'.format('full_sphere', score_auc, score_acc, score_loss,\n                                                        datetime.now().strftime('%Y-%m-%d'))\nprint(filename)\nsubmit.to_csv(filename, index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__LB = 0.845__"},{"metadata":{},"cell_type":"markdown","source":"It doesn't change significantly. We removed the some kind of noise. All points are really located on this sphere.\n\nLet's explore this set.\n\nFirst, let's take a closer look at the location of the points relative to the \"bisectors\" of the quadrants. For this let's define the average density of points in each quadrant."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_signes = np.sign(df_full_sphere).astype(int)\ndf_signes.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_signes.replace(-1, 2).astype(str).apply(lambda x: ''.join(x), axis = 1).nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Surprize! There are 20000 unique combinations of coordinate signs. In each quadrant is no more than one point! \n\nBut...\n\nThere are 2**300 quadrants in the 300D space. It's a very big number:"},{"metadata":{"trusted":true},"cell_type":"code","source":"2**300","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And we have only 20000 points. Can we accidentally get such a distribution of points? Yes. Is it an accident here? I hope no. This is a synthetic dataset.\n\nLet's explore the distribution of the quadrants with points."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_stats['positive_cnt'] = (df_signes > 0).sum(axis = 1)\ndf_stats['positive_cnt'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On average, for each point half the coordinates are positive. Not less 109 and not more 185. It means, there are no points with almost all positive or all negative coordinates.\n\nIs it useful? Let's check."},{"metadata":{"trusted":true},"cell_type":"code","source":"np.corrcoef(df_stats['positive_cnt'].loc[index_train], target)[0, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mannwhitneyu(df_stats['positive_cnt'].loc[index_train], df_stats['positive_cnt'].loc[index_test])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is rather small correlation for using count of positive coordinates for prediction. But not the smallest of the values found :)\n\nWhat about the quadrants themselves? Let's replace coordinates of points to coordinates of \"bisectors\" of quadrants and look at the prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"step = 'quadrants'\nlogreg_scores = logreg_scores.T\nlogreg_scores[step] = get_logreg_score(df_signes.loc[index_train], target)\nlogreg_scores = logreg_scores.T\nlogreg_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit = get_submit(df_signes.loc[index_train], df_signes.loc[index_test], target)\n\nscore_auc = logreg_scores.loc[step, 'auc']\nscore_acc = logreg_scores.loc[step, 'acc']\nscore_loss = logreg_scores.loc[step, 'loss']\nfilename = 'subm_{}_{:.4f}_{:.4f}_{:.4f}_{}.csv'.format('quad', score_auc, score_acc, score_loss,\n                                                        datetime.now().strftime('%Y-%m-%d'))\nprint(filename)\nsubmit.to_csv(filename, index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__LB = 0.748__"},{"metadata":{},"cell_type":"markdown","source":"The distribution of quadrants contains meaningful information for prediction, but not all of its. The distribution of points within quandrants is important too.\n\nLet's calculate, for example, angles between vector of point and vector of \"bisector\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_stats['angle_w_bis'] = np.arccos(abs(df_full_sphere).sum(axis = 1) / rad_sphere_sqr)\ndf_stats['angle_w_bis'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hmm... It looks like another spheres with the same radius which centered at the intersection of \"bisectors\" with the source sphere. Each ponts is located near the intersection of such sphere with the source one."},{"metadata":{"trusted":true},"cell_type":"code","source":"np.corrcoef(df_stats['angle_w_bis'].loc[index_train], target)[0, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mannwhitneyu(df_stats['angle_w_bis'].loc[index_train], df_stats['angle_w_bis'].loc[index_test])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The magnitude of the angle is not important. \n\nThe distribution of angles on the train and test sets differs only slightly higher than for previous statistics. But it can still be considered the same."},{"metadata":{},"cell_type":"markdown","source":"## To be continued...\n\nI hope :)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":1}