{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import eli5\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold, cross_val_score, GridSearchCV, RepeatedStratifiedKFold\n\nfrom eli5.sklearn import PermutationImportance\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\nfrom sklearn.feature_selection import GenericUnivariateSelect, SelectPercentile, SelectKBest, f_classif, RFE, mutual_info_classif\n\n\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/dont-overfit-ii/train.csv')\ntest = pd.read_csv('/kaggle/input/dont-overfit-ii/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_all(df):\n    with pd.option_context(\"display.max_rows\", 1000):\n        with pd.option_context(\"display.max_columns\", 1000):\n            display(df)\n            \ndef reduce_memory_usage(df, verbose=True):\n    \"\"\" \n    Reduces the size of given dataframe by assigning \n    datatype appropriately.\n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_max = df[col].max()\n            c_min = df[col].min()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                elif c_min > np.finfo(np.float64).min and c_max < np.finfo(np.float64).max:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose:\n        print(f'Mem. usage decreased to {end_mem} Mb {(((start_mem - end_mem)/start_mem))*100} % reduction.')\n    return df\n                    \ndef describe_table(df):\n    \"\"\"Describes the statistics of given dataframe\"\"\"\n    print(f'Dataset Shape is: {df.shape}')\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary.rename(columns={'index':'Name'}, inplace=True)\n    summary['Missing'] = df.isnull().sum().values\n    summary['Uniques'] = df.nunique().values\n    summary['First Values'] = df.loc[0].values\n    summary['Second Values'] = df.loc[1].values\n    summary['Third Values'] = df.loc[2].values\n    display(summary)\n    \n    return summary\n\ndef train_model(X, X_test, y, params, folds, n_splits, model_type='lgb', plot_feature_importance=False, averaging='usual', model=None):\n    oof = np.zeros(len(X))\n    prediction = np.zeros(len(X_test))\n    scores = []\n    feature_importance = pd.DataFrame()\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n        # print('Fold', fold_n, 'started at', time.ctime())\n        X_train, X_valid = X[train_index], X[valid_index]\n        y_train, y_valid = y[train_index], y[valid_index]\n        \n        if model_type == 'lgb':\n            train_data = lgb.Dataset(X_train, label=y_train)\n            valid_data = lgb.Dataset(X_valid, label=y_valid)\n            \n            model = lgb.train(params,\n                    train_data,\n                    num_boost_round=2000,\n                    valid_sets = [train_data, valid_data],\n                    verbose_eval=500,\n                    early_stopping_rounds = 200)\n            \n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X_tr.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X_tr.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)\n        \n        if model_type == 'cat':\n            model = CatBoostClassifier(iterations=20000,  eval_metric='AUC', **params)\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test)\n            \n            \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = roc_auc_score(y_valid, y_pred_valid)\n            # print(f'Fold {fold_n}. AUC: {score:.4f}.')\n            # print('')\n            \n            y_pred = model.predict_proba(X_test)[:, 1]\n            \n        if model_type == 'glm':\n            model = sm.GLM(y_train, X_train, family=sm.families.Binomial())\n            model_results = model.fit()\n            model_results.predict(X_test)\n            y_pred_valid = model_results.predict(X_valid).reshape(-1,)\n            score = roc_auc_score(y_valid, y_pred_valid)\n            \n            y_pred = model_results.predict(X_test)\n            \n        oof[valid_index] = y_pred_valid.reshape(-1,)\n        scores.append(roc_auc_score(y_valid, y_pred_valid))\n        \n        if averaging == 'usual':\n            prediction += y_pred\n        elif averaging == 'rank':\n            prediction += pd.Series(y_pred).rank().values  \n        \n        if model_type == 'lgb':\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = X.columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction /= n_splits\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    if model_type == 'lgb':\n        feature_importance[\"importance\"] /= n_fold\n        if plot_feature_importance:\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n        \n            return oof, prediction, feature_importance\n        return oof, prediction, scores\n    else:\n        return oof, prediction, scores\n    \ndef get_correlation():\n    \"\"\"Get correlation between features\"\"\"\n    corr_matrix = train.corr()\n    corr = corr_matrix.abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\n    corr = corr[corr['level_0'] != corr['level_1']]\n    return corr_matrix, corr\n\ndef standardized_features(x):\n    \"\"\" Normalize the features. \"\"\"\n    sc = StandardScaler()\n    x = sc.fit_transform(x)\n    return x\n\ndef remove_columns_null(df, null_per=0.9, target=''):\n    \"\"\"Remove columns from dataframe which have null values greater than null_per\"\"\"\n    drop_cols = [col for col in df if df[col].isnull().sum() / df.shape[0] > null_per]\n    if target and target in drop_cols:\n        drop_cols.remove(target)\n    df.drop(columns=drop_cols, inplace=True)\n    return df\n    \ndef remove_columns_top_values(df, top_values_per=0.9, target=''):\n    \"\"\"Remove columns from dataframe which have top_values greater than top_values_per\"\"\"\n    drop_cols = [col for col in df.columns if df[col].value_counts(dropna=False, normalize=True).values[0] > top_values_per]\n    if target and target in drop_cols:\n        drop_cols.remove(target)\n    df.drop(columns=drop_cols, inplace=True)\n    return df\n\ndef convert_categorical(df, target=''):\n    \"\"\"Convert categorical to labels\"\"\"\n    for col in df.columns:\n        if df[col].dtype=='object' and col != target:\n            le = preprocessing.LabelEncoder()\n            col_val = list(df[col].values)\n            le.fit(col_val)\n            df[col] = le.transform(col_val)\n    return df\n\ndef pca(df, cols, n_components, prefix='PCA_', rand_seed=4):\n    pca = PCA(n_components=n_components, random_state=rand_seed)\n    principal_components = pca.fit_transform(df[cols])\n    principal_df = pd.DataFrame(principal_components)\n    df.drop(col, axis=1, inplace=True)\n    principal_df.rename(columns=lambda x: str(prefix)+str(x), inplace=True)\n    df = pd.concat([df, principal_df], axis=1)\n    return df ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display_all(train.head().T)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"des_tab = describe_table(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix, corr = get_correlation()\ncorr.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\nx_test = test.drop(['id'], axis=1)\n\n#folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\nfolds = RepeatedStratifiedKFold(n_splits=20, n_repeats=20, random_state=42)\nx_train = standardized_features(x_train)\nx_test = standardized_features(x_test)\n\nmodel = LogisticRegression(class_weight='balanced', penalty='l1', C=0.1,solver='liblinear')\noof, prediction_lr, scores = train_model(x_train, x_test, y_train, params=None, model_type='sklearn',\n                                         folds=folds, n_splits=20, model=model)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Eli5 Feature Importance**"},{"metadata":{"trusted":true},"cell_type":"code","source":"eli5.explain_weights(model, top=50)\neli5_features = eli5.format_as_dataframe(eli5.explain_weights(model, top=100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_features = [feat[1:] for feat in eli5_features[eli5_features['feature'] != '<BIAS>']['feature']]\nx_train = standardized_features(train[top_features])\nx_test = standardized_features(test[top_features])\n\nmodel = LogisticRegression(class_weight='balanced', penalty='l1', C=0.1,solver='liblinear')\noof, prediction_lr, scores = train_model(x_train, x_test, y_train, params=None, model_type='sklearn',\n                                         folds=folds, n_splits=20, model=model)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Eli5 Permutation Importance**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_perm = PermutationImportance(model, random_state=1).fit(x_train, y_train)\neli5.show_weights(model_perm, top=50)\neli5_features = eli5.format_as_dataframe(eli5.explain_weights(model, top=100))\ntop_features = [feat[1:] for feat in eli5_features[eli5_features['feature'] != '<BIAS>']['feature']]\nx_train = standardized_features(train[top_features])\nx_test = standardized_features(test[top_features])\n\nmodel = LogisticRegression(class_weight='balanced', penalty='l1', C=0.1,solver='liblinear')\noof, prediction_lr, scores = train_model(x_train, x_test, y_train, params=None, model_type='sklearn',\n                                         folds=folds, n_splits=20, model=model)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Mlxtend Sequential Feature Selector**"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\nx_test = test.drop(['id'], axis=1)\nx_train = standardized_features(x_train)\nx_test = standardized_features(x_test)\n\nmodel = LogisticRegression(class_weight='balanced', penalty='l1', C=0.1,solver='liblinear')\noof, prediction_lr, scores = train_model(x_train, x_test, y_train, params=None, model_type='sklearn',\n                                         folds=folds, n_splits=20, model=model)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sfs = SFS(model,\n    k_features=(10,15),\n    forward=True,\n    floating=False,\n    verbose=2,\n    scoring='roc_auc',\n    cv=folds,\n    n_jobs=-1)\nsfs = sfs.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Mlxtend Feature Names:-',sfs.k_feature_names_)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_features = list(sfs.k_feature_names_)\nx_train = standardized_features(train[top_features])\nx_test = standardized_features(test[top_features])\n\nkwargs = {'C':0.1,\n        'class_weight':'balanced',\n        'penalty':'l1', \n        'solver':'liblinear'}\n#model = LogisticRegression(**kwargs)\nmodel = LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\noof_lr, prediction_lr, _ = train_model(x_train, x_test, y_train, params=None,\n                                      model_type='sklearn', model=model, folds=folds,\n                                       n_splits=20)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['id'] = test['id']\nsubmission['target'] = model.predict_proba(test[top_features])[:, 1]\nsubmission.to_csv('lr_mlxtend.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plot_sfs(sfs.get_metric_dict(), kind='std_dev')\n\nplt.ylim([0.8, 1])\nplt.title('Sequential Forward Selection (StdDev)')\nplt.grid()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Sklearn Feature Selection**"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\nx_test = test.drop(['id'], axis=1)\nx_train = standardized_features(x_train)\nx_test = standardized_features(x_test)\nfolds = RepeatedStratifiedKFold(n_splits=20, n_repeats=20, random_state=42)\nmodel = LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_lst = []\nfor i in range(10, 301, 5):\n    rfe = RFE(model, i, step=1)\n    x_train_rfe = rfe.fit_transform(x_train, y_train.values.astype(int))\n    x_test_rfe = rfe.transform(x_test)\n    oof, prediction, scores = train_model(x_train_rfe, x_test_rfe, y_train, params=None, model_type='sklearn',\n                                         folds=folds, n_splits=20, model=model)\n    scores_lst.append(np.mean(scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfe = RFE(model, 20, step=1)\nx_train_rfe = rfe.fit_transform(x_train, y_train.values.astype(int))\nx_test_rfe = rfe.transform(x_test)\noof, prediction, scores = train_model(x_train_rfe, x_test_rfe, y_train, folds=folds, \n                                      params=None, model_type='sklearn', n_splits=20, model=model) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Modeling**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_estimator(params, etype):\n    \"\"\" Returns estimator based on given estimator type. \"\"\"\n    \n    if etype == 'lr':\n        model = LogisticRegression(**params)\n    elif etype == 'knn':\n        model = KNeighborsClassifier(**params)\n    elif etype == 'gb':\n        model = GaussianNB(**params)\n    elif etype == 'svm':\n        model = SVC(**params)  \n    elif etype == 'ad':\n        model = AdaBoostClassifier(**params)\n    elif etype == 'et':\n        model = ExtraTreesClassifier(**params)\n    elif etype == 'rf':\n        model = RandomForestClassifier(**params)\n    elif etype == 'sgd':\n        model = SGDClassifier(**params)\n    \n    return model  \n\ndef grid_search(x, y, params, model, folds):\n    \"\"\" Returns best parameters using Grid Search. \"\"\"\n    gs = GridSearchCV(model, param_grid=params, cv=folds, \n                      scoring=\"roc_auc\", verbose=1, n_jobs=-1)\n    gs.fit(x, y)\n    return gs.best_score_, gs.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\nx_test = test.drop(['id'], axis=1)\nx_train = standardized_features(x_train)\nx_test = standardized_features(x_test)\n\nmodel = LogisticRegression(class_weight='balanced', penalty='l1', C=0.1,solver='liblinear')\noof, prediction_lr, scores = train_model(x_train, x_test, y_train, params=None, model_type='sklearn',\n                                        folds=folds, n_splits=20, model=model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'solver':'liblinear', 'max_iter':10000}\nlr = get_estimator(params, 'lr')\nparams = {\n    'class_weight': ['balanced', None],\n    'penalty': ['l2'],\n    'C': [0.001, 0.01, 0.08, 0.1, 0.15, 1, 10, 100]\n}\ngs_score, gs_params = grid_search(x_train, y_train, params, lr, folds)\nlr = get_estimator(gs_params, 'lr')\noof_lr, prediction_lr, scores_lr = train_model(x_train, x_test, y_train, folds=folds, \n                                    params=None, model_type='sklearn',model=lr, n_splits=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = get_estimator({}, 'knn')\nparams = {\n    'n_neighbors': [2, 3, 5, 10, 20],\n    'weights': ['uniform', 'distance'],\n    'leaf_size': [5, 10, 30]\n}\ngs_score, gs_params = grid_search(x_train, y_train, params, knn, folds)\nknn = get_estimator(gs_params, 'knn')\noof_knn, predictions_knn, scores_knn = train_model(x_train, x_test, y_train, folds=folds,\n                                                  params=None, n_splits=20, model=knn, model_type='sklearn')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gb = get_estimator({}, 'gb')\noof_gb, predictions_gb, scores_gb = train_model(x_train, x_test, y_train, folds=folds,\n                                                  params=None, n_splits=20, model=gb, model_type='sklearn')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nparams = {'probability':True, 'gamma':'scale'}\nsvm = get_estimator(params, 'svm')\nparams = {'C': [0.001, 0.01, 0.1, 1.0, 10.0],\n        'kernel': ['linear', 'poly', 'rbf'],\n         }\ngs_score, gs_params = grid_search(x_train, y_train, params, svm, folds)\ngs_params['probability'] = True\nsvm = get_estimator(gs_params, 'svm')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_svm, predictions_svm, scores_svm = train_model(x_train, x_test, y_train, folds=folds, \n                                                  params=None, n_splits=20, model=svm, model_type='sklearn')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ad = get_estimator({}, 'ad')\nparams = {\n    'n_estimators': [5, 10, 20, 50, 100],\n    'learning_rate': [0.001, 0.01, 0.1, 1, 10]\n}\ngs_score, gs_params = grid_search(x_train, y_train, params, ad, folds)\nad = get_estimator(gs_params, 'ad')\noof_ad, predictions_ad, scores_ad = train_model(x_train, x_test, y_train, folds=folds, \n                                    params=None, n_splits=20, model=ad, model_type='sklearn')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"et = get_estimator({}, 'et')\nparams = {\n    'n_estimators': [10, 50, 100, 1000],\n    'max_depth': [None, 3, 5, 15]\n}\ngs_score, gs_params = grid_search(x_train, y_train, params, et, folds)\net = get_estimator(gs_params, 'et')\noof_et, predictions_et, scores_et = train_model(x_train, x_test, y_train, folds=folds, \n                                                  params=None, n_splits=20, model=et, model_type='sklearn')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = get_estimator({}, 'rf')\nparams = {\n    'n_estimators': [10, 50, 100, 1000],\n    'max_depth': [None, 3, 5, 15]\n}\ngs_score, gs_params = grid_search(x_train, y_train, params, rf, folds)\nrf = get_estimator(gs_params, 'rf')\noof_rf, predictions_rf, scores_rf = train_model(x_train, x_test, y_train, folds=folds, \n                                                  params=None, n_splits=20, model=rf, model_type='sklearn')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sgd = SGDClassifier(eta0=1, max_iter=1000, tol=0.0001)\nparams = {'loss': ['log', 'modified_huber'],\n          'penalty': ['l1', 'l2', 'elasticnet'],\n          'alpha': [0.001, 0.01],\n          'l1_ratio': [0, 0.15, 0.5, 1.0],\n          'learning_rate': ['optimal', 'invscaling', 'adaptive']\n        }\ngs_score, gs_params = grid_search(x_train, y_train, params, sgd, folds)\ngs_params['eta0'] = 1\ngs_params['max_iter'] = 1000\ngs_params['tol'] = 0.0001\nsgd = get_estimator(gs_params, 'sgd')\noof_rf, predictions_rf, scores_rf = train_model(x_train, x_test, y_train, folds=folds, \n                                                  params=None, n_splits=20, model=sgd, model_type='sklearn')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selector = SelectKBest(f_classif, k=15)\nX_trainK = selector.fit_transform(x_train, y_train.values.astype(int))\nX_testK = selector.transform(x_test)\noof_lr_1, prediction_lr_1, scores = train_model(X_trainK, X_testK, y_train, params=None, model_type='sklearn',\n                                                model=model, n_splits=20, folds=folds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_glm, prediction_glm, scores = train_model(X_trainK, X_testK, y_train, folds=folds, \n                                              params=None, model_type='glm', n_splits=20)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Feature Engineering**"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\nx_test = test.drop(['id'], axis=1)\n\n#folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\nfolds = RepeatedStratifiedKFold(n_splits=20, n_repeats=20, random_state=42)\nx_train = standardized_features(x_train)\nx_test = standardized_features(x_test)\n\nmodel = LogisticRegression(class_weight='balanced', penalty='l1', C=0.1,solver='liblinear')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"poly = PolynomialFeatures(2)\nx_train_poly = poly.fit_transform(x_train) \nx_test_poly = poly.fit_transform(x_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cor_poly = pd.DataFrame(x_train_poly).corrwith(y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_scores = []\nfor i in range(10, 510, 5):\n    print(i)\n    top_corr_cols = list(cor_poly.abs().sort_values().tail(i).reset_index()['index'].values)\n    x_train_tmp = x_train_poly[:, top_corr_cols]\n    x_test_tmp = x_test_poly[:, top_corr_cols]\n    oof_poly, prediction_poly, scores = train_model(x_train_tmp, x_test_tmp, y_train,folds=folds,\n                                                   params=None, model_type='sklearn', model=model, n_splits=20)\n    corr_scores.append(scores)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = [go.Scatter(\n        x = list(range(10, 510, 5)),\n        y = [np.round(np.mean(i), 4) for i in corr_scores],\n        name = 'CV Scores'\n)]\nlayout = go.Layout(dict(title= 'Top N poly features vs CV',\n                    xaxis= dict(title= 'Top N features'),\n                    yaxis= dict(title= 'CV Score')\n                       ))\npy.iplot(dict=(data=data, layout=layout), filename='basic-line')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neigh = NearestNeighbors(5, n_jobs=-1)\nneigh.fit(x_train)\n\ndists, _ = neigh.kneighbors(x_train, n_neighbors=5)\nmean_dist = dists.mean(axis=1).reshape(-1, 1)\nmax_dist = dists.max(axis=1).reshape(-1, 1)\nmin_dist = dists.min(axis=1).reshape(-1, 1)\nx_train = np.hstack((x_train, mean_dist, max_dist,\n                   min_dist, x_train.std(1).reshape(-1, 1)))\n\ntest_dists, _ = neigh.kneighbors(x_test, n_neighbors=5)\ntest_mean_dist = test_dists.mean(axis=1).reshape(-1, 1)\ntest_max_dist = test_dists.max(axis=1).reshape(-1, 1)\ntest_min_dist = test_dists.min(axis=1).reshape(-1, 1)\nx_test = np.hstack((x_test, test_mean_dist, test_max_dist,\n                   test_min_dist, x_test.std(1).reshape(-1, 1)))\n\nx_train = standardized_features(x_train)\nx_test = standardized_features(x_test)\n\nmodel = LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\noof, prediction_lr, scores = train_model(x_train, x_test, y_train, params=None, model_type='sklearn',\n                                        folds=folds, n_splits=20, model=model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# np.hstack((x_train, mean_dist, max_dist,\n#                    min_dist)).shape\nx_train.shape","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}