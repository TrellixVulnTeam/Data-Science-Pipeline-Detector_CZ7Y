{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import Imputer, StandardScaler\nfrom sklearn import model_selection\nfrom sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\nfrom mlxtend.preprocessing import standardize\nfrom mlxtend.classifier import StackingClassifier, StackingCVClassifier\nfrom mlxtend.feature_selection import ColumnSelector\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":46,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train =  pd.read_csv(\"../input/train.csv\")\ntrain.head()","execution_count":47,"outputs":[{"output_type":"execute_result","execution_count":47,"data":{"text/plain":"   id  target      0      1      2  ...      295    296    297    298    299\n0   0     1.0 -0.098  2.165  0.681  ...   -2.097  1.051 -0.414  1.038 -1.065\n1   1     0.0  1.081 -0.973 -0.383  ...   -1.624 -0.458 -1.099 -0.936  0.973\n2   2     1.0 -0.523 -0.089 -0.348  ...   -1.165 -1.544  0.004  0.800 -1.211\n3   3     1.0  0.067 -0.021  0.392  ...    0.467 -0.562 -0.254 -0.533  0.238\n4   4     1.0  2.347 -0.831  0.511  ...    1.378  1.246  1.478  0.428  0.253\n\n[5 rows x 302 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n      <th>21</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n      <th>25</th>\n      <th>26</th>\n      <th>27</th>\n      <th>28</th>\n      <th>29</th>\n      <th>30</th>\n      <th>31</th>\n      <th>32</th>\n      <th>33</th>\n      <th>34</th>\n      <th>35</th>\n      <th>36</th>\n      <th>37</th>\n      <th>...</th>\n      <th>260</th>\n      <th>261</th>\n      <th>262</th>\n      <th>263</th>\n      <th>264</th>\n      <th>265</th>\n      <th>266</th>\n      <th>267</th>\n      <th>268</th>\n      <th>269</th>\n      <th>270</th>\n      <th>271</th>\n      <th>272</th>\n      <th>273</th>\n      <th>274</th>\n      <th>275</th>\n      <th>276</th>\n      <th>277</th>\n      <th>278</th>\n      <th>279</th>\n      <th>280</th>\n      <th>281</th>\n      <th>282</th>\n      <th>283</th>\n      <th>284</th>\n      <th>285</th>\n      <th>286</th>\n      <th>287</th>\n      <th>288</th>\n      <th>289</th>\n      <th>290</th>\n      <th>291</th>\n      <th>292</th>\n      <th>293</th>\n      <th>294</th>\n      <th>295</th>\n      <th>296</th>\n      <th>297</th>\n      <th>298</th>\n      <th>299</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1.0</td>\n      <td>-0.098</td>\n      <td>2.165</td>\n      <td>0.681</td>\n      <td>-0.614</td>\n      <td>1.309</td>\n      <td>-0.455</td>\n      <td>-0.236</td>\n      <td>0.276</td>\n      <td>-2.246</td>\n      <td>1.825</td>\n      <td>-0.912</td>\n      <td>-0.107</td>\n      <td>0.305</td>\n      <td>0.102</td>\n      <td>0.826</td>\n      <td>0.417</td>\n      <td>0.177</td>\n      <td>-0.673</td>\n      <td>-0.503</td>\n      <td>1.864</td>\n      <td>0.410</td>\n      <td>-1.927</td>\n      <td>0.102</td>\n      <td>-0.931</td>\n      <td>1.763</td>\n      <td>1.449</td>\n      <td>-1.097</td>\n      <td>-0.686</td>\n      <td>-0.250</td>\n      <td>-1.859</td>\n      <td>1.125</td>\n      <td>1.009</td>\n      <td>-2.296</td>\n      <td>0.385</td>\n      <td>-0.876</td>\n      <td>1.528</td>\n      <td>-0.144</td>\n      <td>-1.078</td>\n      <td>...</td>\n      <td>-0.681</td>\n      <td>1.250</td>\n      <td>-0.565</td>\n      <td>-1.318</td>\n      <td>-0.923</td>\n      <td>0.075</td>\n      <td>-0.704</td>\n      <td>2.457</td>\n      <td>0.771</td>\n      <td>-0.460</td>\n      <td>0.569</td>\n      <td>-1.320</td>\n      <td>-1.516</td>\n      <td>-2.145</td>\n      <td>-1.120</td>\n      <td>0.156</td>\n      <td>0.820</td>\n      <td>-1.049</td>\n      <td>-1.125</td>\n      <td>0.484</td>\n      <td>0.617</td>\n      <td>1.253</td>\n      <td>1.248</td>\n      <td>0.504</td>\n      <td>-0.802</td>\n      <td>-0.896</td>\n      <td>-1.793</td>\n      <td>-0.284</td>\n      <td>-0.601</td>\n      <td>0.569</td>\n      <td>0.867</td>\n      <td>1.347</td>\n      <td>0.504</td>\n      <td>-0.649</td>\n      <td>0.672</td>\n      <td>-2.097</td>\n      <td>1.051</td>\n      <td>-0.414</td>\n      <td>1.038</td>\n      <td>-1.065</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.0</td>\n      <td>1.081</td>\n      <td>-0.973</td>\n      <td>-0.383</td>\n      <td>0.326</td>\n      <td>-0.428</td>\n      <td>0.317</td>\n      <td>1.172</td>\n      <td>0.352</td>\n      <td>0.004</td>\n      <td>-0.291</td>\n      <td>2.907</td>\n      <td>1.085</td>\n      <td>2.144</td>\n      <td>1.540</td>\n      <td>0.584</td>\n      <td>1.133</td>\n      <td>1.098</td>\n      <td>-0.237</td>\n      <td>-0.498</td>\n      <td>0.283</td>\n      <td>-1.100</td>\n      <td>-0.417</td>\n      <td>1.382</td>\n      <td>-0.515</td>\n      <td>-1.519</td>\n      <td>0.619</td>\n      <td>-0.128</td>\n      <td>0.866</td>\n      <td>-0.540</td>\n      <td>1.238</td>\n      <td>-0.227</td>\n      <td>0.269</td>\n      <td>-0.390</td>\n      <td>-2.721</td>\n      <td>1.659</td>\n      <td>0.106</td>\n      <td>-0.121</td>\n      <td>1.719</td>\n      <td>...</td>\n      <td>0.971</td>\n      <td>-1.489</td>\n      <td>0.530</td>\n      <td>0.917</td>\n      <td>-0.094</td>\n      <td>-1.407</td>\n      <td>0.887</td>\n      <td>-0.104</td>\n      <td>-0.583</td>\n      <td>1.267</td>\n      <td>-1.667</td>\n      <td>-2.771</td>\n      <td>-0.516</td>\n      <td>1.312</td>\n      <td>0.491</td>\n      <td>0.932</td>\n      <td>2.064</td>\n      <td>0.422</td>\n      <td>1.215</td>\n      <td>2.012</td>\n      <td>0.043</td>\n      <td>-0.307</td>\n      <td>-0.059</td>\n      <td>1.121</td>\n      <td>1.333</td>\n      <td>0.211</td>\n      <td>1.753</td>\n      <td>0.053</td>\n      <td>1.274</td>\n      <td>-0.612</td>\n      <td>-0.165</td>\n      <td>-1.695</td>\n      <td>-1.257</td>\n      <td>1.359</td>\n      <td>-0.808</td>\n      <td>-1.624</td>\n      <td>-0.458</td>\n      <td>-1.099</td>\n      <td>-0.936</td>\n      <td>0.973</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>1.0</td>\n      <td>-0.523</td>\n      <td>-0.089</td>\n      <td>-0.348</td>\n      <td>0.148</td>\n      <td>-0.022</td>\n      <td>0.404</td>\n      <td>-0.023</td>\n      <td>-0.172</td>\n      <td>0.137</td>\n      <td>0.183</td>\n      <td>0.459</td>\n      <td>0.478</td>\n      <td>-0.425</td>\n      <td>0.352</td>\n      <td>1.095</td>\n      <td>0.300</td>\n      <td>-1.044</td>\n      <td>0.270</td>\n      <td>-1.038</td>\n      <td>0.144</td>\n      <td>-1.658</td>\n      <td>-0.946</td>\n      <td>0.633</td>\n      <td>-0.772</td>\n      <td>1.786</td>\n      <td>0.136</td>\n      <td>-0.103</td>\n      <td>-1.223</td>\n      <td>2.273</td>\n      <td>0.055</td>\n      <td>-2.032</td>\n      <td>-0.452</td>\n      <td>0.064</td>\n      <td>0.924</td>\n      <td>-0.692</td>\n      <td>-0.067</td>\n      <td>-0.917</td>\n      <td>1.896</td>\n      <td>...</td>\n      <td>-0.540</td>\n      <td>-0.299</td>\n      <td>1.074</td>\n      <td>-0.748</td>\n      <td>1.086</td>\n      <td>-0.766</td>\n      <td>-0.931</td>\n      <td>0.432</td>\n      <td>1.345</td>\n      <td>-0.491</td>\n      <td>-1.602</td>\n      <td>-0.727</td>\n      <td>0.346</td>\n      <td>0.780</td>\n      <td>-0.527</td>\n      <td>-1.122</td>\n      <td>-0.208</td>\n      <td>-0.730</td>\n      <td>-0.302</td>\n      <td>2.535</td>\n      <td>-1.045</td>\n      <td>0.037</td>\n      <td>0.020</td>\n      <td>1.373</td>\n      <td>0.456</td>\n      <td>-0.277</td>\n      <td>1.381</td>\n      <td>1.843</td>\n      <td>0.749</td>\n      <td>0.202</td>\n      <td>0.013</td>\n      <td>0.263</td>\n      <td>-1.222</td>\n      <td>0.726</td>\n      <td>1.444</td>\n      <td>-1.165</td>\n      <td>-1.544</td>\n      <td>0.004</td>\n      <td>0.800</td>\n      <td>-1.211</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>1.0</td>\n      <td>0.067</td>\n      <td>-0.021</td>\n      <td>0.392</td>\n      <td>-1.637</td>\n      <td>-0.446</td>\n      <td>-0.725</td>\n      <td>-1.035</td>\n      <td>0.834</td>\n      <td>0.503</td>\n      <td>0.274</td>\n      <td>0.335</td>\n      <td>-1.148</td>\n      <td>0.067</td>\n      <td>-1.010</td>\n      <td>1.048</td>\n      <td>-1.442</td>\n      <td>0.210</td>\n      <td>0.836</td>\n      <td>-0.326</td>\n      <td>0.716</td>\n      <td>-0.764</td>\n      <td>0.248</td>\n      <td>-1.308</td>\n      <td>2.127</td>\n      <td>0.365</td>\n      <td>0.296</td>\n      <td>-0.808</td>\n      <td>1.854</td>\n      <td>0.118</td>\n      <td>0.380</td>\n      <td>0.999</td>\n      <td>-1.171</td>\n      <td>2.798</td>\n      <td>0.394</td>\n      <td>-1.048</td>\n      <td>1.078</td>\n      <td>0.401</td>\n      <td>-0.486</td>\n      <td>...</td>\n      <td>-0.083</td>\n      <td>-0.831</td>\n      <td>1.251</td>\n      <td>-0.206</td>\n      <td>-0.933</td>\n      <td>-1.215</td>\n      <td>0.281</td>\n      <td>0.512</td>\n      <td>-0.424</td>\n      <td>0.769</td>\n      <td>0.223</td>\n      <td>-0.710</td>\n      <td>2.725</td>\n      <td>0.176</td>\n      <td>0.845</td>\n      <td>-1.226</td>\n      <td>1.527</td>\n      <td>-1.701</td>\n      <td>0.597</td>\n      <td>0.150</td>\n      <td>1.864</td>\n      <td>0.322</td>\n      <td>-0.214</td>\n      <td>1.282</td>\n      <td>0.408</td>\n      <td>-0.910</td>\n      <td>1.020</td>\n      <td>-0.299</td>\n      <td>-1.574</td>\n      <td>-1.618</td>\n      <td>-0.404</td>\n      <td>0.640</td>\n      <td>-0.595</td>\n      <td>-0.966</td>\n      <td>0.900</td>\n      <td>0.467</td>\n      <td>-0.562</td>\n      <td>-0.254</td>\n      <td>-0.533</td>\n      <td>0.238</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>1.0</td>\n      <td>2.347</td>\n      <td>-0.831</td>\n      <td>0.511</td>\n      <td>-0.021</td>\n      <td>1.225</td>\n      <td>1.594</td>\n      <td>0.585</td>\n      <td>1.509</td>\n      <td>-0.012</td>\n      <td>2.198</td>\n      <td>0.190</td>\n      <td>0.453</td>\n      <td>0.494</td>\n      <td>1.478</td>\n      <td>-1.412</td>\n      <td>0.270</td>\n      <td>-1.312</td>\n      <td>-0.322</td>\n      <td>-0.688</td>\n      <td>-0.198</td>\n      <td>-0.285</td>\n      <td>1.042</td>\n      <td>-0.315</td>\n      <td>-0.478</td>\n      <td>0.024</td>\n      <td>-0.190</td>\n      <td>1.656</td>\n      <td>-0.469</td>\n      <td>-1.437</td>\n      <td>-0.581</td>\n      <td>-0.308</td>\n      <td>-0.837</td>\n      <td>-1.739</td>\n      <td>0.037</td>\n      <td>0.336</td>\n      <td>-1.102</td>\n      <td>2.371</td>\n      <td>0.554</td>\n      <td>...</td>\n      <td>-1.050</td>\n      <td>-0.347</td>\n      <td>0.904</td>\n      <td>-1.324</td>\n      <td>-0.849</td>\n      <td>3.432</td>\n      <td>0.222</td>\n      <td>0.416</td>\n      <td>0.174</td>\n      <td>-1.517</td>\n      <td>-0.337</td>\n      <td>0.055</td>\n      <td>-0.464</td>\n      <td>0.014</td>\n      <td>-1.073</td>\n      <td>0.325</td>\n      <td>-0.523</td>\n      <td>-0.692</td>\n      <td>0.190</td>\n      <td>-0.883</td>\n      <td>-1.830</td>\n      <td>1.408</td>\n      <td>2.319</td>\n      <td>1.704</td>\n      <td>-0.723</td>\n      <td>1.014</td>\n      <td>0.064</td>\n      <td>0.096</td>\n      <td>-0.775</td>\n      <td>1.845</td>\n      <td>0.898</td>\n      <td>0.134</td>\n      <td>2.415</td>\n      <td>-0.996</td>\n      <td>-1.006</td>\n      <td>1.378</td>\n      <td>1.246</td>\n      <td>1.478</td>\n      <td>0.428</td>\n      <td>0.253</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train.target \nX = train.iloc[:, 2:]","execution_count":48,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nX = scaler.fit_transform(X)","execution_count":49,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_imputer = Imputer()\nX = my_imputer.fit_transform(X)","execution_count":50,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Initializing models\n#clf1 = KNeighborsClassifier(n_neighbors=5)\n#clf2 = RandomForestClassifier(n_estimators=100, random_state=1)\n#clf3 = GaussianNB()\n#clf4 = ExtraTreesClassifier(n_estimators=100)\n#clf5 = SVC()\n#clf6 = RidgeClassifier()\n#clf7 = AdaBoostClassifier(n_estimators=1000, learning_rate=0.7)\n#clf8 = GradientBoostingClassifier(n_estimators=1000, min_samples_split=5)\n#clf9 = MLPClassifier(random_state=1, alpha=1e-1)\n#clf10 = LogisticRegression(C=0.1)\n\n#rf = RandomForestClassifier(n_estimators=20, random_state=1)\n\n#sclf = StackingClassifier(classifiers=[clf1, clf2, clf3, clf4, clf5, clf6, clf7, clf8, clf9, clf10],\n#                            verbose=1,\n#                            meta_classifier=rf)\n\n#print('3-fold cross validation:\\n')\n\n#for clf, label in zip([clf1, clf2, clf3, clf4, clf5, clf6, clf7, clf8, clf9, clf10, sclf], \n#                      ['KNN', \n#                       'Random Forest', \n#                       'Naive Bayes',\n#                       'Extra Trees',\n#                       'Support Vector',\n##                       'Ridge',\n#                       'AdaBoost',\n#                       'GradientBoost',\n#                       'Neural Net',\n#                       'Logistic',\n#                       'Ensemble']):\n\n#    scores = model_selection.cross_val_score(clf, X, y, \n#                                              cv=3, scoring='accuracy')\n#    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" \n#          % (scores.mean(), scores.std(), label))#\n\n#params = {'kneighborsclassifier__n_neighbors': [1, 10],\n#          'randomforestclassifier__n_estimators': [10, 20],\n#          'extratreesclassifier__n_estimators': [10, 20],\n#          'adaboostclassifier__n_estimators':[1000, 2000],\n#          'gradientboostingclassifier__n_estimators':[50,100],\n#          'logisticregression__C':[0.1, 10.0],\n#          'meta-randomforestclassifier__n_estimators':[10, 20]}\n#grid = GridSearchCV(estimator=sclf, \n#                    param_grid=params, \n#                    cv=5,\n#                    refit=True,\n#                    n_jobs=-1)\n#grid.fit(X, y)\n#print('Best parameters: %s' % grid.best_params_)\n#print('Accuracy: %.2f' % grid.best_score_)\n#Best parameters: {'adaboostclassifier__n_estimators': 1000, 'extratreesclassifier__n_estimators': 10, \n#'gradientboostingclassifier__n_estimators': 50, 'kneighborsclassifier__n_neighbors': 1, 'logisticregression__C': 0.1, \n#'meta-randomforestclassifier__n_estimators': 20, 'randomforestclassifier__n_estimators': 10}\n#Accuracy: 0.73","execution_count":82,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initializing models\nclf2 = RandomForestClassifier(n_estimators=100, random_state=1)\nclf3 = GaussianNB()\nclf4 = ExtraTreesClassifier(n_estimators=100)\nclf7 = AdaBoostClassifier(n_estimators=1000, learning_rate=0.7)\nclf8 = GradientBoostingClassifier(n_estimators=1000, min_samples_split=5)\nclf9 = GradientBoostingClassifier(n_estimators=2000, max_depth=4)\nclf10 = GradientBoostingClassifier(n_estimators=100, max_depth=2, learning_rate=0.1, subsample=0.5)\n\n\nrf = RandomForestClassifier(n_estimators=20, random_state=1)\n\nsclf = StackingClassifier(classifiers=[clf2, clf3, clf4, clf7, clf8, clf9, clf10],\n                            verbose=1,\n                            meta_classifier=rf)\nsclf.fit(X, y)","execution_count":76,"outputs":[{"output_type":"stream","text":"Fitting 7 classifiers...\nFitting classifier1: randomforestclassifier (1/7)\nFitting classifier2: gaussiannb (2/7)\nFitting classifier3: extratreesclassifier (3/7)\nFitting classifier4: adaboostclassifier (4/7)\nFitting classifier5: gradientboostingclassifier (5/7)\nFitting classifier6: gradientboostingclassifier (6/7)\nFitting classifier7: gradientboostingclassifier (7/7)\n","name":"stdout"},{"output_type":"execute_result","execution_count":76,"data":{"text/plain":"StackingClassifier(average_probas=False,\n          classifiers=[RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_f...     subsample=0.5, tol=0.0001, validation_fraction=0.1,\n              verbose=0, warm_start=False)],\n          meta_classifier=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=None,\n            oob_score=False, random_state=1, verbose=0, warm_start=False),\n          store_train_meta_features=False, use_clones=True,\n          use_features_in_secondary=False, use_probas=False, verbose=1)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for clf, label in zip([clf2, clf3, clf4, clf7, clf8, clf9, clf10, sclf], \n                      ['Random Forest', \n                       'Naive Bayes',\n                       'Extra Trees',\n                       'AdaBoost',\n                       'GradientBoost1',\n                       'GradientBoost2',\n                       'GradientBoost3',\n                       'Ensemble']):\n\n    scores = model_selection.cross_val_score(clf, X, y, \n                                              cv=3, scoring='accuracy')\n    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" \n          % (scores.mean(), scores.std(), label))","execution_count":77,"outputs":[{"output_type":"stream","text":"Accuracy: 0.68 (+/- 0.01) [Random Forest]\nAccuracy: 0.68 (+/- 0.00) [Naive Bayes]\nAccuracy: 0.65 (+/- 0.02) [Extra Trees]\nAccuracy: 0.67 (+/- 0.04) [AdaBoost]\nAccuracy: 0.72 (+/- 0.02) [GradientBoost1]\nAccuracy: 0.69 (+/- 0.02) [GradientBoost2]\nAccuracy: 0.68 (+/- 0.03) [GradientBoost3]\nFitting 7 classifiers...\nFitting classifier1: randomforestclassifier (1/7)\nFitting classifier2: gaussiannb (2/7)\nFitting classifier3: extratreesclassifier (3/7)\nFitting classifier4: adaboostclassifier (4/7)\nFitting classifier5: gradientboostingclassifier (5/7)\nFitting classifier6: gradientboostingclassifier (6/7)\nFitting classifier7: gradientboostingclassifier (7/7)\nFitting 7 classifiers...\nFitting classifier1: randomforestclassifier (1/7)\nFitting classifier2: gaussiannb (2/7)\nFitting classifier3: extratreesclassifier (3/7)\nFitting classifier4: adaboostclassifier (4/7)\nFitting classifier5: gradientboostingclassifier (5/7)\nFitting classifier6: gradientboostingclassifier (6/7)\nFitting classifier7: gradientboostingclassifier (7/7)\nFitting 7 classifiers...\nFitting classifier1: randomforestclassifier (1/7)\nFitting classifier2: gaussiannb (2/7)\nFitting classifier3: extratreesclassifier (3/7)\nFitting classifier4: adaboostclassifier (4/7)\nFitting classifier5: gradientboostingclassifier (5/7)\nFitting classifier6: gradientboostingclassifier (6/7)\nFitting classifier7: gradientboostingclassifier (7/7)\nAccuracy: 0.72 (+/- 0.02) [Ensemble]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/test.csv')\ntest = scaler.transform(test.iloc[:, 1:])\ntest = my_imputer.transform(test)\npreds = sclf.predict_proba(test)","execution_count":78,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def softmax(X, theta = 1.0, axis = None):\n    \"\"\"\n    Compute the softmax of each element along an axis of X.\n\n    Parameters\n    ----------\n    X: ND-Array. Probably should be floats.\n    theta (optional): float parameter, used as a multiplier\n        prior to exponentiation. Default = 1.0\n    axis (optional): axis to compute values along. Default is the\n        first non-singleton axis.\n\n    Returns an array the same size as X. The result will sum to 1\n    along the specified axis.\n    \"\"\"\n\n    # make X at least 2d\n    y = np.atleast_2d(X)\n\n    # find axis\n    if axis is None:\n        axis = next(j[0] for j in enumerate(y.shape) if j[1] > 1)\n\n    # multiply y against the theta parameter,\n    y = y * float(theta)\n\n    # subtract the max for numerical stability\n    y = y - np.expand_dims(np.max(y, axis = axis), axis)\n\n    # exponentiate y\n    y = np.exp(y)\n\n    # take the sum along the specified axis\n    ax_sum = np.expand_dims(np.sum(y, axis = axis), axis)\n\n    # finally: divide elementwise\n    p = y / ax_sum\n\n    # flatten if X was 1D\n    if len(X.shape) == 1: p = p.flatten()\n\n    return p","execution_count":79,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = softmax(preds, axis=1)[:, 1]","execution_count":80,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv(\"../input/sample_submission.csv\")","execution_count":83,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.target = preds\nsub.to_csv('sub.csv', index=False)","execution_count":84,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":85,"outputs":[{"output_type":"execute_result","execution_count":85,"data":{"text/plain":"    id    target\n0  250  0.731059\n1  251  0.731059\n2  252  0.574443\n3  253  0.731059\n4  254  0.731059","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>250</td>\n      <td>0.731059</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>251</td>\n      <td>0.731059</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>252</td>\n      <td>0.574443</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>253</td>\n      <td>0.731059</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>254</td>\n      <td>0.731059</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}