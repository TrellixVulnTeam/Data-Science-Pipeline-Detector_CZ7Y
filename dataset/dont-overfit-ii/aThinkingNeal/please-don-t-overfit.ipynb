{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# General information\n\nIn Don't Overfit! II competition we have a binary classification task. 300 columns, 250 training samples and 79 times more samples in test data! We need to be able to build a model without overfitting.\n\nIn this kernel I'll try the following things:\n\n\n* EDA on the features;\n* Feature Selection and Feature Engineering using Principle Components Analysis;\n* Find the number of efficient features for Feature selection and for Feature Engineering;\n* Do crossvalidation on Random Forest for Hyperparameter optimization;\n* Save the model;\n\n\n![](https://cdn-images-1.medium.com/max/1600/1*vuZxFMi5fODz2OEcpG-S1g.png)"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# Libraries\nimport numpy as np\nimport pandas as pd\npd.set_option('max_columns', None)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport datetime\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold, cross_val_score, GridSearchCV, RepeatedStratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nimport os\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nimport json\nimport ast\nimport time\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.feature_selection import GenericUnivariateSelect, SelectPercentile, SelectKBest, f_classif, mutual_info_classif, RFE\nimport statsmodels.api as sm\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom catboost import CatBoostClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c971fbc1a1b8249045b120924058402a036e665"},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b40dd26ead2705ebf0058b0aa528c89a18aedbe"},"cell_type":"markdown","source":"## Data exploration"},{"metadata":{"trusted":true,"_uuid":"67a26174603f6a28709b7e0431aa431832ae608d"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Try to see if a data point represents a picture but failed"},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# row_one = train.loc[0,'0':]\n# row_one = np.array(row_one)\n# row_one = np.absolute(row_one)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from PIL import Image\n# img = Image.fromarray(row_one)\n# img.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"504b609f61494c21f8b078182e06191581ecb2a6"},"cell_type":"code","source":"train[train.columns[2:]].std().plot('hist');\nplt.title('Distribution of stds of all columns');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7b0f65d2d5c999a44a3f71e013b1b6a6ff08980"},"cell_type":"code","source":"train[train.columns[2:]].mean().plot('hist');\nplt.title('Distribution of means of all columns');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d205e01b009224a3189903e1858dd592fb222d2d"},"cell_type":"code","source":"# we have no missing values\ntrain.isnull().any().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30e64cca712542d662201263914d8fc25496563e"},"cell_type":"code","source":"print('Distributions of first 28 columns')\nplt.figure(figsize=(26, 24))\nfor i, col in enumerate(list(train.columns)[2:30]):\n    plt.subplot(7, 4, i + 1)\n    plt.hist(train[col])\n    plt.title(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"227daacd39977e5658c7e27db2686d8f65fdff3c"},"cell_type":"code","source":"train['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89e9ed49ceff33d27cd1888336c3c46a38c5c8aa"},"cell_type":"markdown","source":"From this overview we can see the following things:\n* target is binary and has some disbalance: 36% of samples belong to 0 class;\n* values in columns are more or less similar;\n* columns have std of 1 +/- 0.1 (min and max values are 0.889, 1.117 respectively);\n* columns have mean of 0 +/- 0.15 (min and max values are -0.2, 0.1896 respectively);"},{"metadata":{"_uuid":"06df27b43428261da7daf02e708b934519d78ac2"},"cell_type":"markdown","source":"Let's have a look at correlations now!"},{"metadata":{"trusted":true,"_uuid":"ae63462aa70238f0a2858de687dc7d2ae319589a"},"cell_type":"code","source":"corrs = train.corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\ncorrs = corrs[corrs['level_0'] != corrs['level_1']]\ncorrs.tail(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2d921a5d3bf606b88853988c10acad020685334"},"cell_type":"markdown","source":"We can see that correlations between features are lower that 0.3 and the most correlated feature with target has correlation of 0.37. So we have no highly correlated features which we could drop, on the other hand we could drop some columns with have little correlation with the target."},{"metadata":{"_uuid":"a4f28e1e3c847e2fe165034dd870154afb7fe939"},"cell_type":"markdown","source":"## Basic modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nRandomForestClassifier?","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Above is to see the default settings of RandomForest"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\nX_test = test.drop(['id'], axis=1)\nn_fold = 20\nfolds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\nrepeated_folds = RepeatedStratifiedKFold(n_splits=10, n_repeats=20, random_state=42)\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(X, X_test, y, params, folds=folds, model_type='lgb', plot_feature_importance=False, averaging='usual', model=None):\n    oof = np.zeros(len(X))\n    prediction = np.zeros(len(X_test))\n    scores = []\n    feature_importance = pd.DataFrame()\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n        # print('Fold', fold_n, 'started at', time.ctime())\n        X_train, X_valid = X[train_index], X[valid_index]\n        y_train, y_valid = y[train_index], y[valid_index]\n            \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = roc_auc_score(y_valid, y_pred_valid)\n            # print(f'Fold {fold_n}. AUC: {score:.4f}.')\n            # print('')\n            \n            y_pred = model.predict_proba(X_test)[:, 1]\n            \n        oof[valid_index] = y_pred_valid.reshape(-1,)\n        scores.append(roc_auc_score(y_valid, y_pred_valid))\n\n        if averaging == 'usual':\n            prediction += y_pred\n        elif averaging == 'rank':\n            prediction += pd.Series(y_pred).rank().values  \n        \n    prediction /= n_fold\n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    if model_type == 'lgb':\n        feature_importance[\"importance\"] /= n_fold\n        if plot_feature_importance:\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n        \n            return oof, prediction, feature_importance\n        return oof, prediction, scores\n    \n    else:\n        return oof, prediction, scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest Basic Model Creation and Cross Validation on the whole dataset"},{"metadata":{},"cell_type":"markdown","source":"# Cross Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\n\nparameter_grid = {'n_estimators': [10,100,500,1000],\n                  'max_depth': [None,2,3,5]\n                 }\n\ngrid_search = GridSearchCV(rfc, param_grid=parameter_grid, cv=folds, scoring='roc_auc', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))\nrfc = RandomForestClassifier(**grid_search.best_params_)\noof_rfc, prediction_rfc, scores_rfc = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=rfc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = rfc","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"600682b545014ae67e19a8b04724e75767be6014"},"cell_type":"markdown","source":"## ELI5 for Feature Selection\n\nELI5 is a package with provides explanations for ML models. It can do this not only for linear models, but also for tree based like Random Forest or lightgbm."},{"metadata":{"trusted":true,"_uuid":"b97e881aec4c17fda9e106bd2970c763d65abe5d"},"cell_type":"code","source":"perm = PermutationImportance(model, random_state=1).fit(X_train, y_train)\neli5.show_weights(perm, top=50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Select Top 30 features"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_features = [i[1:] for i in eli5.formatters.as_dataframe.explain_weights_df(model).feature][:30]\ntop_features","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c738de31f86152ced6cb35ddb8d3569e7b49a6e"},"cell_type":"markdown","source":""},{"metadata":{"trusted":true,"_uuid":"4c78622313d2963bb28aa2870e0ed9f811f315ba"},"cell_type":"code","source":"X_train = train[top_features]\nX_test = test[top_features]\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_lr, prediction_lr, scores = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=model)\nprint(scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/sample_submission.csv')\nsubmission['target'] = prediction_lr\nsubmission.to_csv('submission_top30.csv', index=False)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}