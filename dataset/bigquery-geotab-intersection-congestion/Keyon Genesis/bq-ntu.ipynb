{"cells":[{"metadata":{},"cell_type":"markdown","source":"Introduction\nThis dataset contains information regarding traffic congestion in major US Interstates and attempts to create a model that accurately predicts future congestion. The development of this notebook will take on the following structure:\n\n* Exploratory Analysis: This stage will explore the data, rename labels as appropriate and discover that kind of pre-processing must be made (whether there are empty datapoints, distribution of data, entropy of each feature).\n* Preprocessing: This stage will pre-process the data to put it in a way the model can make an accurate prediction.\n* Algorithm selection and implementation\n* Model evaluation and optimization stage\n* Custom model creation (optional)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nimport folium\nfrom folium.plugins import HeatMap\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n# Setting up BigQuery library\nPROJECT_ID = 'bigquery-ml-geotab'\nfrom google.cloud import bigquery\nclient = bigquery.Client(project=PROJECT_ID, location=\"US\")\nfrom google.cloud import storage\nstorage_client = storage.Client(project=PROJECT_ID)\nfrom google.cloud import automl_v1beta1 as automl\nautoml_client = automl.AutoMlClient()\nfrom google.cloud.bigquery import magics\nfrom kaggle.gcp import KaggleKernelCredentials\nmagics.context.credentials = KaggleKernelCredentials()\nmagics.context.project = PROJECT_ID\n\n# load biquery commands\n%load_ext google.cloud.bigquery","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Exploratory AnalysisÂ¶\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read data\ndf_train = pd.read_csv('/kaggle/input/bigquery-geotab-intersection-congestion/train.csv')\ndf_test = pd.read_csv('/kaggle/input/bigquery-geotab-intersection-congestion/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our test data however, has a different shape to our training data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training data has 27 features in total while testing data has 13. Some are labeled as objects since they are string data while the rest are numerical data. Next, let's see which, and how much, data is missing. We'll start with numerical data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are only two numerical features that have a numeric data type values. We'll have to fill in these values later in the processing stage. Now let's see how much categorical data is missing:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.select_dtypes.html\nobj_df = df_train.select_dtypes(include=['object'])\n# Return whether any element is True, potentially over an axis.\nobj_df[obj_df.isnull().any(axis=1)].count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like EntryHeading, ExitHeading, Path and City have the same amount of null values. The amount for each of these values represents about 1.5% of our training data for that column. This seems like a small slice of our data but we can't gurantee that ignoring these rows won't affect the final outcome. We'll try to fill in these values later as well.\n\nTo gain a better understanding of the data, let's see what the distirbution is for each of the features. We will start with count for cities since that is the easiest to digest."},{"metadata":{},"cell_type":"markdown","source":"We can clearly see that Philadelphia has the highest data count of all cities. The data is therefore unevenly distributed and this could affect our models. However, this is total count for all cities and that doesn't yield too much more information, let's group the datacount by the number of unique Intersection Ids - this is the id given to each intersection where traffic data is being measured."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for distribution of data BY UNIQUE INTERSECTION ID\nfig = df_train.groupby(['City'])['IntersectionId'].nunique().sort_index().plot.bar()\nfig.set_title('# of Intersections per city in train Set', fontsize=15)\nfig.set_ylabel('# of Intersections', fontsize=15);\nfig.set_xlabel('City', fontsize=17);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interesting, altough Philadelphia has the highest data count, Chicago has the highest number of unique intersections. This tells us that, although Philly has more data, Chicago has the larger share of unique intersections. Does this mean that Chicago has more traffic that Philly? Not necessarily, Chicago can have more intersections but Philadelphia can have more traffic total.\n\nTo explore this assumption, let's find out what is the distribution of the traffic by month and week:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's see the distribution of traffic by month and date\nplt.figure(figsize=(15,12))\n\nplt.subplot(211)\ng = sns.countplot(x=\"Hour\", data=df_train, hue='City', dodge=True)\ng.set_title(\"Distribution by hour and city\", fontsize=20)\ng.set_ylabel(\"Count\",fontsize= 17)\ng.set_xlabel(\"Hours of Day\", fontsize=17)\nsizes=[]\nfor p in g.patches:\n    height = p.get_height()\n    sizes.append(height)\n\ng.set_ylim(0, max(sizes) * 1.15)\n\nplt.subplot(212)\ng1 = sns.countplot(x=\"Month\", data=df_train, hue='City', dodge=True)\ng1.set_title(\"Hour Count Distribution by Month and City\", fontsize=20)\ng1.set_ylabel(\"Count\",fontsize= 17)\ng1.set_xlabel(\"Months\", fontsize=17)\nsizes=[]\nfor p in g1.patches:\n    height = p.get_height()\n    sizes.append(height)\n\ng1.set_ylim(0, max(sizes) * 1.15)\n\nplt.subplots_adjust(hspace = 0.3)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, philly comes out on top when it comes to count of traffic data. However this still doesn't give enough support to the theory that Philly has more traffic simply because it has more data. We could check this assumption by seeing how much actual stopping there is in philly traffic vs other city's traffic."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}