{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Contents\n1. [Introduction](#Introduction)\n2. [Data Loading and Overview](#DataLoad)\n3. [Data Exploration](#data_exp) <br>\n    3.1 [Target Distribution](#data_exp)<br>\n    3.2 [Direction](#direction) <br>\n    3.3 [Location & Time](#locationtime) <br>\n4. [Conclusion](#conclusion)"},{"metadata":{},"cell_type":"markdown","source":"## Introduction\nThis kernel will be an exploratory data analysis on the BigQuery-Geotab Intersection Congestion competition data.\n\nGeotab provides a wide variety of aggregate datasets gathered from commercial vehicle telematics devices. Harnessing the insights from this data has the power to improve safety, optimize operations, and identify opportunities for infrastructure challenges.\n\nWe have a regression problem for which we must predict 6 target values: three statistics: the 20th, 50th, and 80th percentiles, for each of two metrics: the total time a vehicle stopped at an intersection and the distance between the intersection and the first place a vehicle stopped while waiting,for each observation in the test set.  The given data consists of aggregated trip logging metrics from commercial vehicles, such as semi-trucks. The data have been grouped by intersection, month, hour of day, direction driven through the intersection, and whether the day was on a weekend or not.\n\n\n![](https://www.geotab.com/blog/wp-content/uploads/2018/07/traffic-congestion.jpg)\n\nImage Source: *https://www.geotab.com/blog/traffic-congestion/*"},{"metadata":{},"cell_type":"markdown","source":"*Importing libraries..*"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":false,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.style as style \nstyle.use('seaborn-bright')\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tqdm\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data loading and overview <a id = 'DataLoad'></a>"},{"metadata":{},"cell_type":"markdown","source":"*Loading data..*"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"train = pd.read_csv('../input/bigquery-geotab-intersection-congestion/train.csv')\ntest = pd.read_csv('../input/bigquery-geotab-intersection-congestion/test.csv')\nmergedData = train.merge(test, 'outer')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"mergedData.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data features:** <br>\n- RowId: Unique identifier for each row/observation, each of which is an aggregate of some congestion data.\n- IntersectionId: Identifier for specific intersection (2839 unique in train and test sets combined) \n- Latitude\n- Longitude\n- EntryStreetName\n- ExitStreetName\n- EntryHeading: Direction cars entering intersection are moving in (N, W, E, S, NW, SW, SE, NE)\n- ExitHeading: Direction cars exiting intersection are moving in (N, W, E, S, NW, SW, SE, NE)\n- Hour: Hour of day (0-23)\n- Weekend: No: 0, Yes: 1\n- Month: Has unique values (1,5,6,7,8,9,10,11,12) in both train and test sets, which implies that the congestion data ranges from May of one year to January of the next year.\n- Path: Concatenation of strings from the columns: EntryStreetName, EntryHeading, ExitStreetName, ExitHeading\n- City: Atlanta, Boston, Chicago, or Philadelphia\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Train dataset has {train.shape[0]} rows and {train.shape[1]} columns.')\nprint(f'Test dataset has {test.shape[0]} rows and {test.shape[1]} columns.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have two moderately sized datasets, with the number of observations in the test set being notably higher than that of the train set.  The following features are the difference between the two sets:"},{"metadata":{"trusted":true},"cell_type":"code","source":"[col for col in train.columns if col not in test.columns]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The test set contains the same columns from the training set except for the above features. \nThe task is to predict the 20th, 50th, and 80th percentiles of the variables TotalTimeStopped and DistanceToFirstStop.  It seems that additional percentile statistics, as well as a TimeFromFirstStop metric have been provided to assist with model building."},{"metadata":{},"cell_type":"markdown","source":"Examine the columns that have missing data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Print names of columns in train set with null values, along with number of null values:')\ntrainNull = [col for col in  train.columns if train[col].isnull().any()]\ntrainNullD = {}\nfor col in trainNull:\n    trainNullD.update({col: ['Number missing: ' + str(train[col].isnull().sum()), \n                             'Proportion missing: ' + str(round(train[col].isnull().sum() / train.shape[0],3))]})\nfor a,b in trainNullD.items():\n    print(a)\n    print(b)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Print names of columns in test set with null values, along with number of null values:')\ntestNull = [col for col in  test.columns if test[col].isnull().any()]\ntestNullD = {}\nfor col in testNull:\n    testNullD.update({col: ['Number missing: ' + str(test[col].isnull().sum()), \n                             'Proportion missing: ' + str(round(test[col].isnull().sum() / test.shape[0],3))]})\nfor a,b in testNullD.items():\n    print(a)\n    print(b)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The columns with missing data are the same in both train and test sets: EntryStreetName and ExitStreetName. It seems that most columns are without any missing data. And those that do have only a small proportion missing, so it should be safe to simply remove those observations prior to model fitting."},{"metadata":{},"cell_type":"markdown","source":"## Data Exploration: Target Distribution <a id = 'data_exp'></a>"},{"metadata":{},"cell_type":"markdown","source":"First, let's see how each of the given statistics are distributed."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Create lists of columns for each group of statistics\nTotalTimeStoppedCols = list(train.loc[:,'TotalTimeStopped_p20':'TotalTimeStopped_p80'].columns)\nDistanceToFirstStop = list(train.loc[:,'DistanceToFirstStop_p20':'DistanceToFirstStop_p80'].columns)\nTimeFromFirstStopCols = list(train.loc[:,'TimeFromFirstStop_p20':'TimeFromFirstStop_p80'].columns)\n\n\nf1, axes = plt.subplots(3,5, figsize=(20, 10), sharex=False)\n\nfor i, col in enumerate(TotalTimeStoppedCols):\n    sns.distplot(train[col], kde=False, ax=axes[0,i], label = col, color = 'r')\n    \nfor i, col in enumerate(DistanceToFirstStop):\n    sns.distplot(train[col], kde=False, ax=axes[1,i], label = col,  color = 'g')\n    \nfor i, col in enumerate(TimeFromFirstStopCols):  \n    sns.distplot(train[col], kde=False, ax=axes[2,i], label = col, color = 'b')    \n\n    \nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"For each of the three statistics, the number of samples contained in each percentile visibly increases as the histograms are read from left to right, as expected.  Let's zoom in on the highest available percentile for each, which encompasses all available data."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#lastPercentiles = ['TotalTimeStopped_p80', 'DistanceToFirstStop_p80', 'TimeFromFirstStop_p80']\nf2, axes2 = plt.subplots(1,3, figsize=(20, 8), sharex=False)\n\nsns.distplot(train['TotalTimeStopped_p80'], kde=False, ax=axes2[0], color = 'r' )\nsns.distplot(train['DistanceToFirstStop_p80'], kde=False, ax=axes2[1], color = 'g' )\nsns.distplot(train['TimeFromFirstStop_p80'], kde=False, ax=axes2[2], color = 'b' )\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data is skewed towards zero for all three statistics.  This implies that cars are most likely to pass through the intersection without stopping, out of all possible stopping times,  and higher stopping times are increasingly unlikely, which makes sense. TotalTimeStopped by itself should be enough to provide a good idea of how heavy the congestion is.  For each congestion data aggregate (each row), a low value of TotalTimeStopped_p80 should thus be representative of normal or good traffic, while a high value would represent slow traffic.  As a simple test, the average 80th percentile of total stop time is 24.83 seconds on weekdays, and 18.05 seconds on weekends.  This makes sense as most people are more likely to work on weekdays only and rest on weekends, leading to busier streets on weekdays due to commuting to/from work, etc."},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"mergedData.groupby('Weekend')['TotalTimeStopped_p80'].mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Exploration: Direction <a id='direction' > </a>"},{"metadata":{},"cell_type":"markdown","source":"From prior driving experience, one can recall that it would take on average more time to make a left/right turn than to drive straight through an intersection.  This makes sense since oncoming cars in the opposite lane have the right of way, and one would have to wait for them to pass before turning, leading to blocking/slowing down the cars behind you as well.  Let's see if the data agrees with us by creating an 'isSameDirection' variable, which is true if the entry direction and exit direction are the same and false otherwise:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#%%time\ntrain['isSameDirection'] = train['EntryHeading'] == train['ExitHeading']\n\nf3, axes3 = plt.subplots(1,2, figsize=(14, 6))\n\nbar = sns.barplot(x='isSameDirection', y='TotalTimeStopped_p80', data = train, palette = 'rocket', \n                  ax = axes3[0] )\naxes3[0].set_title('Barplot of average stop time vs. isSameDirection')\naxes3[0].set_xlabel('isSameDirection')\naxes3[0].set_ylabel('Avg. of TotalTimeStopped_p80')\n\nstrip = sns.stripplot(x='isSameDirection', y='TotalTimeStopped_p80', data = train, palette = 'rocket', \n                      jitter = 0.5, alpha = 0.3, ax = axes3[1])\naxt = axes3[1].twinx()\nvio = sns.violinplot(x='isSameDirection', y='TotalTimeStopped_p80', data = train, palette = 'rocket',\n                    ax = axt)\n\naxes3[1].set_title('Stripplot of TotalTimeStopped_p80 vs. isSameDirection')\naxes3[1].set_xlabel('isSameDirection')\naxes3[1].set_ylabel('TotalTimeStopped_p80')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"print(train.groupby('isSameDirection')['TotalTimeStopped_p80'].agg('mean'))\nprint(train['isSameDirection'].value_counts(normalize=True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that cars approaching intersections drive straight through with a probability of roughly 0.70, and turn in a different direction with a probability of 0.30 (This distribution is very nearly the same for the test set as well). <br> The barplot supports our earlier guess: that cars travelling through intersections without changing direction would, on average, have lower total stop time.   <br> isSameDirection should be a feature worth keeping for model training.  Taking this idea further, it might be worth investigating the same metric for all combinations of entry and exit direction from (N, W, E, S, NW, SW, SE, NE).  Let's take a look at the counts of all the entry-exit combinations:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#column 'Path' has extraneous information (i.e. street names), we only want path direction\ntrain['pathDirectionOnly'] = train['EntryHeading'] + '_' + train['ExitHeading']\n\npathd = pd.DataFrame({'EntryHeading_ExitHeading': train['pathDirectionOnly'].value_counts().index, \n                      'Count': train['pathDirectionOnly'].value_counts()})\n\nf4, axes4 = plt.subplots( figsize=(13, 14))\nsns.barplot(x='Count', y='EntryHeading_ExitHeading', data = pathd, ax = axes4, palette='Blues_r')\naxes4.set_title('Counts of all entry-exit combinations')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are a total of 64 entry-exit combinations.  The most frequent ones involve no turning (i.e. East -> East: 0 degrees), the next most frequent are simple right/left turns (i.e. East ->North: 90 degrees), followed by turns of abnormal angles (i.e. Northwest -> West: 45 degrees).  It may be more informative to convert these 64 entry-exit combinations into the 5 possible angles associated with a turn (0, 45, 90, 135, 180), and check how this feature compares with stop time."},{"metadata":{},"cell_type":"markdown","source":"![](https://i.imgur.com/FBQ0Uxn.png)"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"#Define function to get angle between 'EntryHeading' and 'ExitHeading'\ndef getAngleOfTurn(df):\n        compassAngle = {'N':0, 'NE':45, 'E':90, 'SE':135, 'S':180, \n                        'SW':225, 'W':270, 'NW':315 }\n        a = df['EntryHeading'] \n        b = df['ExitHeading']\n        angle = abs(compassAngle[a] - compassAngle[b])\n        if angle < 225:\n            return angle\n        elif angle == 225:\n            return 135\n        elif angle == 270:\n            return 90\n        elif angle == 315:\n            return 45        \ntrain['angleOfTurn'] = train.apply(getAngleOfTurn, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"anglesDf = pd.DataFrame({'angleOfTurn': train.groupby('angleOfTurn')['TotalTimeStopped_p80'].agg('mean').index,\n                         'Count': train.groupby('angleOfTurn')['angleOfTurn'].agg('count'),\n                         'StopTimeMean' : train.groupby('angleOfTurn')['TotalTimeStopped_p80'].agg('mean')})\n\nf5, axes5 = plt.subplots( figsize=(12, 8))\n\ndircount = sns.barplot(x='angleOfTurn', y='Count' , data = anglesDf, palette = 'rocket', ax = axes5)\ndircount.set_title('Counts of turning angles and associated avg. stop times')\ndircount.set_ylabel('Count')\ndircount.set_xlabel('Degrees')\n\na = axes5.twinx()\nsns.pointplot(x = 'angleOfTurn', y = 'StopTimeMean', color = 'coral',data = anglesDf, ax = a)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"anglesDf.iloc[:,1:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The frequency of each turn angle is consistent with our previous investigation of 'isSameDirection' - turns of 0 degrees count for around 0.70 of all turns, and frequencies of all other turn angles (45, 90, 135, and 180) sum to roughly 0.30.  Plotting the average 80th percentile of TotalTimeStopped (abbreviated as StopTimeMean) for each of the angle groups on the same axis reveals a linear relationship between the magnitude of the angle turned and the time stopped.  Interestingly, there are even some drivers that made complete U-turns of 180 degrees at the intersection, going back the way they came.  Unsurprisingly, these turned out to have the highest average stop time.  'angleOfTurn' should be a feature worth keeping."},{"metadata":{},"cell_type":"markdown","source":"## Data Exploration: Location & Time <a id = 'locationtime' ></a>"},{"metadata":{},"cell_type":"markdown","source":"![](https://i.imgur.com/Zdohjf3.jpg)"},{"metadata":{},"cell_type":"markdown","source":"Another space-relevant component of data given to us is latitude and longitude.  Latitude provides information about the distance north or south of the equator and ranges from 0 to 90 (0 at the equator, 90 at the North or South Pole), while longitude provides information about the distance east or west of the Prime Meridian, an imaginary line drawn between the North and South Poles, passing through Greenwich, England, and ranges from 0 to 180. "},{"metadata":{},"cell_type":"markdown","source":"The Earth’s axis is tilted 23.5° to the perpendicular, meaning that the amount of sunlight that a particular latitude receives changes with the seasons. From April to September, the Northern Hemisphere is tilted toward the Sun, where it receives more energy; the Southern Hemisphere receives this additional energy between October and March, when it is tilted toward the Sun. *Source: https://enviroliteracy.org/air-climate-weather/climate/latitude-climate-zones/*\n"},{"metadata":{},"cell_type":"markdown","source":"With this in mind, it should be worthwhile to investigate the effect of climate on traffic by plotting the relationship between latitude and average stop times, organized by seasons.  For our purposes here, we'll treat values of latitude as a categorical variable and round them, since our data encompass only four cities, for each of which the values of latitude are sharply distributed."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"f7, axes7 = plt.subplots(1,2, figsize=(14, 4))\nkde1 = sns.kdeplot(data = train['Latitude'], shade = True, color = 'lightskyblue', ax = axes7[0])\nkde1.set_xlabel('Latitude')\nkde1.set_title('KDE of Latitude')\n\nkde2 = sns.countplot(x = 'City', data = train, ax = axes7[1],  palette = 'Blues',\n                     order = ['Atlanta', 'Philadelphia', 'Chicago', 'Boston'] )\nkde2.set_title('Countplot of City')\nkde2.set_ylabel('Count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Atlanta: Latitude ~34° N\n- Philadelphia: Latitude ~40° N\n- Chicago & Boston: Latitude ~ 42° N\n"},{"metadata":{},"cell_type":"markdown","source":"Since Boston and Chicago have nearly the same latitude, and climate differences are unlikely to be noticeable over such a small range of latitude difference, we will categorize these two cities together."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#%%time\ntrain['LatitudeRounded'] = round(train['Latitude'])\n\ntrainSpring = train.loc[train['Month'].isin([3,4,5])]\ntrainSummer = train.loc[train['Month'].isin([6,7,8])]\ntrainFall = train.loc[train['Month'].isin([9,10,11])]\ntrainWinter = train.loc[train['Month'].isin([12,1,2])]\n\nf6, axes6 = plt.subplots(2, 2, figsize=(12, 8))\n##############################################################################\na = sns.stripplot(x='LatitudeRounded', y='TotalTimeStopped_p80', data = trainSpring, \n                  color = 'palegreen', alpha = 0.3, ax=axes6[0,0])\naxt1 = axes6[0,0].twinx()\nvio1 = sns.violinplot(x='LatitudeRounded', y='TotalTimeStopped_p80', data = trainSpring,\n                     color = 'palegreen', alpha = 0.3, ax = axt1)\na.set_title('Spring')\na.set_ylim(0,700)\na.set_autoscaley_on(False)\naxt1.set_ylabel('')\naxt1.set_ylim(0,700)\naxt1.set_autoscaley_on(False)\n##############################################################################\nb = sns.stripplot(x='LatitudeRounded', y='TotalTimeStopped_p80', data = trainSummer, \n                color = 'salmon', alpha = 0.3, ax=axes6[0,1])\naxt2 = axes6[0,1].twinx()\nvio2 = sns.violinplot(x='LatitudeRounded', y='TotalTimeStopped_p80', data = trainSummer,\n                     color = 'salmon', alpha = 0.3, ax = axt2)\nb.set_title('Summer')\nb.set_ylim(0,700)\nb.set_autoscaley_on(False)\naxt2.set_ylabel('')\naxt2.set_ylim(0,700)\naxt2.set_autoscaley_on(False)\n##############################################################################\nc = sns.stripplot(x='LatitudeRounded', y='TotalTimeStopped_p80', data = trainFall,\n                color = 'orange', alpha = 0.3, ax=axes6[1,0])\naxt3 = axes6[1,0].twinx()\nvio3 = sns.violinplot(x='LatitudeRounded', y='TotalTimeStopped_p80', data = trainFall,\n                     color = 'orange', alpha = 0.3, ax = axt3)\nc.set_title('Fall')\nc.set_ylim(0,700)\nc.set_autoscaley_on(False)\naxt3.set_ylabel('')\naxt3.set_ylim(0,700)\naxt3.set_autoscaley_on(False)\n##############################################################################\nd = sns.stripplot(x='LatitudeRounded', y='TotalTimeStopped_p80', data = trainWinter,\n                color = 'lightblue', alpha = 0.3, ax=axes6[1,1])\naxt4 = axes6[1,1].twinx()\nvio4 = sns.violinplot(x='LatitudeRounded', y='TotalTimeStopped_p80', data = trainWinter,\n                     color = 'lightblue', alpha = 0.3, ax = axt4)\nd.set_title('Winter')\nd.set_ylim(0,700)\nd.set_autoscaley_on(False)\naxt4.set_ylabel('')\naxt4.set_ylim(0,700)\naxt4.set_autoscaley_on(False)\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unfortunately for our \"Spring\" dataset, there contains data only from the month of May, with data from March and April missing, and our \"Winter\" dataset is also missing a month, February (Since data was taken for all months except February, March, and April).  Observing each of the remaining seasons, Summer and Fall, it seems that there are differences in stop time among the different locations.  However, this cannot simply be attributed to climate differences since there may have been differences in infrastructure etc. among the cities that were the cause instead.  In Summer, Latitude 34  corresponds to the highest stop times, while in Fall, Latitude 42 corresponds to the highest stop times. However, this too may be due to indirect effects of season on people's behaviors and is not necessarily evidence of a direct effect of climate on traffic. <br>\nIt seems that we are unable to distinguish between effects of infrastructure and climate.  Let's instead try to investigate the differences in infrastructure among the four cities."},{"metadata":{},"cell_type":"markdown","source":"We will revisit our earlier plot of 'Counts of turning angles and associated avg. stop times', but now we will create four different subplots, grouping data by each of the cities.  "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"trainAtlanta = train.loc[train['City'] == 'Atlanta']\ntrainBoston = train.loc[train['City'] == 'Boston']\ntrainChicago = train.loc[train['City'] == 'Chicago']\ntrainPhiladelphia = train.loc[train['City'] == 'Philadelphia']\nt = [trainAtlanta, trainBoston, trainChicago, trainPhiladelphia]\nn = ['Atlanta', 'Boston', 'Chicago', 'Philadelphia']\nI = 0\nf8, axes8 = plt.subplots(2,2, figsize=(12, 8))\nsplots = [axes8[0,0], axes8[0,1], axes8[1,0], axes8[1,1]]\n\nfor item in splots:\n    anglesDf = pd.DataFrame({'angleOfTurn': t[I].groupby('angleOfTurn')['TotalTimeStopped_p80'].agg('mean').index,\n                         'Count': t[I].groupby('angleOfTurn')['angleOfTurn'].agg('count'),\n                         'StopTimeMean' : t[I].groupby('angleOfTurn')['TotalTimeStopped_p80'].agg('mean')})\n    \n    dircount = sns.barplot(x='angleOfTurn', y='Count' , data = anglesDf, palette = 'rocket', ax = item)\n    dircount.set_title(n[I])\n    dircount.set_ylabel('Count')\n    dircount.set_xlabel('Degrees')\n    \n    a = item.twinx()\n    sns.pointplot(x = 'angleOfTurn', y = 'StopTimeMean', color = 'coral',data = anglesDf, ax = a)\n    \n    I = I+1\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although quite abstract, information on the frequency of turn angles should still provide some insight on differences in infrastructure among the four cities.  For all cities except Boston, the trend remains that turn frequencies have the following decreasing order: 0, 90, 45, 135, and 180 degrees.  But there are very clear differences in the proportions of each when comparisons are made among the different cities.  The exact reasons for these differences may be un-extractable from this abstract view of the data.  It may be due to genuine structural differences among the four cities, or it may simply be a matter of where/when Geotab decided to collect their data, or even a combination of the two.\n<br>\nAlso, it seems to be generally true that increasing turn angle means increasing stop time, even when we group the data by city.  The few exceptions that can be seen above are likely attributed to these large-angle turns being made at non-busy intersections and/or times, removing the need for the driver to stop and wait due to blockage from other cars.  "},{"metadata":{},"cell_type":"markdown","source":"Now let us explore the times of day and see which times are busiest.  At the same time, we'll also take a look at the relationship between season and traffic from a different perspective.  Since data for the months of Spring are missing except for its last, May, we will consider May to be part of the season that immediately follows: Summer.  And for the purpose of maintaining (approximately) equal amounts of data in each season set, I don't think anyone would mind if we shifted the last month of Summer \"down\" into the next, and do the same for Fall.  After this process, our new seasons contain the following months... Summer: 5,6,7; Fall: 8,9,10; Winter: 11,12,1."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"trainSummer2 = train.loc[train['Month'].isin([5,6,7])]\ntrainFall2 = train.loc[train['Month'].isin([8,9,10])]\ntrainWinter2 = train.loc[train['Month'].isin([11,12,1])]\n\n\nf9, axes9 = plt.subplots(3,1, figsize=(12, 9))\nc = sns.countplot(x = 'Hour', data = trainSummer2, hue = 'City', palette = 'rocket', ax = axes9[0])\nc.set_title('Summer')\nc.set_ylabel('Count')\n       \naxt = axes9[0].twinx()\np = sns.pointplot(x='Hour', y='TotalTimeStopped_p80', data = trainSummer2, color = 'salmon',ax = axt )\np.set_ylabel('StopTimeMean')\n#####\nc = sns.countplot(x = 'Hour', data = trainFall2, hue = 'City', palette = 'YlOrRd', ax = axes9[1])\nc.set_title('Fall')\nc.set_ylabel('Count')\n       \naxt = axes9[1].twinx()\np = sns.pointplot(x='Hour', y='TotalTimeStopped_p80', data = trainFall2, color = 'orange', ax = axt )\np.set_ylabel('StopTimeMean')\n#####\nc = sns.countplot(x = 'Hour', data = trainWinter2, hue = 'City', palette = 'Blues', ax = axes9[2])\nc.set_title('Winter')\nc.set_ylabel('Count')\n       \naxt = axes9[2].twinx()\np = sns.pointplot(x='Hour', y='TotalTimeStopped_p80', data = trainWinter2, color = 'lightblue', ax = axt )\np.set_ylabel('StopTimeMean')\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For all three seasons, there are two notable peaks in average stop time.  The first of these occurs at around 8 AM and the second occurs at around 5 PM.  Sound familiar?  This pattern suggests the commute to-and-from your typical 9-to-5 day job.  It makes sense to observe peaks in traffic activity at these two times.  There also seems to be a minimum in the amount of data available, centered at around 4 AM.  Makes sense, most people are asleep at this time and so one wouldn't expect high levels of traffic.  There doesn't seem to be a significant difference in stop times among the different seasons.  "},{"metadata":{},"cell_type":"markdown","source":"Let's take a closer look at the data contained in different months."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"f11, axes11 = plt.subplots(figsize=(14, 5))\nm = sns.countplot(y= 'Month', hue = 'City',data = train, palette = 'Blues_r', ax = axes11, \n                  order = [10,9,12,11,8,7,6,1,5], hue_order=['Philadelphia','Boston','Atlanta', 'Chicago'])\n#a = axes11.twinx()\n#p = sns.pointplot(x='Month', y='TotalTimeStopped_p80', data = train[train['Month'].isin([6,7,8,9,10,11,12])], color = 'lightblue', ax = a )\nm.set_title('Countplot of months')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Upon closer inspection, it seems that there are disproportionately few data for the months of January and May compared to the other months.  Earlier, we observed stop times over the time frame of hours and found some sensible trends. Now let's observe stop times over the time frame of months and see if we can find patterns on this larger time scale.  We will exclude January and May since they don't contain much data. "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"trainP = train.loc[train['City'] == 'Philadelphia']\ntrainB = train.loc[train['City'] == 'Boston']\ntrainA = train.loc[train['City'] == 'Atlanta']\ntrainC = train.loc[train['City'] == 'Chicago']\n\nf = plt.figure(figsize = (14,8))\ns = plt.subplot2grid((3,2),(0,0), 1, 3,f)\np = plt.subplot2grid((3,2),(1,0), 1, 1,f)\nb = plt.subplot2grid((3,2),(1,1), 1, 1,f)\na = plt.subplot2grid((3,2),(2,0), 1, 1,f)\nc = plt.subplot2grid((3,2),(2,1), 1, 1,f)\n\nm=sns.countplot(x = 'Month', data = train[train['Month'].isin([6,7,8,9,10,11,12])], \n                palette = 'Blues', ax = s)\nat = s.twinx()\npoint = sns.pointplot(x='Month', y='TotalTimeStopped_p80', data = train[train['Month'].isin([6,7,8,9,10,11,12])], \n                   color = 'lightblue', ax = at )\nm.set_title('Stop times across months (all cities)')\nm.set_ylabel('Count')\npoint.set_ylabel('StopTimeMean')\n#####\ntemp = [[trainP,trainB,trainA,trainC],\n        [p,b,a,c],\n        ['Philadelphia', 'Boston', 'Atlanta', 'Chicago']]\nfor i in range(0,4):\n    \n    count = sns.countplot(x = 'Month', data = temp[0][i][temp[0][i]['Month'].isin([6,7,8,9,10,11,12])], \n                   palette = 'PuBu', ax = temp[1][i])\n    count.set_title(temp[2][i])\n    count.set_ylabel('Count')\n    at = temp[1][i].twinx()\n    point = sns.pointplot(x='Month', y='TotalTimeStopped_p80', data = temp[0][i][temp[0][i]['Month'].isin([6,7,8,9,10,11,12])],\n                      color = 'lightblue', ax = at )\n    \n    point.set_ylabel('StopTimeMean')\n\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Viewing the relationship between month and stop times for all cities, a minimum for the stop time can be seen in the data for July, after which the average stop time climbs upwards reaching a maximum at around October and starts to decrease once again towards the months of November and December.  My guess is that as summer time approaches, a significant portion of people take breaks from work and maybe go on vacation, which would explain the minimum at July.  And similar reasoning should also apply as we approach the holiday months near the end of the year, as evidenced by the second dip in stop times.  The same general trend can be seen if we observe each of the cities individually, with some variance in where the minimum and maximum are for these specific cases.  An indirect effect of season/monthly times on average stop time is thus observed.  Direct effects of climate on traffic seem impossible to isolate from the given data alone."},{"metadata":{},"cell_type":"markdown","source":"## Conclusions <a id = 'conclusion' ></a>\n* The data has only a small proportion of null values, located in EntryStreetName and ExitStreetName.\n* There is no traffic data recorded for months February, March, and April, and a very low amount of data is recorded for the months of January and May compared to the other months with data.\n* TotalTimeStopped turns out to be a useful metric for estimating traffic activity.\n* Drivers drive straight through intersections without turning 0.70 of the time, and turn 0.30 of the time.\n* Average stop times depend on whether or not a driver has made a turn at an intersection, with turning resulting in higher average stop times than not turning.\n* Taking the above idea further, we found that stop time has an approximately linear relationship with the magnitude of the angle of the turn.\n* From our observation of differing distributions of turn-angle frequencies for each city, it seems that there are nontrivial infrastructural differences among the 4 cities.\n* Due to the above, it is likely impossible for the human eye to distinguish the effect of these infrastructural differences from the effect of climate differences at different latitudes (if any).\n"},{"metadata":{},"cell_type":"markdown","source":"Thanks for going through my kernel"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}