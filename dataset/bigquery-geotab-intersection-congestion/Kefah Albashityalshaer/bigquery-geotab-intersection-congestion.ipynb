{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Intersection Congestion in 4 major US cities: Atlanta, Boston, Chicago & Philadelphia**\n\nWe’ve all been there: Stuck at a traffic light, only to be given mere seconds to pass through an intersection, behind a parade of other commuters. Imagine if you could help city planners and governments anticipate traffic hot spots ahead of time and reduce the stop-and-go stress of millions of commuters like you.\n\nGeotab provides a wide variety of aggregate datasets gathered from commercial vehicle telematics devices. Harnessing the insights from this data has the power to improve safety, optimize operations, and identify opportunities for infrastructure challenges.\n\nThe dataset for this competition includes aggregate stopped vehicle information and intersection wait times. Your task is to predict congestion, based on an aggregate measure of stopping distance and waiting times, at intersections in 4 major US cities: Atlanta, Boston, Chicago & Philadelphia.\n\n![](https://hotshotwarriors.com/wp-content/uploads/2018/03/I-95-Traffic.jpg)"},{"metadata":{},"cell_type":"markdown","source":"**Geotab** is advancing security, connecting commercial vehicles to the internet and providing web-based analytics to help customers better manage their fleets. Geotab’s open platform and Marketplace, offering hundreds of third-party solution options, allows both small and large businesses to automate operations by integrating vehicle data with their other data assets. As an IoT hub, the in-vehicle device provides additional functionality through IOX Add-Ons. Processing billions of data points a day, Geotab leverages data analytics and machine learning to help customers improve productivity, optimize fleets through the reduction of fuel consumption, enhance driver safety, and achieve strong compliance to regulatory changes. Geotab’s products are represented and sold worldwide through Authorized Geotab Resellers. To learn more, You could visit www.geotab.com "},{"metadata":{},"cell_type":"markdown","source":"This competition is being hosted in partnership with **BigQuery**, a data warehouse for manipulating, joining, and querying large scale tabular datasets. BigQuery also offers BigQuery ML, an easy way for users to create and run machine learning models to generate predictions through a SQL query interface.\n\nAlright, stop waiting and get started!"},{"metadata":{},"cell_type":"markdown","source":"** What is BigQuery ML and when should you use it? ** \n\nBigQuery Machine Learning (BQML) is a toolset that allows you to train and serve machine learning models directly in BigQuery. This has several advantages:\n\nYou don't have to read your data into local memory. One question I get a lot is \"how can I train my ML model if my dataset is just too big to fit on my computer?\". You can subsample your dataset, of course, but you can also use tools like BQML that train your model directly in your database.\nYou don't have to use multiple languages. Particularly if you're working in a team where most of your teammates don't know Python or R or your preferred language for modelling, working in SQL can make it easier for you to collaborate.\nYou can serve your model immediately after it's trained. Because your model is already in the same place as your data, you can make predictions directly from your database. This lets you get around the hassle of cleaning up your code and either putting it intro production or passing it off to your engineering colleagues.\nBQML probably won't replace all your modelling tools, but it's a nice quick way to train and serve a model without spending a lot of time moving code or data around.\n\n**Models supported by BQML**\n\n  One limitation of BQML is that a limited number of model types are supported. As of August 6, 2019, BQML supports the following         types of models. More model types are being built out, though, so check the documentation for the most\n\n* Linear regression (LINEAR_REG). This is the OG modelling technique, used to predict the value of a continuous variable. This is what you'd use for questions like \"how many units can we expect a custom to buy?\".\n* Logistic regression (LOGISTIC_REG). This regression technique lets you classify which category an observation fits in to. For example, \"will this person buy the blue one or the red one?\".\n* K-means (KMEANS). This is an unsupervised clustering algorithm. It lets you identify categories. For example, \"given all of the customers in our database, how could we identify five distinct groups?\".\n* Tensorflow (TENSORFLOW). If you've already got a trained TensorFlow model, you can upload it to BQML and serve it directly from there. You can't currently train a TensorFlow model in BQML."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import preprocessing\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\n# import keras\nfrom bayes_opt import BayesianOptimization\nimport lightgbm as lgb\nimport os, sys\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install tensorflow --upgrade","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow import feature_column\nfrom tensorflow.keras import layers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Data loading and Exploration "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load data \ntrain = pd.read_csv('../input/bigquery-geotab-intersection-congestion/train.csv')\ntest = pd.read_csv('../input/bigquery-geotab-intersection-congestion/test.csv')\nsubmission = pd.read_csv('../input/bigquery-geotab-intersection-congestion/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The target columns is not found in the testing data  so we don't need to pop it "},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dropna(axis=0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are asked to predict TotalTimeStopped_p20, TotalTimeStopped_p50, TotalTimeStopped_p80, DistanceToFirstStop_p20, DistanceToFirstStop_p50 and DistanceToFirstStop_p80\n\nWe also have a feature called TimeFromFirstStop_px in the training set that can be usefull\n\nOther percentiles for the features mention recently can be found in the training set. Maybee it is a good idea to predict all the percentiles and use it in a smart way to improve our results"},{"metadata":{},"cell_type":"markdown","source":"> Missing Values\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def missing_values(train):\n    df = pd.DataFrame(train.isnull().sum()).reset_index()\n    df.columns = ['Feature', 'Frequency']\n    df['Percentage'] = (df['Frequency']/train.shape[0])*100\n    df['Percentage'] = df['Percentage'].astype(str) + '%'\n    df.sort_values('Percentage', inplace = True, ascending = False)\n    return df\n\nmissing_values(train).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 2 features in the train set and test set that have missing values.\n\nLet's check each feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finding the numerical columns \nnum_cols = train._get_numeric_data().columns\nprint(\"Numerical Columns\")\nprint(num_cols)\n\n# Get list of categorical variables\ns = (train.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)\nfor object_col in object_cols:\n    print(\"---------------------------\")\n    print(train[object_col].unique())    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Submission data\n#the first number being the RowId and the second being the metric id (of the TargetId)\nsubmission\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Data Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in ['TotalTimeStopped_p20', 'TotalTimeStopped_p50', 'TotalTimeStopped_p80', 'DistanceToFirstStop_p20', \n          'DistanceToFirstStop_p50', 'DistanceToFirstStop_p80']:\n    plt.figure(figsize = (12, 8))\n    plt.scatter(train.index, train[i])\n    plt.title('{} distribution'.format(i))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A lot of 0. Let's calculate what is the percentage of 0 in each of our target variables\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def tv_ratio(train, column):\n    df = train[train[column]==0]\n    ratio = df.shape[0] / train.shape[0]\n    return ratio\n\ntarget_variables = ['TotalTimeStopped_p20', 'TotalTimeStopped_p50', 'TotalTimeStopped_p80', \n                    'DistanceToFirstStop_p20', 'DistanceToFirstStop_p50', 'DistanceToFirstStop_p80']\n\nfor i in target_variables:\n    print('{} have a 0 ratio of: '.format(i), tv_ratio(train, i))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Total Time Stopped**"},{"metadata":{},"cell_type":"markdown","source":"In this section, we are going analysis the total time stopped on the intersections in different cities.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=2, ncols=2)\nsns.set_style(\"whitegrid\")\n\ntrain[train['City']=='Atlanta'].groupby('Hour')['TotalTimeStopped_p80'].mean().plot(\n    ax=ax[0,0],title=\"Atlanda's Total Stoppage Time in Hours\", color='r', figsize=(18,15))\n\ntrain[train['City']=='Boston'].groupby('Hour')['TotalTimeStopped_p80'].mean().plot(\n    ax=ax[0,1],title=\"Boston's Total Stoppage Time in Hours\", color='r', figsize=(18,15))\n\n\ntrain[train['City']=='Chicago'].groupby('Hour')['TotalTimeStopped_p80'].mean().plot(\n    ax=ax[1,0],title=\"Chicago's Total Stoppage Time in Hours\", color='r', figsize=(18,15))\n\n\ntrain[train['City']=='Philadelphia'].groupby('Hour')['TotalTimeStopped_p80'].mean().plot(\n    ax=ax[1,1],title=\"Philadelphia's Total Stoppage Time in Hours\", color='r', figsize=(18,15))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_dist(train, test, column, type = 'kde', together = True):\n    if type == 'kde':\n        if together == False:\n            fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (12,8))\n            sns.kdeplot(train[column], ax = ax1, color = 'blue', shade=True)\n            ax1.set_title('{} distribution of the train set'.format(column))\n            sns.kdeplot(test[column], ax = ax2, color = 'red', shade=True)\n            ax2.set_title('{} distribution of the test set'.format(column))\n            plt.show()\n        else:\n            fig , ax = plt.subplots(1, 1, figsize = (12,8))\n            sns.kdeplot(train[column], ax = ax, color = 'blue', shade=True, label = 'Train {}'.format(column))\n            sns.kdeplot(test[column], ax = ax, color = 'red', shade=True, label = 'Test {}'.format(column))\n            ax.set_title('{} Distribution'.format(column))\n            plt.show()\n    else:\n        if together == False:\n            fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (12,8))\n            sns.distplot(train[column], ax = ax1, color = 'blue', kde = False)\n            ax1.set_title('{} distribution of the train set'.format(column))\n            sns.distplot(test[column], ax = ax2, color = 'red', kde = False)\n            ax2.set_title('{} distribution of the test set'.format(column))\n            plt.show()\n        else:\n            fig , ax = plt.subplots(1, 1, figsize = (12,8))\n            sns.distplot(train[column], ax = ax, color = 'blue', kde = False)\n            sns.distplot(test[column], ax = ax, color = 'red', kde = False)\n            plt.show()\n    \nplot_dist(train, test, 'Latitude', type = 'kde', together = True)\nplot_dist(train, test, 'Latitude', type = 'other', together = False)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Time Features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_frec(df, column):\n    df1 = pd.DataFrame(df[column].value_counts(normalize = True)).reset_index()\n    df1.columns = [column, 'Percentage']\n    df1.sort_values(column, inplace = True, ascending = True)\n    return df1\n\n\ndef plot_frec(train, test, column):\n    df = get_frec(train, column)\n    df1 = get_frec(test, column)\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (12,8))\n    sns.barplot(df[column], df['Percentage'], ax = ax1, color = 'blue')\n    ax1.set_title('{} percentages for the train set'.format(column))\n    sns.barplot(df1[column], df1['Percentage'], ax = ax2, color = 'red')\n    ax2.set_title('{} percentages for the test set'.format(column))\n    \nplot_frec(train, test, 'Month')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Pre-Processing "},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"same_street_exact\"] = (train[\"EntryStreetName\"] ==  train[\"ExitStreetName\"]).astype(int)\ntest[\"same_street_exact\"] = (test[\"EntryStreetName\"] ==  test[\"ExitStreetName\"]).astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Skip OHE intersections for now - memory issues**\n\nIntersection IDs aren't unique between cities - so we'll make new ones\n\nRunning fit on just train reveals that the test data has a \"novel\" city + intersection! ( '3Atlanta'!) (We will fix this)\n\nMeans we need to be careful when OHEing the data\nThere are 2,796 intersections, more if we count unique by city (~4K) = many, many columns. gave me memory issues when doing one hot encoding\nCould try count or target mean encoding.\nFor now - ordinal encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"le = preprocessing.LabelEncoder()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"Intersection\"] = train[\"IntersectionId\"].astype(str) + train[\"City\"]\ntest[\"Intersection\"] = test[\"IntersectionId\"].astype(str) + test[\"City\"]\n\nprint(train[\"Intersection\"].sample(6).values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"with ordinal encoder - \nideally we'd encode all the \"new\" cols with a single missing value, but it doesn't really matter given that they're Out of Distribution anyway (no such values in train).\nSo we'll fit on train+Test in order to avoid encoding errors - when using the ordinal encoder! (LEss of a n issue with OHE)"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.concat([train[\"Intersection\"],test[\"Intersection\"]],axis=0).drop_duplicates().values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le.fit(pd.concat([train[\"Intersection\"],test[\"Intersection\"]]).drop_duplicates().values)\ntrain[\"Intersection\"] = le.transform(train[\"Intersection\"])\ntest[\"Intersection\"] = le.transform(test[\"Intersection\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**OneHotEncode**\n\nWe could Create one hot encoding for entry , exit direction fields - but may make more sense to leave them as continous\nIntersection ID is only unique within a city"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.get_dummies(train[\"City\"],dummy_na=False, drop_first=False).head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.concat([train,pd.get_dummies(train[\"City\"],dummy_na=False, drop_first=False)],axis=1).drop([\"City\"],axis=1)\ntest = pd.concat([test,pd.get_dummies(test[\"City\"],dummy_na=False, drop_first=False)],axis=1).drop([\"City\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape,test.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Approach: We will make 6 predictions based on features we derived - IntersectionId , Hour , Weekend , Month , entry & exit directions .\n\nTarget variables will be TotalTimeStopped_p20 ,TotalTimeStopped_p50,TotalTimeStopped_p80,DistanceToFirstStop_p20,DistanceToFirstStop_p50,DistanceToFirstStop_p80 .\n\nI leave in the original IntersectionId just in case there's meaning accidentally encoded in the numbers"},{"metadata":{"trusted":true},"cell_type":"code","source":"FEAT_COLS = [\"IntersectionId\",\n             'Intersection',\n            'same_street_exact',\n           \"Hour\",\"Weekend\",\"Month\",\n          'Latitude', 'Longitude',\n          'Atlanta', 'Boston', 'Chicago',\n       'Philadelphia']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train[FEAT_COLS]\ny1 = train[\"TotalTimeStopped_p20\"]\ny2 = train[\"TotalTimeStopped_p50\"]\ny3 = train[\"TotalTimeStopped_p80\"]\ny4 = train[\"DistanceToFirstStop_p20\"]\ny5 = train[\"DistanceToFirstStop_p50\"]\ny6 = train[\"DistanceToFirstStop_p80\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train[['TotalTimeStopped_p20', 'TotalTimeStopped_p50', 'TotalTimeStopped_p80',\n        'DistanceToFirstStop_p20', 'DistanceToFirstStop_p50', 'DistanceToFirstStop_p80']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testX = test[FEAT_COLS]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = RandomForestRegressor(n_estimators=100,min_samples_split=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.fit(X,y1)\npred1 = lr.predict(testX)\nlr.fit(X,y2)\npred2 = lr.predict(testX)\nlr.fit(X,y3)\npred3 = lr.predict(testX)\nlr.fit(X,y4)\npred4 = lr.predict(testX)\nlr.fit(X,y5)\npred5 = lr.predict(testX)\nlr.fit(X,y6)\npred6 = lr.predict(testX)\n\n\n# Appending all predictions\nall_preds = []\nfor i in range(len(pred1)):\n    for j in [pred1,pred2,pred3,pred4,pred5,pred6]:\n        all_preds.append(j[i])   \n        \nsub  = pd.read_csv(\"../input/bigquery-geotab-intersection-congestion/sample_submission.csv\")\nsub[\"Target\"] = all_preds\nsub.to_csv(\"benchmark_beat_rfr_multimodels.csv\",index = False)\n\nprint(len(all_preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.fit(X,y)\nprint(\"fitted\")\n\nall_preds = lr.predict(testX)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## convert list of lists to format required for submissions\nprint(all_preds[0])\n\ns = pd.Series(list(all_preds) )\nall_preds = pd.Series.explode(s)\n\nprint(len(all_preds))\nprint(all_preds[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub  = pd.read_csv(\"../input/bigquery-geotab-intersection-congestion/sample_submission.csv\")\nprint(sub.shape)\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub[\"Target\"] = all_preds.values\nsub.sample(5)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}