{"cells":[{"metadata":{"_uuid":"30a31a50-8acd-4aa1-8b2f-25fed7a5b569","_cell_guid":"c4770ae5-ccf0-4ca8-9543-db38bd045530","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai.tabular import *\nfrom fastai.imports import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path=Path('/kaggle/input/bigquery-geotab-intersection-congestion/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.read_csv(path/'train.csv');train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test=pd.read_csv(path/'test.csv');test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Utils"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def apply_cats(df, trn):\n    \n    \"\"\"Changes any columns of strings in df into categorical variables using trn as\n    a template for the category codes.\n    Parameters:\n    -----------\n    df: A pandas dataframe. Any columns of strings will be changed to\n        categorical values. The category codes are determined by trn.\n    trn: A pandas dataframe. When creating a category for df, it looks up the\n        what the category's code were in trn and makes those the category codes\n        for df.\n    Examples:\n    ---------\n    >>> df = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['a', 'b', 'a']})\n    >>> df\n       col1 col2\n    0     1    a\n    1     2    b\n    2     3    a\n    note the type of col2 is string\n    >>> train_cats(df)\n    >>> df\n       col1 col2\n    0     1    a\n    1     2    b\n    2     3    a\n    now the type of col2 is category {a : 1, b : 2}\n    >>> df2 = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['b', 'a', 'a']})\n    >>> apply_cats(df2, df)\n           col1 col2\n        0     1    b\n        1     2    a\n        2     3    a\n    now the type of col is category {a : 1, b : 2}\n    \"\"\"\n    for n,c in df.items():\n        if (n in trn.columns) and (trn[n].dtype.name=='category'):\n            df[n] = c.astype('category').cat.as_ordered()\n            df[n].cat.set_categories(trn[n].cat.categories, ordered=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fix_missing(df, col, name, na_dict):\n    \n    \"\"\" Fill missing data in a column of df with the median, and add a {name}_na column\n    which specifies if the data was missing.\n    Parameters:\n    -----------\n    df: The data frame that will be changed.\n    col: The column of data to fix by filling in missing data.\n    name: The name of the new filled column in df.\n    na_dict: A dictionary of values to create na's of and the value to insert. If\n        name is not a key of na_dict the median will fill any missing data. Also\n        if name is not a key of na_dict and there is no missing data in col, then\n        no {name}_na column is not created.\n    Examples:\n    ---------\n    >>> df = pd.DataFrame({'col1' : [1, np.NaN, 3], 'col2' : [5, 2, 2]})\n    >>> df\n       col1 col2\n    0     1    5\n    1   nan    2\n    2     3    2\n    >>> fix_missing(df, df['col1'], 'col1', {})\n    >>> df\n       col1 col2 col1_na\n    0     1    5   False\n    1     2    2    True\n    2     3    2   False\n    >>> df = pd.DataFrame({'col1' : [1, np.NaN, 3], 'col2' : [5, 2, 2]})\n    >>> df\n       col1 col2\n    0     1    5\n    1   nan    2\n    2     3    2\n    >>> fix_missing(df, df['col2'], 'col2', {})\n    >>> df\n       col1 col2\n    0     1    5\n    1   nan    2\n    2     3    2\n    >>> df = pd.DataFrame({'col1' : [1, np.NaN, 3], 'col2' : [5, 2, 2]})\n    >>> df\n       col1 col2\n    0     1    5\n    1   nan    2\n    2     3    2\n    >>> fix_missing(df, df['col1'], 'col1', {'col1' : 500})\n    >>> df\n       col1 col2 col1_na\n    0     1    5   False\n    1   500    2    True\n    2     3    2   False\n    \"\"\"\n    import pandas as pd\n    if is_numeric_dtype(col):\n        if pd.isnull(col).sum() or (name in na_dict):\n            df[name+'_na'] = pd.isnull(col)\n            filler = na_dict[name] if name in na_dict else col.median()\n            df[name] = col.fillna(filler)\n            na_dict[name] = filler\n    return na_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def numericalize(df, col, name, max_n_cat):\n    \n    \"\"\" Changes the column col from a categorical type to it's integer codes.\n    Parameters:\n    -----------\n    df: A pandas dataframe. df[name] will be filled with the integer codes from\n        col.\n    col: The column you wish to change into the categories.\n    name: The column name you wish to insert into df. This column will hold the\n        integer codes.\n    max_n_cat: If col has more categories than max_n_cat it will not change the\n        it to its integer codes. If max_n_cat is None, then col will always be\n        converted.\n    Examples:\n    ---------\n    >>> df = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['a', 'b', 'a']})\n    >>> df\n       col1 col2\n    0     1    a\n    1     2    b\n    2     3    a\n    note the type of col2 is string\n    >>> train_cats(df)\n    >>> df\n       col1 col2\n    0     1    a\n    1     2    b\n    2     3    a\n    now the type of col2 is category { a : 1, b : 2}\n    >>> numericalize(df, df['col2'], 'col3', None)\n       col1 col2 col3\n    0     1    a    1\n    1     2    b    2\n    2     3    a    1\n    \"\"\"\n    if not is_numeric_dtype(col) and ( max_n_cat is None or len(col.cat.categories)>max_n_cat):\n        df[name] = pd.Categorical(col).codes+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scale_vars(df, mapper):\n    \n    warnings.filterwarnings('ignore', category=sklearn.exceptions.DataConversionWarning)\n    if mapper is None:\n        map_f = [([n],StandardScaler()) for n in df.columns if is_numeric_dtype(df[n])]\n        mapper = DataFrameMapper(map_f).fit(df)\n    df[mapper.transformed_names_] = mapper.transform(df)\n    return mapper","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def proc_df(df, y_fld=None, skip_flds=None, ignore_flds=None, do_scale=False, na_dict=None,\n            preproc_fn=None, max_n_cat=None, subset=None, mapper=None):\n    \n    \"\"\" proc_df takes a data frame df and splits off the response variable, and\n    changes the df into an entirely numeric dataframe. For each column of df \n    which is not in skip_flds nor in ignore_flds, na values are replaced by the\n    median value of the column.\n    Parameters:\n    -----------\n    df: The data frame you wish to process.\n    y_fld: The name of the response variable\n    skip_flds: A list of fields that dropped from df.\n    ignore_flds: A list of fields that are ignored during processing.\n    do_scale: Standardizes each column in df. Takes Boolean Values(True,False)\n    na_dict: a dictionary of na columns to add. Na columns are also added if there\n        are any missing values.\n    preproc_fn: A function that gets applied to df.\n    max_n_cat: The maximum number of categories to break into dummy values, instead\n        of integer codes.\n    subset: Takes a random subset of size subset from df.\n    mapper: If do_scale is set as True, the mapper variable\n        calculates the values used for scaling of variables during training time (mean and standard deviation).\n    Returns:\n    --------\n    [x, y, nas, mapper(optional)]:\n        x: x is the transformed version of df. x will not have the response variable\n            and is entirely numeric.\n        y: y is the response variable\n        nas: returns a dictionary of which nas it created, and the associated median.\n        mapper: A DataFrameMapper which stores the mean and standard deviation of the corresponding continuous\n        variables which is then used for scaling of during test-time.\n    Examples:\n    ---------\n    >>> df = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['a', 'b', 'a']})\n    >>> df\n       col1 col2\n    0     1    a\n    1     2    b\n    2     3    a\n    note the type of col2 is string\n    >>> train_cats(df)\n    >>> df\n       col1 col2\n    0     1    a\n    1     2    b\n    2     3    a\n    now the type of col2 is category { a : 1, b : 2}\n    >>> x, y, nas = proc_df(df, 'col1')\n    >>> x\n       col2\n    0     1\n    1     2\n    2     1\n    >>> data = DataFrame(pet=[\"cat\", \"dog\", \"dog\", \"fish\", \"cat\", \"dog\", \"cat\", \"fish\"],\n                 children=[4., 6, 3, 3, 2, 3, 5, 4],\n                 salary=[90, 24, 44, 27, 32, 59, 36, 27])\n    >>> mapper = DataFrameMapper([(:pet, LabelBinarizer()),\n                          ([:children], StandardScaler())])\n    >>>round(fit_transform!(mapper, copy(data)), 2)\n    8x4 Array{Float64,2}:\n    1.0  0.0  0.0   0.21\n    0.0  1.0  0.0   1.88\n    0.0  1.0  0.0  -0.63\n    0.0  0.0  1.0  -0.63\n    1.0  0.0  0.0  -1.46\n    0.0  1.0  0.0  -0.63\n    1.0  0.0  0.0   1.04\n    0.0  0.0  1.0   0.21\n    \"\"\"\n    if not ignore_flds: ignore_flds=[]\n    if not skip_flds: skip_flds=[]\n    if subset: df = get_sample(df,subset)\n    else: df = df.copy()\n    ignored_flds = df.loc[:, ignore_flds]\n    df.drop(ignore_flds, axis=1, inplace=True)\n    if preproc_fn: preproc_fn(df)\n    if y_fld is None: y = None\n    else:\n        if not is_numeric_dtype(df[y_fld]): df[y_fld] = pd.Categorical(df[y_fld]).codes\n        y = df[y_fld].values\n        skip_flds += [y_fld]\n    df.drop(skip_flds, axis=1, inplace=True)\n\n    if na_dict is None: na_dict = {}\n    else: na_dict = na_dict.copy()\n    na_dict_initial = na_dict.copy()\n    for n,c in df.items(): na_dict = fix_missing(df, c, n, na_dict)\n    if len(na_dict_initial.keys()) > 0:\n        df.drop([a + '_na' for a in list(set(na_dict.keys()) - set(na_dict_initial.keys()))], axis=1, inplace=True)\n    if do_scale: mapper = scale_vars(df, mapper)\n    for n,c in df.items(): numericalize(df, c, n, max_n_cat)\n    df = pd.get_dummies(df, dummy_na=True)\n    df = pd.concat([ignored_flds, df], axis=1)\n    res = [df, y, na_dict]\n    if do_scale: res = res + [mapper]\n    return res","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## DataBunch"},{"metadata":{},"cell_type":"markdown","source":"### Settings"},{"metadata":{"trusted":true},"cell_type":"code","source":"dep_var=['TotalTimeStopped_p20','TotalTimeStopped_p40','TotalTimeStopped_p50', 'TotalTimeStopped_p60', 'TotalTimeStopped_p80','TimeFromFirstStop_p20', 'TimeFromFirstStop_p40','TimeFromFirstStop_p50', 'TimeFromFirstStop_p60','TimeFromFirstStop_p80', 'DistanceToFirstStop_p20','DistanceToFirstStop_p40', 'DistanceToFirstStop_p50','DistanceToFirstStop_p60', 'DistanceToFirstStop_p80']\ncat_names=['EntryStreetName','ExitStreetName', 'EntryHeading', 'ExitHeading', 'City','Path']\ncolumns=train.columns;\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cont_names=[];\ncont_names=[x for x in columns if x not in cat_names];\nfor x in dep_var:cont_names.remove(x);\nlen(cont_names)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### DataBunch"},{"metadata":{"trusted":true},"cell_type":"code","source":"procs = [FillMissing, Categorify, Normalize];\ntest_tab = TabularList.from_df(test, path=path, cat_names=cat_names, cont_names=cont_names, procs=procs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = (TabularList.from_df(train, path=path, cat_names=cat_names, cont_names=cont_names, procs=procs)\n        \n        .split_by_rand_pct(valid_pct=0.2,seed=40)\n        \n        .label_from_df(cols=dep_var,label_cls=FloatList)\n        \n        .add_test(test_tab)\n\n        .databunch()\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.show_batch()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Learner"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = tabular_learner(data, layers=[20,10], metrics=rmse)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"learn.lr_find()\nlearn.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(5, 1e-2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"target={\n0:'TotalTimeStopped_p20',\n1:'TotalTimeStopped_p50',\n2:'TotalTimeStopped_p80',\n3:'DistanceToFirstStop_p20',\n4:'DistanceToFirstStop_p50',\n5:'DistanceToFirstStop_p80',\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indx=[]\nfor key in target.keys():\n  xx=dep_var.index(target[key])\n  indx.append(xx)\nprint(indx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds,_=learn.get_preds(ds_type=DatasetType.Test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_total=[]\n\nfor pred in range(len(preds)):\n    preds_array=[x.item() for x in preds[pred]]\n    for id in indx:prediction_total.append(preds_array[id])\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(prediction_total)==(len(indx)*len(test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read submission file\nsubmission=pd.read_csv(path/'sample_submission.csv');submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['Target']=prediction_total\nsubmission.to_csv(\"DL_test_1.csv\", index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}