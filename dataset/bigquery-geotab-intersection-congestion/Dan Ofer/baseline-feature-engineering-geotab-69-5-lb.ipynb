{"cells":[{"metadata":{},"cell_type":"markdown","source":"* Forked from : https://www.kaggle.com/pulkitmehtawork1985/beating-benchmark\n* Copies feature code over from my other kernel; https://www.kaggle.com/danofer/basic-features-geotab-intersections\n\n* V6 - try  a multitask model in addition to a model per target. Likely to have worse performance, but will be faster"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn import preprocessing\n\nfrom sklearn.linear_model import LinearRegression, LassoLarsCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n# from xgboost import XGBRegressor\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Load Data\n\ntrain = pd.read_csv(\"../input/bigquery-geotab-intersection-congestion/train.csv\").sample(frac=0.15,random_state=42)#,nrows=123456)\ntest = pd.read_csv(\"../input/bigquery-geotab-intersection-congestion/test.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Cleaning\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train[\"City\"].unique())\nprint(test[\"City\"].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test.groupby([\"City\"]).apply(np.unique)\ntest.groupby([\"City\"]).nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isna().sum(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isna().sum(axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Add features\n\n##### turn direction: \nThe cardinal directions can be expressed using the equation: $$ \\frac{\\theta}{\\pi} $$\n\nWhere $\\theta$ is the angle between the direction we want to encode and the north compass direction, measured clockwise.\n\n* This is an **important** feature, as shown by janlauge here : https://www.kaggle.com/janlauge/intersection-congestion-eda\n\n* We can fill in this code in python (e.g. based on: https://www.analytics-link.com/single-post/2018/08/21/Calculating-the-compass-direction-between-two-points-in-Python , https://rosettacode.org/wiki/Angle_difference_between_two_bearings#Python , https://gist.github.com/RobertSudwarts/acf8df23a16afdb5837f ) \n\n* TODO: circularize / use angles"},{"metadata":{"trusted":true},"cell_type":"code","source":"directions = {\n    'N': 0,\n    'NE': 1/4,\n    'E': 1/2,\n    'SE': 3/4,\n    'S': 1,\n    'SW': 5/4,\n    'W': 3/2,\n    'NW': 7/4\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['EntryHeading'] = train['EntryHeading'].map(directions)\ntrain['ExitHeading'] = train['ExitHeading'].map(directions)\n\ntest['EntryHeading'] = test['EntryHeading'].map(directions)\ntest['ExitHeading'] = test['ExitHeading'].map(directions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['diffHeading'] = train['EntryHeading']-train['ExitHeading']  # TODO - check if this is right. For now, it's a silly approximation without the angles being taken into consideration\n\ntest['diffHeading'] = test['EntryHeading']-test['ExitHeading']  # TODO - check if this is right. For now, it's a silly approximation without the angles being taken into consideration\n\ntrain[['ExitHeading','EntryHeading','diffHeading']].drop_duplicates().head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### code if we wanted the diffs, without changing the raw variables:\n\n# train['diffHeading'] = train['ExitHeading'].map(directions) - train['EntryHeading'].map(directions)\n# test['diffHeading'] = test['ExitHeading'].map(directions) - test['EntryHeading'].map(directions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* entering and exiting on same street\n* todo: clean text, check if on same boulevard, etc' "},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"same_street_exact\"] = (train[\"EntryStreetName\"] ==  train[\"ExitStreetName\"]).astype(int)\ntest[\"same_street_exact\"] = (test[\"EntryStreetName\"] ==  test[\"ExitStreetName\"]).astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Skip OHE intersections for now - memory issues\n* Intersection IDs aren't unique  etween cities - so we'll make new ones\n\n* Running fit on just train reveals that **the test data has a \"novel\" city + intersection!** ( '3Atlanta'!) (We will fix this)\n     * Means we need to be careful when OHEing the data\n     \n * There are 2,796 intersections, more if we count unique by city (~4K) = many, many columns. gave me memory issues when doing one hot encoding\n     * Could try count or target mean encoding. \n     \n* For now - ordinal encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"le = preprocessing.LabelEncoder()\n# le = preprocessing.OneHotEncoder(handle_unknown=\"ignore\") # will have all zeros for novel categoricals, [can't do drop first due to nans issue , otherwise we'd  drop first value to avoid colinearity","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"Intersection\"] = train[\"IntersectionId\"].astype(str) + train[\"City\"]\ntest[\"Intersection\"] = test[\"IntersectionId\"].astype(str) + test[\"City\"]\n\nprint(train[\"Intersection\"].sample(6).values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pd.concat([train,le.transform(train[\"Intersection\"].values.reshape(-1,1)).toarray()],axis=1).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### with ordinal encoder - ideally we'd encode all the \"new\" cols with a single missing value, but it doesn't really matter given that they're Out of Distribution anyway (no such values in train). \n* So we'll fit on train+Test in order to avoid encoding errors - when using the ordinal encoder! (LEss of a n issue with OHE)"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.concat([train[\"Intersection\"],test[\"Intersection\"]],axis=0).drop_duplicates().values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le.fit(pd.concat([train[\"Intersection\"],test[\"Intersection\"]]).drop_duplicates().values)\ntrain[\"Intersection\"] = le.transform(train[\"Intersection\"])\ntest[\"Intersection\"] = le.transform(test[\"Intersection\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ORIG  OneHotEncode\n##### We could Create one hot encoding for entry , exit direction fields - but may make more sense to leave them as continous\n\n\n* Intersection ID is only unique within a city"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.get_dummies(train[\"City\"],dummy_na=False, drop_first=False).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pd.get_dummies(train[[\"EntryHeading\",\"ExitHeading\",\"City\"]].head(),prefix = {\"EntryHeading\":'en',\"ExitHeading\":\"ex\",\"City\":\"city\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.concat([train,pd.get_dummies(train[\"City\"],dummy_na=False, drop_first=False)],axis=1).drop([\"City\"],axis=1)\ntest = pd.concat([test,pd.get_dummies(test[\"City\"],dummy_na=False, drop_first=False)],axis=1).drop([\"City\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape,test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" #### Approach: We will make 6 predictions based on features we derived - IntersectionId , Hour , Weekend , Month , entry & exit directions .\n * Target variables will be TotalTimeStopped_p20 ,TotalTimeStopped_p50,TotalTimeStopped_p80,DistanceToFirstStop_p20,DistanceToFirstStop_p50,DistanceToFirstStop_p80 .\n \n * I leave in the original IntersectionId just in case there's meaning accidentally encoded in the numbers"},{"metadata":{"trusted":true},"cell_type":"code","source":"FEAT_COLS = [\"IntersectionId\",\n             'Intersection',\n           'diffHeading',  'same_street_exact',\n           \"Hour\",\"Weekend\",\"Month\",\n          'Latitude', 'Longitude',\n          'EntryHeading', 'ExitHeading',\n            'Atlanta', 'Boston', 'Chicago',\n       'Philadelphia']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train[FEAT_COLS]\ny1 = train[\"TotalTimeStopped_p20\"]\ny2 = train[\"TotalTimeStopped_p50\"]\ny3 = train[\"TotalTimeStopped_p80\"]\ny4 = train[\"DistanceToFirstStop_p20\"]\ny5 = train[\"DistanceToFirstStop_p50\"]\ny6 = train[\"DistanceToFirstStop_p80\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train[['TotalTimeStopped_p20', 'TotalTimeStopped_p50', 'TotalTimeStopped_p80',\n        'DistanceToFirstStop_p20', 'DistanceToFirstStop_p50', 'DistanceToFirstStop_p80']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testX = test[FEAT_COLS]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## kaggle kernel performance can be very unstable when trying to use miltuiprocessing\n# lr = LinearRegression()\nlr = RandomForestRegressor(n_estimators=100,min_samples_split=3)#,n_jobs=3) #different default hyperparams, not necessarily any better","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Original: model + prediction per target\n#############\n\nlr.fit(X,y1)\npred1 = lr.predict(testX)\nlr.fit(X,y2)\npred2 = lr.predict(testX)\nlr.fit(X,y3)\npred3 = lr.predict(testX)\nlr.fit(X,y4)\npred4 = lr.predict(testX)\nlr.fit(X,y5)\npred5 = lr.predict(testX)\nlr.fit(X,y6)\npred6 = lr.predict(testX)\n\n\n# Appending all predictions\nall_preds = []\nfor i in range(len(pred1)):\n    for j in [pred1,pred2,pred3,pred4,pred5,pred6]:\n        all_preds.append(j[i])   \n\nsub  = pd.read_csv(\"../input/bigquery-geotab-intersection-congestion/sample_submission.csv\")\nsub[\"Target\"] = all_preds\nsub.to_csv(\"benchmark_beat_rfr_multimodels.csv\",index = False)\n\nprint(len(all_preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ALT : multitask model"},{"metadata":{"trusted":true},"cell_type":"code","source":"## New/Alt: multitask -  model for all targets\n\nlr.fit(X,y)\nprint(\"fitted\")\n\nall_preds = lr.predict(testX)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## convert list of lists to format required for submissions\nprint(all_preds[0])\n\ns = pd.Series(list(all_preds) )\nall_preds = pd.Series.explode(s)\n\nprint(len(all_preds))\nprint(all_preds[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub  = pd.read_csv(\"../input/bigquery-geotab-intersection-congestion/sample_submission.csv\")\nprint(sub.shape)\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub[\"Target\"] = all_preds.values\nsub.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv(\"benchmark_beat_rfr_multitask.csv\",index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Export featurized data\n\n* Uncomment this to get the features exported for further use. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(\"Path\",axis=1).to_csv(\"train_danFeatsV1.csv.gz\",index = False,compression=\"gzip\")\ntest.drop(\"Path\",axis=1).to_csv(\"test_danFeatsV1.csv.gz\",index = False,compression=\"gzip\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}