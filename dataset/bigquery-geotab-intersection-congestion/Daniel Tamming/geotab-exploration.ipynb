{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport statsmodels.api as sm\nfrom tqdm import tqdm\nimport time\nfrom sklearn.preprocessing import LabelEncoder\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Data and Visualize Target Variable Distributions"},{"metadata":{"trusted":true},"cell_type":"code","source":"other_columns = ['TimeFromFirstStop_p20', 'TimeFromFirstStop_p40', \n                 'TimeFromFirstStop_p50', 'TimeFromFirstStop_p60', \n                 'TimeFromFirstStop_p80', 'TotalTimeStopped_p40', \n                 'TotalTimeStopped_p60', 'DistanceToFirstStop_p40', \n                 'DistanceToFirstStop_p60']\ntrain = pd.read_csv('../input/bigquery-geotab-intersection-congestion/train.csv').set_index('RowId').drop(columns=other_columns)\ntest = pd.read_csv('../input/bigquery-geotab-intersection-congestion/test.csv').set_index('RowId')\ntrain_idxs = train.index\ntest_idxs = test.index\ndata = pd.concat([train, test], axis=0, join='outer')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_vars = ['TotalTimeStopped_p20', 'TotalTimeStopped_p50', \n               'TotalTimeStopped_p80', 'DistanceToFirstStop_p20', \n               'DistanceToFirstStop_p50', 'DistanceToFirstStop_p80']\ncities = train.City.unique()\nfig, ax = plt.subplots(nrows=6, ncols=4, figsize=(20,30))\nbins = list(range(0, 200, 10))\nfor i, var in enumerate(target_vars):\n    for j, city in enumerate(cities):\n        sns.distplot(train[train.City == city][var], bins=bins, kde=False, ax=ax[i, j]).set_title(city)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All six distributions are significantly zero-inflated and have significant right skew. The DistanceToFirstStop percentiles have a [tweedie](https://en.wikipedia.org/wiki/Tweedie_distribution) distribution. Also note that except for TotalTimeStopped_p80, the majority of entries have values in the minimum bin, near zero. The distributions do not vary significantly across cities.\n\nTo generate predictions we could use an advanced regression model of the type that fits a tweedie distribution, but given the presence of high cardinality categorical variables we'll opt for a gradient boosted trees model. As we'll see, the fact that we are using a tree-based models will influence several of our feature engineering/encoding decisions. \n\nThroughout the EDA we may use the logarithm of TotalTimeStopped_p80 as a proxy for overall congestion. This will inform our feature engineering and selection decisions. For example, we use the logarithm of TotalTimeStopped_p50 can decide whether to treat a categorical variable as ordinal or nominal. We could instead perform PCA on the six target variable and have the first principal component serve this proxy function. However, the two variables by definition have a strong relationship, as do each variable's three percentiles, so TotalTimeStopped_p80 will likely be a reasonable proxy for the group. "},{"metadata":{},"cell_type":"markdown","source":"# Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isna().sum().sort_values() * 100 / len(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Only two of our non-target variables have missing values. Both are missing less than 1% of their entries. "},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering\n### EntryStreetName and ExitStreetName"},{"metadata":{},"cell_type":"markdown","source":"We could impute the missing values of EntryStreetName and ExitStreetName with the mode of each respective variable, conditioned on its ExitHeading, EntryHeading, IntersectionId, and City. Unfortunately, analysis not shown here revealed that the vast majority of these missing values do not share the four named variables with any other entry, so we'll have to take another approach. \n\nThe type of roadway (street, road, parkway, etc.) may be predictive of traffic density, so we'll extract roadway type from both of these variables. After doing so we will impute missing values with the 'Other' category. Given that less than 1% of each variable is missing, this is not a significant loss of precision. "},{"metadata":{"trusted":true},"cell_type":"code","source":"roadways = ['Street', 'Road', 'Boulevard', 'Avenue', 'Lane', \n            'Drive', 'Parkway', 'Place', 'Way', 'Circle', \n            'Highway', 'Pkwy', 'St', 'Connector', 'Broadway', \n            'Overpass', 'Ave', 'Square', 'Tunner', 'Rd', \n            'Bld', 'Bridge', 'Expressway', 'Pike']\nto_longform = {'Rd': 'Road', 'Bld': 'Boulevard', \n               'Ave': 'Avenue', 'St': 'Street', \n               'Pkwy': 'Parkway'}\n\nstreet_names = pd.concat([data['EntryStreetName'], data['ExitStreetName']], ignore_index=True).dropna()\n\nseen = set()\nfor street in street_names:\n    if all([roadway not in street for roadway in roadways]):\n        if street not in seen:\n            print(street)\n            seen.add(street)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After a quick inspection of the undetected street name values we can conclude that there is not a significant roadway that we have not added to our list of roadway types.\n\n**Implementation Note**: for loops are generally frowned upon when working with Pandas objects. However, the alternative in this case would be to use the *.apply()* method, which often has worse performance than a for loop. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_roadway(StreetName):\n    if pd.isnull(StreetName):\n        return 'Other'\n    for roadway in roadways:\n        if roadway in StreetName:\n            if roadway in to_longform:\n                return to_longform[roadway]\n            else:\n                return roadway\n    return 'Other'\n\nboth_roadway = street_names.apply(to_roadway)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.countplot(both_roadway, order=both_roadway.value_counts().index);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The countplot shows two sharp discontinuities: between Avenue and Road, and between Parkway and Highway. We'll group all categories that have counts less than or equal to Highway into a single 'Other' category. We choose to cut off at this point since we'd like to maintain a distinction between a parkway and a drive, for example, since the difference in congestion between the two may be sharp. "},{"metadata":{"trusted":true},"cell_type":"code","source":"roadways = ['Street', 'Avenue', 'Road', \n            'Boulevard', 'Drive', 'Parkway']\nto_longform = {'St': 'Street', 'Ave': 'Avenue', \n               'Rd': 'Road', 'Bld': 'Boulevard', \n               'Pkwy': 'Parkway'}\ndef to_roadway(StreetName):\n    if pd.isnull(StreetName):\n        return 'Other'\n    for roadway in roadways:\n        if roadway in StreetName:\n            if roadway in to_longform:\n                return to_longform[roadway]\n            else:\n                return roadway\n    return 'Other'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The second *to_roadway* function groups the uncommon categories into a single 'Other' category. "},{"metadata":{"trusted":true},"cell_type":"code","source":"data['EntryRoadway'] = data['EntryStreetName'].apply(to_roadway)\ndata['ExitRoadway'] = data['ExitStreetName'].apply(to_roadway)\ndata.drop(columns=['EntryStreetName', 'ExitStreetName'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(ncols=2, figsize=(20, 6))\nsns.countplot(data.EntryRoadway, order=data.EntryRoadway.value_counts().index, ax=ax[0]);\nsns.countplot(data.ExitRoadway, order=data.ExitRoadway.value_counts().index, ax=ax[1]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we can apply the function to convert entry and exit streetnames to their respective roadway type and have a look at the counts of the resulting features. "},{"metadata":{},"cell_type":"markdown","source":"### EntryHeading and ExitHeading\nWe'll convert both of these values to degrees, which allows us to easily engineer a 'change in heading' feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"degrees_map = dict(zip('E NE N NW W SW S SE'.split(), [i*45 for i in range(8)]))\nentry_heading_degrees = data.EntryHeading.map(degrees_map)\nexit_heading_degrees = data.ExitHeading.map(degrees_map)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(ncols=2, figsize=(22,6))\ndata['DeltaHeading'] = (exit_heading_degrees - entry_heading_degrees + 180) % 360 - 180\nsns.countplot(x='DeltaHeading', data=data, order=sorted(data.DeltaHeading.unique()), ax=ax[0]);\ndata['TotalTimeStopped_p80_log'] = np.log1p(data['TotalTimeStopped_p80'])\nsns.boxplot(x='DeltaHeading', y='TotalTimeStopped_p80_log', data=data, ax=ax[1]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that 90 corresponds to a left turn, and -90 corresponds to a right turn. We see that a sharp left turn (135) and a sharp right turn (-135) do not occur often and that they seem to have roughly the same distribution of TotalTimeStopped_p80_log, so we'll map sharp turns to normal turns. Soft turns (-45 and 45) occur frequently and their distribution over TotalTimeStopped_p80_log is distinct, so we'll leave them as is. \"U-Turns\" (-180) do not occur often and have a similar distribution over TotalTimeStopped_p80_log as left turns. This makes sense since in countries that drive on the right side of the road, a \"U-turn\" is equivalent to a very sharp left turn. \n\nIn general, if EntryHeading and DeltaHeading are known then ExitHeading is redundant. However, we'll be using gradient bossted trees which are not harmed by inputs with strong dependencies, so we'll leave all features in the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"pooled_heading_map = {-180: 'L', -135:'R', -90:'R', -45:'R_soft', \n                      0:'S', 45:'L_soft', 90:'L', 135:'L'}\ndata['DeltaHeading'] = data['DeltaHeading'].map(pooled_heading_map)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Month"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(ncols=2, figsize=(22,6))\nsns.countplot(x='Month', data=data, ax=ax[0]);\nsns.violinplot(x='Month', y='TotalTimeStopped_p80_log', data=data, ax=ax[1]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's difficult to see a pattern between months and TotalTimeStopped_p80_log, so we'll use them as a categorical, rather than ordinal, variable. This is somewhat surprising since one might expect to see more congestion in the winter months."},{"metadata":{},"cell_type":"markdown","source":"### Hour"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(ncols=2, figsize=(22,6))\nsns.countplot(x='Hour', data=data, ax=ax[0]).set(title='Number of Datapoints At Each Hour');\nsns.boxplot(x='Hour', y='TotalTimeStopped_p80_log', data=data, ax=ax[1]).set(title='Bar Plot Of Target vs Hour');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['dist_to_5pm'] = abs(data.Hour - 17)\ndata['dist_to_8am'] = abs(data.Hour - 8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We expect to see spikes in congestion around 8am and 5pm, so we create a variable representing distance to each of these times. Hour will be correlated with these two, but that correlation is complicated by the nature of the absolute value function. We'll leave hour itself as a categorical value. "},{"metadata":{},"cell_type":"markdown","source":"### Latitude and Longitude"},{"metadata":{"trusted":true},"cell_type":"code","source":"centers_data = [['Atlanta', 33.7490, -84.3880], \n                ['Boston', 42.3601, -71.0589], \n                ['Chicago', 41.8781, -87.6298], \n                ['Philadelphia', 39.9509, -75.1575]]\ncenters = pd.DataFrame(centers_data, columns=['City', 'Latitude', 'Longitude']).set_index('City')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(ncols=2, nrows=4, figsize=(20, 20))\nfor row, city in enumerate(data.City.unique()):\n    city_data = data[(data.City == city) & (data.TotalTimeStopped_p80_log > 0)]\n    ax[row,0].axvline(centers.loc[city, 'Longitude'], 0, 600)\n    sns.scatterplot(x='Longitude', y='TotalTimeStopped_p80', data=city_data, ax=ax[row, 0]).set_title(city);\n    ax[row,1].axvline(centers.loc[city, 'Latitude'], 0, 600)\n    sns.scatterplot(x='Latitude', y='TotalTimeStopped_p80', data=city_data, ax=ax[row, 1]).set_title(city);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The target's distribution tends to find its peak near the city center, as indicated by the vertical lines. We'll use distance from the city center in either directions as features. "},{"metadata":{"trusted":true},"cell_type":"code","source":"data['latitude_dist'] = data[['City', 'Latitude']].apply(lambda x : abs(x['Latitude'] - centers.loc[x['City'], 'Latitude']), axis=1)\ndata['longitude_dist'] = data[['City', 'Longitude']].apply(lambda x : abs(x['Longitude'] - centers.loc[x['City'], 'Longitude']), axis=1)\ndata.drop(columns=['Latitude', 'Longitude'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Path\nPath is similar to street, and we don't expect it to be predictive of congestion given the Roadway variables we created. "},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(columns=['Path'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### City and IntersectionId\nBoth can be expected to be strongly predictive of congestion. Differing cities have shared intersection ids, so we'll combine these two variables to maintain the uniqueness of the intersections across cities. The gradient boosting library we'll use in modeling allows categorical variables, but dummy encoding important variables can boost performance. We'll dummy encode City because it is an important variable and because its low cardinality means that doing so does not require significant memory. "},{"metadata":{"trusted":true},"cell_type":"code","source":"data['city_intersection'] = data.City + data.IntersectionId.astype(str)\ndata['city_intersection'] = LabelEncoder().fit_transform(data['city_intersection'])\ndata = pd.get_dummies(data, columns=['City'], prefix=['is'], drop_first=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note**: when using linear models one should always avoid multicollinearity by using k-1 dummies for a feature with cardinality k. When using tree-based models, it is best to use k dummies for such a variable."},{"metadata":{},"cell_type":"markdown","source":"# Save"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(columns=['TotalTimeStopped_p80_log'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[train_idxs].to_csv('train_processed.csv')\ndata.loc[test_idxs].to_csv('test_processed.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}