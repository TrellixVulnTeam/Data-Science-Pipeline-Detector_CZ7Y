{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction \nThis dataset contains information regarding traffic congestion in major US Interstates and attempts to create a model that accurately predicts future congestion. The development of this notebook will take on the following structure:\n\n* Exploratory Analysis: This stage will explore the data, rename labels as appropriate and discover that kind of pre-processing must be made (whether there are empty datapoints, distribution of data, entropy of each feature).\n* Preprocessing: This stage will pre-process the data to put it in a way the model can make an accurate prediction.\n* Algorithm selection and implementation\n* Commentary "},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# Load in packages \nimport numpy as np \nimport pandas as pd \nimport scipy as sp\nfrom scipy import stats\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nimport folium\nfrom folium.plugins import HeatMap\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Analysis"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Read data\ndf_train = pd.read_csv('/kaggle/input/bigquery-geotab-intersection-congestion/train.csv')\ndf_test = pd.read_csv('/kaggle/input/test-1/test (1).csv') #use received file from email on 1 May 2020\ndf_subm = pd.read_csv('/kaggle/input/bigquery-geotab-intersection-congestion/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First thing to check is the data type of each column:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preview and analyse train data\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The test data is different from the training data; it does not contain columns for 'TimeFromFirstStop_p80' and 'DistanceToFirstStop' which are the variables that need to be predicted in this study."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preview and analyse test data\ndf_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Review the number of (row,column) for train and test data respectively\n# Result shows that train data has more additional columns/information for training the dataset\n# Checking for distribution of ALL DATA for each city\ntrain_plot = sns.countplot(x=\"City\", data=df_train)\nprint(df_train.shape)\nprint(df_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can clearly see that Philadelphia has the highest data count of all cities. The data is therefore unevenly distributed and this could affect our models. However, this is total count for all cities and that doesn't yield too much more information."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preview and analyse sample submission data\ndf_subm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comparing the df_test and df_subm, we could tell there is a discrepancy in the RowId. The submission data is only tested from ID 0 to 1920334 whereas test data is from ID 0 to 1921356. Hence, the test data needs to be adjusted so that it could be submitted in the kaggle. However, as we are using the previous test file received from previous kaggle participant, this issue is resolved as both df_test and df_subm have matching RowID."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adjusting test data to match sample submission data\n# As we are using the previous test file received from previous kaggle participant, this issue is resolved as both df_test and df_subm have matching RowID.\n# Hence, this step can be skipped\n\n# df_test=df_test[df_test['RowId']<1920335]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check how many missing values in TRAIN data\n# Only 'EntryStreetName' and 'ExitStreetName' columns contain missing values \ndf_train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check how many missing values in TEST data\n# Only 'EntryStreetName' and 'ExitStreetName' columns contain missing values \ndf_test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Investigate the missing values (NaN) in train data for 'EntryStreetName'\n# There are total 8148 total rows as per above finding\ndf_train[df_train['EntryStreetName'].isnull()==True]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Investigate the missing values (NaN) in train data for 'ExitStreetName'\n# There are total 6287 total rows as per above finding\ndf_train[df_train['ExitStreetName'].isnull()==True]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above, the NaN values only happen in 'EntryStreetName' and 'ExitStreetName' columns.\nThese values don't need to be amended or dropped as these 2 columns won't be used for machine learning process; The columns 'EntryHeading' and 'ExitHeading' will be used instead as they are all correlated.\n\nWith the same understanding, 'Latitude' and 'Longitude' won't be used for the machine learning process too, as they are correlated to the column 'Intersection Id' and 'City'"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Group by the city and preview how many unique data in each column.\n# Result shows that Philadephia has the most data, but Chicago has the most intersectionId.\ndf_train.groupby([\"City\"]).nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's see the distribution of traffic by month and date\nplt.figure(figsize=(15,12))\n\nplt.subplot(211)\ng = sns.countplot(x=\"Hour\", data=df_train, hue='City', dodge=True)\ng.set_title(\"Distribution by hour and city\", fontsize=20)\ng.set_ylabel(\"Count\",fontsize= 17)\ng.set_xlabel(\"Hours of Day\", fontsize=17)\nsizes=[]\nfor p in g.patches:\n    height = p.get_height()\n    sizes.append(height)\n\ng.set_ylim(0, max(sizes) * 1.15)\n\nplt.subplot(212)\ng1 = sns.countplot(x=\"Month\", data=df_train, hue='City', dodge=True)\ng1.set_title(\"Hour Count Distribution by Month and City\", fontsize=20)\ng1.set_ylabel(\"Count\",fontsize= 17)\ng1.set_xlabel(\"Months\", fontsize=17)\nsizes=[]\nfor p in g1.patches:\n    height = p.get_height()\n    sizes.append(height)\n\ng1.set_ylim(0, max(sizes) * 1.15)\n\nplt.subplots_adjust(hspace = 0.3)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, philly comes out on top when it comes to count of traffic data. However this still doesn't give enough support to the theory that Philly has more traffic simply because it has more data. We could check this assumption by seeing how much actual stopping there is in philly traffic vs other city's traffic."},{"metadata":{},"cell_type":"markdown","source":"## Map Analysis\n\nLet's plot out the intersections for each city using the latitude and longitude in the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 6 variables to be predicted\ny1 = \"TotalTimeStopped_p20\"\ny2 = \"TotalTimeStopped_p50\"\ny3 = \"TotalTimeStopped_p80\"\ny4 = \"DistanceToFirstStop_p20\"\ny5 = \"DistanceToFirstStop_p50\"\ny6 = \"DistanceToFirstStop_p80\"\n\n# Group data by City\ndf_train_A = df_train[df_train['City']=='Atlanta']\ndf_train_B = df_train[df_train['City']=='Boston']\ndf_train_C = df_train[df_train['City']=='Chicago']\ndf_train_P = df_train[df_train['City']=='Philadelphia']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Atlanta waiting times at the intersection"},{"metadata":{"trusted":true},"cell_type":"code","source":"# use plotly to plot where intersections are for all cities. Provide observational data. \n# then do a heatmap which groups intersectionId with TotalStoppingTime across space to see where the heaviest traffic is. \n# Investigate what's around here and provide observations.\ntraffic_df=df_train_A.groupby(['Latitude','Longitude'])['IntersectionId'].count().to_frame()\ntraffic_df.columns.values[0]='count1'\ntraffic_df=traffic_df.reset_index()\nlats=traffic_df[['Latitude','Longitude','count1']].values.tolist()\n    \nhmap = folium.Map(location=[33.7638493,-84.3801108], zoom_start=12)\nhmap.add_child(HeatMap(lats, radius = 6))\nhmap","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Boston waiting times at the intersection"},{"metadata":{"trusted":true},"cell_type":"code","source":"# use plotly to plot where intersections are for all cities. Provide observational data. \n# then do a heatmap which groups intersectionId with TotalStoppingTime across space to see where the heaviest traffic is. \n# Investigate what's around here and provide observations.\ntraffic_df=df_train_B.groupby(['Latitude','Longitude'])['IntersectionId'].count().to_frame()\ntraffic_df.columns.values[0]='count1'\ntraffic_df=traffic_df.reset_index()\nlats=traffic_df[['Latitude','Longitude','count1']].values.tolist()\n    \nhmap = folium.Map(location=[42.3158246,-71.0787574], zoom_start=12)\nhmap.add_child(HeatMap(lats, radius = 6))\nhmap","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Chicago waiting times at the intersection"},{"metadata":{"trusted":true},"cell_type":"code","source":"# use plotly to plot where intersections are for all cities. Provide observational data. \n# then do a heatmap which groups intersectionId with TotalStoppingTime across space to see where the heaviest traffic is. \n# Investigate what's around here and provide observations.\ntraffic_df=df_train_C.groupby(['Latitude','Longitude'])['IntersectionId'].count().to_frame()\ntraffic_df.columns.values[0]='count1'\ntraffic_df=traffic_df.reset_index()\nlats=traffic_df[['Latitude','Longitude','count1']].values.tolist()\n    \nhmap = folium.Map(location=[41.8420892,-87.7237629], zoom_start=11)\nhmap.add_child(HeatMap(lats, radius = 6))\nhmap","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Philadelphia waiting times at the intersection"},{"metadata":{"trusted":true},"cell_type":"code","source":"# use plotly to plot where intersections are for all cities. Provide observational data. \n# then do a heatmap which groups intersectionId with TotalStoppingTime across space to see where the heaviest traffic is. \n# Investigate what's around here and provide observations.\ntraffic_df=df_train_P.groupby(['Latitude','Longitude'])['IntersectionId'].count().to_frame()\ntraffic_df.columns.values[0]='count1'\ntraffic_df=traffic_df.reset_index()\nlats=traffic_df[['Latitude','Longitude','count1']].values.tolist()\n    \nhmap = folium.Map(location=[39.9484792,-75.1774329], zoom_start=12)\nhmap.add_child(HeatMap(lats, radius = 6))\nhmap","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The maps show a clear correlation between congestion and distance to city center.\n\nThe objective of the project is to predict congestion, based on an aggregate measure of stopping distance (p20,p50,p80) and waiting times (p20,p50,p80), at intersections in 4 major US cities: Atlanta, Boston, Chicago & Philadelphia.\nFor premiliminary study, the simplest way to predict is to get the mean values of each data group by its city."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_A[['IntersectionId',y1,y2,y3,y4,y5,y6]].groupby('IntersectionId').mean().head(6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_B[['IntersectionId',y1,y2,y3,y4,y5,y6]].groupby('IntersectionId').mean().head(6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_C[['IntersectionId',y1,y2,y3,y4,y5,y6]].groupby('IntersectionId').mean().head(6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_P[['IntersectionId',y1,y2,y3,y4,y5,y6]].groupby('IntersectionId').mean().head(6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above is just a preliminary study to get sense of the data.\nNext, these other factors need to be accounted in training the dataset:\n- Intersection ID\n- Direction: entry or exit with 8 directions: E, N, NE, NW, S, SE, SW, W\n- Hour\n- Weekend\n- Month\n\nThe strategy is to use linear regression model to predict the congestion. From the above, it is observed that 'Intersection Id' values can be the same between all cities despite of different location. For example, 'Intersectiod Id'=2 exist at both Atlanta and Chicago although they are actually at different location. Hence, to prevent this to confuse the model prediction, the regression models need to be separated between the 4 cities and would be combined at last for submission."},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoding the train data with the 8 different directions using .get_dummies to create new columns\n# Using array for different cities\n\nar_train = [df_train_A,df_train_B,df_train_C,df_train_P]\nar_entry = [1,1,1,1]\nar_exit = [1,1,1,1]\n\nfor i in range (0,4):\n    ar_entry[i] = pd.get_dummies(ar_train[i][\"EntryHeading\"],prefix = 'n')\n    ar_exit[i] = pd.get_dummies(ar_train[i][\"ExitHeading\"],prefix = 'x')\n    ar_train[i] = pd.concat([ar_train[i],ar_entry[i]],axis=1)\n    ar_train[i] = pd.concat([ar_train[i],ar_exit[i]],axis=1)\n\nar_train[0].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoding the test data same as the above\n\ndf_test_A = df_test[df_test['City']=='Atlanta']\ndf_test_B = df_test[df_test['City']=='Boston']\ndf_test_C = df_test[df_test['City']=='Chicago']\ndf_test_P = df_test[df_test['City']=='Philadelphia']\n\nar_test = [df_test_A,df_test_B,df_test_C,df_test_P]\n\nfor i in range (0,4):\n    ar_entry[i] = pd.get_dummies(ar_test[i][\"EntryHeading\"],prefix = 'n')\n    ar_exit[i] = pd.get_dummies(ar_test[i][\"ExitHeading\"],prefix = 'x')\n    ar_test[i] = pd.concat([ar_test[i],ar_entry[i]],axis=1)\n    ar_test[i] = pd.concat([ar_test[i],ar_exit[i]],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating the x_train,x_test,y_train_y_test variables for each cities\n\nx_train = [1,1,1,1] \nx_test = [1,1,1,1]\ny1_train = [1,1,1,1]\ny1_test =[1,1,1,1]\ny2_train = [1,1,1,1]\ny2_test =[1,1,1,1]\ny3_train = [1,1,1,1]\ny3_test =[1,1,1,1]\ny4_train = [1,1,1,1]\ny4_test =[1,1,1,1]\ny5_train = [1,1,1,1]\ny5_test =[1,1,1,1]\ny6_train = [1,1,1,1]\ny6_test =[1,1,1,1]\n\ncolumns = [\"IntersectionId\",\"Hour\",\"Weekend\",\"Month\",'n_E','n_N', 'n_NE', 'n_NW', 'n_S', 'n_SE', 'n_SW', 'n_W', 'x_E','x_N', 'x_NE', 'x_NW', 'x_S', 'x_SE', 'x_SW', 'x_W']\nfor i in range (0,4):\n    x_train[i] = ar_train[i][columns]\n    x_test[i] = ar_test[i][columns]\n    y1_train[i] = ar_train[i][y1]\n    y2_train[i] = ar_train[i][y2]\n    y3_train[i] = ar_train[i][y3]\n    y4_train[i] = ar_train[i][y4]\n    y5_train[i] = ar_train[i][y5]\n    y6_train[i] = ar_train[i][y6]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check correlation matrix before proceeding in the learning process\n# all correlation values are below 0.55, hence it is good to proceed\ncorr = x_train[0].corr()\ncorr.style.background_gradient(cmap='coolwarm').set_precision(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Algorithm selection and implementation"},{"metadata":{},"cell_type":"markdown","source":"Various regression methods, namely the LinearRegression, RidgeRegression and LassoRegression with different alpha values, have been used in this study. The highest kaggle score value is when linear_model.Lasso(alpha=0.15) is used. Hence,  this model is used in the project."},{"metadata":{"trusted":true},"cell_type":"code","source":"# import regression package\nfrom sklearn import datasets, linear_model\nregression = linear_model.Lasso(alpha=0.15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# execute the learning process\nfor i in range (0,4):\n    regression.fit(x_train[i],y1_train[i])\n    y1_test[i] = regression.predict(x_test[i])\n\n    regression.fit(x_train[i],y2_train[i])\n    y2_test[i] = regression.predict(x_test[i])\n    \n    regression.fit(x_train[i],y3_train[i])\n    y3_test[i] = regression.predict(x_test[i])\n    \n    regression.fit(x_train[i],y4_train[i])\n    y4_test[i] = regression.predict(x_test[i])\n    \n    regression.fit(x_train[i],y5_train[i])\n    y5_test[i] = regression.predict(x_test[i])\n    \n    regression.fit(x_train[i],y6_train[i])\n    y6_test[i] = regression.predict(x_test[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To confirm if the total numbers of y_test are the same as total rows of sample submission file before combining the y_test \n\n6*(len(y1_test[0])+len(y1_test[1])+len(y1_test[2])+len(y1_test[3])) == len(df_subm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Combining all y_test\n\ny_test = []\nfor i in range (0,4):\n    for j in range(len(y1_test[i])):\n        for k in [y1_test[i],y2_test[i],y3_test[i],y4_test[i],y5_test[i],y6_test[i]]:\n            y_test.append(k[j])\n            \nlen(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preview result\ny_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Combine result with submission file and save to csv\ndf_subm[\"Target\"] = y_test\ndf_subm.to_csv(\"CZ4041.csv\",index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Side Study on Cross-Validation (CV) of the Training Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using KFold to splits the data into 5-fold\nfrom sklearn.model_selection import KFold\nX = np.array(x_train[0]) \ny = y1_train[0] \nkf = KFold(n_splits=5) # Define the split - into 5 folds \nkf.get_n_splits(X) \nprint(kf) \n\n# Enumerate splits\nfor train_index, test_index in kf.split(X):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, some metrics will be reviewed, namely the R2, mean absolute error, mean squared error, and cross-validated score. The cross-validated score (CVS) is done based on 5-fold splitting. Results are grouped based on City and the 6 parameters (y1 to y6)."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n\nCity=['Atlanta','Boston','Chicago','Philadelphia']\ny = [y1,y2,y3,y4,y5,y6]\ny_train=[y1_train,y2_train,y3_train,y4_train,y5_train,y6_train]\nCVS = []\n\nfor j in range (0,6):\n    print(\"\\n----- Score for\",y[j],\"-----\")\n    CVS.append([])\n    for i in range (0,4):\n\n        X_train, X_test, Y_train, Y_test = train_test_split(x_train[i], y_train[j][i], \n                                                        random_state=100, \n                                                        test_size= 0.15)\n        regression = linear_model.LinearRegression()\n        regression.fit(X_train, Y_train)\n\n        fitted_values = regression.predict(X_test)\n\n        r2 = r2_score(Y_test, fitted_values)\n        mae = mean_absolute_error(Y_test, fitted_values)  \n        rmse = mean_squared_error(Y_test, fitted_values)**0.5\n        cvs = cross_val_score(regression, X_train, Y_train, cv=5)\n        CVS[j].append(np.average(cvs))\n        \n        print(City[i])\n        print(\"  r2\", r2)\n        print(\"  MAE\", mae)\n        print(\"  MSE\", rmse)\n        print(\"  Cross-Validated Score (5 fold):\", cvs)\n        print(\"  Average CVS:\", np.average(cvs))\n        \n        #Scroll output below to preview all","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('CVS of y1 to y6 for each City:')\nfor i in range (0,4):\n    print(\" -\", City[i], np.transpose(CVS)[i])\n\nprint('\\nAverage CVS of y1 to y6 for each City:')\nfor i in range (0,4):\n    print(\" -\", City[i], np.average(np.transpose(CVS)[i]))\n    \nprint('\\nAverage CVS of all cities combined')    \nprint(\" -\", np.average(CVS))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The cross-validated score (5-fold) of the training datasets is 1.5%. Atlanta, Boston and Philadephia have similar CVS ranging 1.5-2.0% whereas Chicago is falling behind with 0.7%. This may be due to Chicago has the least amount of datasets as shown in the Exploratory Analysis graph. The R2 value is quite small, which may be due to many learning inputs are involved in the regression model, namely the IntersectionId, Hour, Weekend, Month, Entry and Exit Directions. Meanwhile the MAE and MSE values are also relatively small, which indicate good learning model."},{"metadata":{},"cell_type":"markdown","source":"### Commentary:\nThe dataset leans heavily towards Philadelphia, which can skew the result of our model. From the hour and month plots we can draw some observations:\n* There is less data in the early hours of the morning for all cities, increasing throughout the day\n* Philadelphia data count peaks between 3pm-7pm\n* Boston peaks at 10am and gradually falls\n* Chicago closely follow Boston's trend throughout the day\n* Atlanta stays constant throughout the day starting around 7-8am\n\nWhen it comes to months of the year:\n* There is significantly less data in the spring\n* Data count for Boston increases towards the end of the year\n* Philadelphia has the highest data count of any other city\n\nLatitude and Longitude can help pinpoint us where exactly the vehicle was going at the time the data was captured. 'EntryStreetName' and 'ExitStreetName' provide directional data, which can tell us on *what* direction traffic is flowing.  Perhaps traffic is heavier on certain streets only in one direction rather than the other (think commuters).\n\n'Month', 'Weekend' and 'Day' are really interesting because they provides us a chance to do analysis across time. Seasons change, which can bring more congestion (think of how much traffic slows during snowstorms as opposed to a sunny day). School breaks influence traffic, so do important holidays and recurring cultural events.\n\nEventually this whole study gets score of **Private Score 77.600 and Public Score 79.675** in kaggle. All datasets provided have been used to predict the congestion. "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}