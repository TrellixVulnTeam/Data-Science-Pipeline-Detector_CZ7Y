{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport scipy as sp\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom plotly.offline import iplot, init_notebook_mode\n\nimport plotly.figure_factory as ff\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LinearRegression\n\nfrom functools import partial\nfrom hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING\n\nimport os\nfor dirname,_,filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname,filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/bigquery-geotab-intersection-congestion/train.csv')\ndf_test = pd.read_csv('/kaggle/input/bigquery-geotab-intersection-congestion/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def resumetable(df):\n    print(f\"Dataset Shape:{df.shape}\")\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes']) #データ型の確認\n    summary = summary.reset_index()#indexのリセット\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing']=df.isnull().sum().values\n    summary['Uniques']=df.nunique().values\n    summary['First Value']=df.loc[0].values#0行目をとりだす\n    summary['Second Value']=df.loc[1].values\n    summary['Third Value']=df.loc[2].values\n    \n    #これなに\n    for name in summary['Name'].value_counts().index:\n        print(summary['Name'],df[name])\n        summary.loc[summary['Name'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True),base=2),2)\n        \n    return summary\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics=['int16','int32','int64','float16','float32','float64']\n    start_mem=df.memory_usage().sum()/1024**2\n    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3]=='int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"resumetable(df_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total=len(df_train)\nplt.figure(figsize=(15,19))\n#uique数が少ないデータをcountplot??\nplt.subplot(311)\ng=sns.countplot(x='City',data=df_train)\ng.set_title('City Count Distribution', fontsize=20)\ng.set_ylabel('Count',fontsize=17)\ng.set_xlabel('City Names',fontsize=17)\nsizes=[]\n\nfor p in g.patches:\n    height = p.get_height()\n    sizes.append(height)\n    g.text(p.get_x()+p.get_width()/2.,height+3,'{:1.2f}%'.format(height/total*100),ha='center',fontsize=14)\n    \ng.set_ylim(0, max(sizes)*1.15)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp_hour = df_train.groupby(['City','Hour'])['RowId'].nunique().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,12))\n\nplt.subplot(211)\ng = sns.countplot(x=\"Hour\", data=df_train, hue='City', dodge=True)\ng.set_title(\"Hour Count Distribution by Week and Weekend Days\", fontsize=20)\ng.set_ylabel(\"Count\",fontsize= 17)\ng.set_xlabel(\"Hours of Day\", fontsize=17)\nsizes=[]\nfor p in g.patches:\n    height = p.get_height()\n    sizes.append(height)\n\ng.set_ylim(0, max(sizes) * 1.15)\n\nplt.subplot(212)\ng1 = sns.countplot(x=\"Month\", data=df_train, hue='City', dodge=True)\ng1.set_title(\"Hour Count Distribution by Week and Weekend Days\", fontsize=20)\ng1.set_ylabel(\"Count\",fontsize= 17)\ng1.set_xlabel(\"Hours of Day\", fontsize=17)\nsizes=[]\nfor p in g1.patches:\n    height = p.get_height()\n    sizes.append(height)\n\ng1.set_ylim(0, max(sizes) * 1.15)\n\nplt.subplots_adjust(hspace = 0.3)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.groupby(['City','Hour'])['RowId'].nunique().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,12))\n\ntmp=round(((df_train.groupby(['EntryHeading'])['RowId'].nunique()/total)*100)).reset_index()\ntmp.head()\n\nplt.subplot(211)\ng=sns.countplot(x='EntryHeading',data=df_train,order=list(tmp['EntryHeading'].values),hue='ExitHeading',dodge=True)\nlist(tmp['EntryHeading'].values)\ng.set_title(\"Entry Heading by Exit Heading\", fontsize=20)\ng.set_ylabel(\"Count\",fontsize= 17)\ng.set_xlabel(\"Entry Heading Region\", fontsize=17)\ngt=g.twinx()\ngt=sns.pointplot(x='EntryHeading',y='RowId',data=tmp,order=list(tmp['EntryHeading'].values),color='black',legend=False)\ngt.set_ylim(0,tmp['RowId'].max()*1.1)\ngt.set_ylabel('% of total(black line)',fontsize=16)\nsizes=[]\nfor p in g.patches:\n    height = p.get_height()\n    sizes.append(height)\n\ng.set_ylim(0, max(sizes) * 1.15)\n\nplt.subplot(212)\ng1 = sns.countplot(x=\"EntryHeading\", order=list(tmp['EntryHeading'].values), \n                   data=df_train, hue='City')\ng1.set_title(\"Entry Heading Distribution By Cities\", fontsize=20)\ng1.set_ylabel(\"Count\",fontsize= 17)\ng1.set_xlabel(\"Entry Heading Region\", fontsize=17)\nsizes=[]\nfor p in g1.patches:\n    height = p.get_height()\n    sizes.append(height)\n\ng1.set_ylim(0, max(sizes) * 1.15)\n\nplt.subplots_adjust(hspace = 0.3)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,6))\ndf_train.IntersectionId.value_counts()[:45].plot(kind='bar')\nplt.xlabel(\"Intersection Number\", fontsize=18)\nplt.ylabel(\"Count\", fontsize=18)\nplt.title(\"TOP 45 most commmon IntersectionID's \", fontsize=22)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.groupby(['IntersectionId','EntryHeading','ExitHeading'])['RowId'].count().reset_index().head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t_stopped=['TotalTimeStopped_p20','TotalTimeStopped_p50','TotalTimeStopped_p80']\nt_first_stopped=['TimeFromFirstStop_p20','TimeFromFirstStop_p80','TimeFromFirstStop_p80']\nd_first_stopped=['DistanceToFirstStop_p20','DistanceToFirstStop_p50','DistanceToFirstStop_p80']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,12))\nplt.title('Correlation of Features for Train Set',fontsize=22)\nsns.heatmap(df_train[t_stopped+ \n                     #t_first_stopped + \n                     d_first_stopped].astype(float).corr(),vmax=1.0,annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import minmax_scale\ntarget_cols=t_stopped+d_first_stopped","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in target_cols:\n    df_train[col+str('_minmax')]=(minmax_scale(df_train[col],feature_range=(0,1)))\nmin_max_cols=['TotalTimeStopped_p20_minmax','TotalTimeStopped_p50_minmax','TotalTimeStopped_p80_minmax','DistanceToFirstStop_p20_minmax','DistanceToFirstStop_p50_minmax','DistanceToFirstStop_p80_minmax']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca=PCA(n_components=3,random_state=5)\n#print(df_train[min_max_cols])\nprincipalComponents=pca.fit_transform(df_train[min_max_cols])\n\nprincipalDf=pd.DataFrame(principalComponents)\nprefix='Target_PCA'\nprincipalDf.rename(columns=lambda x:str(prefix)+str(x),inplace=True)\n\ndf_train=pd.concat([df_train,principalDf],axis=1)\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.explained_variance_ratio_[:2].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g=sns.FacetGrid(df_train.sample(50000),col='City',col_wrap=2,height=5,aspect=1.5,hue='Weekend')\n\ng.map(sns.scatterplot,'Target_PCA0','Target_PCA1',alpha=.5).add_legend();\ng.set_titles('{col_name}',fontsize=17)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ssd=[]\n\nK=range(1,10)\nfor k in K:\n    km=KMeans(n_clusters=k, random_state=4)\n    km=km.fit(df_train[min_max_cols])\n    ssd.append(km.inertia_)\n\nplt.plot(K,ssd,'bx-')\nplt.xlabel('k')\nplt.ylabel('Sum pf squared distances')\nplt.title('Elbow Method For Optimal k')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"km=KMeans(n_clusters=4,random_state=4)\nkm=km.fit(df_train[min_max_cols])\ndf_train['clusters_T']=km.predict(df_train[min_max_cols])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp=pd.crosstab(df_train['City'],df_train['clusters_T'],normalize='columns').unstack('City').reset_index().rename(columns={0:'prec'})\n\ntotal=len(df_train)\nplt.figure(figsize=(15,16))\n\nplt.subplot(311)\ng=sns.countplot(x='clusters_T',data=df_train)\ng.set_title('Cluster Target Count Distribution', fontsize=20)\ng.set_ylabel('Count',fontsize=17)\ng.set_xlabel('Target Cluster Distributions',fontsize=17)\nsizes=[]\nfor p in g.patches:\n    height = p.get_height()\n    sizes.append(height)\n    g.text(p.get_x()+p.get_width()/2.,height+3,'{:1.2f}%'.format(height/total*100),ha='center',fontsize=14)\n    \ng.set_ylim(0,max(sizes)*1.15)\n\nplt.subplot(312)\ng1=sns.countplot(x='clusters_T',data=df_train,hue='City')\ng1.set_title('CITES-Cluster Target Distribution',fontsize=17)\nsizes=[]\nfor p in g1.patches:\n    height=p.get_height()\n    sizes.append(height)\n    g1.text(p.get_x()+p.get_width()/2.,\n           height+3,\n           '{:1.2f}%'.format(height/total*100),ha='center',fontsize=10)\ng1.set_ylim(0,max(sizes)*1.15)\n\nplt.subplot(313)\ng1=sns.boxplot(x='clusters_T',y='Target_PCA0',data=df_train,hue='City')\ng1.set_title('PCA Feature - Distribution of PCA by Clusters and Cities',fontsize=20)\ng1.set_ylabel('PCA 0 Values',fontsize=17)\ng1.set_xlabel('Target Cluster Distiributions', fontsize=17)\n\nplt.subplots_adjust(hspace=0.5)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,6))\n\nsns.scatterplot(x='Target_PCA0',y='Target_PCA1',hue='clusters_T',data=df_train,palette='Set1')\n\nplt.title('PCA 0 and PCA 1 by Clusters',fontsize=22)\nplt.ylabel('Target PCA 1 values',fontsize=18)\nplt.xlabel('Target PCA 0 Values', fontsize=18)\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g=sns.FacetGrid(df_train.sample(500000),col='City',col_wrap=2,height=4,aspect=1.5,hue='clusters_T')\ng.map(sns.scatterplot, 'Target_PCA0','Target_PCA1',alpha=.5).add_legend();\ng.set_titles('{col_name}',fontsize=50)\n\nplt.suptitle('CITES \\nPrincipal Component Analysis Dispersion by Cluster',fontsize=22)\nplt.subplots_adjust(hspace=0.3,top=.85)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g=sns.FacetGrid(df_train.sample(500000),col='City',col_wrap=2,height=4,aspect=1.5,hue='clusters_T')\n\ng.map(sns.scatterplot,'Hour','Target_PCA0',alpha=.5).add_legend();\ng.set_titles('{col_name}',fontsize=50)\n\nplt.suptitle('CITES \\nPrincipal Component Analysis Dispersion by HOURS AND CLUSTERS', fontsize=22)\n\nplt.subplots_adjust(hspace=0.3,top=.85)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"round(pd.crosstab([df_train['clusters_T'],df_train['Weekend']],df_train['City'],normalize='index')*100.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train=pd.read_csv('/kaggle/input/bigquery-geotab-intersection-congestion/train.csv')\ndf_test=pd.read_csv('/kaggle/input/bigquery-geotab-intersection-congestion/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def date_cyc_enc(df, col, max_vals):\n    df[col+'_sin']=np.sin(2*np.pi*df[col]/max_vals)\n    df[col+'_cos']=np.cos(2*np.pi*df[col]/max_vals)\n    return df\n\ndf_train=date_cyc_enc(df_train,'Hour',24)\ndf_test=date_cyc_enc(df_test,'Hour',24)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['is_day'] = df_train['Hour'].apply(lambda x: 1 if 7 < x < 18 else 0)\ndf_test['is_day'] = df_test['Hour'].apply(lambda x: 1 if 7 < x < 18 else 0)\n\ndf_train['is_morning'] = df_train['Hour'].apply(lambda x: 1 if 6 < x < 10 else 0)\ndf_test['is_morning'] = df_test['Hour'].apply(lambda x: 1 if 6 < x < 10 else 0)\n\ndf_train['is_night'] = df_train['Hour'].apply(lambda x: 1 if 17 < x < 20 else 0)\ndf_test['is_night'] = df_test['Hour'].apply(lambda x: 1 if 17 < x < 20 else 0)\n\ndf_train['is_day_weekend'] = np.where((df_train['is_day'] == 1) & (df_train['Weekend'] == 1), 1,0)\ndf_test['is_day_weekend'] = np.where((df_test['is_day'] == 1) & (df_train['Weekend'] == 1), 1,0)\n\ndf_train['is_mor_weekend'] = np.where((df_train['is_morning'] == 1) & (df_train['Weekend'] == 1), 1,0)\ndf_test['is_mor_weekend'] = np.where((df_test['is_morning'] == 1) & (df_train['Weekend'] == 1), 1,0)\n\ndf_train['is_nig_weekend'] = np.where((df_train['is_night'] == 1) & (df_train['Weekend'] == 1), 1,0)\ndf_test['is_nig_weekend'] = np.where((df_test['is_night'] == 1) & (df_train['Weekend'] == 1), 1,0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[\"Intersec\"]=df_train[\"IntersectionId\"].astype(str)+df_train[\"City\"]\ndf_test[\"Intersec\"]=df_test[\"IntersectionId\"].astype(str)+df_test[\"City\"]\n\nprint(df_train[\"Intersec\"].sample(6).values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le=LabelEncoder()\n\nle.fit(pd.concat([df_train[\"Intersec\"],df_test[\"Intersec\"]]).drop_duplicates().values)\ndf_train[\"Intersec\"]=le.transform(df_train[\"Intersec\"])\ndf_test[\"Intersec\"]=le.transform(df_test[\"Intersec\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"road_encoding={\n    'Road':1,\n    'Street':2,\n    'Avenue':2,\n    'Drive':3,\n    'Broad':3,\n    'Boulevard':4\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode(x):\n    if pd.isna(x):\n        return 0\n    for road in road_encoding.keys():\n        if road in x:\n            return road_encoding[road]\n        \n    return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['EntryType']=df_train['EntryStreetName'].apply(encode)\ndf_train['ExitType']=df_train['ExitStreetName'].apply(encode)\ndf_test['EntryType']=df_test['EntryStreetName'].apply(encode)\ndf_test['ExitType']=df_test['ExitStreetName'].apply(encode)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"directions = {\n    'N': 0,\n    'NE': 1/4,\n    'E': 1/2,\n    'SE': 3/4,\n    'S': 1,\n    'SW': 5/4,\n    'W': 3/2,\n    'NW': 7/4\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['EntryHeading'] = df_train['EntryHeading'].map(directions)\ndf_train['ExitHeading'] = df_train['ExitHeading'].map(directions)\n\ndf_test['EntryHeading'] = df_test['EntryHeading'].map(directions)\ndf_test['ExitHeading'] = df_test['ExitHeading'].map(directions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['diffHeading'] = df_train['EntryHeading']-df_train['ExitHeading']  \ndf_test['diffHeading'] = df_test['EntryHeading']-df_test['ExitHeading'] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[\"same_str\"] = (df_train[\"EntryStreetName\"] ==  df_train[\"ExitStreetName\"]).astype(int)\ndf_test[\"same_str\"] = (df_test[\"EntryStreetName\"] ==  df_test[\"ExitStreetName\"]).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Concatenating the city and month into one variable\ndf_train['city_month'] = df_train[\"City\"] + df_train[\"Month\"].astype(str)\ndf_test['city_month'] = df_test[\"City\"] + df_test[\"Month\"].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"monthly_rainfall = {'Atlanta1': 5.02, 'Atlanta5': 3.95, 'Atlanta6': 3.63, 'Atlanta7': 5.12, \n                    'Atlanta8': 3.67, 'Atlanta9': 4.09,'Atlanta10': 3.11, 'Atlanta11': 4.10, \n                    'Atlanta12': 3.82, 'Boston1': 3.92, 'Boston5': 3.24, 'Boston6': 3.22,\n                    'Boston7': 3.06, 'Boston8': 3.37, 'Boston9': 3.47, 'Boston10': 3.79,\n                    'Boston11': 3.98, 'Boston12': 3.73, 'Chicago1': 1.75, 'Chicago5': 3.38,\n                    'Chicago6': 3.63, 'Chicago7': 3.51, 'Chicago8': 4.62, 'Chicago9': 3.27,\n                    'Chicago10': 2.71,  'Chicago11': 3.01, 'Chicago12': 2.43, \n                    'Philadelphia1': 3.52, 'Philadelphia5': 3.88, 'Philadelphia6': 3.29,\n                    'Philadelphia7': 4.39, 'Philadelphia8': 3.82, 'Philadelphia9':3.88 ,\n                    'Philadelphia10': 2.75, 'Philadelphia11': 3.16, 'Philadelphia12': 3.31}\n\n# Creating a new column by mapping the city_month variable to it's corresponding average monthly rainfall\n#df_train['city_month'].head(3)\ndf_train[\"average_rainfall\"] = df_train['city_month'].map(monthly_rainfall)\ndf_test[\"average_rainfall\"] = df_test['city_month'].map(monthly_rainfall)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Shape before dummy transformation: {df_train.shape}')\ndf_train=pd.get_dummies(df_train,columns=['City'],prefix=['City'],drop_first=False)\nprint(f'Shape after dummy transformation:{df_train.shape}')\ndf_test=pd.get_dummies(df_test,columns=['City'],prefix=['City'],drop_first=False)\n#pd.get_dummiesはダミー変数の用意\n#print(df_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\nfor col in ['Latitude','Longitude']:\n    scaler.fit(df_train[col].values.reshape(-1,1))\n    df_train[col]=scaler.transform(df_train[col].values.reshape(-1,1))\n    df_test[col]=scaler.transform(df_test[col].values.reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.drop(['RowId','Path','EntryStreetName','ExitStreetName'],axis=1,inplace=True)\ndf_test.drop(['RowId','Path','EntryStreetName','ExitStreetName'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interesting_feat = ['IntersectionId', 'Latitude', 'Longitude', 'EntryHeading',\n                    'ExitHeading', 'Hour', 'Weekend', 'Month',\n                    'is_morning', 'is_night', 'is_day_weekend', 'is_mor_weekend',\n                    'is_nig_weekend', #  'Hour_sin',\n                    'Hour', 'same_str', 'Intersec', 'EntryType',\n                    'ExitType', 'diffHeading', 'average_rainfall', 'is_day',\n                    'City_Boston', 'City_Chicago', 'City_Philadelphia', \n                    'City_Atlanta']\n\ntotal_time = ['TotalTimeStopped_p20',\n              'TotalTimeStopped_p50', \n              'TotalTimeStopped_p80']\n\ntarget_stopped = ['DistanceToFirstStop_p20',\n                  'DistanceToFirstStop_p50',\n                  'DistanceToFirstStop_p80']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=df_train[interesting_feat]\ny=df_train[total_time+target_stopped]\nX_test=df_test[interesting_feat]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Shape of X:{X.shape}')\nprint(f'Shape of X_test: {X_test.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=reduce_mem_usage(X)\nX_test=reduce_mem_usage(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_val,y_train,y_val=train_test_split(X,y,test_size=0.10,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hyper_space = {'objective': 'regression',\n               'metric':'rmse',\n               'boosting':'gbdt', 'gpu_device_id': 0,\n               #'n_estimators': hp.choice('n_estimators', [25, 40, 50, 75, 100, 250, 500]),\n               'max_depth':  hp.choice('max_depth', list(range(6, 18, 2))),\n               'num_leaves': hp.choice('num_leaves', list(range(20, 180, 20))),\n               'subsample': hp.choice('subsample', [.7, .8, .9, 1]),\n               'colsample_bytree': hp.uniform('colsample_bytree', 0.7, 1),\n               'learning_rate': hp.uniform('learning_rate', 0.03, 0.12),\n               #'reg_alpha': hp.choice('reg_alpha', [.1, .2, .3, .4, .5, .6]),\n               #'reg_lambda':  hp.choice('reg_lambda', [.1, .2, .3, .4, .5, .6]),               \n               'min_child_samples': hp.choice('min_child_samples', [20, 45, 70, 100])}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_feat = ['IntersectionId','Hour', 'Weekend','Month', \n            'is_day', 'is_morning', 'is_night', \n            'same_str', 'Intersec', 'City_Atlanta', 'City_Boston',\n            'City_Chicago', 'City_Philadelphia', 'EntryType', 'ExitType']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nimport lightgbm as lgb\n\ndef evaluate_metric(params):\n    \n    all_preds_test ={0:[],1:[],2:[],3:[],4:[],5:[]}\n    \n    print(f'Params: {params}')\n    FOLDS = 4    \n    \n    count=1\n    \n    for i in range(len(all_preds_test)):\n        \n        score_mean = 0\n        \n        kf = KFold(n_splits=FOLDS, shuffle=False, \n                   random_state=42)\n        \n       \n        for tr_idx, val_idx in kf.split(X, y):\n            \n            X_tr, X_vl = X.iloc[tr_idx, :], X.iloc[val_idx, :]\n            y_tr, y_vl = y.iloc[tr_idx], y.iloc[val_idx]\n\n            lgtrain = lgb.Dataset(X_tr, label=y_tr.iloc[:,i])\n            lgval = lgb.Dataset(X_vl, label=y_vl.iloc[:,i])\n\n            lgbm_reg = lgb.train(params, lgtrain, 2000, valid_sets = [lgval],\n                                 categorical_feature=cat_feat,\n                                 verbose_eval=0, \n                                 early_stopping_rounds = 300)\n                        \n        pred_lgb = lgbm_reg.predict(X_val, num_iteration=lgbm_reg.best_iteration)\n        all_preds_test[i] = pred_lgb\n        score_uni = np.sqrt(mean_squared_error(pred_lgb, y_val.iloc[:,i]))\n        print(f'Score Validation : {score_uni}')\n\n\n    pred = pd.DataFrame(all_preds_test).stack()\n    pred = pd.DataFrame(pred)\n    \n    y_val_sc = pd.DataFrame(y_val).stack()\n    y_val_sc = pd.DataFrame(y_val_sc)    \n    \n    count = count +1\n    \n    score = np.sqrt(mean_squared_error(pred[0].values, y_val_sc[0].values ))\n    #score = metric(df_val, pred)\n    \n    print(f'Full Score Run: {score}')\n \n    return {\n        'loss': score,\n        'status': STATUS_OK\n    }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Seting the number of Evals\nMAX_EVALS= 15\n\n# Fit Tree Parzen Estimator\nbest_vals = fmin(evaluate_metric, \n                 space=hyper_space,\n                 verbose=-1,\n                 algo=tpe.suggest, \n                 max_evals=MAX_EVALS)\n\n# Print best parameters\nbest_params = space_eval(hyper_space, best_vals)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_preds ={0:[],1:[],2:[],3:[],4:[],5:[]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nimport lightgbm as lgb\n\nfor i in range(len(all_preds)):\n    print(f'## {i+1} Run')\n    X_tr,X_val,y_tr,y_val=train_test_split(X, y.iloc[:,i],\n                                           test_size=0.10, random_state=31)\n\n    xg_train = lgb.Dataset(X_tr, label = y_tr)\n    xg_valid = lgb.Dataset(X_val, label = y_val )\n    \n    lgbm_reg = lgb.train(best_params, xg_train, 10000,\n                      valid_sets = [xg_valid],\n                      verbose_eval=500, \n                      early_stopping_rounds = 250)\n    \n    all_preds[i] = lgbm_reg.predict(X_test, num_iteration=lgbm_reg.best_iteration)\n    \n    print(f\"{i+1} running done.\" )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub  = pd.read_csv(\"../input/bigquery-geotab-intersection-congestion/sample_submission.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt = pd.DataFrame(all_preds).stack()\ndt = pd.DataFrame(dt)\nsub['Target'] = dt[0].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv(\"lgbm_pred_hyperopt_test.csv\", index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}