{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-08T16:02:42.863659Z","iopub.execute_input":"2021-06-08T16:02:42.86405Z","iopub.status.idle":"2021-06-08T16:02:42.86933Z","shell.execute_reply.started":"2021-06-08T16:02:42.864008Z","shell.execute_reply":"2021-06-08T16:02:42.867957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**What I've done in this notebook**\n\n* Applying various dimension reduction techniques and visualize the reduced data\n    - PCA\n    - kernel PCA\n    - t-SNE\n    - Isomap\n    - LLE\n* Train a random forest to evaluate performances after PCA","metadata":{}},{"cell_type":"markdown","source":"# Import the data and preprocessing","metadata":{}},{"cell_type":"code","source":"train_path = \"../input/tabular-playground-series-jun-2021/train.csv\"\ntrain = pd.read_csv(train_path)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-08T16:02:42.924846Z","iopub.execute_input":"2021-06-08T16:02:42.92534Z","iopub.status.idle":"2021-06-08T16:02:43.982244Z","shell.execute_reply.started":"2021-06-08T16:02:42.925308Z","shell.execute_reply":"2021-06-08T16:02:43.981073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.drop(\"id\", axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T16:02:43.984374Z","iopub.execute_input":"2021-06-08T16:02:43.984828Z","iopub.status.idle":"2021-06-08T16:02:44.03284Z","shell.execute_reply.started":"2021-06-08T16:02:43.984779Z","shell.execute_reply":"2021-06-08T16:02:44.031536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ny = le.fit_transform(train[\"target\"])\nX = train.drop(\"target\", axis=1).values","metadata":{"execution":{"iopub.status.busy":"2021-06-08T16:02:44.035079Z","iopub.execute_input":"2021-06-08T16:02:44.035572Z","iopub.status.idle":"2021-06-08T16:02:44.160425Z","shell.execute_reply.started":"2021-06-08T16:02:44.035524Z","shell.execute_reply":"2021-06-08T16:02:44.159479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"split data set for evaluating performance","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, \n                                                      test_size=0.2, \n                                                      stratify=y,\n                                                      random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T16:02:44.16198Z","iopub.execute_input":"2021-06-08T16:02:44.16246Z","iopub.status.idle":"2021-06-08T16:02:44.503025Z","shell.execute_reply.started":"2021-06-08T16:02:44.162414Z","shell.execute_reply":"2021-06-08T16:02:44.5019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape, y_train.shape, X_valid.shape, y_valid.shape ","metadata":{"execution":{"iopub.status.busy":"2021-06-08T16:02:44.504708Z","iopub.execute_input":"2021-06-08T16:02:44.505133Z","iopub.status.idle":"2021-06-08T16:02:44.511816Z","shell.execute_reply.started":"2021-06-08T16:02:44.505088Z","shell.execute_reply":"2021-06-08T16:02:44.510841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.iloc[:, :-1] = train.iloc[:, :-1].astype(\"int16\")","metadata":{"execution":{"iopub.status.busy":"2021-06-08T16:02:44.513234Z","iopub.execute_input":"2021-06-08T16:02:44.513789Z","iopub.status.idle":"2021-06-08T16:02:46.002091Z","shell.execute_reply.started":"2021-06-08T16:02:44.513745Z","shell.execute_reply":"2021-06-08T16:02:46.001192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**outline of this note**\n\n","metadata":{}},{"cell_type":"markdown","source":"# Visualizing the data with dimensionality reduction techniques","metadata":{}},{"cell_type":"markdown","source":"In this section, I will use several dimensionality reduction techniques to transform our data set into 2D space and visualize the data points to see if these techniques can give us some insights about the data.","metadata":{}},{"cell_type":"markdown","source":"Here's a helper for visualizing the data:","metadata":{}},{"cell_type":"code","source":"def draw_plot_2d(decompose=None, \n                 subset=None,\n                 X_train=X_train, \n                 y_train=y_train):\n    \n    if subset is not None:\n        X_train = X_train[subset, :]\n        y_train = y_train[subset]\n    \n    if decompose is None:\n        decompose_2d = X_train\n    else:\n        decompose_2d = decompose.fit_transform(X_train)\n\n    plt.figure(figsize=(15, 8), dpi=100)\n    sns.scatterplot(x=decompose_2d[:, 0], \n                    y=decompose_2d[:, 1],\n                    hue=[le.classes_[i] for i in y_train]);","metadata":{"execution":{"iopub.status.busy":"2021-06-08T15:48:19.774763Z","iopub.execute_input":"2021-06-08T15:48:19.775181Z","iopub.status.idle":"2021-06-08T15:48:19.781883Z","shell.execute_reply.started":"2021-06-08T15:48:19.77514Z","shell.execute_reply":"2021-06-08T15:48:19.781002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Principal component analysis","metadata":{}},{"cell_type":"code","source":"pca = PCA(n_components=2, random_state=42)\ndraw_plot_2d(pca)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T15:48:19.783578Z","iopub.execute_input":"2021-06-08T15:48:19.783863Z","iopub.status.idle":"2021-06-08T15:48:27.346865Z","shell.execute_reply.started":"2021-06-08T15:48:19.783836Z","shell.execute_reply":"2021-06-08T15:48:27.345723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_ids = np.random.choice(X_train.shape[0], 10000)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T15:48:27.348261Z","iopub.execute_input":"2021-06-08T15:48:27.348565Z","iopub.status.idle":"2021-06-08T15:48:27.352267Z","shell.execute_reply.started":"2021-06-08T15:48:27.348536Z","shell.execute_reply":"2021-06-08T15:48:27.351586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I will use a subset of data (sample of rows) in the following four techniques because they either use tons of memory (kernel PCA) or are time-consuming (the others).","metadata":{}},{"cell_type":"markdown","source":"## Kernel PCA","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import KernelPCA\n\nkernel = KernelPCA(n_components=2, kernel=\"rbf\", n_jobs=-1, copy_X=False)\ndraw_plot_2d(decompose=kernel, subset=sample_ids)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T15:48:35.65423Z","iopub.execute_input":"2021-06-08T15:48:35.654753Z","iopub.status.idle":"2021-06-08T15:48:41.498051Z","shell.execute_reply.started":"2021-06-08T15:48:35.6547Z","shell.execute_reply":"2021-06-08T15:48:41.497087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## t-sne","metadata":{}},{"cell_type":"code","source":"from sklearn.manifold import TSNE\n\ntsne = TSNE()\ndraw_plot_2d(decompose=tsne, subset=sample_ids)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T15:48:47.769756Z","iopub.execute_input":"2021-06-08T15:48:47.770131Z","iopub.status.idle":"2021-06-08T15:49:57.294784Z","shell.execute_reply.started":"2021-06-08T15:48:47.770096Z","shell.execute_reply":"2021-06-08T15:49:57.293656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Isomap","metadata":{}},{"cell_type":"code","source":"from sklearn.manifold import Isomap\n\niso = Isomap()\ndraw_plot_2d(decompose=iso, subset=sample_ids)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T15:49:57.296355Z","iopub.execute_input":"2021-06-08T15:49:57.296687Z","iopub.status.idle":"2021-06-08T15:51:08.860427Z","shell.execute_reply.started":"2021-06-08T15:49:57.29665Z","shell.execute_reply":"2021-06-08T15:51:08.859305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Locally linear embeding (LLE)","metadata":{}},{"cell_type":"code","source":"from sklearn.manifold import LocallyLinearEmbedding\n\nlle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)\ndraw_plot_2d(decompose=lle, subset=sample_ids)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T15:59:09.704496Z","iopub.execute_input":"2021-06-08T15:59:09.704913Z","iopub.status.idle":"2021-06-08T16:01:31.82401Z","shell.execute_reply.started":"2021-06-08T15:59:09.704883Z","shell.execute_reply":"2021-06-08T16:01:31.822948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Unfortunately, since this is a synthesized dataset, these images don't really tell us anything.","metadata":{}},{"cell_type":"markdown","source":"# Can dimension reduction improve or hurt our prediction performance?","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import log_loss\nfrom sklearn.ensemble import RandomForestClassifier\n\ndef train_rf_with_decompose(decompose=None, \n                            subset=None,\n                            X_train=X_train,\n                            y_train=y_train):\n    \n    if subset is not None:\n        X_train = X_train[subset, :]\n        y_train = y_train[subset]\n    \n    if decompose is None:\n        # if no decomposition, we use the original one\n        X_train_transformed = X_train\n        X_valid_transformed = X_valid\n    else:\n        # transform training set and valid set\n        X_train_transformed = decompose.transform(X_train)\n        X_valid_transformed = decompose.transform(X_valid)\n    \n    # train a random forest\n    rf = RandomForestClassifier(n_estimators=100, n_jobs=-1, \n                                random_state=42)\n    rf.fit(X_train_transformed, y_train)\n    pred = rf.predict_proba(X_valid_transformed)\n    \n    return log_loss(y_valid, pred)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T16:05:47.565454Z","iopub.execute_input":"2021-06-08T16:05:47.56583Z","iopub.status.idle":"2021-06-08T16:05:47.573504Z","shell.execute_reply.started":"2021-06-08T16:05:47.565795Z","shell.execute_reply":"2021-06-08T16:05:47.572479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train a random forest with the original dataset:","metadata":{}},{"cell_type":"code","source":"train_rf_with_decompose()","metadata":{"execution":{"iopub.status.busy":"2021-06-08T16:08:36.064831Z","iopub.execute_input":"2021-06-08T16:08:36.065386Z","iopub.status.idle":"2021-06-08T16:08:57.821927Z","shell.execute_reply.started":"2021-06-08T16:08:36.065353Z","shell.execute_reply":"2021-06-08T16:08:57.82087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Use PCA as preliminary reduction","metadata":{}},{"cell_type":"markdown","source":"We usually decide the number of component by finding the \"elbow\" of explained variance.","metadata":{}},{"cell_type":"code","source":"pca_full = PCA(n_components=75).fit(X_train)\n\nplt.plot(pca_full.explained_variance_ratio_.cumsum())\nplt.hlines(0.95, 0.1, 51, \"black\", \"--\")\nplt.vlines(50, 0.05, 0.95, \"black\", \"--\")\nplt.xlabel(\"number of principal components\")\nplt.ylabel(\"Cumulative explained variance ration\")\nplt.xlim(0.5, 80)\nplt.ylim(0.1, 1);","metadata":{"execution":{"iopub.status.busy":"2021-06-08T16:24:37.673414Z","iopub.execute_input":"2021-06-08T16:24:37.67377Z","iopub.status.idle":"2021-06-08T16:24:37.845019Z","shell.execute_reply.started":"2021-06-08T16:24:37.673733Z","shell.execute_reply":"2021-06-08T16:24:37.843938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_rf_with_decompose(decompose=pca_full)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T16:25:22.197853Z","iopub.execute_input":"2021-06-08T16:25:22.19824Z","iopub.status.idle":"2021-06-08T16:26:46.082564Z","shell.execute_reply.started":"2021-06-08T16:25:22.198209Z","shell.execute_reply":"2021-06-08T16:26:46.081533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wow! We use a 33% smaller dataset and get a slighly better performance. Let's apply PCA to our training and validation set.","metadata":{}},{"cell_type":"code","source":"X_train = pca_full.transform(X_train)\nX_valid = pca_full.transform(X_valid)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T16:28:49.17032Z","iopub.execute_input":"2021-06-08T16:28:49.170689Z","iopub.status.idle":"2021-06-08T16:28:49.381389Z","shell.execute_reply.started":"2021-06-08T16:28:49.170654Z","shell.execute_reply":"2021-06-08T16:28:49.380012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have a smaller data set, we can train a more complex model.","metadata":{}},{"cell_type":"code","source":"rf_final = RandomForestClassifier(n_estimators=500, max_depth=15,\n                                  n_jobs=-1, random_state=42).fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T16:56:44.064119Z","iopub.execute_input":"2021-06-08T16:56:44.064482Z","iopub.status.idle":"2021-06-08T17:02:22.945902Z","shell.execute_reply.started":"2021-06-08T16:56:44.064452Z","shell.execute_reply":"2021-06-08T17:02:22.944305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = rf_reduce.predict_proba(X_valid)\nlog_loss(y_valid, pred)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:02:22.948228Z","iopub.execute_input":"2021-06-08T17:02:22.948544Z","iopub.status.idle":"2021-06-08T17:02:23.792657Z","shell.execute_reply.started":"2021-06-08T17:02:22.94851Z","shell.execute_reply":"2021-06-08T17:02:23.791554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Make prediction on the test set","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv(\"../input/tabular-playground-series-jun-2021/test.csv\")\ntest = test.iloc[:, 1:].values\ntest_preds = rf_final.predict_proba(test)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:02:52.63112Z","iopub.execute_input":"2021-06-08T17:02:52.631621Z","iopub.status.idle":"2021-06-08T17:02:56.595378Z","shell.execute_reply.started":"2021-06-08T17:02:52.631588Z","shell.execute_reply":"2021-06-08T17:02:56.594451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv(\"../input/tabular-playground-series-jun-2021/sample_submission.csv\")\nsub.iloc[:, 1:] = test_preds\nsub.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:03:19.901724Z","iopub.execute_input":"2021-06-08T17:03:19.902094Z","iopub.status.idle":"2021-06-08T17:03:22.487346Z","shell.execute_reply.started":"2021-06-08T17:03:19.902061Z","shell.execute_reply":"2021-06-08T17:03:22.486462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}