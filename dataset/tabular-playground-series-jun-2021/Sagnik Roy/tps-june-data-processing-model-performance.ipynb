{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TPS-JUNE 2021 :\n\n![](https://storage.googleapis.com/kaggle-competitions/kaggle/25225/logos/header.png?t=2021-01-27-17-34-26)\n\n\n## UPVOTE if this helps you :)\n\nThis is a quick starter for TPS-kaggle.\n\nAll the major steps have been used to find the best accuracy using the most fundamental approach.\n\nThis notebook not only holds a better way to approach any other competition, but it observes and manipulates the small and tiny changes that can lead us to better Data Engineering. ","metadata":{"id":"i34C8s7XUIb5"}},{"cell_type":"markdown","source":"#### Data Gathering :","metadata":{"id":"KRW8XWlaVUu3"}},{"cell_type":"code","source":"root = '../input/tabular-playground-series-jun-2021/'\ntrain_path = root + 'train.csv'\ntest_path = root + 'test.csv'\nsubm_path = root + 'sample_submission.csv'","metadata":{"papermill":{"duration":0.057919,"end_time":"2021-05-21T05:58:44.614676","exception":false,"start_time":"2021-05-21T05:58:44.556757","status":"completed"},"tags":[],"id":"ZuXdP0AFQoyL","execution":{"iopub.status.busy":"2021-06-29T04:20:03.920987Z","iopub.execute_input":"2021-06-29T04:20:03.92148Z","iopub.status.idle":"2021-06-29T04:20:03.927065Z","shell.execute_reply.started":"2021-06-29T04:20:03.921438Z","shell.execute_reply":"2021-06-29T04:20:03.925898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Libraries :\n\nImporting the basic data manipulation and visualization libraries.","metadata":{"id":"2M-Qtmw2V-fE"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"id":"HEkmsvoDQoyM","execution":{"iopub.status.busy":"2021-06-29T04:20:03.928682Z","iopub.execute_input":"2021-06-29T04:20:03.929029Z","iopub.status.idle":"2021-06-29T04:20:03.946728Z","shell.execute_reply.started":"2021-06-29T04:20:03.928998Z","shell.execute_reply":"2021-06-29T04:20:03.945888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Data Loading :\nAfter loading the data into the data frame/python backend, we can visualize the deeper patterns or manipulate them on our wish.","metadata":{"id":"SQbhoDFYWMbe"}},{"cell_type":"code","source":"train_df = pd.read_csv(train_path)\n\ntrain_df.head()","metadata":{"id":"WxV2expRQoyN","outputId":"131b3344-a866-469e-fb51-bb8729e3a4f5","execution":{"iopub.status.busy":"2021-06-29T04:20:03.958946Z","iopub.execute_input":"2021-06-29T04:20:03.959307Z","iopub.status.idle":"2021-06-29T04:20:04.877223Z","shell.execute_reply.started":"2021-06-29T04:20:03.959278Z","shell.execute_reply":"2021-06-29T04:20:04.876114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(test_path)\n\ntest_df.head()","metadata":{"id":"pgEuarkoQoyO","outputId":"9cc83a61-5382-4e17-bcf3-108ed0b81629","execution":{"iopub.status.busy":"2021-06-29T04:20:04.878707Z","iopub.execute_input":"2021-06-29T04:20:04.879016Z","iopub.status.idle":"2021-06-29T04:20:05.330661Z","shell.execute_reply.started":"2021-06-29T04:20:04.878987Z","shell.execute_reply":"2021-06-29T04:20:05.329633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"samp_sub = pd.read_csv(subm_path)\n\nsamp_sub.head()","metadata":{"id":"cu5J6eMzQoyP","outputId":"ef51ffa5-9823-4435-bfda-e18b20dd6ee9","execution":{"iopub.status.busy":"2021-06-29T04:20:05.334139Z","iopub.execute_input":"2021-06-29T04:20:05.334441Z","iopub.status.idle":"2021-06-29T04:20:05.465045Z","shell.execute_reply.started":"2021-06-29T04:20:05.334414Z","shell.execute_reply":"2021-06-29T04:20:05.463926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The sample submission format tells us that we need to predict the likelihood of the target classes. These values are between 0.0 to 1.0.\n\n#### Approach :\n\n So, now we have to decide on the approach. \n 1. We can process the data and feed it through a [neural network](https://en.wikipedia.org/wiki/Neural_network) and output as a [softmax layer](https://en.wikipedia.org/wiki/Softmax_function).\n 2. We can use the [predict_proba()](https://discuss.analyticsvidhya.com/t/what-is-the-difference-between-predict-and-predict-proba/67376) to the normal Machine Learning bagging or boosting models and prepare the submission file.","metadata":{"id":"G4Bs0U_nWfdr"}},{"cell_type":"markdown","source":"### Exploratory Data Analysis and Data Processing :\n---\n\nWe'll be trying to visualize the deeper data patterns and find out the anomalies that should be omitted to prepare the best trainable data.\n\nAlso an additional point,\n**BEST TRAINABLE DATA** is data that has no noise and duplicates and outliers.","metadata":{"id":"8wm62SXfYdMg"}},{"cell_type":"markdown","source":"1. #### Target Value Count Distribution: \n---\nAt first, we are going to check the target value mass distribution. Cause too much difference in the can led us to a bad model learning.","metadata":{"id":"5ezZnqfEZDuo"}},{"cell_type":"code","source":"# Target Value Count Distribution:\ntarget_mass = train_df['target'].value_counts()\nvalues = target_mass.values.tolist()\nindexes = target_mass.index.tolist()\n\nax,fig = plt.subplots(1,2,figsize=(15,6))\nplt.subplot(1,2,1)\nplt.pie(values , labels = indexes)\nplt.subplot(1,2,2)\nplt.bar(indexes,values)\nplt.show()","metadata":{"id":"rXB_tIQTQoyP","outputId":"d44240bc-a4f0-4208-aebb-ec45f6352568","execution":{"iopub.status.busy":"2021-06-29T04:20:05.467014Z","iopub.execute_input":"2021-06-29T04:20:05.46744Z","iopub.status.idle":"2021-06-29T04:20:05.805174Z","shell.execute_reply.started":"2021-06-29T04:20:05.467397Z","shell.execute_reply":"2021-06-29T04:20:05.803749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that some target classes are present in a very big number and some are very few.\n\n#### Approach :\nWe can take every target class row in the same count. But choosing that will reduce the data size and as we do not know which data to remove we might even remove the important rows. So, we will skip target class equalization.","metadata":{"id":"z7ThSKqUZQJM"}},{"cell_type":"markdown","source":" 2. #### Correlation :\n---\nNow, we must check the data correlation. In this part, we'll be visualizing features to feature correlation.","metadata":{"id":"qvi3B9K9ay7e"}},{"cell_type":"code","source":"fet_set = train_df.drop(labels=['id','target'],axis=1)\ndef plot_diag_heatmap(data):\n    corr = data.corr()\n    mask = np.triu(np.ones_like(corr, dtype=bool))\n    f, ax = plt.subplots(figsize=(11, 9))\n    sns.heatmap(corr, mask=mask, cmap='YlGnBu', center=0,square=True, linewidths=1, cbar_kws={\"shrink\": 1.0})\nplot_diag_heatmap(fet_set)","metadata":{"id":"ZVZgRyxOQoyQ","outputId":"7a3a3dd0-81be-4631-df15-31e292401343","execution":{"iopub.status.busy":"2021-06-29T04:20:05.806455Z","iopub.execute_input":"2021-06-29T04:20:05.806748Z","iopub.status.idle":"2021-06-29T04:20:10.022165Z","shell.execute_reply.started":"2021-06-29T04:20:05.806718Z","shell.execute_reply":"2021-06-29T04:20:10.021252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can see some features are light in the whole plot. So, we can indicate those as low correlated features.\nSo, we are going to drop those features from both train and test data.","metadata":{"id":"4GvRcPhZc4yG"}},{"cell_type":"code","source":"corr = train_df.iloc[:,1:-1].corr()","metadata":{"id":"R5fBN98kQoyR","execution":{"iopub.status.busy":"2021-06-29T04:20:10.023497Z","iopub.execute_input":"2021-06-29T04:20:10.023822Z","iopub.status.idle":"2021-06-29T04:20:13.204768Z","shell.execute_reply.started":"2021-06-29T04:20:10.023791Z","shell.execute_reply":"2021-06-29T04:20:13.203782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr","metadata":{"id":"kzsXZHONQoyR","outputId":"756edcb5-791a-4fdf-e229-4ed5d5dbc1da","execution":{"iopub.status.busy":"2021-06-29T04:20:13.206046Z","iopub.execute_input":"2021-06-29T04:20:13.20635Z","iopub.status.idle":"2021-06-29T04:20:13.245293Z","shell.execute_reply.started":"2021-06-29T04:20:13.20632Z","shell.execute_reply":"2021-06-29T04:20:13.244127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are going to drop those features which are below that baseline.","metadata":{"id":"aD2djMUPdPVu"}},{"cell_type":"code","source":"plt.plot((abs(corr).sum()-1)/len(corr))\nplt.xticks([])\nplt.plot(np.ones(len(corr))*0.06,label = 'baseline',color = 'r')\nplt.legend()\nplt.show()","metadata":{"id":"nzHmYqV8QoyS","outputId":"cbee0526-cde7-4c6a-afff-1e4402b68496","execution":{"iopub.status.busy":"2021-06-29T04:20:13.24879Z","iopub.execute_input":"2021-06-29T04:20:13.249126Z","iopub.status.idle":"2021-06-29T04:20:13.370007Z","shell.execute_reply.started":"2021-06-29T04:20:13.249096Z","shell.execute_reply":"2021-06-29T04:20:13.368821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in corr.columns:\n    if ((sum(corr[col])-1)/(len(corr)-1)) <0.06:\n        print(col , (sum(corr[col])-1)/(len(corr)-1))","metadata":{"id":"48ZPTQh3QoyT","outputId":"afdbc86f-2852-4516-fe7a-e9bc7e750748","execution":{"iopub.status.busy":"2021-06-29T04:20:13.372081Z","iopub.execute_input":"2021-06-29T04:20:13.372377Z","iopub.status.idle":"2021-06-29T04:20:13.586486Z","shell.execute_reply.started":"2021-06-29T04:20:13.372351Z","shell.execute_reply":"2021-06-29T04:20:13.585286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in corr.columns:\n    if ((sum(corr[col])-1)/(len(corr)-1)) <0.06:\n        train_df.drop(col,1,inplace=True)\n        test_df.drop(col,1,inplace=True)","metadata":{"id":"LWYVlLAXQoyT","execution":{"iopub.status.busy":"2021-06-29T04:20:13.589225Z","iopub.execute_input":"2021-06-29T04:20:13.589672Z","iopub.status.idle":"2021-06-29T04:20:14.018034Z","shell.execute_reply.started":"2021-06-29T04:20:13.589633Z","shell.execute_reply":"2021-06-29T04:20:14.017061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"id":"LGivGOK2QoyU","outputId":"616a61b3-1aab-4b74-8184-688a1f20a4b8","execution":{"iopub.status.busy":"2021-06-29T04:20:14.021615Z","iopub.execute_input":"2021-06-29T04:20:14.021943Z","iopub.status.idle":"2021-06-29T04:20:14.04391Z","shell.execute_reply.started":"2021-06-29T04:20:14.021914Z","shell.execute_reply":"2021-06-29T04:20:14.042945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.describe()","metadata":{"id":"dzIJ7gonQoyU","outputId":"b106f5c4-5f4d-4bbf-d6c9-80b0e20d4a17","execution":{"iopub.status.busy":"2021-06-29T04:20:14.045145Z","iopub.execute_input":"2021-06-29T04:20:14.045425Z","iopub.status.idle":"2021-06-29T04:20:14.581138Z","shell.execute_reply.started":"2021-06-29T04:20:14.045397Z","shell.execute_reply":"2021-06-29T04:20:14.579931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3. #### Outliers :\n---\nAs the outliers can not be distinguished very properly but can stay in the data as noise. We can visualize the outliers using [seaborn boxplots](https://seaborn.pydata.org/generated/seaborn.boxplot.html).","metadata":{"id":"DtGSn09Ad7fG"}},{"cell_type":"code","source":"fig,axes = plt.subplots(1,5,figsize=(24,3))\ni=1\nfor col in train_df.columns[1:-1]:\n    plt.subplot(1,5,i)\n    sns.boxplot(train_df['target'],train_df[col])\n   # plt.yaxis('off')\n    plt.xticks([])\n    i+=1\n    if i%5==1 and col!=train_df.columns[-2]:\n        i=1\n        plt.show()\n        fig,axes = plt.subplots(1,5,figsize=(24,3))","metadata":{"id":"VM9QuV3EQoyU","outputId":"990d51fd-658f-4618-cc74-9b978c608f44","execution":{"iopub.status.busy":"2021-06-29T04:20:14.582594Z","iopub.execute_input":"2021-06-29T04:20:14.582966Z","iopub.status.idle":"2021-06-29T04:20:43.370363Z","shell.execute_reply.started":"2021-06-29T04:20:14.582934Z","shell.execute_reply":"2021-06-29T04:20:43.369149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the boxplots, we can see that most of the data is outliers. So, we need to sensitively process data and remove those outliers.\n\n#### Approach:\n\nThe approaches has been taken from [here](https://machinelearningmastery.com/how-to-use-statistics-to-identify-outliers-in-data/).\n\n1. Using Interquartile Range .\n2. Zscore\n\n","metadata":{"id":"etKPvVrReWRu"}},{"cell_type":"code","source":"from  scipy.stats import zscore","metadata":{"execution":{"iopub.status.busy":"2021-06-29T04:20:43.371985Z","iopub.execute_input":"2021-06-29T04:20:43.372403Z","iopub.status.idle":"2021-06-29T04:20:43.377059Z","shell.execute_reply.started":"2021-06-29T04:20:43.372359Z","shell.execute_reply":"2021-06-29T04:20:43.375976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using zscore\n\ntemp_df = train_df\n\nfor col in temp_df.columns[1:-1]:\n    temp_df['zs'] = np.abs(zscore(temp_df[col]))\n    temp_df = temp_df[temp_df['zs'] <= 2.7]\n    temp_df.drop('zs' , 1 , inplace = True)\ntrain_df.drop('zs' , 1 , inplace = True)\nprint(train_df.shape , '--->' , temp_df.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T04:20:43.378751Z","iopub.execute_input":"2021-06-29T04:20:43.37926Z","iopub.status.idle":"2021-06-29T04:20:45.898723Z","shell.execute_reply.started":"2021-06-29T04:20:43.379218Z","shell.execute_reply":"2021-06-29T04:20:45.896889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.stats import iqr","metadata":{"execution":{"iopub.status.busy":"2021-06-29T04:20:45.900041Z","iopub.execute_input":"2021-06-29T04:20:45.900326Z","iopub.status.idle":"2021-06-29T04:20:45.904829Z","shell.execute_reply.started":"2021-06-29T04:20:45.900298Z","shell.execute_reply":"2021-06-29T04:20:45.903717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using interquartile range\n\ntemp_df = train_df\n\nfor col in temp_df.columns[1:-1]:\n    iqr_val = iqr(temp_df[col])\n    q1 = np.quantile(temp_df[col] , 0.03)\n    q3 = np.quantile(temp_df[col] , 0.97)\n    temp_df = temp_df[temp_df[col]>=q1-1.5*iqr_val]\n    temp_df = temp_df[temp_df[col]<=q3+1.5*iqr_val]\nprint(train_df.shape,'--->',temp_df.shape)","metadata":{"id":"z7_HgY_dQoyV","outputId":"d645727f-c4a3-4c8e-b885-f4ded748f833","execution":{"iopub.status.busy":"2021-06-29T04:20:45.906569Z","iopub.execute_input":"2021-06-29T04:20:45.906932Z","iopub.status.idle":"2021-06-29T04:20:47.89064Z","shell.execute_reply.started":"2021-06-29T04:20:45.90684Z","shell.execute_reply":"2021-06-29T04:20:47.889307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On the previous versions I have used a avery small range of iqr. In this version I will be removing the 3% of the furthest outliers using IQR method.","metadata":{"id":"V3HDSX77e2iH"}},{"cell_type":"code","source":"cleaned_train_df = temp_df","metadata":{"id":"dpUT49sZQoyX","execution":{"iopub.status.busy":"2021-06-29T04:20:47.892126Z","iopub.execute_input":"2021-06-29T04:20:47.892511Z","iopub.status.idle":"2021-06-29T04:20:47.896502Z","shell.execute_reply.started":"2021-06-29T04:20:47.892477Z","shell.execute_reply":"2021-06-29T04:20:47.895431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4. #### Dropping ID :","metadata":{"id":"t9h4nfe6kRt7"}},{"cell_type":"code","source":"cleaned_train_df.drop('id',1,inplace=True)\nidx = test_df['id']\ntest_df.drop('id',1,inplace=True)","metadata":{"id":"FfwpDBC9QoyX","execution":{"iopub.status.busy":"2021-06-29T04:20:47.898195Z","iopub.execute_input":"2021-06-29T04:20:47.898598Z","iopub.status.idle":"2021-06-29T04:20:47.930398Z","shell.execute_reply.started":"2021-06-29T04:20:47.898555Z","shell.execute_reply":"2021-06-29T04:20:47.929311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"5. #### Dropping Duplicates :\n---\nNow we are going to drop the duplicate rows and features. This will reduce the dimensionality of the train data.","metadata":{"id":"FQfm6S8kkZl5"}},{"cell_type":"code","source":"cleaned_train_df.drop_duplicates(inplace=True)\n#cleaned_train_df = cleaned_train_df.T.drop_duplicates().T  \n#no need to apply these function .Takes to much unneccessary time","metadata":{"id":"iJlTP2RZQoyY","execution":{"iopub.status.busy":"2021-06-29T04:20:47.931425Z","iopub.execute_input":"2021-06-29T04:20:47.931688Z","iopub.status.idle":"2021-06-29T04:20:47.98634Z","shell.execute_reply.started":"2021-06-29T04:20:47.931662Z","shell.execute_reply":"2021-06-29T04:20:47.985323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cleaned_train_df.shape","metadata":{"id":"1ZIxoRouQoyZ","outputId":"d526b0e7-f555-442d-a085-3034a06960a2","execution":{"iopub.status.busy":"2021-06-29T04:20:47.987723Z","iopub.execute_input":"2021-06-29T04:20:47.98804Z","iopub.status.idle":"2021-06-29T04:20:47.994432Z","shell.execute_reply.started":"2021-06-29T04:20:47.988011Z","shell.execute_reply":"2021-06-29T04:20:47.99342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"6. #### Checking other features :\n---\n Let's check if there's any other pattern if we can find.","metadata":{"id":"-9WPX8_JlLAC"}},{"cell_type":"code","source":"arr = []\nplt.figure(figsize=(10,4))\nfor i in range(1,10):\n    t_df =temp_df[temp_df['target']=='Class_'+str(i)]\n    plt.scatter(t_df['feature_0'],t_df.index,label='Class_'+str(i),s=7)\nplt.legend()\nplt.show()","metadata":{"id":"JxP0cQwcQoyZ","outputId":"e2acf617-0846-4a1c-ee3b-ff4caacdb89e","execution":{"iopub.status.busy":"2021-06-29T04:20:47.995742Z","iopub.execute_input":"2021-06-29T04:20:47.996084Z","iopub.status.idle":"2021-06-29T04:20:49.309255Z","shell.execute_reply.started":"2021-06-29T04:20:47.996053Z","shell.execute_reply":"2021-06-29T04:20:49.308238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Look's like we can forward to the next step.","metadata":{"id":"jp5f0p_PlT5T"}},{"cell_type":"markdown","source":"7. #### Splitting the data into train and validation :\n---\n We are going to have an 80-20 train validation split, also we are going to change the target feature(basically change that into numerical values).","metadata":{"id":"aWDQ4ys-mO1X"}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"id":"Hlh4B3--QoyY","execution":{"iopub.status.busy":"2021-06-29T04:20:49.312464Z","iopub.execute_input":"2021-06-29T04:20:49.312769Z","iopub.status.idle":"2021-06-29T04:20:49.317079Z","shell.execute_reply.started":"2021-06-29T04:20:49.31274Z","shell.execute_reply":"2021-06-29T04:20:49.315951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_data(test_size,data):\n    data = data.sample(frac=1)\n    x_train = data.drop('target',1)\n    y_1 = data['target']\n    x_train = x_train\n    y_1 = y_1.to_numpy()\n    X_train , X_val , y_1 , y_2 = train_test_split( x_train , y_1 ,\n                                                         test_size = test_size ,\n                                                        random_state =1 ,\n                                                        stratify = y_1)\n    y_train = []\n    y_val = []\n    for value in y_1:\n        y_train.append(int(value[-1])-1)\n    for value in y_2:\n        y_val.append(int(value[-1])-1)\n    return X_train , X_val , np.array(y_train) , np.array(y_val)","metadata":{"id":"TaOKULInQoyY","execution":{"iopub.status.busy":"2021-06-29T04:20:49.318902Z","iopub.execute_input":"2021-06-29T04:20:49.319247Z","iopub.status.idle":"2021-06-29T04:20:49.332214Z","shell.execute_reply.started":"2021-06-29T04:20:49.319211Z","shell.execute_reply":"2021-06-29T04:20:49.331069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train , X_val , y_train , y_val = split_data(0.2,cleaned_train_df)\nX_test = test_df[X_train.columns]","metadata":{"id":"S_O2FnkqQoya","execution":{"iopub.status.busy":"2021-06-29T04:20:49.334052Z","iopub.execute_input":"2021-06-29T04:20:49.334479Z","iopub.status.idle":"2021-06-29T04:20:49.442061Z","shell.execute_reply.started":"2021-06-29T04:20:49.334436Z","shell.execute_reply":"2021-06-29T04:20:49.441113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape , X_val.shape , y_train.shape , y_val.shape , X_test.shape","metadata":{"id":"yv-WGYWnQoya","outputId":"ec4bdb7c-94d2-4c4d-e8fd-b88623cf0b78","execution":{"iopub.status.busy":"2021-06-29T04:20:49.443322Z","iopub.execute_input":"2021-06-29T04:20:49.443617Z","iopub.status.idle":"2021-06-29T04:20:49.45277Z","shell.execute_reply.started":"2021-06-29T04:20:49.443589Z","shell.execute_reply":"2021-06-29T04:20:49.451695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"8. #### Scaling :\n---\nNow we need to scale the data as the different magnitudes of data may create irregular clusters.","metadata":{"id":"WlVkevXortgf"}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler as scaler","metadata":{"id":"_JfOP5Sgi8Gf","execution":{"iopub.status.busy":"2021-06-29T04:20:49.454566Z","iopub.execute_input":"2021-06-29T04:20:49.454993Z","iopub.status.idle":"2021-06-29T04:20:49.466461Z","shell.execute_reply.started":"2021-06-29T04:20:49.454949Z","shell.execute_reply":"2021-06-29T04:20:49.465337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def scale(train,test,validation):\n  sc = scaler()\n  columns = train.columns\n  train = sc.fit_transform(train)\n  test = sc.transform(test)\n  validation = sc.transform(validation)\n\n  train = pd.DataFrame(train , columns = columns)\n  test = pd.DataFrame(test , columns = columns)\n  validation = pd.DataFrame(validation , columns = columns)\n\n  return train , test , validation","metadata":{"id":"bALWb6oOjB5H","execution":{"iopub.status.busy":"2021-06-29T04:20:49.46805Z","iopub.execute_input":"2021-06-29T04:20:49.468457Z","iopub.status.idle":"2021-06-29T04:20:49.478828Z","shell.execute_reply.started":"2021-06-29T04:20:49.468413Z","shell.execute_reply":"2021-06-29T04:20:49.47794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train , X_test , X_val = scale(X_train , X_test , X_val)","metadata":{"id":"EO7x1ZiNjs3g","execution":{"iopub.status.busy":"2021-06-29T04:20:49.480106Z","iopub.execute_input":"2021-06-29T04:20:49.480495Z","iopub.status.idle":"2021-06-29T04:20:49.549626Z","shell.execute_reply.started":"2021-06-29T04:20:49.480456Z","shell.execute_reply":"2021-06-29T04:20:49.548758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.head()","metadata":{"id":"_GYQgTDtkJU5","outputId":"80babf89-c844-4f8d-9bc3-6cd0353bcd02","execution":{"iopub.status.busy":"2021-06-29T04:20:49.551148Z","iopub.execute_input":"2021-06-29T04:20:49.551551Z","iopub.status.idle":"2021-06-29T04:20:49.580436Z","shell.execute_reply.started":"2021-06-29T04:20:49.551502Z","shell.execute_reply":"2021-06-29T04:20:49.579497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we will check again the target mass distribution.","metadata":{"id":"HGVJBPXysJ6A"}},{"cell_type":"code","source":"# Target Value Count Distribution:\ntm = pd.DataFrame(y_train,columns=['x'])\ntarget_mass = tm['x'].value_counts()\nvalues = target_mass.values.tolist()\nindexes = target_mass.index.tolist()\n\nax,fig = plt.subplots(1,2,figsize=(15,6))\nplt.subplot(1,2,1)\nplt.pie(values , labels = indexes)\nplt.subplot(1,2,2)\nplt.bar(indexes,values)\nplt.show()","metadata":{"id":"IkfZxZg_Qoyb","outputId":"cef4f548-d868-4239-f004-665700822a18","execution":{"iopub.status.busy":"2021-06-29T04:20:49.581671Z","iopub.execute_input":"2021-06-29T04:20:49.581963Z","iopub.status.idle":"2021-06-29T04:20:49.838461Z","shell.execute_reply.started":"2021-06-29T04:20:49.581936Z","shell.execute_reply":"2021-06-29T04:20:49.837368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It changed heavily :O","metadata":{"id":"BmMP5uIFsRUA"}},{"cell_type":"markdown","source":"### Model Generation and Evaluation :\n\n As the model is training and predicting on a single data feature we might not get the correct accuracy metric. So, we are fitting and generating submission files.","metadata":{"id":"pMtpu3xEtbx4"}},{"cell_type":"code","source":"# importing models\n\nfrom sklearn.ensemble import RandomForestClassifier as rfc\nfrom sklearn.ensemble import ExtraTreesClassifier as ext\nfrom xgboost import XGBClassifier as xgb\nfrom lightgbm import LGBMClassifier as lgb\nfrom catboost import CatBoostClassifier as cbt","metadata":{"id":"0BErfch3Qoyc","execution":{"iopub.status.busy":"2021-06-29T04:20:49.840036Z","iopub.execute_input":"2021-06-29T04:20:49.840444Z","iopub.status.idle":"2021-06-29T04:20:49.846264Z","shell.execute_reply.started":"2021-06-29T04:20:49.840399Z","shell.execute_reply":"2021-06-29T04:20:49.844892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to train and visualize accuracy and predict\n\ndef train_and_predict(model , x_1  , x_2 , x_3 , y_1 , y_2):\n    labels = []\n    for i in range(9):\n        labels.append('Class_'+str(i+1))\n    model.fit(x_1 , y_1)\n    print('Training Completed..........')\n    print('Train Accuracy : ',model.score(x_1,y_1))\n    print('Validation Accuracy : ',model.score(x_2 , y_2))\n    print('Model Prediction started....')\n    y_pred = model.predict_proba(x_3)\n    final_df = pd.DataFrame(y_pred , columns = labels)\n    final_df = pd.concat([idx,final_df]  , axis = 1)    #uncomment this to find the actual submission files.\n    #idxx = pd.DataFrame(np.ones(len(idx)))\n    #final_df = pd.concat([idxx,final_df],axis=1)   # comment this line find actual submission files\n    return final_df","metadata":{"id":"hdXzQTw7Qoyc","execution":{"iopub.status.busy":"2021-06-29T04:20:49.848077Z","iopub.execute_input":"2021-06-29T04:20:49.848553Z","iopub.status.idle":"2021-06-29T04:20:49.861654Z","shell.execute_reply.started":"2021-06-29T04:20:49.848507Z","shell.execute_reply":"2021-06-29T04:20:49.860598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf1 = rfc(random_state = 2)\nclf2 = ext(random_state = 2)\nclf3 = xgb()\nclf4 = lgb()\nclf5 = cbt(verbose=0)\nmodels = [ clf1 , clf2 , clf3 , clf4 , clf5]\nnames = ['rfc' , 'ext' , 'xgb' , 'lgb' , 'cbt']","metadata":{"id":"XroN07aZQoyd","execution":{"iopub.status.busy":"2021-06-29T04:20:49.863339Z","iopub.execute_input":"2021-06-29T04:20:49.863743Z","iopub.status.idle":"2021-06-29T04:20:49.884069Z","shell.execute_reply.started":"2021-06-29T04:20:49.863712Z","shell.execute_reply":"2021-06-29T04:20:49.882858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(models)):\n    model = models[i]\n    print(names[i] , 'model has been opted for training...........')\n    submission = train_and_predict(model , X_train , X_val , X_test , y_train , y_val)\n    print('submission file created................................\\n\\n')\n    submission.to_csv(names[i]+'.csv',index=False)\nprint('Task Completed.............................................')","metadata":{"id":"AYB9u_r6Qoyd","outputId":"5f40c502-ccdf-4526-ad79-5823d496bbc0","execution":{"iopub.status.busy":"2021-06-29T04:20:49.885722Z","iopub.execute_input":"2021-06-29T04:20:49.886203Z","iopub.status.idle":"2021-06-29T04:21:05.390841Z","shell.execute_reply.started":"2021-06-29T04:20:49.886161Z","shell.execute_reply":"2021-06-29T04:21:05.387766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# THANK YOU for visiting !!!!\n\n## You can visit my other works at [kaggle](https://www.kaggle.com/sagnik1511/code) or in [Github](https://github.com/sagnik1511?tab=repositories).","metadata":{"id":"dtep0Pl8uyLI"}},{"cell_type":"markdown","source":"## And Always ....................\n\n![](https://i.ytimg.com/vi/GduXLWFxKhQ/maxresdefault.jpg)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}