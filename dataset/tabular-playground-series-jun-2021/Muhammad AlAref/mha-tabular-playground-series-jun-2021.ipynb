{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CIE 632: Machine Learning Fundamentals - Spring 2021\n# Project - Due Date: June 26, 2021\n# Name: Muhammad Hamdy AlAref","metadata":{}},{"cell_type":"markdown","source":"## Exploring the data","metadata":{}},{"cell_type":"code","source":"from sklearn import *\nimport pandas as pd\n\ndata_train = pd.read_csv('../input/tabular-playground-series-jun-2021/train.csv')\ndata_test = pd.read_csv('../input/tabular-playground-series-jun-2021/test.csv')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_train.describe(include='all')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = data_train.iloc[:, 1:-1].to_numpy()\ny_train = data_train.iloc[:, -1].to_numpy()\nX_test = data_test.iloc[:, 1:].to_numpy()\n\nfor i in (X_train, y_train, X_test): print(i.shape)  # sanity check","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is a classification problem with 200K samples, i.e. $n = 200,000$; each having 75 features (from `feature_0` to `feature_74`), i.e. $p = 75$.\nThere are $9$ possible classes for each observation.\n\nIn the following, the `neg_log_loss` score will be used as the competition evaluates submissions using multi-class logarithmic loss.","metadata":{}},{"cell_type":"markdown","source":"## Standardizing the features","metadata":{}},{"cell_type":"code","source":"scaler = preprocessing.StandardScaler().fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Streamlining models evaluations","metadata":{}},{"cell_type":"code","source":"def choose_model(models, X, y, verbose=False):\n    best_clf, best_score = None, None\n    for model in models:\n        clf = model_selection.GridSearchCV(model['clf'], model['params'], scoring='neg_log_loss', n_jobs=-1).fit(X, y)\n        if best_score is None or best_score < clf.best_score_: best_score, best_clf = clf.best_score_, clf.best_estimator_\n        if verbose: print(f\"{model['name']} got score {clf.best_score_}\" + (f\" with parameters {clf.best_params_}\" if model['params'] else \"\"))\n    return best_clf, best_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Trying different models with all the features","metadata":{}},{"cell_type":"code","source":"models = (\n    {\n        'name'   : 'Gaussian Naive Bayes',\n        'clf'    : naive_bayes.GaussianNB(),\n        'params' : {}\n    },\n    {\n        'name'   : 'Logistic Regression',\n        'clf'    : linear_model.LogisticRegression(),\n        'params' : {'penalty' : ('none',), 'solver' : ('saga',)}\n    },\n    {\n        'name'   : 'Lasso',\n        'clf'    : linear_model.LogisticRegression(),\n        'params' : {'penalty' : ('l1',), 'solver' : ('saga',)}\n    },\n    {\n        'name'   : 'Ridge',\n        'clf'    : linear_model.LogisticRegression(),\n        'params' : {'penalty' : ('l2',), 'solver' : ('saga',)}\n    },\n    {\n        'name'   : 'Linear Discriminant Analysis',\n        'clf'    : discriminant_analysis.LinearDiscriminantAnalysis(),\n        'params' : {}\n    },\n    {\n        'name'   : 'Quadratic Discriminant Analysis',\n        'clf'    : discriminant_analysis.QuadraticDiscriminantAnalysis(),\n        'params' : {}\n    },\n#     {  # Takes forever!\n#         'name'   : 'K Nearest Neighbors',\n#         'clf'    : neighbors.KNeighborsClassifier(),\n#         'params' : {'algorithm' : ('ball_tree',), 'n_neighbors' : range(1, 10, 2)}  # Specifying the algorithm to prevent brute force from exploding.\n#                                                                                     # Selecting BallTree as it is considered better than KDTree in high dimensions.\n#     },\n    {\n        'name'   : 'Decision Tree',\n        'clf'    : tree.DecisionTreeClassifier(),\n        'params' : {'max_depth' : range(10, 101, 10)}\n    },\n    {\n        'name'   : 'Random Forests',\n        'clf'    : ensemble.RandomForestClassifier(),\n        'params' : {'max_depth' : range(10, 51, 10)}\n    },\n    {\n        'name'   : 'AdaBoost',\n        'clf'    : ensemble.AdaBoostClassifier(),\n        'params' : {}\n    },\n#     {  # Takes forever and ever!!! I guess SVM's bad reputation of bad scaling is well-earned!\n#         'name'   : 'Support Vector Machines',\n#         'clf'    : svm.SVC(),\n#         'params' : {'kernel' : ('linear', 'poly', 'rbf'), 'C' : range(1, 10, 2)}\n#     },\n    {\n        'name'   : 'Multi-layer Perceptron',\n        'clf'    : neural_network.MLPClassifier(),\n        'params' : {'hidden_layer_sizes' : range(10, 100, 10)}\n    }\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_clf, best_score = choose_model(models, X_train, y_train, verbose=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Re-tuning random forests as its optimum lies on the border.\nbest_clf, best_score = choose_model(\n    ({\n        'name'   : 'Random Forests',\n        'clf'    : ensemble.RandomForestClassifier(),\n        'params' : {'max_depth' : range(1, 11)}\n    },), X_train, y_train, verbose=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Evidently, Ridge (Logistic Regression with `l2` penalty), LDA, Random Forests and MLP (not-so-deep neural network) yield the best results with Random Forests being slightly better.","metadata":{}},{"cell_type":"markdown","source":"## Re-trying the best models in a lower-dimensional setting\n*MLP is excluded as it is probably best if tuned separately with dedicated neural network libraries supporting deeper models and accelerators (GPUs and TPUs).*","metadata":{}},{"cell_type":"code","source":"best_models = (\n    {\n        'name'   : 'Ridge',\n        'clf'    : linear_model.LogisticRegression(),\n        'params' : {'penalty' : ('l2',), 'solver' : ('saga',)}\n    },\n    {\n        'name'   : 'Linear Discriminant Analysis',\n        'clf'    : discriminant_analysis.LinearDiscriminantAnalysis(),\n        'params' : {}\n    },\n    {\n        'name'   : 'Random Forests',\n        'clf'    : ensemble.RandomForestClassifier(),\n        'params' : {'max_depth' : range(1, 11)}\n    }\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca = decomposition.PCA(n_components='mle').fit(X_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca.n_components_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca.explained_variance_ratio_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca.explained_variance_ratio_.sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like the variability in the data is spread across all the dimensions!","metadata":{}},{"cell_type":"code","source":"X_reduced = pca.transform(X_train)\nbest_clf_reduced, best_score_reduced = choose_model(best_models, X_reduced, y_train, verbose=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Writing predictions","metadata":{}},{"cell_type":"code","source":"# Choosing between the high- and low-dimensional settings automatically; for scripting purposes\nif best_score_reduced > best_score:\n    print('Choosing the low-dimensional setting...')\n    best_score = best_score_reduced\n    best_clf = best_clf_reduced\n    X_test = pca.transform(X_test)\nelse:\n    print('Sticking with the high-dimensional setting...')\n\nprint(f\"Estimated score = {best_score}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_predict = best_clf.predict_proba(X_test)\ndata_predict = pd.read_csv('../input/tabular-playground-series-jun-2021/sample_submission.csv')\ndata_predict.iloc[:, 1:] = y_predict\ndata_predict.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}