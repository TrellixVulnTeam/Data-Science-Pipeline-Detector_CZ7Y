{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TPS-June2021 - Simple FeedForwad N/W\nOriginal notebook was cloned from @pranjalchatterjee","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nimport pylab\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-22T11:39:13.57546Z","iopub.execute_input":"2021-06-22T11:39:13.575772Z","iopub.status.idle":"2021-06-22T11:39:13.583446Z","shell.execute_reply.started":"2021-06-22T11:39:13.575744Z","shell.execute_reply":"2021-06-22T11:39:13.582392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Setting up the Data\nNow, we'll load and take a look at the data.","metadata":{}},{"cell_type":"code","source":"## categorical data encoding\nOH_encoder = OneHotEncoder(handle_unknown='ignore',sparse=False)\n\n## read train data\ntrain_data = pd.read_csv('../input/tabular-playground-series-jun-2021/train.csv', index_col='id')\nX = train_data.iloc[:,:75]\n# One-Hot encode the labels\ny = pd.DataFrame(OH_encoder.fit_transform(pd.DataFrame(train_data.target)))\ny = y.rename(columns={i:f'Class_{i+1}' for i in range(9)})\n# train_data.head()\n\n## scale train data\n\"\"\" I observed StandardScaler gives better performance than MinMaxScaler\"\"\"\nsc = StandardScaler()\nX = sc.fit_transform(X)\n\n## read test data\ntest_data = pd.read_csv('../input/tabular-playground-series-jun-2021/test.csv', index_col='id')\nX_test = test_data.iloc[:,:]\n\n## scale train data\nX_test = sc.fit_transform(X_test)\n# test_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T11:39:13.589217Z","iopub.execute_input":"2021-06-22T11:39:13.589499Z","iopub.status.idle":"2021-06-22T11:39:15.211925Z","shell.execute_reply.started":"2021-06-22T11:39:13.589475Z","shell.execute_reply":"2021-06-22T11:39:15.211053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##class distribution is imbalanced\ntrain_data.target.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T11:39:15.213456Z","iopub.execute_input":"2021-06-22T11:39:15.213788Z","iopub.status.idle":"2021-06-22T11:39:15.259273Z","shell.execute_reply.started":"2021-06-22T11:39:15.213753Z","shell.execute_reply":"2021-06-22T11:39:15.258414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca = PCA(n_components=2)\npca.fit(X)\nX_pca = pca.transform(X)\nX_test_pca = pca.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T11:39:15.261632Z","iopub.execute_input":"2021-06-22T11:39:15.261981Z","iopub.status.idle":"2021-06-22T11:39:17.507597Z","shell.execute_reply.started":"2021-06-22T11:39:15.261947Z","shell.execute_reply":"2021-06-22T11:39:17.506486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## no distinguishable boundary as such\ncolors = []\nfor i in train_data.target:\n    colors.append(int(i.split('_')[-1]))\nplt.scatter(X_pca[:,0], X_pca[:,1], c=colors, cmap='viridis')","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:10:24.783464Z","iopub.execute_input":"2021-06-22T12:10:24.78392Z","iopub.status.idle":"2021-06-22T12:10:28.614797Z","shell.execute_reply.started":"2021-06-22T12:10:24.783877Z","shell.execute_reply":"2021-06-22T12:10:28.614009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## We  observe with no of features variance retained increases linearly. \n## This is interesting, as typically the increase is exponential.\n\nratios = [round(i*0.05,2) for i in range(10,20)]\ndimensions = []\nfor r in ratios:\n    pca = PCA(r)\n    pca.fit(X)\n    X_pca_ratio = pca.transform(X)\n    dimensions.append(X_pca_ratio.shape[1])\nplt.plot(ratios, dimensions)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:11:24.635895Z","iopub.execute_input":"2021-06-22T12:11:24.6362Z","iopub.status.idle":"2021-06-22T12:11:44.967072Z","shell.execute_reply.started":"2021-06-22T12:11:24.636172Z","shell.execute_reply":"2021-06-22T12:11:44.966312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The Model\nHow to use Activation, Dropout and BatchNormalization is still matter of discussions. There is no pre-defined rule here, mot probably a matter of opinion as this is empirical work. I found this [thread](https://stackoverflow.com/questions/39691902/ordering-of-batch-normalization-and-dropout) useful.   \n\n- Activation and Dropout: for some functions such as ReLU order doesn't matter - [check here](https://sebastianraschka.com/faq/docs/dropout-activation.html). In case of non-linear activation function, typically, dropout is applied after activation layer.  \n- BN used after Dropout bcs I found this [argument](https://stackoverflow.com/a/50698801/5094187) intuitive.\n- Dropout rate kept low pertaining to [discussions in this direction](https://stackoverflow.com/a/59001644/5094187).  \n- No dropout in last layer as per [discussions](https://stats.stackexchange.com/questions/361700/lack-of-batch-normalization-before-last-fully-connected-layer)  \nSo the order I've used **Activation->Dropout->BN**\n\n**But** [this paper](https://arxiv.org/pdf/2107.02279.pdf) on design smells in DL programs recommends **Activation->BN->Dropout** with support from literature. Hence need to check with both configs.\n\nWe'll then compile the model with the Adam optimizer, the categorical_crossentropy as loss and metric.","metadata":{}},{"cell_type":"code","source":"def my_model(ip_size):\n    model = keras.Sequential([\n        layers.InputLayer([ip_size]),\n        layers.Dense(96, activation='relu'),\n        layers.Dropout(rate=0.2),\n        layers.BatchNormalization(),\n        layers.Dense(128, activation='relu'),\n        layers.Dropout(rate=0.2),\n        layers.BatchNormalization(),\n        layers.Dense(256, activation='relu'),\n        layers.Dropout(rate=0.25),\n#         layers.BatchNormalization(),\n#         layers.Dense(512, activation='relu'),\n#         layers.Dropout(rate=0.5),\n#         layers.BatchNormalization(),\n        layers.Dense(9, activation='softmax')\n    ])\n    \n    opt = keras.optimizers.Adam(learning_rate=0.0005)\n    \n    model.compile(\n        optimizer=opt,\n        loss='categorical_crossentropy',\n        metrics=['categorical_crossentropy']\n    )\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-06-22T11:49:24.696066Z","iopub.execute_input":"2021-06-22T11:49:24.69642Z","iopub.status.idle":"2021-06-22T11:49:24.703598Z","shell.execute_reply.started":"2021-06-22T11:49:24.696388Z","shell.execute_reply":"2021-06-22T11:49:24.702334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training and Validation\nNow, we'll set up the training and validation data.","metadata":{}},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, \n                                                      train_size=0.9, \n                                                      test_size=0.1, \n                                                      stratify=y,\n                                                      random_state=0)\npca_final = PCA(0.85)\nX_train = pca_final.fit_transform(X_train)\nX_valid = pca_final.transform(X_valid)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T11:39:41.347023Z","iopub.execute_input":"2021-06-22T11:39:41.347439Z","iopub.status.idle":"2021-06-22T11:39:45.634056Z","shell.execute_reply.started":"2021-06-22T11:39:41.347403Z","shell.execute_reply":"2021-06-22T11:39:45.63311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then, we'll train the model on the training data and check with validations. We'll use an early stopping metric as well, training on many epochs.","metadata":{}},{"cell_type":"code","source":"early_stopping = keras.callbacks.EarlyStopping(\n    patience=50,\n    min_delta=0.0005,\n    restore_best_weights=False,\n    verbose=2\n)\n\nmodel = my_model(X_train.shape[1])\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=90,\n    epochs=200,\n    callbacks=[early_stopping]\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T11:50:41.79307Z","iopub.execute_input":"2021-06-22T11:50:41.793398Z","iopub.status.idle":"2021-06-22T11:56:56.894058Z","shell.execute_reply.started":"2021-06-22T11:50:41.793367Z","shell.execute_reply":"2021-06-22T11:56:56.893315Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1,2,figsize=(12, 4))\nax[0].plot(history.history['loss'], label=\"Training loss\")\nax[0].plot(history.history['val_loss'], label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['categorical_crossentropy'], label=\"Training accuracy\")\nax[1].plot(history.history['val_categorical_crossentropy'],label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)\n\n\nplt.setp(ax[:], xlabel='epoch')\nplt.setp(ax[0], ylabel='loss')\nplt.setp(ax[1], ylabel='accuracy')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T11:56:56.895741Z","iopub.execute_input":"2021-06-22T11:56:56.896084Z","iopub.status.idle":"2021-06-22T11:56:57.158361Z","shell.execute_reply.started":"2021-06-22T11:56:56.896046Z","shell.execute_reply":"2021-06-22T11:56:57.157414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction","metadata":{}},{"cell_type":"code","source":"predictions = model.predict(pca_final.transform(X_test))\noutput = pd.DataFrame(predictions)\noutput = output.rename(columns={i:f'Class_{i+1}' for i in range(9)})\noutput = output.rename_axis(\"id\", axis='rows')\nidcol = pd.read_csv('../input/tabular-playground-series-jun-2021/test.csv')\nidcol = idcol.iloc[:,0]\noutput = pd.concat([idcol, output], axis=1)\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","metadata":{"execution":{"iopub.status.busy":"2021-06-22T11:58:19.096338Z","iopub.execute_input":"2021-06-22T11:58:19.096647Z","iopub.status.idle":"2021-06-22T11:58:23.574035Z","shell.execute_reply.started":"2021-06-22T11:58:19.096617Z","shell.execute_reply":"2021-06-22T11:58:23.573164Z"},"trusted":true},"execution_count":null,"outputs":[]}]}