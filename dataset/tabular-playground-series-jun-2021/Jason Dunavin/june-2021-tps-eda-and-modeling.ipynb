{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-05T19:35:14.765697Z","iopub.execute_input":"2021-06-05T19:35:14.766195Z","iopub.status.idle":"2021-06-05T19:35:14.782241Z","shell.execute_reply.started":"2021-06-05T19:35:14.76613Z","shell.execute_reply":"2021-06-05T19:35:14.780703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# June 2021 Tabular Playground Series - Exploratory data analysis\nIf you're here, you know the score, especially if you did May's version, as I did. This dataset has more rows and more columns, and furthermore there are 9 classes to predict this time instead of 4. The column names are devoid of meaning and there's no insight as to what the data actually represents in real-world terms. We have a bunch of columns of numbers and each row belongs to one of nine otherwise indistinguishable classes. To me, it's about testing out and tweaking appropriate models than about manipulating data.","metadata":{}},{"cell_type":"code","source":"#Load the data\ntrain_X = pd.read_csv('/kaggle/input/tabular-playground-series-jun-2021/train.csv')\ntest_X = pd.read_csv('/kaggle/input/tabular-playground-series-jun-2021/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-05T19:35:14.790009Z","iopub.execute_input":"2021-06-05T19:35:14.790742Z","iopub.status.idle":"2021-06-05T19:35:16.525867Z","shell.execute_reply.started":"2021-06-05T19:35:14.790701Z","shell.execute_reply":"2021-06-05T19:35:16.524537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The training set has 200,000 rows and 77 columns. One of them is a row ID and useless to us. The last one is the target labels, so there are 75 features. Let's check the data quality. Before that, let's drop the ID column and separate the target column.\n\n## Data setup and cleaning","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:38:39.149589Z","iopub.execute_input":"2021-06-05T15:38:39.149986Z","iopub.status.idle":"2021-06-05T15:38:39.184987Z","shell.execute_reply.started":"2021-06-05T15:38:39.149954Z","shell.execute_reply":"2021-06-05T15:38:39.184194Z"}}},{"cell_type":"code","source":"import seaborn as sns\nfrom matplotlib import pyplot as plt\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2021-06-05T19:35:16.527813Z","iopub.execute_input":"2021-06-05T19:35:16.528165Z","iopub.status.idle":"2021-06-05T19:35:16.879329Z","shell.execute_reply.started":"2021-06-05T19:35:16.52813Z","shell.execute_reply":"2021-06-05T19:35:16.878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Name the rows by their 'id' and drop the superfluous 'id' column\ntrain_X = train_X.drop('id', axis = 1)\ntest_X = test_X.drop('id', axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T19:35:16.880525Z","iopub.execute_input":"2021-06-05T19:35:16.88081Z","iopub.status.idle":"2021-06-05T19:35:16.982712Z","shell.execute_reply.started":"2021-06-05T19:35:16.880781Z","shell.execute_reply":"2021-06-05T19:35:16.981316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split off the target, also create a copy with just the number part of the class - might need it later!\ntrain_y = train_X.pop('target')\ntrain_y_num = [int(x[-1])-1 for x in train_y]","metadata":{"execution":{"iopub.status.busy":"2021-06-05T19:35:16.984477Z","iopub.execute_input":"2021-06-05T19:35:16.984895Z","iopub.status.idle":"2021-06-05T19:35:17.073548Z","shell.execute_reply.started":"2021-06-05T19:35:16.984857Z","shell.execute_reply":"2021-06-05T19:35:17.072165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Missing values? \nprint('Missing values in training set: ', train_X.isnull().sum().sum())\nprint('Missing values in test set: ', test_X.isnull().sum().sum())","metadata":{"execution":{"iopub.status.busy":"2021-06-05T19:35:17.07553Z","iopub.execute_input":"2021-06-05T19:35:17.076041Z","iopub.status.idle":"2021-06-05T19:35:17.123971Z","shell.execute_reply.started":"2021-06-05T19:35:17.075973Z","shell.execute_reply":"2021-06-05T19:35:17.122677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Any duplicates? - Yes, drop them all - we won't make any assumption which one is right\ndupes = train_X.duplicated(keep=False)\nprint(dupes.value_counts())\ndropthese = list(dupes[dupes == True].index)\ntrain_X = train_X.drop(dropthese)\ntrain_y = train_y.drop(dropthese) # Drop their class labels too\ntrain_y_num = [int(x[-1])-1 for x in train_y]","metadata":{"execution":{"iopub.status.busy":"2021-06-05T19:35:17.12578Z","iopub.execute_input":"2021-06-05T19:35:17.126143Z","iopub.status.idle":"2021-06-05T19:35:17.715527Z","shell.execute_reply.started":"2021-06-05T19:35:17.126103Z","shell.execute_reply":"2021-06-05T19:35:17.714101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we want to check if any remaining training observations duplicate rows in the test set. We won't necessarily do anything about it, but it might help us to force the test predictions to match the training version. Last time, this didn't help me - go figure!","metadata":{}},{"cell_type":"code","source":"# Yes there are\ndata_X = train_X.append(test_X, ignore_index = True)\nalldupes = data_X.duplicated(keep=False)\nprint(alldupes.value_counts())\nnotethese = list(alldupes[alldupes == True].index)\n# More to come later maybe","metadata":{"execution":{"iopub.status.busy":"2021-06-05T19:35:17.718475Z","iopub.execute_input":"2021-06-05T19:35:17.718829Z","iopub.status.idle":"2021-06-05T19:35:18.518887Z","shell.execute_reply.started":"2021-06-05T19:35:17.718794Z","shell.execute_reply":"2021-06-05T19:35:18.517666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Class distribution\nNow we're ready to get to some exploring. Let's look at the distribution of predicted classes, and make a dumb prediction based on it.","metadata":{}},{"cell_type":"code","source":"# Count the instances of each class and divide by the length\ndumb_preds = train_y.value_counts() / len(train_y)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T19:35:18.520833Z","iopub.execute_input":"2021-06-05T19:35:18.521227Z","iopub.status.idle":"2021-06-05T19:35:18.574041Z","shell.execute_reply.started":"2021-06-05T19:35:18.521189Z","shell.execute_reply":"2021-06-05T19:35:18.572613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(9,6))\nsns.countplot(data=train_y, x=train_y, order=sorted(train_y.unique()), ax=ax)\nax.set_title(\"Target distribution\", size=16, weight='bold')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T19:35:18.575912Z","iopub.execute_input":"2021-06-05T19:35:18.576326Z","iopub.status.idle":"2021-06-05T19:35:19.268265Z","shell.execute_reply.started":"2021-06-05T19:35:18.576288Z","shell.execute_reply":"2021-06-05T19:35:19.26711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nsubmission1 = pd.DataFrame(columns=sorted(dumb_preds.index), index=test_X.index + 200000)\nfor x in submission1.columns:\n    submission1[x] = dumb_preds[x]\n# submission1.head() -- Quality check if desired\nsubmission1.index.name = 'id'\nsubmission1.to_csv('submission1_naive.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-05T19:35:19.269871Z","iopub.execute_input":"2021-06-05T19:35:19.270203Z","iopub.status.idle":"2021-06-05T19:35:21.371247Z","shell.execute_reply.started":"2021-06-05T19:35:19.27017Z","shell.execute_reply":"2021-06-05T19:35:21.369953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature distributions\nLast time the features had most values = 0 and many were similarly distributed. Let's see if that is the case here. Since there are more features these graphs may be a bit more unwieldy than last time. It does appear that there are differences in the ranges of some features between the training and test set. I might try scaling the combined data sets to the same scale and see if that helps at all.","metadata":{}},{"cell_type":"code","source":"train_X.describe().T.style.bar(subset=['mean'], color='#205ff2')\\\n                            .background_gradient(subset=['std'], cmap='Reds')\\\n                            .background_gradient(subset=['50%'], cmap='coolwarm')","metadata":{"execution":{"iopub.status.busy":"2021-06-05T19:35:21.374398Z","iopub.execute_input":"2021-06-05T19:35:21.375196Z","iopub.status.idle":"2021-06-05T19:35:22.035005Z","shell.execute_reply.started":"2021-06-05T19:35:21.37514Z","shell.execute_reply":"2021-06-05T19:35:22.033846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_X.describe().T.style.bar(subset=['mean'], color='#205ff2')\\\n                            .background_gradient(subset=['std'], cmap='Reds')\\\n                            .background_gradient(subset=['50%'], cmap='coolwarm')","metadata":{"execution":{"iopub.status.busy":"2021-06-05T19:35:22.036581Z","iopub.execute_input":"2021-06-05T19:35:22.037269Z","iopub.status.idle":"2021-06-05T19:35:22.457532Z","shell.execute_reply.started":"2021-06-05T19:35:22.037219Z","shell.execute_reply":"2021-06-05T19:35:22.456473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check pairwise correlations\nIt doesn't look like there are any meaningful pairwise correlations to me.","metadata":{}},{"cell_type":"code","source":"datacorr = data_X.corr()\nplt.subplots(figsize=(16,16))\nsns.heatmap(datacorr)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T19:35:22.458974Z","iopub.execute_input":"2021-06-05T19:35:22.459538Z","iopub.status.idle":"2021-06-05T19:35:29.342174Z","shell.execute_reply.started":"2021-06-05T19:35:22.459498Z","shell.execute_reply":"2021-06-05T19:35:29.340842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check unique value counts\nLet's look to see how many values each feature takes on. From the graph below, it looks like all of the features take on at least 15 different values. Low counts would be suspected of being categories. Of course, some of them can still be - but we have no underlying knowledge of where the data came from, so we can't know this for sure. Because it makes my life easier, I'm going to pretend I did not have this thought. In addition, a few features in the test set take on values that aren't in the training set. Again we are going to overlook that for now.","metadata":{}},{"cell_type":"code","source":"# This was shamelessly stolen from a May TPS competitor\nfig, ax = plt.subplots(1, 1, figsize=(18, 6))\n\ny = np.array([train_X[f'feature_{i}'].nunique() for i in range(75)])\ny2 = np.array([test_X[f'feature_{i}'].nunique() for i in range(75)])\ncomp = y-y2\n\nax.bar(range(75), y2, alpha=0.7, label='Test Dataset')\nax.bar(range(75),  comp*(comp>0), bottom=y2, alpha=0.7, label='Test > Train')\nax.bar(range(75), comp*(comp<0), bottom=y2-comp*(comp<0), alpha=0.7, label='Test < Train')\n\nax.set_yticks(range(0, 120, 10))\nax.margins(0.02)\nax.grid(axis='y', linestyle='--', zorder=5)\nax.set_title('# of Features Unique Values (Train/Test)', loc='left', fontweight='bold')\nax.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T19:35:29.343679Z","iopub.execute_input":"2021-06-05T19:35:29.344049Z","iopub.status.idle":"2021-06-05T19:35:30.414099Z","shell.execute_reply.started":"2021-06-05T19:35:29.34401Z","shell.execute_reply":"2021-06-05T19:35:30.41249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBoost\nI'm going to try it without any messing around.","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom xgboost import plot_importance\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.model_selection import GridSearchCV \nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2021-06-05T19:35:30.41568Z","iopub.execute_input":"2021-06-05T19:35:30.416034Z","iopub.status.idle":"2021-06-05T19:35:30.501886Z","shell.execute_reply.started":"2021-06-05T19:35:30.415972Z","shell.execute_reply":"2021-06-05T19:35:30.500647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1 = XGBClassifier(random_state = 14000605, use_label_encoder=False)\nmodel1.fit(train_X, train_y_num)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T19:35:30.503047Z","iopub.execute_input":"2021-06-05T19:35:30.503383Z","iopub.status.idle":"2021-06-05T19:40:12.722935Z","shell.execute_reply.started":"2021-06-05T19:35:30.503341Z","shell.execute_reply":"2021-06-05T19:40:12.722091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = model1.predict_proba(test_X)\nsubmission2 = pd.DataFrame(preds, columns=sorted(dumb_preds.index), index=test_X.index + 200000)\nsubmission2.index.name = 'id'\nsubmission2.head()\nsubmission2.to_csv('submission2_xgb.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-05T19:43:04.39568Z","iopub.execute_input":"2021-06-05T19:43:04.3963Z","iopub.status.idle":"2021-06-05T19:43:06.951484Z","shell.execute_reply.started":"2021-06-05T19:43:04.39624Z","shell.execute_reply":"2021-06-05T19:43:06.950274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## That's it for now\nIf any of you have any commentary or wisdom - please drop it in the comments! ","metadata":{}}]}