{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook works by honing in on an XGBClassifier paramterization using Optuna.\n\nKaggle/your own GPU is reccomended here since we are training hundreds of XGB models.","metadata":{}},{"cell_type":"code","source":"#import required libraries\nimport pandas as pd\nimport numpy as np\nimport optuna\nfrom numpy import mean, std\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import XGBClassifier\nfrom optuna import Trial\nfrom optuna.samplers import TPESampler","metadata":{"execution":{"iopub.status.busy":"2021-06-19T14:36:50.957075Z","iopub.execute_input":"2021-06-19T14:36:50.957487Z","iopub.status.idle":"2021-06-19T14:36:50.9678Z","shell.execute_reply.started":"2021-06-19T14:36:50.957455Z","shell.execute_reply":"2021-06-19T14:36:50.962965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#bring in data\nraw_training = pd.read_csv(\"../input/tabular-playground-series-jun-2021/train.csv\")\nraw_test = pd.read_csv(\"../input/tabular-playground-series-jun-2021/test.csv\")\n\n#pull out features\nX = raw_training.iloc[:, 1:76]\nX = np.ascontiguousarray(X)\n\n#pull out responses\ny = raw_training.iloc[:, 76]\ny = LabelEncoder().fit_transform(y)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T14:36:52.407187Z","iopub.execute_input":"2021-06-19T14:36:52.407661Z","iopub.status.idle":"2021-06-19T14:36:53.831501Z","shell.execute_reply.started":"2021-06-19T14:36:52.407628Z","shell.execute_reply":"2021-06-19T14:36:53.830387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thanks to Adesh Bansode for starter code on optuna\n\nhttps://medium.com/subex-ai-labs/efficient-hyperparameter-optimization-for-xgboost-model-using-optuna-3ee9a02566b1","metadata":{}},{"cell_type":"code","source":"# #define objective function\n# def objective(trial: Trial,X,y) -> float:\n    \n#     #create a data split for validation\n#     train_X,test_X,train_y,test_y = train_test_split(X, y, test_size = 0.30, random_state = 101)\n    \n#     #create a parameter space\n#     #Note: I started very general with wide intervals and honed in with repeated studies.\n#     #The history dataframe in the next cell is your friend.\n#     #Read the table to pare down your intervals after each study.\n    \n#     param = {                \n#                  'learning_rate':trial.suggest_uniform('learning_rate', 0.03, 0.06),\n#                  'gamma':trial.suggest_uniform('gamma', .2, .5),\n#                  'reg_alpha':trial.suggest_int('reg_alpha', 1, 5),\n#                  'reg_lambda':trial.suggest_int('reg_lambda', 1, 7),\n#                  'n_estimators':trial.suggest_int('n_estimators', 300, 500),\n#                  'colsample_bynode':trial.suggest_uniform('colsample_bynode', .2, .4),\n#                  'colsample_bylevel':trial.suggest_uniform('colsample_bylevel', .65, .75),\n#                  'subsample':trial.suggest_uniform('subsample', .55, .75),               \n#                  'min_child_weight':trial.suggest_int('min_child_weight', 100, 200),\n#                  'colsample_bytree':trial.suggest_uniform('colsample_bytree',0.2, .4)\n#             }\n    \n#     #set up the baseline model parameters.\n#     #be careful about how high you let max_depth go\n#     #this can lead to long training times and overfitting\n    \n#     model = XGBClassifier(objective='multi:softprob', eval_metric = \"mlogloss\", num_class = 9,\n#                           tree_method = 'gpu_hist', max_depth = 13, use_label_encoder=False, **param)\n    \n#     #fit to training sample\n#     model.fit(train_X, train_y)\n    \n#     #return the cv score\n#     return -cross_val_score(model, test_X, test_y, cv = 3, scoring = 'neg_log_loss').mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #run an Optuna study\n# study = optuna.create_study(direction='minimize', sampler=TPESampler())\n# study.optimize(lambda trial : objective(trial, X, y), n_trials = 25)\n\n# #print our best outcome\n# print('Best trial: score {},\\nparams {}'.format(study.best_trial.value,study.best_trial.params))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #take a look at the trial runs to pare down parameter intervals\n# hist = study.trials_dataframe()\n# hist = hist.sort_values(by = ['value'])\n# hist","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#here are the parameters from the study\nparms = {'learning_rate': 0.03817329673009776, 'gamma': 0.3993428240049768, 'reg_alpha': 3,\n         'reg_lambda': 1, 'n_estimators': 334, 'colsample_bynode': 0.2695766080178446,\n         'colsample_bylevel': 0.6832712495239914, 'subsample': 0.6999062848890633,\n         'min_child_weight': 100, 'colsample_bytree': 0.34663755614898173}\n\n#create the xgb model we will feed to the probabiltiy calibrator\nXGBmod = XGBClassifier(objective='multi:softprob',\n                         eval_metric = \"mlogloss\",\n                         num_class = 9,\n                         tree_method = 'gpu_hist',\n                         max_depth = 13,\n                         use_label_encoder=False, **parms)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T14:36:55.913064Z","iopub.execute_input":"2021-06-19T14:36:55.913469Z","iopub.status.idle":"2021-06-19T14:36:55.920805Z","shell.execute_reply.started":"2021-06-19T14:36:55.913428Z","shell.execute_reply":"2021-06-19T14:36:55.919412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#take a look at the model perfomance                     \nXGBscores = -cross_val_score(XGBmod, X, y, cv = 5, scoring = 'neg_log_loss')\nprint(\"Mean log loss: \", XGBscores.mean())\nprint(\"Sigma of log loss: \", XGBscores.std())\n\n#make final fit of model\nXGBmod.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T14:17:43.000356Z","iopub.execute_input":"2021-06-19T14:17:43.0012Z","iopub.status.idle":"2021-06-19T14:20:59.491325Z","shell.execute_reply.started":"2021-06-19T14:17:43.001148Z","shell.execute_reply":"2021-06-19T14:20:59.490033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#grab test features\nX_test = raw_test.iloc[:,1:76]\nX_test = np.ascontiguousarray(X_test)\n\n#predict probabilities on test data\ntest_pred = XGBmod.predict_proba(X_test)\ntest_pred = pd.DataFrame(test_pred)\ntest_pred","metadata":{"execution":{"iopub.status.busy":"2021-06-19T14:22:32.436341Z","iopub.execute_input":"2021-06-19T14:22:32.436806Z","iopub.status.idle":"2021-06-19T14:22:43.74458Z","shell.execute_reply.started":"2021-06-19T14:22:32.436772Z","shell.execute_reply":"2021-06-19T14:22:43.743352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#prepare output and save\noutput = pd.DataFrame(raw_test.iloc[:,0])\n\noutput = output.merge(test_pred, left_index = True, right_index = True)\n\noutput.columns = [\"id\", \"Class_1\", \"Class_2\", \"Class_3\", \"Class_4\",\n                  \"Class_5\", \"Class_6\", \"Class_7\", \"Class_8\", \"Class_9\"]\n\noutput.to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}