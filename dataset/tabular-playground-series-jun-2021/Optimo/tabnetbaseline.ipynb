{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# About this notebook\n\nThis is just a very simple baseline for TabNet usage as I saw some starters with very poor results.\n\nThere is nothing very specific to this competition in this notebook. This is a simple adaptation of a previous notebook I shared on an other competition : https://www.kaggle.com/optimo/tabnetregressor-baseline\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-08T15:19:19.226717Z","iopub.execute_input":"2021-06-08T15:19:19.227076Z","iopub.status.idle":"2021-06-08T15:19:27.977638Z","shell.execute_reply.started":"2021-06-08T15:19:19.227Z","shell.execute_reply":"2021-06-08T15:19:27.976636Z"}}},{"cell_type":"code","source":"!pip install pytorch-tabnet\n\n# install develop branch\n# !pip install  \"git+https://github.com/dreamquark-ai/tabnet.git@develop#egg=pytorch_tabnet\" --upgrade","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom pytorch_tabnet.tab_model import TabNetClassifier\nimport torch\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import roc_auc_score\n\nimport plotly.express as px\nfrom matplotlib import pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-06-08T15:19:27.98166Z","iopub.execute_input":"2021-06-08T15:19:27.981975Z","iopub.status.idle":"2021-06-08T15:19:33.715693Z","shell.execute_reply.started":"2021-06-08T15:19:27.981941Z","shell.execute_reply":"2021-06-08T15:19:33.71471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/tabular-playground-series-jun-2021/train.csv')\ndf_test = pd.read_csv('/kaggle/input/tabular-playground-series-jun-2021/test.csv')\ndf_sub = pd.read_csv('/kaggle/input/tabular-playground-series-jun-2021/sample_submission.csv')\n# set submission preds to 0\ndf_sub[[col for col in df_sub.columns if col.startswith(\"Class\")]] = 0","metadata":{"execution":{"iopub.status.busy":"2021-06-08T15:49:10.02912Z","iopub.execute_input":"2021-06-08T15:49:10.029493Z","iopub.status.idle":"2021-06-08T15:49:11.440808Z","shell.execute_reply.started":"2021-06-08T15:49:10.029461Z","shell.execute_reply":"2021-06-08T15:49:11.43898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Consider everything as numerical\n# CAT_COLS = [] \n# NUM_COLS = [c for c in df_train.columns if c.startswith(\"feature_\")]\n\n# Consider everything as categorical variables might be useful : this is the only trick of this notebook\nCAT_COLS = [c for c in df_train.columns if c.startswith(\"feature_\")] \nNUM_COLS = [] \n\nFEATURES = CAT_COLS + NUM_COLS\n\n\nencoders = {}\n# Categorical features need to be LabelEncoded\nfor cat_col in CAT_COLS:\n    label_enc = LabelEncoder()\n        \n    df_train[cat_col] = label_enc.fit_transform(df_train[cat_col])\n    encoders[cat_col] = label_enc\n    \n# Encode test set\nfor cat_col in CAT_COLS:\n    label_enc = encoders[cat_col]\n    le_dict = dict(zip(label_enc.classes_, label_enc.transform(label_enc.classes_)))\n    # Replace unknown values by the most common value\n    # Changing this to another value might make more sense\n    if le_dict.get(\"low_frequency\") is not None:\n        default_val = le_dict[\"low_frequency\"]\n    else:\n        default_val = df_train[cat_col].mode().values[0]\n    df_test[cat_col] = df_test[cat_col].apply(lambda x: le_dict.get(x, default_val ))\n    \n# Clip numerical features in test set to match training set\nfor num_col in NUM_COLS:\n    df_test[num_col] = np.clip(df_test[num_col], df_train[num_col].min(), df_train[num_col].max())","metadata":{"execution":{"iopub.status.busy":"2021-06-08T15:19:35.724663Z","iopub.execute_input":"2021-06-08T15:19:35.725Z","iopub.status.idle":"2021-06-08T15:19:44.549126Z","shell.execute_reply.started":"2021-06-08T15:19:35.724962Z","shell.execute_reply":"2021-06-08T15:19:44.548188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_dims = df_train[CAT_COLS].nunique().to_list()\ncat_idxs = [FEATURES.index(cat_col) for cat_col in CAT_COLS]\ncat_emb_dims = np.ceil(np.log(cat_dims)).astype(np.int).tolist()\n# cat_emb_dims = np.ceil(np.clip((np.array(cat_dims)) / 2, a_min=1, a_max=50)).astype(np.int).tolist()\n# cat_emb_dims=1\n\nX = df_train[FEATURES].values\ny = df_train[\"target\"].values\n\nX_test = df_test[FEATURES].values","metadata":{"execution":{"iopub.status.busy":"2021-06-08T15:19:44.55077Z","iopub.execute_input":"2021-06-08T15:19:44.551216Z","iopub.status.idle":"2021-06-08T15:19:44.73713Z","shell.execute_reply.started":"2021-06-08T15:19:44.551167Z","shell.execute_reply":"2021-06-08T15:19:44.736058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pretraining","metadata":{}},{"cell_type":"code","source":"from pytorch_tabnet.pretraining import TabNetPretrainer\n\nN_D = 64 #64 # 32\nN_A = 64 # 32\nN_INDEP = 1 #2\nN_SHARED = 1 #2\nN_STEPS = 3 #2\nMASK_TYPE = \"sparsemax\"\nGAMMA = 1.2\nBS = 256\nMAX_EPOCH =  30\nPRETRAIN = True\n\n\nif PRETRAIN:\n    pretrain_params = dict(n_d=N_D, n_a=N_A, n_steps=N_STEPS,  #0.2,\n                           n_independent=N_INDEP, n_shared=N_SHARED,\n                           cat_idxs=cat_idxs,\n                           cat_dims=cat_dims,\n                           cat_emb_dim=cat_emb_dims,\n                           gamma=GAMMA,\n                           lambda_sparse=0., optimizer_fn=torch.optim.Adam,\n                           optimizer_params=dict(lr=2e-2),\n                           mask_type=MASK_TYPE,\n                           scheduler_params=dict(mode=\"min\",\n                                                 patience=3,\n                                                 min_lr=1e-5,\n                                                 factor=0.5,),\n                           scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,                         \n                           verbose=1,\n                          )\n\n    pretrainer = TabNetPretrainer(**pretrain_params)\n\n    pretrainer.fit(X_train=X_test, \n                   eval_set=[X],\n                   max_epochs=MAX_EPOCH,\n                   patience=25, batch_size=BS, virtual_batch_size=BS, #128,\n                   num_workers=1, drop_last=True,\n                   pretraining_ratio=0.5 # The bigger your pretraining_ratio the harder it is to reconstruct\n                  )","metadata":{"execution":{"iopub.status.busy":"2021-06-08T15:20:10.31795Z","iopub.execute_input":"2021-06-08T15:20:10.318349Z","iopub.status.idle":"2021-06-08T15:37:59.743354Z","shell.execute_reply.started":"2021-06-08T15:20:10.318311Z","shell.execute_reply":"2021-06-08T15:37:59.741731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Actual training","metadata":{}},{"cell_type":"code","source":"BS = 2048\nMAX_EPOCH =  100\nLAMBDA_SPARSE = 1e-5 #1e-5\n\nN_SPLITS = 5\nNB_FOLDS = 5 # max N_SPLITS\nskf = StratifiedKFold(n_splits=N_SPLITS, random_state=2021, shuffle=True)\n\n\nLR = 1e-1 # 5e-2\nfold_nb = 1\nfor train_index, valid_index in skf.split(X, y):\n    X_train, X_valid = X[train_index], X[valid_index]\n    y_train, y_valid = y[train_index], y[valid_index]\n\n    tabnet_params = dict(n_d=N_D, \n                         n_a=N_A,\n                         n_steps=N_STEPS, gamma=GAMMA,\n                         n_independent=N_INDEP, n_shared=N_SHARED,\n                         lambda_sparse=LAMBDA_SPARSE,\n                         seed=0,\n                         clip_value=2,\n                         cat_idxs=cat_idxs,\n                         cat_dims=cat_dims,\n                         cat_emb_dim=cat_emb_dims,\n                         mask_type=MASK_TYPE,\n                         device_name='auto',\n                         optimizer_fn=torch.optim.Adam,\n                         optimizer_params=dict(lr=LR, weight_decay=1e-5),\n#                          scheduler_params=dict(max_lr=LR,\n#                                                steps_per_epoch=int(X_train.shape[0] / BS),\n#                                                epochs=MAX_EPOCH,\n#                                                #final_div_factor=100,\n#                                                is_batch_level=True),\n#                         scheduler_fn=torch.optim.lr_scheduler.OneCycleLR,\n                              scheduler_params=dict(mode='min',\n                                                    factor=0.5,\n                                                    patience=3,\n                                                    is_batch_level=False,),\n                              scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n                         verbose=1)\n    # Defining TabNet model\n    model = TabNetClassifier(**tabnet_params)\n\n    model.fit(X_train=X_train, y_train=y_train,\n              from_unsupervised=pretrainer if PRETRAIN else None,\n              eval_set=[(X_train, y_train), (X_valid, y_valid)],\n              eval_name=[\"train\", \"valid\"],\n              eval_metric=[\"logloss\"],\n              batch_size=BS,\n              virtual_batch_size=256,\n              max_epochs=MAX_EPOCH,\n              drop_last=True,\n              pin_memory=True,\n              patience=10,\n             )  \n    \n    test_preds = model.predict_proba(X_test)\n    df_sub[model.classes_] += test_preds\n    fold_nb+=1\n    \n    if fold_nb > NB_FOLDS:\n        break\n\ndf_sub[model.classes_] = df_sub[model.classes_] / NB_FOLDS\n","metadata":{"execution":{"iopub.status.busy":"2021-06-08T15:37:59.74589Z","iopub.execute_input":"2021-06-08T15:37:59.746358Z","iopub.status.idle":"2021-06-08T15:45:03.730075Z","shell.execute_reply.started":"2021-06-08T15:37:59.746306Z","shell.execute_reply":"2021-06-08T15:45:03.729262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub.to_csv('submission.csv', index=None)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T15:45:31.697735Z","iopub.execute_input":"2021-06-08T15:45:31.698048Z","iopub.status.idle":"2021-06-08T15:45:31.740544Z","shell.execute_reply.started":"2021-06-08T15:45:31.698019Z","shell.execute_reply":"2021-06-08T15:45:31.739301Z"},"trusted":true},"execution_count":null,"outputs":[]}]}