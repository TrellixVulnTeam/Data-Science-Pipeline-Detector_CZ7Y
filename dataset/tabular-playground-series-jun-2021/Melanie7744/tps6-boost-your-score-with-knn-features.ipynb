{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Boost your score with KNN Feature extraction\n\nFor TPS 6, I wanted to follow a different approach than before. Instead of focusing on modeling, I focused on feature engineering. Given the fact, that this competition has anonymized features, this might not be an obvious choise.\nHowever my [EDA](https://www.kaggle.com/melanie7744/tps6-eda-comparison-to-tps5) also led me to the original dataset. I.e. the dataset used to generate the synthetic one we got for TPS 6. So I researched the solutions that worked back then. Many of them did not seem to be viable for me. But one looked promising: **KNN feature extraction**. \n\nMy quest for knowledge then led me to a nice package: **fastknn**, written by [David Pinto](http://www.kaggle.com/davidpinto/fastknn-show-to-glm-what-knn-see-0-96#Feature-Engineering-with-KNN). It seemed to provide everything I wanted. Unfortunately it was written in R.\n\nSo the search continued until I found .... a Python implementation by Momijiame of this same package! It is called **Gokinjo**, which means neighborhood in Japanese and is available on [Github](https://github.com/momijiame/gokinjo). \n\n\n \n</div>\n        \n### What is KNN feature extraction? \n\nfrom [David Pinto](https://davpinto.github.io/fastknn/articles/knn-extraction.html):\n\n<div class=\"alert alert-success\">\nThe fastknn provides a function to do feature extraction using KNN. It generates k * c new features, where c is the number of class labels. The new features are computed from the distances between the observations and their k nearest neighbors inside each class, as follows:\n<ul>\n<li>The first test feature contains the distances between each test instance and its nearest neighbor inside the first class.</li>\n<li>The second test feature contains the sums of distances between each test instance and its 2 nearest neighbors inside the first class.</li>\n<li>The third test feature contains the sums of distances between each test instance and its 3 nearest neighbors inside the first class.</li>\n<li>And so on.</li>\n</ul>\nThis procedure repeats for each class label, generating k * c new features. Then, the new training features are generated using a n-fold CV approach, in order to avoid overfitting. Parallelization is available. You can specify the number of threads via nthread parameter.\n</div>\n\nSo, let's try it out!","metadata":{}},{"cell_type":"code","source":"pip install gokinjo","metadata":{"execution":{"iopub.status.busy":"2021-06-12T18:14:55.674166Z","iopub.execute_input":"2021-06-12T18:14:55.674699Z","iopub.status.idle":"2021-06-12T18:15:04.70304Z","shell.execute_reply.started":"2021-06-12T18:14:55.674588Z","shell.execute_reply":"2021-06-12T18:15:04.701917Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom sklearn.preprocessing import LabelEncoder\n\nfrom gokinjo import knn_kfold_extract\nfrom gokinjo import knn_extract\n\n# list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-12T18:15:04.706738Z","iopub.execute_input":"2021-06-12T18:15:04.707045Z","iopub.status.idle":"2021-06-12T18:15:05.939716Z","shell.execute_reply.started":"2021-06-12T18:15:04.707013Z","shell.execute_reply":"2021-06-12T18:15:05.938545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# read competiton data\ndf_train = pd.read_csv('../input/tabular-playground-series-jun-2021/train.csv')\ndf_test = pd.read_csv('../input/tabular-playground-series-jun-2021/test.csv') \n\n# label encode the target column\nle = LabelEncoder()\ndf_train.target = le.fit_transform(df_train.target)\n\n# define X and y for training data\nX = df_train.drop(columns=[\"id\",\"target\"])\ny = df_train.target\n\n# prepare test data\nX_test=df_test.drop(columns=\"id\")\n\nprint(\"First five rows of training data:\")\ndisplay(X.head())\nprint(\"First five rows of test data:\")\ndisplay(X_test.head())","metadata":{"execution":{"iopub.status.busy":"2021-06-12T18:15:05.941578Z","iopub.execute_input":"2021-06-12T18:15:05.941894Z","iopub.status.idle":"2021-06-12T18:15:08.195663Z","shell.execute_reply.started":"2021-06-12T18:15:05.941865Z","shell.execute_reply":"2021-06-12T18:15:08.194666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert to numpy because gokinjo expects np arrays\nX = X.to_numpy()\ny = y.to_numpy()\nX_test = X_test.to_numpy()\n# check shapes\nprint(\"X, shape: \", np.shape(X))\nprint(\"X_test, shape: \", np.shape(X_test))","metadata":{"execution":{"iopub.status.busy":"2021-06-12T18:15:08.197177Z","iopub.execute_input":"2021-06-12T18:15:08.197458Z","iopub.status.idle":"2021-06-12T18:15:08.203623Z","shell.execute_reply.started":"2021-06-12T18:15:08.197431Z","shell.execute_reply":"2021-06-12T18:15:08.202668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# KNN feature extraction for train, as the data has not been normalized previously, let knn_kfold_extract do it\n# you can set a different value for k, just be aware about the increase in computation time\nKNN_feat_train = knn_kfold_extract(X, y, k=1, normalize='standard')\nprint(\"KNN features for training set, shape: \", np.shape(KNN_feat_train))\nKNN_feat_train[0]","metadata":{"execution":{"iopub.status.busy":"2021-06-12T18:15:08.204946Z","iopub.execute_input":"2021-06-12T18:15:08.205252Z","iopub.status.idle":"2021-06-12T18:21:16.044575Z","shell.execute_reply.started":"2021-06-12T18:15:08.205223Z","shell.execute_reply":"2021-06-12T18:21:16.043629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create KNN features for test set, as the data has not been normalized previously, let knn_extract do it\nKNN_feat_test = knn_extract(X, y, X_test, k=1, normalize='standard')\nprint(\"KNN features for test set, shape: \", np.shape(KNN_feat_test))\nKNN_feat_test[0]","metadata":{"execution":{"iopub.status.busy":"2021-06-12T18:21:16.045821Z","iopub.execute_input":"2021-06-12T18:21:16.046096Z","iopub.status.idle":"2021-06-12T18:25:02.76781Z","shell.execute_reply.started":"2021-06-12T18:21:16.046069Z","shell.execute_reply":"2021-06-12T18:25:02.766803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note: generating the KNN features for the test set was not straight-forward for me. I did not find any sample code, so I digged into the source code of the Gokinjo package until I found the solution showed here. I hope this is how it is supposed to be done. Should anybody have experience with this package... your feedback is very welcome.\n","metadata":{}},{"cell_type":"code","source":"# add KNN feature to normal features\nX, X_test = np.append(X, KNN_feat_train, axis=1), np.append(X_test, KNN_feat_test, axis=1) \nprint(\"Train set, shape: \", np.shape(X))\nprint(\"Test set, shape: \", np.shape(X_test))","metadata":{"execution":{"iopub.status.busy":"2021-06-12T18:25:02.769396Z","iopub.execute_input":"2021-06-12T18:25:02.77009Z","iopub.status.idle":"2021-06-12T18:25:02.894116Z","shell.execute_reply.started":"2021-06-12T18:25:02.770044Z","shell.execute_reply":"2021-06-12T18:25:02.893099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# store KNN features, they are computationally expensive\nnp.save('add_feat_train', KNN_feat_train)\nnp.save('add_feat_test', KNN_feat_test)\n\n# to load them in your notebook you can use:\n#new_features = np.load('add_feat_train.npy')","metadata":{"execution":{"iopub.status.busy":"2021-06-12T18:25:02.895279Z","iopub.execute_input":"2021-06-12T18:25:02.895562Z","iopub.status.idle":"2021-06-12T18:25:02.98219Z","shell.execute_reply.started":"2021-06-12T18:25:02.895536Z","shell.execute_reply":"2021-06-12T18:25:02.980989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I used those extra features with an XGBoost Model: \n* My validation logloss improved from 1.75280 to 1.75089\n* My public score improved from 1.75592 to 1.75338\n* From the 10 most important features (as ranked by XGBoost), 8 were KNN features\n\nI'd be curious to know what happens if you use a better model, i.e. one that has already a lower logloss than my XGBoost.\n* Will such a model already have learned the extra insights from the KNN features and have threrfore no improvement in score? \n* Will such a model be able to use the additional KNN features more effectively and get a higher improvement in score?\n\nIf anybody tries this out, please comment below!","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}