{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Synopsis\n\nStudy of a variety of FE methods with Logistic Regression and LightGBM\n\nIn this study we use a variety of feature engineering methods and check their effect on untuned logistic regression and boosting models.  We include both standard and experimental methods.\n\nThe one library that is not pre-installed is [kaggler](https://github.com/jeongyoonlee/Kaggler) by Jeong Yoon Lee.","metadata":{"_uuid":"deba99c0-fb52-47e9-9788-957428163b15","_cell_guid":"77e7fc22-48b4-47b7-9968-de8a8f36393c","trusted":true}},{"cell_type":"markdown","source":"# Outline\n\n* **[Setup](#Setup)** - imports and variables\n* **[Classes](#Classes)** - Custom classes used\n* **[Functions](#Functions)** - Custom functions used\n* **[Baselines](#Baselines)** - Various baseline to evaluate our results\n    * **[Equal Classes](#Equal-Classes)**\n    * **[Weighted Classes](#Weighted-Classes)**\n    * **[Unmodified Features](#Unmodified-Features)**\n* **[Decomposition](#Decomposition)** - Tests of Decomposition Methods\n    * **[PCA](#PCA)**\n    * **[Factor Analysis](#Factor-Analysis)**\n    * **[Factor Analysis Rotated](#Factor-Analysis-Rotated)**\n    * **[Fast ICA](#Fast-ICA)**\n* **[Supervised DAE](#Supervised-DAE)** - Test of Supervised DAE from kaggler\n* **[Target Encoding](#Target-Encoding)**\n* **[Numeric Binning](#Numeric-Binning)** - Experimental\n* **[One-Hot Encoding](#One-Hot-Encoding)**\n* **[Rotate Features and Values](#Rotate-Features-and-Values)** - Experimental\n* **[Positive Encoding](#Positive-Encoding)** - Experimental\n* **[Positive Encoding Plus Original](#Positive-Encoding-Plus-Original)** - Experimental\n* **[Embedding](#Embedding)**\n* **[Summary](#Summary)**\n\n","metadata":{}},{"cell_type":"markdown","source":"# Setup","metadata":{"_uuid":"10e90e4b-6b92-4a61-893e-e84da23813a3","_cell_guid":"051d2cb1-1ab8-474c-b5e1-99e8faf8172e","trusted":true}},{"cell_type":"code","source":"import copy\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom pandas.io.formats import style\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport sklearn\nimport sklearn.preprocessing as sk_prep\nimport sklearn.model_selection as sk_ms\nimport sklearn.feature_selection as sk_fs\nimport sklearn.pipeline as sk_pipe\nimport sklearn.compose as sk_comp\nimport sklearn.base as sk_base\nimport sklearn.ensemble as sk_ens\nimport sklearn.metrics as sk_met\nimport sklearn.linear_model as sk_lm\nimport sklearn.tree as sk_tree\nimport sklearn.svm as sk_svm\nimport sklearn.decomposition as sk_de\nimport category_encoders as ce\n\nfrom scipy import stats\n\nimport lightgbm as lgbm","metadata":{"_uuid":"a3c709db-66e2-492d-a11c-a15d539951fc","_cell_guid":"fc6313d4-37ba-408c-9cf6-a40c61d28f4f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-06-17T14:12:55.372058Z","iopub.execute_input":"2021-06-17T14:12:55.372675Z","iopub.status.idle":"2021-06-17T14:12:57.840617Z","shell.execute_reply.started":"2021-06-17T14:12:55.372591Z","shell.execute_reply":"2021-06-17T14:12:57.839908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install kaggler","metadata":{"_uuid":"35bec9db-684f-4665-b716-7f075bc7b87f","_cell_guid":"00c2ba0e-cc8c-4719-bf7c-cf78fb9ade66","collapsed":false,"_kg_hide-output":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-06-17T14:12:57.841819Z","iopub.execute_input":"2021-06-17T14:12:57.842272Z","iopub.status.idle":"2021-06-17T14:13:39.89135Z","shell.execute_reply.started":"2021-06-17T14:12:57.84223Z","shell.execute_reply":"2021-06-17T14:13:39.89032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import kaggler\nimport kaggler.preprocessing as kag_prep\nprint(kaggler.__version__)","metadata":{"_uuid":"3529e202-1691-4c0a-bd14-5c61650b0bc8","_cell_guid":"464f8395-6481-44ed-8eaf-cd1d31371ae4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-06-17T14:13:39.893275Z","iopub.execute_input":"2021-06-17T14:13:39.893523Z","iopub.status.idle":"2021-06-17T14:13:45.571274Z","shell.execute_reply.started":"2021-06-17T14:13:39.893497Z","shell.execute_reply":"2021-06-17T14:13:45.570323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch import optim\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader","metadata":{"execution":{"iopub.status.busy":"2021-06-17T14:13:45.573012Z","iopub.execute_input":"2021-06-17T14:13:45.573379Z","iopub.status.idle":"2021-06-17T14:13:46.575582Z","shell.execute_reply.started":"2021-06-17T14:13:45.573339Z","shell.execute_reply":"2021-06-17T14:13:46.574613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"3cba2f81-355b-4a45-972f-8552e86237f8","_cell_guid":"27a01fa0-1401-441d-aa2f-227f52a9e5c4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-06-17T14:13:46.576733Z","iopub.execute_input":"2021-06-17T14:13:46.576988Z","iopub.status.idle":"2021-06-17T14:13:46.582465Z","shell.execute_reply.started":"2021-06-17T14:13:46.576958Z","shell.execute_reply":"2021-06-17T14:13:46.581437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_DIR = '/kaggle/input/tabular-playground-series-jun-2021'\nRANDOM_STATE = 9003","metadata":{"_uuid":"1e568bbb-31c4-4748-aa19-68bd35730be4","_cell_guid":"1a953721-c8d6-4e18-8e67-f1652fe4f650","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-06-17T14:13:46.583811Z","iopub.execute_input":"2021-06-17T14:13:46.584184Z","iopub.status.idle":"2021-06-17T14:13:46.592003Z","shell.execute_reply.started":"2021-06-17T14:13:46.584144Z","shell.execute_reply":"2021-06-17T14:13:46.591174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Classes\n\nClasses used for feature engineering","metadata":{}},{"cell_type":"code","source":"class NumCategorizer(sk_base.TransformerMixin, sk_base.BaseEstimator):\n    \"\"\"\n    Transform numeric features into ordered categorical features based on\n    splits determined by a Decision Tree.\n    This is coded as a Scikit-Learn Transformer.\n    \"\"\"\n    \n    \n    def __init__(self, min_samples_leaf=1000, min_impurity_decrease=0.0001, random_state=1):\n        self.min_samples_leaf = min_samples_leaf\n        self.min_impurity_decrease = min_impurity_decrease\n        self.random_state = random_state\n        \n        \n    def fit(self, X, y):\n        model1 = sk_tree.DecisionTreeClassifier(\n            min_samples_leaf=self.min_samples_leaf,\n            min_impurity_decrease=self.min_impurity_decrease,\n            random_state=self.random_state\n        )\n\n        bin_boundaries = {}\n\n        for col in X:\n            model1.fit(X.loc[:, [col]], y)\n            tree = model1.tree_\n\n            bounds = tree.threshold\n            bounds = np.sort(np.unique(bounds))\n            bounds[0] = -np.Inf\n            bounds = np.append(bounds, [np.Inf])\n            bin_boundaries[col] = bounds\n\n        self.bin_boundaries_ = bin_boundaries\n        self.names_ = X.columns\n    \n        return self\n    \n    \n    def transform(self, X, y=None):\n        tr_trans = pd.DataFrame(index=X.index)\n\n        for col in X:\n            bounds = self.bin_boundaries_[col]\n            tr_trans[col] = pd.cut(X[col], bins=bounds, labels=range(len(bounds) - 1))\n            \n        return tr_trans\n\n    \n    def get_feature_names(self):\n        return self.names_","metadata":{"execution":{"iopub.status.busy":"2021-06-17T14:13:46.593189Z","iopub.execute_input":"2021-06-17T14:13:46.593442Z","iopub.status.idle":"2021-06-17T14:13:46.603668Z","shell.execute_reply.started":"2021-06-17T14:13:46.593419Z","shell.execute_reply":"2021-06-17T14:13:46.602962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyPolynomialWrapper(sk_base.TransformerMixin, sk_base.BaseEstimator):\n    \"\"\"\n    This is like Scikit-Learn's PolynomialWrapper, but I had inconsistent results\n    with that class so I have made my own.\n    \"\"\"\n    \n    \n    def __init__(self, encoder):\n        self.encoder = encoder\n    \n    \n    def fit(self, X, y):\n        target_cats = np.sort(np.unique(y))\n#         target_cats = target_cats[:-1] # remove last class\n        self.target_cats = target_cats\n        encoder = copy.deepcopy(self.encoder)\n        \n        enc_list = []\n        for tcat in target_cats:\n            encoder.fit(X, (y == tcat).astype(int))\n            enc_list.append(encoder)\n        \n        self.enc_list = enc_list\n            \n        return self\n    \n    \n    def transform(self, X, y=None):\n        trans_list = []\n        for i in range(len(self.target_cats)):\n            tcat = self.target_cats[i]\n            encoder = self.enc_list[i]\n\n            X_e = encoder.transform(X)\n            \n            new_cols = [str(tcat) + '_' + str(col) for col in X_e]\n            X_e.columns = new_cols\n            trans_list.append(X_e)\n            \n        return pd.concat(trans_list, axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T14:13:46.605509Z","iopub.execute_input":"2021-06-17T14:13:46.605962Z","iopub.status.idle":"2021-06-17T14:13:46.62012Z","shell.execute_reply.started":"2021-06-17T14:13:46.60593Z","shell.execute_reply":"2021-06-17T14:13:46.619471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EmbeddingModel(nn.Module):\n    \"\"\"\n    This is a simple pytorch NN for the purpose of training our embedding.\n    \"\"\"\n    \n    \n    def __init__(self, input_node_cnt: int, output_node_cnt: int = 1, drop_rate: float = 0.0, embeddings=100, embedding_dim=3):\n        super(EmbeddingModel, self).__init__()\n        self.embed1 = nn.Embedding(embeddings, embedding_dim)\n        self.lin1 = nn.Linear(input_node_cnt * embedding_dim, 100)\n        self.lin2 = nn.Linear(100, 50)\n        self.out = nn.Linear(50, output_node_cnt)\n        self.drop_rate = drop_rate\n        self.dropout = nn.Dropout(drop_rate)\n    \n    def forward(self, input):\n        z = nn.Flatten()(self.embed1(input))\n        z = self.dropout(z)\n        z = self.dropout(nn.ReLU()(self.lin1(z)))\n        z = self.dropout(nn.ReLU()(self.lin2(z)))\n        output = self.out(z)\n        \n        return output\n        \n    def encode(self, input):\n        z = nn.Flatten()(self.embed1(input))\n        output = self.dropout(z)\n        return output\n        ","metadata":{"execution":{"iopub.status.busy":"2021-06-17T14:13:46.621496Z","iopub.execute_input":"2021-06-17T14:13:46.62195Z","iopub.status.idle":"2021-06-17T14:13:46.635562Z","shell.execute_reply.started":"2021-06-17T14:13:46.621909Z","shell.execute_reply":"2021-06-17T14:13:46.634528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Functions\n\nFunctions for the NN training used for feature engineering","metadata":{}},{"cell_type":"code","source":"def loss_batch(model, loss_func, xb, yb, opt=None):\n    \"\"\"\n    This is a basic function from pytorch examples to handle the updates for one batch or get the loss for evaluation.\n    In evaluation mode, no optimizer should be passed to the function.\n    \"\"\"\n    loss = loss_func(model(xb), yb)\n\n    if opt is not None:\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n\n    return loss.item()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T14:13:46.636811Z","iopub.execute_input":"2021-06-17T14:13:46.637329Z","iopub.status.idle":"2021-06-17T14:13:46.652291Z","shell.execute_reply.started":"2021-06-17T14:13:46.637296Z","shell.execute_reply":"2021-06-17T14:13:46.650994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def nn_train(X, y, model, epochs):\n    \"\"\"\n    This is a basic function to handle training of our pytorch model.\n    \"\"\"\n    \n    dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    model.to(dev)\n    \n    loss_func = nn.CrossEntropyLoss()\n    opt = optim.SGD(model.parameters(),\n                lr=0.2,\n                momentum=0.9)\n    \n    X_train = torch.tensor(X, dtype=torch.long).to(dev)\n    y_train = torch.tensor(y, dtype=torch.long).to(dev)\n    \n    model.train()\n    \n    for epoch in range(epochs):\n        loss1 = loss_batch(model, loss_func, X_train, y_train, opt)\n        \n        print(f'Epoch: {epoch}  Loss: {loss1}')\n    \n    model.eval()\n    \n    return loss1 # Final training loss","metadata":{"execution":{"iopub.status.busy":"2021-06-17T14:13:46.653568Z","iopub.execute_input":"2021-06-17T14:13:46.653907Z","iopub.status.idle":"2021-06-17T14:13:46.663744Z","shell.execute_reply.started":"2021-06-17T14:13:46.65388Z","shell.execute_reply":"2021-06-17T14:13:46.662816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{"_uuid":"4090a628-a096-4f42-a13a-39634ab90066","_cell_guid":"e35a7d2e-edfd-4de1-8a7b-8d911347bd64","trusted":true}},{"cell_type":"code","source":"train_set = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'))\ntrain_data = train_set.iloc[:, 1:-1] # Feature columns\ntrain_ar = train_data.to_numpy()\n\nle_targ = sk_prep.LabelEncoder()\ntrain_y = train_set['target']\ntrain_y_num = le_targ.fit_transform(train_y)\nclasses = le_targ.classes_\n\n# Train data with label encoded target\ntrain_w_targ = train_data.copy()\ntrain_w_targ['target'] = train_y_num\n\nprint(train_set.shape)\ntrain_set","metadata":{"_uuid":"a6078072-9bcc-47ae-b24d-a7517459b883","_cell_guid":"78628a80-8fcf-460e-8a14-83fde2156ed8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-06-17T14:13:46.665083Z","iopub.execute_input":"2021-06-17T14:13:46.665345Z","iopub.status.idle":"2021-06-17T14:13:48.206889Z","shell.execute_reply.started":"2021-06-17T14:13:46.665319Z","shell.execute_reply":"2021-06-17T14:13:48.206308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Separate train / validation split for initial tests\n\n# I have chosen not to use a stratified split here.\n\ntrain_tv_X, val_tv_X, train_tv_y, val_tv_y = sk_ms.train_test_split(train_data, train_y, test_size=0.3, random_state=RANDOM_STATE)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T14:13:48.518731Z","iopub.execute_input":"2021-06-17T14:13:48.518963Z","iopub.status.idle":"2021-06-17T14:13:48.682429Z","shell.execute_reply.started":"2021-06-17T14:13:48.518941Z","shell.execute_reply":"2021-06-17T14:13:48.681592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_list_1 = []","metadata":{"execution":{"iopub.status.busy":"2021-06-17T14:13:48.683444Z","iopub.execute_input":"2021-06-17T14:13:48.683675Z","iopub.status.idle":"2021-06-17T14:13:48.687222Z","shell.execute_reply.started":"2021-06-17T14:13:48.683652Z","shell.execute_reply":"2021-06-17T14:13:48.686435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Baselines","metadata":{}},{"cell_type":"markdown","source":"## Equal Classes\n\nThe most basic prediction is the expectation that all classes have an equal chance for all samples.  This only uses the classes of y and does not require any training data.  This sets a distant outer limit for our scores.","metadata":{}},{"cell_type":"code","source":"pred_1 = 1.0 / len(classes)\n\ny_hat = pd.DataFrame(columns=classes, index=val_tv_y.index)\ny_hat.iloc[:, :] = pred_1\n\neq_loss = sk_met.log_loss(val_tv_y, y_hat)\neq_loss","metadata":{"execution":{"iopub.status.busy":"2021-06-17T14:15:59.448853Z","iopub.execute_input":"2021-06-17T14:15:59.449159Z","iopub.status.idle":"2021-06-17T14:15:59.757939Z","shell.execute_reply.started":"2021-06-17T14:15:59.44913Z","shell.execute_reply":"2021-06-17T14:15:59.757063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Weighted Classes\n\nThis goes one step further and uses the frequency of the classes in y in the training data.  It still does not use any features in the data.  Any model with a score close to this is not profiting at all from the features that are passed to it.","metadata":{}},{"cell_type":"code","source":"y_hat = pd.DataFrame(columns=classes, index=val_tv_y.index)\n\ny_hat_vals = train_tv_y.value_counts().sort_index().T / len(train_tv_y)\ny_hat.iloc[:, :] = y_hat_vals\n\nwt_loss = sk_met.log_loss(val_tv_y, y_hat)\nwt_loss","metadata":{"execution":{"iopub.status.busy":"2021-06-17T14:17:50.252785Z","iopub.execute_input":"2021-06-17T14:17:50.253132Z","iopub.status.idle":"2021-06-17T14:17:50.56007Z","shell.execute_reply.started":"2021-06-17T14:17:50.253079Z","shell.execute_reply":"2021-06-17T14:17:50.5593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Unmodified Features","metadata":{}},{"cell_type":"code","source":"%%time\n\nmodel = sk_lm.LogisticRegression(solver='lbfgs', max_iter=1000)\n\nmodel.fit(train_tv_X, train_tv_y)\n\ny_hat = model.predict_proba(val_tv_X)\n\nlr_loss = sk_met.log_loss(val_tv_y, y_hat)\nlr_loss","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:00:52.243857Z","iopub.execute_input":"2021-06-16T21:00:52.244235Z","iopub.status.idle":"2021-06-16T21:02:16.446488Z","shell.execute_reply.started":"2021-06-16T21:00:52.244203Z","shell.execute_reply":"2021-06-16T21:02:16.445404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nmodel = lgbm.LGBMClassifier(random_state=RANDOM_STATE)\n\nmodel.fit(train_tv_X, train_tv_y)\n\ny_hat = model.predict_proba(val_tv_X)\n\nlgbm_loss = sk_met.log_loss(val_tv_y, y_hat)\nlgbm_loss","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:02:16.448135Z","iopub.execute_input":"2021-06-16T21:02:16.448537Z","iopub.status.idle":"2021-06-16T21:02:31.541739Z","shell.execute_reply.started":"2021-06-16T21:02:16.448491Z","shell.execute_reply":"2021-06-16T21:02:31.540742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_list_1.append(['Original', lr_loss, lgbm_loss])","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:02:31.54312Z","iopub.execute_input":"2021-06-16T21:02:31.54343Z","iopub.status.idle":"2021-06-16T21:02:31.547775Z","shell.execute_reply.started":"2021-06-16T21:02:31.543399Z","shell.execute_reply":"2021-06-16T21:02:31.546832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decomposition","metadata":{}},{"cell_type":"markdown","source":"## PCA\n\nAlthough there is not a lot of correlation between features, we will check if PCA makes the data easier for either of the two test models to handle.","metadata":{}},{"cell_type":"code","source":"# Scaling\n\nscaler_in = sk_prep.StandardScaler()\ntr1_X = scaler_in.fit_transform(train_tv_X)\nval1_X = scaler_in.transform(val_tv_X)\n\n# PCA\n\npca_in = sk_de.PCA(random_state=RANDOM_STATE)\ntr2_X = pca_in.fit_transform(tr1_X)\nval2_X = pca_in.transform(val1_X)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:02:31.551064Z","iopub.execute_input":"2021-06-16T21:02:31.551372Z","iopub.status.idle":"2021-06-16T21:02:32.731063Z","shell.execute_reply.started":"2021-06-16T21:02:31.551343Z","shell.execute_reply":"2021-06-16T21:02:32.730048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nmodel = sk_lm.LogisticRegression(solver='lbfgs', max_iter=1000)\n\nmodel.fit(tr2_X, train_tv_y)\n\n\ny_hat = model.predict_proba(val2_X)\n\nlr_loss = sk_met.log_loss(val_tv_y, y_hat)\nlr_loss","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:02:32.732765Z","iopub.execute_input":"2021-06-16T21:02:32.733252Z","iopub.status.idle":"2021-06-16T21:02:42.257762Z","shell.execute_reply.started":"2021-06-16T21:02:32.733206Z","shell.execute_reply":"2021-06-16T21:02:42.256664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nmodel = lgbm.LGBMClassifier(random_state=RANDOM_STATE)\n\nmodel.fit(tr2_X, train_tv_y)\n\ny_hat = model.predict_proba(val2_X)\n\nlgbm_loss = sk_met.log_loss(val_tv_y, y_hat)\nlgbm_loss","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:02:42.258869Z","iopub.execute_input":"2021-06-16T21:02:42.259207Z","iopub.status.idle":"2021-06-16T21:03:09.711628Z","shell.execute_reply.started":"2021-06-16T21:02:42.259174Z","shell.execute_reply":"2021-06-16T21:03:09.710627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_list_1.append(['PCA', lr_loss, lgbm_loss])","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:03:09.712686Z","iopub.execute_input":"2021-06-16T21:03:09.712954Z","iopub.status.idle":"2021-06-16T21:03:09.717368Z","shell.execute_reply.started":"2021-06-16T21:03:09.71292Z","shell.execute_reply":"2021-06-16T21:03:09.716326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Factor Analysis","metadata":{}},{"cell_type":"code","source":"# Scaling\n\nscaler_in = sk_prep.StandardScaler()\ntr1_X = scaler_in.fit_transform(train_tv_X)\nval1_X = scaler_in.transform(val_tv_X)\n\n# FA\n\nfa_in = sk_de.FactorAnalysis(random_state=RANDOM_STATE)\ntr2_X = fa_in.fit_transform(tr1_X)\nval2_X = fa_in.transform(val1_X)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:03:09.721527Z","iopub.execute_input":"2021-06-16T21:03:09.721797Z","iopub.status.idle":"2021-06-16T21:03:15.936864Z","shell.execute_reply.started":"2021-06-16T21:03:09.721771Z","shell.execute_reply":"2021-06-16T21:03:15.935691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nmodel = sk_lm.LogisticRegression(solver='lbfgs', max_iter=1000)\n\nmodel.fit(tr2_X, train_tv_y)\n\n\ny_hat = model.predict_proba(val2_X)\n\nlr_loss = sk_met.log_loss(val_tv_y, y_hat)\nlr_loss","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:03:15.944194Z","iopub.execute_input":"2021-06-16T21:03:15.947027Z","iopub.status.idle":"2021-06-16T21:03:44.808617Z","shell.execute_reply.started":"2021-06-16T21:03:15.946944Z","shell.execute_reply":"2021-06-16T21:03:44.80758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nmodel = lgbm.LGBMClassifier(random_state=RANDOM_STATE)\n\nmodel.fit(tr2_X, train_tv_y)\n\ny_hat = model.predict_proba(val2_X)\n\nlgbm_loss = sk_met.log_loss(val_tv_y, y_hat)\nlgbm_loss","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:03:44.810201Z","iopub.execute_input":"2021-06-16T21:03:44.810837Z","iopub.status.idle":"2021-06-16T21:04:01.078225Z","shell.execute_reply.started":"2021-06-16T21:03:44.810792Z","shell.execute_reply":"2021-06-16T21:04:01.077057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_list_1.append(['FA', lr_loss, lgbm_loss])","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:04:01.079431Z","iopub.execute_input":"2021-06-16T21:04:01.079722Z","iopub.status.idle":"2021-06-16T21:04:01.083553Z","shell.execute_reply.started":"2021-06-16T21:04:01.079693Z","shell.execute_reply":"2021-06-16T21:04:01.082583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We could add tests to run with different number of components--sometimes FA works better when only the first set of components are used.","metadata":{}},{"cell_type":"markdown","source":"## Factor Analysis Rotated","metadata":{}},{"cell_type":"code","source":"# Scaling\n\nscaler_in = sk_prep.StandardScaler()\ntr1_X = scaler_in.fit_transform(train_tv_X)\nval1_X = scaler_in.transform(val_tv_X)\n\n# FA\n\nfa_in = sk_de.FactorAnalysis(random_state=RANDOM_STATE, rotation='varimax')\ntr2_X = fa_in.fit_transform(tr1_X)\nval2_X = fa_in.transform(val1_X)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:04:01.084777Z","iopub.execute_input":"2021-06-16T21:04:01.085076Z","iopub.status.idle":"2021-06-16T21:04:07.425818Z","shell.execute_reply.started":"2021-06-16T21:04:01.085048Z","shell.execute_reply":"2021-06-16T21:04:07.42469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nmodel = sk_lm.LogisticRegression(solver='lbfgs', max_iter=1000)\n\nmodel.fit(tr2_X, train_tv_y)\n\n\ny_hat = model.predict_proba(val2_X)\n\nlr_loss = sk_met.log_loss(val_tv_y, y_hat)\nlr_loss","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:04:07.427487Z","iopub.execute_input":"2021-06-16T21:04:07.428195Z","iopub.status.idle":"2021-06-16T21:04:36.430395Z","shell.execute_reply.started":"2021-06-16T21:04:07.428148Z","shell.execute_reply":"2021-06-16T21:04:36.429234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nmodel = lgbm.LGBMClassifier(random_state=RANDOM_STATE)\n\nmodel.fit(tr2_X, train_tv_y)\n\ny_hat = model.predict_proba(val2_X)\n\nlgbm_loss = sk_met.log_loss(val_tv_y, y_hat)\nlgbm_loss","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:04:36.431806Z","iopub.execute_input":"2021-06-16T21:04:36.43211Z","iopub.status.idle":"2021-06-16T21:04:52.059576Z","shell.execute_reply.started":"2021-06-16T21:04:36.432063Z","shell.execute_reply":"2021-06-16T21:04:52.058853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_list_1.append(['FA-Rot', lr_loss, lgbm_loss])","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:04:52.060573Z","iopub.execute_input":"2021-06-16T21:04:52.060971Z","iopub.status.idle":"2021-06-16T21:04:52.064294Z","shell.execute_reply.started":"2021-06-16T21:04:52.060942Z","shell.execute_reply":"2021-06-16T21:04:52.063518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fast ICA","metadata":{}},{"cell_type":"code","source":"# # Scaling\n\n# scaler_in = sk_prep.StandardScaler()\n# tr1_X = scaler_in.fit_transform(train_tv_X)\n# val1_X = scaler_in.transform(val_tv_X)\n\n# FastICA\n\nfa_in = sk_de.FastICA(random_state=RANDOM_STATE)\ntr2_X = fa_in.fit_transform(train_tv_X)\nval2_X = fa_in.transform(val_tv_X)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:04:52.065279Z","iopub.execute_input":"2021-06-16T21:04:52.065668Z","iopub.status.idle":"2021-06-16T21:05:11.938874Z","shell.execute_reply.started":"2021-06-16T21:04:52.065639Z","shell.execute_reply":"2021-06-16T21:05:11.936528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nmodel = sk_lm.LogisticRegression(solver='lbfgs', max_iter=1000)\n\nmodel.fit(tr2_X, train_tv_y)\n\n\ny_hat = model.predict_proba(val2_X)\n\nlr_loss = sk_met.log_loss(val_tv_y, y_hat)\nlr_loss","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:05:11.941002Z","iopub.execute_input":"2021-06-16T21:05:11.943168Z","iopub.status.idle":"2021-06-16T21:05:22.67057Z","shell.execute_reply.started":"2021-06-16T21:05:11.943117Z","shell.execute_reply":"2021-06-16T21:05:22.669792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nmodel = lgbm.LGBMClassifier(random_state=RANDOM_STATE)\n\nmodel.fit(tr2_X, train_tv_y)\n\ny_hat = model.predict_proba(val2_X)\n\nlgbm_loss = sk_met.log_loss(val_tv_y, y_hat)\nlgbm_loss","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:05:22.67155Z","iopub.execute_input":"2021-06-16T21:05:22.671934Z","iopub.status.idle":"2021-06-16T21:05:51.736254Z","shell.execute_reply.started":"2021-06-16T21:05:22.671904Z","shell.execute_reply":"2021-06-16T21:05:51.73518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_list_1.append(['FastICA', lr_loss, lgbm_loss])","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:05:51.737567Z","iopub.execute_input":"2021-06-16T21:05:51.737852Z","iopub.status.idle":"2021-06-16T21:05:51.742411Z","shell.execute_reply.started":"2021-06-16T21:05:51.737823Z","shell.execute_reply":"2021-06-16T21:05:51.741425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"None of these decompostion methods have helped logistic regression, but Fast ICA made it much worse--not much better than weighted classes w/o features.  Boosting was not affected much either way by any of these.  PCA or FA might be better using a smaller set of their outputs; this is especially true for FA.","metadata":{}},{"cell_type":"markdown","source":"# Supervised DAE\n\nThis uses libraries from kaggler and the example at https://www.kaggle.com/jeongyoonlee/tps-6-supervised-dae-keras-gpu","metadata":{"_uuid":"a5bb4f99-d29f-4a14-8a8d-dcb7b48cbf03","_cell_guid":"93daca8d-cd00-45b4-b13d-eedcea603267","trusted":true}},{"cell_type":"code","source":"%%time\n\n# Train SDAE on train data of split\n\ntrain_tv_y_num = le_targ.transform(train_tv_y)\n\nsdae = kag_prep.SDAE(cat_cols=train_data.columns.tolist(), encoding_dim=256, n_layer=1, noise_std=.001, batch_size=65536,\n            n_epoch=10, random_state=RANDOM_STATE)\nsdae.fit(train_tv_X.copy(), train_tv_y_num.copy())\n\ntr2_X = sdae.transform(train_tv_X.copy())\nval2_X = sdae.transform(val_tv_X.copy())","metadata":{"_uuid":"f3ddcc94-66c0-40b7-9d19-37f0b18743e3","_cell_guid":"40c8a971-f0cb-4605-9406-918dc1cd03e4","collapsed":false,"jupyter":{"outputs_hidden":false},"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-16T21:05:51.743994Z","iopub.execute_input":"2021-06-16T21:05:51.744432Z","iopub.status.idle":"2021-06-16T21:08:35.568166Z","shell.execute_reply.started":"2021-06-16T21:05:51.744387Z","shell.execute_reply":"2021-06-16T21:08:35.566616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nmodel = sk_lm.LogisticRegression(solver='lbfgs', max_iter=1000)\n\nmodel.fit(tr2_X, train_tv_y)\n\ny_hat = model.predict_proba(val2_X)\n\nlr_loss = sk_met.log_loss(val_tv_y, y_hat)\nlr_loss","metadata":{"_uuid":"652036ba-2298-488f-8711-2fb0497f6d9b","_cell_guid":"40124088-3a9a-4509-a2ef-3d255641980c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-06-16T21:08:35.569555Z","iopub.execute_input":"2021-06-16T21:08:35.569814Z","iopub.status.idle":"2021-06-16T21:10:53.088303Z","shell.execute_reply.started":"2021-06-16T21:08:35.569789Z","shell.execute_reply":"2021-06-16T21:10:53.087118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nmodel = lgbm.LGBMClassifier(random_state=RANDOM_STATE)\n\nmodel.fit(tr2_X, train_tv_y)\n\ny_hat = model.predict_proba(val2_X)\n\nlgbm_loss = sk_met.log_loss(val_tv_y, y_hat)\nlgbm_loss","metadata":{"_uuid":"319edba0-10f3-4282-94d6-386936715f46","_cell_guid":"b2846ac5-f332-4b8e-85a6-22edbc2c3be2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-06-16T21:10:53.090012Z","iopub.execute_input":"2021-06-16T21:10:53.090469Z","iopub.status.idle":"2021-06-16T21:11:32.983272Z","shell.execute_reply.started":"2021-06-16T21:10:53.090421Z","shell.execute_reply":"2021-06-16T21:11:32.982517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_list_1.append(['Sup DAE', lr_loss, lgbm_loss])","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:11:32.984287Z","iopub.execute_input":"2021-06-16T21:11:32.984705Z","iopub.status.idle":"2021-06-16T21:11:32.988725Z","shell.execute_reply.started":"2021-06-16T21:11:32.984674Z","shell.execute_reply":"2021-06-16T21:11:32.987781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Target Encoding","metadata":{}},{"cell_type":"code","source":"%%time\n\nenc2 = MyPolynomialWrapper(ce.target_encoder.TargetEncoder(cols=list(train_data.columns), smoothing=300))\n\ntr2_X = enc2.fit_transform(train_tv_X, train_tv_y)\nval2_X = enc2.transform(val_tv_X)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-16T21:11:32.989765Z","iopub.execute_input":"2021-06-16T21:11:32.990319Z","iopub.status.idle":"2021-06-16T21:13:03.518329Z","shell.execute_reply.started":"2021-06-16T21:11:32.990289Z","shell.execute_reply":"2021-06-16T21:13:03.517189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nmodel = sk_lm.LogisticRegression(solver='lbfgs', max_iter=1000)\n\nmodel.fit(tr2_X, train_tv_y)\n\ny_hat = model.predict_proba(val2_X)\n\nlr_loss = sk_met.log_loss(val_tv_y, y_hat)\nlr_loss","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:13:03.519674Z","iopub.execute_input":"2021-06-16T21:13:03.519985Z","iopub.status.idle":"2021-06-16T21:22:32.105073Z","shell.execute_reply.started":"2021-06-16T21:13:03.519943Z","shell.execute_reply":"2021-06-16T21:22:32.103943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nmodel = lgbm.LGBMClassifier(random_state=RANDOM_STATE)\n\nmodel.fit(tr2_X, train_tv_y)\n\ny_hat = model.predict_proba(val2_X)\n\nlgbm_loss = sk_met.log_loss(val_tv_y, y_hat)\nlgbm_loss","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:22:32.107052Z","iopub.execute_input":"2021-06-16T21:22:32.107369Z","iopub.status.idle":"2021-06-16T21:24:28.25751Z","shell.execute_reply.started":"2021-06-16T21:22:32.107322Z","shell.execute_reply":"2021-06-16T21:24:28.256531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_list_1.append(['Targ Enc', lr_loss, lgbm_loss])","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:24:28.258936Z","iopub.execute_input":"2021-06-16T21:24:28.259272Z","iopub.status.idle":"2021-06-16T21:24:28.26398Z","shell.execute_reply.started":"2021-06-16T21:24:28.259239Z","shell.execute_reply":"2021-06-16T21:24:28.262911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Numeric Binning\n\nWhile the concept of binning numeric columns is not new, I did not find previous work that used a decision tree to decide on the best thresholds for the bins.  This method could be further tuned by adjusting the tree parameters.  The class in this notebook allows for adjusting min_samples_leaf and min_impurity_decrease.","metadata":{}},{"cell_type":"code","source":"%%time\n\n# Binning numbers using a tree classifier (with my own wrapper class)\n\nnc = NumCategorizer(random_state=RANDOM_STATE)\n\ntr1_X = nc.fit_transform(train_tv_X, train_tv_y)\nval1_X = nc.transform(val_tv_X)\n\n# One-hot encoding to use the new categories\n\nohe = sk_prep.OneHotEncoder()\ntr2_X = ohe.fit_transform(tr1_X)\nval2_X = ohe.transform(val1_X)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:24:28.2655Z","iopub.execute_input":"2021-06-16T21:24:28.265838Z","iopub.status.idle":"2021-06-16T21:24:57.848601Z","shell.execute_reply.started":"2021-06-16T21:24:28.265805Z","shell.execute_reply":"2021-06-16T21:24:57.847535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nmodel = sk_lm.LogisticRegression(solver='lbfgs', max_iter=1000)\n\nmodel.fit(tr2_X, train_tv_y)\n\ny_hat = model.predict_proba(val2_X)\n\nlr_loss = sk_met.log_loss(val_tv_y, y_hat)\nlr_loss","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:24:57.850281Z","iopub.execute_input":"2021-06-16T21:24:57.850651Z","iopub.status.idle":"2021-06-16T21:26:07.850246Z","shell.execute_reply.started":"2021-06-16T21:24:57.85061Z","shell.execute_reply":"2021-06-16T21:26:07.849436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nmodel = lgbm.LGBMClassifier(random_state=RANDOM_STATE)\n\nmodel.fit(tr2_X, train_tv_y)\n\ny_hat = model.predict_proba(val2_X)\n\nlgbm_loss = sk_met.log_loss(val_tv_y, y_hat)\nlgbm_loss","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:26:07.851508Z","iopub.execute_input":"2021-06-16T21:26:07.851951Z","iopub.status.idle":"2021-06-16T21:26:26.853412Z","shell.execute_reply.started":"2021-06-16T21:26:07.851911Z","shell.execute_reply":"2021-06-16T21:26:26.852394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_list_1.append(['Num Bin', lr_loss, lgbm_loss])","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:26:26.854757Z","iopub.execute_input":"2021-06-16T21:26:26.855052Z","iopub.status.idle":"2021-06-16T21:26:26.859455Z","shell.execute_reply.started":"2021-06-16T21:26:26.855021Z","shell.execute_reply":"2021-06-16T21:26:26.858426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# One-Hot Encoding","metadata":{}},{"cell_type":"code","source":"# One-hot encoding on original values\n\nohe = sk_prep.OneHotEncoder(handle_unknown='ignore')\ntr2_X = ohe.fit_transform(train_tv_X)\nval2_X = ohe.transform(val_tv_X)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:26:26.860698Z","iopub.execute_input":"2021-06-16T21:26:26.861031Z","iopub.status.idle":"2021-06-16T21:26:28.051981Z","shell.execute_reply.started":"2021-06-16T21:26:26.861002Z","shell.execute_reply":"2021-06-16T21:26:28.050904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nmodel = sk_lm.LogisticRegression(solver='lbfgs', max_iter=1000)\n\nmodel.fit(tr2_X, train_tv_y)\n\ny_hat = model.predict_proba(val2_X)\n\nlr_loss = sk_met.log_loss(val_tv_y, y_hat)\nlr_loss","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:26:28.053523Z","iopub.execute_input":"2021-06-16T21:26:28.053919Z","iopub.status.idle":"2021-06-16T21:33:22.42192Z","shell.execute_reply.started":"2021-06-16T21:26:28.053879Z","shell.execute_reply":"2021-06-16T21:33:22.420849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nmodel = lgbm.LGBMClassifier(random_state=RANDOM_STATE)\n\nmodel.fit(tr2_X, train_tv_y)\n\ny_hat = model.predict_proba(val2_X)\n\nlgbm_loss = sk_met.log_loss(val_tv_y, y_hat)\nlgbm_loss","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:33:22.423698Z","iopub.execute_input":"2021-06-16T21:33:22.424136Z","iopub.status.idle":"2021-06-16T21:33:46.459978Z","shell.execute_reply.started":"2021-06-16T21:33:22.424066Z","shell.execute_reply":"2021-06-16T21:33:46.459001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_list_1.append(['One-Hot', lr_loss, lgbm_loss])","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:33:46.461606Z","iopub.execute_input":"2021-06-16T21:33:46.462Z","iopub.status.idle":"2021-06-16T21:33:46.467673Z","shell.execute_reply.started":"2021-06-16T21:33:46.461959Z","shell.execute_reply":"2021-06-16T21:33:46.466299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Rotate Features and Values\n\nI do not know a formal name for this.\n\nEmbedding assumes some relationship across all features for a given value.  So the value 5, for example, would be encoded in the same way if it occurred in feature_0, feature_1, etc.  For some types of data, it might be worthwhile to count how often a particular value occurs with no concern for where it occurs.  This is not likely to be the case for this data, but we will take a look.","metadata":{}},{"cell_type":"code","source":"tr1_ar = train_tv_X.values\n\nmax_val = np.max(tr1_ar) # will ignore values above the range of the training set since we wouldn't have any predictions for them.\n\ntr2_ar = np.zeros((tr1_ar.shape[0], max_val + 1))\n\nfor i in range(max_val + 1):\n    tr2_ar[:, i] = np.sum(tr1_ar == i, axis=1)\n    \ntr2_X = pd.DataFrame(tr2_ar, columns=np.arange(max_val + 1))\ntr2_X","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:33:46.469427Z","iopub.execute_input":"2021-06-16T21:33:46.469939Z","iopub.status.idle":"2021-06-16T21:33:54.325261Z","shell.execute_reply.started":"2021-06-16T21:33:46.469897Z","shell.execute_reply":"2021-06-16T21:33:54.324479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val1_ar = val_tv_X.values\n\nval2_ar = np.zeros((val1_ar.shape[0], max_val + 1))\n\nfor i in range(max_val + 1):\n    val2_ar[:, i] = np.sum(val1_ar == i, axis=1)\n    \nval2_X = pd.DataFrame(val2_ar, columns=np.arange(max_val + 1))\nval2_X","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:33:54.326575Z","iopub.execute_input":"2021-06-16T21:33:54.327134Z","iopub.status.idle":"2021-06-16T21:33:57.458765Z","shell.execute_reply.started":"2021-06-16T21:33:54.327067Z","shell.execute_reply":"2021-06-16T21:33:57.457723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nmodel = sk_lm.LogisticRegression(solver='lbfgs', max_iter=1000)\n\nmodel.fit(tr2_X, train_tv_y)\n\ny_hat = model.predict_proba(val2_X)\n\nlr_loss = sk_met.log_loss(val_tv_y, y_hat)\nlr_loss","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:33:57.460053Z","iopub.execute_input":"2021-06-16T21:33:57.460357Z","iopub.status.idle":"2021-06-16T21:40:04.881309Z","shell.execute_reply.started":"2021-06-16T21:33:57.460328Z","shell.execute_reply":"2021-06-16T21:40:04.880312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nmodel = lgbm.LGBMClassifier(random_state=RANDOM_STATE)\n\nmodel.fit(tr2_X, train_tv_y)\n\ny_hat = model.predict_proba(val2_X)\n\nlgbm_loss = sk_met.log_loss(val_tv_y, y_hat)\nlgbm_loss","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:40:04.882707Z","iopub.execute_input":"2021-06-16T21:40:04.883203Z","iopub.status.idle":"2021-06-16T21:40:16.4732Z","shell.execute_reply.started":"2021-06-16T21:40:04.883151Z","shell.execute_reply":"2021-06-16T21:40:16.472227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_list_1.append(['Rotate', lr_loss, lgbm_loss])","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:40:16.474462Z","iopub.execute_input":"2021-06-16T21:40:16.474746Z","iopub.status.idle":"2021-06-16T21:40:16.479863Z","shell.execute_reply.started":"2021-06-16T21:40:16.474717Z","shell.execute_reply":"2021-06-16T21:40:16.478422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This method hurt the boosting model, which is no surprise.  It is more interesting that it did not hurt the logistic regression model.","metadata":{}},{"cell_type":"markdown","source":"# Positive Encoding\n\nAll feature values are simplified to 0 or 1--0 if it was originally 0 and 1 if it was positive. We will call this \"positive encoding\" in this study.\n\nThis was not originally a serious suggestion by itself; the point was to see how much value remained when the data were simplified in this way.","metadata":{}},{"cell_type":"code","source":"tr2_ar = train_tv_X.values > 0\ntr2_X = pd.DataFrame(tr2_ar.astype('int32'), columns=train_data.columns, index=train_tv_X.index)\n\nval2_ar = val_tv_X.values > 0\nval2_X = pd.DataFrame(val2_ar.astype('int32'), columns=train_data.columns, index=val_tv_X.index)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:40:16.48154Z","iopub.execute_input":"2021-06-16T21:40:16.48188Z","iopub.status.idle":"2021-06-16T21:40:16.515117Z","shell.execute_reply.started":"2021-06-16T21:40:16.481824Z","shell.execute_reply":"2021-06-16T21:40:16.514046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nmodel = sk_lm.LogisticRegression(solver='lbfgs', max_iter=1000)\n\nmodel.fit(tr2_X, train_tv_y)\n\ny_hat = model.predict_proba(val2_X)\n\nlr_loss = sk_met.log_loss(val_tv_y, y_hat)\nlr_loss","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:40:16.516585Z","iopub.execute_input":"2021-06-16T21:40:16.516881Z","iopub.status.idle":"2021-06-16T21:40:46.356Z","shell.execute_reply.started":"2021-06-16T21:40:16.516852Z","shell.execute_reply":"2021-06-16T21:40:46.354805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nmodel = lgbm.LGBMClassifier(random_state=RANDOM_STATE)\n\nmodel.fit(tr2_X, train_tv_y)\n\ny_hat = model.predict_proba(val2_X)\n\nlgbm_loss = sk_met.log_loss(val_tv_y, y_hat)\nlgbm_loss","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:40:46.357572Z","iopub.execute_input":"2021-06-16T21:40:46.357868Z","iopub.status.idle":"2021-06-16T21:41:00.948594Z","shell.execute_reply.started":"2021-06-16T21:40:46.35784Z","shell.execute_reply":"2021-06-16T21:41:00.947552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_list_1.append(['PosEnc', lr_loss, lgbm_loss])","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:41:00.949965Z","iopub.execute_input":"2021-06-16T21:41:00.95029Z","iopub.status.idle":"2021-06-16T21:41:00.955242Z","shell.execute_reply.started":"2021-06-16T21:41:00.950259Z","shell.execute_reply":"2021-06-16T21:41:00.954189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that logistic regression improved in spite of the lost data.  This suggests that zero is not simply one end of the range of numbers, but a special value of its own.  It is remarkable that with this simplification, logistic regression can do as well as boosting did on the original data.  I have looked at this in more detail in another [notebook](https://www.kaggle.com/bruceharold/the-power-of-positive-encoding).","metadata":{}},{"cell_type":"markdown","source":"# Positive Encoding Plus Original","metadata":{}},{"cell_type":"code","source":"tr2_ar = train_tv_X.values > 0\ntr1_X = pd.DataFrame(tr2_ar.astype('int32'), columns='pos_' + train_data.columns, index=train_tv_X.index)\ntr2_X = pd.concat([train_tv_X, tr1_X], axis=1)\n\nval2_ar = val_tv_X.values > 0\nval1_X = pd.DataFrame(val2_ar.astype('int32'), columns='pos_' + train_data.columns, index=val_tv_X.index)\nval2_X = pd.concat([val_tv_X, val1_X], axis=1)\n\ntr2_X","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:41:00.956709Z","iopub.execute_input":"2021-06-16T21:41:00.957001Z","iopub.status.idle":"2021-06-16T21:41:01.07502Z","shell.execute_reply.started":"2021-06-16T21:41:00.956965Z","shell.execute_reply":"2021-06-16T21:41:01.073878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nmodel = sk_lm.LogisticRegression(solver='lbfgs', max_iter=1000)\n\nmodel.fit(tr2_X, train_tv_y)\n\ny_hat = model.predict_proba(val2_X)\n\nlr_loss = sk_met.log_loss(val_tv_y, y_hat)\nlr_loss","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:41:01.07651Z","iopub.execute_input":"2021-06-16T21:41:01.077145Z","iopub.status.idle":"2021-06-16T21:44:35.50297Z","shell.execute_reply.started":"2021-06-16T21:41:01.077076Z","shell.execute_reply":"2021-06-16T21:44:35.502242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nmodel = lgbm.LGBMClassifier(random_state=RANDOM_STATE)\n\nmodel.fit(tr2_X, train_tv_y)\n\ny_hat = model.predict_proba(val2_X)\n\nlgbm_loss = sk_met.log_loss(val_tv_y, y_hat)\nlgbm_loss","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:44:35.504058Z","iopub.execute_input":"2021-06-16T21:44:35.504457Z","iopub.status.idle":"2021-06-16T21:44:55.210603Z","shell.execute_reply.started":"2021-06-16T21:44:35.50443Z","shell.execute_reply":"2021-06-16T21:44:55.209662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_list_1.append(['PosEnc+', lr_loss, lgbm_loss])","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:44:55.211839Z","iopub.execute_input":"2021-06-16T21:44:55.212144Z","iopub.status.idle":"2021-06-16T21:44:55.216398Z","shell.execute_reply.started":"2021-06-16T21:44:55.212105Z","shell.execute_reply":"2021-06-16T21:44:55.215249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Adding in the original data did not improve the scores very much, again underlining how much this dataset depends on the zero vs positive distinction.","metadata":{}},{"cell_type":"markdown","source":"# Embedding\n\nHere we use a pytorch neural network to do the original training on the embedding, and then we will use the embedding layer to encode the data for our models.","metadata":{}},{"cell_type":"code","source":"# Get numeric version of y\n\ntr2_y = le_targ.transform(train_tv_y)\nval2_y = le_targ.transform(val_tv_y)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:44:55.217881Z","iopub.execute_input":"2021-06-16T21:44:55.218488Z","iopub.status.idle":"2021-06-16T21:44:55.29181Z","shell.execute_reply.started":"2021-06-16T21:44:55.218448Z","shell.execute_reply":"2021-06-16T21:44:55.291066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Train model with embedding for sake of the embedding layer.\n\ndev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\nembeddings = np.max(train_tv_X.values) + 1\n\nemb_model = EmbeddingModel(75, 9, drop_rate=0.3, embeddings=embeddings, embedding_dim=4)\n\nnn_train(train_tv_X.values, tr2_y, emb_model, 10)\n\n# Check that model is basically trained.  We are not concerned about a \"good\" score here, just being in the ballpark.\n\nval_X_ten = torch.from_numpy(val_tv_X.values).to(dev)\ny_hat_ten = nn.Softmax(dim=1)(emb_model(val_X_ten))\ny_hat = y_hat_ten.detach().cpu().numpy()\n\nsk_met.log_loss(val2_y, y_hat)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:44:55.293045Z","iopub.execute_input":"2021-06-16T21:44:55.293622Z","iopub.status.idle":"2021-06-16T21:48:54.447889Z","shell.execute_reply.started":"2021-06-16T21:44:55.29358Z","shell.execute_reply":"2021-06-16T21:48:54.446899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr_X_ten = torch.from_numpy(train_tv_X.values).to(dev)\ntr2_X = emb_model.encode(tr_X_ten).detach().cpu().numpy()\n\nval_X_ten = torch.from_numpy(val_tv_X.values).to(dev)\nval2_X = emb_model.encode(val_X_ten).detach().cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:48:54.449517Z","iopub.execute_input":"2021-06-16T21:48:54.449934Z","iopub.status.idle":"2021-06-16T21:48:54.576329Z","shell.execute_reply.started":"2021-06-16T21:48:54.44989Z","shell.execute_reply":"2021-06-16T21:48:54.575302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nmodel = sk_lm.LogisticRegression(solver='lbfgs', max_iter=1000)\n\nmodel.fit(tr2_X, train_tv_y)\n\ny_hat = model.predict_proba(val2_X)\n\nlr_loss = sk_met.log_loss(val_tv_y, y_hat)\nlr_loss","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:48:54.577643Z","iopub.execute_input":"2021-06-16T21:48:54.577947Z","iopub.status.idle":"2021-06-16T21:53:08.580732Z","shell.execute_reply.started":"2021-06-16T21:48:54.577912Z","shell.execute_reply":"2021-06-16T21:53:08.579835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nmodel = lgbm.LGBMClassifier(random_state=RANDOM_STATE)\n\nmodel.fit(tr2_X, train_tv_y)\n\ny_hat = model.predict_proba(val2_X)\n\nlgbm_loss = sk_met.log_loss(val_tv_y, y_hat)\nlgbm_loss","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:53:08.586925Z","iopub.execute_input":"2021-06-16T21:53:08.587225Z","iopub.status.idle":"2021-06-16T21:53:59.769305Z","shell.execute_reply.started":"2021-06-16T21:53:08.587195Z","shell.execute_reply":"2021-06-16T21:53:59.768267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_list_1.append(['Embedding', lr_loss, lgbm_loss])","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:53:59.770567Z","iopub.execute_input":"2021-06-16T21:53:59.770837Z","iopub.status.idle":"2021-06-16T21:53:59.77487Z","shell.execute_reply.started":"2021-06-16T21:53:59.77081Z","shell.execute_reply":"2021-06-16T21:53:59.773969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Summary","metadata":{}},{"cell_type":"code","source":"print(f'Equal Class Loss:    {eq_loss}')\nprint(f'Weighted Class Loss: {wt_loss}')\n\nloss_df = pd.DataFrame(loss_list_1, columns=['Method', 'LR_Loss', 'LGBM_Loss'])\nval_min = np.min(loss_df.iloc[:, 1:].values)\nval_max = np.max(loss_df.iloc[:, 1:].values)\nstyle.Styler(loss_df, precision=4).background_gradient(cmap='viridis', vmin=val_min, vmax=val_max)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T21:53:59.776113Z","iopub.execute_input":"2021-06-16T21:53:59.776397Z","iopub.status.idle":"2021-06-16T21:53:59.807502Z","shell.execute_reply.started":"2021-06-16T21:53:59.77636Z","shell.execute_reply":"2021-06-16T21:53:59.806336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that logistic regression improved from a number of these feature engineering methods.  In most cases where we found improvement, the amount of improvement was very similar.  Boosting received little improvement from any of the methods tried here.\n\nMany of these FE methods can be tweaked, so some that show no gain in this study may be useful if they are handled in a different way.","metadata":{}}]}