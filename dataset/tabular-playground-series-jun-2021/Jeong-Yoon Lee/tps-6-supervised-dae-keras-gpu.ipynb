{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# UPDATE - 6/12/2021\n\nIn today's `Kaggler` v0.9.13 release, I added transfer learning between `DAE`/`SDAE`. So you can train `SDAE` only with training data, then initialize `DAE` with the trained `SDAE` model, train it with both trainging and test data, or vice versa.\n\nExample code is as follows:\n```python\n# train supervised DAE only with trianing data\nsdae = SDAE(cat_cols=cat_cols, num_cols=num_cols, encoding_dim=encoding_dim, random_state=RANDOM_SEED)\n_ = sdae.fit_transform(trn[feature_cols], trn[TARGET_COL])\n\n# initialize unsupervied DAE and train it with both training and test data\ndae = DAE(cat_cols=cat_cols, num_cols=num_cols, encoding_dim=encoding_dim, random_state=RANDOM_SEED,\n          pretrained_model=sdae, freeze_embedding=True)\n_ = dae.fit_transform(df[feature_cols])\n\n# initialize another supervised DAE and train it with training data\nsdae2 = SDAE(cat_cols=cat_cols, num_cols=num_cols, encoding_dim=encoding_dim, random_state=RANDOM_SEED,\n             pretrained_model=dae, freeze_embedding=False)\n_ = sdae2.fit_transform(trn[feature_cols], trn[TARGET_COL])\n```\n\nHope it helps!\n\n# TPS 6 - Supervised DAE + Keras (GPU)\n\nIn this notebook, I will show how to train a neural network model with supervised denoising autoencoder (SDAE) and target encoded features in Keras (GPU).\n\nI added the supervised version of DAE, `SDAE` to `Kaggler` in today's v0.9.8 release. At Kaggle, DAE is mostly used as a unsupervised feature extraction method. However, it's possible to train DAE in a supervised manner with a target variable.\n\nTo transform features with `SDAE`, you can do as follows:\n\n```python\nsdae = SDAE(cat_cols=feature_cols, encoding_dim=encoding_dim, n_layer=1, noise_std=.001, random_state=seed)\nsdae.fit(trn[feature_cols], y)\nX = sdae.transform(df[feature_cols])\n```\n\nThe contents of the notebooks are organized as follows:\n\n1. Installing and loading libraries: installs `Kaggler` and load data and libraries\n2. Feature engineering: shows how to transform features with target encoding with `Kaggler`\n3. Model definition and training: shows how to define and train a NN model with skip connection in `Keras`\n4. Submission\n\nEnjoy~!","metadata":{}},{"cell_type":"markdown","source":"# Part 1. Loading Libraries and Data","metadata":{}},{"cell_type":"code","source":"import gc\nimport joblib\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport seaborn as sns\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K\nfrom warnings import simplefilter","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-12T14:21:47.063776Z","iopub.execute_input":"2021-06-12T14:21:47.064146Z","iopub.status.idle":"2021-06-12T14:21:52.914544Z","shell.execute_reply.started":"2021-06-12T14:21:47.064069Z","shell.execute_reply":"2021-06-12T14:21:52.913424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# limit the GPU memory growth\ngpu = tf.config.list_physical_devices('GPU')\nprint(\"Num GPUs Available: \", len(gpu))\nif len(gpu) > 0:\n    tf.config.experimental.set_memory_growth(gpu[0], True)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T14:21:52.916394Z","iopub.execute_input":"2021-06-12T14:21:52.916984Z","iopub.status.idle":"2021-06-12T14:21:53.098788Z","shell.execute_reply.started":"2021-06-12T14:21:52.916946Z","shell.execute_reply":"2021-06-12T14:21:53.09791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install kaggler","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-12T14:21:53.100929Z","iopub.execute_input":"2021-06-12T14:21:53.101613Z","iopub.status.idle":"2021-06-12T14:22:32.712578Z","shell.execute_reply.started":"2021-06-12T14:21:53.101572Z","shell.execute_reply":"2021-06-12T14:22:32.711668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import kaggler\nfrom kaggler.model import AutoLGB\nfrom kaggler.preprocessing import DAE, SDAE, TargetEncoder, LabelEncoder\nprint(kaggler.__version__)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T14:22:32.714425Z","iopub.execute_input":"2021-06-12T14:22:32.714776Z","iopub.status.idle":"2021-06-12T14:22:35.533153Z","shell.execute_reply.started":"2021-06-12T14:22:32.714735Z","shell.execute_reply":"2021-06-12T14:22:35.532276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.style.use('fivethirtyeight')\npd.set_option('max_columns', 100)\nsimplefilter('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-06-12T14:22:35.534434Z","iopub.execute_input":"2021-06-12T14:22:35.534937Z","iopub.status.idle":"2021-06-12T14:22:35.540207Z","shell.execute_reply.started":"2021-06-12T14:22:35.534896Z","shell.execute_reply":"2021-06-12T14:22:35.53939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_name = 'le_te_sdae'\nalgo_name = 'fc'\nversion = 11\nmodel_name = f'{algo_name}_{feature_name}_v{version}'\n\ndata_dir = Path('../input/tabular-playground-series-jun-2021')\ntrain_file = data_dir / 'train.csv'\ntest_file = data_dir / 'test.csv'\nsample_file = data_dir / 'sample_submission.csv'\n\nbuild_dir = Path('.')\npredict_val_file = build_dir / f'{model_name}.val.txt'\npredict_tst_file = build_dir / f'{model_name}.tst.txt'\nsubmission_file = build_dir / f'{model_name}.sub.csv'\n\nid_col = 'id'\ntarget_col = 'target'\n\nn_fold = 5\nencoding_dim = 128\nn_encoder = 3\nseed = 42\nn_class = 9\nn_stop = 5\nn_epoch = 100\nn_emb = 16\nn_hidden_unit = 128 \ndropout = .3\nratio = 4\nbatch_size = 64 * ratio\nlr = 0.0001 * ratio","metadata":{"execution":{"iopub.status.busy":"2021-06-12T14:33:30.006501Z","iopub.execute_input":"2021-06-12T14:33:30.006861Z","iopub.status.idle":"2021-06-12T14:33:30.014141Z","shell.execute_reply.started":"2021-06-12T14:33:30.006825Z","shell.execute_reply":"2021-06-12T14:33:30.012969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trn = pd.read_csv(train_file, index_col=id_col)\ntst = pd.read_csv(test_file, index_col=id_col)\nsub = pd.read_csv(sample_file, index_col=id_col)\nprint(trn.shape, tst.shape, sub.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T14:22:35.554208Z","iopub.execute_input":"2021-06-12T14:22:35.55456Z","iopub.status.idle":"2021-06-12T14:22:37.384949Z","shell.execute_reply.started":"2021-06-12T14:22:35.554515Z","shell.execute_reply":"2021-06-12T14:22:37.383485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = trn[target_col].str.split('_').str[1].astype(int) - 1\nn_trn = trn.shape[0]\ndf = pd.concat([trn.drop(target_col, axis=1), tst], axis=0)\nprint(df.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T14:22:37.387451Z","iopub.execute_input":"2021-06-12T14:22:37.387804Z","iopub.status.idle":"2021-06-12T14:22:38.141573Z","shell.execute_reply.started":"2021-06-12T14:22:37.387774Z","shell.execute_reply":"2021-06-12T14:22:38.140669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_cols = df.columns.tolist()\nprint(len(feature_cols))","metadata":{"execution":{"iopub.status.busy":"2021-06-12T14:22:38.14319Z","iopub.execute_input":"2021-06-12T14:22:38.143712Z","iopub.status.idle":"2021-06-12T14:22:38.148952Z","shell.execute_reply.started":"2021-06-12T14:22:38.143655Z","shell.execute_reply":"2021-06-12T14:22:38.14808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 2. Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"## DAE vs. SDAE Comparison/Transfer Learning","metadata":{}},{"cell_type":"code","source":"sdae = SDAE(cat_cols=feature_cols, encoding_dim=encoding_dim, n_layer=1, n_encoder=n_encoder, noise_std=.01, \n            batch_size=batch_size, learning_rate=lr, n_epoch=3, random_state=seed, label_encoding=True)\nsdae.fit(trn[feature_cols], y)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-12T14:28:41.552155Z","iopub.execute_input":"2021-06-12T14:28:41.552595Z","iopub.status.idle":"2021-06-12T14:29:33.979503Z","shell.execute_reply.started":"2021-06-12T14:28:41.552555Z","shell.execute_reply":"2021-06-12T14:29:33.978609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dae = DAE(cat_cols=feature_cols, encoding_dim=encoding_dim, n_layer=1, n_encoder=n_encoder, noise_std=.01, \n          batch_size=batch_size, learning_rate=lr, n_epoch=10, random_state=seed, label_encoding=True,\n          pretrained_model=sdae, freeze_embedding=True)\ndae.fit(df[feature_cols])","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-12T14:29:33.983212Z","iopub.execute_input":"2021-06-12T14:29:33.983489Z","iopub.status.idle":"2021-06-12T14:32:28.78456Z","shell.execute_reply.started":"2021-06-12T14:29:33.983462Z","shell.execute_reply":"2021-06-12T14:32:28.783747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = dae.transform(df[feature_cols].sample(n=1_000, random_state=seed))\nprint(X.shape)\nax = sns.heatmap(X)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T14:32:28.787314Z","iopub.execute_input":"2021-06-12T14:32:28.787636Z","iopub.status.idle":"2021-06-12T14:32:31.417233Z","shell.execute_reply.started":"2021-06-12T14:32:28.787602Z","shell.execute_reply":"2021-06-12T14:32:31.416408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sdae_cols = [f'sdae_{i + 1}' for i in range(encoding_dim * n_encoder)]","metadata":{"execution":{"iopub.status.busy":"2021-06-12T14:33:34.534516Z","iopub.execute_input":"2021-06-12T14:33:34.534875Z","iopub.status.idle":"2021-06-12T14:33:34.539472Z","shell.execute_reply.started":"2021-06-12T14:33:34.534841Z","shell.execute_reply":"2021-06-12T14:33:34.538549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le = LabelEncoder(min_obs=50)\ndf_le = le.fit_transform(df[feature_cols])\ndf_le.columns = [f'le_{x}' for x in df.columns]","metadata":{"execution":{"iopub.status.busy":"2021-06-12T14:33:36.443567Z","iopub.execute_input":"2021-06-12T14:33:36.443932Z","iopub.status.idle":"2021-06-12T14:33:37.289363Z","shell.execute_reply.started":"2021-06-12T14:33:36.443897Z","shell.execute_reply":"2021-06-12T14:33:37.288519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)\nte = TargetEncoder(cv=cv)\nte.fit(trn[feature_cols], y)\ndf_te = te.transform(df[feature_cols])\ndf_te.columns = [f'te_{x}' for x in df.columns]","metadata":{"execution":{"iopub.status.busy":"2021-06-12T14:33:37.290846Z","iopub.execute_input":"2021-06-12T14:33:37.291198Z","iopub.status.idle":"2021-06-12T14:33:54.442115Z","shell.execute_reply.started":"2021-06-12T14:33:37.29116Z","shell.execute_reply":"2021-06-12T14:33:54.441262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_df = pd.concat([df_le, df_te], axis=1)\nprint(all_df.shape)\nall_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T14:33:54.443776Z","iopub.execute_input":"2021-06-12T14:33:54.444089Z","iopub.status.idle":"2021-06-12T14:33:54.759242Z","shell.execute_reply.started":"2021-06-12T14:33:54.444061Z","shell.execute_reply":"2021-06-12T14:33:54.758252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_feature_cols = all_df.columns.tolist()\nn_feature = len(all_feature_cols)\ncat_cols = df_le.columns.tolist()\nnum_cols = [x for x in all_feature_cols if x not in cat_cols]\nn_cat_col = len(cat_cols)\nn_num_col = len(num_cols)\nprint(n_feature, n_cat_col, n_num_col)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T14:33:54.760861Z","iopub.execute_input":"2021-06-12T14:33:54.76122Z","iopub.status.idle":"2021-06-12T14:33:54.768752Z","shell.execute_reply.started":"2021-06-12T14:33:54.761182Z","shell.execute_reply":"2021-06-12T14:33:54.767477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 3. Keras NN Model Training","metadata":{}},{"cell_type":"markdown","source":"We will use a neural network model with skip connections.","metadata":{}},{"cell_type":"code","source":"def build_model(n_emb=16, n_hidden_unit=128, dropout=.3):\n    cat_inputs = []\n    embs = []\n    for i, col in enumerate(cat_cols):\n        inp = keras.layers.Input((1,), name=f'{col}')\n        emb = keras.layers.Embedding(input_dim=all_df[col].nunique(), output_dim=n_emb)(inp)\n        cat_inputs.append(inp)\n        embs.append(emb)\n\n    num_inputs = keras.layers.Input((len(num_cols) + len(sdae_cols),))\n    \n    inputs = cat_inputs + [num_inputs]\n    merged_inputs = keras.layers.Concatenate()(inputs)\n    x = keras.layers.Dense(n_hidden_unit, 'relu')(merged_inputs)\n    x = keras.layers.Dropout(dropout)(x)\n    ox = x\n    \n    x = keras.layers.Dense(n_hidden_unit, 'relu')(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Add()([ox, x])\n    \n    x = keras.layers.Dense(n_hidden_unit, 'relu')(x)\n    x = keras.layers.Dropout(dropout)(x)\n    ox = x\n    \n    x = keras.layers.Dense(n_hidden_unit, 'relu')(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Add()([ox, x])\n\n    x = keras.layers.Dense(n_hidden_unit, 'relu')(x)\n    x = keras.layers.Dropout(dropout)(x)\n\n    outputs = keras.layers.Dense(n_class, 'softmax')(x)\n    \n    model = keras.Model(inputs, outputs)\n    model.compile(optimizer=keras.optimizers.Adam(lr), loss='sparse_categorical_crossentropy')\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-06-12T14:33:54.770375Z","iopub.execute_input":"2021-06-12T14:33:54.770797Z","iopub.status.idle":"2021-06-12T14:33:54.785191Z","shell.execute_reply.started":"2021-06-12T14:33:54.770756Z","shell.execute_reply":"2021-06-12T14:33:54.784291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = build_model(n_emb, n_hidden_unit, dropout)\nmodel.summary()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-12T14:34:12.436994Z","iopub.execute_input":"2021-06-12T14:34:12.43733Z","iopub.status.idle":"2021-06-12T14:34:13.075906Z","shell.execute_reply.started":"2021-06-12T14:34:12.437296Z","shell.execute_reply":"2021-06-12T14:34:13.075104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To avoid overfitting, we will generate `SDAE` features for each cross-validation fold.","metadata":{}},{"cell_type":"code","source":"es = keras.callbacks.EarlyStopping(patience=n_stop, restore_best_weights=True)\nrlr  = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=n_stop, cooldown=0, min_lr=1e-7)\n\ncv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)\n\nX = all_df.iloc[:n_trn].values\nX_tst = all_df.iloc[n_trn:].values\n\nP = np.zeros((n_trn, n_class), dtype=float)\nP_tst = np.zeros((X_tst.shape[0], n_class), dtype=float)\nfor i, (i_trn, i_val) in enumerate(cv.split(X, y), 1):\n    y_trn, y_val = y[i_trn], y[i_val]\n    \n    sdae = SDAE(cat_cols=df.columns.tolist(), encoding_dim=encoding_dim, n_layer=1, n_encoder=3,\n                noise_std=.01, batch_size=batch_size, learning_rate=lr, n_epoch=20, random_state=seed, \n                label_encoding=True, pretrained_model=dae, freeze_embedding=False)\n    X_sdae_trn = sdae.fit_transform(trn[feature_cols].iloc[i_trn], y_trn,\n                                    validation_data=(trn[feature_cols].iloc[i_val], y_val))\n    X_sdae_val = sdae.transform(trn[feature_cols].iloc[i_val])\n    X_sdae_tst = sdae.transform(tst[feature_cols])\n    \n    X_trn_i = np.hstack([X[i_trn], X_sdae_trn])\n    X_val_i = np.hstack([X[i_val], X_sdae_val])\n    X_tst_i = np.hstack([X_tst, X_sdae_tst])\n    \n    joblib.dump(X_trn_i, str(build_dir / f'{feature_name}.trn{i}.joblib'))\n    joblib.dump(X_val_i, str(build_dir / f'{feature_name}.val{i}.joblib'))\n    joblib.dump(X_tst_i, str(build_dir / f'{feature_name}.tst{i}.joblib'))\n    \n    model = build_model()\n    history = model.fit([X_trn_i[:, i] for i in range(n_cat_col)] + [X_trn_i[:, n_cat_col:]], \n                        y_trn,\n                        validation_data=([X_val_i[:, i] for i in range(n_cat_col)] + [X_val_i[:, n_cat_col:]], \n                                         y_val),\n                        epochs=n_epoch, batch_size=batch_size, callbacks=[es, rlr], verbose=0)\n    P[i_val] = model.predict([X_val_i[:, i] for i in range(n_cat_col)] + [X_val_i[:, n_cat_col:]])\n    P_tst += model.predict([X_tst_i[:, i] for i in range(n_cat_col)] + [X_tst_i[:, n_cat_col:]]) / n_fold\n    print(f'CV #{i} Loss: {log_loss(y[i_val], P[i_val]):.6f}')\n\n    del model, history\n    gc.collect()\n    K.clear_session()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-12T14:34:18.105431Z","iopub.execute_input":"2021-06-12T14:34:18.105789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'CV Loss: {log_loss(y, P):.6f}')\nnp.savetxt(predict_val_file, P, fmt='%.6f')\nnp.savetxt(predict_tst_file, P_tst, fmt='%.6f')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 4. Submission","metadata":{}},{"cell_type":"code","source":"sub[sub.columns] = P_tst\nsub.to_csv(submission_file)\nsub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hope this helps.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}