{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Adversarial Validation - TPS 6","metadata":{}},{"cell_type":"markdown","source":"Adversarial validation is a method to check how different feature distributions between the training and test data. We train a binary classifier with a new target variable indicating whether a sample belongs to the test data (1) or not (0).\n\nFrom the [EDA](https://www.kaggle.com/subinium/tps-jun-this-is-original-eda-viz) by @subinium, we already saw that feature distrubutions across the training and test data are quite similar, but here, let's use adversarial validation to validate the finding.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import lightgbm as lgb\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport seaborn\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom warnings import simplefilter","metadata":{"execution":{"iopub.status.busy":"2021-06-01T20:18:10.113013Z","iopub.execute_input":"2021-06-01T20:18:10.113581Z","iopub.status.idle":"2021-06-01T20:18:12.177089Z","shell.execute_reply.started":"2021-06-01T20:18:10.113495Z","shell.execute_reply":"2021-06-01T20:18:12.176153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install kaggler","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-01T20:18:12.178452Z","iopub.execute_input":"2021-06-01T20:18:12.17889Z","iopub.status.idle":"2021-06-01T20:18:58.989507Z","shell.execute_reply.started":"2021-06-01T20:18:12.17886Z","shell.execute_reply":"2021-06-01T20:18:58.988655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import kaggler\nfrom kaggler.model import AutoLGB\nprint(kaggler.__version__)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T20:18:58.991692Z","iopub.execute_input":"2021-06-01T20:18:58.992034Z","iopub.status.idle":"2021-06-01T20:18:59.554776Z","shell.execute_reply.started":"2021-06-01T20:18:58.992Z","shell.execute_reply":"2021-06-01T20:18:59.553699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.style.use('fivethirtyeight')\npd.set_option('max_columns', 100)\nsimplefilter('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-06-01T20:18:59.556811Z","iopub.execute_input":"2021-06-01T20:18:59.557274Z","iopub.status.idle":"2021-06-01T20:18:59.562801Z","shell.execute_reply.started":"2021-06-01T20:18:59.557228Z","shell.execute_reply":"2021-06-01T20:18:59.561873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = Path('../input/tabular-playground-series-jun-2021')\ntrain_file = data_dir / 'train.csv'\ntest_file = data_dir / 'test.csv'\nsample_file = data_dir / 'sample_submission.csv'\n\nid_col = 'id'\ntarget_col = 'target'\n\nn_fold = 5\nseed = 42","metadata":{"execution":{"iopub.status.busy":"2021-06-01T20:18:59.564505Z","iopub.execute_input":"2021-06-01T20:18:59.564895Z","iopub.status.idle":"2021-06-01T20:18:59.573428Z","shell.execute_reply.started":"2021-06-01T20:18:59.564856Z","shell.execute_reply":"2021-06-01T20:18:59.57266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trn = pd.read_csv(train_file, index_col=id_col)\ntst = pd.read_csv(test_file, index_col=id_col)\nsub = pd.read_csv(sample_file, index_col=id_col)\nprint(trn.shape, tst.shape, sub.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T20:18:59.574655Z","iopub.execute_input":"2021-06-01T20:18:59.575052Z","iopub.status.idle":"2021-06-01T20:19:01.864149Z","shell.execute_reply.started":"2021-06-01T20:18:59.575011Z","shell.execute_reply":"2021-06-01T20:19:01.863105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_trn = trn.shape[0]\ndf = pd.concat([trn.drop(target_col, axis=1), tst], axis=0)\nprint(df.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T20:19:01.865655Z","iopub.execute_input":"2021-06-01T20:19:01.866127Z","iopub.status.idle":"2021-06-01T20:19:02.001785Z","shell.execute_reply.started":"2021-06-01T20:19:01.866084Z","shell.execute_reply":"2021-06-01T20:19:02.000815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)\nX = df\ny = pd.Series(np.concatenate([np.zeros(n_trn,), np.ones(df.shape[0] - n_trn,)]))\np = np.zeros_like(y, dtype=float)\nfor i, (i_trn, i_val) in enumerate(cv.split(X, y)):\n    if i == 0:\n        clf = AutoLGB(objective='binary', metric='auc', random_state=seed, feature_selection=False)\n        clf.tune(X.iloc[i_trn], y[i_trn])\n        features = clf.features\n        params = clf.params\n        n_best = clf.n_best\n        print(f'{n_best}')\n        print(f'{params}')\n        print(f'{features}')\n    \n    trn_data = lgb.Dataset(X.iloc[i_trn], y[i_trn])\n    val_data = lgb.Dataset(X.iloc[i_val], y[i_val])\n    clf = lgb.train(params, trn_data, n_best, val_data, verbose_eval=100)\n    p[i_val] = clf.predict(X.iloc[i_val])\n    print(f'CV #{i + 1} AUC: {roc_auc_score(y[i_val], p[i_val]):.6f}')","metadata":{"execution":{"iopub.status.busy":"2021-06-01T20:19:02.003023Z","iopub.execute_input":"2021-06-01T20:19:02.003314Z","iopub.status.idle":"2021-06-01T20:19:17.444727Z","shell.execute_reply.started":"2021-06-01T20:19:02.003287Z","shell.execute_reply":"2021-06-01T20:19:17.443663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'CV AUC: {roc_auc_score(y, p):.6f}')","metadata":{"execution":{"iopub.status.busy":"2021-06-01T20:19:17.448015Z","iopub.execute_input":"2021-06-01T20:19:17.448384Z","iopub.status.idle":"2021-06-01T20:19:17.539988Z","shell.execute_reply.started":"2021-06-01T20:19:17.448352Z","shell.execute_reply":"2021-06-01T20:19:17.539036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n\nAdverarial validation AUC is close to 50%. In other words, it confirms that the feature distributions between the training and test data are similar.\n\nLet's have some fun. :)","metadata":{}}]}