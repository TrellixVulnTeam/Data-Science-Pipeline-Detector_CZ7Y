{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gc\n\nimport pandas as pd\nimport numpy as np\n\nfrom scipy.sparse import hstack\n\nfrom sklearn.preprocessing import OrdinalEncoder,OneHotEncoder,StandardScaler, MinMaxScaler, LabelEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score,roc_auc_score, log_loss\nfrom sklearn.model_selection import StratifiedKFold, KFold, cross_val_score,cross_validate\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectKBest, f_classif, chi2, SelectPercentile\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.calibration import CalibratedClassifierCV, calibration_curve\n\nimport optuna\nimport lightgbm as lgbm\nimport xgboost as xgb\nimport pickle\nimport category_encoders as ce\nfrom catboost import CatBoostClassifier, Pool\n\nimport string\nimport re\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport tensorflow.keras.backend as K\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import activations,callbacks\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import initializers\n\nfrom keras.models import Model","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-30T05:37:07.314384Z","iopub.execute_input":"2021-06-30T05:37:07.314763Z","iopub.status.idle":"2021-06-30T05:37:15.479368Z","shell.execute_reply.started":"2021-06-30T05:37:07.314681Z","shell.execute_reply":"2021-06-30T05:37:15.478371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-jun-2021/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-jun-2021/test.csv')\nsample_sub = pd.read_csv('../input/tabular-playground-series-jun-2021/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T05:37:15.480931Z","iopub.execute_input":"2021-06-30T05:37:15.48125Z","iopub.status.idle":"2021-06-30T05:37:17.501436Z","shell.execute_reply.started":"2021-06-30T05:37:15.481204Z","shell.execute_reply":"2021-06-30T05:37:17.5005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T05:37:17.502983Z","iopub.execute_input":"2021-06-30T05:37:17.503249Z","iopub.status.idle":"2021-06-30T05:37:17.534701Z","shell.execute_reply.started":"2021-06-30T05:37:17.503226Z","shell.execute_reply":"2021-06-30T05:37:17.533826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T05:37:17.535952Z","iopub.execute_input":"2021-06-30T05:37:17.536228Z","iopub.status.idle":"2021-06-30T05:37:17.552382Z","shell.execute_reply.started":"2021-06-30T05:37:17.536202Z","shell.execute_reply":"2021-06-30T05:37:17.551528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Regression","metadata":{}},{"cell_type":"markdown","source":"X = train.drop(['id', 'target'], axis=1, inplace=False).copy()\ny = train['target'].values\n\nX_test = test.drop(['id'], axis=1, inplace=False).copy()\nrandom_seed = 0","metadata":{"execution":{"iopub.status.busy":"2021-06-29T15:38:34.936188Z","iopub.execute_input":"2021-06-29T15:38:34.93656Z","iopub.status.idle":"2021-06-29T15:38:35.061455Z","shell.execute_reply.started":"2021-06-29T15:38:34.93652Z","shell.execute_reply":"2021-06-29T15:38:35.06059Z"}}},{"cell_type":"markdown","source":"encoder = OneHotEncoder()\nall_encoded = encoder.fit_transform(X.append(X_test))\n\nX = all_encoded.tocsr()[0:len(X)]\nX_test = all_encoded[len(train):]","metadata":{"execution":{"iopub.status.busy":"2021-06-29T15:38:35.062697Z","iopub.execute_input":"2021-06-29T15:38:35.063064Z","iopub.status.idle":"2021-06-29T15:38:37.361824Z","shell.execute_reply.started":"2021-06-29T15:38:35.063029Z","shell.execute_reply":"2021-06-29T15:38:37.360973Z"}}},{"cell_type":"markdown","source":"params = {\n    'penalty': 'l2',\n    'multi_class':'ovr',\n    'solver':'lbfgs',\n    'C':0.01,\n    'max_iter':10000,\n    'class_weight':None\n}","metadata":{"execution":{"iopub.status.busy":"2021-06-29T15:38:37.363107Z","iopub.execute_input":"2021-06-29T15:38:37.36348Z","iopub.status.idle":"2021-06-29T15:38:37.369471Z","shell.execute_reply.started":"2021-06-29T15:38:37.363437Z","shell.execute_reply":"2021-06-29T15:38:37.368598Z"}}},{"cell_type":"markdown","source":"name = 'Logistic_regression'\nk = 5\nseed_list = [0, 1, 2]\nkf = StratifiedKFold(n_splits = k, shuffle=True, random_state=random_seed)\noof = np.zeros((len(train), 9))\ntest_preds_list = []\nscore_list = []\nfold = 1\n\nsplits = list(kf.split(X, y))\nfor train_idx, val_idx in splits:\n    X_train, X_val = X[train_idx], X[val_idx]\n    y_train, y_val = y[train_idx], y[val_idx]\n    \n    val_preds_list = []\n    \n    for seed in seed_list:\n        base_model = LogisticRegression(**params, random_state=seed)\n        model = CalibratedClassifierCV(base_model, method='sigmoid', cv=k)\n        \n        model.fit(X_train, y_train)\n        \n        val_preds_list.append(model.predict_proba(X_val))\n        test_preds_list.append(model.predict_proba(X_test))\n        \n    oof[val_idx] = np.mean(val_preds_list, axis=0)\n    score = log_loss(y_val, oof[val_idx])\n    print(f'fold: {fold}, log_loss: {score}')\n    score_list.append(score)\n    \n    fold += 1\n    \ncv_logloss = np.mean(score_list)\nprint(f'{name}, log_loss: {cv_logloss}')\n\npreds = np.mean(test_preds_list, axis = 0)\n\nsample_sub[['Class_1','Class_2','Class_3','Class_4','Class_5','Class_6','Class_7','Class_8','Class_9']] = preds\nsample_sub.to_csv('submission_1.csv',index=None)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T15:38:37.372328Z","iopub.execute_input":"2021-06-29T15:38:37.372719Z"}}},{"cell_type":"markdown","source":"## Random Forest","metadata":{}},{"cell_type":"code","source":"X = train.drop(['id', 'target'], axis=1, inplace=False).copy()\ny = train['target'].values\n\nX_test = test.drop(['id'], axis=1, inplace=False).copy()\nrandom_seed = 0","metadata":{"execution":{"iopub.status.busy":"2021-06-30T05:37:49.983701Z","iopub.execute_input":"2021-06-30T05:37:49.984077Z","iopub.status.idle":"2021-06-30T05:37:50.138171Z","shell.execute_reply.started":"2021-06-30T05:37:49.984043Z","shell.execute_reply":"2021-06-30T05:37:50.137192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder = OrdinalEncoder()\nall_encoded = encoder.fit_transform(X.append(X_test))\n\nX = all_encoded[0:len(X)]\nX_test = all_encoded[len(X):]","metadata":{"execution":{"iopub.status.busy":"2021-06-30T05:37:51.928826Z","iopub.execute_input":"2021-06-30T05:37:51.929245Z","iopub.status.idle":"2021-06-30T05:37:54.042677Z","shell.execute_reply.started":"2021-06-30T05:37:51.929207Z","shell.execute_reply":"2021-06-30T05:37:54.04199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {\n          'bootstrap':True,\n          'max_depth':30,\n          'max_features':'auto'  ,\n          'min_samples_leaf' :10,\n          'min_samples_split':5,\n          'n_estimators':500\n          }","metadata":{"execution":{"iopub.status.busy":"2021-06-30T05:38:07.218798Z","iopub.execute_input":"2021-06-30T05:38:07.219173Z","iopub.status.idle":"2021-06-30T05:38:07.223846Z","shell.execute_reply.started":"2021-06-30T05:38:07.21914Z","shell.execute_reply":"2021-06-30T05:38:07.222874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"name = 'Random_forest'\nk = 5\nseed_list = [0, 1, 2]\nkf = StratifiedKFold(n_splits = k, shuffle=True, random_state=random_seed)\noof = np.zeros((len(train), 9))\ntest_preds_list = []\nscore_list = []\nfold = 1\n\nsplits = list(kf.split(X, y))\nfor train_idx, val_idx in splits:\n    X_train, X_val = X[train_idx], X[val_idx]\n    y_train, y_val = y[train_idx], y[val_idx]\n    \n    val_preds_list = []\n    \n    for seed in seed_list:\n        base_model = RandomForestClassifier(**params, random_state = seed)\n        model = CalibratedClassifierCV(base_model, method='sigmoid', cv=k)\n        \n        model.fit(X_train, y_train)\n        \n        val_preds_list.append(model.predict_proba(X_val))\n        test_preds_list.append(model.predict_proba(X_test))\n    \n    oof[val_idx] = np.mean(val_preds_list, axis=0)\n    score = log_loss(y_val, oof[val_idx])\n    print(f'fold: {fold}, log_loss: {score}')\n    score_list.append(score)\n    fold += 1\n    \ncv_logloss = np.mean(score_list)\nprint(f'{name}, log_loss: {cv_logloss}')\n\npreds = np.mean(test_preds_list, axis = 0)\n\nsample_sub[['Class_1','Class_2','Class_3','Class_4','Class_5','Class_6','Class_7','Class_8','Class_9']] = preds\nsample_sub.to_csv('submission_2.csv',index=None)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T05:38:11.830657Z","iopub.execute_input":"2021-06-30T05:38:11.831015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LightGBM","metadata":{}},{"cell_type":"code","source":"X = train.drop(['id', 'target'], axis=1, inplace=False).copy()\ny = train['target'].values\n\nX_test = test.drop(['id'], axis=1, inplace=False).copy()\nrandom_seed = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder = OrdinalEncoder()\nall_encoded = encoder.fit_transform(X.append(X_test))\nX = all_encoded[0:len(X)]\nX_test = all_encoded[len(X):]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {\n  'learning_rate': .02,\n   'max_depth': 3,\n    'num_leaves': 6,\n    'min_split_gain': 0.17865452483871047,\n    'reg_alpha': 9.540720621520459,\n    'reg_lambda': 4.5781292529661375,\n    'colsample_bytree': 0.0644950794287173,\n    'subsample': 0.9314592865852914,\n    'subsample_freq': 7,\n    'min_child_samples': 57\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params_lgbm = params\nparams_lgbm['boosting_type'] = 'gbdt'\nparams_lgbm['device'] = 'gpu'\nparams_lgbm ['objective'] = 'multiclasss'\nparams_lgbm ['num_classes'] = 9,\n\nparams_lgbm ['metric'] = 'multi_logloss'\nparams_lgbm ['verbosity'] = -1\nparams_lgbm ['n_estimators']= 500\n#params_lgbm[\"cat_feature\"] = cat_features\n\nname = 'lighgbm_3seeds_5fold'\nk = 5\nseed_list = [0, 1, 2]\nkf = StratifiedKFold(n_splits = k, shuffle=True, random_state=random_seed)\noff = np.zeros((len(train), 9))\ntest_preds_list = []\nscore_list = []\nfold = 1\n\nsplits = list(kf.split(X, y))\nfor train_idx, val_idx in splits:\n    X_train, X_val = X[train_idx], X[val_idx]\n    y_train, y_val = y[train_idx], y[val_idx]\n    \n    val_preds_list = []\n    \n    for seed in seed_list:\n        params_lgbm['random_state'] = seed\n        model = lgbm.LGBMClassifier(**params_lgbm)\n    \n        model.fit(X_train, y_train, eval_set = [(X_train,y_train),(X_val,y_val)],\n                 early_stopping_rounds=100,\n                 eval_names=['train','val'],verbose=200)\n        \n        val_preds_list.append(model.predict_proba(X_val))\n        test_preds_list.append(model.predict_proba(X_test))\n    \n    oof[val_idx] = np.mean(val_preds_list, axis=0)\n    score = log_loss(y_val, oof[val_idx])\n    print(f'fold: {fold}, log_loss: {score}')\n    score_list.append(score)\n    fold += 1\n    \ncv_logloss = np.mean(score_list)\nprint(f'{name}, log_loss: {cv_logloss}')\n\npreds = np.mean(test_preds_list, axis = 0)\n\nsample_sub[['Class_1','Class_2','Class_3','Class_4','Class_5','Class_6','Class_7','Class_8','Class_9']] = preds\nsample_sub.to_csv('submission_3.csv',index=None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBoost","metadata":{}},{"cell_type":"code","source":"X = train.drop(labels=['id','target'],axis=1,inplace=False).copy()\ny = train['target'].map({\"Class_1\":0,\"Class_2\":1,\"Class_3\":2,\"Class_4\":3,\"Class_5\":4,\"Class_6\":5, \"Class_7\":6, \"Class_8\":7, \"Class_9\":8}).values\nX_test = test.drop(labels=['id'],axis=1,inplace=False).copy()\nrandom_seed = 0\n\nencoder = OrdinalEncoder()\nall_encoded = encoder.fit_transform(X.append(X_test))\nX = all_encoded[0:len(X)]\nX_test = all_encoded[len(X):]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params =  {'lambda': 1.3718620937297796, \n           'alpha': 6.395781966352342, \n           'colsample_bytree': 0.2390564723786096, \n           'colsample_bynode': 0.7459555518737353, \n           'colsample_bylevel': 0.36002014547566097, \n           'subsample': 0.6302863949739616,\n           'eta': 0.01, \n           'grow_policy': 'lossguide', \n           'max_depth': 19, \n           'min_child_weight': 28, \n           'max_bin': 258, \n           'deterministic_histogram': False}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params_xgb = params\nparams_xgb[\"tree_method\"] = \"gpu_hist\"\nparams_xgb[\"predictor\"] = 'gpu_predictor'\nparams_xgb[\"objective\"] = 'multi:softprob'\nparams_xgb[\"num_class\"] = 9\n#params_xgb[\"eval_metric\"] ='logloss'\n\nname = 'xgboost_3seeds_5fold'\nk=5\nseed_list=[0,1,2]\nkf = StratifiedKFold(n_splits=k,shuffle=True,random_state=random_seed)\noof = np.zeros((len(train),9))\ntest_preds_list = []\nscore_list = []\nfold=1\n  \nsplits = list(kf.split(X,y))\nfold = 1\nfor train_idx, val_idx in splits:\n    X_train, X_val = X[train_idx], X[val_idx]\n    y_train, y_val = y[train_idx], y[val_idx]\n\n    val_preds_list = []\n\n    for seed in seed_list:\n    \n        # fit and run model\n        params_xgb['seed'] = seed\n    \n        dtrain = xgb.DMatrix(data=X_train, label=y_train)\n        dval = xgb.DMatrix(data=X_val, label=y_val)\n        dtest = xgb.DMatrix(data=X_test)\n    \n        model = xgb.train(params_xgb, dtrain,\\\n                       evals=[(dtrain,'train'),(dval,'val')],\\\n                       verbose_eval=100,\n                       early_stopping_rounds=100,\n                       num_boost_round=100000)\n    \n    \n\n    \n        val_preds_list.append(model.predict(dval))\n        test_preds_list.append(model.predict(dtest))\n    \n    oof[val_idx] = np.mean(val_preds_list,axis=0)\n    score = log_loss(y_val, oof[val_idx])\n    print(f\"fold: {fold},log_loss: {score}\")\n    score_list.append(score)\n    # print(f\"fold: {fold}, class0 tr %: {y_train.value_counts()[0]/len(y_train)}, class0 val %: {y_val.value_counts()[0]/len(y_val)} \")\n    fold +=1\n  \ncv_logloss = np.mean(score_list)\nprint(f\"{name} ,log_loss: {cv_logloss}\")\n\npreds= np.mean(test_preds_list,axis=0)\n\nsample_sub[['Class_1','Class_2','Class_3','Class_4','Class_5','Class_6','Class_7','Class_8','Class_9']] = preds\nsample_sub.to_csv('submission_4.csv',index=None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CatBoost","metadata":{}},{"cell_type":"code","source":"X = train.drop(labels=['id','target'],axis=1,inplace=False).copy()\ny = train['target'].values\nX_test = test.drop(labels=['id'],axis=1,inplace=False).copy()\nrandom_seed = 0\n\nencoder = OrdinalEncoder()\nall_encoded = encoder.fit_transform(X.append(X_test))\nX = all_encoded[0:len(X)]\nX_test = all_encoded[len(X):]\n\nX = X.astype(int)\nX_test = X_test.astype(int)\ncat_features = np.arange(0,X.shape[1]).tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params =   {'learning_rate': 0.03470328317940195, \n           'depth': 2, \n           'l2_leaf_reg': 820.7804346737378, \n           'random_strength': 0.336019499813798, \n           'border_count': 128,\n           'grow_policy': 'Lossguide',\n           'min_data_in_leaf': 267}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params_cb = params\n\n#params_cb[\"cat_features\"] = cat_features\n#params_cb [\"learning_rate\"] = 0.01\n#params_cb [\"depth\"] = 4\nparams_cb [\"loss_function\"] = 'MultiClass'\nparams_cb [\"od_wait\"] = 1000\nparams_cb [\"od_type\"] = 'Iter'\n#params_cb [\"min_data_in_leaf\"] = 1\n#params_cb [\"max_ctr_complexity\"] = 15\nparams_cb [\"task_type\"] = \"GPU\"\nparams_cb[\"cat_features\"] = cat_features\n            \n\nname = 'catboost_3seeds_5fold'\nk=5\nseed_list=[0,1,2]\nkf = StratifiedKFold(n_splits=k,shuffle=True,random_state=random_seed)\noof = np.zeros((len(train),9))\ntest_preds_list = []\nscore_list = []\nfold=1\n  \nsplits = list(kf.split(X,y))\nfold = 1\nfor train_idx, val_idx in splits:\n    X_train, X_val = X[train_idx], X[val_idx]\n    y_train, y_val = y[train_idx], y[val_idx]\n\n    val_preds_list = []\n\n    for seed in seed_list:\n    \n    # fit and run model\n        params_cb['random_state'] = seed\n        \n        model = CatBoostClassifier(**params_cb,\n            iterations=50000,\n            use_best_model=True,\n            )\n\n        model.fit(X_train,y=y_train,\n              use_best_model=True,\n              eval_set=[(X_val,y_val)],\n              verbose=100)\n    \n\n    \n        val_preds_list.append(model.predict_proba(X_val))\n        test_preds_list.append(model.predict_proba(X_test))\n    \n    oof[val_idx] = np.mean(val_preds_list,axis=0)\n    score = log_loss(y_val, oof[val_idx])\n    print(f\"fold: {fold},log_loss: {score}\")\n    score_list.append(score)\n  # print(f\"fold: {fold}, class0 tr %: {y_train.value_counts()[0]/len(y_train)}, class0 val %: {y_val.value_counts()[0]/len(y_val)} \")\n    fold +=1\n  \ncv_logloss = np.mean(score_list)\nprint(f\"{name} ,log_loss: {cv_logloss}\")\n\npreds= np.mean(test_preds_list,axis=0)\n\nsample_sub[['Class_1','Class_2','Class_3','Class_4','Class_5','Class_6','Class_7','Class_8','Class_9']] = preds\nsample_sub.to_csv('submission_5.csv',index=None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CNN","metadata":{}},{"cell_type":"code","source":"targets = pd.get_dummies(train['target'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def custom_metric(y_true, y_pred):\n    y_pred = K.clip(y_pred, 1e-15, 1-1e-15)\n    loss = K.mean(cce(y_true, y_pred))\n    return loss\n\ncce = tf.keras.losses.CategoricalCrossentropy()\n\nes = tf.keras.callbacks.EarlyStopping(\n    monitor='val_custom_metric', min_delta=1e-05, patience=5, verbose=0,\n    mode='min', baseline=None, restore_best_weights=True)\n\nplateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_custom_metric', factor=0.7, patience=2, verbose=0,\n    mode='min')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def conv_model():\n\n    conv_inputs = layers.Input(shape = (75))\n    embed = layers.Embedding (input_dim = 354, \n                              output_dim = 7,\n                              embeddings_regularizer='l2')(conv_inputs)\n    embed = layers.Conv1D(12,1,activation = 'relu')(embed)        \n    embed = layers.Flatten()(embed)\n    hidden = layers.Dropout(0.3)(embed)\n    \n    hidden = tfa.layers.WeightNormalization(\n                layers.Dense(\n                units=32,\n                activation ='selu',\n                kernel_initializer = \"lecun_normal\"))(hidden)\n    \n    output = layers.Dropout(0.3)(layers.Concatenate()([embed, hidden]))\n    output = tfa.layers.WeightNormalization(\n    layers.Dense(\n                units = 32,\n                activation='relu',\n                kernel_initializer = \"lecun_normal\"))(output) \n    output = layers.Dropout(0.4)(layers.Concatenate()([embed, hidden, output]))\n    output = tfa.layers.WeightNormalization(\n    layers.Dense(\n                units = 32, \n                activation = 'relu',\n                kernel_initializer = \"lecun_normal\"))(output)\n    \n    conv_outputs = layers.Dense(\n                units = 9, \n                activation ='softmax',\n                kernel_initializer =\"lecun_normal\")(output)\n    \n    model = Model(conv_inputs,conv_outputs)\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof_NN_a = np.zeros((train.shape[0],9))\npred_NN_a = np.zeros((test.shape[0],9))\n\nN_FOLDS = 5\nSEED = 2021\nEPOCH = 60\n\n\nskf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n\nfor fold, (tr_idx, ts_idx) in enumerate(skf.split(train,train.iloc[:,-1])):\n    print(f\"\\n ====== TRAINING FOLD {fold} =======\\n\")\n\n    X_train = train.iloc[:,1:-1].iloc[tr_idx]\n    y_train = targets.iloc[tr_idx]\n    X_test = train.iloc[:,1:-1].iloc[ts_idx]\n    y_test = targets.iloc[ts_idx]\n\n    K.clear_session()\n    \n    print(\"\\n-----Convolution model Training----\\n\")\n\n    model_conv = conv_model()\n\n    model_conv.compile(loss='categorical_crossentropy', \n                            optimizer = keras.optimizers.Adam(learning_rate=2e-4), \n                            metrics=custom_metric)\n    model_conv.fit(X_train, y_train,\n              batch_size = 256, epochs = EPOCH,\n              validation_data=(X_test, y_test),\n              callbacks=[es, plateau],\n              verbose = 0)\n   \n    pred_a = model_conv.predict(X_test) \n    oof_NN_a[ts_idx] += pred_a \n    score_NN_a = log_loss(y_test, pred_a)\n    print(f\"\\nFOLD {fold} Score convolution model: {score_NN_a}\\n\")\n    pred_NN_a += model_conv.predict(test.iloc[:,1:]) / N_FOLDS \n \nscore_a = log_loss(targets, oof_NN_a)\nprint(f\"\\n=== FINAL SCORE CONVOLUTION MODEL : {score_a}===\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_embedding = pred_NN_a","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_sub['Class_1']=pred_embedding[:,0]\nsample_sub['Class_2']=pred_embedding[:,1]\nsample_sub['Class_3']=pred_embedding[:,2]\nsample_sub['Class_4']=pred_embedding[:,3]\nsample_sub['Class_5']=pred_embedding[:,4]\nsample_sub['Class_6']=pred_embedding[:,5]\nsample_sub['Class_7']=pred_embedding[:,6]\nsample_sub['Class_8']=pred_embedding[:,7]\nsample_sub['Class_9']=pred_embedding[:,8]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_sub.to_csv(\"submission_6.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}