{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom datetime import datetime\nfrom functools import wraps\nfrom contextlib import contextmanager\nfrom tqdm import tqdm\n\n\n@contextmanager\ndef timer(msg):\n    st = datetime.now()\n    yield\n    cost = datetime.now() - st\n    print(f'{msg} Done. It cost {cost}')\n\n\ndef clock(func):\n    @wraps(func)\n    def clocked(*args, **kwargs):\n        st = datetime.now()\n        res = func(*args, **kwargs)\n        cost_ = datetime.now() - st\n        print(f'{func.__name__} cost {cost_}')\n        return res\n    return clocked\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-20T07:23:33.403079Z","iopub.execute_input":"2021-06-20T07:23:33.403402Z","iopub.status.idle":"2021-06-20T07:23:33.416873Z","shell.execute_reply.started":"2021-06-20T07:23:33.40333Z","shell.execute_reply":"2021-06-20T07:23:33.415678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# inference: https://www.kaggle.com/fusioncenter/residual-network-for-tabular-data","metadata":{"execution":{"iopub.status.busy":"2021-06-14T21:08:43.631896Z","iopub.execute_input":"2021-06-14T21:08:43.632594Z","iopub.status.idle":"2021-06-14T21:08:43.638727Z","shell.execute_reply.started":"2021-06-14T21:08:43.632552Z","shell.execute_reply":"2021-06-14T21:08:43.637818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load data","metadata":{}},{"cell_type":"code","source":"with timer('drop dumpliced'):\n    train_df = pd.read_csv('/kaggle/input/tabular-playground-series-jun-2021/train.csv')\n    test_df = pd.read_csv('/kaggle/input/tabular-playground-series-jun-2021/test.csv')\n    submit_df = pd.read_csv('/kaggle/input/tabular-playground-series-jun-2021/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-20T07:23:55.294047Z","iopub.execute_input":"2021-06-20T07:23:55.294394Z","iopub.status.idle":"2021-06-20T07:23:56.972753Z","shell.execute_reply.started":"2021-06-20T07:23:55.294364Z","shell.execute_reply":"2021-06-20T07:23:56.971877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## drop dumplicates rows","metadata":{}},{"cell_type":"code","source":"need_columns = [i for i in train_df.columns if 'feat' in i] + ['target']\nwith timer('drop duplicate'):\n    print('Before drop duplicate train_df.shape:', train_df.shape)\n    train_df = train_df.drop_duplicates(subset=need_columns).reset_index(drop=True)\n    print('After drop duplicate train_df.shape:', train_df.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T07:23:58.147606Z","iopub.execute_input":"2021-06-20T07:23:58.147961Z","iopub.status.idle":"2021-06-20T07:23:58.515405Z","shell.execute_reply.started":"2021-06-20T07:23:58.14793Z","shell.execute_reply":"2021-06-20T07:23:58.514525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Aggregate low freq categries ","metadata":{}},{"cell_type":"code","source":"@clock\ndef low_freqence_detector(df, vars_to_agg, agg_threshold=0.999):\n    \"\"\"\n    将低频值归为一类\n    \"\"\"\n    replace_dict = {}\n    for col in tqdm(vars_to_agg):\n        a_cumsum = df[col].value_counts(normalize=True).cumsum()\n        value_count_series = df[col].value_counts()\n        will_be_replaced_values = value_count_series[a_cumsum >= agg_threshold].index.tolist()\n        n = len(will_be_replaced_values)\n        replace_value = min(will_be_replaced_values)\n        tmp_dict = (will_be_replaced_values, replace_value)\n        replace_dict[col] = tmp_dict\n    return replace_dict\n\ndef aggregate_low_freq_values(df, replace_dict):\n    df_out = df.copy(deep=True)\n    for replace_feat in tqdm(replace_dict):\n        need_replaced_values = replace_dict[replace_feat][0]\n        replace_value = replace_dict[replace_feat][1]\n        df_out.loc[df[replace_feat].isin(need_replaced_values), replace_feat] = replace_value\n    return df_out\n\ndef quick_agg_low_freq_values(tr_df, te_df, vars_to_agg, agg_threshold=0.999):\n    c = pd.concat([tr_df[vars_to_agg], te_df[vars_to_agg]], ignore_index=True)\n    replace_dict = low_freqence_detector(c, vars_to_agg, agg_threshold)\n    return aggregate_low_freq_values(tr_df, replace_dict), aggregate_low_freq_values(te_df, replace_dict)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T07:24:05.771529Z","iopub.execute_input":"2021-06-20T07:24:05.771883Z","iopub.status.idle":"2021-06-20T07:24:05.780058Z","shell.execute_reply.started":"2021-06-20T07:24:05.771848Z","shell.execute_reply":"2021-06-20T07:24:05.77928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df, test_df = quick_agg_low_freq_values(\n    train_df, test_df,\n    [i for i in train_df.columns if 'feat' in i]\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T07:24:09.432107Z","iopub.execute_input":"2021-06-20T07:24:09.432431Z","iopub.status.idle":"2021-06-20T07:24:10.329961Z","shell.execute_reply.started":"2021-06-20T07:24:09.432399Z","shell.execute_reply":"2021-06-20T07:24:10.328996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # 增加freq\n# frequnce_add_col = ['feature_60', 'feature_15', 'feature_28', 'feature_61', 'feature_62']\n# for col_ in tqdm(frequnce_add_col):\n#     dict_ = train_df[col_].value_counts(normalize=True).to_dict()\n#     train_df[f'{col_}_freq'] = train_df[col_].map(dict_)\n#     test_df[f'{col_}_freq'] = test_df[col_].map(dict_)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T21:08:47.623036Z","iopub.execute_input":"2021-06-14T21:08:47.623406Z","iopub.status.idle":"2021-06-14T21:08:47.627235Z","shell.execute_reply.started":"2021-06-14T21:08:47.623365Z","shell.execute_reply":"2021-06-14T21:08:47.626179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# residual net model\n## residual blocks","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Embedding, Dense, Input, SpatialDropout1D, Conv1D, GlobalMaxPooling1D\nfrom tensorflow.keras.layers import concatenate, Dropout, Flatten, BatchNormalization\nfrom tensorflow.keras import Model, backend, layers\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam, Adamax\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix","metadata":{"execution":{"iopub.status.busy":"2021-06-20T07:24:15.725576Z","iopub.execute_input":"2021-06-20T07:24:15.725927Z","iopub.status.idle":"2021-06-20T07:24:21.025088Z","shell.execute_reply.started":"2021-06-20T07:24:15.725894Z","shell.execute_reply":"2021-06-20T07:24:21.024234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tf2_logloss(obs, pre):\n    pre = tf.clip_by_value(pre, 0.0005, 1-0.0005)\n    return tf.keras.metrics.categorical_crossentropy(obs, pre)\n\ndef plot_heatmap(y_true, y_pred_prob):\n    y_pred = np.argmax(y_pred_prob, axis=1)\n    conf = confusion_matrix(y_true, y_pred)\n    conf_p = conf/ conf.sum(axis=0).reshape(1, -1)\n    conf_r = conf/ conf.sum(axis=1).reshape(-1, 1)\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    sns.heatmap(conf_p, annot=True, fmt='.2f', ax=axes[0])\n    axes[0].set_title('Percision')\n    sns.heatmap(conf_r, annot=True, fmt='.2f', ax=axes[1])\n    axes[1].set_title('Recall')\n    plt.show()\n\n\ndef embedding_residul_block(\n    max_cnt, \n    embed_size=3, \n    feature_nums=75,\n    max_len=1, \n    number_of_blocks=4,\n    output_shape=9\n):\n    _input = Input(shape=(feature_nums,), dtype='float32')\n    _embed = Embedding(max_cnt, embed_size, input_length=max_len, mask_zero=False)(_input)\n    _embed = Flatten()(_embed)\n    \n    block_list = []\n    for i in range(number_of_blocks):\n        if i == 0:\n            blocki = BatchNormalization()(_input)\n            blocki = Dropout(0.2)(blocki)\n            blocki = Dense(64, activation='relu')(blocki)\n            blocki = concatenate([blocki, _embed])\n            block_list.append(blocki)\n            continue\n\n        blocki_bf = block_list[i-1]\n        blocki_next = BatchNormalization()(blocki_bf)\n        blocki_next = Dropout(0.2)(blocki_next)\n        blocki_next = Dense(128, activation='relu')(blocki_next)\n        blocki_next = concatenate([blocki_next, block_list[0]])\n        block_list.append(blocki_next)\n\n    _output = block_list[-1]\n    _output = BatchNormalization()(_output)\n    _output = Dense(output_shape, activation='softmax')(_output)\n\n    model = Model(inputs=_input, outputs = _output)\n    \n    model.compile(loss = 'categorical_crossentropy',\n                  optimizer=Adam(lr = 0.01),\n                  metrics=['accuracy']) #[tf2_logloss]) #\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-06-20T07:24:32.643396Z","iopub.execute_input":"2021-06-20T07:24:32.643728Z","iopub.status.idle":"2021-06-20T07:24:32.656184Z","shell.execute_reply.started":"2021-06-20T07:24:32.643683Z","shell.execute_reply":"2021-06-20T07:24:32.655367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## model train","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.metrics import f1_score, log_loss\nfrom copy import deepcopy\n\n\nmodel_save_path='residual_nn.hdf5'\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, min_delta=0.00001)\nmodel_checkpoint = ModelCheckpoint(\n    model_save_path,\n    save_best_only=True,\n    save_weights_only=True,\n)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.01, patience=5)\n\ndef pred_57_f1(y_true_in, y_pred_proba_in):\n    y_true = deepcopy(y_true_in)\n    y_pred_proba = deepcopy(y_pred_proba_in)\n    mean_5 = np.percentile(y_pred_proba[:, 5], 65)\n    mean_7 = np.percentile(y_pred_proba[:, 7], 65)\n    y_pred_proba[ y_pred_proba[:, 5] <= mean_5, 5] = 0.0001\n    y_pred_proba[ y_pred_proba[:, 7] <= mean_7, 7] = 0.0001\n    y_pred = np.argmax(y_pred_proba, axis=1)\n    y_pred57 = (y_pred == 5) | (y_pred == 7) \n    y_pred[~y_pred57]=0\n    \n    y_true57 = (y_true == 5) | (y_true == 7) \n    y_true[~y_true57]=0\n    \n    return f1_score(y_true, y_pred, average='macro'), y_pred_proba, log_loss(y_true_in, y_pred_proba )\n","metadata":{"execution":{"iopub.status.busy":"2021-06-20T07:24:42.935982Z","iopub.execute_input":"2021-06-20T07:24:42.936304Z","iopub.status.idle":"2021-06-20T07:24:42.961195Z","shell.execute_reply.started":"2021-06-20T07:24:42.936268Z","shell.execute_reply":"2021-06-20T07:24:42.960389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tf.config.list_physical_devices())\nfor i in tf.config.list_physical_devices():\n    if 'GPU' in i[1]:\n        print (i[1],'可用,GPU名称: ',i[0])\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n        print ('Turn on GPU')","metadata":{"execution":{"iopub.status.busy":"2021-06-20T07:24:52.085442Z","iopub.execute_input":"2021-06-20T07:24:52.085788Z","iopub.status.idle":"2021-06-20T07:24:52.094998Z","shell.execute_reply.started":"2021-06-20T07:24:52.085749Z","shell.execute_reply":"2021-06-20T07:24:52.09419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nimport matplotlib.pyplot as plt\nfrom sklearn.isotonic import IsotonicRegression\nlb = LabelEncoder()\n# print(train_df.columns)\nX = train_df.drop(columns = ['id', 'target']).values\ny = lb.fit_transform(train_df['target'].values)\ntest_array = test_df.drop(columns = ['id']).values\nnfold = 5\nepochs = 50\noutput_shape = 9\n# kf = StratifiedKFold(nfold)\npred_arr_list = []\nfor seed_ in [2021, 42, 1921]:\n    tf.random.set_seed(seed_)\n    kf = StratifiedKFold(nfold, shuffle=True, random_state=seed_)\n\n    for foldi, (tr_idx, val_idx) in enumerate(kf.split(X, y)):\n        print(f\"Fold: {foldi} | seed{seed_}\")\n        tr_x, tr_y = X[tr_idx], y[tr_idx]\n        val_x, val_y = X[val_idx], y[val_idx]\n        nn_model = embedding_residul_block(\n            max_cnt=500, # 500\n            embed_size=3,#3, \n            feature_nums=tr_x.shape[1],\n            max_len=1, \n            number_of_blocks=4,#4,\n            output_shape=9\n        )\n    #     nn_model.summary()\n        nn_model.fit(tr_x, to_categorical(tr_y, output_shape),\n                         validation_data = (val_x, to_categorical(val_y, output_shape)),\n                         epochs = epochs,\n                         batch_size = 128, #128,\n                         shuffle=True,\n                         callbacks = [early_stopping, model_checkpoint, reduce_lr]\n                    )\n        # load best model\n        nn_model.load_weights(model_save_path)\n        pred = nn_model.predict(val_x)\n        pred_cp = deepcopy(pred)\n        for i in range(pred.shape[1]):\n            ir = IsotonicRegression()\n            ir.fit(pred[:, i], to_categorical(val_y, output_shape)[:, i])\n            pred_cp[:, i] = ir.predict(pred[:, i])\n\n        ir_loss = log_loss(val_y, pred_cp )\n        loss_o = log_loss(val_y, pred )\n        print(f'original loss: {loss_o:.3f}, ir_loss:{ir_loss:.3f}')\n        plot_heatmap(val_y, pred_cp)\n        if foldi == 0:\n            pred_out = nn_model.predict(test_array)\n            pred_array = pred_out\n        else:\n            pred_out = nn_model.predict(test_array)\n            pred_array += pred_out\n        pred_arr_list.append(pred_out)\n        print(\"-\"*50)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T07:26:09.837035Z","iopub.execute_input":"2021-06-20T07:26:09.837373Z","iopub.status.idle":"2021-06-20T08:16:26.843289Z","shell.execute_reply.started":"2021-06-20T07:26:09.837342Z","shell.execute_reply":"2021-06-20T08:16:26.84225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Merge","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import mutual_info_classif\nfrom sklearn.model_selection import train_test_split\nfrom mlxtend.preprocessing import minmax_scaling\n\ndef make_mi_scores(X, y, discrete_features):\n    \"\"\"\n    https://www.kaggle.com/mehrankazeminia/1-tps-jun-21-histgradient-catboost-nn/output\n    估计离散目标变量的互信息\n    \"\"\"\n    mi_scores = mutual_info_classif(X, y, discrete_features=discrete_features, random_state=123)    \n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)                         \n    mi_scores = mi_scores.sort_values(ascending=False)                          \n    return mi_scores\n\ndef plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")","metadata":{"execution":{"iopub.status.busy":"2021-06-20T08:19:38.483032Z","iopub.execute_input":"2021-06-20T08:19:38.48341Z","iopub.status.idle":"2021-06-20T08:19:38.490449Z","shell.execute_reply.started":"2021-06-20T08:19:38.483374Z","shell.execute_reply":"2021-06-20T08:19:38.489505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model0 - tabluar-residual-nn","metadata":{}},{"cell_type":"code","source":"n = 1\nfor arr_ in pred_arr_list:\n    if n ==1:\n        pred_f = arr_\n        n += 1\n        continue\n    pred_f += arr_\n    n += 1\n\npred_f = pred_f/n\n\nsubmit_df0 = submit_df.copy(deep=True)\nsubmit_df0.loc[:, ['Class_1','Class_2', 'Class_3', 'Class_4','Class_5','Class_6', 'Class_7', 'Class_8', 'Class_9']] =\\\n    np.clip(pred_f, 10**-15, 1-10**-15)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T08:19:47.903935Z","iopub.execute_input":"2021-06-20T08:19:47.90425Z","iopub.status.idle":"2021-06-20T08:19:48.156861Z","shell.execute_reply.started":"2021-06-20T08:19:47.904221Z","shell.execute_reply":"2021-06-20T08:19:48.155854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model1- LGBMClassifier","metadata":{}},{"cell_type":"code","source":"from lightgbm import LGBMClassifier\nlgb_params = {\n        'num_leaves': 10,\n        'min_data_in_leaf': 63,\n        'learning_rate': 0.05,\n        'min_sum_hessian_in_leaf': 8.140308692805194,\n        'bagging_fraction': 1.0,\n        'bagging_freq': 5,\n        'boost_from_average':'false',\n        'subsample': 0.749948437333368,\n        'colsample_bytree': 0.6168504947710284,\n         'reg_alpha': 0.227796749807186,\n         'reg_lambda': 70.2792417704872,\n        'min_gain_to_split': 0.4758826409257615,\n        'max_depth': 14, \n        'n_jobs': -1,\n        'boosting_type': 'gbdt',\n        'metric':'multi_logloss',\n#         'early_stopping_round' : 100,\n        'n_estimators': 500,\n        'tree_learner': 'serial',\n    }\nmodel1 = LGBMClassifier(**lgb_params)\n\nmodel1.fit(X, y, verbose=50)\npred1 = model1.predict_proba(test_array)\nsubmit_df1 = submit_df.copy(deep=True)\nsubmit_df1.loc[:, ['Class_1','Class_2', 'Class_3', 'Class_4','Class_5','Class_6', 'Class_7', 'Class_8', 'Class_9']] =\\\n    np.clip(pred1, 10**-15, 1-10**-15)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T08:25:58.650338Z","iopub.execute_input":"2021-06-20T08:25:58.650748Z","iopub.status.idle":"2021-06-20T08:28:46.957665Z","shell.execute_reply.started":"2021-06-20T08:25:58.650699Z","shell.execute_reply":"2021-06-20T08:28:46.95684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model2 - CatBoostClassifier","metadata":{}},{"cell_type":"code","source":"from catboost import CatBoostClassifier\nmodel2 = CatBoostClassifier(depth=8,\n                            iterations=1000,\n                            learning_rate=0.02,                            \n                            eval_metric='MultiClass',\n                            loss_function='MultiClass', \n                            bootstrap_type= 'Bernoulli',\n                            leaf_estimation_method='Gradient',\n                            random_state=123,\n                            task_type='GPU')   \n\nmodel2.fit(X, y, verbose=100)\npred2 = model2.predict_proba(test_array)\nsubmit_df2 = submit_df.copy(deep=True)\nsubmit_df2.loc[:, ['Class_1','Class_2', 'Class_3', 'Class_4','Class_5','Class_6', 'Class_7', 'Class_8', 'Class_9']] =\\\n    np.clip(pred2, 10**-15, 1-10**-15)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T08:33:17.730902Z","iopub.execute_input":"2021-06-20T08:33:17.731217Z","iopub.status.idle":"2021-06-20T08:33:47.312971Z","shell.execute_reply.started":"2021-06-20T08:33:17.731189Z","shell.execute_reply":"2021-06-20T08:33:47.312115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate(main, support, coeff):\n    g = main.copy()    \n    for i in main.columns[1:]:\n        \n        res = []\n        lm, Is = [], []        \n        lm = main[i].tolist()\n        ls = support[i].tolist()  \n        \n        for j in range(len(main)):\n            res.append((lm[j] * coeff) + (ls[j] * (1.- coeff)))            \n        g[i] = res\n        \n    return g\n\n\nsub = generate(submit_df1, submit_df2, 0.85)\nsub = generate(submit_df0, sub , 0.85)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T08:33:47.314394Z","iopub.execute_input":"2021-06-20T08:33:47.31473Z","iopub.status.idle":"2021-06-20T08:33:48.144889Z","shell.execute_reply.started":"2021-06-20T08:33:47.314678Z","shell.execute_reply":"2021-06-20T08:33:48.14395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stacking","metadata":{}},{"cell_type":"code","source":"class SklearnHelper:\n    def __init__(self, clf, seed, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n        \n    def train(self, x_tr, y_tr):\n        self.clf.fit(x_tr, y_tr)\n    \n    def predict(self, x):\n        try:\n            pred = self.predict_proba(x)\n        except AttributeError:\n            pred = self.clf.predict(x)\n        return pred\n    \n    def predict_proba(self, x):\n        return self.clf.predict_proba(x)\n    \n    def feature_importances(self,x,y):\n        print(self.clf.fit(x,y).feature_importances_)\n    \n    def __repr__(self):\n        return str(self.clf)\n\n        \nfrom sklearn.model_selection import KFold\nntrain = train_df.shape[0]\nntest = test_df.shape[0]\nNFOLDS = 5\nSEED = 2021\nkf = KFold(n_splits= NFOLDS, shuffle=True ,random_state=42)\n\ndef get_oof(clf, x_train, y_train, x_test, class_nums):\n    oof_train = np.zeros((ntrain, class_nums))\n    oof_test = np.zeros((ntest, class_nums))\n    oof_test_skf = np.zeros((ntest, class_nums))\n\n    for i, (train_index, test_index) in enumerate(kf.split(x_train, y_train)):\n        x_tr = x_train[train_index]\n        y_tr = y_train[train_index]\n        x_te = x_train[test_index]\n        y_te = y_train[test_index]\n\n        clf.train(x_tr, y_tr)\n        \n        pred = clf.predict(x_te)\n        oof_train[test_index] = pred\n        oof_test_skf += clf.predict(x_test)\n        \n        loss = log_loss(y_te, pred)\n        print(f\"{str(clf)} | fold {i} | Log loss: {loss}\")\n        print(\"-\"*50)\n\n    oof_test = oof_test_skf / NFOLDS\n    return oof_train, oof_test","metadata":{"execution":{"iopub.status.busy":"2021-06-20T09:49:59.018143Z","iopub.execute_input":"2021-06-20T09:49:59.018471Z","iopub.status.idle":"2021-06-20T09:49:59.029861Z","shell.execute_reply.started":"2021-06-20T09:49:59.018441Z","shell.execute_reply":"2021-06-20T09:49:59.028873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stacking models","metadata":{}},{"cell_type":"code","source":"from catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\n# lgb\nlgb_params = {\n        'num_leaves': 10,\n        'min_data_in_leaf': 63,\n        'learning_rate': 0.05,\n        'min_sum_hessian_in_leaf': 8.140308692805194,\n        'bagging_fraction': 1.0,\n        'bagging_freq': 5,\n        'boost_from_average':'false',\n        'subsample': 0.749948437333368,\n        'colsample_bytree': 0.6168504947710284,\n         'reg_alpha': 0.227796749807186,\n         'reg_lambda': 70.2792417704872,\n        'min_gain_to_split': 0.4758826409257615,\n        'max_depth': 14, \n        'n_jobs': -1,\n        'boosting_type': 'gbdt',\n        'metric':'multi_logloss',\n#         'early_stopping_round' : 100,\n        'n_estimators': 500,\n        'tree_learner': 'serial',\n        'verbose': 0\n    }\n\n# Extra Trees Parameters\net_params = {\n    'n_jobs': -1,\n    'n_estimators':500,\n    #'max_features': 0.5,\n    'max_depth': 8,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n\n# Random Forest parameters\nrf_params = {\n    'n_jobs': -1,\n    'n_estimators': 500,\n     'warm_start': True, \n     #'max_features': 0.2,\n    'max_depth': 6,\n    'min_samples_leaf': 2,\n    'max_features' : 'sqrt',\n    'verbose': 0\n}\n\n\n# AdaBoost parameters\nada_params = {\n    'n_estimators': 500,\n    'learning_rate' : 0.75\n}\n\n\n# catboost\ncatboost_param = dict(depth=8,\niterations=1000,\nlearning_rate=0.02,                            \neval_metric='MultiClass',\nloss_function='MultiClass', \nbootstrap_type= 'Bernoulli',\nleaf_estimation_method='Gradient',\nrandom_state=123,\ntask_type='GPU')\n","metadata":{"execution":{"iopub.status.busy":"2021-06-20T10:11:21.777388Z","iopub.status.idle":"2021-06-20T10:11:21.777769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\net = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\nada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\ngbm = SklearnHelper(clf=LGBMClassifier, seed=SEED, params=lgb_params)\nctb = SklearnHelper(clf=CatBoostClassifier, seed=SEED, params=catboost_param)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T10:11:21.778826Z","iopub.status.idle":"2021-06-20T10:11:21.779363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nrf_oof_train, rf_oof_test = get_oof(rf,X, y, test_array, class_nums=9) # Random Forest\n# ada_oof_train, ada_oof_test = get_oof(ada, X, y, test_array, class_nums=9) # AdaBoost \nlgb_oof_train, lgb_oof_test = get_oof(gbm,X, y, test_array, class_nums=9) # lgb\net_oof_train, et_oof_test = get_oof(et, X, y, test_array, class_nums=9) # Extra Trees\nctb_oof_train, ctb_oof_test = get_oof(ctb,X, y, test_array, class_nums=9) # catboost","metadata":{"execution":{"iopub.status.busy":"2021-06-20T10:03:28.711488Z","iopub.execute_input":"2021-06-20T10:03:28.711835Z","iopub.status.idle":"2021-06-20T10:11:21.776561Z","shell.execute_reply.started":"2021-06-20T10:03:28.711793Z","shell.execute_reply":"2021-06-20T10:11:21.773821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Focus \nclass6(5) - class8(7)\n","metadata":{}},{"cell_type":"code","source":"# #### view\n# import scipy.stats as sts\n# view_foucs5_bool = (train_df.target == 'Class_6') \n# view_foucs7_bool =(train_df.target == 'Class_8')\n\n# skew_5_7_diff_beyond_05 = []\n# for i in [i for i in train_df.columns if 'feat' in i]:\n#     print(i)\n#     train_df.loc[view_foucs5_bool & (~train_df[i].isin([0, 1, 2, 3])), i].hist()\n#     skew_5 = sts.skew(train_df.loc[view_foucs5_bool & (~train_df[i].isin([0, 1, 2, 3])), i])\n#     plt.title(f'{i}&class_5 total_mean: {train_df.loc[view_foucs5_bool, i].mean():.3f} \\\n#     | limit> 3 mean: {train_df.loc[view_foucs5_bool & (~train_df[i].isin([0, 1, 2, 3])), i].mean():.3f} | \\\n#     skew : {skew_5:.3f}')\n#     plt.show()\n#     train_df.loc[view_foucs7_bool & (~train_df[i].isin([0, 1, 2, 3])), i].hist()\n#     skew_7 = sts.skew(train_df.loc[view_foucs7_bool & (~train_df[i].isin([0, 1, 2, 3])), i])\n#     plt.title(f'{i}&class_7 total_mean: {train_df.loc[view_foucs7_bool, i].mean():.3f} \\\n#     | limit> 3 mean: {train_df.loc[view_foucs7_bool & (~train_df[i].isin([0, 1, 2, 3])), i].mean():.3f}| \\\n#     skew : {skew_7:.3f}')\n\n#     plt.show()\n#     if abs(skew_7-skew_5) > 0.5 :\n#         print(f'abs(skew_7-skew_5) > 0.5: {abs(skew_7-skew_5):.2f}')\n#         skew_5_7_diff_beyond_05.append(i)\n#     print('--'*25)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T22:07:33.690174Z","iopub.execute_input":"2021-06-14T22:07:33.690548Z","iopub.status.idle":"2021-06-14T22:07:33.704211Z","shell.execute_reply.started":"2021-06-14T22:07:33.69051Z","shell.execute_reply":"2021-06-14T22:07:33.703251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# skew_5_7_diff_beyond_05\n# # nd_cols = [i for i in train_df.columns if 'feat' in i]","metadata":{"execution":{"iopub.status.busy":"2021-06-14T22:07:33.705422Z","iopub.execute_input":"2021-06-14T22:07:33.705816Z","iopub.status.idle":"2021-06-14T22:07:33.71821Z","shell.execute_reply.started":"2021-06-14T22:07:33.705715Z","shell.execute_reply":"2021-06-14T22:07:33.717238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.preprocessing import LabelEncoder\n# from sklearn.linear_model import LogisticRegression\n# from lightgbm import LGBMClassifier\n# import matplotlib.pyplot as plt\n\n# lb = LabelEncoder()\n# X = train_df.loc[view_foucs5_bool | view_foucs7_bool, nd_cols].values\n# y = lb.fit_transform(train_df.loc[view_foucs5_bool | view_foucs7_bool, 'target'].values)\n# test_array = train_df.loc[view_foucs5_bool | view_foucs7_bool, nd_cols].values\n# nfold = 5\n# epochs = 50\n# output_shape = 2\n# kf = StratifiedKFold(nfold)\n# for foldi, (tr_idx, val_idx) in enumerate(kf.split(X, y)):\n#     print(f\"Fold: {foldi}\")\n#     tr_x, tr_y = X[tr_idx], y[tr_idx]\n#     val_x, val_y = X[val_idx], y[val_idx]\n#     lr = LGBMClassifier(is_unbalance=True)\n#     lr.fit(tr_x, tr_y)\n#     pred = lr.predict_proba(val_x)\n#     plot_heatmap(val_y, pred)\n#     if foldi == 0:\n#         pred_array = lr.predict_proba(test_array)\n#     else:\n#         pred_array += lr.predict_proba(test_array)\n#     print(\"-\"*50)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T22:07:33.719377Z","iopub.execute_input":"2021-06-14T22:07:33.719702Z","iopub.status.idle":"2021-06-14T22:07:33.728057Z","shell.execute_reply.started":"2021-06-14T22:07:33.71967Z","shell.execute_reply":"2021-06-14T22:07:33.727317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submit","metadata":{}},{"cell_type":"code","source":"from datetime import datetime\nnow_ = datetime.now().strftime('%Y%m%d_%H_%M')\n# submit_df.loc[:, ['Class_1','Class_2', 'Class_3', 'Class_4','Class_5','Class_6', 'Class_7', 'Class_8', 'Class_9']] =\\\n#     np.clip(pred_f, 10**-15, 1-10**-15)\n\n\n# submit_df = submit_df.fillna(0.0001)\ndisplay(submit_df0.head())\nsubmit_df0.to_csv(f'residul_nn_model_{now_}.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T08:34:52.29613Z","iopub.execute_input":"2021-06-20T08:34:52.296477Z","iopub.status.idle":"2021-06-20T08:34:54.192681Z","shell.execute_reply.started":"2021-06-20T08:34:52.296442Z","shell.execute_reply":"2021-06-20T08:34:54.191802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ['KAGGLE_USERNAME'] = \"scchuy\" # username from the json file \nos.environ['KAGGLE_KEY'] = \"59c271f7739fbc0d21b8d2c5f8789670\"\n!kaggle competitions submit -c tabular-playground-series-jun-2021 -f ./residul_nn_model_{now_}.csv -m \"Message\"","metadata":{"execution":{"iopub.status.busy":"2021-06-20T08:34:56.378125Z","iopub.execute_input":"2021-06-20T08:34:56.378441Z","iopub.status.idle":"2021-06-20T08:34:58.662907Z","shell.execute_reply.started":"2021-06-20T08:34:56.378411Z","shell.execute_reply":"2021-06-20T08:34:58.661991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}