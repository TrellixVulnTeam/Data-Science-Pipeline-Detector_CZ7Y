{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":" # Load Dependencies","metadata":{}},{"cell_type":"code","source":"!pip uninstall -y typing\n!pip install  \"git+https://github.com/dreamquark-ai/tabnet.git@develop#egg=pytorch_tabnet\" --upgrade","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-17T14:33:23.903601Z","iopub.execute_input":"2021-06-17T14:33:23.904017Z","iopub.status.idle":"2021-06-17T14:33:53.093843Z","shell.execute_reply.started":"2021-06-17T14:33:23.903959Z","shell.execute_reply":"2021-06-17T14:33:53.092653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport time\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\n#from sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\n\n#from pytorch_tabnet.multitask import TabNetMultiTaskClassifier\nfrom pytorch_tabnet.tab_model import TabNetClassifier\nfrom pytorch_tabnet.pretraining import TabNetPretrainer\nimport torch\n\nimport optuna\nfrom optuna.visualization import plot_optimization_history, plot_param_importances\n\nfrom matplotlib import pyplot as plt\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-17T14:33:53.098623Z","iopub.execute_input":"2021-06-17T14:33:53.098861Z","iopub.status.idle":"2021-06-17T14:33:55.977883Z","shell.execute_reply.started":"2021-06-17T14:33:53.098833Z","shell.execute_reply":"2021-06-17T14:33:55.976936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train=pd.read_csv('../input/tabular-playground-series-jun-2021/train.csv')\ntest=pd.read_csv('../input/tabular-playground-series-jun-2021/test.csv')\nsub=pd.read_csv('../input/tabular-playground-series-jun-2021/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-17T14:33:55.979536Z","iopub.execute_input":"2021-06-17T14:33:55.979895Z","iopub.status.idle":"2021-06-17T14:33:57.921496Z","shell.execute_reply.started":"2021-06-17T14:33:55.979863Z","shell.execute_reply":"2021-06-17T14:33:57.920544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conditions = [\n    (train.target == \"Class_1\"), (train.target == \"Class_2\"), (train.target == \"Class_3\"),\n    (train.target == \"Class_4\"), (train.target == \"Class_5\"), (train.target == \"Class_6\"),\n    (train.target == \"Class_7\"), (train.target == \"Class_8\"), (train.target == \"Class_9\")\n]\nchoices = [0, 1, 2, 3, 4, 5, 6, 7, 8]\ntrain[\"target\"] = np.select(conditions, choices)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T14:33:57.92262Z","iopub.execute_input":"2021-06-17T14:33:57.922885Z","iopub.status.idle":"2021-06-17T14:33:58.048765Z","shell.execute_reply.started":"2021-06-17T14:33:57.922857Z","shell.execute_reply":"2021-06-17T14:33:58.047792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Simple preprocessing","metadata":{}},{"cell_type":"code","source":"full = pd.concat([train, test], axis=0)\nfull.iloc[:,1:] = full.iloc[:,1:].applymap(str)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T14:33:58.050099Z","iopub.execute_input":"2021-06-17T14:33:58.050497Z","iopub.status.idle":"2021-06-17T14:34:05.852677Z","shell.execute_reply.started":"2021-06-17T14:33:58.050458Z","shell.execute_reply":"2021-06-17T14:34:05.851783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nunique = full.nunique()\ntypes = full.dtypes\n\ncategorical_columns = []\ncategorical_dims =  {}\nfor col in full.drop(['id', 'target'], axis=1).columns:\n    if types[col] == 'object' or nunique[col] < 200:\n        print(col, full[col].nunique())\n        l_enc = LabelEncoder()\n        full[col] = full[col].fillna(\"X\")\n        full[col] = l_enc.fit_transform(full[col].values)\n        categorical_columns.append(col)\n        categorical_dims[col] = len(l_enc.classes_)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-17T14:34:05.853827Z","iopub.execute_input":"2021-06-17T14:34:05.854084Z","iopub.status.idle":"2021-06-17T14:34:38.379469Z","shell.execute_reply.started":"2021-06-17T14:34:05.85406Z","shell.execute_reply":"2021-06-17T14:34:38.378437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define cat features for embeddings","metadata":{}},{"cell_type":"code","source":"unused_feat = ['Set', 'id']\n\nfeatures = [ col for col in full.columns if col not in unused_feat+['target']] \n\ncat_idxs = [ i for i, f in enumerate(features) if f in categorical_columns]\ncat_dims = [ categorical_dims[f] for i, f in enumerate(features) if f in categorical_columns]\ncat_emb_dims = np.ceil(np.log(cat_dims)).astype(np.int).tolist()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T14:34:38.380713Z","iopub.execute_input":"2021-06-17T14:34:38.381118Z","iopub.status.idle":"2021-06-17T14:34:38.38786Z","shell.execute_reply.started":"2021-06-17T14:34:38.381075Z","shell.execute_reply":"2021-06-17T14:34:38.386675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = full[features].values[full.id.isin(train.id)]\ny = full['target'].values[full.id.isin(train.id)]\n\nX_test = full[features].values[full.id.isin(test.id)]\ny_test = full['target'].values[full.id.isin(test.id)]","metadata":{"execution":{"iopub.status.busy":"2021-06-17T14:34:38.390031Z","iopub.execute_input":"2021-06-17T14:34:38.390359Z","iopub.status.idle":"2021-06-17T14:34:38.699396Z","shell.execute_reply.started":"2021-06-17T14:34:38.390324Z","shell.execute_reply":"2021-06-17T14:34:38.698496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"see: https://www.kaggle.com/gomes555/tps-jun2021-lightautoml","metadata":{}},{"cell_type":"code","source":"oof_lightautoml=pd.read_csv('../input/tps-jun2021-lightautoml/oof_lightautoml.csv')\nsub_lightautoml=pd.read_csv('../input/tps-jun2021-lightautoml/sub_lightautoml.csv')\n\noof_lightautoml = oof_lightautoml.drop('id', axis=1)\noof_lightautoml.columns = ['pred_lightautoml' + str(i) for i in range(1, 10)]\n\nsub_lightautoml = sub_lightautoml.drop('id', axis=1)\nsub_lightautoml.columns = ['pred_lightautoml' + str(i) for i in range(1, 10)]\n\nX = pd.concat([pd.DataFrame(X), oof_lightautoml], axis=1).values\nX_test = pd.concat([pd.DataFrame(X_test), sub_lightautoml], axis=1).values","metadata":{"execution":{"iopub.status.busy":"2021-06-17T14:37:23.05443Z","iopub.execute_input":"2021-06-17T14:37:23.054921Z","iopub.status.idle":"2021-06-17T14:37:23.598757Z","shell.execute_reply.started":"2021-06-17T14:37:23.054892Z","shell.execute_reply":"2021-06-17T14:37:23.598011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Self Supervised Training","metadata":{}},{"cell_type":"code","source":"# TabNetPretrainer\nunsupervised_model = TabNetPretrainer(\n    n_d=64, n_a=64,\n    n_steps=3,\n    n_independent=1,\n    n_shared=1,\n    cat_idxs=cat_idxs,\n    cat_dims=cat_dims,\n    cat_emb_dim=cat_emb_dims,\n    gamma=1.2,\n    lambda_sparse=0.,\n    optimizer_fn=torch.optim.Adam,\n    optimizer_params=dict(lr=2e-2),\n    mask_type='sparsemax', \n    scheduler_params=dict(mode=\"min\",\n                          patience=3,\n                          min_lr=1e-5,\n                          factor=0.5,),\n    scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n    verbose=1\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unsupervised_model.fit(\n    X_train=X_test,\n    eval_set=[X],\n    max_epochs=30 , \n    patience=25,\n    batch_size=256,\n    virtual_batch_size=256,\n    num_workers=1,\n    drop_last=True,\n    pretraining_ratio=0.5\n\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make reconstruction from a dataset\nreconstructed_X, embedded_X = unsupervised_model.predict(X)\nassert(reconstructed_X.shape==embedded_X.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#unsupervised_explain_matrix, unsupervised_masks = unsupervised_model.explain(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fig, axs = plt.subplots(1, 3, figsize=(20,20))\n#\n#for i in range(3):\n#    axs[i].imshow(unsupervised_masks[i][:50])\n#    axs[i].set_title(f\"mask {i}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Save and load\nunsupervised_model.save_model('./test_pretrain')\nloaded_pretrain = TabNetPretrainer()\nloaded_pretrain.load_model('./test_pretrain.zip')","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Baseline","metadata":{}},{"cell_type":"code","source":"N_SPLITS=5\n\nskf = StratifiedKFold(n_splits=N_SPLITS, random_state=2021, shuffle=True)\ntab_pred = 0\ntab_oof = np.zeros((X.shape[0], 9))\n\nfor fold, (train_index, valid_index) in enumerate(skf.split(X, y)):\n    print(f\"➜ FOLD :{fold}\")\n    X_train, X_valid = X[train_index], X[valid_index]\n    y_train, y_valid = y[train_index], y[valid_index]\n    \n    start = time.time()\n    \n    clf = TabNetClassifier(n_d=64,\n                           n_a=64,\n                           n_steps=3,\n                           gamma=1.2,\n                           n_independent=1,\n                           n_shared=1,\n                           lambda_sparse=1e-5,\n                           seed=0,\n                           clip_value=2,\n                           cat_idxs=cat_idxs,\n                           cat_dims=cat_dims,\n                           cat_emb_dim=cat_emb_dims,\n                           optimizer_fn=torch.optim.Adam,\n                           optimizer_params=dict(lr=1e-1, weight_decay=1e-5),\n                           scheduler_params=dict(mode='min',\n                                                        factor=0.5,\n                                                        patience=3,\n                                                        is_batch_level=False,),\n                           scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n                           mask_type='sparsemax',\n                           verbose=1\n                          )\n\n    clf.fit(\n        X_train=X_train, y_train=y_train,\n        eval_set=[(X_train, y_train), (X_valid, y_valid)],\n        eval_name=['train', 'valid'],\n        eval_metric=['logloss'],\n        max_epochs=100 ,\n        batch_size=2048, \n        virtual_batch_size=256,\n        num_workers=0,\n        drop_last=True,\n        pin_memory=True,\n        patience=10,\n        from_unsupervised=loaded_pretrain\n    )\n    \n    tab_oof[valid_index,:] = clf.predict_proba(X_valid)\n    tab_pred += clf.predict_proba(X_test)/N_SPLITS\n    \n    tab_logloss = log_loss(y_valid, tab_oof[valid_index])\n    print(f\"score: {tab_logloss:.6f} \")\n    print(f\"elapsed: {time.time()-start:.2f} sec\\n\")\n    \n    del clf\n    \ntab_logloss = log_loss(y, tab_oof)\nprint(f\"Final logloss score: {tab_logloss} ✔️ \")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.iloc[:, 1:] = tab_pred\nsub.to_csv(\"sub_tab_default.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof_tab = pd.concat([train.id,\n                     pd.DataFrame(tab_oof, \n                                  columns=[\"Class_1\", \"Class_2\", \"Class_3\",\n                                           \"Class_4\", \"Class_5\", \"Class_6\",\n                                           \"Class_7\", \"Class_8\", \"Class_9\"])],\n                    axis=1)\noof_tab.to_csv(\"oof_tab_optuned.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Draft","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"#X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=314, stratify=y)\n#\n#X_train, y_train = X_train.values, y_train.values\n#X_val, y_val = X_val.values, y_val.values\n#\n#scaler = MinMaxScaler()\n#scaler.fit(X_train)\n#\n#X_train = scaler.transform(X_train)\n#X_val = scaler.transform(X_val)\n#X_test = scaler.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## TabNetPretrainer\n#unsupervised_model = TabNetPretrainer(\n#    optimizer_fn=torch.optim.Adam,\n#    optimizer_params=dict(lr=2e-2),\n#    mask_type='sparsemax'\n#)\n#\n#unsupervised_model.fit(\n#    X_train=X_train,\n#    eval_set=[X_val],\n#    pretraining_ratio=0.5,\n#)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model = TabNetClassifier(verbose = 1)\n#\n#model.fit(\n#    X_train=X_train, y_train=y_train,\n#    eval_set=[(X_train, y_train), (X_val, y_val)],\n#    eval_name=['train', 'val'],\n#    eval_metric=['logloss'],\n#    max_epochs=30, \n#    patience=15,\n#    from_unsupervised=unsupervised_model\n#)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(12, 5))\n#\n## plot losses\n#axs[0].plot(model.history['loss'])\n#\n## plot logloss\n#axs[1].plot(model.history['train_logloss'])\n#axs[1].plot(model.history['val_logloss'])\n#\n## plot learning rates\n#axs[2].plot(model.history['lr'])\n#\n#fig.tight_layout()\n#plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#val_preds = model.predict_proba(X_val)\n#val_logloss = [log_loss(y_pred=task_pred, y_true=y_val[:,1])\n#             for task_idx, task_pred in enumerate(val_preds)]\n#\n#np.mean(val_logloss)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#task_preds = model.predict_proba(X_test)\n#tab_pred = np.mean(task_preds, axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sub.iloc[:, 1:] = tab_pred\n#sub.to_csv(\"sub_tab_default.csv\", index=False)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Optuna","metadata":{}},{"cell_type":"code","source":"#def objective(trial):\n#    \n#    global X, y, X_test\n#    \n#    hyperparams = {\n#        'n_a_d': trial.suggest_categorical('n_a_d', [8, 16, 24, 32, 64, 128]),\n#        'n_steps': trial.suggest_int('n_steps', 3, 10, 1),\n#        'gamma': trial.suggest_categorical('gamma', [1.0, 1.2, 1.5, 2.0]),\n#        'lambda': trial.suggest_categorical('lambda', [0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001, 0]),\n#        'batch_size': trial.suggest_categorical('batch_size', [1024, 2048, 4096, 8192, 16384, 32768]),\n#        'virtual_batch_size': trial.suggest_categorical('virtual_batch_size', [128, 256, 512, 1024]),\n#        'lr': trial.suggest_categorical('lr', [0.005, 0.01, 0.02, 0.025]),\n#        'gamma_decay': trial.suggest_categorical('gamma_decay', [0.4, 0.8, 0.9, 0.95]),\n#        #'mask_type': trial.suggest_categorical('mask_type', ['entmax', 'sparsemax']),\n#        'batch_momentum': trial.suggest_categorical('batch_momentum', [0.6, 0.7, 0.8, 0.9, 0.95, 0.98]),\n#    }\n#    \n#    model = TabNetMultiTaskClassifier(\n#        n_d=hyperparams['n_a_d'],\n#        n_a=hyperparams['n_a_d'],\n#        gamma=hyperparams['gamma'],\n#        optimizer_fn=torch.optim.Adam,\n#        optimizer_params={'lr':hyperparams['lr']},\n#        scheduler_params={\"step_size\":hyperparams['n_steps'],\n#                          \"gamma\":hyperparams['gamma_decay']},\n#        scheduler_fn=torch.optim.lr_scheduler.StepLR,\n#        mask_type='entmax',\n#        lambda_sparse=hyperparams['lambda'],\n#        momentum=hyperparams['batch_momentum'],\n#        verbose = 0\n#    )\n#\n#    model.fit(\n#        X_train=X_train, y_train=y_train,\n#        eval_set=[(X_train, y_train), (X_val, y_val)],\n#        eval_name=['train', 'val'],\n#        max_epochs=MAX_EPOCHS, \n#        patience=PATIENCE,\n#        batch_size=hyperparams['batch_size'],\n#        virtual_batch_size=hyperparams['virtual_batch_size'],\n#        num_workers=0,\n#        drop_last=False\n#    )\n#\n#    val_preds = model.predict_proba(X_val)\n#    val_logloss = [log_loss(y_pred=task_pred, y_true=y_val[:,task_idx])\n#                 for task_idx, task_pred in enumerate(val_preds)]\n#    \n#    del model\n#\n#    return np.mean(val_logloss)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#study = optuna.create_study(direction='minimize',\n#                            sampler=optuna.samplers.TPESampler(multivariate=True, seed=123))\n#\n#study.optimize(objective, \n#               timeout=60*60*6, \n#               #n_trials=2, \n#               gc_after_trial=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#study.best_value","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot_optimization_history(study)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#optuna.visualization.plot_parallel_coordinate(study)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot_param_importances(study)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#study.best_params","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final Model","metadata":{}},{"cell_type":"code","source":"#tab_oof = np.zeros((X.shape[0], 9))\n#tab_pred = 0\n#\n#for fold, (train_idx, val_idx) in enumerate(kf.split(X=X, y=y)):\n#    print(f\"➜ FOLD :{fold}\")\n#    X_train = X.values[train_idx]\n#    y_train = y.values[train_idx]\n#    X_val = X.values[val_idx]\n#    y_val = y.values[val_idx]\n#    \n#    y_train = y_train.reshape(-1, 1)\n#    y_train = np.hstack([y_train]*NB_TASKS)\n#\n#    y_val = y_val.reshape(-1, 1)\n#    y_val = np.hstack([y_val]*NB_TASKS)\n#\n#    scaler = StandardScaler()\n#    scaler.fit(X_train)\n#    \n#    X_train = scaler.transform(X_train)\n#    X_val = scaler.transform(X_val)\n#    X_test = scaler.transform(X_test)\n#    \n#    \n#    start = time.time()\n#    \n#    model = TabNetMultiTaskClassifier(\n#        n_d=study.best_params['n_a_d'],\n#        n_a=study.best_params['n_a_d'],\n#        optimizer_fn=torch.optim.Adam,\n#        optimizer_params=dict(lr=0.2),\n#        scheduler_params={\"step_size\":study.best_params['n_steps'],\n#                          \"gamma\":study.best_params['gamma']},\n#        scheduler_fn=torch.optim.lr_scheduler.StepLR,\n#        mask_type='entmax',\n#        lambda_sparse=study.best_params['lambda'],\n#        momentum=study.best_params['batch_momentum'],\n#        verbose = 1\n#    )\n#    \n#    model.fit(\n#        X_train=X_train, y_train=y_train,\n#        eval_set=[(X_train, y_train), (X_val, y_val)],\n#        eval_name=['train', 'val'],\n#        max_epochs=MAX_EPOCHS, \n#        patience=PATIENCE,\n#        batch_size=BATCH_SIZE, \n#        virtual_batch_size=VIRTUAL_BATCH_SIZE,\n#        num_workers=0,\n#        drop_last=False\n#    )\n#    \n#    val_preds = model.predict_proba(X_val)\n#    val_logloss = [log_loss(y_pred=task_pred, y_true=y_val[:,task_idx])\n#                 for task_idx, task_pred in enumerate(val_preds)]\n#    \n#    tab_oof[val_idx,:] = np.mean(val_preds, axis=0)\n#    \n#    task_pred = model.predict_proba(X_test)\n#    tab_pred += np.mean(task_pred, axis=0) / K\n#    \n#    print(f\"score: {np.mean(val_logloss):.6f} \")\n#    print(f\"elapsed: {time.time()-start:.2f} sec\\n\")\n#    \n#    del model\n#\n#tab_logloss = log_loss(y, tab_oof)\n#print(f\"Final logloss score: {tab_logloss} ✔️ \")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#PATIENCE = 15","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#final_model = TabNetMultiTaskClassifier(\n#    n_d=study.best_params['n_a_d'],\n#    n_a=study.best_params['n_a_d'],\n#    gamma=study.best_params['gamma'],\n#    optimizer_fn=torch.optim.Adam,\n#    optimizer_params=dict(lr=study.best_params['lr']),\n#    scheduler_params={\"step_size\":study.best_params['n_steps'],\n#                      \"gamma\":study.best_params['gamma_decay']},\n#    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n#    mask_type='entmax',\n#    lambda_sparse=study.best_params['lambda'],\n#    momentum=study.best_params['batch_momentum'],\n#    verbose = 1\n#)\n#\n#final_model.fit(\n#    X_train=X_train, y_train=y_train,\n#    eval_set=[(X_train, y_train), (X_val, y_val)],\n#    eval_name=['train', 'val'],\n#    max_epochs=MAX_EPOCHS, \n#    patience=PATIENCE,\n#    batch_size=study.best_params['batch_size'], \n#    virtual_batch_size=study.best_params['virtual_batch_size']\n#)\n#\n#task_pred = final_model.predict_proba(X_test)\n#tab_pred = np.mean(task_pred, axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(12, 5))\n#\n## plot losses\n#axs[0].plot(final_model.history['loss'])\n#\n## plot logloss\n#axs[1].plot(final_model.history['train_logloss'])\n#axs[1].plot(final_model.history['val_logloss'])\n#\n## plot learning rates\n#axs[2].plot(final_model.history['lr'])\n#\n#fig.tight_layout()\n#plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sub","metadata":{}},{"cell_type":"code","source":"#sub.iloc[:, 1:] = tab_pred\n#sub.to_csv(\"sub_tab_optuned.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#oof_tab = pd.concat([train.id,\n#                     pd.DataFrame(tab_oof, \n#                              columns=[\"Class_1\", \"Class_2\", \"Class_3\",\n#                                       \"Class_4\", \"Class_5\", \"Class_6\",\n#                                       \"Class_7\", \"Class_8\", \"Class_9\"])],\n#                    axis=1)\n#oof_tab.to_csv(\"oof_tab_optuned.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sources\n\n- https://arxiv.org/pdf/1908.07442.pdf\n- https://reposhub.com/python/deep-learning/dreamquark-ai-tabnet.html\n- https://towardsdatascience.com/modelling-tabular-data-with-googles-tabnet-ba7315897bfb\n- https://github.com/google-research/google-research/blob/master/tabnet/tabnet_model.py\n- https://github.com/hussius/tabnet_fork/blob/master/opt_tabnet.py\n- https://www.kaggle.com/optimo/tabnetbaseline/","metadata":{}}]}