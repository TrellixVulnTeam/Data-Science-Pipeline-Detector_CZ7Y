{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Patch\nimport time\nimport random\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.decomposition import PCA\nfrom sklearn.neighbors import LocalOutlierFactor\n\n# Pandas setting to display more dataset rows and columns\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', None)\npd.set_option('display.float_format', lambda x: '%.5f' % x)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-24T05:58:17.855234Z","iopub.execute_input":"2021-06-24T05:58:17.855635Z","iopub.status.idle":"2021-06-24T05:58:22.071699Z","shell.execute_reply.started":"2021-06-24T05:58:17.85555Z","shell.execute_reply":"2021-06-24T05:58:22.070577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data import**","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/tabular-playground-series-jun-2021/train.csv\", low_memory=False)#, nrows=10000)\ntest = pd.read_csv(\"/kaggle/input/tabular-playground-series-jun-2021/test.csv\", low_memory=False)\ntrain.info(memory_usage=\"deep\")","metadata":{"execution":{"iopub.status.busy":"2021-06-24T05:58:22.073878Z","iopub.execute_input":"2021-06-24T05:58:22.074355Z","iopub.status.idle":"2021-06-24T05:58:24.59486Z","shell.execute_reply.started":"2021-06-24T05:58:22.074292Z","shell.execute_reply":"2021-06-24T05:58:24.593408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info(memory_usage=\"deep\")","metadata":{"execution":{"iopub.status.busy":"2021-06-24T05:58:24.597396Z","iopub.execute_input":"2021-06-24T05:58:24.598066Z","iopub.status.idle":"2021-06-24T05:58:24.63429Z","shell.execute_reply.started":"2021-06-24T05:58:24.598019Z","shell.execute_reply":"2021-06-24T05:58:24.633191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Colors to be used for plots\ncolors = [\"lightcoral\", \"sandybrown\", \"darkorange\", \"mediumseagreen\",\n          \"lightseagreen\", \"cornflowerblue\", \"mediumpurple\", \"palevioletred\",\n          \"lightskyblue\", \"sandybrown\", \"yellowgreen\", \"indianred\",\n          \"lightsteelblue\", \"mediumorchid\", \"deepskyblue\"]","metadata":{"execution":{"iopub.status.busy":"2021-06-24T05:58:24.636542Z","iopub.execute_input":"2021-06-24T05:58:24.636986Z","iopub.status.idle":"2021-06-24T05:58:24.643027Z","shell.execute_reply.started":"2021-06-24T05:58:24.636942Z","shell.execute_reply":"2021-06-24T05:58:24.641425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(ncols=2, nrows=1, figsize=(18, 8), gridspec_kw={'width_ratios': [2, 1]})\n\nbars = axs[0].bar(train[\"target\"].value_counts().sort_index().index,\n                  train[\"target\"].value_counts().sort_index().values,\n                  color=colors,\n                  edgecolor=\"black\")\naxs[0].set_title(\"Target distribution\", fontsize=20, pad=15)\naxs[0].set_ylabel(\"Count\", fontsize=14, labelpad=15)\naxs[0].set_xlabel(\"Target label\", fontsize=14, labelpad=10)\naxs[0].bar_label(bars, train[\"target\"].value_counts().sort_index().values,\n                 padding=3, fontsize=12)\naxs[0].bar_label(bars, [f\"{x:2.1f}%\" for x in train[\"target\"].value_counts().sort_index().values/2000],\n                 padding=-20, fontsize=12)\naxs[0].margins(0.025, 0.06)\naxs[0].grid(axis=\"y\")\n\npie = axs[1].pie(train[\"target\"].value_counts(sort=False).sort_index().values,\n                 labels=train[\"target\"].value_counts(sort=False).sort_index().index,\n                 colors=colors,\n                 rotatelabels=True,\n                 textprops={\"fontsize\": 14})\naxs[1].axis(\"equal\")\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-06-24T05:58:24.644833Z","iopub.execute_input":"2021-06-24T05:58:24.64581Z","iopub.status.idle":"2021-06-24T05:58:25.386667Z","shell.execute_reply.started":"2021-06-24T05:58:24.645744Z","shell.execute_reply":"2021-06-24T05:58:25.385436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check if these classes are evenly distributed in the dataset.","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame()\ndf[\"id\"] = train[\"id\"]\ndf[\"target\"] = train[\"target\"]\ndf[\"id\"] = pd.cut(df[\"id\"], np.arange(0, 201000, 1000), right=False)\nvalues = df.groupby(\"id\")[\"target\"].value_counts(sort=False).values\nclasses = [\"Class_\" + str(x) for x in np.arange(9)]\n\ncols = 3\nrows = 3\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,rows*4), sharey=False)\n\nplt.subplots_adjust(hspace = 0.35)\n\ni=0\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        axs[r, c].plot(np.arange(0, 200, 1),\n                       [values[x] for x in np.arange(i, 1800, 9)],\n                       color=colors[i])\n        axs[r, c].set_title(classes[i], fontsize=12, pad=5)\n        axs[r, c].set_xticks(np.arange(0, 250, 50))\n        axs[r, c].set_xticklabels([str(int(x))+\"k\" for x in axs[r, c].get_xticks()])\n        axs[r, c].set_xlabel(\"Dataframe id\")\n        axs[r, c].set_ylabel(\"Class labels qty per 1k rows\")\n        axs[r, c].set_ylim(0, 320)\n        i+=1\nfig.suptitle(\"Class labels distribution in the train dataset\", fontsize=20)\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-06-24T05:58:25.387922Z","iopub.execute_input":"2021-06-24T05:58:25.388193Z","iopub.status.idle":"2021-06-24T05:58:26.637424Z","shell.execute_reply.started":"2021-06-24T05:58:25.388165Z","shell.execute_reply":"2021-06-24T05:58:26.636331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So it looks like class labels are distributed pretty evenly across the dataset.","metadata":{}},{"cell_type":"markdown","source":"## Original features value distribution in the train dataset","metadata":{}},{"cell_type":"code","source":"def make_data_plots(df, i=0):\n    \"\"\"\n    Makes value distribution histogram plots for a given dataframe features\n    \"\"\"\n    columns = df.drop([\"target\", \"id\"], axis=1).columns.values\n\n    cols = 3\n    rows = (len(columns) - i) // cols + 1\n\n    fig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,rows*4), sharey=True)\n    plt.subplots_adjust(hspace = 0.2)\n    for r in np.arange(0, rows, 1):\n        for c in np.arange(0, cols, 1):\n            if i >= len(columns):\n                axs[r, c].set_visible(False)\n            else:\n                axs[r, c].hist(df[columns[i]].values, bins = 30,\n                               color=random.choice(colors),\n                               edgecolor=\"black\")\n                axs[r, c].set_title(columns[i], fontsize=12, pad=5)\n            i+=1","metadata":{"execution":{"iopub.status.busy":"2021-06-24T05:58:26.639073Z","iopub.execute_input":"2021-06-24T05:58:26.639635Z","iopub.status.idle":"2021-06-24T05:58:26.651479Z","shell.execute_reply.started":"2021-06-24T05:58:26.639592Z","shell.execute_reply":"2021-06-24T05:58:26.650376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_data_plots(train)","metadata":{"execution":{"iopub.status.busy":"2021-06-24T05:58:26.655876Z","iopub.execute_input":"2021-06-24T05:58:26.656368Z","iopub.status.idle":"2021-06-24T05:58:44.185325Z","shell.execute_reply.started":"2021-06-24T05:58:26.656322Z","shell.execute_reply":"2021-06-24T05:58:44.184189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Original features nonzero value distribution in the train dataset","metadata":{}},{"cell_type":"code","source":"def make_nonzero_data_plots(df, i=0):\n    \"\"\"\n    Makes nonzero value distribution histogram plots for a given dataframe features\n    \"\"\"    \n    \n    columns = df.drop([\"target\", \"id\"], axis=1).columns.values\n\n    cols = 3\n    rows = (len(columns) - i) // cols + 1\n\n    fig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,rows*4), sharey=False)\n    plt.subplots_adjust(hspace = 0.2)\n    for r in np.arange(0, rows, 1):\n        for c in np.arange(0, cols, 1):\n            if i >= len(columns):\n                axs[r, c].set_visible(False)\n            else:\n                axs[r, c].hist(df[df[columns[i]] > 0][columns[i]].values, bins = 30,\n                               color=random.choice(colors),\n                               edgecolor=\"black\")\n                axs[r, c].set_title(columns[i], fontsize=12, pad=5)\n            i+=1","metadata":{"execution":{"iopub.status.busy":"2021-06-24T05:58:44.188269Z","iopub.execute_input":"2021-06-24T05:58:44.188741Z","iopub.status.idle":"2021-06-24T05:58:44.199531Z","shell.execute_reply.started":"2021-06-24T05:58:44.188696Z","shell.execute_reply":"2021-06-24T05:58:44.198213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_nonzero_data_plots(train)","metadata":{"execution":{"iopub.status.busy":"2021-06-24T05:58:44.201445Z","iopub.execute_input":"2021-06-24T05:58:44.202416Z","iopub.status.idle":"2021-06-24T05:59:03.019193Z","shell.execute_reply.started":"2021-06-24T05:58:44.202369Z","shell.execute_reply":"2021-06-24T05:59:03.012975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fraction of nonzero values in the both datasets","metadata":{}},{"cell_type":"code","source":"x = -1*np.arange(len(test.drop([\"id\"], axis=1).columns))\n\nfig, ax = plt.subplots(figsize=(16, 30))\nbars = ax.barh(x+0.2, train.drop([\"id\", \"target\"], axis=1).astype(bool).sum(axis=0).values / train.shape[0],\n               height=0.4, color=\"cornflowerblue\", label=\"Train dataset\", edgecolor=\"black\")\nbars2 = ax.barh(x-0.2, test.drop([\"id\"], axis=1).astype(bool).sum(axis=0).values / test.shape[0],\n                height=0.4, color=\"palevioletred\", label=\"Test dataset\", edgecolor=\"black\")\nax.set_title(\"Fraction of nonzero values in the both datasets\", fontsize=30, pad=15)\nax.set_ylabel(\"Feature names\", fontsize=20, labelpad=15)\nax.set_xlabel(\"Fraction of nonzero values\", fontsize=20, labelpad=15)\nax.set_xticks(np.arange(0, 0.8, 0.05))\nax.set_yticks(x)\nax.set_yticklabels(list(test.drop([\"id\"], axis=1).columns.values))\nax.tick_params(axis=\"x\", labelsize=15)\nax.tick_params(axis=\"y\", labelsize=14)\nax.grid(axis=\"x\")\nax.legend(fontsize=15)\nax2 = ax.secondary_xaxis('top')\nax2.set_xticks(np.arange(0, 0.8, 0.05))\nax2.set_xlabel(\"Fraction of nonzero values\", fontsize=20, labelpad=15)\nax2.tick_params(axis=\"x\", labelsize=15)\nplt.margins(0.05, 0.01)","metadata":{"execution":{"iopub.status.busy":"2021-06-24T05:59:03.021278Z","iopub.execute_input":"2021-06-24T05:59:03.021794Z","iopub.status.idle":"2021-06-24T05:59:05.985105Z","shell.execute_reply.started":"2021-06-24T05:59:03.021748Z","shell.execute_reply":"2021-06-24T05:59:05.984037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## PCA","metadata":{}},{"cell_type":"code","source":"pca = PCA(n_components=2)\nX_reduced = pd.DataFrame(pca.fit_transform(train.drop([\"id\", \"target\"], axis=1)))\n\nfig, ax = plt.subplots(figsize=(16, 9))\nax.scatter(X_reduced[0], X_reduced[1], c=LabelEncoder().fit_transform(train[\"target\"]), cmap=\"tab10\")\nax.set_title(\"Scatter plot of the original train set reduced to 2 dimensions\", fontsize=20, pad=15)\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-06-24T05:59:05.986793Z","iopub.execute_input":"2021-06-24T05:59:05.987266Z","iopub.status.idle":"2021-06-24T05:59:13.069912Z","shell.execute_reply.started":"2021-06-24T05:59:05.987219Z","shell.execute_reply":"2021-06-24T05:59:13.068752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()\nX_scaled = scaler.fit_transform(train.drop([\"id\", \"target\"], axis=1))\n\npca = PCA(n_components=2)\nX_reduced = pd.DataFrame(pca.fit_transform(X_scaled))\n\nfig, ax = plt.subplots(figsize=(16, 9))\nax.scatter(X_reduced[0], X_reduced[1], c=LabelEncoder().fit_transform(train[\"target\"]), cmap=\"tab10\")\nax.set_title(\"Scatter plot of the scaled train set reduced to 2 dimensions\", fontsize=20, pad=15)\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-06-24T05:59:13.071391Z","iopub.execute_input":"2021-06-24T05:59:13.072156Z","iopub.status.idle":"2021-06-24T05:59:20.12051Z","shell.execute_reply.started":"2021-06-24T05:59:13.072097Z","shell.execute_reply":"2021-06-24T05:59:20.119419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, the data does not have distinct clusters after reducing dimensions.","metadata":{}},{"cell_type":"markdown","source":"## Detecting anomalies with IsolationForest","metadata":{}},{"cell_type":"code","source":"# iso_forest = IsolationForest(n_jobs=-1, random_state=42, n_estimators=3000)\n# iso_forest.fit(train.drop([\"id\", \"target\"], axis=1))\n# scores = iso_forest.decision_function(train.drop([\"id\", \"target\"], axis=1))\n# to_drop = train.loc[scores < 0].index\n# print(f\"Anomalies found in the train dataset: {(scores < 0).sum()}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-24T05:59:20.122093Z","iopub.execute_input":"2021-06-24T05:59:20.122754Z","iopub.status.idle":"2021-06-24T06:10:01.857345Z","shell.execute_reply.started":"2021-06-24T05:59:20.12271Z","shell.execute_reply":"2021-06-24T06:10:01.854045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Target distribution in detected anomalies\n# train.loc[(scores < 0), \"target\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-24T06:10:01.860134Z","iopub.execute_input":"2021-06-24T06:10:01.86128Z","iopub.status.idle":"2021-06-24T06:10:01.877601Z","shell.execute_reply.started":"2021-06-24T06:10:01.86123Z","shell.execute_reply":"2021-06-24T06:10:01.87616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Detecting anomalies with LocalOutlierFactor","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\nX_scaled = scaler.fit_transform(train.drop([\"id\", \"target\"], axis=1))\nlof = LocalOutlierFactor(n_jobs=-1)\nlof.fit(X_scaled)\nscores = lof.negative_outlier_factor_\nscore_threshold = -1.93\nto_drop = train.loc[scores < score_threshold].index\nprint(f\"Anomalies found in the train dataset: {(scores < score_threshold).sum()}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Target distribution in detected anomalies\ntrain.loc[(scores < score_threshold), \"target\"].value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data preparation**","metadata":{}},{"cell_type":"code","source":"# Dropping anomaly rows detected with Isolation Forest\ntrain.drop(axis=0, index=set(to_drop), inplace=True)\ntrain.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.drop([\"id\", \"target\"], axis=1).duplicated(keep=False).sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-24T06:10:01.880135Z","iopub.execute_input":"2021-06-24T06:10:01.881563Z","iopub.status.idle":"2021-06-24T06:10:02.280867Z","shell.execute_reply.started":"2021-06-24T06:10:01.881514Z","shell.execute_reply":"2021-06-24T06:10:02.279373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are some duplicates in the dataset. If they have identical feature but different target values it may decrease machine learning performance.","metadata":{}},{"cell_type":"code","source":"def delete_duplicates(df):\n    \"\"\"\n    Finds duplicates in a given DataFrame and deletes rows with identical features values but different target. \n    \"\"\"\n    \n    # Copying duplicate rows in a new dataset and getting their indices\n    idx = df.drop([\"id\", \"target\"], axis=1).duplicated(keep=False)\n    duplicates = df.loc[idx == True].copy()\n    features = [x for x in duplicates.columns if \"feature\" in x]\n    idx = duplicates[\"id\"]\n    \n    # Checking if which rows with equal feature values have different target\n    indx_to_drop = []\n    for index in idx:\n        for row in idx:\n            if (row != index) and (row not in indx_to_drop):\n                if duplicates.loc[index, features].equals(duplicates.loc[row, features]):\n                    if duplicates.loc[index, \"target\"] != duplicates.loc[row, \"target\"]:\n    #                     print(f\"Found duplicates with different targets: {index} - {duplicates.loc[index, 'target']} and {row} - {duplicates.loc[row, 'target']}\")\n                        indx_to_drop.append(index)\n                        indx_to_drop.append(row)\n    #                 else:\n    #                     print(f\"Found duplicates with the same target: {index} and {row}\")\n    \n    # Reporting results\n    print(f\"There are {len(duplicates['id'])} duplicated rows in the dataset.\")\n    print(f\"{len(set(indx_to_drop))} of them have different target. They will be deleted from the dataset.\")\n    print(f\"The datatframe has {len(df['id'])} rows.\")\n    df.drop(axis=0, index=set(indx_to_drop), inplace=True)\n    print(f\"After duplicated deletion there are {len(df['id'])} rows.\")\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-06-24T06:10:02.283117Z","iopub.execute_input":"2021-06-24T06:10:02.283596Z","iopub.status.idle":"2021-06-24T06:10:02.298016Z","shell.execute_reply.started":"2021-06-24T06:10:02.283549Z","shell.execute_reply":"2021-06-24T06:10:02.296552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = delete_duplicates(train)","metadata":{"execution":{"iopub.status.busy":"2021-06-24T06:10:02.300582Z","iopub.execute_input":"2021-06-24T06:10:02.301865Z","iopub.status.idle":"2021-06-24T06:11:20.690511Z","shell.execute_reply.started":"2021-06-24T06:10:02.301818Z","shell.execute_reply":"2021-06-24T06:11:20.689215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Target feature encoding\nencoder = LabelEncoder()\ntrain[\"target\"] = encoder.fit_transform(train[\"target\"])\ntrain[\"target\"].value_counts(sort=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-24T06:11:20.692731Z","iopub.execute_input":"2021-06-24T06:11:20.693872Z","iopub.status.idle":"2021-06-24T06:11:20.796649Z","shell.execute_reply.started":"2021-06-24T06:11:20.69359Z","shell.execute_reply":"2021-06-24T06:11:20.795233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def transform_dataset(data):\n    \"\"\"\n    Adds new custom features and transforms original features into custom categories\n    \"\"\"\n    \n    # Copying features in a temporary dataset which will be transformed with MinMaxScaler\n    df = data[[x for x in data.columns if \"feature_\" in x]].copy()\n    \n    # Adding custom features\n    data[\"feature_75\"] = df.max(axis=1)\n    data[\"feature_76\"] = df.mean(axis=1)\n    data[\"feature_77\"] = df.median(axis=1)\n    data[\"feature_78\"] = df.nunique(axis=1)\n    data[\"feature_79\"] = (df == 0).astype(int).sum(axis=1)\n    data[\"feature_80\"] = (df != 0).sum(axis=1)\n    data[\"feature_81\"] = (df == 0).astype(int).sum(axis=1) / 75\n    data[\"feature_82\"] = (df != 0).sum(axis=1) / 75\n    \n    \n    # Scaling original features and adding new features basing on them\n    scaled_df = pd.DataFrame(index = data.index.values, columns = df.columns.values)\n    for col in df.columns.values:\n        scaler = MinMaxScaler()\n        scaled_df[col] = scaler.fit_transform(np.array(df[col]).reshape(-1, 1))\n    data[\"feature_83\"] = scaled_df.mean(axis=1)\n    data[\"feature_84\"] = (scaled_df == 1).sum(axis=1)\n    \n    # Cutting original features into custom intevals [0, 1), [1, 15), [15, 30) ...\n    intervals = np.insert(np.arange(15, 370, 15), 0, [0, 1])\n    intervals_text = pd.cut(train[\"feature_0\"], intervals, right=False).value_counts().sort_index().index.astype(\"string\")\n    map_dict = dict(zip(intervals_text, list(np.arange(len(intervals_text)))))\n    for i, column in enumerate(data.drop([\"id\"], axis=1).columns):\n        if ((i < 75) and (column!=\"target\")):\n            data[column] = pd.cut(data[column], intervals, right=False).astype(\"string\")\n            data[column].replace(map_dict, inplace=True)\n    \n    return data","metadata":{"execution":{"iopub.status.busy":"2021-06-24T06:11:20.799042Z","iopub.execute_input":"2021-06-24T06:11:20.799497Z","iopub.status.idle":"2021-06-24T06:11:20.821012Z","shell.execute_reply.started":"2021-06-24T06:11:20.79943Z","shell.execute_reply":"2021-06-24T06:11:20.819516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transforming the train dataset and making value distribution plots for new custom features\nX_train = transform_dataset(train.copy())\nmake_data_plots(X_train, i=75)","metadata":{"execution":{"iopub.status.busy":"2021-06-24T06:11:20.823344Z","iopub.execute_input":"2021-06-24T06:11:20.824011Z","iopub.status.idle":"2021-06-24T06:21:48.342868Z","shell.execute_reply.started":"2021-06-24T06:11:20.823964Z","shell.execute_reply":"2021-06-24T06:21:48.341507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Machine Learning**","metadata":{}},{"cell_type":"code","source":"X_train.drop([\"id\", \"target\"], axis=1, inplace=True)\ny_train = train[\"target\"]\nX_test = transform_dataset(test).drop(\"id\", axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-24T06:21:48.34541Z","iopub.execute_input":"2021-06-24T06:21:48.345888Z","iopub.status.idle":"2021-06-24T06:27:03.678811Z","shell.execute_reply.started":"2021-06-24T06:21:48.345833Z","shell.execute_reply":"2021-06-24T06:27:03.677695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameters tuning using Optuna","metadata":{}},{"cell_type":"markdown","source":"The code below is commented in order to save runtime.","metadata":{}},{"cell_type":"code","source":"# def train_optuna_cb(trial, X=X_train, y=y_train):\n#     \"\"\"\n#     A function to train a model using different hyperparamerters combinations provided by Optuna. \n#     Log loss of validation data predictions is returned to estimate hyperparameters effectiveness.\n#     \"\"\"\n#     cat_features = [x for x in X.columns[:75]]\n#     preds = 0\n#     split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n#     for train_idx, valid_idx in split.split(X, y):\n#         X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n#         y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n        \n#         # A set of hyperparameters to optimize by optuna\n#         cb_params = {\n#                  \"iterations\": trial.suggest_int('iterations', 1000, 10000),\n#                  \"learning_rate\": trial.suggest_float('learning_rate', 0.001, 1.0),\n#                  \"depth\": trial.suggest_int('depth', 1, 6),\n#                  \"loss_function\": 'MultiClass',\n#                  \"eval_metric\": 'MultiClass',\n#                  \"leaf_estimation_method\": trial.suggest_categorical(\"leaf_estimation_method\", [\"Newton\", \"Gradient\"]),#, \"Exact\"]),\n#                  \"od_type\": \"Iter\",\n#                  \"early_stopping_rounds\": 500,\n#                  \"l2_leaf_reg\": trial.suggest_float('l2_leaf_reg', 0.00001, 10),\n#                  \"random_strength\": trial.suggest_float('random_strength', 1.0, 2.0),\n#                  \"bagging_temperature\": trial.suggest_float('bagging_temperature', 0.0, 10.0),\n#                  \"border_count\": 254,\n#                  \"use_best_model\": trial.suggest_categorical(\"use_best_model\", [False, True]),\n#                  \"grow_policy\": \"SymmetricTree\",#trial.suggest_categorical(\"grow_policy\", [\"SymmetricTree\", \"Depthwise\", \"Lossguide\"]),\n# #                  \"max_leaves\": trial.suggest_int('max_leaves', 1, 64),\n#                  \"task_type\": \"GPU\",\n#                     }\n            \n#         model = CatBoostClassifier(**cb_params)\n#         model.fit(\n#                     X_train, y_train,\n#                     eval_set=(X_valid, y_valid),\n#                     verbose=False,\n#                     cat_features=cat_features\n#                 )\n\n#         oof = model.predict_proba(X_valid)\n    \n#     return log_loss(y_valid, oof)","metadata":{"execution":{"iopub.status.busy":"2021-06-24T06:27:03.686237Z","iopub.execute_input":"2021-06-24T06:27:03.686837Z","iopub.status.idle":"2021-06-24T06:27:03.694102Z","shell.execute_reply.started":"2021-06-24T06:27:03.686777Z","shell.execute_reply":"2021-06-24T06:27:03.692533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n# Creating Optuna object and defining its parameters\n# study = optuna.create_study(direction='minimize')\n# study.optimize(train_optuna_cb, n_trials = 200)\n\n# Showing optimization results\n# print('Number of finished trials:', len(study.trials))\n# print('Best trial parameters:', study.best_trial.params)\n# print('Best score:', study.best_value)","metadata":{"execution":{"iopub.status.busy":"2021-06-24T06:27:03.76225Z","iopub.execute_input":"2021-06-24T06:27:03.76271Z","iopub.status.idle":"2021-06-24T06:27:03.76814Z","shell.execute_reply.started":"2021-06-24T06:27:03.762668Z","shell.execute_reply":"2021-06-24T06:27:03.766656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def train_optuna_lgbm(trial, X=X_train, y=y_train):\n#     \"\"\"\n#     A function to train a model using different hyperparamerters combinations provided by Optuna. \n#     Log loss of validation data predictions is returned to estimate hyperparameters effectiveness.\n#     \"\"\"    \n#     preds = 0\n#     split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n#     for train_idx, valid_idx in split.split(X, y):\n#         X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n#         y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n\n#         # A set of hyperparameters to optimize by optuna\n#         lgbm_params = {\n#             'objective': 'multiclass',\n#             'reg_alpha': trial.suggest_float('reg_alpha', 1.0, 20.0),\n#             'reg_lambda': trial.suggest_float('reg_lambda', 0.00001, 0.1),\n#             'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n#             'num_leaves': trial.suggest_int('num_leaves', 2, 20),\n#             'min_child_samples': trial.suggest_int('min_child_samples', 5, 40),\n#             'subsample_freq': trial.suggest_int('subsample_freq', 1, 5),\n#             'max_depth': trial.suggest_int('max_depth', 1, 30),\n#             'learning_rate': trial.suggest_float('learning_rate', 0.0001, 0.1),\n#             'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.8),\n#             'n_estimators': 20000,\n#             'random_state': 42,\n#             'boosting_type': 'gbdt',\n#             'metric': 'multi_logloss',\n#             'num_class': 9,\n#             'device': 'GPU'\n#         }\n        \n#         model = LGBMClassifier(**lgbm_params)\n#         model.fit(\n#                     X_train, y_train,\n#                     eval_set=(X_valid, y_valid),\n#                     eval_metric='multi_logloss',\n#                     early_stopping_rounds=500,\n#                     verbose=False,\n# #                     categorical_feature=cat_features\n#                 )\n\n#         oof = model.predict_proba(X_valid)\n    \n#     return log_loss(y_valid, oof)","metadata":{"execution":{"iopub.status.busy":"2021-06-24T06:27:03.770973Z","iopub.execute_input":"2021-06-24T06:27:03.771505Z","iopub.status.idle":"2021-06-24T06:27:03.781642Z","shell.execute_reply.started":"2021-06-24T06:27:03.771458Z","shell.execute_reply":"2021-06-24T06:27:03.779871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n# Defining and running Optuna using the function above\n# study = optuna.create_study(direction='minimize')\n# study.optimize(train_optuna_lgbm, n_trials = 60)\n\n# Showing optimization results\n# print('Number of finished trials:', len(study.trials))\n# print('Best trial:', study.best_trial.params)\n# print('Best value:', study.best_value)","metadata":{"execution":{"iopub.status.busy":"2021-06-24T06:27:03.783799Z","iopub.execute_input":"2021-06-24T06:27:03.784808Z","iopub.status.idle":"2021-06-24T06:27:03.797594Z","shell.execute_reply.started":"2021-06-24T06:27:03.78476Z","shell.execute_reply":"2021-06-24T06:27:03.796013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Models training using optimized hyperparameters","metadata":{}},{"cell_type":"code","source":"def train_with_folds(X, y, X_test, models, splits=10):\n    cat_features = [x for x in X.columns[:75]]\n    preds = 0\n    fi_df = pd.DataFrame(columns=[x.__class__.__name__ for x in models])\n    for model in models:\n        skf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=42)\n        oof = np.zeros((train.shape[0], y.nunique()))\n        feature_importances = 0\n        for num, (train_idx, valid_idx) in enumerate(skf.split(X, y)):\n            X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n            y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n\n            if \"CatBoost\" in model.__class__.__name__:\n                model.fit(X_train, y_train,\n                          eval_set=(X_valid, y_valid),\n                          verbose=False,\n                          cat_features=cat_features)           \n            elif \"LGBM\" in model.__class__.__name__:\n                model.fit(X_train, y_train,\n                          eval_set=(X_valid, y_valid),\n                          eval_metric='multi_logloss',\n                          early_stopping_rounds=400,\n                          verbose=False,\n                          categorical_feature=cat_features)\n            else:\n                model.fit(X_train, y_train)\n                \n            oof[valid_idx] = model.predict_proba(X_valid)\n            preds += model.predict_proba(X_test) / (splits*len(models))\n            print(f\"{model.__class__.__name__} fold {num} logloss: {log_loss(y_valid, oof[valid_idx])}\")\n            feature_importances += model.feature_importances_ / splits\n        fi_df[str(model.__class__.__name__)] = feature_importances\n        print(f\"\\n{model.__class__.__name__} overall logloss: {log_loss(y, oof)}\\n\")\n    \n    return preds, fi_df","metadata":{"execution":{"iopub.status.busy":"2021-06-24T06:27:03.800601Z","iopub.execute_input":"2021-06-24T06:27:03.801899Z","iopub.status.idle":"2021-06-24T06:27:03.822289Z","shell.execute_reply.started":"2021-06-24T06:27:03.801851Z","shell.execute_reply":"2021-06-24T06:27:03.820524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nmodels = [LGBMClassifier(objective=\"multiclass\",\n                         n_estimators=10000,\n                         boosting_type=\"gbdt\",\n                         random_state=42,\n                         learning_rate=0.00786559751278979,\n                         max_depth=28,\n                         num_leaves=19,\n                         subsample=0.7548376269285053,\n                         subsample_freq=4,\n                         colsample_bytree=0.10277352165216944,\n                         reg_alpha=15.857914898332481,\n                         reg_lambda=0.03275652415252568,\n                         min_child_samples=12,\n                         device=\"gpu\"),\n    \n            CatBoostClassifier(random_state=42,\n                               thread_count=4,\n                               verbose=False,\n                               iterations=5300,\n                               learning_rate=0.004996686623648068,\n                               grow_policy=\"SymmetricTree\",\n                               loss_function=\"MultiClass\",\n                               eval_metric=\"MultiClass\",\n                               classes_count=9,\n                               od_type=\"Iter\",\n                               depth=6,\n                               l2_leaf_reg=3.994384171429022,\n                               random_strength=1.8493809581160419,\n                               bagging_temperature=0.6721279933587145,\n                               early_stopping_rounds=400,\n                               task_type=\"GPU\")\n          \n            \n         ]\n\npreds, feature_importances = train_with_folds(X_train, y_train, X_test, models)","metadata":{"execution":{"iopub.status.busy":"2021-06-24T06:27:03.825516Z","iopub.execute_input":"2021-06-24T06:27:03.82618Z","iopub.status.idle":"2021-06-24T06:30:51.138827Z","shell.execute_reply.started":"2021-06-24T06:27:03.82612Z","shell.execute_reply":"2021-06-24T06:30:51.137529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Feature importances**","metadata":{}},{"cell_type":"code","source":"# Transforming different estimators' feaature importances to the same scale\nfor col in feature_importances:\n    feature_importances[col] = feature_importances[col] / feature_importances[col].sum()\n\nfeature_importances[\"Feature\"] = X_test.columns\nfeature_importances.loc[:74, \"Label\"] = \"Original feature\"\nfeature_importances.loc[75:, \"Label\"] = \"Custom feature\"","metadata":{"execution":{"iopub.status.busy":"2021-06-24T06:30:51.141097Z","iopub.execute_input":"2021-06-24T06:30:51.141536Z","iopub.status.idle":"2021-06-24T06:30:51.242693Z","shell.execute_reply.started":"2021-06-24T06:30:51.141494Z","shell.execute_reply":"2021-06-24T06:30:51.240611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = feature_importances.copy()\ndf.sort_values(\"CatBoostClassifier\", axis=0, ascending=False, inplace=True)\ndf.reset_index(inplace=True, drop=True)\n\nx = np.arange(0, len(df[\"Feature\"]))\nheight = 0.4\n\n# Custom legend elements\nlegend_lines = [Patch(facecolor=\"cornflowerblue\", label=\"Original CatBoost features\"),\n                Patch(facecolor=\"cornflowerblue\", hatch='|', label=\"Custom CatBoost features\"),\n                Patch(facecolor=\"palevioletred\", label=\"Original LGBM features\"),\n                Patch(facecolor=\"palevioletred\", hatch='|', label=\"Custom LGBM features\"),\n                ]\n\nfig, ax = plt.subplots(figsize=(16, 30))\nbars1 = ax.barh(x-height/2, df[\"CatBoostClassifier\"], height=height,\n                color=\"cornflowerblue\", edgecolor=\"black\", label=\"CatBoostClassifier\", hatch=['|' if x==\"Custom feature\" else '' for x in df[\"Label\"]])\nbars2 = ax.barh(x+height/2, df[\"LGBMClassifier\"], height=height,\n                color=\"palevioletred\", edgecolor=\"black\", label=\"LGBMClassifier\", hatch=['|' if x==\"Custom feature\" else '' for x in df[\"Label\"]])\nax.set_title(\"Feature importances of CatBoost and LGBM models\", fontsize=30, pad=15)\nax.set_ylabel(\"Feature names\", fontsize=20, labelpad=15)\nax.set_xlabel(\"Feature importance\", fontsize=20, labelpad=15)\nax.set_yticks(x)\nax.set_yticklabels(df[\"Feature\"], fontsize=15)\nax.tick_params(axis=\"x\", labelsize=15)\nax.grid(axis=\"x\")\nax2 = ax.secondary_xaxis('top')\nax2.set_xlabel(\"Feature importance\", fontsize=20, labelpad=15)\nax2.tick_params(axis=\"x\", labelsize=15)\nax.legend(handles=legend_lines, fontsize=15, loc=1, bbox_to_anchor=(0, 0, 1, 0.90))\nplt.margins(0.04, 0.01)\nplt.gca().invert_yaxis()","metadata":{"execution":{"iopub.status.busy":"2021-06-24T06:30:51.244212Z","iopub.status.idle":"2021-06-24T06:30:51.245063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(columns=[\"Feature\", \"Importance\"])\ndf[\"Feature\"] = feature_importances.loc[:75, \"Feature\"]\ndf[\"Importance\"] = feature_importances.loc[:75, \"CatBoostClassifier\"]\ndf.sort_values(\"Importance\", axis=0, ascending=True, inplace=True)\n\n\nheight = 0.7\n\nfig, axs = plt.subplots(ncols=2, nrows=1, figsize=(16,8))\n\nbars1 = axs[0].barh(df[\"Feature\"].iloc[-15:], df[\"Importance\"].iloc[-15:],\n                    height=height,\n                    color=\"mediumorchid\",\n                    edgecolor=\"black\")\naxs[0].set_title(\"Top 15 most important CatBoost original features\", fontsize=15)\naxs[0].set_xlabel(\"Feature importance\", fontsize=15, labelpad=10)\n\nbars2 = axs[1].barh(train.drop([\"id\", \"target\"], axis=1).astype(bool).sum(axis=0).sort_values(ascending=True).index[-15:],\n                    train.drop([\"id\", \"target\"], axis=1).astype(bool).sum(axis=0).sort_values(ascending=True).values[-15:] / train.shape[0],\n                    height=height,\n                    color=\"mediumseagreen\",\n                    edgecolor=\"black\")\naxs[1].set_title(\"Top 15 original features with the most nonzero values\", fontsize=15)\naxs[1].set_xlabel(\"Fraction of nonzero values\", fontsize=15, labelpad=10)\naxs[1].bar_label(bars2, \n                 [\"Top 15 important feature\" if x in list(df[\"Feature\"].iloc[-15:]) else \"\" for x in train.drop([\"id\", \"target\"], axis=1).astype(bool).sum(axis=0).sort_values(ascending=True).index[-15:]],\n                 padding=-175, fontsize=12, color=\"white\", weight=\"bold\")\n\nfig.suptitle(\"Top 15 original features with the most importance for CatBoost and nonzero values\", fontsize=20)\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-06-24T06:30:51.246784Z","iopub.status.idle":"2021-06-24T06:30:51.247658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(columns=[\"Feature\", \"Importance\"])\ndf[\"Feature\"] = feature_importances.loc[:75, \"Feature\"]\ndf[\"Importance\"] = feature_importances.loc[:75, \"LGBMClassifier\"]\ndf.sort_values(\"Importance\", axis=0, ascending=True, inplace=True)\n\n\nheight = 0.7\n\nfig, axs = plt.subplots(ncols=2, nrows=1, figsize=(16,8))\n\nbars1 = axs[0].barh(df[\"Feature\"].iloc[-15:], df[\"Importance\"].iloc[-15:],\n                    height=height,\n                    color=\"mediumorchid\",\n                    edgecolor=\"black\")\naxs[0].set_title(\"Top 15 most important LGBM original features\", fontsize=15)\naxs[0].set_xlabel(\"Feature importance\", fontsize=15, labelpad=10)\n\nbars2 = axs[1].barh(train.drop([\"id\", \"target\"], axis=1).astype(bool).sum(axis=0).sort_values(ascending=True).index[-15:],\n                    train.drop([\"id\", \"target\"], axis=1).astype(bool).sum(axis=0).sort_values(ascending=True).values[-15:] / train.shape[0],\n                    height=height,\n                    color=\"mediumseagreen\",\n                    edgecolor=\"black\")\naxs[1].set_title(\"Top 15 original features with the most nonzero values\", fontsize=15)\naxs[1].set_xlabel(\"Fraction of nonzero values\", fontsize=15, labelpad=10)\naxs[1].bar_label(bars2, \n                 [\"Top 15 important feature\" if x in list(df[\"Feature\"].iloc[-15:]) else \"\" for x in train.drop([\"id\", \"target\"], axis=1).astype(bool).sum(axis=0).sort_values(ascending=True).index[-15:]],\n                 padding=-175, fontsize=12, color=\"white\", weight=\"bold\")\n\nfig.suptitle(\"Top 15 original features with the most importance for LGBM and nonzero values\", fontsize=20)\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-06-24T06:30:51.249374Z","iopub.status.idle":"2021-06-24T06:30:51.250239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(columns=[\"Feature\", \"Importance\"])\ndf[\"Feature\"] = feature_importances.loc[:75, \"Feature\"]\ndf[\"Importance\"] = feature_importances.loc[:75, \"CatBoostClassifier\"]\ndf.sort_values(\"Importance\", axis=0, ascending=False, inplace=True)\n\ncolors = [\"mediumorchid\" if x in df[\"Feature\"].iloc[-15:] else \"mediumseagreen\" for x in train.drop([\"id\", \"target\"], axis=1).astype(bool).sum(axis=0).sort_values(ascending=True).index[-15:]]\n\nheight = 0.7\n\nfig, axs = plt.subplots(ncols=2, nrows=1, figsize=(16,8))\n\nbars1 = axs[0].barh(df[\"Feature\"].iloc[-15:], df[\"Importance\"].iloc[-15:],\n                    height=height,\n                    color=\"mediumorchid\",\n                    edgecolor=\"black\")\naxs[0].set_title(\"Top 15 least important CatBoost original features\", fontsize=15)\naxs[0].set_xlabel(\"Feature importance\", fontsize=15, labelpad=10)\n\nbars2 = axs[1].barh(train.drop([\"id\", \"target\"], axis=1).astype(bool).sum(axis=0).sort_values(ascending=False).index[-15:],\n                    train.drop([\"id\", \"target\"], axis=1).astype(bool).sum(axis=0).sort_values(ascending=False).values[-15:] / train.shape[0],\n                    height=height,\n                    color=\"lightcoral\",\n                    edgecolor=\"black\")\naxs[1].set_title(\"Top 15 original features with the least nonzero values\", fontsize=15)\naxs[1].set_xlabel(\"Fraction of nonzero values\", fontsize=15, labelpad=10)\naxs[1].bar_label(bars2, [\"Top 15 least important feature\" if x in list(df[\"Feature\"].iloc[-15:]) else \"\" for x in train.drop([\"id\", \"target\"], axis=1).astype(bool).sum(axis=0).sort_values(ascending=False).index[-15:]],\n                 padding=-215, fontsize=12, color=\"white\", weight=\"bold\")\n\nfig.suptitle(\"Top 15 original features with the least importance for CatBoost and nonzero values\", fontsize=20)                \nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-06-24T06:30:51.251702Z","iopub.status.idle":"2021-06-24T06:30:51.252501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(columns=[\"Feature\", \"Importance\"])\ndf[\"Feature\"] = feature_importances.loc[:75, \"Feature\"]\ndf[\"Importance\"] = feature_importances.loc[:75, \"LGBMClassifier\"]\ndf.sort_values(\"Importance\", axis=0, ascending=False, inplace=True)\n\ncolors = [\"mediumorchid\" if x in df[\"Feature\"].iloc[-15:] else \"mediumseagreen\" for x in train.drop([\"id\", \"target\"], axis=1).astype(bool).sum(axis=0).sort_values(ascending=True).index[-15:]]\n\nheight = 0.7\n\nfig, axs = plt.subplots(ncols=2, nrows=1, figsize=(16,8))\n\nbars1 = axs[0].barh(df[\"Feature\"].iloc[-15:], df[\"Importance\"].iloc[-15:],\n                    height=height,\n                    color=\"mediumorchid\",\n                    edgecolor=\"black\")\naxs[0].set_title(\"Top 15 least important LGBM original features\", fontsize=15)\naxs[0].set_xlabel(\"Feature importance\", fontsize=15, labelpad=10)\n\nbars2 = axs[1].barh(train.drop([\"id\", \"target\"], axis=1).astype(bool).sum(axis=0).sort_values(ascending=False).index[-15:],\n                    train.drop([\"id\", \"target\"], axis=1).astype(bool).sum(axis=0).sort_values(ascending=False).values[-15:] / train.shape[0],\n                    height=height,\n                    color=\"lightcoral\",\n                    edgecolor=\"black\")\naxs[1].set_title(\"Top 15 original features with the least nonzero values\", fontsize=15)\naxs[1].set_xlabel(\"Fraction of nonzero values\", fontsize=15, labelpad=10)\naxs[1].bar_label(bars2, [\"Top 15 least important feature\" if x in list(df[\"Feature\"].iloc[-15:]) else \"\" for x in train.drop([\"id\", \"target\"], axis=1).astype(bool).sum(axis=0).sort_values(ascending=False).index[-15:]],\n                 padding=-215, fontsize=12, color=\"white\", weight=\"bold\")\n\nfig.suptitle(\"Top 15 original features with the least importance for LGBM and nonzero values\", fontsize=20)                \nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-06-24T06:30:51.253924Z","iopub.status.idle":"2021-06-24T06:30:51.25487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Predictions submission**","metadata":{}},{"cell_type":"code","source":"predictions = pd.DataFrame()\npredictions[\"id\"] = test[\"id\"]\npredictions = pd.concat([predictions, pd.DataFrame(preds, columns=[\"Class_\" + str(x) for x in np.arange(1, 10, 1)])], axis=1)\npredictions.to_csv('submission.csv', index=False, header=predictions.columns)\npredictions.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-24T06:30:51.256214Z","iopub.status.idle":"2021-06-24T06:30:51.256945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}