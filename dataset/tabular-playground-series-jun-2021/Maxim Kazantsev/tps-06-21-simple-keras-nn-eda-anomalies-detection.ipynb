{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, OneHotEncoder\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.metrics import log_loss, confusion_matrix\nimport matplotlib.pyplot as plt\nfrom matplotlib.lines import Line2D\nimport time\nimport random\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.decomposition import PCA\nfrom sklearn.neighbors import LocalOutlierFactor\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# Pandas setting to display more dataset rows and columns\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', None)\npd.set_option('display.float_format', lambda x: '%.5f' % x)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-30T19:07:51.55968Z","iopub.execute_input":"2021-06-30T19:07:51.5604Z","iopub.status.idle":"2021-06-30T19:07:58.755243Z","shell.execute_reply.started":"2021-06-30T19:07:51.560305Z","shell.execute_reply":"2021-06-30T19:07:58.754167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data import**","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/tabular-playground-series-jun-2021/train.csv\", low_memory=False)#, nrows=10000)\ntest = pd.read_csv(\"/kaggle/input/tabular-playground-series-jun-2021/test.csv\", low_memory=False)\ntrain.info(memory_usage=\"deep\")","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:07:58.756723Z","iopub.execute_input":"2021-06-30T19:07:58.757046Z","iopub.status.idle":"2021-06-30T19:08:01.731602Z","shell.execute_reply.started":"2021-06-30T19:07:58.75701Z","shell.execute_reply":"2021-06-30T19:08:01.730693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info(memory_usage=\"deep\")","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:08:01.733157Z","iopub.execute_input":"2021-06-30T19:08:01.73363Z","iopub.status.idle":"2021-06-30T19:08:01.761792Z","shell.execute_reply.started":"2021-06-30T19:08:01.733586Z","shell.execute_reply":"2021-06-30T19:08:01.761039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data analysis**","metadata":{}},{"cell_type":"code","source":"# Colors to be used for plots\ncolors = [\"lightcoral\", \"sandybrown\", \"darkorange\", \"mediumseagreen\",\n          \"lightseagreen\", \"cornflowerblue\", \"mediumpurple\", \"palevioletred\",\n          \"lightskyblue\", \"sandybrown\", \"yellowgreen\", \"indianred\",\n          \"lightsteelblue\", \"mediumorchid\", \"deepskyblue\"]","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:08:01.763154Z","iopub.execute_input":"2021-06-30T19:08:01.763609Z","iopub.status.idle":"2021-06-30T19:08:01.767142Z","shell.execute_reply.started":"2021-06-30T19:08:01.763567Z","shell.execute_reply":"2021-06-30T19:08:01.766487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Target distribution","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(ncols=2, nrows=1, figsize=(18, 8), gridspec_kw={'width_ratios': [2, 1]})\n\nbars = axs[0].bar(train[\"target\"].value_counts().sort_index().index,\n                  train[\"target\"].value_counts().sort_index().values,\n                  color=colors,\n                  edgecolor=\"black\")\naxs[0].set_title(\"Target distribution\", fontsize=20, pad=15)\naxs[0].set_ylabel(\"Count\", fontsize=14, labelpad=15)\naxs[0].set_xlabel(\"Target label\", fontsize=14, labelpad=10)\naxs[0].bar_label(bars, train[\"target\"].value_counts().sort_index().values,\n                 padding=3, fontsize=12)\naxs[0].bar_label(bars, [f\"{x:2.1f}%\" for x in train[\"target\"].value_counts().sort_index().values/2000],\n                 padding=-20, fontsize=12)\naxs[0].margins(0.025, 0.06)\naxs[0].grid(axis=\"y\")\n\npie = axs[1].pie(train[\"target\"].value_counts(sort=False).sort_index().values,\n                 labels=train[\"target\"].value_counts(sort=False).sort_index().index,\n                 colors=colors,\n                 rotatelabels=True,\n                 textprops={\"fontsize\": 14})\naxs[1].axis(\"equal\")\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:08:01.76826Z","iopub.execute_input":"2021-06-30T19:08:01.768718Z","iopub.status.idle":"2021-06-30T19:08:02.634511Z","shell.execute_reply.started":"2021-06-30T19:08:01.768655Z","shell.execute_reply":"2021-06-30T19:08:02.633755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check if these classes are evenly distributed in the dataset.","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame()\ndf[\"id\"] = train[\"id\"]\ndf[\"target\"] = train[\"target\"]\ndf[\"id\"] = pd.cut(df[\"id\"], np.arange(0, 201000, 1000), right=False)\nvalues = df.groupby(\"id\")[\"target\"].value_counts(sort=False).values\nclasses = [\"Class_\" + str(x) for x in np.arange(9)]\n\ncols = 3\nrows = 3\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,rows*4), sharey=False)\n\nplt.subplots_adjust(hspace = 0.35)\n\ni=0\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        axs[r, c].plot(np.arange(0, 200, 1),\n                       [values[x] for x in np.arange(i, 1800, 9)],\n                       color=colors[i])\n        axs[r, c].set_title(classes[i], fontsize=12, pad=5)\n        axs[r, c].set_xticks(np.arange(0, 250, 50))\n        axs[r, c].set_xticklabels([str(int(x))+\"k\" for x in axs[r, c].get_xticks()])\n        axs[r, c].set_xlabel(\"Dataframe id\")\n        axs[r, c].set_ylabel(\"Class labels qty per 1k rows\")\n        axs[r, c].set_ylim(0, 320)\n        i+=1\nfig.suptitle(\"Class labels distribution in the train dataset\", fontsize=20)\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:08:02.63576Z","iopub.execute_input":"2021-06-30T19:08:02.636233Z","iopub.status.idle":"2021-06-30T19:08:03.643419Z","shell.execute_reply.started":"2021-06-30T19:08:02.6362Z","shell.execute_reply":"2021-06-30T19:08:03.642279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So it looks like class labels are distributed pretty evenly across the dataset.","metadata":{}},{"cell_type":"markdown","source":"## Original features value distribution in the train dataset","metadata":{}},{"cell_type":"code","source":"def make_data_plots(df, i=0):\n    \"\"\"\n    Makes value distribution histogram plots for a given dataframe features\n    \"\"\"\n    columns = df.drop([\"target\", \"id\"], axis=1).columns.values\n\n    cols = 3\n    rows = (len(columns) - i) // cols + 1\n\n    fig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,rows*4), sharey=True)\n    plt.subplots_adjust(hspace = 0.2)\n    for r in np.arange(0, rows, 1):\n        for c in np.arange(0, cols, 1):\n            if i >= len(columns):\n                axs[r, c].set_visible(False)\n            else:\n                axs[r, c].hist(df[columns[i]].values, bins = 30,\n                               color=random.choice(colors),\n                               edgecolor=\"black\")\n                axs[r, c].set_title(columns[i], fontsize=12, pad=5)\n            i+=1","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:08:03.644978Z","iopub.execute_input":"2021-06-30T19:08:03.6454Z","iopub.status.idle":"2021-06-30T19:08:03.655774Z","shell.execute_reply.started":"2021-06-30T19:08:03.645354Z","shell.execute_reply":"2021-06-30T19:08:03.654732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_data_plots(train)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:08:03.658623Z","iopub.execute_input":"2021-06-30T19:08:03.659024Z","iopub.status.idle":"2021-06-30T19:08:18.004341Z","shell.execute_reply.started":"2021-06-30T19:08:03.65899Z","shell.execute_reply":"2021-06-30T19:08:18.003419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Original features nonzero value distribution in the train dataset","metadata":{}},{"cell_type":"code","source":"def make_nonzero_data_plots(df, i=0):\n    \"\"\"\n    Makes nonzero value distribution histogram plots for a given dataframe features\n    \"\"\"    \n    \n    columns = df.drop([\"target\", \"id\"], axis=1).columns.values\n\n    cols = 3\n    rows = (len(columns) - i) // cols + 1\n\n    fig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,rows*4), sharey=False)\n    plt.subplots_adjust(hspace = 0.2)\n    for r in np.arange(0, rows, 1):\n        for c in np.arange(0, cols, 1):\n            if i >= len(columns):\n                axs[r, c].set_visible(False)\n            else:\n                axs[r, c].hist(df[df[columns[i]] > 0][columns[i]].values, bins = 30,\n                               color=random.choice(colors),\n                               edgecolor=\"black\")\n                axs[r, c].set_title(columns[i], fontsize=12, pad=5)\n            i+=1","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:08:18.006024Z","iopub.execute_input":"2021-06-30T19:08:18.00651Z","iopub.status.idle":"2021-06-30T19:08:18.015873Z","shell.execute_reply.started":"2021-06-30T19:08:18.006455Z","shell.execute_reply":"2021-06-30T19:08:18.015081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_nonzero_data_plots(train)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:08:18.017257Z","iopub.execute_input":"2021-06-30T19:08:18.017577Z","iopub.status.idle":"2021-06-30T19:08:34.325287Z","shell.execute_reply.started":"2021-06-30T19:08:18.017542Z","shell.execute_reply":"2021-06-30T19:08:34.319097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fraction of nonzero values in the both datasets","metadata":{}},{"cell_type":"code","source":"x = -1*np.arange(len(test.drop([\"id\"], axis=1).columns))\n\nfig, ax = plt.subplots(figsize=(16, 30))\nbars = ax.barh(x+0.2, train.drop([\"id\", \"target\"], axis=1).astype(bool).sum(axis=0).values / train.shape[0],\n               height=0.4, color=\"cornflowerblue\", label=\"Train dataset\", edgecolor=\"black\")\nbars2 = ax.barh(x-0.2, test.drop([\"id\"], axis=1).astype(bool).sum(axis=0).values / test.shape[0],\n                height=0.4, color=\"palevioletred\", label=\"Test dataset\", edgecolor=\"black\")\nax.set_title(\"Fraction of nonzero values in the both datasets\", fontsize=30, pad=15)\nax.set_ylabel(\"Feature names\", fontsize=20, labelpad=15)\nax.set_xlabel(\"Fraction of nonzero values\", fontsize=20, labelpad=15)\nax.set_xticks(np.arange(0, 0.8, 0.05))\nax.set_yticks(x)\nax.set_yticklabels(list(test.drop([\"id\"], axis=1).columns.values))\nax.tick_params(axis=\"x\", labelsize=15)\nax.tick_params(axis=\"y\", labelsize=14)\nax.grid(axis=\"x\")\nax.legend(fontsize=15)\nax2 = ax.secondary_xaxis('top')\nax2.set_xticks(np.arange(0, 0.8, 0.05))\nax2.set_xlabel(\"Fraction of nonzero values\", fontsize=20, labelpad=15)\nax2.tick_params(axis=\"x\", labelsize=15)\nplt.margins(0.05, 0.01)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:08:34.326476Z","iopub.execute_input":"2021-06-30T19:08:34.326904Z","iopub.status.idle":"2021-06-30T19:08:36.613145Z","shell.execute_reply.started":"2021-06-30T19:08:34.32686Z","shell.execute_reply":"2021-06-30T19:08:36.612152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## PCA","metadata":{}},{"cell_type":"code","source":"pca = PCA(n_components=2)\nX_reduced = pd.DataFrame(pca.fit_transform(train.drop([\"id\", \"target\"], axis=1)))\n\nfig, ax = plt.subplots(figsize=(16, 9))\nax.scatter(X_reduced[0], X_reduced[1], c=LabelEncoder().fit_transform(train[\"target\"]), cmap=\"tab10\")\nax.set_title(\"Scatter plot of the original train set reduced to 2 dimensions\", fontsize=20, pad=15)\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:08:36.614488Z","iopub.execute_input":"2021-06-30T19:08:36.615048Z","iopub.status.idle":"2021-06-30T19:08:42.626721Z","shell.execute_reply.started":"2021-06-30T19:08:36.615001Z","shell.execute_reply":"2021-06-30T19:08:42.625716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()\nX_scaled = scaler.fit_transform(train.drop([\"id\", \"target\"], axis=1))\n\npca = PCA(n_components=2)\nX_reduced = pd.DataFrame(pca.fit_transform(X_scaled))\n\nfig, ax = plt.subplots(figsize=(16, 9))\nax.scatter(X_reduced[0], X_reduced[1], c=LabelEncoder().fit_transform(train[\"target\"]), cmap=\"tab10\")\nax.set_title(\"Scatter plot of the scaled train set reduced to 2 dimensions\", fontsize=20, pad=15)\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:08:42.627883Z","iopub.execute_input":"2021-06-30T19:08:42.62818Z","iopub.status.idle":"2021-06-30T19:08:48.949913Z","shell.execute_reply.started":"2021-06-30T19:08:42.628153Z","shell.execute_reply":"2021-06-30T19:08:48.948578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, the data does not have distinct clusters after reducing dimensions.","metadata":{}},{"cell_type":"markdown","source":"## Detecting anomalies with IsolationForest","metadata":{}},{"cell_type":"code","source":"iso_forest = IsolationForest(n_jobs=-1, random_state=42, n_estimators=1000)\niso_forest.fit(train.drop([\"id\", \"target\"], axis=1))\nscores = iso_forest.decision_function(train.drop([\"id\", \"target\"], axis=1))\nto_drop = train.loc[scores < 0].index\nprint(f\"Anomalies found in the train dataset: {(scores < 0).sum()}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:08:48.951215Z","iopub.execute_input":"2021-06-30T19:08:48.951529Z","iopub.status.idle":"2021-06-30T19:11:28.186218Z","shell.execute_reply.started":"2021-06-30T19:08:48.951496Z","shell.execute_reply":"2021-06-30T19:11:28.185096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Target distribution in detected anomalies\ntrain.loc[(scores < 0), \"target\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:11:28.187461Z","iopub.execute_input":"2021-06-30T19:11:28.187742Z","iopub.status.idle":"2021-06-30T19:11:28.198236Z","shell.execute_reply.started":"2021-06-30T19:11:28.187714Z","shell.execute_reply":"2021-06-30T19:11:28.197284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Detecting anomalies with LocalOutlierFactor","metadata":{}},{"cell_type":"code","source":"# scaler = StandardScaler()\n# X_scaled = scaler.fit_transform(train.drop([\"id\", \"target\"], axis=1))\n# lof = LocalOutlierFactor(n_jobs=-1)\n# lof.fit(X_scaled)\n# scores = lof.negative_outlier_factor_\n# score_threshold = -1.93\n# to_drop = train.loc[scores < score_threshold].index\n# print(f\"Anomalies found in the train dataset: {(scores < score_threshold).sum()}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:11:28.199597Z","iopub.execute_input":"2021-06-30T19:11:28.199879Z","iopub.status.idle":"2021-06-30T19:11:28.211847Z","shell.execute_reply.started":"2021-06-30T19:11:28.199851Z","shell.execute_reply":"2021-06-30T19:11:28.210858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Target distribution in detected anomalies\n# train.loc[(scores < score_threshold), \"target\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:11:28.21334Z","iopub.execute_input":"2021-06-30T19:11:28.213634Z","iopub.status.idle":"2021-06-30T19:11:28.224498Z","shell.execute_reply.started":"2021-06-30T19:11:28.213606Z","shell.execute_reply":"2021-06-30T19:11:28.223464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data preparation**","metadata":{}},{"cell_type":"code","source":"# Dropping anomaly rows detected with Isolation Forest\ntrain.drop(axis=0, index=set(to_drop), inplace=True)\ntrain.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:11:28.225634Z","iopub.execute_input":"2021-06-30T19:11:28.225926Z","iopub.status.idle":"2021-06-30T19:11:28.292751Z","shell.execute_reply.started":"2021-06-30T19:11:28.225872Z","shell.execute_reply":"2021-06-30T19:11:28.291623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.drop([\"id\", \"target\"], axis=1).duplicated(keep=False).sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:11:28.294322Z","iopub.execute_input":"2021-06-30T19:11:28.294746Z","iopub.status.idle":"2021-06-30T19:11:28.631573Z","shell.execute_reply.started":"2021-06-30T19:11:28.294701Z","shell.execute_reply":"2021-06-30T19:11:28.63057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are some duplicates in the dataset. If they have identical feature but different target values it may decrease machine learning performance.","metadata":{}},{"cell_type":"code","source":"def delete_duplicates(df):\n    \"\"\"\n    Finds duplicates in a given DataFrame and deletes rows with identical features values but different target. \n    \"\"\"\n    \n    # Copying duplicate rows in a new dataset and getting their indices\n    idx = df.drop([\"id\", \"target\"], axis=1).duplicated(keep=False)\n    duplicates = df.loc[idx == True].copy()\n    features = [x for x in duplicates.columns if \"feature\" in x]\n    idx = duplicates[\"id\"]\n    \n    # Checking if which rows with equal feature values have different target\n    indx_to_drop = []\n    for index in idx:\n        for row in idx:\n            if (row != index) and (row not in indx_to_drop):\n                if duplicates.loc[index, features].equals(duplicates.loc[row, features]):\n                    if duplicates.loc[index, \"target\"] != duplicates.loc[row, \"target\"]:\n    #                     print(f\"Found duplicates with different targets: {index} - {duplicates.loc[index, 'target']} and {row} - {duplicates.loc[row, 'target']}\")\n                        indx_to_drop.append(index)\n                        indx_to_drop.append(row)\n    #                 else:\n    #                     print(f\"Found duplicates with the same target: {index} and {row}\")\n    \n    # Reporting results\n    print(f\"There are {len(duplicates['id'])} duplicated rows in the dataset.\")\n    print(f\"{len(set(indx_to_drop))} of them have different target. They will be deleted from the dataset.\")\n    print(f\"The datatframe has {len(df['id'])} rows.\")\n    df.drop(axis=0, index=set(indx_to_drop), inplace=True)\n    print(f\"After duplicated deletion there are {len(df['id'])} rows.\")\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:11:28.632778Z","iopub.execute_input":"2021-06-30T19:11:28.633088Z","iopub.status.idle":"2021-06-30T19:11:28.643259Z","shell.execute_reply.started":"2021-06-30T19:11:28.633058Z","shell.execute_reply":"2021-06-30T19:11:28.642093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = delete_duplicates(train)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:11:28.644621Z","iopub.execute_input":"2021-06-30T19:11:28.644949Z","iopub.status.idle":"2021-06-30T19:12:34.132786Z","shell.execute_reply.started":"2021-06-30T19:11:28.644873Z","shell.execute_reply":"2021-06-30T19:12:34.132001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Target feature encoding\nencoder = LabelEncoder()\ntrain[\"target\"] = encoder.fit_transform(train[\"target\"])\ntrain[\"target\"].value_counts(sort=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:12:34.134225Z","iopub.execute_input":"2021-06-30T19:12:34.134515Z","iopub.status.idle":"2021-06-30T19:12:34.206756Z","shell.execute_reply.started":"2021-06-30T19:12:34.134487Z","shell.execute_reply":"2021-06-30T19:12:34.206047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def transform_dataset(data):\n    \"\"\"\n    Adds new custom features and transforms original features into custom categories\n    \"\"\"\n    \n    # Copying features in a temporary dataset which will be transformed with MinMaxScaler\n    df = data[[x for x in data.columns if \"feature_\" in x]].copy()\n    \n    # Adding custom features\n    data[\"feature_75\"] = df.max(axis=1)\n    data[\"feature_76\"] = df.mean(axis=1)\n    data[\"feature_77\"] = df.median(axis=1)\n    data[\"feature_78\"] = df.nunique(axis=1)\n    data[\"feature_79\"] = (df == 0).astype(int).sum(axis=1)\n    data[\"feature_80\"] = (df != 0).sum(axis=1)\n    \n    # Scaling original features and adding new features basing on them\n    scaled_df = pd.DataFrame(index = data.index.values, columns = df.columns.values)\n    for col in df.columns.values:\n        scaler = MinMaxScaler()\n        scaled_df[col] = scaler.fit_transform(np.array(df[col]).reshape(-1, 1))\n    data[\"feature_81\"] = scaled_df.mean(axis=1)\n    data[\"feature_82\"] = (scaled_df == 1).sum(axis=1)\n    \n    # Cutting original features into custom intevals [0, 1), [1, 15), [15, 30) ...\n    intervals = np.insert(np.arange(15, 370, 15), 0, [0, 1])\n    intervals_text = pd.cut(train[\"feature_0\"], intervals, right=False).value_counts().sort_index().index.astype(\"string\")\n    map_dict = dict(zip(intervals_text, list(np.arange(len(intervals_text)))))\n    for i, column in enumerate(data.drop([\"id\"], axis=1).columns):\n        if ((i < 75) and (column!=\"target\")):\n            data[column] = pd.cut(data[column], intervals, right=False).astype(\"string\")\n            data[column].replace(map_dict, inplace=True)\n    \n    return data","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:12:34.209951Z","iopub.execute_input":"2021-06-30T19:12:34.210441Z","iopub.status.idle":"2021-06-30T19:12:34.222214Z","shell.execute_reply.started":"2021-06-30T19:12:34.210408Z","shell.execute_reply":"2021-06-30T19:12:34.221366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transforming the train dataset and making value distribution plots for new custom features\ntrain_data  = transform_dataset(train.copy())\nmake_data_plots(train_data, i=75)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:12:34.223914Z","iopub.execute_input":"2021-06-30T19:12:34.224227Z","iopub.status.idle":"2021-06-30T19:15:20.273122Z","shell.execute_reply.started":"2021-06-30T19:12:34.224197Z","shell.execute_reply":"2021-06-30T19:15:20.272169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.drop([\"id\", \"target\"], axis=1, inplace=True)\ntest_data = transform_dataset(test).drop(\"id\", axis=1)\ny = train[\"target\"]","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:15:20.274438Z","iopub.execute_input":"2021-06-30T19:15:20.274739Z","iopub.status.idle":"2021-06-30T19:16:43.227126Z","shell.execute_reply.started":"2021-06-30T19:15:20.274709Z","shell.execute_reply":"2021-06-30T19:16:43.226067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scaling both train and test data to [0, 1] interval\nX = pd.DataFrame(index=train_data.index)\nX_test = pd.DataFrame(index=test_data.index)\nfor column in train_data.columns:\n    scaler = MinMaxScaler()\n    X[column] = scaler.fit_transform(np.array(train_data[column]).reshape(-1, 1))\n    X_test[column] = scaler.transform(np.array(test_data[column]).reshape(-1, 1))\nX.shape, X_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:16:43.228419Z","iopub.execute_input":"2021-06-30T19:16:43.228717Z","iopub.status.idle":"2021-06-30T19:16:43.584823Z","shell.execute_reply.started":"2021-06-30T19:16:43.228671Z","shell.execute_reply":"2021-06-30T19:16:43.583747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Machine Learning**","metadata":{}},{"cell_type":"code","source":"# Splitting train data into train and valid sets\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.075, random_state=42)\nfor train_idx, valid_idx in split.split(X, y):\n    X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n    y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\nX_train.shape, X_valid.shape, y_train.shape, y_valid.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:16:43.586243Z","iopub.execute_input":"2021-06-30T19:16:43.586548Z","iopub.status.idle":"2021-06-30T19:16:43.968807Z","shell.execute_reply.started":"2021-06-30T19:16:43.58651Z","shell.execute_reply":"2021-06-30T19:16:43.96768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# onehot = OneHotEncoder(sparse=False, dtype=\"int\")\n# # y_train = pd.DataFrame(onehot.fit_transform(np.array(y_train).reshape(-1, 1)))\n# # y_valid = pd.DataFrame(onehot.fit_transform(np.array(y_valid).reshape(-1, 1)))\n# y_train = onehot.fit_transform(np.array(y_train).reshape(-1, 1))\n# y_valid = onehot.fit_transform(np.array(y_valid).reshape(-1, 1))\n# y_train.shape, y_valid.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:16:43.970563Z","iopub.execute_input":"2021-06-30T19:16:43.971035Z","iopub.status.idle":"2021-06-30T19:16:43.97528Z","shell.execute_reply.started":"2021-06-30T19:16:43.970977Z","shell.execute_reply":"2021-06-30T19:16:43.974468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y), y=y)\n# class_weights_dict={}\n# for label in np.sort(y.unique()):\n#     class_weights_dict[label] = class_weights[label]\n# class_weights_dict","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:16:43.97648Z","iopub.execute_input":"2021-06-30T19:16:43.976919Z","iopub.status.idle":"2021-06-30T19:16:43.989973Z","shell.execute_reply.started":"2021-06-30T19:16:43.976869Z","shell.execute_reply":"2021-06-30T19:16:43.988927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = keras.models.Sequential()\n# An input layer\nmodel.add(keras.layers.Input(shape=[X_train.shape[1],]))\n# Two hidden layers\nmodel.add(keras.layers.Dense(300, activation=\"tanh\"))\nmodel.add(keras.layers.Dense(100, activation=\"tanh\"))\n# An output layer\nmodel.add(keras.layers.Dense(9, activation=\"softmax\"))","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:16:43.99134Z","iopub.execute_input":"2021-06-30T19:16:43.991763Z","iopub.status.idle":"2021-06-30T19:16:44.098269Z","shell.execute_reply.started":"2021-06-30T19:16:43.991733Z","shell.execute_reply":"2021-06-30T19:16:44.097322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking layers setup\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:16:44.099306Z","iopub.execute_input":"2021-06-30T19:16:44.099701Z","iopub.status.idle":"2021-06-30T19:16:44.106966Z","shell.execute_reply.started":"2021-06-30T19:16:44.099672Z","shell.execute_reply":"2021-06-30T19:16:44.106044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A callback to stop training when overfitting is detected\nearly_stopping_cb = keras.callbacks.EarlyStopping(patience=50,\n                                                  restore_best_weights=True)\n\n# Compiling a model\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001),\n#               metrics=\"accuracy\",\n             )","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:16:44.108206Z","iopub.execute_input":"2021-06-30T19:16:44.108723Z","iopub.status.idle":"2021-06-30T19:16:44.127997Z","shell.execute_reply.started":"2021-06-30T19:16:44.108671Z","shell.execute_reply":"2021-06-30T19:16:44.12701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nhistory = model.fit(X_train, y_train, epochs=1000,\n                    validation_data=(X_valid, y_valid), \n                    callbacks=[early_stopping_cb],\n                    verbose=2,\n#                     class_weight=class_weights_dict\n                   )","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:16:44.129201Z","iopub.execute_input":"2021-06-30T19:16:44.129664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Valid dataset log loss is {model.evaluate(X_valid, y_valid, verbose=0)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Validation confusion matrix**","metadata":{}},{"cell_type":"code","source":"conf_mx = confusion_matrix(y_valid, np.array([np.argmax(x) for x in model.predict(X_valid)]).reshape(-1,1))\nconf_mx","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 10))\nax.matshow(conf_mx, cmap=plt.cm.get_cmap(\"viridis\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Predictions submission**","metadata":{}},{"cell_type":"code","source":"preds = model.predict(X_test)\npreds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = pd.DataFrame()\npredictions[\"id\"] = test[\"id\"]\npredictions = pd.concat([predictions, pd.DataFrame(preds, columns=[\"Class_\" + str(x) for x in np.arange(1, 10, 1)])], axis=1)\npredictions.to_csv('submission.csv', index=False, header=predictions.columns)\npredictions.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}