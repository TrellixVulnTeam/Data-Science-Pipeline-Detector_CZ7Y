{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<br>\n<h1 style=\"color:pink; text-align:center; font-size:30px; font-family:Arial Black; border-radius:30px 30px; background-color:black; line-height: 50px; padding: 15px 15px 15px 2.5%;\">üí•LightBGM Ensembleüí•</h1>\n<br>","metadata":{}},{"cell_type":"markdown","source":"# Approach","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\"><p style='color:black;'>\n    1. Import libraries<br>\n    2. Read the data<br>\n    3. Check for missing values and target distribution<br>\n    4. Train different LGBMClassifiers and predict for test data<br>\n    5. Take the average of the predictions<br>\n    6. Create submission files<br>\n</p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"# ‚úÖ Importing Required Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom lightgbm import LGBMClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss\n\nfrom tqdm.notebook import tqdm_notebook\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-29T17:53:45.388831Z","iopub.execute_input":"2021-06-29T17:53:45.389215Z","iopub.status.idle":"2021-06-29T17:53:48.362987Z","shell.execute_reply.started":"2021-06-29T17:53:45.389135Z","shell.execute_reply":"2021-06-29T17:53:48.362132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_rows', 100)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-29T17:53:48.364407Z","iopub.execute_input":"2021-06-29T17:53:48.364734Z","iopub.status.idle":"2021-06-29T17:53:48.371923Z","shell.execute_reply.started":"2021-06-29T17:53:48.364697Z","shell.execute_reply":"2021-06-29T17:53:48.371127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ‚úÖReading the Data","metadata":{}},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-29T17:53:49.05079Z","iopub.execute_input":"2021-06-29T17:53:49.051118Z","iopub.status.idle":"2021-06-29T17:53:49.058168Z","shell.execute_reply.started":"2021-06-29T17:53:49.051089Z","shell.execute_reply":"2021-06-29T17:53:49.057102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/tabular-playground-series-jun-2021/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/tabular-playground-series-jun-2021/test.csv\")\nss = pd.read_csv(\"/kaggle/input/tabular-playground-series-jun-2021/sample_submission.csv\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-29T17:53:49.568087Z","iopub.execute_input":"2021-06-29T17:53:49.568447Z","iopub.status.idle":"2021-06-29T17:53:51.384387Z","shell.execute_reply.started":"2021-06-29T17:53:49.568413Z","shell.execute_reply":"2021-06-29T17:53:51.383507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üîçBasic Data Checks","metadata":{}},{"cell_type":"code","source":"print(f'Shape of Train dataset is : {train.shape}')\nprint(f'Shape of Test dataset is : {test.shape}')\nprint(f'Shape of Sample Submission dataset is : {ss.shape}')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-29T17:53:53.938625Z","iopub.execute_input":"2021-06-29T17:53:53.938958Z","iopub.status.idle":"2021-06-29T17:53:53.947186Z","shell.execute_reply.started":"2021-06-29T17:53:53.938927Z","shell.execute_reply":"2021-06-29T17:53:53.946413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_NAN_columns(df, df_name):\n    if len(df.columns[df.isnull().any()]) == 0:\n        print(f'No missing data in {df_name} dataset')\n    else:\n        print(f'The following columns are having missing data in {df_name} dataset:')\n        print(df.columns[df.isnull().any()])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-29T17:53:54.922404Z","iopub.execute_input":"2021-06-29T17:53:54.922705Z","iopub.status.idle":"2021-06-29T17:53:54.927584Z","shell.execute_reply.started":"2021-06-29T17:53:54.922679Z","shell.execute_reply":"2021-06-29T17:53:54.926487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datasets = {\n    'Train': train,\n    'Test': test,\n    'Sample Submission': ss,\n}\nfor df_name, df in datasets.items():\n    check_NAN_columns(df, df_name)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-29T17:53:58.362038Z","iopub.execute_input":"2021-06-29T17:53:58.362375Z","iopub.status.idle":"2021-06-29T17:53:58.403057Z","shell.execute_reply.started":"2021-06-29T17:53:58.362336Z","shell.execute_reply":"2021-06-29T17:53:58.402187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.drop(columns=['id']).describe().T","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-29T17:53:59.739583Z","iopub.execute_input":"2021-06-29T17:53:59.739908Z","iopub.status.idle":"2021-06-29T17:54:00.291767Z","shell.execute_reply.started":"2021-06-29T17:53:59.739878Z","shell.execute_reply":"2021-06-29T17:54:00.290766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.drop(columns=['id']).describe().T","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-29T17:54:01.759544Z","iopub.execute_input":"2021-06-29T17:54:01.759854Z","iopub.status.idle":"2021-06-29T17:54:02.112024Z","shell.execute_reply.started":"2021-06-29T17:54:01.759824Z","shell.execute_reply":"2021-06-29T17:54:02.111076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12, 6))\nsns.countplot(x='target', data=train)\nax.set_title('Target Distribution')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-29T17:54:11.764543Z","iopub.execute_input":"2021-06-29T17:54:11.764865Z","iopub.status.idle":"2021-06-29T17:54:12.121167Z","shell.execute_reply.started":"2021-06-29T17:54:11.764836Z","shell.execute_reply":"2021-06-29T17:54:12.120206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LGBMClassifier model","metadata":{}},{"cell_type":"code","source":"X = train.drop([\"id\",\"target\"], axis=1)\ny = train.target\nX_test = test.drop(\"id\", axis=1)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-29T17:54:15.358508Z","iopub.execute_input":"2021-06-29T17:54:15.35883Z","iopub.status.idle":"2021-06-29T17:54:15.419493Z","shell.execute_reply.started":"2021-06-29T17:54:15.358802Z","shell.execute_reply":"2021-06-29T17:54:15.418618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\"><p style='color:black;'>We are gonna create <b>100 different instances of LGBMClassifier with different random states</b> and then take the average of the predictions.</p></div>","metadata":{}},{"cell_type":"code","source":"iterations = 100\nlgbm_pred = 0\nlogloss = []\nfor i in tqdm_notebook(range(iterations)):\n    random_state = np.random.randint(0,2000)\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y , test_size=0.2, stratify=y, random_state=random_state)\n    lgbm = LGBMClassifier(random_state=random_state)\n    lgbm.fit(X_train,y_train)\n    valid_preds = lgbm.predict_proba(X_valid)\n    lgbm_pred += lgbm.predict_proba(X_test)/iterations\n    logloss.append(log_loss(y_valid,valid_preds))\n    print(f'Iteration {i} : Random State = {random_state}, Log Loss = {log_loss(y_valid,valid_preds)}')\nprint(f'Average Log Loss = {sum(logloss)/len(logloss)}')","metadata":{"execution":{"iopub.status.busy":"2021-06-29T17:17:54.114569Z","iopub.execute_input":"2021-06-29T17:17:54.115133Z","iopub.status.idle":"2021-06-29T17:22:27.199607Z","shell.execute_reply.started":"2021-06-29T17:17:54.115089Z","shell.execute_reply":"2021-06-29T17:22:27.198741Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üìÅ Submission file","metadata":{}},{"cell_type":"code","source":"ss[\"Class_1\"] = lgbm_pred[:,0]\nss[\"Class_2\"] = lgbm_pred[:,1]\nss[\"Class_3\"] = lgbm_pred[:,2]\nss[\"Class_4\"] = lgbm_pred[:,3]\nss[\"Class_5\"] = lgbm_pred[:,4]\nss[\"Class_6\"] = lgbm_pred[:,5]\nss[\"Class_7\"] = lgbm_pred[:,6]\nss[\"Class_8\"] = lgbm_pred[:,7]\nss[\"Class_9\"] = lgbm_pred[:,8]\nss.to_csv(\"/kaggle/working/lgbm_100_ensemble_sub.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T17:22:27.200806Z","iopub.execute_input":"2021-06-29T17:22:27.201307Z","iopub.status.idle":"2021-06-29T17:22:29.572659Z","shell.execute_reply.started":"2021-06-29T17:22:27.201268Z","shell.execute_reply":"2021-06-29T17:22:29.571615Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Kindly Upvote, if you like this notebook.","metadata":{}}]}