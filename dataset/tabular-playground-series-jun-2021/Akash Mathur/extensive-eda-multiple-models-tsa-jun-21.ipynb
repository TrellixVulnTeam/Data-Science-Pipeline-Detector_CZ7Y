{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-11T12:08:02.060937Z","iopub.execute_input":"2021-06-11T12:08:02.061565Z","iopub.status.idle":"2021-06-11T12:08:02.081399Z","shell.execute_reply.started":"2021-06-11T12:08:02.061423Z","shell.execute_reply":"2021-06-11T12:08:02.079888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 100)\nimport scipy as sp\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\nimport datetime\n\nimport plotly.express as px\nimport plotly.graph_objs as go\n\nfrom sklearn.metrics import log_loss\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_val_score\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:08:02.084174Z","iopub.execute_input":"2021-06-11T12:08:02.08462Z","iopub.status.idle":"2021-06-11T12:08:04.905458Z","shell.execute_reply.started":"2021-06-11T12:08:02.084571Z","shell.execute_reply":"2021-06-11T12:08:04.904638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sneak Peak into Dataset","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/tabular-playground-series-jun-2021/train.csv')\ntest = pd.read_csv('/kaggle/input/tabular-playground-series-jun-2021/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:08:04.906946Z","iopub.execute_input":"2021-06-11T12:08:04.907389Z","iopub.status.idle":"2021-06-11T12:08:07.197545Z","shell.execute_reply.started":"2021-06-11T12:08:04.907356Z","shell.execute_reply":"2021-06-11T12:08:07.196307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:08:07.199537Z","iopub.execute_input":"2021-06-11T12:08:07.19997Z","iopub.status.idle":"2021-06-11T12:08:07.258029Z","shell.execute_reply.started":"2021-06-11T12:08:07.199929Z","shell.execute_reply":"2021-06-11T12:08:07.257273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Handy Functions","metadata":{}},{"cell_type":"code","source":"def resumetable(df):\n    print(f\"Dataset Shape: {df.shape}\")\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['First Value'] = df.loc[0].values\n    summary['Second Value'] = df.loc[1].values\n    summary['Third Value'] = df.loc[2].values\n    return summary\n\n\n## Function to compute outliers for normally distributed column\ndef CalcOutliers(df_num): \n\n    # calculating mean and std of the array\n    data_mean, data_std = np.mean(df_num), np.std(df_num)\n\n    # seting the cut line to both higher and lower values\n    # You can change this value\n    cut = data_std * 3\n\n    #Calculating the higher and lower cut values\n    lower, upper = data_mean - cut, data_mean + cut\n\n    # creating an array of lower, higher and total outlier values \n    outliers_lower = [x for x in df_num if x < lower]\n    outliers_higher = [x for x in df_num if x > upper]\n    outliers_total = [x for x in df_num if x < lower or x > upper]\n\n    # array without outlier values\n    outliers_removed = [x for x in df_num if x > lower and x < upper]\n    \n    print('Identified lowest outliers: %d' % len(outliers_lower)) # printing total number of values in lower cut of outliers\n    print('Identified upper outliers: %d' % len(outliers_higher)) # printing total number of values in higher cut of outliers\n    print('Total outlier observations: %d' % len(outliers_total)) # printing total number of values outliers of both sides\n    print('Non-outlier observations: %d' % len(outliers_removed)) # printing total number of non outlier values\n    print(\"Total percentual of Outliers: \", round((len(outliers_total) / len(outliers_removed) )*100, 4)) # Percentual of outliers in points\n    \n    return","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:08:07.259257Z","iopub.execute_input":"2021-06-11T12:08:07.259759Z","iopub.status.idle":"2021-06-11T12:08:07.270862Z","shell.execute_reply.started":"2021-06-11T12:08:07.259689Z","shell.execute_reply":"2021-06-11T12:08:07.269872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resumetable(train)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:08:07.272165Z","iopub.execute_input":"2021-06-11T12:08:07.272654Z","iopub.status.idle":"2021-06-11T12:08:07.592107Z","shell.execute_reply.started":"2021-06-11T12:08:07.272601Z","shell.execute_reply":"2021-06-11T12:08:07.591315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resumetable(test)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:08:07.593876Z","iopub.execute_input":"2021-06-11T12:08:07.594361Z","iopub.status.idle":"2021-06-11T12:08:07.747229Z","shell.execute_reply.started":"2021-06-11T12:08:07.594315Z","shell.execute_reply":"2021-06-11T12:08:07.745843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Unique Values Counts","metadata":{}},{"cell_type":"code","source":"t1 = resumetable(train)\nt2 = resumetable(test)\n\nfeature_cols = [f for f in train.columns if f.startswith('feature')]\nt1_ = t1.loc[t1['Name'].isin(feature_cols), ['Name','Uniques']].rename({'Uniques':'Train_Uniques'}, axis=1)\nt2_ = t2.loc[t2['Name'].isin(feature_cols), ['Uniques']].rename({'Uniques':'Test_Uniques'}, axis=1)\nt_concat = pd.concat([t1_,t2_], axis=1)\n\nfig = go.Figure()\nfig.add_trace(go.Bar(x=t_concat['Name'], y=t_concat['Train_Uniques'],\n                marker_color='crimson',\n                name='Train Unique Count'))\nfig.add_trace(go.Bar(x=t_concat['Name'], y=t_concat['Test_Uniques'],\n                marker_color='lightslategrey',\n                name='Test Unique Count'\n                ))\n\nfig.update_layout(title='Unique Count acorss Dataset')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:08:07.749932Z","iopub.execute_input":"2021-06-11T12:08:07.750272Z","iopub.status.idle":"2021-06-11T12:08:08.240416Z","shell.execute_reply.started":"2021-06-11T12:08:07.750237Z","shell.execute_reply":"2021-06-11T12:08:08.239436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Key Observation : \n- There are few features where there is a difference in the unique values across Test and Train Dataset. Let's try to take a closer look at those features :","metadata":{}},{"cell_type":"code","source":"t_concat['Difference'] = t_concat['Train_Uniques']- t_concat['Test_Uniques']\nt_concat[t_concat['Difference']!=0].sort_values(by='Difference')\\\n            .plot(x='Name', y='Difference', kind='bar')\n\nplt.title('Count Difference between Train and Test Dataset', fontsize='16')\nplt.xlabel('Feature Names')\nplt.ylabel('Count')\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T12:36:26.55175Z","iopub.execute_input":"2021-06-10T12:36:26.552115Z","iopub.status.idle":"2021-06-10T12:36:26.749757Z","shell.execute_reply.started":"2021-06-10T12:36:26.552067Z","shell.execute_reply":"2021-06-10T12:36:26.748845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Key Observation : \n- There are 6 features where there is a difference in unique value counts.  ","metadata":{}},{"cell_type":"code","source":"f15 = np.setdiff1d(train['feature_15'].values, test['feature_15'].values)\nf46 = np.setdiff1d(train['feature_46'].values, test['feature_46'].values)\nf73 = np.setdiff1d(train['feature_73'].values, test['feature_73'].values)\nf28 = np.setdiff1d(train['feature_28'].values, test['feature_28'].values)\nf59 = np.setdiff1d(train['feature_59'].values, test['feature_59'].values)\nf60 = np.setdiff1d(train['feature_60'].values, test['feature_60'].values)\n\nprint(f\"feature_15 : There are {len(f15)} different values in Train dataset and these values are : {f15}\")\nprint(f\"feature_46 : There are {len(f46)} different values in Train dataset and these values are : {f46}\")\nprint(f\"feature_73 : There are {len(f73)} different values in Train dataset and these values are : {f73}\")\nprint(f\"feature_28 : There are {len(f28)} different values in Train dataset and these values are : {f28}\")\nprint(f\"feature_59 : There are {len(f59)} different values in Train dataset and these values are : {f59}\")\nprint(f\"feature_60 : There are {len(f60)} different values in Train dataset and these values are : {f60}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-10T13:13:42.740408Z","iopub.execute_input":"2021-06-10T13:13:42.741032Z","iopub.status.idle":"2021-06-10T13:13:42.801863Z","shell.execute_reply.started":"2021-06-10T13:13:42.740982Z","shell.execute_reply":"2021-06-10T13:13:42.800846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Most Common Unique Values ","metadata":{}},{"cell_type":"code","source":"arr_train = [train[col].values for col in feature_cols]\narr_test = [test[col].values for col in feature_cols]\n\ntrain_array = np.sort(np.concatenate(arr_train, axis=0))\ntest_array = np.sort(np.concatenate(arr_test, axis=0))\n\nunique1, counts1 = np.unique(train_array, return_counts=True)\nunique2, counts2 = np.unique(test_array, return_counts=True)\n\ndf_ = pd.DataFrame([unique1, counts1, unique2, counts2]).T\\\n                            .rename({0:\"Train Unique Value\", 1:\"Train Unique Count\", 2:\"Test Unique Value\", 3:\"Test Unique Count\"}, axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:38:29.593675Z","iopub.execute_input":"2021-06-10T11:38:29.594096Z","iopub.status.idle":"2021-06-10T11:38:31.07275Z","shell.execute_reply.started":"2021-06-10T11:38:29.594054Z","shell.execute_reply":"2021-06-10T11:38:31.0715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_graph = df_.head(20)\nfig = go.Figure()\n# Create and style traces\nfig.add_trace(go.Scatter(x=df_graph['Train Unique Value'], y=df_graph['Train Unique Count'], name = 'Train Unique Count',\n                         line=dict(color='royalblue', width=4, dash='dashdot')))\nfig.add_trace(go.Scatter(x=df_graph['Train Unique Value'], y=df_graph['Test Unique Count'], name = 'Test Unique Count',\n                         line=dict(color='firebrick', width=4,\n                              dash='dash')))\n\nfig.update_layout(title='Top 10 Unique Count of Train and Test Dataset Values',\n                   xaxis_title='Unique Values in Dataset',\n                   yaxis_title='Unique Count')\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:39:51.496065Z","iopub.execute_input":"2021-06-10T11:39:51.4965Z","iopub.status.idle":"2021-06-10T11:39:51.509522Z","shell.execute_reply.started":"2021-06-10T11:39:51.496458Z","shell.execute_reply":"2021-06-10T11:39:51.508825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Desriptive Analysis","metadata":{}},{"cell_type":"code","source":"train.drop('id', axis=1).describe().T.style.bar(subset=['mean'], color='#205ff2')\\\n                            .background_gradient(subset=['std'], cmap='Reds')\\\n                            .background_gradient(subset=['50%'], cmap='PuRd')\\\n                            .background_gradient(subset=['max'], cmap='Greens')","metadata":{"execution":{"iopub.status.busy":"2021-06-11T07:18:24.215578Z","iopub.execute_input":"2021-06-11T07:18:24.215929Z","iopub.status.idle":"2021-06-11T07:18:24.847536Z","shell.execute_reply.started":"2021-06-11T07:18:24.215902Z","shell.execute_reply":"2021-06-11T07:18:24.846527Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.drop('id', axis=1).describe().T.style.bar(subset=['mean'], color='#205ff2')\\\n                            .background_gradient(subset=['std'], cmap='Reds')\\\n                            .background_gradient(subset=['50%'], cmap='PuRd')\\\n                            .background_gradient(subset=['max'], cmap='Greens')","metadata":{"execution":{"iopub.status.busy":"2021-06-11T07:18:42.989544Z","iopub.execute_input":"2021-06-11T07:18:42.989936Z","iopub.status.idle":"2021-06-11T07:18:43.446081Z","shell.execute_reply.started":"2021-06-11T07:18:42.989902Z","shell.execute_reply":"2021-06-11T07:18:43.44486Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Numeric Features Distribution","metadata":{}},{"cell_type":"code","source":"numeric_features = train.select_dtypes('int64').columns.to_list()\nnumeric_features.remove('id')","metadata":{"execution":{"iopub.status.busy":"2021-06-10T13:41:09.85894Z","iopub.execute_input":"2021-06-10T13:41:09.859344Z","iopub.status.idle":"2021-06-10T13:41:09.925158Z","shell.execute_reply.started":"2021-06-10T13:41:09.859297Z","shell.execute_reply":"2021-06-10T13:41:09.923768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplots(25,3, figsize=(25,60))\nplt.suptitle(\"Distribution of Numeric Features in Train Dataset\", fontsize=20, y=1)\n\nfor e, feature in enumerate(numeric_features,1):\n    plt.subplot(25,3,e)\n    sns.histplot(np.log1p(train[feature]), kde=True)\n        \n    plt.xlabel('Values', size=12, labelpad=15)\n    plt.ylabel('Count', size=12, labelpad=15)    \n    plt.title(f\"{feature.capitalize()}\", size=16)\n\nplt.tight_layout(h_pad=2)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T13:52:17.227278Z","iopub.execute_input":"2021-06-10T13:52:17.227684Z","iopub.status.idle":"2021-06-10T13:52:26.309552Z","shell.execute_reply.started":"2021-06-10T13:52:17.227645Z","shell.execute_reply":"2021-06-10T13:52:26.308824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Target Variable Distribution","metadata":{}},{"cell_type":"code","source":"target_count = (train['target'].value_counts(normalize=True)*100).round(1).to_frame()\n\nplt.subplots(1,1, figsize=(15,7))\nplt.suptitle('Target Variable/Classes Distribution', size=22)\n\nplt.subplot(1,1,1)\ng = sns.countplot(x='target', data=train, order=target_count.index)\ngt = g.twinx()\ngt = sns.pointplot(x= target_count.index, y= 'target', data=target_count, color='black')\ngt.set_ylabel(\"% of Class Distribution\", fontsize=16)\n\ng.set_xlabel('Classes', size=20, labelpad=15)\ng.set_ylabel('Count', size=20, labelpad=15)    \n\nsizes=[]\ntotal = len(train)\n\nfor p in g.patches:\n    height = p.get_height()\n    sizes.append(height)\n    g.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:1.2f}%'.format((height/total)*100),\n            ha=\"left\", fontsize=14)\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T07:38:18.637844Z","iopub.execute_input":"2021-06-11T07:38:18.638237Z","iopub.status.idle":"2021-06-11T07:38:19.2556Z","shell.execute_reply.started":"2021-06-11T07:38:18.638203Z","shell.execute_reply":"2021-06-11T07:38:19.254309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Bi-Variate Analysis","metadata":{}},{"cell_type":"code","source":"# Find correlations among feature columns \ncorr = train[feature_cols].corr()\n\n# Heatmap of correlations\nfig, ax = plt.subplots(figsize=(35,15))\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nsns.heatmap(corr,\n         square=True, center=0, linewidth=0.2,\n         cmap=sns.diverging_palette(240, 10, as_cmap=True),\n         mask=mask, ax=ax) \n\nax.set_title('Feature Correlation', loc='left', fontweight='bold')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T07:33:00.204823Z","iopub.execute_input":"2021-06-11T07:33:00.205423Z","iopub.status.idle":"2021-06-11T07:33:06.21434Z","shell.execute_reply.started":"2021-06-11T07:33:00.205372Z","shell.execute_reply":"2021-06-11T07:33:06.213375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_agg = train.groupby('target')[[col for col in train.columns if(col.startswith(\"feature\"))]].mean()\ntrain_agg","metadata":{"execution":{"iopub.status.busy":"2021-06-10T13:52:54.690333Z","iopub.execute_input":"2021-06-10T13:52:54.690699Z","iopub.status.idle":"2021-06-10T13:52:54.957112Z","shell.execute_reply.started":"2021-06-10T13:52:54.690667Z","shell.execute_reply":"2021-06-10T13:52:54.956062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_idmax = train_agg.idxmax(axis=1).to_frame().reset_index().rename({0:'Feature'}, axis=1)\ntrain_max = train_agg.max(axis=1).to_frame().reset_index(drop=True).rename({0:'Mean Value'}, axis=1)\n\ntrain_combined = pd.concat([train_idmax, train_max], axis=1)\ntrain_combined","metadata":{"execution":{"iopub.status.busy":"2021-06-10T13:52:54.95888Z","iopub.execute_input":"2021-06-10T13:52:54.959332Z","iopub.status.idle":"2021-06-10T13:52:54.977604Z","shell.execute_reply.started":"2021-06-10T13:52:54.959291Z","shell.execute_reply":"2021-06-10T13:52:54.976903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\n\nfig = px.bar(train_combined, x='target', y='Mean Value',\n             hover_data=['Feature', 'Mean Value'], color='Feature',\n             labels={'pop':'Mean Value'}, height=400, title=\"Top Feature affecting the Target Variable/Classes based on Mean\")\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T13:52:54.97933Z","iopub.execute_input":"2021-06-10T13:52:54.979636Z","iopub.status.idle":"2021-06-10T13:52:56.203984Z","shell.execute_reply.started":"2021-06-10T13:52:54.979606Z","shell.execute_reply":"2021-06-10T13:52:56.203075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's deep dive in feature_19 and feature_54 variable.","metadata":{}},{"cell_type":"markdown","source":"## Data Pre-processing","metadata":{}},{"cell_type":"markdown","source":"#### Train Dataset","metadata":{}},{"cell_type":"code","source":"train_model = train.copy()\n\nfrom sklearn import preprocessing\n  \nlabel_encoder = preprocessing.LabelEncoder()\ntrain_model['target']= label_encoder.fit_transform(train_model['target'])\n\ny=train_model['target']","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:09:04.807704Z","iopub.execute_input":"2021-06-11T12:09:04.808159Z","iopub.status.idle":"2021-06-11T12:09:04.980136Z","shell.execute_reply.started":"2021-06-11T12:09:04.808123Z","shell.execute_reply":"2021-06-11T12:09:04.979123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train_model[feature_cols]\n\nX = StandardScaler().fit_transform(X)\nX","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:09:04.982027Z","iopub.execute_input":"2021-06-11T12:09:04.982459Z","iopub.status.idle":"2021-06-11T12:09:05.545132Z","shell.execute_reply.started":"2021-06-11T12:09:04.982416Z","shell.execute_reply":"2021-06-11T12:09:05.543991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### PCA","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca =  PCA(n_components= 30)\npca.fit(X)\npca_samples = pca.transform(X)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:09:05.555595Z","iopub.execute_input":"2021-06-11T12:09:05.556121Z","iopub.status.idle":"2021-06-11T12:09:08.739535Z","shell.execute_reply.started":"2021-06-11T12:09:05.556023Z","shell.execute_reply":"2021-06-11T12:09:08.738056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ps = pd.DataFrame(pca_samples)\nps.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:09:08.74163Z","iopub.execute_input":"2021-06-11T12:09:08.74239Z","iopub.status.idle":"2021-06-11T12:09:08.797585Z","shell.execute_reply.started":"2021-06-11T12:09:08.742334Z","shell.execute_reply":"2021-06-11T12:09:08.796525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Development","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2)\n\nprint(f\"Train and Test Shape: \\n {[x.shape for x in [X_train, X_test, y_train, y_test]]}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:09:08.799238Z","iopub.execute_input":"2021-06-11T12:09:08.799923Z","iopub.status.idle":"2021-06-11T12:09:09.166252Z","shell.execute_reply.started":"2021-06-11T12:09:08.799871Z","shell.execute_reply":"2021-06-11T12:09:09.16488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model 1 : Logistic Regression","metadata":{}},{"cell_type":"code","source":"lr = LogisticRegression(penalty='l2',random_state=42)\n\nlr.fit(X_train, y_train)\n\n# Predict y_pred\ny_pred = lr.predict_proba(X_test)\nlog_loss_score_lr = log_loss(y_test,y_pred)\nprint(f\"Logistic Regression : {log_loss_score_lr}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:09:09.167962Z","iopub.execute_input":"2021-06-11T12:09:09.168289Z","iopub.status.idle":"2021-06-11T12:09:17.827078Z","shell.execute_reply.started":"2021-06-11T12:09:09.168255Z","shell.execute_reply":"2021-06-11T12:09:17.82583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model 2 : Random Forest","metadata":{}},{"cell_type":"code","source":"# Hyperparameter grid\nparam_grid = {\n    \"criterion\" : [\"gini\", \"entropy\"], \n    \"min_samples_leaf\" : [1, 5, 10], \n    \"min_samples_split\" : [2, 4, 10, 12, 16], \n    \"n_estimators\": [50, 100, 150, 200],\n    'max_depth': list(np.linspace(5, 30).astype(int)),\n    'max_leaf_nodes': list(np.linspace(10, 50, 500).astype(int)),\n    'bootstrap': [True, False]\n}\n\n# Estimator for use in random search\nrf = RandomForestClassifier(max_features='auto', oob_score=True, random_state=1, n_jobs=-1)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:09:17.829882Z","iopub.execute_input":"2021-06-11T12:09:17.830624Z","iopub.status.idle":"2021-06-11T12:09:17.842525Z","shell.execute_reply.started":"2021-06-11T12:09:17.83057Z","shell.execute_reply":"2021-06-11T12:09:17.841111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the random search model\nrs = RandomizedSearchCV(rf, param_grid, n_jobs = -1, \n                        cv = 3, verbose = 1, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:09:17.846448Z","iopub.execute_input":"2021-06-11T12:09:17.848942Z","iopub.status.idle":"2021-06-11T12:09:17.860293Z","shell.execute_reply.started":"2021-06-11T12:09:17.848574Z","shell.execute_reply":"2021-06-11T12:09:17.858481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rs.fit(X_train, y_train)\n\n# Predict y_pred\ny_pred = rs.predict_proba(X_test)\nlog_loss_score_rs = log_loss(y_test,y_pred)\nprint(f\"RandomForestClassifier : {log_loss_score_rs}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:09:17.865764Z","iopub.execute_input":"2021-06-11T12:09:17.867249Z","iopub.status.idle":"2021-06-11T12:11:33.358931Z","shell.execute_reply.started":"2021-06-11T12:09:17.867178Z","shell.execute_reply":"2021-06-11T12:11:33.357395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predict Test Dataset","metadata":{}},{"cell_type":"code","source":"test_model = test.drop('id', axis=1)\n\nX_test = test_model[feature_cols]\n\nX_test = StandardScaler().fit_transform(X_test)\nX_test","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:16:11.494417Z","iopub.execute_input":"2021-06-11T12:16:11.494892Z","iopub.status.idle":"2021-06-11T12:16:11.805051Z","shell.execute_reply.started":"2021-06-11T12:16:11.494838Z","shell.execute_reply":"2021-06-11T12:16:11.803764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_test_dataset = rs.predict_proba(X_test)\ny_pred_test_dataset","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:16:31.781647Z","iopub.execute_input":"2021-06-11T12:16:31.782165Z","iopub.status.idle":"2021-06-11T12:16:32.93575Z","shell.execute_reply.started":"2021-06-11T12:16:31.782119Z","shell.execute_reply":"2021-06-11T12:16:32.934522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_pred=pd.DataFrame(y_pred_test_dataset)\ntest_pred","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:17:15.744556Z","iopub.execute_input":"2021-06-11T12:17:15.744957Z","iopub.status.idle":"2021-06-11T12:17:15.767412Z","shell.execute_reply.started":"2021-06-11T12:17:15.744922Z","shell.execute_reply":"2021-06-11T12:17:15.766416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_pred.columns = label_encoder.inverse_transform(test_pred.columns)\ntest_pred","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:17:35.801192Z","iopub.execute_input":"2021-06-11T12:17:35.801728Z","iopub.status.idle":"2021-06-11T12:17:35.830973Z","shell.execute_reply.started":"2021-06-11T12:17:35.801664Z","shell.execute_reply":"2021-06-11T12:17:35.829887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_submiss = pd.concat([test.id, test_pred], axis=1)\nfinal_submiss","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:23:01.047954Z","iopub.execute_input":"2021-06-11T12:23:01.048354Z","iopub.status.idle":"2021-06-11T12:23:01.082283Z","shell.execute_reply.started":"2021-06-11T12:23:01.048315Z","shell.execute_reply":"2021-06-11T12:23:01.08118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Final Submission","metadata":{}},{"cell_type":"code","source":"final_submiss.to_csv(\"result.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:35:44.764723Z","iopub.execute_input":"2021-06-11T12:35:44.765164Z","iopub.status.idle":"2021-06-11T12:35:46.696681Z","shell.execute_reply.started":"2021-06-11T12:35:44.765124Z","shell.execute_reply":"2021-06-11T12:35:46.695873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Please Upvote if you like it !","metadata":{}}]}