{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport time as time\nimport lightgbm\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import log_loss\nfrom tqdm.notebook import tqdm\nimport catboost\nimport xgboost","metadata":{"execution":{"iopub.status.busy":"2021-06-06T21:29:16.577525Z","iopub.execute_input":"2021-06-06T21:29:16.578056Z","iopub.status.idle":"2021-06-06T21:29:19.086851Z","shell.execute_reply.started":"2021-06-06T21:29:16.577962Z","shell.execute_reply":"2021-06-06T21:29:19.085799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"directory = '/kaggle/input/tabular-playground-series-jun-2021/'\ntrain = pd.read_csv(directory + 'train.csv')\ntest = pd.read_csv(directory + 'test.csv')\nsubmission = pd.read_csv(directory + 'sample_submission.csv')\ntrain, test, submission = train.set_index('id'), test.set_index('id'), submission.set_index('id')\ntrain.sample(3)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T21:29:19.088285Z","iopub.execute_input":"2021-06-06T21:29:19.088649Z","iopub.status.idle":"2021-06-06T21:29:21.415869Z","shell.execute_reply.started":"2021-06-06T21:29:19.088556Z","shell.execute_reply":"2021-06-06T21:29:21.41484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With the following plot, we can see that class 6 and 8 are the most frequent in the training set by a large margin. Classes 1, 4, and 5 are very infrequent. ","metadata":{}},{"cell_type":"code","source":"print(train['target'].value_counts())\nsns.histplot(train['target'])","metadata":{"execution":{"iopub.status.busy":"2021-06-06T21:29:21.418352Z","iopub.execute_input":"2021-06-06T21:29:21.418771Z","iopub.status.idle":"2021-06-06T21:29:22.015919Z","shell.execute_reply.started":"2021-06-06T21:29:21.418725Z","shell.execute_reply":"2021-06-06T21:29:22.01491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"converting the string labels to integers and preparing the data for machine learning:","metadata":{}},{"cell_type":"code","source":"labels = [int(x[-1]) for x in train['target'].values]\nX_train, X_valid, y_train, y_valid = train_test_split(train.drop('target', axis=1).values, labels, shuffle=True, test_size=0.3, random_state=2021)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T21:29:22.017466Z","iopub.execute_input":"2021-06-06T21:29:22.017778Z","iopub.status.idle":"2021-06-06T21:29:22.377Z","shell.execute_reply.started":"2021-06-06T21:29:22.017748Z","shell.execute_reply":"2021-06-06T21:29:22.376177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Because there are so many features in the data, time consumption could be a factor in parameter optimization and cv. The following cell measures how long a base lightgbm model performs on all features.","metadata":{}},{"cell_type":"code","source":"start = time.time()\nlgb = lightgbm.LGBMClassifier()\nlgb.fit(X_train, y_train)\nlgb_pred = lgb.predict_proba(X_valid)\nlgb_base_score = log_loss(y_valid, lgb_pred)\nend = time.time()\nprint(f\"lightgbm scored {lgb_base_score} in {end-start} seconds\")","metadata":{"execution":{"iopub.status.busy":"2021-06-06T21:29:22.378568Z","iopub.execute_input":"2021-06-06T21:29:22.379236Z","iopub.status.idle":"2021-06-06T21:29:35.954706Z","shell.execute_reply.started":"2021-06-06T21:29:22.379161Z","shell.execute_reply":"2021-06-06T21:29:35.953529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With the trained lightgbm, we can find out which features are the most important and which can be dropped. Lightgbm also allows you to plot the feature importance using seaborn:","metadata":{}},{"cell_type":"code","source":"fig_dims = (20, 20)\nfig, ax = plt.subplots(figsize=fig_dims)\nlightgbm.plot_importance(lgb, ax=ax)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T21:29:35.956242Z","iopub.execute_input":"2021-06-06T21:29:35.956891Z","iopub.status.idle":"2021-06-06T21:29:37.460772Z","shell.execute_reply.started":"2021-06-06T21:29:35.956823Z","shell.execute_reply":"2021-06-06T21:29:37.459658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"important_features_lgb = list(lgb.feature_importances_)\nfeatures = list(train.drop('target', axis=1).columns)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T21:29:37.462437Z","iopub.execute_input":"2021-06-06T21:29:37.463044Z","iopub.status.idle":"2021-06-06T21:29:37.506861Z","shell.execute_reply.started":"2021-06-06T21:29:37.462997Z","shell.execute_reply":"2021-06-06T21:29:37.506024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following code gets the scores and time consumption for training a lightgbm on x features.The minimum amount of features is 29, and the maximum is 74, with increments of 5 after each iteration.","metadata":{}},{"cell_type":"code","source":"times = []\nscores = []\nnum_features_counts = []\nfor num_features in tqdm(range(29, 75, 5)):\n    num_features_counts.append(num_features)\n    feature_names = features.copy()\n    features_copy = important_features_lgb.copy()\n    important_features = []\n    for num in range(num_features):\n        important_feature = feature_names[features_copy.index(max(features_copy))]\n        features_copy[features_copy.index(max(features_copy))] = -100\n        important_features.append(important_feature)\n    condensed_train = train[important_features]\n    X_train, X_valid = train_test_split(condensed_train.values, shuffle=True, test_size=0.3, random_state=2021)\n    start = time.time()\n    lgb = lightgbm.LGBMClassifier()\n    lgb.fit(X_train, y_train)\n    lgb_pred = lgb.predict_proba(X_valid)\n    lgb_base_score = log_loss(y_valid, lgb_pred)\n    end = time.time()\n    times.append(end-start)\n    scores.append(lgb_base_score)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T21:29:37.510042Z","iopub.execute_input":"2021-06-06T21:29:37.510702Z","iopub.status.idle":"2021-06-06T21:31:40.594846Z","shell.execute_reply.started":"2021-06-06T21:29:37.510653Z","shell.execute_reply":"2021-06-06T21:31:40.593823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following three plots plot the times, scores, and amount of features against one another. ","metadata":{}},{"cell_type":"code","source":"fig_dims = (10, 5)\nfig, ax = plt.subplots(figsize=fig_dims)\nax1 = sns.lineplot(x=times, y=scores, ax=ax)\nax1.set(xlabel='times', ylabel='scores')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-06T21:31:40.599052Z","iopub.execute_input":"2021-06-06T21:31:40.599361Z","iopub.status.idle":"2021-06-06T21:31:40.77981Z","shell.execute_reply.started":"2021-06-06T21:31:40.59933Z","shell.execute_reply":"2021-06-06T21:31:40.778626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig_dims = (10, 5)\nfig, ax = plt.subplots(figsize=fig_dims)\nax1 = sns.lineplot(x=num_features_counts, y=scores, ax=ax)\nax1.set(xlabel='number of features', ylabel='scores')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-06T21:31:40.781241Z","iopub.execute_input":"2021-06-06T21:31:40.781605Z","iopub.status.idle":"2021-06-06T21:31:40.95011Z","shell.execute_reply.started":"2021-06-06T21:31:40.781572Z","shell.execute_reply":"2021-06-06T21:31:40.948754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig_dims = (10, 5)\nfig, ax = plt.subplots(figsize=fig_dims)\nax1 = sns.lineplot(x=num_features_counts, y=times, ax=ax)\nax1.set(xlabel='number of features', ylabel='times')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-06T21:31:40.95422Z","iopub.execute_input":"2021-06-06T21:31:40.954591Z","iopub.status.idle":"2021-06-06T21:31:41.119709Z","shell.execute_reply.started":"2021-06-06T21:31:40.954559Z","shell.execute_reply":"2021-06-06T21:31:41.118219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For lightgbm, a very quick model, using all the features is probably ok because it finishes quite quickly. The following code does the same but for catboost.","metadata":{}},{"cell_type":"code","source":"times = []\nscores = []\nnum_features_counts = []\nfor num_features in tqdm(range(29, 75, 5)):\n    num_features_counts.append(num_features)\n    feature_names = features.copy()\n    features_copy = important_features_lgb.copy()\n    important_features = []\n    for num in range(num_features):\n        important_feature = feature_names[features_copy.index(max(features_copy))]\n        features_copy[features_copy.index(max(features_copy))] = -100\n        important_features.append(important_feature)\n    condensed_train = train[important_features]\n    X_train, X_valid = train_test_split(condensed_train.values, shuffle=True, test_size=0.3, random_state=2021)\n    start = time.time()\n    ctb = catboost.CatBoostClassifier()\n    ctb.fit(X_train, y_train, verbose=False)\n    ctb_pred = ctb.predict_proba(X_valid)\n    ctb_base_score = log_loss(y_valid, ctb_pred)\n    end = time.time()\n    times.append(end-start)\n    scores.append(ctb_base_score)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T21:31:41.1216Z","iopub.execute_input":"2021-06-06T21:31:41.122049Z","iopub.status.idle":"2021-06-06T22:00:45.501385Z","shell.execute_reply.started":"2021-06-06T21:31:41.122Z","shell.execute_reply":"2021-06-06T22:00:45.500387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig_dims = (10, 5)\nfig, ax = plt.subplots(figsize=fig_dims)\nax1 = sns.lineplot(x=times, y=scores, ax=ax)\nax1.set(xlabel='times', ylabel='scores')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-06T22:00:45.502699Z","iopub.execute_input":"2021-06-06T22:00:45.503184Z","iopub.status.idle":"2021-06-06T22:00:45.693584Z","shell.execute_reply.started":"2021-06-06T22:00:45.503128Z","shell.execute_reply":"2021-06-06T22:00:45.692147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig_dims = (10, 5)\nfig, ax = plt.subplots(figsize=fig_dims)\nax1 = sns.lineplot(x=num_features_counts, y=scores, ax=ax)\nax1.set(xlabel='number of features', ylabel='scores')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-06T22:00:45.695381Z","iopub.execute_input":"2021-06-06T22:00:45.695746Z","iopub.status.idle":"2021-06-06T22:00:45.875451Z","shell.execute_reply.started":"2021-06-06T22:00:45.695703Z","shell.execute_reply":"2021-06-06T22:00:45.874329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig_dims = (10, 5)\nfig, ax = plt.subplots(figsize=fig_dims)\nax1 = sns.lineplot(x=num_features_counts, y=times, ax=ax)\nax1.set(xlabel='number of features', ylabel='times')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-06T22:00:45.878689Z","iopub.execute_input":"2021-06-06T22:00:45.879068Z","iopub.status.idle":"2021-06-06T22:00:46.046648Z","shell.execute_reply.started":"2021-06-06T22:00:45.879034Z","shell.execute_reply":"2021-06-06T22:00:46.045935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"catboost is a significantly slower model and to run cv, or optuna optimization on it, using all the features could take an incredibly large amount of time. The following does the same for xgboost.","metadata":{}},{"cell_type":"code","source":"times = []\nscores = []\nnum_features_counts = []\nfor num_features in tqdm(range(29, 75, 5)):\n    num_features_counts.append(num_features)\n    feature_names = features.copy()\n    features_copy = important_features_lgb.copy()\n    important_features = []\n    for num in range(num_features):\n        important_feature = feature_names[features_copy.index(max(features_copy))]\n        features_copy[features_copy.index(max(features_copy))] = -100\n        important_features.append(important_feature)\n    condensed_train = train[important_features]\n    X_train, X_valid = train_test_split(condensed_train.values, shuffle=True, test_size=0.3, random_state=2021)\n    start = time.time()\n    xgb = xgboost.XGBClassifier()\n    xgb.fit(X_train, y_train, verbose=False)\n    xgb_pred = xgb.predict_proba(X_valid)\n    xgb_base_score = log_loss(y_valid, xgb_pred)\n    end = time.time()\n    times.append(end-start)\n    scores.append(xgb_base_score)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T22:00:46.047648Z","iopub.execute_input":"2021-06-06T22:00:46.04805Z","iopub.status.idle":"2021-06-06T22:21:24.487174Z","shell.execute_reply.started":"2021-06-06T22:00:46.048015Z","shell.execute_reply":"2021-06-06T22:21:24.486503Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig_dims = (10, 5)\nfig, ax = plt.subplots(figsize=fig_dims)\nax1 = sns.lineplot(x=times, y=scores, ax=ax)\nax1.set(xlabel='times', ylabel='scores')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-06T22:21:24.488329Z","iopub.execute_input":"2021-06-06T22:21:24.488717Z","iopub.status.idle":"2021-06-06T22:21:24.661491Z","shell.execute_reply.started":"2021-06-06T22:21:24.488688Z","shell.execute_reply":"2021-06-06T22:21:24.660728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig_dims = (10, 5)\nfig, ax = plt.subplots(figsize=fig_dims)\nax1 = sns.lineplot(x=num_features_counts, y=scores, ax=ax)\nax1.set(xlabel='number of features', ylabel='scores')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-06T22:21:24.662503Z","iopub.execute_input":"2021-06-06T22:21:24.662996Z","iopub.status.idle":"2021-06-06T22:21:24.823534Z","shell.execute_reply.started":"2021-06-06T22:21:24.662965Z","shell.execute_reply":"2021-06-06T22:21:24.822827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig_dims = (10, 5)\nfig, ax = plt.subplots(figsize=fig_dims)\nax1 = sns.lineplot(x=num_features_counts, y=times, ax=ax)\nax1.set(xlabel='number of features', ylabel='times')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-06T22:21:24.824542Z","iopub.execute_input":"2021-06-06T22:21:24.824953Z","iopub.status.idle":"2021-06-06T22:21:25.149895Z","shell.execute_reply.started":"2021-06-06T22:21:24.824922Z","shell.execute_reply":"2021-06-06T22:21:25.149174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The main conclusion I can draw from this is that for models like xgboost and catboost, the amount of features used in the dataset heavily increases time for training the model. Perhaps for optimizing the parameters of xgboost or catboost, you can use less features and once you have good parameters, use all the features. I apologize for not adding any infrence and I want to create one for infrence later.","metadata":{}}]}