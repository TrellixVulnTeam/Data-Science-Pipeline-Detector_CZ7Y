{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from pathlib import Path\nimport pickle\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import log_loss\n\nimport optuna","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-09T19:45:53.288022Z","iopub.execute_input":"2021-06-09T19:45:53.288443Z","iopub.status.idle":"2021-06-09T19:45:55.184433Z","shell.execute_reply.started":"2021-06-09T19:45:53.28836Z","shell.execute_reply":"2021-06-09T19:45:55.183409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing Data","metadata":{}},{"cell_type":"code","source":"SEED = 4651\n\nINPUT_DIR = Path(\"../input/tabular-playground-series-jun-2021\")\n\ntrain_fn = INPUT_DIR/\"train.csv\"\ntest_fn = INPUT_DIR/\"test.csv\"\n\ntarget = \"target\"\n\nXnames = [f\"feature_{i}\" for i in range(75)]","metadata":{"execution":{"iopub.status.busy":"2021-06-09T19:49:14.465465Z","iopub.execute_input":"2021-06-09T19:49:14.46595Z","iopub.status.idle":"2021-06-09T19:49:14.470787Z","shell.execute_reply.started":"2021-06-09T19:49:14.465917Z","shell.execute_reply":"2021-06-09T19:49:14.469896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(train_fn, index_col='id')\ntest = pd.read_csv(test_fn, index_col='id')\nnrows = len(train)\n\nytrain = train[target]\nXtrain = train[Xnames]\ntrain_target = train[target].map(lambda x: int(x.split('_')[-1])-1)\ntargets = sorted(list(ytrain.unique()))","metadata":{"execution":{"iopub.status.busy":"2021-06-09T19:49:14.638821Z","iopub.execute_input":"2021-06-09T19:49:14.639124Z","iopub.status.idle":"2021-06-09T19:49:16.607117Z","shell.execute_reply.started":"2021-06-09T19:49:14.639094Z","shell.execute_reply":"2021-06-09T19:49:16.606407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_te = train.copy()\nfor t in targets:\n    train_te[t] = train[target].apply(lambda x: x==t).astype(int)\ntest_te = test.copy()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T19:49:16.608181Z","iopub.execute_input":"2021-06-09T19:49:16.60855Z","iopub.status.idle":"2021-06-09T19:49:16.993512Z","shell.execute_reply.started":"2021-06-09T19:49:16.608524Z","shell.execute_reply":"2021-06-09T19:49:16.992551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploring","metadata":{}},{"cell_type":"code","source":"Xtrain.dtypes.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T12:27:50.034118Z","iopub.execute_input":"2021-06-09T12:27:50.034677Z","iopub.status.idle":"2021-06-09T12:27:50.042372Z","shell.execute_reply.started":"2021-06-09T12:27:50.03462Z","shell.execute_reply":"2021-06-09T12:27:50.041279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All features are made up of integers","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(data=ytrain.value_counts().sort_index()/nrows)\nplt.ylabel('Occurrence of Class')","metadata":{"execution":{"iopub.status.busy":"2021-06-09T12:27:25.178235Z","iopub.execute_input":"2021-06-09T12:27:25.178605Z","iopub.status.idle":"2021-06-09T12:27:25.407254Z","shell.execute_reply.started":"2021-06-09T12:27:25.178573Z","shell.execute_reply":"2021-06-09T12:27:25.406295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is a lot of class imbalance, with half of the instances in the trianing set belonging to Classes 6 and 8, and only a few percent belonging to Classes 4 and 5.","metadata":{}},{"cell_type":"code","source":"corr = train_te.corr()\nsns.heatmap(data=corr, vmin=-1, vmax=1, cmap='bwr')","metadata":{"execution":{"iopub.status.busy":"2021-06-09T12:28:11.345031Z","iopub.execute_input":"2021-06-09T12:28:11.345559Z","iopub.status.idle":"2021-06-09T12:28:16.108508Z","shell.execute_reply.started":"2021-06-09T12:28:11.345526Z","shell.execute_reply":"2021-06-09T12:28:16.107397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The correlations between all of the features are all fairly small (and positive interestingly). There is a bit of correlation between the individual features","metadata":{}},{"cell_type":"code","source":"sns.heatmap(data=corr[targets].drop(targets).T, vmin=-1, vmax=1, cmap='bwr')","metadata":{"execution":{"iopub.status.busy":"2021-06-09T12:30:02.215435Z","iopub.execute_input":"2021-06-09T12:30:02.215965Z","iopub.status.idle":"2021-06-09T12:30:02.749418Z","shell.execute_reply.started":"2021-06-09T12:30:02.215933Z","shell.execute_reply":"2021-06-09T12:30:02.748366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There isn't much correlation between the features and classes directly, but interestingly the ones that do show some corelation seem to have roughly the same sign and magnitude of correlation with most of the features.","metadata":{}},{"cell_type":"code","source":"max_counts = Xtrain.apply(lambda x: x.value_counts().max())\nmin_counts = Xtrain.apply(lambda x: x.value_counts().min())\nnum_unique = Xtrain.nunique()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T12:29:10.836758Z","iopub.execute_input":"2021-06-09T12:29:10.837161Z","iopub.status.idle":"2021-06-09T12:29:11.356607Z","shell.execute_reply.started":"2021-06-09T12:29:10.837125Z","shell.execute_reply":"2021-06-09T12:29:11.355446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(7, 7))\nsns.scatterplot(data=max_counts/nrows, ax=ax1)\nax1.set_xticks([])\nax1.set_ylabel(\"Fraction of total in observation\")\nax1.set_title(\"Most frequent value\")\nsns.scatterplot(data=min_counts/nrows, ax=ax2)\nax2.set_xticks([])\nax2.set_title(\"Least frequent value\")\nsns.scatterplot(data=num_unique, ax=ax3)\nax3.set_xticks([])\nax3.set_ylabel(\"Unique values per feature\")\nprint(f\"There are {nrows} total observations\")","metadata":{"execution":{"iopub.status.busy":"2021-06-09T12:29:13.230955Z","iopub.execute_input":"2021-06-09T12:29:13.231388Z","iopub.status.idle":"2021-06-09T12:29:14.079531Z","shell.execute_reply.started":"2021-06-09T12:29:13.231355Z","shell.execute_reply":"2021-06-09T12:29:14.078385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that there are much fewer unique values per feature than there are total observations, and the most frequent value of each feature is a sizeable fraction of the total number of observations for each feature. Because of this, it seems reasonable to assume all of these features are categorical, since we have no other information about them. This would suggest that any models we try to apply would do best if we treat them accordingly.","metadata":{}},{"cell_type":"markdown","source":"# Target Encoding","metadata":{}},{"cell_type":"markdown","source":"One way of dealing with categorical variables which have a fairly high cardinality is to target encode them, there's a good discussion of how this works [written by Max Halford](https://maxhalford.github.io/blog/target-encoding/). The work presented in that post only talks about a binary target, but our target has nine unique values which it can assume. We will therefore need to modify the method a bit.\n\nWhat seems like it would be most natural would be to take each feature and generate nine new features (the cardinality of our target), one applying the target encoding framework to each possible value that our target can assume.","metadata":{}},{"cell_type":"code","source":"from collections import defaultdict\n\n# Slightly modified from https://maxhalford.github.io/blog/target-encoding/\ndef calc_smooth_mean(df, by, on, m):\n    # Compute the global mean\n    mean = df[on].mean()\n\n    # Compute the number of values and the mean of each group\n    agg = df.groupby(by)[on].agg(['count', 'mean'])\n    counts = agg['count']\n    means = agg['mean']\n\n    # Compute the \"smoothed\" means\n    smooth = (counts * means + m * mean) / (counts + m)\n\n    # Replace each value by the according smoothed mean\n    # Generate a mapping to the smoothed mean which returns the global mean if\n    # it encounters any value unseen in the training set\n    return defaultdict(lambda: mean, smooth)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T19:49:25.228319Z","iopub.execute_input":"2021-06-09T19:49:25.228644Z","iopub.status.idle":"2021-06-09T19:49:25.234457Z","shell.execute_reply.started":"2021-06-09T19:49:25.228614Z","shell.execute_reply":"2021-06-09T19:49:25.233283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_te = train.copy()\nfor t in targets:\n    train_te[t] = train[target].apply(lambda x: x==t).astype(int)\ntest_te = test.copy()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T19:49:25.534959Z","iopub.execute_input":"2021-06-09T19:49:25.535309Z","iopub.status.idle":"2021-06-09T19:49:25.91969Z","shell.execute_reply.started":"2021-06-09T19:49:25.535277Z","shell.execute_reply":"2021-06-09T19:49:25.918719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfor fidx, f in enumerate(Xnames):\n    for tidx, t in enumerate(targets):\n        nname = f\"feature_te{fidx}-{tidx+1}\"\n        smooth=calc_smooth_mean(train_te, f, t, 100)\n        train_te[nname] = train_te[f].map(smooth)\n        test_te[nname] = test_te[f].map(smooth)\ntrain_te = train_te.drop(Xnames+targets+[target], axis=1)\ntest_te = test_te.drop(Xnames, axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T19:49:26.054638Z","iopub.execute_input":"2021-06-09T19:49:26.054966Z","iopub.status.idle":"2021-06-09T19:50:17.157686Z","shell.execute_reply.started":"2021-06-09T19:49:26.054935Z","shell.execute_reply":"2021-06-09T19:50:17.156485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_te.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-09T19:50:17.159039Z","iopub.execute_input":"2021-06-09T19:50:17.159361Z","iopub.status.idle":"2021-06-09T19:50:17.166689Z","shell.execute_reply.started":"2021-06-09T19:50:17.15933Z","shell.execute_reply":"2021-06-09T19:50:17.165539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now have 675 target encoded features, one for each combination of target value and feature.","metadata":{}},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"markdown","source":"We will now preprocess the data. After target encoding, we have 675 features, which would be a burden to work with. We will use PCA to thin down the number of total features we end up using, and hopefully help avoid overfitting on the way.\n\nWe will also split up the full training set into train and validation subsets.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA","metadata":{"execution":{"iopub.status.busy":"2021-06-09T19:50:17.168249Z","iopub.execute_input":"2021-06-09T19:50:17.168522Z","iopub.status.idle":"2021-06-09T19:50:17.181093Z","shell.execute_reply.started":"2021-06-09T19:50:17.168494Z","shell.execute_reply":"2021-06-09T19:50:17.180196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xtrain, Xvalid, ytrain, yvalid = train_test_split(train_te, train_target, test_size=0.2, random_state=SEED)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T19:50:17.182552Z","iopub.execute_input":"2021-06-09T19:50:17.182843Z","iopub.status.idle":"2021-06-09T19:50:18.288563Z","shell.execute_reply.started":"2021-06-09T19:50:17.182816Z","shell.execute_reply":"2021-06-09T19:50:18.287686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler_te = StandardScaler()\nXtrain_r = scaler_te.fit_transform(Xtrain)\nXvalid_r = scaler_te.transform(Xvalid)\nXtest_r = scaler_te.transform(test_te)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T19:50:18.289686Z","iopub.execute_input":"2021-06-09T19:50:18.290195Z","iopub.status.idle":"2021-06-09T19:50:21.2459Z","shell.execute_reply.started":"2021-06-09T19:50:18.290138Z","shell.execute_reply":"2021-06-09T19:50:21.244976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca_test = PCA()\npca_test.fit(Xtrain_r)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T19:50:21.246961Z","iopub.execute_input":"2021-06-09T19:50:21.247404Z","iopub.status.idle":"2021-06-09T19:50:40.458376Z","shell.execute_reply.started":"2021-06-09T19:50:21.247361Z","shell.execute_reply":"2021-06-09T19:50:40.457353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_cutoff = 100\n\nfig, (ax1, ax2) = plt.subplots(2)\nax1.plot(pca_test.explained_variance_ratio_)\nax1.axvline(feature_cutoff, color='red')\n\ncumulative_explained = np.cumsum(pca_test.explained_variance_ratio_)\nax2.plot(cumulative_explained)\nax2.axvline(feature_cutoff, color='red')\nax2.axhline(cumulative_explained[feature_cutoff], color='red')","metadata":{"execution":{"iopub.status.busy":"2021-06-09T19:50:40.460162Z","iopub.execute_input":"2021-06-09T19:50:40.460675Z","iopub.status.idle":"2021-06-09T19:50:40.787567Z","shell.execute_reply.started":"2021-06-09T19:50:40.460615Z","shell.execute_reply":"2021-06-09T19:50:40.78672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Taking the first 100 principal components seems to put us past the kinks in the cumulative explained PCA variance and hence seems like a reasonable number of features to take. This should also allow us to fit models relatively quickly compared to the full 675 target encoded features.","metadata":{}},{"cell_type":"code","source":"te_pca = PCA(n_components=feature_cutoff)\nXtrain_pca = te_pca.fit_transform(Xtrain_r)\nXvalid_pca = te_pca.transform(Xvalid_r)\nXtest_pca = te_pca.transform(Xtest_r)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T19:50:40.789147Z","iopub.execute_input":"2021-06-09T19:50:40.789551Z","iopub.status.idle":"2021-06-09T19:50:53.712383Z","shell.execute_reply.started":"2021-06-09T19:50:40.789521Z","shell.execute_reply":"2021-06-09T19:50:53.711223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fitting a Random Forest","metadata":{}},{"cell_type":"markdown","source":"I've tried a few of the simpler models provided by Scikit Learn, and the didn't do so well. A random forest seemed to generalize well, however. We will use Optuna to tune our hyperparameters.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","metadata":{"execution":{"iopub.status.busy":"2021-06-09T12:41:33.881084Z","iopub.execute_input":"2021-06-09T12:41:33.881499Z","iopub.status.idle":"2021-06-09T12:41:33.886343Z","shell.execute_reply.started":"2021-06-09T12:41:33.88146Z","shell.execute_reply":"2021-06-09T12:41:33.885116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp = Path(\"/kaggle/temp/\")\nif not tmp.exists():\n    tmp.mkdir()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T12:43:47.939599Z","iopub.execute_input":"2021-06-09T12:43:47.940276Z","iopub.status.idle":"2021-06-09T12:43:47.944796Z","shell.execute_reply.started":"2021-06-09T12:43:47.940137Z","shell.execute_reply":"2021-06-09T12:43:47.943702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objectiveRF(trial):\n    max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2', Xtrain_pca.shape[1]])\n    max_depth = trial.suggest_int('max_depth', 2, 12)\n    \n    # Optuna sometimes suggests a parameter combination it's used before.\n    # We don't want to waste time with those.\n    tdf = study.trials_dataframe().iloc[:-1]\n    seen = np.logical_and(tdf['params_max_features']==max_features, tdf['params_max_depth']==max_depth)\n    if seen.any():\n        print(\"REPEAT SKIPPED: {}, {}\".format(max_features, max_depth))\n        return tdf[seen]['value'].values[-1]*1.001\n    \n    clf = RandomForestClassifier(\n        n_estimators=100, \n        criterion='gini', \n        max_features=max_features, \n        max_depth=max_depth, \n        random_state=SEED, n_jobs=-1)\n\n    clf.fit(Xtrain_pca, ytrain)\n    # save trained models to use later\n    with open(tmp/\"{}_RF.pickle\".format(trial.number), \"wb\") as fout:\n        pickle.dump(clf, fout)\n    return log_loss(yvalid, clf.predict_proba(Xvalid_pca))\n    ","metadata":{"execution":{"iopub.status.busy":"2021-06-09T14:15:53.624453Z","iopub.execute_input":"2021-06-09T14:15:53.624826Z","iopub.status.idle":"2021-06-09T14:15:53.634057Z","shell.execute_reply.started":"2021-06-09T14:15:53.624796Z","shell.execute_reply":"2021-06-09T14:15:53.632996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study = optuna.create_study(\n    direction='minimize',\n    sampler=optuna.samplers.TPESampler(seed=SEED)\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T14:15:56.212848Z","iopub.execute_input":"2021-06-09T14:15:56.213498Z","iopub.status.idle":"2021-06-09T14:15:56.220618Z","shell.execute_reply.started":"2021-06-09T14:15:56.213447Z","shell.execute_reply":"2021-06-09T14:15:56.219549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study.optimize(objectiveRF, n_trials=20)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T14:16:14.54001Z","iopub.execute_input":"2021-06-09T14:16:14.540605Z","iopub.status.idle":"2021-06-09T15:29:01.641088Z","shell.execute_reply.started":"2021-06-09T14:16:14.540556Z","shell.execute_reply":"2021-06-09T15:29:01.639606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are able to get to a log loss of about 1.74, which seems pretty good when compared with the top of the leaderboard. Better models such as gradient boosted trees may end up with a slightly better end result, but this is good enough for now. Doing more feature engineering may have more of an effect, such as possibly including a denoising autoencoder or finding a better way to encode the features.\n\nFor now, we re-load the best forest and generate our submission:","metadata":{}},{"cell_type":"code","source":"with open(tmp/\"{}_RF.pickle\".format(study.best_trial.number), \"rb\") as fin:\n    best_clf = pickle.load(fin)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:04:40.312622Z","iopub.execute_input":"2021-06-09T16:04:40.313011Z","iopub.status.idle":"2021-06-09T16:04:40.367602Z","shell.execute_reply.started":"2021-06-09T16:04:40.31298Z","shell.execute_reply":"2021-06-09T16:04:40.366352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ypred = best_clf.predict_proba(Xtest_pca)\nsubmission_df = pd.DataFrame(ypred, index=test_te.index, columns=[f\"Class_{i+1}\" for i in range(9)])\nsubmission_df.to_csv(\"submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:04:55.364554Z","iopub.execute_input":"2021-06-09T16:04:55.364922Z","iopub.status.idle":"2021-06-09T16:04:58.09027Z","shell.execute_reply.started":"2021-06-09T16:04:55.364894Z","shell.execute_reply":"2021-06-09T16:04:58.089076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}