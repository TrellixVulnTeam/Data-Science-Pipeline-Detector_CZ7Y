{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tabular Playground Series - Jun 2021 [EDA]\nThis notebook is based on the `TPS - Jun 2021` Competition organised by Kaggle. Through out this notebook we deal to understand the dataset and create patterns between the different features with the target variables. We try to understand which features affect the target field. ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Load the dataset\nIn this section, we first import all the required libraries and then load the training data into the notebook. We also check the number of columns, number of missing values and other factors to understand our dataset.","metadata":{}},{"cell_type":"code","source":"!pip install chart-studio --quiet","metadata":{"execution":{"iopub.status.busy":"2021-06-11T10:20:26.500487Z","iopub.execute_input":"2021-06-11T10:20:26.50093Z","iopub.status.idle":"2021-06-11T10:20:33.936096Z","shell.execute_reply.started":"2021-06-11T10:20:26.50083Z","shell.execute_reply":"2021-06-11T10:20:33.934826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport plotly.express as ex\nimport cufflinks as cf\nimport chart_studio.plotly as py\nfrom plotly.offline import iplot, init_notebook_mode, plot, download_plotlyjs\nimport plotly.graph_objects as go\nimport seaborn as sns\nfrom plotly.subplots import make_subplots\nimport matplotlib.pyplot as plt\nimport matplotlib\n\n\ninit_notebook_mode(connected=True)\ncf.go_offline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprimary_color=\"#342A21\"\nsecondary_color=\"#DA667B\"","metadata":{"execution":{"iopub.status.busy":"2021-06-11T10:20:33.939797Z","iopub.execute_input":"2021-06-11T10:20:33.940108Z","iopub.status.idle":"2021-06-11T10:20:37.693942Z","shell.execute_reply.started":"2021-06-11T10:20:33.940078Z","shell.execute_reply":"2021-06-11T10:20:37.692872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds = pd.read_csv(\"../input/tabular-playground-series-jun-2021/train.csv\")\nds.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T10:20:37.695729Z","iopub.execute_input":"2021-06-11T10:20:37.696076Z","iopub.status.idle":"2021-06-11T10:20:38.955793Z","shell.execute_reply.started":"2021-06-11T10:20:37.696043Z","shell.execute_reply":"2021-06-11T10:20:38.955003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds = pd.read_csv(\"../input/tabular-playground-series-jun-2021/test.csv\")\ntest_ds.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T10:20:38.956953Z","iopub.execute_input":"2021-06-11T10:20:38.95731Z","iopub.status.idle":"2021-06-11T10:20:39.501243Z","shell.execute_reply.started":"2021-06-11T10:20:38.957283Z","shell.execute_reply":"2021-06-11T10:20:39.500292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Number of columns: {len(ds.columns)}\")\nprint(f\"Length of the dataset: {len(ds)}\")\nprint(f\"Number of missing columns: {(ds.isna().sum() != 0).sum()}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-11T10:20:39.502309Z","iopub.execute_input":"2021-06-11T10:20:39.502722Z","iopub.status.idle":"2021-06-11T10:20:39.555615Z","shell.execute_reply.started":"2021-06-11T10:20:39.502678Z","shell.execute_reply":"2021-06-11T10:20:39.554699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, we don't have a missing value in our dataset and have 77 columns which includes the target variable also. Also we have a large dataset of 200K rows. Lets check out more deeper into the dataset through different statistics operations.","metadata":{}},{"cell_type":"markdown","source":"# Perform Statistics\nIn this section, we perform different statistics operation which includes mean, min, max, skew, etc. We also try to understand some statistics operation using the data visualization as the dataset is too large for undestand through values.","metadata":{}},{"cell_type":"code","source":"ds.describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T10:20:39.556779Z","iopub.execute_input":"2021-06-11T10:20:39.557059Z","iopub.status.idle":"2021-06-11T10:20:40.186736Z","shell.execute_reply.started":"2021-06-11T10:20:39.557032Z","shell.execute_reply":"2021-06-11T10:20:40.185588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since we see that the min value and max value have vast difference of the feature columns which conclude that there must be probabilty of getting the outliers in the dataset is high. So we need to check out some of the columns more deeply to check wheather outliers present or not in the dataset.","metadata":{"execution":{"iopub.status.busy":"2021-06-02T11:50:26.877235Z","iopub.status.idle":"2021-06-02T11:50:26.878048Z"}}},{"cell_type":"code","source":"fig = ex.box(ds[:10000], y=['feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4', 'feature_5'], title=\"Outliers Graph\")\nfig.update_traces(marker_color=secondary_color)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T10:20:40.188154Z","iopub.execute_input":"2021-06-11T10:20:40.188528Z","iopub.status.idle":"2021-06-11T10:20:41.914538Z","shell.execute_reply.started":"2021-06-11T10:20:40.188488Z","shell.execute_reply":"2021-06-11T10:20:41.91383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we expected, we have outliers present in our dataset which we need to deal while fitting the data into the ml model. Lets check the skewness in our dataset.","metadata":{}},{"cell_type":"code","source":"def skew_frame(dataframe, train=True):\n    skew_value = {}\n    columns = []\n    values = []\n    if train:\n        for col in dataframe.columns[1:-1]:\n            columns.append(col)\n            values.append(ds[col].skew())\n    else:\n        for col in dataframe.columns[1:]:\n            columns.append(col)\n            values.append(ds[col].skew())\n    skew_value['columns'] = columns\n    skew_value['values'] = values\n    skew_ds = pd.DataFrame(skew_value, columns=['columns', 'values'])\n    return skew_ds\n\ntrain_skew = skew_frame(ds)\ntest_skew = skew_frame(test_ds, train=False)\n    \nfig = go.Figure(data=[go.Bar(x=train_skew['columns'], y=train_skew['values'], name=\"train\", marker=dict(color=primary_color)),\n                     go.Bar(x=test_skew['columns'], y=test_skew['values'], name=\"test\", marker=dict(color=secondary_color))]) \nfig.update_layout(barmode='stack', xaxis=dict(title=\"columns\"), \n                  yaxis=dict(title=\"frequency\"), title=\"Skewness Graph\")","metadata":{"execution":{"iopub.status.busy":"2021-06-11T10:20:41.915864Z","iopub.execute_input":"2021-06-11T10:20:41.916303Z","iopub.status.idle":"2021-06-11T10:20:42.122502Z","shell.execute_reply.started":"2021-06-11T10:20:41.916255Z","shell.execute_reply":"2021-06-11T10:20:42.121658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, we have lots of columns that are not normalize and contains skewness in it. We need to clear out the skewness from the dataset before training the ml model.\n\nLets perform the EDA and understand more about the dataset and found patterns in it.","metadata":{}},{"cell_type":"markdown","source":"# Exploratory Data Analysis\nIn this section, we perform the EDA between features and try to find out more insight from the data.","metadata":{}},{"cell_type":"code","source":"fig = ex.bar(x=ds['target'].value_counts().keys(), y=ds['target'].value_counts(), \n             labels={'x': 'targets', 'y': 'frequency'}, title=\"Target Graph\")\nfig.update_traces(marker_color=primary_color)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T10:20:42.124438Z","iopub.execute_input":"2021-06-11T10:20:42.124691Z","iopub.status.idle":"2021-06-11T10:20:42.230946Z","shell.execute_reply.started":"2021-06-11T10:20:42.124665Z","shell.execute_reply":"2021-06-11T10:20:42.229715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**So, we have a unbalanced dataset with `Class_6` and `Class_8` highly densed dataset with more than 50K entities.**","metadata":{}},{"cell_type":"code","source":"ds.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T10:20:42.232774Z","iopub.execute_input":"2021-06-11T10:20:42.23322Z","iopub.status.idle":"2021-06-11T10:20:42.252469Z","shell.execute_reply.started":"2021-06-11T10:20:42.233176Z","shell.execute_reply":"2021-06-11T10:20:42.251387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def unique(dataframe, train=True):\n    unique_values = []\n    if train:\n        for col in dataframe.columns[1:-1]:\n            unique_values.append(len(dataframe[col].unique()))\n    else:\n        for col in dataframe.columns[1:]:\n            unique_values.append(len(dataframe[col].unique()))\n    return unique_values\ntrain_unique = unique(ds)\ntest_unique = unique(test_ds, train=False)\nfig = go.Figure(data=[go.Bar(x=ds.columns[1:-1], y=train_unique, marker=dict(color=primary_color), name=\"train\"),\n                     go.Bar(x=test_ds.columns[1:], y=test_unique, marker=dict(color=secondary_color), name='test')]) \nfig.update_layout(barmode='stack', xaxis=dict(title=\"columns\"), yaxis=dict(title=\"unique frequency\"), title=\"Unique Value Graph\")","metadata":{"execution":{"iopub.status.busy":"2021-06-11T10:20:42.253725Z","iopub.execute_input":"2021-06-11T10:20:42.254016Z","iopub.status.idle":"2021-06-11T10:20:42.428956Z","shell.execute_reply.started":"2021-06-11T10:20:42.253988Z","shell.execute_reply":"2021-06-11T10:20:42.427958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We have lots of columns having more than 50 unique values in both train and the test dataset.**","metadata":{}},{"cell_type":"code","source":"num_row=19\nnum_col=4\nfig, ax = plt.subplots(num_row, num_col, figsize=(20, 20))\nfor col, index in zip(ds.columns[1:-1], np.arange(len(ds.columns[1:-1]))):\n    i, j = (index // num_col, index % num_col)\n    sns.lineplot(ds[col].value_counts().keys(), ds[col].value_counts(), ax=ax[i, j])\n    ax[i, j].set_ylabel(\"\")\n    ax[i, j].set_title(col, fontweight='bold')\nfig.tight_layout()\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T10:20:42.430294Z","iopub.execute_input":"2021-06-11T10:20:42.430671Z","iopub.status.idle":"2021-06-11T10:20:51.011625Z","shell.execute_reply.started":"2021-06-11T10:20:42.430633Z","shell.execute_reply":"2021-06-11T10:20:51.010904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**In most of the columns, we have high percentage of zeros present in the dataset.**","metadata":{"execution":{"iopub.status.busy":"2021-06-03T12:22:53.724389Z","iopub.execute_input":"2021-06-03T12:22:53.724896Z","iopub.status.idle":"2021-06-03T12:22:53.73045Z","shell.execute_reply.started":"2021-06-03T12:22:53.72486Z","shell.execute_reply":"2021-06-03T12:22:53.72947Z"}}},{"cell_type":"code","source":"corr = ds[1:].corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\nplt.figure(figsize=(30, 20))\ncmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [primary_color, secondary_color])\nsns.heatmap(ds[1:].corr(), mask=mask, cmap=cmap, linewidth=0.2)\nplt.title('Features Heat Map', fontweight='bold', fontsize=24);","metadata":{"execution":{"iopub.status.busy":"2021-06-11T10:31:30.93015Z","iopub.execute_input":"2021-06-11T10:31:30.93051Z","iopub.status.idle":"2021-06-11T10:31:39.748987Z","shell.execute_reply.started":"2021-06-11T10:31:30.930478Z","shell.execute_reply":"2021-06-11T10:31:39.747878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have some relation between the columns while some columns have a very low relation with each columns. We need to extract the features while training the model, we need to perform feature selection with a high feature frequency with different approaches and find the best one from it.","metadata":{"execution":{"iopub.status.busy":"2021-06-11T10:20:51.017102Z","iopub.execute_input":"2021-06-11T10:20:51.017482Z","iopub.status.idle":"2021-06-11T10:20:51.051131Z","shell.execute_reply.started":"2021-06-11T10:20:51.017443Z","shell.execute_reply":"2021-06-11T10:20:51.050271Z"}}},{"cell_type":"markdown","source":"**Other EDA can be perform for understanding the dataset more clearly. All the possible suggestion are invited in the comment section. Please upvote it you find this useful.**","metadata":{"execution":{"iopub.status.busy":"2021-06-11T10:20:51.052165Z","iopub.execute_input":"2021-06-11T10:20:51.052592Z","iopub.status.idle":"2021-06-11T10:20:51.063279Z","shell.execute_reply.started":"2021-06-11T10:20:51.052549Z","shell.execute_reply":"2021-06-11T10:20:51.062459Z"}}}]}