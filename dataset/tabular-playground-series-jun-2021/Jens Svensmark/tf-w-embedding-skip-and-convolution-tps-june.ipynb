{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Inspired by Oscar Villarreal Escamilla's notebook https://www.kaggle.com/oxzplvifi/tabular-residual-network I am trying embedding with residual network, here in Python instead of R","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-21T11:22:13.578867Z","iopub.execute_input":"2021-06-21T11:22:13.579401Z","iopub.status.idle":"2021-06-21T11:22:13.588754Z","shell.execute_reply.started":"2021-06-21T11:22:13.579319Z","shell.execute_reply":"2021-06-21T11:22:13.587926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tempfile\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow.keras import layers\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold","metadata":{"execution":{"iopub.status.busy":"2021-06-21T11:22:13.604009Z","iopub.execute_input":"2021-06-21T11:22:13.604548Z","iopub.status.idle":"2021-06-21T11:22:20.328279Z","shell.execute_reply.started":"2021-06-21T11:22:13.604504Z","shell.execute_reply":"2021-06-21T11:22:20.327549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv(\"../input/tabular-playground-series-jun-2021/train.csv\")\ntest_data = pd.read_csv(\"../input/tabular-playground-series-jun-2021/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-06-21T11:22:20.32944Z","iopub.execute_input":"2021-06-21T11:22:20.329817Z","iopub.status.idle":"2021-06-21T11:22:22.274215Z","shell.execute_reply.started":"2021-06-21T11:22:20.329789Z","shell.execute_reply":"2021-06-21T11:22:22.27323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = list(train_data.columns)\ncols.remove(\"id\")\ncols.remove(\"target\")\nclasses = [f\"Class_{i}\" for i in range(1, 10)]\nn_classes = len(classes)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T11:22:22.275885Z","iopub.execute_input":"2021-06-21T11:22:22.27622Z","iopub.status.idle":"2021-06-21T11:22:22.281423Z","shell.execute_reply.started":"2021-06-21T11:22:22.27618Z","shell.execute_reply":"2021-06-21T11:22:22.280452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def custom_loss(y_true, y_pred):\n    y_pred = tf.clip_by_value(y_pred, 1e-15, 1-1e-15)\n    return tf.keras.losses.CategoricalCrossentropy()(y_true, y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T11:22:22.282749Z","iopub.execute_input":"2021-06-21T11:22:22.283065Z","iopub.status.idle":"2021-06-21T11:22:22.294519Z","shell.execute_reply.started":"2021-06-21T11:22:22.283033Z","shell.execute_reply":"2021-06-21T11:22:22.293468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data = pd.concat((train_data, test_data))[cols]\ncategories = {col: sorted(all_data[col].unique()) for col in all_data}","metadata":{"execution":{"iopub.status.busy":"2021-06-21T11:22:22.296638Z","iopub.execute_input":"2021-06-21T11:22:22.29695Z","iopub.status.idle":"2021-06-21T11:22:22.609826Z","shell.execute_reply.started":"2021-06-21T11:22:22.296921Z","shell.execute_reply":"2021-06-21T11:22:22.608915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frequencies = train_data.target.value_counts().sort_index()/len(train_data)\nbiases_calc = np.log(frequencies).values\nprint(frequencies)\n# print(biases_calc)\ndef softmax(x):\n    return np.exp(x) / np.sum(np.exp(x))\nprint(np.exp(biases_calc) / np.sum(np.exp(biases_calc)))\n\ndef bias_init(bias_shape,dtype):\n    return tf.Variable(biases_calc, dtype=dtype)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T11:22:22.611051Z","iopub.execute_input":"2021-06-21T11:22:22.611546Z","iopub.status.idle":"2021-06-21T11:22:22.686737Z","shell.execute_reply.started":"2021-06-21T11:22:22.611497Z","shell.execute_reply":"2021-06-21T11:22:22.686014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train_data[cols]\ny = train_data[\"target\"]\ny_ohe = pd.get_dummies(y)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T11:22:22.687815Z","iopub.execute_input":"2021-06-21T11:22:22.688273Z","iopub.status.idle":"2021-06-21T11:22:22.750948Z","shell.execute_reply.started":"2021-06-21T11:22:22.688217Z","shell.execute_reply":"2021-06-21T11:22:22.750099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def assemble_model(hidden_layers = (32, ), third_skip=False, conv_layer=False, embed_opts={\"output_dim\": 2}, conv_opts={\"filters\": 12, \"kernel_size\": 1}):\n    inputs = layers.Input(len(X.columns), dtype=\"int32\")\n    feat_layer = layers.Embedding(X.max().max()+1, **embed_opts)(inputs)\n    if conv_layer:\n        feat_layer = layers.Conv1D(activation=\"relu\", **conv_opts)(feat_layer)\n        feat_layer = layers.Dropout(0.3)(feat_layer)\n    feat_layer = layers.Flatten()(feat_layer)\n    cur_layer = feat_layer\n    for layer_size in hidden_layers:\n        cur_layer = layers.Dropout(.2)(cur_layer)\n        lay = tfa.layers.WeightNormalization(layers.Dense(layer_size, activation=\"selu\",\n                                 kernel_initializer='lecun_normal',\n                                ))\n        cur_layer = lay(cur_layer)\n\n    first_hidden_layer = cur_layer\n\n    cur_layer = layers.Concatenate()([feat_layer, cur_layer])\n    cur_layer = layers.Dropout(.3)(cur_layer)\n    cur_layer = tfa.layers.WeightNormalization(layers.Dense(hidden_layers[-1], activation=\"relu\"))(cur_layer)\n    second_skip_layer = cur_layer\n    \n    cur_layer = layers.Concatenate()([feat_layer, cur_layer, first_hidden_layer])\n    cur_layer = layers.Dropout(.4)(cur_layer)\n    cur_layer = tfa.layers.WeightNormalization(layers.Dense(hidden_layers[-1], activation=\"elu\", kernel_initializer='lecun_normal',))(cur_layer)\n\n    if third_skip:\n        cur_layer = layers.Concatenate()([feat_layer, cur_layer, second_skip_layer, first_hidden_layer])\n        cur_layer = layers.Dropout(.3)(cur_layer)\n        cur_layer = tfa.layers.WeightNormalization(layers.Dense(hidden_layers[-1], activation=\"elu\"))(cur_layer)\n\n    \n    out = layers.Dense(n_classes, activation=\"softmax\", bias_initializer=bias_init\n                      )(cur_layer)\n\n    model = tf.keras.Model(inputs,\n                           out)\n\n    model.compile(\n            optimizer=\"adam\",\n            loss=tf.keras.losses.CategoricalCrossentropy(),\n            metrics=[tf.keras.metrics.CategoricalCrossentropy(), custom_loss],\n        )\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-06-21T11:22:22.75271Z","iopub.execute_input":"2021-06-21T11:22:22.753181Z","iopub.status.idle":"2021-06-21T11:22:22.766405Z","shell.execute_reply.started":"2021-06-21T11:22:22.753133Z","shell.execute_reply":"2021-06-21T11:22:22.765539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pack_test_data(proba):\n    predicted = pd.DataFrame(proba, columns=classes)\n    predicted[\"id\"] = test_data.id\n    predicted = predicted[[\"id\"]+classes]\n    return predicted","metadata":{"execution":{"iopub.status.busy":"2021-06-21T11:22:22.767749Z","iopub.execute_input":"2021-06-21T11:22:22.768196Z","iopub.status.idle":"2021-06-21T11:22:22.785255Z","shell.execute_reply.started":"2021-06-21T11:22:22.768154Z","shell.execute_reply":"2021-06-21T11:22:22.78419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"earlystop_callback = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', min_delta=0.005, patience=10, verbose=0,\n    mode='auto', baseline=None, restore_best_weights=True\n)\nreduce_lr_on_plateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.7, patience=2, verbose=0\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T11:22:22.78658Z","iopub.execute_input":"2021-06-21T11:22:22.787072Z","iopub.status.idle":"2021-06-21T11:22:22.796834Z","shell.execute_reply.started":"2021-06-21T11:22:22.787025Z","shell.execute_reply":"2021-06-21T11:22:22.795564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_predict_cv(get_model, nfolds=10):\n    kf = StratifiedKFold(n_splits=nfolds, shuffle=True)\n\n    probas = []\n    losses = []\n    for train_idx, test_idx in kf.split(X, y):\n\n        X_train, y_train = X.loc[train_idx], y_ohe.loc[train_idx]\n        X_test, y_test = X.loc[test_idx], y_ohe.loc[test_idx]\n        model = get_model()\n        model.fit(X_train, y_train, epochs=100, \n                  batch_size=256,\n                  validation_data=(X_test, y_test),\n                callbacks=[earlystop_callback, reduce_lr_on_plateau], verbose=0)\n        eval_dict = model.evaluate(X_test, y_test, return_dict=True, verbose=0)\n        print(eval_dict)\n        losses.append(eval_dict[\"custom_loss\"])\n        proba = model.predict(test_data[cols])\n        probas.append(proba)\n    print(f\"Mean loss: {np.mean(losses)}, std: {np.std(losses)}\")\n    return sum(probas)/nfolds","metadata":{"execution":{"iopub.status.busy":"2021-06-21T11:22:44.797122Z","iopub.execute_input":"2021-06-21T11:22:44.797515Z","iopub.status.idle":"2021-06-21T11:22:44.80632Z","shell.execute_reply.started":"2021-06-21T11:22:44.797468Z","shell.execute_reply":"2021-06-21T11:22:44.805192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nproba = train_predict_cv(lambda: assemble_model(hidden_layers=(32,), conv_layer=True, \n                                                     embed_opts={\"output_dim\": 14, \"embeddings_regularizer\": 'l2'},\n                                                     conv_opts={\"filters\": 12, \"kernel_size\": 1}), nfolds=20)\npack_test_data(proba).to_csv(\"tf_model_conv.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T11:22:53.893907Z","iopub.execute_input":"2021-06-21T11:22:53.894257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I tried clipping the output probabilities to avoid extreme values affecting the score, but it did not seem to help much though. Here I try a few more cutoffs for the clipping","metadata":{}},{"cell_type":"code","source":"def clip_and_save(proba, clip_percent=5, renormalize=False, fname=\"tf_model\"):\n    proba_clipped = np.clip(proba, clip_percent/100, 1-clip_percent/100)\n    if renormalize:\n        sums = proba.sum(axis=1)\n        proba = proba/sums[:, np.newaxis]\n        sums = proba.sum(axis=1)\n        fname += \"_normed\"\n        \n    predicted = pack_test_data(proba_clipped)\n    predicted.to_csv(fname+f\"_clipped_{clip_percent}.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clip_and_save(proba, clip_percent=1.25, renormalize=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:59:33.994484Z","iopub.execute_input":"2021-06-20T12:59:33.994854Z","iopub.status.idle":"2021-06-20T12:59:35.458138Z","shell.execute_reply.started":"2021-06-20T12:59:33.994821Z","shell.execute_reply":"2021-06-20T12:59:35.457094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clip_and_save(proba, clip_percent=0.0, renormalize=False)","metadata":{},"execution_count":null,"outputs":[]}]}