{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"As a start I just copied over my notebook from the yabular playground may competition https://www.kaggle.com/jenssvensmark/scikit-stacking-tabular-play-may, adopted to the number of classes in this new dataset, and ran it as is.","metadata":{}},{"cell_type":"markdown","source":"# Load and inspect data","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-02T12:52:07.979932Z","iopub.execute_input":"2021-06-02T12:52:07.980331Z","iopub.status.idle":"2021-06-02T12:52:07.989654Z","shell.execute_reply.started":"2021-06-02T12:52:07.980251Z","shell.execute_reply":"2021-06-02T12:52:07.988677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/tabular-playground-series-jun-2021/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/tabular-playground-series-jun-2021/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-06-02T12:52:08.043651Z","iopub.execute_input":"2021-06-02T12:52:08.044153Z","iopub.status.idle":"2021-06-02T12:52:10.05296Z","shell.execute_reply.started":"2021-06-02T12:52:08.044121Z","shell.execute_reply":"2021-06-02T12:52:10.052089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T12:52:10.054143Z","iopub.execute_input":"2021-06-02T12:52:10.054406Z","iopub.status.idle":"2021-06-02T12:52:10.089245Z","shell.execute_reply.started":"2021-06-02T12:52:10.054381Z","shell.execute_reply":"2021-06-02T12:52:10.088405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T12:52:10.09076Z","iopub.execute_input":"2021-06-02T12:52:10.091027Z","iopub.status.idle":"2021-06-02T12:52:10.162239Z","shell.execute_reply.started":"2021-06-02T12:52:10.091001Z","shell.execute_reply":"2021-06-02T12:52:10.161219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No missing data, all the features are integers, except the target.\nThere are a lot of features here! And looks like a lot of zeros as well.\nLet's check out the range of values in each feature","metadata":{}},{"cell_type":"code","source":"train_data.describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T12:52:10.16389Z","iopub.execute_input":"2021-06-02T12:52:10.164187Z","iopub.status.idle":"2021-06-02T12:52:10.72809Z","shell.execute_reply.started":"2021-06-02T12:52:10.164158Z","shell.execute_reply":"2021-06-02T12:52:10.72716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = list(train_data.columns)\ncols.remove(\"id\")\ncols.remove(\"target\")\nclasses = [f\"Class_{i}\" for i in range(1, 10)]","metadata":{"execution":{"iopub.status.busy":"2021-06-02T12:52:10.729437Z","iopub.execute_input":"2021-06-02T12:52:10.729729Z","iopub.status.idle":"2021-06-02T12:52:10.734573Z","shell.execute_reply.started":"2021-06-02T12:52:10.729703Z","shell.execute_reply":"2021-06-02T12:52:10.733717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-06-02T12:52:10.735871Z","iopub.execute_input":"2021-06-02T12:52:10.736177Z","iopub.status.idle":"2021-06-02T12:52:11.475656Z","shell.execute_reply.started":"2021-06-02T12:52:10.736149Z","shell.execute_reply":"2021-06-02T12:52:11.474552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cufflinks as cf # for using plotly with pandas\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T12:52:11.476914Z","iopub.execute_input":"2021-06-02T12:52:11.477206Z","iopub.status.idle":"2021-06-02T12:52:14.122177Z","shell.execute_reply.started":"2021-06-02T12:52:11.477177Z","shell.execute_reply":"2021-06-02T12:52:14.121148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.describe().loc[\"max\", cols].iplot(kind=\"bar\")","metadata":{"execution":{"iopub.status.busy":"2021-06-02T12:52:14.124613Z","iopub.execute_input":"2021-06-02T12:52:14.125025Z","iopub.status.idle":"2021-06-02T12:52:15.474286Z","shell.execute_reply.started":"2021-06-02T12:52:14.124986Z","shell.execute_reply":"2021-06-02T12:52:15.47331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(25, 6))\nsns.boxplot(data=train_data.drop(\"id\", axis=1), ax=ax)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T12:52:15.475842Z","iopub.execute_input":"2021-06-02T12:52:15.476103Z","iopub.status.idle":"2021-06-02T12:52:21.873128Z","shell.execute_reply.started":"2021-06-02T12:52:15.476077Z","shell.execute_reply":"2021-06-02T12:52:21.872131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(0, 50, 20):\n    col = f\"feature_{i}\"\n    data = train_data[[col, \"target\"]]\n    #data = data[data[col] != 0]\n    sns.countplot(x=col, data=data, #hue='target', #multiple='stack'\n               )\n    plt.yscale(\"log\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T12:52:21.874895Z","iopub.execute_input":"2021-06-02T12:52:21.875297Z","iopub.status.idle":"2021-06-02T12:52:24.128443Z","shell.execute_reply.started":"2021-06-02T12:52:21.875254Z","shell.execute_reply":"2021-06-02T12:52:24.127464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Above I just plotted a few selected features. The count appears to change continously with each feature.\n\nAll the features are integers, but there are a lot of features, and a lot of values of each features. I have no idea if these features are categorical, ordinal or numerical.","metadata":{}},{"cell_type":"code","source":"train_data.groupby(\"target\")[\"target\"].count()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T12:52:24.129775Z","iopub.execute_input":"2021-06-02T12:52:24.13006Z","iopub.status.idle":"2021-06-02T12:52:24.171446Z","shell.execute_reply.started":"2021-06-02T12:52:24.130031Z","shell.execute_reply":"2021-06-02T12:52:24.170586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset is quite imbalanced, there are a lot of `Class_2`, and fewer of the other three classes.","metadata":{}},{"cell_type":"code","source":"sns.heatmap(train_data.drop([\"id\", ], axis=1).corr())","metadata":{"execution":{"iopub.status.busy":"2021-06-02T12:52:24.172702Z","iopub.execute_input":"2021-06-02T12:52:24.17298Z","iopub.status.idle":"2021-06-02T12:52:27.875264Z","shell.execute_reply.started":"2021-06-02T12:52:24.172951Z","shell.execute_reply":"2021-06-02T12:52:27.874466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Doesn't look like the features are correlated (although I don't know if all the zeros in the data affects this).\n\nI'm not really sure whether we can eliminate some of the features, or do feature engineering, so I will just use all the features as is in the following","metadata":{}},{"cell_type":"markdown","source":"# scikit models","metadata":{}},{"cell_type":"markdown","source":"Imports and a couple of utility functions","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, MaxAbsScaler, OneHotEncoder\nfrom sklearn.metrics import log_loss\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.dummy import DummyClassifier","metadata":{"execution":{"iopub.status.busy":"2021-06-02T12:52:27.876232Z","iopub.execute_input":"2021-06-02T12:52:27.876525Z","iopub.status.idle":"2021-06-02T12:52:28.25865Z","shell.execute_reply.started":"2021-06-02T12:52:27.876499Z","shell.execute_reply":"2021-06-02T12:52:28.257742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_model(model):\n    model.fit(X_train, y_train)\n    stats = just_test_model(model)\n    return stats\n\ndef just_test_model(model):\n    train_score = model.score(X_train, y_train)\n    test_score = model.score(X_test, y_test)\n    loss = log_loss(y_test, model.predict_proba(X_test))\n    stats = {\"train\": train_score, \"test\": test_score, \"loss\": loss}\n    return stats","metadata":{"execution":{"iopub.status.busy":"2021-06-02T12:52:28.259932Z","iopub.execute_input":"2021-06-02T12:52:28.260201Z","iopub.status.idle":"2021-06-02T12:52:28.265863Z","shell.execute_reply.started":"2021-06-02T12:52:28.260174Z","shell.execute_reply":"2021-06-02T12:52:28.264757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_model_partial(model, X, y, select=slice(None)):\n    X = X[select]\n    y = y[select]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n    model.fit(X_train, y_train)\n    train_score = model.score(X_train, y_train)\n    test_score = model.score(X_test, y_test)\n    loss = log_loss(y_test, model.predict_proba(X_test))\n    stats = {\"train\": train_score, \"test\": test_score, \"loss\": loss}\n    return stats","metadata":{"execution":{"iopub.status.busy":"2021-06-02T12:52:28.26717Z","iopub.execute_input":"2021-06-02T12:52:28.267528Z","iopub.status.idle":"2021-06-02T12:52:28.281596Z","shell.execute_reply.started":"2021-06-02T12:52:28.267429Z","shell.execute_reply":"2021-06-02T12:52:28.280653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_proba_df(model, X, labels=classes):\n    proba = model.predict_proba(X)\n    proba = pd.DataFrame(proba, columns=labels)\n    return proba\n\ndef plot_class_distribution(model, X_test, y_test, labels=classes):\n    proba = predict_proba_df(model, X_test, labels=labels)\n    proba[\"target\"] = y_test.values\n    plot_class_proba_distribution(proba)\n\ndef plot_class_proba_distribution(proba_df):\n    fig, axes = plt.subplots(1, 4, figsize=(20, 2))\n    for (real_val, group), ax in zip(proba_df.groupby(\"target\"), axes):\n        group.pop(\"target\")\n        sns.kdeplot(data=group, fill=True, ax=ax)\n        ax.set_title(real_val)\n        ax.set_ylim(top=10)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T12:52:28.282951Z","iopub.execute_input":"2021-06-02T12:52:28.283264Z","iopub.status.idle":"2021-06-02T12:52:28.292469Z","shell.execute_reply.started":"2021-06-02T12:52:28.283237Z","shell.execute_reply":"2021-06-02T12:52:28.291608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = train_data.target\nX = train_data.drop([\"target\", \"id\"], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T12:52:28.293718Z","iopub.execute_input":"2021-06-02T12:52:28.294043Z","iopub.status.idle":"2021-06-02T12:52:28.483128Z","shell.execute_reply.started":"2021-06-02T12:52:28.293965Z","shell.execute_reply":"2021-06-02T12:52:28.482228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As a baseline I start with a dummy classifier that just predicts probabilities based on how often each class occurs in the training data","metadata":{}},{"cell_type":"code","source":"dummy = DummyClassifier(strategy=\"prior\")\nprint(\"prior dummy\", test_model(dummy))","metadata":{"execution":{"iopub.status.busy":"2021-06-02T12:52:28.484434Z","iopub.execute_input":"2021-06-02T12:52:28.484754Z","iopub.status.idle":"2021-06-02T12:52:29.327095Z","shell.execute_reply.started":"2021-06-02T12:52:28.484727Z","shell.execute_reply":"2021-06-02T12:52:29.325933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below I build a list of categories to be used by the one hot encoder. I'm including the test data in case there is a label in there not present in the train data","metadata":{}},{"cell_type":"code","source":"all_data = pd.concat((train_data, test_data))[cols]\ncategories = [sorted(all_data[col].unique()) for col in all_data]","metadata":{"execution":{"iopub.status.busy":"2021-06-02T12:52:29.328464Z","iopub.execute_input":"2021-06-02T12:52:29.328788Z","iopub.status.idle":"2021-06-02T12:52:29.609865Z","shell.execute_reply.started":"2021-06-02T12:52:29.328757Z","shell.execute_reply":"2021-06-02T12:52:29.608893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below I try out logistic regression considering the features as numerical using `StandardScaler` and as categorical using `OneHotEncoder`.","metadata":{}},{"cell_type":"code","source":"models = {\"LogReg\": Pipeline([(\"scaler\", StandardScaler()), \n                             (\"model\", LogisticRegression())]),\n                                     }\nfor C in [0.001, 0.01, 0.1, 1.0, 10]:\n    models[f\"LogReg_ohe_C{C}\"] =  Pipeline([(\"ohe\", OneHotEncoder(#drop=\"first\", \n                                                       categories=categories, dtype=\"int\")), \n                             (\"model\", LogisticRegression(max_iter=150, C=C))])\nfor name, model in models.items():\n    print(name, test_model(model))","metadata":{"execution":{"iopub.status.busy":"2021-06-02T12:52:29.611076Z","iopub.execute_input":"2021-06-02T12:52:29.611357Z","iopub.status.idle":"2021-06-02T12:58:29.76823Z","shell.execute_reply.started":"2021-06-02T12:52:29.611329Z","shell.execute_reply":"2021-06-02T12:58:29.767022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the loss it appears that the one hot encoded model did better than considering the features as numerical, although neither seems to improve much compared to the priors dummy. The logistic regressions failed to converge, but I'm choosing to ignore this since they got pretty good scores.\n\nLet's look at whether the imbalance of the dataset might be an issue","metadata":{}},{"cell_type":"code","source":"def loss_per_class(model):\n    test_data = X_test.copy()\n    test_data[\"target\"] = y_test\n    for idx, group in test_data.groupby(\"target\"):\n        loss = log_loss(group.target, model.predict_proba(group.drop(columns=[\"target\"])), labels=classes)\n        print(idx, \" loss\", loss)\n\nloss_per_class(models[\"LogReg_ohe_C0.01\"])\nplot_class_distribution(models[\"LogReg_ohe_C0.01\"], X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T12:58:29.769381Z","iopub.execute_input":"2021-06-02T12:58:29.769765Z","iopub.status.idle":"2021-06-02T12:58:32.366363Z","shell.execute_reply.started":"2021-06-02T12:58:29.769736Z","shell.execute_reply":"2021-06-02T12:58:32.36567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So the model is heavily imbalanced towards predicting `Class_6`. Let's see what happens if we apply class weights to even this out","metadata":{}},{"cell_type":"code","source":"%%time\nmodel =  Pipeline([(\"ohe\", OneHotEncoder(categories=categories, dtype=\"int\", sparse=False)),\n                   (\"model\", LogisticRegression(max_iter=200, C=0.01, \n                                                class_weight=\"balanced\",\n                                               ))])\nmodels[\"ohe_log_weighted\"] = model\nmodel = model.fit(X_train, y_train)\njust_test_model(model)\nloss_per_class(model)\nplot_class_distribution(model, X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T12:58:32.367305Z","iopub.execute_input":"2021-06-02T12:58:32.367549Z","iopub.status.idle":"2021-06-02T13:09:00.511151Z","shell.execute_reply.started":"2021-06-02T12:58:32.367524Z","shell.execute_reply":"2021-06-02T13:09:00.510111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, now losses are the same across the classes, but the total loss is somewhat larger than the previous models.","metadata":{}},{"cell_type":"markdown","source":"Below I test if assuming that features with less than 10 elements are categorical and that the remainder are numerical would work well","metadata":{}},{"cell_type":"code","source":"bool_array = X.nunique() < 10\ncat_cols = list(X.columns[bool_array])\ncat_cols_idx = np.flatnonzero(bool_array)\n#len(cat_cols_idx)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T13:09:00.514145Z","iopub.execute_input":"2021-06-02T13:09:00.514418Z","iopub.status.idle":"2021-06-02T13:09:00.676633Z","shell.execute_reply.started":"2021-06-02T13:09:00.51439Z","shell.execute_reply":"2021-06-02T13:09:00.675742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selected_cats = list(np.array(categories, dtype='object')[cat_cols_idx])\nohe = OneHotEncoder(categories=selected_cats, dtype=\"int\", drop=\"first\", sparse=False)\nct_ohe = ColumnTransformer([(\"ohe\", ohe, cat_cols_idx)],\n                            remainder=\"passthrough\"\n                                              )\nmodel = Pipeline([(\"ct_ohe\", ct_ohe), \n                  (\"scaler\", StandardScaler()),\n                    (\"model\", LogisticRegression(max_iter=100, C=C))])\ntest_model(model)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T13:09:00.678128Z","iopub.execute_input":"2021-06-02T13:09:00.6784Z","iopub.status.idle":"2021-06-02T13:09:12.336161Z","shell.execute_reply.started":"2021-06-02T13:09:00.678373Z","shell.execute_reply":"2021-06-02T13:09:12.335208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Not particularly...\n\nOkay, let's just try a bunch of classifiers","metadata":{}},{"cell_type":"code","source":"%%time\nmodel = GradientBoostingClassifier()\nmodels[\"gradboost\"] = model\ntest_model(model)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T13:09:12.337489Z","iopub.execute_input":"2021-06-02T13:09:12.337874Z","iopub.status.idle":"2021-06-02T13:20:00.532017Z","shell.execute_reply.started":"2021-06-02T13:09:12.33784Z","shell.execute_reply":"2021-06-02T13:20:00.531015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nforest = RandomForestClassifier(max_depth=10)\nmodels[\"forest\"] = forest\nprint(\"forest\", test_model(forest))","metadata":{"execution":{"iopub.status.busy":"2021-06-02T13:20:00.533159Z","iopub.execute_input":"2021-06-02T13:20:00.533432Z","iopub.status.idle":"2021-06-02T13:20:28.293524Z","shell.execute_reply.started":"2021-06-02T13:20:00.533404Z","shell.execute_reply":"2021-06-02T13:20:28.292613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"None of these classifiers did particularly well. Let's just stack them all together...","metadata":{}},{"cell_type":"code","source":"%%time\nvoter_list = [\"gradboost\", \"forest\", \"LogReg\", \"LogReg_ohe_C0.01\", \"ohe_log_weighted\"]\nvoters = [(voter, models[voter]) for voter in voter_list]\nsc = StackingClassifier(voters)\ntest_model(sc)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T13:36:51.271836Z","iopub.execute_input":"2021-06-02T13:36:51.272216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"... and submit that","metadata":{}},{"cell_type":"code","source":"def predict_test_data(model):\n    model.fit(X, y)\n    proba = model.predict_proba(test_data.drop([\"id\"], axis=1))\n    predicted = pd.DataFrame(proba, columns=classes)\n    predicted[\"id\"] = test_data.id\n    predicted = predicted[[\"id\"]+classes]\n    return predicted","metadata":{"execution":{"iopub.status.busy":"2021-06-02T13:33:29.454053Z","iopub.execute_input":"2021-06-02T13:33:29.454423Z","iopub.status.idle":"2021-06-02T13:33:29.46337Z","shell.execute_reply.started":"2021-06-02T13:33:29.454382Z","shell.execute_reply":"2021-06-02T13:33:29.462496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted = predict_test_data(sc)\npredicted.to_csv(\"stacking_model.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T13:33:29.464503Z","iopub.execute_input":"2021-06-02T13:33:29.464968Z","iopub.status.idle":"2021-06-02T13:36:45.659429Z","shell.execute_reply.started":"2021-06-02T13:33:29.464936Z","shell.execute_reply":"2021-06-02T13:36:45.656813Z"},"trusted":true},"execution_count":null,"outputs":[]}]}