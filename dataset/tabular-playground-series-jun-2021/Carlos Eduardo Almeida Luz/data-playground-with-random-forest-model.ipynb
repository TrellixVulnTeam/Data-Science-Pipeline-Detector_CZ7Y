{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In order to practice machine learning, I started to work with this playground tabular data. This notebook is one of my first work in a long time without create ML models and I tried to use some strategies I read.\n\nI did the following tests here:\n1. Trained a random forest regressor with default hiperparameters to set a baseline model and score.\n2. Used min max scaler to try to reduce the dimensions on data. Here I made a mistake, i think, because min max is influenced by outliers, which I was trying to \"fix\".\n3. Tried StandardScaler with \"with_std=False\".\n4. Tried a model without scaling the data. This was the best model so far => RandomForestRegressor(n_estimators=150, max_depth=70, min_samples_split=2, min_samples_leaf=4, max_features='sqrt', bootstrap=True,random_state=0)\n\nAlso, I used an encoder with target variables and used sklearn pipelines to train and predict with the model.\n\nThere is a lot to improve here because the best placement I got was around 900 (by Jun 30). I used a random grid CV to get the best parameters for the model.\n\nI will try a random forest classifier as well to see what changes it will return.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-29T21:52:51.656463Z","iopub.execute_input":"2021-06-29T21:52:51.656798Z","iopub.status.idle":"2021-06-29T21:52:51.665622Z","shell.execute_reply.started":"2021-06-29T21:52:51.656731Z","shell.execute_reply":"2021-06-29T21:52:51.664599Z"}}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2021-07-01T00:05:18.314937Z","iopub.execute_input":"2021-07-01T00:05:18.315395Z","iopub.status.idle":"2021-07-01T00:05:18.322061Z","shell.execute_reply.started":"2021-07-01T00:05:18.315363Z","shell.execute_reply":"2021-07-01T00:05:18.320641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Loading and taking a look at train data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/tabular-playground-series-jun-2021/train.csv')\ntest = pd.read_csv('/kaggle/input/tabular-playground-series-jun-2021/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-01T00:15:43.672757Z","iopub.execute_input":"2021-07-01T00:15:43.673134Z","iopub.status.idle":"2021-07-01T00:15:44.858946Z","shell.execute_reply.started":"2021-07-01T00:15:43.673101Z","shell.execute_reply":"2021-07-01T00:15:44.857971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T00:15:45.359686Z","iopub.execute_input":"2021-07-01T00:15:45.360041Z","iopub.status.idle":"2021-07-01T00:15:45.384009Z","shell.execute_reply.started":"2021-07-01T00:15:45.360002Z","shell.execute_reply":"2021-07-01T00:15:45.382488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if there is any NaN in both datasets\nprint(\"Number os missing data trainset:\", train.isna().any().sum())\nprint(\"Number os missing data testset:\", test.isna().any().sum())","metadata":{"execution":{"iopub.status.busy":"2021-07-01T00:15:47.302445Z","iopub.execute_input":"2021-07-01T00:15:47.302917Z","iopub.status.idle":"2021-07-01T00:15:47.342342Z","shell.execute_reply.started":"2021-07-01T00:15:47.302887Z","shell.execute_reply":"2021-07-01T00:15:47.340775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Main statistics from train dataset\ntrain.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T00:16:29.512239Z","iopub.execute_input":"2021-07-01T00:16:29.512721Z","iopub.status.idle":"2021-07-01T00:16:30.428948Z","shell.execute_reply.started":"2021-07-01T00:16:29.512673Z","shell.execute_reply":"2021-07-01T00:16:30.427755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting data into X and y values and one hot encoding y value in order to train a model and get predictions in output file desireble format\ny = train.target\nX = train.drop(['id','target'], axis=1)\n\nencoder = OneHotEncoder(categories = 'auto')\ny_enc = encoder.fit_transform(y.values.reshape(X.shape[0],1)).toarray()\n\n# Split validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y_enc, train_size=0.8, test_size=0.2, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T00:19:47.975379Z","iopub.execute_input":"2021-07-01T00:19:47.975747Z","iopub.status.idle":"2021-07-01T00:19:48.262681Z","shell.execute_reply.started":"2021-07-01T00:19:47.975714Z","shell.execute_reply":"2021-07-01T00:19:48.261564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Those lines below I used once to find best parameters to train a model using sklean's randomized search\n\n#from sklearn.model_selection import RandomizedSearchCV\n# # Number of trees in random forest\n# n_estimators = [int(x) for x in np.linspace(start = 50, stop = 200, num = 50)]\n# # Number of features to consider at every split\n# max_features = ['auto', 'sqrt']\n# # Maximum number of levels in tree\n# max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n# max_depth.append(None)\n# # Minimum number of samples required to split a node\n# min_samples_split = [2, 5, 10]\n# # Minimum number of samples required at each leaf node\n# min_samples_leaf = [1, 2, 4]\n# # Method of selecting samples for training each tree\n# bootstrap = [True, False]\n# # Create the random grid\n# random_grid = {'n_estimators': n_estimators,\n#                'max_features': max_features,\n#                'max_depth': max_depth,\n#                'min_samples_split': min_samples_split,\n#                'min_samples_leaf': min_samples_leaf,\n#                'bootstrap': bootstrap}\n","metadata":{"execution":{"iopub.status.busy":"2021-06-30T00:39:50.108385Z","iopub.execute_input":"2021-06-30T00:39:50.108763Z","iopub.status.idle":"2021-06-30T00:39:50.115814Z","shell.execute_reply.started":"2021-06-30T00:39:50.10873Z","shell.execute_reply":"2021-06-30T00:39:50.114776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Use the random grid to search for best hyperparameters\n# # First create the base model to tune\n# rf = RandomForestRegressor()\n# # Random search of parameters, using 3 fold cross validation, \n# # search across 100 different combinations, and use all available cores\n# rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# # Fit the random search model\n# rf_random.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T00:39:50.615649Z","iopub.execute_input":"2021-06-30T00:39:50.615991Z","iopub.status.idle":"2021-06-30T00:42:15.821157Z","shell.execute_reply.started":"2021-06-30T00:39:50.615959Z","shell.execute_reply":"2021-06-30T00:42:15.820346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# rf_random.best_params_","metadata":{"execution":{"iopub.status.busy":"2021-06-30T00:42:54.973411Z","iopub.execute_input":"2021-06-30T00:42:54.973831Z","iopub.status.idle":"2021-06-30T00:42:54.983073Z","shell.execute_reply.started":"2021-06-30T00:42:54.973798Z","shell.execute_reply":"2021-06-30T00:42:54.981951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I then created a model with best parameters and used a Pipeline to train the model.\nmodel = RandomForestRegressor(n_estimators=150, max_depth=70, min_samples_split=2, min_samples_leaf=4, max_features='sqrt', bootstrap=True,random_state=0)\n\nmy_pipeline = Pipeline(steps=[  ('scale', StandardScaler()),\n                                ('model', model)])\n\n# Preprocessing of training data, fit model \nmy_pipeline.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds = my_pipeline.predict(X_valid)\n\n# Evaluate the model\nscore = mean_absolute_error(y_valid, preds)\nprint('MAE:', score)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-01T00:22:31.99312Z","iopub.execute_input":"2021-07-01T00:22:31.993643Z","iopub.status.idle":"2021-07-01T00:23:51.572388Z","shell.execute_reply.started":"2021-07-01T00:22:31.993582Z","shell.execute_reply":"2021-07-01T00:23:51.570109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing item ID from test data.\ntest_ = test.iloc[:,1:]","metadata":{"execution":{"iopub.status.busy":"2021-07-01T00:26:04.850529Z","iopub.execute_input":"2021-07-01T00:26:04.850867Z","iopub.status.idle":"2021-07-01T00:26:04.857153Z","shell.execute_reply.started":"2021-07-01T00:26:04.850837Z","shell.execute_reply":"2021-07-01T00:26:04.855547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get predictions\noutput = my_pipeline.predict(test_) # Your code here","metadata":{"execution":{"iopub.status.busy":"2021-07-01T00:26:08.496413Z","iopub.execute_input":"2021-07-01T00:26:08.496781Z","iopub.status.idle":"2021-07-01T00:26:10.681388Z","shell.execute_reply.started":"2021-07-01T00:26:08.496749Z","shell.execute_reply":"2021-07-01T00:26:10.680324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_df = pd.DataFrame(output, columns = ['Class_1','Class_2','Class_3','Class_4','Class_5','Class_6','Class_7','Class_8','Class_9'])\npred_df['id'] =  test['id'].values","metadata":{"execution":{"iopub.status.busy":"2021-07-01T00:26:11.7095Z","iopub.execute_input":"2021-07-01T00:26:11.70996Z","iopub.status.idle":"2021-07-01T00:26:11.718486Z","shell.execute_reply.started":"2021-07-01T00:26:11.709915Z","shell.execute_reply":"2021-07-01T00:26:11.716836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pred_df","metadata":{"execution":{"iopub.status.busy":"2021-06-30T00:50:23.837082Z","iopub.execute_input":"2021-06-30T00:50:23.837434Z","iopub.status.idle":"2021-06-30T00:50:23.860463Z","shell.execute_reply.started":"2021-06-30T00:50:23.8374Z","shell.execute_reply":"2021-06-30T00:50:23.859757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating an output file to submit and verify competition score.\noutput = pred_df[['id','Class_1','Class_2','Class_3','Class_4','Class_5','Class_6','Class_7','Class_8','Class_9']]\noutput.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T00:26:14.792684Z","iopub.execute_input":"2021-07-01T00:26:14.793174Z","iopub.status.idle":"2021-07-01T00:26:17.957602Z","shell.execute_reply.started":"2021-07-01T00:26:14.793142Z","shell.execute_reply":"2021-07-01T00:26:17.956115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}