{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpcful packages to load\nimport os \nimport sys\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\nimport random\nimport csv\nimport cv2\nimport math\nimport PIL\nfrom collections import namedtuple, OrderedDict\nimport io\nfrom PIL import Image\nfrom collections import namedtuple, OrderedDict\n\n%matplotlib inline\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_DIR = '../input/airbus-ship-detection/' \nROOT_DIR = '/kaggle/working'\nos.chdir(ROOT_DIR)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Training data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_v2_list = os.listdir(DATA_DIR + 'train_v2')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(DATA_DIR + \"train_ship_segmentations_v2.csv\")\ntrain_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['ShipCount'] = train_df.groupby('ImageId')['ImageId'].transform('count')\ntrain_df.loc[train_df['EncodedPixels'].isnull().values,'ShipCount'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_df = train_df.groupby('ShipCount').count()\ncount_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nax.bar(count_df.index.values.tolist(), list(count_df['ImageId']))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Image Enhancement"},{"metadata":{"trusted":true},"cell_type":"code","source":"sampleList = ['001aee007.jpg','001234638.jpg','001f04ca3.jpg','000d26c17.jpg']\nsampleImgList = []\nfor x in sampleList:\n    sampleImgList.append(mpimg.imread(DATA_DIR + 'train_v2/' + x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(1,figsize=(20,10))\nfor i in range(len(sampleImgList)):\n    image_tmp = sampleImgList[i]\n    ax = fig.add_subplot(1,4,i+1)\n    ax.imshow(image_tmp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from skimage.feature import canny\nfrom skimage.filters import scharr, unsharp_mask\nfrom skimage import exposure\nfrom skimage.color.adapt_rgb import adapt_rgb, each_channel, hsv_value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(1,figsize=(20,20))\nfor i in range(len(sampleImgList)):\n    image_tmp = sampleImgList[i]\n    ax = fig.add_subplot(2,4,i+1)\n    ax.imshow(image_tmp)\n    image_tmp = unsharp_mask(sampleImgList[i], radius=4, amount=2)\n    sampleImgList[i] = image_tmp\n    ax = fig.add_subplot(1,4,i+1)\n    ax.imshow(image_tmp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decoding the pixels"},{"metadata":{"trusted":true},"cell_type":"code","source":"def rle_to_pixels(rle_code):\n    '''\n    Transforms a RLE code string into a list of pixels of a (768, 768) canvas\n    '''\n    rle_code = [int(i) for i in rle_code.split()]\n#     pixels = [(pixel_position % 768, pixel_position // 768) \n#                  for start, length in list(zip(rle_code[0:-1:2], rle_code[1::2])) \n#                  for pixel_position in range(start, start + length)]\n    \n    pixels = []\n    for start, length in list(zip(rle_code[0:-1:2], rle_code[1::2])):\n        for pixel_position in range(start, start + length):\n            pixels.append((pixel_position % 768, pixel_position // 768))\n            #if pixel_position < 0 or pixel_position > 768:\n                #print(pixel_position)\n    \n    return pixels\n\ndef get_all_masks(image_id):\n    ret = []\n    s = train_df[train_df['ImageId'] == image_id]['EncodedPixels']\n    if not s.isnull().values.any():\n        for x in s:\n            ret.append(rle_to_pixels(x))\n    return ret\n\ndef show_masks(image_id):\n    canvas = np.zeros((768, 768))\n    masks = get_all_masks(image_id)\n    for x in masks:\n       canvas[tuple(zip(*x))] = 1\n    return canvas\n\ndef find_bounding_box(pixels):\n    xmin = 767\n    xmax = 1\n    ymin = 767\n    ymax = 1\n    for p in pixels:\n        px = p[0]\n        py = p[1]\n        if px !=0 and py != 0 and px !=768 and py != 768:\n            if px < xmin:\n                xmin = px\n            if px > xmax:\n                xmax = px\n            if py < ymin:\n                ymin = py\n            if py > ymax:\n                ymax = py\n    return xmin, ymin, xmax, ymax\n\ndef get_all_boxes(image_id):\n    ret = []\n    masks = get_all_masks(image_id)\n    for x in masks:\n        ret.append(find_bounding_box(x))\n    return ret\n\ndef show_bounding_box(image_id):\n    canvas = np.array(PIL.Image.open(DATA_DIR + 'train_v2/' + image_id))\n    boxes = get_all_boxes(image_id)\n    for x in boxes:\n        xmin, ymin, xmax, ymax = x[0],x[1], x[2], x[3]\n        canvas[xmin][ymin : ymax] = [0,255,0]\n        canvas[xmax][ymin : ymax] = [0,255,0]\n        canvas[:,ymin][xmin : xmax] = [0,255,0]\n        canvas[:,ymax][xmin : xmax] = [0,255,0]\n    return canvas","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(1,figsize=(20,20))\nfor i in range(len(sampleList)): \n    image_tmp = show_masks(sampleList[i])\n    ax = fig.add_subplot(1,4,i+1)\n    ax.imshow(image_tmp)\n    image_tmp = show_bounding_box(sampleList[i])\n    ax = fig.add_subplot(2,4,i+1)\n    ax.imshow(image_tmp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tensorflow Object Detection API setup"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip uninstall -y tensorflow\n!pip uninstall -y tensorflow-gpu\n!pip uninstall -y tensorflow-estimator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Change TF version\n!pip list | grep tensorflow\n!pip install tensorflow-gpu==1.15 #1.15\n# !pip uninstall -y tensorflow==2.2\n# !pip list | grep tensorflow\n!pip install tensorflow-estimator==1.15\n!pip list | grep tensorflow","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nos.getcwd()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\ntf.__version__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.python.client import device_lib \nprint(device_lib.list_local_devices())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.test.is_gpu_available(\n    cuda_only=False, min_cuda_compute_capability=None\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install tensorflow-object-detection-api==0.1.0 --no-dependencies","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.chdir(ROOT_DIR)\n!git clone https://github.com/tensorflow/models.git\n#!git clone https://github.com/tensorflow/models/archive/v2.2.0.zip\n# !wget  https://github.com/tensorflow/models/archive/v2.2.0.zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.chdir(ROOT_DIR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!apt-get -y install protobuf-compiler\n!pip install Cython\n!pip install pillow\n!pip install lxml\n!pip install jupyter\n!pip install matplotlib\n!pip install tf_slim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.chdir(ROOT_DIR+\"/models/research/\")\n!protoc object_detection/protos/*.proto --python_out=.\n!export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim\nos.environ['PYTHONPATH'] += ':/kaggle/working/models/research/:/kaggle/working/models/research/slim/:/kaggle/working/models'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.getcwd()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Test if set up is successful\n!python ./models/research/object_detection/builders/model_builder_test.py\n# !python object_detection/builders/model_builder_tf2_test.py","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.chdir(ROOT_DIR)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LabelMap"},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(ROOT_DIR+'/labelmap.pbtxt', 'w+') as the_file:\n    the_file.write('item\\n')\n    the_file.write('{\\n')\n    the_file.write('id :{}'.format(int(1)))\n    the_file.write('\\n')\n    the_file.write(\"name :'{0}'\".format('ship'))\n    the_file.write('\\n')\n    the_file.write('}\\n')\n    the_file.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generate annotations"},{"metadata":{},"cell_type":"markdown","source":"## Generate xml files"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xml.etree.cElementTree as ET\ndef generate_xml(imageId):\n    annotation = ET.Element(\"annotation\")\n    ET.SubElement(annotation, \"folder\").text = \"train_v2\"\n    ET.SubElement(annotation, \"filename\").text = imageId\n    source = ET.SubElement(annotation, \"source\")\n    ET.SubElement(source, \"database\").text = \"Unknown\"\n    size = ET.SubElement(annotation, \"size\")\n    ET.SubElement(size, \"width\").text = \"768\"\n    ET.SubElement(size, \"height\").text = \"768\"\n    ET.SubElement(size, \"depth\").text = \"3\"\n    ET.SubElement(annotation, \"segmented\").text = \"0\"\n    \n    boxes = get_all_boxes(imageId)\n    for b in boxes:\n        object1 = ET.SubElement(annotation, \"object\")\n        ET.SubElement(object1, \"name\").text = \"ship\"\n        ET.SubElement(object1, \"name\").text = \"ship\"\n        ET.SubElement(object1, \"pose\").text = \"Unspecified\"\n        ET.SubElement(object1, \"truncated\").text = \"0\"\n        ET.SubElement(object1, \"difficult\").text = \"0\"\n        bndbox = ET.SubElement(object1, \"bndbox\")\n        xmin, ymin, xmax, ymax = b\n        ET.SubElement(bndbox, \"xmin\").text = str(xmin)\n        ET.SubElement(bndbox, \"ymin\").text = str(ymin)\n        ET.SubElement(bndbox, \"xmax\").text = str(xmax)\n        ET.SubElement(bndbox, \"ymax\").text = str(ymax)\n\n    tree = ET.ElementTree(annotation)\n    tree.write(\"test.xml\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Generate dataframe for TFRecords"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = []\n###############################################DEBUG\nfor x in train_df['ImageId'][:100]:\n    boxes = get_all_boxes(x)\n    for b in boxes:\n        xmin, ymin, xmax, ymax = b\n        data.append((x, 768, 768, 'ship', xmin, ymin, xmax, ymax))\ncolumns_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\nTFRcords_df = pd.DataFrame(data=data, columns=columns_name)\nTFRcords_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training & validation split"},{"metadata":{"trusted":true},"cell_type":"code","source":"#A naive way to split for code testing\n\ntrain_set = TFRcords_df[0:20]\nvalid_set = TFRcords_df[21:25]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pip install tf-nightly","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sys.path.append(\"..\")\nfrom models.research.object_detection.utils import dataset_util\nfrom models.research.object_detection.utils import label_map_util","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to group data and return the same\n# Group by imagefile name\ndef make_groups(df, field=None):\n    if field==None:\n        field = 'filename'\n\n    data = namedtuple('object', ['filename', 'info'])\n    grouped = df.groupby(field)\n\n    grouped_data = []\n    for filename, x in zip(grouped.groups.keys(), grouped.groups):\n        grouped_data.append(data(filename, grouped.get_group(x)))\n\n    return grouped_data\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a tf record sample\ndef create_tf_example(group, img_path, label_map_dict):\n    # Read the imagefile. This will be used in features later \n    with tf.io.gfile.GFile(os.path.join(img_path, '{}'.format(group.filename)), 'rb') as f:\n        img_file = f.read()\n\n        # Encode to bytes and read using PIL. Could be done directly too\n        encoded_img = io.BytesIO(img_file)\n        # Read the image using PIL\n        img = Image.open(encoded_img)\n        width, height = img.size\n\n      # Encode the name of the img file\n        filename = group.filename.encode('utf8')\n\n      # Define the format of the image file\n        img_format = b'jpg'   # The name will be in bytes\n\n\n      # Define the variables that you need as features\n        xmins = []\n        xmaxs = []\n        ymins = []\n        ymaxs = []\n        classes_text = []\n        classes = []\n\n      # Iterate over the namedtuple object\n        for index, row in group.info.iterrows():\n            xmins.append(row['xmin'] / width)   # store normalized values for bbox\n            xmaxs.append(row['xmax'] / width)\n            ymins.append(row['ymin'] / height)\n            ymaxs.append(row['ymax'] / height)\n            classes_text.append(row['class'].encode('utf8'))\n            classes.append(label_map_dict[row['class']])\n\n        tf_example = tf.train.Example(features=tf.train.Features(feature={\n          'image/height': dataset_util.int64_feature(height),\n          'image/width': dataset_util.int64_feature(width),\n          'image/filename': dataset_util.bytes_feature(filename),\n          'image/source_id': dataset_util.bytes_feature(filename),\n          'image/encoded': dataset_util.bytes_feature(img_file),\n          'image/format': dataset_util.bytes_feature(img_format),\n          'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n          'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n          'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n          'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n          'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n          'image/object/class/label': dataset_util.int64_list_feature(classes),}))\n\n        return tf_example","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create TFRecord Files"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Path where all the images are present\nimg_path = DATA_DIR + 'train_v2'\n# Label map\nlabel_map_dict = label_map_util.get_label_map_dict(ROOT_DIR + '/labelmap.pbtxt')\n\nwriter = tf.compat.v1.python_io.TFRecordWriter('./train.record')\n\n# create groups in the df. One image may contain several instances of an object hence the grouping thing\nimg_groups = make_groups(train_set, field='filename')\n# Iterate over the samples in each group create a TFRecord\nfor group in img_groups:\n    tf_example = create_tf_example(group, img_path, label_map_dict)\n    writer.write(tf_example.SerializeToString())\n# close the writer\nwriter.close()\nprint(\"TFRecords for training data  created successfully\")\n\n\nwriter = tf.compat.v1.python_io.TFRecordWriter('./valid.record')\n# create groups \nimg_groups = make_groups(valid_set, field='filename')\n# Iterate over the samples in each group create a TFRecord\nfor group in img_groups:\n    tf_example = create_tf_example(group, img_path, label_map_dict)\n    writer.write(tf_example.SerializeToString())\n# close the writer\nwriter.close()\nprint(\"TFRecords for validation data created successfully\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#!cp /kaggle/working/models/research/object_detection/samples/configs/ssd_inception_v2_coco.config /kaggle/working\n\n#!cp /kaggle/working/models/research/object_detection/samples/configs/faster_rcnn_inception_v2_coco.config /kaggle/working\n!cp /kaggle/working/models/research/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_coco.config /kaggle/working","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!wget download.tensorflow.org/models/object_detection/ssd_inception_v2_coco_2017_11_17.tar.gz\n#!wget download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_inception_resnet_v2_640x640_coco17_tpu-8.tar.gz\n#!wget download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_inception_resnet_v2_1024x1024_coco17_tpu-8.tar.gz\n!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_inception_resnet_v2_1024x1024_coco17_tpu-8.tar.gz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!tar -xzf ssd_inception_v2_coco_2017_11_17.tar.gz\n#!tar -xzf faster_rcnn_inception_resnet_v2_640x640_coco17_tpu-8.tar.gz\n!tar -xzf faster_rcnn_inception_resnet_v2_1024x1024_coco17_tpu-8.tar.gz\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!mv ssd_inception_v2_coco_2017_11_17 mymodel\n#!mv faster_rcnn_inception_resnet_v2_640x640_coco17_tpu-8 mymodel\n!mv faster_rcnn_inception_resnet_v2_1024x1024_coco17_tpu-8 mymodel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install Cython\n!git clone https://github.com/pdollar/coco.git\nos.chdir('coco/PythonAPI')\n!make\n!make install\n!python setup.py install\nos.chdir(ROOT_DIR)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n# Configure the model config file"},{"metadata":{"trusted":true},"cell_type":"code","source":"os.chdir(ROOT_DIR)\nos.getcwd()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Configures the .config automatically\n#fin = open(\"ssd_inception_v2_coco.config\", \"rt\")\n#fin = open(\"faster_rcnn_inception_v2_coco.config\", \"rt\")\nfin = open(\"faster_rcnn_inception_resnet_v2_atrous_coco.config\", \"rt\")\n\nfout = open(\"configfile.config\", \"wt\")\n\n\nfor line in fin:\n    if 'num_classes:' in line:\n        fout.write('\\t\\tnum_classes: 1\\n')\n    else:\n        line = line.replace('PATH_TO_BE_CONFIGURED/model.ckpt', '/kaggle/working/mymodel/model.ckpt')\n        line = line.replace('PATH_TO_BE_CONFIGURED/mscoco_train.record-?????-of-00100', '/kaggle/working/train.record')\n        line = line.replace('PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt', '/kaggle/working/labelmap.pbtxt')\n        line = line.replace('PATH_TO_BE_CONFIGURED/mscoco_val.record-?????-of-00010','/kaggle/working/valid.record')\n        line = line.replace('num_steps: 200000','num_steps: 5000')\n        fout.write(line)\n\nfin.close()\nfout.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir checkpoints\n!cp /kaggle/working/models/research/object_detection/legacy/train.py /kaggle/working\n!cp /kaggle/working/models/research/object_detection/legacy/eval.py /kaggle/working\n!cp /kaggle/working/models/research/object_detection/export_inference_graph.py /kaggle/working\n!cp /kaggle/working/models/research/object_detection/model_main.py /kaggle/working","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.__version__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.chdir(ROOT_DIR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.mkdir(\"Output_point\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#run the training\n# import tensorflow.compat.v2 as tf\n# !python train.py --logtostderr --train_dir=/kaggle/working/checkpoints/ --pipeline_config_path=/kaggle/working/configfile.config\n!python model_main.py  --logtostderr --model_dir=/kaggle/working/Output_point/ --pipeline_config_path=/kaggle/working/configfile.config","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip list | grep tensorflow","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir /kaggle/working/trained","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# os.listdir(\"/kaggle/working/checkpoints/\")\nos.listdir(\"/kaggle/working/Output_point/\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!python export_inference_graph.py --input_type image_tensor --pipeline_config_path /kaggle/working/configfile.config --trained_checkpoint_prefix ./Output_point/model.ckpt-50 --output_directory /kaggle/working/trained","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls -al /kaggle/working/trained","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !tar -cvzf /kaggle/working/trained_model.tar /kaggle/working/trained\n# !gzip -y /kaggle/working/trained_model.tar","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test Data\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_v2_list = os.listdir(DATA_DIR + 'test_v2')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv(DATA_DIR + \"sample_submission_v2.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.getcwd())\n!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # List of the strings that is used to add correct label for each box.\n# PATH_TO_LABELS = '/kaggle/working/labelmap.pbtxt'\n# category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clmpc(label_map, max_num_classes,use_display_name=True):\n    categories = []\n    list_of_ids_already_added = []\n    if not label_map:\n        label_id_offset = 1\n        for class_id in range(max_num_classes):\n            categories.append({\n                'id': class_id + label_id_offset,\n                'name': 'category_{}'.format(class_id + label_id_offset)})\n        return categories\n    for item in label_map.item:\n        if not 0 < item.id <= max_num_classes:\n#             logging.info('Ignore item %d since it falls outside of requested ''label range.', item.id)\n            continue\n        if use_display_name and item.HasField('display_name'):\n            name = item.display_name\n        else:\n            name = item.name\n        if item.id not in list_of_ids_already_added:\n            list_of_ids_already_added.append(item.id)\n            categories.append({'id': item.id, 'name': name})\n    return categories","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# What model to download.\nMODEL_NAME = 'trained'\n\n# Path to frozen detection graph. This is the actual model that is used for the object detection.\nPATH_TO_FROZEN_GRAPH = MODEL_NAME + '/frozen_inference_graph.pb'\n\n# List of the strings that is used to add correct label for each box.\nPATH_TO_LABELS = '/kaggle/working/labelmap.pbtxt'\n\nNUM_CLASSES = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"detection_graph = tf.Graph()\nwith detection_graph.as_default():\n    od_graph_def = tf.GraphDef()\n    with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:\n        serialized_graph = fid.read()\n        od_graph_def.ParseFromString(serialized_graph)\n        tf.import_graph_def(od_graph_def, name='')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\ncategories = clmpc(label_map,max_num_classes=NUM_CLASSES)\n\ncategory_index = label_map_util.create_category_index(categories)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(category_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clmpc(label_map, max_num_classes,use_display_name=True):\n    categories = []\n    list_of_ids_already_added = []\n    if not label_map:\n        label_id_offset = 1\n        for class_id in range(max_num_classes):\n            categories.append({\n                'id': class_id + label_id_offset,\n                'name': 'category_{}'.format(class_id + label_id_offset)})\n        return categories\n    for item in label_map.item:\n        if not 0 < item.id <= max_num_classes:\n#             logging.info('Ignore item %d since it falls outside of requested ''label range.', item.id)\n            continue\n        if use_display_name and item.HasField('display_name'):\n            name = item.display_name\n        else:\n            name = item.name\n        if item.id not in list_of_ids_already_added:\n            list_of_ids_already_added.append(item.id)\n            categories.append({'id': item.id, 'name': name})\n    return categories","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_image_into_numpy_array(image):\n  (im_width, im_height) = image.size\n  return np.array(image.getdata()).reshape(\n      (im_height, im_width, 3)).astype(np.uint8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH_TO_TEST_IMAGES_DIR = '../input/airbus-ship-detection/test_v2/'\nTEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR,i ) for i in test_df['ImageId'][10:20]]\n\n# Size, in inches, of the output images.\nIMAGE_SIZE = (12, 8)\nprint(TEST_IMAGE_PATHS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_inference_for_single_image(image, graph):\n    with graph.as_default():\n        with tf.Session() as sess:\n            # Get handles to input and output tensors\n            ops = tf.get_default_graph().get_operations()\n            all_tensor_names = {output.name for op in ops for output in op.outputs}\n            tensor_dict = {}\n            for key in ['num_detections', 'detection_boxes', 'detection_scores','detection_classes', 'detection_masks']:\n                tensor_name = key + ':0'\n                if tensor_name in all_tensor_names:\n                    tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(tensor_name)\n            if 'detection_masks' in tensor_dict:\n                # The following processing is only for single image\n                detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n                detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n                # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n                real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n                detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n                detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n                detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n                    detection_masks, detection_boxes, image.shape[0], image.shape[1])\n                detection_masks_reframed = tf.cast(\n                    tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n                    # Follow the convention by adding back the batch dimension\n                tensor_dict['detection_masks'] = tf.expand_dims(detection_masks_reframed, 0)\n            image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n\n            # Run inference\n            output_dict = sess.run(tensor_dict,\n                             feed_dict={image_tensor: np.expand_dims(image, 0)})\n\n          # all outputs are float32 numpy arrays, so convert types as appropriate\n            output_dict['num_detections'] = int(output_dict['num_detections'][0])\n            output_dict['detection_classes'] = output_dict['detection_classes'][0].astype(np.uint8)\n            output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n            output_dict['detection_scores'] = output_dict['detection_scores'][0]\n        if 'detection_masks' in output_dict:\n            output_dict['detection_masks'] = output_dict['detection_masks'][0]\n    return output_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from object_detection.utils import visualization_utils as vis_util\nfor image_path in TEST_IMAGE_PATHS:\n    image = Image.open(image_path)\n    # the array based representation of the image will be used later in order to prepare the\n    # result image with boxes and labels on it.\n    image_np = load_image_into_numpy_array(image)\n    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n    image_np_expanded = np.expand_dims(image_np, axis=0)\n    # Actual detection.\n    output_dict = run_inference_for_single_image(image_np, detection_graph)\n    # Visualization of the results of a detection.\n    vis_util.visualize_boxes_and_labels_on_image_array(\n      image_np,\n      output_dict['detection_boxes'],\n      output_dict['detection_classes'],\n      output_dict['detection_scores'],\n      category_index,\n      instance_masks=output_dict.get('detection_masks'),\n      use_normalized_coordinates=True,\n      line_thickness=8)\n    plt.figure(figsize=IMAGE_SIZE)\n    plt.imshow(image_np)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}