{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Overview","metadata":{"_uuid":"1b8c18e1d49e580b84b194084a08d59e2a497b54"}},{"cell_type":"code","source":"!cd models; ls","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from fastai.conv_learner import *\nfrom fastai.dataset import *\n\nimport pandas as pd\nimport numpy as np\nimport os\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data + Pretrained code","metadata":{"_uuid":"ddd71a95c05902b29ce4e3768a8127c2a0b7098d"}},{"cell_type":"code","source":"PATH = './'\nTRAIN = '../input/airbus-ship-detection/train_v2/'\nTEST = '../input/airbus-ship-detection/test_v2/'\nSEGMENTATION = '../input/airbus-ship-detection/train_ship_segmentations_v2.csv'\n# PRETRAINED = '../input/fine-tuning-resnet34-on-ship-detection/models/Resnet34_lable_256_1.h5'\nexclude_list = ['6384c3e78.jpg','13703f040.jpg', '14715c06d.jpg',  '33e0ff2d5.jpg',\n                '4d4e09f2a.jpg', '877691df8.jpg', '8b909bb20.jpg', 'a8d99130e.jpg', \n                'ad55c3143.jpg', 'c8260c541.jpg', 'd6c7f17c7.jpg', 'dc3e7c901.jpg',\n                'e44dffe88.jpg', 'ef87bad36.jpg', 'f083256d8.jpg'] #corrupted images","metadata":{"_uuid":"b93d5422dfd31f43df9cdd3e301547cc95f25980","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr_arr = np.array(pd.read_csv('../input/traintestval-airbus/train_df.csv')['0'].reset_index(drop=True))\nval_arr = np.array(pd.read_csv('../input/traintestval-airbus/val_df.csv')['0'].reset_index(drop=True))\ntest_arr = np.array(pd.read_csv('../input/traintestval-airbus/test_df.csv')['0'].reset_index(drop=True))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nw = 2   #number of workers for data loader\narch = resnet34 #specify target architecture","metadata":{"_uuid":"9a72e076dc04cf1786edae26a1d2f15ec3de234a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for el in exclude_list:\n    if(el in tr_arr): tr_arr.remove(el)\n    if(el in val_arr): val_arr.remove(el)\n    if(el in test_arr): test_arr.remove(el)\nsegmentation_df = pd.read_csv(os.path.join(PATH, SEGMENTATION)).set_index('ImageId')","metadata":{"_uuid":"49199ea17e9e9ba9893c10d06c1fb419e22aeb1b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr_n = tr_arr\nval_n = val_arr\ntest_n = test_arr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cut_empty(names):\n    return [name for name in names \n            if(type(segmentation_df.loc[name]['EncodedPixels']) != float)]\n\ntr_n = cut_empty(tr_n)\nval_n = cut_empty(val_n)\ntest_n = cut_empty(test_n)","metadata":{"_uuid":"6a3d8c70e03964738322ca99084f088adc1c5f3e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_n = tr_n + val_n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_mask(img_id, df):\n    shape = (768,768)\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    masks = df.loc[img_id]['EncodedPixels']\n    if(type(masks) == float): return img.reshape(shape)\n    if(type(masks) == str): masks = [masks]\n    for mask in masks:\n        s = mask.split()\n        for i in range(len(s)//2):\n            start = int(s[2*i]) - 1\n            length = int(s[2*i+1])\n            img[start:start+length] = 1\n    return img.reshape(shape).T\n\ndef Show_images(x,yp,yt):\n    columns = 3\n    rows = min(bs,8)\n    fig=plt.figure(figsize=(columns*4, rows*4))\n    for i in range(rows):\n        fig.add_subplot(rows, columns, 3*i+1)\n        plt.axis('off')\n        plt.imshow(x[i])\n        fig.add_subplot(rows, columns, 3*i+2)\n        plt.axis('off')\n        plt.imshow(yp[i])\n        fig.add_subplot(rows, columns, 3*i+3)\n        plt.axis('off')\n        plt.imshow(yt[i])\n    plt.show()","metadata":{"_uuid":"505f803555b8d9d0378a73d227da4b8174f2086d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class pdFilesDataset(FilesDataset):\n    def __init__(self, fnames, path, transform):\n        self.segmentation_df = pd.read_csv(SEGMENTATION).set_index('ImageId')\n        super().__init__(fnames, transform, path)\n    \n    def get_x(self, i):\n        img = open_image(os.path.join(self.path, self.fnames[i]))\n        if self.sz == 768: return img \n        else: return cv2.resize(img, (self.sz, self.sz))\n    \n    def get_y(self, i):\n        mask = np.zeros((768,768), dtype=np.uint8) if (self.path == TEST) \\\n            else get_mask(self.fnames[i], self.segmentation_df)\n        img = Image.fromarray(mask).resize((self.sz, self.sz)).convert('RGB')\n        return np.array(img).astype(np.float32)\n    \n    def get_c(self): return 0\n     \n\ndef get_data(sz,bs):\n    #data augmentation\n    aug_tfms = [RandomRotate(45,p=0.75, tfm_y=TfmType.CLASS)]\n    tfms = tfms_from_model(arch, sz, crop_type=CropType.NO, tfm_y=TfmType.CLASS,aug_tfms=aug_tfms)\n    tr_names = tr_n if (len(tr_n)%bs == 0) else tr_n[:-(len(tr_n)%bs)] #cut incomplete batch\n    ds = ImageData.get_ds(pdFilesDataset, (tr_names,TRAIN), (val_n,TRAIN), tfms, test=(test_n,TRAIN))\n    md = ImageData(PATH, ds, bs, num_workers=nw, classes=None)\n    #md.is_multi = False\n    return md\n\ndef get_data_no_aug(sz,bs):\n    #data augmentation\n    tfms = tfms_from_model(arch, sz, crop_type=CropType.NO, tfm_y=TfmType.CLASS)\n    tr_names = tr_n if (len(tr_n)%bs == 0) else tr_n[:-(len(tr_n)%bs)] #cut incomplete batch\n    ds = ImageData.get_ds(pdFilesDataset, (tr_names,TRAIN), (val_n,TRAIN), tfms, test=(test_n,TRAIN))\n    md = ImageData(PATH, ds, bs, num_workers=nw, classes=None)\n    #md.is_multi = False\n    return md\n\n# Use test set as val\ndef get_data_test_no_aug(sz,bs):\n    tfms = tfms_from_model(arch, sz, crop_type=CropType.NO, tfm_y=TfmType.CLASS)\n    tr_names = tr_n if (len(tr_n)%bs == 0) else tr_n[:-(len(tr_n)%bs)] #cut incomplete batch\n    ds = ImageData.get_ds(pdFilesDataset, (tr_names,TRAIN), (test_n,TRAIN), tfms, test=(test_n,TRAIN))\n    md = ImageData(PATH, ds, bs, num_workers=nw, classes=None)\n    #md.is_multi = False\n    return md\n\ndef get_data_test_aug(sz,bs):\n    #data augmentation\n    aug_tfms = [RandomRotate(45,p=0.75, tfm_y=TfmType.CLASS)]\n    tfms = tfms_from_model(arch, sz, crop_type=CropType.NO, tfm_y=TfmType.CLASS,aug_tfms=aug_tfms)\n    tr_names = tr_n if (len(tr_n)%bs == 0) else tr_n[:-(len(tr_n)%bs)] #cut incomplete batch\n    ds = ImageData.get_ds(pdFilesDataset, (test_n,TRAIN), (val_n,TRAIN), tfms, test=(test_n,TRAIN))\n    md = ImageData(PATH, ds, bs, num_workers=nw, classes=None)\n    #md.is_multi = False\n    return md","metadata":{"_uuid":"f67a326e1269bffca392fcf4ac10bf4a532ca56b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model","metadata":{"_uuid":"2b77a79e1e1e3f4c8a9e7120feb9a843b41492f8","trusted":true}},{"cell_type":"code","source":"cut,lr_cut = model_meta[arch]","metadata":{"_uuid":"5bfefb757e1d40da7d8761c824170214a4bec08b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_base():                   #load ResNet34 model\n    layers = cut_model(arch(True), cut)\n    return nn.Sequential(*layers)\n\ndef get_base(pre=True):              #load ResNet34 model\n    layers = cut_model(arch(pre), cut)\n    return nn.Sequential(*layers)","metadata":{"_uuid":"7b0649b17721919f475a99f41f47f09c4fcc41d6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class UnetBlock(nn.Module):\n    def __init__(self, up_in, x_in, n_out):\n        super().__init__()\n        up_out = x_out = n_out//2\n        self.x_conv  = nn.Conv2d(x_in,  x_out,  1)\n        self.tr_conv = nn.ConvTranspose2d(up_in, up_out, 2, stride=2)\n        self.bn = nn.BatchNorm2d(n_out)\n        \n    def forward(self, up_p, x_p):\n        up_p = self.tr_conv(up_p)\n        x_p = self.x_conv(x_p)\n        cat_p = torch.cat([up_p,x_p], dim=1)\n        return self.bn(F.relu(cat_p))\n\nclass SaveFeatures():\n    features=None\n    def __init__(self, m): self.hook = m.register_forward_hook(self.hook_fn)\n    def hook_fn(self, module, input, output): self.features = output\n    def remove(self): self.hook.remove()\n    \nclass Unet34(nn.Module):\n    def __init__(self, rn):\n        super().__init__()\n        self.rn = rn\n        self.sfs = [SaveFeatures(rn[i]) for i in [2,4,5,6]]\n        self.up1 = UnetBlock(512,256,256)\n        self.up2 = UnetBlock(256,128,256)\n        self.up3 = UnetBlock(256,64,256)\n        self.up4 = UnetBlock(256,64,256)\n        self.up5 = nn.ConvTranspose2d(256, 1, 2, stride=2)\n        \n    def forward(self,x):\n        x = F.relu(self.rn(x))\n        x = self.up1(x, self.sfs[3].features)\n        x = self.up2(x, self.sfs[2].features)\n        x = self.up3(x, self.sfs[1].features)\n        x = self.up4(x, self.sfs[0].features)\n        x = self.up5(x)\n        return x[:,0]\n    \n    def close(self):\n        for sf in self.sfs: sf.remove()\n            \nclass UnetModel():\n    def __init__(self,model,name='Unet'):\n        self.model,self.name = model,name\n\n    def get_layer_groups(self, precompute):\n        lgs = list(split_by_idxs(children(self.model.rn), [lr_cut]))\n        return lgs + [children(self.model)[1:]]","metadata":{"_uuid":"77ca73006aec0c9dfe4be6deb2d6cf524840cd62","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loss function","metadata":{"_uuid":"9c71a3af2a83b1122902a3f39e6fda1af5afbb99","trusted":true}},{"cell_type":"code","source":"def dice_loss(input, target):\n    input = torch.sigmoid(input)\n    smooth = 1.0\n\n    iflat = input.view(-1)\n    tflat = target.view(-1)\n    intersection = (iflat * tflat).sum()\n    \n    return ((2.0 * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))","metadata":{"_uuid":"8de4222fd28596dbae5d55f8172ac28271f678f2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FocalLoss(nn.Module):\n    def __init__(self, gamma):\n        super().__init__()\n        self.gamma = gamma\n        \n    def forward(self, input, target):\n        if not (target.size() == input.size()):\n            raise ValueError(\"Target size ({}) must be the same as input size ({})\"\n                             .format(target.size(), input.size()))\n\n        max_val = (-input).clamp(min=0)\n        loss = input - input * target + max_val + \\\n            ((-max_val).exp() + (-input - max_val).exp()).log()\n\n        invprobs = F.logsigmoid(-input * (target * 2.0 - 1.0))\n        loss = (invprobs * self.gamma).exp() * loss\n        \n        return loss.mean()","metadata":{"_uuid":"456acf8ffe7b80cd94eb73bfaeb6b1061bbb44c7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MixedLoss(nn.Module):\n    def __init__(self, alpha, gamma):\n        super().__init__()\n        self.alpha = alpha\n        self.focal = FocalLoss(gamma)\n        \n    def forward(self, input, target):\n        loss = self.alpha*self.focal(input, target) - torch.log(dice_loss(input, target))\n        return loss.mean()","metadata":{"_uuid":"1cfb00a50821861c69534ee7398bd220c02900d8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dice(pred, targs):\n    pred = (pred>0).float()\n    return 2.0 * (pred*targs).sum() / ((pred+targs).sum() + 1.0)\n\ndef IoU(pred, targs):\n    pred = (pred>0).float()\n    intersection = (pred*targs).sum()\n    return intersection / ((pred+targs).sum() - intersection + 1.0)","metadata":{"_uuid":"37baddf38bb569cbf2d2f186a5171767402b94fa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train Augmented Learner","metadata":{"_uuid":"0b3a20e5e2fdedf1439737f40c14c78f9a69e29a"}},{"cell_type":"code","source":"sz = 768 #image size\nbs = 8  #batch size\n\nmd = get_data(sz,bs)\nno_aug_md = get_data_no_aug(sz,bs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Build model","metadata":{}},{"cell_type":"code","source":"!cp ../input/traintestval-airbus/Unet34_768_aug_0.h5 ./models/Unet34_768_aug_0.h5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m_base = get_base()\nm = to_gpu(Unet34(m_base))\nmodels = UnetModel(m)","metadata":{"_uuid":"1a3b8f6b454a8d8b48505fbc5964e900a59ef09e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn = ConvLearner(md, models)\nlearn.load('Unet34_768_aug_0')\nlearn.opt_fn=optim.Adam\nlearn.crit = MixedLoss(10.0, 2.0)\nlearn.metrics=[accuracy_thresh(0.5),dice,IoU]\nwd=1e-7\nlr = 1e-2","metadata":{"_uuid":"eb1016214b0de75f56d2036241a27684b4c4bd1b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# donnt run this after","metadata":{}},{"cell_type":"code","source":"learn.freeze_to(1)\nlearn.fit(lr,1,wds=wd,cycle_len=1,use_clr=(5,8))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.save('Unet34_768_aug_0')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" The lr of the head part is still 1e-3, while the middle layers of the model are trained with 1e-4 lr, and the base is trained with even smaller lr, 1e-5, since low level detectors do not vary much from one image data set to another.","metadata":{}},{"cell_type":"markdown","source":"# Run this","metadata":{}},{"cell_type":"code","source":"# Unfreeze and train with differential learning rate\nlrs = np.array([lr/100,lr/10,lr])\nlearn.unfreeze() #unfreeze the encoder\nlearn.bn_freeze(True)\n\nlearn.fit(lrs,2,wds=wd,cycle_len=1,use_clr=(20,8))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.sched.plot_lr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.save('Unet34_768_aug_full')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cd models; ls","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training non-augmented learner","metadata":{}},{"cell_type":"code","source":"sz = 768 #image size\nbs = 8  #batch size\n\nno_aug_md = get_data_no_aug(sz,bs)\n\nm_base = get_base()\nstate_na = torch.load('../input/traintestval-airbus/Unet34_768_no_aug_0.h5')\nm_base.load_state_dict(state_na, strict=False)\nm = to_gpu(Unet34(m_base))\n\nlearn_na = ConvLearner(no_aug_md, models)\nlearn_na.opt_fn=optim.Adam\nlearn_na.crit = MixedLoss(10.0, 2.0)\nlearn_na.metrics=[accuracy_thresh(0.5),dice,IoU]\nwd=1e-7\nlr = 1e-2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Deprecated\nlearn_na.freeze_to(1)\nlearn_na.fit(lr,1,wds=wd,cycle_len=1,use_clr=(5,8))\nlearn_na.save('Unet34_768_no_aug_0')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Unfreeze and train with differential learning rate\nlrs = np.array([lr/100,lr/10,lr])\nlearn_na.unfreeze() #unfreeze the encoder\nlearn_na.bn_freeze(True)\n\nlearn_na.fit(lrs,2,wds=wd,cycle_len=1,use_clr=(20,8))\n\nlearn_na.save('Unet34_768_no_aug_1')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualization","metadata":{"_uuid":"a5f92ae7e3e348f95129a3742ad76e4a502bf6d1"}},{"cell_type":"code","source":"def Show_images(x,yp,yt):\n    columns = 3\n    rows = min(bs,8)\n    fig=plt.figure(figsize=(columns*4, rows*4))\n    for i in range(rows):\n        fig.add_subplot(rows, columns, 3*i+1)\n        plt.axis('off')\n        plt.imshow(x[i])\n        fig.add_subplot(rows, columns, 3*i+2)\n        plt.axis('off')\n        plt.imshow(yp[i])\n        fig.add_subplot(rows, columns, 3*i+3)\n        plt.axis('off')\n        plt.imshow(yt[i])\n    plt.show()\n    \n    \n# def Show_images(x,y,x1,y1):\n#     columns = 4\n#     rows = min(bs,8)\n#     fig=plt.figure(figsize=(columns*4, rows*4))\n#     for i in range(rows):\n#         fig.add_subplot(rows, columns, 4*i+1)\n#         plt.axis('off')\n#         plt.imshow(x[i])\n#         fig.add_subplot(rows, columns, 4*i+2)\n#         plt.axis('off')\n#         plt.imshow(y[i])\n#         fig.add_subplot(rows, columns, 4*i+3)\n#         plt.axis('off')\n#         plt.imshow(x1[i])\n#         fig.add_subplot(rows, columns, 4*i+4)\n#         plt.axis('off')\n#         plt.imshow(y1[i])\n#     plt.show()","metadata":{"_uuid":"fb4a8386560ebde1f2277d94ecb5ee4a1bbc50d2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sz = 768 #image size\nbs = 8  #batch size\n\nmd = get_data(sz,bs)\nno_aug_md = get_data_no_aug(sz,bs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x,y= md[0].trn_ds[2]\nox,oy = md[1].trn_ds[2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(md[0].trn_ds.denorm(x)[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(md[1].trn_ds.denorm(ox)[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(oy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test Model","metadata":{}},{"cell_type":"markdown","source":"### Evaluate scoring","metadata":{}},{"cell_type":"code","source":"from scipy import ndimage","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def IoU(pred, targs):\n    pred = (pred > 0.5).astype(float)\n    intersection = (pred*targs).sum()\n    return intersection / ((pred+targs).sum() - intersection + 1.0)\n\ndef get_iou(pred, true):\n    n_masks = len(true)\n    n_pred = len(pred)\n    tot_iou = []\n    \n    for mask in true:\n        for p in pred: \n            temp_iou = IoU(p,mask)\n            tot_iou.append(temp_iou)\n    \n    return np.mean(np.array(tot_iou))\n\ndef get_score(pred, true):\n    n_th = 10\n    b = 4\n    thresholds = [0.5 + 0.05*i for i in range(n_th)]\n    n_masks = len(true)\n    n_pred = len(pred)\n    \n    ious = []\n    score = 0\n    for mask in true:\n        buf = []\n        for p in pred: \n            buf.append(IoU(p,mask))\n        ious.append(buf)\n        \n    for t in thresholds:   \n        tp, fp, fn = 0, 0, 0\n        for i in range(n_masks):\n            match = False\n            for j in range(n_pred):\n                if ious[i][j] > t: match = True\n            if not match: fn += 1\n        \n        for j in range(n_pred):\n            match = False\n            for i in range(n_masks):\n                if ious[i][j] > t: match = True\n            if match: tp += 1\n            else: fp += 1\n        score += ((b+1)*tp)/((b+1)*tp + b*fn + fp)\n    \n    return score/n_th\n\ndef split_mask(mask):\n    threshold = 0.5\n    threshold_obj = 30 #ignor predictions composed of \"threshold_obj\" pixels or less\n    labled,n_objs = ndimage.label(mask > threshold)\n    result = []\n    for i in range(n_objs):\n        obj = (labled == i + 1).astype(int)\n        if(obj.sum() > threshold_obj): result.append(obj)\n    return result\n\ndef get_mask_ind(img_id, df, shape = (768,768)): #return mask for each ship\n    masks = df.loc[img_id]['EncodedPixels']\n    if(type(masks) == float): return []\n    if(type(masks) == str): masks = [masks]\n    result = []\n    for mask in masks:\n        img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n        s = mask.split()\n        for i in range(len(s)//2):\n            start = int(s[2*i]) - 1\n            length = int(s[2*i+1])\n            img[start:start+length] = 1\n        result.append(img.reshape(shape).T)\n    return result\n\nclass IoU_eval():\n    def __init__(self):\n        self.segmentation_df = pd.read_csv(SEGMENTATION).set_index('ImageId')\n        self.count = 0\n        self.iou = 0.0\n        \n    def put(self,pred,name):\n        true = get_mask_ind(name, self.segmentation_df)\n        self.iou += get_iou(pred,true)        \n        self.count += 1\n        \n    def evaluate(self):\n        return (self.iou / self.count)\n\nclass Score_eval():\n    def __init__(self):\n        self.segmentation_df = pd.read_csv(SEGMENTATION).set_index('ImageId')\n        self.score, self.count = 0.0, 0\n        \n    def put(self,pred,name):\n        true = get_mask_ind(name, self.segmentation_df)\n        self.score +=  get_score(pred,true)\n        self.count += 1\n        \n    def evaluate(self):\n        return (self.score/self.count)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TTA","metadata":{}},{"cell_type":"code","source":"def aug_unit(x,fwd=True,mask=False):\n    return x\n\ndef aug_flipV(x,fwd=True,mask=False):\n    return x.flip(2) if mask else x.flip(3)\n\ndef aug_flipH(x,fwd=True,mask=False):\n    return x.flip(1) if mask else x.flip(2)\n\ndef aug_T(x,fwd=True,mask=False):\n    return torch.transpose(x,1,2) if mask else torch.transpose(x,2,3)\n\ndef aug_rot_2(x,fwd=True,mask=False): #rotate pi/2\n    return aug_flipV(aug_flipH(x,fwd,mask),fwd,mask)\n\ndef aug_rot_4cr(x,fwd=True,mask=False): #rotate pi/4 counterclockwise\n    return aug_flipV(aug_T(x,fwd,mask),fwd,mask) if fwd else \\\n        aug_T(aug_flipV(x,fwd,mask),fwd,mask)\n\ndef aug_rot_4cw(x,fwd=True,mask=False): #rotate pi/4 clockwise\n    return aug_flipH(aug_T(x,fwd,mask),fwd,mask) if fwd else \\\n        aug_T(aug_flipH(x,fwd,mask),fwd,mask)\n\ndef aug_rot_2T(x,fwd=True,mask=False): #transpose and rotate pi/2\n    return aug_rot_2(aug_T(x,fwd,mask),fwd,mask)\n\ntrms_side_on = [aug_unit,aug_flipH]\ntrms_top_down = [aug_unit,aug_flipV]\ntrms_dihedral = [aug_unit,aug_flipH,aug_flipV,aug_T,aug_rot_2,aug_rot_2T,\n                 aug_rot_4cw,aug_rot_4cr]\ndef enc_img(img):\n    return torch.transpose(torch.tensor(img),0,2).unsqueeze(0)\n\ndef dec_img(img):\n    return to_np(torch.transpose(img.squeeze(0),0,2))\n\ndef display_augs(x,augs=aug_unit):\n    columns = 4\n    n = len(augs)\n    rows = n//4 + 1\n    fig=plt.figure(figsize=(columns*4, rows*4))\n    img = enc_img(x)\n    for i in range(rows):\n        for j in range(columns):\n            idx = j+i*columns\n            if idx >= n: break\n            fig.add_subplot(rows, columns, idx+1)\n            plt.axis('off')\n            plt.imshow(dec_img(augs[idx](img)))\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_pred(learner, dl, F_save): #if use train dl, disable shuffling\n    learner.model.eval();\n    name_list = dl.dataset.fnames\n    num_batchs = len(dl)\n    t = tqdm(iter(dl), leave=False, total=num_batchs)\n    count = 0\n    for x,y in t:\n        py = to_np(torch.sigmoid(learn.model(V(x))))\n        batch_size = len(py)\n        for i in range(batch_size):\n            F_save(py[i],to_np(y[i]),name_list[count])\n            count += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test augmented model","metadata":{}},{"cell_type":"code","source":"sz = 768 #image size\nbs = 8  #batch size\ntest_md = get_data_test_no_aug(sz,bs)\n\ntest_aug_md = get_data_test_aug(sz,bs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp ../input/traintestval-airbus/Unet34_768_no_aug_1.h5 ./models/Unet34_768_no_aug_1.h5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test Aug on no aug\n\nm = to_gpu(Unet34(get_base(False)))\nmodels = UnetModel(m)\n\nlearn = ConvLearner(test_md, models)\n\nlearn.load('Unet34_768_no_aug_1')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = Score_eval()\nprocess_pred = lambda yp, y, name : score.put(split_mask(yp),name)\n\niou_score = IoU_eval()\nprocess_iou_pred = lambda yp, y, name : iou_score.put(split_mask(yp),name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_pred(learn, test_md.val_dl, process_pred)\nprint('\\n',score.evaluate())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def IoU(pred, targs):\n    pred = (pred>0).float()\n    intersection = (pred*targs).sum()\n    return intersection / ((pred+targs).sum() - intersection + 1.0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Mean iou pixle pred starts here","metadata":{}},{"cell_type":"code","source":"sz = 768 #image size\nbs = 8  #batch size\ntest_md = get_data_test_no_aug(sz,bs)\n\ntest_aug_md = get_data_test_aug(sz,bs)\n\n# Test Aug on no aug\nm = to_gpu(Unet34(get_base(False)))\nmodels = UnetModel(m)\n\nlearn = ConvLearner(test_md, models)\n\nlearn.load('Unet34_768_aug_2')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_metrics(y_pred, gt):\n    y_pred = y_pred.astype(int)\n    y_pred = y_pred.squeeze()\n    \n    y_true = np.array(gt)\n    y_true = y_true.squeeze()\n        \n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    tn = np.sum((y_true == 0) & (y_pred == 0))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    \n#     print(tp,tn,fp,fn,y_true.size,tp + tn + fp + fn)\n    \n    assert tp + tn + fp + fn == y_true.size\n\n    n = tp + fp + tn + fn\n\n    acc = (tp+tn)/n\n\n    if (tp + fn == 0 or tp + fn + fp == 0):\n        recall = 1\n        precision = 1\n        iou = 1\n    else:\n        recall = (tp/(tp+fn))\n        precision = tp / (tp + fp)\n        iou = tp /(tp + fn + fp)\n    return iou","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RES DICT\nres_dict = []\n\n# RUN IT UP TURBO\nfor r,(img, mask) in tqdm(enumerate(test_aug_md.trn_dl)):\n\n    learn.model.eval();\n    yp = to_np(F.sigmoid(learn.model(V(img))))\n    yp[yp >= 0.5] = 1\n    yp[yp < 0.5] = 0\n    \n    metrics = get_metrics(yp, to_np(mask))\n\n    res_dict.append(metrics)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.mean(np.array(res_dict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m_base = get_base()\nm = to_gpu(Unet34(m_base))\nmodels = UnetModel(m)\nlearn = ConvLearner(md, models)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cd models; ls","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.load('Unet34_768_no_aug_1')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.model.eval();\nx,y = next(iter(test_md.val_dl))\nyp = to_np(F.sigmoid(learn.model(V(x))))\nyp[yp >= 0.5] = 1\nyp[yp < 0.5] = 0\n\nprint(get_metrics(yp, to_np(y)))\n\nShow_images(np.asarray(test_md.val_ds.denorm(x)), yp, y)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RotEqNet","metadata":{}},{"cell_type":"code","source":"def ntuple(n):\n    \"\"\" Ensure that input has the correct number of elements \"\"\"\n    def parse(x):\n        if isinstance(x, collections.Iterable):\n            return x\n        return tuple(itertools.repeat(x, n))\n    return parse\n\ndef getGrid(siz):\n    \"\"\" Returns grid with coordinates from -siz[0]/2 : siz[0]/2, -siz[1]/2 : siz[1]/2, ....\"\"\"\n    space = [np.linspace( -(N/2), (N/2), N ) for N in siz]\n    mesh = np.meshgrid( *space, indexing='ij' )\n    mesh = [np.expand_dims( ax.ravel(), 0) for ax in mesh]\n\n    return np.concatenate(mesh)\n\ndef rotate_grid_2D(grid, theta):\n    \"\"\" Rotate grid \"\"\"\n    theta = np.deg2rad(theta)\n\n    x0 = grid[0, :] * np.cos(theta) - grid[1, :] * np.sin(theta)\n    x1 = grid[0, :] * np.sin(theta) + grid[1, :] * np.cos(theta)\n\n    grid[0, :] = x0\n    grid[1, :] = x1\n    return grid\n\ndef rotate_grid_3D(theta, axis, grid):\n    \"\"\" Rotate grid \"\"\"\n    theta = np.deg2rad(theta)\n    axis = np.array(axis)\n    rot_mat = expm(np.cross(np.eye(3), axis / norm(axis) * theta))\n    rot_mat  =np.expand_dims(rot_mat,2)\n    grid = np.transpose( np.expand_dims(grid,2), [0,2,1])\n\n    return np.einsum('ijk,jik->ik',rot_mat,grid)\n\n\ndef get_filter_rotation_transforms(kernel_dims, angles):\n    \"\"\" Return the interpolation variables needed to transform a filter by a given number of degrees \"\"\"\n\n    dim = len(kernel_dims)\n\n    # Make grid (centered around filter-center)\n    grid = getGrid(kernel_dims)\n\n    # Rotate grid\n    if dim == 2:\n        grid = rotate_grid_2D(grid, angles)\n    elif dim == 3:\n        grid = rotate_grid_3D(angles[0], [1, 0, 0], grid)\n        grid = rotate_grid_3D(angles[1], [0, 0, 1], grid)\n\n\n    # Radius of filter\n    radius = np.min((np.array(kernel_dims)-1) / 2.)\n\n    #Mask out samples outside circle\n    radius = np.expand_dims(radius,-1)\n    dist_to_center = np.sqrt(np.sum(grid**2,axis=0))\n    mask = dist_to_center>=radius+.0001\n    mask = 1-mask\n\n    # Move grid to center\n    grid += radius\n\n    return compute_interpolation_grids(grid, kernel_dims, mask)\n\ndef compute_interpolation_grids(grid, kernel_dims, mask):\n\n    #######################################################\n    # The following part is part of nd-linear interpolation\n\n    #Add a small eps to grid so that floor and ceil operations become more stable\n    grid += 0.000000001\n\n    # Make list where each element represents a dimension\n    grid = [grid[i, :] for i in range(grid.shape[0])]\n\n    # Get left and right index (integers)\n    inds_0 = [ind.astype(np.integer) for ind in grid]\n    inds_1 = [ind + 1 for ind in inds_0]\n\n    # Get weights\n    weights = [float_ind - int_ind for float_ind, int_ind in zip(grid, inds_0)]\n\n    # Special case for when ind_1 == size (while ind_0 == siz)\n    # In that case we select ind_0\n    ind_1_out_of_bounds = np.logical_or.reduce([ind == siz for ind, siz in zip(inds_1, kernel_dims)])\n    for i in range(len(inds_1)):\n        inds_1[i][ind_1_out_of_bounds] = 0\n\n\n    # Get samples that are out of bounds or outside mask\n    inds_out_of_bounds = np.logical_or.reduce([ind < 0 for ind in itertools.chain(inds_0, inds_1)] + \\\n                                              [ind >= siz for ind, siz in zip(inds_0, kernel_dims)] + \\\n                                              [ind >= siz for ind, siz in zip(inds_1, kernel_dims)] +\n                                              (1-mask).astype('bool')\n                                              )\n\n\n    # Set these samples to zero get data from upper-left-corner (which will be put to zero)\n    for i in range(len(inds_0)):\n        inds_0[i][inds_out_of_bounds] = 0\n        inds_1[i][inds_out_of_bounds] = 0\n\n    #Reshape\n    inds_0 = [np.reshape(ind,[1,1]+kernel_dims) for ind in inds_0]\n    inds_1 = [np.reshape(ind,[1,1]+kernel_dims) for ind in inds_1]\n    weights = [np.reshape(weight,[1,1]+kernel_dims)for weight in weights]\n\n    #Make pytorch-tensors of the interpolation variables\n    inds_0 = [Variable(torch.LongTensor(ind)) for ind in inds_0]\n    inds_1 = [Variable(torch.LongTensor(ind)) for ind in inds_1]\n    weights = [Variable(torch.FloatTensor(weight)) for weight in weights]\n\n    #Make mask pytorch tensor\n    mask = mask.reshape(kernel_dims)\n    mask = mask.astype('float32')\n    mask = np.expand_dims(mask, 0)\n    mask = np.expand_dims(mask, 0)\n    mask = torch.FloatTensor(mask)\n\n    # Uncomment for nearest interpolation (for debugging)\n    #inds_1 = [ind*0 for ind in inds_1]\n    #weights  = [weight*0 for weight in weights]\n\n    return inds_0, inds_1, weights, mask\n\ndef apply_transform(filter, interp_vars, filters_size, old_bilinear_interpolation=True):\n    \"\"\" Apply a transform specified by the interpolation_variables to a filter \"\"\"\n\n    dim = 2 if len(filter.size())==4 else 3\n\n    if dim == 2:\n\n\n        if old_bilinear_interpolation:\n            [x0_0, x1_0], [x0_1, x1_1], [w0, w1] = interp_vars\n            rotated_filter = (filter[:, :, x0_0, x1_0] * (1 - w0) * (1 - w1) +\n                          filter[:, :, x0_1, x1_0] * w0 * (1 - w1) +\n                          filter[:, :, x0_0, x1_1] * (1 - w0) * w1 +\n                          filter[:, :, x0_1, x1_1] * w0 * w1)\n        else:\n\n            # Expand dimmentions to fit filter\n            interp_vars = [[inner_el.expand_as(filter) for inner_el in outer_el] for outer_el in interp_vars]\n\n            [x0_0, x1_0], [x0_1, x1_1], [w0, w1] = interp_vars\n\n            a = torch.gather(torch.gather(filter, 2, x0_0), 3, x1_0) * (1 - w0) * (1 - w1)\n            b = torch.gather(torch.gather(filter, 2, x0_1), 3, x1_0)* w0 * (1 - w1)\n            c = torch.gather(torch.gather(filter, 2, x0_0), 3, x1_1)* (1 - w0) * w1\n            d = torch.gather(torch.gather(filter, 2, x0_1), 3, x1_1)* w0 * w1\n            rotated_filter = a+b+c+d\n\n        rotated_filter = rotated_filter.view(filter.size()[0],filter.size()[1],filters_size[0],filters_size[1])\n\n    elif dim == 3:\n        [x0_0, x1_0, x2_0], [x0_1, x1_1, x2_1], [w0, w1, w2] = interp_vars\n\n        rotated_filter = (filter[x0_0, x1_0, x2_0] * (1 - w0) * (1 - w1)* (1 - w2) +\n                          filter[x0_1, x1_0, x2_0] * w0       * (1 - w1)* (1 - w2) +\n                          filter[x0_0, x1_1, x2_0] * (1 - w0) * w1      * (1 - w2) +\n                          filter[x0_1, x1_1, x2_0] * w0       * w1      * (1 - w2) +\n                          filter[x0_0, x1_0, x2_1] * (1 - w0) * (1 - w1)* w2 +\n                          filter[x0_1, x1_0, x2_1] * w0       * (1 - w1)* w2 +\n                          filter[x0_0, x1_1, x2_1] * (1 - w0) * w1      * w2 +\n                          filter[x0_1, x1_1, x2_1] * w0       * w1      * w2)\n\n        rotated_filter = rotated_filter.view(filter.size()[0], filter.size()[1], filters_size[0], filters_size[1], filters_size[2])\n\n    return rotated_filter\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RotConv(nn.Module):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n                 padding=0, dilation=1, n_angles = 8, mode=1):\n        super(RotConv, self).__init__()\n\n        kernel_size = ntuple(2)(kernel_size)\n        stride = ntuple(2)(stride)\n        padding = ntuple(2)(padding)\n        dilation = ntuple(2)(dilation)\n\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n\n        self.mode = mode\n\n        #Angles\n        self.angles = np.linspace(0,360,n_angles, endpoint=False)\n        self.angle_tensors = []\n\n        #Get interpolation variables\n        self.interp_vars = []\n        for angle in self.angles:\n            out = get_filter_rotation_transforms(list(self.kernel_size), angle)\n            self.interp_vars.append(out[:-1])\n            self.mask = out[-1]\n\n            self.angle_tensors.append( Variable(torch.FloatTensor( np.array([angle/ 180. * np.pi]) )) )\n\n        self.weight1 = Parameter(torch.Tensor( out_channels, in_channels , *kernel_size))\n        #If input is vector field, we have two filters (one for each component)\n        if self.mode == 2:\n            self.weight2 = Parameter(torch.Tensor( out_channels, in_channels, *kernel_size))\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        n = self.in_channels\n        for k in self.kernel_size:\n            n *= k\n        stdv = 1. / math.sqrt(n)\n        self.weight1.data.uniform_(-stdv, stdv)\n        if self.mode == 2:\n            self.weight2.data.uniform_(-stdv, stdv)\n\n    def mask_filters(self):\n        self.weight1.data[self.mask.expand_as(self.weight1) == 0] = 1e-8\n        if self.mode == 2:\n            self.weight2.data[self.mask.expand_as(self.weight1) == 0] = 1e-8\n\n    def _apply(self, func):\n        # This is called whenever user calls model.cuda()\n        # We intersect to replace tensors and variables with cuda-versions\n        self.mask = func(self.mask)\n        self.interp_vars = [[[func(el2) for el2 in el1] for el1 in el0] for el0 in self.interp_vars]\n        self.angle_tensors = [func(el) for el in self.angle_tensors]\n\n        return super(RotConv, self)._apply(func)\n\n\n    def forward(self,input):\n        #Uncomment this to turn on filter-masking\n        #Todo: fix broken convergence when filter-masking is on\n        #self.mask_filters()\n\n        if self.mode == 1:\n            outputs = []\n\n            #Loop through the different filter-transformations\n            for ind, interp_vars in enumerate(self.interp_vars):\n                #Apply rotation\n                weight = apply_transform(self.weight1, interp_vars, self.kernel_size)\n\n                #Do convolution\n                out = F.conv2d(input, weight, None, self.stride, self.padding, self.dilation)\n                outputs.append(out.unsqueeze(-1))\n\n        if self.mode == 2:\n            u = input[0]\n            v = input[1]\n\n            outputs = []\n            # Loop through the different filter-transformations\n            for ind, interp_vars in enumerate(self.interp_vars):\n                angle = self.angle_tensors[ind]\n                # Apply rotation\n                wu = apply_transform(self.weight1, interp_vars, self.kernel_size)\n                wv = apply_transform(self.weight2, interp_vars, self.kernel_size)\n\n                # Do convolution for u\n                wru = torch.cos(angle) * wu - torch.sin(angle ) * wv\n                u_out = F.conv2d(u, wru, None, self.stride, self.padding, self.dilation)\n\n                # Do convolution for v\n                wrv = torch.sin(angle) * wu + torch.cos(angle) * wv\n                v_out = F.conv2d(v, wrv, None, self.stride, self.padding, self.dilation)\n\n                #Compute magnitude (p)\n                outputs.append(  (u_out + v_out).unsqueeze(-1) )\n                \n\n        # Get the maximum direction (Orientation Pooling)\n        strength, max_ind = torch.max(torch.cat(outputs, -1), -1)\n\n        # Convert from polar representation q\n        angle_map = max_ind.float() * (360. / len(self.angles) / 180. * np.pi)\n        u = F.relu(strength) * torch.cos(angle_map)\n        v = F.relu(strength) * torch.sin(angle_map)\n\n\n        return u, v","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RotUnetBlock(nn.Module):\n    def __init__(self, up_in, x_in, n_out):\n        super().__init__()\n        up_out = x_out = n_out//2\n        self.x_conv  = nn.RotConv(x_in,  x_out,  1)\n        self.tr_conv = nn.ConvTranspose2d(up_in, up_out, 2, stride=2)\n        self.bn = nn.BatchNorm2d(n_out)\n        \n    def forward(self, up_p, x_p):\n        up_p = self.tr_conv(up_p)\n        x_p = self.x_conv(x_p)\n        cat_p = torch.cat([up_p,x_p], dim=1)\n        return self.bn(F.relu(cat_p))\n    \nclass RotUnet34(nn.Module):\n    def __init__(self, rn):\n        super().__init__()\n        self.rn = rn\n        self.sfs = [SaveFeatures(rn[i]) for i in [2,4,5,6]]\n        self.up1 = RotUnetBlock(512,256,256)\n        self.up2 = RotUnetBlock(256,128,256)\n        self.up3 = RotUnetBlock(256,64,256)\n        self.up4 = RotUnetBlock(256,64,256)\n        self.up5 = nn.ConvTranspose2d(256, 1, 2, stride=2)\n        \n    def forward(self,x):\n        x = F.relu(self.rn(x))\n        x = self.up1(x, self.sfs[3].features)\n        x = self.up2(x, self.sfs[2].features)\n        x = self.up3(x, self.sfs[1].features)\n        x = self.up4(x, self.sfs[0].features)\n        x = self.up5(x)\n        return x[:,0]\n    \n    def close(self):\n        for sf in self.sfs: sf.remove()\n            \nclass RotUnetModel():\n    def __init__(self,model,name='Rot_Unet'):\n        self.model,self.name = model,name\n\n    def get_layer_groups(self, precompute):\n        lgs = list(split_by_idxs(children(self.model.rn), [lr_cut]))\n        return lgs + [children(self.model)[1:]]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sz = 768 #image size\nbs = 8  #batch size\n\nno_aug_md = get_data_no_aug(sz,bs)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m_base = get_base()\nrot_m = to_gpu(RotUnet34(m_base))\nrot_models = RotUnetModel(rot_m)\n\n\nlearn = ConvLearner(md, rot_models)\nlearn.opt_fn=optim.Adam\nlearn.crit = MixedLoss(10.0, 2.0)\nlearn.metrics=[accuracy_thresh(0.5),dice,IoU]\nwd=1e-7\nlr = 1e-2\nlearn.freeze_to(1)\nlearn.fit(lr,1,wds=wd,cycle_len=1,use_clr=(5,8))","metadata":{},"execution_count":null,"outputs":[]}]}