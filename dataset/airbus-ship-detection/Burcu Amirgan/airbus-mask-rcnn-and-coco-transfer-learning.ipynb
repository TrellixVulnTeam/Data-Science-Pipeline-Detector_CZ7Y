{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Mask-RCNN Starter Model for the Airbus Ship Detection Challenge with transfer learning **\n\nUsing pre-trained COCO weights trained on http://cocodataset.org as in https://github.com/matterport/Mask_RCNN/tree/master/samples/balloon\n\nWe get some amazing performance training only within the 6hrs kaggle kernel limit.","metadata":{"id":"KBeAf8WgaeSk","_uuid":"7c1fce19a11f95416168ced03c2c70fa818b21a5"}},{"cell_type":"code","source":"debug = False\ndebug = True","metadata":{"_uuid":"cdb40bf9115f53810c9e13f0a50e53ed9eb6221b","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-11-26T06:51:13.64888Z","iopub.execute_input":"2021-11-26T06:51:13.64912Z","iopub.status.idle":"2021-11-26T06:51:13.668066Z","shell.execute_reply.started":"2021-11-26T06:51:13.649063Z","shell.execute_reply":"2021-11-26T06:51:13.667487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os \nimport sys\nimport random\nimport math\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport json\nfrom imgaug import augmenters as iaa\nfrom tqdm import tqdm\nimport pandas as pd \nimport glob ","metadata":{"id":"4kjcC6QqywWl","_uuid":"40c67b3ff0fa04587dec508363308adaa3ceaf34","execution":{"iopub.status.busy":"2021-11-26T06:51:13.669414Z","iopub.execute_input":"2021-11-26T06:51:13.669692Z","iopub.status.idle":"2021-11-26T06:51:14.672706Z","shell.execute_reply.started":"2021-11-26T06:51:13.669649Z","shell.execute_reply":"2021-11-26T06:51:14.671985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_DIR = '/kaggle/input/airbus-ship-detection'\n\n# Directory to save logs and trained model\nROOT_DIR = '/kaggle/working'","metadata":{"id":"yP0XLJx_x_6o","_uuid":"6e5764759e6a0a9b698b44645658f66873edd807","execution":{"iopub.status.busy":"2021-11-26T06:51:14.674806Z","iopub.execute_input":"2021-11-26T06:51:14.675087Z","iopub.status.idle":"2021-11-26T06:51:14.679821Z","shell.execute_reply.started":"2021-11-26T06:51:14.675042Z","shell.execute_reply":"2021-11-26T06:51:14.678011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Install Matterport's Mask-RCNN model from github.\nSee the [Matterport's implementation of Mask-RCNN](https://github.com/matterport/Mask_RCNN).","metadata":{"id":"kdYzLq1zfKL4","_uuid":"576df4c47a23d08b1bdb384245e09aa69f88bbd3"}},{"cell_type":"code","source":"!git clone https://www.github.com/matterport/Mask_RCNN.git\nos.chdir('Mask_RCNN')\n#!python setup.py -q install","metadata":{"id":"KgllzLnDr7kF","outputId":"6c978df7-2013-437e-acd1-5011048dfb53","_uuid":"b37d22551d332f0f7b722cc7204eb614524b6c21","execution":{"iopub.status.busy":"2021-11-26T06:51:14.681617Z","iopub.execute_input":"2021-11-26T06:51:14.682025Z","iopub.status.idle":"2021-11-26T06:51:20.532236Z","shell.execute_reply.started":"2021-11-26T06:51:14.681979Z","shell.execute_reply":"2021-11-26T06:51:20.531093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import Mask RCNN\nsys.path.append(os.path.join(ROOT_DIR, 'Mask_RCNN'))  # To find local version of the library\nfrom mrcnn.config import Config\nfrom mrcnn import utils\nimport mrcnn.model as modellib\nfrom mrcnn import visualize\nfrom mrcnn.model import log","metadata":{"id":"-KZXyWwhzOVU","outputId":"2576cc17-7484-4311-ad72-3c5643dcb5bb","_uuid":"3acbbbe055b6a409d3c50ae0f893acf51b5ae7ba","execution":{"iopub.status.busy":"2021-11-26T06:51:20.533675Z","iopub.execute_input":"2021-11-26T06:51:20.534013Z","iopub.status.idle":"2021-11-26T06:51:20.913008Z","shell.execute_reply.started":"2021-11-26T06:51:20.533957Z","shell.execute_reply":"2021-11-26T06:51:20.912359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dicom_dir = os.path.join(DATA_DIR, 'train_v2')\ntest_dicom_dir = os.path.join(DATA_DIR, 'test_v2')","metadata":{"id":"FghMmiMjzOX2","_uuid":"50089cc61791871cdf6a5c0037dc4f28b7b7d7cc","execution":{"iopub.status.busy":"2021-11-26T06:51:20.913848Z","iopub.execute_input":"2021-11-26T06:51:20.914074Z","iopub.status.idle":"2021-11-26T06:51:20.921116Z","shell.execute_reply.started":"2021-11-26T06:51:20.914033Z","shell.execute_reply":"2021-11-26T06:51:20.920384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Download COCO pre-trained weights","metadata":{"_uuid":"f108beef7838be8a64dd512d395c5dc0ad952790"}},{"cell_type":"code","source":"!wget --quiet https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5\n!ls -lh mask_rcnn_coco.h5\n\nCOCO_WEIGHTS_PATH = \"mask_rcnn_coco.h5\"","metadata":{"_uuid":"c3ee0cd0ee0b1defdec97b94bc736587c1f7631f","execution":{"iopub.status.busy":"2021-11-26T06:51:20.922037Z","iopub.execute_input":"2021-11-26T06:51:20.922278Z","iopub.status.idle":"2021-11-26T06:51:25.98822Z","shell.execute_reply.started":"2021-11-26T06:51:20.922237Z","shell.execute_reply":"2021-11-26T06:51:25.987164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Some setup functions and classes for Mask-RCNN\n\n- dicom_fps is a list of the dicom image path and filenames \n- image_annotions is a dictionary of the annotations keyed by the filenames\n- parsing the dataset returns a list of the image filenames and the annotations dictionary","metadata":{"id":"gj-tvDvEaDiC","_uuid":"032cc5fe4baa051108106675e6ca4f4fdb2846ed"}},{"cell_type":"code","source":"# The following parameters have been selected to reduce running time for demonstration purposes \n# These are not optimal \n\nclass DetectorConfig(Config):    \n    # Give the configuration a recognizable name  \n    NAME = 'airbus'\n    \n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 9\n    \n    BACKBONE = 'resnet50'\n    \n    NUM_CLASSES = 2  # background and ship classes\n    \n    IMAGE_MIN_DIM = 384\n    IMAGE_MAX_DIM = 384\n    RPN_ANCHOR_SCALES = (8, 16, 32, 64)\n    TRAIN_ROIS_PER_IMAGE = 64\n    MAX_GT_INSTANCES = 14\n    DETECTION_MAX_INSTANCES = 10\n    DETECTION_MIN_CONFIDENCE = 0.95\n    DETECTION_NMS_THRESHOLD = 0.0\n    \n    STEPS_PER_EPOCH = 15 if debug else 150\n    VALIDATION_STEPS = 10 if debug else 125\n    \n    ## balance out losses\n    LOSS_WEIGHTS = {\n        \"rpn_class_loss\": 30.0,\n        \"rpn_bbox_loss\": 0.8,\n        \"mrcnn_class_loss\": 6.0,\n        \"mrcnn_bbox_loss\": 1.0,\n        \"mrcnn_mask_loss\": 1.2\n    }\n\nconfig = DetectorConfig()\nconfig.display()","metadata":{"id":"_SfzTa-1zOck","outputId":"91ae8935-bccb-4b8e-9a7e-aa690f95fd9b","_uuid":"dfcffc4eaa94a41497717851dee9f702d8a2a73b","execution":{"iopub.status.busy":"2021-11-26T06:51:25.995075Z","iopub.execute_input":"2021-11-26T06:51:25.999855Z","iopub.status.idle":"2021-11-26T06:51:26.023372Z","shell.execute_reply.started":"2021-11-26T06:51:25.999799Z","shell.execute_reply":"2021-11-26T06:51:26.022563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom skimage.io import imread\nimport matplotlib.pyplot as plt\nfrom matplotlib.cm import get_cmap\nfrom skimage.segmentation import mark_boundaries\nfrom skimage.util import montage\nfrom skimage.morphology import binary_opening, disk, label\nimport gc; gc.enable() # memory is tight","metadata":{"execution":{"iopub.status.busy":"2021-11-26T06:51:26.029103Z","iopub.execute_input":"2021-11-26T06:51:26.031064Z","iopub.status.idle":"2021-11-26T06:51:26.040278Z","shell.execute_reply.started":"2021-11-26T06:51:26.031014Z","shell.execute_reply":"2021-11-26T06:51:26.03955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train: 192556 (768,768,3)**  \n**Test :  15606 (768,768,3)**","metadata":{}},{"cell_type":"code","source":"montage_rgb = lambda x: np.stack([montage(x[:, :, :, i]) for i in range(x.shape[3])], -1) # to rgb function\n\ndef multi_rle_encode(img, **kwargs):\n    '''\n    Encode connected regions as separated masks\n    '''\n    labels = label(img)\n    if img.ndim > 2:\n        return [rle_encode(np.sum(labels==k, axis=2), **kwargs) for k in np.unique(labels[labels>0])]\n    else:\n        return [rle_encode(labels==k, **kwargs) for k in np.unique(labels[labels>0])]\n\n# ref: https://www.kaggle.com/paulorzp/run-length-encode-and-decode\ndef rle_encode(img, min_max_threshold=1e-3, max_mean_threshold=None): # raster to rle coding \n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    if np.max(img) < min_max_threshold:\n        return '' ## no need to encode if it's all zeros\n    if max_mean_threshold and np.mean(img) > max_mean_threshold:\n        return '' ## ignore overfilled mask\n    pixels = img.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\ndef rle_decode(mask_rle, shape=(768, 768)): # rle coding to raster\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height,width) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape).T  # Needed to align to RLE direction\n\ndef masks_as_image(in_mask_list):\n    # Take the individual ship masks and create a single mask array for all ships\n    all_masks = np.zeros((768, 768), dtype = np.uint8)\n    for mask in in_mask_list:\n        if isinstance(mask, str):\n            all_masks |= rle_decode(mask)\n    return all_masks\n\ndef masks_as_color(in_mask_list):\n    # Take the individual ship masks and create a color mask array for each ships\n    all_masks = np.zeros((768, 768), dtype = np.float)\n    scale = lambda x: (len(in_mask_list)+x+1) / (len(in_mask_list)*2) ## scale the heatmap image to shift \n    for i,mask in enumerate(in_mask_list):\n        if isinstance(mask, str):\n            all_masks[:,:] += scale(i) * rle_decode(mask)\n    return all_masks","metadata":{"_uuid":"6136132b1f1b311e297d9432772ec4a81230924f","execution":{"iopub.status.busy":"2021-11-26T06:51:26.043695Z","iopub.execute_input":"2021-11-26T06:51:26.046207Z","iopub.status.idle":"2021-11-26T06:51:26.076434Z","shell.execute_reply.started":"2021-11-26T06:51:26.04605Z","shell.execute_reply":"2021-11-26T06:51:26.075612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nfrom sklearn.model_selection import train_test_split\n\n# hatalı resim listesi\nexclude_list = ['6384c3e78.jpg','13703f040.jpg', '14715c06d.jpg',  '33e0ff2d5.jpg',\n                '4d4e09f2a.jpg', '877691df8.jpg', '8b909bb20.jpg', 'a8d99130e.jpg', \n                'ad55c3143.jpg', 'c8260c541.jpg', 'd6c7f17c7.jpg', 'dc3e7c901.jpg',\n                'e44dffe88.jpg', 'ef87bad36.jpg', 'f083256d8.jpg'] #corrupted images\n\ntrain_names = [f for f in os.listdir(train_dicom_dir) if f not in exclude_list]\ntest_names = [f for f in os.listdir(test_dicom_dir) if f not in exclude_list]\n\nprint(len(train_names), len(test_names))","metadata":{"_uuid":"d3e05fa1a38c637fa228acd62b92dd41117a6672","execution":{"iopub.status.busy":"2021-11-26T06:51:26.077756Z","iopub.execute_input":"2021-11-26T06:51:26.078328Z","iopub.status.idle":"2021-11-26T06:51:29.624158Z","shell.execute_reply.started":"2021-11-26T06:51:26.078281Z","shell.execute_reply":"2021-11-26T06:51:29.623298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training dataset\nSEGMENTATION = DATA_DIR + '/train_ship_segmentations_v2.csv'\nanns = pd.read_csv(SEGMENTATION)\nanns.head()","metadata":{"_uuid":"3050fa77026411ffdc27bed4a9b667ec0467e4ce","execution":{"iopub.status.busy":"2021-11-26T06:51:29.625147Z","iopub.execute_input":"2021-11-26T06:51:29.625399Z","iopub.status.idle":"2021-11-26T06:51:31.0922Z","shell.execute_reply.started":"2021-11-26T06:51:29.625353Z","shell.execute_reply":"2021-11-26T06:51:31.091562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_names = anns[anns.EncodedPixels.notnull()].ImageId.unique().tolist()  ## override with ships gemi içeren uniq görüntü listesi\nprint(\"Uniq image list containing ships:\",np.shape(train_names))\n\ntest_size = config.VALIDATION_STEPS * config.IMAGES_PER_GPU # 10*9\nimage_fps_train, image_fps_val = train_test_split(train_names, test_size=test_size, random_state=42)\n\nif debug:\n    image_fps_train = image_fps_train[:100]\n    image_fps_val = image_fps_val[:100]\n    test_names = test_names[:100]\n    \nprint(\"train:\",len(image_fps_train), \"val:\",len(image_fps_val), \"test:\",len(test_names))","metadata":{"_uuid":"904636402355a305f7b2ccacb8cc55d52151d2e6","execution":{"iopub.status.busy":"2021-11-26T06:51:31.093177Z","iopub.execute_input":"2021-11-26T06:51:31.093415Z","iopub.status.idle":"2021-11-26T06:51:31.15282Z","shell.execute_reply.started":"2021-11-26T06:51:31.093373Z","shell.execute_reply":"2021-11-26T06:51:31.152158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DetectorDataset(utils.Dataset):\n    \"\"\"Dataset class for training our dataset.\n    \"\"\"\n\n    def __init__(self, image_fps, image_annotations, orig_height, orig_width):\n        super().__init__(self)\n        \n        # Add classes\n        self.add_class('ship', 1, 'Ship')\n        \n        # add images \n        for i, fp in enumerate(image_fps):\n            annotations = image_annotations.query('ImageId==\"' + fp + '\"')['EncodedPixels']\n            self.add_image('ship', image_id=i, path=os.path.join(train_dicom_dir, fp), \n                           annotations=annotations, orig_height=orig_height, orig_width=orig_width)\n            \n    def image_reference(self, image_id):\n        info = self.image_info[image_id]\n        return info['path']\n\n    def load_image(self, image_id):\n        info = self.image_info[image_id]\n        fp = info['path']\n        image = imread(fp)\n        # If grayscale. Convert to RGB for consistency.\n        if len(image.shape) != 3 or image.shape[2] != 3:\n            image = np.stack((image,) * 3, -1)\n        return image\n\n    def load_mask(self, image_id):\n        info = self.image_info[image_id]\n        annotations = info['annotations']\n#         print(image_id, annotations)\n        count = len(annotations)\n        if count == 0:\n            mask = np.zeros((info['orig_height'], info['orig_width'], 1), dtype=np.uint8)\n            class_ids = np.zeros((1,), dtype=np.int32)\n        else:\n            mask = np.zeros((info['orig_height'], info['orig_width'], count), dtype=np.uint8)\n            class_ids = np.zeros((count,), dtype=np.int32)\n            for i, a in enumerate(annotations):\n                mask[:, :, i] = rle_decode(a)\n                class_ids[i] = 1\n        return mask.astype(np.bool), class_ids.astype(np.int32)","metadata":{"id":"8EBVA1M60yAj","_uuid":"52bd3ffbdde0173a363055482d675da51c2aba99","execution":{"iopub.status.busy":"2021-11-26T06:51:31.153678Z","iopub.execute_input":"2021-11-26T06:51:31.153914Z","iopub.status.idle":"2021-11-26T06:51:31.169641Z","shell.execute_reply.started":"2021-11-26T06:51:31.153871Z","shell.execute_reply":"2021-11-26T06:51:31.168803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Examine the annotation data, parse the dataset, and view dicom fields","metadata":{"id":"9RlMo04ckd98","_uuid":"1cb852e262b69d348743767d675573368ab672c9"}},{"cell_type":"code","source":"image_fps, image_annotations = train_names, anns\nprint(\"Uniq image list containing ships:\",np.shape(image_fps))\nprint(\"Train ship segmentations csv file:\",np.shape(image_annotations))","metadata":{"id":"Mxz-pNbt5txY","_uuid":"7aebc88f910b232e3b8759421914a007c6ffed94","execution":{"iopub.status.busy":"2021-11-26T06:51:31.170928Z","iopub.execute_input":"2021-11-26T06:51:31.171499Z","iopub.status.idle":"2021-11-26T06:51:31.187047Z","shell.execute_reply.started":"2021-11-26T06:51:31.17143Z","shell.execute_reply":"2021-11-26T06:51:31.186115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds = imread(os.path.join(train_dicom_dir, image_fps[0])) # read  image from filepath \nplt.imshow(ds)\n\n# Original image size: 768 x 768\nORIG_SIZE = ds.shape[0]\nprint(\"Image original size:\",ORIG_SIZE)","metadata":{"id":"YPqjEIXWRhSf","_uuid":"6c386dcef041b972f6209dd19e247d547c3c349f","execution":{"iopub.status.busy":"2021-11-26T06:51:31.188561Z","iopub.execute_input":"2021-11-26T06:51:31.189024Z","iopub.status.idle":"2021-11-26T06:51:31.508254Z","shell.execute_reply.started":"2021-11-26T06:51:31.188829Z","shell.execute_reply":"2021-11-26T06:51:31.507301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create and prepare the training dataset using the DetectorDataset class.","metadata":{"id":"9KUvacUbgiEX","_uuid":"a5143c19dc22bc00d318a3b28cb7e13c7fbacc8a"}},{"cell_type":"code","source":"%%time\n# prepare the training dataset\ndataset_train = DetectorDataset(image_fps_train, image_annotations, ORIG_SIZE, ORIG_SIZE)\ndataset_train.prepare()","metadata":{"id":"jwMkhotP0yFf","_uuid":"86c3333d4dfb8b7d00ce1f401693d0df4e6254e1","execution":{"iopub.status.busy":"2021-11-26T06:51:31.509576Z","iopub.execute_input":"2021-11-26T06:51:31.513359Z","iopub.status.idle":"2021-11-26T06:51:32.428434Z","shell.execute_reply.started":"2021-11-26T06:51:31.513298Z","shell.execute_reply":"2021-11-26T06:51:32.426908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# prepare the validation dataset\ndataset_val = DetectorDataset(image_fps_val, image_annotations, ORIG_SIZE, ORIG_SIZE)\ndataset_val.prepare()","metadata":{"id":"K1TkWuGP0yHl","_uuid":"313347d838fa8321a714858c8073f98c50c5be26","execution":{"iopub.status.busy":"2021-11-26T06:51:32.429687Z","iopub.execute_input":"2021-11-26T06:51:32.430026Z","iopub.status.idle":"2021-11-26T06:51:33.245002Z","shell.execute_reply.started":"2021-11-26T06:51:32.429971Z","shell.execute_reply":"2021-11-26T06:51:33.244109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Display a random image with bounding boxes","metadata":{"id":"pEXEt8fygWuC","_uuid":"600a8135d4e382f62797d69e9358f5697873c8f9"}},{"cell_type":"code","source":"# Load and display random sample and their bounding boxes\n\nclass_ids = [0]\nwhile class_ids[0] == 0:  ## look for a mask\n    image_id = random.choice(dataset_val.image_ids) # random image id 0-90\n    image_fp = dataset_val.image_reference(image_id) # current image\n    image = dataset_val.load_image(image_id) # load image\n    mask, class_ids = dataset_val.load_mask(image_id)\nprint(\"image shape:\",image.shape)\nprint(\"mask shape :\",mask.shape)\nprint(\"image file:\",image_fp, \"class_id:\",class_ids)\n\nplt.figure(figsize=(10, 10))\nplt.subplot(1, 2, 1)\nplt.imshow(image)\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nmasked = np.zeros(image.shape[:2])\nfor i in range(mask.shape[2]):\n    masked += mask[:, :, i] ## * image[:, :, 0]\nplt.imshow(masked, cmap='gray')\nplt.axis('off')","metadata":{"id":"4xwsrf9G1lHR","outputId":"a13386d3-a918-41fe-8824-13625c9d7b08","_uuid":"491b78ec96d28fcdbbf8e2d7f9320a05d64c9249","execution":{"iopub.status.busy":"2021-11-26T06:51:33.248173Z","iopub.execute_input":"2021-11-26T06:51:33.250427Z","iopub.status.idle":"2021-11-26T06:51:33.767089Z","shell.execute_reply.started":"2021-11-26T06:51:33.250369Z","shell.execute_reply":"2021-11-26T06:51:33.766259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Image Augmentation. Try finetuning some variables to custom values","metadata":{"id":"ustAIH78hZI_","_uuid":"342b6008873fe7a6a0870a712ee47a87f0d2828d"}},{"cell_type":"code","source":"# Image augmentation (light but constant)\naugmentation = iaa.Sequential([\n    iaa.OneOf([ ## rotate\n        iaa.Affine(rotate=0),\n        iaa.Affine(rotate=90),\n        iaa.Affine(rotate=180),\n        iaa.Affine(rotate=270),\n    ]),\n    iaa.Fliplr(0.5),\n    iaa.Flipud(0.5),\n    iaa.OneOf([ ## brightness or contrast\n        iaa.Multiply((0.9, 1.1)),\n        iaa.ContrastNormalization((0.9, 1.1)),\n    ]),\n    iaa.OneOf([ ## blur or sharpen\n        iaa.GaussianBlur(sigma=(0.0, 0.1)),\n        iaa.Sharpen(alpha=(0.0, 0.1)),\n    ]),\n])\n\n# test on the same image as above\nimggrid = augmentation.draw_grid(image, cols=4, rows=2)\nplt.figure(figsize=(15, 15))\nplt.imshow(imggrid.astype(int))","metadata":{"id":"STZnQTE61lME","_uuid":"4ab9d6086ce611a46f189c047956c43b29783e6d","execution":{"iopub.status.busy":"2021-11-26T06:51:33.769969Z","iopub.execute_input":"2021-11-26T06:51:33.770226Z","iopub.status.idle":"2021-11-26T06:51:35.211118Z","shell.execute_reply.started":"2021-11-26T06:51:33.770179Z","shell.execute_reply":"2021-11-26T06:51:35.210357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now it's time to train the model. Note that training even a basic model can take a few hours. \n\nNote: the following model is for demonstration purpose only. We have limited the training to one epoch, and have set nominal values for the Detector Configuration to reduce run-time. \n\n- dataset_train and dataset_val are derived from DetectorDataset \n- DetectorDataset loads images from image filenames and  masks from the annotation data\n- model is Mask-RCNN","metadata":{"id":"M4kt7LKuc78e","_uuid":"7e65d2cecb283f446f34cdde19b663a8a8e9590f"}},{"cell_type":"code","source":"model = modellib.MaskRCNN(mode='training', config=config, model_dir=ROOT_DIR) #  ROOT_DIR:output path\n\n# Exclude the last layers because they require a matching\n# number of classes\nmodel.load_weights(COCO_WEIGHTS_PATH, by_name=True, exclude=[\n    \"mrcnn_class_logits\", \"mrcnn_bbox_fc\",\n    \"mrcnn_bbox\", \"mrcnn_mask\"])","metadata":{"_uuid":"138d6197fc8dce9f1f8a7b5a6c27aa2069698e03","execution":{"iopub.status.busy":"2021-11-26T06:51:35.212145Z","iopub.execute_input":"2021-11-26T06:51:35.212507Z","iopub.status.idle":"2021-11-26T06:51:46.584717Z","shell.execute_reply.started":"2021-11-26T06:51:35.212466Z","shell.execute_reply":"2021-11-26T06:51:46.583932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LEARNING_RATE = 0.003\n\n# Train Mask-RCNN Model \nimport warnings \nwarnings.filterwarnings(\"ignore\")","metadata":{"id":"RVgNhHjl1lOS","outputId":"2cba9efc-eeea-472d-d155-3c3d856585bf","_uuid":"64cce2581ffdb8c2b1cb07948ada4a93f64874b0","execution":{"iopub.status.busy":"2021-11-26T06:51:46.585704Z","iopub.execute_input":"2021-11-26T06:51:46.585945Z","iopub.status.idle":"2021-11-26T06:51:46.592692Z","shell.execute_reply.started":"2021-11-26T06:51:46.585902Z","shell.execute_reply":"2021-11-26T06:51:46.591903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n## train heads with higher lr to speedup the learning  2*LEARNING_RATE\nmodel.train(dataset_train, dataset_val,\n            learning_rate=LEARNING_RATE*2,\n            epochs=2,\n            layers='heads',\n            augmentation=None)  ## no need to augment yet\n\nhistory = model.keras_model.history.history","metadata":{"_uuid":"cf339a499519d174bcdf2311a1802f0e3acb1758","execution":{"iopub.status.busy":"2021-11-26T06:51:46.593446Z","iopub.execute_input":"2021-11-26T06:51:46.593688Z","iopub.status.idle":"2021-11-26T06:56:52.456871Z","shell.execute_reply.started":"2021-11-26T06:51:46.593646Z","shell.execute_reply":"2021-11-26T06:56:52.451737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n # LEARNING_RATE\nmodel.train(dataset_train, dataset_val,\n            learning_rate=LEARNING_RATE,\n            epochs=4 if debug else 14,\n            layers='all',\n            augmentation=augmentation)\n\nnew_history = model.keras_model.history.history\nfor k in new_history: history[k] = history[k] + new_history[k]","metadata":{"_uuid":"8004790d27f041793562e994bbe95edf67f8978b","execution":{"iopub.status.busy":"2021-11-26T06:56:52.458995Z","iopub.execute_input":"2021-11-26T06:56:52.459359Z","iopub.status.idle":"2021-11-26T07:02:02.660983Z","shell.execute_reply.started":"2021-11-26T06:56:52.459291Z","shell.execute_reply":"2021-11-26T07:02:02.656973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n### LEARNING_RATE/2 fine tuning işleminde overfiting olmaması için lr traine göre düşürülür.\nmodel.train(dataset_train, dataset_val,\n            learning_rate=LEARNING_RATE/2,\n            epochs=6 if debug else 22,\n            layers='all',\n            augmentation=augmentation)\n\nnew_history = model.keras_model.history.history\nfor k in new_history: history[k] = history[k] + new_history[k]","metadata":{"_uuid":"e0f55437aaa49e58ae60225a035fa8a3f6b604d3","execution":{"iopub.status.busy":"2021-11-26T07:02:02.663496Z","iopub.execute_input":"2021-11-26T07:02:02.667352Z","iopub.status.idle":"2021-11-26T07:07:04.518922Z","shell.execute_reply.started":"2021-11-26T07:02:02.663793Z","shell.execute_reply":"2021-11-26T07:07:04.51769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = range(1, len(history['loss'])+1)\npd.DataFrame(history, index=epochs)","metadata":{"_uuid":"71abf32327a102e1c22e944b24d98690c71d9560","execution":{"iopub.status.busy":"2021-11-26T07:07:04.520805Z","iopub.execute_input":"2021-11-26T07:07:04.521139Z","iopub.status.idle":"2021-11-26T07:07:04.627864Z","shell.execute_reply.started":"2021-11-26T07:07:04.521073Z","shell.execute_reply":"2021-11-26T07:07:04.626225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(21,11))\n\nplt.subplot(231)\nplt.plot(epochs, history[\"loss\"], label=\"Train loss\")\nplt.plot(epochs, history[\"val_loss\"], label=\"Valid loss\")\nplt.legend()\nplt.subplot(232)\nplt.plot(epochs, history[\"rpn_class_loss\"], label=\"Train RPN class ce\")\nplt.plot(epochs, history[\"val_rpn_class_loss\"], label=\"Valid RPN class ce\")\nplt.legend()\nplt.subplot(233)\nplt.plot(epochs, history[\"rpn_bbox_loss\"], label=\"Train RPN box loss\")\nplt.plot(epochs, history[\"val_rpn_bbox_loss\"], label=\"Valid RPN box loss\")\nplt.legend()\nplt.subplot(234)\nplt.plot(epochs, history[\"mrcnn_class_loss\"], label=\"Train MRCNN class ce\")\nplt.plot(epochs, history[\"val_mrcnn_class_loss\"], label=\"Valid MRCNN class ce\")\nplt.legend()\nplt.subplot(235)\nplt.plot(epochs, history[\"mrcnn_bbox_loss\"], label=\"Train MRCNN box loss\")\nplt.plot(epochs, history[\"val_mrcnn_bbox_loss\"], label=\"Valid MRCNN box loss\")\nplt.legend()\nplt.subplot(236)\nplt.plot(epochs, history[\"mrcnn_mask_loss\"], label=\"Train Mask loss\")\nplt.plot(epochs, history[\"val_mrcnn_mask_loss\"], label=\"Valid Mask loss\")\nplt.legend()\n\nplt.show()","metadata":{"_uuid":"fb3b69242b91dcc49697ff076ceeb957347372e1","execution":{"iopub.status.busy":"2021-11-26T07:07:04.629161Z","iopub.execute_input":"2021-11-26T07:07:04.62945Z","iopub.status.idle":"2021-11-26T07:07:05.688765Z","shell.execute_reply.started":"2021-11-26T07:07:04.629404Z","shell.execute_reply":"2021-11-26T07:07:05.687988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_epoch = np.argmin(history[\"val_loss\"])\nscore = history[\"val_loss\"][best_epoch]\nprint(f'Best Epoch:{best_epoch+1} val_loss:{score}')","metadata":{"_uuid":"5c2b38ecbc84575295dd62657ed175c5a0b72021","execution":{"iopub.status.busy":"2021-11-26T07:07:05.689958Z","iopub.execute_input":"2021-11-26T07:07:05.690471Z","iopub.status.idle":"2021-11-26T07:07:05.69899Z","shell.execute_reply.started":"2021-11-26T07:07:05.69042Z","shell.execute_reply":"2021-11-26T07:07:05.698286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# select trained model \ndir_names = next(os.walk(model.model_dir))[1]\nkey = config.NAME.lower()\ndir_names = filter(lambda f: f.startswith(key), dir_names)\ndir_names = sorted(dir_names)\n\nif not dir_names:\n    import errno\n    raise FileNotFoundError(\n        errno.ENOENT,\n        \"Could not find model directory under {}\".format(self.model_dir))\n\nfps = []\n# Pick last directory\nfor d in dir_names: \n    dir_name = os.path.join(model.model_dir, d)\n    # Find the last checkpoint\n    checkpoints = next(os.walk(dir_name))[2]\n    checkpoints = filter(lambda f: f.startswith(\"mask_rcnn\"), checkpoints)\n    checkpoints = sorted(checkpoints)\n    if not checkpoints:\n        print('No weight files in {}'.format(dir_name))\n    else:\n        checkpoint = os.path.join(dir_name, checkpoints[best_epoch])\n        fps.append(checkpoint)\n\nmodel_path = sorted(fps)[-1]\nprint('Found model {}'.format(model_path))","metadata":{"id":"eraRlzgPmmIZ","outputId":"de9e688c-ba4f-4b62-f842-dbcf00ce397c","_uuid":"db5c10d3f7da099e5751a04a6e6d49819882ecd4","execution":{"iopub.status.busy":"2021-11-26T07:07:05.706866Z","iopub.execute_input":"2021-11-26T07:07:05.707309Z","iopub.status.idle":"2021-11-26T07:07:05.720818Z","shell.execute_reply.started":"2021-11-26T07:07:05.707114Z","shell.execute_reply":"2021-11-26T07:07:05.719896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class InferenceConfig(DetectorConfig):\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 1\n\ninference_config = InferenceConfig()\n\n# Recreate the model in inference mode\nmodel = modellib.MaskRCNN(mode='inference', \n                          config=inference_config,\n                          model_dir=ROOT_DIR)\n\n# Load trained weights (fill in path to trained weights here)\nassert model_path != \"\", \"Provide path to trained weights\"\nprint(\"Loading weights from \", model_path)\nmodel.load_weights(model_path, by_name=True)","metadata":{"id":"TgpT9AzC2Bgz","outputId":"60f5a175-4666-497d-b4e8-0bdab39a92d0","_uuid":"52138636b2ae5bf444bba808518cd8313bde65cd","execution":{"iopub.status.busy":"2021-11-26T07:07:05.722899Z","iopub.execute_input":"2021-11-26T07:07:05.723449Z","iopub.status.idle":"2021-11-26T07:07:15.895839Z","shell.execute_reply.started":"2021-11-26T07:07:05.723242Z","shell.execute_reply":"2021-11-26T07:07:15.89504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set color for class\ndef get_colors_for_class_ids(class_ids):\n    colors = []\n    for class_id in class_ids:\n        if class_id == 1:\n            colors.append((.941, .204, .204))\n    return colors","metadata":{"id":"9mTBig7D2BjU","_uuid":"e13c61bee23b791c61ecf1256f7512295cd4d9ab","execution":{"iopub.status.busy":"2021-11-26T07:07:15.896786Z","iopub.execute_input":"2021-11-26T07:07:15.897045Z","iopub.status.idle":"2021-11-26T07:07:15.903438Z","shell.execute_reply.started":"2021-11-26T07:07:15.896998Z","shell.execute_reply":"2021-11-26T07:07:15.902544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### How does the predicted box compared to the expected value? Let's use the validation dataset to check. ","metadata":{"id":"A8EiL2LOiCr_","_uuid":"f99fbd3f31ff1a2bd66764835c9b646375364598"}},{"cell_type":"code","source":"# Show few example of ground truth vs. predictions on the validation dataset \ndataset = dataset_val\nfig = plt.figure(figsize=(10, 40))\n\nfor i in range(8):\n\n    image_id = random.choice(dataset.image_ids)\n    \n    original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n        modellib.load_image_gt(dataset_val, inference_config, \n                               image_id, use_mini_mask=False)\n    \n#     print(original_image.shape)\n    plt.subplot(8, 2, 2*i + 1)\n    visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n                                dataset.class_names,\n                                colors=get_colors_for_class_ids(gt_class_id), ax=fig.axes[-1])\n    \n    plt.subplot(8, 2, 2*i + 2)\n    results = model.detect([original_image]) #, verbose=1)\n    r = results[0]\n    visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n                                dataset.class_names, r['scores'], \n                                colors=get_colors_for_class_ids(r['class_ids']), ax=fig.axes[-1])","metadata":{"id":"irheTbrW2Bl0","outputId":"56041ad4-173d-45ab-af67-f54e8333511e","_uuid":"186412199e25b98719f71cfe5e8869abcce516c4","execution":{"iopub.status.busy":"2021-11-26T07:07:15.904681Z","iopub.execute_input":"2021-11-26T07:07:15.905147Z","iopub.status.idle":"2021-11-26T07:07:22.269945Z","shell.execute_reply.started":"2021-11-26T07:07:15.905093Z","shell.execute_reply":"2021-11-26T07:07:22.269177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Final steps - Create the filtered submission file","metadata":{"id":"WcV1cL_aiSc4","_uuid":"164e18701a830bc6c42a791feea13549de37289b"}},{"cell_type":"code","source":"# Get filenames of test dataset images\ntest_image_fps = test_names","metadata":{"_uuid":"5a124f21c2918ac4cb40ce99c852b86ea223d7e4","execution":{"iopub.status.busy":"2021-11-26T07:07:22.27112Z","iopub.execute_input":"2021-11-26T07:07:22.271577Z","iopub.status.idle":"2021-11-26T07:07:22.275822Z","shell.execute_reply.started":"2021-11-26T07:07:22.271506Z","shell.execute_reply":"2021-11-26T07:07:22.275004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DETECTION_TEST_PRED = '/kaggle/input/fine-tuning-resnet34-on-ship-detection-new-data/ship_detection.csv'\nship_detection = pd.read_csv(DETECTION_TEST_PRED, index_col='id')\nship_detection.head()","metadata":{"_uuid":"a5b7afbfbcde9afe9ef3d80263ad7f55a85531f0","execution":{"iopub.status.busy":"2021-11-26T07:07:22.277121Z","iopub.execute_input":"2021-11-26T07:07:22.277608Z","iopub.status.idle":"2021-11-26T07:07:22.331261Z","shell.execute_reply.started":"2021-11-26T07:07:22.277551Z","shell.execute_reply":"2021-11-26T07:07:22.330572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"THRESHOLD = 0.45\ntest_names_nothing = ship_detection.loc[ship_detection['p_ship'] <= THRESHOLD].index.tolist()\nlen(test_names_nothing), len(ship_detection), len(test_names_nothing)/len(ship_detection)","metadata":{"_uuid":"02321d63a0610d91f9dae621002dc7d4cce57034","execution":{"iopub.status.busy":"2021-11-26T07:07:22.332177Z","iopub.execute_input":"2021-11-26T07:07:22.332406Z","iopub.status.idle":"2021-11-26T07:07:22.345872Z","shell.execute_reply.started":"2021-11-26T07:07:22.332361Z","shell.execute_reply":"2021-11-26T07:07:22.344814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions on test images, write out sample submission\ndef predict(image_fps, filepath='submission.csv', min_conf=config.DETECTION_MIN_CONFIDENCE):\n    # assume square image\n    resize_factor = ORIG_SIZE / config.IMAGE_SHAPE[0]\n    #resize_factor = ORIG_SIZE\n    with open(filepath, 'w') as file:\n        file.write(\"ImageId,EncodedPixels\\n\")\n\n        for image_id in tqdm(image_fps):\n            found = False\n            \n            if image_id not in test_names_nothing:\n                image = imread(os.path.join(test_dicom_dir, image_id))\n                # If grayscale. Convert to RGB for consistency.\n                if len(image.shape) != 3 or image.shape[2] != 3:\n                    image = np.stack((image,) * 3, -1)\n#                 image, window, scale, padding, crop = utils.resize_image(\n#                     image,\n#                     min_dim=config.IMAGE_MIN_DIM,\n#                     min_scale=config.IMAGE_MIN_SCALE,\n#                     max_dim=config.IMAGE_MAX_DIM,\n#                     mode=config.IMAGE_RESIZE_MODE)\n\n                results = model.detect([image])\n                r = results[0]\n\n                assert( len(r['rois']) == len(r['class_ids']) == len(r['scores']) )\n                if len(r['rois']) == 0:\n                    pass  ## no ship\n                else:\n                    num_instances = len(r['rois'])\n\n                    for i in range(num_instances):\n                        if r['scores'][i] > min_conf:\n#                             print(r['scores'][i], r['rois'][i], r['masks'].shape, np.sum(r['masks'][...,i]))\n#                             plt.imshow(r['masks'][...,i], cmap=get_cmap('jet'))\n                            file.write(image_id + \",\" + rle_encode(r['masks'][...,i]) + \"\\n\")\n                            found = True\n\n            if not found:\n                file.write(image_id + \",\\n\")  ## no ship","metadata":{"id":"C6UWVrbM2Bob","_uuid":"4a5c0c6134408ddbf5a34496d7e9d7be5692e9a1","execution":{"iopub.status.busy":"2021-11-26T07:07:22.347576Z","iopub.execute_input":"2021-11-26T07:07:22.348231Z","iopub.status.idle":"2021-11-26T07:07:22.36041Z","shell.execute_reply.started":"2021-11-26T07:07:22.348147Z","shell.execute_reply":"2021-11-26T07:07:22.359646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_fp = os.path.join(ROOT_DIR, 'submission.csv')\npredict(test_image_fps, filepath=submission_fp)\nprint(submission_fp)","metadata":{"id":"C5cBpNka2Bsv","outputId":"a2af9176-d9d6-49f6-f22a-5a1c455d144f","_uuid":"0406e7f5aaa4867782c4f9c064f90bba386128e7","execution":{"iopub.status.busy":"2021-11-26T07:07:22.361181Z","iopub.execute_input":"2021-11-26T07:07:22.361411Z","iopub.status.idle":"2021-11-26T07:07:25.622265Z","shell.execute_reply.started":"2021-11-26T07:07:22.361367Z","shell.execute_reply":"2021-11-26T07:07:25.620703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv(submission_fp)\nprint(sub.EncodedPixels.isnull().sum(), sub.ImageId.nunique(), sub.EncodedPixels.isnull().sum()/sub.ImageId.nunique())\nsub.head(50)","metadata":{"id":"_BjPE_Ee9rbA","outputId":"67b5f053-112b-494a-9ab3-d017bfb440c2","_uuid":"3fd8d178fc51ef0bca94fbb3f423160f08a77edc","execution":{"iopub.status.busy":"2021-11-26T07:07:25.623485Z","iopub.execute_input":"2021-11-26T07:07:25.623807Z","iopub.status.idle":"2021-11-26T07:07:25.660763Z","shell.execute_reply.started":"2021-11-26T07:07:25.623758Z","shell.execute_reply":"2021-11-26T07:07:25.659348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show a few test image detection example\ndef visualize_test(): \n    image_id = random.choice(test_names)\n    \n    # original image\n#     print(image_id)\n    image = imread(os.path.join(test_dicom_dir, image_id))\n    \n    # assume square image \n    resize_factor = 1 ## ORIG_SIZE / config.IMAGE_SHAPE[0]\n    \n    # If grayscale. Convert to RGB for consistency.\n    if len(image.shape) != 3 or image.shape[2] != 3:\n        image = np.stack((image,) * 3, -1) \n#     image, window, scale, padding, crop = utils.resize_image(\n#         image,\n#         min_dim=config.IMAGE_MIN_DIM,\n#         min_scale=config.IMAGE_MIN_SCALE,\n#         max_dim=config.IMAGE_MAX_DIM,\n#         mode=config.IMAGE_RESIZE_MODE)\n\n    results = model.detect([image])\n    r = results[0]\n    for bbox in r['rois']: \n#         print(bbox)\n        x1 = int(bbox[1] * resize_factor)\n        y1 = int(bbox[0] * resize_factor)\n        x2 = int(bbox[3] * resize_factor)\n        y2 = int(bbox[2]  * resize_factor)\n        cv2.rectangle(image, (x1,y1), (x2,y2), (77, 255, 9), 3, 1)\n        width = x2 - x1 \n        height = y2 - y1 \n#         print(\"x {} y {} h {} w {}\".format(x1, y1, width, height))\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n    ax1.set_title(f\"{image_id}\")\n    ax1.imshow(image)\n    ax2.set_title(f\"{len(r['rois'])} masks if prob:{ship_detection.loc[image_id][0]:.6f}\")\n    ax2.imshow(masks_as_color(sub.query(f\"ImageId=='{image_id}'\")['EncodedPixels']))\n\nfor i in range(8):\n    visualize_test()","metadata":{"_uuid":"ea110f197abc2acb1c3435383f7259079dc0eb0e","execution":{"iopub.status.busy":"2021-11-26T07:07:25.661921Z","iopub.execute_input":"2021-11-26T07:07:25.662195Z","iopub.status.idle":"2021-11-26T07:07:31.455713Z","shell.execute_reply.started":"2021-11-26T07:07:25.66215Z","shell.execute_reply":"2021-11-26T07:07:31.454815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove files to allow committing (hit files limit otherwise)\n!rm -rf /kaggle/working/Mask_RCNN","metadata":{"_uuid":"835a15c9d018acd5deb16e9e02f9b765f68d0e78","execution":{"iopub.status.busy":"2021-11-26T07:07:31.458858Z","iopub.execute_input":"2021-11-26T07:07:31.461117Z","iopub.status.idle":"2021-11-26T07:07:32.248236Z","shell.execute_reply.started":"2021-11-26T07:07:31.461056Z","shell.execute_reply":"2021-11-26T07:07:32.247238Z"},"trusted":true},"execution_count":null,"outputs":[]}]}