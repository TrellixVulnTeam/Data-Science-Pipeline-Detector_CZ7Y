{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1=pd.read_csv('../input/forest-cover-type-prediction/train.csv')\ndf_test1=pd.read_csv('../input/forest-cover-type-prediction/test.csv')\ndf_test2=pd.read_csv('../input/forest-cover-type-prediction/test3.csv')\ndf=df1.copy()\ndf_test=df_test1.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns',None)\ndf.drop(columns=['Id','Cover_Type'],inplace=True)\ndf_test.drop(columns=['Id'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train=df\nY_train=df1.iloc[:,-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\nfrom lightgbm import LGBMClassifier\n\nsns.set(style='white', context='notebook', palette='deep')\nkfold = StratifiedKFold(n_splits=10)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Comparing all models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nrandom_state = 2\nclassifiers = []\nclassifiers.append(SVC(random_state=random_state))\nclassifiers.append(DecisionTreeClassifier(random_state=random_state))\nclassifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\nclassifiers.append(RandomForestClassifier(random_state=random_state))\nclassifiers.append(ExtraTreesClassifier(random_state=random_state))\nclassifiers.append(GradientBoostingClassifier(random_state=random_state))\nclassifiers.append(MLPClassifier(random_state=random_state))\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(LogisticRegression(random_state = random_state))\nclassifiers.append(LinearDiscriminantAnalysis())\nclassifiers.append(XGBClassifier(random_state = random_state))\nclassifiers.append(LGBMClassifier(random_state = random_state))\n\ncv_results = []\nfor classifier in classifiers :\n    score=cross_val_score(classifier, X_train, y = Y_train, scoring = \"accuracy\", cv = kfold, n_jobs=-1)\n    cv_results.append(score)\n    print('{} crossvalidation score:{}\\n'.format(classifier,score.mean()))\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"SVC\",\"DecisionTree\",\"AdaBoost\",\n\"RandomForest\",\"ExtraTrees\",\"GradientBoosting\",\"MultipleLayerPerceptron\",\"KNeighboors\",\"LogisticRegression\",\"LinearDiscriminantAnalysis\",'XGboost','LGboost']})\n\ng = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Best models are random forst,extra trees, xgboost and lgboost","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nxtrain,xtest,ytrain,ytest=train_test_split(X_train.values,Y_train.values,test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nRFC = RandomForestClassifier(random_state=random_state)\nRFC.fit(xtrain,ytrain)\nypred=RFC.predict(xtest)\nscore=cross_val_score(RFC,X_train,Y_train,scoring='accuracy',cv=kfold,n_jobs=-1)\n\n# Best score\nprint('Crossval score for random forest: {}'.format(score.mean()))\nprint('Accuracy score for random forest: {}'.format(accuracy_score(ytest,ypred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tuned Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"RFC.get_params()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nRFC2 = RandomForestClassifier(random_state=random_state,\n                             n_estimators=500,\n                             max_depth=32,\n                             min_samples_leaf=1,\n                             criterion='entropy')\nRFC2.fit(xtrain,ytrain)\nypred=RFC2.predict(xtest)\nscore=cross_val_score(RFC2,X_train,Y_train,scoring='accuracy',cv=kfold,n_jobs=-1)\n\n# Best score\nprint('Crossval score for random forest: {}'.format(score.mean()))\nprint('Accuracy score for random forest: {}'.format(accuracy_score(ytest,ypred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Extra Trees Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"et=ExtraTreesClassifier(random_state=random_state)\net.fit(xtrain,ytrain)\nypred=et.predict(xtest)\nscore=cross_val_score(et,X_train,Y_train,scoring='accuracy',cv=kfold,n_jobs=-1)\n\n# Best score\nprint('Crossval score for extra trees classifier: {}'.format(score.mean()))\nprint('Accuracy score for extra trees classifier: {}'.format(accuracy_score(ytest,ypred)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tuned extra trees model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"et2=ExtraTreesClassifier()\net2.get_params()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"et2=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n                     criterion='entropy', max_depth=38, max_features='auto',\n                     max_leaf_nodes=None, max_samples=None,\n                     min_impurity_decrease=0.0, min_impurity_split=None,\n                     min_samples_leaf=1, min_samples_split=2,\n                     min_weight_fraction_leaf=0.0, n_estimators=500,\n                     n_jobs=None, oob_score=False, random_state=0, verbose=0,\n                     warm_start=False)\net2.fit(xtrain,ytrain)\nypred=et2.predict(xtest)\nscore=cross_val_score(et2,X_train,Y_train,scoring='accuracy',cv=kfold,n_jobs=-1)\n\n# Best score\nprint('Crossval score for extra trees classifier: {}'.format(score.mean()))\nprint('Accuracy score for extra trees classifier: {}'.format(accuracy_score(ytest,ypred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LightGb Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb2=LGBMClassifier(random_state=random_state)\nlgb2.fit(xtrain,ytrain)\nypred=lgb2.predict(xtest)\nscore=cross_val_score(lgb2,X_train,Y_train,scoring='accuracy',cv=kfold,n_jobs=-1)\n\n# Best score\nprint('Crossval score for Lightgb classifier: {}'.format(score.mean()))\nprint('Accuracy score for Lightgb classifier: {}'.format(accuracy_score(ytest,ypred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tuned LightGB","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n        importance_type='split', learning_rate=0.2, max_depth=-1,\n        min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n        n_estimators=200, n_jobs=4, num_leaves=63, objective=None,\n        random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n        subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\nlgb.fit(xtrain,ytrain)\nypred=lgb.predict(xtest)\nscore=cross_val_score(lgb,X_train,Y_train,scoring='accuracy',cv=kfold,n_jobs=-1)\n\n# Best score\nprint('Crossval score for Lightgb classifier: {}'.format(score.mean()))\nprint('Accuracy score for Lightgb classifier: {}'.format(accuracy_score(ytest,ypred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ENSEMBLE VOTING CLASSIFIER","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vc= VotingClassifier(estimators=[('rfc', RFC2), ('extc', et2),\n('lgb',lgb)], voting='soft', n_jobs=-1)\nvc.fit(xtrain,ytrain)\nypred=vc.predict(xtest)\nscore=cross_val_score(vc,X_train,Y_train,scoring='accuracy',cv=kfold,n_jobs=-1)\n\n# Best score\nprint('Crossval score for Lightgb classifier: {}'.format(score.mean()))\nprint('Accuracy score for Lightgb classifier: {}'.format(accuracy_score(ytest,ypred)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ENSEMBLE STACKING CLASSIFIER","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nfrom sklearn.ensemble import StackingClassifier\nestimators = [ ('rf', RFC2),\n     ('et', et2)]\n\nsc= StackingClassifier(estimators=estimators, final_estimator=lgb)\nsc.fit(xtrain,ytrain)\nypred=sc.predict(xtest)\nscore=cross_val_score(sc,X_train,Y_train,scoring='accuracy',cv=kfold,n_jobs=-1)\n\n# Best score\nprint('Crossval score for Lightgb classifier: {}'.format(score.mean()))\nprint('Accuracy score for Lightgb classifier: {}'.format(accuracy_score(ytest,ypred)))\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vc.fit(X_train,Y_train)\nypred=vc.predict(df_test.values)\n\nid=df_test1['Id']\ndict={'ID':id,'Cover_Type':ypred}\ndfsub=pd.DataFrame(dict)\ndfsub.to_csv('./submission_ensemblevoting.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hyperparameter tuning of RF And Extratrees","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n#ExtraTrees \net2= ExtraTreesClassifier()\n\n\n## Search grid for optimal parameters\nex_param_grid = {\n \n \n 'criterion': ['gini','entropy'],\n 'max_depth':[5,10,25],\n 'max_features':[1,3,7],\n 'max_samples': [0.2],\n 'min_samples_leaf': [1,2,5],\n 'min_samples_split': [2,5,7],\n 'n_estimators': [100,200,300],\n }\n\n\ngset = GridSearchCV(et2,param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs=-1, verbose = 1)\n\ngset.fit(X_train,Y_train)\ngset_best = gset.best_estimator_\n\n# Best score\nprint(gset.best_score_)\nprint(gset.best_estimator_)\"\"\"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n# RFC Parameters tunning \nRFC = RandomForestClassifier()\n\n\n## Search grid for optimal parameters\nrf_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\nrf_param_grid = {\n    'bootstrap': [True],\n    'max_depth': [80, 90, 100, 110],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4, 5],\n    'min_samples_split': [8, 10, 12],\n    'n_estimators': [100, 200, 300, 1000]\n}\n\n\ngsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsRFC.fit(X_train,Y_train)\n\nRFC_best = gsRFC.best_estimator_\n\n# Best score\ngsRFC.best_score_\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nRFC2 = RandomForestClassifier()\nrf_param_grid = {\n    'bootstrap': [True],\n    'max_depth': [32],\n    'max_features': [2],\n    'min_samples_leaf': [1],\n    'min_samples_split': [6],\n    'n_estimators': [300]\n}\n\n\ngsRFC2 = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsRFC2.fit(X_train,Y_train)\ngsRFC2.best_score_\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Importance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(RFC.feature_importances_,\n             index=X_train.columns, columns=['Importance']).sort_values(\n    by='Importance', ascending=False)[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(et.feature_importances_,\n             index=X_train.columns, columns=['Importance']).sort_values(\n    by='Importance', ascending=False)[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plotting learning curves","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\ng = plot_learning_curve(RFC,\"Random Forest learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(et,\"Extra trees learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsRFC2,\"Random Forest tuned learning curves\",X_train,Y_train,cv=kfold)\n#g = plot_learning_curve(gsExtC.best_estimator_,\"ExtraTrees learning curves\",X_train,Y_train,cv=kfold)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\ng = plot_learning_curve(lgb,\"lgb tuned learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(lgb2,\"Normal lgb learning curves\",X_train,Y_train,cv=kfold)\n\n#g = plot_learning_curve(gsExtC.best_estimator_,\"ExtraTrees learning curves\",X_train,Y_train,cv=kfold)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#g = plot_learning_curve(vc,\"voting classifier learning curves\",X_train,Y_train,cv=kfold)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Both models are little overfitting ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Submisssion File","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"gset_best.fit(X_train,Y_train)\nypred=gset_best.predict(df_test.values)\n\nid=df_test1['Id']\ndict={'ID':id,'Cover_Type':ypred}\ndfsub=pd.DataFrame(dict)\ndfsub.to_csv('./submission_gset.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}