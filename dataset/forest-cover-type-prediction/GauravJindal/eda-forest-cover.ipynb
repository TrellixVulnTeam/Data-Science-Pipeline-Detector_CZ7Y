{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"39d1aad3-3915-f882-08e3-0abff6ec4af8","collapsed":true},"outputs":[],"source":"# Supress unnecessary warnings so that presentation looks clean\nimport warnings\nwarnings.filterwarnings('ignore')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5ba1562f-9f93-7f75-5696-9406f658a055"},"outputs":[],"source":"#Let's first import required Python packages.\n#Importing required Python packages\nimport matplotlib.pylab as plt\nimport numpy as np\nimport pandas as pd\nfrom scipy import sparse\nfrom sklearn.datasets import make_classification, make_blobs, load_boston\nfrom sklearn.decomposition import PCA\nfrom sklearn.cross_validation import ShuffleSplit, train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\nfrom sklearn.learning_curve import learning_curve\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.grid_search import GridSearchCV\nfrom pprint import pprint\nimport pandas as pd\nfrom pandas.tools.plotting import scatter_matrix\nimport urllib\nimport seaborn\n\nnp.random.seed(sum(map(ord, \"aesthetics\")))\nseaborn.set_context('notebook')\n\npd.set_option('display.mpl_style', 'default') # Make the graphs a bit prettier\nplt.rcParams['figure.figsize'] = (15, 5) \n\n# Set some Pandas options\npd.set_option('display.notebook_repr_html', False)\npd.set_option('display.max_columns', 40)\npd.set_option('display.max_rows', 25)\npd.options.display.max_colwidth = 50\n\n%matplotlib inline"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f6950d03-b748-b823-8286-58c34f526b88"},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dbb6e689-037c-f0da-8e7a-a982d2c39522"},"outputs":[],"source":"rf_initial = RandomForestClassifier(n_estimators = 1000, bootstrap = True, oob_score = True)\nrf_initial.fit(traincsv.ix[:,:-1].values, traincsv.ix[:,-1:].values.ravel())\nprint \"Initial Traincsv score: %.2f\" %rf_initial.score(traincsv.ix[:,:-1].values, traincsv.ix[:,-1:].values.ravel())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2d7d57e9-f766-3590-1b32-a64e8da0c898"},"outputs":[],"source":"#Make a copy of the test.csv file\ntemp=testcsv.copy()\n\n#Run Predictions on test.csv\ntemp['Cover_Type']=rf_initial.predict(temp.values)\n\n#Create Submissions csv file\ntemp=temp['Cover_Type']\ntemp.to_csv('RF-Initial.csv', header=True)\n\n# The submission to Kaggle scored 0.75366, taking us to better than 50% of the leaderboard. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e44f7460-ade9-5129-8534-403148d59e64"},"outputs":[],"source":"\n\n#Always call fit on the estimator before invoking this method.\ndef importances(estimator, col_array, title):\n\n# Calculate the feature ranking - Top 10\n    importances = estimator.feature_importances_\n    indices = np.argsort(importances)[::-1] #It returns the indices that would sort an array.\n\n    print \"%s Top 20 Important Features\\n\" %title\n\n    for f in range(20):\n        print(\"%d. %s   (%f)\" % (f + 1, col_array.columns[indices[f]], importances[indices[f]]))\n    \n#Mean Feature Importance\n    print \"\\nMean Feature Importance %.6f\" %np.mean(importances)\n    \n#Plot the feature importances of the forest\n    indices=indices[:10]\n    plt.figure()\n    plt.title(title+\" Top 10 Feature importances\")\n    plt.bar(range(10), importances[indices],\n            color=\"gr\", align=\"center\")\n    plt.xticks(range(10), col_array.columns[indices], fontsize=14, rotation=90)\n    plt.xlim([-1, 10])\n    plt.show()\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7ebfb32c-59c5-33c1-cca9-73fdbc78b5f8"},"outputs":[],"source":"#Call the method we just created to display the feature importances\nimportances(rf_initial, traincsv, \"Cover Type (Initial RF)\")\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7f7ff50e-724d-3c54-1444-6e65c665ae28"},"outputs":[],"source":"traincsv.ix[:,:10].hist(figsize=(16,10), bins=50)\nplt.show()\n#Looks like there're some missing values for Hillshade at 3 PM. \n#If there was shade at 9 AM, I'd expect little shade at 3 PM but 0 doesn't sit right. \n#We'll explore this further down the line, since Hillshade 3 PM is in the Top 10.\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cc6e7152-da6a-bb8e-57cb-b103fca04fc8"},"outputs":[],"source":"with seaborn.axes_style('white'): \n    smaller_frame=traincsv[['Elevation', 'Horizontal_Distance_To_Roadways', \n                            'Horizontal_Distance_To_Fire_Points', \n                            'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology']]\n    smaller_frame.columns=smaller_frame.columns.map(lambda x: x.replace('Horizontal_Distance_To','HD'))\n    smaller_frame.columns=smaller_frame.columns.map(lambda x: x.replace('Vertical_Distance_To','VD'))\n    scatter_matrix(smaller_frame, figsize=(14, 14), diagonal=\"kde\") \nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fcb627db-55a4-7862-d1a1-71813746bfbc"},"outputs":[],"source":"#Let's add Cover Type to smaller_frame so we can color code.\nsmaller_frame['Cover_Type']=traincsv.Cover_Type"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"70df9523-8961-819d-d56d-ba4d63621f49"},"outputs":[],"source":"#Elevation vs HD Hydrology\nplt.figure(figsize=(4,4))\nplt.scatter(smaller_frame.Elevation, smaller_frame.HD_Hydrology, c=smaller_frame.Cover_Type, s=75, cmap=plt.cm.Greens_r)\nplt.xlabel(\"Elevation\")\nplt.ylabel(\"HD Hydrology\")\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"101a1337-82cb-2579-1adc-bd1361678aea"},"outputs":[],"source":"#Elevation adjusted vs HD Hydrology\nplt.figure(figsize=(4,4))\nplt.scatter(smaller_frame.Elevation-0.2*smaller_frame.HD_Hydrology, \n            smaller_frame.HD_Hydrology, c=smaller_frame.Cover_Type, s=75, cmap=plt.cm.Greens_r)\nplt.xlabel(\"Elevation to HD Hydrology\")\nplt.ylabel(\"HD Hydrology\")\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"75c3e7fb-265e-0360-f912-3d992f745133"},"outputs":[],"source":"#Elevation vs HD Roadways\nplt.figure(figsize=(8,8))\nplt.scatter(smaller_frame.Elevation-0.05*smaller_frame.HD_Roadways, \n            smaller_frame.HD_Roadways, c=smaller_frame.Cover_Type, s=75, cmap=plt.cm.Greens_r)\nplt.xlabel(\"Elevation to HD Roadways\")\nplt.ylabel(\"HD Roadways\")\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d2796e48-58c8-fbc0-792f-b6d09ff0997e"},"outputs":[],"source":"\n\n#Elevation vs VD Hyrdrology\nplt.figure(figsize=(8,8))\nplt.scatter(smaller_frame.Elevation-smaller_frame.VD_Hydrology, \n            smaller_frame.VD_Hydrology, c=smaller_frame.Cover_Type, s=75, cmap=plt.cm.Greens_r)\nplt.xlabel(\"Elevation to VD Hydrology\")\nplt.ylabel(\"VD Hydrology\")\nplt.show()\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7470723f-d573-fa57-a2c9-dd119de8000b"},"outputs":[],"source":"temp=traincsv.copy()\ncols=temp.columns.tolist()\ncols=cols[:8]+cols[9:]+[cols[8]]\ntemp=temp[cols]\ndel temp['Cover_Type']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"91abcaf7-fcd8-3d4a-d815-a4aa48a04ee7"},"outputs":[],"source":"#Split the train.csv file into train (available values) and missing values based on hillshade 3 PM.\nX,y,X_traincsv_missing,y_traincsv_missing= temp[temp.Hillshade_3pm!=0].values[:,:-1], \\\n                                           temp[temp.Hillshade_3pm!=0].values[:,-1:].ravel(), \\\n                                           temp[temp.Hillshade_3pm==0].values[:,:-1], \\\n                                           temp[temp.Hillshade_3pm==0].values[:,-1:].ravel()\n            \n#Let's very quickly do a train/cv split so we can see how the model will generalize. Note that we're calling this cv since \n#we're not going to be doing exhaustive cross-validation. We'll test performance using the cv set and predict on test. I am\n#calling this cv so as to not confuse with terminology.\nX_train,X_test,y_train,y_test=train_test_split(X,y)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2bb8c1f1-0369-17c2-dee8-365ff3b737ee"},"outputs":[],"source":"#Fit a Gradient Boosted Regression Tree model to this dataset.\ngbrt=GradientBoostingRegressor(n_estimators=1000)\ngbrt.fit(X_train,y_train)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"05996870-fc25-519e-152c-276cb5d9facd"},"outputs":[],"source":"print 'Training R-squared value: %.2f' %gbrt.score(X_train, y_train)\nprint 'Test R-squared value: %.2f' %gbrt.score(X_test, y_test)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cd67a5a2-c24c-07a5-dd8b-7878e8142ddb"},"outputs":[],"source":"# Calculate the feature ranking - Top 10 \nimportances(gbrt, temp, \"Hillshade 3 PM\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4641a333-1c79-f4e3-b4df-4e2aa7f0ebb7"},"outputs":[],"source":"#Predict and fill missing values in train.csv \ntemp.Hillshade_3pm.loc[temp.Hillshade_3pm==0]=gbrt.predict(X_traincsv_missing)\ntraincsv.Hillshade_3pm=temp.Hillshade_3pm\n\n\n#Make a copy and Reorder test.csv columns\ntemp=testcsv.copy()\ncols=temp.columns.tolist()\ncols=cols[:8]+cols[9:]+[cols[8]]\ntemp=temp[cols]\n\n#Extract missing rows from test.csv then predict and fill in the blanks.\nX_testcsv_missing= temp[temp.Hillshade_3pm==0].values[:,:-1]\ntemp.Hillshade_3pm.loc[temp.Hillshade_3pm==0]=gbrt.predict(X_testcsv_missing)\ntestcsv.Hillshade_3pm=temp.Hillshade_3pm\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"95f7df23-a5d6-134d-9910-14b315d124af"},"outputs":[],"source":"fig, ax=plt.subplots(1,2,figsize=(12,4)) \nax[0].scatter(traincsv.Hillshade_3pm,traincsv.Hillshade_Noon, c=traincsv.Cover_Type, s=50, cmap=plt.cm.OrRd)\nax[0].set_xlabel('Hillshade @ 3 PM')\nax[0].set_ylabel('Hillshade @ Noon')\nax[0].set_title('Train.csv', fontsize=14)\nax[1].scatter(testcsv.Hillshade_3pm,testcsv.Hillshade_Noon, s=50, cmap=plt.cm.PuBu)\nax[1].set_xlabel('Hillshade @ 3 PM')\nax[1].set_ylabel('Hillshade @ Noon')\nax[1].set_title('Test.csv', fontsize=14)\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"637446bd-0c2c-4c8e-a71a-c2d7eacccb5e"},"outputs":[],"source":"\n\n#75/25 Split\nX_train, X_test, y_train, y_test = train_test_split(traincsv.ix[:,:-1].values, traincsv.ix[:,-1:].values.ravel())\nprint X_train.shape, X_test.shape, y_train.shape, y_test.shape\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"57067392-0130-bb2f-44cf-94ada0bf1c8f","collapsed":true},"outputs":[],"source":"def CrossValidate(estimator, param_grid, n_jobs):\n\n#Choose cross-validation generator - let's choose ShuffleSplit which randomly shuffles and selects Train and CV sets\n#for each iteration. There are other methods like the KFold split.\n    cv = ShuffleSplit(X_train.shape[0], n_iter=3, test_size=0.2)\n\n#Apply the cross-validation iterator on the Training set using GridSearchCV. This will run the classifier on the \n#different train/cv splits using parameters specified and return the model that has the best results\n\n#Note that we are tuning based on Classification Accuracy, which is the metric used by Kaggle for evaluating results for \n#this competition. Ideally, we would use a better metric such as the mean-squared error.\n    classifier = GridSearchCV(estimator=estimator, cv=cv, param_grid=param_grid, n_jobs=n_jobs, scoring='accuracy')\n\n#Also note that we're feeding multiple neighbors to the GridSearch to try out.\n\n#We'll now fit the training dataset to this classifier\n    classifier.fit(X_train, y_train)\n\n#Let's look at the best estimator that was found by GridSearchCV\n    print \"Best Estimator learned through GridSearch\"\n    print classifier.best_estimator_\n    \n    return cv, classifier.best_estimator_"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bd7607b6-2ebf-d2e3-4cf2-3ed51dc0be7c"},"outputs":[],"source":"\n\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import cross_validation\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.datasets import load_digits\nfrom sklearn.learning_curve import learning_curve\n\n\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,n_jobs = 4, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"\n    Generate a simple plot of the test and traning learning curve.\n\n    Parameters\n    ----------\n    estimator : object type that implements the \"fit\" and \"predict\" methods\n        An object of that type which is cloned for each validation.\n\n    title : string\n        Title for the chart.\n\n    X : array-like, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n        Target relative to X for classification or regression;\n        None for unsupervised learning.\n\n    ylim : tuple, shape (ymin, ymax), optional\n        Defines minimum and maximum yvalues plotted.\n\n    cv : integer, cross-validation generator, optional\n        If an integer is passed, it is the number of folds (defaults to 3).\n        Specific cross-validation objects can be passed, see\n        sklearn.cross_validation module for the list of possible objects\n\n    n_jobs : integer, optional\n        Number of jobs to run in parallel (default 1).\n    \"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs = n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1d70d78b-8e62-98b5-b761-16e570362002"},"outputs":[],"source":"estimator=RandomForestClassifier()\nparam_grid={'n_estimators': [1000], #[100, 1000, 5000],\n            'max_depth': [8, 10, 12, 14],\n#           'max_features': None,\n#             'oob_score': ['True']\n#             'n_jobs': [4]\n           }\n\n#Let's fit RF to the training dataset by calling the function we just created.\ncv,best_estimator=CrossValidate(estimator, param_grid, 4)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"176714b4-4017-1025-882c-b0117344e855","collapsed":true},"outputs":[],"source":"#OK great, so we got back the best estimator parameters as follows:\nprint \"Best Estimator Parameters\"\nprint\"---------------------------\"\nprint \"n_estimators: %d\" %best_estimator.n_estimators\nprint \"max_depth: %d\" %best_estimator.max_depth\nprint\nprint \"Training Score(F1): %.2f\" %best_estimator.score(X_train,y_train)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fa144d8e-8f03-c46d-fb50-a06bdb36312d","collapsed":true},"outputs":[],"source":"\n\n#Calling fit on the estimator so we can look at feature_importances.\n# best_estimator.fit(X_train, y_train)\n\n#Call the Importances Method\nimportances(best_estimator, traincsv, \"Cover Type - Random Forests try I\")\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4e7ebb26-8d7b-c477-a665-030feb98d99e","collapsed":true},"outputs":[],"source":"title = \"Learning Curves (Random Forests, n_estimators=%d, max_depth=%.6f)\" %(best_estimator.n_estimators, \\\n                                                                              best_estimator.max_depth)\nplot_learning_curve(best_estimator, title, X_train, y_train, cv=cv, n_jobs=4)\nplt.show()\n\nbest_estimator.set_params(max_features=25, min_samples_leaf=2)\ntitle = \"Learning Curves (Random Forests, n_estimators=%d, max_depth=%.6f, max_features=%d, min_samples_leaf=%d)\"  \\\n        %(best_estimator.n_estimators, best_estimator.max_depth, best_estimator.max_features, \\\n          best_estimator.min_samples_leaf)\nplot_learning_curve(best_estimator, title, X_train, y_train, cv=cv, n_jobs=4)\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0dd0cd43-3c3f-6a8a-d44f-113b44ec1696","collapsed":true},"outputs":[],"source":"best_estimator.set_params(max_features=25, min_samples_leaf=1)\n#Call fit on X_train so we can run predictions\nbest_estimator.fit(X_train, y_train)\n#Run predictions on the Test set\ny_pred=best_estimator.predict(X_test)\n\nprint \"Training Score: %.2f\" %best_estimator.score(X_train,y_train)\nprint \"Test Score: %.2f\" %best_estimator.score(X_test,y_test)\nprint\nprint \"Classification Report - Test\"\nprint metrics.classification_report(y_test, y_pred)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"944e3cfc-0d21-c85d-016b-2b203553b1ba","collapsed":true},"outputs":[],"source":"#Make a copy of the test.csv file\ntemp=testcsv.copy()\n\n#Run Predictions on test.csv\ntemp['Cover_Type']=best_estimator.predict(temp.values)\n\n#Create Submissions csv file\ntemp=temp['Cover_Type']\ntemp.to_csv('RF-tune1.csv', header=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e359b476-6583-d8d7-0dcd-0604eed89a00","collapsed":true},"outputs":[],"source":"#Derive counts of each predicted label in the testcsv dataset\nclass_weights=pd.DataFrame({'Class_Count':temp.groupby(temp).agg(len)}, index=None)\nprint class_weights\nclass_weights['Class_Weights'] = temp.groupby(temp).agg(len)/len(temp)\nprint class_weights\nsample_weights=class_weights.ix[y_train]\nprint sample_weights\nbest_estimator.fit(X_train, y_train, sample_weight=sample_weights.Class_Weights.values)\n\n#Run predictions on the Test set\ny_pred=best_estimator.predict(X_test)\n\nprint \"Training Score: %.2f\" %best_estimator.score(X_train,y_train)\nprint \"Test Score: %.2f\" %best_estimator.score(X_test,y_test)\nprint\nprint \"Classification Report - Test\"\nprint metrics.classification_report(y_test, y_pred)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2e17ac97-a97a-f81b-477f-c8af2b538db1","collapsed":true},"outputs":[],"source":"#Make a copy of the test.csv file\ntemp=testcsv.copy()\n\n#Run Predictions on test.csv\ntemp['Cover_Type']=best_estimator.predict(temp.values)\n\n#Create Submissions csv file\ntemp=temp['Cover_Type']\ntemp.to_csv('RF-tune2.csv', header=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"42c9c892-c0c3-edb7-934a-ed945c02183c","collapsed":true},"outputs":[],"source":"#90/10 Split\nX_train, X_test, y_train, y_test = train_test_split(traincsv.ix[:,:-1].values, traincsv.ix[:,-1:].values.ravel(), \n                                                    test_size=0.1)\nprint X_train.shape, X_test.shape, y_train.shape, y_test.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a3b14f43-800b-5f1a-047c-91478558a4d7","collapsed":true},"outputs":[],"source":"#Regenerate Sample Weights since train size has changed\nsample_weights=class_weights.ix[y_train]\n\n#Call Fit on the training set\nbest_estimator.fit(X_train, y_train, sample_weight=sample_weights.Class_Weights.values)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6528ee75-0921-51be-dc17-9de5fa296b59","collapsed":true},"outputs":[],"source":"#Run predictions on the Test set\ny_pred=best_estimator.predict(X_test)\n\nprint \"Training Score: %.2f\" %best_estimator.score(X_train,y_train)\nprint \"Test Score: %.2f\" %best_estimator.score(X_test,y_test)\nprint\nprint \"Classification Report - Test\"\nprint metrics.classification_report(y_test, y_pred)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"28c09709-6474-66b4-8019-141b4128b56e","collapsed":true},"outputs":[],"source":"#Make a copy of the test.csv file\ntemp=testcsv.copy()\n\n#Run Predictions on test.csv\ntemp['Cover_Type']=best_estimator.predict(temp.values)\n\n#Create Submissions csv file\ntemp=temp['Cover_Type']\ntemp.to_csv('RF-tune3.csv', header=True)\n\n\nrf_final=best_estimator\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"60df9b99-9f8d-f199-2a9b-b676cbc739f2","collapsed":true},"outputs":[],"source":"ext=ExtraTreesClassifier(n_estimators=1000, oob_score=True, bootstrap=True)\next.fit(X_train, y_train, sample_weight=sample_weights.Class_Weights.values)\n#Run predictions on the Test set\ny_pred=ext.predict(X_test)\n\nprint \"Training Score: %.2f\" %ext.score(X_train,y_train)\nprint \"Test Score: %.2f\" %ext.score(X_test,y_test)\nprint \"OOB Generalization Score: %.2f\" %ext.oob_score_\nprint\nprint \"Classification Report - Test\"r \nprint metrics.classification_report(y_test, y_pred)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4555ec1e-a948-c8b8-ee5a-b74de51e1121","collapsed":true},"outputs":[],"source":"#Make a copy of the test.csv file\ntemp=testcsv.copy()\n\n#Run Predictions on test.csv\ntemp['Cover_Type']=ext.predict(temp.values)\n\n#Create Submissions csv file\ntemp=temp['Cover_Type']\ntemp.to_csv('ET-tune1.csv', header=True)\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ba908349-6615-66ba-5b71-1207ae1ace3e","collapsed":true},"outputs":[],"source":"estimators = [best_estimator, ext]\ntemp=testcsv.copy()\nX_blend_train=[]\nX_blend_test=[]\nX_blend_testcsv=[]\nfor i, est in enumerate(estimators):\n    X_blend_train.append(est.predict(X_train))\n    X_blend_test.append(est.predict(X_test))\n    X_blend_testcsv.append(est.predict(temp.values))\n    \n    \n    "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c55ea303-243d-bb7e-f079-2ac0234ee0e8","collapsed":true},"outputs":[],"source":"X_blend_train = np.array(X_blend_train).T\nX_blend_test = np.array(X_blend_test).T\nX_blend_testcsv = np.array(X_blend_testcsv).T\nfrom sklearn.linear_model import LogisticRegression\nlo=LogisticRegression().fit(X_blend_train, y_train)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e36c6167-7ee2-1317-ec3a-ff2a72f76883","collapsed":true},"outputs":[],"source":"y_pred=lo.predict(X_blend_test)\n\nprint \"Ensemble Learning - Blending Random Forests & Extra Trees Results\"\nprint \"-----------------------------------------------------------------\"\nprint \"Blended Training Score: %.2f\" %lo.score(X_blend_train,y_train)\nprint \"Blended Test Score: %.2f\" %lo.score(X_blend_test,y_test)\nprint\nprint \"Classification Report - Blended Test\"\nprint metrics.classification_report(y_test, y_pred)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b6c3b5e6-700c-7e68-8a7f-cdceb234b524","collapsed":true},"outputs":[],"source":"#Make a copy of the test.csv file\ntemp=testcsv.copy()\n\n#Run Predictions on test.csv\ntemp['Cover_Type']=lo.predict(X_blend_testcsv)\n\n#Create Submissions csv file\ntemp=temp['Cover_Type']\ntemp.to_csv('Blended-tune1.csv', header=True)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}