{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2022-04-01T09:35:49.804393Z","iopub.execute_input":"2022-04-01T09:35:49.804752Z","iopub.status.idle":"2022-04-01T09:35:49.820147Z","shell.execute_reply.started":"2022-04-01T09:35:49.804705Z","shell.execute_reply":"2022-04-01T09:35:49.818912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_dir = '../input/forest-cover-type-prediction/'\nimport numpy as np\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport copy\nfrom copy import deepcopy as dp\nimport seaborn as sns\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport warnings\nwarnings.filterwarnings('ignore')\ndef g_table(list1):\n    table_dic = {}\n    for i in list1:\n        if i not in table_dic.keys():\n            table_dic[i] = 1\n        else:\n            table_dic[i] += 1\n    return(table_dic)\n\nfrom torch.nn.modules.loss import _WeightedLoss\nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight,\n                                                  pos_weight = pos_weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss\n\nclass TrainDataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n\n    def __len__(self):\n        return (self.features.shape[0])\n\n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n\nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n\n    def __len__(self):\n        return (self.features.shape[0])\n\n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct\n\n\ndef train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n\n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        final_loss += loss.item()\n\n    final_loss /= len(dataloader)\n\n    return final_loss\n\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n\n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n\n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n\n    final_loss /= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n\n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n\n    for data in dataloader:\n        inputs = data['x'].to(device)\n        with torch.no_grad():\n            outputs = model(inputs)\n\n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n\n    preds = np.concatenate(preds)\n\n    return preds\n\nclass Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        cha_1 = 256\n        cha_2 = 512\n        cha_3 = 512\n\n        cha_1_reshape = int(hidden_size/cha_1)\n        cha_po_1 = int(hidden_size/cha_1/2)\n        cha_po_2 = int(hidden_size/cha_1/2/2) * cha_3\n\n        self.cha_1 = cha_1\n        self.cha_2 = cha_2\n        self.cha_3 = cha_3\n        self.cha_1_reshape = cha_1_reshape\n        self.cha_po_1 = cha_po_1\n        self.cha_po_2 = cha_po_2\n\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dropout1 = nn.Dropout(0.1)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n\n        self.batch_norm_c1 = nn.BatchNorm1d(cha_1)\n        self.dropout_c1 = nn.Dropout(0.1)\n        self.conv1 = nn.utils.weight_norm(nn.Conv1d(cha_1,cha_2, kernel_size = 5, stride = 1, padding=2,  bias=False),dim=None)\n\n        self.ave_po_c1 = nn.AdaptiveAvgPool1d(output_size = cha_po_1)\n\n        self.batch_norm_c2 = nn.BatchNorm1d(cha_2)\n        self.dropout_c2 = nn.Dropout(0.1)\n        self.conv2 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_2, kernel_size = 3, stride = 1, padding=1, bias=True),dim=None)\n\n        self.batch_norm_c2_1 = nn.BatchNorm1d(cha_2)\n        self.dropout_c2_1 = nn.Dropout(0.3)\n        self.conv2_1 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_2, kernel_size = 3, stride = 1, padding=1, bias=True),dim=None)\n\n        self.batch_norm_c2_2 = nn.BatchNorm1d(cha_2)\n        self.dropout_c2_2 = nn.Dropout(0.2)\n        self.conv2_2 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_3, kernel_size = 5, stride = 1, padding=2, bias=True),dim=None)\n\n        self.max_po_c2 = nn.MaxPool1d(kernel_size=4, stride=2, padding=1)\n\n        self.flt = nn.Flatten()\n\n        self.batch_norm3 = nn.BatchNorm1d(cha_po_2)\n        self.dropout3 = nn.Dropout(0.2)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(cha_po_2, num_targets))\n\n    def forward(self, x):\n\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        x = F.celu(self.dense1(x), alpha=0.06)\n\n        x = x.reshape(x.shape[0],self.cha_1,\n                      self.cha_1_reshape)\n\n        x = self.batch_norm_c1(x)\n        x = self.dropout_c1(x)\n        x = F.relu(self.conv1(x))\n\n        x = self.ave_po_c1(x)\n\n        x = self.batch_norm_c2(x)\n        x = self.dropout_c2(x)\n        x = F.relu(self.conv2(x))\n        x_s = x\n\n        x = self.batch_norm_c2_1(x)\n        x = self.dropout_c2_1(x)\n        x = F.relu(self.conv2_1(x))\n\n        x = self.batch_norm_c2_2(x)\n        x = self.dropout_c2_2(x)\n        x = F.relu(self.conv2_2(x))\n        x =  x * x_s\n\n        x = self.max_po_c2(x)\n\n        x = self.flt(x)\n\n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-04-01T09:35:49.822798Z","iopub.execute_input":"2022-04-01T09:35:49.823549Z","iopub.status.idle":"2022-04-01T09:35:52.008937Z","shell.execute_reply.started":"2022-04-01T09:35:49.823502Z","shell.execute_reply":"2022-04-01T09:35:52.007798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install fast_ml\ndef convert_to_one_hot(y):\n    seen = False\n    for i in range(len(y)):\n        l = [0 for i in range(7)]\n        l[y[i] - 1] = 1\n        if not seen:\n            y_o = np.array(l.copy())\n            seen = True\n        else:\n            y_o = np.vstack([y_o, l.copy()])\n    return y_o\n    \ntrain = pd.read_csv(input_dir + 'train.csv')\ntest = pd.read_csv(input_dir + 'test.csv')\ntrain = train.drop(\"Id\", axis = 1)\n# x_train = train.drop(\"Cover_Type\", axis = 1)\n# y_train = train[train.columns[-1]]\nfrom fast_ml.model_development import train_valid_test_split\n\nx_train, y_train, x_valid, y_valid, x_test, y_test = train_valid_test_split(train, target = 'Cover_Type', \n                                                                            train_size=0.8, valid_size=0.1, test_size = 0.1)\nx_train = x_train.to_numpy()\ny_train = y_train.to_numpy()\ny_train = convert_to_one_hot(y_train)\n\n\nx_valid = x_valid.to_numpy()\ny_valid = y_valid.to_numpy()\ny_valid = convert_to_one_hot(y_valid)\n\nx_test = x_test.to_numpy()\ny_test = y_test.to_numpy()\ny_test = convert_to_one_hot(y_test)\n\nprint(x_train.shape)\nprint(y_train.shape)\n\nprint(x_valid.shape)\nprint(y_valid.shape)\n\nprint(x_test.shape)\nprint(y_test.shape)\n\n\nprint(train.shape)\ny = train[train.columns[-1]]\ny = y.to_numpy()\ny = convert_to_one_hot(y)\ntarget_cols = [\"Spruce/Fir\", \"Lodgepole Pine\", \"Ponderosa Pine\", \"Cottonwood/Willow\", \"Aspen\", \"Douglas-fir\", \"Krummholz\"]\ntrain[target_cols] = y\ntrain = train.drop(\"Cover_Type\", axis = 1)\nprint(train.shape)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-01T09:35:52.010591Z","iopub.execute_input":"2022-04-01T09:35:52.011491Z","iopub.status.idle":"2022-04-01T09:36:08.365323Z","shell.execute_reply.started":"2022-04-01T09:35:52.011438Z","shell.execute_reply":"2022-04-01T09:36:08.364145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# HyperParameters\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 100\nBATCH_SIZE = 128\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nNFOLDS = 5\nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = False\n\ntarget_cols = [\"Spruce/Fir\", \"Lodgepole Pine\", \"Ponderosa Pine\", \"Cottonwood/Willow\", \"Aspen\", \"Douglas-fir\", \"Krummholz\"]\n#target_cols = [i for i in range(7)]\nnum_features=x_train.shape[1]\nnum_targets=len(target_cols)\nhidden_size=4096\n\ntar_freq = np.array([np.min(list(g_table(train[target_cols].iloc[:,i]).values())) for i in range(len(target_cols))])\ntar_weight0 = np.array([np.log(i+100) for i in tar_freq])\ntar_weight0_min = dp(np.min(tar_weight0))\ntar_weight = tar_weight0_min/tar_weight0\npos_weight = torch.tensor(tar_weight).to(DEVICE)\n\ntrain_dataset = TrainDataset(x_train, y_train)\nvalid_dataset = TrainDataset(x_valid, y_valid)\ntrainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nvalidloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\nmodel = Model(\n            num_features=num_features,\n            num_targets=num_targets,\n            hidden_size=hidden_size,\n        )\n\nmodel.to(DEVICE)\n\nmodel.dense3 = nn.utils.weight_norm(nn.Linear(model.cha_po_2, num_targets))\nmodel.to(DEVICE)\n    \n    \noptimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\nscheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                          max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n\nloss_tr = SmoothBCEwLogits(smoothing = 0.001)\nloss_va = nn.BCEWithLogitsLoss()    \n\nearly_stopping_steps = EARLY_STOPPING_STEPS\nearly_step = 0\n\noof = np.zeros((len(train), num_targets))\nbest_loss = np.inf\n\nmod_name = f\"1D_CNN.pth\"\n\nfor epoch in range(EPOCHS):\n\n    train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n    valid_loss, valid_preds = valid_fn(model, loss_va, validloader, DEVICE)\n    print(f\"EPOCH: {epoch},train_loss: {train_loss}, valid_loss: {valid_loss}\")\n\n    if valid_loss < best_loss:\n\n        best_loss = valid_loss\n        oof = valid_preds\n        torch.save(model.state_dict(), mod_name)\n\n    elif(EARLY_STOP == True):\n\n        early_step += 1\n        if (early_step >= early_stopping_steps):\n            break\n\n#--------------------- PREDICTION---------------------\ntestdataset = TestDataset(x_test)\ntestloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n\nmodel = Model(\n    num_features=num_features,\n    num_targets=num_targets,\n    hidden_size=hidden_size,\n)\n\nmodel.load_state_dict(torch.load(mod_name))\nmodel.to(DEVICE)\n\npredictions = np.zeros((len(test), len(target_cols)))\npredictions = inference_fn(model, testloader, DEVICE)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-01T09:36:08.368538Z","iopub.execute_input":"2022-04-01T09:36:08.369093Z","iopub.status.idle":"2022-04-01T09:37:26.854504Z","shell.execute_reply.started":"2022-04-01T09:36:08.369029Z","shell.execute_reply":"2022-04-01T09:37:26.852918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sum = 0\nfor i in range(len(oof)):\n    x = np.argmax(oof[i])\n    y = np.argmax(y_valid[i])\n    sum += (x == y)\nprint(sum * 100/len(oof))\n","metadata":{"execution":{"iopub.status.busy":"2022-04-01T09:37:26.856088Z","iopub.status.idle":"2022-04-01T09:37:26.856847Z","shell.execute_reply.started":"2022-04-01T09:37:26.856507Z","shell.execute_reply":"2022-04-01T09:37:26.856536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from hyperopt import STATUS_OK, Trials, fmin, hp, tpe","metadata":{"execution":{"iopub.status.busy":"2022-04-01T09:37:36.308841Z","iopub.execute_input":"2022-04-01T09:37:36.30918Z","iopub.status.idle":"2022-04-01T09:37:36.314777Z","shell.execute_reply.started":"2022-04-01T09:37:36.309149Z","shell.execute_reply":"2022-04-01T09:37:36.313412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from hyperopt import fmin, type, hp\nbest = fmin(fn=lambda x: x ** 2,\n    space=hp.uniform('x', -10, 10),\n    algo=tpe.suggest,\n    max_evals=100)\nprint(best)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T09:38:25.103802Z","iopub.execute_input":"2022-04-01T09:38:25.104155Z","iopub.status.idle":"2022-04-01T09:38:25.127449Z","shell.execute_reply.started":"2022-04-01T09:38:25.104123Z","shell.execute_reply":"2022-04-01T09:38:25.12566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def f(lr, bs):\n    DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n    EPOCHS = 25\n    BATCH_SIZE = bs\n    LEARNING_RATE = lr\n    WEIGHT_DECAY = 1e-5\n    NFOLDS = 5\n    EARLY_STOPPING_STEPS = 10\n    EARLY_STOP = False\n\n    target_cols = [\"Spruce/Fir\", \"Lodgepole Pine\", \"Ponderosa Pine\", \"Cottonwood/Willow\", \"Aspen\", \"Douglas-fir\", \"Krummholz\"]\n    #target_cols = [i for i in range(7)]\n    num_features=x_train.shape[1]\n    num_targets=len(target_cols)\n    hidden_size=4096\n\n    tar_freq = np.array([np.min(list(g_table(train[target_cols].iloc[:,i]).values())) for i in range(len(target_cols))])\n    tar_weight0 = np.array([np.log(i+100) for i in tar_freq])\n    tar_weight0_min = dp(np.min(tar_weight0))\n    tar_weight = tar_weight0_min/tar_weight0\n    pos_weight = torch.tensor(tar_weight).to(DEVICE)\n\n    train_dataset = TrainDataset(x_train, y_train)\n    valid_dataset = TrainDataset(x_valid, y_valid)\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n    model = Model(\n                num_features=num_features,\n                num_targets=num_targets,\n                hidden_size=hidden_size,\n            )\n\n    model.to(DEVICE)\n\n    model.dense3 = nn.utils.weight_norm(nn.Linear(model.cha_po_2, num_targets))\n    model.to(DEVICE)\n\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                              max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n\n    loss_tr = SmoothBCEwLogits(smoothing = 0.001)\n    loss_va = nn.BCEWithLogitsLoss()    \n\n    early_stopping_steps = EARLY_STOPPING_STEPS\n    early_step = 0\n\n    oof = np.zeros((len(train), num_targets))\n    best_loss = np.inf\n\n    mod_name = f\"1D_CNN.pth\"\n\n    for epoch in range(EPOCHS):\n\n        train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n        valid_loss, valid_preds = valid_fn(model, loss_va, validloader, DEVICE)\n        print(f\"EPOCH: {epoch},train_loss: {train_loss}, valid_loss: {valid_loss}\")\n\n        if valid_loss < best_loss:\n\n            best_loss = valid_loss\n            oof = valid_preds\n            torch.save(model.state_dict(), mod_name)\n\n        elif(EARLY_STOP == True):\n\n            early_step += 1\n            if (early_step >= early_stopping_steps):\n                break\n\n    return model, train_loss","metadata":{"execution":{"iopub.status.busy":"2022-04-01T09:40:02.810085Z","iopub.execute_input":"2022-04-01T09:40:02.810755Z","iopub.status.idle":"2022-04-01T09:40:02.829412Z","shell.execute_reply.started":"2022-04-01T09:40:02.810718Z","shell.execute_reply":"2022-04-01T09:40:02.828274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def obj(space):\n    model, train_loss = f(space[\"lr\"], space[\"bs\"])\n    return train_loss","metadata":{"execution":{"iopub.status.busy":"2022-04-01T09:40:02.989139Z","iopub.execute_input":"2022-04-01T09:40:02.990035Z","iopub.status.idle":"2022-04-01T09:40:02.997616Z","shell.execute_reply.started":"2022-04-01T09:40:02.989988Z","shell.execute_reply":"2022-04-01T09:40:02.996424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\ntrials = Trials()\nspace = {\"lr\" : hp.loguniform(\"lr\", math.exp(-4), 0.5), \"bs\" : hp.choice(\"bs\", [1024, 2048, 4096, 8192, 512])}\nbest = fmin(fn = obj, space = space, algo = tpe.suggest, max_evals = 10, trials = trials)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T09:49:15.651955Z","iopub.execute_input":"2022-04-01T09:49:15.652325Z","iopub.status.idle":"2022-04-01T09:53:15.840853Z","shell.execute_reply.started":"2022-04-01T09:49:15.652219Z","shell.execute_reply":"2022-04-01T09:53:15.839741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best","metadata":{"execution":{"iopub.status.busy":"2022-04-01T09:53:36.095475Z","iopub.execute_input":"2022-04-01T09:53:36.095772Z","iopub.status.idle":"2022-04-01T09:53:36.102751Z","shell.execute_reply.started":"2022-04-01T09:53:36.095742Z","shell.execute_reply":"2022-04-01T09:53:36.101503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}