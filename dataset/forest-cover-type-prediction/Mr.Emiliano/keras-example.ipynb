{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Esempio normalizzazione DataSet"},{"metadata":{},"cell_type":"markdown","source":"Verifichiamo l'efficacia della normalizzazione dei Dati in questo esempio tratto da Kaggle [Kaggle competition](https://www.kaggle.com/c/forest-cover-type-prediction).\n\nForest Cover Type Prediction<br>\nUse cartographic variables to classify forest categories\n![](https://storage.googleapis.com/kaggle-competitions/kaggle/3936/logos/front_page.png)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Import pandas, tensorflow e keras\nimport pandas as pd\nimport numpy\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nimport tensorflow as tf\nfrom tensorflow.python.data import Dataset\nimport keras\nfrom keras import regularizers\nfrom keras.utils import to_categorical\nfrom keras import models\nfrom keras import layers\nfrom keras import backend as K\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#   for filename in filenames:\n#       print(os.path.join(dirname, filename))\n#Lettura dati\ndf = pd.read_csv(\"/kaggle/input/forest-cover-type-prediction/train.csv\")\ndfT = pd.read_csv(\"/kaggle/input/forest-cover-type-prediction/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Selezioniamo le caratteristiche\nx = df[df.columns[1:55]]\nxT = dfT[dfT.columns[1:55]]\n#Selezioniamo le etichette (8) \ny = df.Cover_Type\n#Split data into train and test \nx_train, x_test, y_train, y_test = train_test_split(x, y , train_size = 0.7, random_state =  90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Normalizziamo la rete come da teoria\n\\begin{equation}\n  x^{(i)} = x^{(i)}-\\frac{1}{m}\\sum_{i=1}^{m} x^{(i)} \\\\\n  x^{(i)} = \\frac{x^{(i)}}{\\frac{1}{m}\\sum_{i=1}^{m} {x^{(i)}}^2}\n\\end{equation}"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalize Training Data \nscaler = preprocessing.StandardScaler()\nscaler.fit(x_train.values[:,0:10])\nx_train_norm = scaler.transform(x_train.values[:,0:10])\nx_test_norm = scaler.transform(x_test.values[:,0:10])\nx_sub = scaler.transform(xT.values[:,0:10])\nx_train_norm=numpy.concatenate((x_train_norm,x_train.values[:,10:]),axis=1)\nx_test_norm=numpy.concatenate((x_test_norm,x_test.values[:,10:]),axis=1)\nx_sub=numpy.concatenate((x_sub,xT.values[:,10:]),axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creiamo una nuova funzione di regolarizzazione da testare ;-) "},{"metadata":{"trusted":true},"cell_type":"code","source":"def l0_reg(weight_matrix):\n    temp = K.abs(weight_matrix)>0.005\n    if_true = tf.reduce_sum(tf.cast(temp, tf.float32))\n    return if_true","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ## Creiamo la rete Neurale con Keras con Doppio regolarizzatore"},{"metadata":{},"cell_type":"markdown","source":"Creiamo una rete a 5 Livelli in cui abbiamo definito l'architettura, il tipo di inizializzazione dei parametri."},{"metadata":{"trusted":true},"cell_type":"code","source":"modelF = models.Sequential()\nmodelF.add(layers.Dense(32,name=\"Layer_1\",activation='relu',input_dim=54,kernel_initializer='he_normal',kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.08)))\nmodelF.add(layers.BatchNormalization())\nmodelF.add(layers.Dense(16,name=\"Layer_2\",activation='relu'))\nmodelF.add(layers.Dense(64,name=\"Layer_22\",activation='relu'))\nmodelF.add(layers.BatchNormalization())\nmodelF.add(layers.Dense(64,name=\"Layer_23\",activation='relu'))\nmodelF.add(layers.BatchNormalization())\nmodelF.add(layers.Dense(16,name=\"Layer_4\",activation='relu'))\nmodelF.add(layers.Dense(8,name=\"Layer_5\",activation='softmax'))\nmodelF.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\nmodelF.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Net4 = modelF.fit(\n x_train_norm, y_train,\n epochs= 400, batch_size = 256,\n validation_data = (x_test_norm, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Grafichiamo delle curve per la valutazione"},{"metadata":{"trusted":true},"cell_type":"code","source":"_, train_acc = modelF.evaluate(x_train_norm, y_train, verbose=0)\n_, test_acc = modelF.evaluate(x_test_norm, y_test, verbose=0)\nprint('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n# plot loss during training\nplt.rcParams['figure.figsize'] = (12.0, 9.0)\nplt.subplot(211)\nplt.title('Loss')\nplt.plot(Net4.history['loss'], label='train')\nplt.plot(Net4.history['val_loss'], label='test')\nplt.legend()\n# plot accuracy during training\nplt.subplot(212)\nplt.title('Accuracy')\nplt.plot(Net4.history['acc'], label='train')\nplt.plot(Net4.history['val_acc'], label='test')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predictions=modelF.predict_classes(x_sub, batch_size=256, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"solutions = pd.DataFrame({'Id':dfT.Id, 'Cover_Type':test_predictions})\nsolutions.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}