{"nbformat_minor":1,"nbformat":4,"metadata":{"language_info":{"file_extension":".py","mimetype":"text/x-python","pygments_lexer":"ipython3","name":"python","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python","version":"3.6.3"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","source":"This is my first attempt with  this competition,  it is still  work in progress.\nTrying to learn and practice my Python and ML  skills; so any comments and suggestions will  be more than welcome.\n\nThanks\n\n\n\nSome ideas  regarding  variable tratement were  taken from:\nAltair007\nForest Cover Type Prediction (Complete) Part I\n\nSanthosh Sharma\nExploratory study on feature selection\n\n\n\n","metadata":{"_uuid":"fd93bb739056fbfcefdc3b21b8008cb3fcb33f61","_cell_guid":"3e665aaf-df52-4845-a4b6-1c35e75eb3ce"}},{"cell_type":"markdown","source":"**Import  packages and load data**","metadata":{"_uuid":"5910705391e212752a8667f7d24f8d4f51d28b89","_cell_guid":"66a97c6f-f49f-49c0-a4e8-9d35e2341e7f"}},{"cell_type":"code","outputs":[],"source":"import pandas as pd\nimport numpy  as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\n\nfrom xgboost import XGBClassifier \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\nMAXRows = 30000000\n\nCoverNames = ['Spruce/Fir', 'Lodgepole Pine', 'Ponderosa Pine', 'Cottonwood/Willow',\n'Aspen', 'Douglas-fir', 'Krummholz']\n\n#'Vertical_Distance_To_Hydrology',\nIMPORTV =  ['Elevation', 'Horizontal_Distance_To_Roadways', 'Horizontal_Distance_To_Fire_Points', \n            'Horizontal_Distance_To_Hydrology', 'Hillshade_9am', 'Hillshade_Noon', 'Vertical_Distance_To_Hydrology', \n            'Wilderness_Area1', 'Aspect', 'Slope', 'Hillshade_3pm', \n            'Soil_Type3', 'Soil_Type10', 'Soil_Type4', 'Wilderness_Area3', 'Soil_Type39', 'Soil_Type30', 'Wilderness_Area4', \n            'Soil_Type2', 'Soil_Type32', 'Soil_Type38', 'Soil_Type33']\n\n\n\"\"\"\nLoad // setup data\n\"\"\"\npath = '../input/'\nTrain = pd.read_csv(path + 'train.csv', index_col='Id', nrows=MAXRows)\nTrain ['Origin'] = 'Train'\n\nTest =  pd.read_csv(path + 'test.csv', index_col='Id', nrows=MAXRows)\nTest ['Origin'] = 'Test'\n\n# Join everything so that we can treat Train and Test variables with less code\nTodo = pd.concat([Train, Test])\nTrainSize = Train.shape[0]\n","execution_count":null,"metadata":{"_uuid":"025c5c2cc374f5471a661b7e5d4d3bd4ead036e7","_cell_guid":"512015f6-0db3-4c27-880f-b1671d903fc3","collapsed":true}},{"cell_type":"markdown","source":"**Auxiliary routines   to  Graph and Treat variables**","metadata":{"_uuid":"afcbd737c8939d9632be03cf0dc2177923a38470","_cell_guid":"608c3e3e-3dcb-4de9-ad79-83f0aade65c5"}},{"cell_type":"code","outputs":[],"source":"def Graficos():\n    for v in IVSkew.index.values:\n        Train.plot(kind='hist', y=v, color='orange')\n        plt.title(v + ' - Skew: ' +  str(IVSkew.loc[v]) )\n        plt.show()\n\ndef TreatVars(X, drp, skw):\n    \n    # Drop variables that make no difference\n    print ('\\nDropping: ', drp)\n    X.drop(drp, inplace=True, axis=1)\n    \n    # Normalize skewed variables\n    print ('\\nDe Skewing: ', skw)\n    X[skw] = np.log1p( X[skw])\n    return\n","execution_count":null,"metadata":{"_uuid":"3dee0a5278633e7dc8b592fee8bd544651bc3166","_cell_guid":"1c37d569-34fc-4a31-8c16-b05aa4a74d21","collapsed":true}},{"cell_type":"markdown","source":"**Routines to evaluate  model , and to model  **","metadata":{"_uuid":"2e06fb9f1782a71173cd04dd44380635ba9628b9","_cell_guid":"d3018118-6919-4b82-8a1d-4ce4ee032239"}},{"cell_type":"code","outputs":[],"source":"\"\"\"\nEvaluate Stuff\n\"\"\"\n    \ndef EvalStuff (X, y, model)  : \n    y_pred    = model.predict(X)\n    score     = model.score(X, y)\n    accuracy  = accuracy_score(y, y_pred)\n    \n    print('\\nScore: {} Accuracy: {}\\n\\n\\nConfusion Matrix'.format(score, accuracy))\n    print(confusion_matrix(y, y_pred))\n    plt.matshow(confusion_matrix(y, y_pred))\n    plt.colorbar()\n    plt.show()\n    \n    print(classification_report(y, y_pred))\n    return \n\n\n\n\n\"\"\"\nInitial modeling, raw\n\"\"\"\n    \ndef ModelStuff (X, y, estimator, estimator_parms):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=32)\n    \n    model = GridSearchCV(estimator, estimator_parms, cv=3)\n    model.fit(X_train, y_train)\n    print('GridSearchCV - \\n Best Estimator: {}  \\n Best Score: {}'.format(model.best_estimator_, model.best_score_))\n    \n    EvalStuff(X_test, y_test, model)\n    \n#    ImpFeat = pd.DataFrame(list(zip(X.columns, model.feature_importances_)), \n#                  columns=['Feature', 'Importance'])\n#    ImpFeat.sort_values('Importance', ascending=False, inplace=True)\n#    print(ImpFeat[ImpFeat['Importance'] > 0])\n#    \n#    rtrn = {'Model' : model, 'ImpFeat' : ImpFeat}\n\n    rtrn = {'Model' : model}\n    return rtrn\n","execution_count":null,"metadata":{"_uuid":"51134b20fbbcef75ef46ecc588f585eb1431d733","_cell_guid":"5e83580f-855f-409d-8380-8ad1b22371a2","collapsed":true}},{"cell_type":"markdown","source":"**Identify special variables**\n\n**Here  we find correlations  between  pairs of variables,  and generate python  lists with:\na.  Interval variables\nb.  Interval variables  with  high  skew","metadata":{"_uuid":"da179cbe2185696ed87530750071ddad64499db3","_cell_guid":"1c127d1b-0f89-4049-9699-27816470a7e4"}},{"cell_type":"code","outputs":[],"source":"\"\"\"\nAnalisis de Variables\n\"\"\"\n# Correlation variables\ndfCorr   = Train.corr()\nCorrVars = list(dfCorr.columns.values)\ndfPairs =pd.DataFrame()\nfor key in CorrVars:\n    dfPairs = dfPairs.append({'Column' : key, 'Corr' :   dfCorr[key]['Cover_Type']}, ignore_index=True)\ndfPairs.sort_values('Corr', ascending=False, inplace=True)\nHighCorr = list(dfPairs.iloc[1:15]['Column'])\n\n# Interva variables\nINTVARS = [x for x in list(Train.columns.values) \n    if 'Wilderness_Area' not in x and 'Soil_Type' not in x]\nINTVARS.remove('Origin')\nINTVARS.remove('Cover_Type')\n\n\n#Skewed variables\nIVSkew = Train[INTVARS].skew().abs().sort_values(0, ascending=False)\nIVSkew = list(IVSkew[:6].index)\n","execution_count":null,"metadata":{"_uuid":"9a81438696f604692d8d304cdd8c3508fe78a876","_cell_guid":"af1b9121-9e48-45d0-ac43-e7997cbd9877","collapsed":true}},{"cell_type":"markdown","source":"**Analyze Wilderness  and Soiltype variables**\n\nThese two  sets of variables  are  OneHot encoded, lets see which ones  really make a difference","metadata":{"_uuid":"aff1d7006bff410937af6409c0299615ff9048ce","_cell_guid":"fa366e08-bc0c-4f24-b34b-1edcabe0182a"}},{"cell_type":"code","outputs":[],"source":"# Wilderness / SoilType variables: reverse OneHot encoding to a single column\nWildVars = [x for x in list(Train.columns.values)  if 'Wilderness_Area'  in x ]\nWildVles = Train[WildVars].apply(lambda x: ''.join(x.astype(str)), axis=1)\nWildVles = WildVles.apply(lambda x: x.index('1'))\nSoilVars = [x for x in list(Train.columns.values)  if 'Soil_Type'  in x ]\nSoilVles = Train[SoilVars].apply(lambda x: ''.join(x.astype(str)), axis=1)\nSoilVles = SoilVles.apply(lambda x: x.index('1'))\n\n#NOOP variables\nWildChek = Train[WildVars].apply(lambda x: x.value_counts()).fillna(0).transpose()\nSoilChek = Train[SoilVars].apply(lambda x: x.value_counts()).fillna(0).transpose()\nprint(SoilChek[SoilChek[1] < 20])\nSoilDrop = list(SoilChek[SoilChek[1] < 20].index)\n\n\nTrain2 = Train.drop(WildVars + SoilVars, axis=1)\nTrain2['Wilderness'] = WildVles\nTrain2['SoilType'] = SoilVles\n\n#plt.rc(\"figure\", figsize = (10,5))\nsns.countplot(x = 'Wilderness', hue = 'Cover_Type', data = Train2)\nplt.show()\nsns.countplot(x = 'SoilType', hue = 'Cover_Type', data = Train2)\nplt.show()","execution_count":null,"metadata":{"_uuid":"fbcab0a9c017cef8cdb93c550c2e34089e88c272","_cell_guid":"9ea1ea85-8153-49e6-92d9-cd6e604ae6ff"}},{"cell_type":"markdown","source":"Setup modeling hyperparameters, pipeline and model  variables\n\n","metadata":{"_uuid":"beab5801bb6c643f3cd9b78ec9bce30c6db85a0f","_cell_guid":"0c052258-5e21-4826-97d8-458d3607f235"}},{"cell_type":"code","outputs":[],"source":"\"\"\"\nModela y predice\n\"\"\"\nxgb_params = {  \n    \"learning_rate\": [ 0.01, 0.02],\n    \"reg_alpha\" : [0.05,  0.1, 0.15, 0.2],\n    \"nthread\" : [-1],\n    \"silent\" : [0]\n}\ndtc_params = {  \n              \"criterion\": [\"gini\", \"entropy\"],\n              \"min_samples_split\": [2, 10, 20],\n              \"max_depth\": [None, 2, 5, 10],\n              \"min_samples_leaf\": [1, 5, 10],\n              \"max_leaf_nodes\": [None, 5, 10, 20]}\nrfc_params = {  \n              \"criterion\": [\"gini\", \"entropy\"],\n              \"min_samples_split\": [2, 10, 20],\n              \"max_depth\": [None, 2, 5, 10],\n              \"min_samples_leaf\": [1, 5, 10],\n              \"max_leaf_nodes\": [None, 5, 10, 20]}\n\nscl = StandardScaler()\nxgb = XGBClassifier(objective='multi:softmax')\ndtc = DecisionTreeClassifier()\nrfc = RandomForestClassifier()\n\nsteps = [('scl', scl),\n        ('rfc', rfc)]\npipe_params = {}\nfor key, val in rfc_params.items():\n    pipe_params['rfc__' + key]  = val\npipe = Pipeline(steps)\n","execution_count":null,"metadata":{"_uuid":"804e8fcc4bfea07086e8b38b2738c8610092da52","_cell_guid":"2005a91a-3e2f-4b4c-bebb-80b9c17e6244","collapsed":true}},{"cell_type":"markdown","source":"**Treat variables, prepare for modelling**\n\nDrops variables that have no  major effect, and  applies log1p  to skewed  vars\nSplits  back  data to train and test\n","metadata":{"_uuid":"c426d60cfce13c2a6fc4f8af6e36579b36cfc49f","_cell_guid":"51460905-d9d7-4a85-bbee-7de181e097cc"}},{"cell_type":"code","outputs":[],"source":"# Re-scale so no negative values remain. needed for Log1p\nTodo ['Vertical_Distance_To_Hydrology'] = Todo ['Vertical_Distance_To_Hydrology']  + abs(np.min(Todo ['Vertical_Distance_To_Hydrology'] ))\nTreatVars(Todo, SoilDrop, IVSkew)\n\nTrain = Todo[:TrainSize]\nTest  = Todo[TrainSize:].drop(['Cover_Type'], axis=1)\n","execution_count":null,"metadata":{"_uuid":"3efad736be8c16b7391185a4c1cba71604e19118","_cell_guid":"b937482a-b2a0-4c84-a78b-630e15c2fa00"}},{"cell_type":"markdown","source":"Perform classification\n\nDoes this  twice:\na. First, by calling the modelling routine which  returns the 'best' model\nb. Again, to  classify on the whole training data","metadata":{"_uuid":"ed8f0f90bdc554688ca4e86a311d5f6bfc9260ed","_cell_guid":"ec276bae-6ae4-4bb0-85c8-41cf272b7c52"}},{"cell_type":"code","outputs":[],"source":"X = Train.drop(['Cover_Type', 'Origin'], axis=1)\ny = Train['Cover_Type']\nrtrn = ModelStuff(X, y, pipe, pipe_params)\nmodel = rtrn['Model']   \n\n# Evaluate on all data\nprint('\\n\\nRESULTS: Whole Train dataset')\nEvalStuff (X, y, model)      ","execution_count":null,"metadata":{"_uuid":"50bb0c24894ed35a684b0409d377801de7efc749","_cell_guid":"4c9665c2-eddb-4fb3-b32c-519145130d28"}},{"cell_type":"markdown","source":"**Predict**\nOn test  dataset\n","metadata":{"_uuid":"95e72b9134f1b9882d48e39d8eb0cef2fa5a1fde","_cell_guid":"bde9b3ed-5a03-44ad-a13f-e2ba7ec6f0c1"}},{"cell_type":"code","outputs":[],"source":"# Now, predict on Test dataset         \ny = model.predict(Test.drop(['Origin'], axis=1))    \nresult = pd.DataFrame({'Id' : Test.index.values, 'Cover_Type' : y} )\n\nresult[['Id', 'Cover_Type']].to_csv('result.csv', index=False)\n\nprint (\"Current date and time: \" , datetime.datetime.now())\n\n\n\n","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"_uuid":"ad5f88d3fc6ed0e45d761365660f3a60c23d5582","_cell_guid":"1f5c2f22-a179-4e94-8bf9-0491c64d7880"}}]}