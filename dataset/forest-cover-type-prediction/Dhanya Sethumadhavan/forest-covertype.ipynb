{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#This Python 3 environment comes with many helpful analytics libraries installed\n#It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n#For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\n\n#Input data files are available in the \"../input/\" directory.\n#For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n#Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/forest-cover-type-prediction/train.csv')\ndf_test = pd.read_csv('../input/forest-cover-type-prediction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_test1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns', None)\ndf_train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Soil Type 7 and 15 are constant and to be dropped. There are no missing data.\nOne hot coded data like wilderness_Area and soil_type can be converted for later analysis.\nRescaling and standardisation needed for some fields.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_train.drop(['Soil_Type7', 'Soil_Type15'], axis = 1)\ndf_test = df_test.drop(['Soil_Type7', 'Soil_Type15'], axis = 1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_train.iloc[:,1:]\ndf_test = df_test.iloc[:,1:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation Formula\n* r = ((x-mean(x))(y-mean(y)))/sqrt(sum(sqr(x-mean(x))))(sum(sqr(y-mean(y)))))"},{"metadata":{"trusted":true},"cell_type":"code","source":"size = 10\ncorrmat = df_train.iloc[:, :size].corr()\nf, ax = plt.subplots(figsize = (10,8))\nsns.heatmap(corrmat, vmax = 0.8, square = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Correlation Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df_train.iloc[:, :size]\ncols = data.columns\n#Running pearson coefficient for all combinations\ndata_corr = data.corr()\nthreshold = 0.5\ncorr_list = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_corr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sorting the highly correlated values\nfor i in range(0, size):\n    for j in range(i+1, size):\n        if data_corr.iloc[i, j] >= threshold and data_corr.iloc[i, j]<1\\\n        or data_corr.iloc[i,j] < 0 and data_corr.iloc[i,j]<=-threshold:\n            corr_list.append([data_corr.iloc[i,j],i,j])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Sorting values\ns_corr_list = sorted(corr_list, key = lambda x: -abs(x[0]))\n\n#print the higher values\nfor v, i, j in s_corr_list:\n    print(\"%s and %s = %.2f\" % (cols[i], cols[j], v))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**SKEWNESS**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.iloc[:, :10].skew()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Visualization**"},{"metadata":{"trusted":true},"cell_type":"code","source":"for v, i, j in s_corr_list:\n    sns.pairplot(data = df_train, hue = 'Cover_Type', size = 6, x_vars = cols[i], y_vars = cols[j])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# A violin plot is a hybrid of a box plot and a kernel density plot, which shows peaks in the data.\ncols = df_train.columns\nsize = len(cols) - 1 # We don't need the target attribute\n# x-axis has target attributes to distinguish between classes\nx = cols[size]\ny = cols[0:size]\n\nfor i in range(0, size):\n    sns.violinplot(data=df_train, x=x, y=y[i])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Elevation has a seperate distribution for each class, hence an important attribute for prediction\n* Aspect plot contains couple of normal distribution for several classes\n* Horizontal distance to hydrology and roadways is quite similar\n* Hillshade 9am and 12pm displays left skew (long tail towards left)\n* Wilderness_Area3 gives no class distinction. As values are not present, others give some scope to distinguish\n* Soil_Type, 1,5,8,9,12,14,18-22, 25-30 and 35-40 offer class distinction as values are not present for many classes\n* From the violin graph soiltype_8 is present only at covertype 2 and is very  negligible to consider.\n* soiltype_25 is present only at covertype2 and is very negligible to consider."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.Wilderness_Area2.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Too many zero values means attributes like it shows class distinction"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Group one-hot encoded variables of a category into one single variable\ncols = df_train.columns\nr,c = df_train.shape\n\n# Create a new dataframe with r rows, one column for each encoded category, and target in the end\nnew_data = pd.DataFrame(index= np.arange(0,r), columns=['Wilderness_Area', 'Soil_Type', 'Cover_Type'])\n\n# Make an entry in data for each r for category_id, target_value\nfor i in range(0,r):\n    p = 0;\n    q = 0;\n    # Category1_range\n    for j in range(10,14):\n        if (df_train.iloc[i,j] == 1):\n            p = j-9 # category_class\n            break\n    # Category2_range\n    for k in range(14,54):\n        if (df_train.iloc[i,k] == 1):\n            q = k-13 # category_class\n            break\n            # Make an entry in data for each r\n    new_data.iloc[i] = [p,q,df_train.iloc[i, c-1]]\n    \n# plot for category1\nsns.countplot(x = 'Wilderness_Area', hue = 'Cover_Type', data = new_data)\nplt.show()\n\n# Plot for category2\nplt.rc(\"figure\", figsize = (25,10))\nsns.countplot(x='Soil_Type', hue = 'Cover_Type', data= new_data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wilderness_Area4 has lot of presence of cover_type 4, good class distinction\nSoilType 1-6,9-13,15, 20-22, 27-31,35,36-38 offer lot of class distinction as counts for some are very high"},{"metadata":{},"cell_type":"markdown","source":"**Data Preparation**\n* Check for Data Transformation"},{"metadata":{},"cell_type":"markdown","source":"Some of the soil_types are present in very fewer cover_types."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the value count for different soil_types\nfor i in range(10, df_train.shape[1]-1):\n    j = df_train.columns[i]\n    print (df_train[j].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_train.drop(['Soil_Type8', 'Soil_Type25'], axis=1)\ndf_test = df_test.drop(['Soil_Type8', 'Soil_Type25'], axis=1)\ndf_train1 = df_train # To be used for algos like SVM where we need normalization and StandardScaler\ndf_test1 = df_test # To be used under normalization and StandardScaler","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Normality**\n* (Needed only for few ML algorithms like SVM)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for data transformation (take only non-categorical values)\ndf_train.iloc[:,:10].skew()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data Transformation needed in "},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\nplt.figure(figsize =(8,6))\nsns.distplot(df_train1['Horizontal_Distance_To_Hydrology'], fit = stats.norm)\nfig = plt.figure(figsize=(8,6))\nres = stats.probplot(df_train1['Horizontal_Distance_To_Hydrology'], plot=plt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Positive Skewness. Square root or log transformation is required."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train1['Horizontal_Distance_To_Hydrology'] = np.sqrt(df_train1['Horizontal_Distance_To_Hydrology'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,6))\nsns.distplot(df_train1['Horizontal_Distance_To_Hydrology'], fit = stats.norm)\nfig = plt.figure(figsize=(8,6))\nres = stats.probplot(df_train1['Horizontal_Distance_To_Hydrology'], plot=plt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Vertical_Distance_To_Hydrology\nplt.figure(figsize=(8,6))\nsns.distplot(df_train1['Vertical_Distance_To_Hydrology'], fit = stats.norm)\nfig = plt.figure(figsize=(8,6))\nres = stats.probplot(df_train1['Vertical_Distance_To_Hydrology'], plot=plt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Positive skewness"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Horizontal_Distance_To_Roadways\nplt.figure(figsize=(8,6))\nsns.distplot(df_train1['Horizontal_Distance_To_Roadways'], fit=stats.norm)\nfig = plt.figure(figsize=(8,6))\nres = stats.probplot(df_train1['Horizontal_Distance_To_Roadways'], plot=plt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Shows Positive skewness"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train1['Horizontal_Distance_To_Roadways'] = np.sqrt(df_train1['Horizontal_Distance_To_Roadways'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot again after sqrt transformation\nplt.figure(figsize=(8,6))\nsns.distplot(df_train1['Horizontal_Distance_To_Roadways'], fit = stats.norm)\nfig = plt.figure(figsize=(8,6))\nres = stats.probplot(df_train1['Horizontal_Distance_To_Roadways'], plot=plt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nsns.distplot(df_train1['Hillshade_9am'], fit=stats.norm)\nplt.figure(figsize=(8, 6))\nres = stats.probplot(df_train1['Hillshade_9am'], plot = plt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Negative Skewness detected.\nPerforming square transform."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Hillshade_9am'] = np.square(df_train1['Hillshade_9am'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (8,6))\nsns.distplot(df_train['Hillshade_9am'], fit = stats.norm)\nfig = plt.figure(figsize = (8,6))\nres = stats.probplot(df_train1['Hillshade_9am'], plot = plt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reasonable improvement is seen"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hillshade_Noon\nfig = plt.figure(figsize=(8,6))\nsns.distplot(df_train1['Hillshade_Noon'],fit=stats.norm)\nfig = plt.figure(figsize=(8,6))\nres = stats.probplot(df_train1['Hillshade_Noon'],plot=plt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Negative skewness present"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train1['Hillshade_Noon'] = np.square(df_train1['Hillshade_Noon'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot again after square transformation\nfig = plt.figure(figsize=(8,6))\nsns.distplot(df_train1['Hillshade_Noon'],fit=stats.norm)\nfig = plt.figure(figsize=(8,6))\nres = stats.probplot(df_train1['Hillshade_Noon'],plot=plt)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Horizontal_Distance_To_Fire_Points\nplt.figure(figsize=(8,6))\nsns.distplot(df_train1['Horizontal_Distance_To_Fire_Points'], fit=stats.norm)\nplt.figure(figsize=(8,6))\nres = stats.probplot(df_train1['Horizontal_Distance_To_Fire_Points'],plot=plt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Shows positive skewness"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train1['Horizontal_Distance_To_Fire_Points'] = np.sqrt(df_train1['Horizontal_Distance_To_Fire_Points'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot again after sqrt transformation\nplt.figure(figsize=(8,6))\nsns.distplot(df_train1['Horizontal_Distance_To_Fire_Points'], fit=stats.norm)\nplt.figure(figsize=(8,6))\nres = stats.probplot(df_train1['Horizontal_Distance_To_Fire_Points'],plot=plt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Determined significant improvement in the plot."},{"metadata":{"trusted":true},"cell_type":"code","source":"# To be used in case of algorithms like SVM\ndf_test1[['Horizontal_Distance_To_Hydrology','Horizontal_Distance_To_Fire_Points'\\\n        ,'Horizontal_Distance_To_Roadways']] = np.sqrt(df_test1[['Horizontal_Distance_To_Hydrology',\\\n        'Horizontal_Distance_To_Fire_Points','Horizontal_Distance_To_Roadways']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To be used in case of algorithms like SVM\ndf_test1[['Hillshade_9am','Hillshade_Noon']] = np.square(df_test1[['Hillshade_9am','Hillshade_Noon']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n## **Test and Train Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#non categorical variables only\n\nSize = 10\nX_train_temp = df_train.iloc[:,:Size]\nX_test_temp = df_test.iloc[:,:Size]\nX_train_temp1 = df_train1.iloc[:,:Size]\nX_test_temp1 = df_test1.iloc[:,:Size]\n\nX_train_temp1 = StandardScaler().fit_transform(X_train_temp1)\nX_test_temp1 = StandardScaler().fit_transform(X_test_temp1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train1.iloc[:,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r,c = df_train.shape\nX_train = np.concatenate((X_train_temp,df_train.iloc[:,Size:c-1]),axis=1)\nX_train1 = np.concatenate((X_train_temp1, df_train1.iloc[:,Size:c-1]), axis=1) # to be used for SVM\ny_train = df_train.Cover_Type.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ML Algorithm\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Support Vector Machine\n* SVM can be used for both regression and Classification problems\n* SVM creates a hyplane or a line that separates data by the classes.\n* Then it analyses at which part the test data will be falling."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm\nfrom sklearn.model_selection import train_test_split\n#In the new version these are in the model_selection module. Use this: from sklearn.model_selection import learning_curve, GridSearchCV.\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_data, x_test_data, y_data, y_test_data = train_test_split(X_train1,y_train,test_size=0.2, random_state=123)\nsvm_para = [{'kernel':['rbf'],'C': [1,10,100,100]}]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We tried two kernels: polynomial and radial basis function. Result obtained with RBF got us a score of 0.49844 in Kaggle, while linear got us 0.4718. Flipping a coin has a better result...\n* rbf or rdial basis function is the gaussian kernel"},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier = GridSearchCV(svm.SVC(),svm_para,cv=3,verbose=2)\nclassifier.fit(x_data,y_data)\nclassifier.best_params_\n#classifier.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parameters optimized using the code in above cell\n#C_opt = 10 # reasonable option\n#clf = svm.SVC(C=C_opt,kernel='rbf')\n#clf.fit(X_train1,y_train)\nclassifier.fit(X_train1,y_train)\nclassifier.score(X_train1,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The data which are almost linearly separable SVM's can still be made to work pretty well by using the right value for c\n* Linearly separable data works well with SVMs\n* For data which are non linear, we can project them to a space where it is linearly separable.That is projecting it to higher dimensions.(1 to 3 or 4)"},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.cv_results_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly, if we are using a linear classifier, we are never going to be able to perfectly separate the labels. We also don’t want to discard the linear classifier altogether because it does seem like a good fit for the problem except for a few errant points.\n## How do SVMs deal with this? They allow you to specify how many errors you are willing to accept?\n\nYou can provide a parameter called “C” to your SVM; this allows you to dictate the tradeoff between:\nHaving a wide margin.\nCorrectly classifying training data. A higher value of C implies you want lesser errors on the training data.\nIt bears repeating that this is a tradeoff. You get better classification of training data at the expense of a wide margin.\n## How SVM finds a Classifier?\n* Case 1: If there are n points in our dataset,the SVM needs only the dotproduct of each pair of points to find a classifier.\n* Case 2: If we want to project data to higher dimensions, SVM takes two points and gives the dot products in a projected space.\n\n## So I project the data first and then run the SVM?\nNo. To make the above example easy to grasp I made it sound like we need to project the data first. The fact is you ask the SVM to do the projection for you. \n\n## Kernels\nKernels does the above calculation job with less computation.\n* Instead of first calculating projection and then the dot product which needs 13 steps in case of 2 data points, kernel firt calculate the dotproducts and then square K(X1,X2)=((x1.X2)^2) , which is done in 4 steps.\n\n#Linear Kernel\n* When we don't use a projection , but compute only a dot product.\n#Polynomial kernel\n#RBF Kernal\n* Radial Basis Function(Rbf) kernel\n* When you want to project to infinite dimensions\n* think about how we compute sums of infinite series. This is similar. There are infinite terms in the dot product, but there happens to exist a formula to calculate their sum.\n\n## To summarize:\n* We typically don’t define a specific projection for our data. Instead, we pick from available kernels, tweaking them in some cases, to find one best suited to the data.\n* Of course, nothing stops us from defining our own kernels, or performing the projection ourselves, but in many cases we don’t need to. Or we at least start by trying out what’s already available.\n* If there is a kernel available for the projection we want, we prefer to use the kernel, because that’s often faster.\n* RBF kernels can project points to infinite dimensions.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#y_pred = classifier.predict(df_test1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_Test1 = pd.read_csv('../input/forest-cover-type-prediction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#solution = pd.DataFrame({'Id':df_Test1.Id, 'Cover_Type':y_pred}, columns = ['Id','Cover_Type'])\n#solution.to_csv('SVMcover_sol.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The higher the gamma value it tries to exactly fit the training data set.\n"},{"metadata":{},"cell_type":"markdown","source":"## Extra Tree Classifier\n\nIn an Extra Trees classifier, the features and splits are selected at random; hence, “Extremely Randomized Tree”. Since splits are chosen at random for each feature in the Extra Trees Classifier, it’s less computationally expensive than a Random Forest.\n\nIt is very similar to a Random Forest Classifier and only differs from it in the manner of construction of the decision trees in the forest.\n\nEach Decision Tree in the Extra Trees Forest is constructed from the original training sample. Then, at each test node, Each tree is provided with a random sample of k features from the feature-set from which each decision tree must select the best feature to split the data based on some mathematical criteria (typically the Gini Index). This random sample of features leads to the creation of multiple de-correlated decision trees.\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.metrics import classification_report\nx_data, x_test_data, y_data, y_test_data = train_test_split(X_train,y_train,test_size= 0.3, random_state=0)\netc_para = [{'n_estimators': [20, 30, 100], 'max_depth':[5, 10, 15], 'max_features': [0.1, 0.2, 0.3]}]\n#default number of features is sqrt(n)\n#default number of min_samples_leaf is 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ETC = GridSearchCV(ExtraTreesClassifier(),param_grid=etc_para, cv=10, n_jobs=-1)\nETC.fit(x_data, y_data)\nETC.best_params_\nETC.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print ('Best accuracy obtained: {}'.format(ETC.best_score_))\nprint ('Parameters:')\nfor key, value in ETC.best_params_.items():\n    print('\\t{}:{}'.format(key,value))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classification Report\nY_pred = ETC.predict(x_test_data)\ntarget = ['class1', 'class2','class3','class4','class5','class6','class7' ]\nprint (classification_report(y_test_data, Y_pred, target_names=target))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Learning Curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\ndef plot_learning_curve(model,title, X, y,n_jobs = 1, ylim = None, cv = None,train_sizes = np.linspace(0.1, 1, 5)):\n    \n    # Figrue parameters\n    plt.figure(figsize=(10,8))\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel('Training Examples')\n    plt.ylabel('Score')\n    \n    train_sizes, train_score, test_score = learning_curve(model, X, y, cv = cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    \n    # Calculate mean and std\n    train_score_mean = np.mean(train_score, axis=1)\n    train_score_std = np.std(train_score, axis=1)\n    test_score_mean = np.mean(test_score, axis=1)\n    test_score_std = np.std(test_score, axis=1)\n    \n    plt.grid()\n    plt.fill_between(train_sizes, train_score_mean - train_score_std, train_score_mean + train_score_std,\\\n                    alpha = 0.1, color = 'r')\n    plt.fill_between(train_sizes, test_score_mean - test_score_std, test_score_mean + test_score_std,\\\n                    alpha = 0.1, color = 'g')\n    \n    plt.plot(train_sizes, train_score_mean, 'o-', color=\"r\", label=\"Training score\")\n    plt.plot(train_sizes, test_score_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n    \n    plt.legend(loc = \"best\")\n    return plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 'max_features': 0.3, 'n_estimators': 100, 'max_depth': 15, 'min_samples_leaf: 1'\netc = ExtraTreesClassifier(bootstrap=True, oob_score=True, n_estimators=100, max_depth=10, max_features=0.3, \\\n                           min_samples_leaf=1)\n\netc.fit(X_train, y_train)\n# yy_pred = etc.predict(X_test)\netc.score(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r,c = df_test.shape\nX_test = np.concatenate((X_test_temp, df_test.iloc[:,Size:c]), axis = 1)\nyy_pred = etc.predict(X_test)\nsolution = pd.DataFrame({'Id':df_Test1.Id, 'Cover_Type':yy_pred}, columns = ['Id','Cover_Type'])\nsolution.to_csv('ETCcover_sol.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting learning curve\ntitle = 'Learning Curve (ExtraTreeClassifier)'\n# cross validation with 50 iterations to have a smoother curve\ncv = ShuffleSplit(n_splits=50, test_size=0.2, random_state=0)\nmodel = etc\nplot_learning_curve(model,title,X_train, y_train, n_jobs=-1,ylim=None,cv=cv)\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}