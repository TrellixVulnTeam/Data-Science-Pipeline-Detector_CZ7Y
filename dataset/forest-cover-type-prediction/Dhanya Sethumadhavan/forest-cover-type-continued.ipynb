{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/forest-cover-type-prediction/train.csv')\ndf_Test = pd.read_csv('../input/forest-cover-type-prediction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop soil types which has values only zero(soil_Type7 and soil_Type15)\n#drop soil types which has only singe one(soil_Type8 and soil_Type25)\n#from train and test data\n#Data Frame is 2 dimensional object with two axes. Axis =0 and axis =1. Axis = 0 represents row and\n#axis = a represents column.\ndf_train = df_train.drop(['Soil_Type7', 'Soil_Type15', 'Soil_Type8', 'Soil_Type25'], axis =1)\n#Moving values to a temporary test variable form Test variable\ndf_test = df_Test.drop(['Soil_Type7', 'Soil_Type15', 'Soil_Type8', 'Soil_Type25'], axis =1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.dtypes\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train and Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Taking only non-categorical values\nSize = 10 \nX_temp = df_train.iloc[:, :Size]\nX_test_temp = df_test.iloc[:, :Size]\nX_temp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Doubt: But Horizontal_distance to fire point is not a categorical variable.\nr, c = df_train.shape\ndf_train.iloc[:,Size:c-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Doubt: why to split and then concanenate to get the same set\nr,c = df_train.shape\nX_train = np.concatenate((X_temp,df_train.iloc[:,Size:c-1]),axis=1)\ny_train = df_train.Cover_Type.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r,c = df_test.shape\nX_test = np.concatenate((X_test_temp, df_test.iloc[:,Size:c]), axis = 1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest\nThe basic idea behind this is to combine multiple decision trees in determining the final output rather than relying on individual decision trees.\n* Multiple subsets are created from the original data set, selecting observations with replacement.\n* A subset of features is selected randomly and whichever feature gives the best split is used to split the node iteratively.\n* The tree is grown to the largest.\n* Repeat the above steps and prediction is given based on the aggregation of predictions from n number of trees.\n\nDecision Trees:\nIn a decision tree, each node is a criterion(most relevant feature, for example color is red/blue) to split into a new branch. The next node formed at the end of new branch consider the next most relevant feature(criteria) to form the split.\n\n\nRandom forest is a crowd of decision trees.\n\nThis can be explained with the help of an example where someone trying to win a bet by playing a game and winning it.\n\nIf he tries to play a game once to win the bet, his chance of winning is very less. This is like we are trying to classify using a single decision tree.\n\nIf he tries to play a game 100 times to win a bet with a condition that if he win the game 60 times he will win the bet,then the chances of winning is more. This is like classifying dataset using random forest, there is more chance of higher accuracy.\n\nSo like each game in 100 games have no relation in winning(less or no correlation), each gtree in the random forest has no or are less correlated."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nX_data, x_test_data, y_data, y_test_data = train_test_split(X_train, y_train, test_size = 0.3)\n#n_estimator: number of trees , can consider 50 and 100 and choose the best,\n#\n#min_samples_leaf: The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered \n#if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of \n#smoothing the model, especially in regression.\n#\n#Bootstrap : The idea is to repeatedly sample data with replacement from the original training set\n#in order to produce multiple separate training sets. These are then used to allow \"meta-learner\" or \"ensemble\" methods\n#to reduce the variance of their predictions,thus greatly improving their predictive performance.\n#\nrf_para = [{'n_estimators':[50, 100], 'max_depth':[5, 10, 15], 'max_features':[0.1, 0.3],\\\n            'min_samples_leaf':[1,3], 'bootstrap':[True, False]}]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = GridSearchCV(RandomForestClassifier(), param_grid = rf_para, cv = 10, n_jobs = -1)\nrfc.fit(X_data, y_data)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#rfc.grid_scores_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#RFC = RandomForestClassifier(n_estimators=100, max_depth=10, max_features=0.3, bootstrap=True, min_samples_leaf=1,\\\n#                             n_jobs=-1)\n#kaggle score 0.60\n#when the below parameters were added, in the classification report the accuracy score hiked from 0.83 to 0.97\n#kaggle score 0.70400\nRFC = RandomForestClassifier(n_estimators=100, max_depth=15, max_features=0.3, bootstrap='False', min_samples_leaf=1,\\\n                             n_jobs=-1)\nRFC.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nY_val_pred = RFC.predict(x_test_data)\ntarget = ['class1', 'class2','class3','class4','class5','class6','class7' ]\nprint (classification_report(y_test_data, Y_val_pred, target_names=target))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred = RFC.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"solution = pd.DataFrame({'Id':df_Test.Id, 'Cover_Type':Y_pred}, columns = ['Id','Cover_Type'])\nsolution.to_csv('RFCcover_sol.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LEARNING CURVE"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\ndef plot_learning_curve(model,title, X, y,n_jobs = 1, ylim = None, cv = None,train_sizes = np.linspace(0.1, 1, 5)):\n    \n    # Figrue parameters\n    plt.figure(figsize=(10,8))\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel('Training Examples')\n    plt.ylabel('Score')\n    \n    train_sizes, train_score, test_score = learning_curve(model, X, y, cv = cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    \n    # Calculate mean and std\n    train_score_mean = np.mean(train_score, axis=1)\n    train_score_std = np.std(train_score, axis=1)\n    test_score_mean = np.mean(test_score, axis=1)\n    test_score_std = np.std(test_score, axis=1)\n    \n    plt.grid()\n    plt.fill_between(train_sizes, train_score_mean - train_score_std, train_score_mean + train_score_std,\\\n                      alpha = 0.1, color = 'r')\n    plt.fill_between(train_sizes, test_score_mean - test_score_std, test_score_mean + test_score_std,\\\n                    alpha = 0.1, color = 'g')\n    \n    plt.plot(train_sizes, train_score_mean, 'o-', color=\"r\", label=\"Training score\")\n    plt.plot(train_sizes, test_score_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n    \n    plt.legend(loc = \"best\")\n    return plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting Learning Curve\ntitle = 'Learning Curve(Random Forest)'\nmodel = RFC\ncv = ShuffleSplit(n_splits=50, test_size=0.2,random_state=0)\nplot_learning_curve(model,title,X_train, y_train, n_jobs=-1,ylim=None,cv=cv)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost.sklearn import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom scipy.stats import randint,uniform ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = StratifiedKFold(y_train, n_folds = 10, shuffle = True)\n\nparams_dist_grid = {\n    'max_depth':[1, 5, 10],\n    'gamma':[0, 0.5,1],\n    'n_estimators': randint(1, 1001),\n    'learning_rate':uniform(),\n    'subsample':uniform(),\n    'colsample_bytree': uniform(),\n    'reg_lambda': uniform(),\n    'reg_alpha': uniform()\n}\n\nxgbc_fixed = {'booster':['gbtree'], 'silent':1}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_gridd = RandomizedSearchCV(estimator = XGBClassifier(*xgbc_fixed), param_distributions  = params_dist_grid\\\n                               ,scoring = 'accuracy', cv = cv, n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bst_gridd.fit(X_train, y_train)\n# bst_gridd.grid_scores_\n\n# print ('Best accuracy obtained: {}'.format(bst_gridd.best_score_))\n# print ('Parameters:')\n# for key, value in bst_gridd.best_params_.items():\n    # print('\\t{}:{}'.format(key,value))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best parameters selected using code in above cell\n# Splitting the train data to test the best parameters\nfrom sklearn.model_selection import train_test_split\nseed = 123\nx_data, x_test_data, y_data, y_test_data = train_test_split(X_train, y_train, test_size = 0.3,random_state=seed)\n\neval_set = [(x_test_data, y_test_data)]\n\nXGBC = XGBClassifier(silent=1,n_estimators=641,learning_rate=0.2,max_depth=10,gamma=0.5,nthread=-1,\\\n                    reg_alpha = 0.05, reg_lambda= 0.35, max_delta_step = 1, subsample = 0.83, colsample_bytree = 0.6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XGBC.fit(x_data, y_data, early_stopping_rounds=100, eval_set=eval_set, eval_metric='merror', verbose=True)\n\npred = XGBC.predict(x_test_data)\n\naccuracy = accuracy_score(y_test_data, pred);\nprint ('accuracy:%0.2f%%'%(accuracy*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XGBC.fit(X_train, y_train)\nxgbc_pred= XGBC.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# saving to a csv file to make submission\nsolution = pd.DataFrame({'Id':df_Test.Id, 'Cover_Type':xgbc_pred}, columns = ['Id','Cover_Type'])\nsolution.to_csv('Xgboost_sol.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}