{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/forest-cover-type-prediction/train.csv')\ntest_df = pd.read_csv('/kaggle/input/forest-cover-type-prediction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.head(10)) \nprint(len(df))\n# print(df.describe())\nprint(df.columns)\nprint(test_df.head(10)) \nprint(len(test_df))\n# print(df.describe())\nprint(test_df.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Cover_Type'].describe()\npd.value_counts(df['Cover_Type'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\ndf.drop(axis=1, columns=['Soil_Type7','Soil_Type15'], inplace=True)\n# Convert the Wilderness Area one hot encoded to single column\ncolumns = ['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3',\n       'Wilderness_Area4']\nwilderness_types = []\nfor index, row in df.iterrows():\n    dummy = 'Wilderness_Area_NA'\n    for col in columns:\n        if row[col] == 1:\n            dummy = col\n            break\n    wilderness_types.append(dummy)\ndf['Wilderness_Areas'] = wilderness_types\n# Convert the Soil Type one hot encoded to single column\ncolumns = ['Soil_Type1', 'Soil_Type2', 'Soil_Type3',\n       'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type8',\n       'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12',\n       'Soil_Type13', 'Soil_Type14', 'Soil_Type16',\n       'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20',\n       'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24',\n       'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28',\n       'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32',\n       'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36',\n       'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40']\nsoil_types = []\nfor index, row in df.iterrows():\n    dummy = 'Soil_Type_NA'\n    for col in columns:\n        if row[col] == 1:\n            dummy = col\n            break\n    soil_types.append(dummy)\ndf['Soil_Types'] = soil_types\n\n\nprint(pd.value_counts(df['Soil_Types']))\nax = df['Soil_Types'].value_counts().plot(kind='bar',\n                                    figsize=(8,5),\n                                    title=\"Number for each Soli Type\")\nax.set_xlabel(\"Soil Types\")\nax.set_ylabel(\"Frequency\")\nplt.show()\n\nprint(pd.value_counts(df['Wilderness_Areas']))\nax1 = df['Wilderness_Areas'].value_counts().plot(kind='bar',\n                                    figsize=(8,5),\n                                    title=\"Number for each Soli Type\")\nax1.set_xlabel(\"Wilderness_Areas\")\nax1.set_ylabel(\"Frequency\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(axis=1, columns=['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3',\n       'Wilderness_Area4'], inplace=True)\ndf.drop(axis=1, columns=['Soil_Type1', 'Soil_Type2', 'Soil_Type3',\n       'Soil_Type4', 'Soil_Type5', 'Soil_Type6',  'Soil_Type8',\n       'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12',\n       'Soil_Type13', 'Soil_Type14', 'Soil_Type16',\n       'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20',\n       'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24',\n       'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28',\n       'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32',\n       'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36',\n       'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40'], inplace=True)\ndf['Soil_Types'].replace(to_replace={'Soil_Type8': 'Soil_Type_NA', 'Soil_Type25': 'Soil_Type_NA'}, inplace=True)\nprint(pd.value_counts(df['Soil_Types']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Apply to test_df\ntest_df.drop(axis=1, columns=['Soil_Type7','Soil_Type15'], inplace=True)\n# Convert the Wilderness Area one hot encoded to single column\ncolumns = ['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3',\n       'Wilderness_Area4']\nwilderness_types = []\nfor index, row in test_df.iterrows():\n    dummy = 'Wilderness_Area_NA'\n    for col in columns:\n        if row[col] == 1:\n            dummy = col\n            break\n    wilderness_types.append(dummy)\ntest_df['Wilderness_Areas'] = wilderness_types\n# Convert the Soil Type one hot encoded to single column\ncolumns = ['Soil_Type1', 'Soil_Type2', 'Soil_Type3',\n       'Soil_Type4', 'Soil_Type5', 'Soil_Type6',  'Soil_Type8',\n       'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12',\n       'Soil_Type13', 'Soil_Type14', 'Soil_Type16',\n       'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20',\n       'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24',\n       'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28',\n       'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32',\n       'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36',\n       'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40']\nsoil_types = []\nfor index, row in test_df.iterrows():\n    dummy = 'Soil_Type_NA'\n    for col in columns:\n        if row[col] == 1:\n            dummy = col\n            break\n    soil_types.append(dummy)\ntest_df['Soil_Types'] = soil_types\n\n\nprint(pd.value_counts(test_df['Soil_Types']))\n\nprint(pd.value_counts(test_df['Wilderness_Areas']))\n\ntest_df.drop(axis=1, columns=['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3',\n       'Wilderness_Area4'], inplace=True)\ntest_df.drop(axis=1, columns=['Soil_Type1', 'Soil_Type2', 'Soil_Type3',\n       'Soil_Type4', 'Soil_Type5', 'Soil_Type6',  'Soil_Type8',\n       'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12',\n       'Soil_Type13', 'Soil_Type14', 'Soil_Type16',\n       'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20',\n       'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24',\n       'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28',\n       'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32',\n       'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36',\n       'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40'], inplace=True)\n\ntest_df['Soil_Types'].replace(to_replace={'Soil_Type8': 'Soil_Type_NA', 'Soil_Type25': 'Soil_Type_NA', 'Soil_Type7': 'Soil_Type_NA', 'Soil_Type15': 'Soil_Type_NA'}, inplace=True)\nprint(pd.value_counts(test_df['Soil_Types']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# Split the data into train, validation and test\ntrain, test = train_test_split(df, test_size=0.2)\ntrain, val = train_test_split(train, test_size=0.2)\n\nprint(len(train), 'train examples')\nprint(len(val), 'validation examples')\nprint(len(test), 'test examples')\nprint(\"Train dataset :\")\nprint(pd.value_counts(train['Cover_Type']))\nprint(\"Val dataset :\")\nprint(pd.value_counts(val['Cover_Type']))\nprint(\"Test dataset :\")\nprint(pd.value_counts(test['Cover_Type']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalize the feature columns\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\n\n\n# Remove Id column as well\ntrain.drop(axis=1, columns=['Id'], inplace=True)\nval.drop(axis=1, columns=['Id'], inplace=True)\ntest.drop(axis=1, columns=['Id'], inplace=True)\n\n# Create a min max processor object\nmin_max_scaler = preprocessing.MinMaxScaler()\n# Use the fit transform function on processor object\nfeature_columns = ['Elevation', 'Aspect', 'Slope',\n       'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\n       'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon',\n       'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points']\nfor column in feature_columns:\n    print(\"Transforming column {}:\".format(column))\n    train[column] = min_max_scaler.fit_transform(train[[column]].values.astype(float))\n    test[column] = min_max_scaler.fit_transform(test[[column]].values.astype(float))\n    val[column] = min_max_scaler.fit_transform(val[[column]].values.astype(float))\n    print(train[column].describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nprint('TF: {}'.format(tf.__version__))\n\n# A utility method to create a tf.data dataset from a Pandas Dataframe\ndef df_to_dataset(dataframe, shuffle=True, batch_size=32):\n  dataframe = dataframe.copy()\n  labels = dataframe.pop('Cover_Type')\n  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n  if shuffle:\n    ds = ds.shuffle(buffer_size=len(dataframe))\n  ds = ds.batch(batch_size)\n  return ds\n\nbatch_size = 30\ntrain_ds = df_to_dataset(train, batch_size=batch_size)\nval_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\ntest_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\n\n\n\nfor feature_batch, label_batch in train_ds.take(1):\n  print('Every feature:', list(feature_batch.keys()))\n  print('A batch of Slope:', feature_batch['Slope'])\n  print('A batch of targets:', label_batch )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow import feature_column\n\nfeature_cols_for_training = []\n\n# numeric cols\nfor header in ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points']:\n  feature_cols_for_training.append(feature_column.numeric_column(header))\n\n# indicator cols\nwilderness = feature_column.categorical_column_with_vocabulary_list(\n      'Wilderness_Areas', ['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3',\n       'Wilderness_Area4'])\nwilderness_one_hot = feature_column.indicator_column(wilderness)\nfeature_cols_for_training.append(wilderness_one_hot)\n\nsoil_types = feature_column.categorical_column_with_vocabulary_list(\n      'Soil_Types', ['Soil_Type1', 'Soil_Type2', 'Soil_Type3',\n       'Soil_Type4', 'Soil_Type5', 'Soil_Type6',\n       'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12',\n       'Soil_Type13', 'Soil_Type14', 'Soil_Type16',\n       'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20',\n       'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28',\n       'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32',\n       'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36',\n       'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40', 'Soil_Type_NA'])\nsoil_types_one_hot = feature_column.indicator_column(soil_types)\nfeature_cols_for_training.append(soil_types_one_hot)\n\n# Dense layer as input to the model\nfeature_layer = tf.keras.layers.DenseFeatures(feature_cols_for_training)\n\ndef build_model():\n  model = keras.Sequential([\n    feature_layer,\n    layers.Dense(100, activation='relu'),\n    layers.Dense(50, activation='relu'),\n    layers.Dense(8)\n  ])\n\n  model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n                optimizer='adam',\n                metrics=['accuracy'])\n  return model\n\nmodel = build_model()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(train_ds,\n          validation_data=val_ds,\n          epochs=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()\nloss, accuracy = model.evaluate(test_ds)\nprint(\"Accuracy\", accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\npredictions = np.argmax(model.predict(test_ds), axis=-1)\nprint(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"original_labels = tf.Variable(test['Cover_Type'], tf.int32)\npredictions =  tf.Variable(predictions, tf.int32)\nprint(type(predictions))\ntf.math.confusion_matrix(original_labels, predictions, num_classes=8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nimport time\ntest_features = []\nstart_time = time.time()\ntest_predictions = np.argmax(model.predict(test_ds), axis=-1)\n# Comparing the predictions to actual actual forest cover types for the sentences\nprint(classification_report(test['Cover_Type'],test_predictions))\nprint(\"Time taken to predict the model \" + str(time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract the Id column because we need it\nids = test_df['Id']\ntest_df.drop(axis=1, columns=['Id'], inplace=True)\n\n# Normalize the values for few columns\nfeature_columns = ['Elevation', 'Aspect', 'Slope',\n       'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\n       'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon',\n       'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points']\nfor column in feature_columns:\n    print(\"Transforming column {}:\".format(column))\n    test_df[column] = min_max_scaler.fit_transform(test_df[[column]].values.astype(float))\n    \ndef df_to_dataset_test(dataframe, shuffle=True, batch_size=32):\n  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe)))\n  if shuffle:\n    ds = ds.shuffle(buffer_size=len(dataframe))\n  ds = ds.batch(batch_size)\n  return ds\ntest_ds = df_to_dataset_test(test_df, shuffle=False, batch_size=batch_size)\nfinal_predictions = np.argmax(model.predict(test_ds), axis=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(final_predictions))\nprint(len(ids))\nfinal_df = pd.DataFrame()\nfinal_df = final_df.from_dict({'Id': ids, 'Cover_Type': final_predictions})\nprint(final_df.head())\npd.value_counts(final_df['Cover_Type'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df.to_csv('tensorflow_ffnn_output.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}