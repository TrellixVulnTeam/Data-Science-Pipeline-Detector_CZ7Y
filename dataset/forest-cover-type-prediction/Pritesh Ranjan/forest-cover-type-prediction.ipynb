{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Forest Cover Type Prediction\n\nWe shall follow the following steps to complete this challange:\n1. Understand the business problem\n2. Get the data\n3. Discover and visualize insights (univariate and multi variate analysis)\n4. Prepare data for ML algorithms\n5. Select a model and train it\n6. Fine tune your model\n7. Launch, monitor and maintain your system (not needed in this case).\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-04T06:43:10.257256Z","iopub.execute_input":"2021-08-04T06:43:10.257943Z","iopub.status.idle":"2021-08-04T06:43:10.272997Z","shell.execute_reply.started":"2021-08-04T06:43:10.257829Z","shell.execute_reply":"2021-08-04T06:43:10.271854Z"}}},{"cell_type":"markdown","source":"## Understand business problem:\nDesign and implement a system which can process the unscaled and binary features and predict the forest cover type. This is a multi-class classification project.\nThe test data is very large when compared to the train data. ","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:39:59.499884Z","iopub.execute_input":"2021-08-07T15:39:59.500286Z","iopub.status.idle":"2021-08-07T15:39:59.509395Z","shell.execute_reply.started":"2021-08-07T15:39:59.500252Z","shell.execute_reply":"2021-08-07T15:39:59.508613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get the data\nWe are mainly concerned with train.csv and test.csv here.\nLet's load it into a pandas dataframe.\n","metadata":{}},{"cell_type":"code","source":"dataset = pd.read_csv('/kaggle/input/forest-cover-type-prediction/train.csv')\ndataset.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:39:59.547829Z","iopub.execute_input":"2021-08-07T15:39:59.54833Z","iopub.status.idle":"2021-08-07T15:39:59.626777Z","shell.execute_reply.started":"2021-08-07T15:39:59.5483Z","shell.execute_reply":"2021-08-07T15:39:59.625999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.Cover_Type.unique()","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:39:59.627874Z","iopub.execute_input":"2021-08-07T15:39:59.62826Z","iopub.status.idle":"2021-08-07T15:39:59.633813Z","shell.execute_reply.started":"2021-08-07T15:39:59.628231Z","shell.execute_reply":"2021-08-07T15:39:59.633117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we know this is a multi class classification task.\nNow let's load the validation set from test.csv","metadata":{}},{"cell_type":"markdown","source":"We don't have the Cover_Type variable as it is the independent variable that we will predict.","metadata":{}},{"cell_type":"code","source":"dataset.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:39:59.666805Z","iopub.execute_input":"2021-08-07T15:39:59.667337Z","iopub.status.idle":"2021-08-07T15:39:59.685835Z","shell.execute_reply.started":"2021-08-07T15:39:59.667305Z","shell.execute_reply":"2021-08-07T15:39:59.68482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:39:59.711295Z","iopub.execute_input":"2021-08-07T15:39:59.711658Z","iopub.status.idle":"2021-08-07T15:39:59.8835Z","shell.execute_reply.started":"2021-08-07T15:39:59.711628Z","shell.execute_reply":"2021-08-07T15:39:59.882322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Discover and visualize insights\nLet's start with univariate and bivariate analysis to understand our data.","metadata":{}},{"cell_type":"code","source":"dataset.groupby(['Cover_Type']).agg(['count'])['Id']","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:39:59.885581Z","iopub.execute_input":"2021-08-07T15:39:59.886044Z","iopub.status.idle":"2021-08-07T15:40:00.010371Z","shell.execute_reply.started":"2021-08-07T15:39:59.885999Z","shell.execute_reply":"2021-08-07T15:40:00.009414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sns.countplot(x='Cover_Type', data=dataset)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:40:00.012096Z","iopub.execute_input":"2021-08-07T15:40:00.012389Z","iopub.status.idle":"2021-08-07T15:40:00.173709Z","shell.execute_reply.started":"2021-08-07T15:40:00.012361Z","shell.execute_reply":"2021-08-07T15:40:00.172757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare data for ML algorithms\nAs we have seen, the features have very different scales. So we need to bring them in same scale. Here we will use standard scaler class from sklearn.\nBut before that, its really important to split the dataset into train and test sets.\nLet's do that first","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# first drop id column\ndataset.drop(['Id'], axis=1, inplace=True)\n\nX_train = dataset.iloc[:, :-1]\ny_train = dataset.iloc[:, -1]\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n# X_test = None\n# y_test = None\nprint(dataset.shape)\n# print(X.shape)\n# print(y.shape)\n# print(X_train.shape)\n# print(y_train.shape)\n# print(X_test.shape)\n# print(y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:40:00.175509Z","iopub.execute_input":"2021-08-07T15:40:00.175846Z","iopub.status.idle":"2021-08-07T15:40:00.187167Z","shell.execute_reply.started":"2021-08-07T15:40:00.175816Z","shell.execute_reply":"2021-08-07T15:40:00.185946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nto_be_scaled_features = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n       'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n       'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n       'Horizontal_Distance_To_Fire_Points']\n\nsc_X = StandardScaler()\nX_train[to_be_scaled_features] = sc_X.fit_transform(X_train[to_be_scaled_features])\n# X_test[to_be_scaled_features] = sc_X.transform(X_test[to_be_scaled_features])\n","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:40:00.188703Z","iopub.execute_input":"2021-08-07T15:40:00.189131Z","iopub.status.idle":"2021-08-07T15:40:00.268961Z","shell.execute_reply.started":"2021-08-07T15:40:00.189084Z","shell.execute_reply":"2021-08-07T15:40:00.267806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.columns","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:40:00.270282Z","iopub.execute_input":"2021-08-07T15:40:00.2706Z","iopub.status.idle":"2021-08-07T15:40:00.277488Z","shell.execute_reply.started":"2021-08-07T15:40:00.270557Z","shell.execute_reply":"2021-08-07T15:40:00.276476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:40:00.278835Z","iopub.execute_input":"2021-08-07T15:40:00.279159Z","iopub.status.idle":"2021-08-07T15:40:00.314737Z","shell.execute_reply.started":"2021-08-07T15:40:00.279128Z","shell.execute_reply":"2021-08-07T15:40:00.313313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we have scaled the data and we don't have any missing values, let's train multiple machine learning models and see which performs best.","metadata":{}},{"cell_type":"markdown","source":"Let's try couple other classifiers","metadata":{}},{"cell_type":"code","source":"# from sklearn.ensemble import RandomForestClassifier\n\n# rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n# rf_classifier.fit(X_train, y_train)\n\n# y_pred = rf_classifier.predict(X_test)\n\n# from sklearn.metrics import accuracy_score, confusion_matrix\n\n# print(accuracy_score(y_test, y_pred))\n# rf_proba = rf_classifier.predict_proba(X_test)\n# print(roc_auc_score(y_test, rf_proba, multi_class='ovr'))\n# confusion_matrix(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:40:00.31744Z","iopub.execute_input":"2021-08-07T15:40:00.317803Z","iopub.status.idle":"2021-08-07T15:40:00.325069Z","shell.execute_reply.started":"2021-08-07T15:40:00.317772Z","shell.execute_reply":"2021-08-07T15:40:00.324039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see Random forest classifier performs the best here.\nBefore we optimise this further, lets try Xgboost classifier\n","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier(\n    learning_rate=0.09,\n    n_estimators=500,\n    max_depth=30,\n    nthread=4,\n    objective='multi:softprob',\n    subsample = 0.75,\n)\n\nxgb.fit(X_train, y_train)\n\n# y_pred = xgb.predict(X_test)\n\n# from sklearn.metrics import accuracy_score, classification_report \n\n# print(accuracy_score(y_test, y_pred)*100)\n# xgb_proba = xgb.predict_proba(X_test)\n# print(roc_auc_score(y_test, xgb_proba, multi_class='ovr')*100)\n# print(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:40:00.3265Z","iopub.execute_input":"2021-08-07T15:40:00.326825Z","iopub.status.idle":"2021-08-07T15:42:02.594283Z","shell.execute_reply.started":"2021-08-07T15:40:00.326795Z","shell.execute_reply":"2021-08-07T15:42:02.593314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from lightgbm import LGBMClassifier\n\nlgb = LGBMClassifier(learning_rate=0.09,\n                       num_leaves = 500,\n                       boosting_type='gbdt',\n                       objective= 'multiclass',\n                       metric= 'multi_logloss',\n                       max_depth = 30,\n#                        n_estimators=3000,\n#                        subsample_for_bin=4000, \n#                        min_split_gain=2,\n#                        min_child_weight=2,\n#                       min_child_samples=5,\n                      subsample=0.75\n                    )\n\nlgb.fit(X_train, y_train)\n\n\n# y_pred = lgb.predict(X_test)\n\n# from sklearn.metrics import accuracy_score, classification_report \n\n# print(accuracy_score(y_test, y_pred)*100)\n# xgb_proba = xgb.predict_proba(X_test)\n# print(roc_auc_score(y_test, xgb_proba, multi_class='ovr')*100)\n# print(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:42:02.595613Z","iopub.execute_input":"2021-08-07T15:42:02.59589Z","iopub.status.idle":"2021-08-07T15:42:16.779243Z","shell.execute_reply.started":"2021-08-07T15:42:02.595864Z","shell.execute_reply":"2021-08-07T15:42:16.778218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.ensemble import ExtraTreesClassifier\n\n# et = ExtraTreesClassifier(n_estimators=500,n_jobs=-1,random_state=0)\n# et.fit(X_train, y_train)\n\n# y_pred = et.predict(X_train)\n\n# from sklearn.metrics import accuracy_score, classification_report \n\n# print(accuracy_score(y_train, y_pred)*100)\n# xgb_proba = et.predict_proba(X_train)\n# print(roc_auc_score(y_train, xgb_proba, multi_class='ovr')*100)\n# print(classification_report(y_train, y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:42:16.780505Z","iopub.execute_input":"2021-08-07T15:42:16.780867Z","iopub.status.idle":"2021-08-07T15:42:16.784706Z","shell.execute_reply.started":"2021-08-07T15:42:16.780837Z","shell.execute_reply":"2021-08-07T15:42:16.783827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_set = pd.read_csv('/kaggle/input/forest-cover-type-prediction/test.csv')\n# validation_set.head()\nids = validation_set['Id']\nvalidation_set.drop(['Id'], axis=1, inplace=True)\nvalidation_set[to_be_scaled_features] = sc_X.transform(validation_set[to_be_scaled_features])\ny_result = xgb.predict(validation_set)\n\ny_result = pd.Series(y_result, name='Cover_Type')\nids = pd.Series(ids, name='Id')\nsubmission = pd.concat([ids,y_result], axis=1)\nsubmission.to_csv('/kaggle/working/submission_xgb.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:42:16.785909Z","iopub.execute_input":"2021-08-07T15:42:16.786211Z","iopub.status.idle":"2021-08-07T15:42:51.949286Z","shell.execute_reply.started":"2021-08-07T15:42:16.786166Z","shell.execute_reply":"2021-08-07T15:42:51.948156Z"},"trusted":true},"execution_count":null,"outputs":[]}]}