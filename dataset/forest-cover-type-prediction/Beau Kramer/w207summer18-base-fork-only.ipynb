{"cells":[{"metadata":{"_uuid":"bf6859491a4f6d3ce41afb38b9a55a193895e54c","trusted":true,"collapsed":true,"_kg_hide-output":true},"cell_type":"code","source":"{\"cells\":[{\"metadata\":{\"_uuid\":\"8f2839f25d086af736a60e9eeb907d3b93b6e0e5\",\"_cell_guid\":\"b1076dfc-b9ad-4769-8c92-a6c4dae69d19\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\n# For example, here's several helpful packages to load in \\n\\nimport numpy as np # linear algebra\\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\\n\\n# Input data files are available in the \\\"../input/\\\" directory.\\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\\n\\nimport os\\nprint(os.listdir(\\\"../input\\\"))\\n\\n# Any results you write to the current directory are saved as output.\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_cell_guid\":\"79c7e3d0-c299-4dcb-8224-4455121ee9b0\",\"collapsed\":true,\"_uuid\":\"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a\",\"trusted\":false},\"cell_type\":\"code\",\"source\":\"\",\"execution_count\":null,\"outputs\":[]}],\"metadata\":{\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"},\"language_info\":{\"name\":\"python\",\"version\":\"3.6.4\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"}},\"nbformat\":4,\"nbformat_minor\":1}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68d179de712750aee5e4c8980f673ff131130eb9"},"cell_type":"markdown","source":"# Import packages"},{"metadata":{"trusted":true,"_uuid":"09a3bb4f8b256d646da2c8dbe25e4ea41f3ec0e4"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn import cross_validation # Use for train-test split\n\n# Packages for Normalization\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import Normalizer\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.dummy import DummyClassifier # For baselines\n# import warnings\n# warnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42e0323ed09752b247b1d6c57878f7a5b8e02541"},"cell_type":"markdown","source":"# Read and display the data"},{"metadata":{"trusted":true,"_uuid":"c418176aa0a92ac701400f95a4f39038d083d78a"},"cell_type":"code","source":"dataset = pd.read_csv(\"../input/train.csv\") \n#Drop the first column 'Id' since it just has serial numbers. Not useful in the prediction process.\ndataset = dataset.iloc[:,1:]\n#Look at a summary of the dataset\ndisplay(dataset.describe())\n# No missing data!!!!\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e55e609ec00402a0e4698689d78627033812aef"},"cell_type":"markdown","source":"## Remove unnecessary variables and examine class proportions"},{"metadata":{"trusted":true,"_uuid":"c26bf6a8c2f93d6416f68283712a9a1e05ced9c7"},"cell_type":"code","source":"# soil_type7 annd soil_type15 are constant so can be removed\ndataset.drop(['Soil_Type7','Soil_Type15'], axis=1, inplace=True)\n\ndataset.groupby('Cover_Type').size()\n# All classes have equal representation","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aed3ef817130a27904f236ddb422fbb15aabd7a0"},"cell_type":"markdown","source":"## Split Data Into Train and Dev"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4757515f55e413a33f8c031a45b295be3457498d"},"cell_type":"code","source":"r,c = dataset.shape\n# Extract just the values from the dataset\narray = dataset.values\nX = array[:,0:(c-1)] # Take all but the last column as the inputs\nY = array[:,(c-1)] # Take the last column as the output (Cover_Type)\nseed = 0\nval_size=0.1\nX_train, X_dev, y_train, y_dev = cross_validation.train_test_split(X, Y, test_size=val_size, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e7bc6402bf78459ce7e1c32fa7e54d59ecf40ea"},"cell_type":"markdown","source":"## Standardize Data"},{"metadata":{"trusted":true,"_uuid":"5807c6641b0424fccc688b35ca82da14a1aed776"},"cell_type":"code","source":"continuouscatbreakpoint = 10 # the first ten columns are continuous variables, the remainder are categorical\n# STANDARD SCALER\nX_temp = StandardScaler().fit_transform(X_train[:,0:continuouscatbreakpoint])\nX_dev_temp = StandardScaler().fit_transform(X_dev[:,0:continuouscatbreakpoint])\n# MINMAX SCALER\n# X_temp = MinMaxScaler().fit_transform(X_train[:,0:continuouscatbreakpoint])\n# X_dev_temp = MinMaxScaler().fit_transform(X_dev[:,0:continuouscatbreakpoint])\n# Normalizer\n# X_temp = Normalizer().fit_transform(X_train[:,0:continuouscatbreakpoint])\n# X_dev_temp = Normalizer().fit_transform(X_dev[:,0:continuouscatbreakpoint])\n\n\nX_train = np.concatenate((X_temp,X_train[:,continuouscatbreakpoint:]),axis=1)\nX_dev = np.concatenate((X_dev_temp,X_dev[:,continuouscatbreakpoint:]),axis=1)\n# EXAMINE THAT ONLY CONTINUOUS VARIABLES WERE CHANGED AND THAT COLUMNS ARE STILL CORRECT\ndf = pd.DataFrame(X_train)\ndf.columns = dataset.columns[:-1]\ndisplay(df.describe())\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ec20040e8d4b4a731936ddee337e061ed406607"},"cell_type":"markdown","source":"## Correlations"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"16a0f8387022916ffa815e01aab390f2e291bcbd"},"cell_type":"code","source":"# We will use these pandas dataframes of the training and dev data for some baselines\ndataset_train = pd.DataFrame(data=(X_train))\ndataset_train['Cover_Type'] = pd.Series(y_train)\ndataset_train.columns = dataset.columns\n\ndataset_dev = pd.DataFrame(data=(X_dev))\ndataset_dev['Cover_Type'] = pd.Series(y_dev)\ndataset_dev.columns = dataset.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c0afc895690a5fd43e278ff3871b0ff00fe92e22"},"cell_type":"code","source":"for variable in range(X_train.shape[1]):\n    rho = np.corrcoef(X_train[:,variable], y_train)[0,1]\n    if abs(rho) > 0.7:\n        print(dataset_train.columns[variable], np.corrcoef(X_train[:,variable], y_train)[0,1], \"***\")\n    elif abs(rho) > 0.5:\n        print(dataset_train.columns[variable], np.corrcoef(X_train[:,variable], y_train)[0,1], \"**\")\n    elif abs(rho) > 0.2:\n        print(dataset_train.columns[variable], np.corrcoef(X_train[:,variable], y_train)[0,1], \"*\")\n    else:\n        print(dataset_train.columns[variable], np.corrcoef(X_train[:,variable], y_train)[0,1])\n\n# Look at correlations of continuous variables\nsns.heatmap(df.iloc[:,:continuouscatbreakpoint].corr(), center=0, cmap=\"vlag\",annot=True)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c91cef6227291ab28fb04e133991c6bd17bcf83f"},"cell_type":"markdown","source":"## Baselines\n We will run a few baselines to get a sense of the complexity in the data.\n1.  The DummyClassifier Package\n2. We will predict based on the most common tree by soil_type."},{"metadata":{"_uuid":"18efbecb0623d7e48e052bd722fe6f877fe5b66d"},"cell_type":"markdown","source":"### DummyClassifier"},{"metadata":{"trusted":true,"_uuid":"3bac0f3284d5421395f6ef6a67608da39179b3a5"},"cell_type":"code","source":"# Run the various dummy classification strategies\ndummy_mf = DummyClassifier(strategy='most_frequent',random_state=0)\ndummy_mf.fit(X_train, y_train)\nprint(\"Most Frequent \",dummy_mf.score(X_dev, y_dev))\n\ndummy_st = DummyClassifier(strategy='stratified', random_state=0)\ndummy_st.fit(X_train, y_train)\nprint(\"Stratified\", dummy_st.score(X_dev, y_dev))\n\ndummy_pr = DummyClassifier(strategy='prior', random_state=0)\ndummy_pr.fit(X_train, y_train)\nprint(\"Prior\", dummy_pr.score(X_dev, y_dev))\n\ndummy_un = DummyClassifier(strategy='uniform', random_state=0)\ndummy_un.fit(X_train, y_train)\nprint(\"Uniform\", dummy_un.score(X_dev, y_dev))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e1aac00dc4211ed1c13017450b67416c3b3dbe0d"},"cell_type":"markdown","source":"### Predicting by soil_type\nThis naive strategy simply tallies the cover_type for each soil type. Then when predicting on new data, it simply predicts the most common cover_type for the soil_type in the new data."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d74906e399b6c27212f056e61e2b09764cf530e6"},"cell_type":"code","source":"# Undo one hot encoding\ndef get_soil(row):\n    for c in dataset_train.columns[14:]:\n        if row[c]==1:\n            return c\ndataset_train['Soil_Type'] = dataset_train.apply(get_soil, axis=1)\ndataset_dev['Soil_Type'] = dataset_dev.apply(get_soil, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bae1fba1928f1c86817eea4a0414a08ae5b6f760","scrolled":true},"cell_type":"code","source":"dataset_train.sort_values(\"Soil_Type\") # add column for soil_type\nsoil_counts = dataset_train.groupby('Soil_Type').Cover_Type.apply(lambda x: x.mode()) # Find the most common cover_type by soil_type\n# Clean up index\nsoil_counts = pd.DataFrame(soil_counts)\nsoil_counts.reset_index(inplace=True)\nsoil_counts.drop(\"level_1\",axis=1, inplace=True)\n# Add column labels\nsoil_counts.columns = [\"Soil_Type\",\"Most_Frequent_By_Soil_Type\"]\n\n# Assign the predicted cover_type to each datapoint\ndataset_train = dataset_train.merge(soil_counts[[\"Soil_Type\",\"Most_Frequent_By_Soil_Type\"]],on=[\"Soil_Type\"])\ndataset_dev = dataset_dev.merge(soil_counts[[\"Soil_Type\",\"Most_Frequent_By_Soil_Type\"]],on=[\"Soil_Type\"])\n\n# this is moving covertype so it is on the end of the dataframe\ndf1 = dataset_train.pop('Cover_Type') # remove column b and store it in df1\ndataset_train['Cover_Type'] = df1\ndf1 = dataset_dev.pop('Cover_Type') # remove column b and store it in df1\ndataset_dev['Cover_Type'] = df1\n\n# Calculate the accuracy of this strategy\ncorrect = 0\nfor row in range(dataset_dev.shape[0]):\n    if dataset_dev.iloc[row][\"Cover_Type\"] == dataset_dev.iloc[row][\"Most_Frequent_By_Soil_Type\"]:\n        correct += 1\n    else:\n        pass\ncorrect / dataset_dev.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"76a3863d95b2a8831ea5d29adc1106ca3a50334b","collapsed":true},"cell_type":"code","source":"parameters = np.arange(1,20)\nfor n in parameters:\n    knn = KNeighborsClassifier(n_neighbors=n)\n    knn.fit(X_train, y_train)\n    pred = knn.predict(X_dev)\n    print(n,accuracy_score(Y_dev, pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"edf39d6dbefad457a2cbcd0276189e2bfc4c282b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e5825021ea0512e506e49d0099186687f8adf93d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2dab164bbe3d3c3d63a940b80e332af03cac159b","collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}