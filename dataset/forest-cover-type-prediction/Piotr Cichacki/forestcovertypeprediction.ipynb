{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Forest Cover Type Prediction\n\n_[Kaggle competition](https://www.kaggle.com/c/forest-cover-type-prediction/overview)_\n\nAuthor: Piotr Cichacki<br/>\nDate: 18.02.2021"},{"metadata":{},"cell_type":"markdown","source":"<b>1) Goal: to predict the forest cover type from strictly cartographic variables.</b>\n\n<b>2) Data description</b><br/>\nThe training set (15120 observations) contains both features information and the cover type. Each observation is a 30m x 30m patch.<br/>\nThere are 4 binary columns for wilderness area and 40 binary columns for soil type in which 0 = absence and 1 = presence. <br/>\nSeven cover types (our target variable): spruce/fir, lodgepole pine, ponderosa pine, cottonwood/willow, aspen, douglas-fir, krummholz.\n\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train = pd.read_csv('../input/forest-cover-type-prediction/train.csv')\ntrain.drop('Id', axis=1, inplace=True)\ntest = pd.read_csv('../input/forest-cover-type-prediction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Quick overview of our data"},{"metadata":{"trusted":false},"cell_type":"code","source":"print(\"Training data shpae: \", train.shape)\nprint(\"Test data shpae: \", test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(\"There are missing values in the training dataset: \", train.isnull().sum().values.sum() > 0)\nprint(\"There are missing values in the test dataset: \", test.isnull().sum().values.sum() > 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Conclusions:\n\nWe have 56 columns in our train dataset but only 12 attributes (without ID and our target variable) because there are 4 columns dedicated to wilderness area and 40 columns dedicated to soil type. <br/>\nAll attributes are of type int (wilderness area and soil type columns are binary which means that they can have only value 0 or 1). <br/>\nWe do not have to deal with missing values in our datasets. "},{"metadata":{},"cell_type":"markdown","source":"### Our target variable: cover type"},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.title(\"Distribution of cover type\")\nsns.barplot(train['Cover_Type'].value_counts().index, train['Cover_Type'].value_counts().values)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that we have the same number of occurences for each type of cover."},{"metadata":{},"cell_type":"markdown","source":"### Exploratory data analysis"},{"metadata":{},"cell_type":"markdown","source":"At the beginning I will convert our train dataset to have Soil Type and Wilderness Area in single columns. Then I will present data in contingency table to check whether there is significant difference in proportions between groups. "},{"metadata":{"trusted":false},"cell_type":"code","source":"soil_type = train.loc[:, 'Soil_Type1':'Soil_Type40'].stack()\nsoil_type = pd.Series(soil_type[soil_type!=0].index.get_level_values(1))\nfor i in range(soil_type.size):\n    soil_type.values[i] = int((soil_type.values[i])[9:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"wilderness_area = train.loc[:, 'Wilderness_Area1':'Wilderness_Area4'].stack()\nwilderness_area = pd.Series(wilderness_area[wilderness_area!=0].index.get_level_values(1))\nfor i in range(wilderness_area.size):\n    wilderness_area.values[i] = int((wilderness_area.values[i])[15:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data = pd.concat([train.iloc[:, 0:10], wilderness_area, soil_type, train['Cover_Type']], axis=1)\ndata = data.rename(columns={0:'Wilderness_Area', 1:'Soil_Type'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pd.crosstab(data['Wilderness_Area'], data['Cover_Type'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.catplot(data=data, kind='count', x='Cover_Type', hue='Wilderness_Area')\nplt.title('Distribution of cover type between wilderness areas')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pd.crosstab(data['Cover_Type'], data['Soil_Type'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that wilderness area and soil type have significant influence on cover type because there are a lot of zeros in our tables which means that certain types of cover occur only in certain conditions."},{"metadata":{},"cell_type":"markdown","source":"Now let's focus on remaining attributes."},{"metadata":{"trusted":false},"cell_type":"code","source":"data.groupby(['Cover_Type']).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"columns = data.columns[:-3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"for column in columns:\n    sns.displot(data, x=data[column], hue='Cover_Type', kind='kde', fill=True, palette='Paired')\n    plt.title(column + ' distribution between cover types')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"for column in columns:\n    sns.boxplot(x='Cover_Type', y=column, data=data, palette='Paired')\n    plt.title(column + ' distribution between cover types')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"scaler = MinMaxScaler()\ntrain['Slope'] = scaler.fit_transform(np.array(train['Slope']).reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"columns = ['Elevation', 'Aspect', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', \n           'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm']\nscaler = MinMaxScaler()\nfor column in columns:\n    train[column] = scaler.fit_transform(np.array(train[column]).reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.heatmap(data.corr().round(2), annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Building model\n\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(train.drop(['Cover_Type'], axis=1), train['Cover_Type'], random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"lr = LogisticRegression(C=1)\nlr.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(\"Accuracy on training set: \", lr.score(X_train, y_train))\nprint(\"Accuracy on test set: \", lr.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators=1000)\nrfc.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(\"Accuracy on training set: \", rfc.score(X_train, y_train))\nprint(\"Accuracy on test set: \", rfc.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's now evaluate our model using K-fold cross-validation."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cross_val_score(rfc, X_train, y_train, cv=3, scoring='accuracy')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next step is to build confusion matrix to see how our model make predictions."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import cross_val_predict","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_train_pred = cross_val_predict(rfc, X_train, y_train, cv=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"conf_matrix = confusion_matrix(y_train, y_train_pred)\nconf_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.rcParams[\"figure.figsize\"] = (8, 8)\nplt.matshow(conf_matrix, interpolation ='nearest', cmap='plasma')\nplt.title(\"Confusion matrix\", fontdict={'fontsize':12})\nplt.colorbar()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"row_sums = conf_matrix.sum(axis=1, keepdims=True)\nnorm_conf_matrix = conf_matrix / row_sums","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"np.fill_diagonal(norm_conf_matrix, 0)\n\nplt.rcParams[\"figure.figsize\"] = (8, 8)\nplt.matshow(norm_conf_matrix, interpolation ='nearest', cmap='plasma')\nplt.title('Plot of the errors\\n (divided by number of observations in the corresponding class)', fontdict={'fontsize':12})\nplt.colorbar()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above plot we can see that many Lodgepole Pine (1 type) are classified as Spruce/Fir (0 type) and another way around. "},{"metadata":{},"cell_type":"markdown","source":"### Making predictions"},{"metadata":{"trusted":false},"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['Id'] = test['Id']\nsubmission['Cover_Type'] = rfc.predict(test.drop('Id', axis=1))\nsubmission.set_index('Id', inplace=True)\nsubmission.to_csv('submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}