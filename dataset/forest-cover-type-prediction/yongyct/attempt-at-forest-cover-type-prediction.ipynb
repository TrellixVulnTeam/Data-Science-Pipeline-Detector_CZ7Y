{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom datetime import datetime as dt\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n### Read Data ###\ndf = pd.read_csv('../input/forest-cover-type-prediction/train.csv')\n#### Check Null Data ###\nif df[df.isnull().any(axis=1) == True].shape[0] != 0:\n    print('Warning, null data present')\n\n### Transform / Wrangle Data ###\nX_train = df.iloc[:, :-1]\nY_train = df.iloc[:, -1]\n\nX_test = pd.read_csv('../input/forest-cover-type-prediction/test.csv')\nX_test_ids = X_test.iloc[:, 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Helper Classes/Functions"},{"metadata":{"trusted":false},"cell_type":"code","source":"class FeatureTransformer(TransformerMixin):\n    '''\n    Helper class for transforming input dataframes into desired input features. \n    Implements the feature engineering logic.\n    '''\n    def __init__(self):\n        pass\n    \n    def fit(self, X):\n        ignore_cols = ['Id']\n        for col in X.columns:\n            if X[col].std() == 0:\n                print('Columns to drop: {}, std={}'.format(col, X[col].std()))\n                ignore_cols.append(col)\n        self.ignore_cols = ignore_cols\n        return self\n    \n    def transform(self, X):\n        X = X.copy()\n        self.__clean_columns(X)\n        return X\n\n    def __clean_columns(self, X):\n        drop_cols = self.ignore_cols\n        for col in drop_cols:\n            if col not in X.columns:\n                drop_cols.remove(col)\n        X.drop(labels=self.ignore_cols, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def predict_results(estimator, X_test, X_test_ids):\n    '''\n    Helper function for predicting and saving test results\n    '''\n    Y_Pred = pd.DataFrame(estimator.predict(X_test), columns=['Cover_Type'])\n    results = pd.concat([X_test_ids, Y_Pred], axis=1)\n    results.to_csv('../input/forest-cover-type-prediction/submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def get_feature_importances(estimator, X):\n    return pd.DataFrame(\n        np.array([X.columns, estimator.feature_importances_]).T, \n        columns=['Features', 'Importance']\n    ).sort_values(by='Importance', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{"trusted":false},"cell_type":"code","source":"feature_transformer = FeatureTransformer()\nX_train = feature_transformer.fit_transform(X_train)\nX_test = feature_transformer.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using of Plain Single Algorithm Approaches\n1. LogisticRegression\n2. SVC\n3. ExtraTreesClassifier\n4. RandomForestClassifier\n5. LGBMClassifier\n6. XGBClassifier"},{"metadata":{},"cell_type":"markdown","source":"## 1. LogisticRegression\n<i>Approach not chosen as many iterations needed for convergence</i>"},{"metadata":{"trusted":false},"cell_type":"code","source":"# %%time\n# lrc = LogisticRegression()\n# param_grid = [\n#     {\n#         'n_jobs': [2],\n#         'solver': ['lbfgs', 'saga'],\n#         'tol': [1e-4, 1e-5],\n#         'C': [0.5, 1, 5],\n#         'multi_class': ['auto']\n#     }\n# ]\n\n# gscv = GridSearchCV(estimator=lrc, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=2)\n# gscv.fit(X_train, Y_train)\n# print('Best Params: ', gscv.best_params_)\n# print('Best Score: ', gscv.best_score_)\n\n# lrc = gscv.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. SVC"},{"metadata":{"trusted":false},"cell_type":"code","source":"# %%time\n# svc = SVC()\n# param_grid = [\n#     {\n#         'kernel': ['linear', 'rbf'],\n#         'tol': [1e-4, 0.001],\n#         'C': [0.5, 1, 5],\n#         'gamma': ['scale', 'auto']\n#     }\n# ]\n\n# gscv = GridSearchCV(estimator=svc, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=2)\n# gscv.fit(X_train, Y_train)\n# print('Best Params: ', gscv.best_params_)\n# print('Best Score: ', gscv.best_score_)\n\n# svc = gscv.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. RandomForestClassifier"},{"metadata":{"trusted":false},"cell_type":"code","source":"# %%time\n# rfc = RandomForestClassifier()\n# param_grid = [\n#     {\n#         'n_jobs': [2],\n#         'criterion': ['gini', 'entropy'], \n#         'n_estimators': [200, 500, 700], \n#         'max_depth': [3, 15, 30, None],\n#         'max_features': [0.3, 0.6, 'auto']\n#     }\n# ]\n\n# gscv = GridSearchCV(estimator=rfc, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=2)\n# gscv.fit(X_train, Y_train)\n# print('Best Params: ', gscv.best_params_)\n# print('Best Score: ', gscv.best_score_)\n\n# rfc = gscv.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. ExtraTreesClassifier"},{"metadata":{"trusted":false},"cell_type":"code","source":"# %%time\n# etc = ExtraTreesClassifier()\n# param_grid = [\n#     {\n#         'n_jobs': [2],\n#         'criterion': ['gini', 'entropy'], \n#         'n_estimators': [200, 500, 700], \n#         'max_depth': [3, 15, 30, None],\n#         'max_features': [0.3, 0.6, 'auto']\n#     }\n# ]\n\n# gscv = GridSearchCV(estimator=etc, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=2)\n# gscv.fit(X_train, Y_train)\n# print('Best Params: ', gscv.best_params_)\n# print('Best Score: ', gscv.best_score_)\n\n# etc = gscv.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. LGBMClassifier"},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"# %%time\n# lgbmc = LGBMClassifier()\n# param_grid = [\n#     {\n#         'n_jobs': [4],\n#         'max_depth': [2, 3, -1], \n#         'n_estimators': [150, 200, 250], \n#         'num_leaves': [31, 45, 63, 67],\n#         'learning_rate': [0.15, 0.2, 0.25],\n#         'reg_lambda': [0, 1.5]\n#     }\n# ]\n\n# gscv = GridSearchCV(estimator=lgbmc, param_grid=param_grid, n_jobs=4, scoring='accuracy', cv=5)\n# gscv.fit(X_train, Y_train)\n# print('Best Params: ', gscv.best_params_)\n# print('Best Score: ', gscv.best_score_)\n\n# lgbmc = gscv.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. XGBClassifier\n<i>XGBoost Best Params:  {'max_depth': 2, 'n_estimators': 50, 'n_threads': 4, 'reg_lambda': 1.6, 'tree_method': 'hist'}\nXGBoost Best Score:  0.658531746031746</i>"},{"metadata":{"trusted":false},"cell_type":"code","source":"# %%time\n# xgbc = XGBClassifier()\n# param_grid = [\n#     {\n#         'n_jobs': [4],\n#         'max_depth': [2, 3, 10, len(X_train.columns)],\n#         'n_estimators': [50, 100, 200], \n#         'reg_lambda': [0, 1.6]\n#     }\n# ]\n\n# gscv = GridSearchCV(estimator=xgbc, param_grid=param_grid, n_jobs=4, scoring='accuracy', cv=5)\n# gscv.fit(X_train, Y_train)\n# print('Best Params: ', gscv.best_params_)\n# print('Best Score: ', gscv.best_score_)\n\n# xgbc = gscv.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Best Classifiers of Each Algorithm Tested"},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"%%time\nlrc = LogisticRegression(solver='lbfgs', multi_class='auto')\nsvc = SVC(gamma='scale')\nrfc = RandomForestClassifier(criterion='entropy', max_features=0.6, n_estimators=500, n_jobs=6)\netc = ExtraTreesClassifier(criterion='entropy', max_features=0.6, n_estimators=500, n_jobs=6)\nlgbmc = LGBMClassifier(learning_rate=0.2, n_estimators=200, num_leaves=63, n_jobs=6)\nxgbc = XGBClassifier(max_depth=2, n_estimators=50, reg_lambda=1.6, tree_method='hist', n_jobs=6)\n\nprint('LogisticRegression Accuracy: ', cross_val_score(estimator=lrc, X=X_train, y=Y_train, scoring='accuracy', cv=3))\nprint('SVC Accuracy: ', cross_val_score(estimator=svc, X=X_train, y=Y_train, scoring='accuracy', cv=3))\nprint('RandomForestClassifier Accuracy: ', cross_val_score(estimator=rfc, X=X_train, y=Y_train, scoring='accuracy', cv=3))\nprint('ExtraTreesClassifier Accuracy: ', cross_val_score(estimator=etc, X=X_train, y=Y_train, scoring='accuracy', cv=3))\nprint('LGBMClassifier Accuracy: ', cross_val_score(estimator=lgbmc, X=X_train, y=Y_train, scoring='accuracy', cv=3))\nprint('XGBClassifier Accuracy: ', cross_val_score(estimator=xgbc, X=X_train, y=Y_train, scoring='accuracy', cv=3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<strong>ExtraTreesClassifier works best here</strong>"},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\netc = ExtraTreesClassifier(criterion='entropy', max_features=0.6, n_estimators=500, n_jobs=6)\n# Fitting best estimator\netc.fit(X_train, Y_train)\n# Predicting and getting output prediction file\npredict_results(estimator=etc, X_test=X_test, X_test_ids=X_test_ids)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Kaggle Score:\n* <strong style=\"color: green\">ExtraTreesClassifier - 0.78275</strong>\n* RandomForestClassifier - 0.75646\n* LGBMClassifier - 0.76851\n* XGBClassifier - 0.58489"},{"metadata":{"trusted":false},"cell_type":"code","source":"get_feature_importances(etc, X_train).head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Extra Part: Exploring Ensemble Methods\n### Variables are split and grouped in 4 segments:\n#### 1) Soil Group Vars --> RFC to get proba\n#### 2) Wilderness Area Group Vars --> RFC to get proba\n#### 3) Inclination Group Vars --> RFC or LGBMC to get proba\n#### 4) Spatial Group Vars --> LGBMC to get proba\n"},{"metadata":{},"cell_type":"markdown","source":"## Perform Study\n### Prepare Group Vars"},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"print(X_train.columns)\nprint(X_train.columns.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_soil = X_train.loc[:, ['Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5',\n       'Soil_Type6', 'Soil_Type8', 'Soil_Type9', 'Soil_Type10', 'Soil_Type11',\n       'Soil_Type12', 'Soil_Type13', 'Soil_Type14', 'Soil_Type16',\n       'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20',\n       'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24',\n       'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28',\n       'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32',\n       'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36',\n       'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40']]\nX_wild_area = X_train.loc[:, ['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4']]\nX_incline = X_train.loc[:, ['Aspect', 'Slope', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm']]\nX_spatial = X_train.loc[:, ['Elevation', 'Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology', \n                      'Horizontal_Distance_To_Roadways','Horizontal_Distance_To_Fire_Points']]\n\nX_soil_test = X_test.loc[:, ['Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5',\n       'Soil_Type6', 'Soil_Type8', 'Soil_Type9', 'Soil_Type10', 'Soil_Type11',\n       'Soil_Type12', 'Soil_Type13', 'Soil_Type14', 'Soil_Type16',\n       'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20',\n       'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24',\n       'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28',\n       'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32',\n       'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36',\n       'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40']]\nX_wild_area_test = X_test.loc[:, ['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4']]\nX_incline_test = X_test.loc[:, ['Aspect', 'Slope', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm']]\nX_spatial_test = X_test.loc[:, ['Elevation', 'Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology', \n                      'Horizontal_Distance_To_Roadways','Horizontal_Distance_To_Fire_Points']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Soil Group Vars"},{"metadata":{"trusted":false},"cell_type":"code","source":"# %%time\n# ### Soil Type RF Classifier ###\n# rfc_soil = RandomForestClassifier()\n# #### Perform GridSearchCV to optimize params ####\n# rfc_soil_param_grid = [\n#     {\n#         'n_jobs': [6],\n#         'n_estimators': [10, 100, 150],\n#         'max_depth': [3, 5, None],\n#         'criterion': ['gini', 'entropy']\n#     }\n# ]\n\n# rfc_gscv = GridSearchCV(\n#     estimator=rfc_soil, \n#     param_grid=rfc_soil_param_grid, \n#     scoring='neg_log_loss', \n#     cv=5, n_jobs=6\n# )\n# rfc_gscv.fit(X_soil, Y)\n\n# print('RFC Soil Best Params: ', rfc_gscv.best_params_)\n# print('RFC Soil Best Score: ', rfc_gscv.best_score_)\n\n# #### Get best estimator and predict proba ####\n# rfc_soil = rfc_gscv.best_estimator_\n# Y_proba_soil_test = rfc_soil.predict_proba(X_soil_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Wilderness Area Group Vars"},{"metadata":{"trusted":false},"cell_type":"code","source":"# %%time\n# ### Wilderness Area RF Classifier ###\n# rfc_wild_area = RandomForestClassifier()\n# #### Perform GridSearchCV to optimize params ####\n# rfc_wild_area_param_grid = [\n#     {\n#         'n_jobs': [6],\n#         'n_estimators': [75, 100, 125],\n#         'max_depth': [2, None],\n#         'criterion': ['gini', 'entropy']\n#     }\n# ]\n\n# rfc_wild_area_gscv = GridSearchCV(\n#     estimator=rfc_wild_area, \n#     param_grid=rfc_wild_area_param_grid, \n#     scoring='neg_log_loss', \n#     cv=5, n_jobs=6\n# )\n# rfc_wild_area_gscv.fit(X_wild_area, Y)\n\n# print('RFC Wilderness Best Params: ', rfc_wild_area_gscv.best_params_)\n# print('RFC Wilderness Best Score: ', rfc_wild_area_gscv.best_score_)\n\n# #### Get best estimator and predict proba ####\n# rfc_wild_area = rfc_wild_area_gscv.best_estimator_\n# Y_proba_wild_area_test = rfc_wild_area.predict_proba(X_wild_area_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Inclination Group Vars"},{"metadata":{"trusted":false},"cell_type":"code","source":"# %%time\n# ### Inclination RF Classifier ###\n# rfc_incline = RandomForestClassifier()\n# #### Perform GridSearchCV to optimize params ####\n# rfc_incline_param_grid = [\n#     {\n#         'n_jobs': [6],\n#         'n_estimators': [10, 100, 150, 200],\n#         'max_depth': [3, 5, None],\n#         'criterion': ['gini', 'entropy']\n#     }\n# ]\n\n# rfc_incline_gscv = GridSearchCV(\n#     estimator=rfc_incline, \n#     param_grid=rfc_incline_param_grid, \n#     scoring='neg_log_loss', \n#     cv=5, n_jobs=6\n# )\n# rfc_incline_gscv.fit(X_incline, Y)\n\n# print('RFC Inclination Best Params: ', rfc_incline_gscv.best_params_)\n# print('RFC Inclination Best Score: ', rfc_incline_gscv.best_score_)\n\n# #### Get best estimator and predict proba ####\n# rfc_incline = rfc_incline_gscv.best_estimator_\n# Y_proba_incline_test = rfc_incline.predict_proba(X_incline_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Spatial Group Vars"},{"metadata":{"trusted":false},"cell_type":"code","source":"# %%time\n# ### Inclination RF Classifier ###\n# lgbmc_spatial = LGBMClassifier()\n# #### Perform GridSearchCV to optimize params ####\n# lgbmc_spatial_param_grid = [\n#     {\n#         'n_jobs': [6],\n#         'n_estimators': [200, 250, 275],\n#         'learning_rate': [0.125, 0.15, 0.175, 0.2],\n#         'num_leaves': [65, 67, 70]\n#     }\n# ]\n\n# lgbmc_spatial_gscv = GridSearchCV(\n#     estimator=lgbmc_spatial, \n#     param_grid=lgbmc_spatial_param_grid, \n#     scoring='neg_log_loss', \n#     cv=5, n_jobs=6\n# )\n# lgbmc_spatial_gscv.fit(X_spatial, Y)\n\n# print('LGBMC Spatial Best Params: ', lgbmc_spatial_gscv.best_params_)\n# print('LGBMC Spatial Best Score: ', lgbmc_spatial_gscv.best_score_)\n\n# #### Get best estimator and predict proba ####\n# lgbmc_spatial = lgbmc_spatial_gscv.best_estimator_\n# Y_proba_spatial_test = lgbmc_spatial.predict_proba(X_spatial_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Get final prediction\n### Methods:\n* 1) Get weights for group vars, and use softmax to derive final probabilities + one-hot class for final prediction\n* 2) Build another ensemble estimator from the other estimators, and make final prediction\n* 3) TODO: find out ways to feed group vars outputs as intermediate inputs, and feed to another estimator for making final prediction"},{"metadata":{},"cell_type":"markdown","source":"## 1) Weights for each output\n### i) Prepare wrapper classes for each classifier"},{"metadata":{"trusted":false},"cell_type":"code","source":"class SegmentClassifier(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, classifier, columns):\n        self.classifier = classifier\n        self.columns = columns\n    \n    def fit(self, X, y):\n        X = X.loc[:, self.columns]\n        self.classifier.fit(X, y)\n        return self\n    \n    def predict(self, X):\n        X = X.loc[:, self.columns]\n        return self.classifier.predict(X)\n    \n    def predict_proba(self, X):\n        X = X.loc[:, self.columns]\n        return self.classifier.predict_proba(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\nsoil_classifier = SegmentClassifier(\n    classifier=RandomForestClassifier(criterion='entropy', n_estimators=100, n_jobs=6),\n    columns=['Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5',\n       'Soil_Type6', 'Soil_Type8', 'Soil_Type9', 'Soil_Type10', 'Soil_Type11',\n       'Soil_Type12', 'Soil_Type13', 'Soil_Type14', 'Soil_Type16',\n       'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20',\n       'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24',\n       'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28',\n       'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32',\n       'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36',\n       'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40']\n)\n\nwild_area_classifier = SegmentClassifier(\n    classifier=RandomForestClassifier(criterion='gini', n_estimators=100, n_jobs=6),\n    columns=['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4']\n)\n\nincline_classifier = SegmentClassifier(\n    classifier=RandomForestClassifier(criterion='entropy', n_estimators=150, max_depth=5, n_jobs=6),\n    columns=['Aspect', 'Slope', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm']\n)\n\nspatial_classifier = SegmentClassifier(\n    classifier=LGBMClassifier(learning_rate=0.125, n_estimators=200, num_leaves=65, n_jobs=6),\n    columns=['Elevation', 'Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology', \n                      'Horizontal_Distance_To_Roadways','Horizontal_Distance_To_Fire_Points']\n)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"# ensemble_classifier = VotingClassifier(\n#     estimators=[\n#         ('soil_classifier', soil_classifier),\n#         ('wild_area_classifier', wild_area_classifier),\n#         ('incline_classifier', incline_classifier),\n#         ('spatial_classifier', spatial_classifier)\n#     ]\n# )","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# %%time\n# param_grid = [\n#     {\n#         'voting': ['soft', 'hard'],\n#         'weights': [[1,1,2,16], [1,2,3,4], [1,1,4,10]]\n#     }\n# ]\n# gscv = GridSearchCV(ensemble_classifier, param_grid=param_grid, n_jobs=4, cv=5)\n# gscv.fit(X_train, Y_train)\n# print('Best Params: ', gscv.best_params_)\n# print('Best Score: ', gscv.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# predict_results(estimator=gscv.best_estimator_, X_test=X_test, X_test_ids=X_test_ids)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Kaggle Score: 0.69541"},{"metadata":{},"cell_type":"markdown","source":"Score is lower than the single <strong>LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n               importance_type='split', learning_rate=0.15, max_depth=-1,\n               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n               n_estimators=250, n_jobs=6, num_leaves=70, objective=None,\n               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)</strong>.\n               \nHow about ensembling the single classifier with the ensemble classifier above?\n\n* Method 1 => Use the 4 SegmentClassifiers + 1 LGBMClassifier in 1 VotingClassifier\n* Method 2 => Use a new VotingClassifier with VotingClassifier from the SegmentClassifers + LGBMClassifer as estimators"},{"metadata":{"trusted":false},"cell_type":"code","source":"### Method 1 ###\nensemble_classifier = VotingClassifier(\n    estimators=[\n        ('soil_classifier', soil_classifier),\n        ('wild_area_classifier', wild_area_classifier),\n        ('incline_classifier', incline_classifier),\n        ('spatial_classifier', spatial_classifier),\n        ('original_classifier', etc)\n    ]\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\nparam_grid = [\n    {\n        'voting': ['soft'],\n        'weights': [[1,1,2,3,5], [1,2,3,4,5], [0,0,0,0,1], [0,0,0,2,5]]\n#         'weights': [[0,0,0,3,5], [0,0,0,2,5]]\n    }\n]\ngscv = GridSearchCV(ensemble_classifier, param_grid=param_grid, n_jobs=6, cv=5)\ngscv.fit(X_train, Y_train)\nprint('Best Params: ', gscv.best_params_)\nprint('Best Score: ', gscv.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"predict_results(estimator=gscv.best_estimator_, X_test=X_test, X_test_ids=X_test_ids)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Kaggle Score (from earlier score of params = {'voting': 'soft', 'weights': [0, 0, 0, 0, 1]}: 0.76769):\n* params = {'voting': 'soft', 'weights': [1, 1, 2, 3, 5]}: 0.76706\n* params = {'voting': 'soft', 'weights': [0, 0, 2, 3, 5]}: 0.76787\n* params = {'voting': 'soft', 'weights': [0, 0, 0, 3, 5]}: 0.76817\n* params = {'voting': 'soft', 'weights': [0, 0, 0, 2, 5]}: 0.76866"},{"metadata":{},"cell_type":"markdown","source":"To try more complex feature engineering as approach to increase score (since ensemble does not increase the score much)\n# Feature Engineering Approach"},{"metadata":{},"cell_type":"markdown","source":"Trying out approach by https://www.kaggle.com/jianyu/my-first-submission"},{"metadata":{"trusted":false},"cell_type":"code","source":"class FeatureTransformer(TransformerMixin):\n    '''\n    Implementing __enhance_columns method to add more sophisticated features.\n    '''\n    def __init__(self):\n        pass\n    \n    def fit(self, X):\n        ignore_cols = ['Id']\n        for col in X.columns:\n            if X[col].std() == 0:\n                print('Columns to drop: {}, std={}'.format(col, X[col].std()))\n                ignore_cols.append(col)\n        self.ignore_cols = ignore_cols\n        return self\n    \n    def transform(self, X):\n        X = X.copy()\n        self.__clean_columns(X)\n        self.__enhance_columns(X)\n        return X\n\n    def __clean_columns(self, X):\n        drop_cols = self.ignore_cols\n        for col in drop_cols:\n            if col not in X.columns:\n                drop_cols.remove(col)\n        X.drop(labels=self.ignore_cols, axis=1, inplace=True)\n        \n    def __enhance_columns(self, X):\n        X.loc[:, 'Distance_To_Hydrology'] = (X.loc[:, 'Horizontal_Distance_To_Hydrology'] ** 2 \n            + X.loc[:, 'Vertical_Distance_To_Hydrology'] ** 2) ** 0.5\n        X.loc[:, 'Distance_To_Amenities_Avg'] = X.loc[:, [\n            'Horizontal_Distance_To_Hydrology', \n            'Horizontal_Distance_To_Roadways', \n            'Horizontal_Distance_To_Fire_Points'\n        ]].mean(axis=1)\n        X.loc[:, 'Elevation_Minus_Disthy'] = X.loc[:, 'Elevation'] - X.loc[:, 'Vertical_Distance_To_Hydrology']\n        X.loc[:, 'Elevation_Plus_Disthy'] = X.loc[:, 'Elevation'] + X.loc[:, 'Vertical_Distance_To_Hydrology']\n        X.loc[:, 'Disthx_Minus_Distfx'] = X.loc[:, 'Horizontal_Distance_To_Hydrology'] - X.loc[:, 'Horizontal_Distance_To_Fire_Points']\n        X.loc[:, 'Disthx_Plus_Distfx'] = X.loc[:, 'Horizontal_Distance_To_Hydrology'] + X.loc[:, 'Horizontal_Distance_To_Fire_Points']\n        X.loc[:, 'Disthx_Minus_Distrx'] = X.loc[:, 'Horizontal_Distance_To_Hydrology'] - X.loc[:, 'Horizontal_Distance_To_Roadways']\n        X.loc[:, 'Disthx_Plus_Distrx'] = X.loc[:, 'Horizontal_Distance_To_Hydrology'] + X.loc[:, 'Horizontal_Distance_To_Roadways']\n        X.loc[:, 'Distfx_Minus_Distrx'] = X.loc[:, 'Horizontal_Distance_To_Fire_Points'] - X.loc[:, 'Horizontal_Distance_To_Roadways']\n        X.loc[:, 'Distfx_Minus_Distrx'] = X.loc[:, 'Horizontal_Distance_To_Fire_Points'] - X.loc[:, 'Horizontal_Distance_To_Roadways']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\nfeature_transformer_new = FeatureTransformer()\nX_train = feature_transformer_new.fit_transform(X_train)\nX_test = feature_transformer_new.transform(X_test)\n\netc = ExtraTreesClassifier(criterion='entropy', max_features=0.6, n_estimators=500, n_jobs=6)\n# Fitting best estimator\netc.fit(X_train, Y_train)\n# Predicting and getting output prediction file\npredict_results(estimator=etc, X_test=X_test, X_test_ids=X_test_ids)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Kaggle Score: 0.80805"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_feature_importances(etc, X_train).head(10)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":1}