{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# forest cover\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef sigmoid(z):\n    a=np.exp(-z)\n    a=1+a\n    a=1/a\n    return a\n\ndata=np.loadtxt(\"../input/train.csv\",delimiter=',',skiprows=1)\n\nm=np.shape(data)[0]\nn=np.shape(data)[1]\nx=data[:,1:n-1]            # excluded ids and final ans\ny=data[:,n-1]             #  contains final ans\n\nx_2=x**2            #MODEL2 X+X**2\nx=np.column_stack((x,x_2))\nx_3=x**3            #MODEL2 X+X**2\nx=np.column_stack((x,x_3))\nx_4=x**4            #MODEL2 X+X**2\nx=np.column_stack((x,x_4))\nn=np.shape(x)[1]\n#MODEL 4 IS MODIFIED MORE COMPLEX MODEL 2\nprint (\"reached 1\")\nfor q in range(0,n-1):\n#    for w in range(q+1):\n#        temp=(x[:,q])*(x[:,w])\n#        x=np.column_stack((x,temp))\n     temp=(x[:,q])*(x[:,q+1])\n     x=np.column_stack((x,temp))\n                    \nprint (\"reached 2\")\n\n\nn=np.shape(x)[1]\n\n\ny_orig=y\n# feature scale the data\nx_avg=np.mean(x,0)\nx_std=np.std(x,0)\na=(x_std==0)\nb=(a==False)\n#x=x[:,b]\n#x_std[a]=1\nx_avg=np.mean(x[:,b],0)\nx_std=np.std(x[:,b],0)\nx[:,b]=(x[:,b]-x_avg)/x_std\n\ny1=(y==1)\n#modify y into m*7\nfor i in range(2,8):\n    y2=(y==i)\n    y1=np.column_stack((y1,y2))\n\ny=y1\nn_y=np.shape(y)[1]\n# divide 80-20\na=np.arange(0,m)\nnp.random.shuffle(a)\nm_train=int(0.8*m)\na1=a[0:m_train]\na2=a[m_train:]\nx_train=x[a1]\nx_test=x[a2]\ny_train=y[a1]\ny_test=y[a2]\nm_test=np.shape(x_test)[0]\n\n# add coloumn of 1\no=np.ones(m_train)\nx_train=np.column_stack((o,x_train))\no=np.ones(m_test)\nx_test=np.column_stack((o,x_test))\nn=np.shape(x_train)[1]\n\n\n# m_train,m_test,n,n_y\n\ntheta=np.zeros((n,n_y))\nh=sigmoid(np.dot(x_train,theta))\nlamb=0\ncost=(sum((-y_train)*(np.log(h))-(1-y_train)*np.log(1-h)))/m_train\nx_train_t=np.transpose(x_train)\ngrad=(np.dot(x_train_t,h-y_train))/m_train\nalpha=0.03\nj_hist=[]           # empty list so that can append\n\ni=0\n\n\nwhile(True):                \n    theta[1:]=theta[1:]*(1-((alpha*lamb)/m_train))-alpha*grad[1:] #update rule\n    theta[0]=theta[0]-alpha*grad[0]                               #update rule for theta0\n    h=sigmoid(np.dot(x_train,theta))\n    grad=(np.dot(x_train_t,h-y_train))/m_train\n    #j_hist[i]=(sum((-y_train)*(np.log(h))-(1-y_train)*np.log(1-h)))/m_train\n    #j_hist.append((sum((-y_train)*(np.log(h))-(1-y_train)*np.log(1-h)))/m_train)\n    j_hist.append((sum((y_train)*(-np.log(h))-(1-y_train)*np.log(1-h),0))/m_train)\n    if(i==400):\n        itera=i    \n        break\n    i=i+1\n\nprint (\"reached 3\")    \n            \nj_hist=np.array(j_hist)    # converting list to array\nx_axis=np.linspace(0,itera-1,200)\nx_axis=x_axis.astype(int)        \nplt.plot(x_axis,j_hist[x_axis],label='MODEL 1')\nplt.legend(loc='upper right')\nplt.show()\n\n# prediction code on training data (80%)  MODEL 1\n\npred1=sigmoid(np.dot(x_train,theta))\nind=np.argmax(pred1,1)\nfor i in range(m_train):\n    pred1[i]=0\n\nfor i in range(m_train):\n    pred1[i,ind[i]]=1\nqq=np.zeros(m_train)\nfor i in range(m_train):\n    qq[i]=np.array_equal(pred1[i],y_train[i])\n\nacc=sum(qq)\nacc=float(acc)\nacc=(acc/m_train)*100\nprint (\"MODEL 1: train accuracy is\" ,acc,\"%\")\n\n\n# prediction code on test data (20%)  MODEL 1\n\npred2=sigmoid(np.dot(x_test,theta))\nind=np.argmax(pred2,1)\nfor i in range(m_test):\n    pred2[i]=0\n\nfor i in range(m_test):\n    pred2[i,ind[i]]=1\nqq=np.zeros(m_test)\nfor i in range(m_test):\n    qq[i]=np.array_equal(pred2[i],y_test[i])       \nacc=sum(qq)\nacc=float(acc)\nacc=(acc/m_test)*100\nprint (\"MODEL 1: test accuracy is\" ,acc,\"%\")\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}