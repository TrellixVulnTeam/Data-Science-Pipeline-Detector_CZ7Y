{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\n# Restrict minor warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n\n# Import test and train data\ndf_train = pd.read_csv('../input/forest-cover-type-prediction/train.csv')\ndf_Test = pd.read_csv('../input/forest-cover-type-prediction/test.csv')\ndf_test = df_Test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All of the features in our datasets are numerical, except for Wilderness Area, Soil Type, and ****Cover Type**** "},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns', None) # we need to see all the columns\ndf_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# frequency count of column A \ncount_train = df_train['Cover_Type'].value_counts() \nprint(count_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,5))\nplt.title(\"Distribution of forest categories(Target Variable)\")\nax = sns.distplot(df_train[\"Cover_Type\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Exploration results**\n  \n* each class is having 2160 entries and \n* The id is unique for each entry so it can be removed\n*  Total 15120 entries and each column  is having the same number of entries, we can conclude no missing values \n* Each value of Soil_Type7 and Soil_Type15 are equal to zero, so we can remove these attributes.\n*  Rescaling and normalization may need because the scales are different for different attribute \n*  Wilderness_Area[1-4] and Soil_Type[1-40] are one-hot encoded. \n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop 'Id'\ndf_train = df_train.iloc[:,1:]\ndf_test = df_test.iloc[:,1:]\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removing ID,Soil_Type7 and Soil_Type15\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop Soil_Type7 and Soil_Type15\ndf_train =df_train.drop(['Soil_Type7','Soil_Type15'], axis = 1)\ndf_test =df_test.drop(['Soil_Type7','Soil_Type15'], axis = 1)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Correlation analysis of countinous attributes using heatmap\n* Ignore biary valued attributes like Wilderness_Area[1-4] and Soil_Type[1-40] for the correlation analysis\n  \n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"no_of_continuous_attributes = 10\ncorrelation_matrix =df_train.iloc[:,:no_of_continuous_attributes].corr()\nf, ax = plt.subplots(figsize = (10,8))\nsns.heatmap(correlation_matrix,vmax=0.8,square=True);\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Correlation values\n\ndata = df_train.iloc[:,:no_of_continuous_attributes]\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get name of the continuous attributes\ncols = data.columns\nprint(cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the pearson correlation coefficients for all combinations\ndata_corr = data.corr()\n\n# Threshold ( only highly correlated ones matter)\nthreshold = 0.5\ncorr_list = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get correlation matrix\ndata_corr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Bubble sorting of correlation array\n             \n                \n    for i in range(0, no_of_continuous_attributes):\n        for j in range(i+1, no_of_continuous_attributes):\n            if data_corr.iloc[i,j]>= threshold and data_corr.iloc[i,j]<1\\\n                or data_corr.iloc[i,j] <0 and data_corr.iloc[i,j]<=-threshold:\n                    corr_list.append([data_corr.iloc[i,j],i,j])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sort the correlation values\n\ns_corr_list = sorted(corr_list,key= lambda x: -abs(x[0]))\n\n# print the higher values\nfor v,i,j in s_corr_list:\n    print(\"%s and %s = %.2f\" % (cols[i], cols[j], v))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Skewness  of countinous attributes**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.iloc[:,:10].skew()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Skewness is present in all the attribute  **"},{"metadata":{},"cell_type":"markdown","source":"****Exploring the relationship between attributes****"},{"metadata":{"trusted":true},"cell_type":"code","source":"for v,i,j in s_corr_list:\n    sns.pairplot(data = df_train, hue='Cover_Type', size= 10, x_vars=cols[i], y_vars=cols[j])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 'Horizontal_Distance_To_Hydrology' and  'Vertical_Distance_To_Hydrology' seems to have a linear relation\n* Hillside and Aspect seems to have a sigmoid relation"},{"metadata":{},"cell_type":"markdown","source":"**Violin Plot**\nViolin Plot is a method to visualize the distribution of numerical data of different variables. It is similar to Box Plot but with a rotated plot on each side, giving more information about the density estimate on the y-axis."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_train.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = df_train.columns\n#target value is not needed\nsize = len(columns)-1\n\n# x-axis has target attributes to distinguish between classes\nx = columns[size]\ny = columns[0:size]\n\nfor i in range(0, size):\n    sns.violinplot(data=df_train, x=x, y=y[i])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****\n* Elevation has a seperate distribution for each class, hence an important attribute for prediction\n* Aspect plot contains couple of normal distribution for '1' and '6'classes\n* 'Horizontal_Distance_To_Hydrology' and 'Horizontal_Distance_To_Roadways' are quite similar\n* 'Hillshade_9am'and  'Hillshade_Noon'are left skewed\n* In 'Wilderness_Area3' all the classes are distributed similarly\n* 'Wilderness_Area1','Wilderness_Area2' and 'Wilderness_Area4' offer class distinction as values are not present for many classes\n* Soil_Type, 1,5,8,9,12,14,18-22, 25-30 and 35-40 offer class distinction as values are not present for many classes"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_train.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.Wilderness_Area1.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.Wilderness_Area2.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.Wilderness_Area3.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.Wilderness_Area4.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#grouping of one-hot encoded variables into a single variable\ncols = df_train.columns\nr,c = df_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a new dataframe with r rows, one column for each encoded category[Wilderness_Area(1-4),Soil_type[1-40], and target in the end\nnew_data = pd.DataFrame(index= np.arange(0,r), columns=['Wilderness_Area', 'Soil_Type', 'Cover_Type'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Make an entry in data for each r for category_id, target_value\nfor i in range(0,r):\n    p = 0;\n    q = 0;\n    # Category1_range\n    for j in range(10,14):\n        if (df_train.iloc[i,j] == 1):\n            p = j-9 # category_class\n            break\n    # Category2_range\n    for k in range(14,54):\n        if (df_train.iloc[i,k] == 1):\n            q = k-13 # category_class\n            break\n    # Make an entry in data for each r\n    new_data.iloc[i] = [p,q,df_train.iloc[i, c-1]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot for category1\nsns.countplot(x = 'Wilderness_Area', hue = 'Cover_Type', data = new_data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Wilderness_Area4 has lot of presence of cover_type 4, good class distinction\n\n"},{"metadata":{},"cell_type":"markdown","source":"An attribut is having too many zero values means that shows class distiction because Wilderness_Area[1-4]are one hot encoded data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot for category2\nplt.rc(\"figure\", figsize = (25,10))\nsns.countplot(x='Soil_Type', hue = 'Cover_Type', data= new_data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SoilType 1-6,9-13,15, 20-22, 27-31,35,36-38 offer lot of class distinction as counts for some are very high. in all these one or two types of covertype is dominating .This type of attributes will help to classify the data ."},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculating the count of soil_type[1-40] as well as  Wilderness_Area[1-4] \nfor i in range(10,df_train.shape[1]-1):\n    j = df_train.columns[i]\n    print (df_train[j].value_counts())\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 'Soil_Type8'and  'Soil_Type25'having one count towrds 1. It wont contribute much for the classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's drop them\ndf_train = df_train.drop(['Soil_Type8', 'Soil_Type25'], axis=1)\ndf_test = df_test.drop(['Soil_Type8', 'Soil_Type25'], axis=1)\ndf_train1 = df_train # To be used for algos like SVM where we need normalization and StandardScaler\ndf_test1 = df_test # To be used under normalization and StandardScaler","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Normality checking**\n* Needed for a few algorithms like SVM\n* Consider only non categorical values \n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.iloc[:,:10].skew()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data transformation needed in: 'Horizontal_Distance_To_Hydrology,Horizontal_Distance_To_Roadways and Vertical_Distance_To_Hydrology  ', 'Hillshade_9am & Hillshade_noon'**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Horizontal_Distance_To_Hydrology\nfrom scipy import stats\nplt.figure(figsize=(8,6))\nsns.distplot(df_train1['Horizontal_Distance_To_Hydrology'], fit = stats.norm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(8,6))\nres = stats.probplot(df_train1['Horizontal_Distance_To_Hydrology'], plot=plt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These plots shows the Horizontal_Distance_To_Hydrology is right/poitively skewed .It shows positive skewness (log or squared transformations will be a good option)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train2=df_train1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train1['Horizontal_Distance_To_Hydrology'] = np.sqrt(df_train1['Horizontal_Distance_To_Hydrology'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot again after sqrt transformation\nplt.figure(figsize=(8,6))\nsns.distplot(df_train1['Horizontal_Distance_To_Hydrology'], fit = stats.norm)\nfig = plt.figure(figsize=(8,6))\nres = stats.probplot(df_train1['Horizontal_Distance_To_Hydrology'], plot=plt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Both the transformations are working properly for this data but squared one gives better result "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,6))\nsns.distplot(df_train1['Horizontal_Distance_To_Roadways'], fit = stats.norm)\nfig = plt.figure(figsize=(8,6))\nres = stats.probplot(df_train1['Horizontal_Distance_To_Roadways'], plot=plt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train1['Horizontal_Distance_To_Roadways'] = np.sqrt(df_train1['Horizontal_Distance_To_Roadways'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot again after sqrt transformation\nplt.figure(figsize=(8,6))\nsns.distplot(df_train1['Horizontal_Distance_To_Roadways'], fit = stats.norm)\nfig = plt.figure(figsize=(8,6))\nres = stats.probplot(df_train1['Horizontal_Distance_To_Roadways'], plot=plt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot again after sqrt transformation\nplt.figure(figsize=(8,6))\nsns.distplot(df_train2['Horizontal_Distance_To_Roadways'], fit = stats.norm)\nfig = plt.figure(figsize=(8,6))\nres = stats.probplot(df_train2['Horizontal_Distance_To_Roadways'], plot=plt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Hillshade_9am\nfig = plt.figure(figsize=(8,6))\nsns.distplot(df_train1['Hillshade_9am'],fit=stats.norm)\nfig = plt.figure(figsize=(8,6))\nres = stats.probplot(df_train1['Hillshade_9am'],plot=plt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****Negative skewness****"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train1['Hillshade_9am'] = np.square(df_train1['Hillshade_9am'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot again after square transformation\nplt.figure(figsize=(8,6))\nsns.distplot(df_train1['Hillshade_9am'], fit = stats.norm)\nfig = plt.figure(figsize=(8,6))\nres = stats.probplot(df_train1['Hillshade_9am'], plot=plt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Squared data transformation is good **"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hillshade_Noon\nfig = plt.figure(figsize=(8,6))\nsns.distplot(df_train1['Hillshade_Noon'],fit=stats.norm)\nfig = plt.figure(figsize=(8,6))\nres = stats.probplot(df_train1['Hillshade_Noon'],plot=plt)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Negative skewness"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train1['Hillshade_Noon'] = np.square(df_train1['Hillshade_Noon'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hillshade_Noon\nfig = plt.figure(figsize=(8,6))\nsns.distplot(df_train1['Hillshade_Noon'],fit=stats.norm)\nfig = plt.figure(figsize=(8,6))\nres = stats.probplot(df_train1['Hillshade_Noon'],plot=plt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,6))\nsns.distplot(df_train1['Horizontal_Distance_To_Fire_Points'], fit=stats.norm)\nplt.figure(figsize=(8,6))\nres = stats.probplot(df_train1['Horizontal_Distance_To_Fire_Points'],plot=plt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train1['Horizontal_Distance_To_Fire_Points'] = np.sqrt(df_train1['Horizontal_Distance_To_Fire_Points'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot again after sqrt transformation\nplt.figure(figsize=(8,6))\nsns.distplot(df_train1['Horizontal_Distance_To_Fire_Points'], fit=stats.norm)\nplt.figure(figsize=(8,6))\nres = stats.probplot(df_train1['Horizontal_Distance_To_Fire_Points'],plot=plt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Vertical_Distance_To_Hydrology\nplt.figure(figsize=(8,6))\nsns.distplot(df_train1['Vertical_Distance_To_Hydrology'], fit = stats.norm)\nfig = plt.figure(figsize=(8,6))\nres = stats.probplot(df_train1['Vertical_Distance_To_Hydrology'], plot=plt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,6))\nsns.distplot(df_train1['Vertical_Distance_To_Hydrology'], fit=stats.norm)\nplt.figure(figsize=(8,6))\nres = stats.probplot(df_train1['Vertical_Distance_To_Hydrology'],plot=plt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# performing same transformation in test dataset \ndf_test1[['Horizontal_Distance_To_Hydrology','Horizontal_Distance_To_Fire_Points'\\\n        ,'Horizontal_Distance_To_Roadways']] = np.sqrt(df_test1[['Horizontal_Distance_To_Hydrology',\\\n        'Horizontal_Distance_To_Fire_Points','Horizontal_Distance_To_Roadways']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test1[['Hillshade_9am','Hillshade_Noon']] = np.square(df_test1[['Hillshade_9am','Hillshade_Noon']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"******Training and Testing  Data******"},{"metadata":{},"cell_type":"markdown","source":"**Preprocessing**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Taking only non-categorical values\nSize = 10\nX_temp = df_train.iloc[:,:Size]\nX_test_temp = df_test.iloc[:,:Size]\nX_temp1 = df_train1.iloc[:,:Size]\nX_test_temp1 = df_test1.iloc[:,:Size]\na = df_train.iloc[:,:Size]\n\n\nX_temp1 = StandardScaler().fit_transform(X_temp1)\nX_test_temp1 = StandardScaler().fit_transform(X_test_temp1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r,c = df_train.shape\nprint(df_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r,c = df_train.shape\nX_train = np.concatenate((X_temp,df_train.iloc[:,Size:c-1]),axis=1)\nX_train1 = np.concatenate((X_temp1, df_train1.iloc[:,Size:c-1]), axis=1) # to be used for SVM\ny_train = df_train.Cover_Type.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"******Support Vector Machine******"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm as svm\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting parameters\nx_data, x_test_data, y_data, y_test_data = train_test_split(X_train1,y_train,test_size=0.2, random_state=123)\nsvm_para = [{'kernel':['rbf'],'C': [1,10,100,100]}]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parameters optimized using the code in above cell\nC_opt = 10 # reasonable option\nclf = svm.SVC(C=C_opt,kernel='rbf')\nclf.fit(X_train1,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.score(X_train1,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#y_pred = clf.predict(X_test1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****ExtraTreesClassifier****"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# setting parameters\nx_data, x_test_data, y_data, y_test_data = train_test_split(X_train,y_train,test_size= 0.3, random_state=0)\netc_para = [{'n_estimators':[20,30,100], 'max_depth':[5,10,15], 'max_features':[0.1,0.2,0.3]}] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ETC = GridSearchCV(ExtraTreesClassifier(),param_grid=etc_para, cv=3, n_jobs=-1)\n#ETC.fit(x_data, y_data)\n#ETC.best_params_\n#ETC.grid_scores_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# setting parameters\nx_data, x_test_data, y_data, y_test_data = train_test_split(X_train,y_train,test_size= 0.3, random_state=0)\netc_para = [{'n_estimators':100, 'max_depth':15, 'max_features':0.3}] \n# Default number of features is sqrt(n)\n# Default number of min_samples_leaf is 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classification Report\nY_pred = ETC.predict(x_test_data)\ntarget = ['class1', 'class2','class3','class4','class5','class6','class7' ]\nprint (classification_report(y_test_data, Y_pred, target_names=target))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****Learning Curve for Extra Trees Classifier****"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\ndef plot_learning_curve(model,title, X, y,n_jobs = 1, ylim = None, cv = None,train_sizes = np.linspace(0.1, 1, 5)):\n    \n    # Figrue parameters\n    plt.figure(figsize=(10,8))\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel('Training Examples')\n    plt.ylabel('Score')\n    \n    train_sizes, train_score, test_score = learning_curve(model, X, y, cv = cv, n_jobs=n_jobs, train_sizes=train_sizes)\n\n    # Calculate mean and std\n    train_score_mean = np.mean(train_score, axis=1)\n    train_score_std = np.std(train_score, axis=1)\n    test_score_mean = np.mean(test_score, axis=1)\n    test_score_std = np.std(test_score, axis=1)\n    \n    plt.grid()\n    plt.fill_between(train_sizes, train_score_mean - train_score_std, train_score_mean + train_score_std,\\\n                    alpha = 0.1, color = 'r')\n    plt.fill_between(train_sizes, test_score_mean - test_score_std, test_score_mean + test_score_std,\\\n                    alpha = 0.1, color = 'g')\n    \n    plt.plot(train_sizes, train_score_mean, 'o-', color=\"r\", label=\"Training score\")\n    plt.plot(train_sizes, test_score_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n    \n    plt.legend(loc = \"best\")\n    return plt\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 'max_features': 0.3, 'n_estimators': 100, 'max_depth': 15, 'min_samples_leaf: 1'\netc = ExtraTreesClassifier(bootstrap=True, oob_score=True, n_estimators=100, max_depth=10, max_features=0.3, \\\n                           min_samples_leaf=1)\n\netc.fit(X_train, y_train)\n# yy_pred = etc.predict(X_test)\netc.score(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting learning curve\ntitle = 'Learning Curve (ExtraTreeClassifier)'\n# cross validation with 50 iterations to have a smoother curve\ncv = ShuffleSplit(n_splits=50, test_size=0.2, random_state=0)\nmodel = etc\nplot_learning_curve(model,title,X_train, y_train, n_jobs=-1,ylim=None,cv=cv)\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}