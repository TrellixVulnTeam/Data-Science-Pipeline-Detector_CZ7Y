{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Classify the presence and correct placement of tubes on chest x-rays to save lives\n\nNote: This notebook is still in progress.\nAny suggestions are welcome.\n\n***Please don't forget to upvote, if you find this helpful.***"},{"metadata":{},"cell_type":"markdown","source":"## TODO\n\n* Improve readibility\n    * Introductory Write-ups\n    * More comments\n* Image preprocessing\n    * Current architecture requires 3 channel inputs, need to fix it.\n    * Image preprocessing to improve the clarity of images\n    * More augmentation\n    * Different image sizes\n* Class balancing\n    * Weighted Loss Functions\n    * Oversampling\n* Architecture tuning\n* Ensembling"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# ! pip install -q efficientnet >> /dev/null","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{},"cell_type":"markdown","source":"## Import modules"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport re\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import classification_report, roc_auc_score, roc_curve, confusion_matrix\n\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.mixed_precision import experimental as mixed_precision","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/tfresnestregnetdetrgenet')\n\nfrom models.model_factory import get_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\n# sys.path.append('../input/efficientnet-keras-source-code')\n# from efficientnet as efn","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Set parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_PATH = '/kaggle/input/ranzcr-clip-catheter-line-classification'\n\nMODEL_PATH = '/kaggle/working/models'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DEVICE = 'TPU' # ['CPU' GPU' 'TPU']\n\nENABLE_MIXED_PRECISION = True # [True False]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 42\n\nFOLDS = 5 \n\nIMG_SIZE = 224 #512\n\nBATCH_SIZE = 64 # [8, 16, 32, 64, 128, 256, 512]\n\nEPOCHS = 25\n\nVERBOSE = 1 # [0: silent, 1: progress bar, 2: single line]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_TF_RECS = len(os.listdir(f'{DATA_PATH}/train_tfrecords'))\n\nprint(NUM_TF_RECS)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Setup devices and settings"},{"metadata":{"trusted":true},"cell_type":"code","source":"# For kaggle tpus\nfrom kaggle_datasets import KaggleDatasets\nif DEVICE == 'TPU':\n    DATA_PATH = KaggleDatasets().get_gcs_path(DATA_PATH.split('/')[-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if DEVICE == 'CPU':\n\n    strategy = tf.distribute.get_strategy()\n    print('\\nUsing Default Distribution Strategy  for CPU')\n\n\nif DEVICE == 'GPU':\n\n    gpu_accelerarors = tf.config.list_physical_devices('GPU')\n        \n    if len(gpu_accelerarors) > 1:\n        strategy = tf.distribute.MirroredStrategy()\n        print(f'Number of GPUs available: {len(gpu_accelerarors)}')\n        print('\\n Using Mirrored Distribution Strategy')\n        \n    else:\n        strategy = tf.distribute.get_strategy()\n        if len(gpu_accelerarors) == 1:\n            print(f'Number of GPUs available: 1')\n            print('\\nUsing Default Distribution Strategy for GPU')\n        else:\n            print('ERROR: GPU not available')\n            print('\\nUsing Default Distribution Strategy  for CPU')\n        \nif DEVICE == 'TPU':\n\n    try:\n        resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(resolver)\n        tf.tpu.experimental.initialize_tpu_system(resolver)\n        strategy = tf.distribute.experimental.TPUStrategy(resolver)\n        tpu_accelerarors = tf.config.list_logical_devices('TPU')\n        print(f'Number of TPU cores available: {len(tpu_accelerarors)}')\n        print(f'\\nUsing TPU Distribution Strategy')\n        \n    except:\n        print('ERROR: TPU not available')\n        print('\\nUsing Default Distribution Strategy for CPU')\n        strategy = tf.distribute.get_strategy()\n        \n        \nif ENABLE_MIXED_PRECISION:\n    \n    print('\\nMixed Precision enabled:')\n    \n    if DEVICE == 'GPU':\n        policy = mixed_precision.Policy('mixed_float16')\n        \n    if DEVICE == 'TPU':\n        policy = mixed_precision.Policy('mixed_bfloat16')\n        \n    mixed_precision.set_policy(policy)\n    \n    print('\\t...Compute dtype: %s' % policy.compute_dtype)\n    print('\\t...Variable dtype: %s' % policy.variable_dtype)\n\n\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'\\nREPLICAS: {REPLICAS}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Helper functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Dataset:\n    \n    feature_description = {\n        \"StudyInstanceUID\"           : tf.io.FixedLenFeature([], tf.string),\n        \"image\"                      : tf.io.FixedLenFeature([], tf.string),\n        \"ETT - Abnormal\"             : tf.io.FixedLenFeature([], tf.int64), \n        \"ETT - Borderline\"           : tf.io.FixedLenFeature([], tf.int64), \n        \"ETT - Normal\"               : tf.io.FixedLenFeature([], tf.int64), \n        \"NGT - Abnormal\"             : tf.io.FixedLenFeature([], tf.int64), \n        \"NGT - Borderline\"           : tf.io.FixedLenFeature([], tf.int64), \n        \"NGT - Incompletely Imaged\"  : tf.io.FixedLenFeature([], tf.int64), \n        \"NGT - Normal\"               : tf.io.FixedLenFeature([], tf.int64), \n        \"CVC - Abnormal\"             : tf.io.FixedLenFeature([], tf.int64), \n        \"CVC - Borderline\"           : tf.io.FixedLenFeature([], tf.int64), \n        \"CVC - Normal\"               : tf.io.FixedLenFeature([], tf.int64), \n        \"Swan Ganz Catheter Present\" : tf.io.FixedLenFeature([], tf.int64),\n    }\n    \n    def __init__(self, image_size):\n        self.image_size = image_size\n        \n    def parse_function(self, example_proto):\n        example = tf.io.parse_single_example(example_proto, self.feature_description)\n        image = tf.io.decode_image(example['image'], channels=3)\n        label = [example['ETT - Abnormal'],\n                 example['ETT - Borderline'],\n                 example['ETT - Normal'],\n                 example['NGT - Abnormal'],\n                 example['NGT - Borderline'],\n                 example['NGT - Incompletely Imaged'],\n                 example['NGT - Normal'],\n                 example['CVC - Abnormal'],\n                 example['CVC - Borderline'],\n                 example['CVC - Normal'],\n                 example['Swan Ganz Catheter Present']]\n        return image, label \n    \n    def augment_function(self, image, label):\n        image = tf.image.random_flip_left_right(image)\n        image = tf.image.random_flip_up_down(image)\n        image = tf.image.random_contrast(image, 0.8, 1.2)\n        image = tf.image.random_brightness(image, 0.1)   \n        return image, label \n    \n    def process_function(self, image, label):\n        image.set_shape([None, self.image_size, self.image_size, 3])\n        label.set_shape([None, 11])\n        image = tf.image.resize(image, [224, 244], 'bilinear')/255 #[self.image_size, self.image_size], 'bilinear')/255\n        return image, label\n            \n    def generator(self, files, batch_size=1, repeat=False, augment=False, shuffle=True):\n        AUTO = tf.data.experimental.AUTOTUNE\n        ds = tf.data.TFRecordDataset(files, num_parallel_reads=AUTO)\n        if shuffle: \n            opt = tf.data.Options()\n            opt.experimental_deterministic = False\n            ds = ds.with_options(opt)\n            ds = ds.shuffle(2000)\n        ds = ds.map(self.parse_function, num_parallel_calls=AUTO)\n        if repeat:\n            ds = ds.repeat()\n        if augment:\n            ds = ds.map(self.augment_function, num_parallel_calls=AUTO)\n        ds = ds.batch(batch_size)\n        ds = ds.map(self.process_function, num_parallel_calls=AUTO)\n        ds = ds.prefetch(AUTO)\n        return ds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EfficientNet"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"def create_model(name, input_shape, classes, output_bias=None):\n    \n    # Dictionary mapping name to model function\n    \n    EFFICIENT_NETS = {'B0': efn.EfficientNetB0, \n                      'B1': efn.EfficientNetB1, \n                      'B2': efn.EfficientNetB2, \n                      'B3': efn.EfficientNetB3, \n                      'B4': efn.EfficientNetB4, \n                      'B5': efn.EfficientNetB5, \n                      'B6': efn.EfficientNetB6,\n                      'B7': efn.EfficientNetB7}\n    \n    # Output layer bias initialization\n    \n    if output_bias is None:\n        output_bias = 'zeros'\n    else:\n        output_bias = tf.keras.initializers.Constant(output_bias)\n        \n    \n    # Base model\n    \n    base_model = EFFICIENT_NETS[name](include_top=False, \n                                      weights='imagenet', \n                                      input_shape=input_shape)\n    \n    # Model\n    \n    inputs = tf.keras.Input(shape=input_shape)\n    x = base_model(inputs)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = tf.keras.layers.Dense(classes, bias_initializer=output_bias)(x)\n    outputs = tf.keras.layers.Activation('sigmoid', dtype='float32')(x) # Supports mixed-precision training\n    \n    model = tf.keras.Model(inputs, outputs)\n    \n    return model"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"def compile_model(model, lr=0.0001):\n    \n    optimizer = tf.keras.optimizers.Adam(lr=lr)\n    \n    loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.05)\n        \n    metrics = [\n        tf.keras.metrics.AUC(name='auc')\n    ]\n\n    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n\n    return model"},{"metadata":{},"cell_type":"markdown","source":"## Callbacks"},{"metadata":{"trusted":true},"cell_type":"code","source":"if not os.path.exists(MODEL_PATH):\n        os.makedirs(MODEL_PATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_callbacks(model_save_path, fold, verbose=1):\n    \n    verbose = int(verbose>0)\n    \n    if not os.path.exists(model_save_path):\n        os.makedirs(model_save_path)\n    \n    cpk_path = f'{model_save_path}/model-f{fold}.h5'\n\n    checkpoint = ModelCheckpoint(\n        filepath=cpk_path,\n        monitor='val_auc',\n        mode='max',\n        save_best_only=True,\n        save_weights_only=True,\n        verbose=verbose\n    )\n\n    reducelr = ReduceLROnPlateau(\n        monitor='val_auc',\n        mode='max',\n        factor=0.1,\n        patience=3,\n        verbose=0\n    )\n\n    earlystop = EarlyStopping(\n        monitor='val_auc',\n        mode='max',\n        patience=10, \n        verbose=verbose\n    )\n    \n    callbacks = [checkpoint, reducelr, earlystop] #\n    \n    return callbacks","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## count_items for counting the remained steps"},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Main Training Pipeline\n\n***Note: Running for 1 Fold only for experimental purpose***"},{"metadata":{"trusted":true},"cell_type":"code","source":"folds_val_auc = [None] * FOLDS # Store the validation auc for each fold\n\nskf = KFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n\nprint(f'Training...')\n\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(np.arange(NUM_TF_RECS))):\n    \n    print(f'\\n\\n{\"*\"*100} \\nFOLD: {fold+1}')\n    \n    # Input Pipeline ******************************************************\n    \n    train_files = tf.io.gfile.glob(f'{DATA_PATH}/train_tfrecords/{idx:02}*.tfrec' for idx in train_idx)\n    valid_files = tf.io.gfile.glob(f'{DATA_PATH}/train_tfrecords/{idx:02}*.tfrec' for idx in valid_idx)\n    \n    ds = Dataset(IMG_SIZE)\n    \n    train_ds = ds.generator(train_files, \n                            BATCH_SIZE*REPLICAS, \n                            repeat=True, \n                            augment=True, \n                            shuffle=True)\n\n    valid_ds = ds.generator(valid_files, \n                            BATCH_SIZE*REPLICAS,  \n                            repeat=False, \n                            augment=False, \n                            shuffle=False)\n    \n    # Calculate the steps_per_epoch\n    \n    steps_per_epoch = count_items(train_files)//(BATCH_SIZE*REPLICAS) * 2\n    \n    \n    # Build Model ******************************************************\n    \n    tf.keras.backend.clear_session()\n        \n    with strategy.scope():\n        model = get_model(model_name='ResNest50',\n                          input_shape=[224,244,3],\n                          n_classes=11,\n                          fc_activation='softmax',\n                          active='relu', # relu or mish\n                          verbose=True)\n        # model.summary()\n        model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(), metrics =[tf.keras.metrics.AUC(name='auc')])\n    \n    print(f'\\nModel initialized and compiled: ResNest50')\n    \n#         model = create_model(name=EFF_NET, \n#                      input_shape=(IMG_SIZE,IMG_SIZE,3), \n#                      classes=11)\n#         model = compile_model(model, lr=0.0001)\n        \n#     print(f'\\nModel initialized and compiled: EfficientNet-{EFF_NET}')\n    \n    \n        \n    # Train ******************************************************\n   \n    callbacks = create_callbacks(MODEL_PATH, fold+1, verbose=VERBOSE)\n\n    print(f'\\nModel training...\\n')\n    \n    history = model.fit(train_ds, \n                        epochs=EPOCHS, #EPOCHS, \n                        steps_per_epoch=steps_per_epoch,\n                        validation_data=valid_ds,\n                        callbacks=callbacks,\n                        verbose=True)\n\n#     break\n#     # Save acc for each fold in a list\n#     folds_val_auc[fold] = max(history.history['val_auc'])\n    \n#     print(f'\\nModel trained \\n\\nFOLD-{fold+1} Validation AUC = {folds_val_auc[fold]}')\n    \n#     break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Traing Graph"},{"metadata":{"trusted":true},"cell_type":"code","source":"history.history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\ndef plot_hist(hist):\n    plt.plot(hist.history['auc'], 'r', label='train auc')\n    plt.plot(hist.history['val_auc'], 'g', label='val auc')\n    plt.title(\"model auc\")\n    plt.ylabel(\"auc\")\n    plt.xlabel(\"epoch\")\n    plt.legend(loc='upper left')\n    plt.show()\n\n\nplot_hist(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"regnety400  \n```\nelif model_name == 'resnest_detr':\n            model = ResNest50_DETR(verbose=verbose, input_shape=input_shape,\n            n_classes=n_classes, dropout_rate=dropout_rate, fc_activation=fc_activation, **kwargs).build()\n    elif model_name == 'genet_light':\n            model = GENet(verbose=verbose, model_name='light',input_shape=input_shape,\n            n_classes=n_classes, fc_activation=fc_activation, **kwargs).build()\n    elif model_name == 'genet_normal':\n            model = GENet(verbose=verbose, model_name='normal',input_shape=input_shape,\n            n_classes=n_classes, fc_activation=fc_activation, **kwargs).build()\n    elif model_name == 'genet_large':\n            model = GENet(verbose=verbose, model_name='large',input_shape=input_shape,\n            n_classes=n_classes, fc_activation=fc_activation, **kwargs).build()\n```"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"# Inference\n- (Run cells Above) Setup devices and settings"},{"metadata":{},"cell_type":"markdown","source":"## Import modules"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re,os,cv2,random\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom kaggle_datasets import KaggleDatasets\nimport tensorflow as tf\nimport albumentations as A\nfrom sklearn.model_selection import train_test_split\n\n# importing neural network architecture\n# from efficientnet.tfkeras import EfficientNetB7\n\n# importing other useful tools: layers, optimizers, loss functions\nfrom tensorflow.keras.layers import Flatten,Dense,Dropout,BatchNormalization\nfrom tensorflow.keras.models import Model,Sequential\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import models, layers\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\nfrom keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n\n# ignoring warnings\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\n%matplotlib inline \nprint(\"Tensorflow version \" + tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Path Setting"},{"metadata":{"trusted":true},"cell_type":"code","source":"WORK_DIR = '../input/ranzcr-clip-catheter-line-classification'\nos.listdir(WORK_DIR)\n\nGCS_DS_PATH = KaggleDatasets().get_gcs_path('ranzcr-clip-catheter-line-classification')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data connection\ntrain = pd.read_csv(os.path.join(WORK_DIR, \"train.csv\"))\n# train.display(5)\n\ntrain_images = GCS_DS_PATH + \"/train/\" + train['StudyInstanceUID'] + '.jpg'\n\nss = pd.read_csv(os.path.join(WORK_DIR, 'sample_submission.csv'))\ntest_images = GCS_DS_PATH + \"/test/\" + ss['StudyInstanceUID'] + '.jpg'\n\nlabel_cols = ss.columns[1:]\nlabels = train[label_cols].values\n\n# train_annot = pd.read_csv(os.path.join(WORK_DIR, \"train_annotations.csv\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# main parameters\nAUTO = tf.data.experimental.AUTOTUNE\n# TARGET_SIZE = 224","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_decoder(with_labels = True,\n                  target_size = (224, 244), \n                  ext = 'jpg'):\n    def decode(path):\n        file_bytes = tf.io.read_file(path)\n        if ext == 'png':\n            img = tf.image.decode_png(file_bytes, channels = 3)\n        elif ext in ['jpg', 'jpeg']:\n            img = tf.image.decode_jpeg(file_bytes, channels = 3)\n        else:\n            raise ValueError(\"Image extension not supported\")\n\n        img = tf.cast(img, tf.float32) / 255.0\n        img = tf.image.resize(img, target_size)\n\n        return img\n    \n    def decode_with_labels(path, label):\n        return decode(path), label\n    \n    return decode_with_labels if with_labels else decode\n\n# in this part you can choose any type of augmentation from the ones suggested above\ndef build_augmenter(with_labels = True):\n    def augment(img):\n        #img = NeedleAugmentation(img, n_needles=2, dark_needles=False, p=0.5)\n        img = tf.image.random_flip_left_right(img)\n        img = tf.image.random_flip_up_down(img)\n        img = tf.image.random_brightness(img, 0.9, 1)\n        img = tf.image.random_contrast(img, 0.9, 1)\n        #img = tf.image.random_saturation(img, 0.9, 1) \n        \n        return img\n    \n    def augment_with_labels(img, label):\n        return augment(img), label\n    \n    return augment_with_labels if with_labels else augment\n\n\ndef build_dataset(paths, labels = None, bsize = 32, cache = True,\n                  decode_fn = None, augment_fn = None,\n                  augment = True, repeat = True, shuffle = 1024, \n                  cache_dir = \"\"):\n    if cache_dir != \"\" and cache is True:\n        os.makedirs(cache_dir, exist_ok=True)\n    \n    if decode_fn is None:\n        decode_fn = build_decoder(labels is not None)\n    \n    if augment_fn is None:\n        augment_fn = build_augmenter(labels is not None)\n    \n    slices = paths if labels is None else (paths, labels)\n    \n    dset = tf.data.Dataset.from_tensor_slices(slices)\n    dset = dset.map(decode_fn, num_parallel_calls = AUTO)\n    dset = dset.cache(cache_dir) if cache else dset\n    dset = dset.map(augment_fn, num_parallel_calls = AUTO) if augment else dset\n    dset = dset.repeat() if repeat else dset\n    dset = dset.shuffle(shuffle) if shuffle else dset\n    dset = dset.batch(bsize).prefetch(AUTO)\n    \n    return dset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = build_dataset(\n    test_images, bsize = BATCH_SIZE, repeat = False, \n    shuffle = False, augment = False, cache = False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\npred = model.predict(test_df, verbose=1)\npred.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    modelld = get_model(model_name='ResNest50',\n                        input_shape=[224,244,3],\n                        n_classes=11,\n                        fc_activation='softmax',\n                        active='relu', # relu or mish\n                        verbose=True)\n    modelld.load_weights('./models/model-f1.h5')\nmodelld.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"maxpred = modelld.predict(test_df, verbose=1)\nmaxpred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submmit"},{"metadata":{"trusted":true},"cell_type":"code","source":"sm_df = pd.DataFrame(columns=train.columns.tolist()[:-1])\nfor i, p in enumerate(maxpred):\n    sm_df = sm_df.append(pd.Series([ss['StudyInstanceUID'][i]] + list(map(float,p)), index=sm_df.columns), ignore_index=True)\nsm_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_df = pd.DataFrame(columns=train.columns.tolist()[:-1])\nfor i, p in enumerate(pred):\n    tr_df = tr_df.append(pd.Series([ss['StudyInstanceUID'][i]] + list(map(float,p)), index=tr_df.columns), ignore_index=True)\ntr_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sm_df.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tr_df.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model.save_weights(\"resnest50_weights.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model.save(\"resnest50.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# np.savetxt('test.csv',[1,2,3,4,5], fmt='%f', delimiter=',', header='StudyInstanceUID,ETT - Abnormal,ETT - Borderline,ETT - Normal,NGT - Abnormal', comments='')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print('Generating submission.csv file...')\n# # test_ids_ds = test_df.map(lambda image, idnum: idnum).unbatch()\n# # test_ids = next(iter()).numpy().astype('U') # all in one batch\n\n# np.savetxt('submission.csv', np.rec.fromarrays([ss['StudyInstanceUID'][i]] + [*pred[i]]), fmt=['%s', '%f','%f' , '%f', '%f','%f' , '%f', '%f','%f' , '%f', '%f','%f'  ], delimiter=',', header='StudyInstanceUID,ETT - Abnormal,ETT - Borderline,ETT - Normal,NGT - Abnormal,NGT - Borderline,NGT - Incompletely Imaged,NGT - Normal,CVC - Abnormal,CVC - Borderline,CVC - Normal,Swan Ganz Catheter Present', comments='')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"def activation_layer_vis(img, activation_layer = 0, layers = 10):\n    layer_outputs = [layer.output for layer in model.layers[:layers]]\n    activation_model = models.Model(inputs = model.input, outputs = layer_outputs)\n    activations = activation_model.predict(img)\n    \n    rows = int(activations[activation_layer].shape[3] / 3)\n    cols = int(activations[activation_layer].shape[3] / rows)\n    fig, axes = plt.subplots(rows, cols, figsize = (15, 15 * cols))\n    axes = axes.flatten()\n    \n    for i, ax in zip(range(activations[activation_layer].shape[3]), axes):\n        ax.matshow(activations[activation_layer][0, :, :, i], cmap = 'viridis')\n        ax.axis('off')\n    plt.tight_layout()\n    plt.show()"},{"metadata":{},"cell_type":"markdown","source":"activation_layer_vis(img_tensor)"},{"metadata":{},"cell_type":"markdown","source":"img_tensor = build_dataset(\n    pd.Series(train_img[0]), bsize = 1,repeat = False, \n    shuffle = False, augment = False, cache = False)"},{"metadata":{},"cell_type":"markdown","source":"x = tf.keras.layers.Lambda(lambda x: tf.math.divide(x, tf.reduce_max(x)))(x)"},{"metadata":{},"cell_type":"markdown","source":"embed_layer = tf.keras.layers.Lambda(lambda x: x + get_position_encoding(timesteps, embed_size))(embed_layer)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}