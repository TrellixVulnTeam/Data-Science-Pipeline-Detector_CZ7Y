{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import stuff\nimport random\nimport numpy as np\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport pandas as pd\nfrom torch.utils.data import Dataset\nfrom glob import glob\nimport random\nimport torch.distributed as dist\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nfrom torchvision.transforms import ToTensor\nimport torchvision\nimport cv2\nfrom sklearn.metrics import roc_curve, auc\nimport os\nimport pytorch_lightning as pl\nimport torch.nn.functional as F\nimport re\nimport ast\nimport random\nfrom torchvision import models\nfrom pytorch_lightning.loggers import WandbLogger\nfrom scipy import signal","metadata":{"execution":{"iopub.status.busy":"2022-06-19T22:58:09.560571Z","iopub.execute_input":"2022-06-19T22:58:09.561054Z","iopub.status.idle":"2022-06-19T22:58:17.999278Z","shell.execute_reply.started":"2022-06-19T22:58:09.560943Z","shell.execute_reply":"2022-06-19T22:58:17.998427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Dataset():\n\n    def __init__(self, file_list, mode, ett_annotations, carina_annotations):\n        self.file_list = file_list\n        self.mode = mode\n        self.ett_annotations = ett_annotations\n        self.carina_annotations = carina_annotations\n\n    def __getitem__(self,idx):\n        image_filepath = self.file_list[idx]\n        image, code, original_imsize = self.loadimage(image_filepath)\n        \n        gt_map, ett_coords = self.get_GT(code, original_imsize)\n#         for i in range(0,len(image)):\n#             image[i] = self.crop_center(image[i], ett_coords)\n        \n        sample = {'Image': image,\n                  'GT': gt_map,\n                  'Code': code,\n                  'ETT_coords': ett_coords,\n              }\n\n        return sample\n\n    def __len__(self):\n        return len(self.file_list)\n\n    def loadimage(self, image_filepath):\n        img = cv2.imread(image_filepath, 0)\n        original_imsize = img.shape\n        img = self.crop_center(img, 1280, 1280)\n        image = ToTensor()(img) \n        image = 2*(image/torch.max(image))-1\n        code = image_filepath.split('/')[4][:-4] \n        return image, code, original_imsize\n    \n    def get_GT(self, code, original_imsize):\n        ett_annotations = self.ett_annotations\n        ids = ett_annotations['StudyInstanceUID'].tolist()\n        index_ett = ids.index(code) \n        \n        xs = (ett_annotations['data'][index_ett])\n        points = self.str2array(xs)\n        y_coords = points[:,1]\n        lowest_y = np.argmax(y_coords)\n#         gt_map_et = np.zeros([original_imsize[0], original_imsize[1]], dtype='float')\n        x_ett, y_ett = points[lowest_y]\n        ett_coords = np.array([x_ett,y_ett])\n        gt_map_et = self.get_gaussian_kernel(ett_coords, original_imsize)\n#         X, Y = np.mgrid[x_ett-20:x_ett+20, y_ett-20:y_ett+20]\n#         coords_et = np.vstack((X.ravel(), Y.ravel()))\n#         for i in range(0,len(coords_et[0])):\n#             gt_map_et[coords_et[1][i]][coords_et[0][i]] = 1\n        \n        gt_map_et = self.crop_center(gt_map_et, 1280, 1280)\n\n        return gt_map_et, ett_coords\n\n    def crop_center(self,img,cropx,cropy):\n        y,x = img.shape\n        startx = x//2-(cropx//2)\n        # crop closer to top  \n        return img[0:cropy,startx:startx+cropx]\n    \n    def str2array(self, s):\n        # Remove space after [\n        s=re.sub('\\[ +', '[', s.strip())\n        # Replace commas and spaces\n        s=re.sub('[,\\s]+', ', ', s)\n        return np.array(ast.literal_eval(s))\n    \n    def get_gaussian_kernel(self, coords, imshape):\n        N = 201 # kernel size\n        k1d = signal.gaussian(N, std=20).reshape(N, 1)\n        kernel = np.outer(k1d, k1d)\n        x, y = imshape\n        A = np.zeros([x,y])\n        A[coords[1],coords[0]] = 1    # random\n        row, col = np.where(A == 1)\n        if row[0] >= N//2 and col[0] >= N//2:\n            A[row[0]-(N//2):row[0]+(N//2)+1, col[0]-(N//2):col[0]+(N//2)+1] = kernel\n        return A\n ","metadata":{"execution":{"iopub.status.busy":"2022-06-19T22:58:34.642868Z","iopub.execute_input":"2022-06-19T22:58:34.643462Z","iopub.status.idle":"2022-06-19T22:58:34.661898Z","shell.execute_reply.started":"2022-06-19T22:58:34.643429Z","shell.execute_reply":"2022-06-19T22:58:34.66102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"carina_annotations = pd.read_json('../input/nihcsvdata/carina.json')\nett_annotations = pd.read_csv('../input/nihcsvdata/ETT_Annotations.csv')\ncombined_df = pd.read_csv('../input/nihcsvdata/combined_NIH_CLIP.csv')\npaths = ett_annotations['StudyInstanceUID'].tolist()\n        \ncombined_df = combined_df[combined_df['Original Image Pixel Spacing x'].notna()]\ncombined_df = combined_df.reset_index()\ncombined_df = combined_df.drop(['index'], axis=1)\ncombined_df = combined_df.drop(['Unnamed: 0'], axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-19T22:58:41.297013Z","iopub.execute_input":"2022-06-19T22:58:41.297446Z","iopub.status.idle":"2022-06-19T22:58:41.536258Z","shell.execute_reply.started":"2022-06-19T22:58:41.297409Z","shell.execute_reply":"2022-06-19T22:58:41.535421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"real_paths = []\npaths = ett_annotations['StudyInstanceUID'].tolist()\nfor path in paths:\n    if path in combined_df['StudyInstanceUID'].tolist():\n        real_paths.append(path)\n\ntrain_dir = '../input/ranzcr-clip-catheter-line-classification/train/'\ntotal_files =[train_dir + i + '.jpg' for i in real_paths]\nval_length = int(len(total_files) * 0.2)\nrandom.seed(10)\nrandom.shuffle(total_files)\ntrain_files = total_files[val_length:]\nval_files = total_files[:val_length]\n# test_files = []\n# test_files.extend(sorted(glob(test_dir + '/*.jpg')))\n\n# # 10/2/3/ initial split\n# train_set = paths[0:10]\n# val_set = paths[10:12]\n# test_set = paths[12:15]\n\n# CHANGE SHUFFLE BACK\nTrain_Dataset = Dataset(train_files[:100], \"Train\", ett_annotations, carina_annotations)\nTrain_dataloader = DataLoader(Train_Dataset, shuffle=True, num_workers=2, batch_size=1, pin_memory=True)\n\nVal_Dataset = Dataset(val_files[:25], \"Val\", ett_annotations, carina_annotations)\nVal_dataloader = DataLoader(Val_Dataset, shuffle=False, num_workers=2, batch_size=1, pin_memory=True)\n\n# Test_Dataset = Dataset(test_set, \"Test\", ett_annotations, carina_annotations)\n# Test_dataloader = DataLoader(Test_Dataset, shuffle=False, num_workers=2, batch_size=1, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T22:58:51.502369Z","iopub.execute_input":"2022-06-19T22:58:51.502857Z","iopub.status.idle":"2022-06-19T22:58:54.143131Z","shell.execute_reply.started":"2022-06-19T22:58:51.502814Z","shell.execute_reply":"2022-06-19T22:58:54.14231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example cropped image from dataloader\nfor i, data in enumerate(Train_dataloader):\n    image = data['Image']\n    gt = data['GT']\n    gt = gt.squeeze(0)\n    print(gt.shape)\n    print(data['Code'])\n    gt = gt.cpu().detach().numpy()\n\n    image = image.cpu().detach().numpy()[0][0]\n    \n#     for g in range(0,1280):\n#         for j in range(0,1280):\n#             if gt[g][j] != 0:\n#                 image[g][j] = gt[g][j]\n    plt.imshow(image, interpolation='nearest', cmap='Greys')\n    plt.imshow(gt, cmap='Greys', alpha=1)\n    break \n#     print(i)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T00:10:28.174645Z","iopub.execute_input":"2022-06-19T00:10:28.175008Z","iopub.status.idle":"2022-06-19T00:10:28.88959Z","shell.execute_reply.started":"2022-06-19T00:10:28.174979Z","shell.execute_reply":"2022-06-19T00:10:28.888609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class resconv_block(nn.Module):\n    def __init__(self,ch_in,ch_out):\n        super(resconv_block,self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(ch_in, ch_out, kernel_size=3,stride=1,padding=1,bias=True),\n            nn.BatchNorm2d(ch_out),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(ch_out, ch_out, kernel_size=3,stride=1,padding=1,bias=True),\n            nn.BatchNorm2d(ch_out),\n            nn.ReLU(inplace=True)\n        )\n        self.Conv_1x1 = nn.Conv2d(ch_in,ch_out,kernel_size=1,stride=1,padding=0)\n\n    def forward(self,x):\n\n        residual =  self.Conv_1x1(x)\n        x = self.conv(x)\n\n        return residual+x\n\nclass up_conv(nn.Module):\n    def __init__(self,ch_in,ch_out):\n        super(up_conv,self).__init__()\n        self.up = nn.Sequential(\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(ch_in,ch_out,kernel_size=3,stride=1,padding=1,bias=True),\n            nn.BatchNorm2d(ch_out),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self,x):\n        x = self.up(x)\n        return x\n\nclass ResU_Net(nn.Module):\n    def __init__(self,img_ch=1,output_ch=1):\n        super(ResU_Net,self).__init__()\n\n        self.Maxpool = nn.MaxPool2d(kernel_size=2,stride=2)\n        self.Softmax = nn.Softmax(dim=1)\n        self.Sigmoid = nn.Sigmoid()\n\n        self.Conv1 = resconv_block(ch_in=img_ch,ch_out=64)\n        self.Conv2 = resconv_block(ch_in=64,ch_out=128)\n        self.Conv3 = resconv_block(ch_in=128,ch_out=256)\n        self.Conv4 = resconv_block(ch_in=256,ch_out=512)\n        self.Conv5 = resconv_block(ch_in=512,ch_out=1024)\n\n        self.Up5 = up_conv(ch_in=1024,ch_out=512)\n        self.Up_conv5 = resconv_block(ch_in=1024, ch_out=512)\n\n        self.Up4 = up_conv(ch_in=512,ch_out=256)\n        self.Up_conv4 = resconv_block(ch_in=512, ch_out=256)\n\n        self.Up3 = up_conv(ch_in=256,ch_out=128)\n        self.Up_conv3 = resconv_block(ch_in=256, ch_out=128)\n\n        self.Up2 = up_conv(ch_in=128,ch_out=64)\n        self.Up_conv2 = resconv_block(ch_in=128, ch_out=64)\n\n        self.Conv_1x1 = nn.Conv2d(64,output_ch,kernel_size=1,stride=1,padding=0)\n\n\n    def forward(self,x):\n        # encoding path\n        x1 = self.Conv1(x)\n\n        x2 = self.Maxpool(x1)\n        x2 = self.Conv2(x2)\n\n        x3 = self.Maxpool(x2)\n        x3 = self.Conv3(x3)\n\n        x4 = self.Maxpool(x3)\n        x4 = self.Conv4(x4)\n\n        x5 = self.Maxpool(x4)\n        x5 = self.Conv5(x5)\n\n        # decoding + concat path\n        d5 = self.Up5(x5)\n        d5 = torch.cat((x4,d5),dim=1)\n\n        d5 = self.Up_conv5(d5)\n\n        d4 = self.Up4(d5)\n        d4 = torch.cat((x3,d4),dim=1)\n        d4 = self.Up_conv4(d4)\n\n        d3 = self.Up3(d4)\n        d3 = torch.cat((x2,d3),dim=1)\n        d3 = self.Up_conv3(d3)\n\n        d2 = self.Up2(d3)\n        d2 = torch.cat((x1,d2),dim=1)\n        d2 = self.Up_conv2(d2)\n\n        d1 = self.Conv_1x1(d2)\n        d1 = self.Sigmoid(d1)\n\n        return d1","metadata":{"execution":{"iopub.status.busy":"2022-06-19T22:58:58.392853Z","iopub.execute_input":"2022-06-19T22:58:58.393191Z","iopub.status.idle":"2022-06-19T22:58:58.4161Z","shell.execute_reply.started":"2022-06-19T22:58:58.393163Z","shell.execute_reply":"2022-06-19T22:58:58.415057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = UNetWithResnet50Encoder(checkpoint_pth = '../input/nihcsvdata/medaug_chexpert_resnet50.pth.tar')\nmodel = ResU_Net()\nmodel = model.cuda()\n# criterion=nn.BCELoss()\ncriterion=nn.MSELoss()\ncriterion = criterion.cuda()\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n# with torch.no_grad():\n\n# g = torch.rand(1,1,1280,1280)\n# g = g.cuda()\n# out = model(g)\n# print(out.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T22:59:13.99628Z","iopub.execute_input":"2022-06-19T22:59:13.996841Z","iopub.status.idle":"2022-06-19T22:59:18.452467Z","shell.execute_reply.started":"2022-06-19T22:59:13.996809Z","shell.execute_reply":"2022-06-19T22:59:18.451696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef calculate_metrics_no_carina(probability_map, code, combined_df, ett_gt):\n        et_tube = probability_map.cpu().detach().numpy()\n        ett_gt = ett_gt.cpu().detach().numpy()\n        et_tube_coords = np.array(np.unravel_index(np.argmax(et_tube, axis=None), et_tube.shape))\n        combined_list = combined_df['StudyInstanceUID'].tolist()\n        ind = combined_list.index(code)\n\n        x_conversion = combined_df['Original Image Pixel Spacing x'][int(ind)]\n        y_conversion = combined_df['Original Image Pixel Spacing y'][int(ind)]\n        et_tube_coords[0] = et_tube_coords[0] * x_conversion\n        et_tube_coords[1] = et_tube_coords[1] * y_conversion\n        ett_gt[0] = ett_gt[0] * x_conversion\n        ett_gt[1] = ett_gt[1] * y_conversion\n        \n        ett_abs_error = np.linalg.norm(et_tube_coords - ett_gt)/10\n        \n        if ett_abs_error <= 1:\n            ett_correct = True\n        else:\n            ett_correct = False\n\n        return ett_abs_error, ett_correct\n\n# Train/val\n\ntrain_global_losses = []\nval_global_losses = []\ntrain_global_dists = []\nval_global_dists = []\n\nfor epoch in range(0, 50):\n    \n    # Declare lists to keep track of losses and metrics within the epoch\n    train_epoch_losses = []\n    val_epoch_losses = []\n    train_epoch_dists = []\n    val_epoch_dists = []\n    \n    model.train()\n\n    count = 0\n\n    for i, data in enumerate(Train_dataloader):\n\n        input_img = data['Image']\n        gt = data['GT']\n        code = data['Code']\n        \n        input_img = input_img.cuda()\n        gt = gt.cuda()\n        gt = gt.to(torch.float)\n\n#         output = torch.rand(1,1024,1024).cuda()\n        output = model(input_img)[0]\n        loss = criterion(output, gt)\n        train_epoch_losses.append(loss.item())\n        \n        ett_coords_gt = data['ETT_coords']\n        ett_abs_error, ett_correct = calculate_metrics_no_carina(output[0],code[0],combined_df, ett_coords_gt[0])\n        train_epoch_dists.append(ett_abs_error)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    # Transition to val mode\n    model.eval()\n\n    # Avoid computing gradients during validation to save memory\n    with torch.no_grad():\n        count = 0\n\n        for i, data in enumerate(Val_dataloader):\n\n            input_img = data['Image']\n            gt = data['GT']\n            code = data['Code']\n\n            input_img = input_img.cuda()\n            gt = gt.cuda()\n            gt = gt.to(torch.float)\n\n            output = model(input_img)[0]\n#             output = torch.rand(1,1024,1024).cuda()\n            loss = criterion(output, gt)\n            val_epoch_losses.append(loss.item())\n\n            ett_coords_gt = data['ETT_coords']\n            ett_abs_error, ett_correct = calculate_metrics_no_carina(output[0],code[0],combined_df, ett_coords_gt[0])\n            val_epoch_dists.append(ett_abs_error)\n\n    train_net_loss = sum(train_epoch_losses) / len(train_epoch_losses)\n    val_net_loss = sum(val_epoch_losses) / len(val_epoch_losses)\n    train_global_losses.append(train_net_loss)\n    val_global_losses.append(val_net_loss)\n    \n    train_net_dists = sum(train_epoch_dists) / len(train_epoch_dists)\n    val_net_dists = sum(val_epoch_dists) / len(val_epoch_dists)\n    train_global_dists.append(train_net_dists)\n    val_global_dists.append(val_net_dists)\n\n    print('Epoch: {} | Train Loss: {} | Val Loss: {} | Train Dist: {} | Val Dist: {} |'.format(epoch, train_net_loss, val_net_loss,train_net_dists, val_net_dists))\n\n    checkpoint_dir = './'\n    # Save the model if it reaches a new min validation loss\n    if val_global_losses[-1] == min(val_global_losses):\n        print('saving model at the end of epoch ' + str(epoch))\n        if epoch > 5:\n            best_epoch = epoch\n            file_name = os.path.join(checkpoint_dir, 'model_epoch_{}.pth'.format(epoch))\n            torch.save({\n                'epoch': epoch,\n                'state_dict': model.state_dict(),\n                'optim_dict': optimizer.state_dict(),\n                },\n                file_name)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T22:59:25.450614Z","iopub.execute_input":"2022-06-19T22:59:25.451037Z","iopub.status.idle":"2022-06-20T00:54:57.172845Z","shell.execute_reply.started":"2022-06-19T22:59:25.451004Z","shell.execute_reply":"2022-06-20T00:54:57.171797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test\n\nbest_epoch = np.argmin(np.array(val_global_losses))\n\nmodel = ResU_Net()\n\nload_dir = './model_epoch_' + str(best_epoch) + '.pth'\ncheckpoint = torch.load(load_dir)\nmodel.load_state_dict(checkpoint['state_dict'])\nmodel.cuda()\noutput_map = []\ngts = []\ncodes = []\n\nwith torch.no_grad():\n    count = 0\n    for i, data in enumerate(Val_dataloader):\n        input_img = data['Image']\n        gt = data['GT']\n        codes.append(data['Code'])\n\n        input_img = input_img.cuda()\n        gt = gt.cuda()\n        gt = gt.to(torch.float)\n        gts.append(gt)\n        \n        output = model(input_img)\n        output_map.append(output[0].cpu().detach().numpy())\n        \n        if count > 5:\n            break\n        \n        count += 1\n        \n","metadata":{"execution":{"iopub.status.busy":"2022-06-20T00:54:57.175211Z","iopub.execute_input":"2022-06-20T00:54:57.175796Z","iopub.status.idle":"2022-06-20T00:55:01.291692Z","shell.execute_reply.started":"2022-06-20T00:54:57.17575Z","shell.execute_reply":"2022-06-20T00:55:01.290672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualization (original test image with GT)\n\ndef crop_center(img,cropx,cropy):\n    y,x = img.shape\n    startx = x//2-(cropx//2)\n    starty = y//3-(cropy//2)    \n    return img[starty:starty+cropy,startx:startx+cropx]\n\nind = 0\ntest_path_1 = val_files[ind]\ntest_img = cv2.imread(test_path_1, 0)\ntest_img = 2*(test_img/np.max(test_img))-1\ng = crop_center(test_img, 1280, 1280)\nplt.imshow(g, interpolation='nearest', cmap='Greys')\ngt = gts[ind].cpu().detach().numpy()\nprint(gt.shape)\nplt.imshow(gt[0], cmap='Greys', alpha=0.2)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T01:23:17.198631Z","iopub.execute_input":"2022-06-20T01:23:17.199101Z","iopub.status.idle":"2022-06-20T01:23:17.812156Z","shell.execute_reply.started":"2022-06-20T01:23:17.199061Z","shell.execute_reply":"2022-06-20T01:23:17.810836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualization (test image output)\n\n# map_1 = output_map[ind]\nmap_1 = output_map[0]\n\n# plt.imshow(g, interpolation='nearest', cmap='Greys')\n# plt.imshow(map_1[0], cmap='Greys', alpha=0.3)\nplt.imshow(map_1[0], cmap='Greys')","metadata":{"execution":{"iopub.status.busy":"2022-06-20T01:23:01.997607Z","iopub.execute_input":"2022-06-20T01:23:01.998438Z","iopub.status.idle":"2022-06-20T01:23:02.295074Z","shell.execute_reply.started":"2022-06-20T01:23:01.998401Z","shell.execute_reply":"2022-06-20T01:23:02.294328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# old dataloader\n\n# class Dataset():\n\n#     def __init__(self, file_list, mode, ett_annotations, carina_annotations):\n#         self.file_list = file_list\n#         self.mode = mode\n#         self.ett_annotations = ett_annotations\n#         self.carina_annotations = carina_annotations\n\n#     def __getitem__(self,idx):\n#         image_filepath = self.file_list[idx]\n#         image, code, original_imsize = self.loadimage(image_filepath)\n        \n#         gt_map, ett_coords = self.get_GT(code, original_imsize)\n# #         for i in range(0,len(image)):\n# #             image[i] = self.crop_center(image[i], ett_coords)\n        \n#         sample = {'Image': image,\n#                   'GT': gt_map,\n#                   'Code': code,\n#                   'ETT_coords': ett_coords,\n#               }\n\n#         return sample\n\n#     def __len__(self):\n#         return len(self.file_list)\n\n#     def loadimage(self, image_filepath):\n#         img = cv2.imread(image_filepath, 0)\n#         original_imsize = img.shape\n# #         img = self.crop_center(img, 1024, 1024)\n#         image = ToTensor()(img) \n#         image = 2*(image/torch.max(image))-1\n#         code = image_filepath.split('/')[4][:-4] \n#         return image, code, original_imsize\n    \n#     def get_GT(self, code, original_imsize):\n#         ett_annotations = self.ett_annotations\n#         ids = ett_annotations['StudyInstanceUID'].tolist()\n#         index_ett = ids.index(code) \n        \n#         xs = (ett_annotations['data'][index_ett])\n#         points = self.str2array(xs)\n#         y_coords = points[:,1]\n#         lowest_y = np.argmax(y_coords)\n#         gt_map_et = np.zeros([original_imsize[0], original_imsize[1]], dtype='float')\n#         x_ett, y_ett = points[lowest_y]\n#         ett_coords = np.array([x_ett,y_ett])\n#         X, Y = np.mgrid[x_ett-20:x_ett+20, y_ett-20:y_ett+20]\n#         coords_et = np.vstack((X.ravel(), Y.ravel()))\n#         for i in range(0,len(coords_et[0])):\n#             gt_map_et[coords_et[1][i]][coords_et[0][i]] = 1\n#         gt_map_et = self.crop_center(gt_map_et, ett_coords)\n\n#         return gt_map_et, ett_coords\n\n#     def crop_center(self,img,ett_coords):\n#         x,y = ett_coords[0], ett_coords[1]\n#         print(img.shape)\n#         x_im,y_im = img.shape\n#         if x - 512 >= 0:\n#             startx = x-512\n#         else:\n#             startx = 0\n#         if x + 512 < x_im:\n#             endx = x+512\n#         else:\n#             endx = x_im\n            \n#         if y - 100 >= 0:\n#             starty = y-100\n#         else:\n#             starty = 0\n#         if y + 924 < y_im:\n#             endy = y+924\n#         else:\n#             endy = y_im\n            \n#         return img[starty:endy,startx:endx]\n    \n#     def str2array(self, s):\n#         # Remove space after [\n#         s=re.sub('\\[ +', '[', s.strip())\n#         # Replace commas and spaces\n#         s=re.sub('[,\\s]+', ', ', s)\n#         return np.array(ast.literal_eval(s))\n ","metadata":{},"execution_count":null,"outputs":[]}]}