{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Training notebook can be found [here](https://www.kaggle.com/bipinkrishnan/efficientnet-b2-no-cross-validation-lb-0-921)**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from PIL import Image\nimport sys\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.metrics import roc_auc_score\n\nimport torch\nfrom torchvision.transforms import transforms\nfrom torch.utils.data import DataLoader, Dataset\n\nsys.path.append('../input/efficient-net-deps-1/')\nfrom efficientnet_pytorch.model import EfficientNet\nfrom tqdm.notebook import tqdm","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"root_path = '../input/ranzcr-clip-catheter-line-classification/'\ntest_imgs = root_path+ 'test/'\n\nsubmission = pd.read_csv(root_path+'sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LoadData(Dataset):\n    def __init__(self, df, transform, test=False):\n        super().__init__()\n        self.df = df\n        self.transform = transform\n        self.test = test\n        \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        if not self.test:\n            img = Image.open(train_imgs+row[0]+'.jpg').convert('RGB')\n            labels = torch.from_numpy(row[1:-1].astype(np.float32).values)\n        \n            return self.transform(img), labels \n        else:\n            img = Image.open(test_imgs+row[0]+'.jpg').convert('RGB')\n            return self.transform(img)\n        \n    def __len__(self): return len(self.df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(model='effnet'):\n    #building the model for transfer learning\n    if model == 'effnet':\n        effnet = EfficientNet.from_name('efficientnet-b2', num_classes=11)\n        return effnet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 32\nDEVICE = 'cuda'\nIMG_SIZE = 256\n\ntransform = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)),\n                                transforms.ToTensor(),\n                                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                                     std=[0.229, 0.224, 0.225])])\n\ntest_ds = LoadData(submission, transform, test=True)\ntest_dl = DataLoader(test_ds, BATCH_SIZE, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#path to your saved best model\nmodel_path = '../input/effnet-model/model_92.pt'\nmodel = build_model().to(DEVICE)\nmodel.load_state_dict(torch.load(model_path, map_location=DEVICE))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.eval()\nout = []\nwith torch.no_grad():\n    for data in tqdm(test_dl, total=len(test_dl)):\n        out.append((model(data.to(DEVICE)).sigmoid()).cpu())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = torch.cat(out).numpy()\nsolution = pd.DataFrame(preds, columns=submission.columns[1:])\nsolution['StudyInstanceUID'] = submission['StudyInstanceUID']\nsolution = solution.reindex(columns=list(submission))\nsolution.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}