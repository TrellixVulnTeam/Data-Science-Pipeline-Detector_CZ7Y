{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install efficientnet -q","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\n\nimport efficientnet.tfkeras as efficientnet\nimport numpy as np\nimport pandas as pd\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\n\n\ndef auto_select_accelerator():\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        print(\"Running on TPU:\", tpu.master())\n    except ValueError:\n        strategy = tf.distribute.get_strategy()\n    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n    \n    return strategy\n\n\ndef build_decoder(with_labels=True, target_size=(256, 256), ext='jpg'):\n    def decode(path):\n        file_bytes = tf.io.read_file(path)\n        if ext == 'png':\n            img = tf.image.decode_png(file_bytes, channels=3)\n        elif ext in ['jpg', 'jpeg']:\n            img = tf.image.decode_jpeg(file_bytes, channels=3)\n        else:\n            raise ValueError(\"Image extension not supported\")\n\n        img = tf.cast(img, tf.float32) / 255.0\n        img = tf.image.resize(img, target_size)\n\n        return img\n    \n    def decode_with_labels(path, label):\n        return decode(path), label\n    \n    return decode_with_labels if with_labels else decode\n\n\ndef build_augmenter(with_labels=True):\n    def augment(img):\n        img = tf.image.random_flip_left_right(img)\n        img = tf.image.random_flip_up_down(img)\n        img = tf.image.random_brightness(img, 0.025)\n        img = tf.image.random_contrast(img, 0.975, 1.025)\n        return img\n    \n    def augment_with_labels(img, label):\n        return augment(img), label\n    \n    return augment_with_labels if with_labels else augment\n\n\ndef build_dataset(\n    paths, labels=None, bsize=32, cache=True, decode_fn=None, augment_fn=None,\n    augment=True, repeat=True, shuffle=1024, cache_dir=''):\n    if cache_dir != '' and cache is True:\n        os.makedirs(cache_dir, exist_ok=True)\n    \n    if decode_fn is None:\n        decode_fn = build_decoder(labels is not None)\n    \n    if augment_fn is None:\n        augment_fn = build_augmenter(labels is not None)\n    \n    AUTO = tf.data.experimental.AUTOTUNE\n    slices = paths if labels is None else (paths, labels)\n    \n    dset = tf.data.Dataset.from_tensor_slices(slices)\n    dset = dset.map(decode_fn, num_parallel_calls=AUTO)\n    dset = dset.cache(cache_dir) if cache else dset\n    dset = dset.map(augment_fn, num_parallel_calls=AUTO) if augment else dset\n    dset = dset.repeat() if repeat else dset\n    dset = dset.shuffle(shuffle) if shuffle else dset\n    dset = dset.batch(bsize).prefetch(AUTO)\n    \n    return dset\n\ndef load_csv(params, file_path, key, sampling_rate=1.0):\n    f = pd.read_csv(file_path, index_col=0)\n\n    if sampling_rate < 1.0:\n        f = f.sample(frac=sampling_rate)\n\n    f['path'] = '{}/{}/'.format(params['gcs_path'], key) + f.index + '.jpg'\n    return f","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"info = {}\ninfo['competition_name'] = 'ranzcr-clip-catheter-line-classification'\ninfo['gcs_path'] = KaggleDatasets().get_gcs_path(info['competition_name'])\ninfo['input_path'] = '/kaggle/input/{}'.format(info['competition_name'])\ninfo['strategy'] = auto_select_accelerator()\ninfo['batch_size'] = info['strategy'].num_replicas_in_sync * 16\ninfo['image_sizes'] = (224, 240, 260, 300, 380, 456, 528, 616)\ninfo['weight_level'] = 7\ninfo['labels'] = [\n    'ETT - Abnormal', 'ETT - Borderline', 'ETT - Normal',\n    'NGT - Abnormal', 'NGT - Borderline', 'NGT - Incompletely Imaged', 'NGT - Normal',\n    'CVC - Abnormal', 'CVC - Borderline', 'CVC - Normal',\n    'Swan Ganz Catheter Present'\n]\ninfo['label_size'] = len(info['labels'])\ninfo['epochs'] = 20\ninfo['patience'] = 3\ninfo['min_lr'] = 1e-6\ninfo","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndef learn(info):\n    f = load_csv(info, '{}/train.csv'.format(info['input_path']), 'train')\n    f.sort_values('PatientID', inplace=True)\n\n    tests = load_csv(info, '{}/sample_submission.csv'.format(info['input_path']), 'test')\n\n    tra_paths, val_paths, tra_labels, val_labels = train_test_split(f['path'], f[info['labels']], test_size=0.2, shuffle=True, random_state=info['random_state'])\n    test_paths = tests['path']\n\n    image_size = (info['image_sizes'][info['weight_level']], info['image_sizes'][info['weight_level']])\n    decoder = build_decoder(with_labels=True, target_size=image_size)\n    test_decoder = build_decoder(with_labels=False, target_size=image_size)\n\n    tra_dataset = build_dataset(tra_paths, tra_labels, bsize=info['batch_size'], decode_fn=decoder)\n    val_dataset = build_dataset(val_paths, val_labels, bsize=info['batch_size'], decode_fn=decoder, repeat=False, shuffle=False, augment=False)\n    test_dataset = build_dataset(test_paths, cache=False, bsize=info['batch_size'], decode_fn=test_decoder, repeat=False, shuffle=False, augment=False)\n\n    input_shape = (info['image_sizes'][info['weight_level']], info['image_sizes'][info['weight_level']], 3)\n    with info['strategy'].scope():\n        model = tf.keras.Sequential([\n            getattr(efficientnet, 'EfficientNetB{}'.format(info['weight_level']))(input_shape=input_shape, weights='imagenet', include_top=False),\n            tf.keras.layers.GlobalAveragePooling2D(), tf.keras.layers.Dense(info['label_size'], activation='sigmoid')\n        ])\n        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name='auc', multi_label=True)])\n\n    steps_per_epoch = tra_paths.shape[0] // info['batch_size']\n    checkpoint = tf.keras.callbacks.ModelCheckpoint('model_616_{}.h5'.format(info['random_state']), save_best_only=True, monitor='val_auc', mode='max')\n    lr_reducer = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_auc', patience=info['patience'], min_lr=info['min_lr'], mode='max')\n\n    history = model.fit(tra_dataset, validation_data=val_dataset, epochs=info['epochs'], verbose=2, callbacks=[checkpoint, lr_reducer], steps_per_epoch=steps_per_epoch)\n\n\nfor state in [51]:\n    info['random_state'] = state\n    learn(info)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls -l","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}