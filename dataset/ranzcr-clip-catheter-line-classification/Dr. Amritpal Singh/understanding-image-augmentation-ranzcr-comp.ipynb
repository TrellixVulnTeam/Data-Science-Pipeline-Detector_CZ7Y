{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Understanding Image Augmentation\n\nYou can look into my other notebooks to get more detailed augmentation insights.\nlist of my other notebooks - \n\n1. Understanding Image Augmentation - [link](https://www.kaggle.com/amritpal333/understanding-image-augmentation-ranzcr-comp)\n2. CLAHE augmentation  - [link](https://www.kaggle.com/amritpal333/clahe-augmentation-ranzcr-comp)\n3. Findings your own mean,standerd deviation for the dataset - [link](https://www.kaggle.com/amritpal333/using-custom-mean-std-ranzcr-comp)\n\n\n## Upvote the notebooks if you find them insightful."},{"metadata":{"_uuid":"af89a040-95d5-49ba-bb59-eed78dedff0a","_cell_guid":"7faae6ad-2c75-4b64-aa3f-fae0807a1aee","trusted":true},"cell_type":"code","source":"#!pip install albumentations\nimport albumentations\n\n\nimport torch\nfrom albumentations import ( Compose, OneOf, Normalize, Resize, RandomResizedCrop, RandomCrop, HorizontalFlip, VerticalFlip, \n    RandomBrightness, RandomContrast, RandomBrightnessContrast, Rotate, ShiftScaleRotate, Cutout, IAAAdditiveGaussianNoise, Transpose, ToGray )\nfrom albumentations.pytorch import ToTensorV2\nfrom albumentations import ImageOnlyTransform\n\nimport matplotlib.pyplot as plt\n\nseed = 42\n\nimport pandas as pd\nimport os\nimport cv2\nfrom torch.utils.data import Dataset,DataLoader\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\nclass Ranzcr_jpg_train_dataset(Dataset):    \n\t\n    def __init__(self, files_folder_path, df, num_channels , transfroms = None ):\n\t\tself.files_folder_path = files_folder_path\n\t\tself.df = df\n\t\tself.transforms = transfroms\n\t\tself.num_channels = num_channels\n\n\tdef __len__(self):\n\t\treturn len(self.df)\n\n\tdef __getitem__(self, idx):\n\n\t\timage_id = self.df.StudyInstanceUID.values[idx]\n\t\tif self.num_channels == 1:\n\t\t\timage = cv2.imread(os.path.join(self.files_folder_path, image_id + \".jpg\" ), 0)\n        \n\t\telse:\n\t\t\timage = cv2.imread(os.path.join(self.files_folder_path, image_id + \".jpg\" ))\n\t\t\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            image = image #*(1/255)\n        \n\t\tif self.transforms:\n\t\t\timage = self.transforms(image=image)['image']\n\t\tlabels = self.df[self.df.StudyInstanceUID == image_id].values.tolist()[0][1:-1]\n\t\tlabels = torch.tensor(labels,dtype= torch.float32) #.view(1,-1)\n\n\t\treturn image, labels"},{"metadata":{},"cell_type":"markdown","source":"# defining your own dataset\n\nRather than looking into images from folder at random, i want to look at the images as they are sent to the model.\nthis removes any scope of error."},{"metadata":{"trusted":true},"cell_type":"code","source":"class Ranzcr_jpg_train_dataset(Dataset):    \n\tdef __init__(self, files_folder_path, df, num_channels , transfroms = None ):\n\t\tself.files_folder_path = files_folder_path\n\t\tself.df = df\n\t\tself.transforms = transfroms\n\t\tself.num_channels = num_channels\n\n\tdef __len__(self):\n\t\treturn len(self.df)\n\n\tdef __getitem__(self, idx):\n\n\t\timage_id = self.df.StudyInstanceUID.values[idx]\n\t\timage = cv2.imread(os.path.join(self.files_folder_path, image_id + \".jpg\" ))\n\t\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\t\timage = image*(1/255)\n        \n\t\tif self.transforms:\n\t\t\timage = self.transforms(image=image)['image']\n\t\tlabels = self.df[self.df.StudyInstanceUID == image_id].values.tolist()[0][1:-1]\n\t\tlabels = torch.tensor(labels,dtype= torch.float32) #.view(1,-1)\n\n\t\treturn image, labels","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a98a696-c072-4ebb-ab18-89f167062a99","_cell_guid":"0a640f66-daf1-4038-b127-4d37c172a369","trusted":true},"cell_type":"code","source":"train_path = '../input/ranzcr-clip-catheter-line-classification/train'\ntrain_files = os.listdir(train_path)\n\ntrain_df = pd.read_csv('../input/ranzcr-clip-catheter-line-classification/train.csv')\n\ntrain = train_df.reset_index(drop=True) # reset index on both dataframes\n\nprint(train.shape)\n\nnum_channel = 3\n\nimg_size = 255","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Baseline"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Ranzcr_jpg_train_dataset(Dataset):    \n\tdef __init__(self, files_folder_path, df, num_channels , transfroms = None ):\n\t\tself.files_folder_path = files_folder_path\n\t\tself.df = df\n\t\tself.transforms = transfroms\n\t\tself.num_channels = num_channels\n\n\tdef __len__(self):\n\t\treturn len(self.df)\n\n\tdef __getitem__(self, idx):\n\n\t\timage_id = self.df.StudyInstanceUID.values[idx]\n\t\timage = cv2.imread(os.path.join(self.files_folder_path, image_id + \".jpg\" ))\n\t\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\t\t#image = image*(1/recale_image_by)\n        \n\t\tif self.transforms:\n\t\t\timage = self.transforms(image=image)['image']\n\t\tlabels = self.df[self.df.StudyInstanceUID == image_id].values.tolist()[0][1:-1]\n\t\tlabels = torch.tensor(labels,dtype= torch.float32) #.view(1,-1)\n\n\t\treturn image, labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef Show_Xrays(augmentation):\n    num_channels = 3\n    trainset = Ranzcr_jpg_train_dataset(train_path,  train, num_channels , augmentation )\n    trainloader = DataLoader(trainset, batch_size = 16 , num_workers = 3 , shuffle = True)\n\n    fig = plt.figure()\n    fig.set_size_inches(25, 25)\n    #fig.savefig('test2png.png', dpi=100)\n\n    for batch_i, (data, target) in tqdm(enumerate(trainloader)):\n        #print(data.shape)\n        if batch_i == 3:\n            break\n        for i in range(data.shape[0]):\n            ax = plt.subplot(4,4, i+1)\n            plt.tight_layout()\n            ax.axis('off')\n            plt.imshow(data[i])\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_list = [[ 0.4914168 , 0.4914168 , 0.4914168] , [0.485,0.456,0.406] , [0.9,0.9,0.9] , 0.496 ]\nstd_list = [[0.407278, 0.407278 , 0.407278] , [0.229,0.224,0.225] , [0.9,0.9,0.9] , 0.407278]\n\nmean = mean_list[0]\nstd=  std_list[0]\n\nprint(mean , std)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_augs = albumentations.Compose([albumentations.Resize(height=img_size, width=img_size, p=1.0), \n                                     albumentations.Normalize(mean= mean ,std= std ,),\n                                    ])\n\nShow_Xrays(train_augs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Findings- \nIf we are training from scratch, we need to find a new mean,std.\nIf using pretrained weights, then most likely stick to the values on pretraining dataset.\n\nNotebook on how to find your own mean and std deviation ? https://www.kaggle.com/amritpal333/using-custom-mean-std-ranzcr-comp"},{"metadata":{"_uuid":"cbd85435-4cdd-4ed8-a954-dc87854ffcab","_cell_guid":"a928b971-e98f-48fe-8cb1-c1fce088b4e6","trusted":true},"cell_type":"code","source":"train_augs = albumentations.Compose([   albumentations.RandomResizedCrop(img_size, img_size, scale=(0.9, 1), p=1),\n                                        albumentations.RandomBrightnessContrast(brightness_limit=(-0.2,0.2), contrast_limit=(-0.2, 0.2), p=0.7),                            \n                                    ])\nShow_Xrays(train_augs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_augs = albumentations.Compose([   albumentations.RandomResizedCrop(img_size, img_size, scale=(0.9, 1), p=1),\n                                        albumentations.RandomBrightnessContrast(brightness_limit=(-0.2,0.2), contrast_limit=(-0.2, 0.2), p=0.7), \n                                        albumentations.CLAHE(clip_limit=(1,10), p= 1),\n                                        #albumentations.Normalize(mean= mean ,std= std ,)\n                                    ])\nShow_Xrays(train_augs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Findings- CLAHE is awesome\nCLAHE works like magic to improve the visualisation of the catheters.\nYou can look into my detailed analysis on how CLAHE effects xrays in this competition in my notebook -\n\n### [link](https://www.kaggle.com/amritpal333/clahe-augmentation-ranzcr-comp)"},{"metadata":{},"cell_type":"markdown","source":"# Normalizing data first"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_augs = albumentations.Compose([   albumentations.Normalize(mean= mean ,std= std ,) , \n                                         albumentations.RandomResizedCrop(img_size, img_size, scale=(0.9, 1), p=1),\n                                        albumentations.RandomBrightnessContrast(brightness_limit=(-0.2,0.2), contrast_limit=(-0.2, 0.2), p=0.7), \n                                        #albumentations.CLAHE(clip_limit=(1,10), p= 1)\n                                        \n                                    ])\nShow_Xrays(train_augs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### findings - Normalizing images before using other augmentation works only in simple cases, and raises an error in others.\n\ne.g. using CLAHE on normalized data\n>     raise TypeError(\"clahe supports only uint8 inputs\")\n>     TypeError: clahe supports only uint8 inputs\n\nSo, use Normalize as the last in the list of your augmentations."},{"metadata":{},"cell_type":"markdown","source":"# Trying differnt combinations of image augmentation\n\n(I hope to find one that magically works!)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_augs = albumentations.Compose([albumentations.Resize(img_size, img_size),\n                albumentations.RandomResizedCrop(img_size, img_size, scale=(0.9, 1), p=1), \n\t\t\t\talbumentations.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=10, val_shift_limit=10, p=1),\n\t\t\t\talbumentations.RandomBrightnessContrast(brightness_limit=(-0.2,0.2), contrast_limit=(-0.2, 0.2), p=1),\n\t\t\t\talbumentations.CLAHE(clip_limit=(1,4), p=1),\n                #albumentations.imgaug.transforms.IAASharpen(alpha=(0.2, 0.3), lightness=(0.5, 0.7), p=1),\n                #albumentations.Cutout(max_h_size=int(img_size * 0.05), max_w_size=int(img_size * 0.05), num_holes=5, p= 0.5),\n\t\t\t\t#albumentations.Normalize(mean= mean ,  std= std ,) \n               ])\n\nShow_Xrays(train_augs)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_augs = albumentations.Compose([albumentations.Resize(img_size, img_size),\n                albumentations.RandomResizedCrop(img_size, img_size, scale=(0.9, 1), p=1), \n\t\t\t\t#albumentations.ShiftScaleRotate(shift_limit_x=(-0.0125, 0.0125),shift_limit_y=(-0.0125, 0.0125) ,rotate_limit=(-15, 15) , p=1),\n\t\t\t\talbumentations.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=10, val_shift_limit=10, p=1),\n\t\t\t\talbumentations.RandomBrightnessContrast(brightness_limit=(-0.2,0.2), contrast_limit=(-0.2, 0.2), p=1),\n\t\t\t\talbumentations.CLAHE(clip_limit=(1,4), p=1),\n                ########albumentations.OpticalDistortion(distort_limit=1.0),\n\t\t\t\t#####albumentations.ElasticTransform(alpha=3),\n                #albumentations.GaussNoise(var_limit=[10, 50], p=1),\n                #albumentations.MotionBlur(p=1),\n                #albumentations.MedianBlur(p=1),\n                #albumentations.augmentations.transforms.ISONoise(color_shift=(0.01, 0.05), intensity=(0.1, 0.5), p=1),\n                albumentations.imgaug.transforms.IAASharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=1),\n                albumentations.imgaug.transforms.IAAEmboss(alpha=(0.2, 0.5), strength=(0.2, 0.7), p=1),\n                ###albumentations.imgaug.transforms.IAAPerspective (scale=(0.05, 0.1), keep_size=True, p=1),\n                albumentations.augmentations.transforms.ToGray(p=1),\n                #albumentations.augmentations.transforms.RandomGamma(gamma_limit=(80, 120), eps=None, p=1),\n                #albumentations.Cutout(max_h_size=int(img_size * 0.05), max_w_size=int(img_size * 0.05), num_holes=5, p= 0.5),\n\t\t\t\t#albumentations.Normalize(mean= mean ,  std= std ,) \n               ])\n\nShow_Xrays(train_augs)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Saving all varints of augmentation to folder\n\nto download later on"},{"metadata":{},"cell_type":"markdown","source":"https://albumentations.ai/docs/api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomRain"},{"metadata":{"trusted":true},"cell_type":"code","source":"augment_list = [albumentations.Resize(img_size, img_size),\n                albumentations.RandomResizedCrop(img_size, img_size, scale=(0.9, 1), p=1), \n\t\t\t\talbumentations.ShiftScaleRotate(p=1),\n\t\t\t\talbumentations.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=10, val_shift_limit=10, p=1),\n\t\t\t\talbumentations.RandomBrightnessContrast(brightness_limit=(-0.2,0.2), contrast_limit=(-0.2, 0.2), p=1),\n\t\t\t\talbumentations.CLAHE(clip_limit=(1,4), p=1),\n                albumentations.OpticalDistortion(distort_limit=1.0),\n\t\t\t\talbumentations.ElasticTransform(alpha=3),\n                albumentations.GaussNoise(var_limit=[10, 50], p=1),\n                albumentations.MotionBlur(p=1),\n                albumentations.MedianBlur(p=1),\n                albumentations.augmentations.transforms.ISONoise(color_shift=(0.01, 0.05), intensity=(0.1, 0.5), p=1),\n                albumentations.imgaug.transforms.IAASharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=1),\n                albumentations.imgaug.transforms.IAAEmboss(alpha=(0.2, 0.5), strength=(0.2, 0.7), p=1),\n                albumentations.imgaug.transforms.IAAPerspective (scale=(0.05, 0.1), keep_size=True, p=1),\n                albumentations.augmentations.transforms.ToGray(p=1),\n                albumentations.augmentations.transforms.RandomGamma(gamma_limit=(80, 120), eps=None, p=1),\n                albumentations.Cutout(max_h_size=int(img_size * 0.05), max_w_size=int(img_size * 0.05), num_holes=10, p= 0.5),\n\t\t\t\talbumentations.Normalize(mean= mean ,  std= std ,) \n               ]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"make a smaller version of list , the ones you are interested in."},{"metadata":{"trusted":true},"cell_type":"code","source":"augment_list = [  albumentations.Resize(img_size, img_size),\n                albumentations.RandomResizedCrop(img_size, img_size, scale=(0.9, 1), p=1), \n\t\t\t\t  ########albumentations.HorizontalFlip(p=1),\n\t\t\t\t  albumentations.ShiftScaleRotate(p=1),\n\t\t\t\t   albumentations.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=10, val_shift_limit=10, p=1),\n\t\t\t\t   albumentations.RandomBrightnessContrast(brightness_limit=(-0.2,0.2), contrast_limit=(-0.2, 0.2), p=1),\n\t\t\t\t   albumentations.CLAHE(clip_limit=(1,4), p=1),\n                   albumentations.OpticalDistortion(distort_limit=1.0),\n\t\t\t\t   #albumentations.GridDistortion(num_steps=5, distort_limit=1.),\n\t\t\t\t   albumentations.ElasticTransform(alpha=3),\n                 albumentations.GaussNoise(var_limit=[10, 50], p=1),\n                 #albumentations.GaussianBlur(p=1),\n                 albumentations.MotionBlur(p=1),\n                albumentations.MedianBlur(p=1),\n                albumentations.augmentations.transforms.ISONoise(color_shift=(0.01, 0.05), intensity=(0.1, 0.5), p=1),\n                #albumentations.imgaug.transforms.IAASuperpixels(p_replace=0.1, n_segments=100, p=1),\n                albumentations.imgaug.transforms.IAASharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=1),\n                albumentations.imgaug.transforms.IAAEmboss(alpha=(0.2, 0.5), strength=(0.2, 0.7), p=1),\n                albumentations.imgaug.transforms.IAAPerspective (scale=(0.05, 0.1), keep_size=True, p=1),\n                ######albumentations.augmentations.domain_adaptation.HistogramMatching (reference_images , blend_ratio=(0.5, 1.0), p=0.5),\n                #albumentations.augmentations.transforms.ToSepia(p=1),\n                albumentations.augmentations.transforms.ToGray(p=1),\n                #albumentations.imgaug.transforms.IAAAdditiveGaussianNoise(loc=0, scale=(2.5500000000000003, 12.75), per_channel=False, p=1),\n                \n                albumentations.augmentations.transforms.RandomGamma(gamma_limit=(80, 120), eps=None, p=1),\n                #albumentations.augmentations.transforms.Solarize(threshold=128, p=1),\n                #albumentations.augmentations.transforms.RandomFog(fog_coef_lower=0.3, fog_coef_upper=1, alpha_coef=0.08, p=1),\n                #albumentations.augmentations.transforms.RandomRain(slant_lower=-10, slant_upper=10, drop_length=20, drop_width=1, drop_color=(200, 200, 200), blur_value=7, brightness_coefficient=0.7, rain_type=None, p=1),\n\t\t\t\t#albumentations.Cutout(max_h_size=int(img_size * 0.1), max_w_size=int(img_size * 0.1), num_holes=5, p=1),\n\t\t\t\talbumentations.Normalize(mean= mean ,  std= std ,) \n               ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Show_save_Xrays(augmentation , name):\n    num_channels = 3\n    trainset = Ranzcr_jpg_train_dataset(train_path,  train, num_channels , augmentation )\n    trainloader = DataLoader(trainset, batch_size = 1 , num_workers = 3 , shuffle = False)\n\n    fig = plt.figure()\n    fig.set_size_inches(25, 25)\n    #fig.savefig('test2png.png', dpi=100)\n\n    for batch_i, (data, target) in tqdm(enumerate(trainloader)):\n        #print(data.shape)\n        if batch_i == 1:\n            break\n        for i in range(data.shape[0]):\n            ax = plt.subplot(1 ,1, i+1)\n            plt.tight_layout()\n            ax.axis('off')\n            plt.imshow(data[i])\n            fig.savefig( str(name) + '.png', dpi=200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Save_to_folder = False\n\n\nif Save_to_folder == True:\n    for i in range(len(augment_list)):\n        train_augs = albumentations.Compose([ albumentations.Resize(img_size, img_size) , augment_list[i]])\n        Show_save_Xrays(train_augs , augment_list[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!zip -r -q 'Albumentation_1*1.zip'  ./","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm ./*.zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm ./*.png","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}