{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Based on : https://www.kaggle.com/chabir/bnp-paribas-cardif-claims-management/extratreesclassifier-score-0-45-v5/code\nimport pandas as pd\n\nimport numpy as np\nimport csv\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn import ensemble\nimport matplotlib.pyplot as plt\n\n\ndef find_delimiter(df, col):\n    \"\"\"\n    Function that trying to find an approximate delimiter used for scaling.\n    So we can undo the feature scaling.\n    \"\"\"\n    vals = df[col].dropna().sort_values().round(8)\n    vals = pd.rolling_apply(vals, 2, lambda x: x[1] - x[0])\n    vals = vals[vals > 0.000001]\n    return vals.value_counts().idxmax() \n\nprint('Load data...')\ntrain = pd.read_csv(\"../input/train.csv\")\ntarget = train['target'].values\ntrain = train.drop(['ID','target','v8','v23','v25','v31','v36','v37','v46','v51','v53','v54','v63','v73','v75','v79','v81','v82','v89','v92','v95','v105','v107','v108','v109','v110','v116','v117','v118','v119','v123','v124','v128'],axis=1)\ntest = pd.read_csv(\"../input/test.csv\")\nid_test = test['ID'].values\ntest = test.drop(['ID','v8','v23','v25','v31','v36','v37','v46','v51','v53','v54','v63','v73','v75','v79','v81','v82','v89','v92','v95','v105','v107','v108','v109','v110','v116','v117','v118','v119','v123','v124','v128'],axis=1)\n\nprint('Clearing...')\nnum_vars = ['v1', 'v2', 'v4', 'v5', 'v6', 'v7', 'v9', 'v10', 'v11',\n            'v12', 'v13', 'v14', 'v15', 'v16', 'v17', 'v18', 'v19', 'v20',\n            'v21', 'v26', 'v27', 'v28', 'v29', 'v32', 'v33', 'v34', 'v35', 'v38',\n            'v39', 'v40', 'v41', 'v42', 'v43', 'v44', 'v45', 'v48', 'v49', 'v50',\n            'v55', 'v57', 'v58', 'v59', 'v60', 'v61', 'v62', 'v64', 'v65', 'v67',\n            'v68', 'v69', 'v70', 'v72', 'v76', 'v77', 'v78', 'v80', 'v83', 'v84', \n            'v85', 'v86', 'v87', 'v88', 'v90', 'v93', 'v94', 'v96', 'v97', 'v98', \n            'v99', 'v100', 'v101', 'v102', 'v103', 'v104', 'v106', 'v111', 'v114',\n            'v115', 'v120', 'v121', 'v122', 'v126', 'v127', 'v129', 'v130', 'v131']\n\nvs = pd.concat([train, test])\nfor c in num_vars:\n    if c not in train.columns:\n        continue\n    \n    train.loc[train[c].round(5) == 0, c] = 0\n    test.loc[test[c].round(5) == 0, c] = 0\n\n    delimiter = find_delimiter(vs, c)\n    train[c] *= 1/delimiter\n    test[c] *= 1/delimiter\n\nfor (train_name, train_series), (test_name, test_series) in zip(train.iteritems(),test.iteritems()):\n    if train_series.dtype == 'O':\n        #for objects: factorize\n        train[train_name], tmp_indexer = pd.factorize(train[train_name])\n        test[test_name] = tmp_indexer.get_indexer(test[test_name])\n        #but now we have -1 values (NaN)\n    else:\n        #for int or float: fill NaN\n        tmp_len = len(train[train_series.isnull()])\n        if tmp_len>0:\n            #print \"mean\", train_series.mean()\n            train.loc[train_series.isnull(), train_name] = -999 \n        #and Test\n        tmp_len = len(test[test_series.isnull()])\n        if tmp_len>0:\n            test.loc[test_series.isnull(), test_name] = -999\n\nnum = 0\nindex_to_name = {}\nfor (train_name, train_series) in train.iteritems():\n    index_to_name[num] = train_name\n    num += 1\nX_train = train\nX_test = test\nprint('Training...')\nextc = ExtraTreesClassifier(n_estimators=10,max_features= 50,criterion= 'entropy',min_samples_split= 4,\n                            max_depth= 35, min_samples_leaf= 2, n_jobs = -1)      \n\nextc.fit(X_train,target) \n\nimportances = extc.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in extc.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(X_train.shape[1]):\n    print(\"%d. feature %s (%f)\" % (f + 1, index_to_name[indices[f]], importances[indices[f]]))\n\n# Plot the feature importances of the forest\nplt.figure()\nplt.title(\"Feature importances\")\nplt.bar(range(X_train.shape[1]), importances[indices],\n       color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(X_train.shape[1]), indices)\nplt.xlim([-1, X_train.shape[1]])\nplt.show()\n'''\n#print('Predict...')\n#y_pred = extc.predict_proba(X_test)\n#print y_pred\n#pd.DataFrame({\"ID\": id_test, \"PredictedProb\": y_pred[:,1]}).to_csv('extra_trees.csv',index=False)\n'''\n\n\n\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}