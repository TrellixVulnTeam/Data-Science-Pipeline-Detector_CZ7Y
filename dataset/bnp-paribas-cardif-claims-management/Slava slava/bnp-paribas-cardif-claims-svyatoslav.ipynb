{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# преодобработка данных"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.preprocessing import OneHotEncoder    \nfrom sklearn import tree\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport lightgbm as lgb\nfrom lightgbm import LGBMModel,LGBMClassifier\n\n# импортируем функцию roc_auc_score()\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import PolynomialFeatures, PowerTransformer\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import GridSearchCV\n\n\n#########\n\ndf = pd.DataFrame()\ndf = pd.read_csv('../input/bnp-paribas-cardif-claims-management/train.csv.zip')\ndf_test = pd.read_csv('../input/bnp-paribas-cardif-claims-management/test.csv.zip')\n\n\n#########\n# берем катег признаки\n\ncat_features = df.dtypes[df.dtypes == 'object'].index\n\n\n#########\n\n# смотрим редкие категории\n# for i in cat_features:\n#     abs_freq = df[i].value_counts(dropna = False)\n    \n# избавляемся от редких категорий\n\nr=50\n\nfor i in cat_features:\n    abs_freq = df[i].value_counts(dropna = False)\n    df[i] = np.where(df[i].isin(abs_freq[abs_freq >= r].index.tolist()), df[i],'Other')\n    print(df[i].value_counts())\n\n\n# >>>>>>>>>>>>>>>>>>\n\n########\n\n# сделаем get_dummies в трейн и тест и удалим лишние столбцы из трейн/тест\ndf = pd.get_dummies(df, columns = cat_features, drop_first = True)\n\n# избавляемся от редких категорий\n# сразу вычислим пересечение столбцов трейна и теста, для этого загрузим test выборку\ndf_test = pd.read_csv('../input/bnp-paribas-cardif-claims-management/test.csv.zip')\ncat_features_df_test = df_test.dtypes[df_test.dtypes == 'object'].index\n\n# избавляемся от редких категорий\nfor i in cat_features_df_test:\n    abs_freq = df_test[i].value_counts(dropna = False)\n    df_test[i] = np.where(df_test[i].isin(abs_freq[abs_freq >= r].index.tolist()), df_test[i], 'Other')\n    print(df_test[i].value_counts())\n\n\n#########    \n# делаем get dummies\ndf_test = pd.get_dummies(df_test, columns = cat_features_df_test, drop_first = True)\n\nprint('df.shape, df_test.shape : ', df.shape, df_test.shape)\n\n# определяем общие признаки для обоих датасетов df / df_test и оставляем только их\ndf_list = list(df.columns)\ndf_test_list = list(df_test.columns)\n\ncommon_cols = set.intersection(set(df_test_list) & set(df_list))\nprint('# число общих колонок после get_dummies: / ', len(common_cols))\n\n\n# >>>>>>>>>>>>>>>>>>\n\n\n#########\n\n# Split the train data into train and test data\nx_train, x_test, y_train, y_test = train_test_split(\n    df.drop(labels = ['target'], axis = 1),\n    df['target'],\n    test_size = 0.3,\n    random_state = 0\n)\n\n# >>>>>>>>>>>>>>>>>>\nx_train = x_train[common_cols]\nx_test = x_test[common_cols]\ndf_test = df_test[common_cols]\n# >>>>>>>>>>>>>>>>>>\n\n\n\nprint(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n\n#########\n\n# заменим все NAN в train выборке на средние .mean()  - ничего не дает\n\n# numerical_cols = x_train.dtypes[x_train.dtypes != 'object'].index\n# for i in numerical_cols:\n#     x_train[i].fillna(x_train[i].median(), inplace = True)\n#     x_test[i].fillna(x_train[i].median(), inplace = True)\n\n\n#########\n\n# # используем PowerTransformer для нормализации данных - ничего не дает\n\n# pt = PowerTransformer()\n# pt.fit(x_train)                       ## Fit the PT on training data\n# x_train_pt = pt.transform(x_train)    ## Then apply on all data\n# x_test_pt = pt.transform(x_test)\n# df_test_pt = pt.transform(df_test)\n\n# x_train_pt = pd.DataFrame(x_train_pt)\n# x_test_pt = pd.DataFrame(x_test_pt)\n# df_test_pt = pd.DataFrame(df_test_pt)\n\n# # удалим нан-ы\n\n# for i in x_train_pt.columns:\n#     x_train_pt[i].fillna(x_train_pt[i].median(), inplace = True)\n#     x_test_pt[i].fillna(x_train_pt[i].median(), inplace = True)\n#     df_test_pt[i].fillna(x_train_pt[i].median(), inplace = True)\n\n# for i in x_train_pt.columns:\n#     x_train[i].fillna(x_train[i].median(), inplace = True)\n#     x_test[i].fillna(x_train[i].median(), inplace = True)\n#     df_test[i].fillna(x_train[i].median(), inplace = True)\n\n\n\n#########\n# удалим нан-ы\n\ntrain_features = x_train.dtypes[x_train.dtypes != 'object'].index\n\nfor i in train_features:\n    x_train[i].fillna(x_train[i].median(), inplace = True)\n    x_test[i].fillna(x_train[i].median(), inplace = True)\n    df_test[i].fillna(x_train[i].median(), inplace = True)\n\n# >>>>>>>>>>>>>>>>>>\nx_train.columns = train_features\nx_test.columns = train_features\ndf_test.columns = train_features\n# >>>>>>>>>>>>>>>>>>\n\n\n\nprint('x_train.columns: \\n', x_train.columns, 'df_test.columns: \\n', df_test.columns)\n\n#########\n# # удалим столбцы констант\n# x_train_pt = x_train_pt.loc[:,x_train_pt.apply(pd.Series.nunique) != 1]\n# x_train_pt.apply(pd.Series.nunique) != 1\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Посмотрим на признаки"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mutual information\n\n# мера, показывает как 2 переменные взаимо зависимы друг от друга. х от у и наоборот. \n# условно - это сколько информации дает нам знание одной переменной Х об другой переменной У. и наоборот.\n\nfrom sklearn.feature_selection import mutual_info_classif, mutual_info_regression\nfrom sklearn.feature_selection import SelectKBest, SelectPercentile\nfrom scipy import stats\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix\nimport os\nimport glob\nimport datetime\nimport dateutil.tz\n\n\n# x_train[train_features].isna().sum().sum()\n\n##################\n# нормализация\n# minmaxscaler (x - xmin / xmax - xmin)\n\n# scaler = MinMaxScaler()\n# print(scaler.fit(x_train))\n# print(scaler.data_max_)\n# x_train = scaler.transform(x_train)\n# x_train = pd.DataFrame(x_train)\n\n\n# x_test =  scaler.transform(x_test)\n# x_test = pd.DataFrame(x_test)\n\n\n\n\n##################\n# # удаляем дубли из датасета\n\n# df = df.drop_duplicates(subset = None,\n#                    keep = 'first',\n#                    inplace = True)\n\n\n##################\n# # сделаем box cox\n\n# for i in numerical_cols:\n#     positive_data = x_train[i][x_train[i] > 0]\n#     positive_data, lam = stats.boxcox(positive_data)\n#     print(lam, positive_data)\n# #     x_train[i], fitted_lambda = stats.boxcox(x_train[i])\n# #     x_test[i] = stats.boxcox(x_test[i], fitted_lambda)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.model_selection import GridSearchCV\n\n# ##################\n# # бустинг, подбираем лучшие темп через использование GS\n\n# Lgb = LGBMClassifier(n_estimators = 400, \n#                      random_state = 94, \n#                      max_depth = 5,\n#                      verbose = 1)\n\n# param_grid = {'learning_rate': [0.01, 0.05, 0.07, 0.09, 0.1]}\n\n# gs = GridSearchCV(Lgb,\n#                   param_grid,\n#                   scoring = 'roc_auc',\n#                   cv = 5,\n#                   n_jobs = -1,\n#                   return_train_score = False)\n\n# gs.fit(x_train, y_train)\n\n# print('лучшие значения параметров learn rate: \\n', gs.best_params_) #0.05\n# print('значение скора roc auc: \\n', gs.best_score_)  #0.7528472850502619\n\n\n# # print(accuracy_score(y_test, [i[1] for i in Lgb_pred])) \n# # print(roc_auc_score(y_test, [i[1] for i in Lgb_pred])) #0.7515020019901595\n# # print(roc_auc_score(y_test, Lgb_pred)) #0.7515020019901595\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# gs.cv_results_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # делаем GridSearchCV (минусы - долго, плюсы - перебирает все, высокая точность)\n\n# Lgb12 = LGBMClassifier(n_estimators = 400, \n#                      random_state = 94, \n#                      max_depth = 5,\n#                      learning_rate = 0.07,\n#                      verbose = 1)\n\n# param_grid2 = {'lambda_l1': [0, 5, 10],\n#               'bagging_fraction': [0.3, 0.5, 1],\n#               'feature_fraction': [0.3, 0.5, 1]}\n\n# gs2 = GridSearchCV(Lgb12,\n#                   cv = 5,\n#                   scoring = 'roc_auc',\n#                   param_grid = param_grid2,\n#                   n_jobs = -1,\n#                   return_train_score = False)\n\n# gs2.fit(x_train, y_train)\n\n# # 0.753937566934885 - 0.752869411327\n\n\n# print('лучшие значения параметров: \\n {}'.format(gs2.best_params_)) # {'bagging_fraction': 0.3, 'feature_fraction': 0.5, 'lambda_l1': 0}\n# print('значение скора roc auc: \\n {}'.format(gs2.best_score_)) #0.7539945492050786\n\n# лучшие значения параметров: \n#  {'bagging_fraction': 0.3, 'feature_fraction': 1, 'lambda_l1': 5}\n# значение скора roc auc: \n#  0.7537644557477048","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # делаем RandomizedSearchCV (минусы - рандомный ответ, плюсы - быстро)\n\n# from sklearn.model_selection import RandomizedSearchCV\n\n\n# Lgb13 = LGBMClassifier(n_estimators = 400, \n#                      random_state = 94, \n#                      max_depth = 5,\n#                      learning_rate = 0.07,\n#                      verbose = 1)\n\n# param_dist = {'lambda_l1': [0, 1, 2, 5, 10],\n#               'bagging_fraction': [0.3, 0.5, 0.7, 1],\n#               'feature_fraction': [0.3, 0.5, 0.7, 1]}\n\n# rs1 = RandomizedSearchCV(Lgb13,\n#                          cv = 5,\n#                          scoring = 'roc_auc',\n#                          n_iter = 10,\n#                          param_distributions = param_dist,\n#                          n_jobs = -1,\n#                          return_train_score = False,\n#                          verbose = 1)\n\n# rs1.fit(x_train, y_train)\n\n# print('rs1 лучшие значения параметров: \\n {}'.format(rs1.best_params_)) # {'bagging_fraction': 0.3, 'feature_fraction': 0.5, 'lambda_l1': 0}\n# print('rs1 значение скора roc auc: \\n {}'.format(rs1.best_score_)) #0.7539945492050786 - 0.7549460699800374\n\n# # rs1 лучшие значения параметров: \n# #  {'lambda_l1': 5, 'feature_fraction': 1, 'bagging_fraction': 0.3}\n# # rs1 значение скора roc auc: \n# #  0.7549460699800374\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"############### \n# доделываем тюнинг параметров \n# далее нам надо будет выполнить перекрестную проверку на всем x_train/y_train через\n# функцию cross_validate\n\n\n# мы сразу делаем модель с параметрами, которые нашли исходя из randomizedsearchCV \n# rs1 лучшие значения параметров: \n#  {learning_rate = 0.07, 'lambda_l1': 5, 'feature_fraction': 1, 'bagging_fraction': 0.3} // n_estimators = 400  // rs1 значение скора roc auc: 0.7549460699800374\n\n\nfrom sklearn.model_selection import cross_validate\n\nlgb14 = LGBMClassifier(n_estimators = 400,\n                       random_state = 94,\n                       learning_rate = 0.07,\n                       lambda_l1 = 5,\n                       feature_fraction = 1,\n                       bagging_fraction = 0.3,\n                       importance_type='gain')\n\n\ncv = cross_validate(lgb14, \n                    x_train, \n                    y_train, \n                    cv = 5, \n                    scoring = 'roc_auc', \n                    return_estimator = True, \n                    verbose = 1)\n\n\n# дальше можно посмотреть cv['estimator'][i].feature_importances_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(cv['estimator'][0], cv['test_score'].mean())\ncv.keys()\n\ncv['test_score'].mean()\n# ?cross_validate","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fi = []\n\n# добавим все feature_importances_ в датафрейм\n\nfor i in range(len(cv['estimator'])):\n    fi.append(cv['estimator'][i].feature_importances_)    \nfi_pd = pd.DataFrame(index = x_train.columns.values, columns = range(len(fi)))\n\nfor i in fi_pd.columns:\n    fi_pd[i] = fi[i]\n    \n# for i in range(fi_pd.shape[0]):\n#     fi_pd['mean'][i] = fi_pd.iloc[i][0:5].mean()\nfi_pd['mean'] = fi_pd.mean(axis = 1)\n\nlgbm14_best_feat = fi_pd['mean'].sort_values(ascending = False).head(100)\n\nplt.figure(figsize = (15,14))\nplt.barh(lgbm14_best_feat.index, lgbm14_best_feat, color='g', height=0.4, align='center', alpha=0.4)\n# plt.yticks(fi_pd['mean'])\n# plt.xlabel(lgbm14_best_feat.index)\n\n# You can control the size of the figure using plt.figure (e.g., plt.figure(figsize = (6,12)))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# оставим только ТОП300 признаков и из них уже будем удалять плохие\n# отсортируем по возрастанию важности признаков, вверху будут самые слабые, их начнем удалять по одному\n\nlgbm14_test_feat = fi_pd['mean'].sort_values(ascending = False).iloc[:-377] #мы знаем что последние 377 признаков имеют важность 0\nlgbm14_test_feat_0 = fi_pd['mean'].sort_values(ascending = False).iloc[296:] #мы знаем что последние 377 признаков имеют важность 0\nlgbm14_test_feat = lgbm14_test_feat.sort_values(ascending = True)\nlgbm14_test_feat = list(lgbm14_test_feat.index.values)\n\n#len(lgbm14_test_feat) = 295\nprint(lgbm14_test_feat_0.describe(), lgbm14_test_feat )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # feature_to_remove_auc_score.append(auc_score_mean)\n\n# print(feature_to_remove_auc_score, max(feature_to_remove_auc_score), auc_score_all, auc_score_mean)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# теперь будем удалять по одному признаку и считать модель скор на каждой итерации\n\n# для этого создаем три списка:\n# 1 - для удаляемых признаков 2 - для скоров 3 - для подсчета разницы скоров\n\n# 1 - для удаляемых признаков\nfeature_to_remove = []\n# 2 - для скоров\nfeature_to_remove_auc_score = []\n# 3 - для подсчета разницы скоров\nfeature_to_remove_auc_score_diff = []\n\ncount = 1\n#  {learning_rate = 0.07, 'lambda_l1': 5, 'feature_fraction': 1, 'bagging_fraction': 0.3} // rs1 значение скора roc auc: 0.7549460699800374\n# len(lgbm14_bad_feat)\n\nauc_score_all = cv['test_score'].mean()\nfeature_to_remove_auc_score.append(auc_score_all)\n\n\n# запишем все в датафрейм, для него создадим отдельные списки\nfeatures_for_df = []\nscores_for_df1_after = []\nscores_for_df2_before = []\ndiff_for_df = []\n\nfor i in range(1):\n    \n    feature_to_remove.append(lgbm14_test_feat[i])\n    \n    print('\\n', 'счет номер = ', count, '\\n', 'удаляем признак ', lgbm14_test_feat[i], '\\n', 'список feature_to_remove пополнили:', feature_to_remove)\n    model = LGBMClassifier(n_estimators = 400,\n                           learning_rate = 0.07, \n                           lambda_l1 = 3, \n                           feature_fraction = 1, \n                           bagging_fraction = 0.3,\n                           random_state = 94)\n\n    auc_score = cross_val_score(model,\n                                x_train[lgbm14_test_feat].drop(feature_to_remove, axis = 1),\n                                y_train,\n                                scoring = 'roc_auc',\n                                verbose = 1)\n\n# auc_score_mean - аук модели в цикле на каждой итерации считается свой\n    auc_score_mean = auc_score.mean()\n\n    diff = auc_score_mean - max(feature_to_remove_auc_score)\n\n    \n    # запишем все в датафрейм, для него создадим отдельные списки\n\n    features_for_df.append(lgbm14_test_feat[i])\n    scores_for_df1_after.append(auc_score_mean)\n    scores_for_df2_before.append(max(feature_to_remove_auc_score))\n    diff_for_df. append(diff)\n    \n#     all тоже надо пересчитывать!\n    count = count + 1\n      \n    print('auc модели после удаления', '\\n', \n          auc_score_mean, '\\n',\n          'auc модели со всеми признаками :', '\\n', \n          max(feature_to_remove_auc_score), '\\n')\n    \n    feature_to_remove_auc_score.append(auc_score_mean)\n    \n    if diff > 0:\n        print('видим, что разница скоров > 0.0001, значит стало лучше, от признака можно избавиться')\n        \n    else:\n        print('видим, что разница скоров < 0.0001 или отрицательна, значит стало хуже, признак следует оставить')\n        feature_to_remove.remove(lgbm14_test_feat[i])\n        feature_to_remove_auc_score.remove(auc_score_mean)\n\n# создадим датафрейм, в который будем записывать все значения\ndf_feat_to_remove = pd.DataFrame({'features': features_for_df,\n                                  'scores before remove': scores_for_df2_before,\n                                  'scores after remove': scores_for_df1_after,\n                                  'diff': diff_for_df})\n\n\n# # 75 признаков удаляли\n# auc модели после удаления \n#  0.7548826898632182 \n#  auc модели со всеми признаками : \n#  0.755370837432026 \n\n# видим, что разница скоров < 0.0001 или отрицательна, значит стало хуже, признак следует оставить\n\n#  счет номер =  75 \n#  удаляем признак  v48 \n#  список feature_to_remove пополнили: ['v112_R', 'v113_AF', 'v47_F', 'v113_F', 'v79_M', 'v79_O', 'v56_DH', 'v91_F', 'v48']\n\n# видим, что разница скоров < 0.0001 или отрицательна, значит стало хуже, признак следует оставить\n\n#  счет номер =  63 \n#  удаляем признак  v79_K \n#  список feature_to_remove пополнили: ['v22_QNA', 'v113_U', 'v125_CF', 'v125_R', 'v125_BI', 'v125_CD', 'v56_DJ', 'v30_F', 'v22_WRI', 'v112_B', 'v113_L', 'v125_Z', 'v79_K']\n\n\n#  счет номер =  250 \n#  список feature_to_remove пополнили: ['v22_QNA', 'v113_U', 'v125_CF', 'v125_R', 'v125_BI', 'v125_CD', 'v56_DJ', 'v30_F', 'v22_WRI', 'v112_B', 'v113_L', 'v125_Z', 'v112_P']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('список feature_to_remove: ', '''['v22_QNA', 'v113_U', 'v125_CF', 'v125_R', 'v125_BI', 'v125_CD', 'v56_DJ', 'v30_F', 'v22_WRI', 'v112_B', 'v113_L', 'v125_Z', 'v112_P']''')\nfeature_to_remove = ['v22_QNA', 'v113_U', 'v125_CF', 'v125_R', 'v125_BI', 'v125_CD', 'v56_DJ', 'v30_F', 'v22_WRI', 'v112_B', 'v113_L', 'v125_Z', 'v112_P']\n\nprint('удаляем feature_to_remove из train_features ', '\\n', 'длина train_features до удаления', len(train_features))\n\ntrain_features = [x for x in train_features if x not in feature_to_remove and x not in lgbm14_test_feat_0]\ntrain_features\n\nprint('длина train_features после удаления', len(train_features))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import numpy\n# from sklearn.model_selection import RandomizedSearchCV\n# from sklearn.model_selection import cross_validate\n\n# # lgb14 = LGBMClassifier(n_estimators = 1500,\n# #                        random_state = 94,\n# #                        lambda_l1 = 5,\n# #                        feature_fraction = 1,\n# #                        bagging_fraction = 0.3,\n# #                        importance_type='gain')\n\n# # dist_param = {'learning_rate': [0.01, 0.03, 0.05, 0.08, 0.1, 0.12, 0.18]}\n# # gs_final = RandomizedSearchCV(lgb14,\n# #                               cv = 10,\n# #                               n_iter = 10,\n# #                               param_distributions = dist_param,\n# #                               scoring = 'roc_auc',\n# #                               return_train_score = False)\n\n# # gs_final.fit(x_train[train_features], y_train)\n\n# # print('gs_final лучшие значения параметров: \\n {}'.format(gs_final.best_params_)) \n# # print('gs_final значение скора roc auc: \\n {}'.format(gs_final.best_score_)) \n\n# # # # выяснили что лучший результат достигается при learning_rate: 0.01\n\n# lgb14 = LGBMClassifier(n_estimators = 1500,\n#                        random_state = 94,\n#                        lambda_l1 = 5,\n#                        learning_rate = 0.008,\n#                        feature_fraction = 1,\n#                        bagging_fraction = 0.3,\n#                        importance_type='gain')\n\n# lgb14.fit(x_train[train_features], y_train)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #############\n# # сделаем sub \n\n# pred = lgb14.predict_proba(df_test[train_features])\n\n# sub = pd.DataFrame()\n# sub['ID'] = df_test['ID']\n# sub['PredictedProb'] = [i[1] for i in pred]\n\n# current_time = datetime.datetime.now(dateutil.tz.tzlocal()).strftime('date_%Y-%m-%d__time_%H-%M')\n# flname_sub = 'submission__' + current_time +'.csv' # submission file name\n\n# sub.to_csv(flname_sub, index = False)\n\n# # скор незначительно улучшился - было / стало :: 0.46956 / 0.46984 / 0.46722 / 0.46728","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##############\n# пробуем оценить работу признаков на разных уровнях глубины \n# считаем lgbm для всех признаков на разной глубине\n\nfi_depth = []\n\ndepth = [1,2,3,4,5]\n\nfor i in depth:\n    \n    lgb15 = LGBMClassifier(n_estimators = 400,\n                           max_depth = i,\n                           random_state = 94,\n                           learning_rate = 0.07,\n                           lambda_l1 = 5,\n                           feature_fraction = 1,\n                           bagging_fraction = 0.3,\n                           importance_type='gain')\n    \n    lgb15 = lgb15.fit(x_train[train_features], y_train)\n    fi_depth.append(lgb15.feature_importances_)\n\nfi_depth\n\n# d = ({'features': train_features,\n#       'feature_importance ' + str(range(1,5,1)): fi_depth})\n\n\n    \n# делаем дф со значениями объекты = признаки, колонки = важность глуб1/2/3/4/5, важность средняя\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fi_depth[0][0:50]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fi_depth_df = pd.DataFrame(index = train_features)\nfor i in range(len(fi_depth)):\n    fi_depth_df['col_' + str(i+1)] = fi_depth[i]\n    \nfi_depth_df = fi_depth_df.sort_values(list(fi_depth_df.columns), ascending = [False, False,False, False,False])\n# fi_depth_df.sort_values(list(fi_depth_df.columns), ascending = [False, False, False, False, False])\nfi_depth_df['mean'] = fi_depth_df.mean(axis=1)\n\nfi_depth_df = fi_depth_df.sort_values('mean', ascending = False)\nfi_depth_df[60:120]\n\n# почему то есть признаки которые очень важны на 1ом уровне глубины, но менее важны на втором и наоборот! пример \n# v47_C\t на 1ом уровне = 0.000000, а дальше очень важен 5541.829421\t6040.218602\t7157.042309\t7947.199432\t5337.257953\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# в этом блоке попробуем ORDINAL ENC'\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.preprocessing import OneHotEncoder    \nfrom sklearn import tree\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport lightgbm as lgb\nfrom lightgbm import LGBMModel,LGBMClassifier\n\n# импортируем функцию roc_auc_score()\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import PolynomialFeatures, PowerTransformer\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import GridSearchCV\n\nfrom numpy import asarray\nfrom sklearn.preprocessing import OrdinalEncoder\n\n\n#########\n\ndf = pd.DataFrame()\ndf = pd.read_csv('../input/bnp-paribas-cardif-claims-management/train.csv.zip')\ndf_test = pd.read_csv('../input/bnp-paribas-cardif-claims-management/test.csv.zip')\n\n\n# нам надо сделать по порядку:\n# 1. сделать трейн тест\n# 2. заменить нан на самое частое значение трейна\n# 3. обучить онкод на трейне. сделать ординал енкодинг везде (трейн/валид/тест), но обучим его только на трейне\n\n#########\n# берем катег признаки\n\ncat_features = df.dtypes[df.dtypes == 'object'].index\n\n# разбиваем на трейн тест\nx_train, x_test, y_train, y_test = train_test_split(\n    df.drop(labels = ['target'], axis = 1),\n    df['target'],\n    test_size = 0.3,\n    random_state = 0\n)\n\n\n# заменим все нан на самые частые в трейне\nfor i in x_train[cat_features].columns:\n    x_train[i] = x_train[cat_features][i].fillna(x_train[cat_features][i].value_counts().index[0])\n    x_test[i] = x_test[cat_features][i].fillna(x_train[cat_features][i].value_counts().index[0])\n    df_test[i] = df_test[cat_features][i].fillna(x_train[cat_features][i].value_counts().index[0])\n    \n# обучим на трейне\nenc = OrdinalEncoder()\nenc.fit(x_train[cat_features])\n\n# сделаем OrdinalEncoder для всего\nresult = enc.fit_transform(x_train[cat_features])\nresult2 = enc.fit_transform(x_test[cat_features])\nresult3 = enc.fit_transform(df_test[cat_features])\n\nx_train[cat_features] = result\nx_test[cat_features] = result2\ndf_test[cat_features] = result3\n\n# x_train[[str(i) + '_1' for i in cat_features]] = result\n# x_test[str(i) + '_1' for i in cat_features] = result2\n# df_test[str(i) + '_1' for i in cat_features] = result3\n\n# print(x_train[['v3', 'v3_1']])\n\n# надо еще проверить, что наны в тест заменились на те же значения что и в ТРЕЙНЕ\n\ntrain_features = x_train.dtypes[x_train.dtypes != 'object'].index\n\n# заменим нан в количественных признаках\nfor i in train_features:\n    x_train[i].fillna(x_train[i].median(), inplace = True)\n    x_test[i].fillna(x_train[i].median(), inplace = True)\n    df_test[i].fillna(x_train[i].median(), inplace = True)\n\n\nx_train[train_features][:20]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# подберем лучшие параметры для LGBM\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# нашли лучшие параметры: {'learning_rate': 0.07, 'lambda_l1': 5, 'feature_fraction': 0.5, 'bagging_fraction': 1}\n\n# lgb20 = LGBMClassifier(n_estimators = 400,\n#                        random_state = 94,\n#                        importance_type='gain')\n\n# dist_param = {'learning_rate': [0.07, 0.1, 0.14, 0.2],\n#               'feature_fraction': [0.3, 0.5, 1],\n#               'bagging_fraction': [0.3, 0.5, 1],\n#               'lambda_l1': [0, 5, 10, 15]\n#              }\n\n\n# rs20 = RandomizedSearchCV(lgb20,\n#                         cv = 5,\n#                         scoring = 'roc_auc',\n#                         param_distributions = dist_param,\n#                         return_train_score = True)\n\n\n# rs20.fit(x_train[train_features], y_train)\n# print(rs20.best_params_, rs20.best_score_, rs20.best_estimator_)\n\n\n# дальше можно посмотреть rs20['estimator'][i].feature_importances_\n\n# {'learning_rate': 0.07, 'lambda_l1': 5, 'feature_fraction': 0.5, 'bagging_fraction': 1} 0.7549020977510434 LGBMClassifier(bagging_fraction=1, feature_fraction=0.5, importance_type='gain',\n#                lambda_l1=5, learning_rate=0.07, n_estimators=400,\n#                random_state=94)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train[train_features].shape\n# print(rs20.best_params_, rs20.best_score_, rs20.best_estimator_)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb20 = LGBMClassifier(n_estimators = 400,\n                       random_state = 94,\n                       lambda_l1 = 5,\n                       learning_rate = 0.07,\n                       feature_fraction = 0.5,\n                       bagging_fraction = 1,\n                       importance_type='gain')\n\nlgb20.fit(x_train[train_features], y_train)\n\ncv = cross_val_score(lgb20,\n                     x_train[train_features],\n                     y_train,\n                     cv=5,\n                     scoring = 'roc_auc'\n                    )\n\nprint('cross val score BEFORE lgb20 with features removed: ', cv.mean())\n\n# CORR фичи\n# удалим фичи, которые имеют корреляцию\n\n##################\n# # выберем и удалим наиболее коррелирующие признаки\n\ncorr = x_train.corr()\n# sns.heatmap(corr, cmap='coolwarm')\n# corr.head()\n# minmaxscaler (x - xmin / xmax - xmin)\n\ntrain_features = x_train.dtypes[x_train.dtypes != 'object'].index.values\n\n\ncorr.index = train_features\ncorr.columns = train_features\ncorr.head()\n\n##############\n# создадим список фич где коэф корреляции > 0,95\nlist_corr_features = []\n\nfor i in corr.index:\n    for j in corr.columns:\n        if corr.loc[i,j] > 0.95:\n            list_corr_features.append([i,j,corr.loc[i,j]])\nprint('list_corr_features первые 10 штук: ', list_corr_features[1:10])\n\n# переведем список в датафрейм\nlist_corr_features_1 = []\nlist_corr_features_2 = []\nlist_corr_features_3 = []\n\nfor i in range(len(list_corr_features)):\n    list_corr_features_1.append(list_corr_features[i][0])\n    list_corr_features_2.append(list_corr_features[i][1])\n    list_corr_features_3.append(list_corr_features[i][2])\n\ndf_corr_features = pd.DataFrame(\n    {'feature1': list_corr_features_1,\n     'feature2': list_corr_features_2,\n     'corr_coef': list_corr_features_3,\n    })\n\n##############\n# найдем дубли\ndf_corr_features['feat1+feat2'] = df_corr_features['feature1']+' '+df_corr_features['feature2']\ndf_corr_features['feat2+feat1'] = df_corr_features['feature2']+' '+df_corr_features['feature1']\n# print(df_corr_features[df_corr_features['corr_coef'] != 1])\n\n# нам надо найти строки попарно, где feat1+feat2 = feat2+feat1 и разделить их как дубли\n    \nrows_to_del = []\nfor i in df_corr_features.index:\n    for j in df_corr_features.index:\n        if df_corr_features['feat1+feat2'][i] == df_corr_features['feat2+feat1'][j]:\n            if i != j:\n                rows_to_del.append(j)\nrows_to_del\n\n# теперь возьмем все фичи из колонки feature2 (но можно из feature1, это не важно) и фильтр на строки rows_to_del, их хотели удалять\n# строки не будем удалять, вместо этого возьмем из них сразу фичи, которые имеют высокую корреляцию с другими, поэтому от них можно избавиться\nfeatures_to_del = set(df_corr_features['feature2'].loc[rows_to_del].values)\nfeatures_to_del\n\n\n###############\n# # удалим признаки из списка train features чтобы обучить модель\n# print('features_to_del after corr(): ', features_to_del, 'len(features_to_del): ', len(features_to_del))\n\n# train_features = list(train_features)\n# train_features = [i for i in train_features if i not in features_to_del]\n# print('\\n', 'длина списка train_features после удаления features_to_del: ', len(train_features),'\\n','длина списка features_to_del: ',  len(features_to_del), '\\n', train_features, '\\n', features_to_del,)\n\n\n# # удаление коррелирующих дает незначительный прирост в моделях\n\n# # # ################## Mutual Information\n# # # чем выше скор, тем сильнее влияет признак на таргет\n# # mi = mutual_info_classif(x_train, y_train)\n# # mi = pd.Series(mi)\n# # print(mi.describe())\n# # mi.index = x_train.columns\n# # top50mi = mi.sort_values(ascending=False).head(50)\n# # mi[top50mi.index.values].sort_values(ascending=False).plot.bar(figsize=(20, 8))\n# # mi.describe()\n# # mi.head()\n\n# # # использование MI не дает прироста в моделях\n\n\n# lgb20 = LGBMClassifier(n_estimators = 400,\n#                        random_state = 94,\n#                        lambda_l1 = 5,\n#                        learning_rate = 0.07,\n#                        feature_fraction = 0.5,\n#                        bagging_fraction = 1,\n#                        importance_type='gain')\n\n# lgb20.fit(x_train[train_features], y_train)\n\n# cv = cross_val_score(lgb20,\n#                      x_train[train_features],\n#                      y_train,\n#                      cv=5,\n#                      scoring = 'roc_auc'\n#                     )\n\n# print('cross val score after lgb20 with features removed: ', cv.mean())\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_corr_features[df_corr_features['corr_coef'] < 1]\n\ndf_corr_features[df_corr_features['corr_coef'] < 1]['feature1'].value_counts()\n# df_corr_features['feature2'].values_counts()\n\nconc = pd.concat([df_corr_features[df_corr_features['corr_coef'] < 1]['feature1'].value_counts(),\n                  df_corr_features[df_corr_features['corr_coef'] < 1]['feature2'].value_counts()],\n                  axis = 0)\nconc = conc.reset_index(drop = False)\n\nconc.columns = ['feat', 'quantity']\nconc_gr = conc.groupby(by = 'feat', axis = 0).sum()\nconc_gr.sort_values(by = 'quantity', ascending = False)\n\ncorr_feat_to_remove = [i for i in conc_gr.index if conc_gr['quantity'][i] >= 4]\ncorr_feat_to_remove\n\ntrain_features = [i for i in train_features if i not in corr_feat_to_remove]\n\nlgb20 = LGBMClassifier(n_estimators = 400,\n                       random_state = 94,\n                       lambda_l1 = 5,\n                       learning_rate = 0.07,\n                       feature_fraction = 0.5,\n                       bagging_fraction = 1,\n                       importance_type='gain')\n\nlgb20.fit(x_train[train_features], y_train)\n\ncv = cross_val_score(lgb20,\n                     x_train[train_features],\n                     y_train,\n                     cv=5,\n                     scoring = 'roc_auc'\n                    )\n\nprint('было 0.7529159048596458, стало после удаления  corr_feat_to_remove: ', cv.mean())\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install rfpimp\n# from rfpimp import *\n\n# # вычисляем матрицу взаимозависимостей признаков, значения - это \n# # пермутированные важности признаков,с помощью которых мы пытаемся \n# # предсказать интересующий признак\n# D = feature_dependence_matrix(x_train[train_features], sort_by_dependence=True)\n# viz = plot_dependence_heatmap(D, figsize=(18, 18))\n# viz.view()\n\n# # TypeError: _generate_unsampled_indices() missing 1 required positional argument: 'n_samples_bootstrap'\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMModel,LGBMClassifier\n\n\n# применяем лучшие параметры к модели\nlgb20 = LGBMClassifier(n_estimators = 400,\n                       random_state = 94,\n                       importance_type = 'gain',\n                       learning_rate = 0.07, \n                       lambda_l1 = 5, \n                       feature_fraction = 0.5, \n                       bagging_fraction = 1)\n\n# ищем важности признаков\ncv20 = cross_validate(lgb20,\n                      x_train[train_features],\n                      y_train,\n                      cv = 5,\n                      return_estimator = True,\n                      scoring = 'roc_auc',\n                      verbose = 1)\n\ncv20['test_score'].mean()\n\n# записываем важности в датафрейм\n\nfi20 = []\n\n# добавим все feature_importances_ в датафрейм\n\nfor i in range(len(cv20['estimator'])):\n    fi20.append(cv20['estimator'][i].feature_importances_) \n    \nfi20_pd = pd.DataFrame(index = x_train[train_features].columns.values, columns = range(len(fi20)))\n\nfor i in fi20_pd.columns:\n    fi20_pd[i] = fi20[i]\n    \n# for i in range(fi20_pd.shape[0]):\n#     fi20_pd['mean'][i] = fi20_pd.iloc[i][0:5].mean()\nfi20_pd['mean'] = fi20_pd.mean(axis = 1)\n\npict_data = fi20_pd['mean'].sort_values(ascending = False)\n\nplt.figure(figsize = (15,14))\nplt.barh(pict_data.index, pict_data, color='g', height=0.4, align='center', alpha=0.4)\n# plt.yticks(fi20_pd['mean'])\n# plt.xlabel(lgb21_best_feat.index)\n\n# You can control the size of the figure using plt.figure (e.g., plt.figure(figsize = (6,12)))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# оставим только ТОП300 признаков и из них уже будем удалять плохие\n# отсортируем по возрастанию важности признаков, вверху будут самые слабые, их начнем удалять по одному\n\nlgb20_test_feat = fi20_pd['mean'].sort_values(ascending = False).index \n\nlgb20_test_feat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# # теперь будем удалять по одному признаку и считать модель скор на каждой итерации\n\n# # для этого создаем три списка:\n# # 1 - для удаляемых признаков 2 - для скоров 3 - для подсчета разницы скоров\n\n# # 1 - для удаляемых признаков\n# feature_to_remove = []\n# # 2 - для скоров\n# feature_to_remove_auc_score = []\n# # 3 - для подсчета разницы скоров\n# feature_to_remove_auc_score_diff = []\n\n# # train_features = x_train.dtypes[x_train.dtypes != 'object'].index.values\n\n# count = 1\n\n# auc_score_all = cv20['test_score'].mean()\n# feature_to_remove_auc_score.append(auc_score_all)\n\n\n# # запишем все в датафрейм, для него создадим отдельные списки\n# features_for_df = []\n# scores_for_df1_after = []\n# scores_for_df2_before = []\n# diff_for_df = []\n\n# # len(lgb20_test_feat)\n# for i in range(2):\n    \n#     feature_to_remove.append(lgb20_test_feat[i])\n    \n#     print('\\n', 'счет номер = ', count, '\\n', 'удаляем признак ', lgb20_test_feat[i], '\\n', 'список feature_to_remove пополнили:', feature_to_remove)\n\n#     model = LGBMClassifier(n_estimators = 400,\n#                        random_state = 94,\n#                        importance_type = 'gain',\n#                        learning_rate = 0.07, \n#                        lambda_l1 = 5, \n#                        feature_fraction = 0.5, \n#                        bagging_fraction = 1)\n\n#     auc_score = cross_val_score(model,\n#                                 x_train[lgb20_test_feat].drop(feature_to_remove, axis = 1),\n#                                 y_train,\n#                                 scoring = 'roc_auc',\n#                                 verbose = 1)\n\n# # auc_score_mean - аук модели в цикле на каждой итерации считается свой\n#     auc_score_mean = auc_score.mean()\n\n#     diff = auc_score_mean - max(feature_to_remove_auc_score)\n\n    \n#     # запишем все в датафрейм, для него создадим отдельные списки\n\n#     features_for_df.append(lgb20_test_feat[i])\n#     scores_for_df1_after.append(auc_score_mean)\n#     scores_for_df2_before.append(max(feature_to_remove_auc_score))\n#     diff_for_df. append(diff)\n    \n# #     all тоже надо пересчитывать!\n#     count = count + 1\n      \n#     print('auc модели после удаления', '\\n', \n#           auc_score_mean, '\\n',\n#           'auc модели со всеми признаками :', '\\n', \n#           max(feature_to_remove_auc_score), '\\n')\n    \n#     feature_to_remove_auc_score.append(auc_score_mean)\n    \n#     if diff > 0:\n#         print('видим, что разница скоров > 0.0001, значит стало лучше, от признака можно избавиться')\n        \n#     else:\n#         print('видим, что разница скоров < 0.0001 или отрицательна, значит стало хуже, признак следует оставить')\n#         feature_to_remove.remove(lgb20_test_feat[i])\n#         feature_to_remove_auc_score.remove(auc_score_mean)\n\n# # создадим датафрейм, в который будем записывать все значения\n# df_feat_to_remove = pd.DataFrame({'features': features_for_df,\n#                                   'scores before remove': scores_for_df2_before,\n#                                   'scores after remove': scores_for_df1_after,\n#                                   'diff': diff_for_df})\n# df_feat_to_remove\n# feature_to_remove\n\n# found_list = ['v21', 'ID', 'v23', 'v16', 'v32', 'v55']\n\n# train_features = [x for x in train_features if x not in feature_to_remove and x not in found_list]\n# train_features\n\n\n# lgb20 = LGBMClassifier(n_estimators = 400,\n#                        random_state = 94,\n#                        lambda_l1 = 5,\n#                        learning_rate = 0.07,\n#                        feature_fraction = 0.5,\n#                        bagging_fraction = 1,\n#                        importance_type='gain')\n\n# lgb20.fit(x_train[train_features], y_train)\n\n# #############\n# # сделаем sub \n\n# pred = lgb20.predict_proba(df_test[train_features])\n\n\n# sub = pd.DataFrame()\n# sub['ID'] = df_test['ID']\n# sub['PredictedProb'] = [i[1] for i in pred]\n\n# current_time = datetime.datetime.now(dateutil.tz.tzlocal()).strftime('date_%Y-%m-%d__time_%H-%M')\n# flname_sub = 'submission__' + current_time +'.csv' # submission file name\n\n# sub.to_csv(flname_sub, index = False)\n\n# # скор незначительно улучшился - было / стало :: 0.46956 / 0.46984 / 0.46722 / 0.46728\n\n\n# # ЖАДНЫЙ ОТБОР,  список feature_to_remove пополнили: ['v21', 'ID', 'v23', 'v16', 'v32', 'v55']\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# попробуем другой подход\n# возьмем ТОП20 фичей по важности и начнем добавлять по одной фиче \n\nlgb20_test_feat_top = list(fi20_pd['mean'].sort_values(ascending = False)[0:20].index)\nlgb20_test_feat_add = list(fi20_pd['mean'].sort_values(ascending = False)[21:].index)\n\n\n# применяем лучшие параметры к модели\nlgb20 = LGBMClassifier(n_estimators = 400,\n                       random_state = 94,\n                       importance_type = 'gain',\n                       learning_rate = 0.07, \n                       lambda_l1 = 5, \n                       feature_fraction = 0.5, \n                       bagging_fraction = 1)\n\n# ищем важности признаков\ncv20 = cross_validate(lgb20,\n                      x_train[lgb20_test_feat_top],\n                      y_train,\n                      cv = 5,\n                      return_estimator = True,\n                      scoring = 'roc_auc',\n                      verbose = 1)\n\ncv20['test_score'].mean()\n\n\n# теперь будем добавлять по одному признаку и считать модель скор на каждой итерации\n\n# для этого создаем три списка:\n# 1 - для удаляемых признаков 2 - для скоров 3 - для подсчета разницы скоров\n\n# 1 - для удаляемых признаков\nfeature_to_remove = []\n# 2 - для скоров\nfeature_to_remove_auc_score = []\n# 3 - для подсчета разницы скоров\nfeature_to_remove_auc_score_diff = []\n\n# train_features = x_train.dtypes[x_train.dtypes != 'object'].index.values\n\ncount = 1\n\nauc_score_all = cv20['test_score'].mean()\nfeature_to_remove_auc_score.append(auc_score_all)\n\n\n# запишем все в датафрейм, для него создадим отдельные списки\nfeatures_for_df = []\nscores_for_df1_after = []\nscores_for_df2_before = []\ndiff_for_df = []\n\n# len(lgb20_test_feat)\nfor i in range(len(lgb20_test_feat_add)):\n    \n    lgb20_test_feat_top.append(lgb20_test_feat_add[i])\n    \n    print('\\n', 'счет номер = ', count, '\\n', 'добавляем признак ', lgb20_test_feat_add[i], '\\n')\n    \n    model = LGBMClassifier(n_estimators = 400,\n                       random_state = 94,\n                       importance_type = 'gain',\n                       learning_rate = 0.07, \n                       lambda_l1 = 5, \n                       feature_fraction = 0.5, \n                       bagging_fraction = 1)\n\n    auc_score = cross_val_score(model,\n                                x_train[lgb20_test_feat_top],\n                                y_train,\n                                scoring = 'roc_auc',\n                                verbose = 1)\n\n# auc_score_mean - аук модели в цикле на каждой итерации считается свой\n    auc_score_mean = auc_score.mean()\n\n    diff = auc_score_mean - max(feature_to_remove_auc_score)\n\n    \n    # запишем все в датафрейм, для него создадим отдельные списки\n\n    features_for_df.append(lgb20_test_feat_add[i])\n    scores_for_df1_after.append(auc_score_mean)\n    scores_for_df2_before.append(max(feature_to_remove_auc_score))\n    diff_for_df.append(diff)\n    \n#     all тоже надо пересчитывать!\n    count = count + 1\n      \n    print('auc модели после добавления', '\\n', \n          auc_score_mean, '\\n',\n          'максимальный auc модели со до добавления :', '\\n', \n          max(feature_to_remove_auc_score), '\\n')\n    \n    feature_to_remove_auc_score.append(auc_score_mean)\n    \n    if diff > 0.0003:\n        print('видим, что разница скоров > 0.0003, значит стало лучше, от признак можно оставить, финальный список:', lgb20_test_feat_top)\n        \n    else:\n        print('видим, что разница скоров < 0.0003 или отрицательна, значит стало хуже, признак следует убрать')\n        lgb20_test_feat_top.remove(lgb20_test_feat_add[i])\n        feature_to_remove_auc_score.remove(auc_score_mean)\n\n# создадим датафрейм, в который будем записывать все значения\n# df_feat_to_add = pd.DataFrame({'features': lgb20_test_feat_top,\n#                                   'scores before remove': scores_for_df2_before,\n#                                   'scores after remove': scores_for_df1_after,\n#                                   'diff': diff_for_df})\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = [x for x in lgb20_test_feat_top]\ntrain_features\n\n\nlgb20 = LGBMClassifier(n_estimators = 400,\n                       random_state = 94,\n                       lambda_l1 = 5,\n                       learning_rate = 0.07,\n                       feature_fraction = 0.5,\n                       bagging_fraction = 1,\n                       importance_type='gain')\n\nlgb20.fit(x_train[train_features], y_train)\n\n#############\n# сделаем sub \n\npred = lgb20.predict_proba(df_test[train_features])\n\n\nsub = pd.DataFrame()\nsub['ID'] = df_test['ID']\nsub['PredictedProb'] = [i[1] for i in pred]\n\ncurrent_time = datetime.datetime.now(dateutil.tz.tzlocal()).strftime('date_%Y-%m-%d__time_%H-%M')\nflname_sub = 'submission__' + current_time +'.csv' # submission file name\n\nsub.to_csv(flname_sub, index = False)\n\n# скор незначительно улучшился - было / стало :: 0.46956 / 0.46984 / 0.46722 / 0.46728\n\n\n# ЖАДНЫЙ ОТБОР,  список feature_to_remove пополнили: ['v21', 'ID', 'v23', 'v16', 'v32', 'v55']\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# обучение модели"},{"metadata":{"trusted":true},"cell_type":"code","source":"###################\n\n# лог регрессия + power transform\n\n# logreg = LogisticRegression()\n# logreg.fit(x_train_pt, y_train)\n# log_pred = logreg.predict(x_test_pt)\n# print(accuracy_score(y_test, log_pred)) # 0.777123363559495\n# print(roc_auc_score(y_test, log_pred)) #0.5827874126253638\n\n\n###################\n# лог регрессия\n\n# logreg = LogisticRegression(verbose=1)\n# logreg.fit(x_train, y_train)\n# log_pred = logreg.predict(x_test)\n# print(accuracy_score(y_test, log_pred)) #0.7625156719246581\n# print(roc_auc_score(y_test, log_pred))\n\n# print(cross_val_score(logreg, df_x, df_y, cv=15))\n\n\n###################\n# дерево решеений\n\n# clf = tree.DecisionTreeClassifier()\n# clf.fit(x_train[train_features], y_train)\n# tree_pred = clf.predict(x_test[train_features])\n# print(accuracy_score(y_test, tree_pred)) # 0.6966206956876695\n# print(roc_auc_score(y_test, tree_pred)) #0.5861745101264721\n# print(cross_val_score(clf, df_x, df_y, cv=15))\n\n\n###################\n# кнн\n\n# knn = KNeighborsClassifier(n_neighbors=10)\n# knn.fit(x_train, y_train)\n# knn_pred = knn.predict(x_test)\n# print(accuracy_score(y_test, knn_pred)) # 0.730646995364026\n# print(roc_auc_score(y_test, knn_pred)) #0.5028577536322103\n\n###################\n# кнн + PT\n\n# knn = KNeighborsClassifier(n_neighbors=5)\n# knn.fit(x_train_pt, y_train)\n# knn_pred = knn.predict(x_test_pt)\n# print(accuracy_score(y_test, knn_pred)) # 0.730646995364026\n# print(roc_auc_score(y_test, knn_pred)) #0.4991711748494437\n\n# # >>>>>>>>>>>>>>>>>>\n\n# ###################\n# # случайный лес\n\n# clfRF = RandomForestClassifier(max_depth=7, random_state=0, verbose=1)\n# clfRF.fit(x_train[train_features], y_train)\n# clfRF_pred = clfRF.predict(x_test[train_features])\n# print(accuracy_score(y_test, clfRF_pred)) # 0.7625448289937895\n# print(roc_auc_score(y_test, clfRF_pred)) #0.5\n# # print(cross_val_score(clfRF, df_x, df_y, cv=15))\n\n# # визуализируем важные признаки\n# features = train_features\n# importances = clfRF.feature_importances_[0:20]\n# indices = np.argsort(importances)\n\n# plt.title('Feature Importances')\n# plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n# plt.yticks(range(len(indices)), [features[i] for i in indices])\n# plt.xlabel('Relative Importance')\n# plt.show()\n\n# pred = clfRF.predict(df_test)\n\n# # собираем sub\n# # Lgb\n# sub = pd.DataFrame()\n# sub['ID'] = df_test['ID']\n# sub['PredictedProb'] = pred\n# sub.to_csv('submit_baseline6.csv', index = False)\n\n# print('clfRF importances: \\n', importances, '\\n', 'feat indices: \\n', features[indices])\n\n# # print('clfRF features importance: \\n', features[indices], '\\n', 'VS MI - mi[top50mi.index.values]: \\n', mi[top50mi.index.values])\n\n# # >>>>>>>>>>>>>>>>>>\n\n\n###################\n# случайный лес + PT\n\n# clfRF1 = RandomForestClassifier(max_depth=7, random_state=12)\n# clfRF1.fit(x_train_pt, y_train)\n# clfRF_pred1 = clfRF1.predict(x_test_pt)\n# print(accuracy_score(y_test, clfRF_pred1)) # 0.7625156719246581\n# print(roc_auc_score(y_test, clfRF_pred1)) #0.5\n# print(cross_val_score(clfRF1, x_train_pt, y_train, cv=15))\n\n\n\n# ###################\n# # бустинг\n\n# Lgb = LGBMClassifier(n_estimators=150, silent=False, random_state =94, max_depth=5,num_leaves=31,objective='binary')\n# Lgb.fit(x_train, y_train)\n# Lgb_pred = Lgb.predict_proba(x_test)\n# Lgb_pred\n# # print(accuracy_score(y_test, [i[1] for i in Lgb_pred])) \n# print(roc_auc_score(y_test, [i[1] for i in Lgb_pred])) #0.7515020019901595\n# # print(roc_auc_score(y_test, Lgb_pred)) #0.7515020019901595\n\n\n# # cross_val_score\n# # print(cross_val_score(Lgb, x_train, y_train, cv=5))\n# # [0.73181719 0.7824965  0.77982855 0.78070329 0.78223408]\n\n\n###################\n# бустинг + PT\n\n# Lgb = LGBMClassifier(n_estimators=90, silent=False, random_state = 94, max_depth = 5, num_leaves = 31, objective = 'binary')\n# Lgb.fit(x_train_pt, y_train)\n# Lgb_pred = Lgb.predict_proba(x_test_pt)\n# Lgb_pred\n# # print(accuracy_score(y_test, [i[1] for i in Lgb_pred])) \n# print(roc_auc_score(y_test, [i[1] for i in Lgb_pred])) #0.7504605882482182\n\n# cross_val_score\n# df_pt = pd.concat(x_train_pt, x_test_pt, axis = 0) \n# df_target = pd.concat(y_train, y_test, axis = 0)\n# print(cross_val_score(Lgb, df_pt, df_target, cv=5))\n# [0.73181719 0.7824965  0.77982855 0.78070329 0.78223408]\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# применение модели к test"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# pred = Lgb.predict_proba(df_test[train_features])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submit"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import os\n# import glob\n# import datetime\n# import dateutil.tz\n\n# # собираем sub\n# # get the current time and add it to the submission filename, helps to keep track of submissions\n# current_time = datetime.datetime.now(dateutil.tz.tzlocal()).strftime('date_%Y-%m-%d__time_%H-%M')\n# flname_sub = 'submission__' + current_time +'.csv' # submission file name\n\n\n# # Lgb\n# sub = pd.DataFrame()\n# sub['ID'] = df_test['ID']\n# sub['PredictedProb'] = pred\n# sub.to_csv(flname_sub, index = False)\n\n# # # собираем sub\n# # # Lgb\n# # sub = pd.DataFrame()\n# # sub['ID'] = df_test['ID']\n# # sub['PredictedProb'] = pred\n# # sub.to_csv('clfRF_submit_baseline3.csv', index = False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # проверим есть ли дисбаланс и попробуем избавиться от дисбаланса данных и привести их размерности к одному уровню\n\n# # Shuffle the Dataset.\n# shuffled_df = df.sample(frac=1,random_state=4)\n\n# print('df : ', df['target'].value_counts())\n# print('shuffled_df : ', shuffled_df['target'].value_counts())\n# # Put all the fraud class in a separate dataset.\n# df_one = shuffled_df.loc[shuffled_df['target'] == 1].sample(n=27300, random_state=42)\n# print('df_one : ', df_one['target'].value_counts())\n\n# #Randomly select 492 observations from the non-fraud (majority class)\n# df_zero = shuffled_df.loc[shuffled_df['target'] == 0]\n# print('df_zero : ', df_zero['target'].value_counts())\n\n# # Concatenate both dataframes again\n# normalized_df = pd.concat([df_zero, df_one])\n\n# print('normalized_df : ', normalized_df['target'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    \n# # надо взять каждое значение в колонке v22 (это можно сделать через val counts)\n# # 1 посмотреть число таких значений --> получим столбец v22_numb_total\n# v22_numb_total = pd.DataFrame(x_train['v22'].value_counts().reset_index())\n# v22_numb_total.columns = ['v22', 'v22_numb_total_count']\n# # print(v22_numb_total)\n\n# # 1.1 записать v22_numb_total в x_train через merge\n# # вместо merge лучше использовать .join и .set_index, тогда можно сразу избавиться от дублей\n# # пример: df.set_index('key').join(other.set_index('key'))\n\n# x_train = x_train.set_index('v22').join(v22_numb_total.set_index('v22'))\n\n\n# # x_train[['v22','v22_numb_total_count']].fillna(0)\n# print(x_train)\n# # x_train.shape, x_test.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # что делает reset index\n# # v22 который был индексом останется в датасете, индекс сбросится и перейдет к новому значению\n# x_train = x_train.reset_index()\n\n# # что делает set index\n# # старый индекс сбросится и удалится и перейдет к столбцу ID. столбец изначльный ID удалится, v22 останется\n# x_train = x_train.set_index('ID',drop = False)\n\n# print(x_train.index,\n#       x_train.columns[x_train.columns == 'v22'],\n#       x_train.columns[x_train.columns == 'ID'],\n#       x_train.columns,\n#       x_train.shape)\n\n# # v22 должен остаться, он понадобится для следующего сопоставления","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# # 2 посмотреть число таких значений при y=1 --> получим столбец v22_numb_y1\n\n\n# x_train['target'] = y_train\n\n# v22_numb_y1 = pd.DataFrame(x_train['v22'][x_train['target'] == 1].value_counts().reset_index())\n\n\n# # делаем через .join - пример: df.set_index('key').join(other.set_index('key'))\n\n\n# v22_numb_y1.columns = ['v22', 'v22_numb_y1']\n# v22_numb_y1\n\n# # v22 должен остаться, он понадобится для следующего сопоставления (c test)\n# x_train = x_train.set_index('v22', drop = False).join(v22_numb_y1.set_index('v22', drop = True))\n\n# # колонку ID нужно обратно вернуть в индекс\n\n# x_train = x_train.set_index('ID', drop = True)\n\n# # проверяем, что колонки ID нет, колонка v22 есть\n\n# print(x_train.index,\n#       x_train.columns[x_train.columns == 'v22'],\n#       x_train.columns[x_train.columns == 'ID'],\n#       x_train.columns,\n#       x_train.shape)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # теперь надо поделить v22_numb_y1 на v22_numb_total_count, чтобы получить вероятность, получим prob_v22_numb_y1\n\n# x_train['prob_v22_numb_y1'] = x_train['v22_numb_y1'] / x_train['v22_numb_total_count']\n# # x_train['prob_v22_numb_y1'] = x_train['prob_v22_numb_y1'].fillna(0)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}