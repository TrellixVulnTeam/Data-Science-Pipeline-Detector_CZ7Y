{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import sys\n!cp ../input/rapids/rapids.0.14.0 /opt/conda/envs/rapids.tar.gz\n!cd /opt/conda/envs/ && tar -xzvf rapids.tar.gz > /dev/null\nsys.path = [\"/opt/conda/envs/rapids/lib/python3.7/site-packages\"] + sys.path\nsys.path = [\"/opt/conda/envs/rapids/lib/python3.7\"] + sys.path\nsys.path = [\"/opt/conda/envs/rapids/lib\"] + sys.path \n!cp /opt/conda/envs/rapids/lib/libxgboost.so /opt/conda/lib/","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# xfeat: Flexible Feature Engineering & Exploration Library using GPUs and Optuna.\n\nxfeat provides sklearn-like transformation classes for feature engineering and exploration. Unlike sklearn API, xfeat provides a dataframe-in, dataframe-out interface. xfeat supports both pandas and cuDF dataframes. By using cuDF and CuPy, xfeat can generate features 10 ~ 30 times faster than a naive pandas operation.\n\nhttps://github.com/pfnet-research/xfeat","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"!pip install -q https://github.com/pfnet-research/xfeat/archive/master.zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import KFold\nimport xfeat\nimport cudf\n\nimport catboost as cat\nimport lightgbm as lgb\n\n\nxfeat.utils.cudf_is_available()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH_TRAIN_CSV = \"../input/bnp-paribas-cardif-claims-management/train.csv.zip\"\nPATH_TEST_CSV = \"../input/bnp-paribas-cardif-claims-management/test.csv.zip\"\nPATH_SUB_CSV = \"../input/bnp-paribas-cardif-claims-management/sample_submission.csv.zip\"\n\nUSECOLS = [\n    \"v10\", \"v12\", \"v14\", \"v21\", \"v22\", \"v24\", \"v30\", \"v31\", \"v34\", \"v38\",\n    \"v40\", \"v47\", \"v50\", \"v52\", \"v56\", \"v62\", \"v66\", \"v72\", \"v75\", \"v79\",\n    \"v91\", \"v112\", \"v113\", \"v114\", \"v129\", \"target\"\n]\n\n\ndef preload():\n    # Convert dataset into feather format.\n    xfeat.utils.compress_df(pd.concat([\n        pd.read_csv(PATH_TRAIN_CSV),\n        pd.read_csv(PATH_TEST_CSV),\n    ], sort=False)).reset_index(drop=True)[USECOLS].to_feather(\n        \"../working/train_test.ftr\")\n\n\npreload()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.read_feather(\"../working/train_test.ftr\").head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"(1) Save numerical features\")\nxfeat.SelectNumerical().fit_transform(\n    pd.read_feather(\"../working/train_test.ftr\")\n).reset_index(drop=True).to_feather(\"../working/feature_num_features.ftr\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.read_feather(\"../working/feature_num_features.ftr\").head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"(2) Categorical encoding using label encoding: 13 features\")\nxfeat.Pipeline([\n    xfeat.SelectCategorical(), xfeat.LabelEncoder(output_suffix=\"\")]\n).fit_transform(\n    pd.read_feather(\"../working/train_test.ftr\")\n).reset_index(drop=True).to_feather(\"../working/feature_1way_label_encoding.ftr\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.read_feather(\"../working/feature_1way_label_encoding.ftr\").head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def target_encoding(path_source_dataframe, path_output_dataframe):\n    df_train_test = pd.read_feather(path_source_dataframe)\n    df_train_test.loc[:, \"target\"] = pd.read_feather(\"../working/train_test.ftr\")[\"target\"]\n    df_train = df_train_test.dropna(subset=[\"target\"]).copy()\n    df_test = df_train_test.loc[df_train_test[\"target\"].isnull()].copy()\n\n    fold = KFold(n_splits=5, shuffle=True, random_state=111)\n    encoder = xfeat.TargetEncoder(fold=fold, target_col=\"target\", output_suffix=\"\")\n    df_train = encoder.fit_transform(cudf.from_pandas(df_train))\n    df_test = encoder.transform(cudf.from_pandas(df_test))\n\n    pd.concat([df_train.to_pandas(), df_test.to_pandas()], sort=False).drop(\n        \"target\", axis=1).reset_index(drop=True).to_feather(path_output_dataframe)\n\n\nprint(\"(2') Target encoding of categorical variables\")\ntarget_encoding(\"../working/feature_1way_label_encoding.ftr\",\n                \"../working/feature_1way_label_encoding_with_te.ftr\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.read_feather(\"../working/feature_1way_label_encoding_with_te.ftr\").head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"(3) 2-order combination of categorical features: 78 features (13 * 12 / 2 = 78)\")\nxfeat.Pipeline([\n    xfeat.SelectCategorical(),\n    xfeat.ConcatCombination(drop_origin=True, r=2),\n    xfeat.LabelEncoder(output_suffix=\"\"),\n]).fit_transform(pd.read_feather(\"../working/train_test.ftr\")).reset_index(\n    drop=True\n).to_feather(\n    \"../working/feature_2way_label_encoding.ftr\"\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.read_feather(\"../working/feature_2way_label_encoding.ftr\").head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"(4) 3-order combination of categorical features\")\n# Use `include_cols=` kwargs to reduce the total count of combinations.\n# 66 features (12 * 11 / 2 = 66)\nxfeat.Pipeline([\n    xfeat.SelectCategorical(),\n    xfeat.ConcatCombination(drop_origin=True, include_cols=[\"v22\"], r=3),\n    xfeat.LabelEncoder(output_suffix=\"\"),\n]).fit_transform(pd.read_feather(\"../working/train_test.ftr\")).reset_index(\n    drop=True\n).to_feather(\n    \"../working/feature_3way_including_v22_label_encoding.ftr\"\n)\n\n\nprint(\"(5) Convert numerical to categorical using round: 12 features\")\ndf_rnum = (\n    xfeat.Pipeline(\n        [\n            xfeat.SelectNumerical(),\n            xfeat.LambdaEncoder(\n                lambda x: str(x)[:-2],\n                output_suffix=\"_rnum\",\n                exclude_cols=[\"target\"],\n            ),\n        ]\n    )\n    .fit_transform(pd.read_feather(\"../working/train_test.ftr\"))\n    .reset_index(drop=True)\n)\ndf_rnum.to_feather(\"../working/feature_round_num.ftr\")\nrnum_cols = [col for col in df_rnum.columns if col.endswith(\"_rnum\")]\nxfeat.Pipeline([xfeat.LabelEncoder(output_suffix=\"\")]).fit_transform(\n    pd.read_feather(\"../working/feature_round_num.ftr\")[rnum_cols]\n).reset_index(drop=True).to_feather(\"../working/feature_round_num_label_encoding.ftr\")\n\n\nprint(\"(6) 2-order Arithmetic combinations.\")\nxfeat.Pipeline(\n    [\n        xfeat.SelectNumerical(),\n        xfeat.ArithmeticCombinations(\n            exclude_cols=[\"target\"], drop_origin=True, operator=\"+\", r=2,\n        ),\n    ]\n).fit_transform(pd.read_feather(\"../working/train_test.ftr\")).reset_index(\n    drop=True\n).to_feather(\n    \"../working/feature_arithmetic_combi2.ftr\"\n)\n\n\nprint(\"(7) Add more combinations: 11-order concat combinations.\")\nxfeat.Pipeline(\n    [\n        xfeat.SelectCategorical(),\n        xfeat.ConcatCombination(drop_origin=True, include_cols=[\"v22\"], r=11),\n        xfeat.LabelEncoder(output_suffix=\"\"),\n    ]\n).fit_transform(pd.read_feather(\"../working/train_test.ftr\")).reset_index(\n    drop=True\n).to_feather(\n    \"../working/feature_11way_including_v22_label_encoding.ftr\"\n)\n\n\nprint(\"(3') Target encoding of categorical variables\")\ntarget_encoding(\"../working/feature_2way_label_encoding.ftr\",\n                \"../working/feature_2way_label_encoding_with_te.ftr\")\n\nprint(\"(4') Target encoding of categorical variables\")\ntarget_encoding(\"../working/feature_3way_including_v22_label_encoding.ftr\",\n                \"../working/feature_3way_including_v22_label_encoding_with_te.ftr\")\n\nprint(\"(5') Target encoding of categorical variables\")\ntarget_encoding(\"../working/feature_round_num_label_encoding.ftr\",\n                \"../working/feature_round_num_label_encoding_with_te.ftr\")\n\nprint(\"(7') Target encoding of categorical variables\")\ntarget_encoding(\"../working/feature_11way_including_v22_label_encoding.ftr\",\n                \"../working/feature_11way_including_v22_label_encoding_with_te.ftr\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls -lha ../working/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import catboost as cat\n\n\ndef catboost_model():\n    print(\"Load numerical features\")\n    df_num = pd.concat(\n        [\n            pd.read_feather(\"../working/feature_num_features.ftr\"),\n            pd.read_feather(\"../working/feature_arithmetic_combi2.ftr\"),\n        ],\n        axis=1,\n    )\n    y_train = df_num[\"target\"].dropna()\n    df_num.drop([\"target\"], axis=1, inplace=True)\n\n    print(\"Load categorical features\")\n    df = pd.concat(\n        [\n            pd.read_feather(\"../working/feature_1way_label_encoding.ftr\"),\n            pd.read_feather(\"../working/feature_2way_label_encoding.ftr\"),\n            pd.read_feather(\"../working/feature_3way_including_v22_label_encoding.ftr\"),\n            pd.read_feather(\"../working/feature_round_num_label_encoding.ftr\"),\n            pd.read_feather(\"../working/feature_11way_including_v22_label_encoding.ftr\"),\n        ],\n        axis=1,\n    )\n    cat_cols = df.columns.tolist()\n    df = pd.concat([df, df_num], axis=1)\n\n    print(\"Fit\")\n    params = {\n        \"loss_function\": \"Logloss\",\n        \"eval_metric\": \"Logloss\",\n        \"learning_rate\": 0.03,\n        \"iterations\": 3000,\n        \"l2_leaf_reg\": 3,\n        \"random_seed\": 432013,\n        \"subsample\": 0.66,\n        \"od_type\": \"Iter\",\n        \"rsm\": 0.2,\n        \"depth\": 6,\n        \"border_count\": 128,\n    }\n    model = cat.CatBoostClassifier(**params)\n    train_data = cat.Pool(\n        df.iloc[: y_train.shape[0]], label=y_train, cat_features=cat_cols\n    )\n    fit_model = model.fit(train_data, verbose=30)\n\n    # Predict\n    y_pred = fit_model.predict_proba(df.iloc[y_train.shape[0] :])\n    submission = pd.read_csv(\"../input/bnp-paribas-cardif-claims-management/sample_submission.csv.zip\")\n    submission.loc[:, \"PredictedProb\"] = y_pred[:, 1]\n    submission.to_csv(\"../working/solution_cat.csv\", index=False)\n\n\nLIGHTGBM_BASE_PARAMS = {\n    \"objective\": \"binary\",\n    \"metric\": \"binary_logloss\",\n    \"verbosity\": 1,\n    \"boosting_type\": \"gbdt\",\n    \"num_leaves\": 32,\n    \"feature_fraction\": 0.8,\n    \"bagging_fraction\": 1.0,\n    \"bagging_freq\": 0,\n    \"learning_rate\": 0.03,\n    \"max_bin\": 255,\n    \"seed\": 1,\n    \"min_data_in_leaf\": 20,\n}\nLIGHTGBM_BASE_FIT_PARAMS = {\n    \"verbose_eval\": 30,\n    \"num_boost_round\": 3000,\n}\n\n\ndef lightgbm_model():\n    print(\"Load numerical features\")\n    df_num = pd.concat(\n        [\n            pd.read_feather(\"../working/feature_num_features.ftr\"),\n            pd.read_feather(\"../working/feature_arithmetic_combi2.ftr\"),\n        ],\n        axis=1,\n    )\n    y_train = df_num[\"target\"].dropna()\n    df_num.drop([\"target\"], axis=1, inplace=True)\n\n    print(\"Load categorical features\")\n    df = pd.concat(\n        [\n            pd.read_feather(\"../working/feature_1way_label_encoding_with_te.ftr\"),\n            pd.read_feather(\"../working/feature_2way_label_encoding_with_te.ftr\"),\n            pd.read_feather(\"../working/feature_3way_including_v22_label_encoding_with_te.ftr\"),\n            pd.read_feather(\"../working/feature_round_num_label_encoding_with_te.ftr\"),\n            pd.read_feather(\"../working/feature_11way_including_v22_label_encoding_with_te.ftr\"),\n        ],\n        axis=1,\n    )\n    cat_cols = df.columns.tolist()\n    df = pd.concat([df, df_num], axis=1)\n\n    X_train, X_test = df.values[:y_train.shape[0]], df.values[y_train.shape[0]:]\n    dtrain = lgb.Dataset(X_train, y_train)\n    \n    fit_params = LIGHTGBM_BASE_FIT_PARAMS.copy()\n    fit_params[\"valid_sets\"] = [dtrain]\n\n    bst = lgb.train(LIGHTGBM_BASE_PARAMS, dtrain, **fit_params)\n    y_pred = bst.predict(X_test)\n\n    # Predict\n    submission = pd.read_csv(\"../input/bnp-paribas-cardif-claims-management/sample_submission.csv.zip\")\n    submission.loc[:, \"PredictedProb\"] = y_pred\n    submission.to_csv(\"../working/solution_lgb.csv\", index=False)\n\n\nlightgbm_model()   \ncatboost_model()\n\n\n# Linear blending\ny1 = pd.read_csv(\"../working/solution_cat.csv\")\ny2 = pd.read_csv(\"../working/solution_lgb.csv\")\n\ndf_sub = pd.read_csv(PATH_SUB_CSV)\ndf_sub.loc[:, \"PredictedProb\"] = 0.6 * y1.PredictedProb.values + 0.4 * y2.PredictedProb.values\ndf_sub[[\"ID\", \"PredictedProb\"]].to_csv(\"../working/solution_blend.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}