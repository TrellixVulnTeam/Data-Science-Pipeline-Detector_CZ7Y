{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Background:**\n\nAt the beginning, I attempted to perform a test on the following R solution (link provided) with target encoding, however the actual result was not satisfactory at around 0.46, either using the transformed train set on CatBoost or XgBoost. \n\nAs suggested in many articles, target encoding methods easily result in overfitting without properly handling the preprocessing method. In addition, the main issue in this competition is that a substantial amount of unique value in the column [V22] of test data cannot be found in same column of train data, it causes the transformation to fill in the mean value for most of the empty value in V22 & other concatenated columns in test data.\n\nAt the same time, I am able to obtain satisfied score 0.433 without using target encoding & arithmetical combination by using CatBoost model only, especially CatB show its robustness when dealing high cardinality data set (most of the concatenated columns have more than 10k unique value).\n\nI learned a lot during the testing process and kindly upvote if you find it useful :)\n\n\n**Reference:**\n\nFeature Engineering [link](https://www.kaggle.com/code/rsakata/xgboost-with-combination-of-factors/comments)<br>\nModel [link](https://www.kaggle.com/code/confirm/xfeat-catboost-cpu-only)<br>\nWinning Solution (v22 concept) [link](https://www.kaggle.com/competitions/bnp-paribas-cardif-claims-management/discussion/20247)","metadata":{}},{"cell_type":"markdown","source":"# Import Data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/bnp-paribas-cardif-claims-management/train.csv.zip\")\ntest = pd.read_csv(\"../input/bnp-paribas-cardif-claims-management/test.csv.zip\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape)\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y= train['target']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test.shape)\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Exploration","metadata":{}},{"cell_type":"code","source":"pd.set_option('display.max_rows', 200)\n\nmissing_check = pd.DataFrame(train.isnull().sum() / train.shape[0],columns=['missing'])\nmissing_check.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import operator\n\n#Display correlation for numeric columns and only take top 55 columns for feature extraction\nnumer = train.select_dtypes(include=['number']).columns\n\ncorr_num = {}\nfor col in numer:\n    corr = train['target'].corr(train[col])\n    if not np.isnan(corr):\n#     if corr >= 0.03:\n        corr_num[col] = abs(corr)\n\nsort_num = sorted(corr_num.items(), key=operator.itemgetter(1),reverse=True)[:55]\nsort_num[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Display correlation for categorical columns and use all categorical columns for 1 & 2 way combination plus selected columns\n#in 11 ways combination\ncat = train.select_dtypes(exclude=['number']).columns\n\ncorr_num = {}\nfor col in cat:\n    corr = train['target'].corr(train[col].astype('category').cat.codes)\n    if not np.isnan(corr):\n#     if corr >= 0.03:\n        corr_num[col] = abs(corr)\n\nsort_cat = sorted(corr_num.items(), key=operator.itemgetter(1),reverse=True)\nsort_cat","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"use_col = []\n\nfor i in range(0,len(sort_num)):\n    if sort_num[i][0] != 'target':\n         use_col.append(sort_num[i][0])\n\nfor i in range(0,len(sort_cat)):\n    use_col.append(sort_cat[i][0])\n\nlen(use_col)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train[use_col]\nprint(train.shape)\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test[use_col]\nprint(test.shape)\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_col = train.select_dtypes(exclude=['number']).columns.values\nnum_col = train.select_dtypes(include=['number']).columns.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in num_col:\n    mean = train[col].mean(skipna=True)\n    train[col] = train[col].fillna(mean)\n    test[col] = test[col].fillna(mean)\n    \nfor col in cat_col:\n    mode = train[col].mode(dropna=True)[0]\n    train[col] = train[col].fillna('NA')\n    test[col] = test[col].fillna('NA')\n#     train[col] = train[col].fillna(mode)\n#     test[col] = test[col].fillna(mode)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.isnull().sum().sum())\nprint(test.isnull().sum().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from itertools import combinations\n\ncc = list(combinations(cat_col,2))\ncolumn_names = [c[1]+c[0]  for c in cc]\ndf_comb2 = pd.concat([train[c[1]] + train[c[0]] for c in cc], axis=1,keys=column_names)\ndf_comb2.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df_comb2 = pd.concat([test[c[1]] + test[c[0]] for c in cc], axis=1,keys=column_names)\ntest_df_comb2.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#To ensure not repeating combination like v22v22v33 by creating new list\ncat_col_ex_v22 = np.delete(cat_col, np.where(cat_col == 'v22'))\ncat_col_ex_v22","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from itertools import product\n\n# First generate combination part without v22, then generate a new pair combine with v22 \ncc_ex_v22 = list(combinations(cat_col_ex_v22,2))\ncc_v22_1 = list(product(['v22'],cc_ex_v22))\ncolumn_names = [c[0]+c[1][0]+c[1][1] for c in cc_v22_1]\ndf_comb_v22 = pd.concat([train[c[0]] + train[c[1][0]] + train[c[1][1]] for c in cc_v22_1], axis=1,keys=column_names)\ndf_comb_v22.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df_comb_v22 = pd.concat([test[c[0]] + test[c[1][0]] + test[c[1][1]] for c in cc_v22_1], axis=1,keys=column_names)\ntest_df_comb_v22.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.concat([train,df_comb2,df_comb_v22],axis=1)\ntrain.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.concat([test,test_df_comb2,test_df_comb_v22],axis=1)\ntest.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\n\ndel df_comb2,df_comb_v22, test_df_comb2, test_df_comb_v22\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Subset the categorical list with selected columns to reduct total combination\ndel_list = ['v110','v74','v3','v107','v71','v125','v22']\n\ncat_col = np.delete(cat_col,np.isin(cat_col,del_list))\ncat_col","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cc_ex_v22 = list(combinations(cat_col,10))\ncc_v22_11 = list(product(['v22'],cc_ex_v22))\nlen(cc_v22_11)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"column_names = [col[0]+col[1][0]+col[1][1]+col[1][2]+col[1][3]+col[1][4]+col[1][5]+col[1][6]+col[1][7]+col[1][8]+col[1][9] for col in cc_v22_11]\n\ntrain_11 = pd.concat([train[col[0]]+train[col[1][0]]+train[col[1][1]]+train[col[1][2]]+train[col[1][3]]+train[col[1][4]]+\\\n    train[col[1][5]]+train[col[1][6]]+train[col[1][7]]+train[col[1][8]]+train[col[1][9]] for col in cc_v22_11], axis=1,keys=column_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_11.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.concat([train,train_11],axis=1)\ntrain.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_11 = pd.concat([test[col[0]]+test[col[1][0]]+test[col[1][1]]+test[col[1][2]]+test[col[1][3]]+test[col[1][4]]+\\\n    test[col[1][5]]+test[col[1][6]]+test[col[1][7]]+test[col[1][8]]+test[col[1][9]] for col in cc_v22_11], axis=1,keys=column_names)\n\ntest = pd.concat([test,test_11],axis=1)\ntest.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del test_11,train_11\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_col = train.select_dtypes(exclude=['number']).columns.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Original the train data is transformed with target encoding then used in Catboost fitting, however the score is always around 0.46\n#Fit_transform is required on the train set directly rather than fit>transform, else the model will be extreme overfitting\n\n# from category_encoders import leave_one_out\n\n# te = leave_one_out.LeaveOneOutEncoder(verbose=0,cols=cat_col,random_state=42,sigma=0.05)\n# train = te.fit_transform(train,y)\n# test = te.transform(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape)\nprint(train.isnull().sum().sum())\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test.shape)\nprint(test.isnull().sum().sum())\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"markdown","source":"## CatBoost","metadata":{}},{"cell_type":"code","source":"import catboost as cat\n\n#2400 best score with validation set\nparams = {\n    \"loss_function\": \"Logloss\",\n    \"eval_metric\": \"Logloss\",\n    \"learning_rate\": 0.03,\n    \"iterations\": 2400,\n    \"l2_leaf_reg\": 3,\n    \"random_seed\": 432013,\n    \"subsample\": 0.66,\n    \"od_type\": \"Iter\",\n    \"rsm\": 0.2,\n    \"depth\": 6,\n    \"border_count\": 128\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.model_selection import train_test_split\n\n# X_train1, X_val, y_train1, y_val = train_test_split(train, y, test_size=0.2, random_state=42,stratify=y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = cat.CatBoostClassifier(**params)\n# train_data = cat.Pool(X_train1, label=y_train1,cat_features=cat_col)\n# val_data = cat.Pool(X_val, label=y_val,cat_features=cat_col)\n\n# fit_model = model.fit(train_data, verbose=30,eval_set=val_data,early_stopping_rounds=100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#For this model, numeric columns remained as numeric features, convert numeric value to string then pass to cat_feature should provide better score\n\nmodel = cat.CatBoostClassifier(**params)\ntrain_data = cat.Pool(train, label=y,cat_features=cat_col)\ntest_data = cat.Pool(test,cat_features=cat_col)\n\nfit_model = model.fit(train_data, verbose=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_rows', 500)\n\npd.DataFrame({'feature_importance': model.get_feature_importance(train_data), \n              'feature_names': train.columns}).sort_values(by=['feature_importance'], \n                                                           ascending=False).head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = fit_model.predict_proba(test_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBoost\n\nXgboost plus target encoding using same the features can only return score 0.46 above, therefore this model is not adopted","metadata":{}},{"cell_type":"code","source":"# import xgboost as xgb\n\n# nrounds  = 500\n# params = {\n#     \"eta\": 0.05,\n#     \"max_depth\": 6,\n#     \"colsample_bylevel\": 0.3,\n#     \"objective\": 'binary:logistic',\n#     \"eval_metric\": 'logloss'}\n\n# dtrain = xgb.DMatrix(train, y)\n# dtest = xgb.DMatrix(test)\n# watchlist = [(dtrain, 'train')]\n# model = xgb.train(params=params, dtrain=dtrain, num_boost_round=nrounds,evals=watchlist,verbose_eval=True )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y_pred = model.predict(dtest)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(y_pred[:,1]).describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"submission = pd.read_csv(\"../input/bnp-paribas-cardif-claims-management/sample_submission.csv.zip\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission['PredictedProb'] = y_pred[:,1]\nsubmission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}