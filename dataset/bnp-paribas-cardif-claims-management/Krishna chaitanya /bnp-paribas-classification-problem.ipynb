{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd \n#  reading train data\ndf=pd.read_csv('/kaggle/input/bnp-paribas-cardif-claims-management/train.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\n# counting Null values\n# plt.yticks\nh=df.isna().sum().sort_values()\nh=h.to_frame()\nh.columns\nh.plot(kind='barh',figsize=(150,150))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby('target')['target'].count().plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# identifying columns with categorical data in them\nn_columns=df.columns.where(df.dtypes=='object')\nn_columns=list(n_columns)\nn_columns= [x for x in n_columns if str(x)!='nan']\n\n\n# dropping columns with categorical data\ndf=df.drop(n_columns, axis=1)\n\n#dropping null values in the rest of the dataset.\ndf=df.dropna()     \ndf=df.reset_index()\n# standardizing the data\nn_columns=list(df.columns)\ndel n_columns[0:2]\nm=df[n_columns].mean()\ns=df[n_columns].std()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\n%matplotlib inline\n\n# plt.rcParams['figure.max_open_warning']=300\n\nfor col in df.columns:\n    print(col, \"\\n\", )\n    try:\n        df[col].hist()\n        plt.show()\n    except:\n        df[col].value_counts().plot(kind=\"bar\")\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfeatures = n_columns\n# Separating out the features\nx = df.loc[:, features].values\n# Separating out the target\ny = df.loc[:,['target']].values\n# Standardizing the features\nx = StandardScaler().fit_transform(x)\n\n# showing the coveriance matrix\ndf[n_columns].cov()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PCA\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=35)\nprincipalComponents = pca.fit_transform(x)\nprincipalDf = pd.DataFrame(data = principalComponents , columns = ['principal component 1', 'principal component 2', 'principal component 3','principal component 4', 'principal component 5', 'principal component 6','principal component 7', 'principal component 8', 'principal component 9','principal component 10','principal component 11', 'principal component 12', 'principal component 13','principal component 14', 'principal component 15','principal component 16', 'principal component 17', 'principal component 18','principal component 19', 'principal component 20','principal component 21', 'principal component 22', 'principal component 23','principal component 24', 'principal component 25','principal component 26', 'principal component 27', 'principal component 28','principal component 29', 'principal component 30','principal component 31', 'principal component 32', 'principal component 33','principal component 34', 'principal component 35',])\nfinalDf = pd.concat([principalDf, df[['target']]], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pca.explained_variance_ratio_.sum())   #shows the over variance expressed by the first 35 principal components\nprint(pca.explained_variance_ratio_*100)   #shows variance expressed by each of the first 35 principal components","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\na1=pca.explained_variance_ratio_*100\nb1=a1\nfor i in range(len(a1)):\n    if i>0:\n        b1[i]=b1[i]+b1[i-1]\n    else:\n        continue\n\n\n\nplt.plot(b1)\nplt.xlabel('Number of PCs')\nplt.ylabel('Variance explained')\nplt.title('Variance explained by Principal Component Analysis')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Implementing Random Forest\n\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import MinMaxScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = finalDf.iloc[:,0:35].values\ny= finalDf.iloc[:,35:].values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nregressor = RandomForestClassifier(n_estimators=200, random_state=0)\nregressor.fit(X_train, y_train)\ny_pred = regressor.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# y_pred1=y_pred\ny_pred1= np.rint(y_pred)\ny_pred1=y_pred1.astype(int)\n# print(type(y_pred1[1]))\n\nfrom sklearn import metrics\nimport numpy as np\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred1))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred1))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nprint('Confusion Matrix\\n',confusion_matrix(y_test,y_pred1),end='\\n\\n')\nprint(classification_report(y_test,y_pred1),end='\\n\\n\\n')\nprint('Over all test accuracy with Random Forests {}%'.format(accuracy_score(y_test, y_pred1)*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# navie bayes classification\n\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ny_pred = gnb.fit(X_train, y_train).predict(X_test)\n\nprint('Confusion Matrix\\n',confusion_matrix(y_test,y_pred),end='\\n\\n')\nprint(classification_report(y_test,y_pred),end='\\n\\n\\n')\nprint('Over all test accuracy with Gaussian Navie Bayes {}%'.format(accuracy_score(y_test, y_pred)*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvclassifier = SVC(kernel='rbf')\nsvclassifier.fit(X_train, y_train)\ny_pred = svclassifier.predict(X_test)\n\n\nprint('Confusion Matrix\\n',confusion_matrix(y_test,y_pred),end='\\n\\n')\nprint(classification_report(y_test,y_pred),end='\\n\\n\\n')\nprint('Over all test accuracy with Support Vector Machine {}%'.format(accuracy_score(y_test, y_pred)*100))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\nsample_submission = pd.read_csv(\"../input/bnp-paribas-cardif-claims-management/sample_submission.csv\")\ntest = pd.read_csv(\"../input/bnp-paribas-cardif-claims-management/test.csv\")\ntrain = pd.read_csv(\"../input/bnp-paribas-cardif-claims-management/train.csv\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}