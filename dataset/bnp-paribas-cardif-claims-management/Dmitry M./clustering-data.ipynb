{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Clustering data based on filling of column\n\nimport pandas as pd\n\n# Load train data\ndf_train = pd.read_csv('../input/train.csv')\n# Drop ID and target, they are not needed for the analys\ndf_train = df_train.drop(['ID', 'target'], axis=1)\n\n# Similar for test\ndf_test = pd.read_csv('../input/test.csv')\ndf_test = df_test.drop(['ID'], axis=1)\n\n# Concat both datasets\nall_data = df_train.append(df_test, ignore_index=True)\n\nall_data.info()"},{"cell_type":"markdown","metadata":{},"source":"Now we will find all columns with about 100% filling In the future you will see that we can split our set with 80% border"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"all_l = len(all_data)  # Full length\n\nfilled_lst = []   # 100% filled\nempty_lst = []    # is not empty, but not filled\n\n# Separate draw categorical (str) and float+int\nx_int = []\ny_int = []\n\nx_str = []\ny_str = []\n\nfor i,(name,series) in enumerate(all_data.iteritems()):\n    c = series.count()  # series.count() return count of filled rows\n    fill = c/all_l*100  \n    #print('%s: %.2f  (type: %s)' % (name, fill, series.dtype))\n    if series.dtype == 'O':\n        x_str.append(i)\n        y_str.append(fill)\n    else:\n        x_int.append(i)\n        y_int.append(fill)\n    #\n    if fill>80:\n        filled_lst.append(name)\n    else:\n        empty_lst.append(name)\n    #        "},{"cell_type":"markdown","metadata":{},"source":"100% filled list:"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"filled_lst"},{"cell_type":"markdown","metadata":{},"source":"not filled list (empty_lst) include all other columns"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"len(empty_lst)"},{"cell_type":"markdown","metadata":{},"source":"Now plot:"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"from matplotlib import pyplot as plt\nplt.subplots(figsize=(10, 10))\nplt.plot(x_int,y_int,'o',color = 'red', markersize = 10, alpha = 0.3)\nplt.plot(x_str,y_str,'o',color = 'green', markersize = 10, alpha = 0.3)\nplt.axhline(80,color='r') # our 80% treshold\n\nplt.show()"},{"cell_type":"markdown","metadata":{},"source":"Now we want to build affinity matrix. To do this we need to calculate the distance between each of the two dataseries\n\nCalculate \"distance\" example:"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"tmp_row1 = list(range(10))\ntmp_row2 = list(range(10,20))\ntmp_row2[2:4] = [None,None]\ntmp_row3 = list(range(20,30))\ntmp_row3[2:6] = [None,None,None,None]\nexample_df = pd.DataFrame(data={'v1':tmp_row1, 'v2':tmp_row2, 'v3':tmp_row3})\nexample_df"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"import numpy as np\ndef dist(series1, series2, length):\n    #Calculate correlation between data series\n    c = series1.isnull().values == series2.isnull().values\n    return np.sum(c.astype(int))/length"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"dist(example_df['v1'],example_df['v2'],10)"},{"cell_type":"markdown","metadata":{},"source":"Length of series = 10, and v1 have 8 elemnts filled simultaneously with v2"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"dist(example_df['v1'],example_df['v3'],10)"},{"cell_type":"markdown","metadata":{},"source":"v1 and v3: inly 6 elements filled simultaneously"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"dist(example_df['v1'],example_df['v2'],10)"},{"cell_type":"markdown","metadata":{},"source":"Again 8 elements (NaN - NaN also considered)"},{"cell_type":"markdown","metadata":{},"source":"Go ahead!\n\nLet's build affinity matrix: it will be 102x102 matrix"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# First of all: we need to drop filled data\n# all data low filled\nall_data_lf = all_data.drop(filled_lst, axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Now build matrix\na = np.eye(len(empty_lst))  # This matrix already have 1 in main diagonal\nlength = len(all_data_lf)\n#\nfor i, (name1, series1) in enumerate(all_data_lf.iteritems()):\n    for j, (name2, series2) in enumerate(all_data_lf.iteritems()):\n        if j == i:  # Only under main diag\n            break\n        else:\n            tmp_d = dist(series1, series2, length)\n            a[i,j] = tmp_d\n            a[j,i] = tmp_d\n#           \na.shape"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"size = len(empty_lst)\nfig, ax = plt.subplots(figsize=(15, 15))\nax.matshow(a)\nlocs, labels = plt.xticks(range(size), empty_lst)\nplt.setp(labels, rotation=90)\nplt.yticks(range(size), empty_lst)\nplt.show()"},{"cell_type":"markdown","metadata":{},"source":"How you can see, there is highly correlated filling. Only v30 and v113 have some differences"},{"cell_type":"markdown","metadata":{},"source":"Now, try to cluster our data\n\nTo do this we will use sklearn.cluster.SpectralClustering with setting: affinity='precomputed' \n\nAnd to choose the best partition (n_clusters) we will use sklearn.metrics.silhouette_score (and again metric='precomputed')"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"from sklearn.cluster import SpectralClustering\nfrom sklearn.metrics import silhouette_score\n\nx_score = []\ny_score = []\n\n# Get scores for n_clusters from 2 to 10:\nfor i in range(2,10):\n    tmp_clf = SpectralClustering(n_clusters=i, affinity='precomputed')\n    tmp_clf.fit(a)\n    score = silhouette_score(a, tmp_clf.labels_, metric='precomputed')\n    x_score.append(i)\n    y_score.append(score)\n\n# Draw\nplt.subplots(figsize=(10, 10))\nplt.plot(x_score,y_score)\nplt.grid()\nplt.show()"},{"cell_type":"markdown","metadata":{},"source":"We use the Elbrow method to determining the number of clusters: 3 or 4\n\nLet's print this clusters"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# for 3 clusters:\nclusters_count = 3\nclusters = [[] for i in range(clusters_count)]\nclf = SpectralClustering(n_clusters=clusters_count, affinity='precomputed', random_state=42)\nclf.fit(a)\n\nfor name,cluster_n in zip(empty_lst, clf.labels_):\n    clusters[cluster_n].append(name)\n    \nfor tmp_cluster in clusters:\n    print('---')\n    print(tmp_cluster)"},{"cell_type":"markdown","metadata":{},"source":"If you change clusters_count to 4 you will see that v30 and v113 will be 3 and 4 clusters respectively. So I think that n_clusters=3 - best partition for this dataset\n\nWhat does this partition? If the object is filled with at least one column of the cluster, the whole group will have a high level of filling.\n\nCheck this in the first cluster:"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# This function return mask for DataFrame, elements wherein at least one filled column from list\ndef get_mask_notnull(df,columns_list):\n    i = iter(columns_list)\n    #Take first column in list\n    first_v = next(i)\n    #Get notnull mask\n    current_mask = df[first_v].notnull()\n    for tmp_v in i:\n        current_mask = current_mask | df[tmp_v].notnull() #logical \"or\"\n    #\n    return current_mask\n\n# Get elements from first cluster\ndf_first_cluster = all_data_lf[get_mask_notnull(all_data_lf, clusters[0])]\nprint('objects count from cluster 1: %d' % len(df_first_cluster))\n\n# And draw filling percentage as in the beginning of script\n\nx_int = []\ny_int = []\n\nx_str = []\ny_str = []\n\nall_l=len(df_first_cluster)\nfor i,(name,series) in enumerate(df_first_cluster.iteritems()):\n    c = series.count()\n    fill = c/all_l*100  \n    if series.dtype == 'O':\n        x_str.append(i)\n        y_str.append(fill)\n    else:\n        x_int.append(i)\n        y_int.append(fill)\n    \nplt.subplots(figsize=(10, 10))  \nplt.plot(x_int,y_int,'o',color = 'red', markersize = 10, alpha = 0.3)\nplt.plot(x_str,y_str,'o',color = 'green', markersize = 10, alpha = 0.3)\n\nplt.show()"},{"cell_type":"markdown","metadata":{},"source":"You can see, that columns from cluster 1 almost completely filled\n\n\nActually, even if we combine cluster1+cluster2 we obtain good filling:"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Get elements from combine (1+2) cluster\ndf_combine = all_data_lf[get_mask_notnull(all_data_lf, clusters[0]+clusters[1])]\nprint('objects count from clusters 1 and 2: %d' % len(df_combine))\n\nx_int = []\ny_int = []\n\nx_str = []\ny_str = []\n\nall_l=len(df_combine)\nfor i,(name,series) in enumerate(df_combine.iteritems()):\n    c = series.count()\n    fill = c/all_l*100  \n    if series.dtype == 'O':\n        x_str.append(i)\n        y_str.append(fill)\n    else:\n        x_int.append(i)\n        y_int.append(fill)\n\nplt.subplots(figsize=(10, 10)) \nplt.plot(x_int,y_int,'o',color = 'red', markersize = 10, alpha = 0.3)\nplt.plot(x_str,y_str,'o',color = 'green', markersize = 10, alpha = 0.3)\n\nplt.show()"},{"cell_type":"markdown","metadata":{},"source":"but if you need a more accurate separation is better to use these clusters separately\n\n"},{"cell_type":"markdown","metadata":{},"source":"About 3 cluster ['v30', 'v113']: this cluster have very low filling"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Get elements from 2 cluster\ndf_third = all_data_lf[get_mask_notnull(all_data_lf, clusters[2])]\nprint('objects count from cluster 3: %d' % len(df_third ))\n\nx_int = []\ny_int = []\n\nx_str = []\ny_str = []\n\nall_l=len(df_third )\nfor i,(name,series) in enumerate(df_third .iteritems()):\n    c = series.count()\n    fill = c/all_l*100  \n    if series.dtype == 'O':\n        x_str.append(i)\n        y_str.append(fill)\n    else:\n        x_int.append(i)\n        y_int.append(fill)\n\nplt.subplots(figsize=(10, 10))         \nplt.plot(x_int,y_int,'o',color = 'red', markersize = 10, alpha = 0.3)\nplt.plot(x_str,y_str,'o',color = 'green', markersize = 10, alpha = 0.3)\n\nplt.show()"},{"cell_type":"markdown","metadata":{},"source":"As you can see it have more high filling for 2 cluster, but other columns have low filling"},{"cell_type":"markdown","metadata":{},"source":"CONCLUSION \n\nIt may be useful to consider the problem, dividing it into three parts: \n\n1) filled columns only from 'filled_lst' \n\n2) filled cluster 1 (clusters[0]) \n\n3) filled cluster 2 (clusters[1])\n\n\nAnd in the end I want to draw from (1 variance):"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"df_high_filled = all_data[~get_mask_notnull(all_data, clusters[0]+clusters[1])]\nprint('only high filled objects: %d' % len(df_high_filled))\n\nx_int = []\ny_int = []\n\nx_str = []\ny_str = []\n\nall_l=len(df_high_filled)\nfor i,(name,series) in enumerate(df_high_filled.iteritems()):\n    c = series.count()\n    fill = c/all_l*100  \n    if series.dtype == 'O':\n        x_str.append(i)\n        y_str.append(fill)\n    else:\n        x_int.append(i)\n        y_int.append(fill)\n\nplt.subplots(figsize=(10, 10))         \nplt.plot(x_int,y_int,'o',color = 'red', markersize = 10, alpha = 0.3)\nplt.plot(x_str,y_str,'o',color = 'green', markersize = 10, alpha = 0.3)\n\nplt.show()"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}