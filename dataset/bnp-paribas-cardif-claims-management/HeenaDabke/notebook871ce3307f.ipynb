{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"02a43f09-e6b8-0537-c155-9212a83c9542"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n\nfrom sklearn import ensemble, metrics, linear_model\nimport random\n\n#Some parameters to play with\nrnd=12\nrandom.seed(rnd)\nn_ft=20 #Number of features to add\nmax_elts=3 #Maximum size of a group of linear features\n\nclass addNearestNeighbourLinearFeatures:\n    \n    def __init__(self, n_neighbours=1, max_elts=None, verbose=True, random_state=None):\n        self.rnd=random_state\n        self.n=n_neighbours\n        self.max_elts=max_elts\n        self.verbose=verbose\n        self.neighbours=[]\n        self.clfs=[]\n        \n    def fit(self,train,y):\n        if self.rnd!=None:\n            random.seed(rnd)\n        if self.max_elts==None:\n            self.max_elts=len(train.columns)\n        list_vars=list(train.columns)\n        random.shuffle(list_vars)\n        \n        lastscores=np.zeros(self.n)+1e15\n\n        for elt in list_vars[:self.n]:\n            self.neighbours.append([elt])\n        list_vars=list_vars[self.n:]\n        \n        for elt in list_vars:\n            indice=0\n            scores=[]\n            for elt2 in self.neighbours:\n                if len(elt2)<self.max_elts:\n                    clf=linear_model.LinearRegression(fit_intercept=False, normalize=True, copy_X=True, n_jobs=-1) \n                    clf.fit(train[elt2+[elt]], y)\n                    scores.append(metrics.log_loss(y,clf.predict(train[elt2 + [elt]])))\n                    indice=indice+1\n                else:\n                    scores.append(lastscores[indice])\n                    indice=indice+1\n            gains=lastscores-scores\n            if gains.max()>0:\n                temp=gains.argmax()\n                lastscores[temp]=scores[temp]\n                self.neighbours[temp].append(elt)\n\n        indice=0\n        for elt in self.neighbours:\n            clf=linear_model.LinearRegression(fit_intercept=False, normalize=True, copy_X=True, n_jobs=-1) \n            clf.fit(train[elt], y)\n            self.clfs.append(clf)\n            if self.verbose:\n                print(indice, lastscores[indice], elt)\n            indice=indice+1\n                    \n    def transform(self, train):\n        indice=0\n        for elt in self.neighbours:\n            train['_'.join(pd.Series(elt).sort_values().values)]=self.clfs[indice].predict(train[elt])\n            indice=indice+1\n        return train\n    \n    def fit_transform(self, train, y):\n        self.fit(train, y)\n        return self.transform(train)\n    \n    \ntrain = pd.read_csv(\"../input/train.csv\")\ntarget = train['target'].values\ntest = pd.read_csv(\"../input/test.csv\")\nid_test = test['ID'].values\n\ntrain['v22-1']=train['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[0]))\ntest['v22-1']=test['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[0]))\ntrain['v22-2']=train['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[1]))\ntest['v22-2']=test['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[1]))\ntrain['v22-3']=train['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[2]))\ntest['v22-3']=test['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[2]))\ntrain['v22-4']=train['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[3]))\ntest['v22-4']=test['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[3]))\n\ndrop_list=['v91','v1', 'v8', 'v10', 'v15', 'v17', 'v25', 'v29', 'v34', 'v41', 'v46', 'v54', 'v64', 'v67', 'v97', 'v105', 'v111', 'v122']\ntrain = train.drop(['ID','target'] + drop_list,axis=1).fillna(-999)\ntest = test.drop(['ID'] + drop_list,axis=1).fillna(-999)\n\nrefcols=list(train.columns)\n\nfor elt in refcols:\n    if train[elt].dtype=='O':\n        train[elt], temp = pd.factorize(train[elt])\n        test[elt]=temp.get_indexer(test[elt])\n    else:\n        train[elt]=train[elt].round(5)\n        test[elt]=test[elt].round(5)\n        \na=addNearestNeighbourLinearFeatures(n_neighbours=n_ft, max_elts=max_elts, verbose=True, random_state=rnd)\na.fit(train, target)\n\ntrain = a.transform(train)\ntest = a.transform(test)\n\nclf = ensemble.ExtraTreesClassifier(n_estimators=750,max_features=50,criterion= 'entropy',min_samples_split= 4,\n                        max_depth= 35, min_samples_leaf= 2, n_jobs = -1, random_state=rnd)\n\nclf.fit(train,target)\npred_et=clf.predict_proba(test)\n\nsubmission=pd.read_csv('../input/sample_submission.csv')\nsubmission.index=submission.ID\nsubmission.PredictedProb=pred_et[:,1]\nsubmission.to_csv('./addNNLinearFt.csv', index=False)\nsubmission.PredictedProb.hist(bins=30)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}