{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"from sklearn.metrics import log_loss\nfrom sklearn.ensemble import ExtraTreesClassifier"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"train = pd.read_csv(\"../input/train.csv\",sep=\",\",index_col=0)\nsubmit = pd.read_csv(\"../input/test.csv\",sep=\",\",index_col=0)\nfull_data_filter = train.copy()\ndel full_data_filter['target']\n\t\n# Compute full rows for training data\nfull_data_filter = full_data_filter.notnull().astype(int)\nrow_pct_complete = full_data_filter.sum(axis=1).value_counts()\nfull_rows_model_filter = full_data_filter.sum(axis=1)/131>0.9\nempty_rows_model_filter = full_data_filter.sum(axis=1)/131<0.3\n# compute full rows for submission data\nsubmit_full_data_filter = submit.copy()\nsubmit_full_data_filter = submit_full_data_filter.notnull().astype(int)\nsubmit_row_pct_complete = submit_full_data_filter.sum(axis=1).value_counts()\nsubmit_full_rows_model_filter = submit_full_data_filter.sum(axis=1)/131>0.9\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"cat_features_list = []\n\n# Keep the full features\nnumeric_features = ['v10','v12','v14','v21','v34','v40','v50','v114']\n# Full categorical features\n# full features : v24, v47, v66, v71, v72, v74, v75, v79, v110, \n# features with a few missing values : v52: 3 , v91: 3, v107: 3, v112: 382\ncategorical_features = ['v24','v47','v52','v66','v71','v72','v74','v75','v79','v91','v107','v110','v112']\ncat_features_list.append(categorical_features)\n# Almost full categorical features\n# v3 : 3457 missing, v31 3457 missing\nalmost_full_categorical_features = ['v3','v31']\ncat_features_list.append(almost_full_categorical_features)\n# Half empty categorical features\n# v30 : 60110 missing, v113 : 55304 missing\n# TODO : check wether missing values in synch with numeric missing features\nhalf_empty_categorical_features = ['v30','v113']\ncat_features_list.append(half_empty_categorical_features)\n# v56: 6882 missing, v125: 77 missing, card=90, v22 : 500 missing but caridnality = 18210 \nhigh_cardinality_features = ['v56','v125','v22']\ncat_features_list.append(high_cardinality_features)\n# v38 full, v62 full, v129 : full\ncat_num_features = ['v38','v62','v129']\ncat_features_list.append(cat_num_features)\n\nfeatures_to_keep = set(numeric_features)\nfeatures_to_keep = features_to_keep.union(categorical_features)\nfeatures_to_keep = features_to_keep.union(cat_num_features)\nfeatures_to_keep = features_to_keep.union(almost_full_categorical_features)\nfeatures_to_keep = features_to_keep.union(high_cardinality_features)\nfeatures_to_keep = features_to_keep.union(half_empty_categorical_features)\nfeatures_to_keep = list(features_to_keep)\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"def rnd_inverse(x):\n\t\"\"\"\n\t\tround data to 3 digits then inverse \n\t\"\"\"\n\treturn 1/(.1+round(x,3))\n\ndata_X = train.copy()\ndel data_X['target']\nfor feat_list in cat_features_list:\n\tif set(feat_list) <= set(features_to_keep):\n\t\tfor f in feat_list:\n\t\t\tdata_X[f], indexer = pd.factorize(data_X[f])\nall_features = data_X.columns\nfor f in all_features:\n\tif f not in cat_features_list:\n\t\tthe_mean = data_X[f].mean()\n\t\tdata_X[f].fillna(the_mean,inplace=True)\nfor f in numeric_features:\n\tdata_X[f]=data_X[f].apply(rnd_inverse)\n\t\n# Add completion rate feature\ndata_X[\"complete\"] = full_data_filter.sum(axis=1)/131\n\ndata_X = data_X[full_rows_model_filter]\ndata_Y = train.loc[full_rows_model_filter,'target'].copy()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"np.random.seed(57)\nfrom sklearn.cross_validation import StratifiedKFold\nskf = StratifiedKFold(data_Y.values,n_folds=5)\nfrom sklearn.ensemble import ExtraTreesClassifier\nextc = ExtraTreesClassifier(n_estimators = 600,max_features=25,max_depth=34,\n\t\t\trandom_state=57,criterion='entropy',min_samples_leaf=1,min_samples_split=2,n_jobs=-1)\nclearance = ['v36','v61','v98','v104','v9','v49','v65','v109','v86','v120','v127','v102','v121','v27','v97','v51','v103','v111',\n\t\t\t\t\t'v125','v28','v39','v11','v33','v58','v124','v2','v122','v10','v7','v105','v45',\n\t\t\t\t\t'v94','v90','v119','v1','v100','v95','v93','v123','v25','v118','v43','v108','v48',\n\t\t\t\t\t'v31','v12','v21','v14','v64','v71','v126','v73','v110','v79']\nselect_X = data_X.drop(clearance,axis=1)\nresults = cross_val_score(extc, select_X, data_Y, scoring='log_loss', cv=skf, verbose=0, n_jobs=1)\nprint(\"EXTC results : %0.5f/%0.5f\" %(np.mean(results),np.std(results)))"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}