{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0,"cells":[{"metadata":{"_cell_guid":"6264ba15-eace-58f8-5e53-52e786ee21e3","_active":false,"collapsed":false},"source":"The objective of model is to predict with high accuracy those claims which should be expedited so that the customer gets the best experience and pacification during the recovery settlement. False positives might lead to financial loss to the company due to fraud but false negatives are more affecting and brand diminishing.\n\nSo here we start with exploratory analysis of data and then proceed to the model development via the major steps below :\n\n1. Uni-variate Analysis (For both predictors and response)\n2. Bi-variate Analysis \n3. Correlation analysis\n4. Multi-collinearity checks\n5. MVI and outlier treatment\n6. Variable elimination and standardisation (including transformations)\n7. Model runs (Logistic and others)\n8. Validation and finally submission\n","execution_count":null,"cell_type":"markdown","outputs":[]},{"metadata":{"_cell_guid":"9877e0e3-fb45-f2d6-1d9f-37ad808b247d","_active":true,"collapsed":false},"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\ndf_model = pd.read_csv('../input/train.csv');\n\n# loading and then seeing variable distribution and # records etc. - overview of data\nprint(df_model.describe().transpose());\n\n# perform a univariate of the target variable\nprint(df_model['target'].describe().transpose());\n\n#114,321 records present we shall split to training and validation 70:30 \n# the event rate (claims needed to be processed with acceleration) is ~77% , so let us model the non event\nx = numpy.random.rand(100, 5)\nindices = numpy.random.permutation(x.shape[0])\ntraining_idx, test_idx = indices[:70], indices[70:]\ndf_model_training, df_model_test = x[training_idx,:], x[test_idx,:]\n\n\n","execution_count":4,"cell_type":"code","outputs":[],"execution_state":"idle"}]}