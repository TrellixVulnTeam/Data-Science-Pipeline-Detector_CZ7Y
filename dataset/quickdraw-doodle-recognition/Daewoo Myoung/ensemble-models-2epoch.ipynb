{"cells":[{"metadata":{},"cell_type":"markdown","source":"Train Code : https://www.kaggle.com/ttagu99/train-model","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\ndata_path = \"/kaggle/input/quickdraw-doodle-recognition/\" \nprint(os.listdir(data_path))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nsub_df = pd.read_csv(data_path+'sample_submission.csv')\nprint(\"test data 수:\",len(sub_df))\nsub_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_file_path = \"/kaggle/input/quickdraw-doodle-recognition/train_raw/\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_csvs= os.listdir(train_file_path)\nprint(\"train_raw 폴더 내 파일 수:\", len(train_csvs))\nprint(train_csvs[:5])\n\nfile_size = 0\nlabel_names = []\n\nfor csv_file in train_csvs:\n    file_size += os.path.getsize(train_file_path + csv_file) # data file들의 용량을 계산\n    label_names.append(csv_file.replace('.csv','')) \nprint(\"파일 크기 : \", file_size//(1024*1024*1024) ,\"GB\")\n\nlabel_names = sorted(label_names,key=lambda x : str.lower(x+'.csv')) # at kaggle notebook ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hold_out_set= 'train_k99'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport json    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preds2catids(predictions): # submission을 위해 top3 category로 변환할 함수\n    return pd.DataFrame(np.argsort(-predictions, axis=1)[:, :3], columns=['a', 'b', 'c'])\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def map_at3(y_true, y_pred): \n    map3 = tf.keras.metrics.top_k_categorical_accuracy(y_true, y_pred, k=1)*0.5\n    map3 += tf.keras.metrics.top_k_categorical_accuracy(y_true, y_pred, k=2)*0.17\n    map3 += tf.keras.metrics.top_k_categorical_accuracy(y_true, y_pred, k=3)*0.33    \n    return map3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ext_data_path = '/kaggle/input/doodle-model/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\noutputs = os.listdir(ext_data_path)\nhold_out_probs = [ext_data_path+ f for f in outputs if f.find('ho_prob') >= 0 ] \ntest_out_probs = [ext_data_path+ f for f in outputs if f.find('test_prob') >= 0 ] \nhold_out_probs = sorted(hold_out_probs)\ntest_out_probs = sorted(test_out_probs)\nho_df = pd.read_csv(ext_data_path+hold_out_set)\nho_s = []\nfor prob_path in hold_out_probs:\n    ho = np.load(prob_path)\n    ho = ho[:len(ho_df)]\n    ho_s.append(ho)\ntargets = ho_df.y.to_numpy() # hold out target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ho_arr = np.stack(ho_s,axis=-1)\nho_arr.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del(ho_s)\nimport gc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xin = tf.keras.layers.Input((len(label_names),ho_arr.shape[2]))\nx = tf.keras.layers.Convolution1D(1,kernel_size=1,activation='linear',use_bias=False)(xin)\nx = tf.keras.layers.Reshape(target_shape=(len(label_names),))(x)\nwensemble_model = tf.keras.Model(inputs=xin, outputs=x)\nwensemble_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"check_point=tf.keras.callbacks.ModelCheckpoint(monitor='map_at3',verbose=1\n                               ,filepath='ensemble_w.h5',save_best_only=True,mode='max') \nwensemble_model.compile(optimizer=tf.keras.optimizers.Adam(1),loss='mse', metrics=[ map_at3])\nwensemble_model.fit(x=ho_arr,y=tf.keras.utils.to_categorical(targets)\n                    , epochs=20, batch_size=10000,verbose=1, callbacks=[check_point])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wensemble_model.load_weights('ensemble_w.h5')\nwensemble_model.get_weights()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del(ho_arr)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = np.array(wensemble_model.get_weights()).squeeze()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ens_prob = np.zeros((len(sub_df),len(label_names)))\nfor i, prob_path in enumerate(test_out_probs):\n    ens_prob += (np.load(prob_path) * res[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter,OrderedDict\nfrom operator import itemgetter \n\ndef balancing_predictions(test_prob, factor = 0.1, minfactor = 0.001, patient = 5, permit_cnt=332, max_search=10000, label_num=340):\n    maxk = float('inf')\n    s_cnt = np.zeros(label_num)\n    for i in range(max_search):\n        ctop1 = Counter(np.argmax(test_prob,axis=1))\n        ctop1 = sorted(ctop1.items(), key=itemgetter(1), reverse=True)\n        if maxk > ctop1[0][1]:\n            maxk = ctop1[0][1]\n        else:\n            s_cnt[ctop1[0][0]]+=1\n            if np.max(s_cnt)>patient:\n                if factor< minfactor:\n                    print('stop min factor')\n                    break\n                s_cnt=np.zeros(label_num)\n                factor*=0.99\n                print('reduce factor: ', factor, ', current max category num: ', ctop1[0][1])\n\n        if ctop1[0][1] <= permit_cnt:\n            print('idx: ',ctop1[0][0] ,', num: ', ctop1[0][1]) \n            break\n        test_prob[:,ctop1[0][0]] *= (1.0-factor)\n        \n    return test_prob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bal_test_prob = balancing_predictions(ens_prob)\nbal_top3 = preds2catids(bal_test_prob)\nid2cat = {k: cat.replace(' ', '_') for k, cat in enumerate(label_names)}\nbal_top3cats = bal_top3.replace(id2cat) \nsub_df['word'] = bal_top3cats['a'] + ' ' + bal_top3cats['b'] + ' ' + bal_top3cats['c']\nbal_submission = sub_df[['key_id', 'word']]\nbal_submission.to_csv('submission_bal_ens.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}