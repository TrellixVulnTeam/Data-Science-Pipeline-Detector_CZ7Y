{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import np_utils\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras.layers import BatchNormalization\nfrom keras import applications\n\nimport matplotlib.pyplot as plt\nimport os, platform, time\nimport os.path\nfrom os import walk\nimport numpy as np\nimport pandas as pd\nimport random\nimport cv2\nfrom PIL import Image, ImageDraw\nimport keras\nfrom keras.utils import to_categorical\nfrom keras.optimizers import SGD\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.vgg16 import preprocess_input\nimport cv2\nimport random\nfrom sklearn import preprocessing\n\ndef draw_it(raw_strokes):\n    image = Image.new(\"P\", (255,255), color=255)\n    image_draw = ImageDraw.Draw(image)\n\n    for stroke in eval(raw_strokes):\n        for i in range(len(stroke[0])-1):\n\n            image_draw.line([stroke[0][i], \n                             stroke[1][i],\n                             stroke[0][i+1], \n                             stroke[1][i+1]],\n                            fill=0, width=6)\n    return np.array(image)\n\n#DATA_PATH = \"D:\\\\juptest\\\\datasets\\\\train_simplified\"\n\nif platform.system() == 'Linux':\n    #from google.colab import drive #for mounting google drive\n    #drive.mount('/content/gdrive')\n    DATA_PATH = '/kaggle/input/quickdraw-doodle-recognition/train_simplified/'\nelse:\n    DATA_PATH = \"D:\\\\juptest\\\\datasets\\\\train_simplified\\\\\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#os.chdir('D:\\\\juptest\\\\datasets\\\\')\nFILE_PATH = [] #now 10 files\nfor (dirpath, dirnames, filenames) in walk(DATA_PATH):\n    FILE_PATH.extend(filenames)\n    break\n\nword_set = []\nfor i in FILE_PATH:\n  word_set.append(i[0:-4])\n\nIMG_WIDTH = 48\nIMG_HEIGHT = 48\nIMG_DEPTH = 3\nBATCH_SIZE = 32\next_num = 600","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = applications.ResNet50V2(include_top=True, weights=None, input_shape=(IMG_WIDTH, IMG_HEIGHT, 3), pooling='max', classes=len(word_set))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nle = preprocessing.LabelEncoder()\nle.fit(word_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(10):\n  train_set = [] #set of X\n  label_set = [] #set of Y\n\n  ext_class = random.sample(range(len(FILE_PATH)), int(0.42*len(FILE_PATH)))\n\n  for i in ext_class:\n    thisfile = pd.read_csv(DATA_PATH + FILE_PATH[i])\n    #word_set.append(thisfile['word'][0])\n    extlist = random.sample(range(len(thisfile)), ext_num)\n    for j in extlist:\n        train_set.append(cv2.resize(draw_it(thisfile['drawing'][j]), dsize=(IMG_WIDTH,IMG_HEIGHT)))\n        label_set.append(thisfile['word'][j])\n\n  \n  label_set = le.transform(label_set)\n\n  train_set = np.array(train_set)\n  label_set = np.array(label_set)\n  X_train, X_validation, Y_train, Y_validation = train_test_split(train_set, label_set, test_size = 0.2, random_state=223)\n\n  X_train = X_train.astype('float32') / 255\n  X_validation = X_validation.astype('float32') / 255\n\n  X_train = np.stack([X_train] * 3, axis = 3)\n  X_validation = np.stack([X_validation] * 3, axis = 3)\n\n  Y_train = np_utils.to_categorical(Y_train, len(word_set))\n  Y_validation = np_utils.to_categorical(Y_validation, len(word_set))\n \n\n  history = model.fit(X_train, Y_train,\n            epochs=4,\n            batch_size=BATCH_SIZE,\n            validation_data=(X_validation, Y_validation))\n  #print('\\nAccuracy: {:.4f}'.format(model.evaluate(validation_data, validation_labels)[1]))\n\n  y_vloss = history.history['val_loss']\n  y_loss = history.history['loss']\n\n  x_len = np.arange(len(y_loss))\n  plt.plot(x_len, y_loss, marker='.', c='blue', label=\"Train-set Loss\")\n  plt.plot(x_len, y_vloss, marker='.', c='red', label=\"Validation-set Loss\")\n\n  plt.legend(loc='upper right')\n  plt.grid()\n  plt.xlabel('epoch')\n  plt.ylabel('loss')\n  plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_X = [] #set of X\ntest_Y = [] #set of Y\n\nthisfile = pd.read_csv('/kaggle/input/quickdraw-doodle-recognition/test_simplified.csv')\n\nfor j in range(len(thisfile)):\n    test_X.append(cv2.resize(draw_it(thisfile['drawing'][j]), dsize=(IMG_WIDTH,IMG_HEIGHT)))\n    #test_Y.append(thisfile['word'][j])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntest_X = np.array(test_X)\n#test_Y = np.array(test_Y)\n\n#X_train, X_validation, Y_train, Y_validation = train_test_split(text_X, test_Y, test_size = 0.2, random_state=223)\n\ntest_X = test_X.astype('float32') / 255\ntest_X = np.stack([test_X] * 3, axis = 3)\n\n\ntest_Y = model.predict(test_X)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(test_Y))\nX = []\nfor i in range(len(test_Y)):\n    maxv = -1\n    idx = 0\n    for j in range(len(test_Y[0])):\n        if maxv < test_Y[i][j]:\n            maxv = test_Y[i][j]\n            idx = j\n    X.append(idx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = le.inverse_transform(X)\nans = []\nfor i in range(len(thisfile['key_id'])):\n    ans.append([thisfile['key_id'][i], y[i]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ans = pd.DataFrame(ans)\nans.columns = ['key_id', 'word']\nans.to_csv('/kaggle/working/ans.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"Untitled17.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":1}