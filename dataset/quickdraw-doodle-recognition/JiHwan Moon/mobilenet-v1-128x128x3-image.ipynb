{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Doodle Recognition\n\n프레임워크\n* PyTorch를 사용하였습니다.\n* Tensorflow, Keras 역시 사용할 줄 알지만, 짧은 시간 내에 모델을 구현하고 테스트하는데 있어 PyTorch가 가장 적합하다고 생각했습니다.\n* PyTorch로 다양한 모델을 구현해 본 경험이 있다는 것 역시 PyTorch를 선택한 이유 중 하나입니다.\n\n데이터\n* 별도의 private kernel로 확인한 결과, 각 label에 해당하는 csv 파일은 최소 11만개 이상, 최대 34만개 이상의 데이터를 담고 있습니다.\n* 최선의 방법은 이를 모두 특정 데이터베이스(ex. sqlite)로 옮긴 후 사용하는 것이지만, 이 방법을 사용하려면 데이터를 로컬에 다운로드 한 후, 데이터베이스로 옮기는 작업을 수행하고 Kaggle에 새로운 데이터셋으로 추가하는 등의 작업이 필요합니다.\n* 하지만 저는 주어진 kernel 환경 내에서 해결하고자 하였고, 따라서 제한된 데이터를 사용할 수 밖에 없었습니다.\n* 학습 데이터셋은 각 label 별로 10,000개의 데이터를 담고 있습니다. 따라서 총 340 * 10,000 = 3,400,000개의 데이터를 사용합니다.\n* 검증 데이터로는 label마다 500개의 데이터를 사용하였습니다. 따라서 검증 데이터셋은 500 * 340 = 170,000개의 데이터를 가지고 있습니다.\n* PyTorch의 ConcatDataset을 사용하여 모든 label의 데이터를 하나의 데이터셋으로 통합할 수 있었습니다.\n\n모델\n* MobileNet-v1을 사용하였습니다. 아래의 사진에서 알 수 있듯이 MobileNet-v1은 ResNet-18보다 그 크기가 작지만 성능은 비슷하거나 더 좋습니다.\n![imaegnet performance](https://cdn-images-1.medium.com/max/800/1*kfpO_fJ4bc92sffY4bxnSA.jpeg)\n* MobileNet-v1의 또다른 장점은 논문을 보고 바로 구현하기 편리하다는 것입니다.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import glob\n\nimport ast\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset\n\nimport torchvision\nfrom torchvision import transforms, utils\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport cv2  # to generate image from vectors (strokes)\n\nimport tqdm\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NROWS = 10000\nVAL_NROWS = 500\n\npath = '../input/train_simplified/'\nfilenames = glob.glob(os.path.join(path, '*.csv'))\nNUM_CLASSES = len(filenames)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this drawing function was adopted from https://github.com/ebouteillon/kaggle-quickdraw-doodle-recognition-challenge/blob/master/2-training-resnet18-from-scratch-with-128px-images.ipynb\n\nshift_colors = (\n    (255, 0, 0),\n    (255, 128, 0),\n    (255, 255, 0),\n    (128, 255, 0),\n    (0, 255, 0),\n    (0, 255, 128),\n    (0, 255, 255),\n    (0, 128, 255),\n    (0, 0, 255),\n    (128, 0, 255),\n    (255, 0, 255),\n    (255, 0, 128)\n)\n\n\ndef draw_cv2(raw_strokes, size=128, lw=1):\n    # draw function inspired from https://towardsdatascience.com/10-lessons-learned-from-participating-to-google-ai-challenge-268b4aa87efa\n    BASE_SIZE = 256\n    border = 2  # keep some margin with image border\n\n    img = np.zeros((size, size, 3), np.uint8)\n    coef = (size - 2 * lw - 2 * border) / (BASE_SIZE - 1)\n    num_stokes = len(raw_strokes)\n    for t, stroke in enumerate(raw_strokes[::-1]):  # iterate in reverse order, so that earlier strokes, which are more important, are drawn later so that they are not overlapped\n        rgb = shift_colors[(num_stokes-t-1)%12]\n\n        for i in range(len(stroke[0]) - 1):\n            p1 = (int(coef * stroke[0][i] + lw + border), int(coef * stroke[1][i] + lw+ border))\n            p2 = (int(coef * stroke[0][i + 1] + lw + border), int(coef * stroke[1][i + 1] + lw + border))\n            _ = cv2.line(img, p1, p2, rgb, lw, cv2.LINE_AA)\n    return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encode_dict = {}\npath = '../input/train_simplified/'\n\nfilenames = glob.glob(os.path.join(path, '*.csv'))\nfilenames = sorted(filenames)\nprint(len(filenames))\n\nfor ix, filename in enumerate(filenames):\n    class_name = filename.split('/')[-1].split('.')[0].replace(' ', '_')\n    encode_dict[class_name] = ix\n    \ndecode_dict = {value:key for key, value in encode_dict.items()}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ClassDataset(Dataset):\n    def __init__(self, csv_file_path, mode='train', nrows=15000, skiprows=0, size=128):\n        # try nrows=20000\n        super().__init__()\n        \n        self.df = pd.read_csv(csv_file_path, usecols=['drawing'], nrows=nrows, skiprows=0)\n        self.mode = mode\n        self.size = size\n        if self.mode == 'train':\n            self.class_name = csv_file_path.split('/')[-1].split('.')[0].replace(' ', '_')\n            self.label = encode_dict[self.class_name]\n            \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        raw_strokes = ast.literal_eval(self.df.drawing[index])\n        image = draw_cv2(raw_strokes, size=self.size)  # (size, size, 3)\n        image = image.transpose(2, 0, 1)\n        \n        if self.mode == 'train':\n            return (image/255).astype('float32'), self.label\n        else:\n            return (image/255).astype('float32')\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dset = ConcatDataset([ClassDataset(filename, nrows=NROWS) for filename in filenames])\nval_dset = ConcatDataset([ClassDataset(filename, nrows=VAL_NROWS, skiprows=NROWS) for filename in filenames])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(dset))  # 340 * NROWS\nprint(len(val_dset))   # 340 * VAL_NROWS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dloader = DataLoader(dset, batch_size=128, shuffle=True, num_workers=2)\nval_dloader = DataLoader(val_dset, batch_size=128, num_workers=2, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch = iter(dloader).next()\nprint(batch)  # it works well, shuffled.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(np.transpose(batch[0][0], (1, 2, 0)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# our dataloader is ready.\n# let's make model\n\n# building blocks for mobilenet.\n# name Conv and Conv_dw are following terms used in mobilenet paper (https://arxiv.org/pdf/1704.04861.pdf)\n\nclass Conv(nn.Module):\n    def __init__(self, in_channel, out_channel, kernel_size=3, stride=1, padding=1):\n        super().__init__()\n        self.layers = nn.Sequential(\n                nn.Conv2d(in_channel, out_channel, kernel_size=kernel_size, stride=stride, padding=padding, bias=False),\n                nn.BatchNorm2d(out_channel),\n                nn.ReLU(inplace=True)\n            )\n        \n    def forward(self, input):\n        return self.layers(input)\n    \nclass Conv_dw_Conv(nn.Module):\n    # Conv dw layer followed by Conv layer.\n    # implemented this way since every conv dw layer is followed by conv layer with kernel size 1, stride 1 with some out_channel\n    def __init__(self, in_channel, out_channel, kernel_size=3, stride=1, padding=1):\n        super().__init__()\n        self.layers = nn.Sequential(\n                nn.Conv2d(in_channel, in_channel, kernel_size, stride, padding, bias=False, groups=in_channel),\n                nn.BatchNorm2d(in_channel),\n                nn.ReLU(inplace=True),\n                Conv(in_channel, out_channel, kernel_size=1, stride=1, padding=0)\n            )\n        \n    def forward(self, input):\n        return self.layers(input)\n    \n    \nclass MobileNet(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        \n        self.num_classes = num_classes\n        \n        self.model = nn.Sequential(\n                Conv(3, 32, stride=2),\n            \n                Conv_dw_Conv(32, 64, kernel_size=3, stride=1),\n                Conv_dw_Conv(64, 128, kernel_size=3, stride=2),\n                Conv_dw_Conv(128, 128, kernel_size=3, stride=1),\n                Conv_dw_Conv(128, 256, kernel_size=3, stride=2),\n                Conv_dw_Conv(256, 256, kernel_size=3, stride=1),\n                Conv_dw_Conv(256, 512, kernel_size=3, stride=2),\n            \n                Conv_dw_Conv(512, 512, kernel_size=3, stride=1),\n                Conv_dw_Conv(512, 512, kernel_size=3, stride=1),\n                Conv_dw_Conv(512, 512, kernel_size=3, stride=1),\n                Conv_dw_Conv(512, 512, kernel_size=3, stride=1),\n                Conv_dw_Conv(512, 512, kernel_size=3, stride=1),\n            \n                Conv_dw_Conv(512, 1024, kernel_size=3, stride=2),\n                Conv_dw_Conv(1024, 1024, kernel_size=3, stride=1)\n        )\n        \n        \n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(1024, num_classes)\n        \n    def forward(self, input):\n        x = self.model(input)\n        x = self.avg_pool(x)\n        x = x.view(-1, 1024)\n        out = self.fc(x)\n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# before training, let's set up our metric MAP@3\n# I slightly modified implementation of https://github.com/benhamner/Metrics/blob/master/Python/ml_metrics/average_precision.py\n\ndef apk(actual, predicted, k=3):\n    \"\"\"\n    Computes the average precision at k.\n    This function computes the average prescision at k between two lists of\n    items.\n    Parameters\n    ----------\n    actual : int\n             element that are to be predicted\n    predicted : list\n                A list of predicted elements (order does matter)\n    k : int, optional\n        The maximum number of predicted elements\n    Returns\n    -------\n    score : double\n            The average precision at k over the input lists\n    \"\"\"\n    if len(predicted)>k:\n        predicted = predicted[:k]\n\n    score = 0.0\n\n    for i,p in enumerate(predicted):\n        if p == actual:\n            score = 1 / (i+1.0)\n    \n    return score\n\ndef mapk(actual, predicted, k=3):\n    \"\"\"\n    Computes the mean average precision at k.\n    This function computes the mean average prescision at k between two lists\n    of lists of items.\n    Parameters\n    ----------\n    actual : list\n             A list of elements that are to be predicted \n    predicted : list\n                A list of lists of predicted elements\n                (order matters in the lists)\n    k : int, optional\n        The maximum number of predicted elements\n    Returns\n    -------\n    score : double\n            The mean average precision at k over the input lists\n    \"\"\"\n    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one last thing before start training, we need a function that returns validation map@3.\n\ndef validation_score(model, val_data_loader):\n    model.eval()\n    sum_score = 0\n    count = 0\n    for images, labels in val_data_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        batch_size = images.size(0)\n        output = model(images)\n        topk = output.detach().topk(3, dim=1)[1]\n        sum_score += mapk(labels.cpu().numpy(), topk.cpu().numpy()) * batch_size\n        count += batch_size\n        \n    model.train()\n    return sum_score / count\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train time!\n\nmodel = MobileNet(NUM_CLASSES).to(device)\n\ncriterion = nn.CrossEntropyLoss().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n\nscheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[60000, 130000, 160000, 190000], gamma=0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nprint(\"Start training...\")\n\nepochs = 8\n\nprint_every = 1000 # print every N iterations\nvalidate_every = 10000  # do validation every N iterations\nmodel.train()\n\nbest_val_score = 0\n\ncurr_iter = 0\navg_loss = 0\navg_score = 0\n\nfor epoch in range(epochs):\n\n\n    for ix, (images, labels) in enumerate(dloader):\n        \n        images = images.to(device)\n        labels = labels.to(device)\n        \n        model.zero_grad()\n        output = model(images)\n        loss = criterion(output, labels)\n        loss.backward()\n        optimizer.step()\n        \n        avg_loss += loss.item()\n        \n        topk = output.detach().topk(3, dim=1)[1]\n        avg_score += mapk(labels.cpu().numpy(), topk.cpu().numpy())\n        # scheduler.step()\n            \n        curr_iter += 1\n        if (curr_iter) % print_every == 0:\n            print('Epoch {}, Iteration {} - Train Loss: {:.4f}, MAP@3: {:.3f}'.format(epoch + 1, curr_iter, avg_loss/print_every, avg_score/print_every))\n            avg_loss = 0\n            avg_score = 0\n            \n        if curr_iter % validate_every == 0:\n            val_score = validation_score(model, val_dloader)\n            print('Epoch {}, Iteration {}: validation map@3: {}'.format(epoch + 1, curr_iter, val_score))\n            if val_score > best_val_score:\n                print('New best validation score: {}, saving model...'.format(val_score))\n                best_val_score = val_score\n                torch.save(model.state_dict(), 'model_checkpoint_best_val.ckpt')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save model\n\ntorch.save(model.state_dict(), 'model_checkpoint.ckpt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make subimssion using final model\ntest_dset = ClassDataset('../input/test_simplified.csv', mode='test', nrows=None)\ntest_dloader = DataLoader(test_dset, batch_size=128, shuffle=False, num_workers=0)\n\nimport tqdm\nmodel.eval()\nlabels = []\nfor images in tqdm.tqdm(test_dloader):\n    images = images.to(device)\n    output = model(images)\n    _, pred = output.topk(3, 1)\n    for i in range(len(images)):\n        labels.append(' '.join([decode_dict[pred[i][j].item()]for j in range(3)]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nsubmission = pd.read_csv('../input/test_simplified.csv', index_col='key_id' ,usecols=['key_id'])\nprint(len(submission))\nsubmission['word'] = labels\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission_final.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make submission using best val checkpoint\n\nmodel.load_state_dict(torch.load('model_checkpoint_best_val.ckpt'))\nmodel.eval()\n\nlabels = []\nfor images in tqdm.tqdm(test_dloader):\n    images = images.to(device)\n    output = model(images)\n    _, pred = output.topk(3, 1)\n    for i in range(len(images)):\n        labels.append(' '.join([decode_dict[pred[i][j].item()]for j in range(3)]))\n        \nprint(len(labels))\n        \nsubmission = pd.read_csv('../input/test_simplified.csv', index_col='key_id' ,usecols=['key_id'])\nprint(len(submission))\nsubmission['word'] = labels\n\nsubmission.to_csv('submission_best_val.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}