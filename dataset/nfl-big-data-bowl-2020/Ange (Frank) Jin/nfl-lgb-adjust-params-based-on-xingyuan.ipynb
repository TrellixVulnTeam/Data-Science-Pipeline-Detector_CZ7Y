{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport pandas as pd\nimport numpy as np\nimport datetime as dt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nimport keras\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn import metrics\nimport math\nfrom string import punctuation\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nfrom kaggle.competitions import nflrush","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# You can only call make_env() once, so don't lose it!\nenv = nflrush.make_env()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/nfl-big-data-bowl-2020/train.csv', low_memory=False)\n#train.head(23)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clean windspeed\ntrain['WindSpeed'] = train['WindSpeed'].apply(lambda x: x.lower().replace('mph', '').strip() if not pd.isna(x) else x)\ntrain['WindSpeed'] = train['WindSpeed'].apply(lambda x: (int(x.split('-')[0])+int(x.split('-')[1]))/2 if not pd.isna(x) and '-' in x else x)\ntrain['WindSpeed'] = train['WindSpeed'].apply(lambda x: (int(x.split()[0])+int(x.split()[-1]))/2 if not pd.isna(x) and type(x)!=float and 'gusts up to' in x else x)\n    \ndef str_to_float(txt):\n    try:\n        return float(txt)\n    except:\n        return -1\n    \ntrain['WindSpeed'] = train['WindSpeed'].apply(str_to_float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clean wind direction\ndef clean_WindDirection(txt):\n    if pd.isna(txt):\n        return np.nan\n    txt = txt.lower()\n    txt = ''.join([c for c in txt if c not in punctuation])\n    txt = txt.replace('from', '')\n    txt = txt.replace(' ', '')\n    txt = txt.replace('north', 'n')\n    txt = txt.replace('south', 's')\n    txt = txt.replace('west', 'w')\n    txt = txt.replace('east', 'e')\n    return txt\n\ntrain['WindDirection'] = train['WindDirection'].apply(clean_WindDirection)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def transform_WindDirection(txt):\n    if pd.isna(txt):\n        return np.nan\n    \n    if txt=='n':\n        return 0\n    if txt=='nne' or txt=='nen':\n        return 1/8\n    if txt=='ne':\n        return 2/8\n    if txt=='ene' or txt=='nee':\n        return 3/8\n    if txt=='e':\n        return 4/8\n    if txt=='ese' or txt=='see':\n        return 5/8\n    if txt=='se':\n        return 6/8\n    if txt=='ses' or txt=='sse':\n        return 7/8\n    if txt=='s':\n        return 8/8\n    if txt=='ssw' or txt=='sws':\n        return 9/8\n    if txt=='sw':\n        return 10/8\n    if txt=='sww' or txt=='wsw':\n        return 11/8\n    if txt=='w':\n        return 12/8\n    if txt=='wnw' or txt=='nww':\n        return 13/8\n    if txt=='nw':\n        return 14/8\n    if txt=='nwn' or txt=='nnw':\n        return 15/8\n    return np.nan\n\ntrain['WindDirection'] = train['WindDirection'].apply(transform_WindDirection)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clean Turf\nTurf = {'Field Turf':'Artificial', 'A-Turf Titan':'Artificial', 'Grass':'Natural', 'UBU Sports Speed S5-M':'Artificial', \n        'Artificial':'Artificial', 'DD GrassMaster':'Artificial', 'Natural Grass':'Natural', \n        'UBU Speed Series-S5-M':'Artificial', 'FieldTurf':'Artificial', 'FieldTurf 360':'Artificial', 'Natural grass':'Natural', 'grass':'Natural', \n        'Natural':'Natural', 'Artifical':'Artificial', 'FieldTurf360':'Artificial', 'Naturall Grass':'Natural', 'Field turf':'Artificial', \n        'SISGrass':'Artificial', 'Twenty-Four/Seven Turf':'Artificial', 'natural grass':'Natural'} \n\ntrain['Turf'] = train['Turf'].map(Turf)\ntrain['Turf'] = train['Turf'] == 'Natural'\n\n# solve team name encoding problem\nmap_abbr = {'ARI': 'ARZ', 'BAL': 'BLT', 'CLE': 'CLV', 'HOU': 'HST'}\nfor abb in train['PossessionTeam'].unique():\n    map_abbr[abb] = abb\n    \ntrain['PossessionTeam'] = train['PossessionTeam'].map(map_abbr)\ntrain['HomeTeamAbbr'] = train['HomeTeamAbbr'].map(map_abbr)\ntrain['VisitorTeamAbbr'] = train['VisitorTeamAbbr'].map(map_abbr)\n\n# Before pivot:\n# Creat: IsBallCarrier, ToLeft, std_x, std_y, offense, age, bmi, player number\n\n#train['IsBallCarrier'] = train['NflId'] == train['NflIdRusher']\ntrain['ToLeft'] = train['PlayDirection'] == 'left'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"def define_offense(df):\n    offense = []\n    for i in range(0,len(df)):\n        if df['HomeTeamAbbr'][i] == df['PossessionTeam'][i]:\n            if df['Team'][i] == 'home':\n                offense.append('offense')\n            else:\n                offense.append('defense')\n        else:\n            if df['Team'][i] == 'away':\n                offense.append('offense')\n            else:\n                offense.append('defense')\n    df['Offense'] = np.array(offense)\n    return df\n\ntrain = define_offense(train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bye_XY(df):\n    # 计算出centroid的坐标\n    x = df.groupby(['PlayId','Team'],as_index=False)['X'].mean()\n    x.columns = ['PlayId','Team','x_cen']\n    df = pd.merge(df,x,how=\"inner\",on=['PlayId','Team'])\n    \n    y = df.groupby(['PlayId','Team'],as_index=False)['Y'].mean()\n    y.columns = ['PlayId','Team','y_cen']\n    df = pd.merge(df,y,how=\"inner\",on=['PlayId','Team'])\n    \n    # 计算两点(当前球员和centroid)之间的距离\n    distances = []\n    for i in range(len(df)) : \n      x1 = df.loc[i, \"X\"]\n      y1 = df.loc[i, \"Y\"] \n      x2 = df.loc[i,\"x_cen\"]\n      y2 = df.loc[i,\"y_cen\"]\n      dis_i = math.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n      distances.append(dis_i)\n    \n    df['distances'] = distances\n    \n    # 计算Average distance to centroid\n    avg_distance = df.groupby(['PlayId','Team'],as_index=False)['distances'].mean()\n    avg_distance.columns = ['PlayId','Team','avg_distance']\n    df = pd.merge(df,avg_distance,how=\"inner\",on=['PlayId','Team'])\n    \n#    # 计算qb的位置坐标\n    qb_pos = df[df[\"Position\"] == 'QB']\n    qb_pos = qb_pos[['PlayId','X','Y']]\n    qb_pos.columns = ['PlayId','qb_x','qb_y']\n    qb_pos.drop_duplicates(subset =\"PlayId\",keep = 'first',inplace = True)\n    df = pd.merge(df,qb_pos,how=\"left\",on=['PlayId'])\n    \n    # 计算球员和qb之间的距离\n    distances_qb = []\n    for i in range(len(df)) : \n      x1 = df.loc[i, \"X\"]\n      y1 = df.loc[i, \"Y\"] \n      x2 = df.loc[i,\"qb_x\"]\n      y2 = df.loc[i,\"qb_y\"]\n      dis_i = math.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n      distances_qb.append(dis_i)\n    \n    df['distances_to_qb'] = distances_qb\n    \n    #计算Average distance to QB\n    avg_distance_to_qb = df.groupby(['PlayId','Team'],as_index=False)['distances_to_qb'].mean()\n    avg_distance_to_qb.columns = ['PlayId','Team','avg_distance_to_qb']\n    df = pd.merge(df,avg_distance_to_qb,how=\"inner\",on=['PlayId','Team'])\n    return df\n\n\ntrain = bye_XY(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Clean_Op(df):\n    Op=df.groupby('PlayId',as_index=False).agg({'OffensePersonnel':'first'})\n    Op_split=Op.OffensePersonnel.str.split(',',expand=True)\n    Op_split.columns=[\"s1\",\"s2\",\"s3\",\"s4\",\"s5\"]\n    #pivot s1\n    p_s1=Op_split.s1.str.split(' ',expand=True)\n    p_s1.columns=['number','position']\n    Op=p_s1.pivot(columns='position',values='number')\n    Op['PlayId']=Offense_personal['PlayId']\n    #pivot s2-s4\n    columns=list(Op_split)\n    columns=columns[1:4]\n    for i in columns:\n        new=Op_split[i].str.split(' ',expand=True)\n        new=new.drop(new.columns[0], axis=1)\n        new.columns=['number','position']\n        temp=new.pivot(columns='position',values='number')\n        temp['PlayId']=Offense_personal['PlayId']\n        Op=Op.merge(temp,on='PlayId',suffixes=('_left', '_right'))\n    #pivot s5\n    s5=Op_split.s5.str.split(' ',expand=True)\n    s5.columns=['number','position']\n    temp=s5.pivot(columns='position',values='number')\n    temp['PlayId']=Offense_personal['PlayId']\n    temp=temp.drop(temp.columns[0],axis=1)\n    Op=Op.merge(temp,on='PlayId',suffixes=('_left', '_right'))\n    #Cleaning the data frame\n    Op=Op.replace({np.nan: 0})\n    Op=Op.drop([np.nan],axis=1)\n    Op=Op.apply(pd.to_numeric)\n    Op['RB']=Op['RB_left']+Op['RB_right']\n    Op['TE']=Op['TE_left']+Op['TE_right']\n    Op['WR']=Op['WR_left']+Op['WR_right']\n    Op=Op.drop(['RB_left','RB_right'],axis=1)\n    Op=Op.drop(['TE_left','TE_right'],axis=1)\n    Op=Op.drop(['WR_left','WR_right'],axis=1)\n    \n    return Op\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 调试模块\n#import copy\n#df = copy.deepcopy(train)\n#df = Clean_Op(df)\n#df = dis_max(df)\n#df.head(30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dis_max(df):\n    team_Max_distance=df.groupby(['PlayId','Team'],as_index=False).agg({'X':['max','min'],'Y':['max','min']})\n    team_Max_distance.columns=['PlayId','Team','X_max','X_min','Y_max','Y_min']\n    team_Max_distance.head()\n    team_Max_distance['max_X_distance']=team_Max_distance['X_max']-team_Max_distance['X_min']\n    team_Max_distance['max_Y_distance']=team_Max_distance['Y_max']-team_Max_distance['Y_min']\n    team_Max_distance2=team_Max_distance[['PlayId','Team','max_X_distance','max_Y_distance']]\n    df=pd.merge(df,team_Max_distance2,how=\"inner\",on=['PlayId','Team'])\n    return df\n\ntrain = dis_max(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def define_team_average_age(df):\n    a2 = pd.to_datetime(df['PlayerBirthDate']).dt.year\n    a3 = pd.to_datetime(df['TimeHandoff']).dt.year\n    a4 = a3-a2\n    df['age'] = np.array(a4)\n    team_average_age=df.groupby(['PlayId','Team'],as_index=False)['age'].mean()\n    team_average_age.columns=['PlayId','Team','team_avg_age']\n    df=pd.merge(df,team_average_age,how=\"inner\",on=['PlayId','Team'])\n    df=df.drop('age',axis=1)\n    return df\n\ntrain = define_team_average_age(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def define_bmi(df):\n    df['PlayerHeight'] = df['PlayerHeight'].apply(lambda x: 12*int(x.split('-')[0])+int(x.split('-')[1]))\n    df['PlayerBMI'] = 703*(df['PlayerWeight']/(df['PlayerHeight'])**2)\n    df=df.drop(['PlayerHeight','PlayerWeight'],axis=1)\n    return df\n\n# 给每个队编号，有可能方便后面pivot\ndef append_player_number(df):\n    player_num = []\n    for i in range(0,len(df)):\n        if i+1 <= 11:\n            player_num.append(i+1)\n        else:\n            player_num.append(i%11+1)\n    df['player_num'] = np.array(player_num)\n\ntrain = define_bmi(train)\nappend_player_number(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def define_Top10UniversityAlumni(df):\n    #According to Pro-football-reference \n    Top10University = [\"Notre Dame\",\"USC\",\"Ohio State\",\"Penn State\",\"Michigan\",\"Nebraska\",\"Oklahoma\",\"Alabama\",\"Miami\"]\n    gg=[]\n    for i in df['PlayerCollegeName']:\n        if i in Top10University:\n            gg.append(1)\n        else:\n            gg.append(0)\n    df['Alumni'] = gg\n    GroupTop10U = df.groupby(['PlayId','Team'],as_index=False).agg({'Alumni':['sum']})\n    GroupTop10U.columns=['PlayId','Team','SumTop10UniversityAlumni']\n    df=pd.merge(df,GroupTop10U,how=\"inner\",on=['PlayId','Team'])\n    df=df.drop('Alumni',axis=1)\n    return df\n\ntrain = define_Top10UniversityAlumni(train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_cat_features(df):\n    cat_features = []\n    for col in df.columns:\n        if df[col].dtype == 'object':\n            cat_features.append(col)\n    #cat_features = [x for x in cat_features if x not in ('fieldPosition','StadiumType','GameWeather')]\n    cat_features.append('NflId')\n    df = df.drop(cat_features, axis=1)\n    return df\n\n# 找每行是unique值的column;player_col里面就存了这些columns\ndef find_uni_col(df):\n    uni_col = []\n    for col in df.columns:\n        if df[col][:11].unique().shape[0]!=1:\n            uni_col.append(col)\n    uni_col.append('PlayId')\n    return uni_col","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_data(df):\n    df.fillna(-999, inplace=True)\n    #inplace : boolean, 默认值 False。如果为Ture,在原地填满。\n    #注意：这将修改次对象上的任何其他视图（例如，DataFrame中的列的无复制贴片）\n    \n    #添加X,Y的spread:    \n    team_spread=df.groupby(['PlayId','Team'],as_index=False).agg({'X':['std'],'Y':['std']})\n    team_spread.columns=['PlayId','Team','team_Xspread','team_Yspread']\n    df=pd.merge(df,team_spread,how=\"inner\",on=['PlayId','Team'])\n    \n    #按照offense和defense分组\n    df_offense = df[(df['Offense']=='offense')]\n    df_defense = df[(df['Offense']=='defense')]\n    \n    #light GBM可以直接使用categorical features，所以是不是不用删掉categorical features?\n    df_offense = remove_cat_features(df_offense)\n    df_defense = remove_cat_features(df_defense)\n    \n    uni_a = find_uni_col(df_offense)\n    uni_a.remove('JerseyNumber')\n    uni_a = [x for x in uni_a if x not in ('YardLine','Down','Distance','Yards')]\n    \n    df_unique_offense = df_offense[uni_a]\n    df_unique_defense = df_defense[uni_a]\n    \n    uni_a.remove('PlayId')\n    df_no_unique_offense = df_offense.drop(uni_a+['JerseyNumber','GameId'], axis=1)\n    df_no_unique_defense = df_defense.drop(uni_a+['JerseyNumber','GameId'], axis=1)\n    \n    #example_unique=example_unique.drop(['PlayId'],axis=1)\n    \n    # 注意这里的player_num其实是必要的\n    df_uni_piv_offense = df_unique_offense.pivot(index='PlayId', columns='player_num')\n    df_uni_piv_defense = df_unique_defense.pivot(index='PlayId', columns='player_num')\n    \n    df_no_unique_offense = df_no_unique_offense.drop_duplicates(subset='PlayId')\n\n    df_no_unique_defense = df_no_unique_defense.drop_duplicates(subset='PlayId')\n    \n    df_clean_offense = pd.merge(df_uni_piv_offense,df_no_unique_offense,how='inner',on='PlayId')\n    #不需要df_clean_defense=pd.merge(df_uni_piv_defense,df_no_unique_defense,how='inner',on'PlayId)因为\n    #如果这么做的话，后面再做df_clean的时候会重复列出场地信息\n    \n    df_clean = pd.merge(df_clean_offense,df_uni_piv_defense,how='inner',on='PlayId')\n    \n    return df_clean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = clean_data(train)\ntrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_columns=train.columns.values.tolist() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_columns\nall_columns.remove('PlayId')\nall_columns.remove('Yards')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 要先把X和Y分开\nX_train=pd.DataFrame(data=train,columns=all_columns)\ny_train = np.array([train['Yards'][i] for i in range(0,23171)])\n\n#scaler = StandardScaler()\n#X_train = scaler.fit_transform(X_train)\ny = y_train\ntarget = y[np.arange(0, len(train), 22)]\nstandard_deviation = np.std(target)\nscaler = StandardScaler()\n# 去掉categorical features的一个原因就是，scale的时候不去掉会有问题\nX_train = scaler.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom tqdm import tqdm\n\n\nparams = {\n            'boosting_type': 'gbdt',\n            'objective': 'regression',\n            'metric': 'mae',\n            'learning_rate': 0.1,\n            'num_iterations': 500,\n            'verbosity': -1, \n            \"boost_from_average\" : False,\n            'num_leaves': 44,\n            'bagging_fraction': 0.8,\n            'bagging_freq': 3,\n            'min_child_samples': 43,\n            'n_estimators': 300,\n            'feature_fraction': 0.9,\n            'lambda_l1': 0.13413394854686794,  \n            'lambda_l2': 0.0009122197743451751,\n            'random_state': 42\n            }\n\nfolds = 5\nseed = 999\n\nkf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n\nmodels = []\n\nfor train_index, val_index in kf.split(X_train, y_train):\n    train_X = X_train[train_index]\n    val_X = X_train[val_index]\n    train_y = y_train[train_index]\n    val_y = y_train[val_index]\n    lgb_train = lgb.Dataset(train_X, train_y)\n    lgb_eval = lgb.Dataset(val_X, val_y)\n    gbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=50,\n                valid_sets=(lgb_train, lgb_eval),\n                early_stopping_rounds=200,\n                verbose_eval = 50)\n    models.append(gbm)\n\nprint(models)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# 12/2/2019 对怎么调参数结果都不变这个问题的研究\nimport scipy\n# 首先搞一个自己的test_df:\nfake_test_df = X_train[:1]\n\nfake_y_pred = np.mean([model.predict(fake_test_df, num_iteration=model.best_iteration) for model in models],axis=0)\n#fake_y_pred = np.clip(np.cumsum(fake_y_pred, axis=1), 0, 1).tolist()[0]\n\nfake_pred_df = np.zeros((1, 199))  # 1是number of rows, 199是number of columns\ncurrent_cdf = scipy.stats.norm(loc = fake_y_pred, scale = standard_deviation).cdf(-98)\nfake_pred_df[0][1]\n\nfor A in range(len(fake_pred_df[0])):\n    current_cdf = scipy.stats.norm(loc = fake_y_pred, scale = standard_deviation).cdf(A-99)\n    fake_pred_df[0][A] = current_cdf\n\nlen(current_cdf)\n    \nfake_final_pred_df = pd.DataFrame(data=fake_pred_df)\nfake_final_pred_df"},{"metadata":{},"cell_type":"markdown","source":"num iteration: 1000\nnum of fold = 5\nearly stop = 300\ntraining's l1: 3.4802\tvalid_1's l1: 3.51941\n\nnum iteration: 500\nfold = 5\nearly story = 200\ntraining's l1: 3.4802\tvalid_1's l1: 3.51941\n\nnum iteration: 500\nfold = 10\nearly stop = 200\ntraining's l1: 3.47776\tvalid_1's l1: 3.50066\ntraining's l1: 3.48447\tvalid_1's l1: 3.4561\ntraining's l1: 3.47314\tvalid_1's l1: 3.49199\n"},{"metadata":{},"cell_type":"markdown","source":"最早的参数：\nparams = {\n            'boosting_type': 'gbdt',\n            'objective': 'regression',\n            'metric': {'l2_root'},\n            'subsample': 0.25,\n            'subsample_freq': 1,\n            'learning_rate': 0.1,\n            'num_iterations': 500,\n            'num_leaves': 31,\n            'feature_fraction': 0.8,\n            'lambda_l1': 1,  \n            'lambda_l2': 1\n            }"},{"metadata":{},"cell_type":"markdown","source":"11/25/2019 3:20pm 改动之前的参数：\nparams = {\n            'boosting_type': 'gbdt',\n            'objective': 'regression',\n            'metric': 'mae',\n            'learning_rate': 0.005,\n            'num_iterations': 500,\n            'verbosity': -1, \n            \"boost_from_average\" : False,\n            'num_leaves': 44,\n            'bagging_fraction': 0.9999128827046064,\n            'bagging_freq': 3,\n            'min_child_samples': 43,\n            'n_estimators': 300,\n            'feature_fraction': 0.4271070738920401,\n            'lambda_l1': 0.13413394854686794,  \n            'lambda_l2': 0.0009122197743451751,\n            'random_state': 42\n            }\n改动过的参数结果较好\n\n11/25/2019 3:51pm \n尝试用multiclass这个objective funciton; 同时设置number of class:199，改变metric为multi logloss;\n出现错误：Label must be in [0, 199), but found -4 in label"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tqdm\nimport scipy\nimport scipy.stats as st\n    \nbatch_no = 0\nfor (test_df, sample_prediction_df) in tqdm.tqdm(env.iter_test()):\n    try:\n        dist_to_end_test = test_df.apply(lambda x:(100 - x.loc['YardLine']) if x.loc['own_field']==1 else x.loc['YardLine'],axis=1)\n        test_df['WindSpeed'] = test_df['WindSpeed'].apply(lambda x: x.lower().replace('mph', '').strip() if not pd.isna(x) else x)\n        test_df['WindSpeed'] = test_df['WindSpeed'].apply(lambda x: (int(x.split('-')[0])+int(x.split('-')[1]))/2 if not pd.isna(x) and '-' in x else x)\n        test_df['WindSpeed'] = test_df['WindSpeed'].apply(lambda x: (int(x.split()[0])+int(x.split()[-1]))/2 if not pd.isna(x) and type(x)!=float and 'gusts up to' in x else x)\n        test_df['WindSpeed'] = test_df['WindSpeed'].apply(str_to_float)\n    \n        test_df['WindDirection'] = test_df['WindDirection'].apply(clean_WindDirection)\n        test_df['WindDirection'] = test_df['WindDirection'].apply(transform_WindDirection)\n    \n        Turf = {'Field Turf':'Artificial', 'A-Turf Titan':'Artificial', 'Grass':'Natural', 'UBU Sports Speed S5-M':'Artificial', \n        'Artificial':'Artificial', 'DD GrassMaster':'Artificial', 'Natural Grass':'Natural', \n        'UBU Speed Series-S5-M':'Artificial', 'FieldTurf':'Artificial', 'FieldTurf 360':'Artificial', 'Natural grass':'Natural', 'grass':'Natural', \n        'Natural':'Natural', 'Artifical':'Artificial', 'FieldTurf360':'Artificial', 'Naturall Grass':'Natural', 'Field turf':'Artificial', \n        'SISGrass':'Artificial', 'Twenty-Four/Seven Turf':'Artificial', 'natural grass':'Natural'} \n\n        test_df['Turf'] = test_df['Turf'].map(Turf)\n        test_df['Turf'] = test_df['Turf'] == 'Natural'\n\n    # solve team name encoding problem\n        map_abbr = {'ARI': 'ARZ', 'BAL': 'BLT', 'CLE': 'CLV', 'HOU': 'HST'}\n        for abb in test_df['PossessionTeam'].unique():\n            map_abbr[abb] = abb\n    \n        test_df['PossessionTeam'] = test_df['PossessionTeam'].map(map_abbr)\n        test_df['HomeTeamAbbr'] = test_df['HomeTeamAbbr'].map(map_abbr)\n        test_df['VisitorTeamAbbr'] = test_df['VisitorTeamAbbr'].map(map_abbr)\n\n    # Before pivot:\n    # Creat: IsBallCarrier, ToLeft, std_x, std_y, offense, age, bmi, player number\n    #train['IsBallCarrier'] = train['NflId'] == train['NflIdRusher']\n        test_df['ToLeft'] = test_df['PlayDirection'] == 'left'\n    \n        test_df = define_offense(test_df)\n        test_df = bye_XY(test_df)\n        test_df = dis_max(test_df)\n        test_df = define_team_average_age(test_df)\n    \n        test_df = define_bmi(test_df)\n        append_player_number(test_df)\n        test_df = define_Top10UniversityAlumni(test_df)\n    \n    ## final test data:\n        test_df = clean_data(test_df)\n        test_df = scaler.fit_transform(test_df)\n        y_pred = np.mean([model.predict(test_df, num_iteration=model.best_iteration) for model in models],axis=0)\n        #y_pred = np.clip(np.cumsum(y_pred, axis=1), 0, 1).tolist()[0] #这一行好像有问题\n        \n        #y_pred_p = models.predict(test_df)\n        #y_pred_first = y_pred_p[0]\n        \n    except:\n        #y_pred_first = 1\n        y_pred = 1\n\n    pred_df = np.zeros((1, 199))  \n    for A in range(len(pred_df[0])):\n        current_cdf = scipy.stats.norm(loc = y_pred, scale = standard_deviation).cdf(A-99)\n        pred_df[0][A] = current_cdf\n        \n   #pred_df[0][:80] = 0\n\n    final_pred_df = pd.DataFrame(data=pred_df, columns=sample_prediction_df.columns)\n    env.predict(final_pred_df)\n    batch_no += 1\n\nenv.write_submission_file()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"def get_score(y_pred,cdf,w,dist_to_end):\n    y_pred = int(y_pred)\n    if y_pred ==w:\n        y_pred_array = cdf.copy()\n    elif y_pred - w >0:\n        y_pred_array = np.zeros(199)\n        y_pred_array[(y_pred-w):] = cdf[:(-(y_pred-w))].copy()\n    elif w - y_pred >0:\n        y_pred_array = np.ones(199)\n        y_pred_array[:(y_pred-w)] = cdf[(w-y_pred):].copy()\n    y_pred_array[-1]=1\n    y_pred_array[(dist_to_end+99):]=1\n    return y_pred_array\n\ndist_to_end_test = test_df.apply(lambda x:(100 - x.loc['YardLine']) if x.loc['own_field']==1 else x.loc['YardLine'],axis=1)\n\npred_value = 0\nfor model in models:\n    pred_value += model.predict(X_test)[0]/5\npred_data = list(get_score(pred_value,cdf,4,dist_to_end_test.values[0]))\npred_data = np.array(pred_data).reshape(1,199)\npred_target = pd.DataFrame(index = sample_prediction_df.index, \\\n                               columns = sample_prediction_df.columns, \\\n                               #data = np.array(pred_data))\n                               data = pred_data)\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}