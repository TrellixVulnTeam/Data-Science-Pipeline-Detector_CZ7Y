{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"> # Big Data Bowl Submission","metadata":{}},{"cell_type":"markdown","source":"Below is a compiled notebook explaining my approach to this year's Big Data Bowl competition, including the final code I used for the submission. While I submitted the project with a teammate on kaggle (Lee Sharpe, @LeeSharpeNFL on twitter), Lee had to take on additional work responsibilities right as the competition began, so all of the below is my code. However, early discussions with Lee on the problem were very valuable, and I'm grateful for his partnership throughout the competition.\n\nOf note - this is not the actual notebook I submitted on, as that only included the Neural Network and I wanted to show my earlier approach. However, the late submission on this project seems bugged, so you'll see at the bottom that this code no longer succesfully makes a prediction. This error is pointed out in the discussion board for this competition. \n\nPlease feel free to reach out to me @Chiefsanalytics on twitter if you have any questions!","metadata":{}},{"cell_type":"markdown","source":"## Set-up (imports, load data, etc)","metadata":{}},{"cell_type":"code","source":"pip install --upgrade ngboost\n","metadata":{"execution":{"iopub.status.busy":"2021-08-11T18:13:24.660973Z","iopub.execute_input":"2021-08-11T18:13:24.661234Z","iopub.status.idle":"2021-08-11T18:13:38.288812Z","shell.execute_reply.started":"2021-08-11T18:13:24.661203Z","shell.execute_reply":"2021-08-11T18:13:38.286967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\nimport time\nimport datetime\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold\n\nfrom keras.layers import Dense,Input,Flatten,concatenate,Dropout,Lambda,BatchNormalization\nfrom keras.models import Model\nimport keras.backend as K\nimport re\nfrom keras.losses import binary_crossentropy\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\nimport codecs\nfrom keras.utils import to_categorical\nfrom sklearn.metrics import f1_score\nfrom ngboost import NGBRegressor","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-11T18:13:45.67171Z","iopub.execute_input":"2021-08-11T18:13:45.671999Z","iopub.status.idle":"2021-08-11T18:13:45.680102Z","shell.execute_reply.started":"2021-08-11T18:13:45.671956Z","shell.execute_reply":"2021-08-11T18:13:45.678539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Set display options for our dataframes\npd.set_option('display.max_columns', 25)\npd.set_option('display.max_rows', 100)\n#Silence the warning for chained assignments\npd.set_option('mode.chained_assignment', None)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-08-11T18:13:46.958446Z","iopub.execute_input":"2021-08-11T18:13:46.95882Z","iopub.status.idle":"2021-08-11T18:13:46.964997Z","shell.execute_reply.started":"2021-08-11T18:13:46.958756Z","shell.execute_reply":"2021-08-11T18:13:46.963772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ntrain_df = pd.read_csv('/kaggle/input/nfl-big-data-bowl-2020/train.csv', dtype={'WindSpeed': 'object'})\n\n#Get our outcome variables\noutcomes = train_df[['GameId','PlayId','Yards']].drop_duplicates()\n\n#Look at number of plays in our sample\nprint(len(outcomes))","metadata":{"execution":{"iopub.status.busy":"2021-08-11T18:13:47.103399Z","iopub.execute_input":"2021-08-11T18:13:47.103673Z","iopub.status.idle":"2021-08-11T18:13:54.170935Z","shell.execute_reply.started":"2021-08-11T18:13:47.103642Z","shell.execute_reply":"2021-08-11T18:13:54.169601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kc = train_df.loc[(train_df['Stadium']=='Arrowhead Stadium')]\nkc.groupby(by=['WindDirection','GameId'])['PlayId'].count()","metadata":{"execution":{"iopub.status.busy":"2021-08-11T18:13:54.17299Z","iopub.execute_input":"2021-08-11T18:13:54.17325Z","iopub.status.idle":"2021-08-11T18:13:54.241933Z","shell.execute_reply.started":"2021-08-11T18:13:54.173207Z","shell.execute_reply":"2021-08-11T18:13:54.240216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## First Attempt","metadata":{}},{"cell_type":"markdown","source":"I tried a few different approaches when I started on this project. Having read a decent amount of public work in the NFL analytics community with regards to running back, my first thought was to see if we could get a decent prediction just by estimating the probability density function of all the runs in our data base, and just submitting that as our prediction. In other words, assuming that there's no single to be found here - and that our \"base rate\" is our best guess. This actually performed decently, putting me second on the leaderboard after 48 hours, and with some minor improvements (binning by defenders in the box and yards to go, as you'll see below) held up in the top 5 after a week or so. From there, all the improvements are marginal - but as is the case with kaggle competitions, margins are everything. So I adopted a more sophisticated approach (a neural network) after reading the discussion boards. ","metadata":{}},{"cell_type":"code","source":"#Get how many defenders are in the box - I found that binning the outliers (<5 defenders and >9 defenders) worked well\ntrain_df['Simple_Box'] = train_df['DefendersInTheBox']\ntrain_df.loc[(train_df['DefendersInTheBox'].isnull()),'Simple_Box'] = 6\ntrain_df.loc[(train_df['DefendersInTheBox']<=5),'Simple_Box'] = 5\ntrain_df.loc[(train_df['DefendersInTheBox']>=9),'Simple_Box'] = 9\ntrain_df['Simple_Box'] = train_df['Simple_Box'].astype(str)\n\n#Create our \"predictions\"\nyds = np.linspace(-99,99,199)\nbox = train_df['Simple_Box'].unique()\nbox_cdf = {}\nbox_pdf = {}\nfor n in box:\n    df = train_df.loc[(train_df['Simple_Box']==n)]\n    data = df['Yards']\n    kde1 = stats.gaussian_kde(data)\n    predict_pdf = kde1(yds)\n    box_pdf[n] = predict_pdf\n    predict_cdf = np.cumsum(predict_pdf)\n    predict_cdf = np.clip(predict_cdf,0,1)\n    #This last line gives us a dictionary where the keys are the number of defenders in the box,\n    #and the values are a 199 length array (the probability of each yard gained)\n    #This allows us to just check the testing data for how many defenders in the box, \n    box_cdf[n] = predict_cdf","metadata":{"execution":{"iopub.status.busy":"2021-08-11T18:13:54.243843Z","iopub.execute_input":"2021-08-11T18:13:54.244142Z","iopub.status.idle":"2021-08-11T18:13:58.853943Z","shell.execute_reply.started":"2021-08-11T18:13:54.244105Z","shell.execute_reply":"2021-08-11T18:13:58.853009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.style.use('seaborn-talk')\nplt.style.use('seaborn-darkgrid')\nfor box in sorted(box_pdf):\n    plt.plot(yds,box_cdf[box],label=box)\n    \n\nplt.xlim(-10,30)\nplt.legend(title='PDF by Defenders in the Box')\nplt.ylabel('Cumulative Probability')\nplt.xlabel('Expected Yards Gained')","metadata":{"execution":{"iopub.status.busy":"2021-08-11T18:13:58.854969Z","iopub.execute_input":"2021-08-11T18:13:58.855214Z","iopub.status.idle":"2021-08-11T18:13:59.095111Z","shell.execute_reply.started":"2021-08-11T18:13:58.855169Z","shell.execute_reply":"2021-08-11T18:13:59.094379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, running into more defenders is bad - running into fewer defenders is better. However, what if this is only a function of distance? In other words, when the offense needs 1 yard to go, the defense \"stacks\" the box. To investigate this I added an additional factor, binning on yards to go.","metadata":{}},{"cell_type":"code","source":"#Repeat the same process, this time binning by both defenders in the box AND yards to go\n\n#These numbers seemed to be the most predictive.\ntrain_df.loc[(train_df['Distance']<=8),'binned_yards'] = 'med'\ntrain_df.loc[(train_df['Distance']<=3),'binned_yards'] = 'short'\n# train_df.loc[(train_df['Distance']<=1),'binned_yards'] = 'really short'\ntrain_df.loc[(train_df['Distance']>8),'binned_yards'] = 'long'\n\ntrain_df['box_yards'] = train_df['Simple_Box'] + ' ' + train_df['binned_yards']\n\nbox = train_df['box_yards'].unique()\nbox_cdf = {}\nbox_pdf = {}\nfor n in box:\n    df = train_df.loc[(train_df['box_yards']==n)]\n    print('There are ' + str(len(df)/22) + ' plays for ' + n)\n    data = df['Yards']\n    kde1 = stats.gaussian_kde(data)\n    predict_pdf = kde1(yds)\n    box_pdf[n] = predict_pdf\n    predict_cdf = np.cumsum(predict_pdf)\n    predict_cdf = np.clip(predict_cdf,0,1)\n    box_cdf[n] = predict_cdf","metadata":{"execution":{"iopub.status.busy":"2021-08-11T18:13:59.097618Z","iopub.execute_input":"2021-08-11T18:13:59.098051Z","iopub.status.idle":"2021-08-11T18:14:04.774175Z","shell.execute_reply.started":"2021-08-11T18:13:59.098017Z","shell.execute_reply":"2021-08-11T18:14:04.773474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.style.use('seaborn-talk')\nplt.style.use('seaborn-darkgrid')\nfor box in sorted(box_pdf):\n    plt.plot(yds,box_cdf[box],label=box)\n    \n\nplt.xlim(-10,30)\nplt.legend(title='Defenders in the Box + Yards to Go')\nplt.ylabel('Cumulative Probability')\nplt.xlabel('Expected Yards Gained')","metadata":{"execution":{"iopub.status.busy":"2021-08-11T18:14:04.777793Z","iopub.execute_input":"2021-08-11T18:14:04.778742Z","iopub.status.idle":"2021-08-11T18:14:05.055857Z","shell.execute_reply.started":"2021-08-11T18:14:04.778694Z","shell.execute_reply":"2021-08-11T18:14:05.054613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again, despite the very simplistic approach, this model performed very well in the competition from an \"absolute\" perspective. It had a CRPS of 0.014, which while only good for about 50th percentile by the end of the competition, most of the gains in the top half of the leaderboard are marginable and likely carry little practical relevance. The \"benchmark\" model - predicting every run would be the league average run of 3 yards - had a CRPS of 0.018, while the 2nd place model was 0.012 (1st was 0.0116). In other words, this simplistic approach got us ~2/3 of the way there. \n\nThere are a few different reasons I think this simple method worked so well. The biggest is that we were only given tracking data at the time of handoff to the running back. There is only so much information that can be gleaned from about 1 second into the snap that we didn't already know before the play. Second, while there are many complexities to running the football, the overarching goal is to run where a defender is not. By looking at how many defenders are \"in the box\" - you can think of this as close to the action, ready to defend the run - we get a pretty good idea of how prepared the defense was for this play. And, because the runner and the runner's team are aware that their goal is to get a 1st down - knowing how many yards were left for that goal was highly predictive of how many yards they gained.\n\nNow, on to the more complex model!","metadata":{}},{"cell_type":"markdown","source":"## Feature Engineering Functions","metadata":{}},{"cell_type":"markdown","source":"The approach I took here was to focus largely on player positions and speed. My intuition was that other than a few extreme examples - the end of a game for instance, when players may not be aiming to gain as many yards as possible - the \"kinematics\" of the play (where the players are and where they are moving to) is what would really matter. This ended up being spot on, as the winner of the contest focused solely on these metrics - but used a much better modeling approach (a Convulutional Neural Network). The features derived here reflect that focus - they are simply where each player is relative to eachother, where the players are moving, and how fast they're moving. Some other metrics are calculated from that, like \"time to tackle\" - how long it would take a defensive player to tackle the runner, assuming they move straight for the runner at a constant speed.\n\nSince I did not come up with the clever solution of a CNN, I had to somehow add structure to my data so that the model would consistently be evaluating the \"same\" player in each column. My approach to this was to order the defensive players by their distance to the rusher. For offense, while I tried this approach, it actually did not help the model at all. Simple aggregate meaasures of offensive players was all I needed. ","metadata":{}},{"cell_type":"code","source":"#Function to create the features we want to create for each play - and we won't use our outcomes for when we are testing, \n#hence the need for this boolean\ndef create_features(df, test=False):\n    \n    \n    #Function to standardize the play-by-play coordinates\n    def standardize(df):\n        \n        #Create a binary variable to use for play direction\n        df['ToLeft'] = df.PlayDirection == \"left\"\n        #Binary variable for who is the ball carrier - important for our predictions\n        df['IsBallCarrier'] = df.NflId == df.NflIdRusher\n        \n        #Fix some inconsistencies in team abbreviations betweeten columns\n        df.loc[df.VisitorTeamAbbr == \"ARI\", 'VisitorTeamAbbr'] = \"ARZ\"\n        df.loc[df.HomeTeamAbbr == \"ARI\", 'HomeTeamAbbr'] = \"ARZ\"\n\n        df.loc[df.VisitorTeamAbbr == \"BAL\", 'VisitorTeamAbbr'] = \"BLT\"\n        df.loc[df.HomeTeamAbbr == \"BAL\", 'HomeTeamAbbr'] = \"BLT\"\n\n        df.loc[df.VisitorTeamAbbr == \"CLE\", 'VisitorTeamAbbr'] = \"CLV\"\n        df.loc[df.HomeTeamAbbr == \"CLE\", 'HomeTeamAbbr'] = \"CLV\"\n\n        df.loc[df.VisitorTeamAbbr == \"HOU\", 'VisitorTeamAbbr'] = \"HST\"\n        df.loc[df.HomeTeamAbbr == \"HOU\", 'HomeTeamAbbr'] = \"HST\"\n        \n        #Figure out if home or away team is on offense\n        df['TeamOnOffense'] = \"home\"\n        df.loc[df.PossessionTeam != df.HomeTeamAbbr, 'TeamOnOffense'] = \"away\"\n        #Binary variable for each player to see if they are on offense\n        df['IsOnOffense'] = df.Team == df.TeamOnOffense # Is player on offense?\n        #Create a standardized yardline - from 1-99, instead of 1-50 and 1-50 again (on other side of the field)\n        df['YardLine_std'] = 100 - df.YardLine\n        df.loc[df.FieldPosition.fillna('') == df.PossessionTeam,  \n                  'YardLine_std'\n                 ] = df.loc[df.FieldPosition.fillna('') == df.PossessionTeam,  \n                  'YardLine']\n        \n        df['YardLine'] = df['YardLine_std']\n        #Create standardized coordinates to each coordinate represents a distance away from the team's own goal,\n        #and so that all plays are moving the same direction (positive yardage is going towards the opponent's goal)\n        df['X_std'] = df.X\n        df.loc[df.ToLeft, 'X_std'] = 120 - df.loc[df.ToLeft, 'X'] \n        df['Y_std'] = df.Y\n        df.loc[df.ToLeft, 'Y_std'] = 160/3 - df.loc[df.ToLeft, 'Y']\n        df['X'] = df['X_std'] - 10\n        df['Y'] = df['Y_std']\n        \n        #Derive some standardized directions. \n        \n        #Standardize the direction of player's similar to what we did above for coordinates\n        #Reasoning for this is explained here:https://www.kaggle.com/statsbymichaellopez/nfl-tracking-wrangling-voronoi-and-sonars\n        #But basically here we have 180 degrees to be moving backward \n        #and 0 degrees to be moving directly downfield towards the offense\n        \n        #We are also converting to radians here.\n        df['Dir_rad'] = np.mod(90 - df.Dir, 360) * np.pi/180.0\n        df['Dir_std'] = df.Dir_rad\n        df.loc[df.ToLeft, 'Dir_std'] = np.mod(np.pi + df.loc[df.ToLeft, 'Dir_rad'], 2*np.pi)\n        df['Dir_rad'] = df['Dir_std']\n        \n        #Simple trig to get horizontal and vertical components of movement\n        df[\"Dir_y\"] = np.sin(df[\"Dir_rad\"])\n        df[\"Dir_x\"] = np.cos(df[\"Dir_rad\"])\n        #Get horizontal and vertical components of velocity\n        df['V_y'] = np.absolute(df['Dir_y'] * df['S'])\n        df['V_x'] = np.absolute(df['Dir_x'] * df['S'])\n  \n        #Since we recopied the column values with their standardized versions, we don't need these anymore\n        df.drop(columns=['X_std','Y_std',\n                        'Dir_std','YardLine_std'],inplace=True)\n        return df\n\n    #Features specific to the ball carrier\n    def back_features(df):\n        #Get ballcarrier features - how far back they are from the line of scrimmage, and their X and Y position\n        carriers = df[df['NflId'] == df['NflIdRusher']][['GameId','PlayId','NflIdRusher','X','Y','Orientation','Dir','YardLine','Position']]\n        carriers['back_from_scrimmage'] = carriers['YardLine'] - carriers['X']\n        carriers = carriers.rename(columns={'X':'back_X',\n                                            'Y':'back_Y',\n                                           'Position':'RusherPosition'})\n        carriers = carriers[['GameId','PlayId','NflIdRusher','RusherPosition','back_X','back_Y','back_from_scrimmage']]\n        \n        return carriers\n    \n    #Features relative to the ball carrier\n    def features_relative_to_back(df, carriers):\n        #Get positions of the rest of the offense and defense relative to the running back for every play\n        player_distance = df[['GameId','PlayId','NflId','X','Y']]\n        player_distance = pd.merge(player_distance, carriers, on=['GameId','PlayId'], how='inner')\n        player_distance = player_distance[player_distance['NflId'] != player_distance['NflIdRusher']]\n        \n        player_distance['X_dist_to_back'] = (player_distance['X'] - player_distance['back_X'])\n        player_distance['Y_dist_to_back'] = np.absolute(player_distance['Y'] - player_distance['back_Y'])\n        player_distance['dist_to_back'] = np.sqrt(player_distance['X_dist_to_back']**2 + player_distance['Y_dist_to_back']**2)\n        #Get some aggregated features of the rest of the players on this play\n        player_distance = player_distance.groupby(['GameId','PlayId','back_from_scrimmage'])\\\n                                         .agg({'dist_to_back':['min','max','mean','std'],\n                                              'X_dist_to_back':['min','max','mean','std'],\n                                              'Y_dist_to_back':['min','max','mean','std']})\\\n                                         .reset_index()\n        player_distance.columns = ['GameId','PlayId','back_from_scrimmage',\n                                   'min_dist','max_dist','mean_dist','std_dist',\n                                  'X_min_dist','X_max_dist','X_mean_dist','X_std_dist',\n                                  'Y_min_dist','Y_max_dist','Y_mean_dist','Y_std_dist']\n\n        return player_distance\n\n    def defense_features(df):\n        #Get rusher features\n        rusher = df[df['NflId'] == df['NflIdRusher']][['GameId','PlayId','Team','X','Y','S']]\n        rusher.columns = ['GameId','PlayId','RusherTeam','RusherX','RusherY','RusherS']\n\n        #Merge defense with rusher features\n        defense = pd.merge(df,rusher,on=['GameId','PlayId'],how='inner')\n        defense = defense[defense['Team'] != defense['RusherTeam']][['GameId','PlayId','X','Y',\n                                                                     'S','A',\n                                                                     'Dir','Dis',\n                                                                     'PlayerWeight',\n                                                                     'Orientation',\n                                                                     'V_x','V_y',\n                                                                     'RusherX','RusherY','RusherS']]\n        #Calculate dfense features\n#         defense['def_dist_to_back'] = defense[['X','Y','RusherX','RusherY']].apply(lambda x: euclidean_distance(x[0],x[1],x[2],x[3]), axis=1)\n        defense['X_def_dist_to_back'] = (defense['X'] - defense['RusherX'])\n        defense['Y_def_dist_to_back'] = np.absolute((defense['Y'] - defense['RusherY']))\n        defense['def_dist_to_back'] = np.sqrt(defense['X_def_dist_to_back']**2 + defense['Y_def_dist_to_back']**2)\n        \n        #Calculate the minimum time to tackle a ball carrier, based on how far away the closest defender is\n        defense['def_time_to_tackle'] = defense['def_dist_to_back'] / (defense['S'] + defense['RusherS'])\n        \n        \n        #Get 8 closest defenders\n        defense.sort_values(by=['GameId','PlayId','def_dist_to_back'],inplace=True)\n        defense.reset_index(inplace=True)\n        \n        #Defender 1\n        defense['defender1'] = 0\n        defense['defender1'].loc[np.arange(0, len(defense), 11)] = 1\n        defender1 = defense[defense['defender1'] == 1][['GameId','PlayId',\n                                         'X_def_dist_to_back','Y_def_dist_to_back',\n                                                        'def_dist_to_back',\n                                         'V_x','V_y','A']]\n        defender1 = defender1.rename(columns={'X_def_dist_to_back':'def1_X_dist',\n                                              'Y_def_dist_to_back':'def1_Y_dist',\n                                              'def_dist_to_back':'def1_dist',\n                                              'V_x':'def1_V_x',\n                                              'V_y':'def1_V_y',\n                                              'A':'def1_A',\n                                              'Orientation':'def1_orientation',\n                                              'Dir':'def1_Dir',\n                                              'Dis':'def1_Dis'\n                                             })\n        \n        #Defender 2\n        defense['defender2'] = 0\n        defense['defender2'].loc[np.arange(1, len(defense), 11)] = 1\n        defender2 = defense[defense['defender2'] == 1][['GameId','PlayId',\n                                         'X_def_dist_to_back','Y_def_dist_to_back',\n                                                        'def_dist_to_back',\n                                         'V_x','V_y','A']]\n        defender2 = defender2.rename(columns={'X_def_dist_to_back':'def2_X_dist',\n                                              'Y_def_dist_to_back':'def2_Y_dist',\n                                              'def_dist_to_back':'def2_dist',\n                                              'V_x':'def2_V_x',\n                                              'V_y':'def2_V_y',\n                                              'A':'def2_A',\n                                              'Orientation':'def2_orientation',\n                                              'Dir':'def2_Dir',\n                                              'Dis':'def2_Dis'\n                                             })\n        #Defender 3\n        defense['defender3'] = 0\n        defense['defender3'].loc[np.arange(2, len(defense), 11)] = 1\n        defender3 = defense[defense['defender3'] == 1][['GameId','PlayId',\n                                         'X_def_dist_to_back','Y_def_dist_to_back',\n                                                        'def_dist_to_back',\n                                         'V_x','V_y','A']]\n        defender3 = defender3.rename(columns={'X_def_dist_to_back':'def3_X_dist',\n                                              'Y_def_dist_to_back':'def3_Y_dist',\n                                              'def_dist_to_back':'def3_dist',\n                                              'V_x':'def3_V_x',\n                                              'V_y':'def3_V_y',\n                                              'A':'def3_A',\n                                              'Orientation':'def3_orientation',\n                                              'Dir':'def3_Dir',\n                                              'Dis':'def3_Dis'\n                                             })\n        \n        #Defender 4\n        defense['defender4'] = 0\n        defense['defender4'].loc[np.arange(3, len(defense), 11)] = 1\n        defender4 = defense[defense['defender4'] == 1][['GameId','PlayId',\n                                         'X_def_dist_to_back','Y_def_dist_to_back',\n                                                        'def_dist_to_back',\n                                         'V_x','V_y','A']]\n        defender4 = defender4.rename(columns={'X_def_dist_to_back':'def4_X_dist',\n                                              'Y_def_dist_to_back':'def4_Y_dist',\n                                              'def_dist_to_back':'def4_dist',\n                                              'V_x':'def4_V_x',\n                                              'V_y':'def4_V_y',\n                                              'A':'def4_A',\n                                              'Orientation':'def4_orientation',\n                                              'Dir':'def4_Dir',\n                                              'Dis':'def4_Dis'\n                                             })\n        \n        #Defender 5\n        defense['defender5'] = 0\n        defense['defender5'].loc[np.arange(4, len(defense), 11)] = 1\n        defender5 = defense[defense['defender5'] == 1][['GameId','PlayId',\n                                         'X_def_dist_to_back','Y_def_dist_to_back',\n                                                        'def_dist_to_back',\n                                         'V_x','V_y','A']]\n        defender5 = defender5.rename(columns={'X_def_dist_to_back':'def5_X_dist',\n                                              'Y_def_dist_to_back':'def5_Y_dist',\n                                              'def_dist_to_back':'def5_dist',\n                                              'V_x':'def5_V_x',\n                                              'V_y':'def5_V_y',\n                                              'A':'def5_A',\n                                              'Orientation':'def5_orientation',\n                                              'Dir':'def5_Dir',\n                                              'Dis':'def5_Dis'\n                                             })\n        \n        #Defender 6\n        defense['defender6'] = 0\n        defense['defender6'].loc[np.arange(5, len(defense), 11)] = 1\n        defender6 = defense[defense['defender6'] == 1][['GameId','PlayId',\n                                         'X_def_dist_to_back','Y_def_dist_to_back',\n                                                        'def_dist_to_back',\n                                         'V_x','V_y','A']]\n        defender6 = defender6.rename(columns={'X_def_dist_to_back':'def6_X_dist',\n                                              'Y_def_dist_to_back':'def6_Y_dist',\n                                              'def_dist_to_back':'def6_dist',\n                                              'V_x':'def6_V_x',\n                                              'V_y':'def6_V_y',\n                                              'A':'def6_A',\n                                              'Orientation':'def6_orientation',\n                                              'Dir':'def6_Dir',\n                                              'Dis':'def6_Dis'\n                                             })\n        #Defender 7\n        defense['defender7'] = 0\n        defense['defender7'].loc[np.arange(6, len(defense), 11)] = 1\n        defender7 = defense[defense['defender7'] == 1][['GameId','PlayId',\n                                         'X_def_dist_to_back','Y_def_dist_to_back',\n                                                        'def_dist_to_back',\n                                         'V_x','V_y','A']]\n        defender7 = defender7.rename(columns={'X_def_dist_to_back':'def7_X_dist',\n                                              'Y_def_dist_to_back':'def7_Y_dist',\n                                              'def_dist_to_back':'def7_dist',\n                                              'V_x':'def7_V_x',\n                                              'V_y':'def7_V_y',\n                                              'A':'def7_A',\n                                              'Orientation':'def7_orientation',\n                                              'Dir':'def7_Dir',\n                                              'Dis':'def7_Dis'\n                                             })\n        \n        #Defender 8\n        defense['defender8'] = 0\n        defense['defender8'].loc[np.arange(7, len(defense), 11)] = 1\n        defender8 = defense[defense['defender8'] == 1][['GameId','PlayId',\n                                         'X_def_dist_to_back','Y_def_dist_to_back',\n                                                        'def_dist_to_back',\n                                         'V_x','V_y','A']]\n        defender8 = defender8.rename(columns={'X_def_dist_to_back':'def8_X_dist',\n                                              'Y_def_dist_to_back':'def8_Y_dist',\n                                              'def_dist_to_back':'def8_dist',\n                                              'V_x':'def8_V_x',\n                                              'V_y':'def8_V_y',\n                                              'A':'def8_A',\n                                              'Orientation':'def8_orientation',\n                                              'Dir':'def8_Dir',\n                                              'Dis':'def8_Dis'\n                                             })\n    \n        #Perform Aggregate calculations\n        defense = defense.groupby(['GameId','PlayId'])\\\n                         .agg({'def_dist_to_back':['max','mean','std'],\n                              'def_time_to_tackle':['min','max','mean','std'],\n                              'X_def_dist_to_back':['min','max','mean','std'],\n                              'Y_def_dist_to_back':['min','max','mean','std'],\n                              'V_x':['min','max','mean'],\n                              'V_y':['min','max','mean']})\\\n                         .reset_index()\n        \n        #Rename Columns\n        defense.columns = ['GameId','PlayId','def_max_dist','def_mean_dist','def_std_dist',\n                           'def_min_ttt','def_max_ttt','def_mean_ttt','def_std_ttt',\n                          'X_def_min_dist','X_def_max_dist','X_def_mean_dist','X_def_std_dist',\n                          'Y_def_min_dist','Y_def_max_dist','Y_def_mean_dist','Y_def_std_dist',\n                          'X_def_min_vel','X_def_max_vel','X_def_mean_vel',\n                          'Y_def_min_vel','Y_def_max_vel','Y_def_mean_vel']\n    \n        #Some sensor values are way off - resulting in a player having a very high time to tackle (they may have registered a very small speed)\n        #To correct for this, we clip the time to tackles at reasonable values\n        defense['def_max_ttt'].clip(lower=0,upper=25,inplace=True)\n        defense['def_min_ttt'].clip(lower=0,upper=5,inplace=True)\n        defense['def_std_ttt'].clip(lower=0,upper=11,inplace=True)\n        defense['def_mean_ttt'].clip(lower=0,upper=11,inplace=True)\n        \n        #Merge with individual defender df\n        #This puts our defensive back features in order by distance to the rusher - in other words,\n        #For every play we know the time to tackle of the 1st through 8th closest defender\n        \n        #any more than that seemed to add noise\n        defense = pd.merge(defense,defender1,on=['GameId','PlayId'],how='inner')\n        defense = pd.merge(defense,defender2,on=['GameId','PlayId'],how='inner')\n        defense = pd.merge(defense,defender3,on=['GameId','PlayId'],how='inner')\n        defense = pd.merge(defense,defender4,on=['GameId','PlayId'],how='inner')\n        defense = pd.merge(defense,defender5,on=['GameId','PlayId'],how='inner')\n        defense = pd.merge(defense,defender6,on=['GameId','PlayId'],how='inner')\n        defense = pd.merge(defense,defender7,on=['GameId','PlayId'],how='inner')\n        defense = pd.merge(defense,defender8,on=['GameId','PlayId'],how='inner')\n        \n        return defense\n\n    #Do basically the same thing we did for the defense, but for offense\n    def offense_features(df):\n        rusher = df[df['NflId'] == df['NflIdRusher']][['GameId','PlayId','Team','X','Y','Position']]\n        rusher.columns = ['GameId','PlayId','RusherTeam','RusherX','RusherY','RusherPosition']\n\n        offense = pd.merge(df,rusher,on=['GameId','PlayId'],how='inner')\n       #TRY - Just to Min Gap, Max Gap, Average Gap -- since different number of gaps\n        offense = offense.loc[(offense['Team'] == offense['RusherTeam']) & \n                              (offense['NflId']!=offense['NflIdRusher']) & \n                              (offense['PlayerWeight']>=240)][['GameId','PlayId','X','Y','S',\n                                                               'V_x','V_y','Dir','RusherX',\n                                                               'A',\n                                                               'RusherY','Position',\n                                                               'RusherPosition','PlayerWeight']]\n\n        offense['X_off_dist_to_back'] = (offense['X'] - offense['RusherX'])\n        offense['Y_off_dist_to_back'] = np.absolute((offense['Y'] - offense['RusherY']))\n        offense['off_dist_to_back'] = np.sqrt(offense['X_off_dist_to_back']**2 + offense['Y_off_dist_to_back']**2)\n        offense = offense.groupby(['GameId','PlayId'])\\\n                        .agg({'off_dist_to_back':['min','max','mean','std'],\n                             'V_x':['min','max','mean'],\n                             'V_y':['min','max','mean']})\\\n                        .reset_index()\n        offense.columns = ['GameId','PlayId',\n                           'off_min_dist','off_max_dist',\n                           'off_mean_dist','off_std_dist',\n                          'X_off_min_vel','X_off_max_vel','X_off_mean_vel',\n                          'Y_off_min_vel','Y_off_max_vel','Y_off_mean_vel']\n#         offense = pd.merge(offense,df3,on=['GameId','PlayId'],how='inner')\n\n        return offense\n    \n    def play_features(df):\n        \n        \n        add_new_feas = []\n\n        ## Height\n        df['PlayerHeight_dense'] = df['PlayerHeight'].apply(lambda x: 12*int(x.split('-')[0])+int(x.split('-')[1]))\n        \n        add_new_feas.append('PlayerHeight_dense')\n\n\n        #Get the score differential before the play\n        df[\"diffScoreBeforePlay\"] = df[\"HomeScoreBeforePlay\"] - df[\"VisitorScoreBeforePlay\"]\n        df.loc[(df['TeamOnOffense']=='away'), 'diffScoreBeforePlay'] = -1*df.diffScoreBeforePlay\n        add_new_feas.append(\"diffScoreBeforePlay\")\n        \n        #Get all of our \"play\" features\n        play_features = df[df['NflId'] == df['NflIdRusher']][add_new_feas+['GameId','PlayId','X','Y','S','A','Dis',\n                                                                             'Orientation','Dir_rad','Dir_y','Dir_x',\n                                                                             'V_y','V_x',\n                                                                             'YardLine','Quarter',\n                                                                             'Down','Distance',\n                                                                             'DefendersInTheBox']].drop_duplicates()\n\n        #Fill our missing values to prevent errors\n        play_features.fillna(-999,inplace=True)\n\n            \n\n        return play_features\n\n\n    #Create a function to merge all of these engineered features into one dataframe\n    def combine_features(relative_to_back, offense, defense, play, test=test):\n        df = pd.merge(relative_to_back,defense,on=['GameId','PlayId'],how='inner')\n        df = pd.merge(df,offense,on=['GameId','PlayId'],how='inner')\n        df = pd.merge(df,play,on=['GameId','PlayId'],how='inner')\n\n        if not test:\n            df = pd.merge(df, outcomes, on=['GameId','PlayId'], how='inner')\n\n        return df\n    \n    \n    #Standardize features\n    df = standardize(df)\n    #Get RB features\n    back_feats = back_features(df)\n    rel_back = features_relative_to_back(df, back_feats)\n    #defensive players\n    def_feats = defense_features(df)\n    #offensive players\n    off_feats = offense_features(df)\n    #play features\n    play_feats = play_features(df)\n    #Get our table\n    feature_table = combine_features(rel_back, off_feats, def_feats, play_feats, test=test)\n    #Plays that use fewer than 4 or more than 11 defenders in the box are anamolies and add noise\n    feature_table['DefendersInTheBox'].clip(lower=4,upper=11,inplace=True)\n    #I've found that dropping these columns improves my predictions - \n    #hey just add noise to the model, and are likely accounted for with another combination of features\n    feature_table.drop(columns=['Down','Quarter','X_def_max_dist',\n                            'Y_def_max_dist','Y_def_std_dist',\n                            'Y_max_dist','Y_mean_dist',\n                            'Y_std_dist','std_dist'],inplace=True)\n    return feature_table","metadata":{"execution":{"iopub.status.busy":"2021-08-11T18:18:07.041488Z","iopub.execute_input":"2021-08-11T18:18:07.041847Z","iopub.status.idle":"2021-08-11T18:18:07.176735Z","shell.execute_reply.started":"2021-08-11T18:18:07.041807Z","shell.execute_reply":"2021-08-11T18:18:07.175373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%time train_feats = create_features(train_df, False)\n\n#May have some infinities from dividing by something close to 0 - make these nan\ntrain_feats.replace([np.inf, -np.inf], np.nan, inplace=True)\n#remove any rows with nans we have to prevent the model not running\ntrain_feats = train_feats.dropna()","metadata":{"execution":{"iopub.status.busy":"2021-08-11T18:18:07.178561Z","iopub.execute_input":"2021-08-11T18:18:07.17893Z","iopub.status.idle":"2021-08-11T18:18:19.525214Z","shell.execute_reply.started":"2021-08-11T18:18:07.17887Z","shell.execute_reply":"2021-08-11T18:18:19.523232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Get a list of all our features we engineered\nlist(train_feats)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T18:18:19.527032Z","iopub.execute_input":"2021-08-11T18:18:19.527364Z","iopub.status.idle":"2021-08-11T18:18:19.536256Z","shell.execute_reply.started":"2021-08-11T18:18:19.527307Z","shell.execute_reply":"2021-08-11T18:18:19.535417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Show the first few plays\ntrain_feats.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T18:18:19.539109Z","iopub.execute_input":"2021-08-11T18:18:19.539443Z","iopub.status.idle":"2021-08-11T18:18:19.588352Z","shell.execute_reply.started":"2021-08-11T18:18:19.539394Z","shell.execute_reply":"2021-08-11T18:18:19.587413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Get our explanatory variables\nX = train_feats.copy()\n#Sort them by yards gained\nX.sort_values(by='Yards',inplace=True)\nyards = np.array(X.Yards)\n\n\n#We can't have a 99 yard run here, or our next step won't work (will be out of index)\n# - it's not important to have an exact 99 yard run, as this is an incredibly rare event (just 1 in our data)\n# and no different in practicality than a 98 yard run\nyards = np.clip(yards,-14,98)\n\n#Assign probablity values to the run\ny = np.zeros((yards.shape[0], 199))\nfor idx, target in enumerate(list(yards)):\n    y[idx][98 + target] = 0.1\n    y[idx][99 + target] = 0.8\n    y[idx][100 + target] = 0.1\n\nX.drop(['GameId','PlayId','Yards'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T18:18:19.590197Z","iopub.execute_input":"2021-08-11T18:18:19.590503Z","iopub.status.idle":"2021-08-11T18:18:19.758149Z","shell.execute_reply.started":"2021-08-11T18:18:19.590457Z","shell.execute_reply":"2021-08-11T18:18:19.75742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Standarize our variables. This helps our model train quicker\nscaler = StandardScaler()\nX = scaler.fit_transform(X)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T18:14:18.122761Z","iopub.execute_input":"2021-08-11T18:14:18.123137Z","iopub.status.idle":"2021-08-11T18:14:18.176172Z","shell.execute_reply.started":"2021-08-11T18:14:18.123069Z","shell.execute_reply":"2021-08-11T18:14:18.175252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The Model\n","metadata":{}},{"cell_type":"markdown","source":"For this problem, many of the discussion boards indicated that a neural network of some kind was going to be the best performing model. Aftering trying a tree based model (LightGBM), I saw that the Neural Network was significantly more accurate. Given the heavy time constraints we had for this competition (just a couple months, and I work a full time and part time job), I thought it would be best to take advantage of as many resources as possible. Thus, the code for the model was largely adapted from the public notebook: \nhttps://www.kaggle.com/bestpredict/location-eda-8eb410\n\nUsing this neural network with my own feature engineering launched our team a few hundred spots up the leaderboard from where this public notebook scored - and tuning the model's hyperparameters further increased our score an additional 100 ranks.\n\nAt first, the model seemed to be significantly overfitting. Reducing the batch size and performing 2 5 fold cross validations helped that problem, leading to a reduced training accuracy but a better score on the public leaderboard.","metadata":{}},{"cell_type":"code","source":"class CRPSCallback(Callback):\n    \n    def __init__(self,validation, predict_batch_size=20, include_on_batch=False):\n        super(CRPSCallback, self).__init__()\n        self.validation = validation\n        self.predict_batch_size = predict_batch_size\n        self.include_on_batch = include_on_batch\n        \n        print('validation shape',len(self.validation))\n\n    def on_batch_begin(self, batch, logs={}):\n        pass\n\n    def on_train_begin(self, logs={}):\n        if not ('CRPS_score_val' in self.params['metrics']):\n            self.params['metrics'].append('CRPS_score_val')\n\n    def on_batch_end(self, batch, logs={}):\n        if (self.include_on_batch):\n            logs['CRPS_score_val'] = float('-inf')\n\n    def on_epoch_end(self, epoch, logs={}):\n        logs['CRPS_score_val'] = float('-inf')\n            \n        if (self.validation):\n            X_valid, y_valid = self.validation[0], self.validation[1]\n            y_pred = self.model.predict(X_valid)\n            y_true = np.clip(np.cumsum(y_valid, axis=1), 0, 1)\n            y_pred = np.clip(np.cumsum(y_pred, axis=1), 0, 1)\n            val_s = ((y_true - y_pred) ** 2).sum(axis=1).sum(axis=0) / (199 * X_valid.shape[0])\n            val_s = np.round(val_s, 6)\n            logs['CRPS_score_val'] = val_s","metadata":{"execution":{"iopub.status.busy":"2021-08-11T18:14:18.177215Z","iopub.execute_input":"2021-08-11T18:14:18.177512Z","iopub.status.idle":"2021-08-11T18:14:18.189945Z","shell.execute_reply.started":"2021-08-11T18:14:18.177463Z","shell.execute_reply":"2021-08-11T18:14:18.188487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model(x_tr,y_tr,x_val,y_val):\n    inp = Input(shape = (x_tr.shape[1],))\n\n    x = Dense(512, input_dim=X.shape[1], activation='relu')(inp)\n    x = Dropout(0.5)(x)\n    x = BatchNormalization()(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    x = BatchNormalization()(x)\n    x = Dense(128, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    x = BatchNormalization()(x)\n    \n    out = Dense(199, activation='softmax')(x)\n    model = Model(inp,out)\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[])\n\n    \n    es = EarlyStopping(monitor='CRPS_score_val', \n                       mode='min',\n                       restore_best_weights=True, \n                       verbose=1, \n                       patience=10)\n\n    mc = ModelCheckpoint('best_model.h5',monitor='CRPS_score_val',mode='min',\n                                   save_best_only=True, verbose=1, save_weights_only=True)\n    \n    bsz = 32\n    steps = x_tr.shape[0]/bsz\n    \n\n\n    model.fit(x_tr, y_tr,callbacks=[CRPSCallback(validation = (x_val,y_val)),es,mc], epochs=100, batch_size=bsz,verbose=1)\n    model.load_weights(\"best_model.h5\")\n    \n    y_pred = model.predict(x_val)\n    y_valid = y_val\n    y_true = np.clip(np.cumsum(y_valid, axis=1), 0, 1)\n    y_pred = np.clip(np.cumsum(y_pred, axis=1), 0, 1)\n    val_s = ((y_true - y_pred) ** 2).sum(axis=1).sum(axis=0) / (199 * x_val.shape[0])\n    crps = np.round(val_s, 6)\n\n    return model,crps","metadata":{"execution":{"iopub.status.busy":"2021-08-11T18:14:18.221716Z","iopub.execute_input":"2021-08-11T18:14:18.222046Z","iopub.status.idle":"2021-08-11T18:14:18.241806Z","shell.execute_reply.started":"2021-08-11T18:14:18.221992Z","shell.execute_reply":"2021-08-11T18:14:18.239877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"losses = []\nmodels = []\ncrps_csv = []\n\ns_time = time.time()\n\n\nfor k in range(2):\n    kfold = KFold(5, random_state = 42 + k, shuffle = True)\n    for k_fold, (tr_inds, val_inds) in enumerate(kfold.split(yards)):\n        print(\"-----------\")\n        print(\"-----------\")\n        tr_x,tr_y = X[tr_inds],y[tr_inds]\n        val_x,val_y = X[val_inds],y[val_inds]\n        model,crps = get_model(tr_x,tr_y,val_x,val_y)\n        models.append(model)\n        print(\"the %d fold crps is %f\"%((k_fold+1),crps))\n        print(datetime.datetime.now())\n        crps_csv.append(crps)\n    print(\"mean crps is %f\"%np.mean(crps_csv))\nprint(\"Final mean crps is %f\"%np.mean(crps_csv))\n\n\ndef predict(x_te):\n    model_num = len(models)\n    for k,m in enumerate(models):\n        if k==0:\n#             y_pred = m.predict(x_te,batch_size=1024)\n            y_pred = m.predict(x_te,batch_size=32)\n        else:\n            y_pred+=m.predict(x_te,batch_size=32)\n#             y_pred+=m.predict(x_te,batch_size=32)\n            \n    y_pred = y_pred / model_num\n    \n    return y_pred","metadata":{"execution":{"iopub.status.busy":"2021-08-11T16:20:40.881325Z","iopub.execute_input":"2021-08-11T16:20:40.881672Z","iopub.status.idle":"2021-08-11T16:46:05.830762Z","shell.execute_reply.started":"2021-08-11T16:20:40.881634Z","shell.execute_reply":"2021-08-11T16:46:05.829622Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-11T18:14:27.357981Z","iopub.execute_input":"2021-08-11T18:14:27.35836Z","iopub.status.idle":"2021-08-11T18:14:27.364515Z","shell.execute_reply.started":"2021-08-11T18:14:27.358251Z","shell.execute_reply":"2021-08-11T18:14:27.363051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-08-11T18:15:04.527172Z","iopub.execute_input":"2021-08-11T18:15:04.527518Z","iopub.status.idle":"2021-08-11T18:15:04.53395Z","shell.execute_reply.started":"2021-08-11T18:15:04.527463Z","shell.execute_reply":"2021-08-11T18:15:04.533132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from ngboost.distns import Exponential, Normal, LogNormal\nfrom ngboost.scores import LogScore, CRPScore","metadata":{"execution":{"iopub.status.busy":"2021-08-11T19:43:11.739324Z","iopub.execute_input":"2021-08-11T19:43:11.739703Z","iopub.status.idle":"2021-08-11T19:43:11.77574Z","shell.execute_reply.started":"2021-08-11T19:43:11.739654Z","shell.execute_reply":"2021-08-11T19:43:11.774352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nngb_model = NGBRegressor(Dist=LogNormal, Score=CRPScore).fit(X, yards)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T18:43:08.575178Z","iopub.execute_input":"2021-08-11T18:43:08.575505Z","iopub.status.idle":"2021-08-11T18:43:08.6174Z","shell.execute_reply.started":"2021-08-11T18:43:08.575472Z","shell.execute_reply":"2021-08-11T18:43:08.616365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yard_dists = ngb_model.pred_dist(X)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T18:40:44.87574Z","iopub.execute_input":"2021-08-11T18:40:44.87615Z","iopub.status.idle":"2021-08-11T18:40:52.872784Z","shell.execute_reply.started":"2021-08-11T18:40:44.876101Z","shell.execute_reply":"2021-08-11T18:40:52.871067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yard_dists.params","metadata":{"execution":{"iopub.status.busy":"2021-08-11T18:41:30.337157Z","iopub.execute_input":"2021-08-11T18:41:30.337646Z","iopub.status.idle":"2021-08-11T18:41:30.343748Z","shell.execute_reply.started":"2021-08-11T18:41:30.337599Z","shell.execute_reply":"2021-08-11T18:41:30.343118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"mean crps is %f\"%np.mean(crps_csv))","metadata":{"execution":{"iopub.status.busy":"2021-08-11T16:46:05.832826Z","iopub.execute_input":"2021-08-11T16:46:05.833285Z","iopub.status.idle":"2021-08-11T16:46:05.838793Z","shell.execute_reply.started":"2021-08-11T16:46:05.833212Z","shell.execute_reply":"2021-08-11T16:46:05.837713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Actual Predictions ","metadata":{}},{"cell_type":"markdown","source":"Another way I improved the accuracy of the prediction was quite simple - just imposing hard constrants based on field position. If a player was 20 yards away from the end zone, they have a 0% chance of rushing for more than 20 yards. Similarly, It is incredibly rare for a rusher to lose more than 3 yards from where they got the handoff - and when they do, it seems to be completely at random. So I imposed that constraint as well.\n\n\nOf note - the nflrush","metadata":{}},{"cell_type":"code","source":"from kaggle.competitions import nflrush\nenv = nflrush.make_env()\niter_test = env.iter_test()\n\nfor (test_df, sample_prediction_df) in iter_test:\n    basetable = create_features(test_df, deploy=True)\n    basetable['YardsFromOtherGoal'] = 100 - (basetable['YardLine'])\n    yds_max = np.ceil(basetable['YardsFromOtherGoal'].iloc[0])\n    yds_max = int(yds_max)\n    yds = np.ceil(basetable['back_from_scrimmage'].iloc[0])\n    yds = int(yds)\n    basetable.drop(['GameId','PlayId','YardsFromOtherGoal'], axis=1, inplace=True)\n    scaled_basetable = scaler.transform(basetable)\n\n    y_pred = predict(scaled_basetable)\n    y_pred = np.clip(np.cumsum(y_pred, axis=1), 0, 1).tolist()[0]\n    y_pred = np.array(y_pred)\n    y_pred[0:(99-(yds_min))] = 0\n    y_pred[0:(99-(yds+3))] = 0\n    y_pred[(99+yds_max):] = 1\n    preds_df = pd.DataFrame(data=[y_pred], columns=sample_prediction_df.columns)\n    env.predict(preds_df)\n\nenv.write_submission_file()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle.competitions import nflrush","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env = nflrush.make_env()\niter_test = env.iter_test()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}