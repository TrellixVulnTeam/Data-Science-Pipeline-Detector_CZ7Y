{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pickle\nimport pandas as pd\nimport numpy as np\nimport datetime as dt\nimport time\npd.options.display.max_columns = 100\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n\nimport math","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle.competitions import nflrush\nenv = nflrush.make_env()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/nfl-big-data-bowl-2020/train.csv', sep=',', low_memory=False)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from game clock to seconds\ndef from_gameClock_to_sec(x):\n    x = time.strptime(x[:5],'%M:%S')\n    return dt.timedelta(hours=x.tm_hour,minutes=x.tm_min,seconds=x.tm_sec).total_seconds()\n\n#transform str feet-inches to inches\ndef from_str_height_to_int(x):\n    x = x.split('-')\n    return int(x[0])*12 + int(x[1])\n\n\n# eg - from '5 DL, 3 LB, 2 DB, 1 OL' to different columns with counts\ndef OffensePersonnelSplit(x):\n    dic = {'DB' : 0, 'DL' : 0, 'LB' : 0, 'OL' : 0, 'QB' : 0, 'RB' : 0, 'TE' : 0, 'WR' : 0}\n    for xx in x.split(\",\"):\n        xxs = xx.split(\" \")\n        dic[xxs[-1]] = int(xxs[-2])\n    return dic\n\ndef DefensePersonnelSplit(x):\n    dic = {'DB' : 0, 'DL' : 0, 'LB' : 0, 'OL' : 0}\n    for xx in x.split(\",\"):\n        xxs = xx.split(\" \")\n        dic[xxs[-1]] = int(xxs[-2])\n    return dic\n\ndef sin(x):\n    return np.sin((x + 90) * math.pi / 180)\n\ndef cos(x):\n    return -np.cos((x + 90) * math.pi / 180)\n\ndef tan(x):\n    return np.tan((x + 90) * math.pi / 180)\n    \ndef preproc(df, possesion_def_groupby=[], drop_cols=[], train=True):\n    map_abbr = {'ARI': 'ARZ', 'BAL': 'BLT', 'CLE': 'CLV', 'HOU': 'HST'}\n    for abb in df['PossessionTeam'].unique():\n        map_abbr[abb] = abb\n    for abb in df['HomeTeamAbbr'].unique():\n        map_abbr[abb] = abb\n    for abb in df['VisitorTeamAbbr'].unique():\n        map_abbr[abb] = abb\n\n    df['PossessionTeam'] = df['PossessionTeam'].map(map_abbr)\n    df['HomeTeamAbbr'] = df['HomeTeamAbbr'].map(map_abbr)\n    df['VisitorTeamAbbr'] = df['VisitorTeamAbbr'].map(map_abbr)\n    df['GameClockSec'] = df['GameClock'].apply(from_gameClock_to_sec)\n\n\n    #inpute nan values with mode values\n    df.OffenseFormation = df.OffenseFormation.fillna('SINGLEBACK')\n    df.DefendersInTheBox = df.DefendersInTheBox.fillna(7)\n\n    df['DefendersInTheBox_vs_Distance'] = df['DefendersInTheBox'] / df['Distance']\n\n    df.PlayerHeight = df.PlayerHeight.apply(from_str_height_to_int)\n\n    # impute nan FieldPosition\n    nanfieldpos = df[df['FieldPosition'].isna()]['PlayId'].unique()\n    for i in nanfieldpos:\n        tmp = df[(df['PlayId']==i) & (df['NflIdRusher']==df['NflId'])]\n        if tmp['PlayDirection'].item() == 'left':\n            if tmp['X'].item()>=60:\n                df.loc[df[(df['PlayId']==i)].index, 'FieldPosition'] = tmp['PossessionTeam'].item()\n            elif tmp['PossessionTeam'].item() == tmp['VisitorTeamAbbr'].item():\n                df.loc[df[(df['PlayId']==i)].index, 'FieldPosition'] = tmp['HomeTeamAbbr'].item()\n            else:\n                df.loc[df[(df['PlayId']==i)].index, 'FieldPosition'] = tmp['VisitorTeamAbbr'].item()\n        else:\n            if tmp['X'].item()<=60:\n                df.loc[df[(df['PlayId']==i)].index, 'FieldPosition'] = tmp['PossessionTeam'].item()\n            elif tmp['PossessionTeam'].item() == tmp['VisitorTeamAbbr'].item():\n                df.loc[df[(df['PlayId']==i)].index, 'FieldPosition'] = tmp['HomeTeamAbbr'].item()\n            else:\n                df.loc[df[(df['PlayId']==i)].index, 'FieldPosition'] = tmp['VisitorTeamAbbr'].item()\n\n    df['HomePossesion'] = df['PossessionTeam'] == df['HomeTeamAbbr']\n    df['isHome'] = df.Team == 'home'\n\n    df['score_dif'] = df['HomeScoreBeforePlay'].values - df['VisitorScoreBeforePlay'].values\n    df.loc[~df['HomePossesion'],  'score_dif'] = -df.loc[~df['HomePossesion'],  'score_dif'].values\n\n    df['possesion_win'] = 0\n    df.loc[df.score_dif < 0, 'possesion_win'] = -1\n    df.loc[df.score_dif > 0, 'possesion_win'] = 1\n\n    df['ToLeft'] = df['PlayDirection'].values == \"left\"\n    df['IsBallCarrier'] = df['NflId'].values == df.NflIdRusher\n\n    df.loc[df['Dir'].isna(), 'Dir'] = 0\n    df.loc[df['Orientation'].isna(), 'Orientation'] = 0\n    # df['Dir_std'] = np.mod(90 - df['Dir'].values, 360)\n\n    # time features\n    df['TimeHandoff'] = df['TimeHandoff'].apply(lambda x: dt.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))\n    df['TimeSnap'] = df['TimeSnap'].apply(lambda x: dt.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))\n\n    df.PlayerBirthDate = pd.to_datetime(df.PlayerBirthDate, infer_datetime_format=True)\n    #Player Age\n    df['PlayerAge'] = (df.TimeHandoff - df.PlayerBirthDate).dt.days / 365\n\n    # dif in seconds between Handoff and Snap\n    df['time_dif'] = (df['TimeHandoff'] - df['TimeSnap']).dt.seconds\n\n\n    ## OffensePersonnel\n    temp = df[\"OffensePersonnel\"].iloc[np.arange(0, len(df), 22)].apply(lambda x : pd.Series(OffensePersonnelSplit(x)))\n    temp.columns = [\"Offense_\" + c for c in temp.columns]\n    temp[\"PlayId\"] = df[\"PlayId\"].iloc[np.arange(0, len(df), 22)]\n    df = df.merge(temp, on = \"PlayId\")\n\n    ## DefensePersonnel\n    temp = df[\"DefensePersonnel\"].iloc[np.arange(0, len(df), 22)].apply(lambda x : pd.Series(DefensePersonnelSplit(x)))\n    temp.columns = [\"Defense_\" + c for c in temp.columns]\n    temp[\"PlayId\"] = df[\"PlayId\"].iloc[np.arange(0, len(df), 22)]\n    df = df.merge(temp, on = \"PlayId\")\n\n    # determine defense team for further grouping about loosing and gaining yards for offense and defense separetly   \n    df['DefenseTeam'] = df.HomeTeamAbbr.values\n    df.loc[df.PossessionTeam == df.HomeTeamAbbr, 'DefenseTeam'] = df.loc[df.PossessionTeam == df.HomeTeamAbbr, 'VisitorTeamAbbr'].values\n\n    cat_features = ['OffenseFormation', 'Position', 'PossessionTeam', 'DefenseTeam', ]\n#     print(df.shape)\n    #one hot encoding for cat_features\n    if train == True:\n        ohe = OneHotEncoder(sparse=False, categories='auto', dtype=np.int8)\n\n        values = ohe.fit_transform(df[cat_features])\n\n        filename = 'ohe.preproc'\n        pickle.dump(ohe, open(filename, 'wb'))\n    else:\n        filename = 'ohe.preproc'\n        ohe = pickle.load(open(filename, 'rb'))\n        values = ohe.transform(df[cat_features])\n        \n\n    for ind, col in enumerate(ohe.get_feature_names()):\n        local_ind = int(col[1])\n        col = col.replace('x%s'%local_ind, cat_features[local_ind])\n        df[col] = values[:, ind]\n\n        \n    #####\n    ## normalize X\n    #####\n    \n    \n    df.loc[df.FieldPosition != df.PossessionTeam, 'YardLine'] = 100 - df.loc[df.FieldPosition != df.PossessionTeam, 'YardLine'].values\n    df['yards_for_touchdown'] = 100 - df.YardLine.values\n\n    df.loc[df.ToLeft, 'X'] = 120 - df.loc[df.ToLeft, 'X'].values\n    df.loc[df.ToLeft, 'Y'] = 160/3 - df.loc[df.ToLeft, 'Y'].values\n\n    df.loc[df.ToLeft, 'Orientation'] = np.mod(180 + df.loc[df.ToLeft, 'Orientation'].values, 360)\n    df.loc[df.ToLeft, 'Dir'] = np.mod(180 + df.loc[df.ToLeft, 'Dir'].values, 360)\n\n    # remove bias from Orientation during Season 2017 - https://www.kaggle.com/peterhurford/something-is-the-matter-with-orientation\n    # althought the author states that it's getting worse on LB score\n    df.loc[df['Season'] == 2017, 'Orientation'] = np.mod(90 + df.loc[df['Season'] == 2017, 'Orientation'].values, 360)\n\n\n    # cos - going right - positive, going left - negative, going up or down - neutral\n    df['DirCos'] = cos(df.Dir.values)\n    df['OrientationCos'] = cos(df.Orientation.values)\n\n    if 'PlayDirection' in df.columns:\n        # flip values when change Left PlayDirection to Right PlayDirection\n        df.loc[df.ToLeft, 'DirCos'] = - df.loc[df.ToLeft, 'DirCos'].values\n        df.loc[df.ToLeft, 'OrientationCos'] = - df.loc[df.ToLeft, 'OrientationCos'].values\n\n    # sin - going up - positive, going down - negative, going right or left - neutral\n    df['DirSin'] = sin(df.Dir.values)\n    df['OrientationSin'] = sin(df.Orientation.values)\n    #### ------\n\n    # # YardLine - X = 0\n    df['X'] = df['X'].values - df['YardLine'].values - 10\n\n    df['Distance_to_YardLine'] = abs(df['X'].values)\n    \n    df.drop(['DisplayName', 'JerseyNumber', 'Stadium', \n         'Location', 'StadiumType', 'Turf', 'GameWeather', \n         'Temperature', 'Humidity', 'WindSpeed', 'WindDirection',\n         'PlayDirection', 'GameClock',\n        'OffensePersonnel', 'DefensePersonnel', 'ToLeft', 'PlayDirection', \n        'FieldPosition', 'HomeScoreBeforePlay', 'VisitorScoreBeforePlay',\n        'PlayerBirthDate', 'PlayerCollegeName', 'GameId'], axis=1, inplace=True)\n\n    df = df.merge(df[df.IsBallCarrier][['PlayId', 'X', 'Y', 'isHome', 'Dir', 'Orientation']].rename(columns={'isHome': 'isHome_Carrier', \n                                                                                                             'X': 'X_carrier', \n                                                                                                             'Y': 'Y_carrier',\n                                                                                                             'Dir': 'Dir_carrier',\n                                                                                                             'Orientation': 'Orientation_carrier'}), \n                  on='PlayId')\n\n    df['isPossession'] = df['isHome_Carrier'] == df['isHome']\n    df = df.sort_values(['PlayId', 'IsBallCarrier', 'isPossession', 'X'], ascending=[True, False, False, True]).reset_index(drop=True)\n    \n    df['Distance_to_BallCarrier'] = np.power(np.power(df.Y_carrier - df.Y, 2) + np.power(df.X_carrier - df.X, 2), 0.5)\n\n    #### ----\n    for window in [10, 20, 30, 40]:\n        df['Dir_window_%s'%window] = (df.X > df.X_carrier) & (df.Y < tan(df.Dir_carrier + window) * (df.X - df.X_carrier) + df.Y_carrier) & (df.Y < tan(df.Dir_carrier - window) * (df.X - df.X_carrier) + df.Y_carrier)\n        df['Orientation_window_%s'%window] = (df.X > df.X_carrier) & (df.Y < tan(df.Orientation_carrier + window) * (df.X - df.X_carrier) + df.Y_carrier) & (df.Y < tan(df.Orientation_carrier - window) * (df.X - df.X_carrier) + df.Y_carrier)\n\n    df['upper_members'] = df.Y_carrier > df.Y\n    df['lower_members'] = df.Y_carrier < df.Y\n    #### ----\n\n    only_carrier = df[df.IsBallCarrier].reset_index(drop=True)\n    \n    #### ------\n    #Label Encoding for cat_features\n    for col in cat_features:\n        if train == True:\n            le = LabelEncoder()\n            values = le.fit_transform(only_carrier[col])\n\n            filename = '%s_le.preproc'%col\n            pickle.dump(le, open(filename, 'wb'))\n        else:\n            filename = '%s_le.preproc'%col\n            le = pickle.load(open(filename, 'rb'))\n            \n            values = le.transform(only_carrier[col])\n\n        only_carrier[col] = values\n    #### ------\n    \n    #### ------\n    clmns = ['upper_members', 'lower_members'] + ['%s_window_%s'%(j, window) for window in [10, 20, 30, 40] for j in ['Dir', 'Orientation']]\n    tmp = df[~df.IsBallCarrier].groupby(['PlayId', 'isPossession'])[clmns].agg('sum').reset_index()\n\n    only_carrier = only_carrier.merge(tmp.loc[np.arange(0, tmp.shape[0], 2)].rename(columns={i: i+'_count_offense' for i in clmns}).iloc[:, [i for i in range(tmp.shape[1]) if i!=1]],\n            on = 'PlayId')\n    only_carrier = only_carrier.merge(tmp.loc[np.arange(1, tmp.shape[0], 2)].rename(columns={i: i+'_count_defense' for i in clmns}).iloc[:, [i for i in range(tmp.shape[1]) if i!=1]],\n            on = 'PlayId')\n    #### ------\n\n    only_carrier.drop(clmns, axis=1, inplace=True)\n    df.drop(clmns, axis=1, inplace=True)\n    \n    #### ------\n    tmp = df[~df.IsBallCarrier].groupby(['PlayId', 'isPossession'])['Distance_to_YardLine'].agg(['min', 'max', 'median', 'mean', 'std']).reset_index()\n\n    only_carrier = only_carrier.merge(tmp.loc[np.arange(0, tmp.shape[0], 2)].rename(columns={i: 'Distance_to_YardLine_' + i + '_offense' for i in tmp.columns[2:]}).iloc[:, [i for i in range(tmp.shape[1]) if i!=1]],\n            on = 'PlayId')\n    only_carrier = only_carrier.merge(tmp.loc[np.arange(1, tmp.shape[0], 2)].rename(columns={i: 'Distance_to_YardLine_' + i + '_defense' for i in tmp.columns[2:]}).iloc[:, [i for i in range(tmp.shape[1]) if i!=1]],\n            on = 'PlayId')\n    #### ------\n\n    #### ------\n    tmp = df[~df.IsBallCarrier].groupby(['PlayId', 'isPossession'])['Distance_to_BallCarrier'].agg(['min', 'max', 'median', 'mean', 'std']).reset_index()\n\n    only_carrier = only_carrier.merge(tmp.loc[np.arange(0, tmp.shape[0], 2)].rename(columns={i: 'Distance_to_BallCarrier_' + i + '_offense' for i in tmp.columns[2:]}).iloc[:, [i for i in range(tmp.shape[1]) if i!=1]],\n            on = 'PlayId')\n    only_carrier = only_carrier.merge(tmp.loc[np.arange(1, tmp.shape[0], 2)].rename(columns={i: 'Distance_to_BallCarrier_' + i + '_defense' for i in tmp.columns[2:]}).iloc[:, [i for i in range(tmp.shape[1]) if i!=1]],\n            on = 'PlayId')\n    #### ------\n\n    #### ------\n    # tm = time.time()\n    anal_results = []\n    for i in np.arange(0, df.shape[0], 22):\n        values = df.iloc[i:i+22][['X', 'Y']].values\n\n        vls = np.sqrt(np.power(values[:11, 0].reshape(-1, 1) - values[11:, 0], 2) + np.power(values[:11, 1].reshape(-1, 1) - values[11:, 1], 2))\n        vls[values[:11, 0].reshape(-1, 1)>=values[11:, 0]] = 100000\n\n        distance = []\n        for _ in range(11):\n            if (vls == 100000).all():\n                break\n            i, j = np.unravel_index(vls.argmin(), vls.shape)\n            distance.append(vls[i, j])\n            vls[i, :] = 100000\n            vls[:, j] = 100000\n\n        if distance:\n            anal_results.append((len(distance), min(distance), max(distance), np.mean(distance), np.median(distance), np.std(distance)))\n        else:\n            anal_results.append((0, 0, 0, 0, 0, 0))\n    # print((time.time() - tm))# * 23171\n    anal_results = pd.DataFrame(anal_results, columns=['count_dist', 'min_dist', 'max_dist', 'mean_dist', 'median_dist', 'std_dist'])\n    only_carrier = only_carrier.merge(anal_results, left_index=True, right_index=True)\n    #### ------\n#     print(only_carrier.head())\n    #### ------\n    for ind, j in enumerate(['Possession', 'Defense']):\n        if train == True:\n#             tmp = only_carrier.groupby(['%sTeam'%j, 'Season'])['Yards'].agg({\n#                                                                                               'mean': np.mean, \n#                                                                                               'perc50': lambda x: np.percentile(x, 50), \n#                                                                                               'perc25': lambda x: np.percentile(x, 25),\n#                                                                                               'perc75': lambda x: np.percentile(x, 75),\n#                                                                                               'perc95': lambda x: np.percentile(x, 95),\n#                                                                                               'perc05': lambda x: np.percentile(x, 5),\n#                                                                                               'std': np.std,\n#                                                                                              }).reset_index()#.head()\n\n\n#             if '%sTeamYards_2017_mean'%j not in only_carrier.columns:\n#                 only_carrier = only_carrier.merge(tmp.loc[tmp.Season == 2017, ['%sTeam'%j] + list(tmp.columns[2:])].rename(columns={i: '%sTeamYards_2017_%s'%(j, i) for i in tmp.columns[2:]}), on='%sTeam'%j)\n#                 only_carrier = only_carrier.merge(tmp.loc[tmp.Season == 2018, ['%sTeam'%j] + list(tmp.columns[2:])].rename(columns={i: '%sTeamYards_2018_%s'%(j, i) for i in tmp.columns[2:]}), on='%sTeam'%j)\n        \n            tmp = only_carrier.groupby('%sTeam'%j)['Yards'].agg({\n                                                                                              'mean': np.mean, \n                                                                                              'perc50': lambda x: np.percentile(x, 50), \n                                                                                              'perc25': lambda x: np.percentile(x, 25),\n                                                                                              'perc75': lambda x: np.percentile(x, 75),\n                                                                                              'perc95': lambda x: np.percentile(x, 95),\n                                                                                              'perc05': lambda x: np.percentile(x, 5),\n                                                                                              'std': np.std,\n                                                                                             }).reset_index()\n            \n            tmp = tmp.rename(columns={i: '%sTeamYards_%s'%(j, i) for i in tmp.columns[1:]})\n            possesion_def_groupby.append(tmp)\n            tmp.to_csv('%sTeam.csv'%j, sep=';', index=False)\n            \n            if '%sTeamYards_mean'%j not in only_carrier.columns:\n                only_carrier = only_carrier.merge(tmp, on='%sTeam'%j)\n        else:#eval mode\n            if len(possesion_def_groupby) != 2:\n                tmp = pd.read_csv('%sTeam.csv'%j, sep=';')\n                possesion_def_groupby.append(tmp)\n            else:\n                tmp = possesion_def_groupby[ind]\n                \n            tmp = tmp[tmp['%sTeam'%j] == only_carrier.iloc[0]['%sTeam'%j]]\n            \n            for col in tmp.columns[1:]:\n                only_carrier[col] = tmp[col].values\n\n            \n    #### ------\n    if not drop_cols:\n        drop_cols = [i for i in only_carrier.columns if only_carrier[i].unique().shape[0]==1 or i in ['HomePossesion', \n                                                                                             'isHome_Carrier', \n                                                                                             'Dir_carrier',\n                                                                                             'Orientation_carrier',]]\n    only_carrier.drop(drop_cols, axis=1, inplace=True)\n    if train == True:\n        return df, only_carrier, possesion_def_groupby, drop_cols, cat_features\n    return df, only_carrier, possesion_def_groupby, cat_features\n\npossesion_def_groupby = []\ndf, only_carrier, possesion_def_groupby, drop_cols, cat_features = preproc(df, possesion_def_groupby, drop_cols=[], train=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"only_carrier.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pickle.dump(cat_features, open('cat_featues', 'wb'))\npickle.dump(drop_cols, open('drop_cols', 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ex_columns = ['PlayId', 'Team', 'X_carrier', 'Y_carrier', 'NflId',\n             'Season', 'NflIdRusher', 'TimeHandoff', 'TimeSnap',\n              'HomeTeamAbbr', 'VisitorTeamAbbr', 'YardLine',\n             ]\n\n# tmp = [i + '_' for i in cat_features]\n# ohe_columns = [i for i in only_carrier.columns if any(j in i for j in tmp)]\n\n# another_cat_features = ['Quarter', 'Down', 'Week']\n\n# nn_with_embed_columns = [i for i in only_carrier.columns if i not in ex_columns + ohe_columns and '201' not in i]\nnn_with_ohe_columns = [i for i in only_carrier.columns if i not in ex_columns + cat_features and '201' not in i]\n# print('nn_with_embed_columns', len(nn_with_embed_columns) - 1, 'nn_with_ohe_columns', len(nn_with_ohe_columns) - 1)\n\n# cat_embedings_in = [only_carrier[i].unique().shape[0] for i in cat_features]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Net(nn.Module):\n\n    def __init__(self, \n                 architecture, \n                 cat_embedings_in=[],\n                 embed_in_scale_param=0.75, \n                 is_regression=True, \n                 is_restnet=False,\n                 batch_norm=True,\n                 dropout=0,\n                ):\n        \n        super(Net, self).__init__()\n        \n        self.is_regression = is_regression\n        self.is_restnet = is_restnet\n        self.dropout = dropout\n        self.batch_norm = batch_norm\n        \n        self.cat_embedings_in = cat_embedings_in\n        if cat_embedings_in:\n            self.embeding_dims = [max(1, int(round(embed_in_scale_param * i))) for i in cat_embedings_in]\n            architecture[0] = architecture[0] - len(self.embeding_dims) + sum(self.embeding_dims)\n\n            self.embeds = nn.ModuleList([nn.Embedding(num_embeddings = cat_embedings_in[i], \n                                        embedding_dim = self.embeding_dims[i]) \n                           for i in range(len(cat_embedings_in))])\n        \n        self.fc = []\n        if batch_norm:\n            self.bn = []\n        if dropout>0:\n            self.dp = []\n            \n        for ind in range(len(architecture[:-1])):\n            input_dim = architecture[ind]\n            if is_restnet and ind>0:\n                input_dim += architecture[ind - 1]\n            \n            self.fc.append(nn.Linear(input_dim, architecture[ind + 1]))\n            \n            if batch_norm:\n                self.bn.append(nn.BatchNorm1d(architecture[ind + 1]))\n            if dropout>0:\n                self.dp.append(nn.Dropout(dropout))\n                \n        self.fc = nn.ModuleList(self.fc)\n#         print([i for i in self.fc])\n        if batch_norm:\n            self.bn.pop()\n            if self.bn:\n                self.bn = nn.ModuleList(self.bn)\n#                 print([i for i in self.bn])\n        if dropout>0:\n            self.dp.pop()\n            if self.dp:\n                self.dp = nn.ModuleList(self.dp)\n#                 print([i for i in self.dp])\n                \n            \n\n    def forward(self, x):\n        x = self.feature_extract(x)\n        x = self.fc[-1](x)\n        if self.is_regression:\n            return x\n        return F.softmax(x)\n    \n\n    def feature_extract(self, x):\n        \n        if self.cat_embedings_in:\n            y = None\n            for ind in range(len(self.embeds)):\n                if y is None:\n                    y = self.embeds[ind](x[:, ind].type(torch.long))\n                else:\n                    y = torch.cat((self.embeds[ind](x[:, ind].type(torch.long)), y), dim = 1)\n\n            x = torch.cat((y.float(), x[:, len(self.embeds):].float()), dim=1)\n        else:\n            x = x.float()\n            \n        if self.is_restnet:\n            last_x = x\n            \n        for ind in range(len(self.fc)):\n            \n            if self.is_restnet and ind>0:\n                x = torch.cat((last_x.float(), x), dim=1)\n                last_x = x[:, x.shape[1] - self.fc[ind-1].out_features:]\n                \n            if ind == len(self.fc) - 1:\n                return x\n            \n            x = self.fc[ind](x)\n            if self.bn:\n                x = self.bn[ind](x)\n            if self.dp:\n                x = self.dp[ind](x)\n                \n        return x\n    \n\nclass CustomsDataset(Dataset):\n    \"\"\"Face Landmarks dataset.\"\"\"\n\n    def __init__(self, X, Y):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.X = torch.tensor((X * 1).astype(np.float), dtype=torch.float16)\n        self.Y = torch.tensor(Y.reshape(-1, 1), dtype=torch.float)\n\n    def __len__(self):\n        return self.X.shape[0]\n\n    def __getitem__(self, idx):\n        return [self.X[idx], self.Y[idx]]\n\n\ndef torch_train_eval(model, dataloader, criterion, optimizer, device='cuda:0', isTrain = True):\n    model.train(isTrain)\n    cum_loss = 0.0\n    for i, data in enumerate(dataloader, 0):\n        inputs, labels = data\n        cnt = inputs.shape[0]\n        if 'cuda' in device:\n            inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        \n        outputs = net(inputs)\n        if 'cuda' in device:\n            outputs = outputs.to(device)\n        loss = criterion(outputs, labels)\n        \n        if isTrain:\n            loss.backward()\n            optimizer.step()\n\n        cum_loss += loss.item() * cnt\n        \n    return cum_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef save_model_params():\n    print('Saving model parameters...')\n    pickle.dump({'architecture': [X_train.shape[1], 256, 128, 1], \n                 'dropout': 0.5}, open('model.params', 'wb'))\n\ndef load_model_params():\n    print('Loading model parameters...')\n    return pickle.load(open('model.params', 'rb'))\n\ndef save_model(model, path='model.model', states_only=False):\n    if states_only:\n        torch.save(model.state_dict(), path + '_states')\n    else:\n        torch.save(model, path)\n    print('Model has been saved...')\n    \ndef load_model(path='model.model', states_only=False):\n    print('Loading the model...')\n    if states_only:\n        model = Net(**load_model_params())\n        model.load_state_dict(torch.load(path + '_states'))\n    else:\n        model = torch.load(path)\n    model.eval()\n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scale(only_carrier, scaler=None, non_bin_features=[]):\n\n    # scaled_only_carrier = only_carrier[only_carrier['Yards']<=30][nn_with_embed_columns].reset_index(drop=True).copy()\n    scaled_only_carrier = only_carrier.reset_index(drop=True).copy()\n\n    if scaler is None:\n        non_bin_features = [i for i in nn_with_ohe_columns if i not in cat_features + ['Yards'] and only_carrier[i].unique().shape[0]>2]\n        pickle.dump(non_bin_features, open('non_bin_features', 'wb'))\n        \n        scaler = StandardScaler()\n        scaled_only_carrier[non_bin_features] = scaler.fit_transform(scaled_only_carrier[non_bin_features])\n\n        pickle.dump(scaler, open('StandardScaler.scaler', 'wb'))\n    else: #eval mode\n#         non_bin_features = pickle.load(open('non_bin_features', 'rb'))\n#         scaler = pickle.load(open('StandardScaler.scaler', 'rb'))\n        scaled_only_carrier[non_bin_features] = scaler.transform(scaled_only_carrier[non_bin_features])\n\n    return scaled_only_carrier\n\ndef predict(model, X, device='cuda:0'):\n    if len(X.shape) == 1:\n        X = torch.tensor((X.reshape(1, -1) * 1).astype(np.float), dtype=torch.float16)\n    else:\n        X = torch.tensor((X * 1).astype(np.float), dtype=torch.float16)\n    if 'cuda' in device:\n        X = X.to(device)\n    return model(X).cpu().detach().numpy().flatten()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_only_carrier = scale(only_carrier[nn_with_ohe_columns])\n\ntrain_ratio = 0.8\ny_col = 'Yards'\nother_columns = [i for i in scaled_only_carrier.columns if i not in [y_col] + cat_features]\nX_columns = other_columns\n\nX_train, Y_train = scaled_only_carrier.loc[:int(scaled_only_carrier.shape[0]*train_ratio), X_columns].values, scaled_only_carrier.loc[:int(scaled_only_carrier.shape[0]*train_ratio), y_col].values\nX_test, Y_test = scaled_only_carrier.loc[int(scaled_only_carrier.shape[0]*train_ratio):, X_columns].values, scaled_only_carrier.loc[int(scaled_only_carrier.shape[0]*train_ratio):, y_col].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndevice = \"cpu\" # cpu cuda:0\n\nprint(device)\nnet = Net([X_train.shape[1], 256, 128, 1], cat_embedings_in=[], is_restnet=False, batch_norm=True, dropout=0.5,\n         embed_in_scale_param=0.3\n         )\nnet.to(device)\n\ntorch_trainset = CustomsDataset(X_train, Y_train)\ntorch_validset = CustomsDataset(X_test, Y_test)\n\nbatch_size = 32\ntrain_dataloader = DataLoader(torch_trainset, batch_size=batch_size, shuffle=True)\nvalid_dataloader = DataLoader(torch_validset, batch_size=batch_size, shuffle=True)\n\ncriterion = nn.L1Loss()\noptimizer = optim.Adam(net.parameters(), lr=0.0005)\n\nn_epochs = 5\n# early_stop_epoch = 2\n\nfor epoch in range(n_epochs):  # loop over the dataset multiple times\n    \n    train_loss = torch_train_eval(net, train_dataloader, criterion, optimizer, device=device, isTrain=True)\n    \n    valid_loss = torch_train_eval(net, valid_dataloader, criterion, optimizer, device=device, isTrain=False)\n        \n    print(f'epoch: {epoch}, trainloss: {train_loss/Y_train.shape[0]}, validloss: {valid_loss/Y_test.shape[0]}')\n#     break\n    \nprint('Finished Training')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"save_model(net)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict(net, X_train[0], device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_prediction(prediction, sample_pred):\n    prediction = round(prediction[0], 6)\n    ind = 99 + int(max(-99, min(98, prediction)))\n    sample_pred.iloc[0, np.arange(0, ind, 1)] = 0\n    sample_pred.iloc[0, ind] = 1 - (prediction - int(prediction)) if prediction - int(prediction) >= 0 else abs(prediction - int(prediction))\n    sample_pred.iloc[0,  np.arange(ind+1, 199, 1)] = 1\n    return sample_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_cols = pickle.load(open('drop_cols', 'rb'))\n\ncat_features = ['OffenseFormation', 'Position', 'PossessionTeam', 'DefenseTeam', ]\nex_columns = ['PlayId', 'Team', 'X_carrier', 'Y_carrier', 'NflId',\n         'Season', 'NflIdRusher', 'TimeHandoff', 'TimeSnap',\n          'HomeTeamAbbr', 'VisitorTeamAbbr', 'YardLine',\n         ]\n\n# tmp = [i + '_' for i in cat_features]\n# ohe_columns = [i for i in only_carrier.columns if any(j in i for j in tmp)]\n\nnn_with_ohe_columns = [i for i in only_carrier.columns if i != 'Yards' and i not in ex_columns + cat_features and '201' not in i]\nX_columns = nn_with_ohe_columns\n\nscaler = pickle.load(open('StandardScaler.scaler', 'rb'))\nnon_bin_features = pickle.load(open('non_bin_features', 'rb'))\n\nmodel = load_model()\ndevice = 'cpu'\n\nisTrain = False\n\nfor test_df, sample_pred in env.iter_test():\n    ttt  = test_df.copy()\n    test_df, only_carrier, possesion_def_groupby = preproc(test_df, possesion_def_groupby, drop_cols, train=isTrain)[:3]\n    \n    scaled_only_carrier = scale(only_carrier[nn_with_ohe_columns], scaler=scaler, non_bin_features=non_bin_features)\n    \n    prediction = predict(model, scaled_only_carrier.values, device=device)\n    \n    sample_pred = make_prediction(prediction, sample_pred)\n    env.predict(sample_pred)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env.write_submission_file()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}