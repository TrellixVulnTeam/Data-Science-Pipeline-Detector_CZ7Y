{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\nimport datetime\nfrom kaggle.competitions import nflrush\n\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV,GroupKFold\nfrom sklearn.preprocessing import OneHotEncoder,LabelEncoder,OrdinalEncoder\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\n\nfrom sklearn.pipeline import Pipeline,make_pipeline\n\nfrom scipy.special import softmax\n\nimport matplotlib.patches as patches\n\nimport keras\nfrom keras.callbacks import Callback, EarlyStopping\nimport tensorflow as tf\nimport keras.backend as K\n\nsns.set_style('darkgrid')\nmpl.rcParams['figure.figsize'] = [15,10]\npd.set_option('mode.chained_assignment', None)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"env = nflrush.make_env()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Pipeline\nThe data pipeline is fairly involved for this model because I ended up needing two pipelines.\n\nThe first is for the player level data. I ended up turning these data points into an \"image\" that could be passed to convolutional layers. To acheive this, I sorted players by their Y and X coordinates. I then selected each players' specific features (denoted piv_cols) and used them to create a 22x15x1 numpy array to be passed to convolutional layers.\n\nFor the game condition variables, I used sklearn OrdinalEncoders for variables like Week, Day, Quarter, Down, Defenders in Box, and Formation. This allowed me to pass them to embedding layers easily. I treated other game condition variables as continuous.\n\nAdditionally, I created \"lefty\" versions of each play, where the play's Y coordinates are flipped about the midline. These artificial plays are denoted by a negative PlayId. There are also helper functions for the columns that needed extra love."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_raw = pd.read_csv('../input/nfl-big-data-bowl-2020/train.csv', dtype={'WindSpeed': 'object'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def timesplit(x):\n    x=x.split(':')\n    return(60*int(x[0])+int(x[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def heightsplit(x):\n    x=x.split('-')\n    return(12*int(x[0])+int(x[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"piv_cols=['PlayerHeight','PlayerWeight','PlayerAge','X','Y',\n    'Orientation_x','Orientation_y','Dir_x','Dir_y','S','A','Dis',\n              'is_off','is_rusher','is_rb','is_l','is_sk']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cat_cols=['Week','Quarter','Down','OffenseFormation', 'DefendersInTheBox',\n#  'outside','rain','snow','Turf','is_home']\ncat_cols=['Week','Quarter','Down','OffenseFormation', 'DefendersInTheBox', \n          'outside','rain','snow','Turf','is_home']#\n          #,'PossTeam','DefTeam']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cont_cols=['till_reg','YardsFromOwnGoal','PossScore','DefScore',\n           'Distance','handoff_delay']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tall_pipe=make_column_transformer(\n                                (make_pipeline(SimpleImputer(strategy='mean'), StandardScaler()),['PlayerHeight','PlayerWeight','PlayerAge','X','Y',\n                                                                                                    'Orientation_x','Orientation_y','Dir_x','Dir_y','S','A','Dis']),\n                                ('passthrough',['is_off','is_rusher','is_rb','is_l','is_sk'])\n                                 )\n    \n    \n\n\ncont_pipe=make_column_transformer(\n#         (make_pipeline(SimpleImputer(strategy='most_frequent'),OneHotEncoder(sparse=False)),cat_cols),\n\n\n        (make_pipeline(SimpleImputer(strategy='median'),StandardScaler()),cont_cols)\n)\n\ncat_pipe=make_column_transformer(\n#         (make_pipeline(SimpleImputer(strategy='most_frequent'),OneHotEncoder(sparse=False)),cat_cols),\n\n\n        (make_pipeline(SimpleImputer(strategy='most_frequent'),OrdinalEncoder()),cat_cols)\n)\n         ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef feature_magic(train_in,tall_piper=tall_pipe,cat_piper=cat_pipe,cont_piper=cont_pipe,piv_cols=piv_cols,cat_cols=cat_cols,lefty=False,training=False):\n    \n    train=train_in.copy()\n    if lefty:\n        train_left=train.copy()\n        train_left['PlayId']=-train_left['PlayId']\n        train=pd.concat([train,train_left])\n        train=train.reset_index(drop=True)\n\n    train.loc[train.VisitorTeamAbbr == \"ARI\",'VisitorTeamAbbr'] = \"ARZ\"\n    train.loc[train.HomeTeamAbbr == \"ARI\",'HomeTeamAbbr'] = \"ARZ\"\n\n    train.loc[train.VisitorTeamAbbr == \"BAL\",'VisitorTeamAbbr']= \"BLT\"\n    train.loc[train.HomeTeamAbbr == \"BAL\",'HomeTeamAbbr'] = \"BLT\"\n\n    train.loc[train.VisitorTeamAbbr == \"CLE\",'VisitorTeamAbbr'] = \"CLV\"\n    train.loc[train.HomeTeamAbbr == \"CLE\",'HomeTeamAbbr'] = \"CLV\"\n\n    train.loc[train.VisitorTeamAbbr == \"HOU\",'VisitorTeamAbbr'] = \"HST\"\n    train.loc[train.HomeTeamAbbr == \"HOU\",'HomeTeamAbbr'] = \"HST\"\n    \n    \n    \n    train['is_rusher']=train.NflId==train.NflIdRusher\n    train['is_home']=train.Team=='home'\n    train['is_off']=(train.is_home & (train['HomeTeamAbbr']==train['PossessionTeam'])) | (~train.is_home & (train['HomeTeamAbbr']!=train['PossessionTeam']))\n\n    train['YardsFromOwnGoal']=train['YardLine']\n    train.loc[train.FieldPosition!=train.PossessionTeam,'YardsFromOwnGoal']=50+50-train.loc[train.FieldPosition!=train.PossessionTeam,'YardsFromOwnGoal']\n\n\n    train['to_left']=train.PlayDirection=='left'\n    train.loc[train.to_left,'X']=120-train.loc[train.to_left,'X']\n    \n    train['X']=train['X']-train['YardsFromOwnGoal']\n\n    train.loc[train.to_left,'Y']=(160/3)-train.loc[train.to_left,'Y']\n    \n    train['Y']=train.Y-(160/3)/2\n\n    train.loc[train.PlayId<0,'Y']=-train.loc[train.PlayId<0,'Y']\n\n    train.loc[train.to_left,'Orientation']=train.loc[train.to_left,'Orientation']-180\n    train.loc[train.to_left,'Dir']=train.loc[train.to_left,'Dir']-180\n\n    train.loc[train['Season'] == 2017, 'Orientation'] = np.mod(90 + train.loc[train['Season'] == 2017, 'Orientation'], 360)\n\n    deg_cols=['Orientation','Dir']\n    for deg_col in deg_cols:\n        train[deg_col+'_x']=np.sin(np.radians(train[deg_col]))\n        train[deg_col+'_y']=np.cos(np.radians(train[deg_col]))\n        train.loc[train.PlayId<0,deg_col+'_y']=-train.loc[train.PlayId<0,deg_col+'_y']\n        \n    \n\n    train['TimeSnap']=pd.to_datetime(train.TimeSnap)\n    train['PlayerBirthDate']=pd.to_datetime(train['PlayerBirthDate'],utc=True)    \n\n    train['PlayerAge']=train['TimeSnap']-train['PlayerBirthDate']\n    train['PlayerAge']=train['PlayerAge'].apply(lambda x:x.total_seconds()/(365.25*24*3600))\n\n\n    train.PlayerHeight=train.PlayerHeight.apply(heightsplit)\n    \n    posmap={'CB':'DB','WR':'WR','G':'OL','T':'OL','DE':'DL','DT':'DL','OLB':'LB','TE':'TE','FS':'DB','C':'OL','RB':'RB','QB':'QB',\n    'SS':'DB','ILB':'LB','MLB':'LB','NT':'DL','LB':'LB','OT':'OL','FB':'FB','OG':'OL','DB':'DB','S':'DB','HB':'RB','SAF':'DB','DL':'DL'}\n\n    train.Position=train.Position.apply(lambda x: posmap[x])\n       \n    train['is_rb']=(train.Position=='RB')|(train.Position=='FB')\n    train['is_l']=(train.Position=='OL')|(train.Position=='DL')|(train.Position=='TE')|(train.Position=='FB')\n    train['is_sk']=(train.Position=='WR')|(train.Position=='DB')|(train.Position=='TE')\n    \n    train=train.sort_values(['PlayId','Y','X','is_off'])\n    \n    \n    \n    trains=train.copy()\n    train_run=train[train.is_rusher]\n    \n    if training:\n        train[piv_cols]=tall_piper.fit_transform(train[piv_cols])\n    else:\n        train[piv_cols]=tall_piper.transform(train[piv_cols])    \n                            \n    convdat=train.groupby('PlayId',as_index=False).apply(lambda x:x[piv_cols].values)\n    \n    convdat=np.dstack(convdat,)\n    convdat=np.swapaxes(convdat,0,2)\n    #print(convdat.shape)\n    convdat=convdat.reshape(convdat.shape[0],convdat.shape[1],convdat.shape[2],1)\n\n    \n    train_run.GameClock=train.GameClock.apply(timesplit)\n    train_run['till_reg']=train_run.GameClock+(4-train_run.Quarter)*60*15\n       \n    \n    train_run['outside']=(train_run.StadiumType.str.lower().str.contains(r'ou[a-z]+')|\n    train_run.StadiumType.str.lower().str.contains(r'open')|\n    train_run.StadiumType.str.lower().str.contains(r'heinz field')|\n                      train_run.StadiumType.str.lower().str.contains(r'bowl')\n                     )\n\n    train_run['Turf']=~(train_run.Turf.str.lower()=='grass')|(train_run.Turf.str.lower().str.contains(r'natur'))\n\n    train_run['rain']=(train_run.GameWeather.str.lower().str.contains(r'rain')|\n    train_run.GameWeather.str.lower().str.contains(r'shower'))\n\n    train_run['snow']=(train_run.GameWeather.str.lower().str.contains(r'snow'))\n    \n    train_run['PossScore']=np.select([train_run.is_home,~train_run.is_home],[train_run.HomeScoreBeforePlay,train_run.VisitorScoreBeforePlay])\n    train_run['DefScore']=np.select([train_run.is_home,~train_run.is_home],[train_run.VisitorScoreBeforePlay,train_run.HomeScoreBeforePlay])\n\n    train_run['PossTeam']=np.select([train_run.is_home,~train_run.is_home],[train_run.HomeTeamAbbr,train_run.VisitorTeamAbbr])\n    train_run['DefTeam']=np.select([train_run.is_home,~train_run.is_home],[train_run.VisitorTeamAbbr,train_run.HomeTeamAbbr])\n\n    \n    td=pd.to_datetime(train_run.TimeHandoff)-pd.to_datetime(train_run.TimeSnap)\n    train_run['handoff_delay']=td.dt.total_seconds()\n    \n    \n    if training:\n        yards=train_run.Yards\n        train_run=train_run.drop(columns='Yards')\n        train_cont=cont_piper.fit_transform(train_run)\n        \n        train_cat=cat_piper.fit_transform(train_run)\n        train_cat=train_cat.reshape(train_cat.shape+(1,))\n        \n        train_run['Yards']=yards\n    else:\n        train_cont=cont_piper.transform(train_run)\n        \n        train_cat=cat_piper.transform(train_run)\n        train_cat=train_cat.reshape(train_cat.shape+(1,))\n        train_cat=train_cat.swapaxes(0,1)\n        train_cat=train_cat.tolist()\n\n    train_run.reset_index(drop=True,inplace=True)\n    \n    return(trains,train_run,convdat,train_cont,train_cat,tall_piper,cont_piper,cat_piper)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainn,train_runn,train_conv,train_cont,train_cat,tall_pipe,cont_pipe,cat_pipe=feature_magic(train_raw,lefty=True,training=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cont[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cat.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_raw.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainn.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_runn.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below are some heatmaps of the convolutional layer input. The data is scaled along the variable channel as opposed to the \"image\" channel. As you can see in the heatmap, the data is sorted in asceding Y values. My intuition was that this sorting would pick up on gaps between players."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(train_conv[55].reshape(train_conv[0].shape[0],train_conv[0].shape[1]),yticklabels=piv_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(train_conv[1].reshape(train_conv[0].shape[0],train_conv[0].shape[1]),yticklabels=piv_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For my training and test splits, I used the last 5 weeks of the regular season of 2018 since the test data for the competition will be the last 5 weeks of 2019."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_mask=((train_runn.Season==2018)&((train_runn.Week>=13)))&(train_runn.PlayId>0)\ntrain_mask=~((train_runn.Season==2018)&((train_runn.Week>=13)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_cat=train_cat[train_mask]\ncv_cat=cv_cat.swapaxes(0,1)\ncv_cat=cv_cat.tolist()\n\n\ntest_cat=train_cat[test_mask]\ntest_cat=test_cat.swapaxes(0,1)\ntest_cat=test_cat.tolist()\n\ntrain_catt=train_cat.swapaxes(0,1)\ntrain_catt=train_catt.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_cont=train_cont[train_mask]\ntest_cont=train_cont[test_mask]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cont[test_mask].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_conv=train_conv[train_mask]\n\ntest_conv=train_conv[test_mask]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_c = np.zeros((train_runn.shape[0], 199))\nfor idx, target in enumerate(train_runn['Yards']):\n    Y_c[idx][99 + target] = 1\n    \ncv_Y_c=Y_c[train_mask]\ntest_Y_c=Y_c[test_mask]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Building\nI tried a lot of different architectures, but settled on the one described below. It is using an MAE loss function and using a 3 player window for convolution. I used Average Pooling layers in between convolutions.\n\nFor the game condition network, I used entity embedddings followed by fully connected layers. \n\nI added both dropout and Gaussian Noise to try to control for overfitting between FC layers."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Dense\nfrom keras.models import Sequential,Model\nfrom keras.callbacks import Callback, EarlyStopping\nfrom keras.layers import Dropout, PReLU, BatchNormalization, concatenate,ELU,AveragePooling2D, GaussianNoise, Activation,Dense, Conv2D, Flatten,MaxPooling2D,Embedding,Input,Reshape,Concatenate\nfrom keras.optimizers import Adam\nfrom keras.losses import categorical_crossentropy\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def crps_loss(y_true, y_pred):\n    return K.mean(K.square(K.clip(K.cumsum(y_true, axis=1),0,1) - K.clip(K.cumsum(y_pred, axis=1),0,1)), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ordinal_loss(y_true, y_pred):\n    weights = K.cast(K.abs(K.argmax(y_true, axis=1) - K.argmax(y_pred, axis=1))/(K.int_shape(y_pred)[1] - 1), dtype='float32')\n    return (1.0 + weights) * categorical_crossentropy(y_true, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ordinal_loss_2(y_true, y_pred):\n    weights = K.cast(K.square(K.argmax(y_true, axis=1) - K.argmax(y_pred, axis=1))/(K.int_shape(y_pred)[1] - 1), dtype='float32')\n    return (1.0 + weights) * losses.categorical_crossentropy(y_true, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ordinal_loss_h(y_true, y_pred,delta=10):\n    weights = K.cast(K.abs(K.argmax(y_true, axis=1) - K.argmax(y_pred, axis=1))/(K.int_shape(y_pred)[1] - 1), dtype='float32')\n\n    mae=(1.0 + weights) * categorical_crossentropy(y_true, y_pred)\n    quadratic=K.minimum(mae, delta)\n    linear=mae - quadratic\n    \n    return 0.5 * K.square(quadratic) + delta * linear","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_col_map=dict(train_runn[cat_cols].nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cont_model(cat_cols=cat_cols,cont_cols=cont_cols):\n    \n    inputs=[]\n    embeddings=[]\n    \n    for i in cat_col_map.keys():\n        input=Input(shape=(1,))\n        vals=cat_col_map[i]\n        embs=int(np.sqrt(vals))\n        embedding=Embedding(vals,embs,input_length=1)(input)\n        embedding=Reshape(target_shape=(embs,))(embedding)\n        inputs.append(input)\n        embeddings.append(embedding)\n    \n    cont_input=Input(shape=(len(cont_cols),))\n    inputs.append(cont_input)\n    embeddings.append(cont_input)\n    cont_x=Concatenate()(embeddings)\n    \n    #cont_x=cont_input\n    \n    cont_x=Dense(32,input_dim=inputs, activation=None)(cont_x)\n    cont_x=BatchNormalization()(cont_x)\n    cont_x=PReLU()(cont_x)\n    cont_x=Dropout(0.7)(cont_x)\n    cont_x=GaussianNoise(0.2)(cont_x)\n    \n    cont_x=Dense(32, activation=None)(cont_x)\n    cont_x=BatchNormalization()(cont_x)\n    cont_x=PReLU()(cont_x)\n    cont_x=Dropout(0.5)(cont_x)\n    cont_x=GaussianNoise(0.2)(cont_x)\n    \n    \n    cont_model=Model(inputs,cont_x)\n    \n    return cont_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncont_model_=cont_model()\n#cont_model_.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def conv_model2(cols):\n    conv_input=Input(shape=(cols,22,1))\n    \n    conv_x=conv_input\n\n    conv_3=Conv2D(64, kernel_size=(cols,3), activation='relu',\n                     input_shape=(cols,22,1),padding='same')(conv_x)\n#    conv_x=MaxPooling2D(pool_size=(1,3))(conv_x)\n    \n    \n    conv_5=Conv2D(64, kernel_size=(cols,5), activation='relu',\n                     input_shape=(cols,22,1),padding='same')(conv_x)\n#    conv_x=Conv2D(32, kernel_size=(1,1), activation='relu')(conv_x)\n#    conv_x=MaxPooling2D(pool_size=(1,3))(conv_x)\n    \n#     conv_x=Conv2D(32, kernel_size=(1,1), activation='tanh')(conv_x)\n#     conv_x=MaxPooling2D(pool_size=(1,3))(conv_x)\n\n    conv_7=Conv2D(64, kernel_size=(cols,7), activation='relu',\n                     input_shape=(cols,22,1),padding='same')(conv_x)\n\n    \n    conv_out=concatenate([conv_3,conv_5,conv_7],axis=3)\n    \n    conv_out=Flatten()(conv_out)\n    conv_out=BatchNormalization()(conv_out)\n    conv_out=Dropout(0.7)(conv_out)\n    conv_out=GaussianNoise(0.2)(conv_out)\n    \n    conv_out=Dense(32, activation=None)(conv_out)\n    conv_out=BatchNormalization()(conv_out)\n    conv_out=PReLU()(conv_out)\n    \n    \n    conv_model=Model(conv_input,conv_out)\n\n    return conv_model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def conv_model(cols,player_width):\n    conv_input=Input(shape=(cols,22,1))\n    \n    conv_x=conv_input\n\n    conv_x=Conv2D(64, kernel_size=(cols,player_width), activation='relu',\n                  input_shape=(cols,22,1),padding='same')(conv_x)\n    conv_x=BatchNormalization(axis=1)(conv_x)\n    \n    conv_x=AveragePooling2D(pool_size=(1,player_width),strides=1)(conv_x)\n    \n    conv_x=Conv2D(32, kernel_size=(1,player_width), activation='relu',\n                      padding='same')(conv_x)\n    conv_x=BatchNormalization(axis=1)(conv_x)\n    \n    conv_x=AveragePooling2D(pool_size=(1,player_width),strides=1)(conv_x)\n    \n    conv_x=Conv2D(32, kernel_size=(1,player_width), activation='relu',\n                      padding='same')(conv_x)\n    conv_x=BatchNormalization(axis=1)(conv_x)\n    \n    conv_x=AveragePooling2D(pool_size=(1,player_width),strides=1)(conv_x)\n    \n\n    \n    conv_x=Flatten()(conv_x)\n    conv_x=BatchNormalization()(conv_x)\n    conv_x=Dropout(0.5)(conv_x)\n    conv_x=GaussianNoise(0.2)(conv_x)\n    \n\n    conv_x=Dense(32, activation=None)(conv_x)\n    conv_x=BatchNormalization()(conv_x)\n    conv_x=PReLU()(conv_x)\n    \n    conv_model=Model(conv_input,conv_x)\n\n    return conv_model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cont_inputs=train_cont.shape[1]\ncont_inputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conv_inputs=train_conv.shape[1]\n#conv_model_=conv_model(conv_inputs)\nconv_model_=conv_model(conv_inputs,3)\nconv_model_.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# cont_model_=cont_model(cont_inputs)\n\nmodels=concatenate([cont_model_.output,conv_model_.output])\nmodels=BatchNormalization()(models)\nmodels=Dropout(0.7)(models)\nmodels=GaussianNoise(0.2)(models)\n\ncombout=Dense(32,activation=None)(models)\ncombout=BatchNormalization()(combout)\ncombout=PReLU()(combout)\ncombout=Dropout(0.7)(combout)\ncombout=GaussianNoise(0.2)(combout)\n\n# combout=Dense(32,activation=None)(models)\n# combout=BatchNormalization()(combout)\n# combout=PReLU()(combout)\n# combout=Dropout(0.5)(combout)\n# combout=GaussianNoise(0.2)(combout)\n\ncombout=Dense(32,activation=None)(models)\ncombout=BatchNormalization()(combout)\ncombout=PReLU()(combout)\ncombout=Dropout(0.5)(combout)\ncombout=GaussianNoise(0.2)(combout)\n\ncombout=Dense(199, activation='softmax')(combout)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#conv_model_.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nbig_model=Model(inputs=cont_model_.input+[conv_model_.input],outputs=combout)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"big_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(214)\n#from tensorflow.random import set_seed\n#set_seed(214)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#losss=ordinal_loss_h(delta=10)\n#opt=R\nbig_model.compile(optimizer=Adam(learning_rate=0.005,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    amsgrad=False), loss=ordinal_loss,\n                 metrics=[crps_loss]\n                 )\n\n\nes=EarlyStopping(patience=10, min_delta=5e-5, restore_best_weights=True, monitor='val_crps_loss')\n\nbig_model.fit(cv_cat+[cv_cont,cv_conv], cv_Y_c, callbacks=[es], epochs=100, batch_size=512, \n              verbose=True,validation_data=(test_cat+[test_cont,test_conv],test_Y_c) )\n\n\n# big_model.compile(optimizer=Adam(learning_rate=0.001,\n#     beta_1=0.9,\n#     beta_2=0.999,\n#     epsilon=1e-07,\n#     amsgrad=False), loss=crps_loss,metrics=[ordinal_loss])\n\n# es=EarlyStopping(patience=10, min_delta=5e-5, restore_best_weights=True, monitor='val_loss')\n\n# big_model.fit([cv_cont,cv_conv], cv_Y_c, callbacks=[es], epochs=100, batch_size=256, \n#               verbose=True,validation_data=([test_cont,test_conv],test_Y_c) )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Evalution\n\nBelow are some simple diagnostics on the model. It appears to like draw plays and sweeps against prevent defenses. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_preds=big_model.predict(train_catt+[train_cont,train_conv])\n#test_preds=big_model.predict([test_cont,test_conv])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predslon=np.sum(test_preds*np.arange(-99,100),axis=1)\ntrain_predslon=np.sum(train_preds*np.arange(-99,100),axis=1)\n#test_predslon=np.sum(train_preds*np.arange(-99,100),axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train_predslon,kde=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_runn['pred']=train_predslon\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_football_field(linenumbers=True,\n                          endzones=True,\n                          highlight_line=False,\n                          highlight_line_number=50,\n                          highlighted_name='Line of Scrimmage',\n                          fifty_is_los=False,\n                          figsize=(12, 6.33)):\n    \"\"\"\n    Function that plots the football field for viewing plays.\n    Allows for showing or hiding endzones.\n    \"\"\"\n    rect = patches.Rectangle((0, 0), 120, 53.3, linewidth=0.1,\n                             edgecolor='r', facecolor='lightgreen', zorder=0)\n\n    fig, ax = plt.subplots(1, figsize=figsize)\n    ax.add_patch(rect)\n\n    plt.plot([10, 10, 10, 20, 20, 30, 30, 40, 40, 50, 50, 60, 60, 70, 70, 80,\n              80, 90, 90, 100, 100, 110, 110, 120, 0, 0, 120, 120],\n             [0, 0, 53.3, 53.3, 0, 0, 53.3, 53.3, 0, 0, 53.3, 53.3, 0, 0, 53.3,\n              53.3, 0, 0, 53.3, 53.3, 0, 0, 53.3, 53.3, 53.3, 0, 0, 53.3],\n             color='white')\n    if fifty_is_los:\n        plt.plot([60, 60], [0, 53.3], color='gold')\n        plt.text(62, 50, '<- Player Yardline at Snap', color='gold')\n    # Endzones\n    if endzones:\n        ez1 = patches.Rectangle((0, 0), 10, 53.3,\n                                linewidth=0.1,\n                                edgecolor='r',\n                                facecolor='blue',\n                                alpha=0.2,\n                                zorder=0)\n        ez2 = patches.Rectangle((110, 0), 120, 53.3,\n                                linewidth=0.1,\n                                edgecolor='r',\n                                facecolor='blue',\n                                alpha=0.2,\n                                zorder=0)\n        ax.add_patch(ez1)\n        ax.add_patch(ez2)\n    plt.xlim(0, 120)\n    plt.ylim(-5, 58.3)\n    plt.axis('off')\n    if linenumbers:\n        for x in range(20, 110, 10):\n            numb = x\n            if x > 50:\n                numb = 120 - x\n            plt.text(x, 5, str(numb - 10),\n                     horizontalalignment='center',\n                     fontsize=20,  # fontname='Arial',\n                     color='white')\n            plt.text(x - 0.95, 53.3 - 5, str(numb - 10),\n                     horizontalalignment='center',\n                     fontsize=20,  # fontname='Arial',\n                     color='white', rotation=180)\n    if endzones:\n        hash_range = range(11, 110)\n    else:\n        hash_range = range(1, 120)\n\n    for x in hash_range:\n        ax.plot([x, x], [0.4, 0.7], color='white')\n        ax.plot([x, x], [53.0, 52.5], color='white')\n        ax.plot([x, x], [22.91, 23.57], color='white')\n        ax.plot([x, x], [29.73, 30.39], color='white')\n\n    if highlight_line:\n        hl = highlight_line_number + 10\n        plt.plot([hl, hl], [0, 53.3], color='yellow')\n        plt.text(hl + 2, 50, '<- {}'.format(highlighted_name),\n                 color='yellow')\n    return fig, ax","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_play(play_id,x='X',y='Y',mag='S',ang='Dir',trained=False):\n    \n    play=trainn[trainn.PlayId==play_id]\n    \n    yards=play.Yards.values[0]\n    yardline=play.YardsFromOwnGoal.values[0]+10\n    yardgain=yards+yardline\n    play['X']=play['X']+play['YardsFromOwnGoal']\n    if trained:\n        play[piv_cols]=sp.inverse_transform(play[piv_cols])\n    \n    \n    off=play[play.is_off]\n    deff=play[~play.is_off]\n    rusher=play[play.is_rusher]\n\n    fig, ax = create_football_field()\n    plt.scatter(x=off[x],y=off[y]+53.3/2,c='red',s=30,label='Offense')\n    plt.scatter(x=deff[x],y=deff[y]+53.3/2,c='blue',s=30,label='Defense')\n    plt.scatter(x=rusher[x],y=rusher[y]+53.3/2,color='black',s=15,label='BallCarrier')\n    \n    plt.plot([yardline,yardline],[0,53.3],\n             color='grey')\n    \n    plt.plot([yardgain,yardgain],[0,53.3],\n             color='gold')\n\n    for i, row in off.iterrows():\n        ax.arrow(row[x], row[y]+53.3/2, row[mag]*row[ang+'_x'], row[mag]*row[ang+'_y'], head_width=0.5, head_length=0.7, ec='orange')\n\n\n    for i, row in deff.iterrows():\n        ax.arrow(row[x], row[y]+53.3/2, row[mag]*row[ang+'_x'], row[mag]*row[ang+'_y'], head_width=0.5, head_length=0.7, ec='purple')\n\n   \n    plt.legend(loc=4)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lossess=np.mean(np.square(np.clip(np.cumsum(train_preds, axis=1),0,1) - np.clip(np.cumsum(Y_c, axis=1),0,1)), axis=1)\nlossess.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_runn['loss']=lossess","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_runn.loss.quantile([.1,.25,.5,.75,.8,.9,.95,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lmplot('Yards','loss',data=train_runn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r2_score(train_runn['Yards'],train_runn['pred'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Largest Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"big_preds=train_runn.sort_values('pred',ascending=False)[['PlayId','PossTeam','PossScore','DefTeam','DefScore','DisplayName','Quarter','GameClock','Down','Distance','Yards','pred','loss']].head(10)\nbig_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i,row in big_preds.iterrows():\n    print(row)\n    show_play(row['PlayId'])\n    fig, ax = plt.subplots(1, figsize=(12, 6.33))\n    plt.bar(np.arange(-99,100),train_preds[i])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Smallest Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"small_preds=train_runn.sort_values('pred',ascending=True)[['PlayId','PossTeam','PossScore','DefTeam','DefScore','DisplayName','Quarter','GameClock','Down','Distance','Yards','pred','loss']].head(10)\nsmall_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i,row in small_preds.iterrows():\n    print(row)\n    show_play(row['PlayId'])\n    fig, ax = plt.subplots(1, figsize=(12, 6.33))\n    plt.bar(np.arange(-99,100),train_preds[i])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission\nI added in a final step to clip the function at only possible values. My final submission got a .01355. This one may not have that exact performance but something similar."},{"metadata":{"trusted":true},"cell_type":"code","source":"iter_test=env.iter_test()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clipper(yl,predsin):\n\n    lind=-yl+99\n    #print(lind)\n    rind=100-yl+99\n    #print(rind)\n    #print(len(test[lind:rind]))\n    #preds[0,lind:rind]=preds[0,lind:rind]/np.sum(preds[0,lind:rind])\n    \n    #preds[0,lind:rind]=softmax(preds[0,lind:rind])\n    #predsin[:lind]=0\n    predsin=np.clip(np.cumsum(predsin),0,1)\n    predsin[:lind]=0\n    predsin[rind:]=1\n    return predsin","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfor (test_df, sample_prediction_df) in iter_test:\n    \n    #_,test_runn,test_conv,test_cont,tall_pipe,wide_pipe=feature_magic(test_df,lefty=False,training=False)\n    testt,test_runn,test_conv,test_cont,test_cat,tall_pipe,cont_pipe,cat_pipe=feature_magic(test_df,lefty=False,training=False)\n    \n    \n    test_pred=big_model.predict(test_cat+[test_cont,test_conv])\n    \n    #pred=np.clip(np.cumsum(clipper(test_run['YardsFromOwnGoal'].values[0],pred)),0,1)\n    test_pred=clipper(test_runn.YardsFromOwnGoal.values[0],test_pred)\n    pred_df=pd.DataFrame(data=[test_pred],columns=sample_prediction_df.columns)\n\n    env.predict(pred_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env.write_submission_file()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lessons Learned\nI got big jumps in performance from \n* scaling along the right axis in the preprocessing step (at the variable channel, not the image channe)\n* settling on MAE loss function\n* there was a small lift from using entity embeddings\n\nGetting the pipeline to conform to the competition submission process was difficult. I would be thinking about it from an inference perspective when building features moving forward.\n\n## Things I wish i had time to try\nWindsor scaling to control for outliers (prevent defenses, etc)\nnot including all 22 players in the convolution input, only including the closest 10-15\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dummy_pred=np.zeros(199)\n# pred_df=pd.DataFrame(data=[dummy_pred],columns=sample_prediction_df.columns)\n\n# env.predict(pred_df)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}