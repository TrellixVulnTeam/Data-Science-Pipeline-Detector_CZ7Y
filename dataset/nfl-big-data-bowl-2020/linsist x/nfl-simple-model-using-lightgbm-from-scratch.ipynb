{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## import\nLoad the necessary libraries.","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\nimport random\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nimport gc\nimport pickle\nimport tqdm","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-12-09T03:17:45.669176Z","iopub.execute_input":"2021-12-09T03:17:45.669787Z","iopub.status.idle":"2021-12-09T03:17:47.408979Z","shell.execute_reply.started":"2021-12-09T03:17:45.669707Z","shell.execute_reply":"2021-12-09T03:17:47.407817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## train data\nThe shape of train data is 509762 × 49.\nBut, since one set consists of 22 lines, the actual number of data is 23171.\nI converted it to a format that is easy to use.","metadata":{}},{"cell_type":"code","source":"from kaggle.competitions import nflrush\nenv = nflrush.make_env()\ntrain_df = pd.read_csv('/kaggle/input/nfl-big-data-bowl-2020/train.csv', low_memory=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:17:47.411726Z","iopub.execute_input":"2021-12-09T03:17:47.412051Z","iopub.status.idle":"2021-12-09T03:17:59.006058Z","shell.execute_reply.started":"2021-12-09T03:17:47.412Z","shell.execute_reply":"2021-12-09T03:17:59.005192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_df.columns)\nprint(train_df.head(5))","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:17:59.007712Z","iopub.execute_input":"2021-12-09T03:17:59.008091Z","iopub.status.idle":"2021-12-09T03:17:59.040928Z","shell.execute_reply.started":"2021-12-09T03:17:59.008024Z","shell.execute_reply":"2021-12-09T03:17:59.039747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_df.shape)\nprint(train_df[train_df[\"Yards\"]>0].count)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:17:59.042887Z","iopub.execute_input":"2021-12-09T03:17:59.043334Z","iopub.status.idle":"2021-12-09T03:17:59.431713Z","shell.execute_reply.started":"2021-12-09T03:17:59.043183Z","shell.execute_reply":"2021-12-09T03:17:59.430479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# unused_columns = [\"GameId\",\"PlayId\",\"Team\",\"Yards\",\"TimeHandoff\",\"TimeSnap\"]\nunused_columns = [\"Team\",\"Yards\",\"TimeHandoff\",\"TimeSnap\",\n                 'GameId', 'PlayId','DisplayName','JerseyNumber','HomeTeamAbbr','VisitorTeamAbbr','PlayerCollegeName',\n                 'Location','Stadium','PossessionTeam', 'OffenseFormation', 'OffensePersonnel']","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:17:59.434908Z","iopub.execute_input":"2021-12-09T03:17:59.435271Z","iopub.status.idle":"2021-12-09T03:17:59.439917Z","shell.execute_reply.started":"2021-12-09T03:17:59.435206Z","shell.execute_reply":"2021-12-09T03:17:59.439027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_columns = []\nfor c in train_df.columns:\n    if c not in unused_columns and len(set(train_df[c][:11]))!= 1:\n        unique_columns.append(c)\n        print(c,\" is unique\")","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:17:59.442369Z","iopub.execute_input":"2021-12-09T03:17:59.442667Z","iopub.status.idle":"2021-12-09T03:17:59.46662Z","shell.execute_reply.started":"2021-12-09T03:17:59.442613Z","shell.execute_reply":"2021-12-09T03:17:59.465728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ok = True\nfor i in range(0,509762,22):\n    p=train_df[\"PlayId\"][i]\n    for j in range(1,22):\n        if(p!=train_df[\"PlayId\"][i+j]):\n            ok=False\n            break\nprint(\"train data is sorted by PlayId.\" if ok else \"train data is not sorted by PlayId.\")\nok = True\nfor i in range(0,509762,11):\n    p=train_df[\"Team\"][i]\n    for j in range(1,11):\n        if(p!=train_df[\"Team\"][i+j]):\n            ok=False\n            break\nprint(\"train data is sorted by Team.\" if ok else \"train data is not sorted by Team.\")","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:17:59.469441Z","iopub.execute_input":"2021-12-09T03:17:59.469755Z","iopub.status.idle":"2021-12-09T03:18:18.447208Z","shell.execute_reply.started":"2021-12-09T03:17:59.469694Z","shell.execute_reply":"2021-12-09T03:18:18.446075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the training data was sorted, preprocessing can be done easily.","metadata":{}},{"cell_type":"code","source":"all_columns = []\nfor c in train_df.columns:\n    if c not in unique_columns + unused_columns+[\"DefensePersonnel\",\"GameClock\"]:\n        all_columns.append(c)\nall_columns.append(\"DL\")\nall_columns.append(\"LB\")    \nall_columns.append(\"DB\")\nall_columns.append(\"GameHour\")   \nfor c in unique_columns:\n    for i in range(22):\n        all_columns.append(c+str(i))","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:18:18.449169Z","iopub.execute_input":"2021-12-09T03:18:18.449594Z","iopub.status.idle":"2021-12-09T03:18:18.460538Z","shell.execute_reply.started":"2021-12-09T03:18:18.44952Z","shell.execute_reply":"2021-12-09T03:18:18.45931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(all_columns)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:18:18.462406Z","iopub.execute_input":"2021-12-09T03:18:18.462828Z","iopub.status.idle":"2021-12-09T03:18:18.472977Z","shell.execute_reply.started":"2021-12-09T03:18:18.462756Z","shell.execute_reply":"2021-12-09T03:18:18.472255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lbl_dict = {}\nfor c in train_df.columns:\n    if c == \"DefensePersonnel\":\n        arr = [[int(s[0]) for s in t.split(\", \")] for t in train_df[\"DefensePersonnel\"]]\n        train_df[\"DL\"] = np.array([a[0] for a in arr])\n        train_df[\"LB\"] = np.array([a[1] for a in arr])\n        train_df[\"DB\"] = np.array([a[2] for a in arr])\n    elif c == \"GameClock\":\n        arr = [[int(s) for s in t.split(\":\")] for t in train_df[\"GameClock\"]]\n        train_df[\"GameHour\"] = pd.Series([a[0] for a in arr])\n#     elif c == \"PlayerBirthDate\":\n#         arr = [[int(s) for s in t.split(\"/\")] for t in train_df[\"PlayerBirthDate\"]]\n#         train_df[\"BirthY\"] = pd.Series([a[2] for a in arr])\n    # elif c == \"PlayerHeight\":\n    #     arr = [float(s.split(\"-\")[0]) * 30.48 + float(s.split(\"-\")[1]) * 2.54\n    #         for s in list(train_df[\"PlayerHeight\"])]\n    #     train_df[\"PlayerHeight\"] = pd.Series(arr)\n    elif train_df[c].dtype=='object' and c not in unused_columns: \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(train_df[c].values))\n        lbl_dict[c] = lbl\n        train_df[c] = lbl.transform(list(train_df[c].values))","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:18:18.474465Z","iopub.execute_input":"2021-12-09T03:18:18.474774Z","iopub.status.idle":"2021-12-09T03:18:50.872177Z","shell.execute_reply.started":"2021-12-09T03:18:18.474718Z","shell.execute_reply":"2021-12-09T03:18:50.871106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.columns","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:18:50.87382Z","iopub.execute_input":"2021-12-09T03:18:50.874245Z","iopub.status.idle":"2021-12-09T03:18:50.882998Z","shell.execute_reply.started":"2021-12-09T03:18:50.874113Z","shell.execute_reply":"2021-12-09T03:18:50.881652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data=np.zeros((509762//22,len(all_columns)))\nfor i in tqdm.tqdm(range(0,509762,22)):\n    count=0\n    for c in all_columns:\n        if c in train_df:\n            train_data[i//22][count] = train_df[c][i]\n            count+=1\n#     for c in unique_columns:\n#         for j in range(22):\n#             train_data[i//22][count] = train_df[c][i+j]\n#             count+=1        ","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:18:50.884335Z","iopub.execute_input":"2021-12-09T03:18:50.884601Z","iopub.status.idle":"2021-12-09T03:19:20.358392Z","shell.execute_reply.started":"2021-12-09T03:18:50.884551Z","shell.execute_reply":"2021-12-09T03:19:20.35744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(all_columns)\nprint(unique_columns)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:19:20.360187Z","iopub.execute_input":"2021-12-09T03:19:20.360562Z","iopub.status.idle":"2021-12-09T03:19:20.369599Z","shell.execute_reply.started":"2021-12-09T03:19:20.360465Z","shell.execute_reply":"2021-12-09T03:19:20.367855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_data.shape)\nprint(len(all_columns))\nprint(len(unique_columns))","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:19:20.371672Z","iopub.execute_input":"2021-12-09T03:19:20.372041Z","iopub.status.idle":"2021-12-09T03:19:20.385836Z","shell.execute_reply.started":"2021-12-09T03:19:20.37198Z","shell.execute_reply":"2021-12-09T03:19:20.384752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_ = np.array([train_df[\"Yards\"][i] for i in range(0,509762,22)])\nX_train = pd.DataFrame(data=train_data,columns=all_columns)\nprint(X_train.head)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:19:20.387432Z","iopub.execute_input":"2021-12-09T03:19:20.387729Z","iopub.status.idle":"2021-12-09T03:19:20.882844Z","shell.execute_reply.started":"2021-12-09T03:19:20.387689Z","shell.execute_reply":"2021-12-09T03:19:20.881344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = [0 for i in range(199)]\nfor y in y_train_:\n    data[int(y+99)]+=1\nplt.plot([i-99 for i in range(199)],data)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:19:20.885135Z","iopub.execute_input":"2021-12-09T03:19:20.885549Z","iopub.status.idle":"2021-12-09T03:19:21.152975Z","shell.execute_reply.started":"2021-12-09T03:19:20.885491Z","shell.execute_reply":"2021-12-09T03:19:21.151791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the variance is small, I standardized the objective variable.","metadata":{}},{"cell_type":"code","source":"# scaler = preprocessing.StandardScaler()\n# scaler.fit([[y] for y in y_train_])\n# y_train = np.array([y[0] for y in scaler.transform([[y] for y in y_train_])])\nscaler = preprocessing.StandardScaler()\nscaler.fit(y_train_.reshape(-1, 1))\ny_train = scaler.transform(y_train_.reshape(-1, 1)).flatten()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:28:06.999736Z","iopub.execute_input":"2021-12-09T04:28:07.000107Z","iopub.status.idle":"2021-12-09T04:28:07.01351Z","shell.execute_reply.started":"2021-12-09T04:28:07.000049Z","shell.execute_reply":"2021-12-09T04:28:07.011787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## train\nI used LGBMRegressor.\nI wanted to use multi-class classification, but the number of datasets was small and it was difficult to split them including all labels.","metadata":{}},{"cell_type":"code","source":"# i=0\n# while True:\n#     i+=1\n#     i-=1","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:19:21.167313Z","iopub.execute_input":"2021-12-09T03:19:21.167744Z","iopub.status.idle":"2021-12-09T03:19:26.152799Z","shell.execute_reply.started":"2021-12-09T03:19:21.167675Z","shell.execute_reply":"2021-12-09T03:19:26.150098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y_train好像是经过reshape的，但为什么可以是这样子reshape呢","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:20:48.347032Z","iopub.execute_input":"2021-12-09T03:20:48.347819Z","iopub.status.idle":"2021-12-09T03:20:48.365602Z","shell.execute_reply.started":"2021-12-09T03:20:48.347735Z","shell.execute_reply":"2021-12-09T03:20:48.364369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape)\nprint(X_train.columns)\nfor ele in X_train.columns:\n    print(ele)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:27:54.441947Z","iopub.execute_input":"2021-12-09T04:27:54.442271Z","iopub.status.idle":"2021-12-09T04:27:54.541295Z","shell.execute_reply.started":"2021-12-09T04:27:54.442226Z","shell.execute_reply":"2021-12-09T04:27:54.539844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folds = 5\nseed = 222\nkf = KFold(n_splits = folds, shuffle = True, random_state=seed)\ny_valid_pred = np.zeros(X_train.shape[0])\nmodels = []\n\nfor tr_idx, val_idx in kf.split(X_train, y_train):\n    tr_x, tr_y = X_train.iloc[tr_idx,:], y_train[tr_idx]\n    vl_x, vl_y = X_train.iloc[val_idx,:], y_train[val_idx]\n            \n    print(len(tr_x),len(vl_x))\n    tr_data = lgb.Dataset(tr_x, label=tr_y)\n    vl_data = lgb.Dataset(vl_x, label=vl_y)  \n    clf = lgb.LGBMRegressor(n_estimators=100,learning_rate=0.01)\n    clf.fit(tr_x, tr_y,\n        eval_set=[(vl_x, vl_y)],\n        early_stopping_rounds=20,\n        verbose=False)\n    y_valid_pred[val_idx] += clf.predict(vl_x, num_iteration=clf.best_iteration_)\n    models.append(clf)\n\ngc.collect()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-12-09T03:19:26.154122Z","iopub.status.idle":"2021-12-09T03:19:26.15463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(y_valid_pred)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:19:26.156266Z","iopub.status.idle":"2021-12-09T03:19:26.156835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## evaluation\nContinuous Ranked Probability Score (CRPS) is derived based on the predicted scalar value.\nThe CRPS is computed as follows:\n$$\nC=\\frac{1}{199N}\\sum_{m=1}^N\\sum_{n=-99}^{99}(P(y\\geq n)-H(n-Y_m))^2\n$$\n$H(x)=1$ if $x\\geq 0$ else $0$","metadata":{}},{"cell_type":"code","source":"y_pred = np.zeros((509762//22,199))\ny_ans = np.zeros((509762//22,199))\n\nfor i,p in enumerate(np.round(scaler.inverse_transform(y_valid_pred))):\n    p+=99\n    for j in range(199):\n        if j>=p+10:\n            y_pred[i][j]=1.0\n        elif j>=p-10:\n            y_pred[i][j]=(j+10-p)*0.05\n\nfor i,p in enumerate(scaler.inverse_transform(y_train)):\n    p+=99\n    for j in range(199):\n        if j>=p:\n            y_ans[i][j]=1.0\n\nprint(\"validation score:\",np.sum(np.power(y_pred-y_ans,2))/(199*(509762//22)))","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:19:26.158142Z","iopub.status.idle":"2021-12-09T03:19:26.158653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## make submission\n\nWhen there is a label that does not exist in the training data, it is handled as nan.\nIf you can check the error one by one and complement it, you will get better score.","metadata":{}},{"cell_type":"code","source":"index = 0\nfor (test_df, sample_prediction_df) in tqdm.tqdm(env.iter_test()):\n    for c in test_df.columns:\n        if c == \"DefensePersonnel\":\n            try:\n                arr = [[int(s[0]) for s in t.split(\", \")] for t in test_df[\"DefensePersonnel\"]]\n                test_df[\"DL\"] = [a[0] for a in arr]\n                test_df[\"LB\"] = [a[1] for a in arr]\n                test_df[\"DB\"] = [a[2] for a in arr]\n            except:\n                test_df[\"DL\"] = [np.nan for i in range(22)]\n                test_df[\"LB\"] = [np.nan for i in range(22)]\n                test_df[\"DB\"] = [np.nan for i in range(22)]\n        elif c == \"GameClock\":\n            try:\n                arr = [[int(s) for s in t.split(\":\")] for t in test_df[\"GameClock\"]]\n                test_df[\"GameHour\"] = pd.Series([a[0] for a in arr])\n            except:\n                test_df[\"GameHour\"] = [np.nan for i in range(22)]\n#         elif c == \"PlayerBirthDate\":\n#             try:\n#                 arr = [[int(s) for s in t.split(\"/\")] for t in test_df[\"PlayerBirthDate\"]]\n#                 test_df[\"BirthY\"] = pd.Series([a[2] for a in arr])\n#             except:\n#                 test_df[\"BirthY\"] = [np.nan for i in range(22)]\n        # elif c == \"PlayerHeight\":\n        #     try:\n        #         arr = [float(s.split(\"-\")[0]) * 30.48 + float(s.split(\"-\")[1]) * 2.54\n        #             for s in list(test_df[\"PlayerHeight\"])]\n        #         test_df[\"PlayerHeight\"] = pd.Series(arr)\n        #     except:\n        #         test_df[\"PlayerHeight\"] = [np.nan for i in range(22)]\n        elif c in lbl_dict and test_df[c].dtype=='object'and c not in unused_columns\\\n            and not pd.isnull(test_df[c]).any():\n            try:\n                test_df[c] = lbl_dict[c].transform(list(test_df[c].values))\n            except:\n                test_df[c] = [np.nan for i in range(22)]\n    count=0\n    test_data = np.zeros((1,len(all_columns)))\n\n    for c in all_columns:\n        if c in test_df:\n            try:\n                test_data[0][count] = test_df[c][index]\n            except:\n                test_data[0][count] = np.nan\n            count+=1\n    for c in unique_columns:\n        for j in range(22):\n            try:\n                test_data[0][count] = test_df[c][index + j]\n            except:\n                test_data[0][count] = np.nan\n            count+=1        \n    y_pred = np.zeros(199)        \n    y_pred_p = np.sum(np.round(scaler.inverse_transform(\n        [model.predict(test_data)[0] for model in models])))/folds\n    y_pred_p += 99\n    for j in range(199):\n        if j>=y_pred_p+10:\n            y_pred[j]=1.0\n        elif j>=y_pred_p-10:\n            y_pred[j]=(j+10-y_pred_p)*0.05\n    env.predict(pd.DataFrame(data=[y_pred],columns=sample_prediction_df.columns))\n    index += 22\nenv.write_submission_file()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:19:26.159995Z","iopub.status.idle":"2021-12-09T03:19:26.160447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The organizers seemed to expect to predict one by one, so I did. \nHowever, it seems that it is likely to be faster to predict at once after all the evaluation data is acquired by dummy input.\n\n\nThis model is a simple one that has not been tuned, so I think we can still expect a better score.\nPlease let me know if you have any opinions or advice.","metadata":{}}]}