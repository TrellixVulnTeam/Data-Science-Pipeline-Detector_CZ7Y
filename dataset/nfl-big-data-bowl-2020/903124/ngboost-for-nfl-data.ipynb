{"cells":[{"metadata":{},"cell_type":"markdown","source":"Stanford ML group have release a package call NGBoost (Natural Gradient Boosting) which aims to provide a probablistic estimation on top of a gradient boosting model. Since the competition don't allow internet connection nor external data it's unlikly to be helpful to competition score so it shall only be a demostration."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n!pip install ngboost","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Motivation: in NFL big data bowl 2020 the task is to predict rushing yards gain by offensive formation and result would be judged by Continuous Ranked Probability Score (CRPS). Yards gain data is not distributed by a normal distribution: in fact it is right skewed since the chance of getting positive yardage is much higher than getting negative yards. Therefore we want to try to fit a lognormal distribution which is also righted skewed to see the goodness of fit.\n\nNGBoost package both natively support CRPS and lognormal distribution which is a perfect candidate to test its performance"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../../kaggle/input/nfl-big-data-bowl-2020/train.csv')\n\ndata['YardLine100'] = data['YardLine'] #Normalize yardline to 1-99\ndata.loc[data['FieldPosition'] == data['PossessionTeam'], 'YardLine100'] = 50+  (50-data['YardLine'])\ndata['Touchdown'] =  data.Yards == data.YardLine100\n\ntemp_data = data[data.Touchdown == 0][['YardLine100','Down','Distance','DefendersInTheBox','Yards']].dropna().drop_duplicates()\nX = np.array(temp_data[['YardLine100','Down','Distance','DefendersInTheBox']])\ny = np.array(temp_data['Yards'])*1.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import lognorm\nshape, loc, scale = lognorm.fit(y)\n\nfig, ax = plt.subplots()\nx_axis = np.linspace(-10,50,100)\nax.hist(y,bins=np.arange(-10, y.max() + 1.5) -0.5,density=True)\nax.plot(x_axis,lognorm.pdf(x_axis, shape, loc, scale))\nplt.title('Histogram of rushing yards gain excluding touchdown')\nplt.xlabel('rushing yard gain')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is the histogram of rushing yards and the fit of lognormal distribution. Here we can see the fit is just soso since football data have a sharp peak of 3-4 yards gain and it's difficult to fit by a smooth distribution function. Next we will use NGBoost to see how it works"},{"metadata":{"trusted":true},"cell_type":"code","source":"from ngboost.ngboost import NGBoost\nfrom ngboost.learners import default_tree_learner\nfrom ngboost.scores import CRPS, MLE\nfrom ngboost.distns import LogNormal, Normal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In lognormal distribution the data has to be distributed above zero so a transform is needed to fit the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"ngb =  NGBoost(n_estimators=100, learning_rate=0.1,\n              Dist=LogNormal,\n              Base=default_tree_learner,\n              natural_gradient=False,\n              minibatch_frac=1.0,\n              Score=CRPS())\nngb.fit(X_train,y_train-min(y_train)+0.001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_preds = ngb.predict(X_test)\ny_dists = ngb.pred_dist(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('mean of lognormal scale = %f'% y_dists.scale.mean())\nprint('standard deviation of lognormal scale = %f'%y_dists.scale.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from ngboost.evaluation import *\npctles, observed, slope, intercept = calibration_regression(y_dists, y_test-min(y_test)+0.001)\nplt.subplot(1, 2, 1)\nplot_pit_histogram(pctles, observed, label=\"CRPS\", linestyle = \"-\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In an ideal fit we would expect the bars are close to the dotted line, indicate that it's a somewhat bad fit which is the same case as above.\n\nTo sum up even this competition require using cummulative distribution as competition score, the data is hard to fit by one single distribution so a combination of distribution like Gaussian Mixture model is required."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}