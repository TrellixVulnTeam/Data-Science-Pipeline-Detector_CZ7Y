{"cells":[{"metadata":{},"cell_type":"markdown","source":"# SENG 474 Project\nAndrew Wiggins (V00817291), Matt Ehrler (V00824990), Zenara Daley (V00820899)\n# 1 Introduction\n\nAmerican football is a unique sport that is played by two teams, each with 11 active players on the field at one time. The purpose of the game is for a team to move the ball towards and into the opposing teams end zone. When the ball reaches an end zone a touchdown occurs; this awards six points to the offensive team.   \n\nThe process of moving the ball into the end zone is broken up into downs. A ball must move a minimum of 10 yards towards the defending teams end zone within 4 downs, otherwise possession of the ball is given to the defending team. In summary, the game of football is about gaining territory in order to score points.\n\nRushing is a common action that occurs in football. This simply means that the ball is advanced by a player running with it, in contrast to throwing or kicking the ball. A rush usually occurs when a quarterback player hands off the ball to a runningback player, however the action of rushing is not restricted to just runningbacks; quarterbacks and wide receivers can also rush. The main intent of a rush is to gain as many yards as possible within a down. Approximately one third of all yards gains come from rushing plays. \n\nThe National Football League (NFL) is keen to discover what underlying factors contribute to a successful rush. Understanding what contributes to a successfull rush can \"help teams, media, and fans better understand the skill of players and the strategies of coaches\". Additionally it will help the NFL directly make assesments about the rusher and other game factors.\n\nIn order to find the factors that contribute to a successful rush, the NFL has opened the NFL Big Data Bowl. This competition was published on Kaggle, an online data science and machine learning community. They host large data sets and provides a platform for these collaborative competitions. The NFL Big Data Bowl competition provides a large game and player data set to be used for training. "},{"metadata":{},"cell_type":"markdown","source":"In the following code block, we initialized the project by importing various libraries, defining global variables and map definitions, as well as imported the relevant Kaggle NFL data into a Pandas dataframe."},{"metadata":{"trusted":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport scipy\nimport lightgbm as lgb\nfrom kaggle.competitions import nflrush\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import defaultdict\nfrom sklearn.neighbors import NearestNeighbors\nimport datetime\nimport copy\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nenv = nflrush.make_env()\ncategories = []\nlabelEncoder = LabelEncoder()\nrushers =[]\n\n# open (1) or closed (0)\nin_out_map = defaultdict(int,{\n    \"open\": 1,\n    \"field\": 1, \n    \"out\": 1,\n    \"oud\": 1,\n    \"our\": 1,\n    \"cloud\": 1,\n    \"close\": 0,\n    \"retract\": 0,\n    \"dome\": 0,\n})\n\n#map teams with diffrent visitor/home names\n\n\n# map to values between 0-1\nweather_map = {\n    \"controlled\": 1,\n    \"indoor\": 1,\n    \"indoors\": 1,\n    \"indoors\": 1,\n    \"sunny\": 0.8,\n    \"clear\": 0.6, \n    \"cloudy\": 0.4,\n    \"coudy\": 0.4,\n    \"hazy\": 0.4,\n    \"cool\": 0.4,\n    \"rain\": 0.2,\n    \"rainy\": 0.2,\n    \"cold\": 0,\n    \"snow\": 0,\n}\n\nturf_map = {\n    \"Field Turf\": \"Artificial\",\n    \"A-Turf Titan\": \"Artificial\",\n    \"Grass\": \"Natural\",\n    \"UBU Sports Seed S5-M\": \"Artificial\",\n    \"Artificial\": \"Artificial\",\n    \"DD GrassMaster\": \"Artificial\",\n    \"Natural Grass\": \"Natural\",\n    \"UBU Seed Series-S5-M\": \"Artificial\",\n    \"FieldTurf\": \"Artificial\",\n    \"FieldTurf 360\": \"Artificial\",\n    \"Natural grass\": \"Natural\",\n    \"grass\": \"Natural\",\n    \"Natural\": \"Natural\",\n    \"Artifical\": \"Artificial\",\n    \"FieldTurf360\": \"Artificial\",\n    \"Naturall Grass\": \"Natural\",\n    \"Field turf\": \"Artificial\",\n    \"SISGrass\": \"Artificial\",\n    \"Twenty-Four/Seven Turf\": \"Artificial\",\n    \"natural grass\": \"Natural\" \n}\n\n# Training data is in the competition dataset as usual\ntrain_df = pd.read_csv('/kaggle/input/nfl-big-data-bowl-2020/train.csv', low_memory=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2 Data Collection and Preprocessing\n\nThe following section will describe the steps taken to collect and pre-process the data that will be used for data mining. The collection step is a trivial component and the preprocessing is the non-trivial component of the project. \n\n## 2.1 Collection\n\nAs part of the NFL Big Data Bowl competition, a training data set was given. The data set contains tracking data for running plays in a `.csv` file format. The columns within this file and their descriptions are as follows in the table below:\n\n\n| Column | Description | Type | \n|:-------|:------------|:-----|\n| GameId | a unique game identifier | int |\n| PlayId | a unique play identifier | int |\n| Team | home or away | string |\n| X | player position along the long axis of the field | float |\n| Y | player position along the short axis of the field | float |\n| S | speed in yards/second | float |\n| A | acceleration in yards/second^2 | float |\n| Dis | distance traveled from prior time point, in yards | float |\n| Orientation | orientation of player (deg) | float |\n| Dir | angle of player motion (deg) | float |\n| NflId | a unique identifier of the player | int |\n| DisplayName | player's name | string |\n| JerseyNumber | jersey number | int |\n| Season | year of the season | int |\n| YardLine | the yard line of the line of scrimmage | int |\n| Quarter | game quarter (1-5, 5 == overtime) | int |\n| GameClock | time on the game clock | timestamp |\n| PossessionTeam | team with possession | string |\n| Down | the down (1-4) | int |\n| Distance | yards needed for a first down | int |\n| FieldPosition | which side of the field the play is happening on | string |\n| HomeScoreBeforePlay | home team score before play started | int |\n| VisitorScoreBeforePlay | visitor team score before play started | int |\n| NflIdRusher | the NflId of the rushing player | string |\n| OffenseFormation | offense formation | string |\n| OffensePersonnel | offensive team positional grouping | string |\n| DefendersInTheBox | number of defenders lined up near the line of scrimmage, spanning the width of the offensive line | int |\n| DefensePersonnel | defensive team positional grouping | string |\n| PlayDirection | direction the play is headed | string |\n| TimeHandoff | UTC time of the handoff | timestamp |\n| TimeSnap | UTC time of the snap | timestamp |\n| Yards | the yardage gained on the play (you are predicting this) | int |\n| PlayerHeight | player height (ft-in) | string |\n| PlayerWeight | player weight (lbs) | float |\n| PlayerBirthDate | birth date (mm/dd/yyyy) | string |\n| PlayerCollegeName | where the player attended college | string |\n| Position | the player's position (the specific role on the field that they typically play) | string |\n| HomeTeamAbbr | home team abbreviation | string |\n| VisitorTeamAbbr | visitor team abbreviation | string |\n| Week | week into the season | int |\n| Stadium | stadium where the game is being played | string |\n| Location | city where the game is being player | string |\n| StadiumType | description of the stadium environment | string |\n| Turf | description of the field surface | string |\n| GameWeather | description of the game weather | string |\n| Temperature | temperature (deg F) | float |\n| Humidity | humidity (percentage) | float |\n| WindSpeed | wind speed in miles/hour | float |\n| WindDirection | wind direction | string |"},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Preprocessing\n\nMany of the non-numeric columns did not have a discrete set of values and required data cleaning such as `StadiumType` and `GameWeather`. These columns were resolved using maps of keywords to integer or float values.\n\n\nThere were also numeric columns such as `PlayerHeight` and `WindSpeed` that required cleaning. The `PlayerHeight` column required the height in inches and feet to be converted to a single integer for inches. The `WindSpeed` column required stripping of additional non-numeric strings such as directions. \n\n\nTimestamp columns required special cleaning. The `GameClock` column contained a string of two numbers separated by a colon which represented minutes and seconds; It was split into two columns, `GameClock_minutes` and `GameClock_seconds`. Two columns, `Timesnap` and `Timehandoff` were converted from timestamps to time in seconds since epoch using the python Time library [1].\n\nA major component of preprocessing involved converting both teams to be moving in a standard direction instead of a relative direction according to the `PlayDirection` (right or left). The columns `Orientation`, `X`, `Y`, `Dir`, and `Yardline` were standardized into `StdOrientation`, `StdX`, `StdY`, `StdDir`, and `StdYardline`, respectively. The original columns were then dropped. In order to standardize the `Orientation` and `Direction` columns, 180 degrees was added to all rows with a `PlayDirection` value of left. To standardize the `X` and `Y` columns, the values with a `PlayDirection` value of left were subtracted from 120 and 53.3 (the length and width of the field), respectively.  To convert the `StdYardline` column, the values with a `PlayDirection` value of left were subtracted from 100 (number of yard lines on a field). Finally, the 2017 standard orientation differed from other years and needed to be rotated 90 degrees [2].\n\nIn the following code block, these steps are performed."},{"metadata":{"trusted":false},"cell_type":"code","source":"def windSpeedToInt(windSpeed):\n    x = re.findall('[0-9]*', str(windSpeed))\n    if len(x) > 1 and x[0].isdigit() and x[1].isdigit():\n        return (int(x[0]) + int(x[1]))/ 2\n    if not x[0].isdigit():\n        return np.nan\n    return(x[0])\n\ndef stadiumTypeToInt(stadium):\n    x = re.findall(r\"[\\w']+\", str(stadium).lower())\n    weight = 0\n    if len(x) == 0:\n        return np.nan\n    for word in x:\n        weight += in_out_map[word]\n    weight = weight / len(x)\n    return round(weight)\n\ndef heightToInches(height):\n    return int(height.split('-')[0]) * 12 + int(height.split('-')[1])\n\ndef convertToProb(num):\n    return np.array([1 if i > num + 99 else 0 for i in range(199)])\n    \ndef crps(y_true, y_pred):\n    y_pred = np.clip(np.cumsum(y_pred,axis = 1),0,1)\n    return np.mean((y_pred-y_true)**2)\n\ndef gameWeatherToInt(weather):\n    x = re.findall(r\"[\\w']+\", str(weather).lower())\n    valid = 0\n    weight = 0\n    for word in x:\n        if word in weather_map:\n            weight += weather_map[word]\n            valid += 1\n    if valid == 0:\n        return np.nan\n    return str(weight / valid)\n\ndef gameClockToSeconds(gameClock):\n    times = str(gameClock).split(':')\n    return (int((int(times[0]) * 60) + int(times[1]) * int(times[2]) / 60))\n\ndef cleanData(df):\n    df = df.replace([\"ARZ\",\"BLT\",\"CLV\",\"HST\"],[\"ARI\",\"BAL\",\"CLE\",\"HOU\"])\n    # clean WindSpeed column\n    df[\"WindSpeed\"] = df[\"WindSpeed\"].apply(windSpeedToInt).astype(\"float64\")\n    \n    # convert Height column to inches\n    df[\"PlayerHeight\"] = df[\"PlayerHeight\"].apply(heightToInches)\n    \n    # clean StadiumType column\n    df[\"StadiumType\"] = df[\"StadiumType\"].apply(stadiumTypeToInt).astype(\"category\")\n    \n    # clean GameWeather column\n    df[\"GameWeather\"] = df[\"GameWeather\"].apply(gameWeatherToInt).astype(\"category\")\n    \n    # team offense cleaning\n    df['TeamOnOffense'] = \"home\"\n    df.loc[df.PossessionTeam != df.HomeTeamAbbr, 'TeamOnOffense'] = \"away\"\n    df['IsOnOffense'] = df.Team == df.TeamOnOffense\n    \n    # convert orientation to move in a standard direction\n    df['MovingLeft'] = df.PlayDirection == \"left\"\n    df['StdOrientation'] = df.Orientation\n    df.loc[df.MovingLeft, 'StdOrientation'] = np.mod(180 + df.loc[df.MovingLeft, 'StdOrientation'], 360)\n    \n    # convert X and Y to move in a standard direction\n    df['StdX'] = df.X\n    df.loc[df.MovingLeft, 'StdX'] = 120 - df.loc[df.MovingLeft, 'X'] \n    df['StdY'] = df.Y\n    df.loc[df.MovingLeft, 'StdY'] = 53.3 - df.loc[df.MovingLeft, 'Y'] \n    \n    # convert direction to standard\n    df['StdDir'] = df.Dir\n    df.loc[df.MovingLeft, 'StdDir'] = np.mod(180 + df.loc[df.MovingLeft, 'StdDir'], 360)\n    df.loc[df['Season'] == 2017, 'Orientation'] = np.mod(90 + df.loc[df['Season'] == 2017, 'Orientation'], 360) \n    \n    df['StdYardline'] = 100 - df.YardLine\n    df.loc[df.FieldPosition.fillna('') == df.PossessionTeam,'StdYardline'] = df.loc[df.FieldPosition.fillna('') == df.PossessionTeam,'YardLine']\n    \n    # turf cleaning\n    df['Turf'] = df['Turf'].map(turf_map)\n    \n    # time cleaning\n    df['GameClock_seconds'] = train_df['GameClock'].apply(gameClockToSeconds)\n    df['GameClock_minutes'] = train_df['GameClock'].apply(lambda time: int(time.split(':')[0]))\n    \n    df['TimeHandoff'] = df['TimeHandoff'].apply(lambda time: datetime.datetime.strptime(time, \"%Y-%m-%dT%H:%M:%S.%fZ\"))\n    df['TimeSnap'] = df['TimeSnap'].apply(lambda time: datetime.datetime.strptime(time, \"%Y-%m-%dT%H:%M:%S.%fZ\"))\n    \n    df['TimeDelta'] = df.apply(lambda row: (row['TimeHandoff'] - row['TimeSnap']).total_seconds(), axis=1)\n    \n    df = df.drop([\"X\",\"Y\",\"Dir\",\"Orientation\",\"YardLine\",\"TimeHandoff\",\"TimeSnap\"],axis=1)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.3 Feature Engineering\n\nSince the LGBM model does not work with `NaN`s, any NA/NaN values were converted to `Unknown` using the `fillna` Pandas function. \n\nThe LGBM model works better when any categorical values are encoded as numbers. Therefore, we built a label encoder that encodes every possible category as a number and during test time, if we haven't seen a specific category before, it is encoded as 'unknown'. In order to prevent collisions, every feature value was encoded with the feature title prepended.\n\nAnother portion of the data cleaning condensed each play data into a single row on the rusher index. Before doing this, all other players were taking into consideration through the addition of 2 new columns `NearestOffensivePlayer` and `NearestDefensivePlayer`. These columns were created using the X and Y coordinates and  Scikit Learns neighbours library to find the nearest neighbors to the rusher. Using similar techniques, another column called `NearestLikelyTackle` was created by taking into consideration the distances of every defensive team member with the position of Outside Linebacker (OLB), Inside Linebacker (ILB), Defensive End (DE), or Defensive Tackle (DT), and determining who was closest to the rusher. These positions were chosen due to their likeliness to tackle the rusher during any given play. Since all team players were accounted for in these newly added features, we dropped all rows that weren't the rushing player.\n\nA few other features were constructed because we thought they might help the score. For example, the speed that the rusher is moving towards the end zone at was calculated using trigonometry. We also calculated the distance the rusher is from the first-down line. Considering the first-down line is commonly where the rusher is trying to get it stands that how far away they are from it will be correlated with the amount of yards they run. Next we calculated the force that the rusher is exerting by using Newton's Second Law of Motion (f = ma). This was done because if more force was being exerted then a rusher would be able to run through other players easier. Based on the training data, we calculated the average number of yards that a rusher will carry the ball per play. This was done to determine roughly how 'good' a rusher is doing his job [3]. If a rusher has not been seen in the training data, we set his value to 0.\n\nLastly we dropped many features that we deemed unimportant through vigorous cross-validation testing, such as `Season`, `WindSpeed`, `GameId`, `PlayId`, and `Week`."},{"metadata":{"trusted":false},"cell_type":"code","source":"def pipeline(train_df):\n    train_df = cleanData(train_df)\n    \n    train_df[\"WindDirection\"] = train_df[\"WindDirection\"].fillna(\"Unknown\")\n    train_df[\"FieldPosition\"] = train_df[\"FieldPosition\"].fillna(\"Unknown\")\n    train_df[\"OffenseFormation\"] = train_df[\"OffenseFormation\"].fillna(\"Unknown\")\n    train_df[\"Turf\"] = train_df[\"Turf\"].fillna(\"Unknown\")\n\n    categorical_features = train_df.select_dtypes(include=[\"object\"]).columns\n    \n    global labelEncoder\n    global categories\n    global rushers\n    if len(categories) == 0:\n        for feature in categorical_features:\n            train_df[feature] = feature + \"__\" + train_df[feature].astype(\"str\")\n            categories += train_df[feature].unique().tolist()\n            categories += [feature +\"__Unknown\"]\n        labelEncoder.fit(categories)\n\n        for feature in categorical_features:\n            train_df[feature] = labelEncoder.transform(train_df[feature])\n        rushers = train_df[train_df[\"NflId\"] == train_df[\"NflIdRusher\"]]\n        rushers = rushers.groupby([\"NflId\"])[\"Yards\"].agg(AvgCarry=\"mean\")\n    else:\n        for feature in categorical_features:\n            train_df[feature] = feature + \"__\" + train_df[feature].astype(\"str\")\n            unseenLabels = ~train_df[feature].isin(categories)\n            train_df.loc[unseenLabels,[feature]] = feature + \"__Unknown\"\n            train_df[feature] = labelEncoder.transform(train_df[feature])\n        \n    # get distance to nearest neighbor of rusher\n    train_df[\"NearestOffensivePlayer\"] = 0\n    train_df[\"NearestDefensivePlayer\"] = 0\n    for play in train_df.PlayId.unique():\n        # get all rows in play\n        playXY = train_df.loc[train_df[\"PlayId\"] == play, ['StdX', 'StdY', 'NflId', 'NflIdRusher', 'PossessionTeam', 'Team', 'HomeTeamAbbr',\"Position\"]]\n        # find rusher and their team\n        rusher_team = playXY.loc[playXY['NflId'] == playXY['NflIdRusher'], ['Team']]\n        rusherXY = playXY.loc[playXY['NflId'] == playXY['NflIdRusher'], ['StdX', 'StdY']]\n        # find players and split by offense/defense\n        playersXY = playXY.loc[playXY['NflId'] != playXY['NflIdRusher'], ['StdX', 'StdY', 'Team',\"Position\"]]\n        playerOffenseXY = playersXY.loc[playersXY['Team'] == rusher_team.iloc[0]['Team'], ['StdX', 'StdY',\"Position\"]]\n        playerDefenseXY = playersXY.loc[playersXY['Team'] != rusher_team.iloc[0]['Team'], ['StdX', 'StdY',\"Position\"]]\n        # find X,Y coordinate of nearest offensive/defensive neighbor\n        o_neighbours = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(playerOffenseXY.drop([\"Position\"],axis=1))\n        d_neighbours = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(playerDefenseXY.drop([\"Position\"],axis=1))\n        # find hypotenuse of neighbor\n        o_dist, _ = o_neighbours.kneighbors(rusherXY)\n        d_dist, _ = d_neighbours.kneighbors(rusherXY)\n        train_df.loc[train_df['PlayId'] == play, 'NearestOffensivePlayer'] = o_dist[0][0]\n        train_df.loc[train_df['PlayId'] == play, 'NearestDefensivePlayer'] = d_dist[0][0]\n        \n        # find x, y of likely tackle positions\n        playerOLBXY = playerDefenseXY.loc[playerDefenseXY[\"Position\"] == labelEncoder.transform(['Position__OLB'])[0], ['StdX', 'StdY']]\n        playerILBXY = playerDefenseXY.loc[playerDefenseXY[\"Position\"] == labelEncoder.transform(['Position__ILB'])[0], ['StdX', 'StdY']]\n        playerDEXY = playerDefenseXY.loc[playerDefenseXY[\"Position\"] == labelEncoder.transform(['Position__DE'])[0], ['StdX', 'StdY']]\n        playerDTXY = playerDefenseXY.loc[playerDefenseXY[\"Position\"] == labelEncoder.transform(['Position__DT'])[0], ['StdX', 'StdY']]\n        playersTackleXY = pd.concat([playerOLBXY,playerILBXY,playerDEXY,playerDTXY])\n        if not playersTackleXY.empty:\n            tackle_neighbours = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(playersTackleXY)\n            dis, _ = tackle_neighbours.kneighbors(rusherXY)\n        else:\n            dis = [[1000000]]\n        # find minimum and add feature\n        train_df.loc[train_df['NflId'] == play, 'NearestLikelyTackle'] = dis[0][0]\n        \n    train_df[categorical_features] = train_df[categorical_features].astype('category')\n    \n    train_df = train_df[train_df[\"NflId\"] == train_df[\"NflIdRusher\"]]\n    \n    train_df[\"hspeed\"] = train_df[\"S\"] * np.sin(np.deg2rad(train_df[\"StdDir\"]))\n    train_df[\"to_line\"] = train_df[\"StdYardline\"] - train_df[\"StdX\"]\n    train_df[\"force\"] = train_df[\"A\"] * train_df[\"PlayerWeight\"]\n    train_df = pd.merge(train_df,rushers, on=\"NflId\",how=\"left\")\n    train_df[\"AvgCarry\"] = train_df[\"AvgCarry\"].fillna(0)\n    \n    dropCols = categorical_features.drop([\"Team\",\"OffenseFormation\",\"OffensePersonnel\",\"DefensePersonnel\",\"WindDirection\"])\n    dropCols = dropCols.append(pd.Index([\"NflIdRusher\",\"Season\",\"WindSpeed\",\"GameId\",\"PlayId\",\"Week\"]))\n    #print(dropCols)\n    train_df = train_df.drop(dropCols, axis=1)\n    \n    return train_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3 Data Mining \n\nThe following section will describe how the NFL data was used to train a model and make predictions on the test data. This is a trivial component of the project. \n\n## 3.1 Choosing a Model\n\nSince we were trying to predict a value for each rushing play, we initially tried to solve this as a regression problem with XGBoost. We chose a tree based model as they are easy to use and seem to have been rather successful in past Kaggle competitions. Additionally, the well structured data allowed us to engineer features easily which worked effectively with a tree based algorithm. However, with this approach a lot of data was being lost during the conversion to probability distribution, resulting in a fairly bad score. After seeing the poor quality results, we moved over to classification. Given that all possible values of yards were known, this allowed us to convert to probability distribution without losing data. We also found a fork of XGBoost called LightGBM which is more light weight and provides native support for categorical values. Given that most of our data is categorical, we started using LightGBM with multiclass classification which gave us a significant improvement to our score.\n\n## 3.2 Current Model\n\nThe model used to is [LightGBM](https://lightgbm.readthedocs.io/en/latest/), a light gradient boosting framework. It uses tree based learning algorithms and is designed to be lightweight, using less memory as well as parallel and GPU learning. Our model is training with multiclass classification where each possible discrete value of yards is it's own class. Yards range from -99 to 99 leaving us with 199 classes. This model is used alongside Scikit Learns [KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) library. KFold provides train or test indices to split train or test sets. It will split datasets into k consecutive folds, where a single fold is used as validation and the remaining folds are used to build the training set. For our KFold validator, we use 3 splits and pass in the entire cleaned data set as the training data and the yards column as the target variable. For each split, a LightGBM model is created using the train folds. A prediction is then made with the test folds. The score of the model on each iteration is compared against the best score so far; if the score is better, it replaces the current best score. The best score at the end all iterations is returned as the final prediction model.\n\n## 3.3 Model Output\n\nThe Kaggle competition wanted predictions to be in the form of a cumulative probabilty distribution. Each distribution would have 199 values indexed from -99 yard to 99 yards. Each value corresponds with the probability that the play will gain at most that many yards. For example if we were to predict that the play would gain exactly 3 yards then every index corresponding to 3 yards or higher would be 1. Since our model is doing multiclass classification it outputs the probabilities it thinks will correspond to each class, we convert this to the proper distribution by calculating the cumulative sum and then clipping it to be between 0,1. \n\n## 3.4 Model Parameters\n\nThe input parameters for the model where chosen through cross validation. We tested different values on various folds of the data until we found optimal values. We also kept `num_leaves` low to help prevent overfitting on test data.   "},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"code","source":"def train_my_model(train_df):    \n    train_df = pipeline(train_df)\n\n    columns = list(train_df.columns)\n    columns.remove(\"Yards\")\n    \n    X_train = train_df[columns]\n    Y_train = train_df[\"Yards\"]\n    \n    params = { 'objective' : 'multiclass', 'num_classes': 199,\n              'num_leaves': 19, 'feature_fraction': 0.4,\n              'subsample': 0.4, 'min_child_samples': 10, 'num_threads': 5,\n              'learning_rate': 0.01, 'num_iterations': 100, 'random_state': 42}\n   \n    bestScore = 1000\n    bestModel = \"\"\n    \n    kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n\n    for train_index, test_index in kfold.split(X_train, Y_train):\n        # train folds\n        X_train_fold = X_train.iloc[train_index]\n        Y_train_fold = Y_train.iloc[train_index]\n\n        # test folds\n        X_test_fold = X_train.iloc[test_index]\n        Y_test_fold = Y_train.iloc[test_index]\n        \n        Y_train_fold = Y_train_fold + 99\n \n        dataSet = lgb.Dataset(X_train_fold,label=Y_train_fold)\n        model = lgb.train(params,dataSet)\n    \n        Y_pred = model.predict(X_test_fold)\n       \n        Y_test_fold = np.vstack(pd.Series(Y_test_fold).apply(lambda x: convertToProb(x)))\n        score = crps(Y_test_fold,Y_pred)\n        if score < bestScore:\n            bestModel = copy.deepcopy(model)\n            bestScore = score\n        print(score)\n    return bestModel","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.5 Feature Importance\nSince we are using a tree based model we are able to view the most important features. A significant number of our engineered features are among the most important features when we're selecting in terms of information gain. "},{"metadata":{"trusted":false},"cell_type":"code","source":"model = train_my_model(train_df)\nlgb.plot_importance(model,importance_type=\"gain\",max_num_features=30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.6 Prediction\n\nThe prediction made is a cumulative probability distribution of gained or lost yards (i.e. the probability for a team to gain or lose yards during a play). In order to make a prediction on a line of new test data, it must go through the same data cleaning pipeline described in section 2.2. Next using the model described in section 3.1 a prediction can be made on new test data. This step is repeated for each play in the test data. "},{"metadata":{"trusted":false},"cell_type":"code","source":"def predict(test_df,model):\n    test_df = pipeline(test_df)\n    return  pd.DataFrame(np.clip(np.cumsum(model.predict(test_df),axis = 1),0,1),columns=sample_prediction_df.columns)\n\nfor (test_df, sample_prediction_df) in env.iter_test():\n    predictions_df = predict(test_df, model)\n    env.predict(predictions_df)\n\nenv.write_submission_file()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4 Evaluation\n\nAs part of the NFL Big Data Bowl competion, an evaluation of the model is given once a prediction is submitted. The evaluation is posted on a leaderboard and scores can be ranked and compared. Submissions are ranked using the [Continuous Ranked Probability Score (CRPS)](https://www.kaggle.com/c/nfl-big-data-bowl-2020/overview/evaluation). A CRPS score of zero is the goal. Currently the public leaderboard scores range from `0.01201` to `0.49962`. Our test score of `0.01415` puts us at approximately 1425 out of 2038 total competitors. This was a bit surprising because even after many new features were engineered and implemented that significantly improved our validiation score, the test score did not change at all. This led us to believe there may be some kind of internal Kaggle caching issue. On the other hand, our validation score was `0.01382` which was a significant improvement on our test score. This difference could potentially be due to overfitting."},{"metadata":{},"cell_type":"markdown","source":"# References\n[1] https://www.kaggle.com/mrkmakr/lgbm-multiple-classifier/notebook\n\n[2] https://www.kaggle.com/pednt9/vip-hint-coded\n\n[3] https://www.espn.com/nfl/story/_/id/20114211/the-nfl-stats-matter-most-2017-offseason-bill-barnwell"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}