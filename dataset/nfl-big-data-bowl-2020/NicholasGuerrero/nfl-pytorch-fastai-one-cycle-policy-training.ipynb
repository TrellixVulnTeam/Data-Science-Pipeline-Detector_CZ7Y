{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import mean_squared_error\nimport gc\nfrom tqdm import tqdm_notebook as tqdm\nimport random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\n\nfrom kaggle.competitions import nflrush\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai.basics import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_path = '/kaggle/input/nfl-big-data-bowl-2020/train.csv'\ntrain = pd.read_csv(train_path)\nprint(train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"use_cols = [\n    'GameId', \n    'PlayId', \n    'Team',\n    'Yards',\n    'X',\n    'Y',\n    'PossessionTeam',\n    'HomeTeamAbbr',\n    'VisitorTeamAbbr',\n    'Position',\n]\ntrain = train[use_cols]\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nFind an offense team.\nref: https://www.kaggle.com/c/nfl-big-data-bowl-2020/discussion/112314#latest-648026\n\"\"\"\ndef fe_is_offence(row):\n    if row[\"Team\"] == \"home\":\n        if row[\"HomeTeamAbbr\"] == row[\"PossessionTeam\"]:\n            return 1\n        else:\n            return 0\n    elif row[\"Team\"] == \"away\":\n        if row[\"VisitorTeamAbbr\"] == row[\"PossessionTeam\"]:\n            return 1\n        else:\n            return 0\n\ndef fe_is_offence_from_position(row, off_team):\n    if row[\"Team\"] == off_team:\n        return 1\n    else:\n        return 0\n        \n# def run_fe_is_offence(df):\n#     df['is_offence'] = df.apply(lambda row: fe_is_offence(row), axis=1)\n    \n#     if (df['is_offence'].values == 0).all():\n#         off_team = df[df['Position']=='QB']['Team'].values[0]\n#         df['is_offence'] = df.apply(lambda row: fe_is_offence_from_position(row, off_team), axis=1)\n\n\"\"\"\nbugfix\n\"\"\"\ndef run_fe_is_offence(df):\n    df['is_offence'] = df.apply(lambda row: fe_is_offence(row), axis=1)\n    \n    check_is_offence = df.groupby('PlayId')['is_offence'].nunique()\n    is_offence_not_found_idx = check_is_offence[check_is_offence!=2].index\n    not_found_df = df[df['PlayId'].isin(is_offence_not_found_idx)]\n    found_df = df[~df['PlayId'].isin(is_offence_not_found_idx)]\n#     print('is_offence found: {}'.format(len(found_df)))\n#     print('is_offence not found: {}'.format(len(not_found_df)))\n\n    for u_play_id in not_found_df['PlayId'].unique():\n        tmp_df = not_found_df[not_found_df['PlayId']==u_play_id]\n        pos_list = [pos for pos in tmp_df['Position'].unique() if pos in ['QB', 'RB', 'WR', 'TE']]\n        \n        if len(pos_list) > 0:\n            off_team = tmp_df[tmp_df['Position']==pos_list[0]]['Team'].values[0]\n#         else:\n#             print('Offence position not found')\n#             import pdb;pdb.set_trace()\n\n        target_idx = not_found_df.query('PlayId==@u_play_id and Team==@off_team').index\n        not_found_df.loc[target_idx, 'is_offence'] = 1\n    \n    df = pd.concat([found_df, not_found_df], sort=False)\n#     print('done df: {}'.format(df.shape))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_group_fe(df, group_key, aggs):\n    \n    group_df = df.groupby(group_key).agg(aggs)\n\n    new_cols = [col[0]+'_'+col[1] for col in group_df.columns]\n    group_df.columns = new_cols\n    group_df.reset_index(inplace=True)\n        \n    return group_df\n\ndef adjust_group_df(group_df, is_train):\n    offence_df = group_df[group_df['is_offence']==1]\n    deffence_df = group_df[group_df['is_offence']==0]\n\n    del group_df['is_offence']\n    del offence_df['is_offence']\n    del deffence_df['is_offence']\n    \n    if is_train:\n        off_cols = ['off_{}'.format(col) if col not in ['GameId', 'PlayId', 'Yards'] else col for col in group_df.columns]\n        deff_cols = ['deff_{}'.format(col) if col not in ['GameId', 'PlayId', 'Yards'] else col for col in group_df.columns]\n    else:\n        off_cols = ['off_{}'.format(col) if col not in ['GameId', 'PlayId'] else col for col in group_df.columns]\n        deff_cols = ['deff_{}'.format(col) if col not in ['GameId', 'PlayId'] else col for col in group_df.columns]\n        \n    offence_df.columns = off_cols\n    deffence_df.columns = deff_cols\n    if is_train: del deffence_df['Yards']\n    \n    adjusted_group_df = pd.merge(offence_df, deffence_df, on=['GameId', 'PlayId'])\n    \n    return adjusted_group_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = run_fe_is_offence(train)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_group_key = ['GameId', 'PlayId', 'is_offence', 'Yards']\naggs = {\n    'X': ['mean', 'max', 'min', 'median'],\n    'Y': ['mean', 'max', 'min', 'median'],\n}\nis_train = True\ngroup_df = run_group_fe(train, train_group_key, aggs)\nadjusted_group_df = adjust_group_df(group_df, is_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(adjusted_group_df.shape)\nadjusted_group_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class NFL_NN(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n\n        self.fc1 = nn.Linear(in_features, 216)\n        self.bn1 = nn.BatchNorm1d(216)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Linear(216, 512)\n        self.relu2 = nn.ReLU()\n        self.fc3 = nn.Linear(512, 216)\n        self.relu3 = nn.ReLU()\n        self.dout3 = nn.Dropout(0.2)\n        self.out = nn.Linear(216, out_features)\n        self.out_act = nn.Sigmoid()\n        \n    def forward(self, input_):\n        a1 = self.fc1(input_)\n        bn1 = self.bn1(a1)\n        h1 = self.relu1(bn1)\n        a2 = self.fc2(h1)\n        h2 = self.relu2(a2)\n        a3 = self.fc3(h2)\n        h3 = self.relu3(a3)\n        dout3 = self.dout3(h3)\n        a5 = self.out(dout3)\n        y = self.out_act(a5)\n        return a5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epoch = 10\nbatch_size = 64","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_crps_list = []\nfold = GroupKFold(n_splits=5)\n\n\ny = np.zeros(shape=(adjusted_group_df.shape[0], 199))\nfor i, yard in enumerate(adjusted_group_df['Yards'].values):\n#     print(i, yard)\n    y[i, yard+99:] = np.ones(shape=(1, 100-yard))\n\noof_preds = np.ones((len(adjusted_group_df), y.shape[1]))\n\nfeats = [\n        \"off_X_mean\",\"off_X_max\",\"off_X_min\",\"off_X_median\",\"off_Y_mean\",\"off_Y_max\",\"off_Y_min\",\"off_Y_median\",\n        \"deff_X_mean\",\"deff_X_max\",\"deff_X_min\",\"deff_X_median\",\"deff_Y_mean\",\"deff_Y_max\",\"deff_Y_min\",\"deff_Y_median\",\n    ]\n\nprint('use feats: {}'.format(len(feats)))\n\nfor n_fold, (train_idx, valid_idx) in enumerate(fold.split(adjusted_group_df, y, groups=adjusted_group_df['GameId'])):\n        print('Fold: {}'.format(n_fold+1))\n        \n        train_x, train_y = adjusted_group_df[feats].iloc[train_idx].values, y[train_idx]\n        valid_x, valid_y = adjusted_group_df[feats].iloc[valid_idx].values, y[valid_idx] \n\n#         train_x = torch.tensor(train_x, requires_grad=True).float()\n#         train_y = torch.tensor(train_y, requires_grad=True).float()\n#         valid_x = torch.tensor(valid_x, requires_grad=False).float()\n#         valid_y = torch.tensor(valid_y, requires_grad=False).float()\n\n        train_x = torch.from_numpy(train_x).float()\n        train_y = torch.from_numpy(train_y).float()\n        valid_x = torch.from_numpy(valid_x).float()\n        valid_y = torch.from_numpy(valid_y).long()\n\n        train_dataset = TensorDataset(train_x, train_y)\n        valid_dataset = TensorDataset(valid_x, valid_y)\n\n        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n        valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n        \n        print('train: {}, valid: {}'.format(len(train_dataset), len(valid_dataset)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = DataBunch.create(train_dataset, valid_dataset, bs=batch_size, num_workers=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"in_features = adjusted_group_df[feats].shape[1]\nout_features = y.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# loss_func = nn.MSELoss()\n# optimizer = torch.optim.Adam(model.parameters(), lr=0.005)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CustomLoss(nn.Module):\n    \n    def __init__(self):\n        super(CustomLoss, self).__init__()\n        \n    def forward(self,y_hat,target):\n        return torch.sqrt(nn.MSELoss()(y_hat.float(), target.view((len(target), out_features)).float()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CRPS(Callback):\n    def on_epoch_begin(self, **kwargs):\n        self.crps, self.targ_count = 0, 0\n        \n    def on_batch_end(self, last_output, last_target, **kwargs):\n        self.crps += np.sum(np.power(last_output.numpy() - last_target.numpy(), 2))\n        self.targ_count += len(last_target)\n        \n    def on_epoch_end(self, last_metrics, **kwargs):\n        \n        return add_metrics(last_metrics, self.crps / (199*self.targ_count))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = Learner(data, NFL_NN(in_features, out_features), loss_func=CustomLoss(), metrics=CRPS())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.lr_find()\nlearn.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(5, slice(1e-2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.recorder.plot_losses()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# x_bs,y_bs = next(iter(data.train_dl))\n# x_bs.shape, y_bs.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def min_max_scaler(x):\n    return (x - np.min(x)) / (np.max(x) - np.min(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env = nflrush.make_env()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_df = None\nis_train = False\ntest_group_key = ['GameId', 'PlayId', 'is_offence']\n\nfor (test_df, sample_prediction_df) in tqdm(env.iter_test()):\n    \n    test_df = run_fe_is_offence(test_df)\n    test_group_df = run_group_fe(test_df, test_group_key, aggs)\n    test_adjusted_group_df = adjust_group_df(test_group_df, is_train)\n    \n    feats = [\n        \"off_X_mean\",\"off_X_max\",\"off_X_min\",\"off_X_median\",\"off_Y_mean\",\"off_Y_max\",\"off_Y_min\",\"off_Y_median\",\n        \"deff_X_mean\",\"deff_X_max\",\"deff_X_min\",\"deff_X_median\",\"deff_Y_mean\",\"deff_Y_max\",\"deff_Y_min\",\"deff_Y_median\",\n    ]\n    test = torch.from_numpy(test_adjusted_group_df[feats].values)\n    test_dataset = TensorDataset(test)\n    test_loader = DataLoader(test_dataset, batch_size, shuffle=False)\n    \n    in_features = test_adjusted_group_df[feats].shape[1]\n    out_features = 199\n    \n    learn.model.eval()\n    preds = np.zeros((len(test_dataset), out_features))\n    \n    with torch.no_grad():\n        for i, test_x_batch in enumerate(test_loader):\n            test_values = test_x_batch[0].float()\n            pred = learn.model(test_values)\n            preds[i * batch_size:(i + 1) * batch_size] = pred\n            \n    y_pred = preds.copy()\n    adjust_preds = np.zeros((len(y_pred), y_pred.shape[1]))\n    for idx, pred in enumerate(y_pred):\n        \n        prev = 0\n        for i in range(len(pred)):\n            if pred[i]<prev:\n                pred[i]=prev\n            prev=pred[i]\n        x = min_max_scaler(pred)\n        adjust_preds[idx, :] = x\n\n    adjust_preds[:, -1] = 1\n    adjust_preds[:, 0] = 0\n\n    preds_df = pd.DataFrame(data=adjust_preds.reshape(-1, 199), columns=sample_prediction_df.columns)\n    env.predict(preds_df)\n\n    if result_df is None:\n        result_df = preds_df\n    else:\n        result_df = pd.concat([result_df, preds_df], sort=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env.write_submission_file()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_df.drop_duplicates().shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_df.head(30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}