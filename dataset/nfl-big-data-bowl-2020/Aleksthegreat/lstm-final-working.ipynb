{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# IMPORTS \nimport numpy as np\nimport pandas as pd\nimport datetime\n\nimport sklearn.metrics as mtr\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow.keras as keras\nfrom tensorflow.keras.layers import Dense, LSTM\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import Callback, EarlyStopping\nfrom tensorflow.keras.models import Model\nfrom tensorflow.python.keras.layers import Input, Concatenate, Reshape, Dropout, merge, Add, Layer, BatchNormalization\nfrom tensorflow.keras.layers import Embedding\nfrom keras.models import load_model\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nimport tensorflow_probability as tfp\ntfd = tfp.distributions\n\nfrom sklearn.model_selection import KFold,GroupKFold\nimport warnings\nimport random as rn\nimport os\n\nwarnings.filterwarnings(\"ignore\")\nfrom kaggle.competitions import nflrush\nenv = nflrush.make_env()\niter_test = env.iter_test()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# evaluation metric\ndef crps(y_true, y_pred):\n    y_true = np.clip(np.cumsum(y_true, axis=1), 0, 1)\n    y_pred = np.clip(np.cumsum(y_pred, axis=1), 0, 1)\n    return ((y_true - y_pred) ** 2).sum(axis=1).sum(axis=0) / (199 * y_true.shape[0]) \n\n\n# author : nlgn\n# Link : https://www.kaggle.com/kingychiu/keras-nn-starter-crps-early-stopping\nclass Metric(Callback):\n    def __init__(self, model, callbacks, data):\n        super().__init__()\n        self.model = model\n        self.callbacks = callbacks\n        self.data = data\n\n    def on_train_begin(self, logs=None):\n        for callback in self.callbacks:\n            callback.on_train_begin(logs)\n\n    def on_train_end(self, logs=None):\n        for callback in self.callbacks:\n            callback.on_train_end(logs)\n\n    def on_epoch_end(self, batch, logs=None):\n        X_train, y_train = self.data[0][0], self.data[0][1]\n        y_pred = self.model.predict(X_train)\n        y_true = np.clip(np.cumsum(y_train, axis=1), 0, 1)\n        y_pred = np.clip(np.cumsum(y_pred, axis=1), 0, 1)\n        tr_s = ((y_true - y_pred) ** 2).sum(axis=1).sum(axis=0) / (199 * X_train[-1].shape[0])\n        tr_s = np.round(tr_s, 6)\n        logs['tr_CRPS'] = tr_s\n\n        X_valid, y_valid = self.data[1][0], self.data[1][1]\n\n        y_pred = self.model.predict(X_valid)\n        y_true = np.clip(np.cumsum(y_valid, axis=1), 0, 1)\n        y_pred = np.clip(np.cumsum(y_pred, axis=1), 0, 1)\n        val_s = ((y_true - y_pred) ** 2).sum(axis=1).sum(axis=0) / (199 * X_valid[-1].shape[0])\n        val_s = np.round(val_s, 6)\n        logs['val_CRPS'] = val_s\n        print('tr CRPS', tr_s, 'val CRPS', val_s)\n\n        for callback in self.callbacks:\n            callback.on_epoch_end(batch, logs)\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def strtoseconds(txt):\n    txt = txt.split(':')\n    ans = int(txt[0])*60 + int(txt[1]) + int(txt[2])/60\n    return ans\n\ndef strtofloat(x):\n    try:\n        return float(x)\n    except:\n        return -1\n\ndef map_weather(txt):\n    ans = 1\n    if pd.isna(txt):\n        return 0\n    if 'partly' in txt:\n        ans*=0.5\n    if 'climate controlled' in txt or 'indoor' in txt:\n        return ans*3\n    if 'sunny' in txt or 'sun' in txt:\n        return ans*2\n    if 'clear' in txt:\n        return ans\n    if 'cloudy' in txt:\n        return -ans\n    if 'rain' in txt or 'rainy' in txt:\n        return -2*ans\n    if 'snow' in txt:\n        return -3*ans\n    return 0\n\ndef OffensePersonnelSplit(x):\n    dic = {'DB' : 0, 'DL' : 0, 'LB' : 0, 'OL' : 0, 'QB' : 0, 'RB' : 0, 'TE' : 0, 'WR' : 0}\n    for xx in x.split(\",\"):\n        xxs = xx.split(\" \")\n        dic[xxs[-1]] = int(xxs[-2])\n    return dic\n\ndef DefensePersonnelSplit(x):\n    dic = {'DB' : 0, 'DL' : 0, 'LB' : 0, 'OL' : 0}\n    for xx in x.split(\",\"):\n        xxs = xx.split(\" \")\n        dic[xxs[-1]] = int(xxs[-2])\n    return dic\n\ndef orientation_to_cat(x):\n    x = np.clip(x, 0, 360 - 1)\n    try:\n        return str(int(x/15))\n    except:\n        return \"nan\"\n    \n    \ndef preprocess(df):\n    \n    train = df[['PlayId','GameId','WindSpeed','GameWeather','Turf','OffenseFormation','OffensePersonnel','DefensePersonnel','HomeScoreBeforePlay', 'VisitorScoreBeforePlay']]\n    ## WindSpeed\n    train['WindSpeed_ob'] = train['WindSpeed'].apply(lambda x: x.lower().replace('mph', '').strip() if not pd.isna(x) else x)\n    train['WindSpeed_ob'] = train['WindSpeed_ob'].apply(lambda x: (int(x.split('-')[0])+int(x.split('-')[1]))/2 if not pd.isna(x) and '-' in x else x)\n    train['WindSpeed_ob'] = train['WindSpeed_ob'].apply(lambda x: (int(x.split()[0])+int(x.split()[-1]))/2 if not pd.isna(x) and type(x)!=float and 'gusts up to' in x else x)\n\n    ## Weather\n    train['GameWeather_process'] = train['GameWeather'].str.lower()\n    train['GameWeather_process'] = train['GameWeather_process'].apply(lambda x: \"indoor\" if not pd.isna(x) and \"indoor\" in x else x)\n    train['GameWeather_process'] = train['GameWeather_process'].apply(lambda x: x.replace('coudy', 'cloudy').replace('clouidy', 'cloudy').replace('party', 'partly') if not pd.isna(x) else x)\n    train['GameWeather_process'] = train['GameWeather_process'].apply(lambda x: x.replace('clear and sunny', 'sunny and clear') if not pd.isna(x) else x)\n    train['GameWeather_process'] = train['GameWeather_process'].apply(lambda x: x.replace('skies', '').replace(\"mostly\", \"\").strip() if not pd.isna(x) else x)\n    #train['GameWeather_dense'] = train['GameWeather_process'].apply(map_weather)\n\n    ## Turf\n    Turf = {'Field Turf':'Artificial', 'A-Turf Titan':'Artificial', 'Grass':'Natural', 'UBU Sports Speed S5-M':'Artificial', 'Artificial':'Artificial', 'DD GrassMaster':'Artificial', 'Natural Grass':'Natural', 'UBU Speed Series-S5-M':'Artificial', 'FieldTurf':'Artificial', 'FieldTurf 360':'Artificial', 'Natural grass':'Natural', 'grass':'Natural', 'Natural':'Natural', 'Artifical':'Artificial', 'FieldTurf360':'Artificial', 'Naturall Grass':'Natural', 'Field turf':'Artificial', 'SISGrass':'Artificial', 'Twenty-Four/Seven Turf':'Artificial', 'natural grass':'Natural'} \n    train['Turf'] = train['Turf'].map(Turf)\n\n    ## OffensePersonnel\n    temp = train[\"OffensePersonnel\"].iloc[np.arange(0, len(train), 22)].apply(lambda x : pd.Series(OffensePersonnelSplit(x)))\n    temp.columns = [\"Offense\" + c for c in temp.columns]\n    temp[\"PlayId\"] = train[\"PlayId\"].iloc[np.arange(0, len(train), 22)]\n    train = train.merge(temp, on = \"PlayId\")\n\n    ## DefensePersonnel\n    temp = train[\"DefensePersonnel\"].iloc[np.arange(0, len(train), 22)].apply(lambda x : pd.Series(DefensePersonnelSplit(x)))\n    temp.columns = [\"Defense\" + c for c in temp.columns]\n    temp[\"PlayId\"] = train[\"PlayId\"].iloc[np.arange(0, len(train), 22)]\n    train = train.merge(temp, on = \"PlayId\")\n    train = train.drop_duplicates()\n    \n    ## diff Score\n    train[\"diffScoreBeforePlay\"] = train[\"HomeScoreBeforePlay\"] - train[\"VisitorScoreBeforePlay\"]\n\n    ## sort\n#     train = train.sort_values(by = ['X']).sort_values(by = ['Dis']).sort_values(by=['PlayId', 'Team', 'IsRusher']).reset_index(drop = True)\n#   train = train.sort_values(by = ['X']).sort_values(by = ['Dis']).sort_values(by=['PlayId', 'IsRusherTeam', 'IsRusher']).reset_index(drop = True)\n    return train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# author : ryancaldwell\n# Link : https://www.kaggle.com/ryancaldwell/location-eda\ndef create_features(df, deploy=False):\n    def new_X(x_coordinate, play_direction):\n        if play_direction == 'left':\n            return 120.0 - x_coordinate\n        else:\n            return x_coordinate\n\n    def new_line(rush_team, field_position, yardline):\n        if rush_team == field_position:\n            # offense starting at X = 0 plus the 10 yard endzone plus the line of scrimmage\n            return 10.0 + yardline\n        else:\n            # half the field plus the yards between midfield and the line of scrimmage\n            return 60.0 + (50 - yardline)\n\n    def new_orientation(angle, play_direction):\n        if play_direction == 'left':\n            new_angle = 360.0 - angle\n            if new_angle == 360.0:\n                new_angle = 0.0\n            return new_angle\n        else:\n            return angle\n\n    def euclidean_distance(x1,y1,x2,y2):\n        x_diff = (x1-x2)**2\n        y_diff = (y1-y2)**2\n\n        return np.sqrt(x_diff + y_diff)\n\n    def back_direction(orientation):\n        if orientation > 180.0:\n            return 1\n        else:\n            return 0\n\n    def update_yardline(df):\n        new_yardline = df[df['NflId'] == df['NflIdRusher']]\n        new_yardline['YardLine'] = new_yardline[['PossessionTeam','FieldPosition','YardLine']].apply(lambda x: new_line(x[0],x[1],x[2]), axis=1)\n        new_yardline = new_yardline[['GameId','PlayId','YardLine']]\n\n        return new_yardline\n\n    def update_orientation(df, yardline):\n        df['X'] = df[['X','PlayDirection']].apply(lambda x: new_X(x[0],x[1]), axis=1)\n        df['Orientation'] = df[['Orientation','PlayDirection']].apply(lambda x: new_orientation(x[0],x[1]), axis=1)\n        df['Dir'] = df[['Dir','PlayDirection']].apply(lambda x: new_orientation(x[0],x[1]), axis=1)\n\n        df = df.drop('YardLine', axis=1)\n        df = pd.merge(df, yardline, on=['GameId','PlayId'], how='inner')\n\n        return df\n\n    def back_features(df):\n        carriers = df[df['NflId'] == df['NflIdRusher']][['GameId','PlayId','NflIdRusher','X','Y','Orientation','Dir','YardLine']]\n        carriers['back_from_scrimmage'] = carriers['YardLine'] - carriers['X']\n        carriers['back_oriented_down_field'] = carriers['Orientation'].apply(lambda x: back_direction(x))\n        carriers['back_moving_down_field'] = carriers['Dir'].apply(lambda x: back_direction(x))\n        carriers = carriers.rename(columns={'X':'back_X',\n                                            'Y':'back_Y'})\n        carriers = carriers[['GameId','PlayId','NflIdRusher','back_X','back_Y','back_from_scrimmage','back_oriented_down_field','back_moving_down_field']]\n\n        return carriers\n\n    def features_relative_to_back(df, carriers):\n        player_distance = df[['GameId','PlayId','NflId','X','Y']]\n        player_distance = pd.merge(player_distance, carriers, on=['GameId','PlayId'], how='inner')\n        player_distance = player_distance[player_distance['NflId'] != player_distance['NflIdRusher']]\n        player_distance['dist_to_back'] = player_distance[['X','Y','back_X','back_Y']].apply(lambda x: euclidean_distance(x[0],x[1],x[2],x[3]), axis=1)\n\n        player_distance = player_distance.groupby(['GameId','PlayId','back_from_scrimmage','back_oriented_down_field','back_moving_down_field'])\\\n                                         .agg({'dist_to_back':['min','max','mean','std']})\\\n                                         .reset_index()\n        player_distance.columns = ['GameId','PlayId','back_from_scrimmage','back_oriented_down_field','back_moving_down_field',\n                                   'min_dist','max_dist','mean_dist','std_dist']\n\n        return player_distance\n\n    def defense_features(df):\n        rusher = df[df['NflId'] == df['NflIdRusher']][['GameId','PlayId','Team','X','Y']]\n        rusher.columns = ['GameId','PlayId','RusherTeam','RusherX','RusherY']\n\n        defense = pd.merge(df,rusher,on=['GameId','PlayId'],how='inner')\n        defense = defense[defense['Team'] != defense['RusherTeam']][['GameId','PlayId','X','Y','RusherX','RusherY']]\n        defense['def_dist_to_back'] = defense[['X','Y','RusherX','RusherY']].apply(lambda x: euclidean_distance(x[0],x[1],x[2],x[3]), axis=1)\n\n        defense = defense.groupby(['GameId','PlayId'])\\\n                         .agg({'def_dist_to_back':['min','max','mean','std']})\\\n                         .reset_index()\n        defense.columns = ['GameId','PlayId','def_min_dist','def_max_dist','def_mean_dist','def_std_dist']\n\n        return defense\n    \n    def offensive_features(df):\n        rusher = df[df['NflId'] == df['NflIdRusher']][['GameId','PlayId','Team','X','Y']]\n        rusher.columns = ['GameId','PlayId','RusherTeam','RusherX','RusherY']\n\n        offense = pd.merge(df,rusher,on=['GameId','PlayId'],how='inner')\n        offense = offense[offense['Team'] == offense['RusherTeam']][['GameId','PlayId','X','Y','RusherX','RusherY']]\n        offense['off_dist_to_back'] = offense[['X','Y','RusherX','RusherY']].apply(lambda x: euclidean_distance(x[0],x[1],x[2],x[3]), axis=1)\n\n        offense = offense.groupby(['GameId','PlayId'])\\\n                         .agg({'off_dist_to_back':['min','max','mean','std']})\\\n                         .reset_index()\n        offense.columns = ['GameId','PlayId','off_min_dist','off_max_dist','off_mean_dist','off_std_dist']\n\n        return offense\n\n    def static_features(df):\n        static_features = df[df['NflId'] == df['NflIdRusher']][['GameId','PlayId','X','Y','S','A','Dis','Orientation','Dir',\n                                                            'YardLine','Quarter','Down','Distance','DefendersInTheBox']].drop_duplicates()\n        static_features['DefendersInTheBox'] = static_features['DefendersInTheBox'].fillna(np.mean(static_features['DefendersInTheBox']))\n\n        return static_features\n    \n    def split_personnel(s):\n        splits = s.split(',')\n        for i in range(len(splits)):\n            splits[i] = splits[i].strip()\n\n        return splits\n\n    def defense_formation(l):\n        dl = 0\n        lb = 0\n        db = 0\n        other = 0\n\n        for position in l:\n            sub_string = position.split(' ')\n            if sub_string[1] == 'DL':\n                dl += int(sub_string[0])\n            elif sub_string[1] in ['LB','OL']:\n                lb += int(sub_string[0])\n            else:\n                db += int(sub_string[0])\n\n        counts = (dl,lb,db,other)\n\n        return counts\n\n    def offense_formation(l):\n        qb = 0\n        rb = 0\n        wr = 0\n        te = 0\n        ol = 0\n\n        sub_total = 0\n        qb_listed = False\n        for position in l:\n            sub_string = position.split(' ')\n            pos = sub_string[1]\n            cnt = int(sub_string[0])\n\n            if pos == 'QB':\n                qb += cnt\n                sub_total += cnt\n                qb_listed = True\n            # Assuming LB is a line backer lined up as full back\n            elif pos in ['RB','LB']:\n                rb += cnt\n                sub_total += cnt\n            # Assuming DB is a defensive back and lined up as WR\n            elif pos in ['WR','DB']:\n                wr += cnt\n                sub_total += cnt\n            elif pos == 'TE':\n                te += cnt\n                sub_total += cnt\n            # Assuming DL is a defensive lineman lined up as an additional line man\n            else:\n                ol += cnt\n                sub_total += cnt\n\n        # If not all 11 players were noted at given positions we need to make some assumptions\n        # I will assume if a QB is not listed then there was 1 QB on the play\n        # If a QB is listed then I'm going to assume the rest of the positions are at OL\n        # This might be flawed but it looks like RB, TE and WR are always listed in the personnel\n        if sub_total < 11:\n            diff = 11 - sub_total\n            if not qb_listed:\n                qb += 1\n                diff -= 1\n            ol += diff\n\n        counts = (qb,rb,wr,te,ol)\n\n        return counts\n    \n    def combine_features(relative_to_back, defense, offense, static, prep, deploy=deploy):\n        df = pd.merge(relative_to_back,defense,on=['GameId','PlayId'],how='inner')\n        df = pd.merge(df,offense,on=['GameId','PlayId'],how='inner')\n        df = pd.merge(df,static,on=['GameId','PlayId'],how='inner')\n        df = pd.merge(df,prep,on=['GameId','PlayId'],how='inner')\n\n        if not deploy:\n            df = pd.merge(df, outcomes, on=['GameId','PlayId'], how='inner')\n\n        return df\n    \n    prep = preprocess(df)\n    yardline = update_yardline(df)\n    df = update_orientation(df, yardline)\n    back_feats = back_features(df)\n    rel_back = features_relative_to_back(df, back_feats)\n    def_feats = defense_features(df)\n    off_feats = offensive_features(df)\n    static_feats = static_features(df)\n    basetable = combine_features(rel_back, def_feats, off_feats, static_feats, prep, deploy=deploy)\n    \n    return basetable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain = pd.read_csv('../input/nfl-big-data-bowl-2020/train.csv', dtype={'WindSpeed': 'object'})\noutcomes = train[['GameId','PlayId','Yards']].drop_duplicates()\ntrain_basetable = create_features(train, False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train_basetable.drop_duplicates().copy()\nyards = X.Yards\nX = X.drop(['PlayId', 'Yards'], axis = 1)\n\ny = np.zeros((yards.shape[0], 199))\nfor idx, target in enumerate(list(yards)):\n    y[idx][99 + target] = 1\n\nprint(X.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_features = []\ndense_features = []\nfor col in X.columns:\n    if X[col].dtype =='object':\n        cat_features.append(col)\n        print(\"*cat*\", col, len(X[col].unique()))\n    else:\n        dense_features.append(col)\n        print(\"!dense!\", col, len(X[col].unique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_features = [e for e in cat_features if e not in ['JerseyNumber_ob','Quarter_ob','Week_ob','Down_ob','WindSpeed',\n                                                     'GameClock_minute','StadiumType','TimeDelta_ob','TeamOnOffense',\n                                                     'YardLine_ob', 'PlayerHeight', 'WindSpeed_ob','WindDirection','GameWeather']]\n\ncat_features = cat_features + (['back_oriented_down_field','back_moving_down_field', 'diffScoreBeforePlay','DefendersInTheBox'])\nprint(cat_features)\n\ndense_features = [e for e in dense_features if e not in ['GameClock_sec', 'back_oriented_down_field','back_from_scrimmage',\n                                                         'back_moving_down_field','Position','Team','PlayDirection','Quarter','Down',\n                                                         'Turf','GameWeather_process','diffScoreBeforePlay','HomeScoreBeforePlay','VisitorScoreBeforePlay','DefendersInTheBox',\n                                                         'TimeDelta','WindDirection','OffenseFormation','OffensePersonnel',\n                                                         'DefensePersonnel','num_DL', 'num_LB', 'num_DB', 'num_QB', 'num_RB',\n                                                         'num_WR', 'num_TE', 'num_OL', 'OL_diff', 'OL_TE_diff','Yards','GameId',\n                                                         'OffenseDB','OffenseDL','OffenseLB','OffenseOL','OffenseQB','OffenseRB',\n                                                         'OffenseTE','OffenseWR','DefenseDB','DefenseDL','DefenseLB','DefenseOL']]\nprint(dense_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#clean cat\ncategories = []\nmost_appear_each_categories = {}\nfor col in cat_features:\n    print(col)\n    X.loc[:,col] = X[col].fillna(\"nan\")\n    X.loc[:,col] = col + \"__\" + X[col].astype(str)\n    most_appear_each_categories[col] = list(X[col].value_counts().index)[0]\n    categories.append(X[col].unique())\ncategories = np.hstack(categories)\nprint(len(categories))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#encoding cat\nle = LabelEncoder()\nle.fit(categories)\nfor col in cat_features:\n    print(col)\n    X.loc[:, col] = le.transform(X[col])\nnum_classes = len(le.classes_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sss = {}\nmedians = {}\nfor col in dense_features:\n    print(col)\n    X[col] = X[col].replace([np.inf, -np.inf], np.nan)\n    medians[col] = np.nanmedian(X[col])\n    X.loc[:, col] = X[col].fillna(medians[col])\n    ss = StandardScaler()\n    X.loc[:, col] = ss.fit_transform(X[col].values[:,None])\n    sss[col] = ss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def posterior_mean_field(kernel_size, bias_size=0, dtype=None):\n      n = kernel_size + bias_size\n      c = np.log(np.expm1(1.))\n      return tf.keras.Sequential([\n          tfp.layers.VariableLayer(2 * n, dtype=dtype),\n          tfp.layers.DistributionLambda(lambda t: tfd.Independent(  # pylint: disable=g-long-lambda\n              tfd.Normal(loc=t[..., :n],\n                         scale=1e-5 + tf.nn.softplus(c + t[..., n:])),\n              reinterpreted_batch_ndims=1)),])\n\ndef prior_trainable(kernel_size, bias_size=0, dtype=None):\n      n = kernel_size + bias_size\n      return tf.keras.Sequential([\n          tfp.layers.VariableLayer(n, dtype=dtype),\n          tfp.layers.DistributionLambda(\n              lambda t: tfd.Independent(tfd.Normal(loc=t, scale=1),  # pylint: disable=g-long-lambda\n                                        reinterpreted_batch_ndims=1)),])\n\nclass GaussianLayer(Layer):\n\n    def __init__(self, output_dim, **kwargs):\n        self.output_dim = output_dim\n        super(GaussianLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        super(GaussianLayer, self).build(input_shape) \n\n    def call(self, x):\n        xx = K.arange(-99, 100, dtype=tf.float32)\n        sigma = tf.identity(K.exp(0.5 * tf.reshape(x[:, 1], (-1, 1))), name=\"sigma\")\n        pdf = 1/(sigma* K.sqrt(2 * tf.constant(m.pi))) * K.exp( -(tf.subtract(xx, tf.reshape(x[:, 0], (-1, 1))))**2 / (2 * sigma**2) )\n        cdf = []\n        for i in range(199):\n            if i == 0:\n                cdf += [tf.reshape(pdf[:, i], (-1, 1))]\n            else:\n                cdf += [cdf[i-1] + tf.reshape(pdf[:, i], (-1, 1))]\n        return tf.concat(cdf, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], self.output_dim)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def l1_reg(weight_matrix):\n    return 0.01 * K.sum(K.abs(weight_matrix))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_LSTM_1():\n    inputs = []\n    embeddings = []\n    \n    input_numeric = Input(shape=(len(dense_features),))\n    inputs.append(input_numeric)\n    embeddings.append(input_numeric)\n\n\n    #embedding_numeric = Dense(128, activation='relu')(input_numeric)\n    #embedding_numeric = Dropout(0.5)(embedding_numeric)\n    \n    for col in cat_features:\n        \n        no_of_unique_cat  = int(np.absolute(X[col]).max())\n        embedding_size = min(np.ceil((no_of_unique_cat)/2), 4)\n        \n        input_cat = Input(shape=(1,))\n        inputs.append(input_cat)\n\n        embedding = Embedding(no_of_unique_cat+1, embedding_size, input_length=1, embeddings_regularizer=l1_reg)(input_cat)\n        embedding = LSTM(128)(embedding)\n        #embedding = tfp.layers.Convolution1DFlipout(embedding_size, kernel_size=4, padding='SAME', activation=\"elu\")(embedding)\n        #embedding = Reshape(target_shape=(embedding_size,))(embedding)\n        #embedding = Concatenate()([embedding, embedding_numeric])\n        #embedding = tfp.layers.DenseFlipout(32, activation='relu')(embedding)\n        #embedding = Dropout(0.5)(embedding)\n\n        embeddings.append(embedding)\n            \n    x = Concatenate()(embeddings)\n    x = Dense(1024, input_dim=X.shape[1], activation='relu')(x)\n    x = Dropout(0.5)(x)\n    x = BatchNormalization()(x)\n    x = Dense(512, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    x = BatchNormalization()(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    x = BatchNormalization()(x)\n    x = keras.layers.Dense(128, activation='relu')(x)\n    \n    output = Dense(199, activation='softmax')(x)\n    model = Model(inputs, output)\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def return_step(x):\n    temp = np.zeros(199)\n    temp[x + 99:] = 1\n    return temp\n\n#train_y_raw = train[\"Yards\"].iloc[np.arange(0, len(train), 22)].reset_index(drop = True)\ntrain_y_raw = outcomes['Yards']\ntrain_y = np.vstack(outcomes['Yards'].apply(return_step).values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\nn_splits = 3\nkf = GroupKFold(n_splits=n_splits)\nmodels = []\nscore = []\n\nfor k in range(2):\n    #kfold = KFold(5, random_state = 42 + k, shuffle = True)\n    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42 + k)\n    #for i_369, (tdx, vdx) in enumerate(kf.split(X, y, X['GameId'])):\n    for i_lstm, (tdx, vdx) in enumerate(kfold.split(X, train_y_raw)):\n        print(f'Fold : {i_lstm}')\n        \n        K.clear_session()\n        X_train, X_val, y_train, y_val = X.iloc[tdx], X.iloc[vdx], y[tdx], y[vdx]\n        X_train = [X_train[dense_features]] + [X_train[col] for col in cat_features]\n        X_val = [X_val[dense_features]] + [X_val[col] for col in cat_features]\n\n        model = model_LSTM_1()\n        model.compile(optimizer=keras.optimizers.Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999), loss='categorical_crossentropy', metrics=[])\n\n        es = EarlyStopping(monitor='val_CRPS', \n                       mode='min',\n                       restore_best_weights=True, \n                       verbose=2, \n                       patience=5)\n        \n        es.set_model(model)\n        metric = Metric(model, [es], [(X_train,y_train), (X_val,y_val)])\n        \n        model.fit(X_train, y_train, callbacks=[metric], epochs=200, batch_size=1024, verbose=1)\n        models.append(model)\n\n        score_ = crps(y_val, model.predict(X_val))\n\n        print(f'keras_LSTM_{i_lstm}.h5')\n        print(score_)\n        score.append(score_)\n\nprint(np.mean(score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.utils import plot_model\nplot_model(model, to_file='model.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for (test_df, sample_prediction_df) in iter_test:\n    basetable = create_features(test_df, deploy=True)\n    test = basetable.drop('PlayId', axis = 1)\n\n    ### categorical\n    for col in (cat_features):\n        test.loc[:,col] = test[col].fillna(\"nan\")\n        test.loc[:,col] = col + \"__\" + test[col].astype(str)\n        isnan = ~test.loc[:,col].isin(categories)\n        if np.sum(isnan) > 0:\n    #             print(\"------\")\n    #             print(\"test have unseen label : col\")\n            if not ((col + \"__nan\") in categories):\n    #                 print(\"not nan in train : \", col)\n                test.loc[isnan,col] = most_appear_each_categories[col]\n            else:\n    #                 print(\"nan seen in train : \", col)\n                test.loc[isnan,col] = col + \"__nan\"\n    for col in (cat_features):\n        test.loc[:, col] = le.transform(test[col])\n\n    ### dense\n    for col in dense_features:\n        test[col] = test[col].replace([np.inf, -np.inf], np.nan)\n        test.loc[:, col] = test[col].fillna(medians[col])\n        test.loc[:, col] = sss[col].transform(test[col].values[:,None])\n\n    ### divide\n    #test = [test.iloc[np.arange(k, len(test), 22)].reset_index(drop = True) for k in range(22)]\n    test_ = [test[dense_features]] + [test[col] for col in  cat_features]\n\n    y_pred = np.mean([model.predict(test_) for model in models], axis=0)\n    y_pred = np.clip(np.cumsum(y_pred, axis=1), 0, 1).tolist()[0]\n\n    preds_df = pd.DataFrame(data=[y_pred], columns=sample_prediction_df.columns)\n    env.predict(preds_df)\n\nenv.write_submission_file()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}