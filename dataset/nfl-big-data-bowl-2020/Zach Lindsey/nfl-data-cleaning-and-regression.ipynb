{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom math import sqrt\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cleaning the NFL Dataset\n\nThis notebook is my personal set of functions I'd like to use the clean up the NFL dataset. It mostly:\n\n1. Drops columns I feel would not be useful or involve too much work to prepare (for now)6.\n2. Rolls play data into a single row\n3. Resets play to move to the right for all entries\n4. Makes player X,Y relative to the ball carrier\n5. Standardizes team abbreviations\n6. Other simple conversions\n\nAt the end, we try out some simple regression models."},{"metadata":{"trusted":true},"cell_type":"code","source":"from math import sin\nfrom math import cos\nfrom math import radians","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data = pd.read_csv('/kaggle/input/nfl-big-data-bowl-2020/train.csv')\nraw_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Missing Values\n\nAs we can see from the following, Temperature is missing from nearly 10% of the entries. I don't think it will be so valuable for prediction, so I'll drop the whole column. Humidity is also missing from lots of the entries, so I'll drop it, too. I can always revisit these later.\n\nOffense formation is missing from some, and requires some parsing to make useful, so I'll drop it for now.\n\nDefenders in the box is probably very useful for predicting run yardage, so I'll keep it and drop rows that are missing it.\n\nField position is essential for some of the data pipelines I want to put it, so I will drop plays missing that value for now.\n\nFinally, we have orientation and direction data missing from some of the samples, so I will drop those, as well.\n\nNote that if I want to drop a row, I must actually drop all 22 rows with the same playID."},{"metadata":{"trusted":true},"cell_type":"code","source":"numplays = len(raw_data.index)\nprint(numplays)\nraw_data.isna().sum()/numplays*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rowdrops = ['FieldPosition', 'DefendersInTheBox', 'Orientation', 'Dir', 'OffenseFormation']\n\nbadplays = raw_data[raw_data[rowdrops].isna().sum(axis=1) > 0]['PlayId']\nbadrows = raw_data[raw_data['PlayId'].isin(badplays)]\nraw_data.drop(badrows.index, inplace=True)\nraw_data.shape[0]/22\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Drops\n\nDisplayname is probably not relevant, since NflId is already a unique identifier for each player. PlayerCollegeName is also probably not important for predicting performance compared to other features. \n\nThe snap and handoff times are dropped for now. The delay between snap and handoff is almost always either 1 or 2 seconds, so I'm not sure how informative it is. I also would need to know the kickoff time to determine if length of game would be informative, but this data is also implicit in the quarter and gameclock information.\n\nTemperature and humidity are dropped due to frequently missing values.\n\nThe other rows here need to be parsed before they can be useful, and I'll deal with that later if I want to."},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.drop(['DisplayName', 'PlayerCollegeName'], axis=1, inplace=True)\n\nraw_data.drop(['TimeHandoff', 'TimeSnap'], axis=1, inplace=True)\n\nraw_data.drop(['Temperature', 'Humidity'], axis=1, inplace=True)\n\nparse_later = ['WindSpeed',\n               'WindDirection',\n               'GameWeather',\n               'Turf',\n               'StadiumType',\n               'Location',\n               'Stadium',\n              'OffensePersonnel']\nraw_data.drop(parse_later, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Making Direction of Play Consistent\n\nFirst, let's make the play always go to the right. "},{"metadata":{"trusted":true},"cell_type":"code","source":"dirSign = raw_data['PlayDirection'] == 'right'\n\nraw_data['X'] = raw_data['X']*dirSign+(120-raw_data['X'])*~dirSign\nraw_data['Dir'] = raw_data['Dir']*dirSign - raw_data['Dir']*~dirSign\nraw_data['Orientation'] = raw_data['Orientation']*dirSign - raw_data['Orientation']*~dirSign\n\nraw_data.drop(['PlayDirection'], axis=1, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Standardize Team Abbreviations\n\nBefore doing some of the other preprocessing, we need the team abbreviations to all be consistent."},{"metadata":{"trusted":true},"cell_type":"code","source":"# standardize the abbreviations\n\ndef abbrConv(abbr):\n    '''\n    convert from the XTeamAbbr and fieldPosition to PossesionTeam\n    '''\n    if abbr == 'ARI':\n        return 'ARZ'\n    elif abbr == 'BAL':\n        return 'BLT'\n    elif abbr == 'CLE':\n        return 'CLV'\n    elif abbr == 'HOU':\n        return 'HST'\n    else:\n        return abbr\n    \nraw_data['HomeTeamAbbr'] = raw_data['HomeTeamAbbr'].apply(abbrConv)\nraw_data['VisitorTeamAbbr'] = raw_data['VisitorTeamAbbr'].apply(abbrConv)\nraw_data['FieldPosition'] = raw_data['FieldPosition'].apply(abbrConv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Convert Yardline to X-Value\n\nThe yardline variable is from 0-50, so we should turn it into an X value. Here's how the logic goes:\n\nIf the possesion team is the same as the field position team the ball is on the left side and we just add 10 to the yardline.\nIf the possesion team is not the same as the field position, the ball is on the right side of the field and X = 110-yardline"},{"metadata":{"trusted":true},"cell_type":"code","source":"possmask = raw_data['FieldPosition'] == raw_data['PossessionTeam']\nraw_data['YardLineX'] = possmask*(raw_data['YardLine']+10) + (1-possmask)*(110-raw_data['YardLine'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Convert Angles to X, Y components"},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data['VX'] = raw_data['X'].apply(lambda a: sin(radians(a)))\nraw_data['VY'] = raw_data['Y'].apply(lambda a: cos(radians(a)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Parse age"},{"metadata":{"trusted":true},"cell_type":"code","source":"def getAge(birthday):\n    # epxress birthday in years old\n    return (pd.Timestamp.now() - pd.Timestamp(birthday)).days/365\n\nraw_data['Age'] = raw_data['PlayerBirthDate'].apply(getAge)\nraw_data.drop('PlayerBirthDate', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Parse Height"},{"metadata":{"trusted":true},"cell_type":"code","source":"def parseHeight(height):\n    # convert from ft-in to just inches\n    feet, inches = map(int, height.split('-'))\n    return 12*feet + inches\nraw_data['PlayerHeight'] = raw_data['PlayerHeight'].apply(parseHeight)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Add Feature for Distance to Runner"},{"metadata":{"trusted":true},"cell_type":"code","source":"runner_table = raw_data[raw_data['NflId'] == raw_data['NflIdRusher']]\nraw_data = raw_data.merge(runner_table[['PlayId','X','Y']], on='PlayId', suffixes=('','Runner'))\nraw_data['SqDistToRunner'] = (raw_data['X']-raw_data['XRunner'])**2 + (raw_data['Y']-raw_data['YRunner'])**2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Make X, Y Relative to the Runner"},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data['X'] = raw_data['X'] - raw_data['XRunner']\nraw_data['Y'] = raw_data['Y'] - raw_data['YRunner']\nraw_data.drop(['XRunner','YRunner'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Convert Gameclock to Seconds"},{"metadata":{"trusted":true},"cell_type":"code","source":"def gameClock_to_seconds(clock):\n    # convert mm:ss:xx to seconds\n    minutes, seconds, _ = map(int, clock.split(':'))\n    return 60*minutes+seconds\n\nraw_data['GameClock'] = raw_data['GameClock'].apply(gameClock_to_seconds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Alter \"XScoreBeforePlay\"\n\nInstead of Home and Visitor, make them Offense and Defense. In the process, add columsn that give the team name, whether or not they are on offense. Then drop possteam."},{"metadata":{"trusted":true},"cell_type":"code","source":"# first, add a column to indicate the team abbr\nhomemask = (raw_data['Team'] == 'home')\nraw_data.loc[homemask,'TeamAbbr'] = raw_data[homemask]['HomeTeamAbbr']\nraw_data.loc[~homemask,'TeamAbbr'] = raw_data[~homemask]['VisitorTeamAbbr']\n\nraw_data['Offense'] = raw_data['TeamAbbr'] == raw_data['PossessionTeam']\nraw_data.drop('PossessionTeam', axis=1, inplace=True)\n\nraw_data['foomyscore'] = homemask*raw_data['HomeScoreBeforePlay'] + (1-homemask)*raw_data['VisitorScoreBeforePlay']\nraw_data['footheirscore'] = ~homemask*raw_data['HomeScoreBeforePlay'] + (1-~homemask)*raw_data['VisitorScoreBeforePlay']\nraw_data['OffenseScore'] = raw_data['Offense']*raw_data['foomyscore'] + ~raw_data['Offense']*raw_data['footheirscore']\nraw_data['DefenseScore'] = ~raw_data['Offense']*raw_data['foomyscore'] + raw_data['Offense']*raw_data['footheirscore']\nraw_data.drop(['foomyscore','footheirscore'], axis=1, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Consolidating Rows with Merge\n\nEach play has 22 rows, so we need to convert each of them to one row. The first way I tried to do this took forever, since it didn't rely on column operations, which are slow. Maybe this way will go faster?\n\nFirst, separate out the columns that concern a single play."},{"metadata":{"trusted":true},"cell_type":"code","source":"play_data = raw_data[['GameId',\n                      'PlayId',\n                      'Season',\n                      'YardLine',\n                      'Quarter',\n                      'GameClock',\n                      'Down',\n                      'Distance',\n                      'HomeScoreBeforePlay',\n                      'VisitorScoreBeforePlay',\n                      'NflIdRusher',\n                      'OffenseFormation',\n                      'DefendersInTheBox',\n                      'Yards',\n                      'HomeTeamAbbr',\n                      'VisitorTeamAbbr',\n                      'Week',\n                      'YardLineX',\n                      'OffenseScore',\n                      'DefenseScore']]\nplay_data = play_data.iloc[::22,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now get the player data\nplayer_data = raw_data[['PlayId',\n                        'Team',\n                        'TeamAbbr',\n                        'X',\n                        'Y',\n                        'S',\n                        'A',\n                        'VX',\n                        'VY',\n                        'SqDistToRunner',\n                        'Dis',\n                        'Orientation',\n                        'Dir',\n                        'NflId',\n                        'JerseyNumber',\n                        'PlayerHeight',\n                        'PlayerWeight',\n                        'Position',\n                        'Offense'\n                       ]]\n\nplayer_data.sort_values(['PlayId','Offense','SqDistToRunner'], inplace=True)\nplayer_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fat_table = play_data\nfor i in range(22):\n    fat_table = fat_table.merge(player_data.iloc[i::22,], on='PlayId', suffixes=['',str(i)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fat_table","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fat_table.dtypes.value_counts()\n\nobjectcols = []\nboolcols = []\nfor col in fat_table.columns:\n    if fat_table.dtypes[col] == 'object':\n        objectcols.append(col)\n    elif fat_table.dtypes[col] == 'bool':\n        boolcols.append(col)\n        \nfrom sklearn.preprocessing import LabelEncoder\nobjEncoders = []\nfor col in objectcols:\n    encoder = LabelEncoder()\n    fat_table[col] = encoder.fit_transform(fat_table[col])\n    objEncoders.append( (col,encoder))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Regression Models\n\nLet's start with trying to predict yardage with some orindary regression models. We can refit the output of the model to be appropriate for the official scoring later. Since I'm familiar with them, I want to focus on \n\n1. Ordinary least squares\n2. Ridge regression\n3. Lasso regression\n4. ElasticNet Regression\n5. Logistc regression battery\n\nBefore doing that, let's standardize all the columns.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\n\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.metrics import r2_score as R2\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.preprocessing import StandardScaler\n\nfeatures = [col for col in fat_table.columns if col != 'Yards']\nX = fat_table[features]\ny = fat_table['Yards']\n\n# linreg - no hyperparams\n# ridge - alpha\n# lasso - alpha\n# elastic net - alpha, l1_ratio","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ordinary Least Squares"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX_train, X_test, y_train, y_test = train_test_split(fat_table[features], fat_table['Yards'])\nX_train = StandardScaler().fit_transform(X_train)\nlinreg = LinearRegression().fit(X_train, y_train)\n\nX_test = StandardScaler().fit_transform(X_test)\nyhat = linreg.predict(X_test)\n\n\nprint('RMSE: ', sqrt(MSE(yhat, y_test)))\nprint('R2: ', R2(yhat, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ridge"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n\nalphas = [0.01,0.05,0.1,0.5,1,5,10,50,100,500,1000,5000,10000,50000,100000]\n\nridge_search = GridSearchCV(Ridge(), param_grid={'alpha':alphas}, cv=5)\n\nXscaled = StandardScaler().fit_transform(X)\nridge_search.fit(Xscaled, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_search.cv_results_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = plt.bar(range(1,1+len(alphas)), ridge_search.cv_results_['mean_test_score'], yerr=ridge_search.cv_results_['std_test_score'])\nplt.gca().set_xticklabels(alphas, rotation=50)\nbest_ridge = ridge_search.best_estimator_\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lasso"},{"metadata":{"trusted":true},"cell_type":"code","source":"alphas = [0.01,0.05,0.1,0.5,1,2,4]\n\nlasso_search = GridSearchCV(Lasso(), param_grid={'alpha':alphas}, cv=5)\n\nXscaled = StandardScaler().fit_transform(X)\nlasso_search.fit(Xscaled, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso_search.cv_results_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = plt.bar(range(1,1+len(alphas)), lasso_search.cv_results_['mean_test_score'], yerr=lasso_search.cv_results_['std_test_score'])\nplt.gca().set_xticklabels(alphas, rotation=50)\nbest_lasso = lasso_search.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso_coeffs = pd.Series(best_lasso.coef_, index=features)\nlasso_coeffs.sort_values()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"good_coeffs = lasso_coeffs[abs(lasso_coeffs) > 0.1].index\ngood_coeffs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Summary\n\nJust doing regression on the raw features is not good! However, its nice to see that the closest defender and runner's stats coming up as the most impotant features."},{"metadata":{},"cell_type":"markdown","source":"# Alternative Features - Counting Nearby Players\n\nThe raw regression results are not very promising. Let's see if we can think of some better features. How about we count up the differences in how many players are 1, 2, 3 etc yards from the runner?"},{"metadata":{"trusted":true},"cell_type":"code","source":"d_dist_cols = ['SqDistToRunner'] + ['SqDistToRunner' + str(i) for i in range(1,11)]\nDdists = fat_table[d_dist_cols]\no_dist_cols = ['SqDistToRunner' + str(i) for i in range(11,22)]\nOdists = fat_table[o_dist_cols]\nfor i in range(1,10):\n    fat_table['Owithin' + str(i)] = Odists[Odists < i**2].count(axis=1)\n    fat_table['Dwithin' + str(i)] = Ddists[Ddists < i**2].count(axis=1)\n    fat_table['diffwithin' + str(i)] = fat_table['Owithin'+ str(i)] - fat_table['Dwithin'+ str(i)]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"j = 5\nprint(fat_table['diffwithin'+str(j)].unique())\n\nfor i in fat_table['diffwithin'+str(j)].unique():\n    plt.hist(fat_table[fat_table['diffwithin'+str(j)] == i]['Yards'], alpha=.4, bins=20)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [col for col in fat_table.columns if col != 'Yards']\nX = fat_table[features]\ny = fat_table['Yards']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX_train, X_test, y_train, y_test = train_test_split(fat_table[features], fat_table['Yards'])\nX_train = StandardScaler().fit_transform(X_train)\nlinreg = LinearRegression().fit(X_train, y_train)\n\nX_test = StandardScaler().fit_transform(X_test)\nyhat = linreg.predict(X_test)\n\n\nprint('SMSE: ', sqrt(MSE(yhat, y_test)))\nprint('R2: ', R2(yhat, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alphas = [0.01,0.05,0.1,0.5,1,5,10,50,100,500,1000,5000,10000,50000,100000]\n\nridge_search = GridSearchCV(Ridge(), param_grid={'alpha':alphas}, cv=5)\n\nXscaled = StandardScaler().fit_transform(X)\nridge_search.fit(Xscaled, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_search.cv_results_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = plt.bar(range(1,1+len(alphas)), ridge_search.cv_results_['mean_test_score'], yerr=ridge_search.cv_results_['std_test_score'])\nplt.gca().set_xticklabels(alphas, rotation=50)\nbest_ridge = ridge_search.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alphas = [0.0001,0.005,0.01,0.05,0.1,0.5,1,2,4]\n\nlasso_search = GridSearchCV(Lasso(), param_grid={'alpha':alphas}, cv=5)\n\nXscaled = StandardScaler().fit_transform(X)\nlasso_search.fit(Xscaled, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso_search.cv_results_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = plt.bar(range(0,len(alphas)), lasso_search.cv_results_['mean_test_score'], yerr=lasso_search.cv_results_['std_test_score'])\nplt.gca().set_xticks(range(len(alphas)))\nplt.gca().set_xticklabels(alphas, rotation=50, ha='right')\nbest_lasso = lasso_search.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso_coeffs = pd.Series(best_lasso.coef_, index=features)\nlasso_coeffs.sort_values()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"good_coeffs = lasso_coeffs[abs(lasso_coeffs) > 0.1].index\ngood_coeffs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"F = good_coeffs\nX2 = fat_table[F]\ny2 = fat_table['Yards']\nLinearRegression().fit(X2,y2).score(X2,y2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}