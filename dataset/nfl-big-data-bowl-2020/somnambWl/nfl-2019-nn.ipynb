{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Imports\n\n# Standard library\nimport collections\nimport gc\nimport pickle\nimport glob\nimport re\nimport string\nimport copy\nimport time\nimport os\nimport random\n\n# Specific imports from standard library\nfrom bisect import bisect\nfrom collections import Counter\nfrom functools import partial\n\n# Basic imports\nimport numpy as np\nimport pandas as pd\n\n# Scikit-learn\nimport sklearn\nfrom sklearn.base import TransformerMixin\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.utils import resample\nfrom sklearn.decomposition import PCA\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\n\n# SciPy\nfrom scipy.spatial.distance import squareform, pdist\nfrom scipy.spatial import ConvexHull\n\n# LightGBM\nimport lightgbm\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor, LGBMClassifier\n\n# Graphs\n%matplotlib notebook\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# HyperOpt\nfrom hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING\n\n# Progress bar\nfrom tqdm.auto import tqdm\ntqdm.pandas()\n\n# Keras for NNs\nimport keras\nfrom keras import backend as K\nfrom keras.models import Sequential, Model\nfrom keras.layers import Input, Dense, Embedding, Concatenate, Flatten, BatchNormalization, Dropout\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras.utils.vis_utils import plot_model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Advanced settings for graphs\n\n# Seaborn advanced                                                                                                                                                           \nsns.set(style='ticks',          # 'ticks', 'darkgrid'                                                                                                                        \n        palette='colorblind',   # 'colorblind', 'pastel', 'muted', 'bright'                                                                                                  \n        #palette=sns.color_palette('Accent'),   # 'Set1', 'Set2', 'Dark2', 'Accent'                                                                                          \n        rc = {                                                                                                                                                               \n           'figure.autolayout': True,   # Automaticky nastaví velikost grafu, aby se vešel do obrazu                                                                         \n           'figure.figsize': (10, 6),    # Velikost obrázku - šířka, výška (v palcích)                                                                                       \n           'legend.frameon': True,      # Rámeček okolo legendy                                                                                                              \n           'patch.linewidth': 2.0,      # Velikost čáry okolo rámečku                                                                                                        \n           'lines.markersize': 6,       # Velikost bodů                                                                                                                      \n           'lines.linewidth': 2.0,      # Tloušťka čar                                                                                                                       \n           'font.size': 20,             # Velikost hodnot na osách                                                                                                           \n           'legend.fontsize': 20,       # Velikost textu v legendě                                                                                                           \n           'axes.labelsize': 16,        # Velikost názvů os                                                                                                                  \n           'axes.titlesize': 22,        # Velikost nadpisu                                                                                                                   \n           'axes.grid': True,           # Mřížka                                                                                                                             \n           'grid.color': '0.9',         # Světlost čar mřížky - 1 = bílá, 0 = černá                                                                                          \n           'grid.linestyle': '-',       # Typ čárkování mřížka                                                                                                               \n           'grid.linewidth': 1.0,       # Tloušťka čar mřížky                                                                                                                \n           'xtick.labelsize': 20,       # Velikost popisů na x-ové ose                                                                                                       \n           'ytick.labelsize': 20,       # Velikost popisů na y-ové ose                                                                                                       \n           'xtick.major.size': 8,       # Velikost čárek na x-ové ose                                                                                                        \n           'ytick.major.size': 8,       # Velikost čárek na y-ové ose                                                                                                        \n           'xtick.major.pad': 10.0,     # Vzdálenost čísel na x-ové ose od osy                                                                                               \n           'ytick.major.pad': 10.0,     # Vzdálenost čísel na y-ové ose od osy                                                                                               \n           }                                                                                                                                                                 \n       )                                                                                                                                                                     \nplt.rcParams['image.cmap'] = 'viridis'  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Set random seeds\n# Note this is not 100 % reliable, starting weights still differ\n\nfrom tensorflow import set_random_seed\n\nseed = 312\n\ndef set_seeds(seed):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    set_random_seed(seed)\n    \nset_seeds(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"# Helper functions\n\ndef is_in_hull(points, hull):\n    \"\"\"\n    Datermine whether the list of points lies inside the hull.\n    \n    Returns\n    =======\n    list\n        List of boolean where true means that the point is inside the convex hull.\n    \"\"\"\n    A = hull.equations[:,0:-1]\n    b = np.transpose(np.array([hull.equations[:,-1]]))\n    isInHull = np.all((A @ np.transpose(points)) <= np.tile(-b,(1,len(points))),axis=0)\n    return isInHull","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing and FE"},{"metadata":{},"cell_type":"markdown","source":"The dataset we are given is combination of dataset of plays and dataset of players. \nThe description of all its features is [here](https://www.kaggle.com/c/nfl-big-data-bowl-2020/data).\n\nLet's split the dataset at first, so we can work with both datasets separately.\n\nI had also decided to remove certain features, e.g. wind direction is useless as I have written [here](https://www.kaggle.com/c/nfl-big-data-bowl-2020/discussion/112050#645868). Also, I don't find orientation of players (i.e. the direction in which they are looking) as a reliable feature, because it was measured differently in different season and it can also change very quickly unlike the direction in which they are running (as they have momentum)."},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"%%time\nplayers = pd.read_csv(\"../input/nfl-big-data-bowl-2020/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define columns we wish to keep in the DataFrame of plays\nbasic_plays_columns = [\"GameId\", \"PlayId\", \"Season\", \"YardLine\", \"Quarter\", \"GameClock\", \"PossessionTeam\", \"Down\", \n        \"Distance\", \"FieldPosition\", \"HomeScoreBeforePlay\", \"VisitorScoreBeforePlay\", \"NflIdRusher\", \n        \"OffenseFormation\", \"OffensePersonnel\", \"DefendersInTheBox\", \"DefensePersonnel\", \"PlayDirection\",\n        \"TimeHandoff\", \"TimeSnap\", \"Yards\", \"HomeTeamAbbr\", \"VisitorTeamAbbr\"]\n# Define columns we wish to keep in the DataFrame of plays\nbasic_players_columns = [\"GameId\", \"PlayId\", \"NflId\", \"Team\", \"X\", \"Y\", \"S\", \"A\", \"Dis\", \"Dir\", \"Orientation\", \"DisplayName\",\n        \"JerseyNumber\", \"PlayerHeight\", \"PlayerWeight\", \"PlayerBirthDate\", \"PlayerCollegeName\", \"Position\",\n        \"HomeTeamAbbr\", \"VisitorTeamAbbr\", \"PossessionTeam\", \"FieldPosition\", \"PlayDirection\", \"NflIdRusher\"]\n\nplays = copy.deepcopy(players)[basic_plays_columns].drop_duplicates()\nplayers = players[basic_players_columns]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, there are three types of functions we can use to modify datasets:\n1. Functions changing the DataFrame of players\n2. Aggregations - Functions which take in DataFrame of players and return data about plays.\n3. Functions changing the DataFrame of plays"},{"metadata":{},"cell_type":"markdown","source":"## Dataset functions\n\nHere, we define functions for modifying DataFrames. Functions are divided in three cells according to order in the preceeding list."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Functions changing dataframe of players\n\ndef standardize_player_data(df):\n    \"\"\"\n    Convert units to SI and add additional weight features.\n    \n    Modified features\n    =================\n    - PlayerWeight\n        Converted to kilograms\n    - PlayerHeight\n        Converted to centimeters\n    - PlayerBirthDate\n        Transformed to datetime\n    - PlayerAge\n        Compute player age at the start of the competition\n    \n    New features\n    ============\n    - PlayerBMI\n    - PlayerObesityClassification\n        Note that majority players are overweight according to standard mean,\n        average for NFL players is different\n    \"\"\"\n    # Transfer to SI units\n    df[\"PlayerWeight\"] = df[\"PlayerWeight\"] * 0.45359237   # In kilograms\n    df[\"PlayerHeight\"] = df[\"PlayerHeight\"].str.split(\"-\", expand=True)[0].astype(float) * 30.48 + df[\"PlayerHeight\"].str.split(\"-\", expand=True)[1].astype(float) * 2.54   # in centimeters\n    # Set birthdate column as datetime and add age in years\n    df[\"PlayerBirthDate\"] = pd.to_datetime(df['PlayerBirthDate'])\n    df[\"PlayerAge\"] = (pd.Timestamp('20191010') - df[\"PlayerBirthDate\"]).dt.days / 365   # In years\n    # Compute body mass index (BMI) and add classification\n    df[\"PlayerBMI\"] = df[\"PlayerWeight\"] / (df[\"PlayerHeight\"] / 100)**2    # Weight has to be in kilograms, height in meters (factor 100 to transfer from centimeters)\n    df[\"PlayerObesityClassification\"] = pd.cut(df[\"PlayerBMI\"], bins=[0, 18.5, 25, 30, 35, 40, 100], labels=[\"Underweight\", \"Normal weight\", \"Pre-obesity\", \"Obesity class 1\", \"Obesity class 2\", \"Obesity class 3\"])\n    return df\n\ndef get_team_abbreviations(df):\n    \"\"\"\n    Map an abbreviation of a team to each player.\n    \n    New features\n    ============\n    - TeamAbbr\n    \"\"\"\n    mask = df.eval(\"Team == 'home'\")\n    df.loc[mask, \"TeamAbbr\"] = df.loc[mask, \"HomeTeamAbbr\"]\n    mask = df[\"Team\"]==\"away\"\n    df.loc[mask, \"TeamAbbr\"] = df.loc[mask, \"VisitorTeamAbbr\"] \n    return df\n\ndef unify_team_abbreviations(df):\n    \"\"\"\n    Unify team abbreviations as they are different in different columns.\n    \n    Modified features\n    =================\n    - PossessionTeam\n    - FieldPosition\n    \"\"\"\n    df.loc[df[\"PossessionTeam\"] == \"ARZ\", \"PossessionTeam\"] = \"ARI\"\n    df.loc[df[\"PossessionTeam\"] == \"BLT\", \"PossessionTeam\"] = \"BAL\"\n    df.loc[df[\"PossessionTeam\"] == \"CLV\", \"PossessionTeam\"] = \"CLE\"\n    df.loc[df[\"PossessionTeam\"] == \"HST\", \"PossessionTeam\"] = \"HOU\"\n    df.loc[df[\"FieldPosition\"] == \"ARZ\", \"FieldPosition\"] = \"ARI\"\n    df.loc[df[\"FieldPosition\"] == \"BLT\", \"FieldPosition\"] = \"BAL\"\n    df.loc[df[\"FieldPosition\"] == \"CLV\", \"FieldPosition\"] = \"CLE\"\n    df.loc[df[\"FieldPosition\"] == \"HST\", \"FieldPosition\"] = \"HOU\"\n    return df\n\ndef determine_offense_defense(df):\n    \"\"\"\n    Determine whether a player is in offense or defense.\n    \n    New features\n    ============\n    - OffDef\n        Whether a player's team is in offense or defense\n    \"\"\"\n    mask = df.eval(\"(Team == 'home' and HomeTeamAbbr == PossessionTeam) or (Team == 'away' and VisitorTeamAbbr == PossessionTeam)\")\n    df.loc[mask, \"OffDef\"] = \"offense\"\n    mask = df.eval(\"(Team == 'home' and HomeTeamAbbr != PossessionTeam) or (Team == 'away' and VisitorTeamAbbr != PossessionTeam)\")\n    df.loc[mask, \"OffDef\"] = \"defense\"\n    return df\n\ndef normalize_coordinates_and_directions(plays):\n    \"\"\"\n    Normalize coordinates so that offending team is always moving right.\n    \n    To change coordinates for this, we also need to switch not only X,\n    but also Y and direction Dir, because if we switched only X,\n    players in offense may have been still moving left. If we just\n    flip all variables, it is the easiest solution.\n    \n    Modified features\n    =================\n    - X\n    - Y\n    - Dir\n    \n    New features\n    ============\n    - M\n        Manhattan distance from bottom left corner of the field, sum of X and Y\n    \"\"\"\n    mask = plays[\"PlayDirection\"] != \"right\"\n    plays.loc[mask, \"X\"] = 120 - plays.loc[mask, \"X\"]\n    plays[\"X\"] -= 10\n    plays.loc[mask, \"Y\"] = 160/3 - plays.loc[mask, \"Y\"]\n    plays[\"M\"] = plays[\"X\"] + plays[\"Y\"]\n    plays.loc[mask, \"Dir\"] = (plays.loc[mask, \"Dir\"] + 180) % 360\n    plays.loc[(plays[\"Dir\"].isnull()) & (plays[\"OffDef\"] == \"offense\"), \"Dir\"] = 90\n    plays.loc[(plays[\"Dir\"].isnull()) & (plays[\"OffDef\"] == \"defense\"), \"Dir\"] = 270\n    return plays\n\ndef compute_horizontal_and_vertical_speeds(players):\n    \"\"\"\n    Compute projection of speed to X and Y.\n    \n    New features\n    ============\n    - S_horizontal\n    - S_vertical\n    \"\"\"\n    players[\"S_horizontal\"] = np.sin(players[\"Dir\"]) * players[\"S\"]\n    players[\"S_vertical\"] = np.cos(players[\"Dir\"]) * players[\"S\"]\n    return players\n\ndef compute_distance_to_rusher(df):\n    \"\"\"\n    Compute distance of a player from the rusher.\n    \n    New features\n    ============\n    - DistanceToRusher\n        Euclidean distance of a player from the rusher\n    \"\"\"\n    rushers = df.loc[df[\"NflIdRusher\"]==df[\"NflId\"], [\"PlayId\", \"X\", \"Y\"]]\n    df = pd.merge(df, rushers, how=\"left\", left_on=\"PlayId\", right_on=\"PlayId\", suffixes=(\"\", \"_rusher\"))\n    df[\"dXtoRusher\"] = df[\"X_rusher\"] - df[\"X\"]\n    df[\"dYtoRusher\"] = df[\"Y_rusher\"] - df[\"Y\"]\n    df[\"DistanceToRusher\"] = np.sqrt(df[\"dXtoRusher\"]**2 + df[\"dYtoRusher\"]**2)\n    df = df.drop([\"X_rusher\", \"Y_rusher\"], axis=1)\n    return df\n\ndef compare_speed_with_rusher(df):\n    \"\"\"\n    Compare speed of a player with the rusher.\n    \n    New features\n    ============\n    - RusherSpeedFactorToPlayer\n        How many times faster is the rusher running than a player\n    \"\"\"\n    rushers = df.loc[df[\"NflIdRusher\"]==df[\"NflId\"], [\"PlayId\", \"S\"]]\n    df = pd.merge(df, rushers, how=\"left\", left_on=\"PlayId\", right_on=\"PlayId\", suffixes=(\"\", \"_rusher\"))\n    df[\"RusherSpeedFactorToPlayer\"] = df[\"S_rusher\"] / df[\"S\"]\n    df = df.drop([\"S_rusher\"], axis=1)\n    return df\n\ndef compute_minimum_tackle_time(players):\n    \"\"\"\n    Compute the minimum tackle time if the rusher stayed still.\n    \n    New features\n    ============\n    - MinTackleTime-Basic\n    \"\"\"\n    players[\"MinTackleTime-Basic\"] = players[\"DistanceToRusher\"] / df[\"S\"]\n    return players\n\ndef compute_distances_to_opponents(players, N_closest_opponents=3, N_closest_teammates=3, N_PCA=3):\n    \"\"\"\n    Compute distances between all players in a super fast way and select some of them as features.\n    \n    It is possible to set how many distances to opponents/teammates to save.\n    Distances are sorted, so you will always save the distance to closest opponent/teammate.\n    You can also save components of PCA of distance matrix.\n    This function is really complicated and was taken from CPMP's kernel, \n    which is referenced later, however there was a bug in his kernel and I am not sure it is fixed yet.\n    Anyway, this and his computation is super fast and works for arbitrary number of dimensions, \n    in his former kernel it is used for computing distances of atoms in a molecule in 3D, \n    this use is by one dimension easier.\n    \n    New features\n    ============\n    - ClosestEnemy-{i+1}th\n    - ClosestMate-{i+1}th\n    - DistancePCA-{i+1}th\n    \n    Parameters\n    ==========\n    - N_closest_opponents\n        How many distances to closest opponents to save\n    - N_closest_teammates\n        How many distances to closest mates to save\n    - N_PCA\n        How many PCA components of the distance matrix to save\n        \n    References\n    ==========\n    - https://www.kaggle.com/cpmpml/ultra-fast-distance-matrix-computation (note there was a bug and I am not sure if it was fixed yet)\n    \"\"\"\n    players = players.sort_values(['GameId','PlayId', \"OffDef\", \"DistanceToRusher\"], ascending=[True, True, False, True])\n    values = np.zeros((len(players), N_closest_opponents+N_closest_teammates+N_PCA))\n    xy = players[[\"X\", \"Y\"]].values\n    ss = players.groupby(\"PlayId\").size()\n    ss = ss.cumsum()\n    ssx = np.zeros(len(ss) + 1, 'int')\n    ssx[1:] = ss\n    for idx in range(players[\"PlayId\"].nunique()):\n        start_player, end_player = ssx[idx], ssx[idx+1]\n        locs = xy[start_player:end_player]    \n        loc_tile = np.tile(locs.T, (22,1,1))\n        dist_mat = np.sqrt(((loc_tile - loc_tile.T)**2).sum(axis=1))\n        pca = PCA(n_components=N_PCA)\n        principal_components = pca.fit_transform(dist_mat)\n        for i in range(N_closest_opponents):          \n            closest_defenders = np.partition(dist_mat[:11, 11:], i, axis=1)[:, i]\n            closest_offenders = np.partition(dist_mat[11:, :11], i, axis=1)[:, i]\n            values[start_player:end_player-11, i] = closest_defenders # to offenders\n            values[start_player+11:end_player, i] = closest_offenders # to defenders\n        for i in range(N_closest_teammates):\n            closest_def_teammates = np.partition(dist_mat[:11, :11], i+1, axis=1)[:, i+1]\n            closest_off_teammates = np.partition(dist_mat[11:, 11:], i+1, axis=1)[:, i+1]           \n            values[start_player:end_player-11, N_closest_opponents+i] = closest_def_teammates # to offenders\n            values[start_player+11:end_player, N_closest_opponents+i] = closest_off_teammates # to defenders\n        values[start_player:end_player, (N_closest_opponents+N_closest_teammates):] = principal_components\n    for i in range(N_closest_opponents):\n        players[f\"ClosestEnemy-{i+1}th\"] = values[:, i]\n    for i in range(N_closest_teammates):\n        players[f\"ClosestMate-{i+1}th\"] = values[:, N_closest_opponents+i]\n    for i in range(N_PCA):\n        players[f\"DistancePCA-{i+1}th\"] = values[:, N_closest_opponents+N_closest_teammates+i]\n    return players\n            \ndef add_player_numbers(df):\n    \"\"\"\n    Sort players according to Manhattan distance and label them.\n    \n    New features\n    ============\n    - PlayerNumber\n    - DefenderNumber\n    - OffenderNumber\n    \"\"\"\n    df = df.sort_values(['GameId','PlayId', \"OffDef\", \"M\"], ascending=True)\n    df[\"PlayerNumber\"] = list(range(22)) * int(len(df) / 22)\n    defenders = df.eval(\"OffDef == 'defense'\")\n    offenders = df.eval(\"OffDef == 'offense'\")\n    df.loc[defenders, \"DefenderNumber\"] = list(range(11)) * int(len(df) / 22)\n    df.loc[offenders, \"OffenderNumber\"] = list(range(11)) * int(len(df) / 22)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Aggregations - Functions which take in data about players and return data about plays.\n\n# Note that new features in this section may have been already calculated in the previous section,\n# but these are new features for the DataFrame of plays. Remember these functions are aggregations.\n\ndef get_who_is_offense(players):\n    \"\"\"\n    For each team find out if they are in offense or defense.\n    \n    These new features are not used for ML, \n    but in other feature-generating functions.\n    \n    New features\n    ============\n    - HomeIsOffense\n    - HomeIsDefense\n    - VisitorIsDefense\n    - VisitorIsOffense\n    \"\"\"\n    new = players.loc[players[\"NflIdRusher\"]==players[\"NflId\"], [\"PlayId\", \"Team\", \"OffDef\"]]\n    mask = new.eval(\"(Team == 'home' and OffDef == 'offense') | (Team == 'away' and OffDef == 'defense')\")\n    new.loc[mask, \"HomeIsOffense\"] = True\n    new.loc[mask, \"VisitorIsDefense\"] = True\n    new.loc[mask, \"HomeIsDefense\"] = False\n    new.loc[mask, \"VisitorIsOffense\"] = False\n    mask = new.eval(\"(Team == 'away' and OffDef == 'offense') | (Team == 'home' and OffDef == 'defense')\")\n    new.loc[mask, \"HomeIsDefense\"] = True\n    new.loc[mask, \"VisitorIsOffense\"] = True\n    new.loc[mask, \"HomeIsOffense\"] = False\n    new.loc[mask, \"VisitorIsDefense\"] = False\n    new = new.drop([\"Team\", \"OffDef\"], axis=1)\n    return new    \n\ndef get_circles(df):\n    \"\"\"\n    Draw circles around rusher and compute how many opponents/teammates are in a given circle.\n    \n    New features\n    ============\n    - DefendersInCircleAroundRusher-{i}\n        i is for the circle radius\n    - OffendersInCircleAroundRusher-{i}\n        i is for the circle radius\n    \"\"\"\n    pivoted = df[[\"PlayId\", \"DefenderNumber\", \"DistanceToRusher\"]].dropna().pivot(index=\"PlayId\", columns=\"DefenderNumber\", values=\"DistanceToRusher\")\n    new = pd.DataFrame({\"PlayId\":pivoted.index})\n    # Defenders\n    for i in range(1, 21):\n        new[f\"DefendersInCircleAroundRusher-{i}\"] = pivoted[pivoted < i].count(axis=1).values\n    # Offenders\n    pivoted = df[[\"PlayId\", \"OffenderNumber\", \"DistanceToRusher\"]].dropna().pivot(index=\"PlayId\", columns=\"OffenderNumber\", values=\"DistanceToRusher\")\n    for i in range(1, 21):\n        new[f\"OffendersInCircleAroundRusher-{i}\"] = pivoted[pivoted < i].count(axis=1).values\n    return new\n\ndef get_rusher_info(players):\n    \"\"\"\n    Save information about rusher from the DataFrame of players to the DataFrame of plays.\n    \n    New features\n    ============\n    - Rusher-MainRole\n    - Rusher-Height\n    - Rusher-Weight\n    - Rusher-BMI\n    - Rusher-Speed\n    - Rusher-Acceleration\n    \"\"\"\n    cols = [\"PlayId\", \"Position\", \"PlayerHeight\", \"PlayerWeight\", \"PlayerBMI\", \"S\", \"A\", \"JerseyNumber\", \"X\", \"Y\",\n            \"S_vertical\", \"S_horizontal\", \"PlayerAge\"]\n    new = players.loc[players[\"NflId\"]==players[\"NflIdRusher\"], cols]\n    new = new.rename(columns={\"Position\":\"MainRole\", \"PlayerHeight\":\"Height\", \"PlayerWeight\":\"Weight\", \"PlayerBMI\":\"BMI\",\n                      \"S\":\"Speed\", \"A\":\"Acceleration\"})\n    new = new.add_prefix(\"Rusher-\")\n    new = new.rename(columns={\"Rusher-PlayId\":\"PlayId\"})\n    return new\n\ndef get_players_features(players):\n    \"\"\"\n    Save information about all players (including rusher) \n    from DataFrame of players to the DataFrame of plays.\n    \n    In feature descriptions which will follow, i is the\n    number of the current player and j is value of j-th\n    closest player fullfilling condition or j-th PCA value.\n    \n    New features\n    ============\n    - X_{i}\n    - Y_{i}\n    - DistanceToRusher_{i}\n    - S_{i}\n    - A_{i}\n    - Dir_{i}\n    - JerseyNumber_{i}\n    - PlayerWeight_{i}\n    - PlayerHeight_{i}\n    - PlayerBMI_{i}\n    - S_vertical_{i}\n    - S_horizontal_{i}\n    - PlayerAge_{i}\n    - ClosestEnemy_{j+1}th_{i}\n    - ClosestMate_{j+1}th_{i}\n    - DistancePCA-{j+i}th_{i}\n    \"\"\"\n    new = pd.DataFrame({\"PlayId\":players[\"PlayId\"]}).drop_duplicates()\n    features = [\"X\", \"Y\", \"DistanceToRusher\", \"S\", \"A\", \"Dir\", \"JerseyNumber\", \"PlayerWeight\", \"PlayerHeight\", \"PlayerBMI\",\n                \"S_vertical\", \"S_horizontal\", \"PlayerAge\"] + [f\"ClosestEnemy-{i+1}th\" for i in range(3)] \\\n                + [f\"ClosestMate-{i+1}th\" for i in range(3)] + [f\"DistancePCA-{i+1}th\" for i in range(3)]\n    for feature in features:\n        pivoted = players.pivot(index=\"PlayId\", columns=\"PlayerNumber\", values=feature)\n        pivoted = pivoted.loc[:, :].add_prefix(f\"{feature}_\")\n        new = pd.merge(new, pivoted, how=\"left\", left_on=\"PlayId\", right_index=True)\n    return new    \n\ndef get_player_types_numbers(players):\n    \"\"\"\n    Get information about how many players of each type are playing on each side.\n    \n    NFL players have roles in every play.\n    Their roles (types) are listed e.g. here:\n    https://en.wikipedia.org/wiki/American_football_positions\n    \n    New features\n    ============\n    - OffensePlayerTypes-{type}\n    - DefensePlayerTypes-{type}\n    \"\"\"\n    new = pd.DataFrame({\"PlayId\":players[\"PlayId\"]}).drop_duplicates()\n    offenders = players.loc[players[\"OffDef\"]==\"offense\"]\n    offender_types = players.loc[players[\"OffDef\"]==\"offense\"].groupby(\"PlayId\")[\"Position\"].value_counts().unstack().fillna(0)\n    defender_types = players.loc[players[\"OffDef\"]==\"defense\"].groupby(\"PlayId\")[\"Position\"].value_counts().unstack().fillna(0)\n    player_roles = ['CB', 'WR', 'G', 'T', 'DE', 'DT', 'OLB', 'TE', \n                    'FS', 'C', 'RB', 'QB', 'SS', 'ILB', 'MLB', 'NT', 'LB', \n                    'OT', 'FB', 'OG', 'DB', 'S', 'HB', 'SAF', 'DL']\n    for role in player_roles:\n        if role not in offender_types.columns:\n            offender_types[role] = 0\n        if role not in defender_types.columns:\n            defender_types[role] = 0\n    offender_types = offender_types.add_prefix(\"OffensePlayerTypes-\")\n    defender_types = defender_types.add_prefix(\"DefensePlayerTypes-\")\n    new = pd.merge(new, offender_types, how=\"left\", left_on=\"PlayId\", right_index=True)\n    new = pd.merge(new, defender_types, how=\"left\", left_on=\"PlayId\", right_index=True)  \n    return new.astype(int)\n\ndef get_advanced_distance_features(players):\n    \"\"\"\n    Save information connected with convex hulls.\n    \n    Offensive/defensive convex hull in this case is the smallest area\n    if you connect all offending/defending players and leave out players\n    which are inside the area. \n    Rusher is not counted as edge for offensive convex hull.\n    \n    New features\n    ============\n    - OffensiveHull-area\n        Area of convex hull of offending team\n    - RusherInOffenseHull\n        Whether rusher is in convex hull of his team\n    - DefendersInOffenseHull\n        How many defenders are in the offending convex hull\n    - DefensiveHull-area\n        Area of convex hull of defending team\n    - RusherInDefenseHull\n        Whether rusher is in convex hull of opposing team\n    - OffendersInDefenseHull\n        How many offenders are inside the defending convex hull\n    \"\"\"\n    players = players.sort_values(['GameId','PlayId', \"OffDef\", \"DistanceToRusher\"], ascending=[True, True, False, True])\n    ids = players[[\"PlayId\"]].values\n    xy = players[[\"X\", \"Y\"]].values\n    ss = players.groupby(\"PlayId\").size()\n    ss = ss.cumsum()\n    ssx = np.zeros(len(ss) + 1, 'int')\n    ssx[1:] = ss\n    data = {\"PlayId\":[],\n            \"OffensiveHull-area\":[], \"RusherInOffenseHull\":[], \"DefendersInOffenseHull\":[],\n            \"DefensiveHull-area\":[], \"RusherInDefenseHull\":[], \"OffendersInDefenseHull\":[]}\n    for idx in range(players[\"PlayId\"].nunique()):\n        start_player, end_player = ssx[idx], ssx[idx+1]\n        locs = xy[start_player:end_player]    \n        off_hull = ConvexHull(locs[1:11])\n        rusher_in_offense_hull = is_in_hull(locs[:1], off_hull)\n        defenders_in_offense_hull = is_in_hull(locs[11:], off_hull)\n        def_hull = ConvexHull(locs[11:])\n        rusher_in_defense_hull = is_in_hull(locs[:1], def_hull)\n        offenders_in_defense_hull = is_in_hull(locs[:11], def_hull)\n        data[\"PlayId\"].append(ids[start_player][0])\n        data[\"OffensiveHull-area\"].append(off_hull.area)\n        data[\"RusherInOffenseHull\"].append(rusher_in_offense_hull.sum())\n        data[\"DefendersInOffenseHull\"].append(defenders_in_offense_hull.sum())\n        data[\"DefensiveHull-area\"].append(def_hull.area)\n        data[\"RusherInDefenseHull\"].append(rusher_in_defense_hull.sum())\n        data[\"OffendersInDefenseHull\"].append(offenders_in_defense_hull.sum())\n    new = pd.DataFrame(data)\n    return new\n\ndef compute_centroids(players):\n    \"\"\"\n    Get average position for every team and extract features.\n    \n    In this function we treat a team as a single point or rather\n    a distribution which is spread a bit.\n    Spread is for difference between maximal and minimal position.\n    \n    New features\n    ============\n    - Centroid-Offense-X-mean\n    - Centroid-Offense-X-std\n    - Centroid-Offense-X-spread\n    - Centroid-Offense-Y-mean\n    - Centroid-Offense-Y-std\n    - Centroid-Offense-Y-spread\n    - Centroid-Defense-X-mean\n    - Centroid-Defense-X-std\n    - Centroid-Defense-X-spread\n    - Centroid-Defense-Y-mean\n    - Centroid-Defense-Y-std\n    - Centroid-Defense-Y-spread    \n    \"\"\"\n    new = pd.DataFrame({\"PlayId\":players[\"PlayId\"]}).drop_duplicates()\n    offensive_groupby = players.loc[players[\"OffDef\"]==\"offense\"].groupby(\"PlayId\")\n    new[\"Centroid-Offense-X-mean\"] = offensive_groupby[\"X\"].mean().values\n    new[\"Centroid-Offense-X-std\"] = offensive_groupby[\"X\"].std().values\n    new[\"Centroid-Offense-X-spread\"] = (offensive_groupby[\"X\"].max() - offensive_groupby[\"X\"].min()).values\n    new[\"Centroid-Offense-Y-mean\"] = offensive_groupby[\"Y\"].mean().values\n    new[\"Centroid-Offense-Y-std\"] = offensive_groupby[\"Y\"].std().values\n    new[\"Centroid-Offense-Y-spread\"] = (offensive_groupby[\"Y\"].max() - offensive_groupby[\"Y\"].min()).values\n    defensive_groupby = players.loc[players[\"OffDef\"]==\"defense\"].groupby(\"PlayId\")\n    new[\"Centroid-Defense-X-mean\"] = defensive_groupby[\"X\"].mean().values\n    new[\"Centroid-Defense-X-std\"] = defensive_groupby[\"X\"].std().values\n    new[\"Centroid-Defense-X-spread\"] = (defensive_groupby[\"X\"].max() - defensive_groupby[\"X\"].min()).values\n    new[\"Centroid-Defense-Y-mean\"] = defensive_groupby[\"Y\"].mean().values\n    new[\"Centroid-Defense-Y-std\"] = defensive_groupby[\"Y\"].std().values\n    new[\"Centroid-Defense-Y-spread\"] = (defensive_groupby[\"Y\"].max() - defensive_groupby[\"Y\"].min()).values\n    return new\n\ndef get_global_rusher_features(players):\n    \"\"\"\n    Compute minimal distance between rusher and defense team.\n    \n    New features\n    ============\n    - Defense-MinDistanceToRusher\n    \"\"\"\n    new = pd.DataFrame({\"PlayId\":players[\"PlayId\"]}).drop_duplicates()\n    rushers = players.loc[players[\"NflIdRusher\"]==players[\"NflId\"], [\"PlayId\", \"ClosestEnemy-1th\"]]\n    new[\"Defense-MinDistanceToRusher\"] = rushers[\"ClosestEnemy-1th\"]\n    return new\n\ndef rusher_vs_groups(players, plays):\n    \"\"\"\n    Get distance of rusher to team centroids.\n    \n    New features\n    ============\n    - Rusher-ToOffenseCentroid-X\n    - Rusher-ToOffenseCentroid-Y\n    - Rusher-ToOffenseCentroid-Distance\n    - Rusher-ToDefenseCentroid-X\n    - Rusher-ToDefenseCentroid-Y\n    - Rusher-ToDefenseCentroid-Distance\n    \"\"\"\n    rushers = players.loc[players[\"NflIdRusher\"]==players[\"NflId\"], [\"PlayId\", \"X\", \"Y\"]]\n    plays = pd.merge(plays, rushers, how=\"left\", left_on=\"PlayId\", right_on=\"PlayId\")\n    plays[\"Rusher-ToOffenseCentroid-X\"] = plays[\"X\"] - plays[\"Centroid-Offense-X-mean\"]\n    plays[\"Rusher-ToOffenseCentroid-Y\"] = plays[\"Y\"] - plays[\"Centroid-Offense-Y-mean\"]\n    plays[\"Rusher-ToOffenseCentroid-Distance\"] = np.sqrt(plays[\"Rusher-ToOffenseCentroid-X\"]**2 + plays[\"Rusher-ToOffenseCentroid-Y\"]**2)\n    plays[\"Rusher-ToDefenseCentroid-X\"] = plays[\"X\"] - plays[\"Centroid-Defense-X-mean\"]\n    plays[\"Rusher-ToDefenseCentroid-Y\"] = plays[\"Y\"] - plays[\"Centroid-Defense-Y-mean\"]\n    plays[\"Rusher-ToDefenseCentroid-Distance\"] = np.sqrt(plays[\"Rusher-ToDefenseCentroid-X\"]**2 + plays[\"Rusher-ToDefenseCentroid-Y\"]**2)\n    plays = plays.drop([\"X\", \"Y\"], axis=1)\n    return plays","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Functions changing dataframe of plays\n\ndef count_defense_personnel(df):\n    \"\"\"\n    Count defense personnel.\n    \n    There are certain types of players in the defense.\n    In the DataFrame we have a string column, which needs\n    to be parsed, so we know how many players is there\n    of each type.\n    \n    New features\n    ============\n    - DefensePersonnel-DL\n    - DefensePersonnel-LB\n    - DefensePersonnel-DB\n    - DefensePersonnel-OL\n    \"\"\"\n    defense_personnel_types = [\"DL\", \"LB\", \"DB\", \"OL\"]\n    defense_personnel_extraction = \"^(?:(?P<DPDL>[0-9])DL)?,?(?:(?P<DPLB>[0-9])LB)?,?(?:(?P<DPDB>[0-9])DB)?,?(?:(?P<DPOL>[0-9])OL)?$\"\n    defpersdf = df[\"DefensePersonnel\"].str.replace(\" \",\"\").str.extract(defense_personnel_extraction, expand=True)\n    defpersdf = defpersdf.fillna(0)\n    defpersdf = defpersdf.astype({\"DPDL\": int, \"DPLB\": int, \"DPDB\": int, \"DPOL\": int})\n    df[[\"DefensePersonnel-DL\", \"DefensePersonnel-LB\", \"DefensePersonnel-DB\", \"DefensePersonnel-OL\"]] = defpersdf[[\"DPDL\", \"DPLB\", \"DPDB\", \"DPOL\"]]\n    return df\n\ndef count_offense_personnel(df):\n    \"\"\"\n    Count offense personnel.\n    \n    There are certain types of players in the offense.\n    In the DataFrame we have a string column, which needs\n    to be parsed, so we know how many players is there\n    of each type.\n    \n    New features\n    ============\n    - OffensePersonnel-OL\n    - OffensePersonnel-QB\n    - OffensePersonnel-RB\n    - OffensePersonnel-TE\n    - OffensePersonnel-WR\n    - OffensePersonnel-DL\n    - OffensePersonnel-LB\n    - OffensePersonnel-DB\n    \"\"\"\n    offense_personnel_types = [\"OL\", \"QB\", \"RB\", \"TE\", \"WR\", \"DL\", \"LB\", \"DB\"]\n    offense_personnel_extraction = \"^(?:(?P<OPOL>[0-9])OL)?,?(?:(?P<OPQB>[0-9])QB)?,?(?:(?P<OPRB>[0-9])RB)?,?(?:(?P<OPTE>[0-9])TE)?,?(?:(?P<OPWR>[0-9])WR)?,?(?:(?P<OPDL>[0-9])DL)?,?(?:(?P<OPLB>[0-9])LB)?,?(?:(?P<OPDB>[0-9])DB)?,?$\"\n    offpersdf = df[\"OffensePersonnel\"].str.replace(\" \",\"\").str.extract(offense_personnel_extraction, expand=True)\n    offpersdf = offpersdf.fillna(0)\n    offpersdf = offpersdf.astype({\"OPOL\": int, \"OPQB\": int, \"OPRB\": int, \"OPTE\": int, \"OPWR\": int, \"OPDL\": int, \"OPLB\": int, \"OPDB\": int})\n    df[[\"OffensePersonnel-OL\", \"OffensePersonnel-QB\", \"OffensePersonnel-RB\", \"OffensePersonnel-TE\",\n           \"OffensePersonnel-WR\", \"OffensePersonnel-DL\", \"OffensePersonnel-LB\", \"OffensePersonnel-DB\"]] = offpersdf[[\"OPOL\", \"OPQB\", \"OPRB\", \"OPTE\", \"OPWR\", \"OPDL\", \"OPLB\", \"OPDB\"]]\n    return df\n\ndef get_personnel_frequencies(plays):\n    \"\"\"\n    Get a dict with how often is a formation with \n    given types of players played.\n    \n    Note this is not a functions changing the DataFrame of plays,\n    but it makes a couple with 'set_personnel_frequencies'.\n    \"\"\"\n    frequencies = dict()\n    frequencies[\"DefensePersonnel\"] = dict(plays[\"DefensePersonnel\"].value_counts())\n    frequencies[\"OffenseFormation\"] = dict(plays[\"OffenseFormation\"].value_counts())    \n    frequencies[\"OffensePersonnel\"] = dict(plays[\"OffensePersonnel\"].value_counts())\n    frequencies[\"Rusher-MainRole\"] = dict(plays[\"Rusher-MainRole\"].value_counts())\n    return frequencies\n\ndef set_personnel_frequencies(df, frequencies):\n    \"\"\"\n    Set how often is a formation with given types\n    of players played.\n    \n    New features\n    ============\n    - DefensePersonnel-frequency\n    - OffenseFormation\n    - OffenseFormation-frequency\n    - OffensePersonnel-frequency\n    - Rusher-MainRole-frequency\n    - Rusher-MainRole\n    \"\"\"\n    df[\"DefensePersonnel-frequency\"] = df[\"DefensePersonnel\"].apply(lambda x: frequencies[\"DefensePersonnel\"][x] if x in frequencies[\"DefensePersonnel\"].keys() else 0)\n    df[\"OffenseFormation\"] = df[\"OffenseFormation\"].fillna(\"EMPTY\")\n    df[\"OffenseFormation-frequency\"] = df[\"OffenseFormation\"].apply(lambda x: frequencies[\"OffenseFormation\"][x] if x in frequencies[\"OffenseFormation\"].keys() else 0)\n    df[\"OffensePersonnel-frequency\"] = df[\"OffensePersonnel\"].apply(lambda x: frequencies[\"OffensePersonnel\"][x] if x in frequencies[\"OffensePersonnel\"].keys() else 0)\n    df[\"Rusher-MainRole-frequency\"] = df[\"Rusher-MainRole\"].apply(lambda x: frequencies[\"Rusher-MainRole\"][x])\n    df[\"Rusher-MainRole\"] = df[\"Rusher-MainRole\"].astype(\"category\")\n    return df\n\ndef compute_score_difference(df):\n    \"\"\"\n    Compute score difference between teams.\n    \n    New features\n    ============\n    - OffToDefScore\n        How much more score does the offense have than the defense.\n    \"\"\"\n    mask = (df[\"HomeIsOffense\"]) | (df[\"VisitorIsDefense\"])\n    df.loc[mask, \"OffToDef-Score\"] = df.loc[mask, \"HomeScoreBeforePlay\"] - df.loc[mask, \"VisitorScoreBeforePlay\"]\n    mask = (df[\"VisitorIsOffense\"]) | (df[\"HomeIsDefense\"])\n    df.loc[mask, \"OffToDef-Score\"] = df.loc[mask, \"VisitorScoreBeforePlay\"] - df.loc[mask, \"HomeScoreBeforePlay\"]\n    return df\n\ndef standardize_game_features(df):\n    \"\"\"\n    Transform game clock to second and fill \n    nans in DefendersInTheBox column.\n    \n    New features\n    ============\n    - GameClock\n        How many second till the end of a quarter\n    - DefendersInTheBox\n    \"\"\"\n    df[\"GameClock\"] = df[\"GameClock\"].str.split(\":\", expand=True)[0].astype(int) * 60 + df[\"GameClock\"].str.split(\":\", expand=True)[1].astype(int)   # == time remaining in seconds\n    df.loc[df[\"DefendersInTheBox\"].isna(), \"DefendersInTheBox\"] = df.loc[df[\"DefendersInTheBox\"].isna(), \"DefendersInTheBox\"].fillna(df[\"DefendersInTheBox\"].mean())\n    return df\n\ndef compute_yards_to_go(df):\n    \"\"\"\n    Compute how many yards left to go for the offending team.\n    \n    For YardLine==50, YardsToGo need to be specified specifically,\n    as there is no PossessionTeam.\n    \n    New features\n    ============\n    - YardsToGo\n    \"\"\"\n    mask = df.eval(\"FieldPosition == PossessionTeam\")\n    df.loc[mask, \"YardsToGo\"] = 100 - df.loc[mask, \"YardLine\"]\n    mask = df.eval(\"FieldPosition != PossessionTeam\")\n    df.loc[mask, \"YardsToGo\"] = df.loc[mask, \"YardLine\"]\n    df.loc[df[\"YardLine\"] == 50, \"YardsToGo\"] = 50\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Generating features\n\nNow run all defined functions and create features. Also define a functions which does the same, as we will need to apply it later when we get data for a single play after another single play. I was also thinking of a faster version of preceeding functions. They are vectorized, so they are really fast for big dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nplayers = standardize_player_data(players)\nplayers = get_team_abbreviations(players)\nplayers = unify_team_abbreviations(players)\nplayers = determine_offense_defense(players)\nplayers = normalize_coordinates_and_directions(players)\nplayers = compute_horizontal_and_vertical_speeds(players)\nplayers = add_player_numbers(players)\nplayers = compute_distance_to_rusher(players)\nplayers = compute_distances_to_opponents(players, 3, 3)\n\nplays = pd.merge(plays, get_who_is_offense(players), how=\"left\", left_on=\"PlayId\", right_on=\"PlayId\")\nplays = pd.merge(plays, get_circles(players), how=\"left\", left_on=\"PlayId\", right_on=\"PlayId\")\nplays = pd.merge(plays, get_rusher_info(players), how=\"left\", left_on=\"PlayId\", right_on=\"PlayId\")\nplays = pd.merge(plays, get_players_features(players), how=\"left\", left_on=\"PlayId\", right_on=\"PlayId\")\nplays = pd.merge(plays, get_player_types_numbers(players), how=\"left\", left_on=\"PlayId\", right_on=\"PlayId\")\nplays = pd.merge(plays, get_advanced_distance_features(players), how=\"left\", left_on=\"PlayId\", right_on=\"PlayId\")\nplays = pd.merge(plays, compute_centroids(players), how=\"left\", left_on=\"PlayId\", right_on=\"PlayId\")\nplays = pd.merge(plays, get_global_rusher_features(players), how=\"left\", left_on=\"PlayId\", right_on=\"PlayId\")\nplays = rusher_vs_groups(players, plays)\n\nplays = count_defense_personnel(plays)\nplays = count_offense_personnel(plays)\nfrequencies = get_personnel_frequencies(plays)\nplays = set_personnel_frequencies(plays, frequencies)\nplays = compute_score_difference(plays)\nplays = standardize_game_features(plays)\nplays = compute_yards_to_go(plays)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_data(players, plays, frequencies):\n    \"\"\"\n    Given starting dataframe, generate all features and return DataFrame of plays.\n    \n    We also need to provide frequencies of roles (e.g. how often there is a quarterback in a play).\n    These frequencies can not be computed on test data as it would be leak. \n    Frequencies of occurences always need to be computed on train data.\n    \n    Parameters\n    ==========\n    - players\n        Starting DataFrame of players\n    - plays\n        Starting DataFrame of plays\n    - frequencies\n        Frequencies of roles\n        \n    Returns\n    =======\n    - pandas.DataFrame\n        DataFrame od plays with a lot of features ready for ML\n    \"\"\"\n    players = standardize_player_data(players)\n    players = get_team_abbreviations(players)\n    players = unify_team_abbreviations(players)\n    players = determine_offense_defense(players)\n    players = normalize_coordinates_and_directions(players)\n    players = compute_horizontal_and_vertical_speeds(players)\n    players = add_player_numbers(players)\n    players = compute_distance_to_rusher(players)\n    players = compute_distances_to_opponents(players, 3, 3)\n    plays = pd.merge(plays, get_who_is_offense(players), how=\"left\", left_on=\"PlayId\", right_on=\"PlayId\")\n    plays = pd.merge(plays, get_circles(players), how=\"left\", left_on=\"PlayId\", right_on=\"PlayId\")\n    plays = pd.merge(plays, get_rusher_info(players), how=\"left\", left_on=\"PlayId\", right_on=\"PlayId\")\n    plays = pd.merge(plays, get_players_features(players), how=\"left\", left_on=\"PlayId\", right_on=\"PlayId\")\n    plays = pd.merge(plays, get_player_types_numbers(players), how=\"left\", left_on=\"PlayId\", right_on=\"PlayId\")\n    plays = pd.merge(plays, get_advanced_distance_features(players), how=\"left\", left_on=\"PlayId\", right_on=\"PlayId\")\n    plays = pd.merge(plays, compute_centroids(players), how=\"left\", left_on=\"PlayId\", right_on=\"PlayId\")\n    plays = pd.merge(plays, get_global_rusher_features(players), how=\"left\", left_on=\"PlayId\", right_on=\"PlayId\")\n    plays = rusher_vs_groups(players, plays)\n    plays = count_defense_personnel(plays)\n    plays = count_offense_personnel(plays)\n    frequencies = get_personnel_frequencies(plays)\n    plays = set_personnel_frequencies(plays, frequencies)\n    plays = compute_score_difference(plays)\n    plays = standardize_game_features(plays)\n    plays = compute_yards_to_go(plays)\n    return plays","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing model input\n\nWe have a dataset of features, but now, we need to extract features to a suitable form ready for machine learning and also prepare training values which the model will learn.\n\nAt the start of the competitions, very basic ML performed very bad, but empirical distribution performed very well. So I thought that maybe a delta learning approach would suit this problem, i.e. teach the model only to fix an empirical distribution. In the end I ended predicting full distribution, delta to empirical distribution and number of yards gained/lost. "},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"df = plays","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"print(list(df.columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First 80 values are always zero as no team in train data lost 20+ yards on a rush play.\n# ML model would predict noise here, so in this constant you can set how many values will be skipped.\n# Less values to predict means that the model can focus more on important parts.\nN_zeroed = 80","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I computed empirical distribution as in https://www.kaggle.com/ryches/model-free-benchmark\n# but I didn't use all data at once, but in groups divided by number of yards to go.\ncheckpoints = [0, 14, 21, 25, 30, 34, 38, 43, 48, 54, 60, 67, 75, 83, 91, 101]\ndistributions = [np.histogram(df.query(f\"{start} < YardsToGo <= {last}\").loc[:, \"Yards\"], bins=199, range=(-99,100), density=True)[0].cumsum() for start, last in zip(checkpoints[:-1], checkpoints[1:])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndummy = np.zeros((len(df), 199))\nY_distribution = np.zeros((len(df), 199-N_zeroed))   # Y values for full distribution\n# Y values for delta with empirical distribution\n# Y_delta = np.zeros((len(df), 199-N_zeroed))   # Predict full delta with empirical distribution\nY_delta = np.zeros((len(df), 30))   # Biggest delta with empirical distribution is between 11 and 40\n# Y values for number of yards gained/lost\nY_yards = np.zeros((len(df), 1))\ndistros = np.zeros((len(df), 199-N_zeroed))\nfor i, (playid, row) in enumerate(df[[\"YardsToGo\", \"Yards\"]].iterrows()):\n    Y_yards[i, :] = row[\"Yards\"]\n    bisection_index = bisect(checkpoints, row[\"YardsToGo\"]) - 1\n    dummy[i, (99+int(np.ceil(row[\"Yards\"]))):] = 1.0\n    Y_distribution[i, :] = dummy[i, N_zeroed:]\n    dummy[i, :] -= distributions[bisection_index]\n#     Y_distribution[i, :] = dummy[i, N_zeroed:]\n    Y_delta[i, :] = dummy[i, 89:119]\n#     Y_delta[i, :] = dummy[i, N_zeroed:]\nY = [Y_distribution, Y_delta, Y_yards]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# My ML model is NN and it is described later.\n# Nevertheless, I do not input all features at once, but in groups\n# which I think that belong together.\n# So these groups need to be extracted from the DataFrame\n# and this is the purpose of this cell.\n\ndef get_player_features(df):\n    \"\"\"\n    Get player features and the number of them..\n    \"\"\"\n    selected_columns = list()\n    unique_features = sorted([\"X\", \"Y\", \"DistanceToRusher\", \"S\", \"A\", \"Dir\", \"JerseyNumber\", \"PlayerWeight\", \"PlayerHeight\", \"PlayerBMI\",\n                       \"S_vertical\", \"S_horizontal\", \"PlayerAge\"] + [f\"ClosestEnemy-{i+1}th\" for i in range(3)] \\\n                        + [f\"DistancePCA-{i+1}th\" for i in range(3)])\n    for i in range(22):\n        for feature in unique_features:\n            selected_columns.append(f\"{feature}_{i}\")\n    N_player_features = len(selected_columns)\n    N_unique_player_features = len(unique_features)\n    player_features = df[selected_columns].values\n    return player_features, N_unique_player_features\n\ndef get_offense_features(df):\n    \"\"\"\n    Get offense features and the number of them.\n    \"\"\"\n    offense_features_list = sorted([column for column in df.columns if re.match(\"OffensePersonnel-[A-Z][A-Z]?\", column)] \\\n            + [column for column in df.columns if re.match(\"OffensePlayerTypes-.*\", column)])\n    N_offense_features = len(offense_features_list)\n    offense_features = df[offense_features_list].values\n    return offense_features, N_offense_features\n\ndef get_defense_features(df):\n    \"\"\"\n    Get defense features and the number of them\n    \"\"\"\n    defense_features_list = sorted([column for column in df.columns if re.match(\"DefensePersonnel-[A-Z][A-Z]?\", column)] \\\n            + [column for column in df.columns if re.match(\"DefensePlayerTypes-.*\", column)])\n    N_defense_features = len(defense_features_list)\n    defense_features = df[defense_features_list].values\n    return defense_features, N_defense_features\n\ndef get_global_features(df):\n    \"\"\"\n    Get global features and the number of them.\n    \"\"\"\n    global_features_list = sorted([\"YardsToGo\", \"GameClock\", \"Down\", \"Quarter\", \"DefendersInTheBox\", \"OffToDef-Score\", \n                                   \"Defense-MinDistanceToRusher\"])\n#             \"OffensiveHull-area\", \"RusherInOffenseHull\", \"DefendersInOffenseHull\",\n#             \"DefensiveHull-area\", \"RusherInDefenseHull\", \"OffendersInDefenseHull\"]\n    N_global_features = len(global_features_list)\n    global_features = df[global_features_list].values\n    return global_features, N_global_features\n\ndef get_offense_circle_features(df):\n    \"\"\"\n    Get features connected with number of offenders in the circle around the rusher.\n    And the number of them.\n    \"\"\"\n    offense_features_list = sorted([column for column in df.columns if re.match(\"OffendersInCircleAroundRusher-.*\", column)])\n    N_offense_circle_features = len(offense_features_list)\n    offense_features = df[offense_features_list].values\n    return offense_features, N_offense_circle_features\n\ndef get_defense_circle_features(df):\n    \"\"\"\n    Get features connected with number of defenders in the circle around the rusher.\n    And the number of them.\n    \"\"\"\n    defense_features_list = sorted([column for column in df.columns if re.match(\"DefendersInCircleAroundRusher-.*\", column)])\n    N_defense_circle_features = len(defense_features_list)\n    defense_features = df[defense_features_list].values\n    return defense_features, N_defense_circle_features\n    \ndef get_rusher_features(df):\n    \"\"\"\n    Get rusher features and the number of them.\n    \"\"\"\n    rusher_features_list = sorted([column for column in df.columns if re.match(\"Rusher-.*\", column)])\n    rusher_features_list.remove(\"Rusher-MainRole\")\n    rusher_features_list.remove(\"Rusher-MainRole-frequency\")\n    N_rusher_features = len(rusher_features_list)\n    rusher_features = df[rusher_features_list].values\n    return rusher_features, N_rusher_features\n\ndef get_offense_centroid_features(df):\n    \"\"\"\n    Get offense centroid features and the number of them.\n    \"\"\"\n    offense_centroid_list = sorted([column for column in df.columns if re.match(\"Centroid-Offense-.*\", column)])\n    N_offense_centroid_features = len(offense_centroid_list)\n    offense_centroid_features = df[offense_centroid_list].values\n    return offense_centroid_features, N_offense_centroid_features\n\ndef get_defense_centroid_features(df):\n    \"\"\"\n    Get defense centroid features and the number of them.\n    \"\"\"\n    defense_centroid_list = sorted([column for column in df.columns if re.match(\"Centroid-Defense-.*\", column)])\n    N_defense_centroid_features = len(defense_centroid_list)\n    defense_centroid_features = df[defense_centroid_list].values\n    return defense_centroid_features, N_defense_centroid_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_for_predictions(df, distros=None, scaler=None):\n    \"\"\"\n    Transform DataFrame with engineered features to \n    input needed by ML model.\n    \n    Parameters\n    ==========\n    - df\n        DataFrame with engineered features.\n    - distros\n        Possible empirical distribution to add as features.\n    - scaler\n        If None, new StandardScaler will be created, else\n        provided scaler will be used for scalling features.\n    \"\"\"\n    # Extract features from the DataFrame as not all are needed\n    player_features, N_unique_player_features = get_player_features(df)\n    rusher_features, N_rusher_features = get_rusher_features(df)\n    offense_features, N_offense_features = get_offense_features(df)\n    defense_features, N_defense_features = get_defense_features(df)\n    global_features, N_global_features = get_global_features(df)\n    circle_offense_features, N_offense_circle_features = get_offense_circle_features(df)\n    circle_defense_features, N_defense_circle_features = get_defense_circle_features(df)\n    centroid_offense_features, N_offense_centroid_features = get_offense_centroid_features(df)\n    centroid_defense_features, N_defense_centroid_features = get_defense_centroid_features(df)\n    # Save input lenghts in a list and in a dict\n    input_lengths = 22*[N_unique_player_features] + [N_rusher_features,\n            N_global_features, N_offense_circle_features, N_defense_circle_features, N_offense_centroid_features,\n            N_defense_centroid_features, N_offense_features, N_defense_features]  \n    input_lengths_dict = {\"Players\":N_unique_player_features, \"Rusher\":N_rusher_features, \"Offense\":N_offense_features,\n                          \"Defense\":N_defense_features, \"Global\":N_global_features, \"Circles-Off\":N_offense_circle_features,\n                          \"Circles-Def\":N_defense_circle_features, \"Centroids-Off\":N_offense_centroid_features,\n                          \"Centroids-Def\":N_defense_centroid_features}\n    # Concatenate extracted features\n    X = np.concatenate((player_features, rusher_features, global_features,\n                        circle_offense_features, circle_defense_features, \n                        centroid_offense_features, centroid_defense_features,\n                       ), axis=1)    \n    # Scale data\n    if scaler is not None:   # If scaler is provided use it\n        Xscaler = scaler\n    else:   # If scaler is not provided, create a new one and fit data with it\n        Xscaler = StandardScaler()\n        Xscaler.fit(X)\n    X = Xscaler.transform(X)\n    X = np.concatenate((X, offense_features, defense_features), axis=1)\n    # If an empirical distribution is provided as features for the model, concatenate it with current features\n    if distros is not None:\n        X = np.concatenate((X, distros), axis=1)\n        input_lengths = input_lengths + [199-N_zeroed]\n        input_lengths_dict[\"DistributionLength\"] = 199-N_zeroed\n    return X, input_lengths, Xscaler, input_lengths_dict\n\ndef split_to_inputs(X, feature_lengths):\n    \"\"\"\n    Split input for model to input layers.\n    \n    The NN takes more Input groups,\n    this function takes care of providing each Input layer\n    with its corresponding input in array of all input values called X.\n    \"\"\"\n    arrays = list()\n    start = 0\n    for i in feature_lengths:\n        end = start + i \n        arrays.append(X[:, start:end])\n        start = end\n    return arrays","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X, input_lengths, Xscaler, feature_lengths = prepare_for_predictions(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define model\n\nThis part of the notebook is rather weak and I think that much better model could have been created by e.g. ensembling models. Anyway, I created NN network that takes a lot of different inputs and returns multiple outputs. Features for Input layers are grouped in groups that made sense for me (e.g. features about player, features about offending/defending team, global features...). Multiple outputs are predicted as I wrote earlier. The base of the NN are layers for players. There are 22 players, so there are 22 groups of player's features. These are processed by 22 layers which share weights, later they are concatenated and processed further. Then more features are concatenated etc. etc. It is better to see the architecture of the model in the picture generated by the notebook."},{"metadata":{"trusted":true},"cell_type":"code","source":"def crps(y_true, y_pred):\n    \"\"\"\n    Define CRPS metric for Keras.\n    \n    Returns\n    =======\n    - float\n        CRPS loss\n    \"\"\"\n    loss = K.mean((K.cumsum(y_pred, axis=1) - y_true)**2) * (199-N_zeroed) / 199\n    return loss\n\ndef define_model():\n    \"\"\"\n    Define and return neural network.\n    \n    This is not the most clean part of the code,\n    but you can play with it as I did and turn off \n    and on layers and see whether it has better \n    or worse score. \n    The architecture is described above this code cell\n    and a figure is saved later. However, it needs to be\n    opened in a separate window and zoomed as it is \n    really wide, because there is a separate Input layer\n    for every single player.\n    \n    Returns\n    =======\n    keras.NN\n    \"\"\"\n    player_inputs = [Input(shape=(feature_lengths[\"Players\"],), name=f'PlayerInput-{i}') for i in range(22)] \n    rusher_input = Input(shape=(feature_lengths[\"Rusher\"],), name=\"RusherInput\")\n    # Player part\n    player_layer_1 = Dense(10, activation='relu', name=\"Player-Dense-1\")\n    player_batch_norm_1 = BatchNormalization(name=\"Player-BatchNorm-1\")\n#     player_dropout_1 = Dropout(0.2, name=\"Player-Dropout-1\")\n#     player_layer_2 = Dense(12, activation='relu', name=\"Player-Dense-2\")\n#     player_batch_norm_2 = BatchNormalization(name=\"Player-BatchNorm-2\")\n#     player_dropout_2 = Dropout(0.2, name=\"Player-Dropout-2\")\n#     player_layer_3 = Dense(10, activation='relu', name=\"Player-Dense-3\")\n#     player_batch_norm_3 = BatchNormalization(name=\"Player-BatchNorm-3\")\n#     player_dropout_3 = Dropout(0.2, name=\"Player-Dropout-3\")\n    player_outputs = list()\n    for i in range(22):\n        x1 = player_layer_1(player_inputs[i])\n        x1 = player_batch_norm_1(x1)\n#         x1 = player_dropout_1(x1)\n#         x2 = player_layer_2(x1)\n#         x2 = player_batch_norm_2(x2)\n#         x2 = player_dropout_2(x2)\n#         x3 = player_layer_3(x2)\n#         x3 = player_batch_norm_3(x3)\n#         x3 = player_dropout_3(x3)\n        player_outputs.append(x1)\n#         player_outputs.append(x2)\n#         player_outputs.append(x3)\n    #x = Dense(10, activation=\"relu\", name=\"Rusher-Dense\")(rusher_input)\n    #x = BatchNormalization(name=\"Rusher-BatchNorm\")(x)\n    #player_outputs.append(x)\n    player_values = Concatenate(name=\"ConcatenatePlayerOutputs\")(player_outputs)\n#     player_values = Dropout(0.4)(player_values)\n#     player_values = Dense(400, activation='relu', name=\"AllPlayers-Dense-1\")(player_values)\n#     player_values = BatchNormalization(name=\"AllPlayer-BatchNorm-1\")(player_values)\n#     player_values = Dense(200, activation='relu', name=\"AllPlayers-Dense-2\")(player_values)\n#     player_values = BatchNormalization(name=\"AllPlayer-BatchNorm-2\")(player_values)\n#     player_values = Dense(100, activation='relu', name=\"AllPlayers-Dense-3\")(player_values)\n#     player_values = BatchNormalization(name=\"AllPlayer-BatchNorm-3\")(player_values)\n#     out_delta = Dense(199-N_zeroed, activation=None, name=\"PredictingDelta\")(player_values)\n    out_delta = Dense(30, activation=None, name=\"PredictingDelta\")(player_values)\n    # Team part - Offense\n    offense_input = Input(shape=(feature_lengths[\"Offense\"],), name=\"OffenseInput\")\n    offense_embedding_1 = Embedding(feature_lengths[\"Offense\"], 5, name=\"Offense-Embedding\")\n#     offense_layer_1 = Dense(5, activation=\"relu\", name=\"Offense-Dense\")\n    offense_flatten_1 = Flatten(name=\"OffenseFlatten\")\n    offense_outputs_1 = offense_flatten_1(offense_embedding_1(offense_input))\n#     offense_outputs_1 = offense_layer_1(offense_input)\n#     offense_outputs_1 = BatchNormalization(name=\"Offense-BatchNorm\")(offense_outputs_1)\n    # Team part - Defense\n    defense_input = Input(shape=(feature_lengths[\"Defense\"],), name=\"DefenseInput\")\n    defense_embedding_1 = Embedding(feature_lengths[\"Defense\"], 5, name=\"Defense-Embedding\")\n#     defense_layer_1 = Dense(5, activation=\"relu\", name=\"Defense-Dense\")\n    defense_flatten_1 = Flatten(name=\"DefenseFlatten\")\n    defense_outputs_1 = defense_flatten_1(defense_embedding_1(defense_input))\n#     defense_outputs_1 = defense_layer_1(defense_input)\n#     defense_outputs_1 = BatchNormalization(name=\"Defense-BatchNorm\")(defense_outputs_1)  \n    # Teams together\n    team_values = Concatenate(name=\"Concatenate-OffDef\")([offense_outputs_1, defense_outputs_1])\n    team_values = Dense(5, activation=\"relu\", name=\"OffDef-Dense\")(team_values)\n    team_values = BatchNormalization(name=\"OffDef-BatchNorm\")(team_values)\n#     Global part\n    global_input = Input(shape=(feature_lengths[\"Global\"],), name=\"GlobalInput\")\n    global_values = Dense(15, activation=\"relu\", name=\"Global-Dense\")(global_input)\n    global_values = BatchNormalization(name=\"Global-BatchNorm\")(global_values)\n    # Global and team\n    gt_values = Concatenate(name=\"Concatenate-TeamAndGlobal\")([team_values, global_values])\n    gt_values = Dense(20, activation=\"relu\", name=\"TeamAndGlobal-Dense\")(gt_values)\n    gt_values = BatchNormalization(name=\"TeamAndGlobal-BatchNorm\")(gt_values)\n    # Advanced distances\n    # Circular offense features\n    offense_circle_input = Input(shape=(feature_lengths[\"Circles-Off\"],), name=\"CirclesOffenseInput\")\n    offense_circle_distances = Dense(10, activation=\"relu\", name=\"CirclesOffense-Dense\")(offense_circle_input)\n    offense_circle_distances = BatchNormalization(name=\"CirclesOffense-BatchNorm\")(offense_circle_distances)    \n    # Circular defense features\n    defense_circle_input = Input(shape=(feature_lengths[\"Circles-Def\"],), name=\"CirclesDefenseInput\")\n    defense_circle_distances = Dense(10, activation=\"relu\", name=\"CirclesDefense-Dense\")(defense_circle_input)\n    defense_circle_distances = BatchNormalization(name=\"CirclesDefense-BatchNorm\")(defense_circle_distances)\n    # Centroid offense features\n    offense_centroid_input = Input(shape=(feature_lengths[\"Centroids-Off\"],), name=\"CentroidsOffenseInput\")\n    offense_centroid_distances = Dense(5, activation=\"relu\", name=\"CentroidsOffense-Dense\")(offense_centroid_input)\n    offense_centroid_distances = BatchNormalization(name=\"CentroidsOffense-BatchNorm\")(offense_centroid_distances)    \n    # Centroid defense features\n    defense_centroid_input = Input(shape=(feature_lengths[\"Centroids-Def\"],), name=\"CentroidsDefenseInput\")\n    defense_centroid_distances = Dense(5, activation=\"relu\", name=\"CentroidsDefense-Dense\")(defense_centroid_input)\n    defense_centroid_distances = BatchNormalization(name=\"CentroidsDefense-BatchNorm\")(defense_centroid_distances)\n    # Distances together\n    advanced_distances = Concatenate(name=\"Concatenate-AdvancedDistances\")([\n        offense_circle_distances, defense_circle_distances, offense_centroid_distances, defense_centroid_distances])\n    advanced_distances = Dense(20, activation=\"relu\", name=\"AdvancedDistances-Dense\")(advanced_distances)\n    advanced_distances = BatchNormalization(name=\"AdvancedDistances-BatchNorm\")(advanced_distances)\n#     Everything together\n#     basic_distribution_input = Input(shape=(feature_lengths[\"DistributionLength\"],), name=\"EmpiricalDistribution\")\n#     basic_distribution = Dense(20, activation=\"relu\", name=\"Distribution-Dense\")(basic_distribution_input)\n#     basic_distribution = BatchNormalization(name=\"Distribution-BatchNorm\")(basic_distribution)\n    x = Concatenate(name=\"Concatenate-Everything-1\")([player_values, gt_values, advanced_distances])#, team_values, global_values, offense_outputs_1, defense_outputs_1])#, advanced_distances, basic_distribution])\n    x1 = Dense(200, activation='relu', name=\"All-Dense-1\")(x)\n    x1 = BatchNormalization(name=\"All-BatchNorm-1\")(x1)\n#     x1 = Dropout(0.1, name=\"All-Dropout-1\")(x1)\n    x2 = Dense(100, activation='relu', name=\"All-Dense-2\")(x1)\n    x2 = BatchNormalization(name=\"All-BatchNorm-2\")(x2)\n    x2 = Dropout(0.1, name=\"All-Dropout-2\")(x2)\n    prex = Concatenate(name=\"Concatenate-ForPrePrediction\")([x1, x2])\n    x3 = Dense(100, activation='relu', name=\"All-Dense-3\")(x2)\n    x3 = BatchNormalization(name=\"All-BatchNorm-3\")(x3)\n    x3 = Dropout(0.1, name=\"All-Dropout-3\")(x3)\n    x4 = Dense(100, activation='relu', name=\"All-Dense-4\")(x3)\n    x4 = BatchNormalization(name=\"All-BatchNorm-4\")(x4)\n    x4 = Dropout(0.1, name=\"All-Dropout-4\")(x4)\n    x5 = Dense(100, activation='relu', name=\"All-Dense-5\")(x4)\n    x5 = BatchNormalization(name=\"All-BatchNorm-5\")(x5)\n    x5 = Dropout(0.1, name=\"All-Dropout-5\")(x5)\n    x = Concatenate(name=\"Concatenate-Everything-2\")([x1, x2, x3, x4, x5])\n    x = Dense(100, name=\"FinalDense\")(x2)\n    out_distribution = Dense(199-N_zeroed, activation=\"softmax\", name=\"PredictingFullDistribution\")(x)\n#     out_delta = Dense(199-N_zeroed, activation=None, name=\"PredictingDelta\")(x)\n    out_yards = Dense(1, activation=None, name=\"PredictingYards\")(prex)\n#     print(player_inputs)\n    model = Model(inputs=[player_input for player_input in player_inputs]+[rusher_input, global_input,\n                          offense_circle_input, defense_circle_input, offense_centroid_input, defense_centroid_input,\n                          offense_input, defense_input],#, basic_distribution_input], \n                  outputs=[out_distribution, out_delta, out_yards]) \n    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.005, decay=1e-4),\n                  loss=[crps, keras.losses.mse, keras.losses.mae],\n                  loss_weights=[100.0, 100.0, 0.2])\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training\n\nI chose simple KFold validation, however I found out that when I train the same model multiple times with a different seed and then choose the best performing model of them, the score will improve."},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_Y_for_model(Y, train_indices, test_indices):\n    \"\"\"\n    Split Y values to training and test.\n    \n    The NN predicts more types of outputs:\n    - full distribution\n    - delta to empirical distribution\n    - yards gained/lost\n    This functions is a helper for folding,\n    as KFold only generates train and test indices,\n    but we need to create three arrays for train\n    and three for test ourselves.\n    \"\"\"\n    train_Y = list()\n    test_Y = list()\n    for Y_type in Y:\n        train_Y.append(Y_type[train_indices, :])\n        test_Y.append(Y_type[test_indices, :])\n    return train_Y, test_Y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"models = []\n\nfolds = 8   # Define number of folds here\nkf = KFold(n_splits=folds, random_state=seed)\nCV_scores = []\ni = 1\nfor train_indices, validation_indices in kf.split(X):\n    X_train, X_valid = X[train_indices, :], X[validation_indices,:]\n    Y_train, Y_valid = split_Y_for_model(Y, train_indices, validation_indices)\n    cmodels = []\n    clossess = []\n    # Training the same model on same data multiple times with different seeds\n    # and then selecting the best performing one helps to improve the score.\n    while len(cmodels) < 2:\n        seed += 1\n        set_seeds(seed)\n        checkpoint = ModelCheckpoint('ModelCheckpoints.hdf5', monitor='val_PredictingFullDistribution_loss', verbose=1, save_best_only=True, mode='auto')\n        early_stopping = EarlyStopping(monitor='val_PredictingFullDistribution_loss', mode='min', verbose=1, patience=5)\n        callbacks_list = [checkpoint, early_stopping]\n        model = define_model()\n        model.fit(split_to_inputs(X_train, input_lengths), Y_train, \n                  epochs=30, verbose=0, callbacks=callbacks_list,\n                  validation_data=(split_to_inputs(X_valid, input_lengths), Y_valid))\n        print(\"HISTORY:\", model.history.history)\n        loss = np.min(model.history.history[\"val_PredictingFullDistribution_loss\"])\n        print(\"Score:\",loss)\n        print(\"-\"*20)\n        model.load_weights(\"ModelCheckpoints.hdf5\")\n        cmodels.append(model)\n        clossess.append(loss)\n    loss = min(np.array(clossess))\n    i = np.argmin(np.array(clossess))\n    model = cmodels[i]\n    CV_scores.append(loss)\n    models.append(model)\nCV = np.mean(CV_scores)\nprint(f\"CV: {CV} +- {np.std(CV_scores)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model(model, to_file='ModelArchitecture.png', show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predictions\n\nThere are a few tricks you may have used for predictions. \n\n1. Most important trick - You need to realize that you do not need to predict all 200 values. You need to predict only 100 values and the rest can be computed analytically. A team can not have more than 100 yards in the end of a play and they can not loose more yards than they currently have. So you should go through values after your models predicts them and set those which you know for sure. There was a discussion about it in the forum, but almost noone reacted.\n2. Second trick - First eighty values are always zero. They don't have to be as a team can loose more than 20 yards, but it is very rare and your model will predict some noise in this area anyway, so why not to set it to zero right away."},{"metadata":{"trusted":true},"cell_type":"code","source":"def set_analytic_half(output, yards_to_go):\n    \"\"\"\n    Half of values are zeros or ones, set those which we know for sure.\n    \n    All values lower than -YardsToGo are zero, as team can not loose\n    more yards than they currently have. All values higher than\n    100-YardsToGo are ones as team can not have in the end more yards\n    than one hundred.\n    \"\"\"\n    if int(np.ceil(yards_to_go)) - N_zeroed > 0:\n        output[:int(int(np.ceil(yards_to_go)) - N_zeroed)] = 0.0\n    output[-int(100-np.floor(yards_to_go)):] = 1.0\n    return output\n\ndef correct_growth1(output):\n    \"\"\"\n    The output has to be strictly monotone, correct it.\n    \n    This is very single correction, if a value is lower\n    than preceeding value, set it to the preceeding value instead.\n    \"\"\"\n    new_values = list()\n    for i in range(len(output)):\n        if i != 0 and output[i] < new_values[i-1]:\n            new_values.append(new_values[i-1])    \n        else:\n            new_values.append(output[i])\n    return np.array(new_values)\n\ndef is_increasing(arr):\n    \"\"\"\n    Check if array is monotonously increasing.\n    \"\"\"\n    arr = np.diff(arr)\n    return np.all(arr >= 0.0)\n\ndef correct_growth2(output):\n    \"\"\"\n    The output has to be strictly monotone, correct it.\n    \n    This is more complicated and in the end not used version.\n    \"\"\"\n    diff = np.diff(output)\n    while not np.all(diff >= 0.0):\n        for i in range(1, len(output)-1):\n            if output[i] < output[i-1]:\n                if 0.0 > diff[i-1] > -1e-10:\n                    output[i] = output[i-1]\n                else:\n                    output[i] = np.mean(output[i-1:i+1])\n                    output[i-1] += (output[i]-output[i-1])*0.1\n                    output[i+1] += (output[i]-output[i+1])*0.1\n        if output[len(output)-1] < output[len(output)-2]:\n            output[len(output)-1] = 1.0\n        diff = np.diff(output)\n    return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mandatory cell\n\nfrom kaggle.competitions import nflrush\nenv = nflrush.make_env()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"%%time\nbasic_plays_columns.remove(\"Yards\")\nfor (test_players, sample_prediction_df) in tqdm(env.iter_test()):\n    # Process the test dataset\n    test_plays = copy.deepcopy(test_players).loc[:, basic_plays_columns].drop_duplicates()\n    test_players = test_players.loc[:, basic_players_columns]\n    model_input = prepare_data(test_players, test_plays, frequencies)    \n    # Get empirical distribution\n    bisection_index = bisect(checkpoints, model_input[\"YardsToGo\"].values[0]) - 1\n    basic_distribution = distributions[bisection_index]\n    # Get feature for the model\n    X_test, _, _, _ = prepare_for_predictions(model_input, scaler=Xscaler)\n    # Predict delta of empirical distribution (next commented line) or predict full distribution\n    input_arrays = split_to_inputs(X_test, input_lengths)\n    y_predicted = np.mean([np.cumsum(model.predict(input_arrays)[0], axis=1) for model in models], axis=0)\n    # y_predicted = np.mean([model.predict(input_arrays)[1] for model in models], axis=0)\n    # Add empirical distribution if you computed delta\n    # y_predicted += basic_distribution[N_zeroed:]\n    y_predicted = y_predicted.flatten()\n    # Some values can be computed analytically, exactly 100 values are zero or one.\n    y_predicted = set_analytic_half(y_predicted, model_input[\"YardsToGo\"].values[0])\n    # Fix values which are 0-epsilon or 1+epsilon, so that they fit in zero-one range\n    y_predicted = np.clip(y_predicted, 0, 1)\n    # Result has to be monotonously incresing\n    y_predicted = correct_growth1(y_predicted)\n    y_predicted = set_analytic_half(y_predicted, model_input[\"YardsToGo\"].values[0])\n    # First 80 values are always zero, no need to learn them.\n    y_predicted = np.concatenate((np.zeros(N_zeroed), y_predicted))   \n    final_output = pd.DataFrame(data=np.array(y_predicted).reshape(1,-1), columns=sample_prediction_df.columns)\n    env.predict(final_output)\nenv.write_submission_file()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Errors you may have done (or I did)\n\n- Sort features - If you always sort your features, you will be sure that you provide them to a model everytime in the same order. \n- Use the same scaler - You need to save the scaler used for training data and use it for the test data, do not generate a new one."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}