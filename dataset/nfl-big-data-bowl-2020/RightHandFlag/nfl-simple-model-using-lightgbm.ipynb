{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction\nI will introduce a simple method using lightGBM as a starter.","metadata":{}},{"cell_type":"markdown","source":"## import\nLoad the necessary libraries.","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom kaggle.competitions import nflrush\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\nimport random\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nimport gc\nimport pickle\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization\nimport tqdm","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-06-20T21:44:32.812903Z","iopub.execute_input":"2021-06-20T21:44:32.813322Z","iopub.status.idle":"2021-06-20T21:44:32.821175Z","shell.execute_reply.started":"2021-06-20T21:44:32.813255Z","shell.execute_reply":"2021-06-20T21:44:32.820074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## train data\nThe shape of train data is 509762 × 49.\nBut, since one set consists of 22 lines, the actual number of data is 23171.\nI converted it to a format that is easy to use.","metadata":{}},{"cell_type":"code","source":"env = nflrush.make_env()\ntrain_df = pd.read_csv('/kaggle/input/nfl-big-data-bowl-2020/train.csv', low_memory=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T21:44:35.001584Z","iopub.execute_input":"2021-06-20T21:44:35.001908Z","iopub.status.idle":"2021-06-20T21:44:35.040201Z","shell.execute_reply.started":"2021-06-20T21:44:35.00186Z","shell.execute_reply":"2021-06-20T21:44:35.038586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unused_columns = [\"GameId\",\"PlayId\",\"Team\",\"Yards\",\"TimeHandoff\",\"TimeSnap\"]","metadata":{"execution":{"iopub.status.busy":"2021-06-20T19:41:20.240189Z","iopub.execute_input":"2021-06-20T19:41:20.240481Z","iopub.status.idle":"2021-06-20T19:41:20.245663Z","shell.execute_reply.started":"2021-06-20T19:41:20.240439Z","shell.execute_reply":"2021-06-20T19:41:20.244273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#将不在unused_columns中的且每个球员会有不同数值的数据视为unique_column\nunique_columns = []\nfor c in train_df.columns:\n    if c not in unused_columns+[\"PlayerBirthDate\"] and len(set(train_df[c][:11]))!= 1:\n        unique_columns.append(c)\n        print(c,\" is unique\")\nunique_columns+=[\"BirthY\"]","metadata":{"execution":{"iopub.status.busy":"2021-06-20T19:41:20.247162Z","iopub.execute_input":"2021-06-20T19:41:20.24748Z","iopub.status.idle":"2021-06-20T19:41:20.278244Z","shell.execute_reply.started":"2021-06-20T19:41:20.24744Z","shell.execute_reply":"2021-06-20T19:41:20.276711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#判断数据是否按照playID-Team来排序\nok = True\nfor i in range(0,509762,22):\n    p=train_df[\"PlayId\"][i]\n    for j in range(1,22):\n        if(p!=train_df[\"PlayId\"][i+j]):\n            ok=False\n            break\nprint(\"train data is sorted by PlayId.\" if ok else \"train data is not sorted by PlayId.\")\nok = True\nfor i in range(0,509762,11):\n    p=train_df[\"Team\"][i]\n    for j in range(1,11):\n        if(p!=train_df[\"Team\"][i+j]):\n            ok=False\n            break\nprint(\"train data is sorted by Team.\" if ok else \"train data is not sorted by Team.\")","metadata":{"execution":{"iopub.status.busy":"2021-06-20T19:41:20.285534Z","iopub.execute_input":"2021-06-20T19:41:20.285874Z","iopub.status.idle":"2021-06-20T19:41:38.914033Z","shell.execute_reply.started":"2021-06-20T19:41:20.285818Z","shell.execute_reply":"2021-06-20T19:41:38.912962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the training data was sorted, preprocessing can be done easily.","metadata":{}},{"cell_type":"code","source":"#将（每一行的数据都相同）非特殊列合并起来，并将特殊列逐一添加到末尾（对其中三列需要在之后作特殊处理，因此不在此处合并）\nall_columns = []\nfor c in train_df.columns:\n    if c not in unique_columns + unused_columns+[\"DefensePersonnel\",\"GameClock\",\"PlayerBirthDate\"]:\n        all_columns.append(c)\nall_columns.append(\"DL\")\nall_columns.append(\"LB\")    \nall_columns.append(\"DB\")\nall_columns.append(\"GameHour\")   \nfor c in unique_columns:\n    for i in range(22):\n        all_columns.append(c+str(i))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T19:41:38.916174Z","iopub.execute_input":"2021-06-20T19:41:38.91673Z","iopub.status.idle":"2021-06-20T19:41:38.925731Z","shell.execute_reply.started":"2021-06-20T19:41:38.916507Z","shell.execute_reply":"2021-06-20T19:41:38.924446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lbl_dict = {}\nfor c in train_df.columns:\n    if c == \"DefensePersonnel\":#记录防守人员配置（每个位置上的防守人员数）\n        arr = [[int(s[0]) for s in t.split(\", \")] for t in train_df[\"DefensePersonnel\"]]\n        train_df[\"DL\"] = np.array([a[0] for a in arr])\n        train_df[\"LB\"] = np.array([a[1] for a in arr])\n        train_df[\"DB\"] = np.array([a[2] for a in arr])\n    elif c == \"GameClock\":#比赛时间仅记录当前的小时（主要用来区分比赛进行在一天内的哪一个时间段）\n        arr = [[int(s) for s in t.split(\":\")] for t in train_df[\"GameClock\"]]\n        train_df[\"GameHour\"] = pd.Series([a[0] for a in arr])\n    elif c == \"PlayerBirthDate\":#球员生日仅记录其出生年（主要用于代表球员年龄）\n        arr = [[int(s) for s in t.split(\"/\")] for t in train_df[\"PlayerBirthDate\"]]\n        train_df[\"BirthY\"] = pd.Series([a[2] for a in arr])\n    # elif c == \"PlayerHeight\":\n    #     arr = [float(s.split(\"-\")[0]) * 30.48 + float(s.split(\"-\")[1]) * 2.54\n    #         for s in list(train_df[\"PlayerHeight\"])]\n    #     train_df[\"PlayerHeight\"] = pd.Series(arr)\n    elif train_df[c].dtype=='object' and c not in unused_columns: \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(train_df[c].values))\n        lbl_dict[c] = lbl\n        train_df[c] = lbl.transform(list(train_df[c].values))\n","metadata":{"execution":{"iopub.status.busy":"2021-06-20T19:41:38.927534Z","iopub.execute_input":"2021-06-20T19:41:38.927957Z","iopub.status.idle":"2021-06-20T19:42:08.029889Z","shell.execute_reply.started":"2021-06-20T19:41:38.927879Z","shell.execute_reply":"2021-06-20T19:42:08.028751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#删去不需要的行\ndrop_columns = [\"Team\", \"GameClock\", \"DefensePersonnel\", \"TimeHandoff\", \"TimeSnap\", \"PlayerBirthDate\"]\ntrain_df = train_df.drop(drop_columns, axis=1)#丢弃列","metadata":{"execution":{"iopub.status.busy":"2021-06-20T19:42:08.031239Z","iopub.execute_input":"2021-06-20T19:42:08.031526Z","iopub.status.idle":"2021-06-20T19:42:08.372244Z","shell.execute_reply.started":"2021-06-20T19:42:08.031479Z","shell.execute_reply":"2021-06-20T19:42:08.371359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#将22行数据压缩到一行内\ntrain_data=np.zeros((509762//22,len(all_columns)))\nfor i in tqdm.tqdm(range(0,509762,22)):\n    count=0\n    for c in all_columns:\n        if c in train_df:\n            train_data[i//22][count] = train_df[c][i]\n            count+=1\n    for c in unique_columns:\n        for j in range(22):\n            train_data[i//22][count] = train_df[c][i+j]\n            count+=1        ","metadata":{"_kg_hide-output":false,"scrolled":true,"execution":{"iopub.status.busy":"2021-06-20T19:42:08.373903Z","iopub.execute_input":"2021-06-20T19:42:08.374326Z","iopub.status.idle":"2021-06-20T19:45:10.675445Z","shell.execute_reply.started":"2021-06-20T19:42:08.37425Z","shell.execute_reply":"2021-06-20T19:45:10.674447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#取出训练用的标签，并装好训练数据\ny_train_ = np.array([train_df[\"Yards\"][i] for i in range(0,509762,22)])\nX_train = pd.DataFrame(data=train_data,columns=all_columns)\ntrain_data_df = pd.DataFrame(data=train_data)\ntrain_data_df.to_csv(\"train_data_cdropped.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-06-20T21:25:42.852598Z","iopub.execute_input":"2021-06-20T21:25:42.853076Z","iopub.status.idle":"2021-06-20T21:25:54.298252Z","shell.execute_reply.started":"2021-06-20T21:25:42.853034Z","shell.execute_reply":"2021-06-20T21:25:54.297325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = [0 for i in range(199)]\nfor y in y_train_:#将其转换成概率标签\n    data[int(y+99)]+=1\nplt.plot([i-99 for i in range(199)],data)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T21:26:02.019374Z","iopub.execute_input":"2021-06-20T21:26:02.01972Z","iopub.status.idle":"2021-06-20T21:26:02.240454Z","shell.execute_reply.started":"2021-06-20T21:26:02.019664Z","shell.execute_reply":"2021-06-20T21:26:02.238206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the variance is small, I standardized the objective variable.","metadata":{}},{"cell_type":"code","source":"X_train.head","metadata":{"execution":{"iopub.status.busy":"2021-06-20T21:26:04.539582Z","iopub.execute_input":"2021-06-20T21:26:04.539916Z","iopub.status.idle":"2021-06-20T21:26:04.566837Z","shell.execute_reply.started":"2021-06-20T21:26:04.539865Z","shell.execute_reply":"2021-06-20T21:26:04.565737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try_clf = 1#选择跑策略树算法（0）还是跑回归模型神经网络算法（1）还是跑分类模型神经网络算法（2）","metadata":{"execution":{"iopub.status.busy":"2021-06-20T21:44:44.131483Z","iopub.execute_input":"2021-06-20T21:44:44.131802Z","iopub.status.idle":"2021-06-20T21:44:44.135697Z","shell.execute_reply.started":"2021-06-20T21:44:44.131755Z","shell.execute_reply":"2021-06-20T21:44:44.135011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scaler = preprocessing.StandardScaler()\n# scaler.fit([[y] for y in y_train_])\n# y_train = np.array([y[0] for y in scaler.transform([[y] for y in y_train_])])\n#对数据进行标准化\n\nscaler = preprocessing.StandardScaler()\nscaler.fit(y_train_.reshape(-1, 1))#获得均值和方差\ny_train = scaler.transform(y_train_.reshape(-1, 1)).flatten()\n\nscaler1 = preprocessing.StandardScaler()\nscaler1.fit(X_train)#获得均值和方差\nX_train = pd.DataFrame(data=scaler1.transform(X_train))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T21:44:45.639099Z","iopub.execute_input":"2021-06-20T21:44:45.639698Z","iopub.status.idle":"2021-06-20T21:44:45.88886Z","shell.execute_reply.started":"2021-06-20T21:44:45.639619Z","shell.execute_reply":"2021-06-20T21:44:45.888207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## train\nI used LGBMRegressor.\nI wanted to use multi-class classification, but the number of datasets was small and it was difficult to split them including all labels.","metadata":{}},{"cell_type":"code","source":"y_valid_pred = np.zeros(X_train.shape[0])\nmodels = []\nfolds = 10\nseed = 222\nif try_clf == 0:\n    kf = KFold(n_splits = folds, shuffle = True, random_state=seed)\n\n\n    for tr_idx, val_idx in kf.split(X_train, y_train):#分割数据集、校验集\n        tr_x, tr_y = X_train.iloc[tr_idx,:], y_train[tr_idx]\n        vl_x, vl_y = X_train.iloc[val_idx,:], y_train[val_idx]\n\n        print(len(tr_x),len(vl_x))\n        tr_data = lgb.Dataset(tr_x, label=tr_y)\n        vl_data = lgb.Dataset(vl_x, label=vl_y)  \n        clf = lgb.LGBMRegressor(n_estimators=200,learning_rate=0.01)#使用回归模型\n        clf.fit(tr_x, tr_y,\n            eval_set=[(vl_x, vl_y)],\n            early_stopping_rounds=20,\n            verbose=False)\n        y_valid_pred[val_idx] += clf.predict(vl_x, num_iteration=clf.best_iteration_)\n        models.append(clf)\n\n    gc.collect()\nelif try_clf == 1:\n    from sklearn import neural_network\n    X_train.head\n    X_train[np.isnan(X_train) == True] = 0#把数据集里面的NaN数据全部替换为0\n    kf = KFold(n_splits = folds, shuffle = True, random_state=seed)\n\n    r_y_train = scaler.inverse_transform(y_train)\n    for tr_idx, val_idx in kf.split(X_train, r_y_train):#分割数据集、校验集\n        tr_x, tr_y = X_train.iloc[tr_idx,:], r_y_train[tr_idx]\n        vl_x, vl_y = X_train.iloc[val_idx,:], r_y_train[val_idx]\n\n        print(len(tr_x),len(vl_x))\n        mlp = neural_network.MLPRegressor(hidden_layer_sizes=(256, 128), activation=\"logistic\",\n                     solver='adam', alpha=0.0001,\n                     batch_size='auto', learning_rate=\"adaptive\",\n                     learning_rate_init=0.001,\n                     power_t=0.5, max_iter=200,tol=1e-4)\n        mlp.fit(tr_x, tr_y)\n        \n        y_valid_pred[val_idx] += scaler.transform(mlp.predict(vl_x).reshape(-1, 1)).flatten()\n        \n        y_p = mlp.predict(vl_x)\n        \n        #计算评估函数（局部结果）\n        y_pred = np.zeros((len(val_idx),199))\n        y_ans = np.zeros((len(val_idx),199))\n        for i,p in enumerate(np.round(y_p)):\n            p+=99\n            for j in range(199):\n                if j>=p+10:\n                    y_pred[i][j]=1.0\n                elif j>=p-10:\n                    y_pred[i][j]=(j+10-p)*0.05\n\n        for i,p in enumerate(r_y_train[val_idx]):\n            p+=99\n            for j in range(199):\n                if j>=p:\n                    y_ans[i][j]=1.0\n        print(\"validation score:\",np.sum(np.power(y_pred-y_ans,2))/(199*(len(val_idx))))\n        #计算评估函数（局部结果）\n        \n        \n        #for i in range(100):\n        #    print(y_p[i], r_y_train[i])\n        mlp_loss_curve = mlp.loss_curve_\n        plt.plot(range(1, len(mlp_loss_curve)+1), mlp_loss_curve)\n        plt.show()\n        models.append(mlp)\n\n    gc.collect()\nelif try_clf == 2:\n    from sklearn import neural_network\n    X_train.head\n    X_train[np.isnan(X_train) == True] = 0#把数据集里面的NaN数据全部替换为0\n    kf = KFold(n_splits = folds, shuffle = True, random_state=seed)\n\n    r_y_train = scaler.inverse_transform(y_train)\n    tr_y_1 = np.zeros((len(r_y_train), 199))\n    for i, d in enumerate(r_y_train):\n        for j in range(199):\n            if j == d:\n                tr_y_1[i][j] = 1\n    \n    print(\"begin training...\")\n    for tr_idx, val_idx in kf.split(X_train, r_y_train):#分割数据集、校验集\n        tr_x, tr_y = X_train.iloc[tr_idx,:], r_y_train[tr_idx].astype(\"int\")\n        vl_x = X_train.iloc[val_idx,:]\n        vl_y = tr_y_1[val_idx]\n            \n        print(len(tr_x),len(vl_x))\n        mlp = neural_network.MLPClassifier(hidden_layer_sizes=(256, 128), activation=\"logistic\",\n                     solver='adam', alpha=0.0001,\n                     batch_size='auto', learning_rate=\"adaptive\",\n                     learning_rate_init=0.001,\n                     power_t=0.5, max_iter=200,tol=1e-4)\n        mlp.fit(tr_x, tr_y)\n        y_pred = mlp.predict_proba(vl_x)\n        for i in range(50):\n            print(\"pred:\", y_pred[i,:])\n            print(\"truth:\", vl_y[i,:])\n        print(\"validation score:\",np.sum(np.power(y_pred-vl_y,2))/(199*(len(val_idx))))#评估（局部）\n        mlp_loss_curve = mlp.loss_curve_\n        plt.plot(range(1, len(mlp_loss_curve)+1), mlp_loss_curve)\n        plt.show()\n        \n    gc.collect()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-20T22:19:27.500574Z","iopub.execute_input":"2021-06-20T22:19:27.50101Z","iopub.status.idle":"2021-06-20T22:19:28.116704Z","shell.execute_reply.started":"2021-06-20T22:19:27.500916Z","shell.execute_reply":"2021-06-20T22:19:28.115208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## evaluation\nContinuous Ranked Probability Score (CRPS) is derived based on the predicted scalar value.\nThe CRPS is computed as follows:\n$$\nC=\\frac{1}{199N}\\sum_{m=1}^N\\sum_{n=-99}^{99}(P(y\\geq n)-H(n-Y_m))^2\n$$\n$H(x)=1$ if $x\\geq 0$ else $0$","metadata":{}},{"cell_type":"code","source":"r_y_valid_pred = scaler.inverse_transform(y_valid_pred)\nr_y_train = scaler.inverse_transform(y_train)\nfor i in range(100):\n    print(r_y_valid_pred[i], r_y_train[i])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-06-20T19:45:22.958931Z","iopub.status.idle":"2021-06-20T19:45:22.959401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = np.zeros((509762//22,199))\ny_ans = np.zeros((509762//22,199))\nfor i,p in enumerate(np.round(scaler.inverse_transform(y_valid_pred))):\n    p+=99\n    for j in range(199):\n        if j>=p+10:\n            y_pred[i][j]=1.0\n        elif j>=p-10:\n            y_pred[i][j]=(j+10-p)*0.05\n\nfor i,p in enumerate(scaler.inverse_transform(y_train)):\n    p+=99\n    for j in range(199):\n        if j>=p:\n            y_ans[i][j]=1.0\nprint(\"validation score:\",np.sum(np.power(y_pred-y_ans,2))/(199*(509762//22)))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T22:18:37.180061Z","iopub.execute_input":"2021-06-20T22:18:37.180378Z","iopub.status.idle":"2021-06-20T22:18:51.216858Z","shell.execute_reply.started":"2021-06-20T22:18:37.180339Z","shell.execute_reply":"2021-06-20T22:18:51.215973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_demo = np.zeros(199)\ny_pred_sigmoid_demo = np.zeros(199)\np = 5 + 99\n\nfor j in range(199):\n    y_pred_sigmoid_demo[j] = 1 / (1 + np.exp((p-j)/3))\nplt.plot(range(199), y_pred_sigmoid_demo)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T22:29:18.590938Z","iopub.execute_input":"2021-06-20T22:29:18.591304Z","iopub.status.idle":"2021-06-20T22:29:18.778716Z","shell.execute_reply.started":"2021-06-20T22:29:18.591252Z","shell.execute_reply":"2021-06-20T22:29:18.777497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## make submission\n\nWhen there is a label that does not exist in the training data, it is handled as nan.\nIf you can check the error one by one and complement it, you will get better score.","metadata":{}},{"cell_type":"code","source":"index = 0\nfor (test_df, sample_prediction_df) in tqdm.tqdm(env.iter_test()):\n    for c in test_df.columns:\n        if c == \"DefensePersonnel\":\n            try:\n                arr = [[int(s[0]) for s in t.split(\", \")] for t in test_df[\"DefensePersonnel\"]]\n                test_df[\"DL\"] = [a[0] for a in arr]\n                test_df[\"LB\"] = [a[1] for a in arr]\n                test_df[\"DB\"] = [a[2] for a in arr]\n            except:\n                test_df[\"DL\"] = [np.nan for i in range(22)]\n                test_df[\"LB\"] = [np.nan for i in range(22)]\n                test_df[\"DB\"] = [np.nan for i in range(22)]\n        elif c == \"GameClock\":\n            try:\n                arr = [[int(s) for s in t.split(\":\")] for t in test_df[\"GameClock\"]]\n                test_df[\"GameHour\"] = pd.Series([a[0] for a in arr])\n            except:\n                test_df[\"GameHour\"] = [np.nan for i in range(22)]\n        elif c == \"PlayerBirthDate\":\n            try:\n                arr = [[int(s) for s in t.split(\"/\")] for t in test_df[\"PlayerBirthDate\"]]\n                test_df[\"BirthY\"] = pd.Series([a[2] for a in arr])\n            except:\n                test_df[\"BirthY\"] = [np.nan for i in range(22)]\n        # elif c == \"PlayerHeight\":\n        #     try:\n        #         arr = [float(s.split(\"-\")[0]) * 30.48 + float(s.split(\"-\")[1]) * 2.54\n        #             for s in list(test_df[\"PlayerHeight\"])]\n        #         test_df[\"PlayerHeight\"] = pd.Series(arr)\n        #     except:\n        #         test_df[\"PlayerHeight\"] = [np.nan for i in range(22)]\n        elif c in lbl_dict and test_df[c].dtype=='object'and c not in unused_columns\\\n            and not pd.isnull(test_df[c]).any():\n            try:\n                test_df[c] = lbl_dict[c].transform(list(test_df[c].values))\n            except:\n                test_df[c] = [np.nan for i in range(22)]\n    #删去不需要的行\n    #drop_columns = [\"GameClock\", \"DefensePersonnel\", \"TimeHandoff\", \"TimeSnap\", \"PlayerBirthDate\"]\n    #test_df.drop(drop_columns, axis=1)#丢弃列\n    count=0\n    test_data = np.zeros((1,len(all_columns)))\n\n    for c in all_columns:\n        if c in test_df:\n            try:\n                test_data[0][count] = test_df[c][index]\n            except:\n                if try_clf:\n                    test_data[0][count] = np.nan\n                else:\n                    test_data[0][count] = 0\n            count+=1\n    for c in unique_columns:\n        for j in range(22):\n            try:\n                test_data[0][count] = test_df[c][index + j]\n            except:\n                if try_clf:\n                    test_data[0][count] = np.nan\n                else:\n                    test_data[0][count] = 0\n            count+=1\n    #test_data[np.isnan(test_data) == True] = 0\n    if try_clf == 1:\n        test_data = scaler1.transform(test_data)\n    elif try_clf == 2:\n        test_data = scaler1.transform(test_data)\n    y_pred = np.zeros(199)\n    \n    #两个模型的输出形式不同，因此需要不同的处理方式\n    if try_clf == 0 or try_clf == 1:\n        if try_clf == 0:\n            y_pred_p = np.sum(np.round(scaler.inverse_transform(\n                [model.predict(test_data)[0] for model in models])))/folds\n        elif try_clf == 1:\n            y_pred_p = np.sum(np.round([model.predict(test_data)[0] for model in models]))/folds\n        y_pred_p += 99\n        for j in range(199):\n            if j>=y_pred_p+10:\n                y_pred[j]=1.0\n            elif j>=y_pred_p-10:\n                y_pred[j]=(j+10-y_pred_p)*0.05\n    env.predict(pd.DataFrame(data=[y_pred],columns=sample_prediction_df.columns))\n    index += 22\nenv.write_submission_file()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T19:45:22.961633Z","iopub.status.idle":"2021-06-20T19:45:22.961996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The organizers seemed to expect to predict one by one, so I did. \nHowever, it seems that it is likely to be faster to predict at once after all the evaluation data is acquired by dummy input.\n\n\nThis model is a simple one that has not been tuned, so I think we can still expect a better score.\nPlease let me know if you have any opinions or advice.","metadata":{}}]}