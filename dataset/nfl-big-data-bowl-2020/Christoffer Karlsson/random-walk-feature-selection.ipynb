{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Choosing features using simulated annealing\n\nA quick experiment to see if it's possible to select features using a random walk.\n\nThe idea is simple, try out sets of feature by taking a random walk through the space of all possible features.  At each step, we toggle one feature on or off at random, and if our estimator performs better using this set of features we continue onwards from there.\n\nIf the estimator performs worse, we _probably_ go back to the previous step and try again, but with a small probability — depending on just how much worse — we choose this new set of features anyway.  This helps us avoid getting stuck in a local minimum.\n\nTo make sure we still settle with a good set of features, we decrease the probability of accepting a worse state a little each step.\n\nNow, this is going to be a bit slow if we use our regular estimators for this competion, so just as in many other feature-selection techniques, it's probably better to use some very fast estimator that still can give us an idea of how important various features are.\n\nFor more details see the [repo](https://github.com/chrka/rwfs)."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install git+https://github.com/chrka/rwfs.git","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create dataset\n\nWe'll build a dataset from some of the hopefully more important features.  For simplicity, let's just use per-play features, and not any of the per-player features."},{"metadata":{},"cell_type":"markdown","source":"### Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\n\nnp.random.seed(20191113)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Handle inconsistent team abbreviations."},{"metadata":{"trusted":true},"cell_type":"code","source":"def _rename_team(df, fro, to):\n    df.loc[df['VisitorTeamAbbr'] == fro, 'VisitorTeamAbbr'] = to\n    df.loc[df['HomeTeamAbbr'] == fro, 'HomeTeamAbbr'] = to\n\n    \nclass TeamAbbrCleaner(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        df = X.copy()\n        _rename_team(df, 'BAL', 'BLT')\n        _rename_team(df, 'CLE', 'CLV')\n        _rename_team(df, 'ARI', 'ARZ')\n        _rename_team(df, 'HOU', 'HST')\n        return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create raw feature set (per-play data only)."},{"metadata":{"trusted":true},"cell_type":"code","source":"class FeaturePreprocessor(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        df = X.groupby('PlayId').first().reset_index().copy()\n        \n        # Rename unchanged variables\n        df.rename(columns={\n            'GameId': 'game_id',\n            'PlayId': 'play_id',\n            'Season': 'season',\n            'Quarter': 'quarter',\n            'Down': 'down',\n            'Distance': 'distance',\n            'Week': 'week',\n            'OffenseFormation': 'offense_formation',\n            'DefendersInTheBox': 'defenders_in_the_box'\n        }, inplace=True)\n        \n        # Arrange features from the offense's direction\n        team_on_offense = np.where(df['PossessionTeam'] == df['HomeTeamAbbr'],\n                                  'home', \n                                   'away')\n        df['offense_is_home'] = team_on_offense == 'home'\n\n        df['offense_score'] = np.where(df['offense_is_home'],\n                                      df['HomeScoreBeforePlay'],\n                                      df['VisitorScoreBeforePlay'])\n\n        df['defense_score'] = np.where(df['offense_is_home'],\n                                       df['VisitorScoreBeforePlay'],\n                                       df['HomeScoreBeforePlay'])\n        \n        # This works even at YardLine 50 when FieldPosition is NA\n        df['line_of_scrimmage'] = np.where(df['FieldPosition'] == df['PossessionTeam'],\n                                           df['YardLine'],\n                                           100 - df['YardLine'])\n\n        # Time between snap and handoff\n        time_handoff = pd.to_datetime(df['TimeHandoff'])\n        time_snap = pd.to_datetime(df['TimeSnap'])\n        time_to_handoff = (time_handoff - time_snap).dt.total_seconds()\n        time_to_handoff = np.round(time_to_handoff).astype(int)\n        df['time_to_handoff'] = time_to_handoff\n        \n        # Convert game clock to seconds\n        game_clock = df['GameClock'].str.extract(r'(?P<MM>\\d\\d):(?P<SS>\\d\\d):\\d\\d')\n        df['game_clock'] = 60 * game_clock['MM'].astype(int) + game_clock['SS'].astype(int)\n        \n        return df[['game_id', 'play_id', 'season', 'quarter', 'down', 'distance', 'week',\n                   'offense_formation', 'defenders_in_the_box', \n                   'offense_is_home', 'offense_score', 'defense_score', \n                   'line_of_scrimmage', 'time_to_handoff', 'game_clock']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TargetPreprocessor(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        return X.groupby('PlayId').first().reset_index()['Yards'].copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Base feature set\n\nMake everything numeric and reasonably scaled in order to be able to feed the data into a linear regressor."},{"metadata":{"trusted":true},"cell_type":"code","source":"# ColumnTransformer does the same job, but loses column names.\n# Adding them afterwords is quicker than this, but harder to maintain.\n# A more convenient option is to use a name-preservering substitute or roll\n# one's own.\n\ndef _hot_names(features, hot):\n    names = []\n    for feature, categories in zip(features, hot.categories_):\n        names.extend([f\"{feature}_{cat}\" for cat in categories])\n    return names\n\nclass PerPlayDatasetTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    \n    def fit(self, X, y=None):\n        # One-hot encode 'quarter' and 'down'\n        self.quarter_down_one_hot = OneHotEncoder(handle_unknown='ignore', sparse=False)\n        self.quarter_down_one_hot.fit(X[['quarter', 'down']])\n        \n        # Impute and one-hot encode 'offense_formation'\n        self.encode_formation = Pipeline([\n                ('imp', SimpleImputer(strategy='constant', fill_value='MISSING')),\n                ('hot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n            ])\n        self.encode_formation.fit(X[['offense_formation']])\n        \n        # One-hot encode 'time_to_handoff' for values of 1 and 2\n        self.encode_tth = OneHotEncoder(categories=[[1, 2]], \n                                            handle_unknown='ignore',\n                                            sparse=False)\n        self.encode_tth.fit(X[['time_to_handoff']])\n\n        # Min-max scale\n        self.mm_scale = MinMaxScaler()\n        self.mm_scale.fit(X[['distance', 'week', 'offense_score', 'defense_score', \n                             'line_of_scrimmage', 'game_clock']])\n\n        self.scale_defenders_in_the_box = Pipeline([\n            ('imp', SimpleImputer(strategy='most_frequent')),\n            ('std', StandardScaler())\n        ])\n        self.scale_defenders_in_the_box.fit(X[['defenders_in_the_box']])\n        \n        return self\n    \n    def transform(self, X, y=None):\n        # Passthrough values\n        keep = X[['offense_is_home']]\n\n        # One-hot encode 'quarter' and 'down'\n        qd = self.quarter_down_one_hot.transform(X[['quarter', 'down']])\n        qd = pd.DataFrame(qd, columns=_hot_names(['quarter', 'down'], self.quarter_down_one_hot))\n        \n        # Impute and one-hot encode 'offense_formation'\n        fm = self.encode_formation.transform(X[['offense_formation']])\n        fm = pd.DataFrame(fm, columns=_hot_names(['offense_formation'], \n                                                self.encode_formation.named_steps['hot']))\n        \n        # One-hot encode 'time_to_handoff' for values of 1 and 2\n        tth = self.encode_tth.transform(X[['time_to_handoff']])\n        tth = pd.DataFrame(tth, columns=_hot_names(['time_to_handoff'],\n                                                  self.encode_tth))\n        \n        # Min-max scale\n        mms = self.mm_scale.transform(X[['distance', 'week', 'offense_score', 'defense_score', \n                             'line_of_scrimmage', 'game_clock']])\n        mms = pd.DataFrame(mms, columns=['distance', 'week', 'offense_score', 'defense_score', \n                           'line_of_scrimmage', 'game_clock'])\n        \n        ditb = self.scale_defenders_in_the_box.transform(X[['defenders_in_the_box']])\n        ditb = pd.DataFrame(ditb, columns=['defenders_in_the_box'])\n        \n        return pd.concat([keep, qd, fm, tth, mms, ditb], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_pipeline = Pipeline([\n    ('clean', TeamAbbrCleaner()),\n    ('features', FeaturePreprocessor()),\n    ('play', PerPlayDatasetTransformer())\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_df = pd.read_csv(\"/kaggle/input/nfl-big-data-bowl-2020/train.csv\", low_memory=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_full = dataset_pipeline.fit_transform(raw_df)\nfeature_names = X_full.columns\nX_full = X_full.values\n\ny_full = TargetPreprocessor().fit_transform(raw_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Linear model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def crps_exact(y_true, y_pred):\n    \"\"\"CRPS when y_true and y_pred are given as exact values.\"\"\"\n    return np.abs(y_true - y_pred) / 199","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LinearModel(BaseEstimator):\n    def __init__(self, **lr_params):\n        self.lr = None\n        self.lr_params = lr_params\n    \n    def fit(self, X, y):\n        self.lr = LinearRegression(**self.lr_params)\n        self.lr.fit(X, y)\n        return self\n    \n    def predict(self, X):\n        return np.clip(np.round(self.lr.predict(X)), -99, 99)\n    \n    def score(self, X, y):\n        y_pred = self.predict(X)\n        # NB. Higher scores are better, but lower CRPS is better\n        return -np.mean(crps_exact(y, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import ShuffleSplit\nfrom rwfs import RandomWalkFeatureSelection\n\ncv_nfl = ShuffleSplit(n_splits=10, test_size=0.5, random_state=42)\nmodel_nfl = LinearModel()\nrwfs_nfl = RandomWalkFeatureSelection(model_nfl, cv_nfl, n_steps=1000, \n                                     initial_fraction=0.5,\n                                     temperature=1e-4, cooldown_factor=0.99,\n                                     agg=np.mean,\n                                     cache_scores=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nrwfs_nfl.fit(X_full, y_full, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\n\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diagnostics = pd.DataFrame(rwfs_nfl.diagnostics_)\nplt.plot(-diagnostics['score'], 'g')\nplt.plot(-diagnostics['score'] + diagnostics['se'], 'g--')\nplt.plot(-diagnostics['score'] - diagnostics['se'], 'g--')\nplt.plot(-diagnostics['mean_score'], 'r', alpha=0.5)\n#plt.ylim(0.0, 1.0)\nplt.title(f\"Minimum CRPS found: {rwfs_nfl.best_score_:.5f}\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is quite a lot of variance visible in the CV scores, but at least the mean score is decreasing nicely. There is an option for caching evaluations of feature sets, but by reevaluating at each step, there should be be some extra robustness.\n\nLet's look at the features found in the best-scoring feature set (not necessarily the absolutely most important, we'll look at feature importances later):"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names[list(rwfs_nfl.best_features_)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both `distance` and `defenders_in_the_box` are features I would expect to see here.  The 4th down has quite a bit of different distribution than the other downs, so that also makes sense.\n\nI'm not so sure about the formations, would probably have to check how many observations we have for those, and quarter 4 is a bit of a surprise to me.\n\nNow, it is very much possible that some less important features might show up in the best-scoring feature set; eg., a linear regressor might give some features very low coefficients whereby they wouldn't affect the score much, even if they are included.\n\nInstead, let's look at how often a given feature is included in an accepted state.  That gives us an idea of just how essential it is.\n\n`rwfs.feature_importances_` gives us this by computing the fraction of accepted stats the features are part of. By default, it isues a exponentially weighted mean giving more weight to later observations, whcih can be tweaked by the parameter `gamma` (`gamma=1.0` takes a flat average of all observations)."},{"metadata":{"trusted":true},"cell_type":"code","source":"importances = rwfs_nfl.feature_importances_\nplt.bar(feature_names, importances)\nplt.xticks(rotation='vertical')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names[rwfs_nfl.feature_importances_ > 0.8]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And wee see that `defenders_in_the_box`, `distance`, and `quarter_4` retain their status of being the most important when we pick the features with more than 0.9 feature importance.\n\nDifferent runs can of course produce different results, but truly important features should still show up if we take enough steps and tune our parameters appropriately.  The temperature is highly dependent on the scale of the scores, and you may want to decrease the cooldown for longer runs."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}