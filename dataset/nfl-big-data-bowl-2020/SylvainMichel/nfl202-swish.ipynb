{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n#Main\nfrom kaggle.competitions import nflrush\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# import the necessary packages\nfrom numpy import array\nfrom numpy import argmax\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import MinMaxScaler\n\nmaxo = 30\nmino = -5\n#intero = (maxo - mino) + 1\nintero = 1\ngepoch = 50\ngalpha=0.0005\nglayer1 = 225\nglayer2 = 125\nglayer3 = 25\n#dictionary\ngcleanedListCat = {}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class NeuralNetwork:\n\tdef __init__(self, layers, alpha=0.1):\n\t\t# initialize the list of weights matrices, then store the\n\t\t# network architecture and learning rate\n\t\tself.W = []\n\t\tself.layers = layers\n\t\tself.alpha = alpha\n\n\t\t# start looping from the index of the first layer but\n\t\t# stop before we reach the last two layers\n\t\tfor i in np.arange(0, len(layers) - 2):\n\t\t\t# randomly initialize a weight matrix connecting the\n\t\t\t# number of nodes in each respective layer together,\n\t\t\t# adding an extra node for the bias\n\t\t\tw = np.random.randn(layers[i] + 1, layers[i + 1] + 1)\n\t\t\tself.W.append(w / np.sqrt(layers[i]))\n\n\t\t# the last two layers are a special case where the input\n\t\t# connections need a bias term but the output does not\n\t\tw = np.random.randn(layers[-2] + 1, layers[-1])\n\t\tself.W.append(w / np.sqrt(layers[-2]))\n\n\tdef __repr__(self):\n\t\t# construct and return a string that represents the network\n\t\t# architecture\n\t\treturn \"NeuralNetwork: {}\".format(\n\t\t\t\"-\".join(str(l) for l in self.layers))\n\n\tdef sigmoid(self, x):\n\t\t# compute and return the sigmoid activation value for a\n\t\t# given input value\n\t\treturn 1.0 / (1 + np.exp(-x))\n\n\tdef sigmoid_deriv(self, x):\n\t\t# compute the derivative of the sigmoid function ASSUMING\n\t\t# that `x` has already been passed through the `sigmoid`\n\t\t# function\n\t\treturn x * (1 - x)\n    \n\tdef swish(self, x):\n\t\t# compute and return the sigmoid activation value for a\n\t\t# given input value\n\t\treturn x * self.sigmoid(x)\n\n\tdef swish_deriv(self, x):\n\t\treturn (np.exp(-x) * (x + 1) + 1) / (1 + np.exp(-x))**2\n\n\tdef fit(self, X, y, epochs=1000, displayUpdate=100):\n\t\t# insert a column of 1's as the last entry in the feature\n\t\t# matrix -- this little trick allows us to treat the bias\n\t\t# as a trainable parameter within the weight matrix\n\t\tX = np.c_[X, np.ones((X.shape[0]))]\n\n\t\t# loop over the desired number of epochs\n\t\tfor epoch in np.arange(0, epochs):\n\t\t\t# loop over each individual data point and train\n\t\t\t# our network on it\n\t\t\tfor (x, target) in zip(X, y):\n\t\t\t\tself.fit_partial(x, target)\n\n\t\t\t# check to see if we should display a training update\n\t\t\tif epoch == 0 or (epoch + 1) % displayUpdate == 0:\n\t\t\t\tloss = self.calculate_loss(X, y)\n\t\t\t\tprint(\"[INFO] epoch={}, loss={:.7f}\".format(\n\t\t\t\t\tepoch + 1, loss))\n\n\tdef fit_partial(self, x, y):\n\t\t# construct our list of output activations for each layer\n\t\t# as our data point flows through the network; the first\n\t\t# activation is a special case -- it's just the input\n\t\t# feature vector itself\n\t\tA = [np.atleast_2d(x)]\n\n\t\t# FEEDFORWARD:\n\t\t# loop over the layers in the network\n\t\tfor layer in np.arange(0, len(self.W)):\n\t\t\t# feedforward the activation at the current layer by\n\t\t\t# taking the dot product between the activation and\n\t\t\t# the weight matrix -- this is called the \"net input\"\n\t\t\t# to the current layer\n\t\t\tnet = A[layer].dot(self.W[layer])\n\t\t\tlastlayer = len(self.W)-1\n\t\t\tif (layer == lastlayer):\n\t\t\t\t# derniere couche\n\t\t\t\tout = self.swish(net)\n\t\t\t\t#print('out')\n\t\t\t\t#print(out)\n\t\t\t\t#print(net)                \n\t\t\telse:\n\t\t\t\t# computing the \"net output\" is simply applying our\n\t\t\t\t# non-linear activation function to the net input\n\t\t\t\tout = self.sigmoid(net)\n\t\t\t\t#print('out')\n\t\t\t\t#print(out)\n\t\t\t\t#print(net) \n\t\t\t# once we have the net output, add it to our list of\n\t\t\t# activations\n\t\t\tA.append(out)\n\n\t\t# BACKPROPAGATION\n\t\t# the first phase of backpropagation is to compute the\n\t\t# difference between our *prediction* (the final output\n\t\t# activation in the activations list) and the true target\n\t\t# value\n\t\terror = A[-1] - y\n\n\t\t#print('error')\n\t\t#print(error)\n\t\t#print(y)\n\t\t#print(A[-1])\n        \n\t\t# from here, we need to apply the chain rule and build our\n\t\t# list of deltas `D`; the first entry in the deltas is\n\t\t# simply the error of the output layer times the derivative\n\t\t# of our activation function for the output value\n\t\tD = [error * self.swish_deriv(A[-1])]\n\n\t\t# once you understand the chain rule it becomes super easy\n\t\t# to implement with a `for` loop -- simply loop over the\n\t\t# layers in reverse order (ignoring the last two since we\n\t\t# already have taken them into account)\n\t\tfor layer in np.arange(len(A) - 2, 0, -1):\n\t\t\t# the delta for the current layer is equal to the delta\n\t\t\t# of the *previous layer* dotted with the weight matrix\n\t\t\t# of the current layer, followed by multiplying the delta\n\t\t\t# by the derivative of the non-linear activation function\n\t\t\t# for the activations of the current layer\n\t\t\tdelta = D[-1].dot(self.W[layer].T)\n\t\t\tdelta = delta * self.sigmoid_deriv(A[layer])\n\t\t\tD.append(delta)\n\n\t\t# since we looped over our layers in reverse order we need to\n\t\t# reverse the deltas\n\t\tD = D[::-1]\n\n\t\t# WEIGHT UPDATE PHASE\n\t\t# loop over the layers\n\t\tfor layer in np.arange(0, len(self.W)):\n\t\t\t# update our weights by taking the dot product of the layer\n\t\t\t# activations with their respective deltas, then multiplying\n\t\t\t# this value by some small learning rate and adding to our\n\t\t\t# weight matrix -- this is where the actual \"learning\" takes\n\t\t\t# place\n\t\t\tself.W[layer] += -self.alpha * A[layer].T.dot(D[layer])\n\n\tdef predict(self, X, addBias=True):\n\t\t# initialize the output prediction as the input features -- this\n\t\t# value will be (forward) propagated through the network to\n\t\t# obtain the final prediction\n\t\tp = np.atleast_2d(X)\n\n\t\t# check to see if the bias column should be added\n\t\tif addBias:\n\t\t\t# insert a column of 1's as the last entry in the feature\n\t\t\t# matrix (bias)\n\t\t\tp = np.c_[p, np.ones((p.shape[0]))]\n\n\t\t# loop over our layers in the network\n\t\tfor layer in np.arange(0, len(self.W)):\n\t\t\t# computing the output prediction is as simple as taking\n\t\t\t# the dot product between the current activation value `p`\n\t\t\t# and the weight matrix associated with the current layer,\n\t\t\t# then passing this value through a non-linear activation\n\t\t\t# function\n\t\t\t#p = self.sigmoid(np.dot(p, self.W[layer]))\n\t\t\tlastlayer = len(self.W)-1\n\t\t\tif (layer == lastlayer):\n\t\t\t\t# derniere couche\n\t\t\t\tp = self.swish(np.dot(p, self.W[layer]))\n\t\t\telse:\n\t\t\t\t# computing the \"net output\" is simply applying our\n\t\t\t\t# non-linear activation function to the net input\n\t\t\t\tp = self.sigmoid(np.dot(p, self.W[layer]))\n\t\t# return the predicted value\n\t\treturn p\n\n\tdef calculate_loss(self, X, targets):\n\t\t# make predictions for the input data points then compute\n\t\t# the loss\n\t\ttargets = np.atleast_2d(targets)\n\t\tpredictions = self.predict(X, addBias=False)\n\t\tloss = 1/len(X) * np.sum((predictions - targets) ** 2)\n\n\t\t# return the loss\n\t\treturn loss\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def GetYardEncoded(yards, onehot_encoded):  \n    index = yards - mino\n    \n    if (index > (maxo - mino)) : \n        index = maxo - mino \n    if (index < 0) : \n        index = 0\n    \n    #for number in range(index + 1, intero, 1):\n    #    onehot_encoded[index][number] = 1\n    #print(onehot_encoded[index])\n    \n    return onehot_encoded[index]\n\ndef GetYardDecoded(yards):  \n   \n    for index, val in enumerate(yards, mino):\n        if val == 1 :\n            yardsGain = index\n            break\n \n    return yardsGain\n\ndef getY(ftrain_df):\n   \n    #even_list = list(range(mino,maxo+1))\n    #even_list = array(even_list)  \n\n    # binary encode\n    #onehot_encoder = OneHotEncoder(sparse=False, categories='auto')\n    #even_list = even_list.reshape(len(even_list), 1)\n    #onehot_encoded = onehot_encoder.fit_transform(even_list)\n\n    #Construit toutes les rÃ©ponses possibles\n    listy = list()\n    for number in ftrain_df.Yards:\n        listy.append(number)\n    y = np.array(listy)  \n    return y\n\ndef ComputeDefenderStats(ftrain_df, keepyard):\n    from sklearn.preprocessing import MinMaxScaler\n    #distance to intercept\n    \ndef ComputeOffenseStats(ftrain_df, keepyard):\n    from sklearn.preprocessing import MinMaxScaler\n    # number of blocker in front\n        \ndef FormatTrainSet(ftrain_df, keepyard, min_max_scaler, isfit, traincomplet):\n          \n    ftrain_dfOther = ftrain_df[ftrain_df['NflId'] != ftrain_df['NflIdRusher']]\n    #filter row\n    ftrain_df = ftrain_df[ftrain_df['NflId'] == ftrain_df['NflIdRusher']]\n    other = ftrain_dfOther[[ 'X', 'Y', 'S', 'A', 'Dir', 'Orientation']]\n    arrother = np.array(other)\n    arrother = np.reshape(arrother, (-1, 126))\n    ftrain_df = ftrain_df.reset_index(drop=True)\n    dfother=pd.DataFrame(arrother, columns = [str(i) for i in range(0,126)])\n    ftrain_df = pd.concat([ftrain_df, dfother], axis=1)\n    listcol = list([str(i) for i in range(0,126)])\n    list_one = [ 'X', 'Y', 'S', 'A', 'Dis', 'Dir', 'Position', 'PlayerWeight', 'HomeScoreBeforePlay', 'VisitorScoreBeforePlay', 'Team', 'Orientation', 'YardLine',  'PossessionTeam', 'Down', 'Distance', 'DefendersInTheBox', 'OffenseFormation', 'Yards']    \n    list_one_noyard = [ 'X', 'Y', 'S', 'A', 'Dis', 'Dir', 'Position', 'PlayerWeight', 'HomeScoreBeforePlay', 'VisitorScoreBeforePlay', 'Team', 'Orientation', 'YardLine',  'PossessionTeam', 'Down', 'Distance', 'DefendersInTheBox', 'OffenseFormation']\n    merged_list = list_one + listcol\n    normalise_list = [ 'X', 'Y', 'S', 'A', 'Dis', 'Dir', 'PlayerWeight', 'Orientation', 'YardLine', 'Down', 'Distance', 'DefendersInTheBox', 'HomeScoreBeforePlay', 'VisitorScoreBeforePlay' ]    \n    normalise_list = normalise_list + listcol\n    \n    #print(merged_list)\n    #print(list(ftrain_df.columns)) \n    \n    if keepyard :\n        #Keep only some column\n        ftrain_df = ftrain_df[merged_list]\n        #ftrain_df = ftrain_df[[ 'X', 'Y', 'S', 'A', 'Team', 'Orientation', 'YardLine',  'PossessionTeam', 'Down', 'Distance', 'DefendersInTheBox', 'OffenseFormation', 'Yards']] \n    else :\n        merged_list = list_one_noyard + listcol\n        ftrain_df = ftrain_df[merged_list]\n        #Keep only some column\n        #ftrain_df = ftrain_df[[ 'X', 'Y', 'S', 'A', 'Team', 'Orientation', 'YardLine',  'PossessionTeam', 'Down', 'Distance', 'DefendersInTheBox',  'OffenseFormation']]\n    \n    column_names_to_normalize = normalise_list \n    x = ftrain_df[column_names_to_normalize].values\n    if isfit:\n        x_scaled = min_max_scaler.transform(x)\n    else : \n        x_scaled = min_max_scaler.fit_transform(x)\n        \n    df_temp = pd.DataFrame(x_scaled, columns=column_names_to_normalize, index = ftrain_df.index)\n     \n    #warning here\n    ftrain_df[column_names_to_normalize] = df_temp \n    ftrain_df = ftrain_df.dropna()\n    ftrain_df = ftrain_df.reset_index(drop=True)\n    \n    if isfit:\n        x_scaled = min_max_scaler.transform(x)\n        for col in ftrain_df.select_dtypes(include=[np.object]).columns:\n            ftrain_df[col] = pd.Categorical(ftrain_df[col], categories = gcleanedListCat[col])\n    else : \n        x_scaled = min_max_scaler.fit_transform(x)  \n        for col in ftrain_df.select_dtypes(include=[np.object]).columns:\n            gcleanedListCat[col] = [x for x in traincomplet[col].unique() if x == x]\n            ftrain_df[col] = pd.Categorical(ftrain_df[col], categories = gcleanedListCat[col])\n    \n    ftrain_df = pd.get_dummies(ftrain_df)\n\n    return ftrain_df\n\ndef train_my_model(train_df, nn, y):\n    X = np.array(train_df)\n    #print(X)\n    nn.fit(X, y, gepoch, 1)\n\ndef add_row(df, row):\n    df.loc[-1] = row\n    df.index = df.index + 1  \n    return df.sort_index()\n\ndef make_my_predictionstest(test_df, sample_prediction_df, train_df2, scaler):\n    test = FormatTrainSet(test_df, False, scaler, True, train_df2)\n    print(test)\n    \n\ndef make_my_predictions(test_df, sample_prediction_df, train_df2, scaler):\n   \n    test_df = FormatTrainSet(test_df, False, scaler, True, train_df2)\n    \n    X = np.array(test_df)\n    \n    # now that our network is trained, loop over the XOR data points\n    for (x) in X:\n        # make a prediction on the data point and display the result\n        # to our console\n        pred = nn.predict(x)\n        pred = pred[0]\n        numyard = pred[0]\n\n        print('Prediction')\n        numyard = round(numyard-1.6)\n        print(numyard)\n        \n        even_list = np.zeros(199)\n        index = numyard + 99\n        \n        for idx, val in enumerate(even_list, 0):\n            if idx >= index :\n                even_list[idx] = 1\n        \n        sample_prediction_df.iloc[0] = even_list\n        #print(sample_prediction_df)\n        \n    return sample_prediction_df \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training data is in the competition dataset as usual\ntrain_df = pd.read_csv('/kaggle/input/nfl-big-data-bowl-2020/train.csv', low_memory=False)\n\n#filter row\nmin_max_scaler = MinMaxScaler()\ntrain_df = FormatTrainSet(train_df, True, min_max_scaler, False, train_df)\ny = getY(train_df)\ntrain_df = train_df.drop(columns=['Yards'])\n\nalpha=galpha\nnn = NeuralNetwork([len(train_df.columns), glayer1, glayer2, intero], alpha)\n#nn = NeuralNetwork([len(train_df.columns), glayer1, glayer2, glayer3, intero], alpha)\n#nn = NeuralNetwork([len(train_df.columns), 300, 175, 125, intero], alpha)\n#nn = NeuralNetwork([len(train_df.columns), 77, 88, intero], alpha)\ntrain_my_model(train_df, nn, y)\n\n#official\ntrain_df2 = pd.read_csv('/kaggle/input/nfl-big-data-bowl-2020/train.csv', low_memory=False)\n\nenv = nflrush.make_env()\nfor (test_df, sample_prediction_df) in env.iter_test():\n    predictions_df = make_my_predictions(test_df, sample_prediction_df,train_df2, min_max_scaler)\n    env.predict(predictions_df)\nenv.write_submission_file()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}