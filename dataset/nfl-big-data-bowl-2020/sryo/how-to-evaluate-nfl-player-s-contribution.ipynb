{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Hazard modeling of russing plays"},{"metadata":{},"cell_type":"markdown","source":"The yardage gained on the rushing play in american football distributes to a one-dimenstional distribution which depends on rushing plays information. This notebook adopts **Cox proportional hazard model** to express this distribution and shows the advantage of suvival analysis approach to infer player's contribution on each play."},{"metadata":{},"cell_type":"markdown","source":"# Introduction"},{"metadata":{},"cell_type":"markdown","source":"Survival analysis mainly focus on explaining the duration of time until some events happen. In survival analysis, our aim is to express the hazard function instead of probability density function. Let $y \\in [-99, 99]$ be obtained yardage. Since the obtained yardage takes a discrete value, our aim is to estimate the discrete hazard function\n\n$$\n\\begin{aligned}\nh(y) = \\mathrm{Pr}(y \\leq Y < y + 1 \\mid Y \\geq y),\n\\end{aligned}\n$$\n\nwhich explains the probability of rusher being stopped at $y$.\n\nIn a rushing play, defensive players try to stop a rushing player, whereas offensive players prevent defensive players from stopping the rushing player. In terms of the hazard function, defensive players increase $h(r)$ and offensive players decrease $h(r)$. \n\nOur aim in this notebook is to estimate how each player affects on this hazard. It is not sufficient to evaluate players contribution of rushing play by simple stats like obtained yards, touchdowns and tuckles. Even one running back archieved highest rushing yards, it is unclear how other offensive players contribute to the rushing yards. \nOn the other hand, using hazard enables us to compare players' performance across positions similar to WPA used in baseball."},{"metadata":{},"cell_type":"markdown","source":"# Model definition"},{"metadata":{},"cell_type":"markdown","source":"We adopt **Cox proportional model** to express the hazard function. Cox proportional model divides this hazard function into two parts as\n\n$$\n\\begin{aligned}\n    h(y) = h_0(y) \\cdot \\exp(\\phi(x, y)), \\quad y \\in [-99, 99], \\, \\text{$x$ : covariates.}\n\\end{aligned}\n$$\n\n\nThe former part indicates how the hazard function depends on time and the latter part indicates how it depends on the covariate information.  Estimating the former part in empirical manner enables us to express the complex distribution easily.\n\nHense, we define $\\phi(x, y)$ as the summation of player's contribution.\n$$\n\\begin{aligned}\n    \\phi(x, y) =  \\phi_\\mathrm{R}(x_\\mathrm{R} \\mid x) \\, s_\\mathrm{R}(y) + \\sum_{i=1}^{10} \\phi_\\mathrm{O}(x^{(i)}_\\mathrm{O} \\mid x) \\, s_\\mathrm{O}(y) + \\sum_{j=1}^{11} \\phi_\\mathrm{D}(x^{(j)}_\\mathrm{D} \\mid x) \\, s_\\mathrm{D}(y).\n\\end{aligned}\n$$\nHere, $x_\\mathrm{R}, \\{x^{(i)}_\\mathrm{O}\\}_{i=1, \\cdots, 10} \\,\\, ,  \\{x^{(j)}_\\mathrm{D}\\}_{j=1, \\cdots, 11} \\, \\, $ are player's information for rusher, offensive players and defensive players. $\\phi_\\mathrm{R}(\\cdot), \\phi_\\mathrm{O}(\\cdot), \\phi_\\mathrm{D}(\\cdot)$ are scalar functions and $s_\\mathrm{R}(\\cdot), s_\\mathrm{O}(\\cdot), s_\\mathrm{D}(\\cdot)$ are temporal coefficients.\n\nEach player's contribution to hazard  is also affected by nearby players. We express these interactions as graph expression. \n\nWe firstly obtain Delaunay diagram from player's location those who directly involve rushing play (e.g. RB, OL, DL, LB). Then, omitting some edges which may not influence on the rusher, we construct a graph indicating pairs of block players and tackle players.\nUnder this graph, we adopt **Gated Graph Neural Network** for expressing $\\phi_\\mathrm{R}(\\cdot), \\phi_\\mathrm{O}(\\cdot), \\phi_\\mathrm{D}(\\cdot)$. "},{"metadata":{"_kg_hide-input":true,"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"from IPython.display import Image\nImage(\"../input/gatedgnn/GatedGNN.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Setup"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import numpy as np\nfrom scipy.spatial import Delaunay\nimport pandas as pd\nimport tensorflow as tf\nimport os\nfrom tqdm import tqdm_notebook as tqdm\n\nimport networkx as nx\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define function for etracting features from raw dataset "},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"def extract_feature(play, is_training=True):\n    \n    playDirection, fieldPosition, possessionTeam, yardLine = play.PlayDirection.iloc[0], play.FieldPosition.iloc[0], play.PossessionTeam.iloc[0], play.YardLine.iloc[0]\n    homeTeamAbbr, visitorTeamAbbr = play.HomeTeamAbbr.iloc[0], play.VisitorTeamAbbr.iloc[0]\n    nflIdRusher = play.NflIdRusher.iloc[0]\n\n    if playDirection == 'right':\n        direction = 1\n    else:\n        direction = -1\n\n    home, away = play.Team.values == 'home', play.Team.values == 'away'\n    isRusher = play.NflId.values == nflIdRusher\n\n    if fieldPosition == possessionTeam:\n        yardToEnd = 100 - yardLine\n        start = np.array([120 + (yardLine + 10) * direction, 53.3 / 2]) % 120\n    else:\n        yardToEnd = yardLine\n        start = np.array([120 - (yardLine + 10) * direction, 53.3 / 2]) % 120\n\n    rad = np.nan_to_num(2 * np.pi * (90 - play.Dir.values) / 360)\n    X, Y = play.X.values, play.Y.values\n    S = play.Dis.values * np.logical_not(np.isnan(play.Dir.values)) / 0.1\n\n    loc = np.vstack([(X - start[0]) * direction, (Y - start[1]) * direction]).T\n    loc[:, 1] = loc[:, 1] - loc[isRusher, 1]\n    vel = (S * np.vstack([np.cos(rad), np.sin(rad)])).T * direction\n    \n    x = np.hstack([loc, vel])\n    inBox = np.array([position in ['T', 'C', 'G', 'OG', 'OT', 'RB', 'FB', 'HB', 'TE', 'DE', 'DL', 'DT', 'NT', 'ILB', 'LB', 'MLB', 'OLB'] for position in play.Position])\n\n    if possessionTeam == homeTeamAbbr:\n\n        x = np.vstack([x[isRusher], x[home * np.logical_not(isRusher)], x[away]])\n        inBox = np.hstack([inBox[isRusher], inBox[home * np.logical_not(isRusher)], inBox[away]])\n\n    elif possessionTeam == visitorTeamAbbr:\n\n        x = np.vstack([x[isRusher], x[away * np.logical_not(isRusher)], x[home]])\n        inBox = np.hstack([inBox[isRusher], inBox[away * np.logical_not(isRusher)], inBox[home]])\n      \n    inBox[0] = True\n    inBox = inBox * (x[0, 0] <= x[:, 0])\n    \n    locInBox = x[inBox, :2]\n    ind = np.arange(22)[inBox]\n    \n    try:\n        delau = Delaunay(locInBox)\n        adj = np.zeros((22, 22))\n\n        for i in range(delau.simplices.shape[0]):\n            for j in delau.simplices[i]:\n                adj[ind[j], ind[delau.simplices[i]]] = 1\n\n        adj *= 1 - np.eye(22)\n\n        loc = x[:, :2]\n        D = loc[:, np.newaxis] - loc\n        dist = np.linalg.norm(D, axis=2)\n\n        adj[:1, 11:], adj[11:, :1], adj[1:11, 1:11], adj[11:, 11:] = 0, 0, 0, 0\n        adj[1:11, 11:] = adj[1:11, 11:] * ((D[1:11, 11:] * D[1:11, :1]).sum(2) < 0)\n        adj[11:, 1:11] = adj[1:11, 11:].T\n        \n    except:\n        adj = np.zeros((22, 22))\n    \n    offset = - 99\n    threshold = np.minimum(np.floor(loc[:, 0].max()), yardToEnd) - offset\n    \n    other = np.stack([(play.Season.values == 2017).astype(np.float)]).T\n    x = np.hstack([other, x])\n\n    if is_training:\n\n        yard = play.Yards.iloc[0] - offset\n        \n        c = yard < threshold\n        y = np.minimum(yard, threshold)\n\n        return x, adj, y, c, yardToEnd\n\n    else:\n        return x, adj, yardToEnd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/nfl-big-data-bowl-2020/train.csv', low_memory=False)\n\ndata.loc[data.VisitorTeamAbbr == \"ARI\", 'VisitorTeamAbbr'] = \"ARZ\"\ndata.loc[data.HomeTeamAbbr == \"ARI\", 'HomeTeamAbbr'] = \"ARZ\"\n\ndata.loc[data.VisitorTeamAbbr == \"BAL\", 'VisitorTeamAbbr'] = \"BLT\"\ndata.loc[data.HomeTeamAbbr == \"BAL\", 'HomeTeamAbbr'] = \"BLT\"\n\ndata.loc[data.VisitorTeamAbbr == \"CLE\", 'VisitorTeamAbbr'] = \"CLV\"\ndata.loc[data.HomeTeamAbbr == \"CLE\", 'HomeTeamAbbr'] = \"CLV\"\n\ndata.loc[data.VisitorTeamAbbr == \"HOU\", 'VisitorTeamAbbr'] = \"HST\"\ndata.loc[data.HomeTeamAbbr == \"HOU\", 'HomeTeamAbbr'] = \"HST\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Extract features from training dataset"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"inds = list(data[data.Season != 2019].groupby('PlayId').groups.values())\nn = len(inds)\n\ninds_test = list(data[data.Season == 2019].groupby('PlayId').groups.values())\nn_test = len(inds_test)\n\nyards_index = np.arange(-99, 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"xs, adjs, ys, cs, hs = [], [], [], [], []\n\nfor i in tqdm(range(n)):\n\n    play = data.loc[inds[i]]\n    x, adj, y, c, yardToEnd = extract_feature(play)\n    h = (yards_index >= play['Yards'].iloc[0]).astype(np.float)\n\n    xs.append(x)\n    ys.append(y)\n    cs.append(c)\n    hs.append(h)\n    adjs.append(adj)\n\nxs, adjs, ys, cs, hs = np.stack(xs), np.stack(adjs), np.hstack(ys), np.array(cs).astype(np.int), np.vstack(hs)\n\n# Flip Y\nxs, adjs, ys, cs, hs = np.vstack([xs, xs * np.array([1, 1, -1, 1, -1])]), np.vstack([adjs, adjs]), np.hstack([ys, ys]), np.hstack([cs, cs]), np.vstack([hs, hs])\nn *= 2\n\nxs, adjs, ys, cs, hs = xs[np.argsort(ys)], adjs[np.argsort(ys)], np.sort(ys), cs[np.argsort(ys)], hs[np.argsort(ys)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"ys_unique, ys_index, ys_inverse, ys_count = np.unique(ys, return_index=True, return_inverse=True, return_counts=True)\nys_mask = tf.constant(np.arange(199)[:, np.newaxis] == ys_unique, dtype=tf.float32)\n\ncs_count = []\nfor j in ys_unique:\n    cs_count.append(cs[ys == j].sum())\n\ncs_count = np.array(cs_count)\n\ncs_mask = np.zeros((n, ys_unique.shape[0]))\nfor j, index, count in zip(range(ys_unique.shape[0]), ys_index, ys_count):\n    cs_mask[index:index+count, j] = 1.\ncs_mask = tf.constant(cs_mask, dtype=tf.float32)\n\nmask = tf.constant(ys_index <= np.arange(n)[:, np.newaxis], dtype=tf.float32)\ninf_array = - tf.ones_like(mask, dtype=tf.float32) * np.inf\n\nys_succ_ind = [j in ys_unique for j in np.arange(ys_unique.min(), ys_unique.max()+1)]\nys_succ_ind = np.arange(len(ys_succ_ind))[ys_succ_ind]\nm = np.int(ys_unique[-1] - ys_unique[0] + 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"xs_test, adjs_test, ys_test, cs_test, hs_test = [], [], [], [], []\nyardToEnds_test = []\n\nfor i in tqdm(range(n_test)):\n\n    play = data.loc[inds_test[i]]\n    x, adj, y, c, yardToEnd = extract_feature(play)\n    h = (yards_index >= play['Yards'].iloc[0]).astype(np.float)\n\n    xs_test.append(x)\n    ys_test.append(y)\n    cs_test.append(c)\n    hs_test.append(h)\n    adjs_test.append(adj)\n    \n    yardToEnds_test.append(yardToEnd)\n\nxs_test, adjs_test, ys_test, cs_test, hs_test = np.stack(xs_test), np.stack(adjs_test), np.hstack(ys_test), np.array(cs_test).astype(np.int), np.vstack(hs_test)\nyardToEnds_test = np.hstack([yardToEnds_test])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define Cox proportional model"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"class GGNN(tf.keras.Model):\n\n    def __init__(self):\n\n        super(GGNN, self).__init__()\n\n        self.gru_L = 3\n\n        self.denseGRU_R, self.denseGRU_O, self.denseGRU_D = tf.keras.layers.Dense(n_layerGRU, activation=tf.nn.tanh), tf.keras.layers.Dense(n_layerGRU, activation=tf.nn.tanh), tf.keras.layers.Dense(n_layerGRU, activation=tf.nn.tanh)\n\n        self.update_R, self.update_O, self.update_D = tf.keras.layers.Dense(n_layerGRU, activation=tf.nn.sigmoid, use_bias=False), tf.keras.layers.Dense(n_layerGRU, activation=tf.nn.sigmoid, use_bias=False), tf.keras.layers.Dense(n_layerGRU, activation=tf.nn.sigmoid, use_bias=False)\n        self.reset_R, self.reset_O, self.reset_D = tf.keras.layers.Dense(n_layerGRU, activation=tf.nn.sigmoid, use_bias=False), tf.keras.layers.Dense(n_layerGRU, activation=tf.nn.sigmoid, use_bias=False), tf.keras.layers.Dense(n_layerGRU, activation=tf.nn.sigmoid, use_bias=False)\n        self.modify_R, self.modify_O, self.modify_D = tf.keras.layers.Dense(n_layerGRU, activation=tf.nn.tanh, use_bias=True), tf.keras.layers.Dense(n_layerGRU, activation=tf.nn.tanh, use_bias=True), tf.keras.layers.Dense(n_layerGRU, activation=tf.nn.tanh, use_bias=True)\n        \n        self.dropoutGRU_R, self.dropoutGRU_O, self.dropoutGRU_D = tf.keras.layers.Dropout(dropout_rate), tf.keras.layers.Dropout(dropout_rate), tf.keras.layers.Dropout(dropout_rate)\n        self.dropoutGRU_neighbor_R, self.dropoutGRU_neighbor_O, self.dropoutGRU_neighbor_D = tf.keras.layers.Dropout(dropout_rate), tf.keras.layers.Dropout(dropout_rate), tf.keras.layers.Dropout(dropout_rate)\n        \n        self.dense1_R, self.dense1_O, self.dense1_D = tf.keras.layers.Dense(n_layer1, activation=tf.nn.tanh), tf.keras.layers.Dense(n_layer1, activation=tf.nn.tanh), tf.keras.layers.Dense(n_layer1, activation=tf.nn.tanh)\n        self.dropout1_R, self.dropout1_O, self.dropout1_D = tf.keras.layers.Dropout(dropout_rate), tf.keras.layers.Dropout(dropout_rate), tf.keras.layers.Dropout(dropout_rate)\n        \n        self.dense2_R, self.dense2_O, self.dense2_D = tf.keras.layers.Dense(1, use_bias=False), tf.keras.layers.Dense(1, use_bias=False), tf.keras.layers.Dense(1, use_bias=False)\n      \n        self.temporal_R, self.temporal_O, self.temporal_D = tf.Variable(tf.zeros(m)), tf.Variable(tf.zeros(m)), tf.Variable(tf.zeros(m))\n        self.forward_R, self.forward_O, self.forward_D = tf.keras.layers.GRU(1, return_sequences=True), tf.keras.layers.GRU(1, return_sequences=True), tf.keras.layers.GRU(1, return_sequences=True)\n        self.backward_R, self.backward_O, self.backward_D = tf.keras.layers.GRU(1, return_sequences=True, go_backwards=True), tf.keras.layers.GRU(1, return_sequences=True, go_backwards=True), tf.keras.layers.GRU(1, return_sequences=True, go_backwards=True)\n        self.output_R, self.output_O, self.output_D = tf.keras.layers.Dense(1, use_bias=False), tf.keras.layers.Dense(1, use_bias=False), tf.keras.layers.Dense(1, use_bias=False)\n        \n    @tf.function  \n    def call(self, X, A, is_training=False):\n        \n        X_R = tf.slice(X, [0, 0, 0], [-1, 1, -1])\n        X_O = tf.concat([tf.tile(X_R, (1, 10, 1)), tf.slice(X, [0, 1, n_down_info], [-1, 10, -1])], axis=2)\n        X_D = tf.concat([tf.tile(X_R, (1, 11, 1)), tf.slice(X, [0, 11, n_down_info], [-1, 11, -1])], axis=2)\n        other = tf.squeeze(tf.slice(X, [0, 0, 0], [-1, 1, n_down_info]), axis=1)\n\n        A_R = tf.slice(A, [0, 0, 1], [-1, 1, -1])\n        A_O = tf.concat([tf.slice(A, [0, 1, 0], [-1, 10, 1]), tf.slice(A, [0, 1, 11], [-1, 10, -1])], 2)\n        A_D = tf.slice(A, [0, 11, 0], [-1, -1, -1])\n        \n        layerGRU_R, layerGRU_O, layerGRU_D = self.denseGRU_R(X_R), self.denseGRU_O(X_O), self.denseGRU_D(X_D)\n        \n        maskGRU_update_R, maskGRU_update_O, maskGRU_update_D = self.dropoutGRU_R(tf.ones_like(layerGRU_R), is_training), self.dropoutGRU_O(tf.ones_like(layerGRU_O), is_training), self.dropoutGRU_D(tf.ones_like(layerGRU_D), is_training)\n        maskGRU_reset_R, maskGRU_reset_O, maskGRU_reset_D = self.dropoutGRU_R(tf.ones_like(layerGRU_R), is_training), self.dropoutGRU_O(tf.ones_like(layerGRU_O), is_training), self.dropoutGRU_D(tf.ones_like(layerGRU_D), is_training)\n        maskGRU_modify_R, maskGRU_modify_O, maskGRU_modify_D = self.dropoutGRU_R(tf.ones_like(layerGRU_R), is_training), self.dropoutGRU_O(tf.ones_like(layerGRU_O), is_training), self.dropoutGRU_D(tf.ones_like(layerGRU_D), is_training)\n        \n        maskGRU_update_neighbor_R, maskGRU_update_neighbor_O, maskGRU_update_neighbor_D = self.dropoutGRU_neighbor_R(tf.ones(tf.shape(layerGRU_R)), is_training), self.dropoutGRU_neighbor_O(tf.ones(tf.shape(layerGRU_O) * (1, 1, 2)), is_training), self.dropoutGRU_neighbor_D(tf.ones(tf.shape(layerGRU_D)), is_training)\n        maskGRU_reset_neighbor_R, maskGRU_reset_neighbor_O, maskGRU_reset_neighbor_D = self.dropoutGRU_neighbor_R(tf.ones(tf.shape(layerGRU_R)), is_training), self.dropoutGRU_neighbor_O(tf.ones(tf.shape(layerGRU_O) * (1, 1, 2)), is_training), self.dropoutGRU_neighbor_D(tf.ones(tf.shape(layerGRU_D)), is_training)\n        maskGRU_modify_neighbor_R, maskGRU_modify_neighbor_O, maskGRU_modify_neighbor_D = self.dropoutGRU_neighbor_R(tf.ones(tf.shape(layerGRU_R)), is_training), self.dropoutGRU_neighbor_O(tf.ones(tf.shape(layerGRU_O) * (1, 1, 2)), is_training), self.dropoutGRU_neighbor_D(tf.ones(tf.shape(layerGRU_D)), is_training)\n\n        for l in range(self.gru_L):\n\n            layerGRU_neighbor_R = tf.matmul(tf.slice(A_R, [0, 0, 0], [-1, -1, 10]), layerGRU_O)\n            layerGRU_neighbor_O = tf.concat([tf.matmul(tf.slice(A_O, [0, 0, 0], [-1, -1, 1]), layerGRU_R), tf.matmul(tf.slice(A_O, [0, 0, 1], [-1, -1, -1]), layerGRU_D)], 2)\n            layerGRU_neighbor_D = tf.matmul(tf.slice(A_D, [0, 0, 1], [-1, -1, 10]), layerGRU_O)\n            \n            z_R = self.update_R(tf.concat([layerGRU_R * maskGRU_update_R, layerGRU_neighbor_R * maskGRU_update_neighbor_R], 2))\n            r_R = self.reset_R(tf.concat([layerGRU_R * maskGRU_reset_R, layerGRU_neighbor_R * maskGRU_reset_neighbor_R], 2))\n            layerGRU_modified_R = self.modify_R(tf.concat([layerGRU_R * r_R * maskGRU_modify_R, layerGRU_neighbor_R * maskGRU_modify_neighbor_R], 2))\n\n            z_O = self.update_O(tf.concat([layerGRU_O * maskGRU_update_O, layerGRU_neighbor_O * maskGRU_update_neighbor_O], 2))\n            r_O = self.reset_O(tf.concat([layerGRU_O * maskGRU_reset_O, layerGRU_neighbor_O * maskGRU_reset_neighbor_O], 2))\n            layerGRU_modified_O = self.modify_O(tf.concat([layerGRU_O * r_O * maskGRU_modify_O, layerGRU_neighbor_O * maskGRU_modify_neighbor_O], 2))\n\n            z_D = self.update_D(tf.concat([layerGRU_D * maskGRU_update_D, layerGRU_neighbor_D * maskGRU_update_neighbor_D], 2))\n            r_D = self.reset_D(tf.concat([layerGRU_D * maskGRU_reset_D, layerGRU_neighbor_D * maskGRU_reset_neighbor_D], 2))\n            layerGRU_modified_D = self.modify_D(tf.concat([layerGRU_D * r_D * maskGRU_modify_D, layerGRU_neighbor_D * maskGRU_modify_neighbor_D], 2))\n\n            layerGRU_R = (1. - z_R) * layerGRU_R + z_R * layerGRU_modified_R\n            layerGRU_O = (1. - z_O) * layerGRU_O + z_O * layerGRU_modified_O\n            layerGRU_D = (1. - z_D) * layerGRU_D + z_D * layerGRU_modified_D\n            \n        layer1_R, layer1_O, layer1_D = self.dense1_R(layerGRU_R), self.dense1_O(layerGRU_O), self.dense1_D(layerGRU_D)\n        layer1_R, layer1_O, layer1_D = self.dropout1_R(layer1_R), self.dropout1_O(layer1_O), self.dropout1_D(layer1_D)\n        \n        layer2_R, layer2_O, layer2_D = self.dense2_R(layer1_R), self.dense2_O(layer1_O), self.dense2_D(layer1_D)\n        \n        temporal_R = tf.transpose(tf.gather_nd(tf.nn.softmax(tf.transpose(tf.squeeze(self.output_R(self.backward_R(self.forward_R(self.temporal_R[tf.newaxis, :, tf.newaxis]))), 2)), 0), ys_succ_ind[np.newaxis].T))\n        temporal_O = tf.transpose(tf.gather_nd(tf.nn.softmax(tf.transpose(tf.squeeze(self.output_O(self.backward_O(self.forward_O(self.temporal_O[tf.newaxis, :, tf.newaxis]))), 2)), 0), ys_succ_ind[np.newaxis].T))\n        temporal_D = tf.transpose(tf.gather_nd(tf.nn.softmax(tf.transpose(tf.squeeze(self.output_D(self.backward_D(self.forward_D(self.temporal_D[tf.newaxis, :, tf.newaxis]))), 2)), 0), ys_succ_ind[np.newaxis].T))\n        \n        out = tf.squeeze(layer2_R, 1) * temporal_R + tf.reduce_sum(layer2_O, 1) * temporal_O + tf.reduce_sum(layer2_D, 1) * temporal_D\n        \n        return out\n    \n    def call_players(self, X, A):\n        \n        X_R = tf.slice(X, [0, 0, 0], [-1, 1, -1])\n        X_O = tf.concat([tf.tile(X_R, (1, 10, 1)), tf.slice(X, [0, 1, n_down_info], [-1, 10, -1])], axis=2)\n        X_D = tf.concat([tf.tile(X_R, (1, 11, 1)), tf.slice(X, [0, 11, n_down_info], [-1, 11, -1])], axis=2)\n        other = tf.squeeze(tf.slice(X, [0, 0, 0], [-1, 1, n_down_info]), axis=1)\n\n        A_R = tf.slice(A, [0, 0, 1], [-1, 1, -1])\n        A_O = tf.concat([tf.slice(A, [0, 1, 0], [-1, 10, 1]), tf.slice(A, [0, 1, 11], [-1, 10, -1])], 2)\n        A_D = tf.slice(A, [0, 11, 0], [-1, -1, -1])\n        \n        layerGRU_R, layerGRU_O, layerGRU_D = self.denseGRU_R(X_R), self.denseGRU_O(X_O), self.denseGRU_D(X_D)\n        \n        for l in range(self.gru_L):\n\n            layerGRU_neighbor_R = tf.matmul(tf.slice(A_R, [0, 0, 0], [-1, -1, 10]), layerGRU_O)\n            layerGRU_neighbor_O = tf.concat([tf.matmul(tf.slice(A_O, [0, 0, 0], [-1, -1, 1]), layerGRU_R), tf.matmul(tf.slice(A_O, [0, 0, 1], [-1, -1, -1]), layerGRU_D)], 2)\n            layerGRU_neighbor_D = tf.matmul(tf.slice(A_D, [0, 0, 1], [-1, -1, 10]), layerGRU_O)\n            \n            z_R = self.update_R(tf.concat([layerGRU_R, layerGRU_neighbor_R], 2))\n            r_R = self.reset_R(tf.concat([layerGRU_R, layerGRU_neighbor_R], 2))\n            layerGRU_modified_R = self.modify_R(tf.concat([layerGRU_R * r_R, layerGRU_neighbor_R], 2))\n\n            z_O = self.update_O(tf.concat([layerGRU_O, layerGRU_neighbor_O], 2))\n            r_O = self.reset_O(tf.concat([layerGRU_O, layerGRU_neighbor_O], 2))\n            layerGRU_modified_O = self.modify_O(tf.concat([layerGRU_O * r_O, layerGRU_neighbor_O], 2))\n\n            z_D = self.update_D(tf.concat([layerGRU_D, layerGRU_neighbor_D], 2))\n            r_D = self.reset_D(tf.concat([layerGRU_D, layerGRU_neighbor_D], 2))\n            layerGRU_modified_D = self.modify_D(tf.concat([layerGRU_D * r_D, layerGRU_neighbor_D], 2))\n\n            layerGRU_R = (1. - z_R) * layerGRU_R + z_R * layerGRU_modified_R\n            layerGRU_O = (1. - z_O) * layerGRU_O + z_O * layerGRU_modified_O\n            layerGRU_D = (1. - z_D) * layerGRU_D + z_D * layerGRU_modified_D\n            \n        layer1_R, layer1_O, layer1_D = self.dense1_R(layerGRU_R), self.dense1_O(layerGRU_O), self.dense1_D(layerGRU_D)\n        layer2_R, layer2_O, layer2_D = self.dense2_R(layer1_R), self.dense2_O(layer1_O), self.dense2_D(layer1_D)\n        \n        temporal_R = tf.nn.softmax(tf.squeeze(self.output_R(self.backward_R(self.forward_R(self.temporal_R[tf.newaxis, :, tf.newaxis]))), 2), 1)\n        temporal_O = tf.nn.softmax(tf.squeeze(self.output_O(self.backward_O(self.forward_O(self.temporal_O[tf.newaxis, :, tf.newaxis]))), 2), 1)\n        temporal_D = tf.nn.softmax(tf.squeeze(self.output_D(self.backward_D(self.forward_D(self.temporal_D[tf.newaxis, :, tf.newaxis]))), 2), 1)\n        \n        return tf.concat([layer2_R * temporal_R, layer2_O * temporal_O, layer2_D * temporal_D], axis=1)\n    \n@tf.function\ndef compute_cost(model, X, A):\n\n    out = model.call(X, A, True)\n    \n    out_max = tf.reduce_max(tf.where(tf.cast(mask, tf.bool), out, inf_array), 0)    \n    exp_sum = tf.reduce_sum(tf.exp(out - out_max) * mask, 0)\n    den = (out_max + tf.math.log(exp_sum)) * cs_count\n\n    cost = - tf.reduce_sum(tf.reduce_sum(out * cs_mask, 1) * cs) + tf.reduce_sum(den)\n    \n    return cost\n\n@tf.function\ndef compute_gradients(model, X, A):\n\n    with tf.GradientTape() as tape:\n        cost = compute_cost(model, X, A)\n\n    return tape.gradient(cost, model.trainable_variables), cost\n\n@tf.function\ndef apply_gradients(optimizer, gradients, variables):\n\n    optimizer.apply_gradients(zip(gradients, variables))\n\n\ndef compute_baseline_hazard(model, X, A):\n\n    out = model.call(X, A, False)\n\n    out_max = tf.reduce_max(tf.where(tf.cast(mask, tf.bool), out, inf_array), 0)    \n    exp_sum = tf.reduce_sum(tf.exp(out - out_max) * mask, 0)\n    den = (out_max + tf.math.log(exp_sum)) * cs_count\n\n    baseline_hazard = np.sum(ys_mask * tf.exp(- out_max) * exp_sum.numpy() ** -1 * cs_count, axis=1)\n\n    return baseline_hazard\n\n\ndef compute_hazard_ratio(model, X, A):\n\n    out = model.call(X, A, False)\n    hazard_ratio = np.dot(tf.exp(out).numpy(), tf.transpose(ys_mask))\n\n    return hazard_ratio","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Estimate Cox proportional model"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"X, A = tf.constant(xs, dtype=tf.float32), tf.constant(adjs, dtype=tf.float32)\nX_test, A_test = tf.constant(xs_test, dtype=tf.float32), tf.constant(adjs_test, dtype=tf.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"learning_rate = 0.01\ndropout_rate = 0.5\nn_layerGRU = 64\nn_layer1 = 16\nn_down_info, n_player_info = 1, 4\nn_ties = ys_unique.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"model = GGNN()\noptimizer = tf.keras.optimizers.Adam(learning_rate)\n\ntraning_epochs = 500\nbest_CRPS = np.inf\n\nfor epoch in range(traning_epochs):\n    \n    gradients, cost_epoch = compute_gradients(model, X, A)\n    apply_gradients(optimizer, gradients, model.variables)\n\n    if epoch % 50 == 0:\n\n        print('epoch ' + str(epoch) + ': ' + str(cost_epoch.numpy()))\n\n        baseline_hazard = compute_baseline_hazard(model, X, A)\n        hazard_ratio = compute_hazard_ratio(model, X_test, A_test)\n\n        preds = 1 - np.exp(- np.cumsum(baseline_hazard * hazard_ratio, 1))\n        preds[yards_index > yardToEnds_test[:, np.newaxis]] = 0.\n        preds = preds / preds.max(1)[:, np.newaxis]\n        preds[yards_index >= yardToEnds_test[:, np.newaxis]] = 1.\n        CRPS = np.square(hs_test - preds).mean()\n\n        print('test CRPS: ' + str(CRPS))\n        \n        if CRPS <= best_CRPS:\n            model.save_weights('best_model')\n            best_CRPS = CRPS\n        \nprint('------------------------')\nmodel.load_weights('best_model')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Player's contribution on each play"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"def draw_play(play):\n\n    x, adj,  yardToEnd = extract_feature(play, False)\n    out = model.call_players(tf.constant(x[np.newaxis], dtype=tf.float32), tf.constant(adj[np.newaxis], dtype=tf.float32))\n    scale_R, scale_O, scale_D = temporal_R[0], temporal_O[0], temporal_D[0]\n    \n    playDirection, fieldPosition, possessionTeam, yardLine = play.PlayDirection.iloc[0], play.FieldPosition.iloc[0], play.PossessionTeam.iloc[0], play.YardLine.iloc[0]\n    homeTeamAbbr, visitorTeamAbbr = play.HomeTeamAbbr.iloc[0], play.VisitorTeamAbbr.iloc[0]\n    nflIdRusher = play.NflIdRusher.iloc[0]\n\n    home, away = play.Team.values == 'home', play.Team.values == 'away'\n    isRusher = play.NflId.values == nflIdRusher\n\n    position = play.Position.values\n\n    if possessionTeam == homeTeamAbbr:\n        position = np.hstack([position[isRusher], position[home * np.logical_not(isRusher)], position[away]])\n\n    elif possessionTeam == visitorTeamAbbr:\n        position = np.hstack([position[isRusher], position[away * np.logical_not(isRusher)], position[home]])\n\n    loc, vel = x[:, n_down_info:n_down_info+2], x[:, n_down_info+2:n_down_info+4]\n\n    G = nx.Graph()\n\n    G.add_nodes_from(np.arange(11), bipartite=0)\n    G.add_nodes_from(np.arange(11, 22), bipartite=1)\n    node_color = ['r']\n    node_color.extend(['b' for i in range(10)])\n    node_color.extend(['g' for i in range(11)])\n\n    row, col = np.where(adj != 0)\n    G.add_edges_from(zip(row, col))\n\n    plt.figure(figsize=(18, 18))\n\n    nx.draw_networkx_nodes(G, loc, node_color=node_color, node_size=1000, alpha=1.)\n    nx.draw_networkx_edges(G, loc, alpha=0.5, style='dashed', edge_color='k')\n    nx.draw_networkx_labels(G, loc, {i: position[i] for i in range(22)}, font_weight='bold', font_color='white')\n\n    for i in range(22):\n        plt.arrow(loc[i, 0], loc[i, 1], vel[i, 0] / 2. + 0.01, vel[i, 1] / 2. + 0.01, width=0.01,head_width=0.1,head_length=0.1,length_includes_head=True, color='k', alpha=0.4)\n\n    for i in range(22):\n        if (-8 < loc[i, 0] < 8) & (-8 < loc[i, 1] < 8):\n            if i == 0:\n                score = (out[0, i].numpy().sum() - out_players_mean[i]) * scale_R\n            elif i > 10:\n                score = (out[0, i].numpy().sum() - out_players_mean[i]) * scale_D\n            else:\n                score = (out[0, i].numpy().sum() - out_players_mean[i]) * scale_O\n                \n            plt.text(loc[i, 0]+0.3, loc[i, 1]+0.2, np.around(np.exp(score), 2))\n            \n    plt.text(-7.5, 7, \"Offense:\", fontsize=30)\n    score_R = (tf.reduce_sum(out[:, :1]) - tf.reduce_sum(out_players_mean[:1])).numpy() * scale_R\n    score_O = (tf.reduce_sum(out[:, 1:11]) - tf.reduce_sum(out_players_mean[1:11])).numpy() * scale_O\n    plt.text(-5, 7, np.around(np.exp(score_R + score_O), 2), fontsize=30)\n\n    plt.text(-7.5, 6, \"Defense:\", fontsize=30)\n    score_D = (tf.reduce_sum(out[:, 11:]) - tf.reduce_sum(out_players_mean[11:])).numpy() * scale_D\n    plt.text(-5, 6, np.around(np.exp(score_D), 2), fontsize=30)\n\n    plt.vlines(0, -8, 8, linestyle='solid', alpha=0.2)\n    plt.vlines(play.Down.iloc[0], -8, 8, color='goldenrod', linestyle='solid', alpha=0.5)\n\n    plt.xlim(-8, 8)\n    plt.ylim(-8, 8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"out_players = model.call_players(X_test, A_test)\n\nout_rusher_mean = tf.reduce_mean(tf.reduce_sum(out_players[:, :1], 2, keepdims=True))\nout_offense_mean = tf.reduce_mean(tf.reduce_sum(out_players[:, 1:11], 2, keepdims=True))\nout_defense_mean = tf.reduce_mean(tf.reduce_sum(out_players[:, 11:22], 2, keepdims=True))\nout_players_mean = tf.concat([out_rusher_mean * tf.ones(1), out_offense_mean * tf.ones(10), out_defense_mean * tf.ones(11)], axis=0)\n\ntemporal_R = pd.Series((out_players[0, 0] / tf.reduce_sum(out_players[0, 0])).numpy(), index=np.arange(ys_unique[0]-99, ys_unique[-1]-98))\ntemporal_O = pd.Series((out_players[0, 1] / tf.reduce_sum(out_players[0, 1])).numpy(), index=np.arange(ys_unique[0]-99, ys_unique[-1]-98))\ntemporal_D = pd.Series((out_players[0, 11] / tf.reduce_sum(out_players[0, 11])).numpy(), index=np.arange(ys_unique[0]-99, ys_unique[-1]-98))\n\nscale_R, scale_O, scale_D = temporal_R[0], temporal_O[0], temporal_D[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Each value indicates hazard ratio at 0 yard relative to mean hazard of 2019 Season. \nFor example, if a defensive player have 1.5, then the hazard at 0 yard on this play is 1.5 times larger than the mean hazard due to this player."},{"metadata":{"trusted":true},"cell_type":"code","source":"l = 75\nplay = data.loc[inds[l]]\nprint('Gain yard: ' + str(play.Yards.iloc[0]))\ndraw_play(play)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Player's contribution in 2019 Season"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"score = (tf.reduce_sum(out_players, 2) - out_players_mean).numpy() \nscale = np.hstack([scale_R, scale_O * np.ones(10), scale_D * np.ones(11)])\nscore *= scale","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"positions, names = [], []\noffense_teams, defense_teams = [], []\n\nfor l in tqdm(range(len(inds_test))):\n    \n    play = data.loc[inds_test[l]]\n\n    possessionTeam = play.PossessionTeam.iloc[0]\n    homeTeamAbbr, visitorTeamAbbr = play.HomeTeamAbbr.iloc[0], play.VisitorTeamAbbr.iloc[0]\n    nflIdRusher = play.NflIdRusher.iloc[0]\n\n    home, away = play.Team.values == 'home', play.Team.values == 'away'\n    isRusher = play.NflId.values == nflIdRusher\n\n    position, displayName = play.Position.values, play.DisplayName.values\n\n    if possessionTeam == homeTeamAbbr:\n        position = np.hstack([position[isRusher], position[home * np.logical_not(isRusher)], position[away]])\n        displayName = np.hstack([displayName[isRusher], displayName[home * np.logical_not(isRusher)], displayName[away]])\n        \n    elif possessionTeam == visitorTeamAbbr:\n        position = np.hstack([position[isRusher], position[away * np.logical_not(isRusher)], position[home]])\n        displayName = np.hstack([displayName[isRusher], displayName[away * np.logical_not(isRusher)], displayName[home]])\n    \n    position_replaced = []\n\n    for pos in position:\n\n        if pos in ['QB']:\n            position_replaced.append('QB')\n        elif pos in  ['C', 'T', 'OT', 'G', 'OG', 'TE']:\n            position_replaced.append('OL')\n        elif pos in  ['RB', 'HB', 'FB']:\n            position_replaced.append('RB')\n        elif pos in  ['WR']:\n            position_replaced.append('WR')\n\n        elif pos in ['DE', 'DT', 'DL', 'NT']:\n            position_replaced.append('DL')\n        elif pos in ['LB', 'ILB', 'OLB', 'MLB']:\n            position_replaced.append('LB')\n        elif pos in ['DB', 'CB', 'FS', 'SS', 'S', 'SAF']:\n            position_replaced.append('DB')\n    \n    position_replaced = np.array(position_replaced)\n            \n    names.append(displayName)\n    positions.append(position_replaced)\n    \n    if possessionTeam == homeTeamAbbr:\n        offense_teams.append(homeTeamAbbr)\n        defense_teams.append(visitorTeamAbbr)\n    else:\n        offense_teams.append(visitorTeamAbbr)\n        defense_teams.append(homeTeamAbbr)\n        \n    \nnames = np.vstack(names)\npositions = np.vstack(positions)\noffense_teams = np.hstack(offense_teams)\ndefense_teams = np.hstack(defense_teams)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"def calculate_score_players(position):\n    \n    threshold = 100\n    \n    if position in ['RB', 'QB', 'WR']:\n        ind = np.arange(1)\n    \n    elif position == 'FB':\n        ind = np.arange(1, 11)\n        position = 'RB'\n        \n    elif position in ['OL']:\n        ind = np.arange(1, 11)\n        \n    elif position in ['DL', 'LB', 'DB'] :\n        ind = np.arange(11, 22)\n         \n    players = list(set(names[:, ind][positions[:, ind] == position].flatten().tolist()))\n\n    score_players = {}\n    for player in players:\n        if (names[:, ind] == player).sum() > threshold:\n            score_players[player] = score[:, ind][names[:, ind] == player].mean()\n\n    score_players = pd.Series(score_players)\n    score_players = score_players.sort_values()\n    \n    return score_players\n\n\ndef calculate_score_teams():\n    \n    teams = list(set(offense_teams))\n\n    score_offense = {}\n    score_defense = {}\n    \n    for team in teams:\n        score_offense[team] = score[offense_teams == team].mean()\n        score_defense[team] = score[defense_teams == team].mean()\n\n    score_offense = pd.Series(score_offense)\n    score_offense = score_offense.sort_values()\n    \n    score_defense = pd.Series(score_defense)\n    score_defense = score_defense.sort_values()\n    \n    return score_offense, score_defense","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"score_offense, score_defense = calculate_score_teams()\nscore_OL = calculate_score_players('OL')\nscore_LB = calculate_score_players('LB')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below score is the mean value of log hazard ratio of defensive players at 0 yard in 2019 season.\nLarger value indicates the better rushing defense."},{"metadata":{"trusted":true},"cell_type":"code","source":"score_defense[::-1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below score is the mean value of log hazard ratio of offensive players at 0 yard in 2019 season.\nSmaller value indicates the better rushing offense."},{"metadata":{"trusted":true},"cell_type":"code","source":"score_offense","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below score is the mean value of log hazard ratio of offense line players at 0 yard in 2019 season.\nSmaller value indicates the better rushing offense."},{"metadata":{"trusted":true},"cell_type":"code","source":"score_OL.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below score is the mean value of log hazard ratio of linebackers at 0 yard in 2019 season.\nLarger value indicates the better rushing defense."},{"metadata":{"trusted":true},"cell_type":"code","source":"score_LB.tail(20)[::-1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that these values only indicate the goodness of offensive and defensive formation at handoff.\nHense, these values do not reflect the ability of players, such as rushing speed of RB, tackle rate of LB and so on. "},{"metadata":{},"cell_type":"markdown","source":"# Predict and submit score"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom kaggle.competitions import nflrush\nenv = nflrush.make_env()\niter_test = env.iter_test()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"baseline_hazard = compute_baseline_hazard(model, X, A)\n\nfor (play, prediction_df) in tqdm(iter_test):\n        \n    play.loc[play.HomeTeamAbbr.values == \"ARI\", 'HomeTeamAbbr'] = \"ARZ\"\n    play.loc[play.HomeTeamAbbr.values == \"BAL\", 'HomeTeamAbbr'] = \"BLT\"\n    play.loc[play.HomeTeamAbbr.values == \"CLE\", 'HomeTeamAbbr'] = \"CLV\"\n    play.loc[play.HomeTeamAbbr.values == \"HOU\", 'HomeTeamAbbr'] = \"HST\"\n    \n    x, adj, yardToEnd = extract_feature(play, False)\n    x, adj = tf.constant(x[np.newaxis], dtype=tf.float32), tf.constant(adj[np.newaxis], dtype=tf.float32)\n\n    hazard_ratio = compute_hazard_ratio(model, x, adj)\n\n    pred = 1 - np.exp(- np.cumsum(baseline_hazard * hazard_ratio))\n    pred[yards_index > yardToEnd] = 0.\n    pred = pred / pred.max()\n    pred[yards_index >= yardToEnd] = 1.\n    \n    prediction_df = pd.DataFrame(pred[np.newaxis], columns=prediction_df.columns)\n    env.predict(prediction_df)\n\nenv.write_submission_file()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}