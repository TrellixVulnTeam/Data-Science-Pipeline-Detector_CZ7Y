{"cells":[{"metadata":{},"cell_type":"markdown","source":"\n\nA previous version of this code got a public LB of 0.01299 and is described in https://www.kaggle.com/c/nfl-big-data-bowl-2020/discussion/119430\n\nSince competition end I have implemented few improvements, and in particular all piece of the transformer architecture.  The code uses keras functional model api, which makes it way ore compact than the transformer implementations one can find online.  \n\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F75976%2Fe50d024a978c324b67182a554a2b66bd%2Ftransformer.png?generation=1575305725966574&amp;alt=media)\n\nI also included some data cleaning that was shared by top teams, namely using `S = 10 * Dis`, and averaging `A` in 2017.  The CV is improved by about 0.00025, which could bring LB around 0.01275, not top, but still of interest for a model without any feature engineering besides distance matrix computation."},{"metadata":{},"cell_type":"markdown","source":"First, let's import useful packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib as mpl\nimport datetime\n#from kaggle.competitions import nflrush\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import StandardScaler\nimport os\nimport re\nimport keras\nfrom sklearn.model_selection import KFold, RepeatedKFold\nfrom sklearn.isotonic import IsotonicRegression\ntqdm.pandas()\n\nfrom numba import jit","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from kaggle.competitions import nflrush","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following code can be used to decrease non determinism by setting a number of random seeds.  It also creates a new Tensorflow session."},{"metadata":{"trusted":true},"cell_type":"code","source":"import random as rn\nimport tensorflow as tf\nimport numpy as np\nfrom keras import backend as K\n\ndef init_seeds(seed):\n\n    # The below is necessary for starting Numpy generated random numbers\n    # in a well-defined initial state.\n\n    np.random.seed(seed)\n\n    # The below is necessary for starting core Python generated random numbers\n    # in a well-defined state.\n\n    rn.seed(seed)\n\n    # The below tf.set_random_seed() will make random number generation\n    # in the TensorFlow backend have a well-defined initial state.\n    # For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n\n    tf.set_random_seed(seed)\n\n    sess = tf.Session(graph=tf.get_default_graph())\n    K.set_session(sess)\n    return sess","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Loading training data.[](http://)"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"input_folder = '../input/nfl-big-data-bowl-2020'\ntrain = pd.read_csv(os.path.join(input_folder, 'train.csv'), dtype={'WindSpeed': 'object'})\ntrain.shape\nresults = {}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is the list of features used by the model.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"player_cols = ['X', 'Y', 'X_dir', 'Y_dir', 'X_S', 'Y_S', 'S', 'A', 'IsRusher', 'IsOnOffense']\nplay_cols = ['X_rusher', 'Y_rusher', 'YardLine_std']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We clean and standardize data as in my [initial wrangling & Voronoi areas in Python](https://www.kaggle.com/cpmpml/initial-wrangling-voronoi-areas-in-python) notebook. I also include some data cleaning that was shared by top teams, namely using `S = 10 * Dis`, and averaging `A` in 2017. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def reorient(train, flip_left):\n    train['ToLeft'] = train.PlayDirection == \"left\"\n    #train['IsBallCarrier'] = train.NflId == train.NflIdRusher\n    \n    train.loc[train.VisitorTeamAbbr == \"ARI\", 'VisitorTeamAbbr'] = \"ARZ\"\n    train.loc[train.HomeTeamAbbr == \"ARI\", 'HomeTeamAbbr'] = \"ARZ\"\n    \n    train.loc[train.VisitorTeamAbbr == \"BAL\", 'VisitorTeamAbbr'] = \"BLT\"\n    train.loc[train.HomeTeamAbbr == \"BAL\", 'HomeTeamAbbr'] = \"BLT\"\n    \n    train.loc[train.VisitorTeamAbbr == \"CLE\", 'VisitorTeamAbbr'] = \"CLV\"\n    train.loc[train.HomeTeamAbbr == \"CLE\", 'HomeTeamAbbr'] = \"CLV\"\n    \n    train.loc[train.VisitorTeamAbbr == \"HOU\", 'VisitorTeamAbbr'] = \"HST\"\n    train.loc[train.HomeTeamAbbr == \"HOU\", 'HomeTeamAbbr'] = \"HST\"\n    \n    train['TeamOnOffense'] = \"home\"\n    train.loc[train.PossessionTeam != train.HomeTeamAbbr, 'TeamOnOffense'] = \"away\"\n    train['IsOnOffense'] = train.Team == train.TeamOnOffense # Is player on offense?\n    train['YardLine_std'] = 100 - train.YardLine\n    train.loc[train.FieldPosition.fillna('') == train.PossessionTeam,  \n          'YardLine_std'\n         ] = train.loc[train.FieldPosition.fillna('') == train.PossessionTeam,  \n          'YardLine']\n    train.loc[train.ToLeft, 'X'] = 120 - train.loc[train.ToLeft, 'X'] \n    train.loc[train.ToLeft, 'Y'] = 160/3 - train.loc[train.ToLeft, 'Y'] \n    train.loc[train.ToLeft, 'Orientation'] = np.mod(180 + train.loc[train.ToLeft, 'Orientation'], 360)\n    train['Dir'] = 90 - train.Dir\n    train.loc[train.ToLeft, 'Dir'] = np.mod(180 + train.loc[train.ToLeft, 'Dir'], 360)\n    train.loc[train.IsOnOffense, 'Dir'] = train.loc[train.IsOnOffense, 'Dir'].fillna(0).values\n    train.loc[~train.IsOnOffense, 'Dir'] = train.loc[~train.IsOnOffense, 'Dir'].fillna(180).values\n    \n    train['S'] = 10 * train['Dis']\n    train.loc[train.Season == 2017, 'A'] = train.loc[train.Season == 2017, 'A'].mean()\n    \n    train['IsRusher'] = train['NflId'] == train['NflIdRusher']\n    if flip_left:\n        df = train[train['IsRusher']].copy()\n        #df['left'] = df.Y < 160/6\n        df['left'] = df.Dir < 0\n        train = train.merge(df[['PlayId', 'left']], how='left', on='PlayId')\n        train['Y'] = train.Y\n        train.loc[train.left, 'Y'] = 160/3 - train.loc[train.left, 'Y']\n        train['Dir'] = train.Dir\n        train.loc[train.left, 'Dir'] = np.mod( - train.loc[train.left, 'Dir'], 360)\n        train.drop('left', axis=1, inplace=True)\n        \n    \n    train['X_dir'] = np.cos( (np.pi / 180) * train .Dir)\n    train['Y_dir'] = np.sin( (np.pi / 180) * train.Dir)\n    train['X_S'] = train.X_dir * train.S\n    train['Y_S'] = train.Y_dir * train.S\n    train['X_A'] = train.X_dir * train.A\n    train['Y_A'] = train.Y_dir * train.A\n    train['PlayerHeight'] = train['PlayerHeight'].apply(lambda x: 12*int(x.split('-')[0])+int(x.split('-')[1]))\n    train['BMI'] = (train['PlayerWeight'] * 703) / (train['PlayerHeight'] ** 2)\n    train['Energy'] = train['PlayerWeight'] * (train['S'] ** 2) \n#    train.loc[train['Season'] == 2017, 'S'] = (train['S'][train['Season'] == 2017] - 2.4355) / 1.2930 * 1.4551 + 2.7570\n    train['time_step'] = 0.0\n    train = train.sort_values(by=['PlayId', 'IsOnOffense', 'IsRusher', 'Y']).reset_index(drop=True)\n    return  train\n\ndef add_features(train):\n        \n    df = train[train.IsRusher][['PlayId', 'time_step', 'X', 'Y']].copy()\n    df.columns = ['PlayId', 'time_step', 'X_rusher', 'Y_rusher']\n    train = train.merge(df, how='left', on=['PlayId', 'time_step'])\n    #train.loc[~train.IsRusher, 'X'] = train.loc[~train.IsRusher, 'X'] - train.loc[~train.IsRusher, 'X_rusher']\n    #train.loc[~train.IsRusher, 'Y'] = train.loc[~train.IsRusher, 'Y'] - train.loc[~train.IsRusher, 'Y_rusher']\n    train.X -= train.X_rusher\n    train.Y -= train.Y_rusher\n    \n    #train.drop(['Orientation', 'Dir', 'TeamOnOffense', 'YardLine', 'left'], axis=1, inplace=True)\n    return  train\n\ndef time_forward(train, time_step):\n    train = train.copy()\n    train['X'] = train['X'] + time_step * train['X_S'] + 0.5 * time_step**2 * train['X_A']\n    train['Y'] = train['Y'] + time_step * train['Y_S'] + 0.5 * time_step**2 * train['Y_A']\n    x_s = np.clip(train['X_S'] + time_step * train['X_A'], -14, 14)\n    y_s = np.clip(train['Y_S'] + time_step * train['Y_A'], -14, 14)\n    if 1:\n        train['X_A'] = np.clip(train['X_A'] * train['X_S'] / (x_s), -10, 10)\n        train['Y_A'] = np.clip(train['Y_A'] * train['Y_S'] / (y_s), -10, 10)\n    train['X_S'] = x_s\n    train['Y_S'] = y_s\n    train['time_step'] = time_step\n    return train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_reorient = reorient(train.copy(), flip_left=True)\nlen_train_reorient = len(train_reorient) // 22","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A way to improve deep learning mdoels is to augment data.  Here I augment it by computing positions of players after a given time step, then apply the same data cleaning to the new positions."},{"metadata":{"trusted":true},"cell_type":"code","source":"def augment_data(train_reorient, time_steps):\n    train_reorient = pd.concat(time_forward(train_reorient, time_step) for time_step in time_steps)\n    train_reorient = add_features(train_reorient)\n    return train_reorient","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A little tuning suggests that using two time steps of 0.3 and 0.6 seconds are useful."},{"metadata":{"trusted":true},"cell_type":"code","source":"time_steps = [0, 0.3, 0.6]\ntrain_reorient = augment_data(train_reorient, time_steps)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see what the data for first play looks like.  One thing the data cleaning code does it to sort players, so that defense team is on first 11 rows, the offense team, ending with the rusher."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = train_reorient[player_cols]\ndf.head(22)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use a cross validation such that 2017 data is not used for validation.  In order to do it we keep track of where 2017 ends."},{"metadata":{"trusted":true},"cell_type":"code","source":"len_2017 = train[train.Season == 2017].shape[0] // 22\nlen_2017","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We then compute our training data.  We have three inputs to the model: players data, play data, and distance matrix.  Each data input is scaled."},{"metadata":{"trusted":true},"cell_type":"code","source":"ss_players = StandardScaler()\nX_players = df.values.astype('float32') \nX_players = ss_players.fit_transform(df)\nX_players = X_players.reshape((-1, 22, len(player_cols)))\nX_players.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_play = train_reorient[play_cols].values[::22]\nss_play = StandardScaler()\nX_play = ss_play.fit_transform(X_play)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The last input is the distance matrix for each play.  In order to compute it efficiently we use the numba compiler. This is way faster than using predefined dist function or numpy operations. More deatails can be found in my [Ultra Fast Distance Matrix Computation](https://www.kaggle.com/cpmpml/ultra-fast-distance-matrix-computation) notebook."},{"metadata":{"trusted":true},"cell_type":"code","source":"@jit\ndef get_dmat(X, Y):\n    dmat = np.zeros((22, 22))\n    for i in range(22):\n        for j in range(i+1, 22):\n            d = np.sqrt((X[i] - X[j])**2 + (Y[i] - Y[j])**2)\n            dmat[i, j] = d\n            dmat[j, i] = d\n    dmat = dmat.reshape((1, 22, 22))\n    return dmat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X,Y = train_reorient.X.values, train_reorient.Y.values\ndmats = [get_dmat(X[i:i+22], Y[i:i+22]) for i in range(0, train_reorient.shape[0], 22)]\ndmats = np.vstack(dmats).reshape((-1, 22, 22))\ndmats.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We want to use distance as an input on how each player influences other players.  Given the influence decreases with distance, we tried various decreasing functions of distance, starting with its inverse.  The squared inverse seemed to work better, which is why we're using it now.  Data is also sclaled."},{"metadata":{"trusted":true},"cell_type":"code","source":"inv_dmats = 1. / (1e-2 + dmats)**2\ninv_dmats /= inv_dmats.sum(axis=2, keepdims=True)\n_ = plt.hist(inv_dmats.ravel(), bins=100, log=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's delete the raw train data so that we don't use it by mistake."},{"metadata":{"trusted":true},"cell_type":"code","source":"del train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The transformer relies on layer normalization rather than batch normalization.  I borrowed an implemententation from github."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import backend as K\n\nclass LayerNormalization(keras.layers.Layer):\n\n    def __init__(self,\n                 center=True,\n                 scale=True,\n                 epsilon=None,\n                 gamma_initializer='ones',\n                 beta_initializer='zeros',\n                 gamma_regularizer=None,\n                 beta_regularizer=None,\n                 gamma_constraint=None,\n                 beta_constraint=None,\n                 **kwargs):\n        \"\"\"Layer normalization layer\n        See: [Layer Normalization](https://arxiv.org/pdf/1607.06450.pdf)\n        :param center: Add an offset parameter if it is True.\n        :param scale: Add a scale parameter if it is True.\n        :param epsilon: Epsilon for calculating variance.\n        :param gamma_initializer: Initializer for the gamma weight.\n        :param beta_initializer: Initializer for the beta weight.\n        :param gamma_regularizer: Optional regularizer for the gamma weight.\n        :param beta_regularizer: Optional regularizer for the beta weight.\n        :param gamma_constraint: Optional constraint for the gamma weight.\n        :param beta_constraint: Optional constraint for the beta weight.\n        :param kwargs:\n        \"\"\"\n        super(LayerNormalization, self).__init__(**kwargs)\n        self.supports_masking = True\n        self.center = center\n        self.scale = scale\n        if epsilon is None:\n            epsilon = K.epsilon() * K.epsilon()\n        self.epsilon = epsilon\n        self.gamma_initializer = keras.initializers.get(gamma_initializer)\n        self.beta_initializer = keras.initializers.get(beta_initializer)\n        self.gamma_regularizer = keras.regularizers.get(gamma_regularizer)\n        self.beta_regularizer = keras.regularizers.get(beta_regularizer)\n        self.gamma_constraint = keras.constraints.get(gamma_constraint)\n        self.beta_constraint = keras.constraints.get(beta_constraint)\n        self.gamma, self.beta = None, None\n\n    def get_config(self):\n        config = {\n            'center': self.center,\n            'scale': self.scale,\n            'epsilon': self.epsilon,\n            'gamma_initializer': keras.initializers.serialize(self.gamma_initializer),\n            'beta_initializer': keras.initializers.serialize(self.beta_initializer),\n            'gamma_regularizer': keras.regularizers.serialize(self.gamma_regularizer),\n            'beta_regularizer': keras.regularizers.serialize(self.beta_regularizer),\n            'gamma_constraint': keras.constraints.serialize(self.gamma_constraint),\n            'beta_constraint': keras.constraints.serialize(self.beta_constraint),\n        }\n        base_config = super(LayerNormalization, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\n    def compute_mask(self, inputs, input_mask=None):\n        return input_mask\n\n    def build(self, input_shape):\n        shape = input_shape[-1:]\n        if self.scale:\n            self.gamma = self.add_weight(\n                shape=shape,\n                initializer=self.gamma_initializer,\n                regularizer=self.gamma_regularizer,\n                constraint=self.gamma_constraint,\n                name='gamma',\n            )\n        if self.center:\n            self.beta = self.add_weight(\n                shape=shape,\n                initializer=self.beta_initializer,\n                regularizer=self.beta_regularizer,\n                constraint=self.beta_constraint,\n                name='beta',\n            )\n        super(LayerNormalization, self).build(input_shape)\n\n    def call(self, inputs, training=None):\n        mean = K.mean(inputs, axis=-1, keepdims=True)\n        variance = K.mean(K.square(inputs - mean), axis=-1, keepdims=True)\n        std = K.sqrt(variance + self.epsilon)\n        outputs = (inputs - mean) / std\n        if self.scale:\n            outputs *= self.gamma\n        if self.center:\n            outputs += self.beta\n        return outputs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before building the model, let's define the target and the metric.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"y_mae = train_reorient['Yards'].values[::22]\ny_mean = np.median(y_mae)\ny_crps = np.zeros(shape=(y_mae.shape[0], 199))\nfor i,yard in enumerate(y_mae):\n    y_crps[i, yard+99:] = 1\n    \nyardline = train_reorient['YardLine_std'].values[::22]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def crps(y_true, y_pred):\n    stops = np.arange(-99, 100)\n    unit_steps = stops >= y_true.reshape(-1, 1)\n    crps = np.mean((y_pred - unit_steps)**2)\n    return crps","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"The output of our model must be non decreasing.  One way to achieve this is to run an [isotonic regression](https://scikit-learn.org/stable/auto_examples/plot_isotonic_regression.html)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def nondecreasing(x):\n    X_ir = np.arange(199).astype('float64')\n    ir = IsotonicRegression(0, 1)\n    x = ir.fit_transform(X_ir, x.astype('float64'))        \n    return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's now implement our model.  Its high level architecture is inspired by the transformer architecture:\n\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F75976%2Fe50d024a978c324b67182a554a2b66bd%2Ftransformer.png?generation=1575305725966574&amp;alt=media)\n\nWe start wiht embeddings, then a distance attention bloc, then a transformer encoder bloc with multi head self attention followed by a feed forward bloc. Then we plug the output into an encoder/decoder attention on the play embeddings.  Its output is fed to two output layers that output probabilities.  More can be found in my [competition writeup](https://www.kaggle.com/c/nfl-big-data-bowl-2020/discussion/119430)."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model, load_model\nfrom keras.layers import Input, BatchNormalization, Activation, Add, Multiply, Dot\nfrom keras.layers import Embedding, Permute, Reshape\nfrom keras.layers.core import Dropout, Lambda, Dense, Flatten\nfrom keras.layers.convolutional import Conv1D, Conv2D\nfrom keras.layers.pooling import GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.layers.merge import Concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.optimizers import Adam, SGD, Nadam\nfrom keras import backend as K\n\nfrom keras.engine.topology import Layer\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We defined a custom layer that takes 4 numbers as input and outputs a logistic distribution."},{"metadata":{"trusted":true},"cell_type":"code","source":"class ScaleLayer(Layer):\n\n    def __init__(self, output_dim, **kwargs):\n        self.output_dim = output_dim\n        super(ScaleLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        super(ScaleLayer, self).build(input_shape) \n\n    def call(self, x):\n        xx = K.arange(-99, 100, dtype=tf.float32)\n        mu = y_mean + tf.reshape(x[:, 0], (-1, 1))\n        sigma_minus = tf.identity(K.exp(0.5 * tf.reshape(x[:, 1], (-1, 1))), name=\"sigma\")\n        sigma_plus = tf.identity(K.exp(0.5 * tf.reshape(x[:, 2], (-1, 1))), name=\"sigma\")\n        xx = tf.subtract(xx, mu)\n        pcf = tf.where(xx >= 0, tf.divide (xx, sigma_plus),  tf.divide (xx, sigma_minus))\n        return pcf\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], self.output_dim)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next piece is a distance attention. To update a given player embedding I use a weighted sum of the other players embeddings. The weight depends on the distance. I tried various ways, and a normalized squared inverse was best. I was about to try other transforms when I decided to have them learnt by the model, via a 1x1 convolution bloc on the data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def dist_mult(dist, players):\n    res = Lambda(lambda c: K.batch_dot(c[0], c[1]))([dist, players])\n    return res\n\ndef dist_attention(dist, players, dropout):\n    if 1:\n        dist1 = Reshape((22, 22, 1))(dist)\n        dist1 = Conv2D(16, 1, activation='relu', \n                  kernel_initializer='glorot_uniform',\n                  bias_initializer='glorot_uniform',\n                 )(dist1)\n        dist1 = Conv2D(1, 1, activation='relu', \n                  kernel_initializer='glorot_uniform',\n                  bias_initializer='glorot_uniform',\n                 )(dist1)\n        dist1 = Reshape((22, 22))(dist1)\n        dist = Add()([dist, dist1])\n    dist = LayerNormalization()(dist)\n    att = dist_mult(dist, players, )\n    x_player = Add()([players, att])\n    x_player = LayerNormalization()(x_player)\n    if dropout > 0:\n        x_player = Dropout(dropout)(x_player)\n    return x_player\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next bloc is multi head attention.  I recommend these two tutorials to understand the logic behind the code:\n\nhttp://jalammar.github.io/illustrated-transformer/\n\nhttps://nlp.seas.harvard.edu/2018/04/03/attention.html\n"},{"metadata":{},"cell_type":"markdown","source":"There are two variants of multi head attention. The first one is self attention, used in the encoder part of the transformer. Here we use it on an array of player features.  The second variant is used in the decoder transformer. Here we use it with the play embeddings and the output of the encoder part."},{"metadata":{"trusted":true},"cell_type":"code","source":"def attention(x_inner, x_outer, n_factor, dropout):\n    x_Q =  Conv1D(n_factor, 1, activation='linear', \n                  kernel_initializer='glorot_uniform',\n                  bias_initializer='glorot_uniform',\n                 )(x_inner)\n    x_K =  Conv1D(n_factor, 1, activation='linear', \n                  kernel_initializer='glorot_uniform',\n                  bias_initializer='glorot_uniform',\n                 )(x_outer)\n    x_V =  Conv1D(n_factor, 1, activation='linear', \n                  kernel_initializer='glorot_uniform',\n                  bias_initializer='glorot_uniform',\n                 )(x_outer)\n    x_KT = Permute((2, 1))(x_K)\n    res = Lambda(lambda c: K.batch_dot(c[0], c[1]) / np.sqrt(n_factor))([x_Q, x_KT])\n    att = Lambda(lambda c: K.softmax(c, axis=-1))(res)\n    att = Lambda(lambda c: K.batch_dot(c[0], c[1]))([att, x_V])\n    return att\n\ndef multi_head_self_attention(x, n_factor, n_head, dropout):\n    if n_head == 1:\n        att = attention(x, x, n_factor, dropout)\n    else:\n        n_factor_head = n_factor // n_head\n        heads = [attention(x, x, n_factor_head, dropout) for i in range(n_head)]\n        att = Concatenate()(heads)\n        att = Dense(n_factor, \n                      kernel_initializer='glorot_uniform',\n                      bias_initializer='glorot_uniform',\n                     )(att)\n    x = Add()([x, att])\n    x = LayerNormalization()(x)\n    if dropout > 0:\n        x = Dropout(dropout)(x)\n    return x\n\n#original\n# def multi_head_outer_attention(x_inner, x_outer, n_factor, n_head, dropout):\n#     if n_head == 1:\n#         att = attention(x_inner, x_outer, n_factor, dropout)\n#     else:\n#         n_factor_head = n_factor // n_head\n#         heads = [attention(x_inner, x_outer, n_factor_head, dropout) for i in range(n_head)]\n#         att = Concatenate()(heads)\n#         att = Dense(n_factor, \n#                       kernel_initializer='glorot_uniform',\n#                       bias_initializer='glorot_uniform',\n#                      )(att)\n#     x_inner = Add()([x_inner, att])\n#     x_inner = LayerNormalization()(x_inner)\n#     if dropout > 0:\n#         x = Dropout(dropout)(x_inner)\n#     return x\n\ndef multi_head_outer_attention(x_inner, x_outer, n_factor, n_head, dropout):\n    #inner is play outer is players\n    if n_head == 1:\n        att = attention(x_inner, x_inner, n_factor, dropout)\n    else:\n        n_factor_head = n_factor // n_head\n        heads = [attention(x_inner, x_inner, n_factor_head, dropout) for i in range(n_head)]\n        att = Concatenate()(heads)\n        att = Dense(n_factor, \n                      kernel_initializer='glorot_uniform',\n                      bias_initializer='glorot_uniform',\n                     )(att)\n    x_outer = Add()([x_outer, att])\n    x_outer = LayerNormalization()(x_outer)\n    if dropout > 0:\n        x = Dropout(dropout)(x_outer)\n    return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next bloc is a position based feed forward network.  As noted in the original transformer paper, this is equivalent to a 1D convolution bloc, which is what we use here.  I added squeeze ande excitation, it improves performance a bit."},{"metadata":{"trusted":true},"cell_type":"code","source":"def se_bloc(in_bloc, ch, ratio):\n    x = GlobalAveragePooling1D()(in_bloc)\n    x = Dense(ch//ratio, activation='relu')(x)\n    x = Dense(ch, activation='sigmoid')(x)\n    x = Multiply()([in_bloc, x])\n    return Add()([x, in_bloc])\n\ndef conv_bloc(players, n_factor, n_hidden, se_ratio, dropout):\n    players0 = players\n    players = Conv1D(n_hidden, 1, activation='relu', \n                  kernel_initializer='glorot_uniform',\n                  bias_initializer='glorot_uniform',\n                 )(players)\n    players = Conv1D(n_factor, 1, activation='relu', \n                  kernel_initializer='glorot_uniform',\n                  bias_initializer='glorot_uniform',\n                 )(players)\n    players = Add()([players0, players])\n    players = se_bloc(players, n_factor, se_ratio)\n    players = LayerNormalization()(players)\n    if dropout > 0:\n        players = Dropout(dropout)(players)\n    return players","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can now build our model with these blocs."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model(n_player, n_factor, n_loop, n_head, n_hidden, se_ratio, dropout, n_player_cols, n_play_cols):\n    input_players = Input((n_player, n_player_cols), name=\"players\")\n    input_dmats = Input((n_player, n_player), name=\"inv_dist\")\n    input_play = Input((n_play_cols,), name=\"plays\")\n\n    x_player = input_players\n    x_player = Conv1D(n_factor, 1)(x_player)    \n    x_player = LayerNormalization()(x_player)\n\n    for l in range(n_loop):\n        x_player = dist_attention(input_dmats, x_player, dropout)\n        x_player = conv_bloc(x_player, n_factor, n_hidden, se_ratio, dropout)\n\n        x_player = multi_head_self_attention(x_player, n_factor, n_head, dropout)\n        x_player = conv_bloc(x_player, n_factor, n_hidden, se_ratio, dropout)\n\n    x_play = Dense(n_factor)(input_play)\n    x_play = Reshape((1, -1))(x_play)\n    \n    readout = multi_head_outer_attention(x_play, x_player, n_factor, n_head, dropout)\n    readout = Flatten()(readout)\n\n    out1 = Dense(199, activation='sigmoid')(readout)\n    readout = Dense(4)(readout)\n    readout = ScaleLayer(output_dim=199)(readout)\n    out2 = keras.layers.Activation('sigmoid')(readout)\n    return Model(inputs=[input_players, input_dmats, input_play], outputs=[out1, out2])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's test the code."},{"metadata":{"trusted":true},"cell_type":"code","source":"n_player = 22\nn_factor = 64\nse_ratio = 4\nn_loop = 1\nn_head = 4\nn_hidden = 2*n_factor\ndropout = 0.25\nn_player_cols = len(player_cols)\nn_play_cols = len(play_cols)\nmodel = get_model(n_player, n_factor, n_loop, n_head, n_hidden, se_ratio, dropout, n_player_cols, n_play_cols)\nmodel.summary()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At some point during our development we simulated the public test leaderboard by using that last 10% of the data for test, and the prevoous 90% for cross validation.  That code my not be up to date now."},{"metadata":{"trusted":true},"cell_type":"code","source":"simulate_test = False\n\nif simulate_test:\n    len_data = X_players.shape[0]\n    len_train = int(0.9 * len_data)\nelse:\n    len_data = X_players.shape[0]\n    len_train = len_data\n\nX_players_train, X_players_test = X_players[:len_train], X_players[len_train:]\ninv_dmats_train, inv_dmats_test = inv_dmats[:len_train], inv_dmats[len_train:]\nX_play_train, X_play_test = X_play[:len_train], X_play[len_train:]\ny_mae_train, y_mae_test = y_mae[:len_train], y_mae[len_train:]\ny_crps_train, y_crps_test = y_crps[:len_train], y_crps[len_train:]\nyardline_train, yardline_test = yardline[:len_train], yardline[len_train:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can now define our folds.  As said above, we used 2017 only for training folds, and build validation folds out of the remaining data.  We also downweight 2017 samples by 0.5.  The folds take data augmentation into account: plays derived from an original train play are put in the same fold as the original."},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=12, shuffle=False)\nind_2017 = np.arange(len_2017)\ndef add_2017(x,y):\n    x = np.concatenate((ind_2017, x +  len_2017))\n    y = y + len_2017\n    return x,y\n\nindices = [add_2017(x, y) for x, y in kf.split(X_players_train[len_2017:len_train_reorient], \n                                               y_mae_train[len_2017:len_train_reorient])]\n\nn_steps = len(time_steps)\n\ndef augment_indices(x, y, time_steps):\n    x_a = np.concatenate([x + i*len_train_reorient for i in range(n_steps)])\n    y_a = np.concatenate([y + i*len_train_reorient for i in range(n_steps)])\n    return x_a, y_a\n\nindices = [augment_indices(x, y, time_steps) for x,y in indices ]\nindices = [(i, ind) for i,ind in enumerate(indices)]\n\nw = np.ones(len_train_reorient)\nw[:len_2017] = 0.5\nw = np.concatenate([w for i in range(n_steps)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We run our non decreasing code.  We also fix to 0 or 1 values that are always known. Last, given we use TTA, i.e. we augment data at prediction time, we take the average of predictions.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"def post_process(y_pred, yardline_train, time_steps):\n    upper = 100 - yardline_train\n    lower = - yardline_train\n    for i in range(y_pred.shape[0]):\n        y_pred[i, 99 + upper[i]:] = 1\n        y_pred[i, :99 + lower[i]] = 0\n        y_pred[i] = nondecreasing(y_pred[i].ravel())\n    if 1:\n        n_steps = len(time_steps)\n        len_y_pred = len(y_pred) // n_steps\n        y_pred = np.mean([y_pred[len_y_pred * i: len_y_pred * (i+1)] for i in range(n_steps)], axis=0) \n    return y_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Due to time limit we only run two folds here.  During mdodel development we used 12 folds, which proved to be quite in line with LB."},{"metadata":{"trusted":true},"cell_type":"code","source":"name = 'nn'\nresults[name] = {'crps': [], 'y_test': [], 'imp': [], 'models': []}\n\ntry:\n    del sess\nexcept:\n    pass\n\nsess = init_seeds(0)\n\npred_test = []\n\n\nfor index in indices:\n    if 0 and index[0] < 10:\n        continue\n    train_idx, val_idx = index[1]\n    train_players, train_inv_dmats, train_play, train_y, = (X_players_train[train_idx], \n                                                            inv_dmats_train[train_idx], \n                                                            X_play_train[train_idx], \n                                                            y_crps_train[train_idx],) \n    val_players, val_inv_dmats, val_play, val_y, = (X_players_train[val_idx], \n                                                    inv_dmats_train[val_idx], \n                                                    X_play_train[val_idx], \n                                                    y_crps_train[val_idx], )\n    w_train = w[train_idx]\n\n    n_player = 22\n    n_factor = 64\n    se_ratio = 2\n    n_loop = 1\n    n_head = 4\n    n_hidden = 2*n_factor\n    dropout = 0.25\n    n_player_cols = len(player_cols)\n    n_play_cols = len(play_cols)\n    model = get_model(n_player, n_factor, n_loop, n_head, n_hidden, se_ratio, dropout, n_player_cols, n_play_cols)\n    \n    if 1:\n        opm = keras.optimizers.Adam(lr=1e-3)\n        #opm = RAdam(warmup_proportion=0.1, min_lr=1e-5)\n        model.compile(loss='mse', optimizer=opm, metrics=[])\n        es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', \n                                           restore_best_weights=True, verbose=0, patience=21)\n        lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=10, verbose=0, mode='min', min_delta=0.00001)\n        history = model.fit([train_players, train_inv_dmats, train_play], [train_y, train_y], \n                        verbose=2, batch_size=256, epochs=500, \n                        callbacks=[es,lr], \n                        validation_data=[[val_players, val_inv_dmats, val_play], [val_y, val_y]],\n                        sample_weight=[w_train, w_train])\n    \n    X_ir = np.arange(199).astype('float64')\n    #print(val_players.shape, val_inv_dmats.shape)\n\n    y_pred = model.predict([val_players, val_inv_dmats, val_play])\n    y_pred = (y_pred[0]+y_pred[1])/2.\n    score0 = crps(y_mae_train[val_idx], y_pred)\n    y_pred = post_process(y_pred, yardline_train[val_idx], time_steps)\n    score1 = crps(y_mae_train[val_idx][:len(y_pred)], y_pred)\n    print(' val fold:', index[0], score0)\n    print(' val fold:', index[0], score1)\n    if score0 < score1:\n        break\n    results[name]['models'].append(model)\n    results[name]['crps'].append(score1)\n    #results[name]['imp'].append(model.feature_importances_)\n    if simulate_test:\n        y_pred = model.predict([X_players_test, inv_dmats_test, X_play_test])\n        y_pred = (y_pred[0]+y_pred[1])/2.\n        score0 = crps(y_mae_test, y_pred)\n        y_pred = post_process(y_pred, yardline_test, time_steps)\n        score1 = crps(y_mae_test[:len(y_pred)], y_pred)\n        print('test fold:', index[0], score0)\n        print('test fold:', index[0], score1)\n        pred_test.append(y_pred)\n    print('*' * 40)\nprint('Folds:', results[name]['crps'])\nprint('Average:', np.mean(results[name]['crps']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's delete our training data to make sure we don't use it anymore."},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_reorient","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The rest of the code computes submission.  It performs the same data cleaning and data augmentation than was done on training data."},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\n\nenv = nflrush.make_env()\niter_test = env.iter_test()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for (test_df, sample_prediction_df) in tqdm(iter_test):\n    test_reorient = reorient(test_df.copy(), flip_left=True)\n    test_reorient = augment_data(test_reorient, time_steps)\n    df = test_reorient[player_cols]\n    X_players = df.values.astype('float32') \n    X_players = ss_players.transform(df)\n    X_players = X_players.reshape((-1, 22, len(player_cols)))\n    X,Y = test_reorient.X.values, test_reorient.Y.values\n    dmats = [get_dmat(X[i:i+22], Y[i:i+22]) for i in range(0, test_reorient.shape[0], 22)]\n    dmats = np.vstack(dmats).reshape((-1, 22, 22))   \n    inv_dmats = 1. / (1e-2 + dmats)**2\n    inv_dmats /= inv_dmats.sum(axis=2, keepdims=True)    \n    X_play = test_reorient[play_cols].values[::22]\n    X_play = ss_play.transform(X_play)    \n    yardline = test_reorient['YardLine_std'].values[::22]\n    y_preds = [model.predict([X_players, inv_dmats, X_play]\n                            ) for model in results[name]['models']]\n    y_preds = [(pred[0] + pred[1]) / 2. for pred in y_preds]\n    y_pred = np.mean(y_preds, axis=0)\n    y_pred = post_process(y_pred, yardline, time_steps)\n    sample_prediction_df.iloc[0, :] = y_pred.ravel()\n    env.predict(sample_prediction_df)\nenv.write_submission_file()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" y_pred.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_reorient.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X,Y = test_reorient.X.values, test_reorient.Y.values\ndmats = [get_dmat(X[i:i+22], Y[i:i+22]) for i in range(0, test_reorient.shape[0], 22)]\ndmats = np.vstack(dmats).reshape((-1, 22, 22))\ndmats.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}