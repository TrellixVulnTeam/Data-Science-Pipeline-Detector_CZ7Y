{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"import os, random, gc, pickle\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\nimport datetime\nfrom kaggle.competitions import nflrush\nimport tqdm\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import KFold, GroupKFold\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom scipy.spatial import distance_matrix\nfrom scipy.stats import cumfreq\nfrom scipy.optimize import nnls\nimport lightgbm as lgb\nfrom joblib import Parallel, delayed\nimport multiprocessing\nimport keras\nfrom keras.models import Model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\nfrom keras.engine.saving import load_model\nfrom keras.layers import Dense, Activation, BatchNormalization, Dropout\nfrom keras.layers import Input, Concatenate\nfrom keras.models import Sequential, Model\nfrom keras.layers import Input, Dense, Embedding, Concatenate, Flatten, BatchNormalization, Dropout, Activation, PReLU, Add\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.utils.vis_utils import plot_model\nfrom keras import metrics\nfrom collections import Counter\nfrom scipy.spatial import Voronoi, voronoi_plot_2d, KDTree, ConvexHull \nfrom shapely.ops import polygonize, unary_union\nfrom shapely.geometry import LineString, MultiPolygon, MultiPoint, Point, Polygon\nfrom shapely.geometry import shape, mapping\nimport math\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"mname = 'zoo_keras'\n# path = '/kaggle/input/nfl-big-data-bowl-2020/'\npath = './'\nbuild_data = True\nsearch = False\nnrep = 10\npatience = 21\nw_holdout = True\nperm = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"np.set_printoptions(linewidth=200, precision=6, suppress=True)\npd.set_option('display.float_format', lambda x: '%.6f' % x)\npd.set_option('display.width', 200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# standardize coordinates, angles, yardline so offense is always driving to the right\n# https://www.kaggle.com/kernels/scriptcontent/21906255/\ndef std_cols(df):\n    \n    # fix inconsistent team abbreviations\n    ita = {'ARZ': 'ARI', 'BLT':'BAL', 'CLV':'CLE', 'HST':'HOU'}\n    update = ['PossessionTeam', 'FieldPosition']\n    for col in update:\n        for old, new in ita.items():\n            df.loc[df[col] == old,col] = new\n                \n    df['X'] = df.apply(lambda x: x.X if x.PlayDirection == 'right'\\\n                           else 120-x.X, axis=1) \n    \n    df['Y'] = df.apply(lambda x: x.Y if x.PlayDirection == 'right'\\\n                           else 53.3-x.Y, axis=1) \n    \n#     # adjust 2017 Orientation as it differs by 90 degrees from 2018 and 2019\n#     df.loc[df.Season == 2017, 'Orientation'] = np.mod(df.Orientation + 90, 360)\n#     # set angles so 0 degrees is directly downfield for rusher and range -180 to 180\n#     df['Orientation'] = df.apply(lambda x: 180 - np.mod(x.Orientation + 90 \\\n#                                      if x.PlayDirection == 'right' \\\n#                                      else x.Orientation - 90, 360), axis=1)\n    \n    df['Dir'] = df.apply(lambda x: 180 - np.mod(x.Dir + 90 \\\n                                     if x.PlayDirection == 'right' \\\n                                     else x.Dir - 90, 360), axis=1)\n    \n    df['YardLine'] = df.apply(lambda x: x.YardLine + 10 \\\n                              if (x.FieldPosition == x.PossessionTeam) \\\n                              else 60 + (50-x.YardLine), axis=1)\n    \n    df.loc[:, 'S'] = 10 * df['Dis']\n    \n        \n    return df","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"%%time\nsns.set_style('darkgrid')\nmpl.rcParams['figure.figsize'] = [15,10]\n\ntrain = pd.read_csv(path + 'train1.csv', dtype={'WindSpeed': 'object'})\nprint(train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n# train0 = train.copy()\ntrain = std_cols(train)\n# train_df = train.copy()\ntrain_df = train\nprint(train_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df = train_df\nmode = 'train'\nverbose = True\n\n# make a copy so as not to alter the original df\ndf = df.copy()\n\ndf['OffenseDefense'] = \\\ndf.apply(lambda x: \"Offense\" if ((x.Team == 'home') \\\n                                 & (x.PossessionTeam == \\\n                                    x.HomeTeamAbbr)) | \\\n                                ((x.Team == 'away') & \\\n                                 (x.PossessionTeam == \\\n                                  x.VisitorTeamAbbr)) \\\n                                else \"Defense\", axis=1)\n\ndf['IsRusher'] = df['NflId'] == df['NflIdRusher']\n\ndf.loc[df.IsRusher, 'OffenseDefense'] = \"Rusher\"\n\nkeep = ['GameId','PlayId','X','Y','Dir','S','IsRusher','OffenseDefense']\n\ndf = df[keep]\n\nif verbose:\n    print(df.shape)\n\n# flip defense direction\n# df.loc[df.OffenseDefense=='Defense','Dir'] = df.loc[df.OffenseDefense=='Defense','Dir'] + 180\n\nnewdir = df.Dir\n# newdir = 0.99*df.Dir + 0.01*df.Orientation\ndf['SX'] = df.S * np.cos(newdir/180*np.pi) \ndf['SY'] = df.S * np.sin(newdir/180*np.pi)\n\nbdf = df.loc[df.IsRusher,['PlayId','X','Y','SX','SY']]\nbdf.columns = ['PlayId','Ball_X','Ball_Y','Ball_SX','Ball_SY']\n\n# bdf = df.loc[df.IsRusher,['PlayId','X','Y','S','Dir']]\n# bdf.columns = ['PlayId','Ball_X','Ball_Y','Ball_S','Ball_Dir']\n\ndf = df.merge(bdf, how='left', on='PlayId')\n\ndf['XR'] = df.X - df.Ball_X\ndf['YR'] = df.Y - df.Ball_Y\n\ndf['SXR'] = df.SX - df.Ball_SX\ndf['SYR'] = df.SY - df.Ball_SY \n\n# df['SXR'] = (df.S - df.Ball_S) * np.cos(df.Dir/180*np.pi) \n# df['SYR'] = (df.S - df.Ball_S) * np.sin(df.Dir/180*np.pi) \n\n\nprint(df.head(), df.shape)\n\n# return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"k = ['PlayId','XR','YR','SX','SY','SXR','SYR']\n# b = ['Ball_X','Ball_Y','Ball_SX','Ball_SY']\n# o = df.loc[df.OffenseDefense=='Offense',k]\no = df.loc[df.OffenseDefense=='Offense',k]\nd = df.loc[df.OffenseDefense=='Defense',k]\no.columns = ['PlayId','XRO','YRO','SXO','SYO','SXRO','SYRO']\nd.columns = ['PlayId','XRD','YRD','SXD','SYD','SXRD','SYRD']\nprint(o.shape, d.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"od = o.merge(d, how='outer', on='PlayId')\nprint(od.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"od.shape[0]/23171","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"od.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"od['XOD'] = od.XRO - od.XRD\nod['YOD'] = od.YRO - od.YRD\nod['SXOD'] = od.SXO - od.SXD\nod['SYOD'] = od.SYO - od.SYD\nprint(od.head, od.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"od.drop(['PlayId','XRO','YRO','SXO','SYO','SXRO','SYRO'], axis=1, inplace=True)\nprint(od.shape, od.shape[0]/110)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"od.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"od.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# cols = od.columns\n# scaler = preprocessing.StandardScaler()\n# od = scaler.fit_transform(od)\n# od = np.nan_to_num(od)\n# od = pd.DataFrame(od, columns=cols)\n# od.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# reshape to 4d: play, off, def, feature\nx4 = od.values.reshape(-1,10,11,od.shape[1])\nprint(x4.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# x4 = x4.transpose(0, 2, 1, 3)\n# print(x4.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"x4[0,0,:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# %%time\n# if build_data:\n#     X0 = cruncher0(train_df, mode='train', verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"np.isnan(x4).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"x4 = np.nan_to_num(x4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"np.isnan(x4).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"x4_train = x4.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# target\nn = len(train_df) // 22\nprint(n)\n\ny01_train = train_df[\"Yards\"][::22].values.copy()\n\ny0_train = np.zeros(shape=(n, 199))\nfor i,yard in enumerate(train_df['Yards'][::22]):\n    y0_train[i, yard+99:] = np.ones(shape=(1, 100-yard))\n\n\n# limit target range\ny = y01_train.copy()\nMIN = -10\nMAX = 35\n# MIN = -30\n# MAX = 50\ny[y < MIN] = MIN\ny[y > MAX] = MAX\ny -= MIN\n\nnum_class = MAX - MIN + 1\n\ny_train = np.zeros(shape=(n, num_class))\nfor i, yard in enumerate(y):\n    y_train[i, yard:] = np.ones(shape=(1, num_class-yard))\n\ny1_train = y\n\n# y_train = np.zeros(len(y_train_),dtype=np.float)\n# for i in range(len(y_train)):\n#     y_train[i] = (y_train_[i])\n\n# scaler = preprocessing.StandardScaler()\n# scaler.fit([[y] for y in y_train])\n# y_train = np.array([y[0] for y in scaler.transform([[y] for y in y_train])])\n\nprint(y0_train.shape, y01_train.shape)\nprint(y_train.shape, y1_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# y_true is a vector of scalars and y_pred cdfs\ndef crps0(y_true, y_pred):\n    ans = 0\n    for i, y in enumerate(y_true):\n        h = np.zeros(199)\n        yf = int(np.floor(y))\n        h[(yf+99):] = 1.0\n                \n        ans += mean_squared_error(h, y_pred[i])\n        \n    return ans / (len(y_true))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# enforce monotonicity\ndef mono(p):\n    for pred in p:\n        prev = 0\n        for i in range(len(pred)):\n            if pred[i] < prev:\n                pred[i] = prev\n            prev = pred[i]\n    return p","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"lgb_params = {\n#     'device': 'gpu',\n    'objective':'regression_l1',\n#     'is_unbalance': True,\n    'boosting_type':'gbdt',\n    'metric': 'l1',\n    'n_jobs': -1,\n    'learning_rate': 0.01,\n    'num_leaves': 2**6,\n    'max_depth': 4,\n    'tree_learner':'serial',\n    'colsample_bytree': 0.7,\n#     'subsample_freq': 1,\n    'subsample': 0.7,\n    'max_bin': 255,\n    'verbose': -1,\n    'seed': 123,\n} \n\n# parallelize lgb predictor\n# def gb_cox(tr_yc, v):\n#     cf = cumfreq(tr_yc + v, numbins=199, defaultreallimits=(-99,100))\n#     return cf.cumcount / len(tr_yc) \n\ndef gb_cox(tr_yc, v):\n    cf = cumfreq(tr_yc + v, numbins=num_class, defaultreallimits=(0,num_class))\n    return cf.cumcount / len(tr_yc) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# adjust predictions by modified yardline\ndef adjusty(p, df, y_true=None, reduced=True):\n    n = len(p)\n#     p = np.cumsum(p, axis=1)\n#     p = np.clip(p, 0, 1)\n    if reduced:\n        pred = np.zeros((n, 199))        \n        pred[:, (99+MIN):(100+MAX)] = p\n        pred[:, 100+MAX:] = 1\n    else:\n        pred = p\n    cdf = pred.copy()\n    for i in range(0,n):\n        r = i*22\n        y = df[\"YardLine\"].iloc[r] - 10\n        \n        if y < 99: cdf[i,:(100-y-1)] = 0\n        if y > 1: cdf[i,-(y-1):] = 1\n                \n        # check for improvement, should never be worse\n        if y_true is not None:\n            mse_orig = mean_squared_error(y_true[i], pred[i])\n            mse_new = mean_squared_error(y_true[i], cdf[i])\n            if (mse_new > mse_orig):\n                print('adjusty inconsistency', i, df[\"FieldPosition\"].iloc[r],\n                      df[\"PossessionTeam\"].iloc[r],\n                      df[\"YardLine\"].iloc[r], y, df[\"Yards\"].iloc[r], mse_orig, mse_new)\n                print(y_true[i])\n                print(pred[i])\n                print(cdf[i])\n                break\n            \n    return cdf","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# adjust predictions by modified yardline\ndef adjusty2(p, yardline, y_true=None, reduced=True):\n    n = len(yardline)\n    if reduced:\n#         p = np.cumsum(p, axis=1)\n#         p = np.clip(p, 0, 1)\n        pred = np.zeros((n, 199))        \n        pred[:, (99+MIN):(100+MAX)] = p\n        pred[:, 100+MAX:] = 1\n    else:\n        # pred = np.clip(p, 0, 1)\n        pred = p\n    cdf = pred.copy()\n    for i in range(0,n):\n        y = yardline[i]\n        \n        if y < 99: cdf[i,:(100-y-1)] = 0\n        if y > 1: cdf[i,-(y-1):] = 1\n                \n        # check for improvement, should never be worse\n        if y_true is not None:\n            mse_orig = mean_squared_error(y_true[i], pred[i])\n            mse_new = mean_squared_error(y_true[i], cdf[i])\n            if (mse_new > mse_orig):\n                print('adjusty2 inconsistency', i, y, mse_orig, mse_new)\n                print(y_true[i])\n                print(pred[i])\n                print(cdf[i])\n                break\n            \n    return cdf","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def permutation_importance(X, y, model, func, better='smaller', nrep=5): \n    perm = {}\n    pred = model.predict(X)\n    baseline = func(y, pred)\n    print('\\nPermutation Importance Baseline Score', baseline)\n    for i, c in enumerate(X.columns):\n        values = X[c].values.copy()\n        dtype = X[c].dtype.name\n        score = 0.0\n        for r in range(nrep):\n            X[c] = np.random.permutation(values)\n            X[c] = X[c].astype(dtype) \n            pred = model.predict(X)\n            score = score + func(y, pred)\n        if better=='smaller':\n            perm[c] = score/nrep - baseline\n        else:\n            perm[c] = baseline - score/nrep\n        X[c] = values.copy()\n        X[c] = X[c].astype(dtype) \n        print(f'{i} {perm[c]:11.8f} {c}')\n    \n    df = pd.DataFrame.from_dict(perm, orient='index').reset_index()\n    df.columns = ['Feature','Perm']\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# feature list and X_list assumed to be nested lists of same sizes, X_list contains numpy arrays\ndef permutation_importance_list(feature_list, X_list, y, yardline, model, func, better='smaller', nrep=5): \n    perm = {}\n    p = model.predict(X_list)\n    p = mono(p)\n    pred = adjusty2(p, yardline, y_true=y)\n    baseline = func(y, pred)\n    print('\\npermutation importance baseline score', baseline)\n    for feat, X in zip(feature_list, X_list):\n        if len(feat) == 0: continue\n        for c, f in enumerate(feat):\n            values = X[...,c].copy()\n            score = 0.0\n            for r in range(nrep):\n                X[...,c] = np.random.permutation(values)\n                p = model.predict(X_list)\n                p = mono(p)\n                pred = adjusty2(p, yardline, y_true=y)\n                score = score + func(y, pred)\n            if better=='smaller':\n                perm[f] = score/nrep - baseline\n            else:\n                perm[f] = baseline - score/nrep\n            X[...,c] = values.copy()\n            print(f'{c} {perm[f]:.7f} {f}')\n    \n    df = pd.DataFrame.from_dict(perm, orient='index').reset_index()\n    df.columns = ['Feature','Perm']\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"keras.backend.clear_session()\nimport keras.backend as K\ndef crps(y_true, y_pred):\n    loss = K.mean((K.cumsum(y_pred, axis = 1) - y_true)**2)\n    return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# https://www.kaggle.com/c/nfl-big-data-bowl-2020/discussion/119400#latest-683614\nkeras.backend.clear_session()\n\ndef build_model(inp1, inp2, inp3, units=128, print_summary=False):\n    \n    keras.backend.clear_session()\n    gc.collect()\n    \n    # inputs\n    inputs = keras.layers.Input(shape=(inp1,inp2,inp3))\n    \n    # 4D\n    x = keras.layers.Conv2D(128,(1,1),activation='relu')(inputs)\n    x = keras.layers.Conv2D(160,(1,1),activation='relu')(x)\n    x = keras.layers.Conv2D(128,(1,1),activation='relu')(x)\n    a = keras.layers.AveragePooling2D(pool_size=(inp1,1))(x)\n    a = keras.layers.Lambda(lambda x1 : x1*0.7)(a)\n    m = keras.layers.MaxPooling2D(pool_size=(inp1,1))(x)\n    m = keras.layers.Lambda(lambda x1 : x1*0.3)(m)\n    x = keras.layers.Add()([a,m])\n    x = keras.layers.Reshape((inp2,units))(x)\n    x = keras.layers.BatchNormalization()(x)\n\n    # 3D\n    x = keras.layers.Conv1D(160,(1),activation='relu')(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Conv1D(96,(1),activation='relu')(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Conv1D(96,(1),activation='relu')(x)\n    x = keras.layers.BatchNormalization()(x)\n    a = keras.layers.AveragePooling1D(pool_size=inp2)(x)\n    m = keras.layers.MaxPooling1D(pool_size=inp2)(x)\n    x = keras.layers.Average()([a,m])\n    x = keras.layers.Flatten()(x)\n\n    # 2D\n    x = keras.layers.Dense(96, activation='relu')(x)\n    x = keras.layers.BatchNormalization()(x)\n#     x = keras.layers.Dropout(0.05)(x)  \n    x = keras.layers.Dense(256, activation='relu')(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Dropout(0.3)(x)\n    \n    x = keras.layers.Dense(num_class, activation='sigmoid')(x)\n#     x = keras.layers.Dense(num_class, activation='softmax')(x)\n    \n    model = keras.models.Model(inputs = [inputs], outputs = [x])\n    \n    opt = keras.optimizers.Adam(learning_rate=2e-3, beta_1=0.9, beta_2=0.999, amsgrad=False)\n    model.compile(optimizer=opt, loss='mse')\n    \n#     model.compile(optimizer='adam', loss='mse')\n#     model.compile(optimizer='sgd', loss='mse')\n#     model.compile(optimizer='adam', loss=crps)\n    \n    if print_summary: print(model.summary())\n        \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.reset_index(drop=True, inplace=True)\nprint(df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df1 = df[::22].reset_index(drop=True)\nprint(df1.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\nif w_holdout:\n\n#     train_df['week'] = train_df.groupby('PossessionTeam')['GameId'].rank(method='dense')\n#     print(train_df['week'].describe())\n#     dfw = train_df[['GameId','week']][::22].copy().reset_index(drop=True)\n    y_pred = np.zeros((df.shape[0], 199)) \n\n#     if perm: nrepw = 1\n#     else: nrepw = 10\n    nrepw = 10\n    nepoch = 100\n    batch_size = 64\n    units = [199] * nrepw\n    max_depths = [5] * nrepw\n\n    ncores = multiprocessing.cpu_count()\n\n    if perm:\n        os.makedirs('imp',exist_ok=True)\n\n    # collect modeling results in these lists\n    models_nn = []\n    models_lgb = []\n    models_lgb_bi = []\n    models_tr_yc = []\n    models_w = []\n    models_h = []\n\n    ecdfs = []\n    escores = []\n    bscores = []\n    bscorew = []\n    bscorea = []\n    vscores = []\n\n    n = len(y1_train)\n    print('nn train shape', x4_train.shape)\n    # print('lgb train shape', X_train1.shape)\n    # nn_features = list(X_train0.columns)\n    # lgb_features = list(X_train1.columns)\n    os.makedirs('imp', exist_ok=True)\n    first = True\n\n    for nfold in [1]:\n\n        # kfold = KFold(n_splits=nfold, shuffle=False)\n\n        # kfold = GroupKFold(n_splits=K)\n        # groups = train_df['GameId'][::22]\n\n        # groups = 10 * train_df['Season'][::22] + train_df['Week'][::22]\n\n        # kfold = StratifiedKFold(n_splits = K, \n        #                             random_state = 231, \n        #                             shuffle = True)    \n\n\n        # full_val_preds = np.zeros((n))\n        full_val_preds = np.zeros((n,199))\n\n        # test_preds = np.zeros((np.shape(X_test)[0],K))\n\n        # for f, (f_ind, outf_ind) in enumerate(kfold.split(X_train, y_train)):\n        # for f, (f_ind, outf_ind) in enumerate(kfold.split(X_train, y_train, groups=groups)):\n        for f in range(nfold):\n#             f_ind = df[~df.week.between(30, 32)].index\n#             outf_ind = df[df.week.between(30, 32)].index\n            f_ind = df1[df1.GameId < 2019110000].index\n            outf_ind = df1[df1.GameId >= 2019110000].index\n            print(len(f_ind), len(outf_ind))\n\n            x4_train_f, x4_val_f = x4_train[f_ind].copy(), x4_train[outf_ind].copy()\n            y_train_f, y_val_f = y_train[f_ind], y_train[outf_ind]\n            y1_train_f, y1_val_f = y1_train[f_ind], y1_train[outf_ind]\n            y0_train_f, y0_val_f = y0_train[f_ind], y0_train[outf_ind]\n            y01_train_f, y01_val_f = y01_train[f_ind], y01_train[outf_ind]\n    #         sw_f = sw[f_ind] \n\n            # shuffle data\n            idx = np.arange(len(y_train_f))\n            np.random.shuffle(idx)\n        #     X_train_f = X_train_f[idx]\n            y_train_f = y_train_f[idx]\n            y1_train_f = y1_train_f[idx]\n            y0_train_f = y0_train_f[idx]\n            y01_train_f = y01_train_f[idx]\n            x4_train_f = x4_train_f[idx]\n        #     y_train_f = y_train_f.iloc[idx]\n\n            # track oof prediction for cv scores\n            val_preds = 0\n            vi = np.array([np.array([v*22 + i for i in range(22)]) for v in outf_ind]).flatten()\n            di = train.iloc[vi].copy()\n            di = di.reset_index(drop=True)\n\n            # ecdf, to be ensembled with nn prediction, kind of a cox neural net model\n            nt = len(y1_train_f)\n            nv = len(y1_val_f)\n            cf = cumfreq(y1_train_f, numbins=199, defaultreallimits=(-99,100))\n            ecdf = cf.cumcount / nt\n            ecdfs.append(ecdf)\n            ecdfr = ecdf.repeat(nv).reshape(199,nv).transpose()\n            escore = mean_squared_error(y0_val_f, ecdfr)\n\n            print('')\n            print('*'*10)\n            print(f'Fold {f+1}/{nfold}')\n            print('*'*10)\n\n            print('')\n            print(f'escore {escore:.6f}')\n            escores.append(escore)\n\n            for j in range(nrepw):\n\n                print('')\n                print(f'Rep {j+1}/{nrepw}')\n\n                model= build_model(x4_train.shape[1], x4_train.shape[2], x4_train.shape[3],\n                    print_summary=first)\n                if first: first = False\n\n                es = EarlyStopping(monitor='val_loss', \n                   mode='min',\n                   restore_best_weights=True, \n                   verbose=2, \n                   patience=patience)\n                es.set_model(model)\n                \n                lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n                                                       patience=10, verbose=2, mode='min',\n                                                       min_delta=0.00001)\n                \n#                 oc = OneCycleLR(0.0005)\n\n                h = model.fit(x4_train_f, y_train_f, epochs=nepoch,\n                          # sample_weight=sw_f,\n                          # default batch_size is 32\n                          batch_size=batch_size,\n                          callbacks=[es, lr],\n                          # large batch sizes tend to perform poorly\n        #                   batch_size=2**(j+10),\n                          validation_data=(x4_val_f, y_val_f),\n                          verbose=2)\n\n                models_nn.append(model)\n#                 models_w.append(0.5 / len(fold_list) / nfold / nrep)\n                models_h.append(h)\n\n                vp = model.predict(x4_val_f)\n                vp = mono(vp)\n                vp = adjusty(vp, di, y0_val_f)\n                vs = mean_squared_error(y0_val_f,vp)\n                print(f'nn crps {vs:.6f}')\n\n#                 # lgb\n#                 # print('')\n#                 tr_data = lgb.Dataset(X_train_f1, label=y1_train_f)\n#                 vl_data = lgb.Dataset(X_val_f1, label=y1_val_f) \n#                 # vary max_depth with rep\n#                 lgb_params['max_depth'] = max_depths[j]\n#                 lgb_params['seed'] = 123 + j\n#                 clf = lgb.train(lgb_params, tr_data, valid_sets=[tr_data, vl_data],\n#                                 num_boost_round=20000, early_stopping_rounds=100,\n#                                 verbose_eval=0)\n#                 models_lgb.append(clf)\n#                 models_lgb_bi.append(clf.best_iteration)\n\n#                 vpl = clf.predict(X_val_f1, num_iteration=clf.best_iteration)\n#                 # lgb cox model, shift ecdf so its median is at lgb point prediction\n#                 tr_yc = y1_train_f - np.median(y1_train_f)\n#                 models_tr_yc.append(tr_yc)\n#                 cl = Parallel(n_jobs=ncores)(delayed(gb_cox)(tr_yc,v) for v in vpl)\n#                 c = np.concatenate(cl).reshape(-1,num_class)\n#                 c = mono(c)\n#                 c = adjusty(c, di, y0_val_f)\n#                 print(f'lgb crps {mean_squared_error(y0_val_f,c):.6f}')\n\n#                 # nonnegative least squares to estimate ensemble weights\n#                 b = y0_val_f.flatten()\n#                 A = np.zeros((len(b),3))\n#                 A[:,0] = vp.flatten()\n#                 A[:,1] = c.flatten()\n#                 A[:,2] = ecdfr.flatten()\n#                 bestw = nnls(A,b)[0]\n#                 besta = np.matmul(A,bestw).reshape(-1,199)\n#                 besta = mono(besta)\n#                 besta = adjusty(besta, di, y0_val_f, reduced=False)\n#                 bscore = bests = mean_squared_error(y0_val_f, besta)\n\n#                 # print('')        \n#                 print(f'bscore {bests:.6f} {bestw}')\n#                 bscores.append(bests)\n#                 bscorew.append(bestw)\n#                 bscorea.append(besta)\n\n#                 val_preds += besta / nrepw\n\n                bscores.append(vs)\n                val_preds += vp / nrepw\n\n                # test_preds[:,f] += model.predict(proc_X_test_f)[:,0] / nrep\n\n\n                if perm:\n                    ff = str(nfold) + '_' + str(f+1)\n                    \n#                     feature_imp = pd.DataFrame(zip(lgb_features, clf.feature_importance(),\n#                                                    clf.feature_importance(importance_type='gain')),\n#                                                    columns=['Feature','Splits'+ff,'Gain'+ff])\n                    \n#                     perm_imp = permutation_importance(X_val_f1,\n#                                                       y1_val_f, clf,\n#                                                       mean_absolute_error)\n#                     perm_imp.columns = ['Feature','Perm'+ff]\n#                     feature_imp = feature_imp.merge(perm_imp, how='left', on='Feature')\n\n                    yardline = train_df['YardLine'][::22].values - 10\n                    perm_imp = permutation_importance_list([list(od.columns)],\n                                                           [x4_val_f],\n                                                           y0_val_f, yardline[outf_ind],\n                                                           model, mean_squared_error)\n\n#                     perm_imp = permutation_importance(X_val_f0,\n#                                                       y_val_f, model,\n#                                                       mean_squared_error)\n\n                    perm_imp.columns = ['Feature','PermNN'+ff]\n                    perm_imp = perm_imp.sort_values(by='PermNN'+ff, ascending=False).reset_index(drop=True)\n                    print()\n                    print(perm_imp.head(n=50))\n#                     print()\n#                     print(perm_imp.tail(n=50))\n    \n                    # feature_imp = feature_imp.merge(perm_imp, how='left', on='Feature')\n\n#                     feature_imp.sort_values(by='Splits'+ff, inplace=True, ascending=False)\n#                     print('')\n#                     print(feature_imp.head(n=10))\n\n#                     feature_imp.sort_values(by='Gain'+ff, inplace=True, ascending=False)\n#                     print('')\n#                     print(feature_imp.head(n=10))\n\n#                     feature_imp.sort_values(by='Perm'+ff, inplace=True, ascending=False)\n#                     print('')\n#                     print(feature_imp.head(n=15))\n\n#                     feature_imp.sort_values(by='PermNN'+ff, inplace=True, ascending=False)\n#                     print('')\n#                     print(feature_imp.head(n=15))\n\n#                     print(feature_imp.shape)\n\n#                     fname = 'imp/' + mname + '_imp' + ff + '.csv'\n#                     perm_imp.to_csv(fname, index=False)\n#                     print(fname, feature_imp.shape)\n\n                gc.collect()\n\n            val_preds = mono(val_preds)\n            val_preds = adjusty(val_preds, di, y0_val_f, reduced=False)\n            full_val_preds[outf_ind] += val_preds\n            vscore = mean_squared_error(y0_val_f, val_preds)\n            print(f'\\nvscore {vscore:.6f}')\n            vscores.append(vscore)\n\n        #     if f == 0: break\n\n        nfh = int(np.ceil(nfold / 2))\n        nfq = int(np.ceil(nfold / 4))\n\n        print('')\n        print(f'\\nAll bscores {np.array(bscores)}')\n        print('Mean bscores: %.6f' % np.mean(bscores))\n        print('Mean vscores: %.6f' % np.mean(vscores))\n    #         print('Mean vscores last half: %.6f' % np.mean(vscores[-nfh:]))\n    #         print('Mean vscores last quar: %.6f' % np.mean(vscores[-nfq:]))\n    #     print('Mean ecdf weights last half: %.6f' % np.mean(bscorew[-nfh*nrep:]))\n    #     print('Mean ecdf weights last quar: %.6f' % np.mean(bscorew[-nfq*nrep:]))\n#         print(f'\\nAll bscores {np.array(bscores)}')\n        # print(f'\\nAll vscores {np.array(vscores)}')\n#         print(f'\\nAll lgb iters {np.array(models_lgb_bi)}')\n#         print(f'\\nAll ecdf weights {bscorew}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"nbformat":4,"nbformat_minor":1}