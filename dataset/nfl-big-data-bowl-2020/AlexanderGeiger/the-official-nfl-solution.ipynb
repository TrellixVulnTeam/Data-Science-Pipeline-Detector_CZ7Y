{"cells":[{"metadata":{},"cell_type":"markdown","source":"# NFL Model Solution\n--------------------------------------------\nThe following notebook take a unique aproach to answer the question how many yard will an NFL running back obtain. The following will discuss the innovative features that brought the model to life.\n\n### Feature Engineering\n* Angle arithmetic, transformations and augmentation\n* Individual defender features\n* Normalizing coordates to minimize search space\n* Mathmatically modeling the motion of players\n* Modelling Pitch Control\n\n\n\n### Predictive Modeling Ensemble\n* Pytorch deep learning NN, sigmoid activation output \n* Keras  deep learning NN, softmax output\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# General Packages \nimport numpy as np, pandas as pd, random as rn, scipy as sp\nimport warnings, os, math, datetime, codecs, re, gc, time, random, torch\nimport matplotlib.pyplot as plt\n\n\n# sklearn packages\nimport sklearn.metrics as mtr\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import KFold,GroupKFold, train_test_split\nfrom sklearn.metrics import f1_score, mean_squared_error\n\n# kera packages\nimport keras.callbacks as keras_call\nfrom keras.callbacks import Callback, ModelCheckpoint\nfrom keras.models import Model, Sequential, load_model\nfrom keras.losses import binary_crossentropy\nfrom keras.utils import to_categorical\nimport keras.backend as K\n\n# keras layer imports\nfrom keras.layers import Input, Dense, Concatenate, Reshape, Dropout\nfrom keras.layers import Flatten, Dropout, multiply, Lambda, merge, Add\nfrom keras.layers import GaussianDropout, GaussianNoise, BatchNormalization\nfrom keras.layers.embeddings import Embedding\n\n# lgbm & tensorflow packages\nimport lightgbm as lgb\nimport tensorflow as tf\n\n# import torch functions\nfrom torch.utils.data import TensorDataset, DataLoader\nimport torch.nn.functional as F\nimport torch.nn as nn\n\n\n# get euclidean distance\ndef euclidean_distance(x1,y1,x2,y2):\n    return np.sqrt((x1-x2)**2 + (y1-y2)**2)\n\n# get direction of back\ndef back_direction(orientation):\n    if orientation > 180.0: return 1\n    else:                   return 0\n\ndef strtofloat(x):\n    try:    return float(x)\n    except: return -1\n\ndef orientation_to_cat(x):\n    x = np.clip(x, 0, 360 - 1)\n    try:    return str(int(x/15))\n    except: return \"nan\"\n\ndef sigmoid(x,a,b):\n    return sp.special.expit(a*(b-x))\n\ndef get_ypred_score(y_pred,y_true):\n    return ((y_true - y_pred) ** 2).sum(axis=1).sum(axis=0) / (199 * y_true.shape[0])\n    \ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nwarnings.filterwarnings(\"ignore\")\n\ntry:\n    from kaggle.competitions import nflrush\n    env = nflrush.make_env()\n    iter_test = env.iter_test()\nexcept Exception as e:\n    print(str(e))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Output Transformations\n_______________________\nThe following definitions apply transformations to the networks output layer. The function `keras_transform` calculates the emperical cummulative distribution of a probability distribution. The function `pytorch_transform` uses a cummulative maximization to ensures that every element is greater than those which proceed it.\n* Softmax Layer Output\n* Sigmoid Layer Output"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Softmax layer output transformation\ndef keras_transform(y):\n    return np.clip(np.cumsum(y, axis=1), 0, 1)\n\n# Sigmoid layer output transformation\ndef pytorch_transform(preds):\n    y_pred = preds.copy()\n    adjust_preds = np.zeros((len(y_pred), y_pred.shape[1]))\n    for idx, pred in enumerate(y_pred):\n        prev = 0\n        for i in range(len(pred)):\n            if pred[i]<prev:\n                pred[i]=prev\n            prev=pred[i]\n        adjust_preds[idx, :] = (pred-np.min(pred))/(np.max(pred)-np.min(pred))\n    adjust_preds[:, -1] = 1\n    adjust_preds[:, 0] = 0\n    return adjust_preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Keras Callback Evalulation\n_________________\nThe follow class is used for evaluating ther keras model on batch end and training begin. The definition `get_model_score` is used to evaluate a softmax layer of a keras model. Note: A sigmoid output layer transformation function is different if it is a softmax."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"def get_model_score(model,x,y):\n    y_pred  = model.predict(x)\n    y_true  = np.clip(np.cumsum(y, axis=1), 0, 1)\n    y_pred  = np.clip(np.cumsum(y_pred, axis=1), 0, 1)\n    return ((y_true - y_pred) ** 2).sum(axis=1).sum(axis=0) / (199 * y_true.shape[0])\n\nclass CRPSCallback(Callback):\n    \n    def __init__(self,validation, predict_batch_size=20, include_on_batch=False):\n        super(CRPSCallback, self).__init__()\n        self.validation = validation\n        self.predict_batch_size = predict_batch_size\n        self.include_on_batch = include_on_batch\n        print('validation shape',len(self.validation))\n\n    def on_batch_begin(self, batch, logs={}):\n        pass\n\n    def on_train_begin(self, logs={}):\n        if not ('CRPS_score_val' in self.params['metrics']):\n            self.params['metrics'].append('CRPS_score_val')\n\n    def on_batch_end(self, batch, logs={}):\n        if (self.include_on_batch):\n            logs['CRPS_score_val'] = float('-inf')\n\n    def on_epoch_end(self, epoch, logs={}):\n        logs['CRPS_score_val'] = float('-inf')\n            \n        if (self.validation):\n            X_valid, y_valid = self.validation[0], self.validation[1]\n            val_s =  get_model_score(self.model,X_valid,y_valid)\n            val_s = np.round(val_s, 6)\n            logs['CRPS_score_val'] = val_s","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Keras Model\n________________________\nFor the keras model I used a a dual input layer to capture static and individual player data. Each of the inital hidden layers uses different activation function which includes relu, selu, sigmoid and a softmax layer. Gaussian noise and dropout are both used in the network. The noise is ment to augment player data while a gaussian dropout is used for the static data. To see the code that generates the model expand the following function."},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"def get_model(TR_DATA,TR_TARGET,VAL_DATA,VAL_TARGET, bsz = 1024, nfold=1):\n    \n    # setup column space & input layers\n    LEN0, LEN1 = TR_DATA[0].shape[1], TR_DATA[1].shape[1]\n    inp0, inp1 = Input(shape = (LEN0,)), Input(shape = (LEN1,))\n    \n    # define hidden layer one\n    x0 = Dense(512, input_dim=LEN0, activation='relu')(inp0)\n    x1 = Dense(512, input_dim=LEN1, activation='selu')(inp1)\n    \n    # Augment the static hidden Layer 1\n    x0 = GaussianDropout(0.375)(x0)\n    x0 = BatchNormalization()(x0)\n    \n    # Augment the player intuition hidden layer 1\n    x1 = GaussianNoise(0.7)(x1)\n    x1 = BatchNormalization()(x1)\n    x1 = GaussianDropout(0.25)(x1)\n    x1 = BatchNormalization()(x1)\n    \n    # concatentate the static and intution hidden layers\n    x = Concatenate(axis=1)([x0,x1])\n    \n    # pass the concatenated layers through hidden layer 2\n    x = Dense(256, activation='relu')(x)\n    \n    # apply gaussian drop to hidden layer 2\n    x = GaussianDropout(0.5)(x)\n    x = BatchNormalization()(x)\n    \n    # pass data to hidden layer hidden layer 3\n    x = Dense(256, activation='sigmoid')(x)\n    \n    # augment model results\n    x = GaussianDropout(0.5)(x)\n    x = BatchNormalization()(x)\n    \n    # pass results to the output layer\n    out = Dense(199, activation='softmax')(x)\n    model = Model([inp0,inp1],out)\n    \n    # setup model call backs\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[])\n    es = keras_call.EarlyStopping(monitor='CRPS_score_val',  mode='min',  restore_best_weights=True, verbose=False,  patience=10)\n    mc = ModelCheckpoint('best_model.h5',monitor='CRPS_score_val',mode='min', save_best_only=True, verbose=False, save_weights_only=True)\n    \n    # train model\n    model.fit(TR_DATA, TR_TARGET,callbacks=[CRPSCallback(validation = (VAL_DATA,VAL_TARGET)),es,mc], epochs=100, batch_size=bsz,verbose=False)\n    model.load_weights(\"best_model.h5\")\n    \n    # calculate feature scores\n    tr_s  = np.round(get_model_score(model,TR_DATA,TR_TARGET),6)\n    val_s = np.round(get_model_score(model,VAL_DATA,VAL_TARGET),6)\n    \n    return model,val_s,tr_s","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pytorch Model Evaluation\n_____________________\nThe following code has two classes and a model prediction definition. The class names are `EarlyStoppingIV` and `NFL_NN` for the model definition. To define the Pytorch model and model stopping crietera. To generate model predictions use the definition `model_eval`.\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# pytorch Neural network\nclass NFL_NN(nn.Module):\n    def __init__(self, in_features):\n        super().__init__()\n        \n        # first layer\n        self.fc1 = nn.Linear(in_features, 256)\n        self.bn1 = nn.BatchNorm1d(256)\n        self.relu1 = nn.CELU()\n        \n        # second layer\n        self.dout2 = nn.Dropout(0.5)\n        self.lin2    = nn.Linear(256,512)\n        self.relu2 = nn.ReLU()\n        \n        self.dout3 = nn.Dropout(0.25)\n        self.fc3 = nn.Linear(512, 256)\n        self.relu3 = nn.ReLU()\n        \n        self.dout4 = nn.Dropout(0.25)\n        self.out = nn.Linear(256, 199)\n        self.out_act = nn.Sigmoid()\n        \n    def forward(self, input_):\n        \n        # input & first layer\n        a1  = self.fc1(input_)\n        bn1 = self.bn1(a1)\n        h1  = self.relu1(bn1)\n        \n        d2 =self.dout2(h1)\n        f2 = self.lin2(d2)\n        a2 = self.relu2(f2)\n        \n        d3 = self.dout3(a2)\n        a3 = self.fc3(d3)\n        h3 = self.relu3(a3)\n        \n        d4 = self.dout4(h3)\n        a5 = self.out(d4)\n        y = self.out_act(a5)\n        return a5\n    \nclass EarlyStoppingIV:\n    def __init__(self, patience=2, verbose=False):\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n\n    def __call__(self, val_loss, model, save_name):\n        score = -val_loss\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model, save_name)\n        elif score < self.best_score:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model, save_name)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model, save_name):\n        torch.save(model.state_dict(), save_name)\n        self.val_loss_min = val_loss\n        \n        \n# pytorch model evaluation\ndef model_eval(model, dataset, data_loader, batch_size):\n    model.eval()\n    preds = np.zeros((len(dataset), 199))\n    with torch.no_grad():\n        for i, eval_x_batch in enumerate(data_loader):\n                eval_values = eval_x_batch[0].float()\n                pred = model(eval_values)\n                preds[i * batch_size:(i + 1) * batch_size] = pred\n    return preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pytorch Model Algorithm\nThe following code features the pytorch training algorithm `train_pytorch_model`. Inorder to train the data you must create special data loading objects which are created in the definition `generate_dataloader`."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def generate_dataloader(df, y_val, batch_size, tr_ix, val_ix):\n    \n    # create training data & datasets\n    tr_x, tr_y = torch.from_numpy(df.iloc[tr_ix].values), torch.from_numpy(y_val[tr_ix])\n    val_x, val_y = torch.from_numpy(df.iloc[val_ix].values), torch.from_numpy(y_val[val_ix] )\n    tr_dataset, val_dataset = TensorDataset(tr_x, tr_y), TensorDataset(val_x, val_y)\n\n    # create data loaders\n    tr_loader = DataLoader(tr_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    return tr_dataset, val_dataset, tr_loader, val_loader\n\n\ndef train_pytorch_model(train_dataset, valid_dataset, train_loader, valid_loader,torch_feat_len,epoch,batch_size,n_fold):\n    \n    # setup model\n    early_stopping = EarlyStoppingIV(patience=15, verbose=False)\n    model          = NFL_NN(torch_feat_len)\n    criterion      = nn.MSELoss()\n    optimizer      = torch.optim.Adam(model.parameters(), lr=0.003)\n    out_features   = 199\n    \n    # loop through each epoch\n    for idx in range(epoch):\n        train_batch_loss_sum = 0\n        for param in model.parameters():\n            param.requires_grad = True\n\n        model.train()\n        for x_batch, y_batch in train_loader:\n            y_pred = model(x_batch.float())\n            loss = torch.sum((y_pred.float()-y_batch.view((len(y_batch), out_features)).float()).pow(2))/(199*len(y_pred))\n            train_batch_loss_sum += loss.item()\n\n            del x_batch\n            del y_batch\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            torch.cuda.empty_cache()\n            gc.collect()\n\n        train_epoch_loss = train_batch_loss_sum / len(train_loader)\n        valid_y_pred = model_eval(model, valid_dataset, valid_loader, batch_size)\n        valid_crps = np.sum(np.power(valid_y_pred - valid_dataset[:][1].data.cpu().numpy(), 2))/(199*len(valid_dataset))\n        model_save_name = 'checkpoint_fold_{}.pt'.format(n_fold+1)\n        early_stopping(valid_crps, model, model_save_name)\n        \n        \n        nfl_pred   = pytorch_transform(model_eval(model, train_dataset, train_loader, batch_size))\n        train_crps = np.sum(np.power(nfl_pred     - train_dataset[:][1].data.cpu().numpy(), 2))/(199*len(train_dataset))\n        \n        if early_stopping.early_stop:\n            break\n\n    # create model & load model with weights\n    model = NFL_NN(torch_feat_len)\n    model.load_state_dict(torch.load(model_save_name))\n\n    nfl_pred       = pytorch_transform(model_eval(model, train_dataset, train_loader, batch_size))\n    valid_y_pred   = pytorch_transform(model_eval(model, valid_dataset, valid_loader, batch_size))\n    \n    valid_crps = np.sum(np.power(valid_y_pred - valid_dataset[:][1].data.cpu().numpy(), 2))/(199*len(valid_y_pred))\n    train_crps = np.sum(np.power(nfl_pred     - train_dataset[:][1].data.cpu().numpy(), 2))/(199*len(train_dataset))\n    \n    print('Offical Epoch Loss: {:.5f}, Valid CRPS: {:.5f}, Train CRPS: {:.5f}'.format(train_epoch_loss, valid_crps,train_crps))\n    del criterion, optimizer\n    gc.collect()\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering\n________________________\n## Angle Arithmetic\nThe following functions calculate the degree difference between two vectors, take the absolute angle relative to the x axis, and rotate an angle 180 degrees. The definitions to perform these calculations are `angleDiff`, `absoluteAngle`, `rotate_angle_180`"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def angleAddDeg(x,y):\n    if (x > 0)    & (y >= 0): return 0\n    elif (x < 0)  & (y <= 0): return 180\n    elif (x < 0) & (y > 0):   return -180\n    elif (x > 0) & (y < 0):  return -360\n    elif (x == 0) & (y > 0):  return 90\n    elif (x == 0) & (y > 0):  return 270\n    else: return np.nan\n\ndef getAngleDeg(x,y):\n    if (y == 0) | (x == 0): deg = 0\n    else: deg = abs(np.degrees(np.arctan(y/x)))\n    deg = abs(deg + angleAddDeg(x,y))\n    return deg\n\ndef angleDiff(x1,y1,x2,y2):\n    vec = np.array([getAngleDeg(x1,y1),getAngleDeg(x2,y2)])\n    MAX, MIN = np.max(vec), np.min(vec)\n    if (MAX - MIN) <= 180: return MAX - MIN\n    else: return 360 - MAX + MIN\n    \ndef rotate_angle_180(x):\n    if x < 180: return x + 180\n    else:       return  360 - (x + 180)\n    \ndef absoluteAngle(angle):\n    # map over angle\n    if angle > 180: return 360 - angle\n    else: return angle  \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Update Yard Line\nCall `update_yardline` to stream the definition `new_line` in a lambda function.\nnotes: The new yard line calculation has changed to normalize the field without including the endzones."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Calculate new yard line\ndef new_line(rush_team, field_position, yardline):\n    if rush_team == field_position: return yardline + 0.0 \n    else: return 100.0 - yardline # half the field plus the yards between midfield and the line of scrimmage\n    \n# update yard line\ndef update_yardline(df):\n    new_yardline = df[df['NflId'] == df['NflIdRusher']]\n    new_yardline['YardLine'] = new_yardline[['PossessionTeam','FieldPosition','YardLine']].apply(lambda x: new_line(x[0],x[1],x[2]), axis=1)\n    return new_yardline[['GameId','PlayId','YardLine']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## X & Y Rotations with orientation adjustments\nThe following graph debuts the updated orientation translation function. The function performs far better when compared to it's predecessor because it performs a true 180 degree rotation.\n\n`new_X` flips the x direction, `new_YNEW` flips the y direction `new_orientation` updates the orientation of players. `new_orientation_II` is similar to the version one but allows for a complete symmetric flip.\nnotes: The 2017 orientation had been fixed. The actual Y, Orientation, and Direction of motion had been calculated and has been stored in the variable `YNEW`, `OrientationII`, `DirII` respectfully."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# generally used for plotting points\ndef PhysicsVecXY(v,angle):\n    endy = v* np.sin(np.radians(angle))\n    endx = v* np.cos(np.radians(angle))\n    return [endx],[endy]\n\ndef compassPlot(angle,ax=False,sub=0,color='k'):\n    u,v = PhysicsVecXY(1,angle)\n    ags, ri = np.arctan2(v, u), np.hypot(u, v)\n    if ax == False: ax = plt.subplot(331 + sub, projection='polar')\n    plt.scatter(0,0,color=color)\n    kw = dict(arrowstyle=\"->\", color=color,linewidth=3)\n    [ax.annotate(\"\",xy=(ag, rad),xytext=(0, 0),arrowprops=kw) for ag,rad in zip(ags, ri)]\n    ax.set_ylim(0, np.max(ri))\n    return ax\n\n# Rotate 90 deg based on year\ndef rotate_angle_90(angle,year):\n    if year != 2017: return angle\n    if angle > 270:  return 90 - 360 + angle\n    else:            return angle + 90\n\n# flip x coordinates\ndef new_X(x_coordinate, play_direction):\n    if play_direction == 'left': return 110.0 - x_coordinate\n    else:                        return x_coordinate - 10.0\n\n# flip y coordinates\ndef new_YNEW(y_coordinate, play_direction):\n    if play_direction == 'left': return 53.3 - y_coordinate\n    else:                        return y_coordinate\n\n# Non-actual Angle Transpose\ndef new_orientation(angle, play_direction):\n    if play_direction != 'left': return angle\n    new_angle = 360.0 - angle\n    if new_angle == 360.0: return 0.0\n    return new_angle\n\n# Actual Angle Transpose\ndef new_orientation_II(angle, play_direction):\n    if (angle - 90) <= 0: angleII = 90 - angle\n    else: angleII = 360 - angle + 90\n    if play_direction == 'left': return (180+angleII) % 360\n    else: return angleII\n    \n# update orientation\ndef update_orientation(df, yardline):\n    df['YNEW']          = df[['Y','PlayDirection']].apply(lambda x: new_YNEW(x[0],x[1]), axis=1)\n    df['X']             = df[['X','PlayDirection']].apply(lambda x: new_X(x[0],x[1]), axis=1)\n    df['OrientationII'] = df[['Orientation','PlayDirection']].apply(lambda x: new_orientation_II(x[0],x[1]), axis=1)\n    df['DirII']         = df[['Dir','PlayDirection']].apply(lambda x: new_orientation_II(x[0],x[1]), axis=1)\n    df['Orientation']   = df[['Orientation','PlayDirection']].apply(lambda x: new_orientation(x[0],x[1]), axis=1)\n    df['Dir']           = df[['Dir','PlayDirection']].apply(lambda x: new_orientation(x[0],x[1]), axis=1)\n    df                  = df.drop('YardLine', axis=1)\n    return pd.merge(df, yardline, on=['GameId','PlayId'], how='inner')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Orientation Angle Analysis\nThe 2017 orientation angle was different then in 2018 data. The following analysis convey that and solution to the problem."},{"metadata":{},"cell_type":"markdown","source":"### Orientation Angle Solution\nThe 2017 orientation angle was different then in 2018 data. The following analysis convey that and solution to the problem."},{"metadata":{},"cell_type":"markdown","source":"## Modeling The Physical World\nThese features provide the mathmatical equations forcast the motion of players on the field."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def getTimeDelta():\n    return .625\n\n# update X variable\ndef physicsX(x,v,a,angle,dt=0):\n    if dt == 0: dt = getTimeDelta()\n    projx = np.cos(np.radians(angle))      # get projection on x axis\n    return x + v*projx*dt + .5*a*projx*(dt**2) # calculate x' given the span of time dt\n\n# update Y variale\ndef physicsY(y,v,a,angle,dt=0):\n    if dt == 0: dt = getTimeDelta()\n    projy = np.sin(np.radians(angle))\n    return  y + v*projy*dt + .5*a*projy*(dt**2)\n\n# update velocity\ndef physicsVx(v,a,angle,dt=0):\n    if dt == 0: dt = getTimeDelta()\n    projx = np.cos(np.radians(angle))   \n    return v*projx + a*projx*dt\n\n# update velocity\ndef physicsVy(v,a,angle,dt=0):\n    if dt == 0: dt = getTimeDelta()\n    projy = np.sin(np.radians(angle)) # get projection on y axis\n    return v*projy + a*projy*dt\n\n# generally used for plotting points\ndef PhysicsXY(x,y,v,angle):\n    endy = y + v* np.sin(np.radians(angle))\n    endx = x + v* np.cos(np.radians(angle))\n    return [x,endx], [y,endy]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Count Positions On Field\ncount field positions by calling `personnel_features_II` "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def create_count(df,field):\n    df[field + '_Count']  = 0\n    df.loc[df['Position'] == field,field + '_Count'] = 1\n    df[field + '_Count']  = df.groupby('PlayId')[field + '_Count'].transform('sum')\n\ndef personnel_features_II(df):\n    for key in ['QB','CB','SS','NT','G','S','WR']:\n        create_count(df,key)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Defensive Player Features\nCreate new defensive play features with `defense_features` and `DefensePersonnelSplit`"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def DefensePersonnelSplit(x):\n    dic = {'DB' : 0, 'DL' : 0, 'LB' : 0, 'OL' : 0}\n    for xx in x.split(\",\"):\n        xxs = xx.split(\" \")\n        dic[xxs[-1]] = int(xxs[-2])\n    return dic\n\ndef defense_features(df):\n    calc = ['X','Y','RusherX','RusherY']\n    rusher = df[df['NflId'] == df['NflIdRusher']][['GameId','PlayId','Team','X','Y']]\n    rusher.columns = ['GameId','PlayId','RusherTeam','RusherX','RusherY']\n    defense = pd.merge(df,rusher,on=['GameId','PlayId'],how='inner')\n    defense = defense[defense['Team'] != defense['RusherTeam']][['GameId','PlayId','X','Y','RusherX','RusherY']]\n    defense['def_dist_to_back'] = defense[calc].apply(lambda x: euclidean_distance(x[0],x[1],x[2],x[3]), axis=1)\n    defense = defense.groupby(['GameId','PlayId']).agg({'def_dist_to_back':['min','max','mean','std']}).reset_index()\n    defense.columns = ['GameId','PlayId','def_min_dist','def_max_dist','def_mean_dist','def_std_dist']\n    return defense","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Defensive Players Field Position\nUse the function `defense_pro_features` to generate individual player features"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def defense_pro_features(df):\n\n    # Create Definsive Dataset\n    # ---------------------------------\n    \n    # get rusher\n    meta_data   = ['PlayDirection','Season','YardLine']\n    rush_fields = ['GameId','PlayId','RusherTeam','RusherX','RusherY','RushS','RushA','RushDir','RushOrient']\n    rusher = df[df['NflId'] == df['NflIdRusher']][['GameId','PlayId','Team','X','YNEW','S','A','DirII','OrientationII']]\n    rusher.columns = rush_fields\n    \n    # create table\n    DD = pd.merge(df,rusher,on=['GameId','PlayId'],how='inner')\n    DD = DD[DD['Team'] != DD['RusherTeam']][rush_fields + ['X','YNEW','OrientationII','DirII','S','A'] + meta_data]\n    \n    # Calculate new future points\n    DD['RusherFXV'] = physicsX(DD.RusherX,DD.RushS,DD.RushA,DD.RushDir)\n    DD['RusherFYV'] = physicsY(DD.RusherY,DD.RushS,DD.RushA,DD.RushDir)\n    DD['RusherOXV'] = physicsX(DD.RusherX,DD.RushS,DD.RushA,DD.RushOrient)\n    DD['RusherOYV'] = physicsY(DD.RusherY,DD.RushS,DD.RushA,DD.RushOrient)\n    \n    # measure rushers distance from yard line & map Defenders Acceleration\n    DD['RushDistYardLine'] = DD['RusherFXV'] - DD['YardLine']\n    \n    # calculate new defensive lineman points\n    DD['FXV'] = physicsX(DD.X,DD.S,DD.A,DD.DirII)\n    DD['FYV'] = physicsY(DD.YNEW,DD.S,DD.A,DD.DirII)\n\n    DD['RusherFXV'] = physicsX(DD.RusherX,DD.RushS,DD.RushA,DD.RushDir)\n    DD['RusherFYV'] = physicsY(DD.RusherY,DD.RushS,DD.RushA,DD.RushDir)\n    \n    \n    # calculate rusher velocity\n    DD['RusherVelocityX'] = physicsVx(DD.RushS,DD.RushA,DD.RushDir)\n    DD['RusherVelocityY'] = physicsVy(DD.RushS,DD.RushA,DD.RushDir)\n    \n    # calculate the defensive man's velocity\n    DD['VelocityX'] = physicsVx(DD.S,DD.A,DD.DirII)\n    DD['VelocityY'] = physicsVy(DD.S,DD.A,DD.DirII)\n    \n    # calculate change in velocities\n    DD['VelocityDeltaX'] = DD['VelocityX'] - DD['RusherVelocityX']\n    DD['VelocityDeltaY'] = DD['VelocityY'] - DD['RusherVelocityY']\n    DD['VelocityDelta']  = np.sqrt(DD['VelocityDeltaX']**2 + DD['VelocityDeltaY']**2)\n    \n    # Concept Features\n    # ---------------------------\n\n    # get new distance equations\n    DD['StartDistance']   = euclidean_distance(DD['X'],DD['YNEW'],DD['RusherX'],DD['RusherY'])\n    DD['RunnerHeaded']    = euclidean_distance(DD['FXV'],DD['FYV'],DD['RusherFXV'],DD['RusherFYV'])\n    DD['RunnerThinking']  = euclidean_distance(DD['FXV'],DD['FYV'],DD['RusherOXV'],DD['RusherOYV'])\n    \n    # Calculate Time Till Impact\n    # ---------------------------\n    DD['TimeTillImpact']   = DD['VelocityDelta'] / (DD['RunnerHeaded'] + .01)\n    DD['DistanceEstimate'] = DD['TimeTillImpact']*DD['RusherVelocityX'] + DD['YardLine'] + DD['RusherFXV'] - DD['RusherX']\n    \n    # Defender Assessing\n    # ---------------------------\n\n    # Defensive Frame, looking at Position\n    DD['RushStartNormX'] = (DD['RusherX'] - DD['X'])/DD['StartDistance']\n    DD['RushStartNormY'] = (DD['RusherY'] - DD['YNEW'])/DD['StartDistance']\n\n    # Defensive Frame, Defender Assessing\n    DD['DenfenderLookingX'] = np.cos(np.radians(DD['OrientationII']))\n    DD['DefenderLookingY']  = np.sin(np.radians(DD['OrientationII']))\n\n    # Rusher Assessing\n    # ---------------------------\n\n    # Rusher Frame, Rusher Position\n    DD['DefStartNormX'] = (DD['X']    - DD['RusherX'])/DD['StartDistance']\n    DD['DefStartNormY'] = (DD['YNEW'] - DD['RusherY'])/DD['StartDistance']\n\n    # Rusher Frame, Rusher Assessing\n    DD['RusherLookingX'] = np.cos(np.radians(DD['RushOrient']))\n    DD['RusherLookingY'] = np.sin(np.radians(DD['RushOrient']))\n    \n    # Defender Engagement\n    # ---------------------------\n\n    # Calculate Normalized V Units\n    DD['RushGoingNormX'] = (DD['RusherFXV'] - DD['X'])/DD['StartDistance']\n    DD['RushGoingNormY'] = (DD['RusherFYV'] - DD['YNEW'])/DD['StartDistance']\n\n    # Calculate Normalized Vector\n    DD['DenfenderMovingX'] = np.cos(np.radians(DD['DirII']))\n    DD['DefenderMovingY']  = np.sin(np.radians(DD['DirII']))\n\n\n    # Engagement Orientation\n    # --------------------------\n\n    # Get Orientation Angle\n    DD['DefenderEngagement'] = DD[['RushGoingNormX','RushGoingNormY','DenfenderMovingX','DefenderMovingY']].apply(lambda x: angleDiff(x[0],x[1],x[2],x[3]),axis=1)\n    DD['DefenderAssessing']  = DD[['RushStartNormX','RushStartNormY','DenfenderLookingX','DefenderLookingY']].apply(lambda x: angleDiff(x[0],x[1],x[2],x[3]),axis=1)\n    DD['RusherAssessing']    = DD[['DefStartNormX','DefStartNormY','RusherLookingX','RusherLookingY']].apply(lambda x: angleDiff(x[0],x[1],x[2],x[3]),axis=1)\n    DD['CloseDistance']      = DD['RunnerHeaded'] - DD['StartDistance']\n\n    # get non future location info\n    # -----------------------------\n    DD['XMAP'] = DD['X']      - DD['RusherX']\n    DD['YMAP'] = DD['YNEW']   - DD['RusherY']\n    DD['SMAP'] = abs(DD['S']) - abs(DD['RushS'])\n    \n    \n    # setup dataframe\n    full = pd.DataFrame()\n\n    # map defensive players to table\n    for name, group in DD.groupby(['PlayId','GameId']):\n        \n        # create variables\n        info = {'RushDistYardLine': group['RushDistYardLine'].values[0],\n                'PlayId':           group['PlayId'].values[0],\n                'GameId':           group['GameId'].values[0]}\n        \n        x = 0\n        for index, row in group.sort_values('RunnerHeaded').iterrows():\n            x = x + 1\n            for key in ['SMAP','CloseDistance','DefenderAssessing','RusherAssessing','RunnerHeaded','RunnerThinking','DefenderEngagement','DistanceEstimate','TimeTillImpact']:\n                if key+str(x) not in info.keys():\n                    info[key+str(x)] = [DD.ix[index,key]]\n\n        # concat full dataframe\n        full = pd.concat([full,pd.DataFrame(info)],sort=False).reset_index(drop=True)\n    return full","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Offensive Player Features\nCreate new offsenive play features with `OffensePersonnelSplit` This feature may be removed since it is not used."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def OffensePersonnelSplit(x):\n    dic = {'DB' : 0, 'DL' : 0, 'LB' : 0, 'OL' : 0, 'QB' : 0, 'RB' : 0, 'TE' : 0, 'WR' : 0}\n    for xx in x.split(\",\"):\n        xxs = xx.split(\" \")\n        dic[xxs[-1]] = int(xxs[-2])\n    return dic","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create Features For Model\nThe following code creates features for the model"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def get_offense_prop(df, prefix = 'premier',alpha=3,addXY = True,dt_array = [.2,.3,.4,.5,.6,.7,.8,.9,1.0]):\n    \n    # get rusher\n    meta_data   = ['PlayDirection','Season','YardLine']\n    rush_fields = ['GameId','PlayId','RusherTeam','RusherX','RusherY','RushS','RushA','RushDir','RushOrient']\n    rusher = df[df['NflId'] == df['NflIdRusher']][['GameId','PlayId','Team','X','YNEW','S','A','DirII','OrientationII']]\n    rusher.columns = rush_fields\n\n    # create table\n    GK = pd.merge(df,rusher,on=['GameId','PlayId'],how='inner')\n    \n    # player score factors\n    GK['PlayerFactor']                                         =  1\n    GK.loc[GK['Team']     != GK['RusherTeam'],'PlayerFactor']  = -1\n    GK.loc[GK['NflId']    == GK['NflIdRusher'],'PlayerFactor'] =  0\n    GK.loc[GK['Position'] == 'QB',             'PlayerFactor'] =  0\n\n    ids   = ['GameId','PlayId']\n    new   = []\n    rushx = []\n    rushy = []\n    \n    for dt in dt_array:\n\n        # get the field to be added\n        added_field = prefix + 'PlayerInfluence' + str(int(dt*10000))\n        added_rushx  = prefix + 'RusherX' + str(int(dt*10000))\n        added_rushy  = prefix + 'RusherY' + str(int(dt*10000))\n        \n        # Calculate new future points\n        GK[added_rushx] = physicsX(GK.RusherX,GK.RushS,GK.RushA,GK.RushDir,dt=dt)\n        GK[added_rushy] = physicsY(GK.RusherY,GK.RushS,GK.RushA,GK.RushDir,dt=dt)\n\n        # calculate new defensive lineman points\n        GK['PlayerX'] = physicsX(GK.X,GK.S,GK.A,GK.DirII,dt=dt)\n        GK['PlayerY'] = physicsY(GK.YNEW,GK.S,GK.A,GK.DirII,dt=dt)\n\n        # player distance\n        GK['PlayerDistance'] = euclidean_distance(GK[added_rushx],GK[added_rushy],GK.PlayerX,GK.PlayerY)\n        GK[added_field]      = sigmoid(GK.PlayerDistance,1,alpha)*GK['PlayerFactor']\n        \n        # append new fields\n        rushx.append(added_rushx)\n        rushy.append(added_rushy)\n        new.append(added_field)\n\n    # take aggregate statistics\n    data1 = GK[ids + new].groupby(['GameId','PlayId']).min().reset_index(drop=False)\n    data2 = GK[ids + new].groupby(['GameId','PlayId']).max().reset_index(drop=False)\n    data3 = GK[ids + new].groupby(['GameId','PlayId']).mean().reset_index(drop=False)\n    data4 = GK[ids + new].groupby(['GameId','PlayId']).std().reset_index(drop=False)\n    data5 = GK[ids + rushx].groupby(['GameId','PlayId']).mean().reset_index(drop=False)\n    data6 = GK[ids + rushy].groupby(['GameId','PlayId']).mean().reset_index(drop=False)\n    \n    # change names\n    data1.columns = ids + [s + 'Min'   for s in new]\n    data2.columns = ids + [s + 'Max'   for s in new]\n    data3.columns = ids + [s + 'Mu'    for s in new]\n    data4.columns = ids + [s + 'Sigma' for s in new]\n    data5.columns = ids + [s + 'RushX' for s in rushx]\n    data6.columns = ids + [s + 'RushY' for s in rushy]\n    \n    # merge data\n    data = pd.merge(data2,data1,how='left',on=['GameId','PlayId'])\n    data = pd.merge(data,data3,how='left', on=['GameId','PlayId'])\n    data = pd.merge(data,data4,how='left', on=['GameId','PlayId'])\n    \n    if addXY:\n        data = pd.merge(data,data5,how='left', on=['GameId','PlayId'])\n        data = pd.merge(data,data6,how='left', on=['GameId','PlayId'])\n        \n    return data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create Static Features\nThe following derives static features in the model. These features include the amount of time between snap and handoff `TimeDelta`, player age `PlayerAge`, and other important and interesting features."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def static_features(df):\n\n    # get seconds in a year\n    seconds_in_year = 60*60*24*365.25\n    \n    # Setup Constants\n    f1 = ['GameId','PlayId','CB_Count', 'SS_Count', 'NT_Count', 'G_Count', 'S_Count', 'WR_Count']\n    f2 = ['PlayerAge','PlayerHeight_dense','diffScoreBeforePlay','Dir_sin','Dir_cos']\n    f3 = ['X','Y','S','A','Dis','Orientation','Dir','YardLine','Quarter','Down','Distance','DefendersInTheBox']\n    f4 = ['TimeDelta','OrientationII','is_left','is_home','old_data']\n    f5 = ['YNEW','OffensePersonnel','DefensePersonnel']\n    f6 = []\n    \n    # Time Calculations & Features\n    df['TimeHandoff']     = df['TimeHandoff'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))\n    df['TimeSnap']        = df['TimeSnap'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))\n    df['TimeDelta']       = df.apply(lambda row: (row['TimeHandoff'] - row['TimeSnap']).total_seconds(), axis=1)\n    df['PlayerBirthDate'] = df['PlayerBirthDate'].apply(lambda x: datetime.datetime.strptime(x, \"%m/%d/%Y\"))\n\n    # calculate key data\n    df[\"is_left\"]       = df[\"PlayDirection\"] == \"left\"\n    df[\"is_home\"]       = df[\"Team\"] == \"home\"\n    df[\"old_data\"]      = df[\"Season\"] == 2017\n    \n    ## Age Calculations & Features\n    df['PlayerHeight_dense'] = df['PlayerHeight'].apply(lambda x: 12*int(x.split('-')[0])+int(x.split('-')[1]))\n    df['PlayerAge']          = df.apply(lambda row: (row['TimeHandoff']-row['PlayerBirthDate']).total_seconds()/seconds_in_year, axis=1)\n    \n    # Dir Features\n    df[\"Dir_sin\"] = df[\"Dir\"].apply(lambda x : np.sin(x/360 * 2 * np.pi))\n    df[\"Dir_cos\"] = df[\"Dir\"].apply(lambda x : np.cos(x/360 * 2 * np.pi))\n    \n    # Create formation\n    for Form in ['SHOTGUN','I_FORM','SINGLEBACK','PISTOL']:\n        df[Form + '_Formation']                                 = 0\n        df.loc[df.OffenseFormation == Form,Form + '_Formation'] = 1\n        f6.append(Form + '_Formation')\n\n    ## diff Score\n    df[\"diffScoreBeforePlay\"] = df[\"HomeScoreBeforePlay\"] - df[\"VisitorScoreBeforePlay\"]\n    static_features = df[df['NflId'] == df['NflIdRusher']][f1+f2+f3+f4+f5+f6].drop_duplicates()\n    static_features.fillna(-999,inplace=True)\n    return static_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Get Defensive State\ncool function I made"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def getDefensiveState(df):\n    \n    # Create Rusher Table\n    # ------------------------\n    \n    # Field To Create Rush Table\n    meta_data   = ['GameId','PlayId','Team','Xpoint','Ypoint','S','A','DirII','OrientationII']\n    rush_fields = ['GameId','PlayId','RusherTeam','XpointRush','YpointRush','RushS','RushA','RushDir','RushOrient']\n    add_fields  = ['X','YNEW','NflIdRusher','NflId']\n\n    # X Point\n    df['Xpoint'] = physicsX(df.X,df.S,df.A,df.DirII)\n    df['Ypoint'] = physicsY(df.YNEW,df.S,df.A,df.DirII)\n\n    # Fill Bad Points\n    df['Xpoint'] = np.where(df['Xpoint'].isna(), df['X'],    df['Xpoint'])\n    df['Ypoint'] = np.where(df['Ypoint'].isna(), df['YNEW'], df['Ypoint'])\n\n    # get rusher\n    rusher = df[df['NflId'] == df['NflIdRusher']][meta_data]\n    rusher.columns = rush_fields\n    rusher['Score'] = 1\n    \n    \n    # Create Defensive Table\n    # ------------------------\n    \n    # Field To Create Rush Table\n    meta_data   = ['GameId','PlayId','Team','Xpoint','Ypoint','S','A','DirII','OrientationII']\n    rush_fields = ['GameId','PlayId','RusherTeam','XpointRush','YpointRush','RushS','RushA','RushDir','RushOrient']\n    add_fields  = ['X','YNEW','NflIdRusher','NflId']\n\n    # X Point\n    df['Xpoint'] = physicsX(df.X,df.S,df.A,df.DirII,.6)\n    df['Ypoint'] = physicsY(df.YNEW,df.S,df.A,df.DirII,.6)\n\n    # Fill Bad Points\n    df['Xpoint'] = np.where(df['Xpoint'].isna(), df['X'],    df['Xpoint'])\n    df['Ypoint'] = np.where(df['Ypoint'].isna(), df['YNEW'], df['Ypoint'])\n\n    # get rusher\n    rusher = df[df['NflId'] == df['NflIdRusher']][meta_data]\n    rusher.columns = rush_fields\n    rusher['Score'] = 1\n\n    # create table\n    data = pd.merge(df[meta_data+add_fields],rusher,on=['GameId','PlayId'],how='inner')\n    data['Score'] = data['Score'].fillna(-1)\n    data.loc[data['Team'] != data['RusherTeam'],'Score'] = 1\n\n    # create distance stats\n    data['DistanceToRusher'] = np.sqrt((data['Xpoint'] - data['XpointRush'])**2 + (data['Ypoint'] - data['YpointRush'])**2)\n    data[\"DefenseIndex\"]     = data.groupby(['GameId','PlayId'])[\"DistanceToRusher\"].rank(\"dense\", ascending=True).astype(np.int)\n\n\n    # Fill Bad Points (this may have created bad data*)\n    data['X']    = np.where(data['NflId'] == data['NflIdRusher'],data['Xpoint'], data['X'])\n    data['YNEW'] = np.where(data['NflId'] == data['NflIdRusher'],data['Ypoint'], data['YNEW'])\n\n    # create data table\n    data = data[['GameId','PlayId','X','YNEW','DistanceToRusher','DefenseIndex','Score','S']]\n\n\n    # Compile Feature Set\n    # -------------------------\n    \n    # create full\n    full      = rusher[['GameId','PlayId']]\n    meta_data = ['GameId','PlayId','X','YNEW']\n    def_data  = ['GameId','PlayId','DefenderX','DefenderY']\n\n\n    for key in np.linspace(1,2,2):\n        Defender = data[data['DefenseIndex'] == key][meta_data]\n        Defender.columns = def_data\n\n        DST = data.merge(Defender,on    = ['GameId','PlayId'],how='left')\n        DST['DistanceToRusher']         = np.sqrt((DST['X'] - DST['DefenderX'])**2 + (DST['YNEW'] - DST['DefenderY'])**2)\n        DST['DefensiveInfluence'+str(int(key))] = sigmoid(DST['DistanceToRusher'],1,3)*DST['Score'] \n        DST  = DST.groupby(['GameId','PlayId'])[['DefensiveInfluence'+str(int(key))]].mean().reset_index()\n        full = pd.merge(full,DST,on = ['GameId','PlayId'],how = 'left')\n        \n    # Model Input II Features\n    feats = ['DefensiveInfluence1', 'DefensiveInfluence2']\n\n    full['defenderMax'] = full[feats].max(axis=1)\n    full['defenderMin'] = full[feats].min(axis=1)\n    full['defenderSigma'] = full[feats].std(axis=1)    \n    \n    return full","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Aggregate Feature Function\nThe following code creates a single function for data curation. The feature aggregation methods include all of the following:\n* `update_orientation`\n* `getDefensiveState`\n* `get_offense_prop`\n* `back_features`\n* `defense_pro_features`\n* `features_relative_to_back`\n* `defense_features`\n* `static_features`\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def create_features(df, deploy=False):\n    \n    # get back features\n    def back_features(df):\n        carriers = df[df['NflId'] == df['NflIdRusher']][['GameId','PlayId','NflIdRusher','X','Y','Orientation','Dir','YardLine']]\n        carriers['back_from_scrimmage'] = carriers['YardLine'] - carriers['X']\n        carriers['back_oriented_down_field'] = carriers['Orientation'].apply(lambda x: back_direction(x))\n        carriers['back_moving_down_field'] = carriers['Dir'].apply(lambda x: back_direction(x))\n        carriers = carriers.rename(columns={'X':'back_X','Y':'back_Y'})\n        return carriers[['GameId','PlayId','NflIdRusher','back_X','back_Y','back_from_scrimmage','back_oriented_down_field','back_moving_down_field']].copy()\n\n    # get features relative to back\n    def features_relative_to_back(df, carriers):\n        \n        # Get Info\n        grpflds = ['GameId','PlayId','back_from_scrimmage','back_oriented_down_field','back_moving_down_field']\n        calc    = ['X','Y','back_X','back_Y']\n        player_distance = df[['GameId','PlayId','NflId','X','Y']]\n        player_distance = pd.merge(player_distance, carriers, on=['GameId','PlayId'], how='inner')\n        player_distance = player_distance[player_distance['NflId'] != player_distance['NflIdRusher']]\n        player_distance['dist_to_back'] = player_distance[calc].apply(lambda x: euclidean_distance(x[0],x[1],x[2],x[3]), axis=1)\n        player_distance = player_distance.groupby(grpflds).agg({'dist_to_back':['min','max','mean','std']}).reset_index()\n        player_distance.columns = grpflds+ ['min_dist','max_dist','mean_dist','std_dist']\n        return player_distance\n\n    # combine data\n    def combine_features(relative_to_back, defense, static, deploy=deploy):\n        df = pd.merge(relative_to_back,defense,on=['GameId','PlayId'],how='inner')\n        df = pd.merge(df,static,on=['GameId','PlayId'],how='inner')\n        if deploy: return df\n        return pd.merge(df, outcomes, on=['GameId','PlayId'], how='inner')\n    \n    personnel_features_II(df)\n    yardline      = update_yardline(df)\n    df            = update_orientation(df, yardline)\n    newset        = getDefensiveState(df)\n    \n    # Distance Features\n    meta_feats    = get_offense_prop(df)\n    meta_feats2   = get_offense_prop(df,prefix='alpha1',alpha=1, addXY=False,dt_array = [.8])\n    meta_feats4   = get_offense_prop(df,prefix='alpha7',alpha=7, addXY=False,dt_array = [.2])\n    \n    back_feats    = back_features(df)\n    def_feats_pro = defense_pro_features(df)\n    rel_back      = features_relative_to_back(df, back_feats)\n    def_feats     = defense_features(df)\n    static_feats  = static_features(df)\n    basetable     = combine_features(rel_back, def_feats, static_feats, deploy=deploy)\n    \n    # combine other data\n    basetable     = pd.merge(basetable,def_feats_pro,how='left',on=['GameId','PlayId'])\n    basetable     = pd.merge(basetable,meta_feats,how='left',on=['GameId','PlayId'])\n    basetable     = pd.merge(basetable,meta_feats2,how='left',on=['GameId','PlayId'])\n    basetable     = pd.merge(basetable,meta_feats4,how='left',on=['GameId','PlayId'])\n    basetable     = pd.merge(basetable,newset,how='left',on=['GameId','PlayId'])\n    \n    for key in [ 'premierRusherX10000RushX','premierRusherX2000RushX','premierRusherX4000RushX','premierRusherX6000RushX','premierRusherX7000RushX','premierRusherX8000RushX']:\n        basetable[key] = basetable[key] - basetable['YardLine']\n        \n        \n    agg_cols1 = ['premierPlayerInfluence2000Min', 'premierPlayerInfluence3000Min', 'premierPlayerInfluence4000Min', 'premierPlayerInfluence5000Min', 'premierPlayerInfluence6000Min', 'premierPlayerInfluence7000Min', 'premierPlayerInfluence8000Min', 'premierPlayerInfluence9000Min', 'premierPlayerInfluence10000Min']\n    agg_cols  = ['CloseDistance1', 'CloseDistance2', 'CloseDistance3', 'CloseDistance4', 'CloseDistance5', 'CloseDistance6', 'CloseDistance7', 'CloseDistance8', 'CloseDistance9', 'CloseDistance10', 'CloseDistance11']\n    \n    basetable['CloseDistanceSigma'] = basetable[agg_cols].std(axis=1)\n    basetable['CloseDistanceMu']    = basetable[agg_cols].mean(axis=1)\n    basetable['CloseDistanceMax']   = basetable[agg_cols].max(axis=1)\n    basetable['CloseDistanceMin']   = basetable[agg_cols].min(axis=1)\n    basetable['ThePlaySigmaMin']    = basetable[agg_cols1].std(axis=1)\n    \n    return basetable\n\n    \ndef process_two(t_):\n    \n    # calculate radian angle\n    radian_angle = (90 - t_['Dir']) * np.pi / 180.0\n    \n    t_['fe1'] = pd.Series(np.sqrt(np.absolute(np.square(t_.X.values) - np.square(t_.Y.values))))\n    t_['fe5'] = t_['S'].values*np.cos(radian_angle) + .5 * t_['A'].values *np.cos(radian_angle)\n    t_['fe7'] = np.arccos(np.clip(t_['X'].values / t_['Y'].values, -1, 1))  # N\n    t_['fe8'] = np.square(t_['S'].values) + 2 * t_['A'].values * t_['Dis'].values\n    t_['fe10'] = np.abs(t_['S'] * np.cos(radian_angle))\n    t_['fe11'] = np.abs(t_['S'] * np.sin(radian_angle))\n    \n    t_['OrientationABS'] = t_['Orientation'].apply(lambda x: absoluteAngle(x))\n    t_['Orientation']    = t_['Orientation'].apply(lambda x: rotate_angle_180(x))\n    t_['DirABS']         = t_['Dir'].apply(lambda x: absoluteAngle(x))\n    t_['Dir']            = t_['Dir'].apply(lambda x: rotate_angle_180(x))\n    t_['YREF']           = (t_['Y'] - 26.65).abs()\n    return t_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create The Data Training Set\n_____________________________________\n## Generate feature set\nHere we read in the training data, and create a baseline training table, then make a copy of the training table. The reason a copy is may is so that new features may be added without re-running the algorithm."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import Datasets\ntrain = pd.read_csv('../input/nfl-big-data-bowl-2020/train.csv', dtype={'WindSpeed': 'object'})\noutcomes = train[['GameId','PlayId','Yards']].drop_duplicates()\n\n# transform speed and acceleration\ntrain.loc[train.Season==2017,'S'] = train.loc[train.Season==2017,'S']*1.1320096503100632\ntrain.loc[train.Season==2017,'A'] = train.loc[train.Season==2017,'A']*1.1210484653841495\n\n# transform orientation\ntrain['Orientation'] = train.apply(lambda x: rotate_angle_90(x.Orientation,x.Season),axis=1)\n\n# Create Training & Target Data\ntrain_basetable = create_features(train, False)\ntr_sf = train_basetable.copy()\n\n# save a copy of the featureset\n#train_basetable.to_csv(\"trainingset.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Normalize Training Set Data\nTo normalize the numerical data StandardScaler used from sklearn. For more information see [link](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) for more information.  To normalize the training set for categorical data a label encoder was used. For more information see [link](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) for more information"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"cat = ['back_moving_down_field','SHOTGUN_Formation', 'I_FORM_Formation', \n       'SINGLEBACK_Formation', 'PISTOL_Formation', 'JUMBO_Formation']\nnum = ['DefensiveInfluence1', 'DefensiveInfluence2','defenderMax', 'DefensiveInfluence3', 'DefensiveInfluence4', \n'DefensiveInfluence5','DefensiveInfluence6', 'DefensiveInfluence7', 'DefensiveInfluence8', 'DefensiveInfluence9', \n'DefensiveInfluence10', 'DefensiveInfluence11','premierPlayerInfluence2000Max', 'premierPlayerInfluence3000Max', \n'premierPlayerInfluence4000Max', 'premierPlayerInfluence5000Max',\n'premierPlayerInfluence6000Max', 'premierPlayerInfluence7000Max', \n'premierPlayerInfluence8000Max', 'premierPlayerInfluence9000Max', \n'premierPlayerInfluence10000Max', 'premierPlayerInfluence2000Min', \n'premierPlayerInfluence3000Min', 'premierPlayerInfluence4000Min',\n'premierPlayerInfluence5000Min', 'premierPlayerInfluence6000Min', 'premierPlayerInfluence7000Min',\n'premierPlayerInfluence8000Min', 'premierPlayerInfluence9000Min', 'premierPlayerInfluence10000Min', \n'premierPlayerInfluence2000Mu', 'premierPlayerInfluence3000Mu', 'premierPlayerInfluence4000Mu',\n'premierPlayerInfluence5000Mu', 'premierPlayerInfluence6000Mu', 'premierPlayerInfluence7000Mu', \n'premierPlayerInfluence8000Mu', 'premierPlayerInfluence9000Mu', 'premierPlayerInfluence10000Mu', \n'premierPlayerInfluence2000Sigma', 'premierPlayerInfluence3000Sigma', 'premierPlayerInfluence4000Sigma',\n'premierPlayerInfluence5000Sigma', 'premierPlayerInfluence6000Sigma', 'premierPlayerInfluence7000Sigma',\n'premierPlayerInfluence8000Sigma', 'premierPlayerInfluence9000Sigma', 'premierPlayerInfluence10000Sigma',\n'back_from_scrimmage', 'min_dist', 'max_dist', 'mean_dist', 'std_dist', 'def_min_dist', 'def_max_dist', \n'def_mean_dist', 'def_std_dist','X', 'YREF', 'S', 'A', 'Dis', 'Orientation', 'Dir', 'YardLine', 'Distance',\n'fe1', 'fe5','fe8', 'fe10', 'fe11','Orientation_sin', 'Orientation_cos', 'PlayerAge','PlayerHeight_dense',\n'DefendersInTheBox','TimeDelta','old_data','DirABS','OrientationABS','Dir_sin', 'Dir_cos','SMAP1', 'RushDistYardLine',\n'CloseDistance1','DefenderAssessing1',  'RusherAssessing1', 'RunnerHeaded1', 'RunnerThinking1', 'DefenderEngagement1',\n'DistanceEstimate1','TimeTillImpact1', 'SMAP2', 'CloseDistance2', 'DefenderAssessing2',\n'RusherAssessing2', 'RunnerHeaded2', 'RunnerThinking2', 'DefenderEngagement2','DistanceEstimate2',\n'SMAP3', 'CloseDistance3', 'DefenderAssessing3', 'RusherAssessing3', 'RunnerHeaded3', 'RunnerThinking3',\n'DefenderEngagement3','DistanceEstimate3', 'SMAP4', 'CloseDistance4', 'DefenderAssessing4', 'RusherAssessing4', \n'RunnerHeaded4', 'RunnerThinking4', 'DefenderEngagement4','DistanceEstimate4', 'SMAP5', 'CloseDistance5', \n'DefenderAssessing5', 'RusherAssessing5', 'RunnerHeaded5', 'RunnerThinking5', 'DefenderEngagement5','DistanceEstimate5', \n'SMAP6', 'CloseDistance6', 'DefenderAssessing6', 'RusherAssessing6', 'RunnerHeaded6', 'RunnerThinking6', 'DefenderEngagement6',\n'SMAP7', 'CloseDistance7', 'DefenderAssessing7', 'RusherAssessing7', 'RunnerHeaded7', 'RunnerThinking7', 'DefenderEngagement7',\n'SMAP8', 'CloseDistance8', 'DefenderAssessing8', 'RusherAssessing8', 'RunnerHeaded8', 'RunnerThinking8', 'DefenderEngagement8',\n'SMAP9', 'CloseDistance9', 'DefenderAssessing9', 'RusherAssessing9', 'RunnerHeaded9', 'RunnerThinking9', 'DefenderEngagement9', \n'SMAP10', 'CloseDistance10', 'DefenderAssessing10', 'RusherAssessing10', 'RunnerHeaded10', \n'SMAP11', 'CloseDistance11', 'DefenderAssessing11', 'RusherAssessing11', 'RunnerHeaded11','premierRusherX2000RushX', \n'premierRusherX3000RushX', 'premierRusherX4000RushX', 'premierRusherX5000RushX', 'premierRusherX6000RushX',\n'premierRusherX7000RushX', 'premierRusherX8000RushX','premierRusherX9000RushX','premierRusherX10000RushX', \n'alpha7PlayerInfluence2000Max', 'alpha7PlayerInfluence3000Max',\n'alpha7PlayerInfluence4000Max', 'alpha7PlayerInfluence5000Max', 'alpha7PlayerInfluence6000Max', \n'alpha7PlayerInfluence7000Max', 'alpha7PlayerInfluence8000Max', 'alpha7PlayerInfluence9000Max',\n'alpha7PlayerInfluence10000Max', 'alpha7PlayerInfluence2000Min', 'alpha7PlayerInfluence3000Min',\n'alpha7PlayerInfluence4000Min', 'alpha7PlayerInfluence5000Min', 'alpha7PlayerInfluence6000Min',\n'alpha7PlayerInfluence7000Min', 'alpha7PlayerInfluence8000Min', 'alpha7PlayerInfluence9000Min', \n'alpha7PlayerInfluence10000Min', 'alpha7PlayerInfluence2000Mu', 'alpha7PlayerInfluence3000Mu', \n'alpha7PlayerInfluence4000Mu', 'alpha7PlayerInfluence5000Mu', 'alpha7PlayerInfluence6000Mu', \n'alpha7PlayerInfluence7000Mu', 'alpha7PlayerInfluence8000Mu', 'alpha7PlayerInfluence9000Mu', \n'alpha7PlayerInfluence10000Mu', 'alpha7PlayerInfluence2000Sigma', 'alpha7PlayerInfluence3000Sigma',\n'alpha7PlayerInfluence4000Sigma', 'alpha7PlayerInfluence5000Sigma', 'alpha7PlayerInfluence6000Sigma',\n'alpha7PlayerInfluence7000Sigma', 'alpha7PlayerInfluence8000Sigma', 'alpha7PlayerInfluence9000Sigma',\n'alpha7PlayerInfluence10000Sigma', \n'alpha5PlayerInfluence2000Max', 'alpha5PlayerInfluence3000Max', 'alpha5PlayerInfluence4000Max',\n'alpha5PlayerInfluence5000Max', 'alpha5PlayerInfluence6000Max', 'alpha5PlayerInfluence7000Max',\n'alpha5PlayerInfluence8000Max', 'alpha5PlayerInfluence9000Max', 'alpha5PlayerInfluence10000Max',\n'alpha5PlayerInfluence2000Min', 'alpha5PlayerInfluence3000Min', 'alpha5PlayerInfluence4000Min', \n'alpha5PlayerInfluence5000Min', 'alpha5PlayerInfluence6000Min', 'alpha5PlayerInfluence7000Min', \n'alpha5PlayerInfluence8000Min', 'alpha5PlayerInfluence9000Min', 'alpha5PlayerInfluence10000Min', \n'alpha5PlayerInfluence2000Mu', 'alpha5PlayerInfluence3000Mu', 'alpha5PlayerInfluence4000Mu', \n'alpha5PlayerInfluence5000Mu', 'alpha5PlayerInfluence6000Mu', 'alpha5PlayerInfluence7000Mu',\n'alpha5PlayerInfluence8000Mu', 'alpha5PlayerInfluence9000Mu', 'alpha5PlayerInfluence10000Mu',\n'alpha5PlayerInfluence2000Sigma', 'alpha5PlayerInfluence3000Sigma', 'alpha5PlayerInfluence4000Sigma',\n'alpha5PlayerInfluence5000Sigma', 'alpha5PlayerInfluence6000Sigma', 'alpha5PlayerInfluence7000Sigma', \n'alpha5PlayerInfluence8000Sigma', 'alpha5PlayerInfluence9000Sigma', 'alpha5PlayerInfluence10000Sigma',   \n'alpha1PlayerInfluence2000Max', 'alpha1PlayerInfluence3000Max', 'alpha1PlayerInfluence4000Max',\n'alpha1PlayerInfluence5000Max', 'alpha1PlayerInfluence6000Max', 'alpha1PlayerInfluence7000Max',\n'alpha1PlayerInfluence8000Max', 'alpha1PlayerInfluence9000Max', 'alpha1PlayerInfluence10000Max',\n'alpha1PlayerInfluence2000Min', 'alpha1PlayerInfluence3000Min', 'alpha1PlayerInfluence4000Min',\n'alpha1PlayerInfluence5000Min', 'alpha1PlayerInfluence6000Min', 'alpha1PlayerInfluence7000Min',\n'alpha1PlayerInfluence8000Min', 'alpha1PlayerInfluence9000Min', 'alpha1PlayerInfluence10000Min', \n'alpha1PlayerInfluence2000Mu', 'alpha1PlayerInfluence3000Mu', 'alpha1PlayerInfluence4000Mu', \n'alpha1PlayerInfluence5000Mu', 'alpha1PlayerInfluence6000Mu', 'alpha1PlayerInfluence7000Mu',\n'alpha1PlayerInfluence8000Mu', 'alpha1PlayerInfluence9000Mu', 'alpha1PlayerInfluence10000Mu', \n'alpha1PlayerInfluence2000Sigma', 'alpha1PlayerInfluence3000Sigma', 'alpha1PlayerInfluence4000Sigma',\n'alpha1PlayerInfluence5000Sigma', 'alpha1PlayerInfluence6000Sigma', 'alpha1PlayerInfluence7000Sigma',\n'alpha1PlayerInfluence8000Sigma', 'alpha1PlayerInfluence9000Sigma', 'alpha1PlayerInfluence10000Sigma']\n\n\nnum = list(np.unique(np.array(num)))\ncat = list(np.unique(np.array(cat)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# .011761 Model Input I Features\nX1F = ['SHOTGUN_Formation','I_FORM_Formation','SINGLEBACK_Formation','PISTOL_Formation','back_moving_down_field',\n       'back_from_scrimmage', 'min_dist', 'max_dist', 'mean_dist', 'std_dist', 'def_min_dist',\n'def_max_dist','Orientation','def_mean_dist', 'def_std_dist','X', 'YREF', 'S', 'A', 'Dis', 'YardLine','fe5','fe8',\n'fe10', 'fe11','PlayerAge','PlayerHeight_dense','DefendersInTheBox','TimeDelta','old_data','DirABS',\n'OrientationABS','Dir_sin', 'Dir_cos','alpha7PlayerInfluence2000Max','premierPlayerInfluence5000Max',\n'premierPlayerInfluence6000Max','premierPlayerInfluence7000Max',\n'premierRusherX2000RushX', \n'premierRusherX4000RushX', \n'premierRusherX6000RushX', 'premierRusherX7000RushX',\n'premierRusherX8000RushX',\n'premierRusherX10000RushX',\n'premierPlayerInfluence4000Min', 'premierPlayerInfluence6000Min',\n'premierPlayerInfluence8000Min', 'premierPlayerInfluence10000Min',\n'alpha7PlayerInfluence2000Mu','premierPlayerInfluence4000Mu',\n'premierPlayerInfluence6000Mu','premierPlayerInfluence7000Mu','premierPlayerInfluence9000Mu',\n'premierPlayerInfluence10000Mu','alpha7PlayerInfluence2000Sigma',\n'premierPlayerInfluence4000Sigma','premierPlayerInfluence6000Sigma',\n'premierPlayerInfluence9000Sigma','premierPlayerInfluence10000Sigma']\n\n# Model Input II Features\nX2F = ['SMAP1', 'CloseDistance1', 'DefenderAssessing1', 'RusherAssessing1', 'RunnerHeaded1', 'RunnerThinking1', 'DefenderEngagement1','DefensiveInfluence1','DistanceEstimate1','TimeTillImpact1',\n       'SMAP2', 'CloseDistance2', 'DefenderAssessing2', 'RusherAssessing2', 'RunnerHeaded2', 'RunnerThinking2', 'DefenderEngagement2','DefensiveInfluence2','DistanceEstimate2',\n       'SMAP3', 'CloseDistance3', 'DefenderAssessing3', 'RusherAssessing3', 'RunnerHeaded3', 'RunnerThinking3', 'DefenderEngagement3','DistanceEstimate3',\n       'SMAP4', 'CloseDistance4', 'DefenderAssessing4', 'RusherAssessing4', 'RunnerHeaded4', 'RunnerThinking4', 'DefenderEngagement4','DistanceEstimate4', \n       'SMAP5', 'CloseDistance5', 'DefenderAssessing5', 'RusherAssessing5', 'RunnerHeaded5', 'RunnerThinking5', 'DefenderEngagement5','DistanceEstimate5', \n       'SMAP6', 'CloseDistance6', 'DefenderAssessing6', 'RusherAssessing6', 'RunnerHeaded6', 'RunnerThinking6', 'DefenderEngagement6',\n       'SMAP7', 'CloseDistance7', 'DefenderAssessing7', 'RusherAssessing7', 'RunnerHeaded7', 'RunnerThinking7', 'DefenderEngagement7',\n       'SMAP8', 'CloseDistance8', 'DefenderAssessing8', 'RusherAssessing8', 'RunnerHeaded8', 'RunnerThinking8', 'DefenderEngagement8',\n       'SMAP9', 'CloseDistance9', 'DefenderAssessing9', 'RusherAssessing9', 'RunnerHeaded9', 'RunnerThinking9', 'DefenderEngagement9', \n       'SMAP10', 'CloseDistance10', 'DefenderAssessing10', 'RusherAssessing10', 'RunnerHeaded10', \n       'SMAP11', 'CloseDistance11', 'DefenderAssessing11', 'RusherAssessing11', 'RunnerHeaded11']\n\n\n\ndef get_pytorch_features():\n    return  ['CloseDistanceSigma','CloseDistanceMu','CloseDistanceMax','CloseDistanceMin','ThePlaySigmaMin','SHOTGUN_Formation','I_FORM_Formation','SINGLEBACK_Formation',\n             'PISTOL_Formation','back_moving_down_field','premierRusherX6000RushX', 'premierRusherX7000RushX',\n            'back_from_scrimmage', 'mean_dist', 'std_dist', 'def_min_dist','def_mean_dist', 'def_std_dist','X', 'S','A', 'Dis','YardLine','fe5','fe8','fe10', 'fe11','PlayerAge',\n            'PlayerHeight_dense','DefendersInTheBox','TimeDelta','old_data','OrientationABS','Dir_sin', 'Dir_cos',\n            'premierPlayerInfluence2000Max','premierPlayerInfluence3000Max', 'premierPlayerInfluence4000Max','premierPlayerInfluence5000Max','premierPlayerInfluence6000Max','premierPlayerInfluence7000Max',\n            'premierPlayerInfluence2000Min', 'premierPlayerInfluence4000Min','premierPlayerInfluence5000Min', 'premierPlayerInfluence6000Min','premierPlayerInfluence7000Min',\n             'alpha1PlayerInfluence8000Min', 'premierPlayerInfluence2000Mu','premierPlayerInfluence3000Mu','premierPlayerInfluence4000Mu','premierPlayerInfluence5000Mu','premierPlayerInfluence6000Mu','premierPlayerInfluence7000Mu','alpha1PlayerInfluence8000Mu',\n            'premierPlayerInfluence2000Sigma','premierPlayerInfluence3000Sigma','premierPlayerInfluence4000Sigma','premierPlayerInfluence5000Sigma','premierPlayerInfluence6000Sigma','premierPlayerInfluence7000Sigma','alpha1PlayerInfluence8000Sigma']\n\n\n\npy_feats = get_pytorch_features()\n\n\n# filter variables\nnum = [var for var in num if var in X1F+X2F+py_feats]\ncat = [var for var in cat if var in X1F+X2F+py_feats]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Normalize Training Set Data\nTo normalize the numerical data StandardScaler used from sklearn. For more information see [link](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) for more information.  To normalize the training set for categorical data a label encoder was used. For more information see [link](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) for more information"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def getNFLY(df):\n    y = np.zeros(shape=(df.shape[0], 199))\n    for i, yard in enumerate(df['Yards'].values):\n        y[i, yard+99:] = np.ones(shape=(1, 100-yard))\n    return y\n\n\n# setup training data\nX, GROUP, yards, y_pytorch  = tr_sf.copy(), tr_sf['GameId'].copy(), tr_sf.Yards, getNFLY(tr_sf)\nX = process_two(X)\nX = X.fillna(0)\n\n# setup target data\ny = np.zeros((yards.shape[0], 199))\nfor idx, target in enumerate(list(yards)):\n    y[idx][99 + target] = 1\n    \n# create Standard Scaler Transformation object\nscaler = StandardScaler().fit(X[num])\nX[num] = scaler.transform(X[num])\n\n# create dictionary for categorical features\nle_dict = {}\n\n# create encoding objects\nfor ca in cat:\n    le_dict[ca] = LabelEncoder().fit(X[ca].apply(str))\n    X[ca]  = le_dict[ca].transform(X[ca].apply(str))\n\n# added fields\nadded_fields = ['CloseDistanceSigma','CloseDistanceMu','CloseDistanceMax','CloseDistanceMin','ThePlaySigmaMin']\n\n# retain only important model information\nX   = X[cat+num + added_fields]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define Features To Be Used\nThe following code defines lists of features to be used in the input layer of the neural network. For more information on how they were generated see the code above. The features of the model fall under the following two categories:\n* Static play data\n* Dynamic Data"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\n# Create Training Input Layer Datasets\nX1, X2 = X[X1F], X[X2F]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train The Neural Network\nTo train the model Group K-fold cross validation is used. For more information see [link](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GroupKFold.html)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# seed everything\nseed_everything(1234)\n\n# set up parameters important variables\noof_preds, py_feats = np.ones((X.shape[0], 199)), get_pytorch_features()\ntorch_epoch, torch_batch, torch_len  = 100, 1012, X[py_feats].shape[1]\nlosses, models,pymodels, crps_csv, s_time   = [], [], [], [], time.time()\n\n# Setup constants\nkf = GroupKFold(n_splits=5)\n\n# predictive model\noof_preds = np.ones((X.shape[0], 199))\npyt_oof = np.ones((X.shape[0], 199))\nkyr_oof = np.ones((X.shape[0], 199))\n\n#kfold = KFold(10, random_state = 42 + k, shuffle = True)\nfor k_fold, (tix, vix) in enumerate(kf.split(X, y, GROUP)):\n    \n    # Pytorch Model Sigmoid\n    \n    # obtain model\n    model, crps_v, crps_t = get_model([X1.ix[tix],X2.ix[tix]], y[tix], [X1.ix[vix],X2.ix[vix]], y[vix])\n\n    # append model & valuation\n    models.append(model)\n    crps_csv.append(crps_v) \n    del model\n    gc.collect()\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generate Submission Predictions\n--------------------------------\nThe following function predict, feature a prediction method for the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(x_te):\n    model_num = len(models)\n    for k,m in enumerate(models):\n        if k==0: y_pred =  m.predict(x_te,batch_size=1024)\n        else:    y_pred += m.predict(x_te,batch_size=1024)\n            \n    y_pred = y_pred / model_num\n    return y_pred\n\ndef prediction_adjustment(y,yards):\n    max_yards = 100-yards\n    y[0,max_yards+99:] = np.ones(shape=(1, 100-max_yards))\n    y[0,:yards]        = np.zeros(shape=(1, yards))\n    return y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Upload Results To API\nThe following loop uploads the model predictions to the api"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nfor (test_df, sample_prediction_df) in iter_test:\n    basetable = create_features(test_df, deploy=True)\n    X         = basetable.copy()\n    X         = process_two(X)\n    X         = X.fillna(0)\n    \n    X[num] = scaler.transform(X[num])\n    \n    for ca in cat:\n        X[ca]  = le_dict[ca].transform(X[ca].apply(str))\n    \n    # predict keras transformation\n    y_pred = keras_transform(predict([X[X1F], X[X2F]]))\n    \n    # create prediction data\n    adjust_nfl_pred = prediction_adjustment(y_pred,int(basetable.YardLine.values[0]))\n    \n    preds_df = pd.DataFrame(data=adjust_nfl_pred[0].reshape(1, 199), columns=sample_prediction_df.columns)\n    env.predict(preds_df)\n\nenv.write_submission_file()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Work Cited\n___________________________\n\n* feature selection : https://www.kaggle.com/coolcoder22/nfl-001-feature-selection\n* Modelling Pitch Control http://www.lukebornn.com/papers/fernandez_ssac_2018.pdf"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}