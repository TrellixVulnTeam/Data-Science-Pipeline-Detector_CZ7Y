{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### To Do\n1. Fit on Full Dataset - with best iter\n1. hyperparameter tuning- catboost and LGB\n1. Missing values\n1. backfill - currently set as backfill - try median /mean\n1. NB - add grouped median columns for each period (['x', 'y', 'direction', 'weekday', 'hour', 'minute']) \n\n### DONE\n1. Rolling median/mean \n1. Scaling   --- Scaling on \n1. label encoding/ ohe --- only labelencoding (TBC) \n1. CV with or without s --No shuffle\n1. added features ","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler,LabelEncoder\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\nfrom sklearn.metrics import mean_absolute_error, r2_score\n\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-11T17:57:55.568746Z","iopub.execute_input":"2022-03-11T17:57:55.569288Z","iopub.status.idle":"2022-03-11T17:57:55.576012Z","shell.execute_reply.started":"2022-03-11T17:57:55.569239Z","shell.execute_reply":"2022-03-11T17:57:55.574847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_original = pd.read_csv(\"../input/tabular-playground-series-mar-2022/train.csv\", index_col = 0)\ntest_original = pd.read_csv(\"../input/tabular-playground-series-mar-2022/test.csv\",index_col = 0)\nsub = pd.read_csv(\"../input/tabular-playground-series-mar-2022/sample_submission.csv\",index_col = 0)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T17:57:55.577728Z","iopub.execute_input":"2022-03-11T17:57:55.577975Z","iopub.status.idle":"2022-03-11T17:57:56.175047Z","shell.execute_reply.started":"2022-03-11T17:57:55.577941Z","shell.execute_reply":"2022-03-11T17:57:56.173787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_theme()\nTHRESHOLD = 0.40\n\nSCALING = True\n\nEPOCHS = 10000\nEARLY_STOPPING = 30\n\nPRED_MONDAY = False","metadata":{"execution":{"iopub.status.busy":"2022-03-11T17:57:56.176575Z","iopub.execute_input":"2022-03-11T17:57:56.176763Z","iopub.status.idle":"2022-03-11T17:57:56.182256Z","shell.execute_reply.started":"2022-03-11T17:57:56.176736Z","shell.execute_reply":"2022-03-11T17:57:56.181424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def feature_engineering(df):\n    df= df.copy(deep = True)\n    df['time'] = pd.to_datetime(df['time'])\n    df[\"x_y_direction\"] = df[\"x\"].astype(\"str\") + df[\"y\"].astype(\"str\") + df[\"direction\"].astype(\"str\")\n    \n    return df\n\ntrain = feature_engineering(train_original)\ntest = feature_engineering(test_original)\ntrain","metadata":{"execution":{"iopub.status.busy":"2022-03-11T17:57:56.183133Z","iopub.execute_input":"2022-03-11T17:57:56.183814Z","iopub.status.idle":"2022-03-11T17:57:57.476754Z","shell.execute_reply.started":"2022-03-11T17:57:56.183778Z","shell.execute_reply":"2022-03-11T17:57:57.476181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_groupby = train.groupby([\"time\",'x_y_direction']).sum()\ntrain_groupby","metadata":{"execution":{"iopub.status.busy":"2022-03-11T17:57:57.478383Z","iopub.execute_input":"2022-03-11T17:57:57.479052Z","iopub.status.idle":"2022-03-11T17:57:57.835374Z","shell.execute_reply.started":"2022-03-11T17:57:57.479013Z","shell.execute_reply":"2022-03-11T17:57:57.834158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_pvt = train_groupby.pivot_table(\n        values='congestion', \n        index='time', \n        columns='x_y_direction', \n        aggfunc=np.sum)\ntrain_pvt","metadata":{"execution":{"iopub.status.busy":"2022-03-11T17:57:57.836659Z","iopub.execute_input":"2022-03-11T17:57:57.836981Z","iopub.status.idle":"2022-03-11T17:57:58.358837Z","shell.execute_reply.started":"2022-03-11T17:57:57.836953Z","shell.execute_reply":"2022-03-11T17:57:58.357714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Null values ","metadata":{}},{"cell_type":"code","source":"#Fill null values\ntrain_pvt.isnull().sum()\ntrain_pvt.fillna(0,inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T17:57:58.360345Z","iopub.execute_input":"2022-03-11T17:57:58.360569Z","iopub.status.idle":"2022-03-11T17:57:58.368883Z","shell.execute_reply.started":"2022-03-11T17:57:58.36054Z","shell.execute_reply":"2022-03-11T17:57:58.367714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Correlated Columns analysis","metadata":{}},{"cell_type":"code","source":"train_corr = train_pvt.corr()\ntrain_corr.style.apply(lambda x: [\"background: red\" if v > THRESHOLD else \"background: green\" if v < -THRESHOLD  else \"\" for v in x], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T17:57:58.370291Z","iopub.execute_input":"2022-03-11T17:57:58.371088Z","iopub.status.idle":"2022-03-11T17:57:59.506566Z","shell.execute_reply.started":"2022-03-11T17:57:58.371054Z","shell.execute_reply":"2022-03-11T17:57:59.50537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets try visualise this better","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (25,10))\nsns.heatmap(train_corr, vmin=-1, vmax = 1, annot=False, cmap= \"Spectral\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-11T17:57:59.508091Z","iopub.execute_input":"2022-03-11T17:57:59.508315Z","iopub.status.idle":"2022-03-11T17:58:01.426641Z","shell.execute_reply.started":"2022-03-11T17:57:59.508286Z","shell.execute_reply":"2022-03-11T17:58:01.425947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Correlated features above Threshold","metadata":{}},{"cell_type":"code","source":"# Create a dictionary of highest correlated columns \ncorr_dict= {}\nfor col in train_corr.columns:\n    values = list(train_corr[(train_corr[col] >THRESHOLD) | (train_corr[col] < -THRESHOLD)][col][1:].index)\n    if values:\n        corr_dict[col] = values\nprint(corr_dict)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T17:58:01.427775Z","iopub.execute_input":"2022-03-11T17:58:01.428367Z","iopub.status.idle":"2022-03-11T17:58:01.481422Z","shell.execute_reply.started":"2022-03-11T17:58:01.428332Z","shell.execute_reply":"2022-03-11T17:58:01.480379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_stack = train_corr.stack()\ntrain_stack = train_stack[((train_stack >= THRESHOLD) | (train_stack <= -THRESHOLD)) & (train_stack != 1)]\ntrain_stack.head(20)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T17:58:01.484056Z","iopub.execute_input":"2022-03-11T17:58:01.48426Z","iopub.status.idle":"2022-03-11T17:58:01.494271Z","shell.execute_reply.started":"2022-03-11T17:58:01.484238Z","shell.execute_reply":"2022-03-11T17:58:01.493832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(nrows=16, sharex= True, ncols=2, figsize=(25, 60))\n\nfor dir, ax in zip(train_stack.index.get_level_values(0).unique(), axs.ravel()):\n    sns.barplot(ax = ax, x=  train_stack[(train_stack != 1 )& (train_stack.index.get_level_values(0) == dir)].index.get_level_values(1),\n               y=  train_stack[(train_stack != 1 )& (train_stack.index.get_level_values(0) == dir)].values, palette=\"deep\").set(title=dir)\n\nax.tick_params(axis='x', rotation=90)\nfig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-11T17:58:01.495067Z","iopub.execute_input":"2022-03-11T17:58:01.495353Z","iopub.status.idle":"2022-03-11T17:58:07.990163Z","shell.execute_reply.started":"2022-03-11T17:58:01.495329Z","shell.execute_reply":"2022-03-11T17:58:07.989286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Additional Feature Engineering\nNow that we have the data in a correct pivot format \\ \nwe will create new columns for prediction ","metadata":{}},{"cell_type":"code","source":"train.set_index(\"time\", drop= True, inplace = True)\ntest.set_index(\"time\", drop= True, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T17:58:07.991229Z","iopub.execute_input":"2022-03-11T17:58:07.991411Z","iopub.status.idle":"2022-03-11T17:58:07.997004Z","shell.execute_reply.started":"2022-03-11T17:58:07.991388Z","shell.execute_reply":"2022-03-11T17:58:07.996398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"degree = {'EB':180, 'NB':90, 'SB':270, 'WB':0, 'NE':135, 'SW':315, 'NW': 45, 'SE':225}\n\ndef basic_feats(df):\n    df= df.copy(deep = True)\n    \n    df[\"minute\"] = df.index.minute\n    df[\"hour\"] = df.index.hour    \n    df[\"day\"] = df.index.day\n    df[\"month\"] = df.index.month\n    df[\"dayofweek\"]= df.index.weekday\n    \n    df['moment']  = df.index.hour * 3 + df.index.minute // 20 \n   # df[\"weekofmonth\"]= df['day']//7+1\n    \n    #New features\n    df[\"x+y\"] = df[\"x\"]+df[\"y\"]\n    df['x_y'] = df['x'].astype('str') + df['y'].astype('str')\n    df['hour_direction'] = df['hour'].astype('str') + df['direction'].astype('str')\n\n    df[\"degree\"] = df[\"direction\"].map(degree).astype(\"int32\")\n    #df['rad'] = math.pi * df['degree'] / 180\n    #df['afternoon'] = df['hour'] >= 12\n    #df['weekend'] = (df.index.weekday >= 5)\n\n    #df['is_morning'] = (6 <= df['hour']) & (df['hour'] < 12)#.median()\n    #df['is_afternoon'] = (12 <= df['hour']) & (df['hour'] < 18)#.median()\n    #df['is_evening'] = (18 <= df['hour']) & (df['hour'] <= 23)#.median()\n    #df['is_night'] = (0 <= df['hour']) & (df['hour'] < 6)#.median()\n\n    return df\n\ntrain= basic_feats(train)\ntest= basic_feats(test)\ntrain","metadata":{"execution":{"iopub.status.busy":"2022-03-11T17:58:07.9986Z","iopub.execute_input":"2022-03-11T17:58:07.998814Z","iopub.status.idle":"2022-03-11T17:58:11.52548Z","shell.execute_reply.started":"2022-03-11T17:58:07.998787Z","shell.execute_reply":"2022-03-11T17:58:11.524607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"location_anomalies = [('21', 'NE', 15), ('22', 'SE', 20), ('22', 'NW', 21), ('21', 'NW', 29), ('21', 'SE', 34)]\ndef special_apply(x):\n    if (x[0] ==\"21\" and x[1] ==\"NE\") or (x[0] ==\"22\" and x[1] ==\"SE\") or (x[0] ==\"22\" and x[1] ==\"NW\") or (x[0] ==\"21\" and x[1] ==\"NW\")  or (x[0] ==\"21\" and x[1] ==\"SE\"):\n        y= 1\n    else:\n        y= 0\n    return y\n# train[\"special\"] = train[[\"x_y\",\"direction\"]].apply(special_apply ,axis =1)\n# train[\"special\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-03-11T17:58:11.526757Z","iopub.execute_input":"2022-03-11T17:58:11.527164Z","iopub.status.idle":"2022-03-11T17:58:11.535979Z","shell.execute_reply.started":"2022-03-11T17:58:11.527134Z","shell.execute_reply":"2022-03-11T17:58:11.534656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = list(train.columns)\nfeatures.append(\"ds\")\nprint(features)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T17:58:11.537255Z","iopub.execute_input":"2022-03-11T17:58:11.537496Z","iopub.status.idle":"2022-03-11T17:58:11.558394Z","shell.execute_reply.started":"2022-03-11T17:58:11.53746Z","shell.execute_reply":"2022-03-11T17:58:11.557089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encoding","metadata":{}},{"cell_type":"code","source":"#Label Encoding\nencoder = LabelEncoder()\ndef lencoder(df, col , encoder):\n    df = df.copy(deep = True)\n    df[col] = encoder.fit_transform(df[col]).astype(\"int32\")\n    return df\n\nfor col in [\"x_y_direction\", \"direction\", \"x_y\", \"hour_direction\"]:\n    train = lencoder(train, col , encoder)\n    test = lencoder(test, col , encoder)\n\n# OnHotEncoding\n# all_df = pd.get_dummies(all_df,columns= [col+\"_e\" for col in encode_cols],drop_first=True)\n#all_df.drop(encode_cols,axis = 1, inplace = True)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-11T17:58:11.560226Z","iopub.execute_input":"2022-03-11T17:58:11.560694Z","iopub.status.idle":"2022-03-11T17:58:13.529603Z","shell.execute_reply.started":"2022-03-11T17:58:11.56064Z","shell.execute_reply":"2022-03-11T17:58:13.528736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Correlation Shifting  \nAs we have seen there is a correlation between in target when looked at specific x_y_direction groupings. \\\nAs such we will want to include these values in our training, however these values are part of the target\\\n\nWe have an option of multistep recursion, where we shift the values by 1 period (i.e. 20mins) and use these values \\\nThe process is as follow: \n* Concatenate train and test (needed for shifting train values into test )\n* groupby 'x_y_direction' column \n* shift concatenation by 1 (20mins) \n* add any lost columns needed due to groupby \n* Apply pivot transformation as seen above\n* Merge pivot to concatenated data \n\n\n**NOTE**: I backfilled null values - > with subsequent values --> option to use mean/median?","metadata":{}},{"cell_type":"code","source":"all_df = pd.concat([train.assign(ds=\"train\"),test.assign(ds=\"test\")],axis =0)\n\ndef pvt_shift_values(all_df):\n    \n    all_shift = all_df.groupby(['x_y_direction'])[[\"congestion\"]].shift(1)\n\n    #ensure have the correct column for pivot\n    all_shift[\"x_y_direction\"]  =all_df[\"x\"].astype(\"str\") + all_df[\"y\"].astype(\"str\") +all_df[\"direction\"].astype(\"str\")\n    \n    #Pivot \n    all_pvt = all_shift.pivot_table(\n        values='congestion', \n        index='time', \n        columns='x_y_direction', \n        aggfunc=np.sum)\n    \n    #backfill row 0 as this caused issues in the pvt due to shift \n    all_pvt.iloc[0] =all_pvt.iloc[1]\n    \n    #Merge pivot data to full data set \n    all_merge = pd.merge(all_df[features], all_pvt,  how='left', right_index=True, left_index=True)\n    \n    # Rolling values \n    all_merge[\"rolling_median_4\"] = all_merge.groupby(\"x_y_direction\")[\"congestion\"].transform(lambda s: s.rolling(4, min_periods=1).median())\n    all_merge[\"rolling_median_8\"] = all_merge.groupby(\"x_y_direction\")[\"congestion\"].transform(lambda s: s.rolling(8, min_periods=1).median())\n    all_merge[\"rolling_median_12\"] = all_merge.groupby(\"x_y_direction\")[\"congestion\"].transform(lambda s: s.rolling(10, min_periods=1).median())\n    #all_merge[\"rolling_std_4\"] = all_merge.groupby(\"x_y_direction\")[\"congestion\"].transform(lambda s: s.rolling(4, min_periods=1).std())\n    #all_merge[\"rolling_std_8\"] = all_merge.groupby(\"x_y_direction\")[\"congestion\"].transform(lambda s: s.rolling(8, min_periods=1).std())\n    \n    #backfill std deviation\n    #all_merge.loc[all_merge.index == pd.to_datetime(\"1991-04-01 00:00:00\"), \"rolling_std_4\"] = all_merge.loc[all_merge.index == pd.to_datetime(\"1991-04-01 00:20:00\")][\"rolling_std_4\"].values\n    \n    return all_merge \n\nall_df = pvt_shift_values(all_df)\n\n#check \nall_df[all_df.index == min(test.index)].head(20)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T17:58:13.530789Z","iopub.execute_input":"2022-03-11T17:58:13.531028Z","iopub.status.idle":"2022-03-11T17:58:17.584611Z","shell.execute_reply.started":"2022-03-11T17:58:13.530997Z","shell.execute_reply":"2022-03-11T17:58:17.583758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scaling and splitting ","metadata":{}},{"cell_type":"code","source":"train = all_df[all_df[\"ds\"] ==\"train\"].drop(\"ds\",axis =1)\ntest = all_df[all_df[\"ds\"] ==\"test\"].drop([\"ds\",\"congestion\"],axis =1)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T17:58:17.585595Z","iopub.execute_input":"2022-03-11T17:58:17.58578Z","iopub.status.idle":"2022-03-11T17:58:18.108502Z","shell.execute_reply.started":"2022-03-11T17:58:17.585755Z","shell.execute_reply":"2022-03-11T17:58:18.107264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train.drop(\"congestion\",axis =1 )\ny = train[\"congestion\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T17:58:18.110135Z","iopub.execute_input":"2022-03-11T17:58:18.110389Z","iopub.status.idle":"2022-03-11T17:58:19.066526Z","shell.execute_reply.started":"2022-03-11T17:58:18.110364Z","shell.execute_reply":"2022-03-11T17:58:19.065237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if SCALING:\n    print(\"Applying Scaling\")\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n    test_s = scaler.transform(test)\nelse: \n    test_s = test.copy()","metadata":{"execution":{"iopub.status.busy":"2022-03-11T17:58:19.068953Z","iopub.execute_input":"2022-03-11T17:58:19.069277Z","iopub.status.idle":"2022-03-11T17:58:19.74305Z","shell.execute_reply.started":"2022-03-11T17:58:19.069248Z","shell.execute_reply":"2022-03-11T17:58:19.741776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fit model ","metadata":{}},{"cell_type":"code","source":"# lgb_params = {\n#     \"objective\" : \"regression\",\n#     \"metric\": \"mae\",\n#     \"device_type\": \"cpu\",\n#     'boosting': \"gbdt\",  \n#     \"learning_rate\": 0.05,\n#     \"lambda_l1\": 5, #0.03469015403439412,\n#     #\"lambda_l2\":  9.993162304351474,\n#     \"num_leaves\": 20, #100\n#     \"max_depth\": 7,\n#     \"force_col_wise\" : True,\n#                    }\n\n# lgb_params= {\"force_col_wise\" : True ,\n#            \"objective\" : \"regression\",\n#            \"metric\": \"mae\",\n#            \"device_type\": \"cpu\",\n#            'learning_rate': 0.13498645651250302, \n#            'boosting': 'gbdt', \n#            'lambda_l1': 3.190603236750105e-07,\n#            'lambda_l2': 0.06581234211397913, \n#            'num_leaves': 249, \n#            'max_depth': 15}\n\nlgb_params = {\"force_col_wise\" : True ,\n              \"objective\" : \"regression\",\n              \"num_threads\": -1,\n              \"metric\": \"mae\",\n              'learning_rate': 0.03386223199544998,\n              'boosting': 'gbdt', \n              'lambda_l1': 2.989505976417424e-07, \n              'lambda_l2': 1.6651524609127486e-06, \n              'num_leaves': 236,\n              'max_depth': 12,\n              \"device_type\": \"gpu\",\n              \"gpu_platform_id\" : 0,\n              \"gpu_device_id\" : 0}","metadata":{"execution":{"iopub.status.busy":"2022-03-11T17:58:19.744777Z","iopub.execute_input":"2022-03-11T17:58:19.744987Z","iopub.status.idle":"2022-03-11T17:58:19.752122Z","shell.execute_reply.started":"2022-03-11T17:58:19.74496Z","shell.execute_reply":"2022-03-11T17:58:19.751076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndef fit_model(X_train,y_train,X_test, y_test):\n    \n    #lightgbm\n    train_set = lgb.Dataset(X_train, y_train, params={'verbose': -1})\n    valid_set = lgb.Dataset(X_test, y_test, params={'verbose': -1})\n    model = lgb.train(params=lgb_params,\n                          train_set= train_set, \n                          valid_sets= (valid_set), \n                          num_boost_round= EPOCHS,\n                          callbacks=[lgb.early_stopping(EARLY_STOPPING)]  ) \n\n    #catboost\n#     model = CatBoostRegressor(verbose=False, eval_metric='MAE',\n#                               iterations = EPOCHS,\n#                               thread_count= -1,\n#                              task_type=\"GPU\")\n    \n#     model.fit(X_train,y_train,early_stopping_rounds=EARLY_STOPPING,eval_set=(X_test,y_test))\n    \n    mae =mean_absolute_error(y_test, model.predict(X_test))\n    print(\"\\nMAE lgb:\", mae) \n    \n    return model , mae \n\nmodel , mae  = fit_model(X_train,y_train,X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T17:58:19.753744Z","iopub.execute_input":"2022-03-11T17:58:19.754018Z","iopub.status.idle":"2022-03-11T17:59:51.109001Z","shell.execute_reply.started":"2022-03-11T17:58:19.753986Z","shell.execute_reply":"2022-03-11T17:59:51.108373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Intrinsic residuals & analysis ","metadata":{}},{"cell_type":"code","source":"# intrinsic\nif SCALING:\n    train_preds = model.predict(scaler.transform( X ) )\n    print(\"Intrinsic MAE\", mean_absolute_error(y, train_preds))\nelse:\n    train_preds = model.predict(X)\n    print(\"Intrinsic MAE\", mean_absolute_error(y, train_preds))\ntrain_preds","metadata":{"execution":{"iopub.status.busy":"2022-03-11T17:59:51.110375Z","iopub.execute_input":"2022-03-11T17:59:51.110773Z","iopub.status.idle":"2022-03-11T18:00:03.320115Z","shell.execute_reply.started":"2022-03-11T17:59:51.110742Z","shell.execute_reply":"2022-03-11T18:00:03.319507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (25,5))\nplt.hist((train_preds- y), bins = 100)\nplt.title(\"Residuals\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:00:03.321344Z","iopub.execute_input":"2022-03-11T18:00:03.321741Z","iopub.status.idle":"2022-03-11T18:00:03.703216Z","shell.execute_reply.started":"2022-03-11T18:00:03.321707Z","shell.execute_reply":"2022-03-11T18:00:03.70235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (25,5))\nsns.scatterplot(x = X.reset_index().index , y= (train_preds- y))\nplt.title(\"Residuals\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:00:03.704385Z","iopub.execute_input":"2022-03-11T18:00:03.70461Z","iopub.status.idle":"2022-03-11T18:00:05.948126Z","shell.execute_reply.started":"2022-03-11T18:00:03.704581Z","shell.execute_reply":"2022-03-11T18:00:05.947682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plotImp(model, X , num = 30, fig_size = (20, 10)):\n    feature_imp = pd.DataFrame({'Value':model.feature_importance(),'Feature':X.columns})\n    \n    plt.figure(figsize=fig_size)\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", \n                                                        ascending=False)[0:num])\n    plt.title('LightGBM feature importance ')\n    plt.tight_layout()\n    plt.show()\n    \n#plotImp(model, X)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:00:05.94908Z","iopub.execute_input":"2022-03-11T18:00:05.949324Z","iopub.status.idle":"2022-03-11T18:00:05.955506Z","shell.execute_reply.started":"2022-03-11T18:00:05.949301Z","shell.execute_reply":"2022-03-11T18:00:05.954477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Zero value importances\n# feature_imp = pd.DataFrame({'Value':model.feature_importance(),'Feature':X.columns})\n# feature_imp[feature_imp[\"Value\"]==0].head(20)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:00:05.956471Z","iopub.execute_input":"2022-03-11T18:00:05.956725Z","iopub.status.idle":"2022-03-11T18:00:05.973038Z","shell.execute_reply.started":"2022-03-11T18:00:05.956695Z","shell.execute_reply":"2022-03-11T18:00:05.97159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict the 1 shifted section of test for confirmation \ntest_1pred = model.predict(  scaler.transform (test[test.index == min(test.index)]) ) \ntest_1pred","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:00:05.976492Z","iopub.execute_input":"2022-03-11T18:00:05.976956Z","iopub.status.idle":"2022-03-11T18:00:06.010065Z","shell.execute_reply.started":"2022-03-11T18:00:05.976929Z","shell.execute_reply":"2022-03-11T18:00:06.009536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CV","metadata":{}},{"cell_type":"code","source":"kfold = KFold(n_splits = 3)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:00:06.012698Z","iopub.execute_input":"2022-03-11T18:00:06.013114Z","iopub.status.idle":"2022-03-11T18:00:06.018827Z","shell.execute_reply.started":"2022-03-11T18:00:06.013086Z","shell.execute_reply":"2022-03-11T18:00:06.018059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cross_validation(X,y,test):\n    mae_list = []\n    test_preds = []\n    for idx, (train_idx, val_idx) in enumerate(kfold.split(X,y)):\n        \n        print(f\"\\n######## Fold {idx+1} ########\")\n        X_train, y_train = X.iloc[train_idx,:], y[train_idx]\n        X_test, y_test = X.iloc[val_idx,:], y[val_idx]\n        \n        if SCALING:\n            print(\"Applying Scaling\")\n            scaler = StandardScaler()\n            X_train = scaler.fit_transform(X_train)\n            X_test = scaler.transform(X_test)\n            test_s = scaler.transform(test)\n        else: \n            test_s = test.copy()\n        \n        #lightgbm/ catboost\n        model , mae  = fit_model(X_train,y_train,X_test, y_test)\n        test_preds.append(model.predict(test_s))   \n        mae_list.append(mae)\n        \n    print(\"Ensemble MAE\",np.mean(mae_list))    \n    return np.mean(test_preds,axis =0)\n\n#train on X,y , predict on only 1 period of test \n#test_preds = cross_validation(X,y, test[test.index == min(test.index)] )","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:00:06.020236Z","iopub.execute_input":"2022-03-11T18:00:06.021033Z","iopub.status.idle":"2022-03-11T18:00:06.036112Z","shell.execute_reply.started":"2022-03-11T18:00:06.020999Z","shell.execute_reply":"2022-03-11T18:00:06.035182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.figure(figsize = (20,8))\n\n# sns.lineplot(x = test[test.index == min(test.index)].reset_index().index,  y = np.mean(test_preds,axis =0), label = \"CV model prediction\")\n# sns.lineplot(x = test[test.index == min(test.index)].reset_index().index,  y = test_1pred, label = \"Base model prediction\")\n# plt.title(\"CrossVal vs base model: one step prediction \")\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:00:06.037176Z","iopub.execute_input":"2022-03-11T18:00:06.038115Z","iopub.status.idle":"2022-03-11T18:00:06.053245Z","shell.execute_reply.started":"2022-03-11T18:00:06.038044Z","shell.execute_reply":"2022-03-11T18:00:06.052483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Recursive ","metadata":{}},{"cell_type":"code","source":"PERIOD = 20 #prediction interval in minutes\n\nstart_date = min(test.index) \nend_date = max(test.index)\nprint(start_date)\nprint(end_date)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:00:06.05443Z","iopub.execute_input":"2022-03-11T18:00:06.05504Z","iopub.status.idle":"2022-03-11T18:00:06.075395Z","shell.execute_reply.started":"2022-03-11T18:00:06.055012Z","shell.execute_reply":"2022-03-11T18:00:06.074296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def multi_step_CV(start_date, end_date, period, train_i, test_i):\n    delta = pd.DateOffset(minutes = 20)\n    preds = []\n\n    all_df = pd.concat([train_i.assign(ds=\"train\"),test_i.assign(ds=\"test\")],axis =0)\n\n    while start_date <= end_date:\n\n        print(\"\\n############ Start date\" , start_date, \" ############\")\n\n        #shift and pivot\n        all_df = pvt_shift_values(all_df)\n\n        #prediction period and test slice\n        test_timeframe = (all_df.index>= start_date ) & (all_df.index< start_date+delta)\n\n        test_split = all_df [  test_timeframe ].drop([\"ds\",\"congestion\"],axis =1)\n        \n        X = all_df[ all_df.index< start_date].drop([\"ds\",\"congestion\"],axis =1)\n        y = all_df[ all_df.index< start_date][\"congestion\"]\n        \n        #predict 1 timeframe \n        one_period_preds = cross_validation(X,y, test_split )\n\n        #Add predicted test data back to all_df\n        all_df.loc[ test_timeframe ,\"congestion\"] = one_period_preds\n        \n        print( one_period_preds)\n        \n        #testing\n        preds.extend(list(one_period_preds))\n\n        start_date += delta\n    return preds\n\n#test_preds  = multi_step_CV(start_date, end_date, PERIOD, train, test)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:00:06.07678Z","iopub.execute_input":"2022-03-11T18:00:06.077181Z","iopub.status.idle":"2022-03-11T18:00:06.089097Z","shell.execute_reply.started":"2022-03-11T18:00:06.077133Z","shell.execute_reply":"2022-03-11T18:00:06.088436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def multi_step_fitmodel(start_date, end_date, period, train_i, test_i):\n    delta = pd.DateOffset(minutes = 20)\n    preds = []\n\n    all_df = pd.concat([train_i.assign(ds=\"train\"),test_i.assign(ds=\"test\")],axis =0)\n\n    while start_date <= end_date:\n\n        print(\"\\n############ Start date\" , start_date, \" ############\")\n\n        #shift and pivot\n        all_df = pvt_shift_values(all_df)\n\n        #prediction period and test slice\n        test_timeframe = (all_df.index>= start_date ) & (all_df.index< start_date+delta)\n\n        test_split = all_df [  test_timeframe ].drop([\"ds\",\"congestion\"],axis =1)\n        \n        X = all_df[ all_df.index< start_date].drop([\"ds\",\"congestion\"],axis =1)\n        y = all_df[ all_df.index< start_date][\"congestion\"]\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n        if SCALING:\n            print(\"Applying Scaling\")\n            scaler = StandardScaler()\n            X_train = scaler.fit_transform(X_train)\n            X_test = scaler.transform(X_test)\n            test_split = scaler.transform(test_split)\n\n        #predict 1 timeframe \n        model , mae  = fit_model(X_train,y_train,X_test, y_test)\n        one_period_preds = model.predict(test_split)\n\n        #Add predicted test data back to all_df\n        all_df.loc[ test_timeframe ,\"congestion\"] = one_period_preds\n        \n        print( one_period_preds)\n        #testing\n        preds.extend(list(one_period_preds))\n\n        start_date += delta\n    return preds\n\ntest_preds  = multi_step_fitmodel(start_date, end_date, PERIOD, train, test)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-03-11T18:00:06.090217Z","iopub.execute_input":"2022-03-11T18:00:06.091105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_multistep_rounded = sub.copy(deep = True)\nsub_multistep_rounded[\"congestion\"] =np.round(test_preds)\nsub_multistep_rounded.to_csv(\"sub_multistep_rounded.csv\")\n\nsub_multistep = sub.copy(deep = True)\nsub_multistep[\"congestion\"] =test_preds\nsub_multistep.to_csv(\"sub_multistep.csv\")\nsub_multistep","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.figure(figsize = (25,8))\n# sns.lineplot(data = sub_multistep , x= sub_multistep.index, y = \"congestion\", hue = test[\"x_y_direction\"].values, palette = \"bright\")\n# #sns.lineplot(data = train_original, x= train_original.index, y = \"congestion\", hue = train[\"x_y_direction\"].values, palette = \"bright\")\n# plt.show","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.iloc[-5000:][\"congestion\"].plot(figsize = (25,10))\nplt.plot(test.index, sub_multistep[\"congestion\"])\nplt.title(\"Actual vs predicted\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}