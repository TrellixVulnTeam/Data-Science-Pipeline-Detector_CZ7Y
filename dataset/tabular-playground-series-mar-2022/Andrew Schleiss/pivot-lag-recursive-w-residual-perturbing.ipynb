{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### To Do\n1. hyperparameter tuning- catboost and LGB\n1. different residual model \n1. different main model - random forests \n1. Missing values / backfill - currently set as backfill - try median /mean\n1. NB - add grouped median columns for each period (['x', 'y', 'direction', 'weekday', 'hour', 'minute']) \n\n### DONE\n1. Rolling median/mean \n1. Scaling   --- Scaling on \n1. label encoding/ ohe --- only labelencoding (TBC) \n1. CV with or without s --No shuffle\n1. added features \n1. Fit on full dataset\n1. residual perturbing","metadata":{"papermill":{"duration":0.04446,"end_time":"2022-03-14T10:10:46.521396","exception":false,"start_time":"2022-03-14T10:10:46.476936","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler,LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\nfrom sklearn.metrics import mean_absolute_error, r2_score\nfrom sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":2.838297,"end_time":"2022-03-14T10:10:49.40213","exception":false,"start_time":"2022-03-14T10:10:46.563833","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-15T13:50:43.639553Z","iopub.execute_input":"2022-03-15T13:50:43.639869Z","iopub.status.idle":"2022-03-15T13:50:43.647391Z","shell.execute_reply.started":"2022-03-15T13:50:43.639831Z","shell.execute_reply":"2022-03-15T13:50:43.646427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_original = pd.read_csv(\"../input/tabular-playground-series-mar-2022/train.csv\", index_col = 0)\ntest_original = pd.read_csv(\"../input/tabular-playground-series-mar-2022/test.csv\",index_col = 0)\nsub = pd.read_csv(\"../input/tabular-playground-series-mar-2022/sample_submission.csv\",index_col = 0)","metadata":{"papermill":{"duration":1.055062,"end_time":"2022-03-14T10:10:50.501417","exception":false,"start_time":"2022-03-14T10:10:49.446355","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-15T13:50:43.648935Z","iopub.execute_input":"2022-03-15T13:50:43.649638Z","iopub.status.idle":"2022-03-15T13:50:44.285803Z","shell.execute_reply.started":"2022-03-15T13:50:43.649599Z","shell.execute_reply":"2022-03-15T13:50:44.284729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_theme()\nTHRESHOLD = 0.40\n\nSCALING = False\nSCALER = StandardScaler()\n\nEPOCHS = 1000\nEARLY_STOPPING = 30\n\nLAG_FEATURES = True","metadata":{"papermill":{"duration":0.051461,"end_time":"2022-03-14T10:10:50.595122","exception":false,"start_time":"2022-03-14T10:10:50.543661","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-15T13:50:44.287001Z","iopub.execute_input":"2022-03-15T13:50:44.287245Z","iopub.status.idle":"2022-03-15T13:50:44.294671Z","shell.execute_reply.started":"2022-03-15T13:50:44.287202Z","shell.execute_reply":"2022-03-15T13:50:44.293558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def feature_engineering(df):\n    df= df.copy(deep = True)\n    df['time'] = pd.to_datetime(df['time'])\n    df[\"x_y_direction\"] = df[\"x\"].astype(\"str\") + df[\"y\"].astype(\"str\") + df[\"direction\"].astype(\"str\")\n    \n    return df\n\ntrain = feature_engineering(train_original)\ntest = feature_engineering(test_original)\ntrain","metadata":{"papermill":{"duration":2.443632,"end_time":"2022-03-14T10:10:53.082444","exception":false,"start_time":"2022-03-14T10:10:50.638812","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-15T13:50:44.296613Z","iopub.execute_input":"2022-03-15T13:50:44.296956Z","iopub.status.idle":"2022-03-15T13:50:46.793685Z","shell.execute_reply.started":"2022-03-15T13:50:44.296918Z","shell.execute_reply":"2022-03-15T13:50:46.792679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Additional Feature Engineering\nNow that we have the data in a correct pivot format \\ \nwe will create new columns for prediction ","metadata":{"papermill":{"duration":0.045914,"end_time":"2022-03-14T10:10:53.171708","exception":false,"start_time":"2022-03-14T10:10:53.125794","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train.set_index(\"time\", drop= True, inplace = True)\ntest.set_index(\"time\", drop= True, inplace = True)","metadata":{"papermill":{"duration":0.054715,"end_time":"2022-03-14T10:10:53.278333","exception":false,"start_time":"2022-03-14T10:10:53.223618","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-15T13:50:46.795446Z","iopub.execute_input":"2022-03-15T13:50:46.795758Z","iopub.status.idle":"2022-03-15T13:50:46.803697Z","shell.execute_reply.started":"2022-03-15T13:50:46.79572Z","shell.execute_reply":"2022-03-15T13:50:46.802511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"degree = {'EB':180, 'NB':90, 'SB':270, 'WB':0, 'NE':135, 'SW':315, 'NW': 45, 'SE':225}\n\ndef basic_feats(df):\n    df= df.copy(deep = True)\n    \n    df[\"minute\"] = df.index.minute\n    df[\"hour\"] = df.index.hour    \n    df[\"day\"] = df.index.day\n    df[\"month\"] = df.index.month\n    df[\"dayofweek\"]= df.index.weekday\n    \n    df['moment']  = df.index.hour * 3 + df.index.minute // 20 \n   # df[\"weekofmonth\"]= df['day']//7+1\n    \n    #New features\n    df[\"x+y\"] = df[\"x\"]+df[\"y\"]\n    df['x_y'] = df['x'].astype('str') + df['y'].astype('str')\n    df['hour_direction'] = df['hour'].astype('str') + df['direction'].astype('str')\n\n    df[\"degree\"] = df[\"direction\"].map(degree).astype(\"int32\")\n    #df['rad'] = math.pi * df['degree'] / 180\n#     df['afternoon'] = df['hour'] >= 12\n#     df['weekend'] = (df.index.weekday >= 5)\n\n    #df['saturday'] = df.index.weekday == 5\n    #df['sunday'] = df.index.weekday == 6\n    #df['daytime'] = df.index.hour * 60 + df.index.minute\n    #df['dayofyear'] = df.index.dayofyear # to model the trend\n\n    return df\n\ntrain= basic_feats(train)\ntest= basic_feats(test)\ntrain","metadata":{"papermill":{"duration":4.454231,"end_time":"2022-03-14T10:10:57.775616","exception":false,"start_time":"2022-03-14T10:10:53.321385","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-15T13:50:46.805551Z","iopub.execute_input":"2022-03-15T13:50:46.805867Z","iopub.status.idle":"2022-03-15T13:50:51.206119Z","shell.execute_reply.started":"2022-03-15T13:50:46.805828Z","shell.execute_reply":"2022-03-15T13:50:51.205429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"location_anomalies = [('21', 'NE', 15), ('22', 'SE', 20), ('22', 'NW', 21), ('21', 'NW', 29), ('21', 'SE', 34)]\ndef special_apply(x):\n    if (x[0] ==\"21\" and x[1] ==\"NE\") or (x[0] ==\"22\" and x[1] ==\"SE\") or (x[0] ==\"22\" and x[1] ==\"NW\") or (x[0] ==\"21\" and x[1] ==\"NW\")  or (x[0] ==\"21\" and x[1] ==\"SE\"):\n        y= 1\n    else:\n        y= 0\n    return y\n# train[\"special\"] = train[[\"x_y\",\"direction\"]].apply(special_apply ,axis =1)\n# test[\"special\"] = test[[\"x_y\",\"direction\"]].apply(special_apply ,axis =1)\n# train[\"special\"].value_counts()","metadata":{"papermill":{"duration":0.056671,"end_time":"2022-03-14T10:10:57.875816","exception":false,"start_time":"2022-03-14T10:10:57.819145","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-15T13:50:51.207194Z","iopub.execute_input":"2022-03-15T13:50:51.208065Z","iopub.status.idle":"2022-03-15T13:50:51.21606Z","shell.execute_reply.started":"2022-03-15T13:50:51.207989Z","shell.execute_reply":"2022-03-15T13:50:51.214992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encoding\nAs there is correlation between direction, x,y, my assumption is to:\n1. flatten the data (done further in kernel)\n1. label encode features\n\nIm unsure whether to OneHotEncode for categical features or LabelEncode to show correlation,  so im using both ","metadata":{"papermill":{"duration":0.04359,"end_time":"2022-03-14T10:10:57.963232","exception":false,"start_time":"2022-03-14T10:10:57.919642","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#Label Encoding\nencoder = LabelEncoder()\ndef lencoder(df, col , encoder):\n    df = df.copy(deep = True)\n    df[col] = encoder.fit_transform(df[col]).astype(\"int32\")\n    return df\n\nfor col in [\"x_y_direction\", \"direction\", \"x_y\", \"hour_direction\"]:\n    train = lencoder(train, col , encoder)\n    test = lencoder(test, col , encoder)\n\n# OnHotEncoding\nohe = OneHotEncoder(sparse = False,drop=\"first\")\n\ndef OHE(df, cols):\n    df = df.copy(deep = True)\n    encoded = pd.DataFrame(ohe.fit_transform(df[cols]),columns =ohe.get_feature_names_out(), index = df.index)\n    df = pd.concat([df, encoded], axis =1 )\n    return df \n\ntrain = OHE(train, [\"x_y_direction\"])\ntest = OHE(test, [\"x_y_direction\"])\n\ntrain.head()","metadata":{"papermill":{"duration":4.066604,"end_time":"2022-03-14T10:11:02.073617","exception":false,"start_time":"2022-03-14T10:10:58.007013","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-15T13:50:51.21738Z","iopub.execute_input":"2022-03-15T13:50:51.217617Z","iopub.status.idle":"2022-03-15T13:50:55.470008Z","shell.execute_reply.started":"2022-03-15T13:50:51.217589Z","shell.execute_reply":"2022-03-15T13:50:55.468978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Correlation Shifting  \nAs we have seen there is a correlation between in target when looked at specific x_y_direction groupings. \\\nAs such we will want to include these values in our training, however these values are part of the target\\\n\nWe have an option of multistep recursion, where we shift the values by 1 period (i.e. 20mins) and use these values \\\nThe process is as follow: \n* Concatenate train and test (needed for shifting train values into test )\n* groupby 'x_y_direction' column \n* shift concatenation by 1 (20mins) \n* add any lost columns needed due to groupby \n* Apply pivot transformation as seen above\n* Merge pivot to concatenated data \n\n\n**NOTE**: I backfilled null values - > with subsequent values --> option to use mean/median?","metadata":{"papermill":{"duration":0.043987,"end_time":"2022-03-14T10:11:02.162673","exception":false,"start_time":"2022-03-14T10:11:02.118686","status":"completed"},"tags":[]}},{"cell_type":"code","source":"features = list(train.columns)\nfeatures.append(\"ds\")\nprint(features)","metadata":{"papermill":{"duration":0.054564,"end_time":"2022-03-14T10:11:02.262966","exception":false,"start_time":"2022-03-14T10:11:02.208402","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-15T13:50:55.473668Z","iopub.execute_input":"2022-03-15T13:50:55.474008Z","iopub.status.idle":"2022-03-15T13:50:55.480796Z","shell.execute_reply.started":"2022-03-15T13:50:55.473972Z","shell.execute_reply":"2022-03-15T13:50:55.479571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_df = pd.concat([train.assign(ds=\"train\"),test.assign(ds=\"test\")],axis =0)\n\ndef pvt_shift_values(all_df):\n    \n    all_shift = all_df.groupby(['x_y_direction'])[[\"congestion\"]].shift(1).fillna(method='bfill')\n\n    #ensure have the correct column for pivot\n    all_shift[\"x_y_direction\"]  =all_df[\"x\"].astype(\"str\") + all_df[\"y\"].astype(\"str\") +all_df[\"direction\"].astype(\"str\")\n    \n    #Pivot \n    all_pvt = all_shift.pivot_table(\n        values='congestion', \n        index='time', \n        columns='x_y_direction', \n        aggfunc=np.sum)\n    \n    #backfill row 0 as this caused issues in the pvt due to shift \n    all_pvt.iloc[0] =all_pvt.iloc[1]\n    \n    #Merge pivot data to full data set \n    all_merge = pd.merge(all_df[features], all_pvt,  how='left', right_index=True, left_index=True)\n    \n    # Rolling values \n    #all_merge[\"rolling_median_3\"] = all_merge.groupby(\"x_y_direction\")[\"congestion\"].transform(lambda s: s.rolling(3, min_periods=1).median())\n    #all_merge[\"rolling_median_8\"] = all_merge.groupby(\"x_y_direction\")[\"congestion\"].transform(lambda s: s.rolling(6, min_periods=1).median())\n    #all_merge[\"rolling_median_12\"] = all_merge.groupby(\"x_y_direction\")[\"congestion\"].transform(lambda s: s.rolling(9, min_periods=1).median())\n    #all_merge[\"rolling_std_4\"] = all_merge.groupby(\"x_y_direction\")[\"congestion\"].transform(lambda s: s.rolling(4, min_periods=1).std())\n    #all_merge[\"rolling_std_8\"] = all_merge.groupby(\"x_y_direction\")[\"congestion\"].transform(lambda s: s.rolling(8, min_periods=1).std())\n    \n    #backfill std deviation\n    #all_merge.loc[all_merge.index == pd.to_datetime(\"1991-04-01 00:00:00\"), \"rolling_std_4\"] = all_merge.loc[all_merge.index == pd.to_datetime(\"1991-04-01 00:20:00\")][\"rolling_std_4\"].values\n    \n    return all_merge \n\nall_df = pvt_shift_values(all_df)\n\n#check \nall_df[all_df.index == min(test.index)].head(20)","metadata":{"papermill":{"duration":7.139927,"end_time":"2022-03-14T10:11:09.448744","exception":false,"start_time":"2022-03-14T10:11:02.308817","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-15T13:50:55.483179Z","iopub.execute_input":"2022-03-15T13:50:55.483595Z","iopub.status.idle":"2022-03-15T13:51:02.060519Z","shell.execute_reply.started":"2022-03-15T13:50:55.483547Z","shell.execute_reply":"2022-03-15T13:51:02.059846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# all_df = pd.concat([train.assign(ds=\"train\"),test.assign(ds=\"test\")],axis =0)\n\n# def pvt_shift_values(all_df):\n#     all_df[\"congestion_1\"] = all_df.groupby(['x_y_direction'])[[\"congestion\"]].shift(1)\n#     all_df[\"congestion_2\"] = all_df.groupby(['x_y_direction'])[[\"congestion\"]].shift(2)\n\n#     #Pivot \n#     all_pvt_1 = all_df.pivot_table(\n#         values='congestion_1', \n#         index='time', \n#         columns='x_y_direction', \n#         aggfunc=np.sum)\n#     all_pvt_2 = all_df.pivot_table(\n#         values='congestion_2', \n#         index='time', \n#         columns='x_y_direction', \n#         aggfunc=np.sum)\n\n#     all_pvt= pd.merge(all_pvt_1,all_pvt_2, left_index=True, right_index= True)\n#     all_merge = pd.merge(all_df[features], all_pvt,  how='left', right_index=True, left_index=True)\n#     return all_merge\n\n# all_df = pvt_shift_values(all_df)\n\n# #check \n# all_df[all_df.index == min(test.index)].head(20)","metadata":{"papermill":{"duration":0.05428,"end_time":"2022-03-14T10:11:09.550623","exception":false,"start_time":"2022-03-14T10:11:09.496343","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-15T13:51:02.061596Z","iopub.execute_input":"2022-03-15T13:51:02.062342Z","iopub.status.idle":"2022-03-15T13:51:02.066844Z","shell.execute_reply.started":"2022-03-15T13:51:02.062304Z","shell.execute_reply":"2022-03-15T13:51:02.066092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Target shifting (and Rolling)\nI removed rolling mean/median/std as this seems to cause a lot of incorrect validation so its tricky to analyse results","metadata":{"papermill":{"duration":0.045399,"end_time":"2022-03-14T10:11:09.642258","exception":false,"start_time":"2022-03-14T10:11:09.596859","status":"completed"},"tags":[]}},{"cell_type":"code","source":"shift_list = [1, 3, 6, 9\n             ]\nroll_window = [4,8, 12]","metadata":{"papermill":{"duration":0.053089,"end_time":"2022-03-14T10:11:09.741151","exception":false,"start_time":"2022-03-14T10:11:09.688062","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-15T13:51:02.068337Z","iopub.execute_input":"2022-03-15T13:51:02.068557Z","iopub.status.idle":"2022-03-15T13:51:02.085332Z","shell.execute_reply.started":"2022-03-15T13:51:02.068529Z","shell.execute_reply":"2022-03-15T13:51:02.084556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def shift_target(df, shift_list, groupby):\n    df = df.copy(deep = True)\n    \n    for i in shift_list:\n        #df[f\"shift_{i}\"]  = df.groupby(groupby)['congestion'].shift(1, fill_value=0)\n        df[f\"shift_{i}\"] = df.groupby(groupby)['congestion'].transform(lambda s: s.shift(i))\n        df[f\"shift_{i}\"] =df.groupby(\"x_y_direction\")[f\"shift_{i}\"].backfill()\n    return df\n\ndef rolling(df, roll_window, groupby):\n    df = df.copy(deep = True)\n    \n    for i in roll_window:\n        df[f\"rolling_mean_{i}\"] = df.groupby(groupby)['shift_1'].transform(lambda s: s.rolling(i, min_periods=1).mean())\n        df[f\"rolling_median_{i}\"] = df.groupby(groupby)['shift_1'].transform(lambda s: s.rolling(i, min_periods=1).median())\n        df[f\"rolling_std_{i}\"] = df.groupby(groupby)['shift_1'].transform(lambda s: s.rolling(i, min_periods=1).std())\n    return df.fillna(0)\n\nif LAG_FEATURES:\n    print(\"Lag Target\")\n    all_df = shift_target(all_df,shift_list,groupby= 'x_y_direction')\n    all_df = rolling(all_df,roll_window, groupby= 'x_y_direction')\nall_df","metadata":{"papermill":{"duration":0.2365,"end_time":"2022-03-14T10:11:10.023888","exception":false,"start_time":"2022-03-14T10:11:09.787388","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-15T13:51:02.086851Z","iopub.execute_input":"2022-03-15T13:51:02.087375Z","iopub.status.idle":"2022-03-15T13:51:03.852032Z","shell.execute_reply.started":"2022-03-15T13:51:02.087328Z","shell.execute_reply":"2022-03-15T13:51:03.851124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### shift by a week","metadata":{}},{"cell_type":"code","source":"all_df[\"shift_week\"] = all_df.groupby(\"x_y_direction\")['congestion'].transform(lambda s: s.shift(7, freq = \"D\"))\nall_df['shift_week'] =all_df.groupby(\"x_y_direction\")['shift_week'].backfill()\nall_df ","metadata":{"execution":{"iopub.status.busy":"2022-03-15T13:51:03.853244Z","iopub.execute_input":"2022-03-15T13:51:03.853502Z","iopub.status.idle":"2022-03-15T13:51:04.446635Z","shell.execute_reply.started":"2022-03-15T13:51:03.853471Z","shell.execute_reply":"2022-03-15T13:51:04.445596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check \nall_df[all_df.index ==pd.to_datetime(\"1991-04-08 00:00:00\")]","metadata":{"execution":{"iopub.status.busy":"2022-03-15T13:51:04.447981Z","iopub.execute_input":"2022-03-15T13:51:04.448237Z","iopub.status.idle":"2022-03-15T13:51:05.173747Z","shell.execute_reply.started":"2022-03-15T13:51:04.448189Z","shell.execute_reply":"2022-03-15T13:51:05.172853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Downcasting","metadata":{}},{"cell_type":"code","source":"#reduce memory \n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n\n    for col in df.columns:\n        col_type = df[col].dtypes\n\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n \n    return df\n\nreduce_mem_usage(all_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T13:51:05.174995Z","iopub.execute_input":"2022-03-15T13:51:05.175236Z","iopub.status.idle":"2022-03-15T13:51:27.265184Z","shell.execute_reply.started":"2022-03-15T13:51:05.175195Z","shell.execute_reply":"2022-03-15T13:51:27.264197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scaling and splitting ","metadata":{"papermill":{"duration":0.053541,"end_time":"2022-03-14T10:11:10.127603","exception":false,"start_time":"2022-03-14T10:11:10.074062","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train = all_df[all_df[\"ds\"] ==\"train\"].drop(\"ds\",axis =1)\ntest = all_df[all_df[\"ds\"] ==\"test\"].drop([\"ds\",\"congestion\"],axis =1)\n#Check\ntest[test.index ==pd.to_datetime(\"1991-09-30 13:00:00\")]","metadata":{"papermill":{"duration":1.339902,"end_time":"2022-03-14T10:11:11.517327","exception":false,"start_time":"2022-03-14T10:11:10.177425","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-15T13:51:27.266428Z","iopub.execute_input":"2022-03-15T13:51:27.266737Z","iopub.status.idle":"2022-03-15T13:51:28.328868Z","shell.execute_reply.started":"2022-03-15T13:51:27.266704Z","shell.execute_reply":"2022-03-15T13:51:28.327968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#full fit\nX = train.drop(\"congestion\",axis =1 )\ny = train[\"congestion\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nscaler = SCALER\n\nif SCALING:\n    print(\"Applying Scaling\")\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n    test_s = scaler.transform(test)\nelse: \n    test_s = test.copy()","metadata":{"papermill":{"duration":43.041165,"end_time":"2022-03-14T10:11:54.606836","exception":false,"start_time":"2022-03-14T10:11:11.565671","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-15T13:51:28.330276Z","iopub.execute_input":"2022-03-15T13:51:28.330489Z","iopub.status.idle":"2022-03-15T13:51:29.522478Z","shell.execute_reply.started":"2022-03-15T13:51:28.330463Z","shell.execute_reply":"2022-03-15T13:51:29.521544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fit model: Train data\nLightgbm","metadata":{"papermill":{"duration":0.049797,"end_time":"2022-03-14T10:11:54.707443","exception":false,"start_time":"2022-03-14T10:11:54.657646","status":"completed"},"tags":[]}},{"cell_type":"code","source":"num_cols = [col for col in train.columns if train[col].dtype == 'int64' and col not in [\"congestion\"]]","metadata":{"execution":{"iopub.status.busy":"2022-03-15T13:51:29.523986Z","iopub.execute_input":"2022-03-15T13:51:29.524263Z","iopub.status.idle":"2022-03-15T13:51:29.536878Z","shell.execute_reply.started":"2022-03-15T13:51:29.524197Z","shell.execute_reply":"2022-03-15T13:51:29.535845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if SCALING:\n    X_s = scaler.fit_transform(X)\nelse:\n    X_s = X.copy()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T13:51:29.538252Z","iopub.execute_input":"2022-03-15T13:51:29.53852Z","iopub.status.idle":"2022-03-15T13:51:29.736438Z","shell.execute_reply.started":"2022-03-15T13:51:29.538488Z","shell.execute_reply":"2022-03-15T13:51:29.735684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb_params = {\"force_col_wise\" : True ,\n              \"objective\" : \"regression\",\n              \"num_threads\": -1,\n              \"metric\": \"mae\",\n              'learning_rate': 0.03386223199544998,\n              'boosting': 'gbdt', \n              'lambda_l1': 2.989505976417424e-07, \n              'lambda_l2': 1.6651524609127486e-06, \n              'num_leaves': 236,\n              'max_depth': 12,\n              \"device_type\": \"cpu\",\n              \"gpu_platform_id\" : 0,\n              \"gpu_device_id\" : 0\n             }","metadata":{"papermill":{"duration":0.059322,"end_time":"2022-03-14T10:11:54.816191","exception":false,"start_time":"2022-03-14T10:11:54.756869","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-15T13:51:29.737455Z","iopub.execute_input":"2022-03-15T13:51:29.737902Z","iopub.status.idle":"2022-03-15T13:51:29.744388Z","shell.execute_reply.started":"2022-03-15T13:51:29.737864Z","shell.execute_reply":"2022-03-15T13:51:29.743183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fit_model_lgb(X,y):\n    train_set = lgb.Dataset(X, y, params={'verbose': -1})\n\n    model = lgb.train(params=lgb_params,\n                          train_set= train_set, \n                          #valid_sets= (valid_set), \n                          num_boost_round= EPOCHS,\n                          #callbacks=[lgb.early_stopping(EARLY_STOPPING)]  \n                     ) \n    train_preds = model.predict(X)\n    print(\"\\nIntrinsic MAE\", mean_absolute_error(y, train_preds))\n    print(\"Intrinsic R2\", r2_score(y, train_preds))    \n    return model, train_preds\n\n#predict\nmodel, train_preds   = fit_model_lgb(X_s,y)","metadata":{"papermill":{"duration":341.334365,"end_time":"2022-03-14T10:17:36.200346","exception":false,"start_time":"2022-03-14T10:11:54.865981","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-15T13:51:29.745862Z","iopub.execute_input":"2022-03-15T13:51:29.746094Z","iopub.status.idle":"2022-03-15T13:56:48.266642Z","shell.execute_reply.started":"2022-03-15T13:51:29.746064Z","shell.execute_reply":"2022-03-15T13:56:48.265806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fit Model: Residual Data \nI wanted a different model to predict residuals as I assumed a different model would identify different aspects that the original model didnt \\\n**Option** Use NN or linear model for residuals ","metadata":{"papermill":{"duration":0.060273,"end_time":"2022-03-14T10:17:36.423942","exception":false,"start_time":"2022-03-14T10:17:36.363669","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def fit_model_ET(X,y):\n    train_set = lgb.Dataset(X, y, params={'verbose': -1})\n    model = ExtraTreesRegressor(n_estimators = 20, min_samples_split = 101,n_jobs=-1)\n    model.fit(X,y)\n    train_preds = model.predict(X)\n    print(\"\\nIntrinsic MAE\", mean_absolute_error(y, train_preds))\n    print(\"Intrinsic R2\", r2_score(y, train_preds))\n    return model, train_preds\n\n# fit ET model on residuals \nmodel_res, train_res   = fit_model_ET(X_s,  (y - train_preds) )","metadata":{"papermill":{"duration":0.072126,"end_time":"2022-03-14T10:17:36.559445","exception":false,"start_time":"2022-03-14T10:17:36.487319","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-15T13:56:48.267779Z","iopub.execute_input":"2022-03-15T13:56:48.268152Z","iopub.status.idle":"2022-03-15T14:05:20.508042Z","shell.execute_reply.started":"2022-03-15T13:56:48.268123Z","shell.execute_reply":"2022-03-15T14:05:20.506815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fit_model_cat(X,y):\n    \n    model = CatBoostRegressor(\n        verbose=1000,\n        early_stopping_rounds=10,\n        random_seed=2022,\n        max_depth=12,\n        task_type='CPU',\n        learning_rate=0.035,\n        iterations=EPOCHS,\n        loss_function='MAE',\n        eval_metric= 'MAE'\n    ).fit(X, y)\n    \n    train_preds =model.predict(X) \n    \n    print(\"\\nIntrinsic MAE\", mean_absolute_error(y, train_preds))\n    print(\"Intrinsic R2\", r2_score(y, train_preds))   \n    \n    return model,train_preds,\n\n#model, train_preds   = fit_model_cat(X_s,y)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T14:05:20.514069Z","iopub.execute_input":"2022-03-15T14:05:20.514365Z","iopub.status.idle":"2022-03-15T14:05:20.521507Z","shell.execute_reply.started":"2022-03-15T14:05:20.514332Z","shell.execute_reply":"2022-03-15T14:05:20.520601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Analysis ","metadata":{"papermill":{"duration":0.058947,"end_time":"2022-03-14T10:22:44.248954","exception":false,"start_time":"2022-03-14T10:22:44.190007","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train.iloc[-10000:][\"congestion\"].plot(figsize = (25,10), label = \"actual\")\nplt.plot(train.iloc[-10000:].index, train_preds[-10000:], label = \"prediction\")\nplt.plot(train.iloc[-10000:].index, (train_preds+train_res)[-10000:], label = \"prediction + residuals\")\nplt.title(\"Predicted vs Residuals + Predicted vs Actual\")\nplt.legend()\nplt.show()","metadata":{"papermill":{"duration":1.00215,"end_time":"2022-03-14T10:22:45.308987","exception":false,"start_time":"2022-03-14T10:22:44.306837","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-15T14:05:20.523113Z","iopub.execute_input":"2022-03-15T14:05:20.523661Z","iopub.status.idle":"2022-03-15T14:05:21.185428Z","shell.execute_reply.started":"2022-03-15T14:05:20.523615Z","shell.execute_reply":"2022-03-15T14:05:21.184478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (25,5))\nplt.hist((y-train_preds), bins = 100, label = \"LGB Residuals \" , alpha =0.5)\nplt.hist(train_res, bins = 100 , label = \"ET Predicted Residuals \", alpha =0.5)\nplt.title(\"Residuals\")\nplt.show()","metadata":{"papermill":{"duration":0.500821,"end_time":"2022-03-14T10:22:45.872975","exception":false,"start_time":"2022-03-14T10:22:45.372154","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-15T14:05:21.186852Z","iopub.execute_input":"2022-03-15T14:05:21.187745Z","iopub.status.idle":"2022-03-15T14:05:21.837127Z","shell.execute_reply.started":"2022-03-15T14:05:21.187693Z","shell.execute_reply":"2022-03-15T14:05:21.836219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plotImp(model, X , num = 30, fig_size = (20, 10)):\n    feature_imp = pd.DataFrame({'Value':model.feature_importance(),'Feature':X.columns})\n    \n    plt.figure(figsize=fig_size)\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", \n                                                        ascending=False)[0:num])\n    plt.title('Top LightGBM feature importance ')\n    plt.tight_layout()\n    plt.show()\n    \nplotImp(model, X)","metadata":{"papermill":{"duration":0.612955,"end_time":"2022-03-14T10:22:46.550388","exception":false,"start_time":"2022-03-14T10:22:45.937433","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-15T14:05:21.838527Z","iopub.execute_input":"2022-03-15T14:05:21.83886Z","iopub.status.idle":"2022-03-15T14:05:22.382921Z","shell.execute_reply.started":"2022-03-15T14:05:21.838815Z","shell.execute_reply":"2022-03-15T14:05:22.381919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Zero value importances\nfeature_imp = pd.DataFrame({'Value':model.feature_importance(),'Feature':X.columns})\nfeature_imp[feature_imp[\"Value\"]==0].head(20)","metadata":{"papermill":{"duration":0.080039,"end_time":"2022-03-14T10:22:46.695412","exception":false,"start_time":"2022-03-14T10:22:46.615373","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-15T14:05:22.384525Z","iopub.execute_input":"2022-03-15T14:05:22.384811Z","iopub.status.idle":"2022-03-15T14:05:22.399289Z","shell.execute_reply.started":"2022-03-15T14:05:22.384779Z","shell.execute_reply":"2022-03-15T14:05:22.398621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Recursive + Peturbing \n## Multi-Step Recursive Time Series  Forecasting \nThe concept of multistep recursive is a \"walk forward\" model that predicts one period at a time of the test data (i.e. 20min intervals) \\\nThe predicted data is added to the training data and the next period (or window) is predicted \\ \nThis continues till the full training data is predicted \n\nThis concept is done in a previous notebook  \n\n## Perturbing the predictions \n\nThis concept comes from this [paper](https://www.researchgate.net/publication/220764883_Recursive_Multi-step_Time_Series_Forecasting_by_Perturbing_Data) \nDue to the inherent issue with multi-steo forecasts which suffers from the accumulation of errors (i.e. a poor initial prediction is projected into the subsequent predictions)  \n\nThe process uses a second model to predict the residuals of each time period (window), these residuals are added to the test predictions \\\nI will try 2 approaches: \n1. Save the residuals and add the resudiausl to the full test predictions after the full multi-step process is completed \n1. Add the resdiuals after each step ","metadata":{"papermill":{"duration":0.066195,"end_time":"2022-03-14T10:22:46.826993","exception":false,"start_time":"2022-03-14T10:22:46.760798","status":"completed"},"tags":[]}},{"cell_type":"code","source":"PERIOD = 20 #prediction interval in minutes\n\nstart_date = min(test.index) \nend_date = max(test.index)\nprint(start_date)\nprint(end_date)","metadata":{"papermill":{"duration":0.080415,"end_time":"2022-03-14T10:22:47.109046","exception":false,"start_time":"2022-03-14T10:22:47.028631","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-15T14:05:22.400326Z","iopub.execute_input":"2022-03-15T14:05:22.401157Z","iopub.status.idle":"2022-03-15T14:05:22.413603Z","shell.execute_reply.started":"2022-03-15T14:05:22.401125Z","shell.execute_reply":"2022-03-15T14:05:22.412654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submit","metadata":{"papermill":{"duration":0.495473,"end_time":"2022-03-14T17:05:07.002308","exception":false,"start_time":"2022-03-14T17:05:06.506835","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def multi_step_fitmodel(start_date, end_date, period, train_i, test_i):\n    \n    preds = []\n    residuals = []\n    delta = pd.DateOffset(minutes = PERIOD)\n    all_df = pd.concat([train_i.assign(ds=\"train\"),test_i.assign(ds=\"test\")],axis =0)\n    \n    while start_date <= end_date:\n\n        print(\"\\n############ Start date\" , start_date, \" ############\")\n\n        #shift and pivot\n        all_df = pvt_shift_values(all_df)\n        \n        if LAG_FEATURES:\n            \"Applying lag features\"\n            all_df = shift_target(all_df,shift_list,groupby= 'x_y_direction')\n            all_df = rolling(all_df,roll_window, groupby= 'x_y_direction')\n\n        #prediction period and test slice\n        test_timeframe = (all_df.index>= start_date ) & (all_df.index< start_date+delta)\n        test_split = all_df [  test_timeframe ].drop([\"ds\",\"congestion\"],axis =1)\n        \n        X = all_df[ all_df.index< start_date].drop([\"ds\",\"congestion\"],axis =1)\n        y = all_df[ all_df.index< start_date][\"congestion\"]\n\n        if SCALING:\n            print(\"Applying Scaling\")\n            scaler = StandardScaler()\n            X[num_cols] = scaler.fit_transform(X[num_cols])\n            test_split[num_cols] = scaler.transform(test_split[num_cols])\n        \n        #Fit training data\n        model, train_preds   = fit_model_lgb(X,y)\n        one_period_preds = model.predict(test_split)\n        \n        # fit new model on residuals \n        model_res, train_res   = fit_model_ET(X, y - train_preds)\n        residuals_preds = model_res.predict(test_split)\n\n        #Add predicted test data back to all_df\n        all_df.loc[ test_timeframe ,\"congestion\"] = one_period_preds + residuals_preds\n        preds.extend(list(one_period_preds + residuals_preds))\n        residuals.extend(list(residuals_preds))\n        \n        del test_split\n        del test_timeframe\n\n        start_date += delta\n    return preds, residuals\n\ntest_preds, residuals  = multi_step_fitmodel(start_date, end_date, PERIOD, train, test)","metadata":{"papermill":{"duration":24138.702623,"end_time":"2022-03-14T17:05:06.016272","exception":false,"start_time":"2022-03-14T10:22:47.313649","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-15T14:05:22.414874Z","iopub.execute_input":"2022-03-15T14:05:22.415116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_preds[:20]","metadata":{"papermill":{"duration":0.502292,"end_time":"2022-03-14T17:05:07.993871","exception":false,"start_time":"2022-03-14T17:05:07.491579","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"residuals[:20]","metadata":{"papermill":{"duration":0.500755,"end_time":"2022-03-14T17:05:08.9988","exception":false,"start_time":"2022-03-14T17:05:08.498045","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## residuals added in model\n\nsub_rec_rounded = sub.copy(deep = True)\nsub_rec_rounded[\"congestion\"] =test_preds\nsub_rec_rounded[\"congestion\"] =np.round(sub_rec_rounded[\"congestion\"])\nsub_rec_rounded.to_csv(\"sub_rec_rounded.csv\")\n\nsub_rec = sub.copy(deep = True)\nsub_rec[\"congestion\"] =test_preds\nsub_rec.to_csv(\"sub_rec.csv\")\nsub_rec_rounded","metadata":{"papermill":{"duration":0.547027,"end_time":"2022-03-14T17:05:10.038077","exception":false,"start_time":"2022-03-14T17:05:09.49105","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.iloc[-5000:][\"congestion\"].plot(figsize = (25,10))\nplt.plot(test.index, test_preds)\nplt.title(\"Predicted Only vs Actual\")\nplt.show()","metadata":{"papermill":{"duration":0.941861,"end_time":"2022-03-14T17:05:12.457798","exception":false,"start_time":"2022-03-14T17:05:11.515937","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train.iloc[-5000:][\"congestion\"].plot(figsize = (25,10))\n# plt.plot(test.index, sub_rec_rounded[\"congestion\"])\n# plt.title(\"Predicted + Residuals vs Actual\")\n# plt.show()","metadata":{"papermill":{"duration":0.934486,"end_time":"2022-03-14T17:05:13.885667","exception":false,"start_time":"2022-03-14T17:05:12.951181","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.495615,"end_time":"2022-03-14T17:05:14.879399","exception":false,"start_time":"2022-03-14T17:05:14.383784","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}