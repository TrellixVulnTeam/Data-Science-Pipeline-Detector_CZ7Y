{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Clustering morning commutes\nIntuitively, it makes sense that the congestion on similar mornings will have similar congestion levels for the rest of the day. I therefore tried clustering together the morning commutes and just pooling their afternoon/evening commutes and using them as predictions.","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport re\nimport datetime\nfrom sklearn.neighbors import NearestNeighbors\nimport matplotlib.pyplot as plt\nimport time\nfrom joblib import Parallel, delayed\nimport seaborn as sns\n\ninput_dir = '/kaggle/input/tabular-playground-series-mar-2022/'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-03T13:58:00.74296Z","iopub.execute_input":"2022-03-03T13:58:00.744229Z","iopub.status.idle":"2022-03-03T13:58:00.957897Z","shell.execute_reply.started":"2022-03-03T13:58:00.744133Z","shell.execute_reply":"2022-03-03T13:58:00.956884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Feature engineering\ndef handle_dates(df):\n    df['datetime'] = pd.to_datetime(df['time'])\n    df['time'] = [datetime.datetime.time(d) for d in df.loc[:,'datetime']] \n    time_mapping = {t:ii for ii,t in enumerate(train.time.unique())}\n    df['time_number'] = [time_mapping[d] for d in df.loc[:,'time']] \n    df['date'] = [datetime.datetime.date(d) for d in df.loc[:,'datetime']] \n    df['weekday'] = [d.weekday() for d in df.datetime]\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-03-03T12:04:00.333576Z","iopub.execute_input":"2022-03-03T12:04:00.333824Z","iopub.status.idle":"2022-03-03T12:04:00.342346Z","shell.execute_reply.started":"2022-03-03T12:04:00.333795Z","shell.execute_reply":"2022-03-03T12:04:00.341219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Load and transform data\ntrain = pd.read_csv(input_dir + 'train.csv')\ntest = pd.read_csv(input_dir + 'test.csv')\ntrain = handle_dates(train)\ntest = handle_dates(test)\n\nprint('Train shape: ' + str(train.shape) + ', Test shape:' + str(test.shape))","metadata":{"execution":{"iopub.status.busy":"2022-03-03T12:04:00.34387Z","iopub.execute_input":"2022-03-03T12:04:00.344278Z","iopub.status.idle":"2022-03-03T12:04:14.213473Z","shell.execute_reply.started":"2022-03-03T12:04:00.344238Z","shell.execute_reply":"2022-03-03T12:04:14.212788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Pivot the data to be a single row per day\ntrain.loc[:,'loc_dir_time'] = [str(x)+str(y)+direction+str(t) for _, x,y,direction,t in train.loc[:,['x','y','direction','time_number']].itertuples()]\ntest.loc[:,'loc_dir_time'] = [str(x)+str(y)+direction+str(t) for _, x,y,direction,t in test.loc[:,['x','y','direction','time_number']].itertuples()]\nXy = train.loc[:,['loc_dir_time','weekday','congestion','date']]\nXy = pd.pivot_table(Xy, values='congestion', index=['date', 'weekday'], columns=['loc_dir_time'], ).reset_index()","metadata":{"execution":{"iopub.status.busy":"2022-03-03T12:04:14.214994Z","iopub.execute_input":"2022-03-03T12:04:14.215886Z","iopub.status.idle":"2022-03-03T12:04:17.679579Z","shell.execute_reply.started":"2022-03-03T12:04:14.21578Z","shell.execute_reply":"2022-03-03T12:04:17.678853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Save the 30th of September for the final prediction\nFINAL = Xy.query('date==datetime.date(1991,9,30)')\nXy = Xy.query('date<datetime.date(1991,9,30)')","metadata":{"execution":{"iopub.status.busy":"2022-03-03T12:04:17.681364Z","iopub.execute_input":"2022-03-03T12:04:17.682436Z","iopub.status.idle":"2022-03-03T12:04:18.653245Z","shell.execute_reply.started":"2022-03-03T12:04:17.682352Z","shell.execute_reply":"2022-03-03T12:04:18.652503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Some days have missing timestamps for certain location and direction combinations\n##At the moment I am just imputing using the median value for that location, direction, time, day combination\n##However, I highly suspect this isnt the best method as it may well be causing data leakage in the later analysis\n##Fixing this is on my todo list\nXy = Xy.fillna(Xy.groupby('weekday').transform('median')).set_index('date')","metadata":{"execution":{"iopub.status.busy":"2022-03-03T12:04:18.655086Z","iopub.execute_input":"2022-03-03T12:04:18.655745Z","iopub.status.idle":"2022-03-03T12:04:28.107863Z","shell.execute_reply.started":"2022-03-03T12:04:18.655696Z","shell.execute_reply":"2022-03-03T12:04:28.10682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Split the data into independent and dependent data by using all times from noon-midnight as dependent variables and midnight-noon as independent variables\ncols = Xy.columns\ntime_number = [int(re.findall('(?<=[A-Z])[0-9]+',x)[0]) for x in cols[1:]]\nX = Xy.loc[:,[False]+[t<36 for t in time_number]]\ny = Xy.loc[:,[False]+[t>=36 for t in time_number]]\nX_train = X#.loc[Xy.weekday<5,:]\ny_train = y#.loc[Xy.weekday<5]\nyt = y_train.values\nX_final = FINAL.loc[:,[False,False]+[t<36 for t in time_number]]","metadata":{"execution":{"iopub.status.busy":"2022-03-03T12:04:28.109117Z","iopub.execute_input":"2022-03-03T12:04:28.10938Z","iopub.status.idle":"2022-03-03T12:04:28.134683Z","shell.execute_reply.started":"2022-03-03T12:04:28.10935Z","shell.execute_reply":"2022-03-03T12:04:28.133987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Create some functions for checking the results\n##Try raw predictions, rounding, and taking the floor of them\n##Create some functions for checking the results\n##Try raw predictions, rounding, and taking the floor of them\ndef get_mae(A):\n    S_MAE = [np.mean(a) for a in A]\n    S = np.mean(S_MAE)\n    return S,S_MAE\n    \ndef get_scores(n,c1,c2,res, mae, PP, pooling_method):\n    ABS = [np.abs(yt[ii,:] - PP[ii]) for ii in range(len(PP))]\n    ABS_rounded = [np.abs(yt[ii,:] - np.round(PP[ii])) for ii in range(len(PP))]\n    ABS_int = [np.abs(yt[ii,:] - np.floor(PP[ii])) for ii in range(len(PP))]\n    \n    RAW, RAW_MAE = get_mae(ABS)\n    ROUNDED, ROUNDED_MAE = get_mae(ABS_rounded)\n    FLOOR, FLOOR_MAE = get_mae(ABS_int)\n\n    res.append([n, c1,c2, 'raw', RAW, pooling_method])\n    res.append([n, c1,c2, 'rounded', ROUNDED, pooling_method])\n    res.append([n, c1,c2, 'floor', FLOOR, pooling_method])  \n    \n    mae[0].append(RAW_MAE)\n    mae[1].append(ROUNDED_MAE)\n    mae[2].append(FLOOR_MAE)\n    \n    return res, [RAW,ROUNDED,FLOOR], mae\n\ndef update(best_score,\n           best_params,\n           score,\n           name,\n           method):\n    if score<best_score:\n        best_score = score\n        best_params = [n,cutoff,max_cutoff,name,method]\n    return best_score, best_params","metadata":{"execution":{"iopub.status.busy":"2022-03-03T12:04:28.136356Z","iopub.execute_input":"2022-03-03T12:04:28.136801Z","iopub.status.idle":"2022-03-03T12:04:28.148677Z","shell.execute_reply.started":"2022-03-03T12:04:28.13677Z","shell.execute_reply":"2022-03-03T12:04:28.14806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Try clustering using:\n## 1. different cluster sizes\n## 2. different amounts of morning data (i.e. midnight-1am may not be as important as its no as recent as the other times)\n## 3. pooling the neighbours by mean or median\n\ndef myfunc(arg):\n    global time_number, X_train, y_train, yt, get_scores\n    c1, c2, max_clust = arg\n    NN = NearestNeighbors(n_neighbors=max_clust,metric = 'manhattan')\n    NN.fit(X_train.loc[:,[(t>=c1) & (t<=c2) for t in time_number if t<36]])\n    distances, indices = NN.kneighbors(X_train.loc[:,[(t>=c1) & (t<=c2) for t in time_number if t<36]])\n    res = []\n    mae = [[],[],[]]\n    for n in range(1,max_clust):\n        res,scores, mae = get_scores(n,c1,c2,res,mae, [np.mean(yt[jj[1:(n+1)],:],axis = 0) for jj in indices],'mean')\n        res,scores, mae = get_scores(n,c1,c2,res,mae, [np.median(y_train.iloc[jj[1:(n+1)],:],axis = 0) for jj in indices],'median')\n    return res,mae\n\n##################################################\n##Commented out as it takes a looooong time to run\n##################################################\n#results = res = Parallel(n_jobs=4)(\n#                 map(delayed(myfunc), [(c1,c2, 182) for c1 in range(36) for c2 in range(c1,36)]))\n\nresults = res = Parallel(n_jobs=4)(\n                map(delayed(myfunc), [(c1,c2, 50) for c1 in range(36) for c2 in range(c1,36)]))","metadata":{"execution":{"iopub.status.busy":"2022-03-03T13:46:05.055668Z","iopub.execute_input":"2022-03-03T13:46:05.056832Z","iopub.status.idle":"2022-03-03T13:56:42.99164Z","shell.execute_reply.started":"2022-03-03T13:46:05.056771Z","shell.execute_reply":"2022-03-03T13:56:42.989936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Extract results\n\n#res contains all the results with scores for each day aggregated by mean\nres = [r for r1, _ in results for r in r1]\n\n#mae contains all the nonaggregated socres for each day for each condition tried\n##Going to find the optimum parameter set for:\n    ##Non-national holiday days as they look different\nnon_holidays = [x not in [datetime.date(1991,4,1), datetime.date(1991,5,27), datetime.date(1991,9,2)] for x in Xy.index]\n    ##Weekdays as they are likely to be more similar\ndays_to_use = np.argwhere((Xy.weekday.values<5) & non_holidays).flatten()\n\nmae = [np.mean([r2[jj][ii][kk] for kk in days_to_use]) for _, r2 in results for ii in range(len(r2[0])) for jj in range(3)]","metadata":{"execution":{"iopub.status.busy":"2022-03-03T13:57:41.948761Z","iopub.execute_input":"2022-03-03T13:57:41.949158Z","iopub.status.idle":"2022-03-03T13:57:44.299442Z","shell.execute_reply.started":"2022-03-03T13:57:41.949112Z","shell.execute_reply":"2022-03-03T13:57:44.298235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Extract parameters\nn_clust = [r[0] for r in res]\nstart_cutoff = [r[1] for r in res]\nend_cutoff = [r[2] for r in res]\nadjust = [r[3] for r in res]\nscore = mae\nmethod = [r[5] for r in res]","metadata":{"execution":{"iopub.status.busy":"2022-03-03T13:57:47.084596Z","iopub.execute_input":"2022-03-03T13:57:47.08504Z","iopub.status.idle":"2022-03-03T13:57:47.132907Z","shell.execute_reply.started":"2022-03-03T13:57:47.084994Z","shell.execute_reply":"2022-03-03T13:57:47.131847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plot a heatmap of best score for the window to used to cluster days\nrev_time_mapping = {ii:t for ii,t in enumerate(train.time.unique())}\n\ntemp_df = pd.DataFrame({'score':[-s for s in score],'start_cutoff':[rev_time_mapping[s] for s in start_cutoff],'end_cutoff':[rev_time_mapping[e] for e in end_cutoff]}).groupby(['start_cutoff','end_cutoff']).max().reset_index()\n\nfig = plt.figure(figsize = (10,7))\nsns.heatmap(pd.pivot_table(temp_df,values = 'score', index = 'start_cutoff',columns = 'end_cutoff'))\nplt.xlabel('End of time window',fontsize = 20)\nplt.ylabel('Start of time window',fontsize = 20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-03T14:07:32.485458Z","iopub.execute_input":"2022-03-03T14:07:32.486814Z","iopub.status.idle":"2022-03-03T14:07:33.835545Z","shell.execute_reply.started":"2022-03-03T14:07:32.486766Z","shell.execute_reply":"2022-03-03T14:07:33.833716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Look at how individual parameters vary the score\nfig,ax = plt.subplots(1,3, figsize = (30,10))\n\nax[0].scatter(start_cutoff,score)\nax[0].set_xlabel('Start of clustering window',fontsize = 20)\nax[0].set_ylabel('MAE',fontsize = 20)\nax[0].set_xticks(ticks = range(0,36,5),labels = [rev_time_mapping[s] for s in np.unique(start_cutoff) if s in range(0,36,5)])\n\nax[1].scatter(end_cutoff,score)\nax[1].set_xlabel('End of clustering window',fontsize = 20)\nax[1].set_ylabel('MAE',fontsize = 20)\nax[1].set_xticks(ticks = range(0,36,5),labels = [rev_time_mapping[e] for e in np.unique(end_cutoff) if e in range(0,36,5)])\n\nax[2].scatter(n_clust,score)\nax[2].set_xlabel('Number of neighbours to pool together',fontsize = 20)\nax[2].set_ylabel('MAE',fontsize = 20)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-03T14:11:05.787918Z","iopub.execute_input":"2022-03-03T14:11:05.788296Z","iopub.status.idle":"2022-03-03T14:11:08.013238Z","shell.execute_reply.started":"2022-03-03T14:11:05.788258Z","shell.execute_reply":"2022-03-03T14:11:08.012297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,ax = plt.subplots(1,2, figsize = (15,7))\n\nsns.stripplot(x = adjust, y =score, ax = ax[0])\nax[0].set_xlabel('Adjustment method',fontsize = 20)\nax[0].set_ylabel('MAE',fontsize = 20)\n\nsns.stripplot(x = method, y =score, ax = ax[1])\nax[1].set_xlabel('Pooling method',fontsize = 20)\nax[1].set_ylabel('MAE',fontsize = 20)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-03T14:14:32.61302Z","iopub.execute_input":"2022-03-03T14:14:32.615459Z","iopub.status.idle":"2022-03-03T14:14:33.598349Z","shell.execute_reply.started":"2022-03-03T14:14:32.615365Z","shell.execute_reply":"2022-03-03T14:14:33.597461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Find optimum parameter set\nbest_param_position = np.argmin(score)\nn_neighbours = n_clust[best_param_position]\nmin_cutoff = start_cutoff[best_param_position]\nmax_cutoff = end_cutoff[best_param_position]\nadjust = adjust[best_param_position]\nmethod = method[best_param_position]\n\nif adjust == 'floor':\n    adjust = np.floor\nelif adjust == 'rounded':\n    adjust = np.round\nelse:\n    lambda x: x\n    \nif method == 'median':\n    method = np.median\nelse:\n    method = np.mean","metadata":{"execution":{"iopub.status.busy":"2022-03-03T14:18:15.38522Z","iopub.execute_input":"2022-03-03T14:18:15.385904Z","iopub.status.idle":"2022-03-03T14:18:15.399899Z","shell.execute_reply.started":"2022-03-03T14:18:15.38586Z","shell.execute_reply":"2022-03-03T14:18:15.398257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Find prediction for 30th September\nNN = NearestNeighbors(n_neighbors=n_neighbours,metric = 'manhattan')\nNN.fit(X_train.loc[:,[(t>=min_cutoff) & (t<=max_cutoff) for t in time_number if t<36]])\ndistances, indices = NN.kneighbors(X_final.loc[:,[(t>=min_cutoff) & (t<=max_cutoff) for t in time_number if t<36]])\nsub = pd.melt(pd.DataFrame({k:v for k,v in zip(y_train.columns,adjust(method(y_train.iloc[indices[0],:],axis = 0)))}, index=[0]), value_name = 'congestion', var_name = 'loc_dir_time')\ntest.merge(sub).loc[:,['row_id','congestion']].to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-03T14:18:26.469652Z","iopub.execute_input":"2022-03-03T14:18:26.470728Z","iopub.status.idle":"2022-03-03T14:18:26.658979Z","shell.execute_reply.started":"2022-03-03T14:18:26.470677Z","shell.execute_reply":"2022-03-03T14:18:26.657947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Future steps\n1. Fix the imputation method used prior to analysis\n2. See how well clustering works for each day individually (unlikely, but hopeful it works best for Mondays!)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}