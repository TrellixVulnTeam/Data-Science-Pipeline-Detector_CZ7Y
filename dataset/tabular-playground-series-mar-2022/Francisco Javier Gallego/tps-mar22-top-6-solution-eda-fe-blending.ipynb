{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <b>1 <span style='color:lightseagreen'>|</span> Introduction</b>\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>1.1 | Goal</b></p>\n</div>\n\nForecast twelve-hours of **<span style='color:lightseagreen'>traffic flow</span>** in a U.S. metropolis. The time series in this dataset are labelled with both **<span style='color:lightseagreen'>location coordinates</span>** and a **<span style='color:lightseagreen'>direction of travel</span>** -- a combination of features that will test your skill at **<span style='color:lightseagreen'>spatio-temporal</span>** forecasting within a highly dynamic traffic network.\n\n![](https://www.wsp.com/-/media/Hubs/Global/Congestion-Management/bnr-congestion.jpg)\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>1.2 | Metric</b></p>\n</div>\n\nSubmissions are evaluated on the mean absolute error between predicted and actual congestion values for each time period in the test set. The congestion target has integer values from 0 to 100.\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>1.3 | How to import data ?</b></p>\n</div>\n\nFirst, we import all the datasets needed for this kernel. The required time series column is imported as a datetime column using **<span style='color:lightseagreen'>parse_dates</span>** parameter and is also selected as index of the dataframe using **<span style='color:lightseagreen'>index_col</span>** parameter.\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>1.4 | Timestamps and Periods</b></p>\n</div>\n\nTimestamps are used to represent a point in time. Periods represent an interval in time. Periods can used to check if a specific event in the given period. They can also be converted to each other's form.\n\nðŸ“Œ Video: [How to use dates and times with pandas](https://campus.datacamp.com/courses/manipulating-time-series-data-in-python/working-with-time-series-in-pandas?ex=1): explain **<span style='color:lightseagreen'>TimeStamp and Period</span>** data. \n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>1.5 | Using date_range</b></p>\n</div>\n\ndate_range is a method that returns a fixed **<span style='color:lightseagreen'>frequency datetimeindex</span>**. It is quite useful when creating your own time series attribute for pre-existing data or arranging the whole data around the time series attribute created by you.\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>1.6 | Acknowledgements</b></p>\n</div>\n\n1. [Geunho Yu Kernel](https://www.kaggle.com/zhangcheche/xgboost-tps-2022-03)\n2. [Sy-Tuan Nguyen Kernel](https://www.kaggle.com/sytuannguyen/tps-mar-2022-eda-model)","metadata":{"_uuid":"408aebe7-6ed2-4354-8d46-fe2e455892de","_cell_guid":"c4899efe-0d78-46ad-b0d1-a30b18d7cb8d","trusted":true}},{"cell_type":"code","source":"#%%capture\n#!pip install -U lightautoml\nimport os\nimport warnings\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pandas_profiling as pp\nimport seaborn as sns\nfrom IPython.display import display\nfrom pandas.api.types import CategoricalDtype\n\nfrom category_encoders import MEstimateEncoder\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder\nfrom category_encoders import MEstimateEncoder\n\n# Algorithms\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\n\n# Optuna - Bayesian Optimization \nimport optuna\nfrom optuna.samplers import TPESampler\n\n# Plotly\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\nimport plotly.offline as offline\nimport plotly.graph_objs as go\n\nwarnings.filterwarnings('ignore')\n\ndef load_data():\n    data_dir = Path(\"../input/tabular-playground-series-mar-2022\")\n    df_train = pd.read_csv(data_dir / \"train.csv\")\n    df_test = pd.read_csv(data_dir / \"test.csv\")\n    # Merge the splits so we can process them together\n    df = pd.concat([df_train, df_test])\n    return df\n\ndef plot_feature_importance(importance,names,model_type):\n    \n    #Create arrays from feature importance and feature names\n    feature_importance = np.array(importance)\n    feature_names = np.array(names)\n    \n    #Create a DataFrame using a Dictionary\n    data={'feature_names':feature_names,'feature_importance':feature_importance}\n    fi_df = pd.DataFrame(data)\n    \n    #Sort the DataFrame in order decreasing feature importance\n    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n    \n    #Define size of bar plot\n    plt.figure(figsize=(20,10))\n    #Plot Searborn bar chart\n    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n    #Add chart labels\n    plt.title(model_type + ' FEATURE IMPORTANCE')\n    plt.xlabel('FEATURE IMPORTANCE')\n    plt.ylabel('FEATURE NAMES')\n\ndf_data = load_data()\npp.ProfileReport(df_data)","metadata":{"_uuid":"e3158cdc-fe11-4c18-bac5-d1ca52a07d29","_cell_guid":"196a7b04-2ee2-4071-9e67-474287b1e6dd","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-19T14:10:10.124456Z","iopub.execute_input":"2022-03-19T14:10:10.124711Z","iopub.status.idle":"2022-03-19T14:11:06.551217Z","shell.execute_reply.started":"2022-03-19T14:10:10.124636Z","shell.execute_reply":"2022-03-19T14:11:06.550427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>1.7 | Reducing Memory Usage</b></p>\n</div>\n\nAs we can observe in the previous report from the dataset, we have the amount of more than **<span style='color:lightseagreen'>800k rows</span>**. Due to that, in order to not having **<span style='color:lightseagreen'>issues with memory</span>** in the kernel, we are going to reduce its memory usage with the following function. Below, we can appreciate that reduction was successful as we manage to make a **<span style='color:lightseagreen'>reduction of 39.7%</span>**. ","metadata":{}},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n\n    for col in df.columns:\n        col_type = df[col].dtypes\n\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n \n    return df\n\ndf_data = reduce_mem_usage(df_data)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-19T14:11:06.553271Z","iopub.execute_input":"2022-03-19T14:11:06.553715Z","iopub.status.idle":"2022-03-19T14:11:06.596497Z","shell.execute_reply.started":"2022-03-19T14:11:06.553679Z","shell.execute_reply":"2022-03-19T14:11:06.595844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>1.8 | Datetime Features</b></p>\n</div>\n\nAs we can observe in the previous report we have a dataset with no missing values. Thus, we are going to proceed directly to feature engineering section. We'll start by **<span style='color:lightseagreen'>breaking down the date</span>** into different columns:\n\n* One for the **<span style='color:lightseagreen'>year</span>**\n* One for the **<span style='color:lightseagreen'>month</span>**\n* One for the **<span style='color:lightseagreen'>week</span>**\n* One for the **<span style='color:lightseagreen'>quarter</span>** of the year\n* One for the **<span style='color:lightseagreen'>day of the week</span>**\n* One for **<span style='color:lightseagreen'>weekend</span>**","metadata":{"_uuid":"b0dafcdd-a714-477c-a2af-99e2cc63b7e4","_cell_guid":"4ab8c312-4990-4be5-8c65-d918ca02c38d","trusted":true}},{"cell_type":"code","source":"df_data.time = pd.to_datetime(df_data.time)\ndf_data['year'] = df_data.time.dt.year\ndf_data['month'] = df_data.time.dt.month\ndf_data['week'] = df_data.time.dt.isocalendar().week\ndf_data['hour'] = df_data.time.dt.hour\ndf_data['minute'] = df_data.time.dt.minute\ndf_data['day_of_week'] = df_data.time.dt.day_name()\ndf_data['day_of_year'] = df_data.time.dt.dayofyear\ndf_data['is_weekend'] = (df_data.time.dt.dayofweek >= 5).astype(\"int\")\ndf_data = df_data.set_index('time')","metadata":{"_uuid":"155ff49f-4e79-4811-bf69-0b7bd997947d","_cell_guid":"61af0b65-67b4-403f-b995-d50e4d6abecd","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-19T14:11:06.597963Z","iopub.execute_input":"2022-03-19T14:11:06.598451Z","iopub.status.idle":"2022-03-19T14:11:07.822527Z","shell.execute_reply.started":"2022-03-19T14:11:06.598412Z","shell.execute_reply":"2022-03-19T14:11:07.821821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>2 <span style='color:lightseagreen'>|</span> Exploratory Data Analysis</b>\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>2.1 | General Congestion Analysis</b></p>\n</div>","metadata":{"_uuid":"fe52a473-87b5-40d4-a9bd-3b91495890eb","_cell_guid":"5f992c0e-8159-48d6-b737-e5e95557160e","trusted":true}},{"cell_type":"code","source":"from itertools import cycle\npalette = cycle(px.colors.sequential.Viridis)\ndf_graph = df_data[df_data['congestion'].isnull() == False]\n\n# Defining all our palette colours.\nprimary_blue = \"#496595\"\nprimary_blue2 = \"#85a1c1\"\nprimary_blue3 = \"#3f4d63\"\nprimary_grey = \"#c6ccd8\"\nprimary_black = \"#202022\"\nprimary_bgcolor = \"#f4f0ea\"\n\n# \"coffee\" pallette turqoise-gold.\nf1 = \"#a2885e\"\nf2 = \"#e9cf87\"\nf3 = \"#f1efd9\"\nf4 = \"#8eb3aa\"\nf5 = \"#235f83\"\nf6 = \"#b4cde3\"\n\n# chart\nfig = make_subplots(rows=3, cols=2, \n                    specs=[[{\"type\": \"bar\"}, {\"type\": \"scatter\"}], [{\"colspan\": 2}, None], [{'type':'histogram'}, {'type':'bar'}]],\n                    column_widths=[0.4, 0.6], vertical_spacing=0.1, horizontal_spacing=0.1,\n                    subplot_titles=(\"Mean Congestion per Day of Week\", \"Hourly Congestion Trend\", \"Daily Congestion Trend\", \"Congestion Distribution\",\"Congestion Value Counts\"))\n\n# Upper Left chart\ndf_day = df_graph.groupby(['day_of_week']).agg({\"congestion\" : \"mean\"}).reset_index().sort_values(by='congestion', ascending = False)\nvalues = list(range(7))\nfig.add_trace(go.Bar(x=df_day['day_of_week'], y=df_day['congestion'], marker = dict(color=values, colorscale=\"Viridis\"), \n                     name = 'Day of Week'),\n                      row=1, col=1)\n\nfig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False, row=1, col=1)\nfig.update_yaxes(showgrid = False, linecolor='gray',linewidth=2, zeroline = False, row=1, col=1)\n\n# Upper Right chart\ndf_hour = df_graph.groupby(['hour']).agg({\"congestion\" : \"mean\"}).reset_index('hour')\nfig.add_trace(go.Scatter(x=df_hour['hour'], y=df_hour['congestion'], mode='lines+markers',\n                 marker = dict(color = primary_blue3), name='Hourly Congestion'), row = 1, col = 2)\n\n# Rectangle to highlight range\nfig.add_vrect(x0=12.5, x1=18.5,\n              fillcolor=px.colors.sequential.Viridis[4],\n              layer=\"below\", \n              opacity=0.25, \n              line_width=0, \n              row = 1, col = 2\n)\n\nfig.add_annotation(dict(\n        x=7.9,\n        y=df_hour.loc[8,'congestion']+0.45,\n        text=\"There is a <b>peak at <br>8am</b> coinciding with<br>going to work.\",\n        ax=\"-20\",\n        ay=\"-60\",\n        showarrow = True,\n        arrowhead = 7,\n        arrowwidth = 0.7\n), row=1, col=2)\n\nfig.add_annotation(dict(\n        x=15.50,\n        y=49,\n        text=\"Midday hours are <br><b>the rush hours</b>.\",\n        showarrow = False\n), row=1, col=2)\n\nfig.add_annotation(dict(\n        x=18.5,\n        y=df_hour.loc[18,'congestion'],\n        text=\"From 6pm <br>on <b>congestion<br> ratio falls</b>.\",\n        ax=\"50\",\n        ay=\"-40\",\n        showarrow = True,\n        arrowhead = 7,\n        arrowwidth = 0.7\n), row=1, col=2)\n\nfig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False, row=1, col=2)\nfig.update_yaxes(showgrid = False, linecolor='gray', linewidth=2, row=1, col=2)\n\n# Medium Chart\ndf_week = df_graph.groupby(['day_of_year']).agg({\"congestion\" : \"mean\"}).reset_index()\nfig.add_trace(go.Scatter(x = df_week['day_of_year'], y = df_week['congestion'], mode='lines',\n                        marker = dict(color = px.colors.sequential.Viridis[5]),\n                        name='Daily Congestion'), row = 2, col = 1)\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\ndecomp = seasonal_decompose(df_week['congestion'], period=61, model='additive', extrapolate_trend='freq')\n\nfig.add_trace(go.Scatter(x = df_week['day_of_year'], y = decomp.trend, mode='lines',\n                        marker = dict(color = primary_blue3),\n                        name='Congestion Trend'), row = 2, col = 1)\n    \nfig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, row=2, col=1)\nfig.update_yaxes(gridcolor = 'gray', gridwidth = 0.15, linecolor='gray',linewidth=2, row=2, col=1)\n\n# Left Bottom Chart\nfig.add_trace(go.Histogram(x=df_graph.congestion, name='Congestion Distribution', marker = dict(color = px.colors.sequential.Viridis[3])), row = 3, col = 1)\n\nfig.update_xaxes(showgrid = False, showline = True, linecolor = 'gray', linewidth = 2, row = 3, col = 1)\nfig.update_yaxes(showgrid = False, gridcolor = 'gray', gridwidth = 0.5, showline = True, linecolor = 'gray', linewidth = 2, row = 3, col = 1)\n\n# Right Bottom Chart\ncon_bar = df_graph.copy()\ncon_bar['congestion_group'] = pd.cut(con_bar.congestion, bins=[0,20,40,60,80,100], labels=['0-20', '20-40', '40-60', '60-80', '80-100'])\ncon_bar = con_bar.groupby('congestion_group').agg({'congestion':'count'}).reset_index().sort_values(by='congestion')\n\nvalues = list(range(5))\nfig.add_trace(go.Bar(x=con_bar['congestion'], y=con_bar['congestion_group'], marker = dict(color=values, colorscale=\"Viridis_r\"), name = 'Congestion Values', orientation = 'h'),\n                      row=3, col=2)\n\nfig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False, row=3, col=2)\nfig.update_yaxes(showgrid = False, linecolor='gray',linewidth=2, zeroline = False, row=3, col=2)\n\nfig.add_annotation(dict(\n        x=con_bar.loc[4,'congestion']+0.15,\n        y=0,\n        text=\"Highest congestion <br>ratios are <b> more unusual</b>.\",\n        ax=\"110\",\n        ay=\"-20\",\n        showarrow = True,\n        arrowhead = 7,\n        arrowwidth = 0.7\n), row=3, col=2)\n\n# General Styling\nfig.update_layout(height=1100, bargap=0.2,\n                  margin=dict(b=50,r=30,l=100),\n                  title = \"<span style='font-size:36px; font-family:Times New Roman'>Congestion Analysis</span>\",                  \n                  plot_bgcolor='rgb(242,242,242)',\n                  paper_bgcolor = 'rgb(242,242,242)',\n                  font=dict(family=\"Times New Roman\", size= 14),\n                  hoverlabel=dict(font_color=\"floralwhite\"),\n                  showlegend=False)","metadata":{"_uuid":"2749a569-0d39-4367-8f41-2fb47b98aa9c","_cell_guid":"29e3e998-512a-4444-a9f6-f2bdc4d6ebc4","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-19T14:11:07.824756Z","iopub.execute_input":"2022-03-19T14:11:07.825032Z","iopub.status.idle":"2022-03-19T14:11:08.601878Z","shell.execute_reply.started":"2022-03-19T14:11:07.824996Z","shell.execute_reply":"2022-03-19T14:11:08.600993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ðŸ“Œ **Interpret:** As we can appreeciate, **<span style='color:lightseagreen'>working days</span>** of the week have a similar congestion rate. Likewise, we can see that the weekend days are the ones with the least traffic, with Sunday being the quietest day. Moving to the upper right chart, we observe that there is an **<span style='color:lightseagreen'>increase</span>** in traffic at the **<span style='color:lightseagreen'>beginning of the day</span>**. Busiest hours are between **<span style='color:lightseagreen'>13h - 17h</span>**, and after congestion rate decrease as the night falls. At the middle graph is easily to see a **<span style='color:lightseagreen'>strong seasonality with respect to congestion rate per week</span>**. Moreover, trend remains almost constant, increasing insignificantly over time.. Finally at the bottom, we can see that congestion feature is **<span style='color:lightseagreen'>normally distributed</span>** (as the initial report showed). Likewise, most repeated congestion value rate belongs to **<span style='color:lightseagreen'>40-60 category</span>**. \n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>2.2 | Daily Congestion Analysis</b></p>\n</div>","metadata":{"_uuid":"f43ee765-121f-4b91-9de0-a6dda9de9315","_cell_guid":"8412b7b2-8fae-425a-9dab-a829c46d1617","trusted":true}},{"cell_type":"code","source":"df_graph = df_data[df_data['congestion'].isnull() == False]\n\n# chart\nfig = make_subplots(rows=2, cols=3, \n                    specs=[[{\"type\": \"scatter\"}, {\"type\": \"scatter\"}, {'type':'scatter'}], [{\"type\": \"scatter\"}, {\"type\": \"scatter\"}, {'type':'scatter'}]],\n                    column_widths=[0.33, 0.33, 0.34], vertical_spacing=0.125, horizontal_spacing=0.1,\n                    subplot_titles=(\"Monday Congestion\", \"Tuesday Congestion\", \"Wednesday Congestion\",'Thursday Congestion','Friday Congestion','Weekend Congestion'))\n\n# Upper Left chart\ndf_monday = df_graph[df_graph.day_of_week == 'Monday'].groupby(['hour']).agg({\"congestion\" : \"mean\"}).reset_index()\nfig.add_trace(go.Scatter(x=df_monday['hour'], y=df_monday['congestion'], mode='lines+markers',\n                 marker = dict(color = px.colors.sequential.Viridis[0]), name='Monday Congestion'), row = 1, col = 1)\n\nfig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False, row=1, col=1)\nfig.update_yaxes(showgrid = False, linecolor='gray',linewidth=2, row=1, col=1)\n\n# Upper Medium chart\ndf_tuesday = df_graph[df_graph.day_of_week == 'Tuesday'].groupby(['hour']).agg({\"congestion\" : \"mean\"}).reset_index()\nfig.add_trace(go.Scatter(x=df_tuesday['hour'], y=df_tuesday['congestion'], mode='lines+markers',\n                 marker = dict(color = px.colors.sequential.Viridis[2]), name='Monday Congestion'), row = 1, col = 2)\n\nfig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False, row=1, col=2)\nfig.update_yaxes(showgrid = False, linecolor='gray',linewidth=2, row=1, col=2)\n\n# Upper Right chart\ndf_wednesday = df_graph[df_graph.day_of_week == 'Wednesday'].groupby(['hour']).agg({\"congestion\" : \"mean\"}).reset_index()\nfig.add_trace(go.Scatter(x=df_wednesday['hour'], y=df_wednesday['congestion'], mode='lines+markers',\n                 marker = dict(color = px.colors.sequential.Viridis[4]), name='Monday Congestion'), row = 1, col = 3)\n\nfig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False, row=1, col=3)\nfig.update_yaxes(showgrid = False, linecolor='gray',linewidth=2, row=1, col=3)\n\n# Bottom Left chart\ndf_thursday = df_graph[df_graph.day_of_week == 'Thursday'].groupby(['hour']).agg({\"congestion\" : \"mean\"}).reset_index()\nfig.add_trace(go.Scatter(x=df_thursday['hour'], y=df_thursday['congestion'], mode='lines+markers',\n                 marker = dict(color = px.colors.sequential.Viridis[6]), name='Monday Congestion'), row = 2, col = 1)\n\nfig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False, row=2, col=1)\nfig.update_yaxes(showgrid = False, linecolor='gray',linewidth=2, row=2, col=1)\n\n# Bottom Medium chart\ndf_friday = df_graph[df_graph.day_of_week == 'Friday'].groupby(['hour']).agg({\"congestion\" : \"mean\"}).reset_index()\nfig.add_trace(go.Scatter(x=df_friday['hour'], y=df_friday['congestion'], mode='lines+markers',\n                 marker = dict(color = px.colors.sequential.Viridis[9]), name='Monday Congestion'), row = 2, col = 2)\n\nfig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False, row=2, col=2)\nfig.update_yaxes(showgrid = False,linecolor='gray', linewidth=2, row=2, col=2)\n\n# Bottom Right chart\ndf_weekend = df_graph[df_graph.is_weekend == True].groupby(['hour']).agg({\"congestion\" : \"mean\"}).reset_index()\ndf_saturday = df_graph[df_graph.day_of_week == 'Saturday'].groupby(['hour']).agg({\"congestion\" : \"mean\"}).reset_index()\ndf_sunday = df_graph[df_graph.day_of_week == 'Sunday'].groupby(['hour']).agg({\"congestion\" : \"mean\"}).reset_index()\n\nfig.add_trace(go.Scatter(x=df_weekend['hour'], y=df_weekend['congestion'], mode='lines+markers',\n                 marker = dict(color = px.colors.sequential.Viridis[6]), name='Weekend Average Congestion'), row = 2, col = 3)\nfig.add_trace(go.Scatter(x=df_saturday['hour'], y=df_saturday['congestion'], mode='lines+markers',\n                 marker = dict(color = px.colors.sequential.Viridis[3]), name='Saturday Congestion'), row = 2, col = 3)\nfig.add_trace(go.Scatter(x=df_sunday['hour'], y=df_sunday['congestion'], mode='lines+markers',\n                 marker = dict(color = px.colors.sequential.Viridis[9]), name='Sunday Congestion'), row = 2, col = 3)\n\nfig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False, row=2, col=3)\nfig.update_yaxes(showgrid = False, linecolor='gray',linewidth=2, row=2, col=3)\n\n# General Styling\nfig.update_layout(height=750, bargap=0.2,\n                  margin=dict(b=50,r=30,l=100),\n                  title = \"<span style='font-size:36px; font-family:Times New Roman'>Daily Congestion Analysis</span>\",                  \n                  plot_bgcolor='rgb(242,242,242)',\n                  paper_bgcolor = 'rgb(242,242,242)',\n                  font=dict(family=\"Times New Roman\", size= 14),\n                  hoverlabel=dict(font_color=\"floralwhite\"),\n                  showlegend=False)","metadata":{"_uuid":"d3d08455-1581-49e1-a3da-06a62e98fb44","_cell_guid":"40ec97db-1c5f-44e3-9dc5-a4611d260471","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-19T14:11:08.604421Z","iopub.execute_input":"2022-03-19T14:11:08.604688Z","iopub.status.idle":"2022-03-19T14:11:09.722504Z","shell.execute_reply.started":"2022-03-19T14:11:08.604656Z","shell.execute_reply":"2022-03-19T14:11:09.721834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ðŸ“Œ **Interpret:** As we can appreeciate, in **<span style='color:lightseagreen'>working days</span>** congestion rate is quite similar at each hour. However, this change when we get into the weekend. We can appreciate that, due to the fact that people do not have to work congestion rates are much smaller. Moreover, weekend congestion trend does not have as ups and downs as working days have. \n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>2.3 | Direction and Location Analysis</b></p>\n</div>","metadata":{}},{"cell_type":"code","source":"palette = cycle(px.colors.sequential.Sunsetdark)\ndf_graph = df_data[df_data['congestion'].isnull() == False]\n\n# chart\nfig = make_subplots(rows=1, cols=3, \n                    specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}, {'type':'bar'}]],\n                    column_widths=[0.33, 0.34, 0.33], vertical_spacing=0.1, horizontal_spacing=0.1,\n                    subplot_titles=(\"Mean Congestion per Direction\", \"Mean Congestion Per X Location\", \"Mean Congestion per Y Location\"))\n\n# Left chart\ndf_direction = df_graph.groupby(['direction']).agg({\"congestion\" : \"mean\"})\nvalues = list(range(8))\nfig.add_trace(go.Bar(x=df_direction.index, y=df_direction['congestion'], marker = dict(color=values, colorscale=\"Viridis\"), name = 'Day of Week'),\n                      row=1, col=1)\n\nfig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False, row=1, col=1)\nfig.update_yaxes(showgrid = False, linecolor='gray',linewidth=2, zeroline = False, row=1, col=1)\n\n# Middle chart\ndf_x = df_graph.groupby(['x']).agg({\"congestion\" : \"mean\"})\nvalues = list(range(3))\nfig.add_trace(go.Bar(x=df_x.index, y=df_x['congestion'], marker = dict(color=values, colorscale=\"Viridis\"), name = 'Day of Week'),\n                      row=1, col=2)\n\nfig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False, row=1, col=2)\nfig.update_yaxes(showgrid = False, linecolor='gray',linewidth=2, zeroline = False, row=1, col=2)\n\n# Right chart\ndf_y = df_graph.groupby(['y']).agg({\"congestion\" : \"mean\"})\nvalues = list(range(4))\nfig.add_trace(go.Bar(x=df_y.index, y=df_y['congestion'], marker = dict(color=values, colorscale=\"Viridis\"), name = 'Day of Week'),\n                      row=1, col=3)\n\nfig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False, row=1, col=3)\nfig.update_yaxes(showgrid = False, linecolor='gray',linewidth=2, zeroline = False, row=1, col=3)\n\n# General Styling\nfig.update_layout(height=400, bargap=0.2,\n                  margin=dict(b=50,r=30,l=100),\n                  title = \"<span style='font-size:36px; font-family:Times New Roman'>Direction and Location Analysis</span>\",                  \n                  plot_bgcolor='rgb(242,242,242)',\n                  paper_bgcolor = 'rgb(242,242,242)',\n                  font=dict(family=\"Times New Roman\", size= 14),\n                  hoverlabel=dict(font_color=\"floralwhite\"),\n                  showlegend=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-19T14:11:09.723816Z","iopub.execute_input":"2022-03-19T14:11:09.724213Z","iopub.status.idle":"2022-03-19T14:11:09.943204Z","shell.execute_reply.started":"2022-03-19T14:11:09.724176Z","shell.execute_reply":"2022-03-19T14:11:09.942455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ðŸ“Œ **Interpret:** As we can appreeciate, direction with most large congestion rate is **<span style='color:lightseagreen'>south bound</span>**. Looking at the middle chart we observe that **<span style='color:lightseagreen'>X = 1 location</span>** is the most busiest, while X = 0 is the one with least traffic. Finally, at the right we see that both **<span style='color:lightseagreen'>Y = 0 and Y = 2 location</span>** are the busiest. Difference between these two and the other is a bit significant. \n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>2.4 | Outliers</b></p>\n</div>\n\n### 2.4.1 | Outliers Definition\nOutlier is an observation that is numerically distant from the rest of the data or in a simple word it is the value which is out of the range.letâ€™s take an example to check what happens to a data set with and data set without outliers.\n\n### 2.4.2 | Outliers Detection\n\nOutlier can be of two types: Univariate and Multivariate. These outliers can be found when we look at distribution of a single variable. Multi-variate outliers are outliers in an n-dimensional space. We'll start by detecting whether there are outliers in our dataset or not. \n\n#### 2.4.2.1 | Grubbs Test\n\n$$\n\\begin{array}{l}{\\text { Grubbs' test is defined for the hypothesis: }} \\\\ {\\begin{array}{ll}{\\text { Ho: }}  {\\text { There are no outliers in the data set }} \\\\ {\\mathrm{H}_{\\mathrm{1}} :}  {\\text { There is exactly one outlier in the data set }}\\end{array}}\\end{array}\n$$\n$$\n\\begin{array}{l}{\\text {The Grubbs' test statistic is defined as: }} \\\\ {\\qquad G_{calculated}=\\frac{\\max \\left|X_{i}-\\overline{X}\\right|}{SD}} \\\\ {\\text { with } \\overline{X} \\text { and } SD \\text { denoting the sample mean and standard deviation, respectively. }} \\end{array}\n$$\n$$\nG_{critical}=\\frac{(N-1)}{\\sqrt{N}} \\sqrt{\\frac{\\left(t_{\\alpha /(2 N), N-2}\\right)^{2}}{N-2+\\left(t_{\\alpha /(2 N), N-2}\\right)^{2}}}\n$$\n\n\\begin{array}{l}{\\text { If the calculated value is greater than critical, you can reject the null hypothesis and conclude that one of the values is an outlier }}\\end{array}","metadata":{"_uuid":"74696a67-a23b-43e4-90a5-26b7ee276a2d","_cell_guid":"367ff54e-1929-495c-b3a8-da98801d7bab","trusted":true}},{"cell_type":"code","source":"import scipy.stats as stats\ndef grubbs_test(x):\n    n = len(x)\n    mean_x = np.mean(x)\n    sd_x = np.std(x)\n    numerator = max(abs(x-mean_x))\n    g_calculated = numerator/sd_x\n    print(\"Grubbs Calculated Value:\",g_calculated)\n    t_value = stats.t.ppf(1 - 0.05 / (2 * n), n - 2)\n    g_critical = ((n - 1) * np.sqrt(np.square(t_value))) / (np.sqrt(n) * np.sqrt(n - 2 + np.square(t_value)))\n    print(\"Grubbs Critical Value:\",g_critical)\n    if g_critical > g_calculated:\n        print(\"From grubbs_test we observe that calculated value is lesser than critical value, Accept null hypothesis and conclude that there is no outliers\\n\")\n    else:\n        print(\"From grubbs_test we observe that calculated value is greater than critical value, Reject null hypothesis and conclude that there is an outliers\\n\")\n\ngrubbs_test(df_data[df_data.congestion.isnull() == False]['congestion'])","metadata":{"execution":{"iopub.status.busy":"2022-03-19T14:11:09.944552Z","iopub.execute_input":"2022-03-19T14:11:09.945016Z","iopub.status.idle":"2022-03-19T14:11:10.124218Z","shell.execute_reply.started":"2022-03-19T14:11:09.944975Z","shell.execute_reply":"2022-03-19T14:11:10.123455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.4.2.2 | Z-score method\n\nUsing Z score method,we can find out how many standard deviations value away from the mean. \n\n![minipic](https://i.pinimg.com/originals/cd/14/73/cd1473c4c82980c6596ea9f535a7f41c.jpg)\n\n Figure in the left shows area under normal curve and how much area that standard deviation covers.\n* 68% of the data points lie between + or - 1 standard deviation.\n* 95% of the data points lie between + or - 2 standard deviation\n* 99.7% of the data points lie between + or - 3 standard deviation\n\n$\\begin{array}{l} {R.Z.score=\\frac{0.6745*( X_{i} - Median)}{MAD}}  \\end{array}$\n\nIf the z score of a data point is more than 3 (because it cover 99.7% of area), it indicates that the data value is quite different from the other values. It is taken as outliers.","metadata":{}},{"cell_type":"code","source":"df_outlier = df_data.reset_index().set_index('row_id').copy()\nout=[]\ndef Zscore_outlier(df):\n    m = np.mean(df)\n    sd = np.std(df)\n    row = 0\n    for i in df: \n        z = (i-m)/sd\n        if np.abs(z) > 3: \n            out.append(row)\n        row += 1\n    return out\n\noutliers_index = Zscore_outlier(df_outlier[df_outlier.congestion.isnull() == False]['congestion'])","metadata":{"execution":{"iopub.status.busy":"2022-03-19T14:11:10.125491Z","iopub.execute_input":"2022-03-19T14:11:10.125739Z","iopub.status.idle":"2022-03-19T14:11:12.090548Z","shell.execute_reply.started":"2022-03-19T14:11:10.125712Z","shell.execute_reply":"2022-03-19T14:11:12.089822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_outlier['outlier'] = 0\ndf_outlier.loc[outliers_index,'outlier'] = 1\n\nfig = px.scatter(df_outlier, x=df_outlier.index, y='congestion', color = 'outlier')\nfig.update_xaxes(visible = False, zeroline = False)\nfig.update_yaxes(showgrid = False, gridcolor = 'gray', gridwidth = .5, zeroline = False)\n\n#General Styling\nfig.update_layout(height=400, bargap=0.2,\n                  margin=dict(b=50,r=30,l=100),\n                  title = \"<span style='font-size:36px; font-family:Times New Roman'>Congestion Outliers Analysis</span>\",                  \n                  plot_bgcolor='rgb(242,242,242)',\n                  paper_bgcolor = 'rgb(242,242,242)',\n                  font=dict(family=\"Times New Roman\", size= 14),\n                  hoverlabel=dict(font_color=\"floralwhite\"),\n                  showlegend=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-19T14:11:12.091915Z","iopub.execute_input":"2022-03-19T14:11:12.092153Z","iopub.status.idle":"2022-03-19T14:11:15.937328Z","shell.execute_reply.started":"2022-03-19T14:11:12.092122Z","shell.execute_reply":"2022-03-19T14:11:15.936492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>3 <span style='color:lightseagreen'>|</span> Feature Engineering</b>\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>3.1 | Dataset Score</b></p>\n</div>\n\nHereafter, we are going to define a baseline score which is going to help us to know whether some set of features we've assembled has actually led to any **<span style='color:lightseagreen'>improvement</span>** or not. In this first step, we are going to drop rows containing outliers (after comparison, dataset score is better without outliers).","metadata":{}},{"cell_type":"code","source":"def score_dataset(X, y, model=XGBRegressor(tree_method='gpu_hist', predictor='gpu_predictor'), model_2 = CatBoostRegressor(task_type = 'GPU', silent=True)):\n#def score_dataset(X, y, model=XGBRegressor(), model_2 = CatBoostRegressor(silent=True)):\n    # Label encoding is good for XGBoost and RandomForest, but one-hot\n    # would be better for models like Lasso or Ridge. The `cat.codes`\n    # attribute holds the category levels.\n    for colname in X.select_dtypes([\"object\"]).columns:\n        X[colname] = LabelEncoder().fit_transform(X[colname])\n    X['week'] = X['week'].astype(int)\n    X = X.drop('row_id',axis=1)\n    # Metric for TPS Mar22 competition is MAE (Mean Absolute Error)\n    score_xgb = cross_val_score(\n        model, X, y, cv=5, scoring=\"neg_mean_absolute_error\", n_jobs=-1\n    )\n    \n    score_cat = cross_val_score(\n        model_2, X, y, cv=5, scoring=\"neg_mean_absolute_error\", n_jobs=-1\n    )\n    \n    score = -0.5 * (score_xgb.mean() + score_cat.mean())\n    return score\n\n#df_data = df_data.reset_index().set_index('row_id')\n#df_data = df_data.drop(outliers_index,axis=0)\n#df_data = df_data.reset_index().set_index('time')\n\nx = df_data[df_data['congestion'].isnull() == False].copy()\ny = pd.DataFrame(x.pop('congestion'))\n\nbaseline_score = score_dataset(x, y)\nprint(f\"Baseline score: {baseline_score:.5f} MAE\")","metadata":{"execution":{"iopub.status.busy":"2022-03-19T14:11:15.941208Z","iopub.execute_input":"2022-03-19T14:11:15.941611Z","iopub.status.idle":"2022-03-19T14:13:29.518168Z","shell.execute_reply.started":"2022-03-19T14:11:15.941572Z","shell.execute_reply":"2022-03-19T14:13:29.517169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>3.2 | Cyclical Features</b></p>\n</div>\n\nSince map directions are **<span style='color:lightseagreen'>cyclical</span>**, we can try to capture this by breaking the direction into **<span style='color:lightseagreen'>sin and cos components</span>**. Hereafter, I hand code some value to avoid floating point noise.","metadata":{}},{"cell_type":"code","source":"from math import sin, cos, pi, exp\nsin_vals = {\n    'NB': 0.0,\n    'NE': sin(1 * pi/4),\n    'EB': 1.0,\n    'SE': sin(3 * pi/4),\n    'SB': 0.0,\n    'SW': sin(5 * pi/4),    \n    'WB': -1.0,    \n    'NW': sin(7 * pi/4),  \n}\n\ncos_vals = {\n    'NB': 1.0,\n    'NE': cos(1 * pi/4),\n    'EB': 0.0,\n    'SE': cos(3 * pi/4),\n    'SB': -1.0,\n    'SW': cos(5 * pi/4),    \n    'WB': 0.0,    \n    'NW': cos(7 * pi/4),  \n}\n\ndf_data['sin'] = df_data['direction'].map(sin_vals)\ndf_data['cos'] = df_data['direction'].map(cos_vals)","metadata":{"_uuid":"b5e0e96c-07fe-4215-bfdb-c6acaae7903c","_cell_guid":"aa74dd4a-7611-4649-a20f-4c2d85cba905","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-19T14:13:29.520211Z","iopub.execute_input":"2022-03-19T14:13:29.520727Z","iopub.status.idle":"2022-03-19T14:13:29.783088Z","shell.execute_reply.started":"2022-03-19T14:13:29.520686Z","shell.execute_reply":"2022-03-19T14:13:29.782403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>3.2 | Geography and Direction</b></p>\n</div>\n\nAs congestion is measured for **<span style='color:lightseagreen'>certain points and directions in space</span>**, we can expect that these congestions predict future congestion at the **<span style='color:lightseagreen'>nearest point</span>** in the given direction. For instance, congestion at (0, 1, eastbound) should be correlated with future congestion at (1, 1, ...). The correlation goes both ways so that we can try to predict **<span style='color:lightseagreen'>backwards</span>**: congestion at (1, 1, eastbound) should be correlated with past congestion at (0, 1, ...). At first sight, it is unclear whether we need the geography at all: A simple approach for the competition would be ignoring the geography and creating 65 independent time series. Another simple approach is one-hot encoding the 65 position/direction combinations and using them as features. Although the y coordinate in the diagram grows from bottom to top (south to north), we can't take this for granted. Maybe it should grow from top to bottom.","metadata":{"_uuid":"fa74356a-013d-4c14-8df5-e0bd18b4b70c","_cell_guid":"764ac34a-88c2-45d4-98fb-0038873f8def","trusted":true}},{"cell_type":"markdown","source":"### 3.2.1 | Roadway\nLet's start by creating a feature that allows us to **<span style='color:lightseagreen'>differentiate each of the possible roadways</span>**, taking into account both the x and y coordinates and direction. Then, we will show a graph that will show us if this variable is useful or not. ","metadata":{}},{"cell_type":"code","source":"x = df_data[df_data['congestion'].isnull() == False].copy()\ny = pd.DataFrame(x.pop('congestion'))\nbaseline_score = score_dataset(x, y)\nprint(f\"Baseline score: {baseline_score:.5f} MAE\")","metadata":{"execution":{"iopub.status.busy":"2022-03-19T14:13:29.784619Z","iopub.execute_input":"2022-03-19T14:13:29.785117Z","iopub.status.idle":"2022-03-19T14:14:36.261955Z","shell.execute_reply.started":"2022-03-19T14:13:29.785082Z","shell.execute_reply":"2022-03-19T14:14:36.259844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_data['roadway'] = df_data.x.astype(str) + df_data.y.astype(str) + df_data.direction.astype(str)\npx.box(df_data[df_data.congestion.isnull() == False], x=\"roadway\", y=\"congestion\", color = 'roadway')","metadata":{"execution":{"iopub.status.busy":"2022-03-19T14:14:36.263635Z","iopub.execute_input":"2022-03-19T14:14:36.263904Z","iopub.status.idle":"2022-03-19T14:14:45.630663Z","shell.execute_reply.started":"2022-03-19T14:14:36.263872Z","shell.execute_reply":"2022-03-19T14:14:45.629862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ðŸ“Œ **Interpret:** As we can appreeciate, the feature we have just created allows us to make a good **<span style='color:lightseagreen'>distinction</span>** between the congestion in each of the locations. In other words, this variable will be useful for us. \n### 3.2.2 | Mathematical Transformations\nLet's keep going with our task of feature engineering. Now, we'll add features related with mathematical transformations. We'll **<span style='color:lightseagreen'>multiply</span>** locations coordinates with respective sin/cos function. Finally, we'll multiply it with the hour. ","metadata":{}},{"cell_type":"code","source":"df_data['x_cos_hour'] = df_data.x * df_data.cos * df_data.hour\ndf_data['y_sen_hour'] = df_data.y * df_data.sin * df_data.hour\n\ndf_data = df_data.drop(['year','x','y','direction'], axis=1)\n\nx = df_data[df_data['congestion'].isnull() == False].copy()\ny = pd.DataFrame(x.pop('congestion'))\nbaseline_score = score_dataset(x, y)\nprint(f\"Baseline score: {baseline_score:.5f} MAE\")","metadata":{"execution":{"iopub.status.busy":"2022-03-19T14:14:45.632307Z","iopub.execute_input":"2022-03-19T14:14:45.632738Z","iopub.status.idle":"2022-03-19T14:15:48.962539Z","shell.execute_reply.started":"2022-03-19T14:14:45.632698Z","shell.execute_reply":"2022-03-19T14:15:48.961839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2.4 | Mean, median, maximum, minimum congestion per roadway / time\nHereafter, we are going to create **<span style='color:lightseagreen'>statistical</span>** features related to congestion target value. Those features are going to be **<span style='color:lightseagreen'>grouped by</span>** each of the roadways, and by time also. We are going to create the following features: \n* Mean congestion\n* Median congestion\n* Minimum congestion\n* Maximum congestion","metadata":{}},{"cell_type":"code","source":"df_data = df_data.reset_index()\nkeys = ['roadway', 'day_of_week','hour', 'minute']\n\ndf = df_data.groupby(by=keys).mean().reset_index().set_index(keys)\ndf['mean congestion'] = df['congestion']\ndf_data = df_data.merge(df['mean congestion'], how='left', left_on=keys, right_on=keys)\n\ndf = df_data.groupby(by=keys).median().reset_index().set_index(keys)\ndf['median congestion'] = df['congestion']\ndf_data = df_data.merge(df['median congestion'], how='left', left_on=keys, right_on=keys)\n\ndf = df_data.groupby(by=keys).min().reset_index().set_index(keys)\ndf['min congestion'] = df['congestion']\ndf_data = df_data.merge(df['min congestion'], how='left', left_on=keys, right_on=keys)\n\ndf = df_data.groupby(by=keys).max().reset_index().set_index(keys)\ndf['max congestion'] = df['congestion']\ndf_data = df_data.merge(df['max congestion'], how='left', left_on=keys, right_on=keys)\n\ndf_data = df_data.set_index('time')\n\nx = df_data[df_data['congestion'].isnull() == False].copy()\ny = pd.DataFrame(x.pop('congestion'))\nbaseline_score = score_dataset(x, y)\nprint(f\"Baseline score: {baseline_score:.5f} MAE\")","metadata":{"execution":{"iopub.status.busy":"2022-03-19T14:15:48.966356Z","iopub.execute_input":"2022-03-19T14:15:48.966615Z","iopub.status.idle":"2022-03-19T14:16:55.140044Z","shell.execute_reply.started":"2022-03-19T14:15:48.966583Z","shell.execute_reply":"2022-03-19T14:16:55.139362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>3.3 | Mutual Information</b></p>\n</div>\n\nMutual information describes **<span style='color:lightseagreen'>relationships</span>** in terms of **<span style='color:lightseagreen'>uncertainty</span>**. The mutual information (MI) between two quantities is a measure of the extent to which knowledge of one quantity reduces uncertainty about the other. If you knew the value of a feature, how much more confident would you be about the target? Scikit-learn has two mutual information **<span style='color:lightseagreen'>metrics</span>** in its feature_selection module: one for **<span style='color:lightseagreen'>real-valued targets</span>** (mutual_info_regression) and one for **<span style='color:lightseagreen'>categorical targets</span>** (mutual_info_classif). Our target, price, is real-valued. The next cell computes the MI scores for our features and wraps them up in a nice dataframe. Hereafter, we are going to define a baseline score which is going to help us to know whether some set of features we've assembled has actually led to any **<span style='color:lightseagreen'>improvement</span>** or not.","metadata":{"_uuid":"52c004d9-bb58-4c90-89e9-2f2be8b61239","_cell_guid":"072efe8c-d0ea-4381-91d5-3fafab527983","trusted":true}},{"cell_type":"code","source":"from sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import XGBRegressor\n\ndef make_mi_scores(X, y):\n    X = X.copy()\n    for colname in X.select_dtypes([\"object\"]):\n        X[colname], _ = X[colname].factorize()\n    # All discrete features should now have integer dtypes\n    #discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_regression(X, y, random_state=0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\ny = df_data[df_data['congestion'].isnull() == False]['congestion']\nx = df_data[df_data['congestion'].isnull() == False].drop('congestion', axis=1)\nmi_scores = make_mi_scores(x, y)\nmi_scores = pd.DataFrame(mi_scores).reset_index().rename(columns={'index':'Feature'})","metadata":{"_uuid":"25dfb40c-3d34-4ef8-8443-718ba9e5cbc9","_cell_guid":"6610ef06-9221-4cff-8914-ee89c71f47d2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-19T14:16:55.143662Z","iopub.execute_input":"2022-03-19T14:16:55.143908Z","iopub.status.idle":"2022-03-19T14:19:40.914996Z","shell.execute_reply.started":"2022-03-19T14:16:55.143877Z","shell.execute_reply":"2022-03-19T14:19:40.914248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.bar(mi_scores, x='MI Scores', y='Feature', color=\"MI Scores\",\n             color_continuous_scale='darkmint')\nfig.update_layout(height = 750, title_text=\"Mutual Information Scores\",\n                  title_font=dict(size=29, family=\"Lato, sans-serif\"), xaxis={'categoryorder':'category ascending'}, margin=dict(t=80))","metadata":{"_uuid":"28ff7c5a-72fe-4dff-b82d-b13ba6566e7d","_cell_guid":"bf5eb71a-d732-4c14-92a4-5a5366cfb88e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-19T14:19:40.916465Z","iopub.execute_input":"2022-03-19T14:19:40.916729Z","iopub.status.idle":"2022-03-19T14:19:40.990923Z","shell.execute_reply.started":"2022-03-19T14:19:40.916694Z","shell.execute_reply":"2022-03-19T14:19:40.99024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"qualitative = [col for col in df_data if df_data[col].dtype == 'object']\nfor feature in qualitative:\n    df_data[feature] = LabelEncoder().fit_transform(df_data[feature])\ndf_data = reduce_mem_usage(df_data)\n\ndf_data = df_data.drop(['month','minute','week','day_of_week','is_weekend','day_of_year','cos','sin'],axis=1)\n\ndf_train = df_data[df_data.congestion.isnull() == False]\ndf_test = df_data[df_data.congestion.isnull() == True]\n\nfrom sklearn.model_selection import train_test_split\nX = df_train.drop(['congestion','row_id'],axis = 1)\ny = df_train['congestion']","metadata":{"execution":{"iopub.status.busy":"2022-03-19T14:19:40.992051Z","iopub.execute_input":"2022-03-19T14:19:40.99238Z","iopub.status.idle":"2022-03-19T14:19:41.624903Z","shell.execute_reply.started":"2022-03-19T14:19:40.992344Z","shell.execute_reply":"2022-03-19T14:19:41.623464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>4 <span style='color:lightseagreen'>|</span> Modeling</b>\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>4.1 | Catboost</b></p>\n</div>\n\n### 4.1.1 | Hyperparameter Tuning - Optuna\n\nIn this case, only for Catboost, we are going to make the **<span style='color:lightseagreen'>tuning with Optuna</span>**. I will add the code for hyperparameter tuning below. However, for not **<span style='color:lightseagreen'>wasting CPU time</span>**, since I have run it once, I will simply create the model with the specific features values. I will control whether making hyperparameter tuning or not with **<span style='color:lightseagreen'>allow_optimize</span>** Finally, just say that code for tuning takes plenty of time. Due to that I enabled GPU technology. ","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error as mae\ndef objective(trial):\n    params = {\n        \"random_state\":trial.suggest_categorical(\"random_state\", [2022]),\n        'learning_rate' : trial.suggest_loguniform('learning_rate', 0.0001, 0.3),\n        'bagging_temperature' :trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n        \"n_estimators\": 1000,\n        \"max_depth\":trial.suggest_int(\"max_depth\", 4, 16),\n        'random_strength' :trial.suggest_int('random_strength', 0, 100),\n        \"l2_leaf_reg\":trial.suggest_float(\"l2_leaf_reg\",1e-8,3e-5),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n        \"max_bin\": trial.suggest_int(\"max_bin\", 200, 500),\n        'od_type': trial.suggest_categorical('od_type', ['IncToDec', 'Iter']),\n        'task_type': trial.suggest_categorical('task_type', ['GPU']),\n        'loss_function': trial.suggest_categorical('loss_function', ['MAE']),\n        'eval_metric': trial.suggest_categorical('eval_metric', ['MAE'])\n    }\n\n    model = CatBoostRegressor(**params)\n    X_train_tmp, X_valid_tmp, y_train_tmp, y_valid_tmp = train_test_split(X, y, test_size=0.3, random_state=42)\n    model.fit(\n        X_train_tmp, y_train_tmp,\n        eval_set=[(X_valid_tmp, y_valid_tmp)],\n        early_stopping_rounds=35, verbose=0\n    )\n        \n    y_train_pred = model.predict(X_train_tmp)\n    y_valid_pred = model.predict(X_valid_tmp)\n    train_mae = mae(y_train_tmp, y_train_pred)\n    valid_mae = mae(y_valid_tmp, y_valid_pred)\n    \n    print(f'MAE of Train: {train_mae}')\n    print(f'MAE of Validation: {valid_mae}')\n    \n    return valid_mae\n\nallow_optimize = 0","metadata":{"execution":{"iopub.status.busy":"2022-03-19T14:19:41.62621Z","iopub.execute_input":"2022-03-19T14:19:41.626444Z","iopub.status.idle":"2022-03-19T14:19:41.637927Z","shell.execute_reply.started":"2022-03-19T14:19:41.626412Z","shell.execute_reply":"2022-03-19T14:19:41.636341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRIALS = 100\nTIMEOUT = 3600\n\nif allow_optimize:\n    sampler = TPESampler(seed=42)\n\n    study = optuna.create_study(\n        study_name = 'cat_parameter_opt',\n        direction = 'minimize',\n        sampler = sampler,\n    )\n    study.optimize(objective, n_trials=TRIALS)\n    print(\"Best Score:\",study.best_value)\n    print(\"Best trial\",study.best_trial.params)\n    \n    best_params = study.best_params\n    \n    X_train_tmp, X_valid_tmp, y_train_tmp, y_valid_tmp = train_test_split(X, y, test_size=0.3, random_state=42)\n    model_tmp = CatBoostRegressor(**best_params, n_estimators=30000, verbose=1000).fit(X_train_tmp, y_train_tmp, eval_set=[(X_valid_tmp, y_valid_tmp)], early_stopping_rounds=35)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-19T14:19:41.639277Z","iopub.execute_input":"2022-03-19T14:19:41.639559Z","iopub.status.idle":"2022-03-19T14:19:41.649385Z","shell.execute_reply.started":"2022-03-19T14:19:41.639525Z","shell.execute_reply":"2022-03-19T14:19:41.648598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.1.2 | Fitting - Feature Importances","metadata":{}},{"cell_type":"code","source":"if allow_optimize:\n    model = CatBoostRegressor(**best_params, n_estimators=model_tmp.get_best_iteration(), verbose=1000).fit(X, y)\nelse:\n    model = CatBoostRegressor(\n        verbose=1000,\n        early_stopping_rounds=10,\n        #iterations=5000,\n        random_state = 2022, learning_rate = 0.0824038781081412, bagging_temperature = 0.03568558360430449, max_depth = 16, \n        random_strength = 47, l2_leaf_reg = 7.459775961819184e-06, min_child_samples = 49, max_bin = 320, od_type = 'Iter', \n        task_type = 'GPU', loss_function = 'MAE', eval_metric = 'MAE'\n    ).fit(X, y)    ","metadata":{"execution":{"iopub.status.busy":"2022-03-19T14:19:41.650753Z","iopub.execute_input":"2022-03-19T14:19:41.65113Z","iopub.status.idle":"2022-03-19T14:21:45.56288Z","shell.execute_reply.started":"2022-03-19T14:19:41.651097Z","shell.execute_reply":"2022-03-19T14:21:45.562063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_feature_importance(model.get_feature_importance(),X.columns,'CatBoost')","metadata":{"execution":{"iopub.status.busy":"2022-03-19T14:21:45.564407Z","iopub.execute_input":"2022-03-19T14:21:45.564689Z","iopub.status.idle":"2022-03-19T14:21:49.801259Z","shell.execute_reply.started":"2022-03-19T14:21:45.564652Z","shell.execute_reply":"2022-03-19T14:21:49.800586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.1.3 | Making Predictions","metadata":{}},{"cell_type":"code","source":"x_test = df_test.drop(['congestion','row_id'],axis = 1).copy()\npredictions = model.predict(x_test)\nsubmit_cat = pd.DataFrame({'row_id':df_test.row_id, 'congestion':predictions})\nsubmit_cat = submit_cat.reset_index().drop('time',axis=1).set_index('row_id')","metadata":{"execution":{"iopub.status.busy":"2022-03-19T14:21:49.802662Z","iopub.execute_input":"2022-03-19T14:21:49.803126Z","iopub.status.idle":"2022-03-19T14:21:49.830434Z","shell.execute_reply.started":"2022-03-19T14:21:49.803089Z","shell.execute_reply":"2022-03-19T14:21:49.82978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>4.2 | XGBoost</b></p>\n</div>\n\n### 4.2.1 | Hyperparameter Tuning - GridSearch / RandomizedSearch\n\nThe **<span style='color:lightseagreen'>grid search</span>** approach is fine when you are exploring relatively **<span style='color:lightseagreen'>few combinations</span>**. However, when the hyperparameter **<span style='color:lightseagreen'>search space is large</span>**, it is often preferable to use **<span style='color:lightseagreen'>RandomizedSearchCV</span>** instead. This class can be used in much the same way as the GridSearchCV class, but instead of trying out all possible combinations, it evaluates a given number of random combinations by selecting a **<span style='color:lightseagreen'>random</span>** value for each hyperparameter at every iteration. This approach has two main benefits:\n\n1. If you let the randomized search run for, say, 1,000 iterations, this approach will explore 1,000 different values for each hyperparameter (instead of just a few values per hyperparameter with the grid search approach).\n2. You have more control over the computing budget you want to allocate to hyperparameter search, simply by setting the number of iterations.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nallow_optimize = 0\nif allow_optimize:\n    param_grid={'max_depth': [4,5,6,7,8,9],\n            #'n_estimators': [100,200,300,400,500,600,700,800,900,1000],\n            'min_child_weight' : [1,2,3,4,5,6],\n            'gpu_id' : [0]\n        }\n\n    regressor = XGBRegressor(tree_method = 'gpu_hist', predictor = 'gpu_predictor')\n    CV_regressor = GridSearchCV(regressor, param_grid, cv=3, scoring=\"neg_mean_absolute_error\", n_jobs= -1, return_train_score = True, verbose = 0)\n    CV_regressor.fit(X, y)\n    \n    print(\"The best hyperparameters are : \",\"\\n\")\n    print(CV_regressor.best_params_)","metadata":{"execution":{"iopub.status.busy":"2022-03-19T14:21:49.831661Z","iopub.execute_input":"2022-03-19T14:21:49.831906Z","iopub.status.idle":"2022-03-19T14:21:49.839105Z","shell.execute_reply.started":"2022-03-19T14:21:49.831873Z","shell.execute_reply":"2022-03-19T14:21:49.838434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2.2 | Fitting - Feature Importance","metadata":{}},{"cell_type":"code","source":"if allow_optimize: \n    CV_regressor = CV_regressor.best_estimator_\nelse:\n    CV_regressor = XGBRegressor(tree_method = 'gpu_hist', predictor = 'gpu_predictor', gpu_id = 0, max_depth = 4, n_estimators = 100)\nCV_regressor.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2022-03-19T14:21:49.840342Z","iopub.execute_input":"2022-03-19T14:21:49.841177Z","iopub.status.idle":"2022-03-19T14:21:50.675464Z","shell.execute_reply.started":"2022-03-19T14:21:49.841143Z","shell.execute_reply":"2022-03-19T14:21:50.674662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_feature_importance(CV_regressor.feature_importances_,X.columns,'XGBOOST')","metadata":{"execution":{"iopub.status.busy":"2022-03-19T14:21:50.676805Z","iopub.execute_input":"2022-03-19T14:21:50.67705Z","iopub.status.idle":"2022-03-19T14:21:50.945339Z","shell.execute_reply.started":"2022-03-19T14:21:50.677017Z","shell.execute_reply":"2022-03-19T14:21:50.944638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2.3 | Making Predictions","metadata":{}},{"cell_type":"code","source":"predictions = CV_regressor.predict(x_test)\nsubmit_xgb = pd.DataFrame({'row_id':df_test.row_id, 'congestion':predictions})\nsubmit_xgb = submit_xgb.reset_index().drop('time',axis=1).set_index('row_id')","metadata":{"execution":{"iopub.status.busy":"2022-03-19T14:21:50.946795Z","iopub.execute_input":"2022-03-19T14:21:50.947268Z","iopub.status.idle":"2022-03-19T14:21:50.962836Z","shell.execute_reply.started":"2022-03-19T14:21:50.947229Z","shell.execute_reply":"2022-03-19T14:21:50.961279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>4.3 | Blending and Submitting</b></p>\n</div>\n\nFinally for this kernel, we will the next blending and submit our predictions. We'll be using [Martynov Andrey special value dataset](https://www.kaggle.com/martynovandrey/tps-mar-22-don-t-forget-special-values/data) in order to make our score even better. If you found this kernel interesting or if you think you learned something new, feel free to give it an **<span style='color:lightseagreen'>upvote</span>**. ","metadata":{}},{"cell_type":"code","source":"submit = pd.DataFrame({'congestion': submit_cat['congestion']+0*submit_xgb['congestion']})\nspecial = pd.read_csv('../input/tps-mar-22-special-values/special v2.csv', index_col=\"row_id\")\nspecial = special[['congestion']].rename(columns={'congestion':'special'})\nsubmit = submit.merge(special, left_index=True, right_index=True, how='left')\nsubmit['special'] = submit['special'].fillna(submit['congestion']).round().astype(int)\nsubmit = submit.drop(['congestion'], axis=1).rename(columns={'special':'congestion'})\nsubmit['congestion'] = round(submit['congestion'])\nsubmit.to_csv('./submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-03-19T14:21:50.967931Z","iopub.execute_input":"2022-03-19T14:21:50.96947Z","iopub.status.idle":"2022-03-19T14:21:50.997495Z","shell.execute_reply.started":"2022-03-19T14:21:50.969437Z","shell.execute_reply":"2022-03-19T14:21:50.996795Z"},"trusted":true},"execution_count":null,"outputs":[]}]}