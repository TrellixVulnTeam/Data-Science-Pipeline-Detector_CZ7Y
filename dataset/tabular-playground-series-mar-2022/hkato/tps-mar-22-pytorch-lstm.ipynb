{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 必要なモジュールをインポートする\nimport numpy as np\nimport pandas as pd\nimport warnings\nimport time\nwarnings.simplefilter('ignore')\nimport math\nfrom statistics import mean\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import timedelta\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint('使用デバイス：', device)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-31T15:33:58.173057Z","iopub.execute_input":"2022-03-31T15:33:58.173308Z","iopub.status.idle":"2022-03-31T15:33:58.180629Z","shell.execute_reply.started":"2022-03-31T15:33:58.173279Z","shell.execute_reply":"2022-03-31T15:33:58.179681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# データダウンロード\ntrain_df = pd.read_csv('../input/tabular-playground-series-mar-2022/train.csv')\ntest_df = pd.read_csv('../input/tabular-playground-series-mar-2022/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-03-31T15:33:58.555443Z","iopub.execute_input":"2022-03-31T15:33:58.555911Z","iopub.status.idle":"2022-03-31T15:33:59.222963Z","shell.execute_reply.started":"2022-03-31T15:33:58.555876Z","shell.execute_reply":"2022-03-31T15:33:59.222223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#データ確認\nprint(train_df.shape)  #形を確認\ntrain_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T15:33:59.224725Z","iopub.execute_input":"2022-03-31T15:33:59.224977Z","iopub.status.idle":"2022-03-31T15:33:59.236031Z","shell.execute_reply.started":"2022-03-31T15:33:59.224942Z","shell.execute_reply":"2022-03-31T15:33:59.235376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_df.shape)\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T15:33:59.237463Z","iopub.execute_input":"2022-03-31T15:33:59.237922Z","iopub.status.idle":"2022-03-31T15:33:59.251778Z","shell.execute_reply.started":"2022-03-31T15:33:59.237883Z","shell.execute_reply":"2022-03-31T15:33:59.250856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# idをインデックスに指定\ntrain_df.set_index(keys='row_id', inplace=True)\ntest_df.set_index(keys='row_id', inplace=True)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T15:33:59.253766Z","iopub.execute_input":"2022-03-31T15:33:59.254156Z","iopub.status.idle":"2022-03-31T15:33:59.26757Z","shell.execute_reply.started":"2022-03-31T15:33:59.254121Z","shell.execute_reply":"2022-03-31T15:33:59.266852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 正解ラベルのみ分けておく\ntrain_df_y = pd.DataFrame(train_df['congestion'])\ntrain_df.drop(['congestion'], axis=1, inplace=True)\ntrain_df_y.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T15:33:59.356257Z","iopub.execute_input":"2022-03-31T15:33:59.356514Z","iopub.status.idle":"2022-03-31T15:33:59.400874Z","shell.execute_reply.started":"2022-03-31T15:33:59.356487Z","shell.execute_reply":"2022-03-31T15:33:59.399989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_dfとtest_dfを結合する\nntrain = train_df.shape[0]\nall_data = pd.concat((train_df, test_df))#.reset_index(drop=True)\nprint(all_data.shape)\nall_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T15:33:59.575694Z","iopub.execute_input":"2022-03-31T15:33:59.575896Z","iopub.status.idle":"2022-03-31T15:33:59.617696Z","shell.execute_reply.started":"2022-03-31T15:33:59.575871Z","shell.execute_reply":"2022-03-31T15:33:59.617017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# データの持つ情報を確認\nall_data.info()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T15:33:59.715326Z","iopub.execute_input":"2022-03-31T15:33:59.715549Z","iopub.status.idle":"2022-03-31T15:33:59.882015Z","shell.execute_reply.started":"2022-03-31T15:33:59.715523Z","shell.execute_reply":"2022-03-31T15:33:59.88133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data['time'] = pd.to_datetime(all_data['time'])\nall_data.info()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T15:33:59.915599Z","iopub.execute_input":"2022-03-31T15:33:59.91579Z","iopub.status.idle":"2022-03-31T15:34:00.125527Z","shell.execute_reply.started":"2022-03-31T15:33:59.915765Z","shell.execute_reply":"2022-03-31T15:34:00.124693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 曜日を追加するための関数を定義\ndef get_weekday_jp(dt):\n    w_list = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n    return(w_list[dt.weekday()])\nall_data[\"weekday\"] = all_data[\"time\"].apply(get_weekday_jp)\nall_data['hour'] = all_data['time'].dt.hour\nall_data['minutes'] = all_data['time'].dt.minute\nall_data.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T15:34:01.271918Z","iopub.execute_input":"2022-03-31T15:34:01.272507Z","iopub.status.idle":"2022-03-31T15:34:06.432545Z","shell.execute_reply.started":"2022-03-31T15:34:01.272468Z","shell.execute_reply":"2022-03-31T15:34:06.43069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"week_day = pd.get_dummies(all_data['weekday'])\nall_data = pd.concat([all_data, week_day], axis=1)\n\n# print(all_data.columns)\n# print(all_data.shape)\nall_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T15:34:06.434324Z","iopub.execute_input":"2022-03-31T15:34:06.435183Z","iopub.status.idle":"2022-03-31T15:34:06.575872Z","shell.execute_reply.started":"2022-03-31T15:34:06.435142Z","shell.execute_reply":"2022-03-31T15:34:06.574949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data[\"key\"] = all_data[\"x\"].map(lambda item: str(item)) + \"_\" + all_data[\"y\"].map(lambda item: str(item)) + \"_\" + all_data[\"direction\"]\nprint(all_data['key'].unique())\nall_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T15:34:06.577267Z","iopub.execute_input":"2022-03-31T15:34:06.577629Z","iopub.status.idle":"2022-03-31T15:34:07.671926Z","shell.execute_reply.started":"2022-03-31T15:34:06.577585Z","shell.execute_reply":"2022-03-31T15:34:07.671243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data.info()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T15:34:07.673831Z","iopub.execute_input":"2022-03-31T15:34:07.674317Z","iopub.status.idle":"2022-03-31T15:34:07.927392Z","shell.execute_reply.started":"2022-03-31T15:34:07.674278Z","shell.execute_reply":"2022-03-31T15:34:07.926641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"con_yestarday = train_df_y.shift().rename(columns={'congestion': 'congestion_pre'})\nall_data = pd.concat([all_data, con_yestarday], axis=1)\nprint(all_data['congestion_pre'].mean())\nall_data.at[0, 'congestion_pre'] = all_data['congestion_pre'].mean()\nall_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T15:34:08.078399Z","iopub.execute_input":"2022-03-31T15:34:08.078621Z","iopub.status.idle":"2022-03-31T15:34:08.226308Z","shell.execute_reply.started":"2022-03-31T15:34:08.078594Z","shell.execute_reply":"2022-03-31T15:34:08.225617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#欠損値を確認   ->   conset欠損値なし\nall_data_na = (all_data.isnull().sum() / len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(22)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T15:34:08.283416Z","iopub.execute_input":"2022-03-31T15:34:08.283621Z","iopub.status.idle":"2022-03-31T15:34:08.544208Z","shell.execute_reply.started":"2022-03-31T15:34:08.283596Z","shell.execute_reply":"2022-03-31T15:34:08.543503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 元に戻す\ntrain_df = all_data[:ntrain]\ntest_df = all_data[ntrain:]\nprint(train_df.shape, test_df.shape)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T15:34:11.275016Z","iopub.execute_input":"2022-03-31T15:34:11.275288Z","iopub.status.idle":"2022-03-31T15:34:11.29589Z","shell.execute_reply.started":"2022-03-31T15:34:11.275256Z","shell.execute_reply":"2022-03-31T15:34:11.295157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_num = 3\ny_num = 4\ngraphs_count = 1\naxes = []\nfig = plt.figure(figsize=(18,60))\nfor i in range(3):\n    for j in range(4):\n        for d in ['EB', 'NB', 'SB', 'WB', 'NE', 'SW']:\n            axes.append(fig.add_subplot(x_num*6, y_num, graphs_count))\n            idx = train_df[(train_df['x']==i) & (train_df['y']==j) & (train_df['direction']==d)].index.tolist()\n            axes[graphs_count-1].plot(train_df['time'][idx[:210]], train_df_y['congestion'][idx[:210]])\n            axes[graphs_count-1].set_title(str(i)+'_'+str(j)+'_'+d)\n            graphs_count += 1\nfig.subplots_adjust(wspace=0.3, hspace=0.2)\nfig.autofmt_xdate(rotation=45)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:39:45.395502Z","iopub.execute_input":"2022-03-31T12:39:45.396213Z","iopub.status.idle":"2022-03-31T12:40:04.864109Z","shell.execute_reply.started":"2022-03-31T12:39:45.396178Z","shell.execute_reply":"2022-03-31T12:40:04.863307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#学習用のデータをモデルの学習用とモデルの精度の検証用に分割\n#今回は、モデル用学習データ:精度用の検証データ = 8 : 2 に分割\nlength = len(train_df)\ntrain_size = int(length * 0.8)\ntest_size = length - train_size\nX_train, X_test = train_df[0:train_size], train_df[train_size:length]\ny_train, y_test = train_df_y[0:train_size], train_df_y[train_size:length]\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)\nX_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T15:34:18.230613Z","iopub.execute_input":"2022-03-31T15:34:18.231153Z","iopub.status.idle":"2022-03-31T15:34:18.249136Z","shell.execute_reply.started":"2022-03-31T15:34:18.231114Z","shell.execute_reply":"2022-03-31T15:34:18.248301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data.sampler import SubsetRandomSampler\nclass MyDataset(torch.utils.data.Dataset):\n    def __init__(self, X, sequence_num, y=None, mode='train'):\n        self.data = X\n        self.teacher = y\n        self.sequence_num = sequence_num\n        self.mode = mode\n    def __len__(self):\n        return len(self.teacher)\n\n    def __getitem__(self, idx):\n        out_data = self.data[idx]\n        \n        if self.mode == 'train':\n            out_label =  self.teacher[idx[-1]+1]\n            return out_data, out_label\n        else:\n          return out_data\ndef create_dataset(dataset, dataset_num, sequence_num, input_size, batch_size):\n    sampler = np.array([list(range(i, i+sequence_num)) for i in range(dataset_num-sequence_num)])\n    # np.random.shuffle(sampler)\n    dataloader = DataLoader(dataset, batch_size, sampler=sampler)\n    return dataloader\n\n###########  動作確認　###############\nsequence_num = 10\nX = np.random.rand(1000, 5)\ny = np.random.rand(1000, 1)\n\ndataset = MyDataset(X, y=y, sequence_num=sequence_num)\ndataloader = create_dataset(dataset, X.shape[0], sequence_num, X.shape[1], 32)\n# dataloader = DataLoader(dataset, batch_size=32)#, sampler=sampler)\nfor b, tup in enumerate(dataloader):\n  print('---------')\n  print(tup[0].shape, tup[1].shape)\n  break\nprint(X[-2], y[-1])\n######################################","metadata":{"execution":{"iopub.status.busy":"2022-03-31T15:34:21.326958Z","iopub.execute_input":"2022-03-31T15:34:21.327223Z","iopub.status.idle":"2022-03-31T15:34:21.346461Z","shell.execute_reply.started":"2022-03-31T15:34:21.327191Z","shell.execute_reply":"2022-03-31T15:34:21.345744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LSTM(nn.Module):\n    def __init__(self, input_size=5, hidden_layer_size=100,\n                 output_size=1, batch_size = 32):\n        super().__init__()\n        self.hidden_layer_size = hidden_layer_size\n        self.batch_size = batch_size\n        self.lstm = nn.LSTM(input_size, hidden_layer_size, batch_first=True)\n\n        self.linear = nn.Linear(hidden_layer_size, output_size)\n\n        self.hidden_cell = (torch.zeros(1, self.batch_size, self.hidden_layer_size),\n                            torch.zeros(1, self.batch_size, self.hidden_layer_size))\n        self.logs_train = [[], [np.inf]]\n        self.logs_valid = [[], [np.inf]]\n        self.stdsc = StandardScaler()\n        self.stdsc_y = StandardScaler()\n\n    def forward(self, input_seq):\n        batch_size, seq_len = input_seq.shape[0], input_seq.shape[1]\n        lstm_out, self.hidden_cell = self.lstm(input_seq,#.view(seq_len, batch_size, 1),\n                                               self.hidden_cell) #lstm入力サイズは(バッチサイズ、シーケンスサイズ、特徴量次元数)\n        predictions = self.linear(self.hidden_cell[0].view(batch_size, -1))\n        return predictions[:, 0]\n\n    def fit(self, X, y, num_epochs=50, sequence_num=10, batch_size=32):\n        #GPUが使えるか確認\n        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n        print('使用デバイス：', device)\n        ####   データ処理↓\n        # 標準化\n        X = self.stdsc.fit_transform(X)\n        y = self.stdsc_y.fit_transform(y)\n        # データセット・データローダ作成\n        num_train = len(X)\n        dataset_size = X.shape[0]# データサイズ\n        input_size = X.shape[1]  # 入力サイズ\n        # データセット作成\n        dataset = MyDataset(X, y=y, sequence_num=sequence_num, mode='train')\n        #データローダー作成\n        dataloader = create_dataset(dataset, dataset_size, sequence_num, input_size, batch_size)\n        ####    学習設定\n        # 最適化手法の設定\n        lr = 0.01\n        beta1, beta2 = 0.0, 0.9\n        optimizer = torch.optim.Adagrad(self.parameters(), lr)#, [beta1, beta2])\n        # 誤差関数を定義\n        criterion = nn.MSELoss()\n        #ネットワークをGPUへ\n        self.to(device)\n        #モデルを学習モードに\n        self.train()\n        #ネットワークがある程度固定であれば、高速化させる\n        torch.backends.cudnn.benchark = True\n        #バッチサイズを保存\n        batch_size = dataloader.batch_size\n        #イテレーションカウンタをセット\n        iteration = 1\n        #epochのループ\n        for epoch in range(num_epochs):\n            # 開始時刻を保存\n            t_epoch_strat = time.time()\n            epoch_loss = 0.0\n            print('-------------')\n            print('Epoch {}/{}'.format(epoch, num_epochs))\n            print('-------------')\n            print(' (train) ')\n            ### 学習\n            #データローダーからminibatchずつ取り出すループ\n            for data, targets in dataloader:\n                #GPUで扱えるように変換\n                data = data.to(device)\n                targets = targets.to(device)\n                #勾配初期化\n                optimizer.zero_grad()\n                self.hidden_cell = (torch.zeros(1, len(data), self.hidden_layer_size).to(device),\n                                    torch.zeros(1, len(data), self.hidden_layer_size).to(device))\n\n                #出力を得る\n                data = data.to(torch.float32)\n                output = self.forward(data)\n                output = output.view(1,-1)[0]\n                targets = targets.to(torch.float32)\n                #誤差を計算\n                loss = criterion(output, targets)\n                #誤差逆伝播\n                loss.backward()\n                #ステップ\n                optimizer.step()\n                #誤差を記録\n                epoch_loss += loss.item()\n                iteration += 1\n            #epochごとのloss\n            t_epoch_finish = time.time()\n            print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n                epoch+1, \n                epoch_loss/num_train,\n                ))\n            print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_strat))\n            #データの保存\n            if epoch_loss/num_train < min(self.logs_train[1]):\n                print('--save model--')\n            self.logs_train[0].append(epoch+1)\n            self.logs_train[1].append(epoch_loss/num_train)\n            torch.save(self.state_dict(), './models')\n\n    def predict(self, X, sequence_num=10):\n        #GPUが使えるか確認\n        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n        print('使用デバイス：', device)\n        valid_loss = 0.0\n        num_valid = len(X)\n        indices_valid = list(range(num_valid))\n        #標準化\n        X = self.stdsc.transform(X)\n        y_pred = np.array([])\n        for date in range(num_valid-sequence_num):\n            # # 1. 予測日とその前日までのデータセット作成\n            # valid_dataset = MyDataset(X[date:date+sequence_num], sequence_num, mode='val')\n            # print(X[date:date+sequence_num])\n            # # 2. データローダー作成\n            # valid_loader = create_dataset(valid_dataset, X[date:date+sequence_num].shape[0], sequence_num=sequence_num, input_size=X.shape[1], batch_size=1)\n            # 1. \n            valid_data = X[date:date+sequence_num]\n            # 2. 予測\n            self.eval()\n            data = torch.from_numpy(np.array([valid_data]).astype(np.float32)).clone()\n            data = data.to(device)\n            self.hidden_cell = (torch.zeros(1, len(data), self.hidden_layer_size).to(device),\n                                torch.zeros(1, len(data), self.hidden_layer_size).to(device))\n            output = self.forward(data)\n            output = output.view(1, -1)\n            output = output.to('cpu').detach().numpy().copy()\n            X[date+sequence_num][-1] = output[0][0]\n            y_pred = np.append(y_pred, output[0])\n            # # 3. 予測\n            # model.eval()\n            # y_pred = np.array([])\n            # for data in valid_loader:\n            #     print(data.shape)\n            #     data = data.to(device)\n            #     data = data.to(torch.float32)\n            #     self.hidden_cell = (torch.zeros(1, len(data), self.hidden_layer_size).to(device),\n            #                         torch.zeros(1, len(data), self.hidden_layer_size).to(device))\n            #     output = model.forward(data)\n            #     output = output.view(1, -1)\n            #     output = output.to('cpu').detach().numpy().copy()\n            #     y_pred = np.append(y_pred, output[0])\n\n        y_pred = np.array(y_pred)\n        y_pred = y_pred.reshape(-1, 1)\n        y_pred = self.stdsc_y.inverse_transform(y_pred)\n        return y_pred\n\n###########  動作確認　###############\nmodel = LSTM(input_size=7)\ndata = np.random.rand(32, 10, 7)\ndata = torch.from_numpy(data.astype(np.float32)).clone()\nmodel.hidden_cell = (torch.zeros(1, len(data), model.hidden_layer_size),\n                                    torch.zeros(1, len(data), model.hidden_layer_size))\nmodel(data)\n######################################","metadata":{"execution":{"iopub.status.busy":"2022-03-31T15:43:04.172418Z","iopub.execute_input":"2022-03-31T15:43:04.172851Z","iopub.status.idle":"2022-03-31T15:43:04.218844Z","shell.execute_reply.started":"2022-03-31T15:43:04.172815Z","shell.execute_reply":"2022-03-31T15:43:04.218202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train():\n    # 学習\n    epochs = 20\n    batch_size = 16\n    sequence_num = 16\n    categorical_columns = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday', 'hour', 'minutes', 'congestion_pre']\n    # 各道路種ごとにモデル作成\n    model_dict = {}\n    for load in X_train['key'].unique():\n        print(\"############################  {}  #################################\".format(load))\n        # インスタンス作成\n        model_i = LSTM(input_size=len(categorical_columns))\n        model_i.to(device)\n        X_train_np = X_train[X_train['key'] == load][categorical_columns].values\n        y_train_np = y_train[X_train['key'] == load].values\n        #　テストデータセットは予測日のsequence_num分前のデータも持っておく\n        X_test_np = X_test[X_test['key'] == load][categorical_columns].values\n        X_test_np = np.append(X_test_np, X_train_np[-1*sequence_num:], axis=0)\n        y_test_np = y_test[X_test['key'] == load].values\n        y_test_np = np.append(y_test_np, y_train_np[-1*sequence_num:], axis=0)\n        print(X_train_np.shape, y_train_np.shape)\n        print(X_test_np.shape)\n        # 学習\n        model_i.fit(X_train_np, y_train_np, num_epochs=epochs, sequence_num=sequence_num, batch_size=batch_size)\n        #　予測\n        pred_y = model_i.predict(X_test_np, sequence_num)\n        score = mean_squared_error(y_test_np[sequence_num:], pred_y)\n        print(score)\n        plt.plot(list(range(len(y_test_np[sequence_num:]))), y_test_np[sequence_num:])\n        plt.plot(list(range(len(pred_y))), pred_y)\n        plt.title(load)\n        plt.show()\n        model_dict[load] = model_i\n    return model_dict\nmodel_dict = train()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T16:17:59.159602Z","iopub.execute_input":"2022-03-31T16:17:59.159877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# モデルの保存\nfor load in X_train['key'].unique():\n    model = model_dict[load]\n    torch.save(model, \"model_\"+load)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T16:11:56.887658Z","iopub.execute_input":"2022-03-31T16:11:56.888334Z","iopub.status.idle":"2022-03-31T16:11:57.032376Z","shell.execute_reply.started":"2022-03-31T16:11:56.888292Z","shell.execute_reply":"2022-03-31T16:11:57.031608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 提出データ作成\nsequence_num = 36\ncategorical_columns = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday', 'hour', 'minutes', 'congestion_pre']\npred_y = [0 for i in range(len(test_df))]\nfor i, load in enumerate(X_train['key'].unique()):\n    # モデルロード\n    model_path = 'model_'+load\n    model_i = torch.load(model_path)\n#     model_i = LSTM(input_size=len(categorical_columns)).to(device)\n    # データ作成\n    X_submit =  test_df[test_df['key'] == load][categorical_columns].values\n    print(X_submit.shape)\n    X_submit[0][-1] = train_df_y.values[-1]\n    X_submit = np.append(train_df[train_df['key'] == load][categorical_columns].values[-1*sequence_num:], X_submit, axis=0)\n    print(X_submit.shape)\n    # モデル\n    pred_y_i = model_i.predict(X_submit, sequence_num)\n    print(pred_y_i.shape)\n    for j, p in enumerate(pred_y_i):\n        pred_y[65*j+i] = p\npred_y = np.array(pred_y)\nprint(pred_y)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T16:11:57.033764Z","iopub.execute_input":"2022-03-31T16:11:57.034021Z","iopub.status.idle":"2022-03-31T16:12:07.69473Z","shell.execute_reply.started":"2022-03-31T16:11:57.033988Z","shell.execute_reply":"2022-03-31T16:12:07.694034Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.read_csv('../input/tabular-playground-series-mar-2022/sample_submission.csv')\nsubmission_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T16:12:07.696591Z","iopub.execute_input":"2022-03-31T16:12:07.696847Z","iopub.status.idle":"2022-03-31T16:12:07.715828Z","shell.execute_reply.started":"2022-03-31T16:12:07.696805Z","shell.execute_reply":"2022-03-31T16:12:07.715174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df['congestion'] = pd.DataFrame(pred_y)\nsubmission_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T16:12:07.71854Z","iopub.execute_input":"2022-03-31T16:12:07.718727Z","iopub.status.idle":"2022-03-31T16:12:07.7267Z","shell.execute_reply.started":"2022-03-31T16:12:07.718702Z","shell.execute_reply":"2022-03-31T16:12:07.726037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv(\"./submit.csv\", index=False, header=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T16:12:07.727993Z","iopub.execute_input":"2022-03-31T16:12:07.728482Z","iopub.status.idle":"2022-03-31T16:12:07.748386Z","shell.execute_reply.started":"2022-03-31T16:12:07.728443Z","shell.execute_reply":"2022-03-31T16:12:07.747743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}