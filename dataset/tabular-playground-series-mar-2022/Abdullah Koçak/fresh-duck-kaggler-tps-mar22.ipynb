{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# REFERENCES\n\nThank you all for sharing your ideas in notebooks and discussions. I am very new here and trying to improve my knowledge on data science. This will be my first notebook, I am open to any kind of comments, ideas and critics. I used features and models from other notebooks, try to implement on my way, so I want to mention about these notebooks.\n\n1. Thank you for your brief explanation and visualizations, I get the idea of using mean and median values from this notebook. Moreover I get the reduce memory function from it. https://www.kaggle.com/code/javigallego/tps-mar22-top-6-solution-eda-fe-blending#3-%7C-Feature-Engineering\n\n1. I was troubling adding lag features, thanks to this notebook I figured it out. https://www.kaggle.com/code/martynovandrey/tps-mar-22-fe-the-less-the-better#Time-lags \n\n1. Thanks for generalizing special values. https://www.kaggle.com/code/ambrosm/tpsmar22-generalizing-the-special-values/notebook","metadata":{}},{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"For the March edition of the 2022 Tabular Playground Series you're challenged to forecast twelve-hours of traffic flow in a U.S. metropolis. The time series in this dataset are labelled with both location coordinates and a direction of travel -- a combination of features that will test your skill at spatio-temporal forecasting within a highly dynamic traffic network.\n\n\n<font color = \"blue\">\nContent:\n    \n1. [Read Data](#1)\n1. [Reduce Memory Usage](#2)\n1. [Getting familiar with Data](#3)\n1. [Simple Exploratary Data Analysis](#4)\n1. [Feature Engineering](#5)\n    * [Time Features](#6)\n    * [Mean and Median Values](#7)\n    * [Min and Max Congestion](#8)\n    * [Morning Averages](#9)\n    * [Lag Features](#10)\n    * [Label Encoding](#11)\n1. [Outliers Detection](#12)\n1. [Modelling with CATBOOST](#13)\n1. [Submission](#14)\n\n ","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport optuna\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom catboost import CatBoostRegressor\nimport seaborn as sns\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.graphics.tsaplots import plot_acf\nplt.style.use(\"seaborn-whitegrid\")\nfrom collections import Counter\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-24T20:20:35.28447Z","iopub.execute_input":"2022-03-24T20:20:35.285031Z","iopub.status.idle":"2022-03-24T20:20:37.385694Z","shell.execute_reply.started":"2022-03-24T20:20:35.284992Z","shell.execute_reply":"2022-03-24T20:20:37.384937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"1\"></a><br>\n\n# Read Data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/tabular-playground-series-mar-2022/train.csv\")\ntest = pd.read_csv(\"../input/tabular-playground-series-mar-2022/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-03-24T20:20:37.387299Z","iopub.execute_input":"2022-03-24T20:20:37.388016Z","iopub.status.idle":"2022-03-24T20:20:38.144887Z","shell.execute_reply.started":"2022-03-24T20:20:37.387977Z","shell.execute_reply":"2022-03-24T20:20:38.144172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([train, test], axis = 0)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T20:20:38.146155Z","iopub.execute_input":"2022-03-24T20:20:38.146424Z","iopub.status.idle":"2022-03-24T20:20:38.216648Z","shell.execute_reply.started":"2022-03-24T20:20:38.146387Z","shell.execute_reply":"2022-03-24T20:20:38.215903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"2\"></a><br>\n\n# Reduce Memory Usage","metadata":{}},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n\n    for col in df.columns:\n        col_type = df[col].dtypes\n\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n \n    return df\n\ndf = reduce_mem_usage(df)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T20:20:38.218465Z","iopub.execute_input":"2022-03-24T20:20:38.218784Z","iopub.status.idle":"2022-03-24T20:20:38.255903Z","shell.execute_reply.started":"2022-03-24T20:20:38.218739Z","shell.execute_reply":"2022-03-24T20:20:38.25506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"3\"></a><br>\n\n# Getting familiar with Data","metadata":{}},{"cell_type":"markdown","source":"train.csv - the training set, comprising measurements of traffic congestion across 65 roadways from April through September of 1991.\n\nrow_id - a unique identifier for this instance\n\ntime - the 20-minute period in which each measurementwas taken\n\nx - the east-west midpoint coordinate of the roadway\n\ny - the north-south midpoint coordinate of the roadway\n\ndirection - the direction of travel of the roadway. EB indicates \"eastbound\" travel, for example, while SW indicates a \"southwest\" direction of travel.\n\ncongestion - congestion levels for the roadway during each hour; the target. The congestion measurements have been normalized to the range 0 to 100.\n\ntest.csv - the test set; you will make hourly predictions for roadways identified by a coordinate location and a direction of travel on the day of 1991-09-30.\n\nsample_submission.csv - a sample submission file in the correct format","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T20:20:38.414734Z","iopub.execute_input":"2022-03-24T20:20:38.414986Z","iopub.status.idle":"2022-03-24T20:20:38.584928Z","shell.execute_reply.started":"2022-03-24T20:20:38.414958Z","shell.execute_reply":"2022-03-24T20:20:38.584107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### It is shown above, fortunately there aren't any missing data.","metadata":{}},{"cell_type":"markdown","source":"##### Let's check the unique values to get more intuiton about what we have.","metadata":{}},{"cell_type":"code","source":"unique_x = df.x.unique()\nunique_y = df.y.unique()\nunique_direction = df.direction.unique()\n\nprint(\"Unique values of x: \", unique_x)\nprint(\"Unique values of y: \", unique_y)\nprint(\"Unique directions: \", unique_direction)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T20:20:39.684516Z","iopub.execute_input":"2022-03-24T20:20:39.684792Z","iopub.status.idle":"2022-03-24T20:20:39.755744Z","shell.execute_reply.started":"2022-03-24T20:20:39.684761Z","shell.execute_reply":"2022-03-24T20:20:39.754908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### There are 3 unique values for x and 4 unique values for y, 8 different directions.\n##### It is possible to gather them together and have a new feature later on.","metadata":{}},{"cell_type":"code","source":"df[\"congestion\"].describe()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T20:20:53.767671Z","iopub.execute_input":"2022-03-24T20:20:53.767945Z","iopub.status.idle":"2022-03-24T20:20:53.806728Z","shell.execute_reply.started":"2022-03-24T20:20:53.767915Z","shell.execute_reply":"2022-03-24T20:20:53.806037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"4\"></a><br>\n# Simple Exploratary Data Analysis","metadata":{}},{"cell_type":"code","source":"temp = df.copy()\ntemp[\"DateTime\"] = pd.to_datetime(temp[\"time\"])\ntemp = df.set_index(temp[\"DateTime\"])\ntemp['date'] = temp.index\ntemp['weekofyear'] = temp['date'].dt.weekofyear\ntemp['day_of_week'] = temp['date'].dt.dayofweek","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-24T18:45:45.204322Z","iopub.execute_input":"2022-03-24T18:45:45.204581Z","iopub.status.idle":"2022-03-24T18:45:45.78605Z","shell.execute_reply.started":"2022-03-24T18:45:45.204553Z","shell.execute_reply":"2022-03-24T18:45:45.785348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list1 = [\"x\", \"y\", \"direction\", \"congestion\"]\nsns.heatmap(temp[list1].corr(), annot = True, fmt = \".2f\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T18:45:48.442351Z","iopub.execute_input":"2022-03-24T18:45:48.442621Z","iopub.status.idle":"2022-03-24T18:45:48.716324Z","shell.execute_reply.started":"2022-03-24T18:45:48.442573Z","shell.execute_reply":"2022-03-24T18:45:48.715656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Seems data has no correlation in its very raw own.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 2))\n\ndf.groupby(by = [\"x\",\"y\",\"direction\"])[\"congestion\"].mean().plot()\n","metadata":{"execution":{"iopub.status.busy":"2022-03-24T20:52:10.063071Z","iopub.execute_input":"2022-03-24T20:52:10.06378Z","iopub.status.idle":"2022-03-24T20:52:10.344676Z","shell.execute_reply.started":"2022-03-24T20:52:10.063743Z","shell.execute_reply":"2022-03-24T20:52:10.343954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[(df[\"x\"] == 0) & (df[\"y\"] == 0)][\"direction\"].unique()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T20:37:18.686336Z","iopub.execute_input":"2022-03-24T20:37:18.686982Z","iopub.status.idle":"2022-03-24T20:37:18.707904Z","shell.execute_reply.started":"2022-03-24T20:37:18.686938Z","shell.execute_reply":"2022-03-24T20:37:18.707183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.factorplot(x = \"weekofyear\", y = \"congestion\", kind = \"bar\", data  = temp, size = 6)\ng.set_ylabels(\"Survived Probability\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T18:47:07.542445Z","iopub.execute_input":"2022-03-24T18:47:07.543135Z","iopub.status.idle":"2022-03-24T18:47:20.894033Z","shell.execute_reply.started":"2022-03-24T18:47:07.543099Z","shell.execute_reply":"2022-03-24T18:47:20.893386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Week by week, there is not really a specific trend.","metadata":{}},{"cell_type":"code","source":"g = sns.factorplot(x = \"day_of_week\", y = \"congestion\", kind = \"bar\", data  = temp, size = 6)\ng.set_ylabels(\"Congestion\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T18:48:08.493827Z","iopub.execute_input":"2022-03-24T18:48:08.49452Z","iopub.status.idle":"2022-03-24T18:48:20.778265Z","shell.execute_reply.started":"2022-03-24T18:48:08.494486Z","shell.execute_reply":"2022-03-24T18:48:20.777511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For weekdays, congestion has no remarkable trend, there is slightly less congestion at weekends.","metadata":{}},{"cell_type":"code","source":"temp = temp.groupby((temp['date'].dt.hour)).mean()\ntemp[\"hour\"] = range(0,24)\ng = sns.factorplot(x =\"hour\" ,y = \"congestion\", kind = \"bar\", data  = temp, size = 6)\ng.set_ylabels(\"Congestion\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T18:49:46.252621Z","iopub.execute_input":"2022-03-24T18:49:46.253279Z","iopub.status.idle":"2022-03-24T18:49:46.845127Z","shell.execute_reply.started":"2022-03-24T18:49:46.253243Z","shell.execute_reply":"2022-03-24T18:49:46.844462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we have clear trend for a day.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplt.bar( range(0,101),train.congestion.value_counts().sort_index(), width=1, color = \"#093260\")\nplt.ylabel('Frequency')\nplt.xlabel('Congestion')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T18:50:25.194412Z","iopub.execute_input":"2022-03-24T18:50:25.19512Z","iopub.status.idle":"2022-03-24T18:50:25.522119Z","shell.execute_reply.started":"2022-03-24T18:50:25.195082Z","shell.execute_reply":"2022-03-24T18:50:25.521444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"decomposed_results = seasonal_decompose(train[\"congestion\"], period=24)\n\nfig, ax = plt.subplots(figsize=(12, 6))\nax.plot(decomposed_results.trend, alpha=0.6, color='blue', label='Congestion Trend', linewidth = 1.0)\nax.plot(decomposed_results.trend.rolling(4680).mean().shift(-4680), alpha=1, color='black', label='Congestion Trend (Rolling Mean)', linewidth = 2.0)\nax.legend(loc='best')\nax.set_ylabel(\"Congestion Trend\")\nax.set_xlabel(\"Hours\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T18:51:42.450465Z","iopub.execute_input":"2022-03-24T18:51:42.451005Z","iopub.status.idle":"2022-03-24T18:51:47.722227Z","shell.execute_reply.started":"2022-03-24T18:51:42.450964Z","shell.execute_reply":"2022-03-24T18:51:47.721404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is seen above that congestion has no seasonal trend but it has a trend on daily basis.","metadata":{}},{"cell_type":"code","source":"figure, axis = plt.subplots(3, 2, figsize = (16,10) )\n\ndirec_tr = train[\"direction\"]\ndirec_tr_value = direc_tr.value_counts()\n\nx_tr = train[\"x\"]\nx_tr_value = x_tr.value_counts()\n\ny_tr = train[\"y\"]\ny_tr_value = y_tr.value_counts()\n\ndirec_ts = test[\"direction\"]\ndirec_ts_value = direc_ts.value_counts()\n\nx_ts = test[\"x\"]\nx_ts_value = x_ts.value_counts()\n\ny_ts = test[\"y\"]\ny_ts_value = y_ts.value_counts()\n\n\n\n\n\naxis[0, 0].bar(x_tr_value.index, x_tr_value, color = \"#093260\")\naxis[0, 0].set_title(\"Frequency of x directions in train\")\naxis[0, 1].bar(x_ts_value.index, x_ts_value, color = \"#093260\")\naxis[0, 1].set_title(\"Frequency of x directions in test\")\n\naxis[1, 0].bar(y_tr_value.index, y_tr_value, color= \"orange\")\naxis[1, 0].set_title(\"Frequency of y directions in train\")\naxis[1, 1].bar(y_ts_value.index, y_ts_value, color= \"orange\")\naxis[1, 1].set_title(\"Frequency of y directions in test\")\n\naxis[2, 0].bar(direc_tr_value.index, direc_tr_value, color = \"red\")\naxis[2, 0].set_title(\"Frequency of Directions in train\")\naxis[2, 1].bar(direc_ts_value.index, direc_ts_value, color = \"red\")\naxis[2, 1].set_title(\"Frequency of Directions in test\")\n\nplt.show()\n","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-03-24T18:53:02.354202Z","iopub.execute_input":"2022-03-24T18:53:02.354458Z","iopub.status.idle":"2022-03-24T18:53:03.123638Z","shell.execute_reply.started":"2022-03-24T18:53:02.35443Z","shell.execute_reply":"2022-03-24T18:53:03.122846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For a reasonable model, let's check the distrubition of each feature in train and test data. We can say they contains similar distribution about each feature.","metadata":{}},{"cell_type":"markdown","source":"**Inferences**\n\n* The data has gaussian distribution\n* The raw data does not seem to be useful in this way, time features might be effective. As seen in daily congestion figure, hour feature can be used.\n* Mean and median values may be used since there is no real trend in weeks.","metadata":{}},{"cell_type":"markdown","source":"<a id = \"5\"></a><br>\n\n# Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"<a id = \"6\"></a><br>\n\n## Time features","metadata":{}},{"cell_type":"code","source":"df[\"DateTime\"] = pd.to_datetime(df[\"time\"])\ndf = df.set_index(df[\"DateTime\"])\ndf['date'] = df.index\ndayofyear = df['date'].dt.dayofyear\ndf['hour'] = df['date'].dt.hour\ndf['day_of_week'] = df['date'].dt.dayofweek\ndf['quarter'] = df['date'].dt.quarter\ndf['year'] = df['date'].dt.year\ndf['month'] = df['date'].dt.month\ndf['dayofyear'] = df['date'].dt.dayofyear\ndf['dayofmonth'] = df['date'].dt.day\ndf['weekofyear'] = df['date'].dt.weekofyear\ndf['minute'] = df[\"date\"].dt.minute\ndf['afternoon'] = df['hour'] >= 12\ndf['moment']  = df['date'].dt.hour * 3 + df['date'].dt.minute // 20   \nis_weekend = np.where(df[\"day_of_week\"]>5,1,0)\ndf[\"is_weekend\"] = is_weekend","metadata":{"execution":{"iopub.status.busy":"2022-03-24T12:47:29.831076Z","iopub.execute_input":"2022-03-24T12:47:29.831558Z","iopub.status.idle":"2022-03-24T12:47:31.043094Z","shell.execute_reply.started":"2022-03-24T12:47:29.831521Z","shell.execute_reply":"2022-03-24T12:47:31.042241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"7\"></a><br>\n\n## Mean and Median Values","metadata":{}},{"cell_type":"code","source":"df[\"roadway\"] = df.x.astype(str) + df.y.astype(str) + df.direction.astype(str)\nkeys = [\"day_of_week\",\"hour\", \"minute\",\"roadway\" ]\n\ntemp = df.groupby(by=keys).mean().reset_index().set_index(keys)\ntemp['mean congestion'] = temp['congestion']\ndf = df.merge(temp['mean congestion'], how='left', left_on=keys, right_on=keys)\n\ntemp = df.groupby(by=keys).median().reset_index().set_index(keys)\ntemp['median congestion'] = temp['congestion']\ndf = df.merge(temp['median congestion'], how='left', left_on=keys, right_on=keys)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-24T12:47:31.044701Z","iopub.execute_input":"2022-03-24T12:47:31.044949Z","iopub.status.idle":"2022-03-24T12:47:34.59501Z","shell.execute_reply.started":"2022-03-24T12:47:31.044901Z","shell.execute_reply":"2022-03-24T12:47:34.594266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"8\"></a><br>\n## Min and Max Congestion","metadata":{}},{"cell_type":"code","source":"temp = df.groupby(by=keys).min().reset_index().set_index(keys)\ntemp['min congestion'] = temp['congestion']\ndf = df.merge(temp['min congestion'], how='left', left_on=keys, right_on=keys)\n\ntemp = df.groupby(by=keys).max().reset_index().set_index(keys)\ntemp['max congestion'] = temp['congestion']\ndf = df.merge(temp['max congestion'], how='left', left_on=keys, right_on=keys)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-24T12:47:34.596749Z","iopub.execute_input":"2022-03-24T12:47:34.59699Z","iopub.status.idle":"2022-03-24T12:47:47.405262Z","shell.execute_reply.started":"2022-03-24T12:47:34.596958Z","shell.execute_reply":"2022-03-24T12:47:47.404521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"9\"></a><br>\n## Morning Averages","metadata":{}},{"cell_type":"code","source":"df_mornings = df[(df.hour >= 6) & (df.hour < 12)]\nmorning_avgs = pd.DataFrame(df_mornings.groupby(['month', 'dayofmonth', 'roadway']).congestion.median().astype(int)).reset_index()\nmorning_avgs = morning_avgs.rename(columns={'congestion':'morning_avg'})\ndf = df.merge(morning_avgs, on=['month', 'dayofmonth', 'roadway'], how='left')","metadata":{"execution":{"iopub.status.busy":"2022-03-24T12:47:47.406587Z","iopub.execute_input":"2022-03-24T12:47:47.406819Z","iopub.status.idle":"2022-03-24T12:47:48.003247Z","shell.execute_reply.started":"2022-03-24T12:47:47.406787Z","shell.execute_reply":"2022-03-24T12:47:48.002474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"10\"></a><br>\n## Lag Features","metadata":{}},{"cell_type":"code","source":"for delta in range(1,8):\n    day = df.copy()\n    day['date'] = day['date'] + pd.Timedelta(delta, unit=\"d\")\n    name = f'lag_{delta}'\n    day = day.rename(columns={'congestion':name})[['date', 'roadway', name]]\n    df = df.merge(day, on=['date', 'roadway'], how='left')\ndf=df.fillna(df[\"congestion\"].median())","metadata":{"execution":{"iopub.status.busy":"2022-03-24T12:47:48.005529Z","iopub.execute_input":"2022-03-24T12:47:48.005831Z","iopub.status.idle":"2022-03-24T12:48:05.64738Z","shell.execute_reply.started":"2022-03-24T12:47:48.005791Z","shell.execute_reply":"2022-03-24T12:48:05.646628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"12\"></a><br>\n\n# Outliers Detection","metadata":{}},{"cell_type":"code","source":"def detect_outliers(df, features):\n    outlier_indices = []\n    \n    for c in features:\n        \n        # 1st quartile\n        q1 = np.percentile(df[c], 25)\n        \n        # 3rd quartile\n        q3 = np.percentile(df[c], 75)\n        \n        # IQR\n        iqr = q3 - q1\n        \n        # Outlier step\n        \n        outlier_step = iqr*1.5\n        \n        # detect outlier and their indeces\n        outlier_list_col = df[(df[c] < q1 - outlier_step) | (df[c] > q1 + outlier_step)].index\n        \n        # store indeces\n        \n        outlier_indices.extend(outlier_list_col)\n    \n    \n    return outlier_indices","metadata":{"execution":{"iopub.status.busy":"2022-03-24T12:48:06.140243Z","iopub.execute_input":"2022-03-24T12:48:06.142259Z","iopub.status.idle":"2022-03-24T12:48:06.149882Z","shell.execute_reply.started":"2022-03-24T12:48:06.142215Z","shell.execute_reply":"2022-03-24T12:48:06.149191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outlier_indices = detect_outliers(df, [\"congestion\"])","metadata":{"execution":{"iopub.status.busy":"2022-03-24T12:48:06.151387Z","iopub.execute_input":"2022-03-24T12:48:06.151918Z","iopub.status.idle":"2022-03-24T12:48:06.257972Z","shell.execute_reply.started":"2022-03-24T12:48:06.15188Z","shell.execute_reply":"2022-03-24T12:48:06.257307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print((len(outlier_indices)/len(train))*100)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The ratio of outliers to train data is **7.5%**. This is something that should be considered. Replacing outliers with median may work.","metadata":{}},{"cell_type":"code","source":"for idx in outlier_indices:\n    df[\"congestion\"][idx] = df[\"median congestion\"][idx]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"11\"></a><br>\n## Label Encoding","metadata":{}},{"cell_type":"code","source":"le = LabelEncoder()\ndf['roadway'] = le.fit_transform(df['roadway'])\ndf['afternoon'] = le.fit_transform(df['afternoon'])\ndf['direction'] = le.fit_transform(df['direction'])\n","metadata":{"execution":{"iopub.status.busy":"2022-03-24T12:48:05.648609Z","iopub.execute_input":"2022-03-24T12:48:05.648844Z","iopub.status.idle":"2022-03-24T12:48:06.139031Z","shell.execute_reply.started":"2022-03-24T12:48:05.648813Z","shell.execute_reply":"2022-03-24T12:48:06.138273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"13\"></a><br>\n\n# Modelling with CATBOOST","metadata":{}},{"cell_type":"code","source":"x_train = df[:len(train)]\ny_train = x_train[\"congestion\"]\nx_test = df[len(train):]","metadata":{"execution":{"iopub.status.busy":"2022-03-24T12:48:06.259308Z","iopub.execute_input":"2022-03-24T12:48:06.259555Z","iopub.status.idle":"2022-03-24T12:48:06.264617Z","shell.execute_reply.started":"2022-03-24T12:48:06.259522Z","shell.execute_reply":"2022-03-24T12:48:06.263835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = [\"time\",\"DateTime\",\"congestion\",\"date\",\"day_of_week\",\"quarter\",\"month\",\"weekofyear\",\"dayofmonth\",\"morning_avg\"]\nx_train.drop(features, 1, inplace = True)\nx_test.drop(features, 1, inplace = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = CatBoostRegressor(\n    verbose=1000,\n    early_stopping_rounds=10,\n    random_state = 2022, learning_rate = 0.01, bagging_temperature = 0.02, max_depth = 16, \n    random_strength = 47, l2_leaf_reg = 7.459775961819184e-06, min_child_samples = 49, max_bin = 320, od_type = 'Iter', \n    task_type = 'GPU', loss_function = 'MAE', eval_metric = 'MAE'\n).fit(x_train, y_train)   ","metadata":{"execution":{"iopub.status.busy":"2022-03-24T12:51:29.320997Z","iopub.execute_input":"2022-03-24T12:51:29.321648Z","iopub.status.idle":"2022-03-24T12:52:26.083708Z","shell.execute_reply.started":"2022-03-24T12:51:29.32161Z","shell.execute_reply":"2022-03-24T12:52:26.082979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction = model.predict(x_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:00:23.471139Z","iopub.execute_input":"2022-03-24T13:00:23.471705Z","iopub.status.idle":"2022-03-24T13:00:23.497849Z","shell.execute_reply.started":"2022-03-24T13:00:23.471666Z","shell.execute_reply":"2022-03-24T13:00:23.497235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(pd.Series(model.get_feature_importance(), index=x_train.columns)\n   .nlargest(20)\n   .plot(kind='barh'))","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:00:24.463722Z","iopub.execute_input":"2022-03-24T13:00:24.463975Z","iopub.status.idle":"2022-03-24T13:00:25.67665Z","shell.execute_reply.started":"2022-03-24T13:00:24.463947Z","shell.execute_reply":"2022-03-24T13:00:25.675974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"14\"></a><br>\n\n# SUBMISSION","metadata":{}},{"cell_type":"code","source":"submission = pd.read_csv(\"../input/tabular-playground-series-mar-2022/sample_submission.csv\")\nsubmission[\"congestion\"] = prediction\nsubmission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\n\n# Read and prepare the training data\ntrain = pd.read_csv('../input/tabular-playground-series-mar-2022/train.csv', parse_dates=['time'])\ntrain['hour'] = train['time'].dt.hour\ntrain['minute'] = train['time'].dt.minute\n\nsubmission_in = submission.copy()\n# Compute the quantiles of workday afternoons in September except Labor Day\nsep = train[(train.time.dt.hour >= 12) & (train.time.dt.weekday < 5) &\n            (train.time.dt.dayofyear >= 246)]\nlower = sep.groupby(['hour', 'minute', 'x', 'y', 'direction']).congestion.quantile(0.2).values\nupper = sep.groupby(['hour', 'minute', 'x', 'y', 'direction']).congestion.quantile(0.7).values\n\n# Clip the submission data to the quantiles\nsubmission_out = submission_in.copy()\nsubmission_out['congestion'] = submission_in.congestion.clip(lower, upper)\n\n# Display some statistics\nmae = mean_absolute_error(submission_in.congestion, submission_out.congestion)\nprint(f'Mean absolute modification: {mae:.4f}')\nprint(f\"Submission was below lower bound: {(submission_in.congestion <= lower - 0.5).sum()}\")\nprint(f\"Submission was above upper bound: {(submission_in.congestion > upper + 0.5).sum()}\")\n\n# Round the submission\nsubmission_out['congestion'] = submission_out.congestion.round().astype(int)\nsubmission_out.to_csv('submission.csv', index = False)\nsubmission_out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}