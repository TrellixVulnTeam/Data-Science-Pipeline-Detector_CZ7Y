{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ninputdir = \"/kaggle/input/tabular-playground-series-mar-2022/\"\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-27T09:28:52.708648Z","iopub.execute_input":"2022-03-27T09:28:52.709031Z","iopub.status.idle":"2022-03-27T09:28:52.72121Z","shell.execute_reply.started":"2022-03-27T09:28:52.708989Z","shell.execute_reply":"2022-03-27T09:28:52.720087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Helper for longer listing of series or dataframes\ndef longdisp(x):\n    with pd.option_context(\"display.max_rows\", 999):\n        print(type(x))\n        print(len(x))\n        display(x)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T09:30:58.810186Z","iopub.execute_input":"2022-03-27T09:30:58.811086Z","iopub.status.idle":"2022-03-27T09:30:58.818271Z","shell.execute_reply.started":"2022-03-27T09:30:58.81103Z","shell.execute_reply":"2022-03-27T09:30:58.817272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(inputdir + \"train.csv\", index_col=0)\ndata","metadata":{"execution":{"iopub.status.busy":"2022-03-27T09:32:48.737548Z","iopub.execute_input":"2022-03-27T09:32:48.738603Z","iopub.status.idle":"2022-03-27T09:32:49.39083Z","shell.execute_reply.started":"2022-03-27T09:32:48.738531Z","shell.execute_reply":"2022-03-27T09:32:49.390166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reshaping the data\nI want to see the congestion as a timeseries for each spot/highway (x,y,direction).\nFor simplicity I name each spot by their x,y,direction values.\nSince I don't know if values exist for all timestamps I also add a new time index with entries for all 20 minute intervals.\n","metadata":{}},{"cell_type":"code","source":"data[\"dt\"] = pd.to_datetime(data[\"time\"])\ndata[\"spot\"] = data.apply(lambda r: str(r.x) + str(r.y) + r.direction, axis=1)\ndata","metadata":{"execution":{"iopub.status.busy":"2022-03-27T09:33:01.539349Z","iopub.execute_input":"2022-03-27T09:33:01.54033Z","iopub.status.idle":"2022-03-27T09:33:38.922774Z","shell.execute_reply.started":"2022-03-27T09:33:01.54025Z","shell.execute_reply":"2022-03-27T09:33:38.922052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_pv = data.pivot(index=\"dt\", columns=\"spot\", values=\"congestion\")\ndata_pv","metadata":{"execution":{"iopub.status.busy":"2022-03-27T09:33:55.100638Z","iopub.execute_input":"2022-03-27T09:33:55.101273Z","iopub.status.idle":"2022-03-27T09:33:55.374816Z","shell.execute_reply.started":"2022-03-27T09:33:55.101216Z","shell.execute_reply":"2022-03-27T09:33:55.374173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time_idx = pd.date_range(\"1991-04-01 00:00:00\",\"1991-09-30 11:40:00\", name=\"time\", freq=\"20min\")\ntime_idx","metadata":{"execution":{"iopub.status.busy":"2022-03-27T09:34:04.33632Z","iopub.execute_input":"2022-03-27T09:34:04.336896Z","iopub.status.idle":"2022-03-27T09:34:04.34709Z","shell.execute_reply.started":"2022-03-27T09:34:04.336828Z","shell.execute_reply":"2022-03-27T09:34:04.34621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spot_ts = data_pv.reindex(index = time_idx)\nspot_ts","metadata":{"execution":{"iopub.status.busy":"2022-03-27T09:34:09.989159Z","iopub.execute_input":"2022-03-27T09:34:09.990239Z","iopub.status.idle":"2022-03-27T09:34:10.047359Z","shell.execute_reply.started":"2022-03-27T09:34:09.990172Z","shell.execute_reply":"2022-03-27T09:34:10.04647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Missing values","metadata":{}},{"cell_type":"code","source":"missing = spot_ts.isna()\nmissing_count = 65 - spot_ts.count(axis=1)\nmissing.sum().sum()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T09:35:01.315556Z","iopub.execute_input":"2022-03-27T09:35:01.315927Z","iopub.status.idle":"2022-03-27T09:35:01.338245Z","shell.execute_reply.started":"2022-03-27T09:35:01.315892Z","shell.execute_reply":"2022-03-27T09:35:01.337517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"longdisp(missing_count[missing_count > 0])\n","metadata":{"execution":{"iopub.status.busy":"2022-03-27T09:35:07.053811Z","iopub.execute_input":"2022-03-27T09:35:07.054381Z","iopub.status.idle":"2022-03-27T09:35:07.068153Z","shell.execute_reply.started":"2022-03-27T09:35:07.054292Z","shell.execute_reply":"2022-03-27T09:35:07.067363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"longdisp(missing_count[(missing_count > 0) & (missing_count < 65)])","metadata":{"execution":{"iopub.status.busy":"2022-03-27T09:35:15.107725Z","iopub.execute_input":"2022-03-27T09:35:15.10891Z","iopub.status.idle":"2022-03-27T09:35:15.120718Z","shell.execute_reply.started":"2022-03-27T09:35:15.108854Z","shell.execute_reply":"2022-03-27T09:35:15.119675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data is missing for 81 times for all locations with no apparent pattern, maybe simply measurement downtime?  \nFilling with previous values to have a complete time series.","metadata":{}},{"cell_type":"code","source":"spot_ts.fillna(method=\"ffill\", axis=0, inplace=True)\nspot_ts.isna().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T09:35:47.07732Z","iopub.execute_input":"2022-03-27T09:35:47.077648Z","iopub.status.idle":"2022-03-27T09:35:47.101178Z","shell.execute_reply.started":"2022-03-27T09:35:47.077614Z","shell.execute_reply":"2022-03-27T09:35:47.100367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Frequency analysis\n\nI assume there is a daily and weekly periodicity in the data, doing a fft to confirm that and maybe find other patterns.  \nMeasuring frequency in cycles per day. Removing the mean before transforming to avoid a huge constant component.","metadata":{}},{"cell_type":"code","source":"spot_ts_zero_mean = spot_ts - spot_ts.mean()\nspectra = np.fft.rfft(spot_ts_zero_mean.to_numpy(), axis=0)\nprint(type(spectra))\nprint(spectra.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T09:37:37.982552Z","iopub.execute_input":"2022-03-27T09:37:37.983025Z","iopub.status.idle":"2022-03-27T09:37:38.016696Z","shell.execute_reply.started":"2022-03-27T09:37:37.982962Z","shell.execute_reply":"2022-03-27T09:37:38.015776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Interpreting sample interval and frequencies\n- The time series length is 13140 with sample interval 20 minutes\n- A full FFT calculates 13140 values with base frequency 1/13140 measured in cycles/sample interval, RFFT keeps only first half because of conjugate symmetry.\n- I prefer cycles/day instead of cycles/20 min, using rfftfreq to create a frequency index based on converting 20 minutes to fraction of a day","metadata":{}},{"cell_type":"code","source":"samples_per_day = 24*3\nfreq_idx = np.fft.rfftfreq(len(time_idx), d=1.0/samples_per_day)\nspot_freq = pd.DataFrame(spectra, index=freq_idx, columns=spot_ts.columns)\nspot_freq","metadata":{"execution":{"iopub.status.busy":"2022-03-27T10:00:22.59897Z","iopub.execute_input":"2022-03-27T10:00:22.601018Z","iopub.status.idle":"2022-03-27T10:00:22.696475Z","shell.execute_reply.started":"2022-03-27T10:00:22.600914Z","shell.execute_reply":"2022-03-27T10:00:22.695567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualizing the frequency spectra\nIt would be nice to compare the spectra for different locations, but 65 graphs in one diagram seems too messy. Instead I look only at the amplitude spectrum averaged over all locations.  \nLooking in particular for variations with frequencies that are multiples of 1 (daily), and 1/7 (weekly).","metadata":{}},{"cell_type":"code","source":"# Taking the average of the spectrum over all locations to get an overview\nspot_freq_mean = np.abs(spot_freq).mean(axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T10:16:27.295574Z","iopub.execute_input":"2022-03-27T10:16:27.295987Z","iopub.status.idle":"2022-03-27T10:16:27.316336Z","shell.execute_reply.started":"2022-03-27T10:16:27.295937Z","shell.execute_reply":"2022-03-27T10:16:27.315529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,ax = plt.subplots()\nax.set_xlabel(\"Frequency (cycles/day)\")\nax.set_ylabel(\"Average amplitude spectrum\")\nax.plot(freq_idx, spot_freq_mean)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T10:17:03.016524Z","iopub.execute_input":"2022-03-27T10:17:03.016901Z","iopub.status.idle":"2022-03-27T10:17:03.306061Z","shell.execute_reply.started":"2022-03-27T10:17:03.016857Z","shell.execute_reply":"2022-03-27T10:17:03.305053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Focusing on the low frequencies\nlowfreq_idx = freq_idx[freq_idx < 5]\nfig,ax = plt.subplots()\nax.set_xlabel(\"Frequency (cycles/day)\")\nax.set_ylabel(\"Average amplitude spectrum\")\nax.plot(lowfreq_idx, spot_freq_mean.iloc[0:len(lowfreq_idx)])","metadata":{"execution":{"iopub.status.busy":"2022-03-27T10:17:24.225553Z","iopub.execute_input":"2022-03-27T10:17:24.226048Z","iopub.status.idle":"2022-03-27T10:17:24.449894Z","shell.execute_reply.started":"2022-03-27T10:17:24.226007Z","shell.execute_reply":"2022-03-27T10:17:24.448907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Focusing on the low frequencies\nlowfreq_idx = freq_idx[freq_idx < 0.5]\nfig,ax = plt.subplots()\nax.set_xlabel(\"Frequency (cycles/day)\")\nax.set_ylabel(\"Average amplitude spectrum\")\nax.plot(lowfreq_idx, spot_freq_mean.iloc[0:len(lowfreq_idx)])","metadata":{"execution":{"iopub.status.busy":"2022-03-27T10:17:34.045376Z","iopub.execute_input":"2022-03-27T10:17:34.045762Z","iopub.status.idle":"2022-03-27T10:17:34.278655Z","shell.execute_reply.started":"2022-03-27T10:17:34.045714Z","shell.execute_reply":"2022-03-27T10:17:34.277736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusions\nThe daily cycle is clearly visible, as well as a weekly component at 1/7 (0.14).","metadata":{}},{"cell_type":"markdown","source":"# Reconstruction and prediction\nTo get predictions for the second half of 9/30/1991, I reconstruct a time series based on only the low frequency components.\n- Take away the higher frequencies above 18 (1h 20min cycles). This is quite arbitrarily chosen to get some smoothing without losing to much detail. This should also remove some aliasing effects.\n- Reconstruct a time series with the inverse FFT.","metadata":{}},{"cell_type":"code","source":"# Creating new fft with only low frequency components\nlpcount = np.sum(freq_idx < 18)\nspectra_lp = np.zeros_like(spectra)\nspectra_lp[0:lpcount] = spectra[0:lpcount]\n\nreconstruct = np.fft.irfft(spectra_lp, axis=0)\nprint(type(reconstruct))\nprint(reconstruct.shape)\n\nspot_ts_lp = pd.DataFrame(reconstruct, index=time_idx, columns=spot_ts.columns)\n\n# Adding mean value again since it was removed before the FFT\nspot_ts_lp = spot_ts_lp + spot_ts.mean()\nspot_ts_lp","metadata":{"execution":{"iopub.status.busy":"2022-03-27T10:59:26.163496Z","iopub.execute_input":"2022-03-27T10:59:26.164861Z","iopub.status.idle":"2022-03-27T10:59:26.374894Z","shell.execute_reply.started":"2022-03-27T10:59:26.16476Z","shell.execute_reply":"2022-03-27T10:59:26.374002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For predictions I use the reconstructed time series values from the same time interval one week earlier, rounded to integers.","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv(inputdir + \"test.csv\", index_col=0)\ntest[\"dt\"] = pd.to_datetime(test[\"time\"])\ntest[\"spot\"] = test.apply(lambda r: str(r.x) + str(r.y) + r.direction, axis=1)\ntest[\"previousweek\"] = test[\"dt\"] - pd.Timedelta(days=7)\ntest[\"prediction\"] = test.apply(lambda r: round(spot_ts_lp.loc[r[\"previousweek\"],r[\"spot\"]]), axis=1)\ntest[\"prediction\"] = test[\"prediction\"].astype(\"int32\")\ntest","metadata":{"execution":{"iopub.status.busy":"2022-03-27T11:04:28.400903Z","iopub.execute_input":"2022-03-27T11:04:28.402118Z","iopub.status.idle":"2022-03-27T11:04:28.670556Z","shell.execute_reply.started":"2022-03-27T11:04:28.402044Z","shell.execute_reply":"2022-03-27T11:04:28.669587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.to_csv(\"submission_fft_1.csv\", header=[\"congestion\"], columns=[\"prediction\"])","metadata":{"execution":{"iopub.status.busy":"2022-03-27T11:04:51.142656Z","iopub.execute_input":"2022-03-27T11:04:51.143029Z","iopub.status.idle":"2022-03-27T11:04:51.159356Z","shell.execute_reply.started":"2022-03-27T11:04:51.142991Z","shell.execute_reply":"2022-03-27T11:04:51.158271Z"},"trusted":true},"execution_count":null,"outputs":[]}]}