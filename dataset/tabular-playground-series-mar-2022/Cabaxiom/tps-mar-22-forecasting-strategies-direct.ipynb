{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Forecasting Strategies","metadata":{}},{"cell_type":"markdown","source":"In this notebook I try out some different forecasting techniques:\n\n- A direct forecasting strategy, where we make a 32-step (12 hour) forecast from multiple forecasting origins on the day of the test set. A different regressor is trained for each forecasting step. For example 1 regressors is trained to predict 1 step ahead, another regressor is trained to forecast 2 steps ahead etc.. I decide that this approach is probably not necessary for this competition, but it shows some promise.\n- A direct recursive forecasting strategy. A direct approach as before, but with forecasted congestion predictions from previous steps used as additional lag features for the model of the current step. I show that a recursive strategy will likely not work well in this competition, as the scores for steps above 1 or 2 suffer from error propagation. \n\n\nBefore you read:\n\n- This is my first time implementing these techniques.I would probably do it slightly different if implementing again.\n- The score for this notebook is just the normal forecasting technique; 0-step ahead forecast.\n- Model parameters chosen at random\n\nReferences:\n- These techniques are based on the Kaggle Time Series Forecasting Course https://www.kaggle.com/learn/time-series\n- Clipping the final predictions to be in some range: https://www.kaggle.com/code/ambrosm/tpsmar22-generalizing-the-special-values","metadata":{}},{"cell_type":"markdown","source":"Additional thoughts on the competiton:\n\n- I implemented a SARIMAX model in this competiton. I do not believe they are necessary, they are slow and a little tricky to work with. Additionally seasonal, autoregressive (lag), moving averages and even exogenous features (e.g. similar roadways) can instead be added as features for more normal machine learning techniques such as GBDTs. See this comment I found on the time series Kaggle course https://www.kaggle.com/learn/time-series/discussion/307318\n\n- I do not believe hybrid regressors are necessary for this competition. One of the main advantages of hybrid regressors is they can extrapolate trends past the training data, where one (less powerful model) fits the trend and another (more powerful model) e.g GBDTs fits the residuals. However as we are only predicting for 12 hours ahead there is very little extrapolating required, so hybrid models don't seem necessary. That being said I could be wrong, indeed [MARTYNOV ANDREY](https://www.kaggle.com/martynovandrey) has had great results with them.","metadata":{}},{"cell_type":"markdown","source":"Improvements to be made:\n- 1 day of validation is likely not enough.\n- Add lag and moving average features. The ability to  use these features in one of the main advantages of direct forecasting.\n- For the direct strategy rather than forecasting 36 steps ahead from every possible origin on the test day I probably should have just used forecasting origins from the morning and just used predicted afternoon congesion values.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_absolute_error\n\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\n\nfrom lightgbm import LGBMRegressor","metadata":{"execution":{"iopub.status.busy":"2022-03-26T14:15:51.178841Z","iopub.execute_input":"2022-03-26T14:15:51.180263Z","iopub.status.idle":"2022-03-26T14:15:51.363947Z","shell.execute_reply.started":"2022-03-26T14:15:51.180196Z","shell.execute_reply":"2022-03-26T14:15:51.362745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/tabular-playground-series-mar-2022/train.csv\", index_col='row_id', parse_dates=['time'])\ntest_df = pd.read_csv(\"/kaggle/input/tabular-playground-series-mar-2022/test.csv\", index_col='row_id', parse_dates=['time'])","metadata":{"execution":{"iopub.status.busy":"2022-03-26T13:26:43.34313Z","iopub.execute_input":"2022-03-26T13:26:43.343981Z","iopub.status.idle":"2022-03-26T13:26:44.130978Z","shell.execute_reply.started":"2022-03-26T13:26:43.343931Z","shell.execute_reply":"2022-03-26T13:26:44.129939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[\"roadway\"] = train_df[\"x\"].astype(str) + train_df[\"y\"].astype(str) + train_df[\"direction\"]\ntest_df[\"roadway\"] = test_df[\"x\"].astype(str) + test_df[\"y\"].astype(str) + test_df[\"direction\"]\ntrain_df.drop(columns=[\"x\",\"y\",\"direction\"], inplace=True)\ntest_df.drop(columns=[\"x\",\"y\",\"direction\"], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T13:26:44.134246Z","iopub.execute_input":"2022-03-26T13:26:44.134555Z","iopub.status.idle":"2022-03-26T13:26:46.367684Z","shell.execute_reply.started":"2022-03-26T13:26:44.134524Z","shell.execute_reply":"2022-03-26T13:26:46.366723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are no missing (nan) values, however there are missing times; dealing with these:","metadata":{}},{"cell_type":"code","source":"def fill_missing_times():\n    \"\"\"\n    Filling times is a little more complicated as times are not unique (multiple matching time entries, one for each roadway)\n    \n    We fill missing values with the median for each timeofday,dayofweek and roadway\n    \n    There are no missing values on Mondays, so providing we do not use non-mondays as validation we will not introduce data leakage from median calculations\n    \"\"\"\n    #Introduce the missing times\n    print(\"Total times (orginal):\", len(train_df[\"time\"].unique()))\n    new_df = train_df.set_index(\"time\").pivot(columns=\"roadway\").asfreq('20Min').stack(dropna=False).reset_index()\n    print(\"Total times (new):\", len(new_df[\"time\"].unique()))\n    print(\"Missing values to fill:\",new_df.isnull().sum().sum())\n    \n    #Create temportary features required for median calculations\n    new_df[\"minute\"] = new_df['time'].dt.hour * 60 + new_df['time'].dt.minute\n    new_df[\"dayofweek\"] = new_df['time'].dt.dayofweek\n    \n    #fill missing values with median\n    fill_df = new_df.groupby([\"minute\",\"roadway\",\"dayofweek\"])[\"congestion\"].median().rename(\"median_congestion\").reset_index()\n    new_df = new_df.merge(fill_df, on=[\"minute\", \"roadway\",\"dayofweek\"])\n    new_df = new_df.fillna({\"congestion\": new_df[\"median_congestion\"]})\n    print(\"Missing values remaining:\",new_df.isnull().sum().sum())\n    \n    # return train_df to orginal format\n    new_df = new_df.drop(columns=[\"minute\",\"dayofweek\",\"median_congestion\"]) \n    new_df = new_df.sort_values([\"time\",\"roadway\"]).reset_index(drop=True)\n    return new_df\n","metadata":{"execution":{"iopub.status.busy":"2022-03-26T13:26:46.370237Z","iopub.execute_input":"2022-03-26T13:26:46.370541Z","iopub.status.idle":"2022-03-26T13:26:46.382077Z","shell.execute_reply.started":"2022-03-26T13:26:46.370511Z","shell.execute_reply":"2022-03-26T13:26:46.381086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = fill_missing_times()\ndisplay(train_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T13:26:46.383945Z","iopub.execute_input":"2022-03-26T13:26:46.384189Z","iopub.status.idle":"2022-03-26T13:26:48.092417Z","shell.execute_reply.started":"2022-03-26T13:26:46.384161Z","shell.execute_reply":"2022-03-26T13:26:48.091473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_features(df):\n    new_df = df.copy()\n    new_df['minutes'] = df['time'].dt.hour * 60 + df['time'].dt.minute\n    new_df['dayofweek'] = df['time'].dt.dayofweek.astype(\"category\")\n    new_df['weekend'] = (df['time'].dt.dayofweek >= 5).astype(int).astype(\"category\")\n    \n    \n    new_df['date'] = df['time'].dt.date # Drop later, just makes processing a bit easier\n    \n    return new_df","metadata":{"execution":{"iopub.status.busy":"2022-03-26T13:26:48.094126Z","iopub.execute_input":"2022-03-26T13:26:48.094367Z","iopub.status.idle":"2022-03-26T13:26:48.100911Z","shell.execute_reply.started":"2022-03-26T13:26:48.094338Z","shell.execute_reply":"2022-03-26T13:26:48.100081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# These features don't work well with multioutput forecasting\ndef descriptive_features(df, limit_date = datetime.date(1991, 9, 30)):\n    \"\"\" Set limit_date to avoid data leakage, we only calculate descriptive statistics on congestion values BEFORE the limit date\n    \n    Example: limit_date of datetime.date(1991, 9, 23) will calculate median on all times before 1991,9,23 12:00\n    \n    default = all available times\n    \"\"\"\n    new_df = df.copy()\n    \n    limit_df = train_df_2[(train_df_2.date < limit_date) | ( (train_df_2.date == limit_date) & (train_df_2.minutes < 720))]\n    print(\"Descriptive features added using calculations done on all times before and including:\",limit_df.time.max())\n    \n    median_df = limit_df.groupby([\"minutes\",\"roadway\",\"dayofweek\"])[\"congestion\"].median().rename(\"median_congestion\").reset_index()\n    new_df = new_df.merge(median_df, on=[\"minutes\", \"roadway\",\"dayofweek\"])\n    \n    mean_df = limit_df.groupby([\"minutes\",\"roadway\",\"dayofweek\"])[\"congestion\"].mean().rename(\"mean_congestion\").reset_index()\n    new_df = new_df.merge(mean_df, on=[\"minutes\", \"roadway\",\"dayofweek\"])\n    \n    stddev_df = limit_df.groupby([\"minutes\",\"roadway\",\"dayofweek\"])[\"congestion\"].std().rename(\"std_congestion\").reset_index()\n    new_df = new_df.merge(stddev_df, on=[\"minutes\", \"roadway\",\"dayofweek\"])\n    \n    #Restore original order\n    new_df = new_df.sort_values([\"time\",\"roadway\"]).reset_index(drop=True)\n\n    return new_df","metadata":{"execution":{"iopub.status.busy":"2022-03-26T13:26:48.101911Z","iopub.execute_input":"2022-03-26T13:26:48.102679Z","iopub.status.idle":"2022-03-26T13:26:48.116964Z","shell.execute_reply.started":"2022-03-26T13:26:48.102641Z","shell.execute_reply":"2022-03-26T13:26:48.116081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_cyclic(df, plot=True):\n    new_df = df.copy()\n    \n    new_df[\"minutes_sin\"] = np.sin(new_df['minutes'] * (2 * np.pi / 1440)) # There's 1440 minutes in a day\n    new_df[\"minutes_cos\"] = np.cos(new_df['minutes'] * (2 * np.pi / 1440))\n    #new_df = new_df.drop(columns=[\"minutes\"])\n    if plot == True:\n        f,ax = plt.subplots(figsize=(20,5))\n        plt.subplot(1,3,1)\n        sns.scatterplot(data = new_df.sample(1000), x=\"minutes_sin\", y=\"minutes_cos\")\n        plt.subplot(1,3,2)\n        sns.lineplot(data = new_df.sample(1000), x=\"minutes\", y=\"minutes_sin\")\n        plt.subplot(1,3,3)\n        sns.lineplot(data = new_df.sample(1000), x=\"minutes\", y=\"minutes_cos\")\n    \n    return new_df","metadata":{"execution":{"iopub.status.busy":"2022-03-26T13:26:48.11846Z","iopub.execute_input":"2022-03-26T13:26:48.11916Z","iopub.status.idle":"2022-03-26T13:26:48.137285Z","shell.execute_reply.started":"2022-03-26T13:26:48.119115Z","shell.execute_reply":"2022-03-26T13:26:48.136241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_full = train_df.congestion\ny_pivot = train_df.set_index(\"time\").pivot(columns = \"roadway\")\ndisplay(y_full.head(2))\ndisplay(y_pivot.head(2))","metadata":{"execution":{"iopub.status.busy":"2022-03-26T13:26:48.139076Z","iopub.execute_input":"2022-03-26T13:26:48.139405Z","iopub.status.idle":"2022-03-26T13:26:48.446682Z","shell.execute_reply.started":"2022-03-26T13:26:48.139336Z","shell.execute_reply":"2022-03-26T13:26:48.445715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df_2 = add_features(train_df)\ntrain_df_2 = descriptive_features(train_df_2, limit_date = datetime.date(1991, 9, 23))\ntrain_df_2 = make_cyclic(train_df_2)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T13:26:48.450436Z","iopub.execute_input":"2022-03-26T13:26:48.450768Z","iopub.status.idle":"2022-03-26T13:26:57.10919Z","shell.execute_reply.started":"2022-03-26T13:26:48.450726Z","shell.execute_reply":"2022-03-26T13:26:57.108182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = train_df_2[(train_df_2.date < datetime.date(1991, 9, 23)) | ((train_df_2.date == datetime.date(1991, 9, 23)) & (train_df_2[\"minutes\"] < 12*60))] # I dont want to use future values in the training data\nX_train = X_train.drop(columns=[\"date\",\"congestion\",\"minutes\"])\ny_train = y_full.loc[y_full.index.isin(X_train.index)]\nX_train = X_train.set_index(\"time\")\n\n\nX_train_pivot = X_train.pivot(columns=\"roadway\")\ny_train_pivot = y_pivot.loc[y_pivot.index.isin(X_train_pivot.index)]\n\n#Number the roadways:\nenc = OrdinalEncoder()\nX_train['roadway'] = enc.fit_transform(X_train[['roadway']])\nX_train['roadway'] = X_train[\"roadway\"].astype(int)\ndisplay(X_train.head(2))\ndisplay(y_train.head(2))","metadata":{"execution":{"iopub.status.busy":"2022-03-26T13:26:57.110579Z","iopub.execute_input":"2022-03-26T13:26:57.11083Z","iopub.status.idle":"2022-03-26T13:26:58.185944Z","shell.execute_reply.started":"2022-03-26T13:26:57.1108Z","shell.execute_reply":"2022-03-26T13:26:58.184895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_multistep_target(ts, steps):\n    return pd.concat(\n        {f'y_step_{i}': ts.shift(-i)\n         for i in range(steps+1)},\n        axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T13:26:58.237147Z","iopub.execute_input":"2022-03-26T13:26:58.238297Z","iopub.status.idle":"2022-03-26T13:26:58.243947Z","shell.execute_reply.started":"2022-03-26T13:26:58.238254Z","shell.execute_reply":"2022-03-26T13:26:58.242796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We make multistep targets, for every time and every roadway we make a y_step_i column that holds the future congestion values at each step i into the future (20 minute intervals). For example y_step_1 for roadway 01EB at time 1991-04-01 00:00:00, contains the known congestion value at 1991-04-01 00:20:00. \n\nThe idea is by having seperate columns we can make use of multiple output regressors to train a different regressor for each time in the future. For example at step 16, we will be using the X training data (containing hour/minute, day of week, moving average features, lagged features etc.) from **step 0** to train a regressor that purely attempts to **predict step 16** (16 20-minute time steps into the future).\n\nWe can notice that at the end of the training data (last 12 hours) we dont have full info of the congestion valus 1,...,36 values into the future, so we have to remove these from the training data. This means we have to remove the corresponding X data from training, so we loose 12 hours worth of training data. When I first implemented this I thought it would be an issue as we would loose AR (lags) and MA(moving average) data from the monday morning of the 1991-09-23; but it shouldnt be an issue, we can just extend the validation data back to start at 1991-09-23 00:00:00 so we can still use lags and MA terms. Note that we dont even loose the 1991-09-23 morning congestion data from training, e.g.the congestion at 11:40 1991-09-23 is contained in y_step_36 of 23:40 1991-09-22. ","metadata":{}},{"cell_type":"code","source":"y_train = make_multistep_target(y_train_pivot, steps=36).dropna()\ny_train = y_train.stack(\"roadway\")\ndisplay(y_train)\nX_train = X_train.loc[X_train.index <= '1991-09-22 23:40:00']\nX_train = X_train.set_index(\"roadway\", append=True, drop=False)\nX_train[\"roadway\"] = X_train[\"roadway\"].astype(\"category\") #Roadways are not ordered\ndisplay(X_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T13:26:58.245596Z","iopub.execute_input":"2022-03-26T13:26:58.246001Z","iopub.status.idle":"2022-03-26T13:27:00.166753Z","shell.execute_reply.started":"2022-03-26T13:26:58.245954Z","shell.execute_reply":"2022-03-26T13:27:00.165665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfrom sklearn.multioutput import MultiOutputRegressor\n\nmodel_lgbm = MultiOutputRegressor(LGBMRegressor(random_state=1, learning_rate=0.05, n_estimators=800, n_jobs=-1))\nmodel_lgbm.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T13:27:00.168844Z","iopub.execute_input":"2022-03-26T13:27:00.169443Z","iopub.status.idle":"2022-03-26T13:37:14.901585Z","shell.execute_reply.started":"2022-03-26T13:27:00.169401Z","shell.execute_reply":"2022-03-26T13:37:14.900649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we have to make a decision on what predictions to use. For predictions to be made on 12:20 1991-09-23 do we use y_step_1 of 12:00? y_step_2 of 11:40, y_step_3 of 11:20? Take the average of all of these predictions? I am not sure which is best.\n\nThis has implications on the validation training data (X_val); we have to use all of the 23rd September to make predictions from, not just the afternoon.\n\nWe can also use training data for afternoon times to make predictions (e.g. 16:00 to forecast 23:00). However this is slightly problematic as we don't have the congestion values for 12:00 - 16:00. This means use of some lags and some moving averages cannot be used as features. So we may want to only use the times before 12:00 to make predictions from. For now we include them all.","metadata":{}},{"cell_type":"code","source":"X_val = train_df_2.loc[train_df_2[\"date\"] == datetime.date(1991,9,23)]\nX_val = X_val.set_index(\"time\").drop(columns=[\"date\",\"congestion\",\"minutes\"])\nX_val = X_val.set_index(\"roadway\", append=True, drop=False)\nX_val[\"roadway\"] = enc.transform(X_val[[\"roadway\"]]).astype(int)\nX_val[\"roadway\"] = X_val[\"roadway\"].astype(\"category\")\n#display(X_val)\n#The validation is the congestion values on the afternoon of September 23rd\ny_val_index = train_df_2.loc[(train_df_2[\"date\"] == datetime.date(1991,9,23)) & (train_df_2[\"minutes\"] >= 12*60)].index\ny_val = y_full.loc[y_val_index]","metadata":{"execution":{"iopub.status.busy":"2022-03-26T13:37:14.905435Z","iopub.execute_input":"2022-03-26T13:37:14.906166Z","iopub.status.idle":"2022-03-26T13:37:15.198713Z","shell.execute_reply.started":"2022-03-26T13:37:14.906131Z","shell.execute_reply":"2022-03-26T13:37:15.197807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_lgbm = model_lgbm.predict(X_val)\npreds_lgbm_df = pd.DataFrame(preds_lgbm, columns = y_train.columns, index = X_val.index)\npreds_lgbm_df = preds_lgbm_df.reset_index(level=\"roadway\")\ndisplay(preds_lgbm_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T13:37:15.200096Z","iopub.execute_input":"2022-03-26T13:37:15.200342Z","iopub.status.idle":"2022-03-26T13:37:19.369643Z","shell.execute_reply.started":"2022-03-26T13:37:15.200312Z","shell.execute_reply":"2022-03-26T13:37:19.36844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To implement, lets first make an emtpy dataframe for the target period.\n- Each row will be for each time period and each roadway that we are predicting congestion values for.\n- Each column will be the y_step value corresponding to the that targeted time. \n\nSome examples:\n- [1991-09-23 12:00:00, y_step_2]  in the new df will be from [1991-09-23 11:20:00, y_step_2] of the old dataframe.\n- [1991-09-23 12:00:00, y_step_36]  in the new df will be from [1991-09-23 00:00:00, y_step_36] of the old dataframe.\n- [1991-09-23 23:40:00, y_step_1] in the new df will be from [1991-09-23 23:20:00, y_step_1] of the old dataframe.\n","metadata":{}},{"cell_type":"code","source":"#First create the empty dataframe\ndef empty_preds_df(preds_df, test=False):\n    if test == False:\n        new_index = preds_df.loc[preds_df.index >= \"1991-09-23 12:00:00\"].index\n    else:\n        new_index = preds_df.loc[preds_df.index >= \"1991-09-30 12:00:00\"].index\n    X_final = pd.DataFrame(index = new_index, columns = preds_df.columns)\n    if test == False:\n        X_final[\"roadway\"] = preds_df.loc[preds_df.index >= \"1991-09-23 12:00:00\",\"roadway\"]\n    else:\n        X_final[\"roadway\"] = preds_df.loc[preds_df.index >= \"1991-09-30 12:00:00\",\"roadway\"]\n    return X_final\n\n#X_val_final = empty_preds_df(preds_lgbm_df, test=False)\n#display(X_val_final.head(2))","metadata":{"execution":{"iopub.status.busy":"2022-03-26T13:37:19.371188Z","iopub.execute_input":"2022-03-26T13:37:19.371441Z","iopub.status.idle":"2022-03-26T13:37:19.381716Z","shell.execute_reply.started":"2022-03-26T13:37:19.371401Z","shell.execute_reply":"2022-03-26T13:37:19.380531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_preds(preds_df, test = False):\n    empty_df = empty_preds_df(preds_df, test=test)\n    for time in empty_df.index.unique():\n        for i in range(0,37):\n            corresponding_time = time - (pd.Timedelta('0 days 00:20:00')*i)\n            corresponding_values = preds_df.loc[preds_df.index == corresponding_time, \"y_step_\"+str(i)]\n            #print(corresponding_values)\n            empty_df.loc[empty_df.index == time, \"y_step_\"+str(i)] = corresponding_values.values\n    return empty_df\n    ","metadata":{"execution":{"iopub.status.busy":"2022-03-26T13:37:19.383206Z","iopub.execute_input":"2022-03-26T13:37:19.38348Z","iopub.status.idle":"2022-03-26T13:37:19.397495Z","shell.execute_reply.started":"2022-03-26T13:37:19.38345Z","shell.execute_reply":"2022-03-26T13:37:19.396421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multioutput_preds = generate_preds(preds_lgbm_df, test=False)\nmultioutput_preds.set_index(\"roadway\",append=True, inplace=True)\nmultioutput_preds[\"mean\"] = multioutput_preds.mean(axis=1)\nmultioutput_preds","metadata":{"execution":{"iopub.status.busy":"2022-03-26T13:37:19.399038Z","iopub.execute_input":"2022-03-26T13:37:19.399248Z","iopub.status.idle":"2022-03-26T13:37:22.154882Z","shell.execute_reply.started":"2022-03-26T13:37:19.399222Z","shell.execute_reply":"2022-03-26T13:37:22.153819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n- There is a fair amount of variation between steps\n- y_step 0 is very different from the other steps\n\n\n**Insights**\n\n- Each row is each time period (and each roadway) that we are predicting congestion values for.\n- Each column is the y_step model required to reach that target time. \n\n","metadata":{}},{"cell_type":"code","source":"def clipped(preds):\n    #Credit to AmbrosM: https://www.kaggle.com/code/ambrosm/tpsmar22-generalizing-the-special-values\n    sep = train_df_2[(train_df_2.time.dt.hour >= 12) & (train_df_2.dayofweek.isin([0,1,2,3,4])) &\n            (train_df_2.time.dt.dayofyear >= 246)]\n    lower = sep.groupby(['minutes', 'roadway']).congestion.quantile(0.15).values\n    upper = sep.groupby(['minutes', 'roadway']).congestion.quantile(0.7).values\n    \n    clipped_preds = preds[\"congestion\"].clip(lower,upper)\n    return clipped_preds","metadata":{"execution":{"iopub.status.busy":"2022-03-26T13:41:55.45479Z","iopub.execute_input":"2022-03-26T13:41:55.455179Z","iopub.status.idle":"2022-03-26T13:41:55.462494Z","shell.execute_reply.started":"2022-03-26T13:41:55.455144Z","shell.execute_reply":"2022-03-26T13:41:55.461574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_mae_steps():\n    mae_preds = []\n    mae_round = []\n    mae_clipped = []\n    mae_clipped_round = []\n    mae_round_clipped = []\n    for i in range(0,37):\n        preds = multioutput_preds[\"y_step_\"+str(i)]\n        mae_preds.append(mean_absolute_error(preds,y_val))\n        mae_round.append(mean_absolute_error(preds.round().astype(int),y_val))\n        mae_clipped.append(mean_absolute_error(clipped(preds).astype(\"float\"),y_val))\n        mae_clipped_round.append(mean_absolute_error(clipped(preds).astype(\"float\").round().astype(int),y_val))\n        mae_round_clipped.append(mean_absolute_error(clipped(preds.astype(\"float\").round().astype(int)),y_val))\n    new_df = pd.DataFrame({\"mae\":mae_preds},index=[\"y_step\"+str(i) for i in range(0,37)])\n    new_df[\"mae_round\"] =  mae_round\n    new_df[\"mae_clipped\"] =  mae_clipped\n    new_df[\"mae_clipped_round\"] =  mae_clipped_round\n    new_df[\"mae_round_clipped\"] =  mae_round_clipped\n    return new_df\ngenerate_mae_steps()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T13:41:55.775236Z","iopub.execute_input":"2022-03-26T13:41:55.776276Z","iopub.status.idle":"2022-03-26T13:42:18.38093Z","shell.execute_reply.started":"2022-03-26T13:41:55.77623Z","shell.execute_reply":"2022-03-26T13:42:18.380256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"MAE mean:\", mean_absolute_error(multioutput_preds[\"mean\"],y_val))\nprint(\"MAE round:\", mean_absolute_error(multioutput_preds[\"mean\"].round().astype(int),y_val))\nprint(\"MAE: clip-round \", mean_absolute_error(clipped(multioutput_preds[\"mean\"].astype(\"float\").round().astype(int).rename(\"congestion\").reset_index()),y_val))","metadata":{"execution":{"iopub.status.busy":"2022-03-26T13:42:18.382274Z","iopub.execute_input":"2022-03-26T13:42:18.383016Z","iopub.status.idle":"2022-03-26T13:42:18.608992Z","shell.execute_reply.started":"2022-03-26T13:42:18.382979Z","shell.execute_reply":"2022-03-26T13:42:18.60778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n- y_step_0 has the lowest MAE.\n- Clipping gives a massive improvement to score\n- Rounding before clipping is slightly better than clipping before rounding\n\n**Insight:**\n\n- Forecasting 0 step ahead recieved the best score, but only because of the added descriptive features (e.g. the median for that time, day of week and roadway). If we remove these descriptive features all forecasts would achieve roughly the same score. This makes sense - afterall how useful is the median congestion for 9:00am on Monday  as a feature when we are trying to predict congestion levels a 13:00 (12 steps ahead). Whereas when predicting the congestion levels at 9:00am (0-step ahead) having the 9:00am median is a very useful feature. This is the primary disadvantage of this technique and the reason I chose not to use it - there may be ways around this problem.","metadata":{}},{"cell_type":"markdown","source":"**Asessing perfromance by forecasting origin**\n\nThis isn't the only way of asessing the performance. We could instead compare mae generated from the same forecasting starting time e.g. when we use X_val data from 11:40am as a starting date should we expect better predictions than X_val data from 00:20am? or perhaps it wont make any difference? But we would have to forecast further than 36 steps ahead to get a full comparison for all times, so I have not included it here.\n\nLets organise the results where the rows are the targetted predicted times, and the columns are the times for which we are forecasting from:","metadata":{}},{"cell_type":"code","source":"new_index = preds_lgbm_df.loc[preds_lgbm_df.index >= \"1991-09-23 12:00:00\"].index\nX_val_final_forecast_date = pd.DataFrame(index = new_index, columns = [\"roadway\"] + preds_lgbm_df.index.unique().tolist())\nX_val_final_forecast_date[\"roadway\"] = preds_lgbm_df.loc[preds_lgbm_df.index >= \"1991-09-23 12:00:00\",\"roadway\"]\ndisplay(X_val_final_forecast_date.head(2))","metadata":{"execution":{"iopub.status.busy":"2022-03-26T13:48:24.953684Z","iopub.execute_input":"2022-03-26T13:48:24.954754Z","iopub.status.idle":"2022-03-26T13:48:24.99453Z","shell.execute_reply.started":"2022-03-26T13:48:24.954706Z","shell.execute_reply":"2022-03-26T13:48:24.993844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Works but not very clean\ndef generate_preds_by_date():\n    new_df = X_val_final_forecast_date.copy()\n    for time in preds_lgbm_df.index.unique():\n        steps_to_12 = int((pd.Timestamp('1991-09-23 12:00:00') - time) / pd.Timedelta('0 days 00:20:00'))\n        if steps_to_12 >= 0:\n            relevant_cols = [\"y_step_\"+str(i) for i in range(steps_to_12,steps_to_12 + 37)]\n            relevant_steps = [i for i in range(steps_to_12,steps_to_12 + 37)]\n        else:\n            relevant_cols = [\"y_step_\"+str(i) for i in range(0,36+steps_to_12)]\n            relevant_steps = [i for i in range(0,36+steps_to_12)]\n        relevant_cols = [i for i in relevant_cols if i in preds_lgbm_df.columns] # Check if real lag value (e.g not y_step_40)\n        relevant_steps = [i for i in relevant_steps if i<37]\n        for n,col in enumerate(relevant_cols):\n            relevant_step = relevant_steps[n]\n            selected_steps = preds_lgbm_df.loc[time, col].values # The congestion values for time and step\n            selected_steps = [item for sublist in selected_steps for item in sublist]\n            #Selecting the correct row to update\n            new_df.loc[new_df.index == (relevant_step*pd.Timedelta('0 days 00:20:00') + time), time] = selected_steps\n    return new_df","metadata":{"execution":{"iopub.status.busy":"2022-03-26T13:48:25.358023Z","iopub.execute_input":"2022-03-26T13:48:25.358608Z","iopub.status.idle":"2022-03-26T13:48:25.36888Z","shell.execute_reply.started":"2022-03-26T13:48:25.358572Z","shell.execute_reply":"2022-03-26T13:48:25.368087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_forecast_date = generate_preds_by_date()\n#display(preds_forecast_date)\n#display(preds_forecast_date.loc[preds_forecast_date[\"roadway\"]==\"00EB\"])","metadata":{"execution":{"iopub.status.busy":"2022-03-26T13:48:26.690609Z","iopub.execute_input":"2022-03-26T13:48:26.69169Z","iopub.status.idle":"2022-03-26T13:48:29.182736Z","shell.execute_reply.started":"2022-03-26T13:48:26.691612Z","shell.execute_reply":"2022-03-26T13:48:29.181805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As of now, there's only 1 starting date that contains all predictions for the afternoon of 31/09/2020 (11:40am).","metadata":{}},{"cell_type":"code","source":"print(\"MAE:\", mean_absolute_error(preds_forecast_date.loc[:,pd.Timestamp('1991-09-23 11:40:00')],y_val))\nprint(\"MAE:\", mean_absolute_error(np.round(preds_forecast_date.loc[:,pd.Timestamp('1991-09-23 11:40:00')].astype(\"float\").round().astype(int)),y_val))","metadata":{"execution":{"iopub.status.busy":"2022-03-26T13:48:33.984901Z","iopub.execute_input":"2022-03-26T13:48:33.985182Z","iopub.status.idle":"2022-03-26T13:48:33.995231Z","shell.execute_reply.started":"2022-03-26T13:48:33.98515Z","shell.execute_reply":"2022-03-26T13:48:33.99432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DirRec Strategy","metadata":{}},{"cell_type":"markdown","source":"The DirRec strategy uses a combination of the direct strategy (above) and a recursive strategy. The previous predicted congested values are used as additional (lag) features in the model used to in the next step-ahead forecast.","metadata":{}},{"cell_type":"code","source":"#display(X_train)\n#display(y_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T15:58:17.759587Z","iopub.execute_input":"2022-03-26T15:58:17.759921Z","iopub.status.idle":"2022-03-26T15:58:17.777893Z","shell.execute_reply.started":"2022-03-26T15:58:17.759836Z","shell.execute_reply":"2022-03-26T15:58:17.7773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfrom sklearn.multioutput import RegressorChain\n\nmodel_lgbm = RegressorChain(LGBMRegressor(random_state=1, learning_rate=0.05, n_estimators=800, n_jobs=-1))\nmodel_lgbm.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T13:49:50.011022Z","iopub.execute_input":"2022-03-26T13:49:50.011349Z","iopub.status.idle":"2022-03-26T14:10:30.870535Z","shell.execute_reply.started":"2022-03-26T13:49:50.011306Z","shell.execute_reply":"2022-03-26T14:10:30.869508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_lgbm = model_lgbm.predict(X_val)\npreds_lgbm_df = pd.DataFrame(preds_lgbm, columns = y_train.columns, index = X_val.index)\npreds_lgbm_df = preds_lgbm_df.reset_index(level=\"roadway\")\n#display(preds_lgbm_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T14:10:30.872286Z","iopub.execute_input":"2022-03-26T14:10:30.872552Z","iopub.status.idle":"2022-03-26T14:10:33.728631Z","shell.execute_reply.started":"2022-03-26T14:10:30.872522Z","shell.execute_reply":"2022-03-26T14:10:33.72773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multioutput_preds = generate_preds(preds_lgbm_df)\nmultioutput_preds.set_index(\"roadway\",append=True, inplace=True)\nmultioutput_preds[\"mean\"] = multioutput_preds.mean(axis=1)\ndisplay(multioutput_preds)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T14:10:33.730687Z","iopub.execute_input":"2022-03-26T14:10:33.730949Z","iopub.status.idle":"2022-03-26T14:10:36.465148Z","shell.execute_reply.started":"2022-03-26T14:10:33.730918Z","shell.execute_reply":"2022-03-26T14:10:36.464148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"MAE:\", mean_absolute_error(multioutput_preds[\"mean\"],y_val))\nprint(\"MAE:\", mean_absolute_error(multioutput_preds[\"mean\"].round().astype(int),y_val))","metadata":{"execution":{"iopub.status.busy":"2022-03-26T14:10:36.466668Z","iopub.execute_input":"2022-03-26T14:10:36.4669Z","iopub.status.idle":"2022-03-26T14:10:36.481936Z","shell.execute_reply.started":"2022-03-26T14:10:36.466874Z","shell.execute_reply":"2022-03-26T14:10:36.481036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_mae_steps()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T14:22:02.320921Z","iopub.execute_input":"2022-03-26T14:22:02.321315Z","iopub.status.idle":"2022-03-26T14:22:25.051936Z","shell.execute_reply.started":"2022-03-26T14:22:02.321282Z","shell.execute_reply":"2022-03-26T14:22:25.051068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n\n- The higher the step, the higher the MAE.\n- Clipping helps to stop the error from propogating as the step increases.\n\n**Insights:** \n\n- We can see the error starts to propogate as the step increases. This likely isn't a good dataset to use a recursive strategy on (perhaps with the exception of the first few lags)","metadata":{}},{"cell_type":"markdown","source":"**Additional evidence against a recursive strategy:**\n\nMore evidence against a recursive strategy. We can use ACF and PACF to decide how many lag values we should include in our model. This is a common strategy for deciding the order of a SARIMAX model (see me [Notebook](https://www.kaggle.com/code/cabaxiom/tps-mar-22-sarima-linear-regression) where I do this). I'll recreate this here:","metadata":{}},{"cell_type":"markdown","source":"- I only consider mondays - that way I only have to worry about the daily seasonality not weekly as well (mondays follow each other in time series)\n- Time series is made stationary before plotting ACF/PACF Functions\n- AR = AutoRegressive (number of lags to use)\n- MA = MovingAverage","metadata":{}},{"cell_type":"code","source":"def plot_series(df):\n    plt.subplots(figsize=(25, 6))\n    plt.title(\"Time series\")\n    xticks = df[df[\"minutes\"]==0][\"time_count\"].values\n    xtick_dates = df[\"week\"].unique()\n    ax = sns.lineplot(data=df, x=\"time_count\", y=\"congestion\", linewidth=1);\n\ndef plot_series_diff(df):\n    temp_df = df.copy()\n\n    temp_df[\"congestion_diff_72\"] = temp_df[\"congestion\"].diff(periods=72)\n    temp_df = temp_df.dropna()\n    plt.subplots(figsize=(25, 6))\n    plt.title(\"Stationary time series (difference 72)\")\n    xticks = df[df[\"minutes\"]==0][\"time_count\"].values\n    xtick_dates = df[\"week\"].unique()\n    ax = sns.lineplot(data=temp_df, x=\"time_count\", y=\"congestion_diff_72\", linewidth=1 );\n\n    return temp_df[\"congestion_diff_72\"]\n\ndef plot_acf_pacf(df):\n    temp_df = df.copy()\n    temp_df[\"congestion_diff_72\"] = temp_df[\"congestion\"].diff(periods=72)\n    temp_df = temp_df.dropna()\n    \n    f,ax= plt.subplots(figsize=(25, 12))\n    ax = plt.subplot(2, 1, 1)\n    plot_acf(temp_df[\"congestion_diff_72\"], lags=300, ax=ax);\n    for i in range(5):\n            plt.axvline(i*72, color='r', lw=1)\n            \n    ax = plt.subplot(2, 1, 2)\n    plot_pacf(temp_df[\"congestion_diff_72\"], lags=300, method='ywm', ax=ax);\n    for i in range(5):\n        plt.axvline(i*72, color='r', lw=1)\n        \ndef decide_orders(df):\n    plot_series(df)\n    series = plot_series_diff(df)\n    plot_acf_pacf(df)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T14:18:24.529105Z","iopub.execute_input":"2022-03-26T14:18:24.52944Z","iopub.status.idle":"2022-03-26T14:18:24.545738Z","shell.execute_reply.started":"2022-03-26T14:18:24.529402Z","shell.execute_reply":"2022-03-26T14:18:24.544225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Removing: \n#Memorial day - Monday May 27 1991\n#Labor Day - Monday 2nd September 1991\nmon = train_df_2.loc[(train_df_2[\"dayofweek\"] == 0) & (~train_df_2[\"date\"].isin([datetime.date(1991, 5, 27),datetime.date(1991, 9, 2)])),:].copy()\n\n#Assign each time a unique ID number to make plotting easier.\nenc = OrdinalEncoder()\nenc.fit(mon[[\"time\"]])\nmon[\"time_count\"] = enc.transform(mon[[\"time\"]]).astype(int)\nmon[\"week\"] = mon[\"time\"].dt.isocalendar().week # Inlcude week for plotting","metadata":{"execution":{"iopub.status.busy":"2022-03-26T14:18:25.476997Z","iopub.execute_input":"2022-03-26T14:18:25.477272Z","iopub.status.idle":"2022-03-26T14:18:25.606759Z","shell.execute_reply.started":"2022-03-26T14:18:25.477243Z","shell.execute_reply":"2022-03-26T14:18:25.605679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp_df = mon[mon[\"roadway\"] == \"02NB\"]\ndecide_orders(temp_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T14:18:26.319336Z","iopub.execute_input":"2022-03-26T14:18:26.319688Z","iopub.status.idle":"2022-03-26T14:18:28.672975Z","shell.execute_reply.started":"2022-03-26T14:18:26.319653Z","shell.execute_reply":"2022-03-26T14:18:28.671712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n\n- Significant ACF spikes at lag 1,71,72,73.\n- The PACF tapers to 0 seasonally.\n- The PACF has 2 significant PACF spikes at lags 1 and 2.\n\nNon-Seasonal terms:\n\n- A Spike at lag 1 in both the ACF and PACF could indicate either MA(1) or AR(1) terms. Perhaps both - ARMA(1,1). As the PCAF early terms might be slightly tapering an MA(1) term is more likely than an AR(1).\n- Perhaps an AR(2) term is also possible.\n\nSeasonal\n\n- There is 1 significant spike at lag 72 in the ACF (spikes at lags 71 and 73 too). A seasonal MA(1) component seems likely.\n\nPossible ARIMA modles to consider (in order of most likely):\n\n- (0,0,1) x (0,1,1)72\n- (1,0,1) x (0,1,1)72\n- (1,0,0) x (0,1,1)72\n- (2,0,0) x (0,1,1)72\n- (2,0,1) x (0,1,1)72\n\nnon_seasonal order(AR order , diff order, MA order) x season_order(AR order, diff order, MA order)_lags in season","metadata":{}},{"cell_type":"markdown","source":"**Insight:**\n\n- The main point to notice here is the AR order for the non-seasonal terms. It seems unlikely that more than 2 lag values will improve the model (atleast for this roadway).  If lag values are not that useful then how can we expect a recursive model that uses previous predictions as additional features to work well?\n- Perhaps this can give more insight about what other features might be appropriate\n- This is only a tool to help decide on lagged/MA features, it does not mean that these features are guaranteed to work, or guarantee that other features will not work.","metadata":{}},{"cell_type":"markdown","source":"# Test","metadata":{}},{"cell_type":"markdown","source":"We use a 0-step ahead forecast from the Direct model for our predictions, as this achieved the highest validaiton score.\n\nUsing all the training data to train the model:","metadata":{}},{"cell_type":"code","source":"train_df_2 = add_features(train_df)\ntrain_df_2 = descriptive_features(train_df_2)\ntest_df_2 = add_features(test_df)\ntest_df_2 = descriptive_features(test_df_2)\n\ntrain_df_2 = make_cyclic(train_df_2)\ntest_df_2 = make_cyclic(test_df_2, plot=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T23:53:38.161449Z","iopub.execute_input":"2022-03-25T23:53:38.161621Z","iopub.status.idle":"2022-03-25T23:53:43.901558Z","shell.execute_reply.started":"2022-03-25T23:53:38.1616Z","shell.execute_reply":"2022-03-25T23:53:43.900484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = train_df_2\nX_train = X_train.drop(columns=[\"date\",\"congestion\",\"minutes\"])\ny_train = y_full.loc[y_full.index.isin(X_train.index)]\nX_train = X_train.set_index(\"time\")\n\n\nX_train_pivot = X_train.pivot(columns=\"roadway\")\ny_train_pivot = y_pivot.loc[y_pivot.index.isin(X_train_pivot.index)]\n\n#Number the roadways:\nenc = OrdinalEncoder()\nX_train['roadway'] = enc.fit_transform(X_train[['roadway']])\nX_train['roadway'] = X_train[\"roadway\"].astype(int)\n\ny_train = make_multistep_target(y_train_pivot, steps=36).dropna()\ny_train = y_train.stack(\"roadway\")\n\nX_train = X_train.loc[X_train.index <= '1991-09-29 23:40:00']\nX_train = X_train.set_index(\"roadway\", append=True, drop=False)\nX_train[\"roadway\"] = X_train[\"roadway\"].astype(\"category\") #Roadways are not ordered\ndisplay(y_train)\ndisplay(X_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T23:53:43.90304Z","iopub.execute_input":"2022-03-25T23:53:43.903269Z","iopub.status.idle":"2022-03-25T23:53:45.884775Z","shell.execute_reply.started":"2022-03-25T23:53:43.903245Z","shell.execute_reply":"2022-03-25T23:53:45.883984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = pd.concat([train_df_2.loc[train_df_2[\"date\"] == datetime.date(1991,9,30)], test_df_2])\nX_test = X_test.set_index(\"time\").drop(columns=[\"date\",\"congestion\",\"minutes\"])\nX_test = X_test.set_index(\"roadway\", append=True, drop=False)\nX_test[\"roadway\"] = enc.transform(X_test[[\"roadway\"]]).astype(int)\nX_test[\"roadway\"] = X_test[\"roadway\"].astype(\"category\")\nX_test[\"dayofweek\"] = X_test[\"dayofweek\"].astype(\"category\")\nX_test[\"weekend\"] = X_test[\"weekend\"].astype(\"category\")\ndisplay(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:08:25.391613Z","iopub.execute_input":"2022-03-26T00:08:25.391866Z","iopub.status.idle":"2022-03-26T00:08:25.477132Z","shell.execute_reply.started":"2022-03-26T00:08:25.391843Z","shell.execute_reply":"2022-03-26T00:08:25.476669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfrom sklearn.multioutput import MultiOutputRegressor\n\nmodel_lgbm = MultiOutputRegressor(LGBMRegressor(random_state=1, learning_rate=0.05, n_estimators=800, n_jobs=-1))\nmodel_lgbm.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T23:53:45.968701Z","iopub.execute_input":"2022-03-25T23:53:45.968875Z","iopub.status.idle":"2022-03-26T00:05:33.551732Z","shell.execute_reply.started":"2022-03-25T23:53:45.968855Z","shell.execute_reply":"2022-03-26T00:05:33.551225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_lgbm = model_lgbm.predict(X_test)\npreds_lgbm_df = pd.DataFrame(preds_lgbm, columns = y_train.columns, index = X_test.index)\npreds_lgbm_df = preds_lgbm_df.reset_index(level=\"roadway\")\n#display(preds_lgbm_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:08:38.313147Z","iopub.execute_input":"2022-03-26T00:08:38.313514Z","iopub.status.idle":"2022-03-26T00:08:42.010831Z","shell.execute_reply.started":"2022-03-26T00:08:38.313489Z","shell.execute_reply":"2022-03-26T00:08:42.009326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multioutput_preds = generate_preds(preds_lgbm_df, test=True)\nmultioutput_preds.set_index(\"roadway\",append=True, inplace=True)\nmultioutput_preds[\"mean\"] = multioutput_preds.mean(axis=1)\nmultioutput_preds","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:16:43.342324Z","iopub.execute_input":"2022-03-26T00:16:43.342776Z","iopub.status.idle":"2022-03-26T00:16:45.52302Z","shell.execute_reply.started":"2022-03-26T00:16:43.342735Z","shell.execute_reply":"2022-03-26T00:16:45.522031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = multioutput_preds.reset_index().loc[:,[\"time\",\"roadway\",\"y_step_0\"]]#.reset_index()\npreds.columns = preds.columns.droplevel(level=1)\npreds = preds.rename(columns={\"y_step_0\":\"congestion\"})\npreds[\"congestion\"] = preds[\"congestion\"].astype(\"float\").round()\npreds[\"congestion\"] = clipped(preds)\npreds","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:31:51.542888Z","iopub.execute_input":"2022-03-26T00:31:51.543122Z","iopub.status.idle":"2022-03-26T00:31:51.805035Z","shell.execute_reply.started":"2022-03-26T00:31:51.543097Z","shell.execute_reply":"2022-03-26T00:31:51.804367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv(\"../input/tabular-playground-series-mar-2022/sample_submission.csv\")\nsubmission['congestion'] = preds[\"congestion\"]\ndisplay(submission)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:33:08.57566Z","iopub.execute_input":"2022-03-26T00:33:08.577253Z","iopub.status.idle":"2022-03-26T00:33:08.596339Z","shell.execute_reply.started":"2022-03-26T00:33:08.577194Z","shell.execute_reply":"2022-03-26T00:33:08.595677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}