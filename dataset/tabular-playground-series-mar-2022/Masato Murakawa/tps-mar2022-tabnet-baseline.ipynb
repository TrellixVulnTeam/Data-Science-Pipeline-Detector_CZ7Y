{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"---\n# [Tabular Playground Series - Mar 2022][1]\n---\n**Comments**: Thanks to previous great Notebooks.\n\n1. [[TPS JAN 22] Base XGB & LGB][2]\n2. [TabNet in Tensorflow 2.0][3]\n\n---\n[1]: https://www.kaggle.com/c/tabular-playground-series-mar-2022\n[2]: https://www.kaggle.com/ranjeetshrivastav/tps-jan-22-base-xgb-lgb\n[3]: https://www.kaggle.com/marcusgawronsky/tabnet-in-tensorflow-2-0","metadata":{}},{"cell_type":"markdown","source":"# 0. Settings","metadata":{}},{"cell_type":"code","source":"# Import dependencies \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \n%matplotlib inline\n\nimport os\nimport pathlib\nimport gc\nimport sys\nimport re\nimport math \nimport random\nimport time \nfrom tqdm import tqdm \nfrom pprint import pprint\nfrom typing import Optional, Union, Tuple\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport sklearn\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport tensorflow_hub as hub\nfrom tensorflow.keras.layers.experimental import preprocessing\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\nimport tensorflow_addons as tfa\n\nprint('import done!')","metadata":{"execution":{"iopub.status.busy":"2022-03-02T06:02:32.538842Z","iopub.execute_input":"2022-03-02T06:02:32.539549Z","iopub.status.idle":"2022-03-02T06:02:32.554101Z","shell.execute_reply.started":"2022-03-02T06:02:32.539511Z","shell.execute_reply":"2022-03-02T06:02:32.55328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# global config\nconfig = {}\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\n# For reproducible results    \ndef seed_all(s):\n    random.seed(s)\n    np.random.seed(s)\n    tf.random.set_seed(s)\n    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n    os.environ['PYTHONHASHSEED'] = str(s) \n    print('Seeds setted!')\n    \nglobal_seed = 42\nseed_all(global_seed)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T06:02:32.556024Z","iopub.execute_input":"2022-03-02T06:02:32.556807Z","iopub.status.idle":"2022-03-02T06:02:32.674428Z","shell.execute_reply.started":"2022-03-02T06:02:32.556769Z","shell.execute_reply":"2022-03-02T06:02:32.673559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"## 1.1 Data Check","metadata":{}},{"cell_type":"code","source":"data_config = {'train_csv_path': '../input/tabular-playground-series-mar-2022/train.csv',\n              'test_csv_path': '../input/tabular-playground-series-mar-2022/test.csv',\n              'sample_submission_path': '../input/tabular-playground-series-mar-2022/sample_submission.csv',\n              }\n\ntrain_df = pd.read_csv(data_config['train_csv_path'])\ntest_df = pd.read_csv(data_config['test_csv_path'])\nsubmission_df = pd.read_csv(data_config['sample_submission_path'])\n\nprint(train_df.shape, test_df.shape, submission_df.shape)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-02T06:02:32.680788Z","iopub.execute_input":"2022-03-02T06:02:32.681015Z","iopub.status.idle":"2022-03-02T06:02:33.104391Z","shell.execute_reply.started":"2022-03-02T06:02:32.68099Z","shell.execute_reply":"2022-03-02T06:02:33.103721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_unique_category(df, column):\n    print(f'unique_category_number: {df[column].nunique()}')\n    print(f'cagetories: {df[column].unique()}')\n    print()\n\nprint_unique_category(train_df, 'direction')\nprint_unique_category(train_df, 'x')\nprint_unique_category(train_df, 'y')","metadata":{"execution":{"iopub.status.busy":"2022-03-02T06:02:33.106449Z","iopub.execute_input":"2022-03-02T06:02:33.10677Z","iopub.status.idle":"2022-03-02T06:02:33.229131Z","shell.execute_reply.started":"2022-03-02T06:02:33.106733Z","shell.execute_reply":"2022-03-02T06:02:33.228294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Null Value Check\ndef null_val_check(df):\n    null_check_df = df.isnull().sum()\n    for key in null_check_df.keys():\n        assert null_check_df[key] == 0, f'{key} has {null_check_df[key]} null values.'\n    print('No Null values.')\n    \nnull_val_check(train_df)\nnull_val_check(test_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T06:02:33.230502Z","iopub.execute_input":"2022-03-02T06:02:33.23083Z","iopub.status.idle":"2022-03-02T06:02:33.397227Z","shell.execute_reply.started":"2022-03-02T06:02:33.230788Z","shell.execute_reply":"2022-03-02T06:02:33.396518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.dtypes","metadata":{"execution":{"iopub.status.busy":"2022-03-02T06:02:33.398452Z","iopub.execute_input":"2022-03-02T06:02:33.398688Z","iopub.status.idle":"2022-03-02T06:02:33.405424Z","shell.execute_reply.started":"2022-03-02T06:02:33.398653Z","shell.execute_reply":"2022-03-02T06:02:33.404704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_unique_category(test_df, 'direction')\nprint_unique_category(test_df, 'x')\nprint_unique_category(test_df, 'y')\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-02T06:02:33.406792Z","iopub.execute_input":"2022-03-02T06:02:33.407256Z","iopub.status.idle":"2022-03-02T06:02:33.42728Z","shell.execute_reply.started":"2022-03-02T06:02:33.407219Z","shell.execute_reply":"2022-03-02T06:02:33.426663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-02T06:02:33.428464Z","iopub.execute_input":"2022-03-02T06:02:33.428687Z","iopub.status.idle":"2022-03-02T06:02:33.43866Z","shell.execute_reply.started":"2022-03-02T06:02:33.428656Z","shell.execute_reply":"2022-03-02T06:02:33.437796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2 Normalization","metadata":{}},{"cell_type":"code","source":"congestion = train_df['congestion']\ncongestion.describe()","metadata":{"execution":{"iopub.status.busy":"2022-03-02T06:02:33.439995Z","iopub.execute_input":"2022-03-02T06:02:33.440778Z","iopub.status.idle":"2022-03-02T06:02:33.473558Z","shell.execute_reply.started":"2022-03-02T06:02:33.440744Z","shell.execute_reply":"2022-03-02T06:02:33.472772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Standardization\n#congestion_mean = congestion.mean()\n#congestion_std = congestion.std()\n#train_df['congestion'] = (congestion - congestion_mean) / congestion_std \n\n# min-max scaling\nc_min = congestion.min()\nc_max = congestion.max()\ntrain_df['congestion'] = (congestion - c_min) / (c_max - c_min)\n\ntrain_df['congestion'].describe()","metadata":{"execution":{"iopub.status.busy":"2022-03-02T06:02:33.476604Z","iopub.execute_input":"2022-03-02T06:02:33.476807Z","iopub.status.idle":"2022-03-02T06:02:33.522325Z","shell.execute_reply.started":"2022-03-02T06:02:33.476777Z","shell.execute_reply":"2022-03-02T06:02:33.521616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.3 Feature Engineering","metadata":{}},{"cell_type":"code","source":"train_df = train_df.drop(['row_id'], axis=1)\ntest_df = test_df.drop(['row_id'], axis=1)\n\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-02T06:02:33.523545Z","iopub.execute_input":"2022-03-02T06:02:33.523899Z","iopub.status.idle":"2022-03-02T06:02:33.551341Z","shell.execute_reply.started":"2022-03-02T06:02:33.523849Z","shell.execute_reply":"2022-03-02T06:02:33.550552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_date_features(df, column='time', drop_column=True):\n    df['datetime'] = pd.to_datetime(df[column])\n    df['year'] = df['datetime'].dt.year\n    df['month'] = df['datetime'].dt.month\n    df['day'] = df['datetime'].dt.day\n    df['dayofweek'] = df['datetime'].dt.dayofweek\n    df['hour'] = df['datetime'].dt.hour\n    df['minute'] = df['datetime'].dt.minute\n    df = df.drop(['datetime'], axis=1)\n    if drop_column:\n        df = df.drop([column], axis=1)\n    return df \n\ntrain_df = make_date_features(train_df)\ntrain_df = train_df.drop(['year'], axis=1) # Because the 'year' column only contains '1991'.\n\ntest_df = make_date_features(test_df)\ntest_df = test_df.drop(['year'], axis=1)\n\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-02T06:02:33.553536Z","iopub.execute_input":"2022-03-02T06:02:33.554025Z","iopub.status.idle":"2022-03-02T06:02:34.215309Z","shell.execute_reply.started":"2022-03-02T06:02:33.553987Z","shell.execute_reply":"2022-03-02T06:02:34.214612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_df = train_df.copy()\nvalid_df = train_df.query('month == 9').reset_index(drop=True)\ntrain_df = train_df.query('month != 9').reset_index(drop=True)\n\nprint(f'all_df length: {len(all_df)}')\nprint(f'train_df length: {len(train_df)}')\nprint(f'valid_df length: {len(valid_df)}')\nvalid_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-02T06:02:34.216498Z","iopub.execute_input":"2022-03-02T06:02:34.216822Z","iopub.status.idle":"2022-03-02T06:02:34.319232Z","shell.execute_reply.started":"2022-03-02T06:02:34.216784Z","shell.execute_reply":"2022-03-02T06:02:34.31851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_all = all_df['congestion']\nX_all = all_df.drop(['congestion'], axis=1)\n\ny_train = train_df['congestion'] \nX_train = train_df.drop(['congestion'], axis=1)\n\ny_valid = valid_df['congestion'] \nX_valid = valid_df.drop(['congestion'], axis=1)\n\nprint(X_all.shape, y_all.shape)\nprint(X_train.shape, y_train.shape)\nprint(X_valid.shape, y_valid.shape)\nprint(test_df.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T06:02:34.32054Z","iopub.execute_input":"2022-03-02T06:02:34.3213Z","iopub.status.idle":"2022-03-02T06:02:34.380373Z","shell.execute_reply.started":"2022-03-02T06:02:34.321258Z","shell.execute_reply":"2022-03-02T06:02:34.379563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical_features = ['direction', 'dayofweek']\nct = ColumnTransformer([('one_hot', OneHotEncoder(), categorical_features)], remainder=\"passthrough\")\nct.fit(X_train)\n\nencoded_X_train = ct.transform(X_train)\nprint(encoded_X_train.shape)\n\nfeature_columns = ct.transformers_[0][1].get_feature_names(categorical_features)\nprint(feature_columns)\n\ncolumns = list(X_train.columns)\nfor feature in categorical_features:\n    columns.remove(feature)\ncolumns = list(feature_columns) + columns\n\nencoded_X_train_df = pd.DataFrame(encoded_X_train, columns=columns)\nencoded_X_train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-02T06:02:34.381841Z","iopub.execute_input":"2022-03-02T06:02:34.382096Z","iopub.status.idle":"2022-03-02T06:02:35.287945Z","shell.execute_reply.started":"2022-03-02T06:02:34.382066Z","shell.execute_reply":"2022-03-02T06:02:35.287244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_X_all = ct.transform(X_all)\nencoded_X_all_df = pd.DataFrame(encoded_X_all, columns=columns)\n\nencoded_X_valid = ct.transform(X_valid)\nencoded_X_valid_df = pd.DataFrame(encoded_X_valid, columns=columns)\n\nencoded_X_test = ct.transform(test_df)\nencoded_X_test_df = pd.DataFrame(encoded_X_test, columns=columns)\n\nprint(encoded_X_all_df.shape, encoded_X_valid_df.shape, encoded_X_test_df.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T06:02:35.289535Z","iopub.execute_input":"2022-03-02T06:02:35.290047Z","iopub.status.idle":"2022-03-02T06:02:35.850799Z","shell.execute_reply.started":"2022-03-02T06:02:35.290009Z","shell.execute_reply":"2022-03-02T06:02:35.850059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.3 Datasets","metadata":{}},{"cell_type":"code","source":"train_ds = tf.data.Dataset.from_tensor_slices((encoded_X_train, y_train))\nvalid_ds = tf.data.Dataset.from_tensor_slices((encoded_X_valid, y_valid))\nprint(len(train_ds), len(valid_ds))\n\nconfig = {'lr': 1e-3,\n          'epochs': 1,\n          'batch_size': 256,\n          'virtual_batch_size': 128,\n          }\n\ntrain_ds = train_ds.batch(config['batch_size'], drop_remainder=True)\nvalid_ds = valid_ds.batch(config['batch_size'], drop_remainder=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T06:02:35.852158Z","iopub.execute_input":"2022-03-02T06:02:35.852581Z","iopub.status.idle":"2022-03-02T06:02:36.101512Z","shell.execute_reply.started":"2022-03-02T06:02:35.852544Z","shell.execute_reply":"2022-03-02T06:02:36.100849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Model Training","metadata":{}},{"cell_type":"markdown","source":"## 2.1 TabNet\n\n![tabnet](https://github.com/titu1994/tf-TabNet/raw/master/images/TabNet.png?raw=true)\n\nIf you would like to know more about TabNet, please check out the [previous notebook][1].\n\n[1]: https://www.kaggle.com/marcusgawronsky/tabnet-in-tensorflow-2-0","metadata":{}},{"cell_type":"code","source":"class GLUBlock(tf.keras.layers.Layer):\n    def __init__(self,\n                 units: Optional[int]=None,\n                 virtual_batch_size: Optional[int]=128,\n                 momentum: Optional[float]=0.02):\n        \n        super().__init__()\n        self.units = units \n        self.virtual_batch_size = virtual_batch_size \n        self.momentum = momentum \n\n    def build(self, input_shape: tf.TensorShape):\n        if self.units is None:\n            self.units = input_shape[-1]\n\n        self.fc_output = tf.keras.layers.Dense(self.units, use_bias=False)\n        self.bn_output = tf.keras.layers.BatchNormalization(virtual_batch_size=self.virtual_batch_size,\n                                                            momentum=self.momentum)\n        self.fc_gate = tf.keras.layers.Dense(self.units, use_bias=False)\n        self.bn_gate = tf.keras.layers.BatchNormalization(virtual_batch_size=self.virtual_batch_size,\n                                                          momentum=self.momentum)\n        \n    def call(self, inputs: Union[tf.Tensor, np.ndarray], training: Optional[bool]=None):\n        output = self.bn_output(self.fc_output(inputs), training=training)\n        gate = self.bn_gate(self.fc_gate(inputs), training=training)\n\n        return output * tf.keras.activations.sigmoid(gate)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T06:02:36.102902Z","iopub.execute_input":"2022-03-02T06:02:36.103341Z","iopub.status.idle":"2022-03-02T06:02:36.113697Z","shell.execute_reply.started":"2022-03-02T06:02:36.103305Z","shell.execute_reply":"2022-03-02T06:02:36.112992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeatureTransformerBlock(tf.keras.layers.Layer):\n    def __init__(self,\n                 units: Optional[int]=None,\n                 virtual_batch_size: Optional[int]=128,\n                 momentum: Optional[float]=0.02,\n                 skip=False):\n        \n        super().__init__()\n        self.units = units \n        self.virtual_batch_size = virtual_batch_size \n        self.momentum = momentum \n        self.skip = skip \n\n    def build(self, input_shape: tf.TensorShape):\n        if self.units is None:\n            self.units = input_shape[-1]\n\n        self.initial = GLUBlock(units=self.units,\n                                virtual_batch_size=self.virtual_batch_size,\n                                momentum=self.momentum)\n        self.residual = GLUBlock(units=self.units,\n                                 virtual_batch_size=self.virtual_batch_size,\n                                 momentum=self.momentum)\n        \n    def call(self, inputs: Union[tf.Tensor, np.ndarray], training: Optional[bool]=None):\n        initial = self.initial(inputs, training=training)\n\n        if self.skip == True:\n            initial += inputs \n\n        residual = self.residual(initial, training=training)\n\n        return (initial + residual) * np.sqrt(0.5)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T06:02:36.114555Z","iopub.execute_input":"2022-03-02T06:02:36.114724Z","iopub.status.idle":"2022-03-02T06:02:36.127412Z","shell.execute_reply.started":"2022-03-02T06:02:36.114701Z","shell.execute_reply":"2022-03-02T06:02:36.126666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AttentiveTransformer(tf.keras.layers.Layer):\n    def __init__(self,\n                 units: Optional[int]=None,\n                 virtual_batch_size: Optional[int]=128,\n                 momentum: Optional[float]=0.02):\n        \n        super().__init__()\n        self.units = units\n        self.virtual_batch_size = virtual_batch_size \n        self.momentum = momentum \n\n    def build(self, input_shape: tf.TensorShape):\n        if self.units is None:\n            self.units = input_shape[-1]\n\n        self.fc = tf.keras.layers.Dense(self.units, use_bias=False)\n        self.bn = tf.keras.layers.BatchNormalization(virtual_batch_size=self.virtual_batch_size,\n                                                     momentum=self.momentum)\n        \n    def call(self, inputs: Union[tf.Tensor, np.ndarray],\n             priors: Optional[Union[tf.Tensor, np.ndarray]]=None,\n             training: Optional[bool]=None) -> tf.Tensor:\n        feature = self.bn(self.fc(inputs), training=training)\n        if priors is None:\n            output = feature \n        else:\n            output = feature * priors \n\n        return tfa.activations.sparsemax(output)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T06:02:36.128577Z","iopub.execute_input":"2022-03-02T06:02:36.129349Z","iopub.status.idle":"2022-03-02T06:02:36.140542Z","shell.execute_reply.started":"2022-03-02T06:02:36.129312Z","shell.execute_reply":"2022-03-02T06:02:36.13984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TabNetStep(tf.keras.layers.Layer):\n    def __init__(self, \n                 units: Optional[int]=None,\n                 virtual_batch_size: Optional[int]=128,\n                 momentum: Optional[float]=0.02):\n        super().__init__()\n        self.units = units \n        self.virtual_batch_size = virtual_batch_size \n        self.momentum = momentum \n\n    def build(self, input_shape: tf.TensorShape):\n        if self.units is None:\n            self.units = input_shape[-1]\n\n        self.unique = FeatureTransformerBlock(units=self.units,\n                                              virtual_batch_size=self.virtual_batch_size,\n                                              momentum=self.momentum,\n                                              skip=True)\n        self.attention = AttentiveTransformer(units=input_shape[-1],\n                                              virtual_batch_size=self.virtual_batch_size,\n                                              momentum=self.momentum)\n        \n    def call(self, inputs, shared, priors, training=None) -> Tuple[tf.Tensor]:\n        split = self.unique(shared, training=training)\n        keys = self.attention(split, priors, training=training)\n        masked = keys * inputs \n\n        return split, masked, keys ","metadata":{"execution":{"iopub.status.busy":"2022-03-02T06:02:36.141412Z","iopub.execute_input":"2022-03-02T06:02:36.142759Z","iopub.status.idle":"2022-03-02T06:02:36.153859Z","shell.execute_reply.started":"2022-03-02T06:02:36.142721Z","shell.execute_reply":"2022-03-02T06:02:36.153186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TabNetEncoder(tf.keras.layers.Layer):\n    def __init__(self,\n                 units: int=1,\n                 n_steps: int=3,\n                 n_features: int=8,\n                 outputs: int=1,\n                 gamma: float=1.3,\n                 epsilon: float=1e-8,\n                 sparsity: float=1e-5,\n                 virtual_batch_size: Optional[int]=128,\n                 momentum: Optional[float]=0.02):\n        \n        super().__init__()\n        self.units = units \n        self.n_steps = n_steps \n        self.n_features = n_features \n        self.virtual_batch_size = virtual_batch_size \n        self.gamma = gamma \n        self.epsilon = epsilon \n        self.momentum = momentum \n        self.sparsity = sparsity \n\n    def build(self, input_shape: tf.TensorShape):\n        self.bn = tf.keras.layers.BatchNormalization(virtual_batch_size=self.virtual_batch_size,\n                                                     momentum=self.momentum)\n        self.shared_block = FeatureTransformerBlock(units=self.n_features,\n                                                     virtual_batch_size=self.virtual_batch_size,\n                                                     momentum=self.momentum)\n        self.initial_step = TabNetStep(units=self.n_features,\n                                       virtual_batch_size=self.virtual_batch_size,\n                                       momentum=self.momentum)\n        self.steps = [TabNetStep(units=self.n_features,\n                                 virtual_batch_size=self.virtual_batch_size,\n                                 momentum=self.momentum) for _ in range(self.n_steps)]\n        self.final = tf.keras.layers.Dense(units=self.units, use_bias=False)\n\n    def call(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool]=None) -> Tuple[tf.Tensor]:\n        entropy_loss = 0. \n        encoded = 0. \n        output = 0. \n        importance = 0. \n        prior = tf.reduce_mean(tf.ones_like(X), axis=0)\n        \n        B = prior * self.bn(X, training=training)\n        shared = self.shared_block(B, training=training)\n        _, masked, keys = self.initial_step(B, shared, prior, training=training)\n\n        for step in self.steps:\n            entropy_loss += tf.reduce_mean(tf.reduce_sum(-keys * tf.math.log(keys + self.epsilon), axis=-1)) / tf.cast(self.n_steps, tf.float32)\n            prior *= (self.gamma - tf.reduce_mean(keys, axis=0))\n            importance += keys \n\n            shared = self.shared_block(masked, training=training)\n            split, masked, keys = step(B, shared, prior, training=training)\n            features = tf.keras.activations.relu(split)\n\n            output += features \n            encoded += split \n\n        self.add_loss(self.sparsity * entropy_loss)\n        prediction = self.final(output)\n        return prediction, encoded, importance ","metadata":{"execution":{"iopub.status.busy":"2022-03-02T06:02:36.155503Z","iopub.execute_input":"2022-03-02T06:02:36.156214Z","iopub.status.idle":"2022-03-02T06:02:36.173672Z","shell.execute_reply.started":"2022-03-02T06:02:36.156083Z","shell.execute_reply":"2022-03-02T06:02:36.172907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2 Supervised Learning","metadata":{}},{"cell_type":"code","source":"#@tf.function\n#def identity(x):\n#    return x\n\nclass TabNetRegressor(tf.keras.Model):\n    def __init__(self,\n                 outputs: int=1,\n                 n_steps: int=3,\n                 n_features: int=3,\n                 gamma: float=1.3,\n                 epsilon: float=1e-8,\n                 sparsity: float=1e-5,\n                 #feature_column: Optional[tf.keras.layers.DenseFeatures]=None,\n                 pretrained_encoder: Optional[tf.keras.layers.Layer]=None,\n                 virtual_batch_size: Optional[int]=128,\n                 momentum: Optional[float]=0.02):\n        \n        super().__init__()\n        self.outputs = outputs \n        self.n_steps = n_steps \n        self.n_features = n_features \n        #self.feature_column = feature_column \n        self.pretrained_encoder = pretrained_encoder \n        self.virtual_batch_size = virtual_batch_size \n        self.gamma = gamma \n        self.epsilon = epsilon \n        self.momentum = momentum \n        self.sparsity = sparsity \n\n        #if feature_column is None:\n        #    self.feature = tf.keras.layers.Lambda(identity)\n        #else:\n        #    self.feature = feature_column \n\n        if pretrained_encoder is None:\n            self.encoder = TabNetEncoder(units=outputs,\n                                         n_steps=n_steps,\n                                         n_features=n_features,\n                                         outputs=outputs,\n                                         gamma=gamma,\n                                         epsilon=epsilon,\n                                         sparsity=sparsity,\n                                         virtual_batch_size=self.virtual_batch_size,\n                                         momentum=momentum)\n        else:\n            self.encoder = pretrained_encoder \n\n    def forward(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool]=None) -> Tuple[tf.Tensor]:\n        #X = self.feature(X)\n        output, encoded, importance = self.encoder(X)\n\n        prediction = tf.keras.activations.sigmoid(output) # for a Classifier\n        #prediction = output # for a Regressor\n        return prediction, encoded, importance \n    \n    def call(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool]=None) -> tf.Tensor:\n        prediction, _, _ = self.forward(X)\n        return prediction \n\n    def transform(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool]=None) -> tf.Tensor:\n        _, encoded, _ = self.forward(X)\n        return encoded \n\n    def explain(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool]=None) -> tf.Tensor:\n        _, _, importance = self.forward(X)\n        return importance ","metadata":{"execution":{"iopub.status.busy":"2022-03-02T06:02:36.176586Z","iopub.execute_input":"2022-03-02T06:02:36.176788Z","iopub.status.idle":"2022-03-02T06:02:36.191295Z","shell.execute_reply.started":"2022-03-02T06:02:36.176763Z","shell.execute_reply":"2022-03-02T06:02:36.190529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = {'regressor_lr': 1e-3,\n          'epochs': 1,\n          'batch_size': 256,\n          'virtual_batch_size': 128,\n          }\n\nm = TabNetRegressor(outputs=1,\n                     n_steps=3,\n                     n_features=2,\n                     virtual_batch_size=config['virtual_batch_size'])\n\nm.compile(tf.keras.optimizers.Adam(learning_rate=config['regressor_lr']),\n          tf.keras.losses.mean_squared_error)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T06:02:36.192537Z","iopub.execute_input":"2022-03-02T06:02:36.192911Z","iopub.status.idle":"2022-03-02T06:02:36.212691Z","shell.execute_reply.started":"2022-03-02T06:02:36.192861Z","shell.execute_reply":"2022-03-02T06:02:36.212067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m.fit(train_ds, validation_data=valid_ds, epochs=config['epochs'])","metadata":{"execution":{"iopub.status.busy":"2022-03-02T06:02:36.21388Z","iopub.execute_input":"2022-03-02T06:02:36.21413Z","iopub.status.idle":"2022-03-02T06:03:36.139391Z","shell.execute_reply.started":"2022-03-02T06:02:36.214098Z","shell.execute_reply":"2022-03-02T06:03:36.138393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m.summary()","metadata":{"execution":{"iopub.status.busy":"2022-03-02T06:03:36.143129Z","iopub.execute_input":"2022-03-02T06:03:36.143452Z","iopub.status.idle":"2022-03-02T06:03:36.166449Z","shell.execute_reply.started":"2022-03-02T06:03:36.143416Z","shell.execute_reply":"2022-03-02T06:03:36.165836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importance_ds = tf.data.Dataset.from_tensor_slices(encoded_X_train)\nimportance_ds = importance_ds.shuffle(buffer_size=len(importance_ds))\nimportance_ds = importance_ds.batch(batch_size=config['batch_size'], drop_remainder=True)\n\nimportance = m.explain(list(importance_ds.take(1))[0]).numpy()\n\nimportant_scores = np.zeros_like(importance)\n\nfor data in importance_ds.take(100):\n    importance = m.explain(data).numpy()\n    important_scores = np.concatenate([important_scores, importance])\n\npd.Series(important_scores.mean(axis=0), index=encoded_X_train_df.columns).plot.bar(title='Global Importances')","metadata":{"execution":{"iopub.status.busy":"2022-03-02T06:03:36.170285Z","iopub.execute_input":"2022-03-02T06:03:36.172285Z","iopub.status.idle":"2022-03-02T06:03:46.060336Z","shell.execute_reply.started":"2022-03-02T06:03:36.172246Z","shell.execute_reply":"2022-03-02T06:03:46.059669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.3 Unsupervised Pretraining","metadata":{}},{"cell_type":"code","source":"class TabNetDecoder(tf.keras.layers.Layer):\n    def __init__(self,\n                 units=1,\n                 n_steps=3,\n                 n_features=8,\n                 outputs=1,\n                 gamma=1.3,\n                 epsilon=1e-8,\n                 sparsity=1e-5,\n                 virtual_batch_size=128,\n                 momentum=0.02):\n        \n        super().__init__()\n        self.units = units \n        self.n_steps = n_steps \n        self.n_features = n_features \n        self.virtual_batch_size = virtual_batch_size \n        self.momentum = momentum \n\n    def build(self, input_shape: tf.TensorShape):\n        self.shared_block = FeatureTransformerBlock(units=self.n_features,\n                                                  virtual_batch_size=self.virtual_batch_size,\n                                                  momentum=self.momentum)\n        self.steps = [FeatureTransformerBlock(units=self.n_features,\n                                             virtual_batch_size=self.virtual_batch_size,\n                                             momentum=self.momentum) for _ in range(self.n_steps)]\n        self.fc = [tf.keras.layers.Dense(units=self.units) for _ in range(self.n_steps)]\n\n    def call(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool]=None) -> tf.Tensor:\n        decoded = 0. \n\n        for ftb, fc in zip(self.steps, self.fc):\n            shared = self.shared_block(X, training=training)\n            feature = ftb(shared, training=training)\n            output = fc(feature)\n            \n            decoded += output \n        return decoded ","metadata":{"execution":{"iopub.status.busy":"2022-03-02T06:03:46.064027Z","iopub.execute_input":"2022-03-02T06:03:46.064224Z","iopub.status.idle":"2022-03-02T06:03:46.074275Z","shell.execute_reply.started":"2022-03-02T06:03:46.064199Z","shell.execute_reply":"2022-03-02T06:03:46.073359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TabNetAutoencoder(tf.keras.Model):\n    def __init__(self,\n                 outputs: int=1,\n                 inputs: int=12,\n                 n_steps: int=3,\n                 n_features: int=8,\n                 gamma: float=1.3,\n                 epsilon: float=1e-8,\n                 sparsity: float=1e-5,\n                 #feature_column: Optional[tf.keras.layers.DenseFeatures]=None,\n                 virtual_batch_size: Optional[int]=128,\n                 momentum: Optional[float]=0.02):\n        \n        super().__init__()\n        self.outputs = outputs \n        self.inputs = inputs \n        self.n_steps = n_steps \n        self.n_features = n_features \n        #self.feature_column = feature_column \n        self.virtual_batch_size = virtual_batch_size \n        self.gamma = gamma \n        self.esplison = epsilon \n        self.momentum = momentum \n        self.sparsity = sparsity \n        \n        #if feature_column is None:\n        #    self.feature = tf.keras.layers.Lambda(identity)\n        #else:\n        #    self.feature = feature_column \n            \n        self.encoder = TabNetEncoder(units=outputs,\n                                     n_steps=n_steps,\n                                     n_features=n_features,\n                                     outputs=outputs,\n                                     gamma=gamma,\n                                     epsilon=epsilon,\n                                     sparsity=sparsity,\n                                     virtual_batch_size=self.virtual_batch_size,\n                                     momentum=momentum)\n        self.decoder = TabNetDecoder(units=inputs,\n                                     n_steps=n_steps,\n                                     n_features=n_features,\n                                     virtual_batch_size=self.virtual_batch_size,\n                                     momentum=momentum)\n        self.bn = tf.keras.layers.BatchNormalization(virtual_batch_size=self.virtual_batch_size,\n                                                     momentum=momentum)\n        self.do = tf.keras.layers.Dropout(0.25)\n        \n    def forward(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool]=None) -> Tuple[tf.Tensor]:\n        #X = self.feature(X)\n        X = self.bn(X)\n\n        # training mask\n        M = self.do(tf.ones_like(X), training=training)\n        D = X * M\n\n        # encoder \n        output, encoded, importance = self.encoder(D)\n        prediction = tf.keras.activations.sigmoid(output) # for a Classifier\n        #prediction = output # for a Regressor\n\n        return prediction, encoded, importance, X, M \n\n    def call(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool]=None) -> tf.Tensor:\n        # encode\n        prediction, encoded, _, X, M = self.forward(X)\n        T = X * (1 - M)\n\n        # decode \n        reconstruction = self.decoder(encoded)\n\n        # loss\n        loss = tf.reduce_mean(tf.where(M != 0., tf.square(T - reconstruction), tf.zeros_like(reconstruction)))\n\n        self.add_loss(loss)\n        return prediction \n\n    def transform(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool]=None) -> tf.Tensor:\n        _, encoded, _, _, _ = self.forward(X)\n        return encoded \n\n    def explain(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool]=None) -> tf.Tensor:\n        _, _, importance, _, _ = self.forward(X)\n        return importance ","metadata":{"execution":{"iopub.status.busy":"2022-03-02T06:03:46.076035Z","iopub.execute_input":"2022-03-02T06:03:46.076599Z","iopub.status.idle":"2022-03-02T06:03:46.09523Z","shell.execute_reply.started":"2022-03-02T06:03:46.076565Z","shell.execute_reply":"2022-03-02T06:03:46.094559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@tf.function \ndef dummy_loss(y, t):\n    return 0.","metadata":{"execution":{"iopub.status.busy":"2022-03-02T06:03:46.096568Z","iopub.execute_input":"2022-03-02T06:03:46.096872Z","iopub.status.idle":"2022-03-02T06:03:46.106778Z","shell.execute_reply.started":"2022-03-02T06:03:46.096838Z","shell.execute_reply":"2022-03-02T06:03:46.106057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = {'regressor_lr': 1e-3,\n          'ae_lr': 5e-4,\n          'epochs': 3,\n          'batch_size': 256,\n          'virtual_batch_size': 128,\n          }\n\nae = TabNetAutoencoder(outputs=1,\n                       inputs=21,\n                       n_steps=3,\n                       n_features=2,\n                       virtual_batch_size=config['virtual_batch_size'])\n\nae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=config['ae_lr']),\n           loss=dummy_loss)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T06:03:46.108263Z","iopub.execute_input":"2022-03-02T06:03:46.108562Z","iopub.status.idle":"2022-03-02T06:03:46.127438Z","shell.execute_reply.started":"2022-03-02T06:03:46.108527Z","shell.execute_reply":"2022-03-02T06:03:46.126821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ae.fit(train_ds, epochs=config['epochs'])","metadata":{"execution":{"iopub.status.busy":"2022-03-02T06:03:46.12862Z","iopub.execute_input":"2022-03-02T06:03:46.128856Z","iopub.status.idle":"2022-03-02T06:06:19.786444Z","shell.execute_reply.started":"2022-03-02T06:03:46.128824Z","shell.execute_reply":"2022-03-02T06:06:19.785737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ae.summary()","metadata":{"execution":{"iopub.status.busy":"2022-03-02T06:06:19.787783Z","iopub.execute_input":"2022-03-02T06:06:19.788056Z","iopub.status.idle":"2022-03-02T06:06:19.80713Z","shell.execute_reply.started":"2022-03-02T06:06:19.788022Z","shell.execute_reply":"2022-03-02T06:06:19.806388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"important_scores = np.zeros_like(importance)\n\nfor data in importance_ds.take(100):\n    importance = ae.explain(data).numpy()\n    important_scores = np.concatenate([important_scores, importance])\n\npd.Series(important_scores.mean(axis=0), index=encoded_X_train_df.columns).plot.bar(title='Global Importances')","metadata":{"execution":{"iopub.status.busy":"2022-03-02T06:06:19.808254Z","iopub.execute_input":"2022-03-02T06:06:19.808583Z","iopub.status.idle":"2022-03-02T06:06:28.821409Z","shell.execute_reply.started":"2022-03-02T06:06:19.808546Z","shell.execute_reply":"2022-03-02T06:06:28.820747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.4 Self-supervised Fine-tuning","metadata":{}},{"cell_type":"code","source":"config = {'cregressor_lr': 1e-3,\n          'ae_lr': 5e-4,\n          'ft_lr': 2e-4,\n          'epochs': 3,\n          'batch_size': 256,\n          'virtual_batch_size': 128,\n          }\n\npm = TabNetRegressor(outputs=1,\n                      n_steps=3,\n                      n_features=2,\n                      pretrained_encoder=ae.layers[0],\n                      virtual_batch_size=128)\n\npm.compile(tf.keras.optimizers.Adam(learning_rate=config['ft_lr']),\n           tf.keras.losses.mean_squared_error)\n\npm.fit(train_ds, validation_data=valid_ds, epochs=config['epochs'])","metadata":{"execution":{"iopub.status.busy":"2022-03-02T06:06:28.822493Z","iopub.execute_input":"2022-03-02T06:06:28.823202Z","iopub.status.idle":"2022-03-02T06:09:39.393505Z","shell.execute_reply.started":"2022-03-02T06:06:28.823163Z","shell.execute_reply":"2022-03-02T06:09:39.392775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"important_scores = np.zeros_like(importance)\nfor data in importance_ds.take(100):\n    importance = pm.explain(data).numpy()\n    important_scores = np.concatenate([important_scores, importance])\n\npd.Series(important_scores.mean(axis=0), index=encoded_X_train_df.columns).plot.bar(title='Global Importances')","metadata":{"execution":{"iopub.status.busy":"2022-03-02T06:09:39.394776Z","iopub.execute_input":"2022-03-02T06:09:39.395113Z","iopub.status.idle":"2022-03-02T06:09:48.073443Z","shell.execute_reply.started":"2022-03-02T06:09:39.395075Z","shell.execute_reply":"2022-03-02T06:09:48.072741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Prediction & Submit","metadata":{}},{"cell_type":"code","source":"test_ds = tf.data.Dataset.from_tensor_slices(encoded_X_test_df)\ntest_ds = test_ds.batch(batch_size=config['batch_size'], drop_remainder=True)\n\npred = pm.predict(test_ds)\n\nif len(pred) != len(submission_df):\n    test_reminder_df = encoded_X_test_df[-(config['batch_size']):]\n    test_reminder_ds = tf.data.Dataset.from_tensor_slices(test_reminder_df)\n    test_reminder_ds = test_reminder_ds.batch(config['batch_size'])\n    \n    reminder_pred = pm.predict(test_reminder_ds)\n    \n    reminder_num = len(encoded_X_test_df) - len(test_ds) * config['batch_size']\n    reminder_pred = reminder_pred[-reminder_num:]\n    \n    pred = np.concatenate([pred, reminder_pred], axis=0)\n\n\n#submission_df['congestion'] = (pred * congestion_std) + congestion_mean #Standardization\nsubmission_df['congestion'] = pred * (c_max - c_min) + c_min #min-max scaling\n\nsubmission_df.to_csv('submission.csv', index=False)\nsubmission_df.tail()","metadata":{"execution":{"iopub.status.busy":"2022-03-02T06:09:48.074538Z","iopub.execute_input":"2022-03-02T06:09:48.074775Z","iopub.status.idle":"2022-03-02T06:09:50.770396Z","shell.execute_reply.started":"2022-03-02T06:09:48.07474Z","shell.execute_reply":"2022-03-02T06:09:50.769725Z"},"trusted":true},"execution_count":null,"outputs":[]}]}