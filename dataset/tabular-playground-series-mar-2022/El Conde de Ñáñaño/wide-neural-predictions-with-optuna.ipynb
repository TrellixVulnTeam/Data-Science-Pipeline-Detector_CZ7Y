{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Wide Data For Geographic Information\n\nThe data in this competition is connected through both location and time.  Many notebooks make good use of the time series aspect of this competition, but I have not see n any that try to make geographic connections between the 65 x-y/direction.  This notebook is my attempt to try this. \n\nInstead of the rows being a timestamp-direction pair with the target being the congestion, my data are transformed to be only a timestamp with the targets being the congestion of all 65 x-y/direction combinations.  The features I use are the 1 day lag, row-wise statistics (mean, var, etc) of the 1 day lag features, and the daily moving median.  ","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport tensorflow as tf\nimport optuna","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-25T11:07:22.505973Z","iopub.execute_input":"2022-03-25T11:07:22.506314Z","iopub.status.idle":"2022-03-25T11:07:28.430099Z","shell.execute_reply.started":"2022-03-25T11:07:22.506237Z","shell.execute_reply":"2022-03-25T11:07:28.429399Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_y = pd.read_csv('/kaggle/input/tabular-playground-series-mar-2022/train.csv')\ntest_y =  pd.read_csv('/kaggle/input/tabular-playground-series-mar-2022/test.csv')\nss = pd.read_csv('/kaggle/input/tabular-playground-series-mar-2022/sample_submission.csv')\n\n\n#Scaling the TARGET values back by 100 for NN learning \nTARGET = 'congestion'\ntrain_y[TARGET] = train_y[TARGET] / 100\ntest_y[TARGET] = 0\n\n#Turning time to DateTime\ntrain_y['time'] = pd.to_datetime(train_y['time'])\ntest_y['time'] = pd.to_datetime(test_y['time'])\nss['time'] = test_y['time']\n\n#Combing all location features \ntrain_y['xydir'] = train_y['x'].astype(str) + '_' + train_y['y'].astype(str) + train_y['direction']\ntest_y['xydir'] = test_y['x'].astype(str) + '_' + test_y['y'].astype(str) + test_y['direction']\nss['xydir'] = test_y['xydir']","metadata":{"execution":{"iopub.status.busy":"2022-03-25T11:07:28.431717Z","iopub.execute_input":"2022-03-25T11:07:28.431946Z","iopub.status.idle":"2022-03-25T11:07:31.071484Z","shell.execute_reply.started":"2022-03-25T11:07:28.431912Z","shell.execute_reply":"2022-03-25T11:07:31.070778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Turning the data from `long` to `wide`\ntrain_y = train_y[['time','xydir','congestion']].set_index(['time','xydir'])\ntrain_y = train_y.unstack()\ntrain_y.columns = train_y.columns.map(lambda x: x[1])\ntrain_y = train_y.reset_index()\n\ntest_y = test_y[['time','xydir','congestion']].set_index(['time','xydir'])\ntest_y = test_y.unstack()\ntest_y.columns = test_y.columns.map(lambda x: x[1])\ntest_y = test_y.reset_index()","metadata":{"execution":{"iopub.status.busy":"2022-03-25T11:07:31.072836Z","iopub.execute_input":"2022-03-25T11:07:31.073073Z","iopub.status.idle":"2022-03-25T11:07:31.40776Z","shell.execute_reply.started":"2022-03-25T11:07:31.073041Z","shell.execute_reply":"2022-03-25T11:07:31.406964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_y.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-25T11:07:36.724866Z","iopub.execute_input":"2022-03-25T11:07:36.725661Z","iopub.status.idle":"2022-03-25T11:07:36.759988Z","shell.execute_reply.started":"2022-03-25T11:07:36.725611Z","shell.execute_reply":"2022-03-25T11:07:36.759328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nFeature Engineering\n'''\n#Feature lists\nRAW_FEATURES = [feat for feat in train_y.columns if feat != 'time']\nHM_MOVING_MEDIANS = [f'HMMM_{feat}' for feat in RAW_FEATURES] #For daily moving medians\nDHM_MOVING_MEDIANS = [f'DHMMM_{feat}' for feat in RAW_FEATURES] #For weekly moving medians\nONE_HOUR_MEAN = ['1hm'+feat for feat in RAW_FEATURES] # for mean over the hour\n\n#Creating day, hour, and minute features\ntrain_y['day'] = train_y.time.dt.day % 7\ntrain_y['hour'] =train_y.time.dt.hour\ntrain_y['minute'] = train_y.time.dt.minute\ntrain_y['dhm'] = train_y['day'].astype(str) + '_' + train_y['hour'].astype(str) + '_' + train_y['minute'].astype(str)\ntrain_y['hm'] = train_y['hour'].astype(str) + '_' + train_y['minute'].astype(str)\ntrain_y['day'] = train_y['day'] / 7\ntrain_y['hour'] =train_y['hour'] / 24\ntrain_y['minute'] = train_y['minute']/40\n\n#1) Row wise mean, median, var, kurtosis\ntrain_y['row_kurtosis'] = train_y[RAW_FEATURES].kurtosis(axis=1)\ntrain_y['row_mean'] = train_y[RAW_FEATURES].mean(axis=1)\ntrain_y['row_median'] = train_y[RAW_FEATURES].median(axis=1)\ntrain_y['row_var'] = train_y[RAW_FEATURES].var(axis=1)\n\n#2) moving median\n#help from https://stackoverflow.com/questions/36969174/pandas-average-value-for-the-past-n-days\n#train_y[HM_MOVING_MEDIANS] = train_y.groupby('hm')[RAW_FEATURES].apply(lambda x: x.shift().expanding(min_periods=1).median())\ntrain_y[HM_MOVING_MEDIANS] = train_y.groupby('hm')[RAW_FEATURES].apply(lambda x: x.expanding(min_periods=1).median())\n\n#3) weekly moving median\ntrain_y[DHM_MOVING_MEDIANS] = train_y.groupby('dhm')[RAW_FEATURES].apply(lambda x: x.expanding(min_periods=1).median())\n\n#4) 1 hour mean\ntrain_y[ONE_HOUR_MEAN] = train_y[RAW_FEATURES].rolling(3).mean().fillna(-1)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-24T15:20:02.35259Z","iopub.execute_input":"2022-03-24T15:20:02.352892Z","iopub.status.idle":"2022-03-24T15:20:04.825325Z","shell.execute_reply.started":"2022-03-24T15:20:02.352858Z","shell.execute_reply":"2022-03-24T15:20:04.824544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nCreating train_X and test_X\n----------------------------\n\ntrain_X is the lag -1 day features and train_y is just the present day\ntest_X, also, is one day behind test_y\n'''\ntrain_X = train_y[['time']].copy()\ntrain_X['time'] = train_X.time - pd.Timedelta(days=1)\ntrain_X = pd.merge(train_X, train_y, on='time', how='left')\n\ntest_X = test_y[['time']].copy()\ntest_X['time'] = test_X.time - pd.Timedelta(days=1)\ntest_X = pd.merge(test_X, train_y, on='time', how='left')\n\n#Discarding all Nan rows from train_X, train_y\nmissing = train_X['0_0EB'].isnull()\ntrain_X = train_X[~missing].reset_index(drop=True)\ntrain_y = train_y[~missing].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T15:20:04.827462Z","iopub.execute_input":"2022-03-24T15:20:04.827751Z","iopub.status.idle":"2022-03-24T15:20:04.941521Z","shell.execute_reply.started":"2022-03-24T15:20:04.827717Z","shell.execute_reply":"2022-03-24T15:20:04.940746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Use train time stamps 1 week before test as validation data\nval_times = test_y.time - pd.Timedelta(days=7) \nval_msk = train_X.time.isin(val_times)\n\nval_X = train_X[val_msk].copy()\nval_y = train_y[val_msk].copy()\n\ntrain_X = train_X[~val_msk].copy()\ntrain_y = train_y[~val_msk].copy()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T15:20:04.943049Z","iopub.execute_input":"2022-03-24T15:20:04.943366Z","iopub.status.idle":"2022-03-24T15:20:04.979094Z","shell.execute_reply.started":"2022-03-24T15:20:04.943327Z","shell.execute_reply":"2022-03-24T15:20:04.978097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_vanilla_nn(FEATURES, RAW_FEATURES, DROPOUT_LEVEL, FIRST_DROPOUT, NUM_NEURONS, NUM_HIDDEN_LAYERS, lr):\n    INPUT_SHAPE, OUTPUT_SHAPE = len(FEATURES), len(RAW_FEATURES) \n    inp = tf.keras.layers.Input(shape = (INPUT_SHAPE,) )\n\n    x = tf.keras.layers.BatchNormalization()(inp)\n    x = tf.keras.layers.Dropout(FIRST_DROPOUT)(x)\n\n    for i in range(NUM_HIDDEN_LAYERS):\n        x = tf.keras.layers.Dense(NUM_NEURONS[i], activation='relu')(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Dropout(DROPOUT_LEVEL)(x)\n\n    x = tf.keras.layers.Dense(OUTPUT_SHAPE, activation= 'sigmoid')(x)\n\n    model = tf.keras.Model(inputs=inp, outputs= x)\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n                  loss=tf.keras.losses.MeanSquaredError())\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-03-24T15:20:04.980641Z","iopub.execute_input":"2022-03-24T15:20:04.980975Z","iopub.status.idle":"2022-03-24T15:20:04.992221Z","shell.execute_reply.started":"2022-03-24T15:20:04.980935Z","shell.execute_reply":"2022-03-24T15:20:04.991412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below is my optuna trial to find both optimal features and optimal hyperparameters for the neural network.  Uncomment if you want to perform this yourself.  It takes around 1 hour for 100 trials","metadata":{}},{"cell_type":"code","source":"'''\ndef objective(trial):\n    ###################################\n    # Generate our trial model.\n    ###################################\n    #Model Architecture specifications\n    NN_params = {}\n    FEATURES = RAW_FEATURES.copy()\n    if trial.suggest_categorical(\"hm_mm\", [True, False]):\n        FEATURES += HM_MOVING_MEDIANS\n    if trial.suggest_categorical(\"dhm_mm\", [True, False]):\n        FEATURES += DHM_MOVING_MEDIANS\n    if trial.suggest_categorical(\"one_hour_mean\", [True, False]):\n        FEATURES += ONE_HOUR_MEAN\n    if trial.suggest_categorical(\"row_stats\", [True, False]):\n        FEATURES += ['row_kurtosis', 'row_mean', 'row_median', 'row_var']\n    if trial.suggest_categorical(\"time_base\", [True, False]):\n        FEATURES += ['day', 'hour', 'minute']\n    NN_params['FEATURES'] = FEATURES\n    NN_params['RAW_FEATURES'] = RAW_FEATURES\n    NN_params[\"DROPOUT_LEVEL\"] = trial.suggest_float(\"dropout\", 0.00,0.6)\n    NN_params[\"FIRST_DROPOUT\"] = trial.suggest_float(\"first_dropout\", 0.00,0.9)\n    NN_params[\"NUM_HIDDEN_LAYERS\"] = trial.suggest_int(\"depth\", 1,8)\n    NN_params[\"lr\"] = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True)\n\n    num_neurons = []\n    for i in range(NN_params[\"NUM_HIDDEN_LAYERS\"]):\n        if i==0:\n            num_neurons.append(trial.suggest_int(f\"num_neurons_l{i}\", len(RAW_FEATURES),5000))\n        else:\n            num_neurons.append(trial.suggest_int(f\"num_neurons_l{i}\", len(RAW_FEATURES), num_neurons[-1]))\n    NN_params[\"NUM_NEURONS\"] = num_neurons\n    \n    model = create_vanilla_nn(**NN_params)\n    BATCH_SIZE = 256\n    ES = tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss', min_delta=0, patience=20, restore_best_weights=True)\n\n    H = model.fit(train_X[NN_params[\"FEATURES\"]], train_y[RAW_FEATURES], batch_size= BATCH_SIZE, epochs=200, \n              validation_split=.05, callbacks = [ES], verbose = 0)\n    \n    \n        #Val Score\n    val_preds = model.predict(val_X[NN_params[\"FEATURES\"]], batch_size=1000)\n    score = 100 * np.mean(np.abs(val_y[RAW_FEATURES].values - val_preds))\n    \n    \n    #Stopping Memory Leaks\n    del model\n    tf.keras.backend.clear_session()\n\n    return score\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=100)\n\nprint(\"Study statistics: \")\nprint(\"  Number of finished trials: \", len(study.trials))\n\nprint(\"Best trial:\")\ntrial = study.best_trial\n\nprint(\"  Value: \", trial.value)\n\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))\n'''","metadata":{"execution":{"iopub.status.busy":"2022-03-24T15:20:04.993801Z","iopub.execute_input":"2022-03-24T15:20:04.994288Z","iopub.status.idle":"2022-03-24T15:20:05.004961Z","shell.execute_reply.started":"2022-03-24T15:20:04.994244Z","shell.execute_reply":"2022-03-24T15:20:05.004108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FEATURES = RAW_FEATURES.copy() + HM_MOVING_MEDIANS + ONE_HOUR_MEAN + ['row_kurtosis', 'row_mean', 'row_median', 'row_var'] + ['day', 'hour', 'minute']\nparams = {'FEATURES': FEATURES, \n          'RAW_FEATURES': RAW_FEATURES, \n          'DROPOUT_LEVEL': 0.12292871613987645, \n          'FIRST_DROPOUT': 0.4302064801787497, \n          'NUM_NEURONS': [4526], \n          'NUM_HIDDEN_LAYERS': 1,\n          'lr': 0.0030322842074454745, }\n\n#Run the model 10 times and average preds\nn_folds = 10\nval_cum = 0\nfor i in range(n_folds):\n    tf.keras.backend.clear_session()\n    model = create_vanilla_nn(**params)\n    BATCH_SIZE = 256\n    ES = tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss', min_delta=0, patience=20, restore_best_weights=True)\n\n    H = model.fit(train_X[FEATURES], train_y[RAW_FEATURES], batch_size= BATCH_SIZE, epochs=200, \n              validation_data=(val_X[FEATURES], val_y[RAW_FEATURES]), callbacks = [ES], verbose=0)\n    val_preds = model.predict(val_X[FEATURES], batch_size=1000)\n    test_y[RAW_FEATURES] += model.predict(test_X[FEATURES]) / n_folds      \n\n    #Val Score\n    val_cum += val_preds / n_folds\n    val_score = 100 * np.mean(np.abs(val_y[RAW_FEATURES].values - val_preds))\n    print(f'FOLD {i+1} of {n_folds}: {val_score}')\n\ncumulative_score = 100 * np.mean(np.abs(val_y[RAW_FEATURES].values - val_cum))\nprint(f'Cumulative validation: {cumulative_score}')","metadata":{"execution":{"iopub.status.busy":"2022-03-24T15:20:05.006548Z","iopub.execute_input":"2022-03-24T15:20:05.006857Z","iopub.status.idle":"2022-03-24T15:22:52.404057Z","shell.execute_reply.started":"2022-03-24T15:20:05.006803Z","shell.execute_reply":"2022-03-24T15:22:52.403244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Getting Predictions back to regular shape from `wide`\ntest_y = pd.melt(test_y, id_vars = 'time', value_vars=RAW_FEATURES, var_name = 'xydir',value_name = 'congestion')\nss = pd.merge(ss[['row_id', 'time','xydir']], test_y, on = ['time','xydir'], how='left')","metadata":{"execution":{"iopub.status.busy":"2022-03-24T15:22:52.406293Z","iopub.execute_input":"2022-03-24T15:22:52.40671Z","iopub.status.idle":"2022-03-24T15:22:52.420748Z","shell.execute_reply.started":"2022-03-24T15:22:52.406657Z","shell.execute_reply":"2022-03-24T15:22:52.419988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#####################\n#Post Processing\n#####################\n#Idea from https://www.kaggle.com/code/ambrosm/tpsmar22-generalizing-the-special-values\ntrain_melt = pd.melt(train_y, id_vars = 'time', value_vars=RAW_FEATURES, var_name = 'xydir',value_name = 'congestion')\ntrain_melt\n\nlast_month = ss.time[0] - pd.Timedelta(days=60)\nlm = train_melt[train_melt.time >last_month].copy()\nlm['day_of_week'] = lm.time.dt.day % 7\nlm['hour'] = lm.time.dt.hour\nlm['minute'] = lm.time.dt.minute\nss['day_of_week'] = ss.time.dt.day % 7\nss['hour'] = ss.time.dt.hour\nss['minute'] = ss.time.dt.minute\n\n#Getting quantiles\nlower = lm.groupby(['day_of_week','hour','minute', 'xydir']).congestion.quantile(0.15).reset_index()\nupper = lm.groupby(['day_of_week','hour','minute', 'xydir']).congestion.quantile(0.7).reset_index()\n\n#place quantiles in ss\non = ['day_of_week','hour','minute', 'xydir']\nss['lower'] = ss[on].merge(lower, on = on, how = 'left')['congestion']\nss['upper'] = ss[on].merge(upper, on = on, how = 'left')['congestion']\n\n#Clip by quantiles\nss['congestion'] = np.clip(ss['congestion'], ss['lower'], ss['upper'])\n\n#Submission\nss['congestion'] = ss['congestion'] *100 #I scored 50+ when I forgot to do this.  Watch out!\nss[['row_id','congestion']].to_csv('ss.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T15:22:52.422106Z","iopub.execute_input":"2022-03-24T15:22:52.422457Z","iopub.status.idle":"2022-03-24T15:22:52.749159Z","shell.execute_reply.started":"2022-03-24T15:22:52.422421Z","shell.execute_reply":"2022-03-24T15:22:52.748336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}