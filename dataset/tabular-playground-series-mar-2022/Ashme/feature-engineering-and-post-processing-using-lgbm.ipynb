{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Kaggle Notebook to Refer Feature Engineering Tips  \nIn generally, feature engineering and EDA are more important than hyperparameter tuning, change models to improve (generalization) score.   \n\nThen I collect kaggle notebook(code) to refer feature engineering techniques and combine these techniques to plot feature importances by using lightgbm.   ","metadata":{}},{"cell_type":"markdown","source":"## Import Library  ","metadata":{}},{"cell_type":"code","source":"import warnings\nimport os\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_absolute_error\nimport lightgbm as lgb\nimport optuna  # optimize model by Bayesian optimization\n\nimport matplotlib.pyplot as plt\n\npd.set_option('display.max_columns', 50)\nwarnings.filterwarnings(\"ignore\")\noptuna.logging.disable_default_handler()  # don't display optuna log\n\n%matplotlib inline","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Data  ","metadata":{}},{"cell_type":"code","source":"DIR = \"../input/tabular-playground-series-mar-2022\"\ntrain = pd.read_csv(os.path.join(DIR, \"train.csv\"))\ntest = pd.read_csv(os.path.join(DIR, \"test.csv\"))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering  \nReference Notebook 1: TPS_2022_03_LGBM  \n@kotrying  \nURL: https://www.kaggle.com/code/kotrying/tps-2022-03-lgbm  \n\nReference Notebook 2: TPS Mar2022 Single LGBM - LB 4.91 - Run time 91s  \n@ifashion  \nURL: https://www.kaggle.com/code/ifashion/tps-mar2022-single-lgbm-lb-4-91-run-time-91s  \n\nI combined method of feature engineering in these notebooks and plot feature importances.  ","metadata":{}},{"cell_type":"code","source":"# make flag of official holiday\n# but delete official holiday data in original notebook(ref1)\ntrain[\"time\"] = pd.to_datetime(train[\"time\"])\ntrain[\"official_holiday\"] = train[\"time\"].dt.date.astype(str).str.contains('1991-05-27|1991-07-04|1991-09-02').astype('int')\ntest[\"official_holiday\"] = 0","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_new_columns(df):\n    # make categorical features by adding original categorical features\n    # \"x\" + \"y\" + \"direction\"\n    df['region_xy'] = df['x'].astype(str) + df['y'].astype(str)\n    df['xydir'] =\\\n        df['x'].astype(str) + df['y'].astype(str) + df['direction']\n\n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_lag_features(df):\n    # target value on yesterday\n    df[\"yesterday\"] = df.groupby([\"x\", \"y\", \"direction\", \"hour\", \"minute\"])[\"congestion\"].transform(lambda x: x.shift(1))\n\n    # target value on last week\n    df[\"lastweek\"] = df.groupby([\"x\", \"y\", \"direction\", \"hour\", \"minute\"])[\"congestion\"].transform(lambda x: x.shift(7))\n\n    # target at 20min, 40min, 1h and 2f ago\n    df[\"lag_1\"] = df.groupby(\"xydir\")[\"congestion\"].shift(1)\n    for i in [2, 3, 6]:\n        df[f\"lag_{i}\"] = df.groupby(\"xydir\")['congestion'].shift(i)\n        df[f\"lag_avg{i}\"] = df.groupby(\"xydir\")[\"congestion\"].transform(lambda x: x.rolling(i).mean().shift(1))\n        df[f\"lag_std{i}\"] = df.groupby(\"xydir\")[\"congestion\"].transform(lambda x: x.rolling(i).std().shift(1))\n\n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_agg_features(df):\n    # median\n    df[\"median_cong\"] = df.groupby([\"x\", \"y\", \"direction\", \"hour\", \"minute\"])[\"congestion\"].transform(lambda x: x.median())\n\n    # rolling std in 1 week\n    # but I think this feature is leak(should shift 1)\n    df[\"rolling_7_std\"] = df.groupby([\"x\", \"y\", \"direction\", \"hour\", \"minute\"])[\"congestion\"].transform(lambda x: x.rolling(7).std().shift(1))\n\n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def transform_time(df):\n    # transform to datetime type\n    df[\"time\"] = pd.to_datetime(df[\"time\"])\n\n    # extract time features(month, day, hour, minute, weekday)\n    df[\"month\"] = df[\"time\"].dt.month\n    df[\"day\"] = df[\"time\"].dt.dayofyear\n    df[\"hour\"] = df[\"time\"].dt.hour\n    df[\"minute\"] = df[\"time\"].dt.minute\n    df[\"weekday\"] = df[\"time\"].dt.weekday\n\n    # the week of the year\n    df[\"week\"] = df[\"time\"].dt.week\n\n    # make flag of am, Saturday, and Sunday\n    df[\"am\"] = ((df[\"hour\"] < 12) & (df[\"hour\"] > 6)).astype(\"int8\")\n    df[\"Saturday\"] = (df[\"weekday\"] == 5).astype(\"int8\")\n    df[\"Sunday\"] = (df[\"weekday\"] == 6).astype(\"int8\")\n\n    # I saw this technique for the first time\n    # transform time feature(plot later)\n    df[\"time\"] = (df[\"time\"].dt.hour-12)*3 + df[\"time\"].dt.minute/20\n\n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tri_transform_time(df):\n    # extract time features(month, day, hour, minute, weekday)\n    df['time'] = pd.to_datetime(df['time'])\n    df['month'] = df['time'].dt.month\n    df['day'] = df['time'].dt.day\n    df['hour'] = df['time'].dt.hour\n    df['minute'] = df['time'].dt.minute\n    df[\"weekday\"] = df[\"time\"].dt.weekday\n\n    # the week of the year\n    df[\"week\"] = df[\"time\"].dt.week\n\n    # make flag of am, Saturday, and Sunday\n    df[\"am\"] = ((df[\"hour\"] < 12) & (df[\"hour\"] > 6)).astype(\"int8\")\n    df[\"Saturday\"] = (df[\"weekday\"] == 5).astype(\"int8\")\n    df[\"Sunday\"] = (df[\"weekday\"] == 6).astype(\"int8\")\n\n    # transform using trigonometric function\n    month_list = [4, 6, 9]\n    df[['day_sin', 'day_cos']] = 0\n    for month in df['month'].unique():\n        if month in month_list:\n            df.loc[df['month'] == month, 'day_sin'] =\\\n                df.query(\"month == @month\")['day'].apply(lambda x: np.sin((2*np.pi*x) / 30))\n            df.loc[df['month'] == month, 'day_cos'] =\\\n                df.query(\"month == @month\")['day'].apply(lambda x: np.cos((2*np.pi*x) / 30))\n        else:\n            df.loc[df['month'] == month, 'day_sin'] =\\\n                df.query(\"month == @month\")['day'].apply(lambda x: np.sin((2*np.pi*x) / 31))\n            df.loc[df['month'] == month, 'day_cos'] =\\\n                df.query(\"month == @month\")['day'].apply(lambda x: np.cos((2*np.pi*x) / 31))\n\n    df['month_sin'] = np.sin(2*np.pi*df['month'] / 12)\n    df['month_cos'] = np.cos(2*np.pi*df['month'] / 12)\n    df['hour_sin'] = np.sin(2*np.pi*df['hour'] / 24)\n    df['hour_cos'] = np.cos(2*np.pi*df['hour'] / 24)\n    df['minute_sin'] = np.sin(2*np.pi*df['minute'] / 60)\n    df['minute_cos'] = np.cos(2*np.pi*df['minute'] / 60)\n\n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocessing(train, test):\n    # store the train data size to divide train data and test data after feature engineering.\n    train_size = train.shape[0]  # len(train)\n\n    # cancatenate train data and test data\n    df = pd.concat((train, test))\n\n    # make categorical features by adding original categorical features\n    df = make_new_columns(df)\n\n    # transform time features\n    df = transform_time(df)\n\n    # lag features\n    df = make_lag_features(df)\n\n    # aggregation features\n    df = make_agg_features(df)\n\n    # categorical features are applied label encoding\n    cat_columns = [\"direction\", \"xydir\", \"region_xy\"]\n    for column in cat_columns:\n        le = LabelEncoder()\n        df[column] = le.fit_transform(df[column])\n\n    # divide train data and test data\n    train, test = df.iloc[:train_size], df.iloc[train_size:]\n\n    return train, test","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrans_train, trans_test = preprocessing(train.copy(), test.copy())\ndisplay(trans_train)\ndisplay(trans_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plot transformd time  \nIn am is negative values(-36 ~ -1.0) and in pm is positive values(1.0 ~ 35) to transform at line 42 in cell 3.  \n\nI think can ensure temporal characteristics by transforming that.  ","metadata":{}},{"cell_type":"code","source":"# extract data by xydir\nxydir = trans_train.loc[0, \"xydir\"]\nex_data = trans_train.query(\"xydir == @xydir\")\n\n# extract data by day\nmonth = ex_data.loc[0, \"month\"]\nday = ex_data.loc[0, \"day\"]\nex_data = ex_data.query(\"month == @month & day == @day\")\n\nprint(ex_data.shape)\nex_data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot time feature\nplt.figure(figsize=(12, 16))\nplt.plot(range(72), ex_data[\"time\"])\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot a 3 days\n# extract data by xydir\nxydir = trans_train.loc[0, \"xydir\"]\nex_data = trans_train.query(\"xydir == @xydir\")\n\n# extract data by day\nmonth = ex_data.loc[0, \"month\"]\nex_data = ex_data.query(\"month == @month & day < 94\")\n\nprint(ex_data.shape)\nex_data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16, 12))\nplt.plot(range(216), ex_data[\"time\"])\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train Model  \nI use lgb.LGBMRegressor and plot feature importances.  ","metadata":{}},{"cell_type":"code","source":"# divide train dataset and validatin dataset\ntrain_data, val_data, test_data =\\\n    trans_train.iloc[:trans_train.shape[0]-4680, :],\\\n    trans_train.iloc[trans_train.shape[0]-4680:trans_train.shape[0]-2340, :],\\\n    trans_train.iloc[trans_train.shape[0]-2340:, :]\ndisplay(train_data)\ndisplay(val_data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make list of use features\ncolumns = [\"x\", \"y\", \"direction\", \"official_holiday\", \"region_xy\",\n           \"xydir\", \"month\", \"day\", \"hour\", \"minute\", \"weekday\", \"week\",\n           \"am\", \"Saturday\", \"Sunday\", \"yesterday\", \"lastweek\", \"lag_1\",\n           \"lag_2\", \"lag_avg2\", \"lag_std2\", \"lag_3\", \"lag_avg3\", \"lag_std3\",\n           \"lag_6\", \"lag_avg6\", \"lag_std6\", \"median_cong\", \"rolling_7_std\"]\n\ntarget = \"congestion\"\n\ntrain_X, val_X, test_X = train_data[columns], val_data[columns], test_data[columns]\ntrain_y, val_y, test_y = train_data[target], val_data[target], test_data[target]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# optimize model by using optuna\ndef objective(trial):\n    # setting parameters\n    params = {\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 10, 500),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 20),\n        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-1),\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 10, 100),\n        \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-3, 1e-1),\n        \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-3, 1e-1),\n        \"n_jobs\": 12\n    }\n\n    # train model\n    model = lgb.LGBMRegressor(**params)\n    model.fit(train_X, train_y,\n              eval_set=(val_X, val_y),\n              early_stopping_rounds=50, verbose=0,\n              eval_metric=\"mae\")\n    pred = model.predict(val_X)\n\n    # evaluation\n    score = mean_absolute_error(val_y, pred)\n    return score\n\n\n# optimization\nstudy = optuna.create_study()\nstudy.optimize(objective, 100)\n\n# display best paramters and score\nprint(study.best_params)\nprint(study.best_value)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train model\nmodel = lgb.LGBMRegressor(**study.best_params)\nmodel.fit(train_X, train_y, eval_set=(val_X, val_y),\n          early_stopping_rounds=50, verbose=100, eval_metric=\"mae\")\n\n# predict\npred = model.predict(test_X)\n\n# round predict values\nround_pred = np.round(pred)\n\nprint(f\"validation MAE: {mean_absolute_error(test_y, pred)}\")\nprint(f\"validation MAE(round): {mean_absolute_error(test_y, round_pred)}\")","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plot feature importance  ","metadata":{}},{"cell_type":"code","source":"# get feature importances\nfeature_importances = model.feature_importances_\n# make indices to sort\nindices = np.argsort(feature_importances)\n\n# sort features importances\nfeature_importances = feature_importances[indices]\n\n# sort feature names\nsort_columns = np.array(columns)[indices]\n\n# plot feature importances\nplt.figure(figsize=(12, 12))\nplt.barh(sort_columns, feature_importances)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Make Prediction  ","metadata":{}},{"cell_type":"code","source":"train_X = trans_train[columns]\ntrain_y = trans_train[target]\n\nmodel = lgb.LGBMRegressor(**study.best_params)\nmodel.fit(train_X, train_y, eval_set=(train_X, train_y),\n          early_stopping_rounds=10, verbose=100, eval_metric=\"mae\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame(columns=[\"row_id\", \"congestion\"])\n\nregion_list = trans_test[\"xydir\"].unique()\nfor region in region_list:\n    # regionごとにDataFrameを抽出\n    train_region_df = trans_train.query(\"xydir == @region\")\n    test_region_df = trans_test.query(\"xydir == @region\")\n    train_size = train_region_df.shape[0]\n\n    region_df = pd.concat((train_region_df, test_region_df)).reset_index(drop=True)\n\n    # 時系列順に予測，予測結果を用いてラグ特徴量を計算\n    for i in range(test_region_df.shape[0]):\n        target_id = train_size + i\n\n        region_df.loc[target_id, \"lag_1\"] = region_df.loc[target_id-1, \"congestion\"]\n        for i in [2, 3, 6, 72]:\n            region_df.loc[target_id, f\"lag_{i}\"] = region_df.loc[target_id-i, \"congestion\"]\n            region_df.loc[target_id, f\"lag_avg{i}\"] = region_df.loc[target_id-i:target_id-1, \"congestion\"].mean()\n            region_df.loc[target_id, f\"lag_std{i}\"] = region_df.loc[target_id-i:target_id-1, \"congestion\"].std()\n\n        # 予測\n        pred = model.predict(region_df.loc[target_id, columns].values.reshape(1, -1))[0]\n\n        # 予測した値を提出ファイルに格納\n        submission = submission.append({\"row_id\": [region_df.loc[target_id, \"row_id\"]][0],\n                                        \"congestion\": pred}, ignore_index=True)\n        # ラグ特徴量の計算のためにDataFrameにも格納\n        region_df.loc[target_id, target] = pred\n\nsubmission = submission.sort_values(\"row_id\").reset_index(drop=True)\nsubmission[\"row_id\"] = submission[\"row_id\"].astype(int)\ndisplay(submission)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Postprocessing  \nPost process in Reference Notebook 1. MAE reduced 0.1 by using this process than don't use. \n\nThis process(clipping) seems to be important in this competittion.  ","metadata":{}},{"cell_type":"code","source":"sep = trans_train[trans_train['month'] >= 9]\nlower = sep.groupby(['time', 'x', 'y', 'direction']).congestion.quantile(0.15).values[2340:]\nupper = sep.groupby(['time', 'x', 'y', 'direction']).congestion.quantile(0.7).values[2340:]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clip_submission = submission.copy()\nclip_submission[\"congestion\"] = submission[\"congestion\"].clip(lower, upper)\ndisplay(clip_submission)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(submission[submission[\"congestion\"] != clip_submission[\"congestion\"]])\ndisplay(clip_submission[submission[\"congestion\"] != clip_submission[\"congestion\"]])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"round_submission = submission.copy()\nround_submission[\"congestion\"] = np.round(submission[\"congestion\"])\ndisplay(round_submission)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}