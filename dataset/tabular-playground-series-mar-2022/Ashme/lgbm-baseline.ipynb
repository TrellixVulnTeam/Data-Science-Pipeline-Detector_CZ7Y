{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TPS March 2022  \n- Make baseline model using LightGBM  \n- Feature engineering: lag features, rolling features, one-hot encoding  ","metadata":{}},{"cell_type":"markdown","source":"## Import Library  ","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport warnings\nfrom typing import List\nimport random\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import TimeSeriesSplit, train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer, QuantileTransformer, RobustScaler\nimport lightgbm as lgb\nimport optuna\n\npd.set_option(\"display.max_columns\", 100)\nwarnings.filterwarnings(\"ignore\")\noptuna.logging.disable_default_handler()  # Don't display optuna log\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-03-26T14:11:04.913073Z","iopub.execute_input":"2022-03-26T14:11:04.91374Z","iopub.status.idle":"2022-03-26T14:11:07.754128Z","shell.execute_reply.started":"2022-03-26T14:11:04.913647Z","shell.execute_reply":"2022-03-26T14:11:07.753401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Read Data  ","metadata":{}},{"cell_type":"code","source":"DIR = \"../input/tabular-playground-series-mar-2022\"\ntrain_df = pd.read_csv(os.path.join(DIR, \"train.csv\"))\n\nrandom_state_list = np.random.randint(0, 1e+4, size=8)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T14:11:07.756212Z","iopub.execute_input":"2022-03-26T14:11:07.756796Z","iopub.status.idle":"2022-03-26T14:11:08.604309Z","shell.execute_reply.started":"2022-03-26T14:11:07.756752Z","shell.execute_reply":"2022-03-26T14:11:08.603693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering   \n- make new features  \n- lag features  \n- moving average/std  \n- one-hot-encoding  ","metadata":{}},{"cell_type":"markdown","source":"### Functions for feature engineering","metadata":{}},{"cell_type":"code","source":"def make_new_columns(df):\n    df['region_xy'] = df['x'].astype(str) + df['y'].astype(str)\n    df['region'] =\\\n        df['x'].astype(str) + df['y'].astype(str) + df['direction']\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-03-26T14:11:08.60562Z","iopub.execute_input":"2022-03-26T14:11:08.605931Z","iopub.status.idle":"2022-03-26T14:11:08.612389Z","shell.execute_reply.started":"2022-03-26T14:11:08.605892Z","shell.execute_reply":"2022-03-26T14:11:08.611499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_lag_features(train_df, test_df=None):\n    if test_df is not None:\n        train_df[\"lag_1\"] = 0\n        test_df[\"lag_1\"] = 0\n        for i in [2, 3, 6, 72]:\n            train_df[f\"lag_{i}\"] = 0\n            train_df[f\"lag_avg{i}\"] = 0\n            train_df[f\"lag_std{i}\"] = 0\n\n            test_df[f\"lag_{i}\"] = 0\n            test_df[f\"lag_avg{i}\"] = 0\n            test_df[f\"lag_std{i}\"] = 0\n\n        region_list = train_df[\"region\"].unique()\n        train_size = train_df.shape[0]\n        lag_df = pd.DataFrame()\n        for region in region_list:\n            train_region_df = train_df.query(\"region == @region\")\n            test_region_df = test_df.query(\"region == @region\")\n\n            region_df = pd.concat((train_region_df, test_region_df)).reset_index(drop=True)\n            region_df[\"lag_1\"] = region_df[\"congestion\"].shift(1)\n\n            for i in [2, 3, 6, 72]:\n                region_df[f\"lag_{i}\"] = region_df[\"congestion\"].shift(i)\n                region_df[f\"lag_avg{i}\"] = region_df[\"congestion\"].rolling(window=i, min_periods=1).mean().shift(1)\n                region_df[f\"lag_std{i}\"] = region_df[\"congestion\"].rolling(window=i, min_periods=1).std().shift(1)\n\n            lag_df = lag_df.append(region_df)\n\n        lag_df = lag_df.sort_values(by=\"row_id\").reset_index(drop=True)\n        train_df, test_df = lag_df.iloc[:train_size], lag_df.iloc[train_size:]\n        return train_df, test_df\n\n    else:\n        train_df[\"lag_1\"] = 0\n        for i in [2, 3, 6, 72]:\n            train_df[f\"lag_{i}\"] = 0\n            train_df[f\"lag_avg{i}\"] = 0\n            train_df[f\"lag_std{i}\"] = 0\n\n        region_list = train_df[\"region\"].unique()\n        lag_df = pd.DataFrame()\n        for region in region_list:\n            # regionごとにDataFrameを抽出\n            region_df = train_df.query(\"region == @region\")\n            region_df[\"lag_1\"] = region_df[\"congestion\"].shift(1)\n\n            for i in [2, 3, 6, 72]:\n                region_df[f\"lag_{i}\"] = region_df[\"congestion\"].shift(i)\n                region_df[f\"lag_avg{i}\"] = region_df[\"congestion\"].rolling(window=i, min_periods=1).mean().shift(1)\n                region_df[f\"lag_std{i}\"] = region_df[\"congestion\"].rolling(window=i, min_periods=1).std().shift(1)\n\n            lag_df = lag_df.append(region_df)\n\n        train_df = lag_df.sort_values(by=\"row_id\").reset_index(drop=True)\n        return train_df","metadata":{"execution":{"iopub.status.busy":"2022-03-26T14:11:08.61501Z","iopub.execute_input":"2022-03-26T14:11:08.615355Z","iopub.status.idle":"2022-03-26T14:11:08.633658Z","shell.execute_reply.started":"2022-03-26T14:11:08.615311Z","shell.execute_reply":"2022-03-26T14:11:08.6329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def transform_times(df):\n    df['time'] = pd.to_datetime(df['time'])\n    df['month'] = df['time'].apply(lambda x: x.month)\n    df['day'] = df['time'].apply(lambda x: x.day)\n    df['hour'] = df['time'].apply(lambda x: x.hour)\n    df['minute'] = df['time'].apply(lambda x: x.minute)\n\n    # transform using trigonometric function\n    month_list = [4, 6, 9]\n    df[['day_sin', 'day_cos']] = 0\n    for month in df['month'].unique():\n        if month in month_list:\n            df.loc[df['month'] == month, 'day_sin'] =\\\n                df.query(\"month == @month\")['day'].apply(lambda x: np.sin((2*np.pi*x) / 30))\n            df.loc[df['month'] == month, 'day_cos'] =\\\n                df.query(\"month == @month\")['day'].apply(lambda x: np.cos((2*np.pi*x) / 30))\n        else:\n            df.loc[df['month'] == month, 'day_sin'] =\\\n                df.query(\"month == @month\")['day'].apply(lambda x: np.sin((2*np.pi*x) / 31))\n            df.loc[df['month'] == month, 'day_cos'] =\\\n                df.query(\"month == @month\")['day'].apply(lambda x: np.cos((2*np.pi*x) / 31))\n\n    df['month_sin'] = np.sin(2*np.pi*df['month'] / 12)\n    df['month_cos'] = np.cos(2*np.pi*df['month'] / 12)\n    df['hour_sin'] = np.sin(2*np.pi*df['hour'] / 24)\n    df['hour_cos'] = np.cos(2*np.pi*df['hour'] / 24)\n    df['minute_sin'] = np.sin(2*np.pi*df['minute'] / 60)\n    df['minute_cos'] = np.cos(2*np.pi*df['minute'] / 60)\n\n    df[\"weekday\"] = df[\"time\"].apply(lambda x: x.weekday())\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-03-26T14:11:08.634669Z","iopub.execute_input":"2022-03-26T14:11:08.634869Z","iopub.status.idle":"2022-03-26T14:11:08.650896Z","shell.execute_reply.started":"2022-03-26T14:11:08.634844Z","shell.execute_reply":"2022-03-26T14:11:08.650349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def aggregation(train_df, test_df=None):\n    train_df[\"region_weekday\"] = train_df[\"region\"] + train_df[\"weekday\"].astype(str)\n\n    region_mean_map = train_df.groupby(\"region\")[\"congestion\"].mean()\n    region_median_map = train_df.groupby(\"region\")[\"congestion\"].median()\n    region_std_map = train_df.groupby(\"region\")[\"congestion\"].std()\n\n    region_weekday_mean_map = train_df.groupby(\"region_weekday\")[\"congestion\"].mean()\n    region_weekday_median_map = train_df.groupby(\"region_weekday\")[\"congestion\"].median()\n    region_weekday_std_map = train_df.groupby(\"region_weekday\")[\"congestion\"].std()\n\n    train_df[\"region_target_mean\"] = train_df[\"region\"].map(region_mean_map)\n    train_df[\"region_target_median\"] = train_df[\"region\"].map(region_median_map)\n    train_df[\"region_target_std\"] = train_df[\"region\"].map(region_std_map)\n\n    train_df[\"region_weekday_target_mean\"] =\\\n        train_df[\"region_weekday\"].map(region_weekday_mean_map)\n    train_df[\"region_weekday_target_median\"] =\\\n        train_df[\"region_weekday\"].map(region_weekday_median_map)\n    train_df[\"region_weekday_target_std\"] =\\\n        train_df[\"region_weekday\"].map(region_weekday_std_map)\n\n    if test_df is not None:\n        test_df[\"region_weekday\"] = test_df[\"region\"] + test_df[\"weekday\"].astype(str)\n        test_df[\"region_target_mean\"] = test_df[\"region\"].map(region_mean_map)\n        test_df[\"region_target_median\"] = test_df[\"region\"].map(region_median_map)\n        test_df[\"region_target_std\"] = test_df[\"region\"].map(region_std_map)\n\n        test_df[\"region_weekday_target_mean\"] =\\\n            test_df[\"region_weekday\"].map(region_weekday_mean_map)\n        test_df[\"region_weekday_target_median\"] =\\\n            test_df[\"region_weekday\"].map(region_weekday_median_map)\n        test_df[\"region_weekday_target_std\"] =\\\n            test_df[\"region_weekday\"].map(region_weekday_std_map)\n\n        return train_df, test_df\n\n    return train_df","metadata":{"execution":{"iopub.status.busy":"2022-03-26T14:11:08.652184Z","iopub.execute_input":"2022-03-26T14:11:08.652399Z","iopub.status.idle":"2022-03-26T14:11:08.667412Z","shell.execute_reply.started":"2022-03-26T14:11:08.652372Z","shell.execute_reply":"2022-03-26T14:11:08.666701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def expand_feature(train_df, test_df=None):\n    expand_df = pd.DataFrame()\n    train_size = train_df.shape[0]\n    if test_df is not None:\n        train_df[\"expand_congestion\"] = 0\n        test_df[\"expand_congestion\"] = 0\n        region_list = train_df[\"region\"].unique()\n        for region in region_list:\n            train_region_df = train_df.query(\"region == @region\")\n            test_region_df = test_df.query(\"region == @region\")\n            train_region_size = train_region_df.shape[0]\n\n            region_df = pd.concat((train_region_df, test_region_df)).reset_index(drop=True)\n            region_df.loc[0, \"expand_congestion\"] = 0\n            for i in range(1, train_region_size+1):  # テストデータの1個目まで作成\n                region_df.loc[i, \"expand_congestion\"] = region_df.loc[:i-1, \"congestion\"].median()\n\n            expand_df = expand_df.append(region_df)\n\n        expand_df = expand_df.sort_values(by=\"row_id\").reset_index(drop=True)\n        train_df, test_df = expand_df.iloc[:train_size], expand_df.iloc[train_size:]\n        return train_df, test_df\n\n    else:\n        train_df[\"expand_congestion\"] = 0\n        region_list = train_df[\"region\"].unique()\n        for region in region_list:\n            region_df = train_df.query(\"region == @region\").reset_index(drop=True)\n            train_region_size = region_df.shape[0]\n\n            region_df.loc[0, \"expand_congestion\"] = 0\n            for i in range(1, train_region_size):\n                region_df.loc[i, \"expand_congestion\"] = region_df.loc[:i-1, \"congestion\"].median()\n\n            expand_df = expand_df.append(region_df)\n\n        train_df = expand_df.sort_values(by=\"row_id\").reset_index(drop=True)\n        return train_df","metadata":{"execution":{"iopub.status.busy":"2022-03-26T14:11:08.668566Z","iopub.execute_input":"2022-03-26T14:11:08.668784Z","iopub.status.idle":"2022-03-26T14:11:08.683624Z","shell.execute_reply.started":"2022-03-26T14:11:08.668759Z","shell.execute_reply":"2022-03-26T14:11:08.682816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocessing(train_df, test_df=None):\n    if test_df is None:\n        # maek new columns\n        train_df = make_new_columns(train_df)\n\n        # make lag features and expanding feature\n        train_df = make_lag_features(train_df)\n        train_df = expand_feature(train_df)\n\n        # transform time feature by using trigonometric function\n        train_df = transform_times(train_df)\n\n        # make aggrigation features\n        train_df = aggregation(train_df)\n        return train_df\n\n    else:\n        train_df = make_new_columns(train_df)\n        test_df = make_new_columns(test_df)\n\n        train_df, test_df = make_lag_features(train_df, test_df)\n        train_df, test_df = expand_feature(train_df, test_df)\n\n        train_df = transform_times(train_df)\n        test_df = transform_times(test_df)\n\n        train_df, test_df = aggregation(train_df, test_df)\n\n        return train_df, test_df","metadata":{"execution":{"iopub.status.busy":"2022-03-26T14:11:08.684877Z","iopub.execute_input":"2022-03-26T14:11:08.685719Z","iopub.status.idle":"2022-03-26T14:11:08.699027Z","shell.execute_reply.started":"2022-03-26T14:11:08.685679Z","shell.execute_reply":"2022-03-26T14:11:08.698234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrans_train = preprocessing(train_df)\ntrans_train","metadata":{"execution":{"iopub.status.busy":"2022-03-26T14:11:08.700132Z","iopub.execute_input":"2022-03-26T14:11:08.700421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## evaluate cv score  ","metadata":{}},{"cell_type":"code","source":"cat_columns = ['direction', 'region_xy', 'weekday']\n\n\nnum_columns = ['lag_1', 'lag_2', 'lag_3', 'lag_6', 'lag_72', 'lag_avg2',\n               'lag_avg3', 'lag_avg6', 'lag_avg72', 'lag_std2', 'lag_std3',\n               'lag_std6', 'lag_std72', 'month_sin', 'month_cos', 'day_sin',\n               'day_cos', 'hour_sin', 'hour_cos', 'minute_sin', 'minute_cos',\n               'region_target_mean', 'region_target_median', 'region_target_std',\n               'region_weekday_target_mean', 'region_weekday_target_median',\n               'region_weekday_target_std', 'expand_congestion']\n\nuse_columns = ['x', 'y']\ntarget = ['congestion']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# one-hot encoding\ndummy_columns = []\nfor column in cat_columns:\n    tmp_train = pd.get_dummies(trans_train[column], prefix=column, drop_first=False)\n    trans_train = pd.concat((trans_train, tmp_train), axis=1)\n\n    dummy_columns.extend(tmp_train.columns.values.tolist())\n\n# devide train data, validation data, and test data\ntrain_df, val_df, test_df =\\\n    trans_train.iloc[:trans_train.shape[0]-4680, :],\\\n    trans_train.iloc[trans_train.shape[0]-4680:trans_train.shape[0]-2340, :],\\\n    trans_train.iloc[trans_train.shape[0]-2340:, :]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"all train data shape: {trans_train.shape}\")\nprint(f\"train data shape: {train_df.shape}\")\nprint(f\"validation data shape: {val_df.shape}\")\nprint(f\"test data shape: {test_df.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective(trial):\n    # setting optimization parameters\n    params = {\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 10, 500),  # 2のmax_depth乗が良いらしい\n        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 20),\n        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-1),\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 10, 100),\n        \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-3, 1e-1),\n        \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-3, 1e-1),\n        \"metric\": \"mae\"\n    }\n\n    # train models\n    feature_columns = dummy_columns + num_columns + use_columns\n    train_X, val_X = train_df[feature_columns], val_df[feature_columns]\n    train_y, val_y = train_df[target], val_df[target]\n\n    model = lgb.LGBMRegressor(**params)\n    model.fit(train_X, train_y,\n              eval_set=(val_X, val_y),\n              early_stopping_rounds=10, verbose=0)\n    pred = model.predict(val_X)\n\n    # evaluation\n    score = mean_absolute_error(val_y, pred)\n    return score\n\n\nstudy = optuna.create_study()\nstudy.optimize(objective, 100)\nprint(study.best_params)\nprint(study.best_value)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train model using best prameters\nfeature_columns = dummy_columns + num_columns + use_columns\ntrain_X, val_X = train_df[feature_columns], val_df[feature_columns]\ntrain_y, val_y = train_df[target], val_df[target]\n\ntest_X, test_y = test_df[feature_columns], test_df[target]\n\nmodel = lgb.LGBMRegressor(**study.best_params)\nmodel.fit(train_X, train_y,\n          eval_set=(val_X, val_y),\n          early_stopping_rounds=10, verbose=10,\n          eval_metric=\"mae\")\npred = model.predict(test_X)\n\nscore = mean_absolute_error(test_y, pred)\nprint(f\"Validation Score: {score}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Make submission  ","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(os.path.join(DIR, \"train.csv\"))\ntest_df = pd.read_csv(os.path.join(DIR, \"test.csv\"))\n\ntrain_df, test_df = preprocessing(train_df, test_df)\n\nfor column in cat_columns:\n    tmp_train = pd.get_dummies(train_df[column], prefix=column, drop_first=False)\n    tmp_test = pd.get_dummies(test_df[column], prefix=column, drop_first=False)\n\n    train_df = pd.concat((train_df, tmp_train), axis=1)\n    test_df = pd.concat((test_df, tmp_test), axis=1)\n\n    dummy_columns.extend(tmp_train.columns.values.tolist())\n\nfor column in train_df.columns:\n    if column not in test_df.columns:\n        test_df[column] = 0\n\nprint(f\"train data shape: {train_df.shape}\")\nprint(f\"test data shape: {test_df.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_dict = dict()\n\ntrain_X = train_df[feature_columns]\ntrain_y = train_df[target]\n\n# train models\nfor i in range(8):\n    random_state = random_state_list[i]\n    model = lgb.LGBMRegressor(**study.best_params, random_state=random_state)\n    model.fit(train_X, train_y)\n\n    model_dict[f\"model{i}\"] = model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_dict = dict()\n\nregion_list = test_df[\"region\"].unique()\nfor i in range(8):\n    submission = pd.DataFrame(columns=[\"row_id\", \"congestion\"])\n    model = model_dict[f\"model{i}\"]\n    for region in region_list:\n        train_region_df = train_df.query(\"region == @region\")\n        test_region_df = test_df.query(\"region == @region\")\n        train_size = train_region_df.shape[0]\n\n        region_df = pd.concat((train_region_df, test_region_df)).reset_index(drop=True)\n\n        for j in range(test_region_df.shape[0]):\n            target_id = train_size + j\n\n            # calculate lag features and rolling features\n            region_df.loc[target_id, \"lag_1\"] = region_df.loc[target_id-1, \"congestion\"]\n            region_df.loc[target_id, \"expand_congestion\"] = region_df.loc[:target_id-1, \"congestion\"].median()\n            for k in [2, 3, 6, 72]:\n                region_df.loc[target_id, f\"lag_{k}\"] = region_df.loc[target_id-k, \"congestion\"]\n                region_df.loc[target_id, f\"lag_avg{k}\"] = region_df.loc[target_id-k:target_id-1, \"congestion\"].mean()\n                region_df.loc[target_id, f\"lag_std{k}\"] = region_df.loc[target_id-k:target_id-1, \"congestion\"].std()\n\n            # predict\n            pred = model.predict(region_df.loc[target_id, feature_columns].values.reshape(1, -1))[0]\n\n            submission = submission.append({\"row_id\": [region_df.loc[target_id, \"row_id\"]][0],\n                                            \"congestion\": pred}, ignore_index=True)\n            region_df.loc[target_id, target] = pred\n\n    submission = submission.sort_values(\"row_id\").reset_index(drop=True)\n    submission[\"row_id\"] = submission[\"row_id\"].astype(int)\n    submission_dict[f\"submission{i}\"] = submission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot feature importances\nfeature_importances = model.feature_importances_\ncolumns = train_X.columns\nindices = feature_importances.argsort()\n\nplt.figure(figsize=(20, 20))\nfeature_importances = feature_importances[indices]\ncolumns = columns[indices]\n\nplt.barh(columns, feature_importances)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(8):\n    submission_dict[f\"submission{i}\"].to_csv(f\"LGBEnsembleSubmission{i}.csv\", index=False)\n    display(submission_dict[f\"submission{i}\"].head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make ensemble submission(mean values)\nsubmission = np.zeros(shape=(2340))\nfor sub_df in submission_dict.values():\n    pred = sub_df[\"congestion\"].values\n    submission += pred\n\nmean_pred = submission / 8\nsubmission = submission_dict[\"submission0\"]\nsubmission[\"congestion\"] = mean_pred\nsubmission.to_csv(\"LGBMEnsembleMean.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make ensemble submission(median values)\nsubmission = np.zeros(shape=(2340))\nfor sub_df in submission_dict.values():\n    pred = sub_df[\"congestion\"].values\n    submission = np.vstack((submission, pred))\n\nsubmission = submission[1:]\nmedian_pred = np.median(submission, axis=0)\n\nsubmission = submission_dict[\"submission0\"]\nsubmission[\"congestion\"] = median_pred\nsubmission.to_csv(\"LGBMEnsembleMedian.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# transform to integer values\nsubmission[\"congestion\"] = submission[\"congestion\"].astype(int)\nsubmission.to_csv(\"LGBMEnsembleMedianInteger.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}