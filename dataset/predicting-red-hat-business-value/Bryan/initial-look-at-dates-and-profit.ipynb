{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b858c8f2-97a2-8cbd-0d86-382f99c2b6e1"},"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n%matplotlib inline\nimport matplotlib.pyplot as plt"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5b5a96ee-a843-870e-251e-ed09a80e0eab"},"outputs":[],"source":"people = pd.read_csv('../input/people.csv',\n                       dtype={'people_id': np.str,\n                              'activity_id': np.str,\n                              'char_38': np.int32},\n                       parse_dates=['date'])\nact_train = pd.read_csv('../input/act_train.csv',\n                        dtype={'people_id': np.str,\n                               'activity_id': np.str,\n                               'otcome': np.int8},\n                        parse_dates=['date'])\nact_test = pd.read_csv('../input/act_test.csv',\n                        dtype={'people_id': np.str,\n                               'activity_id': np.str,\n                               'otcome': np.int8},\n                        parse_dates=['date'])"},{"cell_type":"markdown","metadata":{"_cell_guid":"1d54ae6e-5a34-47c7-def1-512b8e93f032"},"source":"## Checking to see how dates of the train and testing set are distributed. Are we predicting future values or a random sample "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3910f043-fbc1-94fe-ce5e-8d58e36d8eeb"},"outputs":[],"source":"act_train['date'].groupby(act_train.date.dt.date).count().plot(figsize=(10,5), label='Train')\nact_test['date'].groupby(act_test.date.dt.date).count().plot(figsize=(10,5), label='Test')\nplt.legend()\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"b5a442db-34c8-7062-e477-d3e90f629319"},"source":"This clearly shows that we are looking at a random distribution to test, rather than a time later in the future. Now check the distribution of the good and bad events."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6e15a5d8-7073-f56f-f79c-7ddd9f50b04b"},"outputs":[],"source":"goods=act_train[act_train['outcome']==1]\nbads=act_train[act_train['outcome']==0]\n\ngoods['date'].groupby(goods.date.dt.date).count().plot(figsize=(10,5),label='Good')\nbads['date'].groupby(bads.date.dt.date).count().plot(figsize=(10,5),c='r',label='Bad')\nplt.legend()\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"2fbd6b81-c6ae-a0e2-a24a-f71b72b3ea49"},"source":"Most of the \"bad\" events are in the peak around Oct 2022! Finally, look to see if any people are better or worse bets on the return."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cb0eeaf1-0a43-c75b-7a11-443be4295a7c"},"outputs":[],"source":"positive_counts=pd.DataFrame({'positive_counts' : act_train[act_train['outcome']==1].groupby('people_id',as_index=True).size()}).reset_index()\nnegative_counts=pd.DataFrame({'negative_counts' : act_train[act_train['outcome']==0].groupby('people_id',as_index=True).size()}).reset_index()\nhstry=positive_counts.merge(negative_counts, on='people_id',how='outer')\nhstry['positive_counts']=hstry['positive_counts'].fillna('0').astype(np.int64)\nhstry['negative_counts']=hstry['negative_counts'].fillna('0').astype(np.int64)\nhstry['profit']=hstry['positive_counts']-hstry['negative_counts']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2c83eb58-9da9-3959-70e2-4e520fb3a9c1"},"outputs":[],"source":"hstry.sort_values(by='positive_counts',ascending=False).head(10)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"815f0b00-9ee8-4bce-2216-5051848726c1"},"outputs":[],"source":"hstry.sort_values(by='negative_counts',ascending=False).head(10)"},{"cell_type":"markdown","metadata":{"_cell_guid":"f36b5933-8826-8004-9792-408a557c5931"},"source":"## Now lets see if we can find a predictor (in the people descriptions) for who is a good investor and who is not.\n * Note the there is no overlap in the 'people_id' between the training and test set\n * As the 'profit' category has a wide range, we'll split it into very good (1), good(2), bad(3), very bad (4)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"662b0091-8809-cc8e-4157-d6766f11a15c"},"outputs":[],"source":"hstry['profit'].describe()"},{"cell_type":"markdown","metadata":{"_cell_guid":"7ea6859c-4c54-d9ba-2c02-44e7fd7fbdaa"},"source":"I'll break the categories roughly up into the quartiles. \n\n - Profit of < -5 : 4\n - -5 <= Profit <0: 3\n - 0 <= Profit <= 5: 2\n - 5 < Profit : 1"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2619871a-dc39-07f1-6d0e-3f616a739497"},"outputs":[],"source":"hstry['prof_label']=((pd.to_numeric(hstry['profit']<-5).astype(int) * 4 )+\n                     (pd.to_numeric(hstry['profit'].isin(range(-5,1))).astype(int) * 3)+                     \n                     (pd.to_numeric(hstry['profit'].isin(range(1,6))).astype(int) * 2)+\n                     (pd.to_numeric(hstry['profit']>5).astype(int) * 1 )\n                    )\n                     "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d34f90b7-4543-71d4-083a-04b8b0843db4"},"outputs":[],"source":"plt.figure()\nplt.hist(hstry['prof_label'],4,range=(1,5))\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"4dd55841-cd03-0086-0cc1-575382f73b3c"},"source":"Now make a new data frame which contains all of the people info and add to it the profit info"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f8e5a308-25d9-4d5e-09c4-9a1673d1c88e"},"outputs":[],"source":"people2 = pd.merge(people, hstry, on='people_id', how='inner')\npeople2['positive_counts']=people2['positive_counts'].fillna('0').astype(np.int64)\npeople2['negative_counts']=people2['negative_counts'].fillna('0').astype(np.int64)\npeople2['profit']=people2['profit'].fillna('0').astype(np.int64)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cb8c1a50-bb21-2c63-4453-7b5a82196a3c"},"outputs":[],"source":"# Turn all of the categorical data into integers\n\nobs=['group_1']\nfor i in range (1,10):\n    obs.append('char_'+str(i))\n\nfor x in obs:\n    people2[x]=people2[x].fillna('type 0')\n    people2[x]=people2[x].str.split(' ').str[1]\n\nbools=[]\nfor i in range(10,38):\n    bools.append('char_'+str(i))\n\nfor x in list(set(obs).union(set(bools))):\n    people2[x]=pd.to_numeric(people2[x]).astype(int)\npeople2['date']=pd.to_numeric(people2['date']).astype(int)"},{"cell_type":"markdown","metadata":{"_cell_guid":"1cd2b895-347e-deb7-b095-526d26da8e81"},"source":"For now, just look through the boolean categories to see if there is any separation."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"30eb2d40-e9e3-d675-f2a8-b5ab0fe74c4a"},"outputs":[],"source":"#for x in bools:\n#    plt.figure()\n#    fig, ax= plt.subplots()\n#    ax.set_xticks([1.5,2.5,3.5,4.5])\n#    ax.set_xticklabels(('Very\\nGood','Good','Bad','Very\\nBad'))\n#    fig.suptitle(x, fontsize=15)\n#    neg=people2[people2[x]==0]\n#    pos=people2[people2[x]==1]\n#    plt.hist([pos['prof_label'],neg['prof_label']], 4,range=(1,5), \n#             normed=True, stacked=True, label=['Has Trait','No Trait'])\n#    plt.legend()\n#    plt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"2dc2bc74-d4a5-97fa-4d1d-d8091f5f1be6"},"source":"Some of those look quite good. Now train a one v all classifier to predict a persons category"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bcd8deac-47f9-d591-d4f7-3db6ff07e2f1"},"outputs":[],"source":"from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, AdaBoostRegressor\nfrom sklearn.metrics import auc, mean_squared_error\nfrom sklearn.cross_validation import train_test_split, cross_val_score"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ae3035cc-4ac8-b500-0555-8d8d7330a44f"},"outputs":[],"source":"xfeats = list(people2.columns)\nxfeats.remove('people_id')\nxfeats.remove('profit')\nxfeats.remove('prof_label')\nxfeats.remove('positive_counts')\nxfeats.remove('negative_counts')\nprint(xfeats)\n\nX, Y = people2[xfeats],people2['prof_label']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cb6c5c0e-4072-790a-b78f-e153916988c0"},"outputs":[],"source":"X_train, X_test, y_train, y_test = train_test_split(\n    X, Y, test_size=0.2, random_state=42)\n\nclf = RandomForestRegressor(n_estimators=50)\nclf.fit(X_train,y_train)\n\nprint(clf.feature_importances_)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d3e25369-0c30-eef6-bbcf-ec3f5ef3ae06"},"outputs":[],"source":"sortedfeats=sorted(zip(xfeats,clf.feature_importances_), key=lambda x:x[1])\nnewfeats=[]\nfor i in range(1,6):\n    newfeats.append(sortedfeats[len(sortedfeats) -i])\nnewfeats = [x[0] for x in newfeats]\nprint(newfeats)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9381ea32-8c79-eb8c-c792-bae3bf28949b"},"outputs":[],"source":"X, Y = people2[newfeats],people2['prof_label']\n\nX_train2, X_test2, y_train2, y_test2 = train_test_split(\n    X, Y, test_size=0.2, random_state=42)\n\nclf2 = RandomForestRegressor(n_estimators=100)\nclf2.fit(X_train2,y_train2)\n\nprint(clf2.feature_importances_)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"40081bac-b13f-cc2d-ea69-c5152fbed2fc"},"outputs":[],"source":"print(clf.score(X_test,y_test), clf2.score(X_test2,y_test2))\nprint(mean_squared_error(clf.predict(X_test),y_test),mean_squared_error(clf2.predict(X_test2),y_test2))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bd2a21d6-2ccc-6820-d8cc-b6bfaa781727"},"outputs":[],"source":"people2['pred']=clf.predict(people2[xfeats])\npeople2['pred2']=clf2.predict(people2[newfeats])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6f4a736a-776f-4dd4-34d3-fde60ee1bf65"},"outputs":[],"source":"people2[['prof_label','pred']].sample(20)"},{"cell_type":"markdown","metadata":{"_cell_guid":"d97b35bf-7c8c-5e87-0608-24c6e2c6c915"},"source":"This seems to be predicting the label pretty well. There are a few disturbing cases, where a very bad gets predicted as a very good. Otherwise I am not too concerned with the label moving by 1. Maybe I should try a regression instead? "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"88aca6af-c5b3-d786-f0e0-319ae9fd4499"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}