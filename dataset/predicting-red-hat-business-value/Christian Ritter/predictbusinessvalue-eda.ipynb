{"cells":[{"metadata":{},"cell_type":"markdown","source":"# EDA \n\n### Goals\n\n* EDA on REd Hat business data.\n\n### Comments\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\npd.set_option('display.max_columns', 500)\nsns.set_context(\"talk\", font_scale=1.4)\nsns.set_style('whitegrid')\n\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\n\nimport missingno as mso\n\nimport re","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step I: Business Goal\n\n* Business Goal: Identify who, when and how (activity) to approach an potential customer to derive the most potential business value for Red Hat. \n* Objective: Create a classification algorithm that accurately identifies which customers have the most potential business value for Red Hat based on their characteristics and activities.\n\n* Classification performance measured in AUC.\n\n\n* Initial Hypotheses:\n    * I. There are some activities which bring a higher business value than othe activities.\n    * II. During certain times of the year chances are higher to derive business value from customers.\n    * III. Some group of people allow for higher business value.\n    * IV. Characteristics of people and activities are indicative of business value.\n"},{"metadata":{},"cell_type":"markdown","source":"# Step II: Data Extraction\n\n* This competition uses two separate data files that may be joined together to create a single, unified data table: a people file and an activity file.\n* People.csv: Each row in the people file represents a unique person. Each person has a unique people_id. Contains characteristis of people.\n* activity.csv: \n    * The activity file contains all of the unique activities (and the corresponding activity characteristics) that each person has performed over time. Each row in the activity file represents a unique activity performed by a person on a certain date. Each activity has a unique activity_id. **Unique in the sense of (who, how, when), not actual unique activity characteristics?**\n    * The activity file contains several different categories of activities. Type 1 activities are different from type 2-7 activities because there are more known characteristics associated with type 1 activities (nine in total) than type 2-7 activities (which have only one associated characteristic)."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"people = pd.read_csv(\"/kaggle/input/predicting-red-hat-business-value/people.csv.zip\")\npeople.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"activities = pd.read_csv(\"/kaggle/input/predicting-red-hat-business-value/act_train.csv.zip\")\nactivities.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"activities_test = pd.read_csv(\"/kaggle/input/predicting-red-hat-business-value/act_test.csv.zip\")\nactivities_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step III: Meeting and Greet Data\n\n\n* There are 189k potential customers and 2.1M customer activities in the training set. \n* The test set contains 498k customer activities (train/test split of activities 18.5% in test)\n\n\n* potential typos/mistakes in mixed-type fields: \n     * people: ppl_group, ppl_char_1 - 9\n     * activity: act_category, act_char_10\n\n\n* Missing data\n    * ppl: No missing data detected by pandas\n    * activities: act_char_1-9 have same number of missing values. Agreement with documentation, \n    as these are the 9 characteristics only available for activity type 1. **However act_char_10 has**\n    **also missing values. Why? **\n\n\n\n* Critical questions\n\n    * people characteristics: char_1 until 38. group_1 meaning? date could be the first contact with the person\n    * date fields contain timestamps from future dates! Why? \n    "},{"metadata":{},"cell_type":"markdown","source":"* data types\n    * categorical\n        * nominal: ppl_id, act_id, act_outcome (encoded numeric), ppl_char_10-ppl_char_38 (booleans)\n        * ordinal: act_category, act_char_1-act_char_10 (types), ppl_group, ppl_char_1-ppl_char_9 (types) , \n    * numeric: \n        * discrete: ppl_char_38\n    * date\n        * ppl_date, act_date\n        \n* Variables & Assumptions\n   * ppl_id: unique ID of the user\n   * act_id: unique ID of activity\n   * act_date: date of activity\n   * act_category: assume this are the types of activities. actual types are not known. Only type 1 has act_char_# variables\n   * act_outcome: dependent variable. the business value is encoded in this variable. Exact meaning is not known, and business value could mean many things.\n   * ppl_group: specific group of people. more not known\n   * ppl_char_1-ppl_char_9: each variable have multiple types. ordered by number. does not necessary mean there is an order!\n   * ppl_char_38: surprising this is a numerical varaible> meaning not known\n   * act_date: dates are in the future! The variable was likely modified to anonymize it. Assume that the modification was only of the year, hence the order in time is still correct.\n   * ppl_date: dates in future. Assume this was the date of customer acquisition. Validate possibility in multivariate analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"# simplify column naming\nppl = people.rename(columns=dict({name: '_'.join(['ppl',name]) for name in people.columns if 'char' in name}, \n                               **{'date': 'ppl_date', 'group_1': 'ppl_group', 'people_id': 'ppl_id'}))\nactiv = activities.rename(columns=dict({name: '_'.join(['act',name]) for name in activities.columns if 'char' in name}, \n                               **{'activity_category': 'act_category', 'date':'act_date', 'activity_id': 'act_id', 'outcome': 'act_outcome', 'people_id': 'ppl_id'}))\nactiv_test = activities_test.rename(columns=dict({name: '_'.join(['act',name]) for name in activities_test.columns if 'char' in name}, \n                               **{'activity_category': 'act_category', 'date':'act_date', 'activity_id': 'act_id', 'outcome': 'act_outcome', 'people_id': 'ppl_id'}))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ppl.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sorting of columns\nppl = ppl[['ppl_id', 'ppl_date',  'ppl_group', 'ppl_char_1', 'ppl_char_2'] + ppl.columns[5:].to_list()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ppl.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ppl.sample(10, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"activ.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"activ.sample(10, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"activ_test.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ensure same variables in activity train file and activity test file."},{"metadata":{"trusted":true},"cell_type":"code","source":"assert (activ_test.columns == activ.drop('act_outcome', axis=1).columns).all()\nassert (activ_test.dtypes == activ.drop('act_outcome', axis=1).dtypes).all()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ppl.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"activ.info(null_counts=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Convert into more usable data types"},{"metadata":{"trusted":true},"cell_type":"code","source":"for activ_tmp in [activ, activ_test]:\n    activ_tmp['act_date'] = activ_tmp['act_date'].progress_apply(pd.to_datetime)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ppl['ppl_date'] = ppl['ppl_date'].progress_apply(pd.to_datetime)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ppl_char_1_9 = ppl.columns.to_list()[3:12]\nppl_char_1_9","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ppl[['ppl_group']+ppl_char_1_9] = ppl[['ppl_group']+ppl_char_1_9].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"act_cat = activ.columns.to_list()[3:14]\nact_cat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"activ[act_cat] = activ[act_cat].astype('category')\nactiv_test[act_cat] = activ_test[act_cat].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ppl.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"activ.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First we drop the weekday, month etc. and then we add those as separate features:"},{"metadata":{"trusted":true},"cell_type":"code","source":"characts = pd.merge(activ.drop(columns=[\"weekday\", \"monthday\", \"month\"]), \n                    ppl.drop(columns=[\"weekday\", \"monthday\", \"month\"]), how = 'left', on='ppl_id')\ncharacts.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"characts['ppl_weekday'] = characts[['ppl_date']].apply(lambda x: dt.datetime.strftime(x['ppl_date'], '%A'), axis=1)\ncharacts['ppl_monthday'] = characts.ppl_date.dt.day\ncharacts[\"ppl_month\"] = characts.ppl_date.dt.month\n\ncharacts['act_weekday'] = characts[['act_date']].apply(lambda x: dt.datetime.strftime(x['act_date'], '%A'), axis=1)\ncharacts['act_monthday'] = characts.act_date.dt.day\ncharacts[\"act_month\"] = characts.act_date.dt.month","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"activ.shape, ppl.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step IV: Univariate Analysis\n\n* act_outcome: surprisingly fairly balanced classes.\n* act_date: activities are fairly distributed across the time, spanning roughly 1 year and 1 month. I find extreeme values with maximum of 48174 activies in just one day. \n* ppl_id: users. For a significant number of people 20% (in people.csv) we do not have any activities recorded. Can we discard those?\n"},{"metadata":{},"cell_type":"markdown","source":"### Business Value"},{"metadata":{"trusted":true},"cell_type":"code","source":"activ['act_outcome'].astype('bool').value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = activ['act_outcome'].astype('bool').value_counts(normalize=True).mul(100).plot(kind='bar')\nax.set_xlabel('business value'); ax.set_ylabel('% of customers'); plt.xticks(rotation=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Activity Dates\n\nHow ares the activities distributed over time? "},{"metadata":{"trusted":true},"cell_type":"code","source":"activities_per_day = activ.groupby([pd.Grouper(key='act_date', freq='1D')])['act_id'].count().reset_index()\nactivities_per_day_test = activ_test.groupby([pd.Grouper(key='act_date', freq='1D')])['act_id'].count().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"activ['act_date'].agg({'min': 'min', 'max': 'max'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"activ_test['act_date'].agg({'min': 'min', 'max': 'max'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(16,4))\nsns.lineplot(data=activities_per_day, x='act_date', y='act_id', ax=ax, label='train')\nsns.lineplot(data=activities_per_day_test, x='act_date', y='act_id', ax=ax, label='test')\n\nax.set_ylabel('# of activities'); ax.set_xlabel('date')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Days with the most and the least activity.\n\n* Activities in test span span the same time range as activities in train set. Danger of data leakage! This is not a good split in training and test set when using time series!\n* Also plotting the test set here and analyzing it is introducing positive bias, in particular feature analysis along the lines I am doing here.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"activ.act_date.isin(activ_test.act_date).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"activ_test.act_date.isin(activ.act_date).value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Train and test set cover the same days of activity. One could hence use the `act_date` as a feature to get predictions on the test set. However this might beat the business application purpose which might be to apply the model on  future data."},{"metadata":{"trusted":true},"cell_type":"code","source":"activities_per_day.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.concat([activities_per_day[activities_per_day['act_id'] == activities_per_day['act_id'].max()],\nactivities_per_day[activities_per_day['act_id'] == activities_per_day['act_id'].min()]],axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"users: 189118 in the peoples.csv but only 151295 in the characters"},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime as dt\nactiv['weekday'] = activ[['act_date']].apply(lambda x: dt.datetime.strftime(x['act_date'], '%A'), axis=1)\nactiv['monthday'] = activ.act_date.dt.day\nactiv[\"month\"] = activ.act_date.dt.month\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Towards the end of the week the. number of customer activities increases, peaking on Friday. Surprisingly, Monday is the lowest. It could be that on Monday, as first day of the week, people are not willing to engage with a company/RedHat."},{"metadata":{"trusted":true},"cell_type":"code","source":"activ['weekday'].value_counts(normalize=True).plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig , ax = plt.subplots(figsize=(18,4))\nactiv['monthday'].value_counts(normalize=True).sort_index().plot(kind='bar', ax=ax)\nax.set_ylabel('fraction of activities'); ax.set_xlabel('day of month')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"months = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sept\", \"Oct\", \"Nov\", \"Dec\"]\nmap_month_year = {i+1:m for i, m in enumerate(months)}\nfract_act_month = activ['month'].value_counts(normalize=True).sort_index()\nfract_act_month.index =fract_act_month.index.map(map_month_year)\nfract_act_month.plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Number of activities per customer \n\nPotential outliers\n\n* On median there are 5 activities per customer, with a heavy skew to the right (mean 15)\n* Some customers have had just one activity recorded. \n* 14k activities are flagged as potential outliers based on IQR. Some customers have 1000s of activities recorded which seems odd.\n\n> How to deal with the outliers? Flag with variable? Discard?"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots()\nsns.boxplot(activ.groupby('ppl_id').count()['act_id'], ax=ax)\nax.set_xlabel('# of activities by user')\nax.set_xscale('log')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cap them with IQR."},{"metadata":{"trusted":true},"cell_type":"code","source":"q75=activ.groupby('ppl_id').count()['act_id'].quantile(0.75)\nq25=activ.groupby('ppl_id').count()['act_id'].quantile(0.25)\nIQR = q75-q25\nthreshold_outlier = q75 + 1.5*IQR\nthreshold_outlier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outlier_activ = activ.groupby('ppl_id').count()['act_id'][activ.groupby('ppl_id').count()['act_id']>=threshold_outlier]\noutlier_activ.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(16,4))\nsns.distplot(activ.groupby('ppl_id').count()['act_id'][activ.groupby('ppl_id').count()['act_id']<threshold_outlier], ax=ax, bins=range(0,35))\nax.set_xlabel('number of activities per user')\nax.set_xlim(1,35)\n_ = ax.set_xticks(range(1,35, 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"activ.groupby('ppl_id').count()['act_id'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Activity Uniqueness"},{"metadata":{"trusted":true},"cell_type":"code","source":"activ['act_id'].nunique() == activ['act_id'].shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Categories of activities\n\n* Type 2 category is most prevalent with 41%. \n* Type 1 catery which has the 9 characteristics is only available for 7% of the data.\n* Type 6,7 are rare and only in less than 0.1% of the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"activ['act_category'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"activ['act_category'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(16,4))\nsns.barplot(x=activ.act_category.value_counts().index.to_list(), y=activ.act_category.value_counts(normalize=True), ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Activity Characteristics"},{"metadata":{"trusted":true},"cell_type":"code","source":"activity_chars_type1 = activ.columns[4:-4].to_list()\nactivity_chars_type1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Validate that only when category type 1 activity is present, characteristics 1-9 are present. This is indeed the case:"},{"metadata":{"trusted":true},"cell_type":"code","source":"activ[activ['act_category']=='type 1'][activity_chars_type1].isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(~activ[activ['act_category']!='type 1'][activity_chars_type1].isnull()).sum().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The char1 and char 2 have most types."},{"metadata":{"trusted":true},"cell_type":"code","source":"activ[activity_chars_type1].describe().T.sort_values('unique')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The special characteristic `act_char_10` is present."},{"metadata":{"trusted":true},"cell_type":"code","source":"activ[['act_char_10']].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"activ[['act_char_10']].value_counts(normalize=True).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### People ID\n\nPeople ID is unique in the people table as expected."},{"metadata":{"trusted":true},"cell_type":"code","source":" ppl['ppl_id'].nunique() == ppl['ppl_id'].shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Date of Customer Acquisition"},{"metadata":{"trusted":true},"cell_type":"code","source":"ppl.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"customer_acquisition_per_day = ppl.groupby([pd.Grouper(key='ppl_date', freq='1D')])['ppl_id'].count().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ppl['ppl_date'].agg({'min': 'min', 'max': 'max'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20,4))\nsns.lineplot(data=customer_acquisition_per_day, x='ppl_date', y='ppl_id', ax=ax)\nax.set_ylabel('# of activities'); ax.set_xlabel('date')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ppl['weekday'] = ppl[['ppl_date']].apply(lambda x: dt.datetime.strftime(x['ppl_date'], '%A'), axis=1)\nppl['monthday'] = ppl.ppl_date.dt.day\nppl[\"month\"] = ppl.ppl_date.dt.month","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ppl['weekday'].value_counts(normalize=True).plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig , ax = plt.subplots(figsize=(18,4))\nppl['monthday'].value_counts(normalize=True).sort_index().plot(kind='bar', ax=ax)\nax.set_ylabel('fraction of people date'); ax.set_xlabel('day of month')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fract_ppl_month = ppl['month'].value_counts(normalize=True).sort_index()\nfract_ppl_month.index =fract_ppl_month.index.map(map_month_year)\nfract_ppl_month.plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### People Group\n\nWe find one dominant group with 41%, and all other 34223 groups are <1%. "},{"metadata":{"trusted":true},"cell_type":"code","source":"ppl['ppl_group'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ppl['ppl_group'].value_counts(normalize=True).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### People Characteristics\n\n* character types: some variables have 2 types and up to 43 different types. This indicates that some features might carry significant more information.\n* character boolean features: each value has at least ~20% of all values, hence there is no extreme imbalance\n* the special numerical feature (ppl_char_38): could be percentage. The large amounts of zero values are suspicious."},{"metadata":{"trusted":true},"cell_type":"code","source":"ppl_char_types = ppl.columns[3:12].to_list()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ppl[ppl_char_types].describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ppl_char_types:\n    print(\"col : \", col)\n    print(ppl[col].value_counts(normalize=True).head(5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> \t\n    "},{"metadata":{"trusted":true},"cell_type":"code","source":"ppl_char_bool = ppl.columns[12:-1].to_list()\nppl[ppl_char_bool].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ppl[ppl_char_bool].apply(pd.value_counts, normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ppl['ppl_char_38'].describe().to_frame().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(16,4))\nsns.distplot(ppl['ppl_char_38'], ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step V: Multivariate Analysis\n\n* What is the relationship between the ID variables in the files? Are all ppl_id's in the people.csv in the activities csv?\n\n\n* Answer initial hypothesis:\n\n    * I. There are some activities which bring a higher business value than othe activities.\n    * II. During certain times of the year chances are higher to derive business value from customers.\n    * III. Some group of people allow for higher business value.\n    * IV. Characteristics of people and activities are indicative of business value.\n\n* Is there a relationship between missing values?\n\n"},{"metadata":{},"cell_type":"markdown","source":"### Customer ID relationships in files"},{"metadata":{},"cell_type":"markdown","source":"Confirm that the activitiy user ids are all in the ppl user data."},{"metadata":{"trusted":true},"cell_type":"code","source":"ppl_set = set(ppl['ppl_id'].unique())\nactiv_ppl_set = set(activ['ppl_id'].unique())\nactiv_ppl_set_test = set(activ_test['ppl_id'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"activ_ppl_set.issubset(ppl_set), activ_ppl_set_test.issubset(ppl_set)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Users in test set are not in training set:"},{"metadata":{"trusted":true},"cell_type":"code","source":"activ_ppl_set_test.intersection(activ_ppl_set)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no users which are in the people.csv but not in the activity files (we  could have discarded those if they existed)"},{"metadata":{"trusted":true},"cell_type":"code","source":"ppl_set - activ_ppl_set - activ_ppl_set_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No IDs in activity file are in the test file (by accident)."},{"metadata":{"trusted":true},"cell_type":"code","source":"set(activ['act_id']).intersection(set(activ_test['act_id']))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Is there a relationship between missing values?\n"},{"metadata":{},"cell_type":"markdown","source":"It looks like that those fields of type1 activity have no `act_char_10` field. I confirm this below. Missingness of one variable depends on the value of another."},{"metadata":{"trusted":true},"cell_type":"code","source":"mso.matrix(activ)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"activ[activ['act_category']=='type 1']['act_char_10'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"activ[activ['act_category']!='type 1']['act_char_10'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hypothesis I.: There are some activities which bring a higher business value than othe activities.\n\nType 6 activity has the highest chance with 55% of successfull business outcomes."},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = activ.groupby('act_category')['act_outcome'].mean().sort_values(ascending=False).plot(kind='barh', figsize=(16,4), color=['r','b', 'y', 'k', 'grey'])\nax.set_title('fraction of activity categories with business value ')\nax.set_ylabel('fraction')\n_=plt.xticks([0.1,0.2,0.3,0.4,0.5, 0.6])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"activ.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with sns.plotting_context(\"talk\", font_scale=1):\n    fig, ax = plt.subplots(len(activ.columns[4:13].to_list()), 1, figsize=(20,20), sharex=True)\n    axes = ax.flatten()\n    for i, activ_col in enumerate(activ.columns[4:13].to_list()):\n        sns.barplot(data=activ.groupby(activ_col)['act_outcome'].mean().reset_index(), x='act_outcome', y=activ_col, ax=axes[i], \n                    order=activ.groupby(activ_col)['act_outcome'].mean().sort_values().index) #, palette=sns.color_palette(\"Blues_d\", n_colors=60))\n        axes[i].set_ylabel(activ_col); axes[i].set_xlabel('')\n        axes[i].get_yaxis().set_ticks([])\n        #axes[i].set_title(activ_col)\naxes[i].set_xlabel('fraction')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Time dependence of customer outcomes"},{"metadata":{},"cell_type":"markdown","source":"Day of the week matters and it appears (!) like where there is a higher chance for customer success, there are more activities taking place."},{"metadata":{"trusted":true},"cell_type":"code","source":"cats = [ 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\nfig, axes = plt.subplots(2, figsize=(16,8), sharex=True)\nactiv.groupby(\"weekday\")[\"act_outcome\"].value_counts(normalize=True).sort_index().unstack().reindex(cats).plot(kind='bar', ax=axes[0] )\naxes[0].legend(loc=4)\nactiv[\"weekday\"].value_counts(normalize=True).reindex(cats).plot(kind='bar', ax=axes[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig , ax = plt.subplots(figsize=(18,4))\nactiv.groupby('monthday')['act_outcome'].mean().sort_index().plot(kind='bar', ax=ax)\nplt.title('fraction of positive activity outcome'); ax.set_xlabel('day of month')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Success of business activitate appears to depend on the month"},{"metadata":{"trusted":true},"cell_type":"code","source":"# fract_act_month = activ['month'].value_counts(normalize=True).sort_index()\n\nfract_act_month_act = activ.groupby('month')['act_outcome'].mean()\nfract_act_month_act.index =fract_act_month_act.index.map(map_month_year)\nfract_act_month_act.plot(kind='bar'); plt.title('fraction of successful business outcomes')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Group and date"},{"metadata":{"trusted":true},"cell_type":"code","source":"characts.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outcome_by_grp_actdate = characts.groupby([\"act_date\", \"ppl_group\"])['act_outcome'].mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This creates a dataframe with all dates for each group, hence missing values for dates."},{"metadata":{"trusted":true},"cell_type":"code","source":"outcome_by_grp_actdate.dropna().head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outcome_by_grp_actdate.dropna().value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> This implies that for a specific day in training set, all activities related to a ppl_group category are either act_outcome=0 or 1. There is no pplt_group where some activities had outcome 0 but others 1.\n\nSimple classifier scheme: `act_date` > `ppl_group` category > fixed outcome.\n\nAs all dates in the test set are also in the training set, one can use the `act_date` as feature. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(2, figsize=(16,8), sharex=True)\ncharacts.groupby(\"ppl_weekday\")[\"act_outcome\"].value_counts(normalize=True).sort_index().unstack().reindex(cats).plot(kind='bar', ax=axes[0] )\naxes[0].legend(loc=4)\ncharacts[\"ppl_weekday\"].value_counts(normalize=True).reindex(cats).plot(kind='bar', ax=axes[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Categorical variables with many values\n\nHow useful are these variables?\n\n* Extreme valuess for act_char_10, ppl_group and to a far less extend ppl_char_38"},{"metadata":{"trusted":true},"cell_type":"code","source":"characts.nunique()[characts.nunique() > 10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlations between features"},{"metadata":{"trusted":true},"cell_type":"code","source":"def cramers_v(x, y):\n    confusion_matrix = pd.crosstab(x,y)\n    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2/n\n    r,k = confusion_matrix.shape\n    phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))\n    rcorr = r-((r-1)**2)/(n-1)\n    kcorr = k-((k-1)**2)/(n-1)\n    return np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"characts.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"characts.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_features = characts.nunique()[characts.nunique() < 5].index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_dummies = pd.get_dummies(characts[corr_features],drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_dummies.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_pearson = corr_dummies.corr() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some categories are highly correlated! Could be removed in modeling.\n\n* Above 90% correlation only: ppl_char_28 and ppl_char_21\n* Some features are above 80% correlation."},{"metadata":{"trusted":true},"cell_type":"code","source":"with sns.plotting_context(\"talk\", font_scale=0.6):\n    fig, ax = plt.subplots(figsize=(22,14))\n    lower_triangle = np.tril(corr_pearson, k = -1)\n    mask = lower_triangle == 0\n    sns.heatmap(corr_pearson, annot=True, ax=ax, fmt=\".2f\", mask=mask)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sort to identify features which are correlated the highest:"},{"metadata":{"trusted":true},"cell_type":"code","source":"high_corr_features = [[pair[0], pair[1]] for pair in corr_pearson[corr_pearson > 0.8].stack().index.tolist() if not pair[1]==pair[0]]\nhigh_corr_features = [[pair[1], pair[0], corr_pearson.loc[pair[1], pair[0]]] for pair in high_corr_features]\npd.DataFrame(sorted(high_corr_features, key=lambda x: x[2])[::-1], columns=['feature1', 'feature2', 'corr_coef']).drop_duplicates(subset=['corr_coef'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_pearson['act_outcome'].sort_values(ascending=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}