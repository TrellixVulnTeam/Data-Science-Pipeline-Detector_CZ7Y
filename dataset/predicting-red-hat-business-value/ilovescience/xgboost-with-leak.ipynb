{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"47e7a2dd-313b-a7c5-5933-47ef77ebfdf5"},"outputs":[],"source":"import pandas as pd\nimport numpy as np\nimport datetime\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cross_validation import KFold\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import roc_auc_score\nimport xgboost as xgb\nimport random\nfrom operator import itemgetter\nimport time\nimport copy\n\nrandom.seed(2016)\n\n\ndef create_feature_map(features):\n    outfile = open('xgb.fmap', 'w')\n    for i, feat in enumerate(features):\n        outfile.write('{0}\\t{1}\\tq\\n'.format(i, feat))\n    outfile.close()\n\n\ndef get_importance(gbm, features):\n    create_feature_map(features)\n    importance = gbm.get_fscore(fmap='xgb.fmap')\n    importance = sorted(importance.items(), key=itemgetter(1), reverse=True)\n    return importance\n\n\ndef intersect(a, b):\n    return list(set(a) & set(b))\n\ndef get_features(train, test):\n    trainval = list(train.columns.values)\n    testval = list(test.columns.values)\n    output = intersect(trainval, testval)\n    output.remove('people_id')\n    return sorted(output)\n\ndef run_single(train, test, features, target, random_state=0):\n    eta = 1.3\n    max_depth = 3\n    subsample = 0.8\n    colsample_bytree = 0.8\n    start_time = time.time()\n\n    print('XGBoost params. ETA: {}, MAX_DEPTH: {}, SUBSAMPLE: {}, COLSAMPLE_BY_TREE: {}'.format(eta, max_depth, subsample, colsample_bytree))\n    params = {\n        \"objective\": \"binary:logistic\",\n        \"booster\" : \"gbtree\",\n        \"eval_metric\": \"auc\",\n        \"eta\": eta,\n        \"tree_method\": 'exact',\n        \"max_depth\": max_depth,\n        \"subsample\": subsample,\n        \"colsample_bytree\": colsample_bytree,\n        \"silent\": 1,\n        \"seed\": random_state,\n    }\n    num_boost_round = 115\n    early_stopping_rounds = 10\n    test_size = 0.1\n\n    X_train, X_valid = train_test_split(train, test_size=test_size, random_state=random_state)\n    print('Length train:', len(X_train.index))\n    print('Length valid:', len(X_valid.index))\n    y_train = X_train[target]\n    y_valid = X_valid[target]\n    dtrain = xgb.DMatrix(X_train[features], y_train)\n    dvalid = xgb.DMatrix(X_valid[features], y_valid)\n\n    watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n    gbm = xgb.train(params, dtrain, num_boost_round, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=True)\n\n    print(\"Validating...\")\n    check = gbm.predict(xgb.DMatrix(X_valid[features]), ntree_limit=gbm.best_iteration+1)\n    score = roc_auc_score(X_valid[target].values, check)\n    print('Check error value: {:.6f}'.format(score))\n\n    imp = get_importance(gbm, features)\n    print('Importance array: ', imp)\n\n    print(\"Predict test set...\")\n    test_prediction = gbm.predict(xgb.DMatrix(test[features]), ntree_limit=gbm.best_iteration+1)\n\n    print('Training time: {} minutes'.format(round((time.time() - start_time)/60, 2)))\n    return test_prediction.tolist()\n\n\ndef simple_load():\n\n    print(\"Read people.csv...\")\n    people = pd.read_csv(\"../input/people.csv\",\n                       dtype={'people_id': np.str,\n                              'activity_id': np.str,\n                              'char_38': np.int32},\n                       parse_dates=['date'])\n\n    print(\"Load train.csv...\")\n    train = pd.read_csv(\"../input/act_train.csv\",\n                        dtype={'people_id': np.str,\n                               'activity_id': np.str,\n                               'outcome': np.int8},\n                        parse_dates=['date'])\n\n    print(\"Load test.csv...\")\n    test = pd.read_csv(\"../input/act_test.csv\",\n                       dtype={'people_id': np.str,\n                              'activity_id': np.str},\n                       parse_dates=['date'])\n\n    print(\"Process tables...\")\n    for table in [train, test]:\n        table['activity_category'] = table['activity_category'].str.lstrip('type ').astype(np.int32)\n        for i in range(1, 11):\n            table['char_' + str(i)].fillna('type -999', inplace=True)\n            table['char_' + str(i)] = table['char_' + str(i)].str.lstrip('type ').astype(np.int32)\n    people['year'] = people['date'].dt.year\n    people['month'] = people['date'].dt.month\n    people['day'] = people['date'].dt.day\n    people['weekday'] = people['date'].dt.weekday\n    people['weekend'] = ((people.weekday == 0) | (people.weekday == 6)).astype(int)\n    people.drop('date', axis=1, inplace=True)\n    people['group_1'] = people['group_1'].str.lstrip('group ').astype(np.int32)\n    for i in range(1, 10):\n        people['char_' + str(i)] = people['char_' + str(i)].str.lstrip('type ').astype(np.int32)\n    for i in range(10, 38):\n        people['char_' + str(i)] = people['char_' + str(i)].astype(np.int32)\n\n    print(\"Merge...\")\n    train = train.merge(people, on=\"people_id\", suffixes=(\"_act\", \"\"))\n    test = test.merge(people, on=\"people_id\", suffixes=(\"_act\", \"\"))\n    \n    # Set index to activity id\n    train = train.set_index(\"activity_id\")\n    test = test.set_index(\"activity_id\")\n    return train, test\n\n\ndef group_decision(train, test, only_certain=True):\n    # Exploit the leak revealed by Loiso and team to try and directly infer any labels that can be inferred\n    # https://www.kaggle.com/c/predicting-red-hat-business-value/forums/t/22807/0-987-kernel-now-available-seems-like-leakage\n\n    # Make a lookup dataframe, and copy those in first since we can be sure of them\n    lookup = train.groupby([\"group_1\", \"date_act\"], as_index=False)[\"outcome\"].mean()\n    test = pd.merge(test.reset_index(), lookup, how=\"left\", on=[\"group_1\", \"date_act\"]).set_index(\"activity_id\")\n\n    # Create some date filling columns that we'll use after we append\n    train[\"date_act_fillfw\"] = train[\"date_act\"]\n    train[\"date_act_fillbw\"] = train[\"date_act\"]\n\n    # Create some group filling columns for later use\n    train[\"group_fillfw\"] = train[\"group_1\"]\n    train[\"group_fillbw\"] = train[\"group_1\"]\n\n    # Put the two data sets together and sort\n    df = train.append(test)\n    df = df.sort_values(by=[\"group_1\", \"date_act\"])\n\n    # Fill the dates\n    df[\"date_act_fillfw\"] = df[\"date_act_fillfw\"].fillna(method=\"ffill\")\n    df[\"date_act_fillbw\"] = df[\"date_act_fillbw\"].fillna(method=\"bfill\")\n\n    # Fill labels\n    df[\"outcome_fillfw\"] = df[\"outcome\"].fillna(method=\"ffill\")\n    df[\"outcome_fillbw\"] = df[\"outcome\"].fillna(method=\"bfill\")\n\n    # Fill the groups\n    df[\"group_fillfw\"] = df[\"group_fillfw\"].fillna(method=\"ffill\")\n    df[\"group_fillbw\"] = df[\"group_fillbw\"].fillna(method=\"bfill\")\n\n    # Create int booleans for whether the fillers are from the same date\n    df[\"fw_same_date\"] = (df[\"date_act_fillfw\"] == df[\"date_act\"]).astype(int)\n    df[\"bw_same_date\"] = (df[\"date_act_fillbw\"] == df[\"date_act\"]).astype(int)\n\n    # Create int booleans for whether the fillers are in the same group\n    df[\"fw_same_group\"] = (df[\"group_fillfw\"] == df[\"group_1\"]).astype(int)\n    df[\"bw_same_group\"] = (df[\"group_fillbw\"] == df[\"group_1\"]).astype(int)\n\n    # Use the filled labels only if the labels were from the same group, unless we're at the end of the group\n    df[\"interfill\"] = (df[\"outcome_fillfw\"] *\n                       df[\"fw_same_group\"] +\n                       df[\"outcome_fillbw\"] *\n                       df[\"bw_same_group\"]) / (df[\"fw_same_group\"] +\n                                               df[\"bw_same_group\"])\n\n    # If the labels are at the end of the group, cushion by 0.5\n    df[\"needs cushion\"] = (df[\"fw_same_group\"] * df[\"bw_same_group\"] - 1).abs()\n    df[\"cushion\"] = df[\"needs cushion\"] * df[\"interfill\"] * -0.1 + df[\"needs cushion\"] * 0.05\n    df[\"interfill\"] = df[\"interfill\"] + df[\"cushion\"]\n\n    # Fill everything\n    df[\"outcome\"] = df[\"outcome\"].fillna(df[\"interfill\"])\n\n    if only_certain == True:\n        # Drop anything we're not 100% certain of\n        df = df[(df[\"outcome\"] == 0.0) | (df[\"outcome\"] == 1.0)]\n\n    # Return outcomes to the original index\n    test[\"outcome\"] = df[\"outcome\"]\n\n    return test[\"outcome\"]\n\ndef xgboost_return(train,test,features):\n    print(\"Process tables... \")\n    for table in [train, test]:\n        table['year'] = table['date'].dt.year\n        table['month'] = table['date'].dt.month\n        table['day'] = table['date'].dt.day\n        table['weekday'] = table['date'].dt.weekday\n        table['weekend'] = ((table.weekday == 0) | (table.weekday == 6)).astype(int)\n        table.drop('date', axis=1, inplace=True)\n    features.remove('date')\n    features.remove('date_act')\n    test[\"extra outcomes\"] = run_single(train,test,features,\"outcome\")\n    return test[\"extra outcomes\"]\n\ndef model():\n\n    # Load in the data set simply by merging together\n    train, test = simple_load()\n    \n    # Get features\n    features = get_features(train,test)\n\n    # Try to just infer the correct dates using the data leak\n    test[\"outcome\"] = group_decision(train, test, only_certain=False)\n\n    # Write the inferred predictions to a template\n    test.reset_index()[[\"activity_id\", \"outcome\"]].to_csv(\"starter_template.csv\", index=False)\n\n    # Fill any missing rows with the mean of the whole column\n    test[\"outcome\"] = test[\"outcome\"].fillna(xgboost_return(train,test,features))\n\n    return test.reset_index()[[\"activity_id\", \"outcome\"]]\n\n\ndef main():\n\n    # Write a benchmark file to the submissions folder\n    model().to_csv(\"submission.csv\", index=False)\n\nif __name__ == \"__main__\":\n    main()"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}