{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"7ebebef4-a4cb-8af8-105f-8b5a8b141e09"},"source":"Let's do some exploratory data analysis on the people data set, and see if it makes sense to use any decomposition techniques. I think there could be a couple good reasons why we might want to do this:\n\n* If a lot of the characteristics of people are interdependent variables, we can consolidate them into single features so as to not muddy our final classifier inputs with repeat information.\n* There's potential for discovering latent features that are only evident by looking at multiple features together.\n* If you're like me, you might be pretty limited on computing resources and want to shrink the people data set before merging it in."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c181ed5f-beea-4dcb-7fc5-d6393fa30099"},"outputs":[],"source":"import pandas as pd\n\n# Import the dataset\ndf = pd.read_csv(\"../input/people.csv\")\n\n# Print the first five rows\nprint(df.head())"},{"cell_type":"markdown","metadata":{"_cell_guid":"9c4c06c8-fc76-38c1-46a9-6904dd3fdc98"},"source":"It looks like all the characteristics are denoted with a \"char_\" prefix. Let's use that to make sure that we don't drag along extra columns. Then we'll run a matrix of chi squared tests to get an idea of which features might be interdependent."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7571f3b1-cdef-b1d5-3011-da3ee5348f4a"},"outputs":[],"source":"from scipy.stats import chisquare\n\n# Create a list of characteristics\nchars = [i for i in df.columns.values if \"char_\" in i]\n\n# Create an empty list for appending flagged features\nflags = []\n\n# For each feature summarize frequencies of each other feature\nfor feat in df[chars]:\n    group = df[chars].groupby(feat)\n    for otherfeat in df[chars].drop(feat, axis=1):\n        summary = group[otherfeat].count()\n        \n        # Run a chi squared test on the frequencies, and check if the p-value is less than 0.05\n        if chisquare(summary)[1] < 0.05:\n            \n            # If so, flag both features\n            flags.append(feat)\n            flags.append(otherfeat)\n\n# Remove duplicates by converting to a set at the end\nflags = set(flags)\n\nprint(\"It looks like {}% of the characteristics might be related to one another.\".format(len(flags)/len(chars)*100))"},{"cell_type":"markdown","metadata":{"_cell_guid":"088f9230-6ec7-6aec-da77-014b7d487409"},"source":"Wow, 100%. Hopefully someone reviewing this can highlight if I implemented those tests wrong, or if chi squared wasn't the right test of choice. At any rate, I'm going to proceed on the assumption that we should definitely be using some decomposition techniques on the data set, if only just to reduce the repeated information. Let's use the scikit-learn implementation of PCA. Some success with PCA would confirm that the chi squared tests were useful."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1b927b29-a816-90c4-f4d7-382c28eb3d33"},"outputs":[],"source":"# Convert to dummy variables\ndums = pd.get_dummies(df[chars])\n\nprint(\"Before PCA the full size of the characteristics is {} features\".format(len(dums.columns.values)))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eb93f861-0541-c714-9a53-b58d493baaf4"},"outputs":[],"source":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler\n\n# As was suggested by dpace, let's also scale the features so that they're all in the range of 0 and 1\nscaledums = MinMaxScaler().fit_transform(dums)\n\n# Now we're ready for PCA. Let's just look at the first two principle components first\npca = PCA(n_components=2)\nfeaturecomponents = pca.fit_transform(scaledums)\n\nprint(pca.explained_variance_ratio_)"},{"cell_type":"markdown","metadata":{"_cell_guid":"7861f2a8-a3b8-d0e6-7902-46c276199e61"},"source":"Awesome, scaling seems to fix the problem of char_38 dominating the first component and the overall variance of the dataset, thanks, dpace."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"afe136ca-8828-fd5f-327c-2853989ee5b4"},"outputs":[],"source":"import numpy as np\n\n# build a dictionary with the names of the components\ncomponents = {}\nindex = 0\nfor feature in dums.columns.values:\n    components[feature] = [pca.components_[0][index]]\n    index += 1\n    \n# Exclude all but the most extreme components, because there are a lot\nsortedcomps = pca.components_[0]\nsortedcomps.sort()\nmaxcap = sortedcomps[-3]\nmincap = sortedcomps[2]\ncomponents = {i:x for i, x in components.items() if x >= maxcap or x <= mincap}\n    \n# Convert to dataframe\ncomponents = pd.DataFrame(components)\n\n# Plot the most extreme components\ncomponents.plot(kind=\"bar\", figsize=(12, 4))"},{"cell_type":"markdown","metadata":{"_cell_guid":"60d91817-a8f2-5f67-f05a-49142abc7343"},"source":"Interesting, and plotting the most extreme contributors to the first principle component, char_38 isn't among them. So scaling down char_38 was really critical."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8960817c-5ef3-fed5-8c50-b3cd2ec21ef5"},"outputs":[],"source":"# Plot the first two principle components\nfeaturecomponents = pd.DataFrame(featurecomponents, columns=[\"Principle Component 1\", \"Principle Component 2\"])\ndf[\"Principle Component 1\"] = featurecomponents[\"Principle Component 1\"]\n\nfeaturecomponents.plot(kind=\"scatter\", x=\"Principle Component 1\", y=\"Principle Component 2\", figsize=(12, 12), s=1)"},{"cell_type":"markdown","metadata":{"_cell_guid":"f03a3efa-33ed-0b48-fd5c-f78adf7815a3"},"source":"I'm curious now what the group_1 feature looks in terms of these first two principle components. I wonder if the groups were made from the characteristics, or something else. Just for curiosity's sake, let's plot a few groups."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9ae8d615-f106-4eea-0fde-b7f510e43bb4"},"outputs":[],"source":"# Add group_1 to the new data from pca\nfeaturecomponents[\"group_1\"] = df[\"group_1\"]\n\n# Get a list of groups to sample from\ngroupslist = list(set(featurecomponents[\"group_1\"].tolist()))\n\n# Pick a group and plot\ngroup = featurecomponents[featurecomponents[\"group_1\"]==groupslist[0]]\ngroup.plot(kind=\"scatter\", x=\"Principle Component 1\", y=\"Principle Component 2\", figsize=(3, 3))\nprint(\"There are {} data points in this group.\".format(len(group.index)))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9993cf1b-de53-b4a9-5191-8935c7e77884"},"outputs":[],"source":"# Pick a group and plot\ngroup = featurecomponents[featurecomponents[\"group_1\"]==groupslist[5]]\ngroup.plot(kind=\"scatter\", x=\"Principle Component 1\", y=\"Principle Component 2\", figsize=(3, 3))\nprint(\"There are {} data points in this group.\".format(len(group.index)))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1b18913a-097f-fca2-2300-cc13c700b6b6"},"outputs":[],"source":"# Pick a group and plot\ngroup = featurecomponents[featurecomponents[\"group_1\"]==groupslist[6]]\ngroup.plot(kind=\"scatter\", x=\"Principle Component 1\", y=\"Principle Component 2\", figsize=(3, 3))\nprint(\"There are {} data points in this group.\".format(len(group.index)))"},{"cell_type":"markdown","metadata":{"_cell_guid":"fc895c0b-ee8d-de08-ab09-454cf977e25b"},"source":"Plotting the a few groups with at least two data points to them, I can't really tell if group_1 was created from clustering the characteristics. However, these first two principle components don't capture all of the information from the characteristics, and browsing through six data points is hardly conclusive. I or someone else will have to explore this more thoroughly.\n\nAs a final question, how many principle components do I need to merge into the train/test sets if I only care about a certain amount of the explained variance?"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b752e01e-d218-af0a-d303-ee35aa36d2aa"},"outputs":[],"source":"# Define a list of possible amounts of explained variance we might care about\ncares = [i/100 for i in range(75, 100, 5)]\n\n# Run the PCA with increased components until each care level is reached\nfor i in range (20, len(dums.columns.values)):\n    pca = PCA(n_components=i)\n    pca.fit(scaledums)\n    try:\n        if pca.explained_variance_ratio_.sum() > cares[0]:\n\n            # If greater, print a statement and drop the first item off the list\n            print(\"To explain {0} of the variance you'll need {1} components\".format(cares[0], i))\n            cares = cares[1:]\n    except:\n        break"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}