{"cells":[{"metadata":{},"cell_type":"markdown","source":"# The aim of this notebook is to build a classifier for the REDHAT predicting business value competition. We will 1) Do some data clean-up with Pandas 2) Use the tf.feature_columns api to apply transformations to the data, and 3) Use Keras to train a Feed-forward Neural Network."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"people = pd.read_csv('/kaggle/input/predicting-red-hat-business-value/people.csv.zip')\nactivites_train = pd.read_csv('/kaggle/input/predicting-red-hat-business-value/act_train.csv.zip')\nactivites_test = pd.read_csv('/kaggle/input/predicting-red-hat-business-value/act_test.csv.zip')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n## The people and activities table can be linked via shared column people_id. Merge the people set with the train and test activities sets to create the full train and test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"def merge_sets(people, activities):\n    return activities.join(people.set_index('people_id'), on='people_id', lsuffix='_activity', rsuffix='_person')\n\ntrain = merge_sets(people, activites_train)\ntest = merge_sets(people, activites_test)\n\ntrain.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_ids = test.activity_id","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## After some data exploration it is noticable that some categorical columns of the activities table only refer to specific types of activities. Instances that represent these types of activities have values for those columns. Instances of a different activity type do not have a value for these columns. Therefore, many activity instances have missing values. We will simply fill these with the value 'missing'."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(dataset):\n    dataset = dataset.drop(['date_person', 'date_activity', 'people_id', 'activity_id'], axis=1) # \n    dataset = dataset.fillna('missing')\n    return dataset\n\ntrain = preprocess(train)\ntest = preprocess(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let us look at how many categories the categorical columns have. We can see that some columns such as group_1 or even char_4_person have many categories. One-hot-encoding these categories would result in too many columns. Therefore, we will use embeddings on columns with more than 12 categories. Let us first extract the column names we will use for one-hot-encoding, embedding, and numerical input."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dtypes # Quick look at the different column types before checking categories","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_data = train.select_dtypes(include=['object'])\ncategorical_data.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"column_names = categorical_data.columns\nprint(\"CATEGORICAL COLUMNS : \\n\" + str(column_names))\n\nembed_column_indicators = categorical_data.nunique() > 12\nonehot_column_indicators = categorical_data.nunique() <= 12\n\nEMBED_COLUMN_NAMES = column_names[embed_column_indicators]\nONEHOT_COLUMN_NAMES = column_names[onehot_column_indicators]\nNUM_COLUMN_NAMES = train.select_dtypes(exclude=['object']).columns\n\nprint(\"\\nEMBED COLUMNS : \\n\" + str(EMBED_COLUMN_NAMES))\nprint(\"\\nONEHOT COLUMNS : \\n\" + str(ONEHOT_COLUMN_NAMES))\nprint(\"\\nNUMERICAL COLUMNS : \\n\" + str(NUM_COLUMN_NAMES))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hardcode the column names to keep them robust from potential future data changes"},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBED_COLUMN_NAMES = ['char_1_activity', 'char_2_activity', 'char_8_activity',\n       'char_9_activity', 'char_10_activity', 'group_1', 'char_3_person',\n       'char_4_person', 'char_7_person']\nONEHOT_COLUMN_NAMES = ['activity_category', 'char_3_activity', 'char_4_activity',\n       'char_5_activity', 'char_6_activity', 'char_7_activity',\n       'char_1_person', 'char_2_person', 'char_5_person', 'char_6_person',\n       'char_8_person', 'char_9_person']\nNUM_COLUMN_NAMES = ['char_10_person', 'char_11', 'char_12', 'char_13', 'char_14',\n       'char_15', 'char_16', 'char_17', 'char_18', 'char_19', 'char_20',\n       'char_21', 'char_22', 'char_23', 'char_24', 'char_25', 'char_26',\n       'char_27', 'char_28', 'char_29', 'char_30', 'char_31', 'char_32',\n       'char_33', 'char_34', 'char_35', 'char_36', 'char_37', 'char_38']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Take a quick look at the maximum values of the numerical columns, so that we know to normalize them later on."},{"metadata":{"trusted":true},"cell_type":"code","source":"np.amax(train[NUM_COLUMN_NAMES], axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since all numerical columns apart from char_38 and the outcome are booleans, let's have a quick look at their distribution, in case we notice anything special."},{"metadata":{"trusted":true},"cell_type":"code","source":"train[NUM_COLUMN_NAMES].astype('float32').hist(figsize=(32,32))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Finally, lets create the bridge of our input pipeline that will turn columns of our Pandas DataFrame into input of our upcoming Neural Network. For this we will use the tf.feature_columns API, that allows us to specify which transformations to perform on each column before feeding the columns to a Neural Network.\n\n# Most importantly, this API allows for more compatability. The code we write with it will allow us to use our Pandas DataFrame set as an input for training, but pretty much the same code will allow us to also use other data sources such as cross-platform TFRecord files at test time. It simply expects a tf.data.Dataset object."},{"metadata":{},"cell_type":"markdown","source":"Before anything however, quickly create a validation set."},{"metadata":{"trusted":true},"cell_type":"code","source":"trainset, valset = train_test_split(train, test_size=0.2)\ntestset = test\nprint(len(trainset), 'train examples')\nprint(len(valset), 'validation examples')\nprint(len(testset), 'test examples')\nprint(type(trainset),type(valset),type(testset))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dataframe_to_dataset(dataframe, shuffle=False, batch_size=32, train=False, buffer_size=50000):\n    dataframe = dataframe.copy()\n    if train:\n        labels = dataframe.pop('outcome')\n    else:\n        labels = tf.ones(shape=dataframe.shape[0]) * -1 # Will be ignored at test time\n        \n    dataset = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n    if shuffle:\n        buffer_size = buffer_size if buffer_size < dataframe.shape[0] else dataframe.shape[0]\n        dataset = dataset.shuffle(buffer_size, seed=42)\n        \n    dataset = dataset.batch(batch_size).prefetch(1)\n    return dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = dataframe_to_dataset(trainset, train=True, shuffle=True, batch_size=4096)\nval_dataset = dataframe_to_dataset(valset, train=True, batch_size=4096)\ntest_dataset = dataframe_to_dataset(testset, batch_size=4096)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will create a definition of what to do with each column."},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_columns = []\n\nfor column_name in NUM_COLUMN_NAMES:\n    feature_columns.append(tf.feature_column.numeric_column(key=column_name, dtype=tf.float32, default_value=-1))\n    \nfor column_name in ONEHOT_COLUMN_NAMES:\n    vocabulary = train[column_name].unique()\n    categorical_column = tf.feature_column.categorical_column_with_vocabulary_list(\n                          key=column_name, vocabulary_list=vocabulary, default_value=-1)\n    \n    onehot_column = tf.feature_column.indicator_column(categorical_column)\n    feature_columns.append(onehot_column)\n    \nfor column_name in EMBED_COLUMN_NAMES:\n    vocabulary = train[column_name].unique()\n    categorical_column = tf.feature_column.categorical_column_with_vocabulary_list(\n                          key=column_name, vocabulary_list=vocabulary, default_value=-1)\n    \n    embedding_dimensions = 16 if len(vocabulary) < 500 else 256\n    embed_column = tf.feature_column.embedding_column(categorical_column, dimension=embedding_dimensions)\n    feature_columns.append(embed_column)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The feature_columns list now contains all the specification that our keras model needs to take a tf.data.Dataset and properly process each column before feeding it into the model. We could use this same specification to parse TFRecord files that are structured the same as our Pandas Dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(feature_columns[0],'\\n')\nprint(feature_columns[35],'\\n')\nprint(feature_columns[48],'\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# All we need to do is use the following layer as the first layer of our keras model."},{"metadata":{"trusted":true},"cell_type":"code","source":"processing_layer = keras.layers.DenseFeatures(feature_columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Finally, lets build and train our model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = keras.models.Sequential()\nmodel.add(processing_layer)\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Dense(16, kernel_initializer='he_normal'))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Activation('relu'))\nmodel.add(keras.layers.Dense(1, activation='sigmoid', kernel_regularizer=keras.regularizers.l2(0.01)))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer=keras.optimizers.Nadam(0.01),\n              metrics=['accuracy'])\n\nearly_stopping = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\nreduce_lr_plateau = keras.callbacks.ReduceLROnPlateau(patience=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(train_dataset, epochs=100, validation_data=val_dataset, callbacks=[early_stopping, reduce_lr_plateau], steps_per_epoch=20, validation_steps=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Some extra code for submitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.plot(history.epoch, history.history[\"lr\"], \"bo-\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Learning Rate\", color='b')\nplt.tick_params('y', colors='b')\nplt.gca().set_xlim(0, 10 - 1)\nplt.grid(True)\n\nax2 = plt.gca().twinx()\nax2.plot(history.epoch, history.history[\"val_loss\"], \"r^-\")\nax2.set_ylabel('Validation Loss', color='r')\nax2.tick_params('y', colors='r')\n\nplt.title(\"Reduce LR on Plateau\", fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(history.history)[['accuracy', 'val_accuracy']].plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0.7, 1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_preds = model.predict(val_dataset, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\ny_true = list(val_dataset.map(lambda x,y: y).unbatch().as_numpy_iterator())\nroc_auc_score(y_true, val_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = model.predict(test_dataset, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(test_preds), test_preds[0], test_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ids = np.array(submission_ids).reshape((-1,)) # make 1-dimensional\npreds = test_preds.reshape((-1,)) # make 1-dimensional\nprint(ids.shape, preds.shape)\n\nmy_submission = pd.DataFrame({'activity_id': ids, 'outcome': preds})\n\nmy_submission.to_csv('REDHAT_submission_2.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}