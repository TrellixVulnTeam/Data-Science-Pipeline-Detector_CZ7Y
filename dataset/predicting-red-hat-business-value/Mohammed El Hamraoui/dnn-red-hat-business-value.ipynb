{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\npd.options.display.float_format = '{:,.2f}'.format\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-24T12:34:01.773566Z","iopub.execute_input":"2022-02-24T12:34:01.773931Z","iopub.status.idle":"2022-02-24T12:34:01.788357Z","shell.execute_reply.started":"2022-02-24T12:34:01.77389Z","shell.execute_reply":"2022-02-24T12:34:01.787224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predicting Red Hat Business Value\n\n### How Can We Identify a Potential Customer?","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/predicting-red-hat-business-value/act_train.csv.zip\", parse_dates=['date'])\nprint(df.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:34:01.790307Z","iopub.execute_input":"2022-02-24T12:34:01.791036Z","iopub.status.idle":"2022-02-24T12:34:06.856448Z","shell.execute_reply.started":"2022-02-24T12:34:01.790986Z","shell.execute_reply":"2022-02-24T12:34:06.855568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"people = pd.read_csv(\"/kaggle/input/predicting-red-hat-business-value/people.csv.zip\", parse_dates=['date'])\nprint(people.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:34:06.85791Z","iopub.execute_input":"2022-02-24T12:34:06.858152Z","iopub.status.idle":"2022-02-24T12:34:07.955449Z","shell.execute_reply.started":"2022-02-24T12:34:06.858123Z","shell.execute_reply":"2022-02-24T12:34:07.954522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Explore the contents of the 'activity' dataset","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:34:07.957273Z","iopub.execute_input":"2022-02-24T12:34:07.95753Z","iopub.status.idle":"2022-02-24T12:34:07.97658Z","shell.execute_reply.started":"2022-02-24T12:34:07.957499Z","shell.execute_reply":"2022-02-24T12:34:07.975599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.sample(5, random_state=16)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:34:07.978272Z","iopub.execute_input":"2022-02-24T12:34:07.97852Z","iopub.status.idle":"2022-02-24T12:34:08.089672Z","shell.execute_reply.started":"2022-02-24T12:34:07.97849Z","shell.execute_reply":"2022-02-24T12:34:08.08909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.tail()","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:34:08.090818Z","iopub.execute_input":"2022-02-24T12:34:08.091205Z","iopub.status.idle":"2022-02-24T12:34:08.106955Z","shell.execute_reply.started":"2022-02-24T12:34:08.091177Z","shell.execute_reply":"2022-02-24T12:34:08.106362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Missing Values \n\nAround nine features have more than 90% null values","metadata":{}},{"cell_type":"code","source":"#Calculating the % of Null values in each column for activity data\npd.DataFrame(df.isnull().sum()/df.shape[0], columns=['Null Value %']).T","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:34:08.10812Z","iopub.execute_input":"2022-02-24T12:34:08.108525Z","iopub.status.idle":"2022-02-24T12:34:09.113262Z","shell.execute_reply.started":"2022-02-24T12:34:08.108495Z","shell.execute_reply":"2022-02-24T12:34:09.112087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Around **nine** features have more than 90% null values. We can't do much to fixe these features.","metadata":{}},{"cell_type":"code","source":"activity_df = df[['people_id', 'activity_id', 'date', 'activity_category', 'char_10', 'outcome']].copy()\nactivity_df.loc[:, activity_df.dtypes=='object'] = activity_df.select_dtypes('object').apply(lambda x: x.astype('category'))","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:34:09.114654Z","iopub.execute_input":"2022-02-24T12:34:09.114872Z","iopub.status.idle":"2022-02-24T12:34:17.830273Z","shell.execute_reply.started":"2022-02-24T12:34:09.114847Z","shell.execute_reply":"2022-02-24T12:34:17.829372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"activity_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:34:17.833794Z","iopub.execute_input":"2022-02-24T12:34:17.834719Z","iopub.status.idle":"2022-02-24T12:34:19.326327Z","shell.execute_reply.started":"2022-02-24T12:34:17.834667Z","shell.execute_reply":"2022-02-24T12:34:19.325365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Rename the 2 columns to avoid name clashes in merged data\nactivity_df = activity_df.rename(columns={'date':'activity_date', 'char_10':'activity_type'})","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:34:19.32765Z","iopub.execute_input":"2022-02-24T12:34:19.327855Z","iopub.status.idle":"2022-02-24T12:34:19.345215Z","shell.execute_reply.started":"2022-02-24T12:34:19.32783Z","shell.execute_reply":"2022-02-24T12:34:19.344386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Replace nulls in the activity_type column with the mode\nactivity_df.activity_type = activity_df.activity_type.fillna(activity_df.activity_type.mode()[0])","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:34:19.346794Z","iopub.execute_input":"2022-02-24T12:34:19.347073Z","iopub.status.idle":"2022-02-24T12:34:19.376376Z","shell.execute_reply.started":"2022-02-24T12:34:19.347043Z","shell.execute_reply":"2022-02-24T12:34:19.375601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the shape of the final activity dataset\nprint(\"Shape of Activity DF:\", activity_df.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:34:19.377565Z","iopub.execute_input":"2022-02-24T12:34:19.37781Z","iopub.status.idle":"2022-02-24T12:34:19.382769Z","shell.execute_reply.started":"2022-02-24T12:34:19.377782Z","shell.execute_reply":"2022-02-24T12:34:19.381936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can now join the two datasets to create a consolidate activity and customer attributes dataset","metadata":{}},{"cell_type":"markdown","source":"## Explore the contents of the 'customer' dataset","metadata":{}},{"cell_type":"code","source":"people.head().T","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:34:19.384121Z","iopub.execute_input":"2022-02-24T12:34:19.384948Z","iopub.status.idle":"2022-02-24T12:34:19.408844Z","shell.execute_reply.started":"2022-02-24T12:34:19.384904Z","shell.execute_reply":"2022-02-24T12:34:19.408028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Missing Values\n\nNone of the columns in the customer dataset has missing values.","metadata":{}},{"cell_type":"code","source":"#Calculating the % of Null values in each column for activity data\npd.DataFrame(people.isnull().sum()/df.shape[0], columns=['Null Value %']).sum()","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:34:19.410266Z","iopub.execute_input":"2022-02-24T12:34:19.410768Z","iopub.status.idle":"2022-02-24T12:34:19.517572Z","shell.execute_reply.started":"2022-02-24T12:34:19.410722Z","shell.execute_reply":"2022-02-24T12:34:19.516714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"people.loc[:, people.dtypes=='object'] = people.select_dtypes('object').apply(lambda x: x.astype('category'))\npeople.loc[:, people.dtypes=='bool'] = people.select_dtypes('bool').apply(lambda x: x.astype('category'))","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:34:19.51864Z","iopub.execute_input":"2022-02-24T12:34:19.518853Z","iopub.status.idle":"2022-02-24T12:34:20.155997Z","shell.execute_reply.started":"2022-02-24T12:34:19.518827Z","shell.execute_reply":"2022-02-24T12:34:20.155247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"people.info()","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:34:20.156986Z","iopub.execute_input":"2022-02-24T12:34:20.157532Z","iopub.status.idle":"2022-02-24T12:34:20.195012Z","shell.execute_reply.started":"2022-02-24T12:34:20.157502Z","shell.execute_reply":"2022-02-24T12:34:20.193959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge the 2 datasets on 'people_id' key\ndf_new = activity_df.merge(people, on=[\"people_id\"], how=\"inner\")\nprint(\"Shape before merging:\",df.shape)\nprint(\"Shape after merging :\",df_new.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:34:20.196643Z","iopub.execute_input":"2022-02-24T12:34:20.196959Z","iopub.status.idle":"2022-02-24T12:34:20.968815Z","shell.execute_reply.started":"2022-02-24T12:34:20.196919Z","shell.execute_reply":"2022-02-24T12:34:20.967928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that there is a good mix in the distribution of potential customers, as around 45% are potential customers","metadata":{}},{"cell_type":"code","source":"print(\"Unique values for outcome:\",df_new[\"outcome\"].unique())\nprint(\"\\nPercentage of distribution for outcome-\")\nprint(df_new[\"outcome\"].value_counts()/df_new.shape[0])","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:34:20.970155Z","iopub.execute_input":"2022-02-24T12:34:20.970375Z","iopub.status.idle":"2022-02-24T12:34:21.002212Z","shell.execute_reply.started":"2022-02-24T12:34:20.970348Z","shell.execute_reply":"2022-02-24T12:34:21.001215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Engineering","metadata":{}},{"cell_type":"code","source":"df_new[['date', 'activity_date']].describe(datetime_is_numeric=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:34:21.003551Z","iopub.execute_input":"2022-02-24T12:34:21.004396Z","iopub.status.idle":"2022-02-24T12:34:21.18922Z","shell.execute_reply.started":"2022-02-24T12:34:21.004349Z","shell.execute_reply":"2022-02-24T12:34:21.188202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"date_ref = np.datetime64(\"2020-01-01\")\ndf_new['date_Day'] = (df_new.date - date_ref).dt.days\ndf_new['activity_date_Day'] = (df_new.activity_date - date_ref).dt.days\ndf_new[['date','date_Day', 'activity_date', 'activity_date_Day']]","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:34:21.190554Z","iopub.execute_input":"2022-02-24T12:34:21.190831Z","iopub.status.idle":"2022-02-24T12:34:21.367758Z","shell.execute_reply.started":"2022-02-24T12:34:21.19076Z","shell.execute_reply":"2022-02-24T12:34:21.366931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us now have a look at the remaining categorical columns, which have very high numbers of distinct values.\n\nIt seems that we can convert all of the preceding categorical columns into numeric by extracting the relevant numeric ID from each of them, \n\nsince each of these columns has values in the form of *someText_someNumber*. \n\nRather than converting these categorical columns into a bloated one-hot encoded dataset, we can temporarily use them as numeric features. \n\nHowever, if the performance of the model doesn’t reach our desired expectations after several experiments, we might have to revisit these features \n\nand try our best to incorporate them differently. But for now, we can consider them as numeric features","metadata":{}},{"cell_type":"code","source":"print(df_new[[\"people_id\",\"activity_type\",\"activity_id\", \"group_1\"]].head())","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:34:21.368943Z","iopub.execute_input":"2022-02-24T12:34:21.369164Z","iopub.status.idle":"2022-02-24T12:34:21.401275Z","shell.execute_reply.started":"2022-02-24T12:34:21.369138Z","shell.execute_reply":"2022-02-24T12:34:21.400574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For people ID, we would need to extract values after '_'\ndf_new.people_id = df_new.people_id.apply(lambda x: x.split(\"_\")[1])\ndf_new.people_id = pd.to_numeric(df_new.people_id)\n\n# For activity ID also, we would need to extract values after '_'\ndf_new.activity_id = df_new.activity_id.apply(lambda x: x.split(\"_\")[1])\ndf_new.activity_id = pd.to_numeric(df_new.activity_id)\n\n# For group_1 , we would need to extract values after ' '\ndf_new.group_1 = df_new.group_1.apply(lambda x: x.split(\" \")[1])\ndf_new.group_1 = pd.to_numeric(df_new.group_1)\n\n# For activity_type , we would need to extract values after ' '\ndf_new.activity_type = df_new.activity_type.apply(lambda x: x.split(\" \")[1])\ndf_new.activity_type = pd.to_numeric(df_new.activity_type)\n\n# Double check the new values in the dataframe\nprint(df_new[[\"people_id\",\"activity_type\",\"activity_id\", \"group_1\"]].head())","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:34:21.402339Z","iopub.execute_input":"2022-02-24T12:34:21.402531Z","iopub.status.idle":"2022-02-24T12:34:30.165831Z","shell.execute_reply.started":"2022-02-24T12:34:21.402507Z","shell.execute_reply":"2022-02-24T12:34:30.164958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  One Hot Encoding","metadata":{}},{"cell_type":"code","source":"categorical = [f\"char_{l}\" for l in np.arange(1,38)] + ['activity_category']\nnumerical = ['people_id', 'activity_id', 'activity_type', 'group_1', 'char_38', 'date_Day', 'activity_date_Day']","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:34:30.167399Z","iopub.execute_input":"2022-02-24T12:34:30.167689Z","iopub.status.idle":"2022-02-24T12:34:30.173816Z","shell.execute_reply.started":"2022-02-24T12:34:30.167649Z","shell.execute_reply":"2022-02-24T12:34:30.172906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df_new[categorical  + numerical].values\nprint(\"\\nShape of final df after onehot encoding:\",X.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:34:30.177409Z","iopub.execute_input":"2022-02-24T12:34:30.17767Z","iopub.status.idle":"2022-02-24T12:34:33.767745Z","shell.execute_reply.started":"2022-02-24T12:34:30.177638Z","shell.execute_reply":"2022-02-24T12:34:33.76683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nX = df_new[categorical  + numerical].values\ncolor_ohe = OneHotEncoder(categories='auto', drop='first')\nc_tranf = ColumnTransformer([\n    ('onehot', color_ohe, np.arange(0,38)),\n    ('nothing', 'passthrough', np.arange(38, 45))\n])\nX_tranf = c_tranf.fit_transform(X).astype(float)\nprint(\"\\nShape of final df after onehot encoding:\",X_tranf.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:34:33.7694Z","iopub.execute_input":"2022-02-24T12:34:33.769715Z","iopub.status.idle":"2022-02-24T12:35:03.151001Z","shell.execute_reply.started":"2022-02-24T12:34:33.769671Z","shell.execute_reply":"2022-02-24T12:35:03.150026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split Datasets 60:20:20\n\nFinally, before we begin with the model development, we need to split our datasets into train, validation, and test","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = X_tranf\ny = df_new[['outcome']].values\n\nX_train_full, X_test, y_train_full, y_test = train_test_split(X, y,\n                                                    stratify=y, \n                                                    test_size=0.2, random_state=16)\nX_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full,\n                                                    stratify=y_train_full, \n                                                    test_size=0.25, random_state=16)\n\n# Check the shape of each new dataset created\nprint(\"Shape of X_train: \",X_train.shape)\nprint(\"Shape of y_train: \",y_train.shape)\nprint(\"\\nShape of X_val: \",X_val.shape)\nprint(\"Shape of y_val: \",y_val.shape)\nprint(\"\\nShape of X_test: \",X_test.shape)\nprint(\"Shape of y_test: \",y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:35:03.152302Z","iopub.execute_input":"2022-02-24T12:35:03.152512Z","iopub.status.idle":"2022-02-24T12:35:19.727565Z","shell.execute_reply.started":"2022-02-24T12:35:03.152487Z","shell.execute_reply":"2022-02-24T12:35:19.725289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining Model Baseline Accuracy\n\nWe can see that there is a good mix in the distribution of potential customers, as around 45% are potential customers in each partition like in the population.\n\nWe can say that if we do not have any model and make all predictions as 0 (the largest class)\n\n—that is, predicting that none of the customers are potential high-value customers— then we would end up with at least 55.6% accuracy either way. \n\nThis is our baseline accuracy. If we build a model that delivers us an overall accuracy anywhere below our benchmark, then it would be of practically no use.","metadata":{}},{"cell_type":"code","source":"unique, counts = np.unique(y_train, return_counts=True)\nprint(\"Unique values for outcome:\",unique)\n\nprint(\"\\nPercentage of distribution for outcome in the training\")\nresult = np.column_stack((unique, counts/y_train.shape[0])).round(3) \nprint (result)\n\nunique, counts = np.unique(y_val, return_counts=True)\nprint(\"\\nPercentage of distribution for outcome in the validation\")\nresult = np.column_stack((unique, counts/y_val.shape[0])).round(3) \nprint (result)\n\nunique, counts = np.unique(y_test, return_counts=True)\nprint(\"\\nPercentage of distribution for outcome in the validation\")\nresult = np.column_stack((unique, counts/y_test.shape[0])).round(3) \nprint (result)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:35:19.729142Z","iopub.execute_input":"2022-02-24T12:35:19.729376Z","iopub.status.idle":"2022-02-24T12:35:19.786323Z","shell.execute_reply.started":"2022-02-24T12:35:19.729349Z","shell.execute_reply":"2022-02-24T12:35:19.785351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Designing the DNN for Classification\n\nThe following code snippet builds a *DNN* with just one layer and 256 neurons. \n\nWe choose *binary_crossentropy* (since this a binary classification problem) as the loss function and *accuracy* as the metric to monitor","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\n\n# Design the deep neural network [Small + 1 layer]\nmodel = Sequential()\nmodel.add(Dense(256, input_dim=X_train.shape[1], activation=\"relu\"))\n\n# activation = sigmoid for binary classification\nmodel.add(Dense(1, activation = \"sigmoid\"))\n\nmodel.compile(optimizer=\"Adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\nhistory = model.fit(X_train,y_train, validation_data=(X_val,y_val), epochs=3, batch_size=64, verbose=2)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:35:19.787781Z","iopub.execute_input":"2022-02-24T12:35:19.788117Z","iopub.status.idle":"2022-02-24T12:39:42.950528Z","shell.execute_reply.started":"2022-02-24T12:35:19.788075Z","shell.execute_reply":"2022-02-24T12:39:42.949831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you closely observe the results from the training output, you will see that the overall accuracy for training as well as validation datasets was around 0.556 (56%), which is identic to our baseline accuracy. \n\nWe can therefore conclude that training this model further might not be a fruitful idea.\n\nLet’s try a deeper network for the same number of neurons. So, we keep everything the same but add one more layer with the same number of \nneurons","metadata":{}},{"cell_type":"code","source":"# Design the deep neural network [Small + 2 layer]\nmodel = Sequential()\nmodel.add(Dense(256, input_dim=X_train.shape[1], activation=\"relu\"))\nmodel.add(Dense(256, activation=\"relu\"))\n\n# activation = sigmoid for binary classification\nmodel.add(Dense(1, activation = \"sigmoid\"))\n\nmodel.compile(optimizer=\"Adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\nhistory = model.fit(X_train,y_train, validation_data=(X_val,y_val), epochs=3, batch_size=64, verbose=2)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:39:43.319961Z","iopub.execute_input":"2022-02-24T12:39:43.320949Z","iopub.status.idle":"2022-02-24T12:44:41.630886Z","shell.execute_reply.started":"2022-02-24T12:39:43.320867Z","shell.execute_reply":"2022-02-24T12:44:41.629964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again, as we can see, the initial results are not at all promising. The training and validation accuracy from the deeper network close to what we would expect. \n\nInstead of trying another deeper network with, say, three to five layers, let us try training with a bigger (medium-sized) network. \n\nWe shall use a new architecture with just one layer but 512 neurons this time. \n\nLet us again train for three epochs and have a look at the metrics to check whether it is in line with what we would expect.","metadata":{}},{"cell_type":"code","source":"# Design the deep neural network [Medium + 1 layer]\nmodel = Sequential()\nmodel.add(Dense(512, input_dim=X_train.shape[1], activation=\"relu\"))\n\n# activation = sigmoid for binary classification\nmodel.add(Dense(1, activation = \"sigmoid\"))\n\nmodel.compile(optimizer=\"Adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\nhistory = model.fit(X_train,y_train, validation_data=(X_val,y_val), epochs=3, batch_size=64, verbose=2)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:44:41.636625Z","iopub.execute_input":"2022-02-24T12:44:41.636924Z","iopub.status.idle":"2022-02-24T12:50:11.294547Z","shell.execute_reply.started":"2022-02-24T12:44:41.636894Z","shell.execute_reply":"2022-02-24T12:50:11.293406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let’s now try increasing the depth for the medium-sized network to see if the results improve more.","metadata":{}},{"cell_type":"code","source":"# Design the deep neural network [Medium + 2 layer]\nmodel = Sequential()\nmodel.add(Dense(512, input_dim=X_train.shape[1], activation=\"relu\"))\nmodel.add(Dense(512, activation=\"relu\"))\n\n# activation = sigmoid for binary classification\nmodel.add(Dense(1, activation = \"sigmoid\"))\n\nmodel.compile(optimizer=\"Adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\nhistory = model.fit(X_train,y_train, validation_data=(X_val,y_val), epochs=3, batch_size=64, verbose=2)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:50:11.676685Z","iopub.execute_input":"2022-02-24T12:50:11.67764Z","iopub.status.idle":"2022-02-24T12:57:51.088135Z","shell.execute_reply.started":"2022-02-24T12:50:11.677597Z","shell.execute_reply":"2022-02-24T12:57:51.087314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Revisiting the Data\n\n### Standardize, Normalize, or Scale the Data\n\nIn standardization, we transform the data into a form where the mean is 0 and the standard deviation is 1. \n\nThe distribution of the data in this form is a great input candidate for our neuron’s activation function and therefore improves the ability to learn more appropriately","metadata":{}},{"cell_type":"markdown","source":"### Transforming the Input Data\n\nTo transform the input data for the development of the model, we should only use the training data to fit the scaler transformation and use the same fitted object to transform the validation and test input data","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler(with_mean=False)\nscaler.fit(X_train)\n\nX_train_scaled = scaler.transform(X_train)\nX_val_scaled = scaler.transform(X_val)\nX_test_scaled = scaler.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:57:51.485112Z","iopub.execute_input":"2022-02-24T12:57:51.48551Z","iopub.status.idle":"2022-02-24T12:57:52.675323Z","shell.execute_reply.started":"2022-02-24T12:57:51.485475Z","shell.execute_reply":"2022-02-24T12:57:52.674387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DNNs for Classification with Improved Data","metadata":{}},{"cell_type":"code","source":"# Design the deep neural network [Medium + 1 layer]\nmodel = Sequential()\nmodel.add(Dense(512, input_dim=X_train_scaled.shape[1], activation=\"relu\"))\n\n# activation = sigmoid for binary classification\nmodel.add(Dense(1, activation = \"sigmoid\"))\n\nmodel.compile(optimizer=\"Adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\nhistory = model.fit(X_train_scaled,y_train, validation_data=(X_val_scaled,y_val), epochs=3, batch_size=64, verbose=2)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T12:57:52.676717Z","iopub.execute_input":"2022-02-24T12:57:52.677077Z","iopub.status.idle":"2022-02-24T13:03:55.385267Z","shell.execute_reply.started":"2022-02-24T12:57:52.677033Z","shell.execute_reply":"2022-02-24T13:03:55.384351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, there we go!\n\nWe can see the drastic improvement in the performance of the network in providing the standardized datasets. We have an almost 94% accuracy \non the training and validation datasets. \n\nLet’s use this model to evaluate the model performance on the test datasets we created earlier","metadata":{}},{"cell_type":"code","source":"result = model.evaluate(X_test_scaled,y_test)\n\nfor i in range(len(model.metrics_names)):\n    print(\"Metric \",model.metrics_names[i],\":\", str(round(result[i],2)))","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:04:15.845018Z","iopub.execute_input":"2022-02-24T13:04:15.845356Z","iopub.status.idle":"2022-02-24T13:04:58.197373Z","shell.execute_reply.started":"2022-02-24T13:04:15.845323Z","shell.execute_reply":"2022-02-24T13:04:58.196416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see great results on the test dataset. Let’s try improving the architecture a bit and see. \n\nWe can a medium-sized deeper network to see if the results are better than with the medium-sized network","metadata":{}},{"cell_type":"code","source":"# Design the deep neural network [Medium + 2 layer]\nmodel = Sequential()\nmodel.add(Dense(512, input_dim=X_train_scaled.shape[1], activation=\"relu\"))\nmodel.add(Dense(512, activation=\"relu\"))\n\n# activation = sigmoid for binary classification\nmodel.add(Dense(1, activation = \"sigmoid\"))\n\nmodel.compile(optimizer=\"Adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\nhistory = model.fit(X_train_scaled,y_train, validation_data=(X_val_scaled,y_val), epochs=3, batch_size=64, verbose=2)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:13:22.950778Z","iopub.execute_input":"2022-02-24T13:13:22.951116Z","iopub.status.idle":"2022-02-24T13:21:47.418706Z","shell.execute_reply.started":"2022-02-24T13:13:22.951083Z","shell.execute_reply":"2022-02-24T13:21:47.417749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The training and validation accuracy has improved even further to 95%. This small increase with just 3 epochs is awesome. \n\nWe can now be confident of the performance for the model with the architecture. \n\nWe can definitely try many more architectures and check the results, but let’s take a final shot with a larger and deeper network and see the results with 3 \nepochs. \n\nIn case we see only small improvements, we will use the same architecture for 25 epochs and use the model for our final predictions.","metadata":{}},{"cell_type":"code","source":"# Design the deep neural network [Large + 2 layer]\nmodel = Sequential()\nmodel.add(Dense(1024, input_dim=X_train_scaled.shape[1], activation=\"relu\"))\nmodel.add(Dense(1024, activation=\"relu\"))\n\n# activation = sigmoid for binary classification\nmodel.add(Dense(1, activation = \"sigmoid\"))\n\nmodel.compile(optimizer=\"Adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\nhistory = model.fit(X_train_scaled,y_train, validation_data=(X_val_scaled,y_val), epochs=3, batch_size=64, verbose=2)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:24:26.802169Z","iopub.execute_input":"2022-02-24T13:24:26.802686Z","iopub.status.idle":"2022-02-24T13:39:56.138218Z","shell.execute_reply.started":"2022-02-24T13:24:26.802652Z","shell.execute_reply":"2022-02-24T13:39:56.13709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see an overall accuracy on the validation dataset as 95% and a similar score for the training dataset. \n\nSo, there really isn’t a lot of improvement in the performance of the model due to increasing the size from a medium (512-neuron) to a larger (1024-neuron) architecture. \n\nWith these results validating our experiments, let’s train a medium-sized (512-neuron) deep network with two layers for 25 epochs, look at the final training and validation accuracy, and then use the trained model to evaluate the test datasets.","metadata":{}},{"cell_type":"code","source":"# Design the deep neural network [Medium + 2 layer]\nmodel = Sequential()\nmodel.add(Dense(512, input_dim=X_train_scaled.shape[1], activation=\"relu\"))\nmodel.add(Dense(512, activation=\"relu\"))\n\n# activation = sigmoid for binary classification\nmodel.add(Dense(1, activation = \"sigmoid\"))\n\nmodel.compile(optimizer=\"Adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\nhistory = model.fit(X_train_scaled,y_train, validation_data=(X_val_scaled,y_val), epochs=25, batch_size=64, verbose=2)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:45:57.191301Z","iopub.execute_input":"2022-02-24T13:45:57.191633Z","iopub.status.idle":"2022-02-24T14:52:08.803297Z","shell.execute_reply.started":"2022-02-24T13:45:57.191596Z","shell.execute_reply":"2022-02-24T14:52:08.801287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The final model with a medium-size architecture of 512 neurons and two layers gave great performance results on the training and validation datasets. \n\nWe have an accuracy of ~98% for both datasets. \n\nLet us now validate the model performance on the test dataset.","metadata":{}},{"cell_type":"code","source":"result = model.evaluate(X_test_scaled,y_test)\n\nfor i in range(len(model.metrics_names)):\n    print(\"Metric \",model.metrics_names[i],\":\", str(round(result[i],2)))","metadata":{"execution":{"iopub.status.busy":"2022-02-24T14:52:09.18009Z","iopub.execute_input":"2022-02-24T14:52:09.18139Z","iopub.status.idle":"2022-02-24T14:52:40.576246Z","shell.execute_reply.started":"2022-02-24T14:52:09.18133Z","shell.execute_reply":"2022-02-24T14:52:40.57561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The performance on the unseen test dataset is also great and consistent. \n\nOur model is performing really well on the test dataset. \n\nLet us have a look at the loss curve for the model. \n\nWe will plot the loss in each epoch (25 in total for this mode) for the training and validation datasets","metadata":{}},{"cell_type":"code","source":"# list all data in history\nprint(history.history.keys())","metadata":{"execution":{"iopub.status.busy":"2022-02-24T14:52:40.582755Z","iopub.execute_input":"2022-02-24T14:52:40.583453Z","iopub.status.idle":"2022-02-24T14:52:40.60255Z","shell.execute_reply.started":"2022-02-24T14:52:40.583415Z","shell.execute_reply":"2022-02-24T14:52:40.60144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title(\"Model's Training & Validation loss across epochs\")\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-24T14:52:40.613399Z","iopub.execute_input":"2022-02-24T14:52:40.613775Z","iopub.status.idle":"2022-02-24T14:52:40.813422Z","shell.execute_reply.started":"2022-02-24T14:52:40.613725Z","shell.execute_reply":"2022-02-24T14:52:40.812224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title(\"Model's Training & Validation Accuracy across epochs\")\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend(['Train', 'Validation'], loc='lower right')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-02-24T14:52:40.814699Z","iopub.execute_input":"2022-02-24T14:52:40.815026Z","iopub.status.idle":"2022-02-24T14:52:40.980361Z","shell.execute_reply.started":"2022-02-24T14:52:40.814984Z","shell.execute_reply":"2022-02-24T14:52:40.979717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using the model\n\nWe now have a model, we can use it for submission","metadata":{}},{"cell_type":"markdown","source":"### Preprocessing ","metadata":{}},{"cell_type":"code","source":"act_test = pd.read_csv(\"/kaggle/input/predicting-red-hat-business-value/act_test.csv.zip\", parse_dates=['date'])\n# date_ref = np.datetime64(\"2020-01-01\")\n\nact_test_df = act_test[['people_id', 'activity_id', 'date', 'activity_category', 'char_10']]\nact_test_df = act_test_df.rename(columns={'date':'activity_date', 'char_10':'activity_type'})\nact_test_df.activity_type = act_test_df.activity_type.fillna(act_test_df.activity_type.mode()[0])\n\nact_test_df_new = act_test_df.merge(people, on=[\"people_id\"], how=\"inner\")\n\nact_test_df_new['date_Day'] = (act_test_df_new.date - date_ref).dt.days\nact_test_df_new['activity_date_Day'] = (act_test_df_new.activity_date - date_ref).dt.days\n\n# For people ID, we would need to extract values after '_'\nact_test_df_new.people_id = act_test_df_new.people_id.apply(lambda x: x.split(\"_\")[1])\nact_test_df_new.people_id = pd.to_numeric(act_test_df_new.people_id)\n\n# For activity ID also, we would need to extract values after '_'\nact_test_df_new.activity_id = act_test_df_new.activity_id.apply(lambda x: x.split(\"_\")[1])\nact_test_df_new.activity_id = pd.to_numeric(act_test_df_new.activity_id)\n\n# For group_1 , we would need to extract values after ' '\nact_test_df_new.group_1 = act_test_df_new.group_1.apply(lambda x: x.split(\" \")[1])\nact_test_df_new.group_1 = pd.to_numeric(act_test_df_new.group_1)\n\n# For activity_type , we would need to extract values after ' '\nact_test_df_new.activity_type = act_test_df_new.activity_type.apply(lambda x: x.split(\" \")[1])\nact_test_df_new.activity_type = pd.to_numeric(act_test_df_new.activity_type)\n\nact_test_X = act_test_df_new[categorical  + numerical].values\nact_test_X_tranf = c_tranf.transform(act_test_X).astype(float)\n\nact_test_X_tranf_scaled = scaler.transform(act_test_X_tranf)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T14:52:40.981531Z","iopub.execute_input":"2022-02-24T14:52:40.982047Z","iopub.status.idle":"2022-02-24T14:52:51.645828Z","shell.execute_reply.started":"2022-02-24T14:52:40.982001Z","shell.execute_reply":"2022-02-24T14:52:51.644802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prediction","metadata":{}},{"cell_type":"code","source":"prediction = model.predict(act_test_X_tranf_scaled)\n\nsample_submission = pd.read_csv(\"/kaggle/input/predicting-red-hat-business-value/sample_submission.csv.zip\")\nsample_submission['outcome'] = prediction.tolist()\nsample_submission['outcome'] = sample_submission['outcome'].apply(lambda x: int(x[0]))\nsample_submission.to_csv(\"submission.csv\", index=False)\nsample_submission","metadata":{"execution":{"iopub.status.busy":"2022-02-24T14:52:51.647289Z","iopub.execute_input":"2022-02-24T14:52:51.647725Z","iopub.status.idle":"2022-02-24T14:53:22.561527Z","shell.execute_reply.started":"2022-02-24T14:52:51.647678Z","shell.execute_reply":"2022-02-24T14:53:22.560556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.outcome.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-02-24T14:53:22.56322Z","iopub.execute_input":"2022-02-24T14:53:22.563651Z","iopub.status.idle":"2022-02-24T14:53:22.576448Z","shell.execute_reply.started":"2022-02-24T14:53:22.563604Z","shell.execute_reply":"2022-02-24T14:53:22.575571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}