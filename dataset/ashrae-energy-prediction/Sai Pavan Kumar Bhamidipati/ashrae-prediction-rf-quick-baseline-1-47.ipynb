{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport gc\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_log_error, mean_squared_error\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/ashrae-energy-prediction'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Read Files"},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n \n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"build_meta = pd.read_csv('/kaggle/input/ashrae-energy-prediction/building_metadata.csv')\nbuild_meta = reduce_mem_usage(build_meta)\nweather_train = pd.read_csv('/kaggle/input/ashrae-energy-prediction/weather_train.csv')\nweather_train = reduce_mem_usage(weather_train)\nweather_test = pd.read_csv('/kaggle/input/ashrae-energy-prediction/weather_test.csv')\nweather_test = reduce_mem_usage(weather_test)\ntrain_df = pd.read_csv('/kaggle/input/ashrae-energy-prediction/train.csv')\ntrain_df = reduce_mem_usage(train_df)\ntest_df = pd.read_csv('/kaggle/input/ashrae-energy-prediction/test.csv')\ntest_df = reduce_mem_usage(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df.shape[0])\nprint(test_df.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import dask.dataframe as dd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Join building & weather based on site_id\ntrain_build_df = dd.merge(train_df,build_meta,on='building_id',how='left')\nprint(train_build_df.shape[0])\ntrain_2_df = dd.merge(train_build_df,weather_train,on=['site_id','timestamp'],how='left')\nprint(train_2_df.shape[0])\n\n# Join test with building meta data & weather data\ntest_build_df = dd.merge(test_df,build_meta,on='building_id',how='left')\nprint(test_build_df.shape[0])\ntest_2_df = dd.merge(test_build_df,weather_test,on=['site_id','timestamp'],how='left')\nprint(test_2_df.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_build_df\ndel test_build_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_2_df.info())","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"print(test_2_df.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ~ 20Million records in train\n* ~ 40 Million records in test"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_2_df.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_2_df.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Missing values in both train & test data sets\n* Ignore those columns for now"},{"metadata":{},"cell_type":"markdown","source":"<font color=blue> As Air_temp,dew_temp & wind_speed missing values are less in number, replace those missing values with mean </font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_final_df = train_2_df[['building_id','meter','timestamp','site_id','primary_use','square_feet','meter_reading']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Convert timestamp column to datetime\n* Extract Day of week, weekday/weekend, Office hours/Post Office hours"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_final_df['timestamp'] = pd.to_datetime(train_final_df['timestamp'], format='%Y-%m-%d %H:%M:%S')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_final_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_final_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_final_df['meter'] = train_final_df['meter'].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_final_df['Day_of_Week'] = train_final_df['timestamp'].dt.weekday\ntrain_final_df['Hour_of_Day'] = train_final_df['timestamp'].dt.hour","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Weekday_Weekend(df):\n    df['Weekend'] = df['Day_of_Week'].apply(lambda x: 'Y' if x>5 else 'N')\n\ndef Office_Hour(df):\n    df['OfficeTime'] = df['Hour_of_Day'].apply(lambda x: 'Y' if (x>=7 & x<=18) else 'N')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from dask.multiprocessing import get","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ddata_train = dd.from_pandas(train_final_df,npartitions=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ddata_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ddout = ddata_train.map_partitions(Weekday_Weekend)\nresult = ddout.compute()\n\nddout = ddata_train.map_partitions(Office_Hour)\nresult = ddout.compute()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ddata_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_final_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# COnvert back to pandas dataframe\ntrain_final_1_df = ddata_train.compute()\ntrain_final_1_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_final_1_df = train_final_1_df[['building_id','meter','primary_use','square_feet','Weekend','OfficeTime','Hour_of_Day','meter_reading']]\ntrain_final_1_df = pd.get_dummies(train_final_1_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import RandomForestRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n# Split the train to 2 data sets, using 1 for training & other for validation\nnrow = train_final_1_df.shape[0]\ntrain = train_final_1_df[:round(nrow/2)]\ntest = train_final_1_df[round(nrow/2)+1:]\n\n\n# Split to train & validation\n#train , test = train_test_split(train_final_1_df, test_size = 0.3)\n\n\nx_train = train.drop('meter_reading', axis=1)\ny_train = train['meter_reading']\nx_test = test.drop('meter_reading', axis = 1)\ny_test = test['meter_reading']\n\n\n# Scale the features\n#scaler = MinMaxScaler(feature_range=(0, 1)\n#x_train_scaled = scaler.fit_transform(x_train)\n#x_train = pd.DataFrame(x_train_scaled)\n#x_test_scaled = scaler.fit_transform(x_test)\n#x_test = pd.DataFrame(x_test_scaled)\n\n\nfrom sklearn.model_selection import GridSearchCV\nimport sklearn\n# Set parameters for grid search\nparam_grid = [\n{'n_estimators':[100,150],\n'criterion':['mae'],\n'max_features':['auto'],\n'min_impurity_decrease':[0],\n'n_jobs':[-1],\n'bootstrap':[False]}\n]\n\n# Fit the model\nxgbreg = RandomForestRegressor(random_state=123,n_estimators=200,n_jobs=-1,verbose=2)\nxgbreg.fit(x_train, y_train)\n\n# Predict for test\n#best_model_RF = grid_search.best_estimator_\ny_pred=xgbreg.predict(x_test)\nprint('RMSLE :',np.sqrt(mean_squared_log_error(y_test+1,y_pred+1)))\nmodel_name = 'Model_1.sav'\npickle.dump(xgbreg, open(model_name, 'wb'))\n\n\n\n# 2nd round\ntrain = train_final_1_df[round(nrow/2)+1:]\ntest = train_final_1_df[:round(nrow/2)]\n\n\n# Split to train & validation\n#train , test = train_test_split(train_final_1_df, test_size = 0.3)\n\n\nx_train = train.drop('meter_reading', axis=1)\ny_train = train['meter_reading']\nx_test = test.drop('meter_reading', axis = 1)\ny_test = test['meter_reading']\n\n\n# Scale the features\n#scaler = MinMaxScaler(feature_range=(0, 1)\n#x_train_scaled = scaler.fit_transform(x_train)\n#x_train = pd.DataFrame(x_train_scaled)\n#x_test_scaled = scaler.fit_transform(x_test)\n#x_test = pd.DataFrame(x_test_scaled)\n\n\nfrom sklearn.model_selection import GridSearchCV\nimport sklearn\n# Set parameters for grid search\nparam_grid = [\n{'n_estimators':[100,150],\n'criterion':['mae'],\n'max_features':['auto'],\n'min_impurity_decrease':[0],\n'n_jobs':[-1],\n'bootstrap':[False]}\n]\n\n# Fit the model\nxgbreg = RandomForestRegressor(random_state=123,n_estimators=200,n_jobs=-1,verbose=2)\nxgbreg.fit(x_train, y_train)\n\n# Predict for test\n#best_model_RF = grid_search.best_estimator_\ny_pred=xgbreg.predict(x_test)\nprint('RMSLE :',np.sqrt(mean_squared_log_error(y_test+1,y_pred+1)))\n\nmodel_name = 'Model_2.sav'\npickle.dump(xgbreg, open(model_name, 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"test_final_df = test_2_df[['building_id','meter','timestamp','site_id','primary_use','square_feet']]\ntest_final_df['timestamp'] = pd.to_datetime(test_final_df['timestamp'], format='%Y-%m-%d %H:%M:%S')\ntest_final_df['meter'] = test_final_df['meter'].astype('category')\ntest_final_df['Day_of_Week'] = test_final_df['timestamp'].dt.weekday\ntest_final_df['Hour_of_Day'] = test_final_df['timestamp'].dt.hour\n\nddata_test = dd.from_pandas(test_final_df,npartitions=40)\nddout_test = ddata_test.map_partitions(Weekday_Weekend)\nresult = ddout_test.compute()\n\nddout_test = ddata_test.map_partitions(Office_Hour)\nresult = ddout_test.compute()\n\n\n# COnvert back to pandas dataframe\ntest_final_1_df = ddata_test.compute()\ntest_final_1_df.info()\n\ntest_final_1_df = test_final_1_df[['building_id','meter','primary_use','square_feet','Weekend','OfficeTime','Hour_of_Day']]\ntest_final_1_df = pd.get_dummies(test_final_1_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the models \nmodel_name1 = 'Model_1.sav'\nmodel_name2 = 'Model_2.sav'\nmodel1 = pickle.load(open(model_name1, 'rb'))\nmodel2 = pickle.load(open(model_name2, 'rb'))\ny_pred_1 = (model1.predict(test_final_1_df))\ny_pred_2 = (model2.predict(test_final_1_df))\nresult = pd.DataFrame({'row_id':test_2_df['row_id'],'pred_1':y_pred_1,'pred_2':y_pred_2})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()\nresult['meter_reading'] = (result['pred_1']+result['pred_2'])/2\nfinal = result[['row_id','meter_reading']]\nfinal.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}