{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from datetime import datetime, timedelta\nfrom tqdm import tqdm_notebook as tqdm\nimport gc\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\n\nimport lightgbm as lgb\n\nSEED = 42","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_pickle('../input/ashrae-preprocessing/train.pkl')\ndf_weather = pd.read_pickle('../input/ashrae-preprocessing/weather.pkl')\ndf_building_metadata = pd.read_pickle('../input/ashrae-preprocessing/building_metadata.pkl')\n\ndf_train = df_train.merge(df_building_metadata, on='building_id', how='left')\ndf_train = df_train.merge(df_weather, on=['site_id', 'timestamp'], how='left')\n\nprint('Training Set Shape = {}'.format(df_train.shape))\nprint('Training Set Memory Usage = {:.2f} MB'.format(df_train.memory_usage().sum() / 1024**2))\n\ndel df_building_metadata, df_weather\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_cols = ['sea_level_pressure', 'wind_direction', 'IsHoliday']\ndf_train.drop(columns=drop_cols, inplace=True)\n    \ngc.collect()\nprint(f'{len(drop_cols)} columns are dropped.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = np.log1p(df_train['meter_reading'])\ndf_train.drop(columns=['meter_reading'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_param = {\n    'num_leaves': 2**9, \n    'learning_rate': 0.05,\n    'bagging_fraction': 0.9,\n    'bagging_freq': 5,\n    'feature_fraction': 0.8,\n    'feature_fraction_bynode': 0.7,\n    'min_data_in_leaf': 1000,\n    'max_depth': -1, \n    'objective': 'regression',\n    'seed': SEED,\n    'feature_fraction_seed': SEED,\n    'bagging_seed': SEED,\n    'drop_seed': SEED,\n    'data_random_seed': SEED,\n    'boosting_type': 'gbdt',\n    'verbose': 1,\n    'metric': 'rmse',\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nCATEGORICALS = ['building_id', 'meter', 'primary_use', 'HourOfDay', 'DayOfWeek', 'site_id']\nN = 5\nkf = KFold(n_splits=N)\n\nimportance = pd.DataFrame(np.zeros((df_train.shape[1] - 1, N)), columns=['Fold_{}'.format(i) for i in range(1, N + 1)], index=df_train.drop(columns=['timestamp']).columns)\nscores = []\noof = np.zeros(df_train.shape[0])\nmodels = []\n\nfor fold, (trn_idx, val_idx) in enumerate(kf.split(df_train), 1):\n    print('Fold {}'.format(fold))\n              \n    trn_data = lgb.Dataset(df_train.iloc[trn_idx, :].drop(columns=['timestamp']), label=target.iloc[trn_idx], categorical_feature=CATEGORICALS)\n    val_data = lgb.Dataset(df_train.iloc[val_idx, :].drop(columns=['timestamp']), label=target.iloc[val_idx], categorical_feature=CATEGORICALS)   \n    \n    model = lgb.train(lgb_param, trn_data, 200, valid_sets=[trn_data, val_data], verbose_eval=50)\n    models.append(model)\n\n    predictions = model.predict(df_train.iloc[val_idx, :].drop(columns=['timestamp']), num_iteration=model.best_iteration) \n    importance.iloc[:, fold - 1] = model.feature_importance()\n    oof[val_idx] = predictions\n\n    score = np.sqrt(mean_squared_error(target.iloc[val_idx].values, predictions))\n    scores.append(score)\n    print('Fold {} RMSLE {}\\n'.format(fold, score))\n    \n    del trn_data, val_data, predictions\n    gc.collect()\n    \nprint('Mean RMSLE {} [STD:{}]'.format(np.mean(scores), np.std(scores)))\nprint('OOF RMSLE {}'.format(np.sqrt(mean_squared_error(target, oof))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"splits = [split for split in kf.split(df_train)]\n\nfig, axes = plt.subplots(ncols=2, nrows=N, figsize=(20, 40), dpi=100)\n\nfor i, split in enumerate(splits):\n    df_train.loc[split[0]].groupby(['timestamp'])['timestamp'].count().plot(label=f'Fold {i} Training', ax=axes[i][0])\n    df_train.loc[split[1]].groupby(['timestamp'])['timestamp'].count().plot(label=f'Fold {i} Validation', ax=axes[i][0])\n\n    df_train.loc[split[0]].groupby(['site_id'])['site_id'].count().plot(label=f'Fold {i} Training', ax=axes[i][1])\n    df_train.loc[split[1]].groupby(['site_id'])['site_id'].count().plot(label=f'Fold {i} Validation', ax=axes[i][1])\n    \n    axes[i][0].legend()\n    axes[i][1].legend()\n    axes[i][0].set_title(f'Fold {i} Timestamp Count RMSLE=[{np.round(scores[i], 5)}]', fontsize=13)\n    axes[i][1].set_title(f'Fold {i} Site Count RMSLE=[{np.round(scores[i], 5)}]', fontsize=13)\n\n    plt.subplots_adjust(hspace=0.45)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importance['Mean_Importance'] = importance.sum(axis=1) / N\nimportance.sort_values(by='Mean_Importance', inplace=True, ascending=False)\n\nplt.figure(figsize=(15, 30))\nsns.barplot(x='Mean_Importance', y=importance.index, data=importance)\n\nplt.xlabel('')\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\nplt.title('LightGBM Feature Importance', size=15)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_pickle('../input/ashrae-preprocessing/test.pkl')\ndf_weather = pd.read_pickle('../input/ashrae-preprocessing/weather.pkl')\ndf_building_metadata = pd.read_pickle('../input/ashrae-preprocessing/building_metadata.pkl')\n\ndf_test = df_test.merge(df_building_metadata, on='building_id', how='left')\ndf_test = df_test.merge(df_weather, on=['site_id', 'timestamp'], how='left')\n\nprint('Test Set Shape = {}'.format(df_test.shape))\nprint('Test Set Memory Usage = {:.2f} MB'.format(df_test.memory_usage().sum() / 1024**2))\n\ndel df_building_metadata, df_weather, df_train, target\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_cols = ['sea_level_pressure', 'wind_direction', 'meter_reading', 'timestamp', 'IsHoliday']\n\ndf_test.drop(columns=drop_cols, inplace=True)\n    \ngc.collect()    \nprint(f'{len(drop_cols)} columns are dropped.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N_SAMPLES = len(df_test)\nITERATIONS = 50\nbatch_size = N_SAMPLES // ITERATIONS\n\ny_pred = []\n\nfor i in tqdm(range(ITERATIONS)):\n    start = i * batch_size\n    fold_preds = [np.expm1(model.predict(df_test.iloc[start:start + batch_size])) for model in models]\n    y_pred.extend(np.mean(fold_preds, axis=0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/ashrae-energy-prediction/sample_submission.csv')\nsubmission['meter_reading'] = np.clip(y_pred, a_min=0, a_max=None)\nsubmission.to_csv('submission.csv', index=False)\nsubmission['meter_reading'].describe()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}