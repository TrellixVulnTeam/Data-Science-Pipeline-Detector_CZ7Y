{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Team ASYCH notebook"},{"metadata":{},"cell_type":"markdown","source":"*First we installed the necessary libraries:*"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport gc\n\nimport matplotlib.pyplot as plt # matplotlib and seaborn for plotting\nimport matplotlib.patches as patches\nimport seaborn as sns\n\nfrom plotly import tools, subplots\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.express as px\npd.set_option('max_columns', 150)\npy.init_notebook_mode(connected=True)\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport os,random, math, psutil, pickle\n\nfrom time import time\nimport datetime\npd.set_option('display.max_columns',100)\npd.set_option('display.float_format', lambda x: '%.5f' % x)\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split,KFold\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\nroot = '../input/ashrae-energy-prediction'\nprint(os.listdir(root))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then we upload the dataframes. \nWe parse the datetime data and use specific dtypes for the building and weather dataframe. \nAlso for the test dataframe we select to read especific columns."},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"train = pd.read_csv(root + \"/train.csv\", parse_dates=['timestamp'])\n\nweather_train = pd.read_csv(root+\"/weather_train.csv\",parse_dates=['timestamp'])\n\ntest_cols_to_read = ['building_id','meter','timestamp']\ntest = pd.read_csv(root+\"/test.csv\",parse_dates=['timestamp'],usecols=test_cols_to_read)\n\nweather_test = pd.read_csv(root + \"/weather_test.csv\", parse_dates=['timestamp'])\n\nbuilding_meta = pd.read_csv(root + \"/building_metadata.csv\")\n\nsample_submission = pd.read_csv(root + \"/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we take a look of the size of the tables:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Size of train data', train.shape)\nprint('Size of weather_train data', weather_train.shape)\nprint('Size of weather_test data', weather_test.shape)\nprint('Size of building_meta data', building_meta.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Timestamps Adjustments"},{"metadata":{"trusted":true},"cell_type":"code","source":"weather = pd.concat([weather_train,weather_test],ignore_index=True)\nweather_key = ['site_id', 'timestamp']\ntemp_skeleton = weather[weather_key + ['air_temperature']].drop_duplicates(subset=weather_key).sort_values(by=weather_key).copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_to_plot = temp_skeleton.copy()\ndata_to_plot[\"hour\"] = data_to_plot[\"timestamp\"].dt.hour\ncount = 1\nplt.figure(figsize=(25, 15))\nfor site_id, data_by_site in data_to_plot.groupby('site_id'):\n    by_site_by_hour = data_by_site.groupby('hour').mean()\n    ax = plt.subplot(4, 4, count)\n    plt.plot(by_site_by_hour.index,by_site_by_hour['air_temperature'],'xb-')\n    ax.set_title('site: '+str(site_id))\n    count += 1\nplt.tight_layout()\nplt.show()\ndel data_to_plot","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We calculate ranks of hourly temperatures within date/site_id chunks.\n* Then create a dataframe of site_ids (0-16) x mean hour rank of temperature within day (0-23).\n* And we subtract the columnID of temperature peak by 14, getting the timestamp alignment gap.\n* Finally we do a function to align the timestamps."},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_skeleton['temp_rank'] = temp_skeleton.groupby(['site_id', temp_skeleton.timestamp.dt.date])['air_temperature'].rank('average')\n\ndf_2d = temp_skeleton.groupby(['site_id', temp_skeleton.timestamp.dt.hour])['temp_rank'].mean().unstack(level=1)\n\nsite_ids_offsets = pd.Series(df_2d.values.argmax(axis=1) - 14)\nsite_ids_offsets.index.name = 'site_id'\n\ndef timestamp_align(df):\n    df['offset'] = df.site_id.map(site_ids_offsets)\n    df['timestamp_aligned'] = (df.timestamp - pd.to_timedelta(df.offset, unit='H'))\n    df['timestamp'] = df['timestamp_aligned']\n    del df['timestamp_aligned']\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We reduce the memory size. \nFunction to reduce the DF size:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"building_site_dict = dict(zip(building_meta['building_id'], building_meta['site_id']))\nsite_meter_raw = train[['building_id', 'meter', 'timestamp', 'meter_reading']].copy()\nsite_meter_raw['site_id'] = site_meter_raw.building_id.map(building_site_dict)\ndel site_meter_raw['building_id']\nsite_meter_to_plot = site_meter_raw.copy()\nsite_meter_to_plot[\"hour\"] = site_meter_to_plot[\"timestamp\"].dt.hour\nelec_to_plot = site_meter_to_plot[site_meter_to_plot.meter == 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count = 1\nplt.figure(figsize=(25, 50))\nfor site_id, data_by_site in elec_to_plot.groupby('site_id'):\n    by_site_by_hour = data_by_site.groupby('hour').mean()\n    ax = plt.subplot(15, 4, count)\n    plt.plot(by_site_by_hour.index,by_site_by_hour['meter_reading'],'xb-')\n    ax.set_title('site: '+str(site_id))\n    count += 1\nplt.tight_layout()\nplt.show()\ndel elec_to_plot, site_meter_to_plot, building_site_dict, site_meter_raw","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Reducing memory:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)\nweather_train = reduce_mem_usage(weather_train)\nweather_test = reduce_mem_usage(weather_test)\nbuilding_meta = reduce_mem_usage(building_meta)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weather_train = timestamp_align(weather_train)\nweather_test = timestamp_align(weather_test)\ndel weather","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We review the dataframes:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('train info',train.info())\nprint('-------------------')\nprint('weather_train info', weather_train.info())\nprint('-------------------')\nprint('test info', test.info()) \nprint('-------------------')\nprint('weather_test info', weather_test.info())\nprint('-------------------')\nprint('building info', building_meta.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis\n## Looking for missing values\n\nWe see the basic statistical measures of the dataframes:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Data contains records from 1st Jan to 31st Dec of 2016.\n* Data has information about 1448 buildings.\n* Data has 4 meter types.\n* Some extremely high values in meter reading which can be explored further."},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Time period in **test data** is 2017 and 2018\n* We see that the test data points are a bit more than the double of the train data points..."},{"metadata":{},"cell_type":"markdown","source":"We check for missing values on the test and train dataframes:"},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_train_test = pd.DataFrame(train.isna().sum()/len(train),columns=[\"Missing_Pct_Train\"])\nmissing_train_test[\"Missing_Pct_Test\"] = test.isna().sum()/len(test)\nmissing_train_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No Missing values in train/test datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"building_meta.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"building_meta.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*For the building metadata we see:*\n* There are only 16 different primary uses and Education is the most frecuent with 549 apperances. \n* For the square feet the maximum is 875000 and the minimum is 283.\n* For the year built there is a range between 1900 till 2017. \n* For the floor count the average and median coincide in 3 floors; we see that the maximum floor count is 26.\n* And that there are some missing values for the year_built and floor_count columns."},{"metadata":{},"cell_type":"markdown","source":"We divide the number of missing values by the overall number of values to see which columns have missing values:"},{"metadata":{"trusted":true},"cell_type":"code","source":"building_meta.isna().sum()/len(building_meta)*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weather_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weather_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weather_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weather_test.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We see that almost all columns have missing values on the weather's datasets.\n* For the columns cloud_coverage and precip_depth_1_hr we see a lot of values = 0.\n\nWe divide the number of missing values by the overall number of values to compare the number of missing values in Weather_Train and Weather_Test:"},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_weather = pd.DataFrame(weather_train.isna().sum()/len(weather_train)*100,columns=[\"Weather_Train_Missing_Pct\"]) \nmissing_weather[\"Weather_Test_Missing_Pct\"] = weather_test.isna().sum()/len(weather_test)*100 \nmissing_weather","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **precip_depth_1_hr variable** and **cloud_coverage variable** have similar number of missing values in both train and test weather data.\n* site_id and timestamp do not have missing values.\n* Other variables have some missing values."},{"metadata":{},"cell_type":"markdown","source":"### Calculating Min_value and Max_value of the columns in Weather_train:"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['air_temperature','cloud_coverage','dew_temperature','precip_depth_1_hr','sea_level_pressure','wind_direction','wind_speed']\nfor col in cols:\n    print (\" Minimum Value of {} column is {}\".format(col,weather_train[col].min()))\n    print (\" Maximum Value of {} column is {}\".format(col,weather_train[col].max()))\n    print (\"----------------------------------------------------------------------\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weather_train['timestamp'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> This data is from 31st Dec 2015 to 31st Dec 2016, similar to the timestamp of the training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating a distplot of columns air_temperature,cloud_coverage,dew_temperature,precip_depth_1_hr,sea_level_pressure,wind_speed\ncols = ['air_temperature','cloud_coverage','dew_temperature','precip_depth_1_hr','sea_level_pressure','wind_speed']\nfor ind,col in enumerate(weather_train[cols]):\n    plt.figure(ind)\n    sns.distplot(weather_train[col].dropna())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Cloud_Coverage takes distinct values unlike these other variables.\n* Dew Temperature looks like a Negatively skewed distribution.\n* Lot of 0 values in precip_depth_1_hr variable.\n* Distribution of sea_level_pressure looks like a normal distribution.\n* Wind_Speed distribution looks like positively skewed."},{"metadata":{"trusted":true},"cell_type":"code","source":"weather_test['timestamp'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> The time duration is similar to the test dataset."},{"metadata":{},"cell_type":"markdown","source":"# Dealing with missing values "},{"metadata":{},"cell_type":"markdown","source":"**Before we start filling missing values, we drop floor_count column as it has more than 75% missing values.** \n> It will manipulate the training data into a different direction if we impute it.<br /> \n> Also,year_built also has a large number of missing columns, but we leave it for now."},{"metadata":{"trusted":true},"cell_type":"code","source":"building_meta.drop('floor_count',axis=1,inplace=True)\nbuilding_meta.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We fill out the missing values in the weather columns both in train and test\n\nAs we read so many popular tutorials and suggestion, there are few main solutions to fill in the missing values.\n1. Just ignore the data row \n2. Back-fill or forward-fill to propagate next or previous values respectively\n3. Replace with some constant value outside fixed value range-999,-1 etc.\n4. Replace with mean or median value. \n\n> Most of tutorials implement the way 'the daily mean of each site per month'. It had been proven the most logical and proven method so far.<br /> \n> We will fill with the mean of the filler object created by grouping site_id, day and month if daily means per month / site_id is not available.\n--Ref2 and Ref3 <br />"},{"metadata":{},"cell_type":"markdown","source":"First, we do for the **building part**:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# since NA values only present in building age fillna can be used\nbuilding_meta.fillna(round(building_meta.year_built.mean(),0),\n                inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"building_meta.head()\nbuilding_meta.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We had quite a deal of missing values in the training and testing dataset, except site_id and time_stamp. "},{"metadata":{},"cell_type":"markdown","source":"First We deal with wheather_train"},{"metadata":{"trusted":true},"cell_type":"code","source":"# add month, day of week, day of month and hour \nweather_train['month'] = weather_train['timestamp'].dt.month.astype(np.int8)\nweather_train['day_of_week'] = weather_train['timestamp'].dt.dayofweek.astype(np.int8)\nweather_train['day_of_month']= weather_train['timestamp'].dt.day.astype(np.int8)\nweather_train['hour'] = weather_train['timestamp'].dt.hour\n\n# add is weekend column\nweather_train['is_weekend'] = weather_train.day_of_week.apply(lambda x: 1 if x>=5 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_season(month):\n    if (month <= 2) | (month == 12):\n        return 0\n    # as winter\n    elif month <= 5:\n        return 1\n    # as spring\n    elif month <= 8:\n        return 2\n    # as summer\n    elif month <= 11:\n        return 3\n    # as fall","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weather_train['season'] = weather_train.month.apply(convert_season)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reset Index for Update for training \nweather_train = weather_train.set_index(\n    ['site_id','day_of_month','month'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Air temperature**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create dataframe of daily means per site id \nair_temperature_filler = pd.DataFrame(weather_train\n                                      .groupby(['site_id','day_of_month','month'])\n                                      ['air_temperature'].mean(),\n                                      columns=[\"air_temperature\"])\nair_temperature_filler.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create dataframe of air_temperatures to fill\ntemporary_df = pd.DataFrame({'air_temperature' : weather_train.air_temperature})\n\n# update NA air_temperature values\ntemporary_df.update(air_temperature_filler, overwrite=False)\n\n# update in the weather train dataset\nweather_train[\"air_temperature\"] = temporary_df[\"air_temperature\"]\n\ndel temporary_df, air_temperature_filler\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Cloud Coverage**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create dataframe of daily means per site id\ncloud_coverage_filler = pd.DataFrame(weather_train\n                                     .groupby(['site_id','day_of_month','month'])\n                                     ['cloud_coverage'].mean(),\n                                     columns = ['cloud_coverage'])\ncloud_coverage_filler.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Because cloud_coverage takes discrete values and still have some NA value, I will fill it again with rounded mean"},{"metadata":{"trusted":true},"cell_type":"code","source":"round(cloud_coverage_filler.cloud_coverage.mean(),0)\ncloud_coverage_filler.fillna(round(cloud_coverage_filler.cloud_coverage.mean(),0), \n                             inplace=True)\n\n# create dataframe of cloud_coverages to fill\ntemporary_df = pd.DataFrame({'cloud_coverage' : weather_train.cloud_coverage})\n\n# update NA cloud_coverage values\ntemporary_df.update(cloud_coverage_filler, overwrite=False)\n\n# update in the weather train dataset\nweather_train[\"cloud_coverage\"] = temporary_df[\"cloud_coverage\"]\n\ndel temporary_df, cloud_coverage_filler\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Dew Temperature**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create dataframe of daily means per site id\ndew_temperature_filler = pd.DataFrame(weather_train\n                                      .groupby(['site_id','day_of_month','month'])\n                                      ['dew_temperature'].mean(),\n                                      columns=[\"dew_temperature\"])\ndew_temperature_filler.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create dataframe of dew_temperatures to fill\ntemporary_df = pd.DataFrame({'dew_temperature' : weather_train.dew_temperature})\n\n# update NA dew_temperature values\ntemporary_df.update(dew_temperature_filler, overwrite=False)\n\n# update in the weather train dataset\nweather_train[\"dew_temperature\"] = temporary_df[\"dew_temperature\"]\n\ndel temporary_df, dew_temperature_filler\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Precip Depth 1 Hour**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create dataframe of daily means per site id\nprecip_depth_filler = pd.DataFrame(weather_train\n                                   .groupby(['site_id','day_of_month','month'])\n                                   ['precip_depth_1_hr'].mean(),\n                                   columns=['precip_depth_1_hr'])\nprecip_depth_filler.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As cloud_coverage, I fill NA values of the filler with the rounded mean since the discrete values and still got some NA"},{"metadata":{"trusted":true},"cell_type":"code","source":"round(precip_depth_filler['precip_depth_1_hr'].mean(),0)\nprecip_depth_filler.fillna(round(precip_depth_filler['precip_depth_1_hr'].mean(),0)\n                           , inplace=True)\n\n# create dataframe of precip_depth_1_hr to fill\ntemporary_df = pd.DataFrame({'precip_depth_1_hr' : weather_train.precip_depth_1_hr})\n\n# update NA precip_depth_1_hr values\ntemporary_df.update(precip_depth_filler, overwrite=False)\n\n# update in the weather train dataset\nweather_train[\"precip_depth_1_hr\"] = temporary_df[\"precip_depth_1_hr\"]\n\ndel precip_depth_filler, temporary_df\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Sea Level Pressure**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create dataframe of daily means per site id\nsea_level_filler = pd.DataFrame(weather_train\n                                .groupby(['site_id','day_of_month','month'])\n                                ['sea_level_pressure'].mean(),\n                                columns=['sea_level_pressure'])\nsea_level_filler.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We did the same as with cloud_coverage:"},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_sea_level_pressure = round(\n    sea_level_filler\n    ['sea_level_pressure']\n    .astype(float)\n    .mean(),2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sea_level_filler.fillna(mean_sea_level_pressure, inplace=True)\n\n# create dataframe of sea_level_pressure to fill\ntemporary_df = pd.DataFrame({'sea_level_pressure' : weather_train.sea_level_pressure})\n\n# update NA sea_level_pressure values\ntemporary_df.update(sea_level_filler, overwrite=False)\n\n# update in the weather train dataset\nweather_train[\"sea_level_pressure\"] = temporary_df[\"sea_level_pressure\"]\n\ndel sea_level_filler, temporary_df\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Wind Direction**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create dataframe of daily means per site id\nwind_direction_filler = pd.DataFrame(weather_train\n                                     .groupby(['site_id','day_of_month','month'])\n                                     ['wind_direction'].mean(),\n                                     columns=['wind_direction'])\nwind_direction_filler.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create dataframe of wind_direction to fill\ntemporary_df = pd.DataFrame({'wind_direction' : weather_train.wind_direction})\n\n# update NA wind_direction values\ntemporary_df.update(wind_direction_filler, overwrite=False)\n\n# update in the weather train dataset\nweather_train[\"wind_direction\"] = temporary_df[\"wind_direction\"]\n\ndel temporary_df, wind_direction_filler\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Wind Speed**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create dataframe of daily means per site id\nwind_speed_filler = pd.DataFrame(weather_train\n                                 .groupby(['site_id','day_of_month','month'])\n                                 ['wind_speed'].mean(),\n                                 columns=['wind_speed'])\nwind_speed_filler.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create dataframe of wind_speed to fill\ntemporary_df = pd.DataFrame({'wind_speed' : weather_train.wind_speed})\n\n# update NA wind_speed values\ntemporary_df.update(wind_speed_filler, overwrite=False)\n\n# update in the weather train dataset\nweather_train[\"wind_speed\"] = temporary_df[\"wind_speed\"]\n\ndel temporary_df, wind_speed_filler\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check if NA values left\nweather_train.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We reset indexes to transfrom weather dataframe to original form:"},{"metadata":{"trusted":true},"cell_type":"code","source":"weather_train = weather_train.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weather_train.drop('day_of_month',axis=1,inplace=True)\nweather_train.drop('month',axis=1,inplace=True)\nweather_train.drop('day_of_week',axis=1,inplace=True)\nweather_train.drop('hour',axis=1,inplace=True)\nweather_train.drop('is_weekend',axis=1,inplace=True)\nweather_train.drop('season',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weather_train.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" Repeat again for wheather_test"},{"metadata":{"trusted":true},"cell_type":"code","source":"# add month, day of week, day of month and hour \nweather_test['month'] = weather_test['timestamp'].dt.month.astype(np.int8)\nweather_test['day_of_week'] = weather_test['timestamp'].dt.dayofweek.astype(np.int8)\nweather_test['day_of_month']= weather_test['timestamp'].dt.day.astype(np.int8)\nweather_test['hour'] = weather_test['timestamp'].dt.hour\n\n# add is weekend column\nweather_test['is_weekend'] = weather_test.day_of_week.apply(lambda x: 1 if x>=5 else 0)\n\nweather_test['season'] = weather_test.month.apply(convert_season)\n\n#Reset Index for Update for training \nweather_test = weather_test.set_index(\n    ['site_id','day_of_month','month'])\n\n#Air temperature\n\n\n# create dataframe of daily means per site id \nair_temperature_filler = pd.DataFrame(weather_test\n                                      .groupby(['site_id','day_of_month','month'])\n                                      ['air_temperature'].mean(),\n                                      columns=[\"air_temperature\"])\nair_temperature_filler.isna().sum()\n\n# create dataframe of air_temperatures to fill\ntemporary_df = pd.DataFrame({'air_temperature' : weather_test.air_temperature})\n\n# update NA air_temperature values\ntemporary_df.update(air_temperature_filler, overwrite=False)\n\n# update in the weather train dataset\nweather_test[\"air_temperature\"] = temporary_df[\"air_temperature\"]\n\ndel temporary_df, air_temperature_filler\ngc.collect()\n\n#Cloud Coverage\n\n# create dataframe of daily means per site id\ncloud_coverage_filler = pd.DataFrame(weather_test\n                                     .groupby(['site_id','day_of_month','month'])\n                                     ['cloud_coverage'].mean(),\n                                     columns = ['cloud_coverage'])\ncloud_coverage_filler.isna().sum()\n\n#Because cloud_coverage takes discrete values and still have some NA value, I will fill it again with rounded mean\n\nround(cloud_coverage_filler.cloud_coverage.mean(),0)\ncloud_coverage_filler.fillna(round(cloud_coverage_filler.cloud_coverage.mean(),0), \n                             inplace=True)\n\n# create dataframe of cloud_coverages to fill\ntemporary_df = pd.DataFrame({'cloud_coverage' : weather_test.cloud_coverage})\n\n# update NA cloud_coverage values\ntemporary_df.update(cloud_coverage_filler, overwrite=False)\n\n# update in the weather train dataset\nweather_test[\"cloud_coverage\"] = temporary_df[\"cloud_coverage\"]\n\ndel temporary_df, cloud_coverage_filler\ngc.collect()\n\n#Dew Temperature\n\n# create dataframe of daily means per site id\ndew_temperature_filler = pd.DataFrame(weather_test\n                                      .groupby(['site_id','day_of_month','month'])\n                                      ['dew_temperature'].mean(),\n                                      columns=[\"dew_temperature\"])\ndew_temperature_filler.isna().sum()\n\n# create dataframe of dew_temperatures to fill\ntemporary_df = pd.DataFrame({'dew_temperature' : weather_test.dew_temperature})\n\n# update NA dew_temperature values\ntemporary_df.update(dew_temperature_filler, overwrite=False)\n\n# update in the weather train dataset\nweather_test[\"dew_temperature\"] = temporary_df[\"dew_temperature\"]\n\ndel temporary_df, dew_temperature_filler\ngc.collect()\n\n#Precip Depth 1 Hour\n\n# create dataframe of daily means per site id\nprecip_depth_filler = pd.DataFrame(weather_test\n                                   .groupby(['site_id','day_of_month','month'])\n                                   ['precip_depth_1_hr'].mean(),\n                                   columns=['precip_depth_1_hr'])\nprecip_depth_filler.isna().sum()\n\n#As cloud_coverage, I fill NA values of the filler with the rounded mean since the discrete values and still got some NA\n\n\nround(precip_depth_filler['precip_depth_1_hr'].mean(),0)\nprecip_depth_filler.fillna(round(precip_depth_filler['precip_depth_1_hr'].mean(),0)\n                           , inplace=True)\n\n# create dataframe of precip_depth_1_hr to fill\ntemporary_df = pd.DataFrame({'precip_depth_1_hr' : weather_test.precip_depth_1_hr})\n\n# update NA precip_depth_1_hr values\ntemporary_df.update(precip_depth_filler, overwrite=False)\n\n# update in the weather train dataset\nweather_test[\"precip_depth_1_hr\"] = temporary_df[\"precip_depth_1_hr\"]\n\ndel precip_depth_filler, temporary_df\ngc.collect()\n\n#Sea Level Pressure\n\n# create dataframe of daily means per site id\nsea_level_filler = pd.DataFrame(weather_test\n                                .groupby(['site_id','day_of_month','month'])\n                                ['sea_level_pressure'].mean(),\n                                columns=['sea_level_pressure'])\nsea_level_filler.isna().sum()\n\n#We did the same as with cloud_coverage:\n\nmean_sea_level_pressure = round(\n    sea_level_filler\n    ['sea_level_pressure']\n    .astype(float)\n    .mean(),2)\n\nsea_level_filler.fillna(mean_sea_level_pressure, inplace=True)\n\n# create dataframe of sea_level_pressure to fill\ntemporary_df = pd.DataFrame({'sea_level_pressure' : weather_test.sea_level_pressure})\n\n# update NA sea_level_pressure values\ntemporary_df.update(sea_level_filler, overwrite=False)\n\n# update in the weather train dataset\nweather_test[\"sea_level_pressure\"] = temporary_df[\"sea_level_pressure\"]\n\ndel sea_level_filler, temporary_df\ngc.collect()\n\n#Wind Direction\n\n# create dataframe of daily means per site id\nwind_direction_filler = pd.DataFrame(weather_test\n                                     .groupby(['site_id','day_of_month','month'])\n                                     ['wind_direction'].mean(),\n                                     columns=['wind_direction'])\nwind_direction_filler.isna().sum()\n\n# create dataframe of wind_direction to fill\ntemporary_df = pd.DataFrame({'wind_direction' : weather_test.wind_direction})\n\n# update NA wind_direction values\ntemporary_df.update(wind_direction_filler, overwrite=False)\n\n# update in the weather train dataset\nweather_test[\"wind_direction\"] = temporary_df[\"wind_direction\"]\n\ndel temporary_df, wind_direction_filler\ngc.collect()\n\n#Wind Speed\n\n\n# create dataframe of daily means per site id\nwind_speed_filler = pd.DataFrame(weather_test\n                                 .groupby(['site_id','day_of_month','month'])\n                                 ['wind_speed'].mean(),\n                                 columns=['wind_speed'])\nwind_speed_filler.isna().sum()\n\n# create dataframe of wind_speed to fill\ntemporary_df = pd.DataFrame({'wind_speed' : weather_test.wind_speed})\n\n# update NA wind_speed values\ntemporary_df.update(wind_speed_filler, overwrite=False)\n\n# update in the weather train dataset\nweather_test[\"wind_speed\"] = temporary_df[\"wind_speed\"]\n\ndel temporary_df, wind_speed_filler\ngc.collect()\n\n# check if NA values left\nweather_test.isna().sum()\n\n#We reset indexes to transfrom weather dataframe to original form:\n\nweather_test = weather_test.reset_index()\n\nweather_test.drop('day_of_month',axis=1,inplace=True)\nweather_test.drop('month',axis=1,inplace=True)\nweather_test.drop('day_of_week',axis=1,inplace=True)\nweather_test.drop('hour',axis=1,inplace=True)\nweather_test.drop('is_weekend',axis=1,inplace=True)\nweather_test.drop('season',axis=1,inplace=True)\n\nweather_test.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## We merge train data (train, building, weather) into 1 dataframe:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_total = pd.merge(train,building_meta,how='left',on='building_id')\ntrain_total = pd.merge(train_total,weather_train,how='left',on=[\"site_id\", \"timestamp\"])\ntrain_total.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_total.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## We merge test data (train, building, weather) into 1 dataframe:"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_total = pd.merge(test,building_meta,how='left',on='building_id')\ntest_total = pd.merge(test_total,weather_test,how='left',on=[\"site_id\", \"timestamp\"])\n\ntest_total.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_total.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Looking for unique values"},{"metadata":{"trusted":true},"cell_type":"code","source":"def feat_value_count(df,colname):\n    \"\"\"value count of each feature\n    \n    Args\n    df: data frame.\n    colname: string. Name of to be valued column\n    \n    Returns\n    df_count: data frame.\n    \"\"\"\n    df_count = df[colname].value_counts().to_frame().reset_index()\n    df_count = df_count.rename(columns={'index':colname+'_values',colname:'counts'})\n    return df_count\n\nfeat_value_count(train,'building_id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A total of 1449 buildings are in **train data**. Building 1298,1249 has the most records and building 403 has the least records."},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_value_count(test,'building_id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A total of 1449 buildings are in **test data**. Buildings 1258,1241,1331,1301... have the most records and building 0,666,667,668,669... have the least records. Many buildings have same amout of records. Maybe make sense to check the distribution."},{"metadata":{"trusted":true},"cell_type":"code","source":"len(set(train.building_id) & set(test.building_id))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We use the function **set()** and the command **&** so that we identify the intersection between the train and the test dataframes. We need to predict all 1449 building meter readings. All buildings that need to be predicted appear in train data."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_total['meter'].replace({0:\"Electricity\",1:\"ChilledWater\",2:\"Steam\",3:\"HotWater\"},inplace=True)\ntest_total['meter'].replace({0:\"Electricity\",1:\"ChilledWater\",2:\"Steam\",3:\"HotWater\"},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_value_count(train_total,'meter')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train_total['meter'])\nplt.title(\"Distribution of Meter Id Code\")\nplt.xlabel(\"Meter Id Code\")\nplt.ylabel(\"Frequency\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Not every building has all meter types. Electricity has the most records."},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_value_count(weather_train,'site_id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 15 locations according to the weather data"},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_value_count(building_meta,'primary_use')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,6))\nbuilding_meta['primary_use'].value_counts().sort_values().plot(kind='bar')\nplt.title(\"Count of Primary_Use Variable in the Metadata table\")\nplt.xlabel(\"Primary Use\")\nplt.ylabel(\"Count\")\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There are 15 primary use of buildings. \n* Most buildings are for education, the least is for religious worship. \n* Education, Office, Entertainment/Public Assembly, Public Services, Lodging/Residential form the bulk of Primary Use. \n* Education and office occupies 57% of all buildings."},{"metadata":{"trusted":true},"cell_type":"code","source":"building_meta['primary_use'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Since there are a lot of categories which are a minor percentage of the whole, it makes sense to combine them.\n\nWe therefore combine all categories with minor percentage:"},{"metadata":{"trusted":true},"cell_type":"code","source":"building_meta['primary_use'].replace({\"Healthcare\":\"Other\",\"Parking\":\"Other\",\"Warehouse/storage\":\"Other\",\"Manufacturing/industrial\":\"Other\",\n                                \"Retail\":\"Other\",\"Services\":\"Other\",\"Technology/science\":\"Other\",\"Food sales and service\":\"Other\",\n                                \"Utility\":\"Other\",\"Religious worship\":\"Other\"},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_value_count(building_meta,'site_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(building_meta['site_id'])\nplt.title(\"Count of Site_id in the Metadata table\")\nplt.xlabel(\"Site_Id\")\nplt.ylabel(\"Count\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Out of 1449 buildings and 15 sites, site 3 has most buildings."},{"metadata":{"trusted":true},"cell_type":"code","source":"building_meta['square_feet'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We created distplot of Distribution of Square Feet variable of Metadata Table**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(building_meta['square_feet'])\nplt.title(\"Distribution of Square Feet variable of Metadata Table\")\nplt.xlabel(\"Area in Square Feet\")\nplt.ylabel(\"Frequency\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like a normal distribution distribution."},{"metadata":{"trusted":true},"cell_type":"code","source":"building_meta['square_feet'] = np.log1p(building_meta['square_feet'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"building_meta.groupby('primary_use')['square_feet'].agg(['mean','median','count']).sort_values(by='count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We obtain the mean, median, count of grouped by columns primary_use, square_feet:\n* Others (Parking) has the highest average although the count is less.\n* Education has the highest count as can be seen in the countplot above."},{"metadata":{"trusted":true},"cell_type":"code","source":"building_meta['year_built'].value_counts().sort_values().plot(kind='bar',figsize=(15,6))\nplt.xlabel(\"Year Built\")\nplt.ylabel(\"Count\")\nplt.title(\"Distribution of Year Built Variable\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"building_meta.groupby('primary_use')['square_feet'].agg(['count','mean','median']).sort_values(by='count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['site_id','primary_use','building_id','year_built']\nfor col in cols:\n    print (\"Number of Unique Values in the {} column are:\".format(col),building_meta[col].nunique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now we take a single building 1258 to analyze the meter variable (our target):"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_one_building = train_total[train_total.building_id == 1258]\ndf_one_building.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(x='timestamp',y='meter_reading',data=df_one_building[train_total.meter == 'Electricity']).set_title('electricity of building 1258')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With a Electricity lineplot we see that **in spring**, less electricity is used."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(x='timestamp',y='meter_reading',data=df_one_building[train_total.meter == 'ChilledWater']).set_title('chilledwater of building 1258')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With a Chilled water lineplot we see that **in summer**, more chilled water is used."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(x='timestamp',y='meter_reading',data=df_one_building[train_total.meter == 'Steam']).set_title('steam of building 1258')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In winter, more steam is used for heating."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(x='timestamp',y='meter_reading',data=df_one_building[train_total.meter == 'HotWater']).set_title('hotwater of building 1258')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In summmer there is very low consumption of hot water, but in winter very high consumption."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_lots_building = train_total[train_total['building_id'].isin([1258,1298,1249])]\nmeasures = ['Electricity', 'ChilledWater', 'Steam', 'HotWater']\nfor i in measures:\n    f, ax = plt.subplots(figsize=(15, 6))\n    sns.lineplot(x='timestamp',y='meter_reading', hue = 'building_id',legend='brief',\n             data=df_lots_building[df_lots_building.meter == i]);\n#del df_lots_building\n#gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In Multiple buildings the patterns of energy consumption is quite consistent. Building 1258 has the highest consumption."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_total.groupby('meter')['meter_reading'].agg(['min','max','mean','median','count','std'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that Steam meter has some values that are very high maximum values, we have to explore further. \nMinimum value for all 4 types of meter is 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in [train_total, test_total]:\n    df['Month'] = df['timestamp'].dt.month.astype(\"uint8\")\n    df['DayOfMonth'] = df['timestamp'].dt.day.astype(\"uint8\")\n    df['DayOfWeek'] = df['timestamp'].dt.dayofweek.astype(\"uint8\")\n    df['Hour'] = df['timestamp'].dt.hour.astype(\"uint8\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Meter_reading is grouped by meter and month. Max, mean, median, count and std are calculated:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_total.groupby(['meter','Month'])['meter_reading'].agg(['max','mean','median','count','std'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We can see that only Steam meter has very high meter_reading values as compared to other types of meters.\n* We can see that the average electricity meter_reading does not vary much across the months.\n* Average Hot Water meter_reading is relatively less from April to October Months.\n* Average Steam meter_reading is way higher from March to June as compared to the other months."},{"metadata":{},"cell_type":"markdown","source":"### Meter_reading is grouped by meter and dayofweek. Max, mean, median, count and std are calculated:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_total.groupby(['meter','DayOfWeek'])['meter_reading'].agg(['max','mean','median','count','std'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Average meter_reading of Steam type of meter is higher as compared to the other meter types."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_total['meter_reading'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We create a distplot Distribution of Log of Meter Reading Variable:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(np.log1p(train_total['meter_reading']),kde=False)\nplt.title(\"Distribution of Log of Meter Reading Variable\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Lot of 0 values as can be seen from the distribution"},{"metadata":{},"cell_type":"markdown","source":"### Converting the dependent variable to logarithmic scale:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_total['meter_reading'] = np.log1p(train_total['meter_reading'])\nsns.distplot(train_total[train_total['meter'] == \"Electricity\"]['meter_reading'],kde=False)\nplt.title(\"Distribution of Meter Reading per MeterID code: Electricity\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train_total[train_total['meter'] == \"ChilledWater\"]['meter_reading'],kde=False)\nplt.title(\"Distribution of Meter Reading per MeterID code: Chilledwater\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train_total[train_total['meter'] == \"Steam\"]['meter_reading'],kde=False)\nplt.title(\"Distribution of Meter Reading per MeterID code: Steam\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train_total[train_total['meter'] == \"HotWater\"]['meter_reading'],kde=False)\nplt.title(\"Distribution of Meter Reading per MeterID code: Hotwater\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> There is some discrepancy in the meter_readings for different ste_id's and buildings. It makes sense to delete them. Ref1"},{"metadata":{"trusted":true},"cell_type":"code","source":"idx_to_drop = list((train_total[(train_total['site_id'] == 0) & (train_total['timestamp'] < \"2016-05-21 00:00:00\")]).index)\ntrain_total.drop(idx_to_drop,axis='rows',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def label_encoder(df, categorical_columns=None):\n    \"\"\"Encode categorical values as integers (0,1,2,3...) with pandas.factorize. \"\"\"\n    # if categorical_colunms are not given than treat object as categorical features\n    if not categorical_columns:\n        categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n    for col in categorical_columns:\n        df[col], uniques = pd.factorize(df[col])\n    return df, categorical_columns;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_total.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Scores results\n\nHERE SHOULD BE ADDED THE SCORES!"},{"metadata":{},"cell_type":"markdown","source":"# Lessons and outlook\n**IN GENERAL:**\n1. We found ‘modin’ library can help accelerate pandas even on the laptop and so we tried to install it in windows. We learned that:\n    * Pip install “modin[dask]” is ran in the command line, therefore, the code should be put in the command in terminal instead of interpreter.\n    * If still get error when using modin to read data, it’s because Windows doesn’t support Ray which is the dependency of modin. To use it, we have to install WSL. But it would easier for laptops using Linux or Mac.\n    * We didn’t use it in the end because we don’t want to install more applications.\n2.\tDataFrame.dtypes for data must be int, float or bool.\n3.\tDifference between statistics modeling (e.g. linear regression) and machine learning:\n    * Statistical modeling is more about finding relationships between variables and the significance of those relationships. Test data is not necessary but we analyze confidential intervals, p value, test value to access the model’s accuracy.\n    * Machine learning is more about prediction results, we don’t care much if the model is interpretable. Test data is normally needed to validate results’ accuracy.\n4.\tEven though in machine learning, we don’t care much about the independency/collinearity problems, but we still should ensure that the training data is as clean as possible, therefore, we should still drop the useless columns, which also reduce the data size and increase the speed.\n5. The cells took to much time to execute, this was very annoying. It felt like a waste of time.\n6. It was initially difficult to understand the different data sets and the connections between them. It took us some time to get the hang of it.\n7. We encountered many unknown methods, so we had to research what each method does in order to use it properly.\n\n**FOR THE MISSING VALUES:**\n\nAfter tried several different solutions without efficiency, we saw a tutorial indicating a really simple but useful action before start looking for proper ML. This tutorial mentioned that it's better **to calculate a ''common-sense'' baseline**. This baseline is defined in how a person who has knowledge in that field would solve the problem without using any data science tricks. Alternatively, it can be a dummy or simple algorithm, consisting of few lines of code, to use as a baseline metric.\nBaseline metrics can be different in regression and classification problems. For a regression problem, it can be a central tendency measure as the result for all predictions, such as the mean or the median. Since this is a regression problem and competition's results will be evaluated for root mean squared logarithmic error. Baseline metrics are important in a way that, if a ML model cannot beat the simple and intuitive prediction of a person's or an algorithm's guess, the original problem needs reconsideration or training data needs reframing.\ne.g.The baseline guess is a score of 4.38\nBaseline Performance on the valid set: RMSE = 2.1070\nHowever, when we try linear regression, RMSE of the linear regression model is: 1.9895562449483846 without too much difference. Then why we are going to use such ML for prediction? By doing so, it's really easy to do the benchmark even if we don't start with any complicated statistics.\n"},{"metadata":{},"cell_type":"markdown","source":"# References\n\nRef1: **As per the discussion in the following thread, https://www.kaggle.com/c/ashrae-energy-prediction/discussion/117083, there is some discrepancy in the meter_readings for different ste_id's and buildings. It makes sense to delete them**\n\nRef2: https://www.kaggle.com/aitude/ashrae-kfold-lightgbm-without-leak-1-08\n\nRef3: https://www.kaggle.com/cereniyim/save-the-energy-for-the-future-2-fe-lightgbm\n\nRef4: https://www.kaggle.com/nz0722/aligned-timestamp-lgbm-by-meter-type"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}