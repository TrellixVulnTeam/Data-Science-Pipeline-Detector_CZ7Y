{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Welcome to the ASHRAE - Great Energy Predictor Competition\nThis notebook is a starter code for all beginners and easy to understand. The train and test data are very large so we will work with a data generator based on the template to generate the data on the fly <br>\nhttps://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n\nAdditionally we follow an efficient workflow. <br>\nWe also use categorical feature encoding techniques, compare <br>\nhttps://www.kaggle.com/drcapa/categorical-feature-encoding-challenge-xgb\n\nFor the first step we will take a simple neural network based on the keras library. After that we will use a RNN.<br>\nCurrent status of the kernel: The workflow is complete.<br>\nNext steps: \n* Improve the LSTM.\n* Expand the feature engineering based on the kernel: https://www.kaggle.com/drcapa/ashrae-feature-engineering"},{"metadata":{},"cell_type":"markdown","source":"# Load Libraries"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport scipy.special\nimport matplotlib.pyplot as plt\nimport os\nimport random","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import Sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, LSTM, Embedding\nfrom keras.optimizers import RMSprop,Adam\nimport keras.backend as K","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"path_in = '../input/ashrae-energy-prediction/'\nprint(os.listdir(path_in))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(path_in+'train.csv', parse_dates=['timestamp'])\ntrain_weather = pd.read_csv(path_in+'weather_train.csv', parse_dates=['timestamp'])\nbuilding_data = pd.read_csv(path_in+'building_metadata.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Help function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_bar(data, name):\n    fig = plt.figure(figsize=(16, 9))\n    ax = fig.add_subplot(111)\n    data_label = data[name].value_counts()\n    dict_train = dict(zip(data_label.keys(), ((data_label.sort_index())).tolist()))\n    names = list(dict_train.keys())\n    values = list(dict_train.values())\n    plt.bar(names, values)\n    ax.set_xticklabels(names, rotation=45)\n    plt.grid()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Handle missing values of building and weather data\nThe missing data are numerical values. So for the first step we can use a simple imputer of the sklearn library."},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_with_missing_train_weather = [col for col in train_weather.columns if train_weather[col].isnull().any()]\ncols_with_missing_building = [col for col in building_data.columns if building_data[col].isnull().any()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(cols_with_missing_train_weather)\nprint(cols_with_missing_building)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imp_most = SimpleImputer(strategy='most_frequent')\ntrain_weather[cols_with_missing_train_weather] = imp_most.fit_transform(train_weather[cols_with_missing_train_weather])\nbuilding_data[cols_with_missing_building] = imp_most.fit_transform(building_data[cols_with_missing_building])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Scale objective label"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['meter_reading'] = np.log1p(train_data['meter_reading'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create new features\n## Train data\nBased on the timestamp we create new features which are cyclic."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['month'] = train_data['timestamp'].dt.month\ntrain_data['day'] = train_data['timestamp'].dt.weekday\ntrain_data['year'] = train_data['timestamp'].dt.year\ntrain_data['hour'] = train_data['timestamp'].dt.hour","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Additionally we create the feature weekend: 5 = saturday and 6 = sunday."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['weekend'] = np.where((train_data['day'] == 5) | (train_data['day'] == 6), 1, 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Weather data\nThe feature wind_direction is cyclic."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_weather['wind_direction'+'_sin'] = np.sin((2*np.pi*train_weather['wind_direction'])/360)\ntrain_weather['wind_direction'+'_cos'] = np.cos((2*np.pi*train_weather['wind_direction'])/360)\ntrain_weather = train_weather.drop(['wind_direction'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Encoding\nThere is a greate encoding competition: https://www.kaggle.com/drcapa/categorical-feature-encoding-challenge-xgb\n## Train data\n### Feature meter\nThere are 4 types of meters: <br>\n0 = electricity, 1 = chilledwater, 2 = steam, 3 = hotwater <br>\nWe use the one hot encoding for this 4 feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.get_dummies(train_data, columns=['meter'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Features month, day and hour\nWe created the features month, day and hour which are cyclic."},{"metadata":{"trusted":true},"cell_type":"code","source":"features_cyc = {'month' : 12, 'day' : 7, 'hour' : 24}\nfor feature in features_cyc.keys():\n    train_data[feature+'_sin'] = np.sin((2*np.pi*train_data[feature])/features_cyc[feature])\n    train_data[feature+'_cos'] = np.cos((2*np.pi*train_data[feature])/features_cyc[feature])\ntrain_data = train_data.drop(features_cyc.keys(), axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building data\nThe feature primary_use is a categorical feature with 16 categories. For the first we use a simple mapping."},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_bar(building_data, 'primary_use')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"map_use = dict(zip(building_data['primary_use'].value_counts().sort_index().keys(),\n                     range(1, len(building_data['primary_use'].value_counts())+1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"building_data['primary_use'] = building_data['primary_use'].replace(map_use)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#building_data = pd.get_dummies(building_data, columns=['primary_use'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Scale building and weather data\n## Weather data"},{"metadata":{"trusted":true},"cell_type":"code","source":"weather_scale = ['air_temperature', 'cloud_coverage', 'dew_temperature', 'sea_level_pressure', 'wind_speed']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean = train_weather[weather_scale].mean(axis=0)\ntrain_weather[weather_scale] = train_weather[weather_scale].astype('float32')\ntrain_weather[weather_scale] -= train_weather[weather_scale].mean(axis=0)\nstd = train_weather[weather_scale].std(axis=0)\ntrain_weather[weather_scale] /= train_weather[weather_scale].std(axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building data"},{"metadata":{"trusted":true},"cell_type":"code","source":"building_scale = ['square_feet', 'year_built', 'floor_count']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean = building_data[building_scale].mean(axis=0)\nbuilding_data[building_scale] = building_data[building_scale].astype('float32')\nbuilding_data[building_scale] -= building_data[building_scale].mean(axis=0)\nstd = building_data[building_scale].std(axis=0)\nbuilding_data[building_scale] /= building_data[building_scale].std(axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Merge data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.merge(train_data, building_data, on='building_id', right_index=True)\ntrain_data = train_data.sort_values(['timestamp'])\ntrain_data = pd.merge_asof(train_data, train_weather, on='timestamp', by='site_id', right_index=True)\ndel train_weather","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build the data generator"},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataGenerator(Sequence):\n    \"\"\" A data generator based on the template\n        https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n        \"\"\"\n    \n    def __init__(self, data, list_IDs, features, batch_size, shuffle=False):\n        self.data = data.loc[list_IDs].copy()\n        self.list_IDs = list_IDs\n        self.features = features\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.on_epoch_end()\n    \n    \n    def __len__(self):\n        return int(np.floor(len(self.list_IDs)/self.batch_size))\n    \n    \n    def __getitem__(self, index):\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n        X, y = self.__data_generation(list_IDs_temp)\n        return X, y\n    \n    \n    def on_epoch_end(self):\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n        \n    def __data_generation(self, list_IDs_temp):        \n        X = np.empty((len(list_IDs_temp), len(self.features)), dtype=float)\n        y = np.empty((len(list_IDs_temp), 1), dtype=float)\n        X = self.data.loc[list_IDs_temp, self.features].values\n        \n        if 'meter_reading' in self.data.columns:\n            y = self.data.loc[list_IDs_temp, 'meter_reading'].values\n        # reshape\n        X = np.reshape(X, (X.shape[0], 1, X.shape[1]))\n        return X, y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split the random input data into train and val\nSince it's a timeseries problem, we split the train and validation data by timestamp and not with a random split."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_size = int(len(train_data.index)*0.75)\nval_size = len(train_data.index) - train_size\ntrain_list, val_list = train_data.index[0:train_size], train_data.index[train_size:train_size+val_size]\nprint(train_size, val_size)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define the features"},{"metadata":{"trusted":true},"cell_type":"code","source":"no_features = ['building_id', 'timestamp', 'meter_reading', 'year']\nfeatures = train_data.columns.difference(no_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define train and validation data via Data Generator"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 1024\ntrain_generator = DataGenerator(train_data, train_list, features, batch_size)\nval_generator = DataGenerator(train_data, val_list, features, batch_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define Recurrent Neural Network\nWe use a simple recurrent neural network for train and prediction. Later we will improve."},{"metadata":{"trusted":true},"cell_type":"code","source":"input_dim = len(features)\nprint(input_dim)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n#model.add(Embedding(input_length=input_dim))\nmodel.add(LSTM(units=4, activation = 'relu', input_shape=(1, input_dim)))\n#model.add(LSTM(units=64, activation = 'relu'))\n#model.add(Dense(128, activation='relu', input_dim=input_dim))\n#model.add(Dense(256, activation='relu'))\n#model.add(Dense(512, activation='relu'))\nmodel.add(Dense(1, activation='relu'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmse(y_true, y_pred):\n    \"\"\" root_mean_squared_error \"\"\"\n    return K.sqrt(K.mean(K.square(y_pred - y_true)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer = Adam(lr=1e-4),\n              loss='mse',\n              metrics=[rmse])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train model"},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit_generator(generator=train_generator,\n                              validation_data=val_generator,\n                              epochs = epochs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analyse results\nA short analysis of the train results."},{"metadata":{"trusted":true},"cell_type":"code","source":"loss = history.history['loss']\nloss_val = history.history['val_loss']\nepochs = range(1, len(loss)+1)\nplt.plot(epochs, loss, 'bo', label='loss_train')\nplt.plot(epochs, loss_val, 'b', label='loss_val')\nplt.title('value of the loss function')\nplt.xlabel('epochs')\nplt.ylabel('value of the loss function')\nplt.legend()\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = history.history['rmse']\nacc_val = history.history['val_rmse']\nepochs = range(1, len(loss)+1)\nplt.plot(epochs, acc, 'bo', label='accuracy_train')\nplt.plot(epochs, acc_val, 'b', label='accuracy_val')\nplt.title('accuracy')\nplt.xlabel('epochs')\nplt.ylabel('value of accuracy')\nplt.legend()\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Delete train data"},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predict test data\n* We following the steps above to prepare the data\n* Build data generator\n* Predict subdate\n* Write data in an array"},{"metadata":{"trusted":true},"cell_type":"code","source":"nrows = 1667904\nbatch_size = 1022\nsteps = 25\ny_test = np.empty(())\ntest_weather = pd.read_csv(path_in+'weather_test.csv', parse_dates=['timestamp'])\ncols_with_missing_test_weather = [col for col in test_weather.columns if test_weather[col].isnull().any()]\ntest_weather[cols_with_missing_test_weather] = imp_most.fit_transform(test_weather[cols_with_missing_test_weather])\n\nmean = test_weather[weather_scale].mean(axis=0)\ntest_weather[weather_scale] = test_weather[weather_scale].astype('float32')\ntest_weather[weather_scale] -= test_weather[weather_scale].mean(axis=0)\nstd = test_weather[weather_scale].std(axis=0)\ntest_weather[weather_scale] /= test_weather[weather_scale].std(axis=0)\n\ntest_weather['wind_direction'+'_sin'] = np.sin((2*np.pi*test_weather['wind_direction'])/360)\ntest_weather['wind_direction'+'_cos'] = np.cos((2*np.pi*test_weather['wind_direction'])/360)\ntest_weather = test_weather.drop(['wind_direction'], axis=1)\n\nfor i in range(0, steps):\n    print('work on step ', (i+1))\n    test_data = pd.read_csv(path_in+'test.csv', skiprows=range(1,i*(nrows)+1), nrows=nrows, parse_dates=['timestamp'])\n    test_data['month'] = test_data['timestamp'].dt.month\n    test_data['day'] = test_data['timestamp'].dt.weekday\n    test_data['year'] = test_data['timestamp'].dt.year\n    test_data['hour'] = test_data['timestamp'].dt.hour\n    test_data['weekend'] = np.where((test_data['day'] == 5) | (test_data['day'] == 6), 1, 0)\n    for feature in features_cyc.keys():\n        test_data[feature+'_sin'] = np.sin((2*np.pi*test_data[feature])/features_cyc[feature])\n        test_data[feature+'_cos'] = np.cos((2*np.pi*test_data[feature])/features_cyc[feature])\n    test_data = test_data.drop(features_cyc.keys(), axis=1)\n    test_data = pd.get_dummies(test_data, columns=['meter'])\n    test_data = pd.merge(test_data, building_data, on='building_id', right_index=True)\n    test_data = test_data.sort_values(['timestamp'])\n    test_data = pd.merge_asof(test_data, test_weather, on='timestamp', by='site_id', right_index=True)\n    test_data = test_data.sort_values(['row_id'])\n    for feature in features:\n        if feature not in test_data:\n            #print('   not in:', feature)\n            test_data[feature] = 0\n    test_generator = DataGenerator(test_data, test_data.index, features, batch_size)\n    predict = model.predict_generator(test_generator, verbose=1, workers=1)\n    predict = np.expm1(predict)\n    y_test = np.vstack((y_test, predict))\n    del test_data\n    del test_generator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = np.delete(y_test, 0, 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Delete data"},{"metadata":{"trusted":true},"cell_type":"code","source":"del test_weather\ndel building_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Write output for submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"output = pd.DataFrame({'row_id': range(0, len(y_test)),\n                       'meter_reading': y_test.reshape(len(y_test))})\noutput = output[['row_id', 'meter_reading']]\noutput.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}