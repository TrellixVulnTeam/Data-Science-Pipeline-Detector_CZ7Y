{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport gc\nfrom sklearn import preprocessing\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Original code from https://www.kaggle.com/gemartin/load-data-reduce-memory-usage by @gemartin\n# Modified to support timestamp type, categorical type\n# Modified to add option to use float16 or not. feather format does not support float16.\nfrom pandas.api.types import is_datetime64_any_dtype as is_datetime\nfrom pandas.api.types import is_categorical_dtype\n\ndef reduce_mem_usage(df, use_float16=False):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n            # skip datetime type or categorical type\n            continue\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train=pd.read_csv(\"/kaggle/input/ashrae-energy-prediction/train.csv\")\nbuilding_info=pd.read_csv(\"/kaggle/input/ashrae-energy-prediction/building_metadata.csv\")\nweather_info_test=pd.read_csv(\"/kaggle/input/ashrae-energy-prediction/weather_test.csv\")\ntest=pd.read_csv(\"/kaggle/input/ashrae-energy-prediction/test.csv\")\nweather_info_train=pd.read_csv(\"/kaggle/input/ashrae-energy-prediction/weather_train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train = reduce_mem_usage(train)\n# test = reduce_mem_usage(test)\n\n# weather_info_train = reduce_mem_usage(weather_info_train)\n# weather_info_test = reduce_mem_usage(weather_info_test)\n# building_info = reduce_mem_usage(building_info)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_all=train.merge(building_info,on='building_id',how='left')\ntrain_all=train_all.merge(weather_info_train,on=['site_id', 'timestamp'],how='left')\ngc.collect()\ntrain_all.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del weather_info_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the correlation between the numerical independent variables and the meter_reading\ntrain_all.corr()\n# based on the correlation matrix, the following columns won't be included in the model:\n# precip_depth_1_hr, sea_level_pressure, wind_direction, dew_temperature.\n# Before dropping any columns, there are 16 columns in train_all.\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# delete the columns I don't want\ntrain_all=train_all.drop(['precip_depth_1_hr','sea_level_pressure','wind_direction','dew_temperature'],axis=1)\n# There  are 12 columns after dropping 4 columns related to weather. There are \n# 20216100 rows in the train_all dataframe before dropping any rows.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This is time series data, and we are trying to predict the meter_reading for each \n# meter type of each building. Thus, we're predicting the time series trend for each meter type in each building.\n# Based on our goal, the weird history of a meter's reading which is not representing the meter's trend and will not be repeated\n# in the future should be deleted.\n# From the EDA, we know All electricity meter is 0 until May 20 for site_id == 0. Thus,\n# data meets these conditions are deleted.\ntrain_all=train_all.drop(train_all[(train_all['building_id']<= 104) & (train_all['meter']==0) & (train_all['timestamp']<= \"2016-05-21\")].index)\n#train_df = train_df.query('not (building_id <= 104 & meter == 0 & timestamp <= \"2016-05-20\")')\n# train_all.shape\n# There are 19867540 rows after dropping those rows.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add time features to the data, and get rid of the timestamp. \n# Why do we need to extract time features instead of using timestamp as a predictor direcly?\n# Use FFT to figure out the major frequency in the data might be helpful.\ntrain_all[\"timestamp\"] = pd.to_datetime(train_all[\"timestamp\"])\ntrain_all[\"hour\"] = train_all[\"timestamp\"].dt.hour# the hour of a day\n# I feel like which day it is in a month might not change the energy consumption pattern much.\ntrain_all[\"day\"] = train_all[\"timestamp\"].dt.day# the day of a month \ntrain_all[\"dayofweek\"] = train_all[\"timestamp\"].dt.weekday# the day of a week; same as dt.dayofweek\ntrain_all[\"month\"] = train_all[\"timestamp\"].dt.month# the month of a year\ntrain_all.drop('timestamp',axis=1,inplace=True)\ntrain_all\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_all[train_all['building_id']==104].shape#(5400, 15)\n# train_all[train_all['building_id']==105].shape#(8784, 15)\n# Since certain timestamps of some buildings are deleted, the time series has different\n# length for each building now.This problem needs to be taken care of!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_all.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# train_all.groupby('site_id')['cloud_coverage'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#  train_all[train_all['site_id']==2].air_temperature.value_counts()\nl=train_all.groupby(['site_id','month'])['air_temperature'].mean()\nl[0]\ntrain_all[(train_all['site_id']==2) & (train_all['month']==12) & (train_all['day']==25)].air_temperature.value_counts()\n# train_all[(train_all['site_id']==2) & (train_all['month']==12) & (train_all['day']==25)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_feacture=['building_id', 'site_id','primary_use']\nnumerical_feature=['square_feet', 'year_built', 'floor_count','air_temperature', 'cloud_coverage', 'wind_speed', 'hour', 'dayofweek','month']\nfeature_columns=categorical_feacture+numerical_feature\nlabel_column='meter_reading'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"train_all.isna().sum()\n# As we can see, there are many missing values in year_built, floor_count, air_temperature, and cloud_coverage.\n# How to deal with the missing values in this case is interesting. \n# I was thinking to fill in the NaNs in air_temperature, and cloud_coverage using the site_id mean.\n# However, I later found that for a certain site_id, there are many different values that are quite different for \n# air_temperature and cloud_coverage.\n# As for year_built and floor_count, these are features related to certain building and I don't think \n# it is reasonable to use the mean values to fill in the NaNs.\n\n# For now, I'll leave the NaN values as they are.\n\n# Later, try 'Fill Nan value in weather dataframe by interpolation'!\n\n# Seems like fill in the NaNs in air_temperature, and cloud_coverage using the mean of temperature, cloud_coverage\n# of day of the month is good. From this notebook:\n# https://www.kaggle.com/aitude/ashrae-missing-weather-data-handling","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare the dataframe to be dataset that the model can read.\n# Each column should be numpy array instead of pandas dataframe\n# This function transfer every column to be numpy array\ndef convert_to_tensor(s):\n    dt = s.dtype\n    if dt == \"float64\" or dt == \"int64\":\n        a = np.asarray(s).astype(\"float32\")\n        a = np.nan_to_num(a, nan=a[~np.isnan(a)].mean())\n        return a\n    elif dt == \"object\":\n        a=np.array(s) # change \n        return a\n    return None\ntrain_all_dataset=[]\nfor col in train_all.columns:\n    t = convert_to_tensor(train_all[col])\n    train_all_dataset.append(t)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_all_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_train_data(df):\n    data_list = []\n    target_list = []\n    for unit_number in df.unit_number.unique():\n        unit = df[df.unit_number == unit_number]\n        #print(unit)\n        data_list.append(np.array(unit[data_cols])[:127,:])\n        #print(data_list)\n        target_list.append(np.array(unit[\"RUL\"])[127])\n        #print(target_list)\n    return (np.stack(data_list), np.array(target_list).T)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# In the columns in the training dataframe, building_id, meter, site_id, and primary_use are categorical variables.\n# We need to encode these variables. Since this notebook will train the model for each meter type, 'meter' won't\n# be included as a predictor here.\n# onehotencoder is used:\nohe = preprocessing.OneHotEncoder()\nbidarr = np.array(train_all['building_id'])\nbidarr = bidarr.reshape(-1,1)\nprint(bidarr.shape)\nohe.fit(bidarr)\nbuilding_id_one_hot = ohe.transform(bidarr).toarray()\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}