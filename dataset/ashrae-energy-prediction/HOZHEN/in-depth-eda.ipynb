{"cells":[{"metadata":{},"cell_type":"markdown","source":"# ASHRAE - Great Energy Predictor III$\\href{https://www.kaggle.com/c/ashrae-energy-prediction/}{\\text{view competition on kaggle}}$\n\nQ: How much does it cost to cool a skyscraper in the summer?\nA: A lot! And not just in dollars, but in environmental impact.\n\nThankfully, significant investments are being made to improve building efficiencies to reduce costs and emissions. The question is, are the improvements working? That’s where you come in. Under pay-for-performance financing, the building owner makes payments based on the difference between their real energy consumption and what they would have used without any retrofits. The latter values have to come from a model. Current methods of estimation are fragmented and do not scale well. Some assume a specific meter type or don’t work with different building types.\n\nIn this competition, you’ll develop accurate models of metered building energy usage in the following areas: chilled water, electric, hot water, and steam meters. The data comes from over 1,000 buildings over a three-year timeframe. With better estimates of these energy-saving investments, large scale investors and financial institutions will be more inclined to invest in this area to enable progress in building efficiencies.\n\n## Data Description\n\nAssessing the value of energy efficiency improvements can be challenging as there's no way to truly know how much energy a building would have used without the improvements. The best we can do is to build counterfactual models. Once a building is overhauled the new (lower) energy consumption is compared against modeled values for the original building to calculate the savings from the retrofit. More accurate models could support better market incentives and enable lower cost financing.\n\nThis competition challenges you to build these counterfactual models across four energy types based on historic usage rates and observed weather. The dataset includes three years of hourly meter readings from over one thousand buildings at several different sites around the world.\n\n## Files\n\n$\\color{red}{train.csv}$\n\n- building_id - Foreign key for the building metadata.\n- meter - The meter id code. Read as {0: electricity, 1: chilledwater, 2: steam, 3: hotwater}. Not every building has all meter types.\n- timestamp - When the measurement was taken\n- meter_reading - The target variable. Energy consumption in kWh (or equivalent). Note that this is real data with measurement error, which we expect will impose a baseline level of modeling error.\n\n$\\color{red}{building\\_meta.csv}$\n\n- site_id - Foreign key for the weather files.\n- building_id - Foreign key for training.csv\n- primary_use - Indicator of the primary category of activities for the building based on EnergyStar property type definitions\n- square_feet - Gross floor area of the building\n- year_built - Year building was opened\n- floor_count - Number of floors of the building\n\n$\\color{red}{weather\\_[train/test].csv}$\n\nWeather data from a meteorological station as close as possible to the site.\n\n- site_id\n- air_temperature - Degrees Celsius\n- cloud_coverage - Portion of the sky covered in clouds, in oktas\n- dew_temperature - Degrees Celsius\n- precip_depth_1_hr - Millimeters\n- sea_level_pressure - Millibar/hectopascals\n- wind_direction - Compass direction (0-360)\n- wind_speed - Meters per second\n\n$\\color{red}{test.csv}$\n\nThe submission files use row numbers for ID codes in order to save space on the file uploads. test.csv has no feature data; it exists so you can get your predictions into the correct order.\n\n- row_id - Row id for your submission file\n- building_id - Building id code\n- meter - The meter id code\n- timestamp - Timestamps for the test data period\n\n$\\color{red}{sample\\_submission.csv}$\n\nA valid sample submission.\n\nAll floats in the solution file were truncated to four decimal places; we recommend you do the same to save space on your file upload.\nThere are gaps in some of the meter readings for both the train and test sets. Gaps in the test set are not revealed or scored.\n"},{"metadata":{},"cell_type":"markdown","source":"## Data exploration"},{"metadata":{},"cell_type":"markdown","source":"Let's import the packages first."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%javascript\nIPython.OutputArea.prototype._should_scroll = function(lines) {\n    return false;\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport sys\nfrom IPython.display import HTML \nimport math\nimport datetime as dt\nimport gc\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\npd.set_option('expand_frame_repr', True)\nnp.random.seed(2019)\n#from sklearn.experimental import enable_iterative_imputer\n#from sklearn.impute import IterativeImputer\n#from sklearn.linear_model import BayesianRidge\n#from sklearn.tree import DecisionTreeRegressor\n#from sklearn.ensemble import ExtraTreesRegressor\n#from sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import make_pipeline\n#to hide the code behind a toggle\n#credit to $\\href{https://stackoverflow.com/questions/27934885/how-to-hide-code-from-cells-in-ipython-notebook-visualized-with-nbviewer}{harshil}$\n#HTML('''<script>\n#code_show=true; \n#function code_toggle() {\n# if (code_show){\n# $('div.input').hide();\n# } else {\n# $('div.input').show();\n# }\n# code_show = !code_show\n#} \n#$( document ).ready(code_toggle);\n#</script>\n#<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')#","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Some miscellaneous functions\n#Make size readable\ndef GetHumanReadable(size,precision=2):\n    suffixes=['B','KB','MB','GB','TB']\n    suffixIndex = 0\n    while size > 1024 and suffixIndex < 4:\n        suffixIndex += 1 #increment the index of the suffix\n        size = size/1024.0 #apply the division\n    return \"%.*f%s\"%(precision,size,suffixes[suffixIndex])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's import the data first!"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import*\n\ndf_metadata = pd.read_csv('../input/ashrae-energy-prediction/building_metadata.csv')\ndf_submission = pd.read_csv('../input/ashrae-energy-prediction/sample_submission.csv')\ndf_test = pd.read_csv('../input/ashrae-energy-prediction/test.csv')\ndf_train = pd.read_csv('../input/ashrae-energy-prediction/train.csv')\ndf_weather_test = pd.read_csv('../input/ashrae-energy-prediction/weather_test.csv')\ndf_weather_train = pd.read_csv('../input/ashrae-energy-prediction/weather_train.csv')\ndata_dict = {'df_metadata': df_metadata, 'df_submission': df_submission\n             , 'df_test':df_test, 'df_train':df_train, 'df_weather_test':df_weather_test\n             , 'df_weather_train':df_weather_train}\nprint('Imported', len(data_dict), 'files:')\nprint('Memory usage:')\nfor i in data_dict:\n    print(i,':', GetHumanReadable(sys.getsizeof(data_dict[i])))\nmeter_type = {0: 'electricity', 1: 'chilledwater', 2: 'steam', 3: 'hotwater'}\nmeter_t = {'electricity':0, 'chilledwater':1, 'steam':2, 'hotwater':3}\nmeter_name = ['electricity', 'chilledwater', 'steam', 'hotwater']\nprimary_type = {'Education':0,'Entertainment/public assembly':1,'Food sales and service':2,'Healthcare':3,'Lodging/residential':4,'Manufacturing/industrial':5,'Office':6,'Other':7,'Parking':8,'Public services':9,'Religious worship':10,'Retail':11,'Services':12,'Technology/science':13,'Utility':14,'Warehouse/storage':15}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks demanding memory wise. We may try to reduce the memory usage later. But first, let's take a quick look at our data!"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in data_dict:\n    print(i,':', data_dict[i].shape[0],'rows' ,data_dict[i].shape[1],'columns')\n    if data_dict[i].isnull().any().any():\n        temp = data_dict[i].isnull().sum()\n        print('Missing data :')\n        print('Identifier  -  count')\n        print(temp[temp>0])\n    else:\n        print('No missing data.')\n    print('Preview : ')\n    print(data_dict[i].head(3))\n    print('---------------------------------------------------------------------------------------------------')\ndel temp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are quite a lot of missing data, notably, we don't have much on the floor count and we are missing almost half of the building built date. The proportion of missing data on the weather seems to be about equal in both the train and test set.\nWe may have to check if there is some sort of pattern on the missing data (for example check the timestamp)."},{"metadata":{},"cell_type":"markdown","source":"# Understand what kind of buildings/site we have\nLet's study the $\\color{blue}{metadata}$ first.\nLet's begin with some $\\color{red}{\\text{univariate}}$ analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(3,2, figsize = (12,18))\nsns.countplot(x=df_metadata.site_id,ax= ax[0,0])\nax[0,0].set_title('Building frequency by site')\nsns.countplot(y=df_metadata.primary_use,linewidth=5,ax= ax[1,0])\nax[1,0].set_title('Building frequency by usage')\n\nsns.distplot(df_metadata.square_feet, kde = True, rug = True, ax = ax[0,1])\nax[0,1].set_title('Building size distribution')\nsns.distplot(df_metadata.square_feet.apply(lambda x : math.log(x)), kde = True, rug = True, ax = ax[1,1])\nax[1,1].set_title('Building log size distribution')\nsns.distplot(df_metadata.year_built.dropna(), kde = False, rug = True, bins = 25, ax = ax[2,1])\nax[2,1].set_title('Building frequency by year built')\nsns.countplot(y = df_metadata.floor_count, ax =ax[2,0])\nax[2,0].set_title('Building frequency by floor count')\nprint('Done')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"$\\color{red}{Takeaway}$ : \n- There is a large concentration near site 3 whereas there is barely any building near site 11. Since we use the same weather data for each building associated to the same site, it may be harder to predict the energy usage for the buildings associated to site 3.\n- Education, logding, office, entertainment and public services takes the large majority of the building usage.\n- The majority of the building are less than 100K square feet but there are some large buildings in excess of 800K square feet.\n- Keeping in mind that half the data on the contruction date is missing, we can say that most of the building were built in the 70s-80s along some more recent (2010s and 2020s).\n- Moreover, notice the lack of building built in the 20s and 50s. This may or may not be due to the world wars. We can argue that less construction were made during these time periods or that documentations were either lost or didn't even exist in the first place. \n- With most of the floor count data missing, there are questions that can be made about the floor count. We can ask whether or not the ground floor is counted as one floor or does it just count additional floor. In the case where  the floor count just include floors in addition to the ground floor, we can make a case that most missing data represent the case where there is only the ground floor, and that let us replace the missing data with 0. However, if it's not the case, we will have to think about way to fill this missing data. We will see about that later.\n- Also notice that from our data, most building have less than 5-6 floors.\n\nIn the next part, we justify our takeaway with better numbers."},{"metadata":{"trusted":true},"cell_type":"code","source":"f,a = plt.subplots(1,3, figsize=(14,4))\ntemp1 = df_metadata['primary_use'].value_counts(normalize = True)\nprint('-The top 5 building usage take', round(sum(temp1[0:5])*100,2), '% of the total building usage.')\nsns.barplot(x = ['Top 5 usage','Rest'], y = [sum(temp1[0:5]),sum(temp1[5:])], ax = a[0])\na[0].set_title('insert title')\n\ndel temp1\n\nprint('-The buildings with size less than 100K square feet account for' ,round(df_metadata[df_metadata.square_feet<100000].square_feet.count()*100/df_metadata.shape[0],2),'% of the total.')\nsns.barplot(x = ['<100K', '100-400K', '400-600K', '>600K']\n                 , y = [df_metadata[df_metadata.square_feet<100000].square_feet.count()/df_metadata.shape[0]\n                        ,df_metadata[(df_metadata.square_feet>100000) & (df_metadata.square_feet<400000)].square_feet.count()/df_metadata.shape[0]\n                        ,df_metadata[(df_metadata.square_feet>400000) & (df_metadata.square_feet<600000)].square_feet.count()/df_metadata.shape[0]\n                        ,df_metadata[(df_metadata.square_feet>600000)].square_feet.count()/df_metadata.shape[0]], ax = a[1])\na[1].set_title('')\n\ntemp1 = df_metadata['floor_count'].value_counts(normalize = True).reset_index().floor_count\nprint('-The buildings with less than 5 floors account for', round(sum(temp1[0:5])*100,2), '% of the total.')\nsns.barplot(x = ['<5 floor','>5 floor'], y = [sum(temp1[0:5]),sum(temp1[5:])], ax = a[2])\na[2].set_title('')\n\ndel temp1\n\nprint('')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With this done, we can start $\\color{red}{\\text{bivariate}}$ analysis.\nLet's begin with the $\\color{green}{\\text{site}}$ and its relationship."},{"metadata":{"trusted":true},"cell_type":"code","source":"Map_freq = pd.DataFrame(index = df_metadata.site_id.unique(),columns=df_metadata.primary_use.unique())\n\nfor (num,y) in enumerate(df_metadata.site_id.unique()):\n    t = []\n    for x in df_metadata.primary_use.unique():\n        t.append(df_metadata[(df_metadata.site_id==y) & (df_metadata.primary_use==x)].count(0).values[0])\n    Map_freq.iloc[num] = t\nMap_freq = Map_freq[Map_freq.columns].astype(float)\n\na1 = plt.figure(figsize = (8,8))\na1 = sns.heatmap(Map_freq.transpose(),square=False, cmap = \"gist_gray_r\", annot = True)\na1.set_title('Building frequency by use and site')\n#a1.set_xticklabels(a1.get_xticklabels(),rotation = 45)\ndel Map_freq","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"$\\color{red}{Takeaway}$:\n- Site 13 looks to be really urban like a city-center.\n- There is a huge concentration of public services and education building on site 3."},{"metadata":{"trusted":true},"cell_type":"code","source":"a1 = plt.figure()\na1 = sns.jointplot(data = df_metadata, y = 'square_feet', x = 'site_id', kind='kde')\na1.fig.suptitle('Kdeplot size/site_id')\na2 = plt.figure()\na2 = sns.jointplot(x = df_metadata.site_id, y = df_metadata.square_feet.apply(lambda x : math.log(x)), kind='kde')\na2.set_axis_labels(ylabel='log square feet', xlabel='site_id')\na2.fig.suptitle('Kdeplot logsize/site_id')\ntemp = df_metadata.groupby('site_id').sum().square_feet\nf, a3 = plt.subplots(2,2, figsize = (12,12))\nsns.barplot(y = temp.values, x = temp.index, ax = a3[0,0])\na3[0,0].set_title('Total size by site id')\nsns.countplot(x = df_metadata.site_id, ax = a3[0,1])\na3[0,1].set_title('In comparison, building number by site id')\nsns.violinplot(data = df_metadata, x = 'site_id', y = 'square_feet', ax = a3[1,0])\na3[1,0].set_title('Violinplot')\nsns.boxplot(data = df_metadata, x = 'site_id', y = 'square_feet', ax = a3[1,1])\na3[1,1].set_title('Boxplot')\nprint('')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"$\\color{red}{Takeaway}:$\n- While the site number 3 has the highest number of building, a lot of them are small sized.\n- In comparison, site 7 has a few but large buildings.\n- There is a uniquely large building in site 8.\n- In total surface area, site 3-9-13 are almost the same.\n- There isn't much in site 11."},{"metadata":{"trusted":true},"cell_type":"code","source":"a1 = plt.figure()\na1 = sns.jointplot(x = 'site_id', y = 'year_built', data = df_metadata, color='k')\na1.fig.suptitle('Joint plot site/year built')\nprint('')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, we have no information on the date of construction from the buildings in site 6 and 8-14. It also seems we have a lot of missing information on site 3."},{"metadata":{"trusted":true},"cell_type":"code","source":"a1 = plt.figure()\na1 = sns.catplot(x = 'site_id', y = 'floor_count', data = df_metadata)\na1.fig.suptitle('Catplot floor count/site_id')\nprint('')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the other side, we have no floor count information on site 0, 2,3,6,9,11,13-15."},{"metadata":{},"cell_type":"markdown","source":"Let's then relate the $\\color{red}{\\text{primary use}}$ with the other variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"a1 = plt.figure()\na1 = sns.boxplot(data = df_metadata, y = 'primary_use', x = 'square_feet')\na1.set_title('Boxplot usage/size')\nprint('')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"$\\color{red}{Takeaway}$:\n- Parking are overall the largest buildings\n- There is a huge healthcare building compared to the rest."},{"metadata":{"trusted":true},"cell_type":"code","source":"a1 = plt.figure()\na1 = sns.boxplot(data = df_metadata, x = 'year_built', y = 'primary_use')\na1.set_title('Boxplot usage/year built')\nprint('')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Though there is a lack of data, we can see that the construction of building aimed for education, lodging, office, entertainment and public service never really stopped."},{"metadata":{"trusted":true},"cell_type":"code","source":"Map_freq = pd.DataFrame(index = df_metadata.primary_use.unique(),columns=df_metadata.floor_count.sort_values(na_position='first').unique())\nn = df_metadata.shape[0]\nfor (num,y) in enumerate(df_metadata.primary_use.unique()):\n    t = []\n    t.append(df_metadata.groupby('primary_use').get_group(y).floor_count.isnull().sum())\n    for x in df_metadata.floor_count.sort_values(na_position = 'first').unique()[1:]:\n        t.append(df_metadata[(df_metadata.primary_use==y) & (df_metadata.floor_count==x)].count(0).values[0])\n    Map_freq.iloc[num] = t\nMap_freq = Map_freq[Map_freq.columns].astype(float)\n\na1 = plt.figure( figsize = (18,8))\na1 = sns.heatmap(Map_freq,square=False, cmap = \"gist_gray_r\", annot=True)\na1.set_title('Heat map floor/usage')\n#a1.set_xticklabels(a1.get_xticklabels(),rotation = 45)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not much can be said with that much missing data."},{"metadata":{},"cell_type":"markdown","source":"What about the $\\color{red}{\\text{size}}$ with respect to the last two variables?"},{"metadata":{"trusted":true},"cell_type":"code","source":"a1 = plt.figure()\na1 = (sns.jointplot( data = df_metadata, y = 'square_feet', x = 'year_built', kind='kde',color =\"k\").plot_joint(sns.scatterplot, color=\"r\", size = 0.01, legend = False))\na1.fig.suptitle('Jointplot size/year built')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, can't say much. Most \"medium\" sized building were built recently though."},{"metadata":{"trusted":true},"cell_type":"code","source":"a1 = plt.figure()\na1 = sns.jointplot(y = 'square_feet', x = 'floor_count', data = df_metadata, color='k')\na1.fig.suptitle('Jointplot size/floor')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most building are small size-wise and have small floor count. Though, one really large building has also a small floor count."},{"metadata":{},"cell_type":"markdown","source":"Finally, the last two variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"a1 = plt.figure()\na1 = sns.jointplot(x = 'year_built', y = 'floor_count', data = df_metadata, color = 'k')\na1.fig.suptitle('Joint plot floor/year built')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# What can we say about the energy usage?"},{"metadata":{},"cell_type":"markdown","source":"Now let's check both data files (train/test). But first, let's reduce the memory usage."},{"metadata":{"trusted":true},"cell_type":"code","source":"#def function\ndef reduce_mem_usage(df):\n    start_mem = df.memory_usage(deep = True).sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.uint8).min and c_max < np.iinfo(np.uint8).max:\n                    df[col] = df[col].astype(np.uint8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.uint16).min and c_max < np.iinfo(np.uint16).max:\n                    df[col] = df[col].astype(np.uint16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.uint32).min and c_max < np.iinfo(np.uint32).max:\n                    df[col] = df[col].astype(np.uint32)                    \n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n                elif c_min > np.iinfo(np.uint64).min and c_max < np.iinfo(np.uint64).max:\n                    df[col] = df[col].astype(np.uint64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Reducing memory usage of \\'df_train\\' ')\ndf_train = reduce_mem_usage(df_train)\nprint('Reducing memory usage of \\'df_metadata\\' ')\ndf_metadata = reduce_mem_usage(df_metadata)\nprint('Reducing memory usage of \\'df_weather_train\\' ')\ndf_weather_train = reduce_mem_usage(df_weather_train)\nprint('Reducing memory usage of \\'df_test\\' ')\ndf_test = reduce_mem_usage(df_test)\nprint('Reducing memory usage of \\'df_weather_test\\' ')\ndf_weather_test = reduce_mem_usage(df_weather_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And that the memory usage has been reduced, and it looks great. Well, let's not get ahead of ourselves, unfortunately, we still have to convert the timestamp column into usable datetime format and we have to convert the meter into categorical.\nBut we can now add the metadata and the weather data into the training data and testing data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_train.join(df_metadata.set_index('building_id'), on = 'building_id')\ndf_train['timestamp'] = df_train['timestamp'].astype(str) \ndf_train.timestamp = pd.to_datetime(df_train.timestamp)\ndf_train.meter = df_train.meter.astype('category')\ndf_train.meter.cat.rename_categories(meter_type, inplace = True)\ndf_weather_train['timestamp'] = df_weather_train['timestamp'].astype(str) \ndf_weather_train.timestamp = pd.to_datetime(df_weather_train.timestamp)\n#df_train = pd.merge(df_weather_train, df_train, on = ['site_id', 'timestamp'])\nprint('df_train joined with df_metadata and df_weather_train. Resulting dataframe has a memory usage of :', GetHumanReadable(df_train.memory_usage().sum()))\ndf_test = df_test.join(df_metadata.set_index('building_id'), on = 'building_id')\ndf_test['timestamp'] = df_test['timestamp'].astype(str) \ndf_test.timestamp = pd.to_datetime(df_test.timestamp)\ndf_test.meter = df_test.meter.astype('category')\ndf_test.meter.cat.rename_categories(meter_type, inplace = True)\ndf_weather_test['timestamp'] = df_weather_test['timestamp'].astype(str) \ndf_weather_test.timestamp = pd.to_datetime(df_weather_test.timestamp)\n#df_test = pd.merge(df_weather_test, df_test, on = ['site_id', 'timestamp'])\nprint('df_test joined with df_metadata and df_weather_test. Resulting dataframe has a memory usage of :', GetHumanReadable(df_test.memory_usage().sum()))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What does the final dataframe looks like?"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train DataFrame:')\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at the datastructure of both train/test sets."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"f, ax1 = plt.subplots(2,1,figsize = (10,8))\nsns.countplot(df_train.building_id, ax =ax1[0], saturation = 1)\nax1[0].set_title('Building reference count in training set')\nax1[0].set(xticks=[])\nsns.countplot(df_test.building_id, ax = ax1[1], saturation = 1)\nax1[1].set_title('Building reference count in testing set')\nax1[1].set(xticks=[])\nprint('')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, things looks rough:\n- there are missing 'timestamp' for some building where we have no data at all. It looks more prevalent in the training set than in the test set,\n- a huge majority of buildings are only equipped with one meter type, and it looks like there are more buildings equipped with three meter type than 2."},{"metadata":{},"cell_type":"markdown","source":"Let's see about that. We can add to the metadata information about the equipped meter."},{"metadata":{"trusted":true},"cell_type":"code","source":"Meter_t = []\nfor building_id in range(1449):\n    Meter_t.append(df_train[df_train.building_id == building_id].meter.unique())\ndf_metadata['electricity'] = [('electricity' in Meter_t[building_id]) for building_id in range(1449)]\ndf_metadata['chilledwater'] = [('chilledwater' in Meter_t[building_id]) for building_id in range(1449)]\ndf_metadata['steam'] = [('steam' in Meter_t[building_id]) for building_id in range(1449)]\ndf_metadata['hotwater'] = [('hotwater' in Meter_t[building_id]) for building_id in range(1449)]\ndel Meter_t\nprint('Done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(1,2, figsize = (14,6))\nsns.countplot(x = df_metadata[['electricity', 'chilledwater', 'steam', 'hotwater']].sum(1), ax = ax[0])\nax[0].set_title('Buildings by number of equipped type of energy')\nax[0].set_xlabel('Number equipped')\nsns.barplot(data = df_metadata[['electricity', 'chilledwater', 'steam', 'hotwater']].sum(0).reset_index(), x = 'index', y=0, ax =ax[1])\nax[1].set_title('Meter by number of buildings')\nax[1].set_xlabel('count')\nax[1].set_ylabel('meter type')\nprint('Done')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That confirms our guess.\nBut about the 'missing' timestamp?"},{"metadata":{"trusted":true},"cell_type":"code","source":"Total = df_metadata[['electricity', 'chilledwater', 'steam', 'hotwater']].sum().sum()\nD = df_train.append(df_test).timestamp.value_counts(sort = False).apply(lambda x: Total-x)\nD = D.reset_index()\nD['year']= D['index'].dt.year\nS1 = df_metadata[['electricity', 'chilledwater', 'steam', 'hotwater']].sum(1)\n\n\nD2 = df_train.building_id.value_counts(sort = False).reset_index()\nD2['count'] = D2.apply(lambda row : (S1[row['index']]*(365*24+24) - row['building_id'])/(S1[row['index']]*(365*24+24)),1)\nD2['origin'] = 'train'\nD3 = df_test.building_id.value_counts(sort = False).reset_index()\nD3['count'] = D3.apply(lambda row : (S1[row['index']]*(364*24*2+24*2) - row['building_id'])/(S1[row['index']]*(364*24*2+24*2)),1)\nD3['origin'] = 'test'\n\ndf_metadata['missing_train'] = D2['count']\nD2 = D2.append(D3)\ndel D3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(2,1,figsize = (16,16))\nsns.lineplot(data = D, y = 'timestamp', x = 'index', hue = 'year',ax= ax[0])\nax[0].set_title('Number of missing timestamp record in train/test data (does not include timestamp with no data at all)')\nax[0].set_ylabel('Count')\nax[0].set_xlabel('Timestamp')\nsns.lineplot(data = D2, y = 'count', x = 'index', hue = 'origin',ax= ax[1])\nax[1].set_title('Proportion of missing timestamp record in train/test data by building_id')\nax[1].set_ylabel('Count')\nax[1].set_xlabel('Building_id')\nprint('Done')\n\ndel D, D2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"$\\color{red}{Takeaway}$:\n- there is a huge amount of \"missing\" timestamp in the beginning of each year\n- it looks like for some building, information not collected continuously or the data is hidden\n- there is a lot more missing timestamp in the training data than in the testing data, even though the testing data is over two year compared to the one year of the training data\n- some building_id don't have much data!"},{"metadata":{},"cell_type":"markdown","source":"Let's study the meter type in detail."},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(1,1, figsize = (16,6))\nsns.scatterplot(x= df_metadata.building_id, y= df_metadata.electricity*1 + ~df_metadata.electricity*(-1), ax =ax, label='electricity')\nsns.scatterplot(x= df_metadata.building_id, y= df_metadata.chilledwater*0.8+ ~df_metadata.chilledwater*(-0.8), ax =ax, label='chilledwater')\nsns.scatterplot(x= df_metadata.building_id, y= df_metadata.steam*0.6+ ~df_metadata.steam*(-0.6), ax =ax, label='steam')\nsns.scatterplot(x= df_metadata.building_id, y= df_metadata.hotwater*0.4+ ~df_metadata.hotwater*(-0.4), ax =ax, label='hotwater')\nax.set_ylabel('')\nax.set_xlabel('building_id')\nax.set_title('Presence(positive) or absence (negative) of meter type by building_id')\nax.legend()\nprint('Done')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, it seems like the some building have no electricity!"},{"metadata":{},"cell_type":"markdown","source":"And what about the repartition of the meter type between site and primary usage?"},{"metadata":{"trusted":true},"cell_type":"code","source":"S = df_metadata.groupby('site_id').sum()[['electricity', 'chilledwater', 'steam', 'hotwater']]\nSS = df_metadata.groupby('primary_use').sum()[['electricity', 'chilledwater', 'steam', 'hotwater']]\nSS = SS.reset_index()\n#SS['primary_use'].cat.rename_categories(primary_type, inplace=True)\n#SS.primary_use = SS.primary_use.astype(int)\nSS = pd.melt(SS, id_vars = ['primary_use'], value_vars = ['electricity', 'chilledwater', 'steam', 'hotwater'], var_name='meter', value_name = 'count')\n#SS.primary_use = SS.primary_use.astype('category')\n#SS['primary_use']= SS.primary_use.cat.rename_categories({key:values for (key, values) in zip(primary_type.values(),primary_type.keys())})\nS = S.reset_index()\nS = pd.melt(S, id_vars = 'site_id', value_vars = ['electricity', 'chilledwater', 'steam', 'hotwater'], var_name='meter', value_name = 'count')\nf, ax = plt.subplots(2,1,figsize = (18,12))\nsns.barplot(data = S, x ='site_id', hue = 'meter', y = 'count', ax= ax[0])\nax[0].set_title('Meter type count by site_id')\nsns.barplot(data= SS, x= 'primary_use', y='count', hue='meter', ax=ax[1])\nax[1].set_xticklabels(\n    ax[1].get_xticklabels(), \n    rotation=45, \n    horizontalalignment='right',\n    fontweight='light',\n    fontsize='x-large'\n\n)\nax[1].set_title('Meter type count by primary use')\ndel S, SS","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"$\\color{red}{Takeaway}$:\n- at first, it looked liked from site 3 having only electricity, we would see a pattern with the building primary use but it remains that ratio meter type ratio is \"preserved\" in each primary use strata\n- for the little data we have, it seems food sales and services/ healthcare, lodging/residential and office are better equipped\n- some site have nothing but electricity whereas others are really well equipped"},{"metadata":{},"cell_type":"markdown","source":"Finally, how does the size of the building relate to the energy type usage?"},{"metadata":{"trusted":true},"cell_type":"code","source":"TEMP = pd.melt(df_metadata, id_vars= 'building_id', value_vars=['electricity', 'chilledwater', 'steam', 'hotwater']).join(df_metadata[['building_id', 'square_feet']].set_index('building_id'), on = 'building_id')\nf,ax = plt.subplots(2,2, figsize = (18,18))\nsns.scatterplot(data = TEMP.loc[(TEMP['variable']=='electricity') & (TEMP['value']== True)], x = 'building_id', y = 'square_feet', ax= ax[0,0])\nax[0,0].set_title('Building with electricity')\nsns.scatterplot(data = TEMP.loc[(TEMP['variable']=='chilledwater') & (TEMP['value']== True)], x = 'building_id', y = 'square_feet', ax= ax[0,1])\nax[0,1].set_title('Building with chilledwater')\nsns.scatterplot(data = TEMP.loc[(TEMP['variable']=='steam') & (TEMP['value']== True)], x = 'building_id', y = 'square_feet', ax= ax[1,0])\nax[1,0].set_title('Building with steam')\nsns.scatterplot(data = TEMP.loc[(TEMP['variable']=='hotwater') & (TEMP['value']== True)], x = 'building_id', y = 'square_feet', ax= ax[1,1])\nax[1,1].set_title('Building with hotwater')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not much to say here."},{"metadata":{},"cell_type":"markdown","source":"Now, let's look in details the variable of interest: $\\color{green}{\\text{meter_reading}}$"},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_series(building_id, meter, ax1):\n    \n    if building_id in df_metadata.building_id:\n        if meter in meter_name:\n            if meter == 'electricity':\n                typein = (df_metadata.iloc[building_id].electricity >0)\n            elif meter == 'chilledwater':\n                typein = (df_metadata.iloc[building_id].chilledwater >0)\n            elif meter == 'steam':\n                typein = (df_metadata.iloc[building_id].steam >0)\n            else:\n                typein = (df_metadata.iloc[building_id].hotwater >0)\n            if typein:\n                df_temp = df_train[(df_train.building_id == building_id) & (df_train.meter == meter)][['timestamp', 'meter_reading']].reset_index()\n                sns.lineplot(data = df_temp, x = 'timestamp', y = 'meter_reading', ax=ax1)\n                ax1.set_title('Building ID: '+str(building_id)+ ' : '+ str(meter))\n                return ax1\n            else:\n                print('Error : Building not equipped with this meter type.')\n        else:\n            print('Error : meter type not recognized')\n    else:\n        print('Error : building id not in metadata.')\ndef prt_srs(building_id):\n    print('Display building number' + str(building_id) + '.')\n    print(df_metadata.loc[df_metadata['building_id'] == building_id])\n    if building_id in df_metadata.building_id:\n        meter = []\n        if (df_metadata.iloc[building_id].electricity >0):\n            meter.append('electricity')\n        if (df_metadata.iloc[building_id].chilledwater >0):\n            meter.append('chilledwater')\n        if (df_metadata.iloc[building_id].steam >0):\n            meter.append('steam')\n        if (df_metadata.iloc[building_id].hotwater >0):\n            meter.append('hotwater')\n        if len(meter)>1:\n            f, ax1 = plt.subplots(len(meter),1, figsize=(12,len(meter)*5))\n            for (ind,m) in enumerate(meter):\n                ax1[ind] = print_series(building_id, m, ax1[ind])\n        else:\n            f, ax1 = plt.subplots(1,1, figsize = (12,5))\n            ax1 = print_series(building_id, meter[0], ax1)\n    else:\n        print('Error : building id not in metadata.')\n    print('Done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prt_srs(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A lot of 0's and few spike in the first 5 months before getting a more regular electricity usage. That's odd. There is also a few regime change in our series."},{"metadata":{},"cell_type":"markdown","source":"Let's see another one."},{"metadata":{"trusted":true},"cell_type":"code","source":"prt_srs(200)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The electricity usage looks more regular. It looks like there is a seasonal tendency for the chilledwater. For the hotwater, we have another long sequence of 0's. There is also some 0's \"spike\" here and there for all 3 meter and not exactly at the same time.\nAlso, notice that the energy usage by chilledwater is a lot higher than electricity."},{"metadata":{},"cell_type":"markdown","source":"Another building with education purpose but this time equipped with all 4 meter and 25 times larger than the first building."},{"metadata":{"trusted":true},"cell_type":"code","source":"prt_srs(1232)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we have a lot of 0's \"spike\", two month of neither chilledwater or hotwater usage, huge spike in hotwater usage at some point."},{"metadata":{},"cell_type":"markdown","source":"Finaly what about the building with a lot of missing timestamp?"},{"metadata":{"trusted":true},"cell_type":"code","source":"prt_srs(df_metadata.loc[df_metadata.missing_train == df_metadata.missing_train.max()].building_id.values[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We only have data on the end of the year..."},{"metadata":{},"cell_type":"markdown","source":"Now, let's see how our variable behaves."},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(1,2,figsize = (16,6))\nsns.distplot(df_train['meter_reading'], ax =ax[0])\nax[0].set_title('Distribution of meter_reading')\nsns.distplot(df_train['meter_reading'].apply(lambda x: np.log1p(x)), ax =ax[1])\nax[1].set_title('Distribution of log(meter_reading)')\nax[1].set_ylabel('log(meter_reading)')\nprint('')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems we have a few outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(1,1,figsize = (16,8))\nsns.distplot(df_train.loc[df_train.meter=='electricity','meter_reading'].apply(np.log1p), label = 'electricity', color = (1,0,0), hist=False, ax=ax)\nsns.distplot(df_train.loc[df_train.meter=='chilledwater','meter_reading'].apply(np.log1p), label = 'chilledwater', hist=False, color = (0.5,0.5,0), ax=ax)\nsns.distplot(df_train.loc[df_train.meter=='steam','meter_reading'].apply(np.log1p), label = 'steam', color = (0,0.5,0.5), hist=False, ax=ax)\nsns.distplot(df_train.loc[df_train.meter=='hotwater','meter_reading'].apply(np.log1p), label = 'hotwater', color = (0.33,0.33,0.34), hist=False, ax=ax)\nax.set_title('Distribution of log(meter_reading) by meter type')\nax.set_xlabel('log(meter_reading)')\nprint('')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"# Aggregate visualisation\nLet's have so aggregate visualisation to try to understand our data better/ find anomalies/extremes."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Adding time variable (df_train)\ndf_train['weekday'] = df_train.timestamp.dt.weekday_name.astype('category')\ndf_train['week'] = df_train.timestamp.dt.week.astype(np.int8)\ndf_train['hour'] = df_train.timestamp.dt.hour.astype(np.int8)\ndf_train['dayofmonth'] = df_train.timestamp.dt.day.astype(np.int8)\ndf_train['month'] = df_train.timestamp.dt.month.astype(np.int8)\ndf_train['dayofyear'] = df_train.timestamp.dt.dayofyear.astype(np.int16)\n#Adding time variable (df_test)\ndf_test['weekday'] = df_test.timestamp.dt.weekday_name.astype('category')\ndf_test['week'] = df_test.timestamp.dt.week.astype(np.int8)\ndf_test['hour'] = df_test.timestamp.dt.hour.astype(np.int8)\ndf_test['dayofmonth'] = df_test.timestamp.dt.day.astype(np.int8)\ndf_test['month'] = df_test.timestamp.dt.month.astype(np.int8)\ndf_test['dayofyear'] = df_test.timestamp.dt.dayofyear.astype(np.int16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_maxmonthly = pd.DataFrame(data=df_train.groupby(['building_id', 'meter', 'month']).max().meter_reading.dropna()).reset_index().join(df_metadata.set_index('building_id'), on ='building_id')\n#df_minmonthly = pd.DataFrame(data=df_train.groupby(['building_id', 'meter', 'month']).min().meter_reading.dropna()).reset_index().join(df_metadata.set_index('building_id'), on ='building_id')\n#df_medianmonthly = pd.DataFrame(data=df_train.groupby(['building_id', 'meter', 'month']).median().meter_reading.dropna()).reset_index().join(df_metadata.set_index('building_id'), on ='building_id')\n#df_summonthly = pd.DataFrame(data=df_train.groupby(['building_id', 'meter', 'month']).sum().meter_reading.dropna()).reset_index().join(df_metadata.set_index('building_id'), on ='building_id')\n#df_maxweekday = pd.DataFrame(data=df_train.groupby(['building_id', 'meter', 'weekday']).max().meter_reading.dropna()).reset_index().join(df_metadata.set_index('building_id'), on ='building_id')\n#df_minweekday = pd.DataFrame(data=df_train.groupby(['building_id', 'meter', 'weekday']).min().meter_reading.dropna()).reset_index().join(df_metadata.set_index('building_id'), on ='building_id')\n#df_medianweekday = pd.DataFrame(data=df_train.groupby(['building_id', 'meter', 'weekday']).median().meter_reading.dropna()).reset_index().join(df_metadata.set_index('building_id'), on ='building_id')\n#df_sumweekday = pd.DataFrame(data=df_train.groupby(['building_id', 'meter', 'weekday']).sum().meter_reading.dropna()).reset_index().join(df_metadata.set_index('building_id'), on ='building_id')\n#df_maxhour = pd.DataFrame(data=df_train.groupby(['building_id', 'meter', 'hour']).max().meter_reading.dropna()).reset_index().join(df_metadata.set_index('building_id'), on ='building_id')\n#df_minhour = pd.DataFrame(data=df_train.groupby(['building_id', 'meter', 'hour']).min().meter_reading.dropna()).reset_index().join(df_metadata.set_index('building_id'), on ='building_id')\n#df_medianhour = pd.DataFrame(data=df_train.groupby(['building_id', 'meter', 'hour']).median().meter_reading.dropna()).reset_index().join(df_metadata.set_index('building_id'), on ='building_id')\n#df_sumhour = pd.DataFrame(data=df_train.groupby(['building_id', 'meter', 'hour']).sum().meter_reading.dropna()).reset_index().join(df_metadata.set_index('building_id'), on ='building_id')\n#df_maxdayofyear = pd.DataFrame(data=df_train.groupby(['building_id', 'meter', 'dayofyear']).max().meter_reading.dropna()).reset_index().join(df_metadata.set_index('building_id'), on ='building_id')\n#df_mindayofyear = pd.DataFrame(data=df_train.groupby(['building_id', 'meter', 'dayofyear']).min().meter_reading.dropna()).reset_index().join(df_metadata.set_index('building_id'), on ='building_id')\n#df_mediandayofyear = pd.DataFrame(data=df_train.groupby(['building_id', 'meter', 'dayofyear']).median().meter_reading.dropna()).reset_index().join(df_metadata.set_index('building_id'), on ='building_id')\n#df_sumdayofyear = pd.DataFrame(data=df_train.groupby(['building_id', 'meter', 'dayofyear']).sum().meter_reading.dropna()).reset_index().join(df_metadata.set_index('building_id'), on ='building_id')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = plt.figure(figsize = (8,8))\nax = sns.lineplot(data = df_maxmonthly, y = 'meter_reading', x = 'month', hue = 'building_id', style = 'meter')\nax.set_title('Monthly max per meter/building_id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice that we have some extreme energy usage there. Let's find out more."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Finding building_id')\nprint(df_maxmonthly.loc[df_maxmonthly.meter_reading == df_maxmonthly.meter_reading.max()])\nprint('Display series')\nprt_srs(df_maxmonthly.loc[df_maxmonthly.meter_reading == df_maxmonthly.meter_reading.max()].building_id.values[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, that is order of magnitude higher than other building steam reading.\nLet's work on this alone so this does not influence our models."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ex_train =df_train.loc[(df_train.building_id == 1099) & (df_train.meter == 'steam')]\ndf_ex_test = df_test.loc[(df_test.building_id == 1099) & (df_test.meter == 'steam')]\nindex_name = df_train.loc[(df_train.building_id == 1099) & (df_train.meter == 'steam')].index\ndf_train.drop(index_name, inplace = True)\nindex_name = df_test.loc[(df_test.building_id == 1099) & (df_test.meter == 'steam')].index\ndf_test.drop(index_name, inplace = True)\ndel index_name","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Computing aggregate')\ndf_maxmonthly = pd.DataFrame(data=df_train.groupby(['building_id', 'meter', 'month']).max().meter_reading.dropna()).reset_index().join(df_metadata.set_index('building_id'), on ='building_id')\ndf_minmonthly = pd.DataFrame(data=df_train.groupby(['building_id', 'meter', 'month']).min().meter_reading.dropna()).reset_index().join(df_metadata.set_index('building_id'), on ='building_id')\ndf_medianmonthly = pd.DataFrame(data=df_train.groupby(['building_id', 'meter', 'month']).median().meter_reading.dropna()).reset_index().join(df_metadata.set_index('building_id'), on ='building_id')\ndf_summonthly = pd.DataFrame(data=df_train.groupby(['building_id', 'meter', 'month']).sum().meter_reading.dropna()).reset_index().join(df_metadata.set_index('building_id'), on ='building_id')\ndf_maxweekday = pd.DataFrame(data=df_train.groupby(['building_id', 'meter', 'weekday']).max().meter_reading.dropna()).reset_index().join(df_metadata.set_index('building_id'), on ='building_id')\ndf_minweekday = pd.DataFrame(data=df_train.groupby(['building_id', 'meter', 'weekday']).min().meter_reading.dropna()).reset_index().join(df_metadata.set_index('building_id'), on ='building_id')\ndf_medianweekday = pd.DataFrame(data=df_train.groupby(['building_id', 'meter', 'weekday']).median().meter_reading.dropna()).reset_index().join(df_metadata.set_index('building_id'), on ='building_id')\ndf_sumweekday = pd.DataFrame(data=df_train.groupby(['building_id', 'meter', 'weekday']).sum().meter_reading.dropna()).reset_index().join(df_metadata.set_index('building_id'), on ='building_id')\ndf_maxhour = pd.DataFrame(data=df_train.groupby(['building_id', 'meter', 'hour']).max().meter_reading.dropna()).reset_index().join(df_metadata.set_index('building_id'), on ='building_id')\ndf_minhour = pd.DataFrame(data=df_train.groupby(['building_id', 'meter', 'hour']).min().meter_reading.dropna()).reset_index().join(df_metadata.set_index('building_id'), on ='building_id')\ndf_medianhour = pd.DataFrame(data=df_train.groupby(['building_id', 'meter', 'hour']).median().meter_reading.dropna()).reset_index().join(df_metadata.set_index('building_id'), on ='building_id')\ndf_sumhour = pd.DataFrame(data=df_train.groupby(['building_id', 'meter', 'hour']).sum().meter_reading.dropna()).reset_index().join(df_metadata.set_index('building_id'), on ='building_id')\n#df_maxdayofyear = pd.DataFrame(data=df_train.groupby(['building_id', 'meter', 'dayofyear']).max().meter_reading.dropna()).reset_index().join(df_metadata.set_index('building_id'), on ='building_id')\n#df_mindayofyear = pd.DataFrame(data=df_train.groupby(['building_id', 'meter', 'dayofyear']).min().meter_reading.dropna()).reset_index().join(df_metadata.set_index('building_id'), on ='building_id')\n#df_mediandayofyear = pd.DataFrame(data=df_train.groupby(['building_id', 'meter', 'dayofyear']).median().meter_reading.dropna()).reset_index().join(df_metadata.set_index('building_id'), on ='building_id')\ndf_sumdayofyear = pd.DataFrame(data=df_train.groupby(['building_id', 'meter', 'dayofyear']).sum().meter_reading.dropna()).reset_index().join(df_metadata.set_index('building_id'), on ='building_id')\nprint('Done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_aggregate( dataframe, groupby, time_type = 'month', graph_type = 'line', building_id = None, primary_use = None,site_id = None, aggregate = 'all',  meter = None, graph_name = '', ax = None):\n    time_candidate = {'month', 'dayofyear', 'dayofmonth', 'week', 'weekday', 'hour'}\n    groupby_candidate = {'building_id', 'site_id', 'meter', 'primary_use', 'square_feet', 'year_built', 'floor_count'}\n    aggregate_candidate = {'all', 'min', 'max', 'sum', 'median'}\n    graph_type_candidate = {'line'}\n    if (time_type not in time_candidate) or (time_type not in dataframe.columns):\n        print('Error : time_type error.')\n        return\n    if (groupby not in groupby_candidate) or (groupby not in dataframe.columns):\n        print('Groupby error.')\n        return\n    if (aggregate not in aggregate_candidate):\n        print('Aggregate type error.')\n        return\n    if graph_type not in graph_type_candidate:\n        print('Graph type error.')\n        return\n    if groupby == 'building_id':\n        if building_id is None:\n            if ax is None:\n                ax = plt.figure()\n                ax = sns.lineplot(data = dataframe, y='meter_reading', x= time_type, hue = 'building_id', style = 'meter')\n                ax.set_title(graph_name)\n            else:\n                sns.lineplot(data = dataframe, y = 'meter_reading', x = time_type, hue='building_id', style = 'meter', ax = ax)\n                ax.set_title(graph_name)\n        elif building_id in dataframe.building_id.unique():\n            if ax is None:\n                ax = plt.figure()\n                ax = sns.lineplot(data = dataframe[dataframe.building_id == building_id].reset_index(), y = 'meter_reading', x = time_type, hue='meter')\n                ax.set_title(graph_name)\n            else: \n                sns.lineplot(data = dataframe[dataframe.building_id == building_id].reset_index(), y = 'meter_reading', x = time_type, hue='meter', ax = ax)\n                ax.set_title(graph_name)\n        else:\n            print('Building_id error.')\n    if groupby == 'site_id': \n        if aggregate == 'all':\n            if (site_id is None) or (site_id not in df_metadata.site_id.unique()):\n                print('Use building_id instead.')\n            else:\n                ax = plt.figure()\n                ax = sns.lineplot(data = dataframe[dataframe.site_id== site_id].reset_index, y='meter_reading', x=time_type, hue = 'building_id', style = 'meter')\n                ax.set_title(graph_name)\n            return\n        if aggregate == 'min':\n            df_t = pd.DataFrame(dataframe.groupby(['site_id','meter', time_type]).min().meter_reading.dropna()).reset_index()\n        if aggregate == 'max':\n            df_t = pd.DataFrame(dataframe.groupby(['site_id','meter', time_type]).max().meter_reading.dropna()).reset_index()\n\n        if aggregate == 'sum':\n            df_t = pd.DataFrame(dataframe.groupby(['site_id','meter', time_type]).sum().meter_reading.dropna()).reset_index()\n\n        if aggregate == 'median':\n            df_t = pd.DataFrame(dataframe.groupby(['site_id','meter', time_type]).median().meter_reading.dropna()).reset_index()\n        if ax is None:\n            ax = plt.figure()\n            ax = sns.lineplot(data=df_t, y = 'meter_reading', x = time_type, style = 'meter', hue = 'site_id')\n            ax.set_title(graph_name)\n        else:\n            sns.lineplot(data=df_t, y = 'meter_reading', x = time_type, style = 'meter', hue = 'site_id',ax= ax)\n            ax.set_title(graph_name)\n    if groupby == 'meter':\n        if aggregate == 'all':\n            if (meter is None) or (meter not in ['electricity', 'chilledwater', 'steam', 'hotwater']):\n                print('Use building_id instead.')\n            else:\n                ax = plt.figure()\n                ax = sns.lineplot(data = dataframe[dataframe.meter == meter].reset_index(), y='meter_reading', x=time_type, hue='building_id')\n                ax.set_title(graph_name)\n            return\n        if aggregate == 'min':\n            df_t = pd.DataFrame(dataframe.groupby(['meter', time_type]).min().meter_reading.dropna()).reset_index()\n            \n        if aggregate == 'max':\n            df_t = pd.DataFrame(dataframe.groupby(['meter', time_type]).max().meter_reading.dropna()).reset_index()\n            \n        if aggregate == 'sum':\n            df_t = pd.DataFrame(dataframe.groupby(['meter', time_type]).sum().meter_reading.dropna()).reset_index()\n            \n        if aggregate == 'median':\n            df_t = pd.DataFrame(dataframe.groupby(['meter', time_type]).median().meter_reading.dropna()).reset_index()\n        if ax is None:\n            ax = plt.figure()\n            ax = sns.lineplot(data=df_t, y = 'meter_reading', x = time_type, hue = 'meter')\n            ax.set_title(graph_name)\n        else:\n            sns.lineplot(data=df_t, y = 'meter_reading', x = time_type, hue = 'meter', ax= ax)\n            ax.set_title(graph_name)\n    if groupby == 'primary_use':\n        if aggregate == 'all':\n            if (primary_use is None) or (meter not in df_metadata.primary_use.unique()):\n                print('Use building_id instead.')\n            else:\n                ax = plt.figure()\n                ax = sns.lineplot(data = dataframe[dataframe.primary_use == primary_use].reset_index(), y='meter_reading', x=time_type, hue='building_id')\n                ax.set_title(graph_name)\n            return\n        if aggregate == 'min':\n            df_t = pd.DataFrame(dataframe.groupby(['primary_use','meter', time_type]).min().meter_reading.dropna()).reset_index()\n        if aggregate == 'max':\n            df_t = pd.DataFrame(dataframe.groupby(['primary_use','meter', time_type]).max().meter_reading.dropna()).reset_index()\n\n        if aggregate == 'sum':\n            df_t = pd.DataFrame(dataframe.groupby(['primary_use','meter', time_type]).sum().meter_reading.dropna()).reset_index()\n\n        if aggregate == 'median':\n            df_t = pd.DataFrame(dataframe.groupby(['primary_use','meter', time_type]).median().meter_reading.dropna()).reset_index()\n        if ax is None:\n            ax = plt.figure(figsize = (12,12))\n            ax = sns.lineplot(data=df_t, y = 'meter_reading', x = time_type, style = 'meter', hue = 'primary_use')\n            ax.set_title(graph_name)\n            box = ax.get_position()\n            ax.set_position([box.x0, box.y0, box.width * 0.85, box.height]) # resize position\n\n            # Put a legend to the right side\n            ax.legend(loc='center right', bbox_to_anchor=(1.25, 0.5), ncol=1)\n        else:\n            sns.lineplot(data=df_t, y = 'meter_reading', x = time_type, style = 'meter', hue = 'primary_use',ax= ax)\n            ax.set_title(graph_name)\n\n    \n    if groupby == 'square_feet':\n        df_t = pd.DataFrame(dataframe.groupby(['building_id', 'meter']).sum().meter_reading.dropna()).reset_index().join(df_metadata.set_index('building_id'), on = 'building_id')\n        if ax is None:\n            ax = plt.figure()\n            ax = sns.scatterplot(data = df_t, x = 'square_feet', y = 'meter_reading', hue = 'meter')\n            ax.set_title(graph_name)\n        else:\n            sns.scatterplot(data = df_t, x = 'square_feet', y = 'meter_reading', hue = 'meter', ax = ax)\n            ax.set_title(graph_name)\n    if groupby == 'year_built':\n        df_t = pd.DataFrame(dataframe.groupby(['building_id', 'meter']).sum().meter_reading.dropna()).reset_index().join(df_metadata.set_index('building_id'), on = 'building_id')\n        if ax is None:\n            ax = plt.figure()\n            ax = sns.scatterplot(data = df_t, x = 'year_built', y = 'meter_reading', hue = 'meter')\n            ax.set_title(graph_name)\n        else:\n            sns.scatterplot(data = df_t, x = 'year_built', y = 'meter_reading', hue = 'meter', ax = ax)\n            ax.set_title(graph_name)\n    if groupby == 'floor_count':\n        df_t = pd.DataFrame(dataframe.groupby(['building_id', 'meter']).sum().meter_reading.dropna()).reset_index().join(df_metadata.set_index('building_id'), on = 'building_id')\n        if ax is None:\n            ax = plt.figure()\n            ax = sns.scatterplot(data = df_t, x = 'floor_count', y = 'meter_reading', hue = 'meter')\n            ax.set_title(graph_name)\n        else:\n            sns.scatterplot(data = df_t, x = 'floor_count', y = 'meter_reading', hue = 'meter', ax = ax)\n            ax.set_title(graph_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax1 = plt.subplots(2,2, figsize=(20,20))\nprint_aggregate(df_maxmonthly, groupby='building_id',graph_name = 'Max monthly', ax = ax1[0,0])\nprint_aggregate(df_minmonthly, groupby='building_id',graph_name = 'Min monthly', ax = ax1[0,1])\nprint_aggregate(df_summonthly, groupby='building_id',graph_name = 'Sum monthly', ax = ax1[1,0])\nprint_aggregate(df_medianmonthly, groupby='building_id',graph_name = 'Median monthly', ax = ax1[1,1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's a sudden spike in chilled water usage that we have..."},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax1 = plt.subplots(2,2, figsize=(20,20))\nprint_aggregate(df_maxweekday,time_type = 'weekday', groupby='building_id',graph_name = 'Max weekday', ax = ax1[0,0])\nprint_aggregate(df_minweekday,time_type = 'weekday', groupby='building_id',graph_name = 'Min weekday', ax = ax1[0,1])\nprint_aggregate(df_sumweekday,time_type = 'weekday', groupby='building_id',graph_name = 'Sum weekday', ax = ax1[1,0])\nprint_aggregate(df_medianweekday,time_type = 'weekday', groupby='building_id',graph_name = 'Median weekday', ax = ax1[1,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax1 = plt.subplots(2,2, figsize=(20,20))\nprint_aggregate(df_maxhour,time_type = 'hour', groupby='building_id',graph_name = 'Max hourly', ax = ax1[0,0])\nprint_aggregate(df_minhour,time_type = 'hour', groupby='building_id',graph_name = 'Min hourly', ax = ax1[0,1])\nprint_aggregate(df_sumhour,time_type = 'hour', groupby='building_id',graph_name = 'Sum hourly', ax = ax1[1,0])\nprint_aggregate(df_medianhour,time_type = 'hour', groupby='building_id',graph_name = 'Median hourly', ax = ax1[1,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax1 = plt.subplots(2,2,figsize = (20,20))\nprint_aggregate(df_summonthly, groupby = 'site_id',aggregate= 'sum',  ax = ax1[0,0], graph_name='Total consumption by site id')\nprint_aggregate(df_summonthly, groupby = 'floor_count', ax = ax1[0,1], graph_name='Total consumption by floor count')\nprint_aggregate(df_summonthly, groupby = 'year_built', ax = ax1[1,0], graph_name='Total consumption by year built')\nprint_aggregate(df_summonthly, groupby = 'square_feet', ax = ax1[1,1], graph_name='Total consumption by square_feet')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You would expect larger building to use more energy and yet this doesn't seem to be the case. Notice the 5 extreme points in figure \"Total consumption by square feet\". In particular, the chilled water point is alone above 1e8 whereas steam are more common."},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax1 = plt.subplots(2,2,figsize = (20,20))\nprint_aggregate(df_sumweekday,time_type='weekday', groupby = 'meter', aggregate='sum', ax = ax1[0,0], graph_name = 'Total kwh by meter type')\nprint_aggregate(df_sumhour,time_type='hour', groupby = 'meter', aggregate='sum', ax = ax1[0,1], graph_name = 'Total kwh by meter type')\nprint_aggregate(df_summonthly, groupby = 'meter', aggregate='sum', ax = ax1[1,0], graph_name = 'Total kwh by meter type')\nprint_aggregate(df_sumdayofyear, time_type='dayofyear',groupby = 'meter', aggregate='sum', ax = ax1[1,1], graph_name = 'Total kwh by meter type')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"$\\color{red}{Takeaway}$:\n- end of the week : generally less energy usage\n- steam usage peak before chilled water and electricity (on a daily basis) whereas hot water has no peak\n- steam usage peak in the beginning of the year and at the end\n- we may want to remove the few \"extreme\" value and reevaluate our interpretations since they are enough to tilt our graphs."},{"metadata":{"trusted":true},"cell_type":"code","source":"#get extreme building_id index\nKK = pd.DataFrame(df_summonthly.groupby(['building_id', 'meter']).sum().meter_reading.dropna()).reset_index()\nextreme_index = KK.loc[KK.meter_reading>2e8].building_id.values\ndel KK\nprint('Without the extremes')\nf, ax1 = plt.subplots(2,2,figsize = (20,20))\nprint_aggregate(df_sumweekday.loc[df_sumweekday.building_id.apply(lambda x: x not in extreme_index)],time_type='weekday', groupby = 'meter', aggregate='sum', ax = ax1[0,0], graph_name = 'Total kwh by meter type')\nprint_aggregate(df_sumhour.loc[df_sumhour.building_id.apply(lambda x: x not in extreme_index)],time_type='hour', groupby = 'meter', aggregate='sum', ax = ax1[0,1], graph_name = 'Total kwh by meter type')\nprint_aggregate(df_summonthly.loc[df_summonthly.building_id.apply(lambda x: x not in extreme_index)], groupby = 'meter', aggregate='sum', ax = ax1[1,0], graph_name = 'Total kwh by meter type')\nprint_aggregate(df_sumdayofyear.loc[df_sumdayofyear.building_id.apply(lambda x: x not in extreme_index)], time_type='dayofyear',groupby = 'meter', aggregate='sum', ax = ax1[1,1], graph_name = 'Total kwh by meter type')\n#and remove them from the training data\nett = df_train.loc[(df_train.building_id.apply(lambda x: x in extreme_index)) & (df_train.meter.apply(lambda x: x in ['steam', 'chilledwater']) )].index\ndf_train.drop(ett)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, the pattern are similar..."},{"metadata":{"trusted":true},"cell_type":"code","source":"del df_maxmonthly,df_minmonthly,df_medianmonthly,df_summonthly,df_maxweekday,df_minweekday,df_medianweekday,df_sumweekday,df_maxhour,df_minhour,df_medianhour,df_sumhour\n#del df_maxdayofyear\n#del df_mindayofyear\n#del df_mediandayofyear\ndel df_sumdayofyear","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Filling the NAs"},{"metadata":{},"cell_type":"markdown","source":"Let's recall what missing data we have:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most missing data comes from the metadata (year_build and floor_count), and the rest comes the weather data, in particular the cloud coverage."},{"metadata":{},"cell_type":"markdown","source":"Let's work on the weather data first."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Appending the testing weather data to the training weather data.')\nDF_WT_Comb = df_weather_train.append(df_weather_test)\nmis_col_n = {'air_temperature','cloud_coverage', 'dew_temperature', 'precip_depth_1_hr', 'sea_level_pressure', 'wind_direction', 'wind_speed'} \nDF_WT_Comb['year'] = DF_WT_Comb.timestamp.dt.year\nDF_WT_Comb['month'] = DF_WT_Comb.timestamp.dt.month\nDF_WT_Comb['dayofyear'] = DF_WT_Comb.timestamp.dt.dayofyear\nDF_WT_Comb['week'] = DF_WT_Comb.timestamp.dt.week\nDF_WT_Comb = DF_WT_Comb.reset_index()\nfor n in mis_col_n:\n    t_name = n+'miss'\n    DF_WT_Comb[t_name] = DF_WT_Comb[n].isnull()\nprint('Done.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.FacetGrid(data = DF_WT_Comb, row = 'site_id', height = 4, aspect = 3)\nax = ax.map(sns.lineplot, 'timestamp',  'air_temperature', label = 'Air temp')\nax.set(ylim = (-30,50))\nfor axe in range(16):\n    ax.axes[axe][0].vlines(x = DF_WT_Comb[(DF_WT_Comb.site_id == axe) & (DF_WT_Comb.air_temperaturemiss == True)].timestamp.values, color=(1,0,0), ymin= -30, ymax = 50, label = 'Missing values')\n    ax.axes[axe][0].set_title('Air temperature vs time : site id number '+ str(axe))\n    ax.axes[axe][0].legend(loc='best')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, there are not a lot of missing data and moreover we can see the structure of the air temperature over time. We got seasonal trend plus innovation. We could try to create and estimate a model for each site (since it looks like each site have a different climate) using the three year data and then predict the missing values or we could use linear interpolation and hope it works since we don't have much missing data. Or we could try the experimental iterative imputer from sklearn.\nFinally, notice that site 7 and 11 have almost the same series."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Ratio of values that are equal between site 7 and site 11:')\nprint((DF_WT_Comb.loc[DF_WT_Comb.site_id ==7].fillna(99999999).reset_index() == DF_WT_Comb.loc[DF_WT_Comb.site_id ==11].fillna(99999999).reset_index()).sum()/len(DF_WT_Comb.loc[DF_WT_Comb.site_id ==11]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So either site 7 and 11 are really close, or some data got replaced. In fact, from a weather point of view, they are the same."},{"metadata":{},"cell_type":"markdown","source":"For now, we will use a cubic interpolation for the weather data. If I have time later, I will try to evaluate other methods (Linear, Cubic, Quadratic, Mean, Median, Iterative Imputer then evaluate using a CV sample)."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_weather_train['air_temperature'].interpolate('cubic', inplace= True)\ndf_weather_test['air_temperature'].interpolate('cubic', inplace= True)\nprint(df_weather_test.air_temperature.isnull().sum())\nprint(df_weather_train.air_temperature.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cloud_diff = pd.Series()\n#for i in range(16):\n#    t = DF_WT_Comb[DF_WT_Comb.site_id == i]['cloud_coverage']\n#    cloud_diff = cloud_diff.append(t)\n#DF_WT_Comb['cloud_diff'] = cloud_diff.values\n#del cloud_diff, t\nax = sns.FacetGrid(data = DF_WT_Comb, row = 'site_id', height = 4, aspect = 3)\nax = ax.map(sns.lineplot,'timestamp','cloud_coverage', label='meter_reading')\n\nax.set(xlim=(pd.Timestamp('2016-01-01 00:00:00'), pd.Timestamp('2018-12-31 23:00:00')))\nax.set(ylim = (-1,10))\nfor axe in range(16):\n    ax.axes[axe][0].vlines(x = DF_WT_Comb[(DF_WT_Comb.site_id == axe) & (DF_WT_Comb.cloud_coveragemiss == True)].timestamp.values, color='red', ymin= -1, ymax = -0.5, linewidth = 0.1, label = 'missing values')\n    ax.axes[axe][0].set_title('Cloud coverage vs time : site id number '+ str(axe))\n    ax.axes[axe][0].legend(loc='best')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.FacetGrid(data = DF_WT_Comb, col = 'site_id',col_wrap=4)\nax = ax.map(sns.distplot,'cloud_coverage', kde=False)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We may want to drop this variable altogether. We have absolutely no data (in both train and test set) on site 8 and 12. Moreover, site 2 has only two values (0 and 9), and the rest of the data is missing. Missing value imputation on these sites should not be appropriate, since we have no data to relate the various sites (for all we know, the site may be on different continent). "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_weather_train.drop('cloud_coverage', inplace = True, axis = 1)\ndf_weather_test.drop('cloud_coverage', inplace = True, axis = 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.FacetGrid(data = DF_WT_Comb, row = 'site_id', height = 4, aspect = 3)\nax = ax.map(sns.lineplot, 'timestamp',  'dew_temperature', label = 'meter_reading')\nax.set(ylim = (-40,30))\nfor axe in range(16):\n    ax.axes[axe][0].vlines(x = DF_WT_Comb[(DF_WT_Comb.site_id == axe) & (DF_WT_Comb.dew_temperaturemiss == True)].timestamp.values, color='red', ymin= -40, ymax = 30, label = 'missing values')\n    ax.axes[axe][0].set_title('Dew temperature vs time : site id number '+ str(axe))\n    ax.axes[axe][0].legend(loc='best')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The trend looks the same as the air temperature, except that the series present more observable low spikes. We can try to use the same method/model as the air temperature series."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_weather_train['dew_temperature'].interpolate(method='cubic', inplace=True)\ndf_weather_test['dew_temperature'].interpolate(method='cubic', inplace=True)\nprint(df_weather_test.dew_temperature.isnull().sum())\nprint(df_weather_train.dew_temperature.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.FacetGrid(data = DF_WT_Comb, row = 'site_id', height = 5, aspect = 3)\n#DF_WT_Comb['logprec'] = DF_WT_Comb['precip_depth_1_hr'].apply(np.log1p)\nax = ax.map(sns.lineplot, 'timestamp',  'precip_depth_1_hr', label = 'meter_reading')\n#ax = ax.map(sns.distplot,  'logprec', label = 'meter_reading', hist=False)\n\n\nfor axe in range(16):\n    ax.axes[axe][0].vlines(x = DF_WT_Comb[(DF_WT_Comb.site_id == axe) & (DF_WT_Comb.precip_depth_1_hrmiss == True)].timestamp.values, color='red', ymin= 0, ymax = 600, label = 'missing values')\n    ax.axes[axe][0].set_title('Precip vs time : site id number '+ str(axe))\n    ax.axes[axe][0].legend(loc='best')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.FacetGrid(data = DF_WT_Comb, col = 'site_id', col_wrap = 4)\nDF_WT_Comb['logprec'] = DF_WT_Comb['precip_depth_1_hr'].apply(np.log1p)\nax = ax.map(sns.distplot,  'logprec', hist=False)\nax.set(xlabel='log(precipitation)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_weather_train.drop('precip_depth_1_hr', inplace=True, axis=1)\ndf_weather_test.drop('precip_depth_1_hr', inplace=True, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.FacetGrid(data = DF_WT_Comb, row = 'site_id', height = 4, aspect = 3)\nax = ax.map(sns.lineplot, 'timestamp',  'sea_level_pressure', label='meter_reading')\nfor axe in range(16):\n    ax.axes[axe][0].vlines(x = DF_WT_Comb[(DF_WT_Comb.site_id == axe) & (DF_WT_Comb.sea_level_pressuremiss == True)].timestamp.values, color='red', ymin= 980, ymax = 1040, label = 'missing values')\n    ax.axes[axe][0].set_title('Sea level pressure vs time : site id number '+ str(axe))\n    ax.axes[axe][0].legend(loc='best')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_weather_train['sea_level_pressure'] = df_weather_train['sea_level_pressure'].fillna(DF_WT_Comb['sea_level_pressure'].median())\ndf_weather_test['sea_level_pressure'] = df_weather_test['sea_level_pressure'].fillna(DF_WT_Comb['sea_level_pressure'].median())\nprint(df_weather_test.sea_level_pressure.isnull().sum())\nprint(df_weather_train.sea_level_pressure.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.FacetGrid(data = DF_WT_Comb[DF_WT_Comb.year==2016], col = 'site_id', col_wrap=2, hue = 'month' ,subplot_kws=dict(projection='polar'),sharex=False, sharey=False, despine=False, height = 6, aspect = 1)\nax = (ax.map(sns.scatterplot,  'wind_direction','wind_speed').add_legend())\nfor axe in range(16):\n    ax.axes[axe].set_title('Wind direction and speed for year 2016: site '+ str(axe))\nax.set(ylabel='')\nax.set(xlabel='')\nprint('')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rose_plot(angles, bins=16, density=None, offset=0, lab_unit=\"degrees\",\n              start_zero=False, **param_dict):\n    #credit to https://stackoverflow.com/questions/22562364/circular-histogram-for-python\n    #see also https://github.com/msmbuilder/msmexplorer/issues/98 for alternative\n    \"\"\"\n    Plot polar histogram of angles on ax. ax must have been created using\n    subplot_kw=dict(projection='polar'). Angles are expected in radians.\n    \"\"\"\n    ax = plt.gca(projection='polar')\n    # Wrap angles to [-pi, pi)\n    angles = (angles + np.pi) % (2*np.pi) - np.pi\n\n    # Set bins symetrically around zero\n    if start_zero:\n        # To have a bin edge at zero use an even number of bins\n        if bins % 2:\n            bins += 1\n        bins = np.linspace(-np.pi, np.pi, num=bins+1)\n\n    # Bin data and record counts\n    count, bin = np.histogram(angles, bins=bins)\n\n    # Compute width of each bin\n    widths = np.diff(bin)\n\n    # By default plot density (frequency potentially misleading)\n    if density is None or density is True:\n        # Area to assign each bin\n        area = count / angles.size\n        # Calculate corresponding bin radius\n        radius = (area / np.pi)**.5\n    else:\n        radius = count\n\n    # Plot data on ax\n    ax.bar(bin[:-1], radius, zorder=1, align='edge', width=widths,\n           edgecolor='C0', fill=False, linewidth=1)\n\n    # Set the direction of the zero angle\n    ax.set_theta_offset(offset)\n\n    # Remove ylabels, they are mostly obstructive and not informative\n    ax.set_yticks([])\n\n    if lab_unit == \"radians\":\n        label = ['$0$', r'$\\pi/4$', r'$\\pi/2$', r'$3\\pi/4$',\n                  r'$\\pi$', r'$5\\pi/4$', r'$3\\pi/2$', r'$7\\pi/4$']\n        ax.set_xticklabels(label)\n    return ax\n\nax = sns.FacetGrid(data = DF_WT_Comb, col = 'site_id', col_wrap=2,subplot_kws=dict(projection='polar'),sharex=False, sharey=False, despine=False, height = 6, aspect = 1)\nax = ax.map(rose_plot,  'wind_direction', bins = 30)\nfor axe in range(16):\n    ax.axes[axe].set_title('Circular histogram wind direction train+test: site '+ str(axe))\nprint('')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pattern? Or a trick?"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_weather_train['wind_direction']=df_weather_train['wind_direction'].astype(float).interpolate(method='pad')\ndf_weather_test['wind_direction']=df_weather_test['wind_direction'].astype(float).interpolate(method='pad')\nprint(df_weather_train['wind_direction'].isnull().sum())\nprint(df_weather_test['wind_direction'].isnull().sum())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.FacetGrid(data = DF_WT_Comb, row = 'site_id', height = 4, aspect = 3)\nax = ax.map(sns.lineplot, 'timestamp',  'wind_speed', label='meter_reading')\nfor axe in range(16):\n    ax.axes[axe][0].vlines(x = DF_WT_Comb[(DF_WT_Comb.site_id == axe) & (DF_WT_Comb.wind_speedmiss == True)].timestamp.values, color='red', ymin= 0, ymax = 16, label = 'missing values')\n    ax.axes[axe][0].set_title('Wind speed vs time : site id number '+ str(axe))\n    ax.axes[axe][0].legend(loc='best')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_weather_train['wind_speed'].interpolate(method='cubic', inplace= True)\ndf_weather_test['wind_speed'].interpolate(method='cubic', inplace= True)\nprint(df_weather_train['wind_speed'].isnull().sum())\nprint(df_weather_test['wind_speed'].isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Merge weather and main df\ndf_train = df_train.merge(df_weather_train, on = ['site_id', 'timestamp'], how='left')\ndf_test = df_test.merge(df_weather_test, on = ['site_id', 'timestamp'], how='left')\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now for year built and floor_count\ndf_train['year_built'] = df_train['year_built'].astype(float).interpolate(method='pad').astype(np.int16)\nprint(df_train['year_built'].isnull().sum())\ndf_test['year_built'] = df_test['year_built'].astype(float).interpolate(method='pad').astype(np.int16)\nprint(df_test['year_built'].isnull().sum())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['floor_count'] = df_train['floor_count'].fillna(0)\ndf_test['floor_count'] = df_test['floor_count'].fillna(0)\nprint(df_train['floor_count'].isnull().sum())\nprint(df_test['floor_count'].isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Last conversion\ndf_train['site_id'] = df_train['site_id'].astype('category')\ndf_train['building_id'] = df_train['building_id'].astype('category')\ndf_train['floor_count'] = df_train['floor_count'].astype(np.uint8)\ndf_train['meter_reading'] = df_train['meter_reading'].apply(np.log1p)\ndf_train['square_feet'] = df_train['square_feet'].apply(np.log1p)\ndf_test['site_id'] = df_test['site_id'].astype('category')\ndf_test['building_id'] = df_test['building_id'].astype('category')\ndf_test['floor_count'] = df_test['floor_count'].astype(np.uint8) \ndf_test['square_feet'] = df_test['square_feet'].apply(np.log1p) #remember to apply expm1\ndf_train['wind_direction'] = df_train['wind_direction'].astype(np.float16)\ndf_test['wind_direction'] = df_test['wind_direction'].astype(np.float16)\ndf_train['square_feet'] = df_train['square_feet'].astype(np.float32)\ndf_test['square_feet'] = df_test['square_feet'].astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#remove extremes from training set or maybe not\n#ett = df_train.loc[(df_train.building_id.apply(lambda x: x in extreme_index)) & (df_train.meter.apply(lambda x: x in ['steam', 'chilledwater']) )].index\n#df_train.drop(ett, inplace= True)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":1}