{"cells":[{"metadata":{},"cell_type":"markdown","source":"I refered the kernel(https://www.kaggle.com/rohanrao/ashrae-half-and-half), Thank you Vopani!"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\"directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import gc\nimport os\nimport random\n\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\n\npath_data = \"/kaggle/input/ashrae-energy-prediction/\"\npath_train = path_data + \"train.csv\"\npath_test = path_data + \"test.csv\"\npath_building = path_data + \"building_metadata.csv\"\npath_weather_train = path_data + \"weather_train.csv\"\npath_weather_test = path_data + \"weather_test.csv\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(path_train)\n\nbuilding_df = pd.read_csv(path_building)\nle = LabelEncoder()\nbuilding_df.primary_use = le.fit_transform(building_df.primary_use)\n\nweather_train_df = pd.read_csv(path_weather_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#reduce the memory usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\n\ntrain_df = reduce_mem_usage(train_df)\nbuilding_df = reduce_mem_usage(building_df)\nweather_train_df = reduce_mem_usage(weather_train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preparing_data(X, bld_data, wth_data, test=False):\n    #merge three data\n    X = X.merge(bld_data, on = \"building_id\", how = \"left\")\n    X = X.merge(wth_data, on = [\"site_id\", \"timestamp\"], how = \"left\")\n    \n    #change the data type\n    X[\"timestamp\"] = pd.to_datetime(X[\"timestamp\"], format=\"%Y-%m-%d %H:%M:%S\")\n    #scale down the square feet\n    X[\"square_feet\"] = np.log1p(X[\"square_feet\"])\n    \n    if not test:\n        X.sort_values(\"timestamp\", inplace = True)\n        X.reset_index(drop=True, inplace = True)\n    \n    #separte timestamp into hour and weekday\n    X[\"hour\"] = X.timestamp.dt.hour\n    X[\"weekday\"] = X.timestamp.dt.weekday\n    \n    # add new feature \"isholiday\"\n    holidays = [\"2016-01-01\", \"2016-01-18\", \"2016-02-15\", \"2016-05-30\", \"2016-07-04\",\"2016-09-05\", \"2016-10-10\",\n                \"2016-11-11\", \"2016-11-24\", \"2016-12-26\", \"2017-01-01\", \"2017-01-16\", \"2017-02-20\", \"2017-05-29\",\n                \"2017-07-04\", \"2017-09-04\", \"2017-10-09\", \"2017-11-10\", \"2017-11-23\", \"2017-12-25\", \"2018-01-01\",\n                \"2018-01-15\", \"2018-02-19\", \"2018-05-28\", \"2018-07-04\", \"2018-09-03\", \"2018-10-08\", \"2018-11-12\",\n                \"2018-11-22\", \"2018-12-25\", \"2019-01-01\"]\n    X[\"isholiday\"] = (X.timestamp.dt.date.astype(\"str\").isin(holidays)).astype(int)\n    \n    X.drop(\"timestamp\", axis=1, inplace=True)\n    \n    if test:\n        row_ids = X.row_id\n        X.drop(\"row_id\", axis=1, inplace=True)\n        return X, row_ids\n    else:\n        y = np.log1p(X.meter_reading)\n        X.drop(\"meter_reading\", axis=1, inplace=True)\n        return X, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_full, y_full = preparing_data(train_df, building_df, weather_train_df)\n\ndel train_df, weather_train_df\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(X_full.shape)\n# X_full.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in X_full.columns:\n    print(i, X_full[i].dtype)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating model\n*Half and Half LGBM model*"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_features = [\"building_id\", \"site_id\", \"meter\", \"primary_use\", \"hour\", \"weekday\", \"isholiday\"]\n\nX_half1 = X_full.iloc[:int(len(X_full)/2)]\nX_half2 = X_full[int(len(X_full)/2):]\ny_half1 = y_full[:int(len(X_full)/2)]\ny_half2 = y_full[int(len(X_full)/2):]\n\nlgb_train1_data = lgb.Dataset(X_half1, label=y_half1, categorical_feature=categorical_features, free_raw_data=False)\nlgb_train2_data = lgb.Dataset(X_half2, label=y_half2, categorical_feature=categorical_features, free_raw_data=False)\n\nparameters = {\n    \"objective\": \"regression\",\n    \"boosting\": \"gbdt\",\n    \"num_leaves\": 100,\n    \"learning_rate\": 0.05,\n    \"feature_fraction\": 0.80,\n    \"reg_lambda\": 2,\n    \"metric\": \"rmse\",\n    \"n_jobs\": 4\n}\n\nprint(\"start building model1_1 : \")\nmodel1_1 = lgb.train(parameters, train_set=lgb_train1_data, num_boost_round=1000, valid_sets = [lgb_train1_data, lgb_train2_data],\n                  verbose_eval=200, early_stopping_rounds=200)\nprint(\"start building model1_2 : \")\nmodel1_2 = lgb.train(parameters, train_set=lgb_train2_data, num_boost_round=1000, valid_sets=[lgb_train2_data, lgb_train1_data],\n                    verbose_eval=200, early_stopping_rounds=200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del X_half1, X_half2, y_half1, y_half2, lgb_train1_data, lgb_train2_data, categorical_features\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1_feature_imp = pd.DataFrame()\ndf1_feature_imp[\"feature\"] = X_full.columns.values\ndf1_feature_imp[\"importance\"] = model1_1.feature_importance()\ndf2_feature_imp = pd.DataFrame()\ndf2_feature_imp[\"feature\"] = X_full.columns.values\ndf2_feature_imp[\"importance\"] = model1_2.feature_importance()\n\ndf_ft_imp = pd.concat([df1_feature_imp, df2_feature_imp], axis=0)\n\n\nplt.figure(figsize=(16, 10))\nsns.barplot(x = \"importance\", y=\"feature\", data = df_ft_imp.sort_values(by=\"importance\", ascending=False))\nplt.title(\"Light GBM Feature Importance\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# preparing test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_full = pd.read_csv(path_test)\nweather_test = pd.read_csv(path_weather_test)\n\ntest_full = reduce_mem_usage(test_full)\nweather_test = reduce_mem_usage(weather_test)\n\nX_test, row_ids = preparing_data(test_full, building_df, weather_test, test=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del test_full, building_df, weather_test\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# scoring test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"predict = np.expm1(model1_1.predict(X_test, num_iteration = model1_1.best_iteration)) / 2\n\ndel model1_1\ngc.collect()\n\npredict += np.expm1(model1_2.predict(X_test, num_iteration = model1_2.best_iteration)) / 2\ndel model1_2\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\"row_id\": row_ids, \"meter_reading\": np.clip(predict, 0, a_max=None)})\nsubmission.to_csv(\"submission.csv\", index=False)\n# del submission, predict\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}