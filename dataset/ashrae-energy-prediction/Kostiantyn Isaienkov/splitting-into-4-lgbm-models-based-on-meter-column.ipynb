{"cells":[{"metadata":{},"cell_type":"markdown","source":"**I decided to split training dataset on 'meter' column and train 4 different LGBM models and check the results.**\n\n**It is just example of the process. I think after optimization of hyperparameters it should provide good results.**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"building_df = pd.read_csv(\"../input/ashrae-energy-prediction/building_metadata.csv\")\nweather_train = pd.read_csv(\"../input/ashrae-energy-prediction/weather_train.csv\")\ntrain = pd.read_csv(\"../input/ashrae-energy-prediction/train.csv\")\n\ntrain = train.merge(building_df, left_on = \"building_id\", right_on = \"building_id\", how = \"left\")\ntrain = train.merge(weather_train, left_on = [\"site_id\", \"timestamp\"], right_on = [\"site_id\", \"timestamp\"])\ndel weather_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[(train['meter']==0) & (train['site_id']==0) & (train['timestamp']<'2016-05-21 00:00:00'), 'drop'] = True\ntrain = train[train['drop']!=True]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def average_imputation(df, column_name):\n    imputation = df.groupby(['timestamp'])[column_name].mean()\n    \n    df.loc[df[column_name].isnull(), column_name] = df[df[column_name].isnull()][[column_name]].apply(lambda x: imputation[df['timestamp'][x.index]].values)\n    del imputation\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = average_imputation(train, 'wind_speed')\n\nbeaufort = [(0, 0, 0.3), (1, 0.3, 1.6), (2, 1.6, 3.4), (3, 3.4, 5.5), (4, 5.5, 8), (5, 8, 10.8), (6, 10.8, 13.9), \n          (7, 13.9, 17.2), (8, 17.2, 20.8), (9, 20.8, 24.5), (10, 24.5, 28.5), (11, 28.5, 33), (12, 33, 200)]\n\nfor item in beaufort:\n    train.loc[(train['wind_speed']>=item[1]) & (train['wind_speed']<item[2]), 'beaufort_scale'] = item[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"timestamp\"] = pd.to_datetime(train[\"timestamp\"])\ntrain[\"weekday\"] = train[\"timestamp\"].dt.weekday\ntrain[\"hour\"] = train[\"timestamp\"].dt.hour\ntrain[\"weekday\"] = train['weekday'].astype(np.uint8)\ntrain[\"hour\"] = train['hour'].astype(np.uint8)\ntrain[\"month\"] = train[\"timestamp\"].dt.month\ntrain['year_built'] = train['year_built']-1900\ntrain['square_feet'] = np.log(train['square_feet'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['group'] = train['month']\ntrain['group'].replace((6, 7, 8), 21, inplace=True)\ntrain['group'].replace((9, 10, 11), 22, inplace=True)\ntrain['group'].replace((3, 4, 5), 23, inplace=True)\ntrain['group'].replace((1, 2, 12), 24, inplace=True)\ntrain['group'].replace((21), 1, inplace=True)\ntrain['group'].replace((22), 2, inplace=True)\ntrain['group'].replace((23), 3, inplace=True)\ntrain['group'].replace((24), 4, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ndel train[\"timestamp\"]\ndel train[\"drop\"]\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ntrain[\"primary_use\"] = le.fit_transform(train[\"primary_use\"])\n\ncategoricals = ['building_id',\"primary_use\", \"hour\", \"weekday\", \"meter\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_cols = ['site_id', \"sea_level_pressure\", \"wind_speed\", 'wind_direction', 'month', 'dew_temperature', \"group\"]\n\nnumericals = [\"square_feet\", \"year_built\", \"air_temperature\", \"cloud_coverage\", 'beaufort_scale', 'precip_depth_1_hr', \"floor_count\"]\n\nfeat_cols = categoricals + numericals","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meter_df = [train[train['meter']==i] for i in range(0,4)]\ndel train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"targets = [np.log1p(item[\"meter_reading\"]) for item in meter_df]\n\nfor item in meter_df:\n    del item[\"meter_reading\"]\n    for col in drop_cols:\n        del item[col]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\nparams = {\n            'boosting_type': 'gbdt',\n            'objective': 'regression',\n            'metric': {'rmse'},\n             \"num_leaves\": 1280,\n            \"learning_rate\": 0.05,\n            \"feature_fraction\": 0.85,\n            \"reg_lambda\": 2\n            }\n\nfolds = 3\nseed = 666\n\nkf = KFold(n_splits=folds, shuffle=False, random_state=seed)\nmodels0 = []\nfor train_index, val_index in kf.split(meter_df[0]):\n    train_X = meter_df[0][feat_cols].iloc[train_index]\n    val_X = meter_df[0][feat_cols].iloc[val_index]\n    train_y = targets[0].iloc[train_index]\n    val_y = targets[0].iloc[val_index]\n    lgb_train = lgb.Dataset(train_X, train_y, categorical_feature=categoricals)\n    lgb_eval = lgb.Dataset(val_X, val_y, categorical_feature=categoricals)\n    gbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=10000,\n                valid_sets=(lgb_train, lgb_eval),\n               early_stopping_rounds=100,\n               verbose_eval = 100)\n    models0.append(gbm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n            'boosting_type': 'gbdt',\n            'objective': 'regression',\n            'metric': {'rmse'},\n             \"num_leaves\": 1280,\n            \"learning_rate\": 0.05,\n            \"feature_fraction\": 0.85,\n            \"reg_lambda\": 2\n            }\n\nfolds = 3\nseed = 666\n\nkf = KFold(n_splits=folds, shuffle=False, random_state=seed)\nmodels1 = []\nfor train_index, val_index in kf.split(meter_df[1]):\n    train_X = meter_df[1][feat_cols].iloc[train_index]\n    val_X = meter_df[1][feat_cols].iloc[val_index]\n    train_y = targets[1].iloc[train_index]\n    val_y = targets[1].iloc[val_index]\n    lgb_train = lgb.Dataset(train_X, train_y, categorical_feature=categoricals)\n    lgb_eval = lgb.Dataset(val_X, val_y, categorical_feature=categoricals)\n    gbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=1000,\n                valid_sets=(lgb_train, lgb_eval),\n               early_stopping_rounds=100,\n               verbose_eval = 100)\n    models1.append(gbm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n            'boosting_type': 'gbdt',\n            'objective': 'regression',\n            'metric': {'rmse'},\n             \"num_leaves\": 1280,\n            \"learning_rate\": 0.05,\n            \"feature_fraction\": 0.85,\n            \"reg_lambda\": 2\n            }\n\nfolds = 3\nseed = 666\n\nkf = KFold(n_splits=folds, shuffle=False, random_state=seed)\nmodels2 = []\nfor train_index, val_index in kf.split(meter_df[2]):\n    train_X = meter_df[2][feat_cols].iloc[train_index]\n    val_X = meter_df[2][feat_cols].iloc[val_index]\n    train_y = targets[2].iloc[train_index]\n    val_y = targets[2].iloc[val_index]\n    lgb_train = lgb.Dataset(train_X, train_y, categorical_feature=categoricals)\n    lgb_eval = lgb.Dataset(val_X, val_y, categorical_feature=categoricals)\n    gbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=10000,\n                valid_sets=(lgb_train, lgb_eval),\n               early_stopping_rounds=100,\n               verbose_eval = 100)\n    models2.append(gbm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n            'boosting_type': 'gbdt',\n            'objective': 'regression',\n            'metric': {'rmse'},\n             \"num_leaves\": 1280,\n            \"learning_rate\": 0.05,\n            \"feature_fraction\": 0.85,\n            \"reg_lambda\": 2\n            }\n\nfolds = 3\nseed = 666\n\nkf = KFold(n_splits=folds, shuffle=False, random_state=seed)\nmodels3 = []\nfor train_index, val_index in kf.split(meter_df[3]):\n    train_X = meter_df[3][feat_cols].iloc[train_index]\n    val_X = meter_df[3][feat_cols].iloc[val_index]\n    train_y = targets[3].iloc[train_index]\n    val_y = targets[3].iloc[val_index]\n    lgb_train = lgb.Dataset(train_X, train_y, categorical_feature=categoricals)\n    lgb_eval = lgb.Dataset(val_X, val_y, categorical_feature=categoricals)\n    gbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=10000,\n                valid_sets=(lgb_train, lgb_eval),\n               early_stopping_rounds=100,\n               verbose_eval = 100)\n    models3.append(gbm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ndel meter_df, train_X, val_X, lgb_train, lgb_eval, train_y, val_y, targets\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#preparing test data\ntest = pd.read_csv(\"../input/ashrae-energy-prediction/test.csv\")\ntest = test.merge(building_df, left_on = \"building_id\", right_on = \"building_id\", how = \"left\")\ndel building_df\ngc.collect()\ntest[\"primary_use\"] = le.transform(test[\"primary_use\"])\n\nweather_test = pd.read_csv(\"../input/ashrae-energy-prediction/weather_test.csv\")\ntest = test.merge(weather_test, left_on = [\"site_id\", \"timestamp\"], right_on = [\"site_id\", \"timestamp\"], how = \"left\")\ndel weather_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = average_imputation(test, 'wind_speed')\n\nfor item in beaufort:\n    test.loc[(test['wind_speed']>=item[1]) & (test['wind_speed']<item[2]), 'beaufort_scale'] = item[0]\n    \ntest['beaufort_scale'] = test['beaufort_scale'].astype(np.uint8)\n\ntest[\"timestamp\"] = pd.to_datetime(test[\"timestamp\"])\ntest[\"hour\"] = test[\"timestamp\"].dt.hour\ntest[\"weekday\"] = test[\"timestamp\"].dt.weekday\ntest[\"weekday\"] = test['weekday'].astype(np.uint8)\ntest[\"hour\"] = test['hour'].astype(np.uint8)\ntest[\"month\"] = test[\"timestamp\"].dt.month\ntest['year_built'] = test['year_built']-1900\ntest['square_feet'] = np.log(test['square_feet'])\n\ntest['group'] = test['month']\ntest['group'].replace((6, 7, 8), 21, inplace=True)\ntest['group'].replace((9, 10, 11), 22, inplace=True)\ntest['group'].replace((3, 4, 5), 23, inplace=True)\ntest['group'].replace((1, 2, 12), 24, inplace=True)\ntest['group'].replace((21), 1, inplace=True)\ntest['group'].replace((22), 2, inplace=True)\ntest['group'].replace((23), 3, inplace=True)\ntest['group'].replace((24), 4, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = test.drop([\"sea_level_pressure\", \"wind_direction\", \"timestamp\", 'site_id', \"wind_speed\", 'month', 'dew_temperature', \"group\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = [test[test['meter']==i] for i in range(0,4)]\ndel test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"row_ids = [list(item['row_id']) for item in test_df]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_list = ['row_id', 'meter']\n\nfor item in test_df:\n    for col in drop_list:\n        del item[col]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\ni=0\nres0=[]\nstep_size = 50000\nfor j in tqdm(range(int(np.ceil(test_df[0].shape[0]/50000)))):\n    res0.append(sum(np.expm1([model.predict(test_df[0].iloc[i:i+step_size]) for model in models0])/folds))\n    i+=step_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i=0\nres1=[]\nstep_size = 50000\nfor j in tqdm(range(int(np.ceil(test_df[1].shape[0]/50000)))):\n    res1.append(sum(np.expm1([model.predict(test_df[1].iloc[i:i+step_size]) for model in models1])/folds))\n    i+=step_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i=0\nres2=[]\nstep_size = 50000\nfor j in tqdm(range(int(np.ceil(test_df[2].shape[0]/50000)))):\n    res2.append(sum(np.expm1([model.predict(test_df[2].iloc[i:i+step_size]) for model in models2])/folds))\n    i+=step_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i=0\nres3=[]\nstep_size = 50000\nfor j in tqdm(range(int(np.ceil(test_df[3].shape[0]/50000)))):\n    res3.append(sum(np.expm1([model.predict(test_df[3].iloc[i:i+step_size]) for model in models3])/folds))\n    i+=step_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res0 = np.concatenate(res0)\nres1 = np.concatenate(res1)\nres2 = np.concatenate(res2)\nres3 = np.concatenate(res3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del test_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res0 = pd.DataFrame(data=res0,columns=['meter_reading'])\nres1 = pd.DataFrame(data=res1,columns=['meter_reading'])\nres2 = pd.DataFrame(data=res2,columns=['meter_reading'])\nres3 = pd.DataFrame(data=res3,columns=['meter_reading'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res0['row_id'] = row_ids[0]\nres1['row_id'] = row_ids[1]\nres2['row_id'] = row_ids[2]\nres3['row_id'] = row_ids[3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del row_ids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = pd.concat([res0, res1, res2, res3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del res0, res1, res2, res3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/ashrae-energy-prediction/sample_submission.csv')\nsubmission = submission.merge(res, left_on='row_id', right_on='row_id', how='inner')\nsubmission = submission[['row_id', 'meter_reading_y']]\nsubmission.columns = ['row_id', 'meter_reading']\nsubmission.loc[submission['meter_reading']<0, 'meter_reading'] = 0\nsubmission.to_csv('submission.csv', index=False)\nsubmission","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}