{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Quick EDA and visualization of Deepfake Detection Challenge\n\nLets take a quick look at sample training set."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nimport keras\nimport glob\nimport cv2\nfrom albumentations import *\nfrom tqdm import tqdm_notebook as tqdm\nimport gc\n\nimport warnings\nwarnings.filterwarnings('ignore')\nPATH = '../input/deepfake-detection-challenge/'\nprint(os.listdir(PATH))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_PATH = 'train_sample_videos/*.mp4'\nTEST_PATH = 'test_videos/*.mp4'\ntrain_img = glob.glob(os.path.join(PATH, TRAIN_PATH))\ntest_img = glob.glob(os.path.join(PATH, TEST_PATH))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gather_info(train_img):\n    train_img = os.path.join(PATH, f\"train_sample_videos/{train_img}\")\n\n    cap = cv2.VideoCapture(train_img)\n\n    success, image = cap.read()\n    count = 0\n    first_trn_image = None\n\n    while success:\n        try:\n            success, image = cap.read()\n            if count == 0:\n                first_trn_image = image\n            x, y, z = image.shape\n\n        except:\n            break\n        count += 1\n        \n    cap.release()\n    cv2.destroyAllWindows()\n    \n    return [x,y,z,count], first_trn_image","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load Metadata"},{"metadata":{"trusted":true},"cell_type":"code","source":"meta = glob.glob(os.path.join(PATH, 'train_sample_videos/*.json'))\nlabel_df = pd.read_json(meta[0]).transpose().reset_index()\nidx = label_df['index']\nimage_meta = []\nfirst_trn_images = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sample Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"# gather training image dimensions\nfor i in tqdm(range(len(idx))):\n    test, first = gather_info(idx.values[i])\n    image_meta.append(test)\n    first_trn_images.append(first)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Top 100 Sample Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display the first image(frame) of each video(.mp4) in sample train\nfor img, lbl in zip(first_trn_images[:100], label_df['label'].values[:100]):\n    # cv2's native channel is BGR, we need to convert to RGB\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.title(f'{lbl}')\n    plt.imshow(img)\n    plt.show()\n    \ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_meta = np.array(image_meta, dtype='int16')\nlabel_df['height'] = image_meta[:,0]\nlabel_df['width'] = image_meta[:,1]\nlabel_df['channel'] = image_meta[:,2]\nlabel_df['frames'] = image_meta[:,3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_df['height'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_df['width'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_df['channel'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_df['frames'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sample Train Target Distribution\nplt.title('Count of each ship type')\nsns.countplot(y=label_df['label'].values)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Public Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"def gather_test(test_img, i):\n    cap = cv2.VideoCapture(test_img[i])\n\n    success, image = cap.read()\n    count = 0\n    first_tst_image = None\n\n    while success:\n        try:\n            success, image = cap.read()\n            if count == 0:\n                first_tst_image = image\n            x, y, z = image.shape\n\n        except:\n            break\n        count += 1\n        \n    cap.release()\n    cv2.destroyAllWindows()\n    \n    return [x,y,z,count], first_tst_image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_image_meta = []\nfirst_tst_images = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# gather test image dimensions\nfor i in tqdm(range(len(test_img))):\n    test, first = gather_test(test_img, i)\n    test_image_meta.append(test)\n    first_tst_images.append(first)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Top 100 Public Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display the first image(frame) of each video(.mp4) in test\nfor img in first_tst_images[:100]:\n    # cv2's native channel is BGR, we need to convert to RGB\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.imshow(img)\n    plt.show()\n    \ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_image_meta = np.array(test_image_meta)\ntest_meta = pd.DataFrame({'height':test_image_meta[:,0],\n                          'width':test_image_meta[:,1],\n                          'channel':test_image_meta[:,2],\n                          'frames':test_image_meta[:,3]})\ntest_meta.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_meta['height'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_meta['width'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_meta['channel'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_meta['frames'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Summary\n\n* There seems to be two resolutions in both sample train and public test: 1080x1920 and 1920x1080. \n* The number of frames per video is 297/299 and 298/299 for train, test respectively, with 299 being the most frequent.\n\n* Right now it's not clear whether private test will be the same distribution.\n\n* What is clear is that this challenge will require a large amount of resources, judging by the size of the frame resolutions."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}