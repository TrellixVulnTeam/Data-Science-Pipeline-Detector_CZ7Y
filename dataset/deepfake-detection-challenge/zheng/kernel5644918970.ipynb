{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir('./')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import cv2\nimport numpy as np\nfrom typing import *\nimport random\nimport json\nimport os\nimport pickle\nfrom matplotlib import pyplot as plt\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\ndef ImageResize(img_array: 'np.ndarray', resize: int) -> np.ndarray:\n    \"\"\"\n    :param img_array: 输入的图片，格式为ndarray\n    :param resize: 缩放大小\n    :return:\n    \"\"\"\n    return cv2.resize(img_array, (resize, resize))\n\n\ndef fft(img):\n    \"\"\"\n    img 是图片矩阵\n    \"\"\"\n    img = ImageResize(img, 1024)  # resize成400\n\n    # 傅里叶变换\n    dft = cv2.dft(np.float32(img), flags=cv2.DFT_COMPLEX_OUTPUT)\n\n    # 将频谱低频从左上角移动至中心位置\n    dft_shift = np.fft.fftshift(dft)\n\n    # 频谱图像双通道复数转换为0-255区间\n    result = 20 * np.log(cv2.magnitude(dft_shift[:, :, 0], dft_shift[:, :, 1])+1e-5) # 加上1e-5防止除0\n    return result\n\n\ndef azimuthalAverage(image, center=None):\n    \"\"\"\n    Calculate the azimuthally averaged radial profile.\n\n    image - The 2D image\n    center - The [x,y] pixel coordinates used as the center. The default is\n             None, which then uses the center of the image (including\n             fracitonal pixels).\n\n    \"\"\"\n    # Calculate the indices from the image\n    y, x = np.indices(image.shape)\n\n    if not center:\n        center = np.array([(x.max() - x.min()) / 2.0, (y.max() - y.min()) / 2.0])\n\n    r = np.hypot(x - center[0], y - center[1])\n\n    # Get sorted radii\n    ind = np.argsort(r.flat)\n    r_sorted = r.flat[ind]\n    i_sorted = image.flat[ind]\n\n    # Get the integer part of the radii (bin size = 1)\n    r_int = r_sorted.astype(int)\n\n    # Find all pixels that fall within each radial bin.\n    deltar = r_int[1:] - r_int[:-1]  # Assumes all radii represented\n    rind = np.where(deltar)[0]  # location of changed radius\n    nr = rind[1:] - rind[:-1]  # number of radius bin\n\n    # Cumulative sum to figure out sums for each radius bin\n    csim = np.cumsum(i_sorted, dtype=float)\n    tbin = csim[rind[1:]] - csim[rind[:-1]]\n\n    radial_prof = tbin / nr\n\n    return radial_prof\n\n\ndef metadataReader(label_path) -> List:\n    \"\"\"\n    数据预处理\n    将meatadata里的数据转化成一个列表\n    列表里面每个元素都是一个字典，形如{'video_file':'xxxx', 'label':0/1}\n    label = 0 -> FAKE\n    label = 1 -> REAL\n    :param label_path: metadata 文件路径\n    :return: record\n    \"\"\"\n    record = []\n\n    with open(label_path) as f:\n        file = f.read()\n        jsonfile = json.loads(file)\n        for key, val in zip(jsonfile.keys(), jsonfile.values()):\n            rec = {'video_file': key,\n                   'label': 1 if val['label'] == \"REAL\" else 0}\n            record.append(rec)\n\n    return record\n\n\ndef CaptureVideoImage(\n        videoFile: str,\n        totalFrame=20,\n) -> np.ndarray:\n    \"\"\"\n    读取videoFile，生成一个[totalFrame, resolution, resolution, channel]形式的数组\n    :param videoFile: 视频文件\n    :param totalFrame: 截取总共帧数，作为一个batch\n    :return: [totalFrame, resolution, resolution, channel] ndarray\n\n    \"\"\"\n    result_list = []\n    # cnt = 0  # 数组计数器\n\n    vidcap = cv2.VideoCapture(videoFile)  # 视频流截图\n    frame_all = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))  # 总帧数\n    frame_start = random.randint(0, frame_all // 2)  # 起始帧\n\n    frame_interval = 5\n    \n    if vidcap.isOpened():\n        for i in range(frame_start, frame_start + totalFrame * frame_interval, frame_interval):\n            vidcap.set(cv2.CAP_PROP_POS_FRAMES, i)  # set方法获取指定帧\n            success, img = vidcap.read()\n            if success:\n                img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n                # print(img.shape)\n                fft_img = fft(img)\n                spectral = azimuthalAverage(fft_img)\n\n                result_list.append(spectral)\n\n    result_array = np.array(result_list)\n    print(result_array.shape)\n    result_array = np.mean(result_array, axis=0)\n\n    return result_array\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pkl_file = open('/kaggle/input/trainframe/train_reproduce.pkl', 'rb')\ndata = pickle.load(pkl_file)\npkl_file.close()\nX_train = data[\"data\"]\nY_train = data[\"label\"]\nfor data in X_train:\n    plt.plot(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_trainsplit, X_train_testsplit, Y_train_split, Y_test_split = train_test_split(X_train, Y_train, test_size=.3)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svclassifier_r = SVC(C=120, kernel='rbf', gamma=0.001, probability=True)\n\nsvclassifier_r.fit(X_train_trainsplit, Y_train_split)\n\nsvclassifier_r.predict_proba(X_train_testsplit)\n\nSVM = svclassifier_r.score(X_train_testsplit, Y_test_split)\n\nprint(\"RBF SVM SCORE IS \", SVM)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg = LogisticRegression(solver='liblinear', max_iter=1000)\nlogreg.fit(X_train_trainsplit, Y_train_split)\nlogreg.predict_proba(X_train_testsplit)\n\nlogregScore = logreg.score(X_train_testsplit, Y_test_split)\nprint(\"Logistic Score is \", logregScore)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# label_path = \"/kaggle/input/deepfake-detection-challenge/train_sample_videos/metadata.json\"\n# video_dir = \"/kaggle/input/deepfake-detection-challenge/train_sample_videos\"\n# records = metadataReader(label_path)\n\n# pickle_dict = {'data':[], 'label':[]}\n# cnt = 0\n# for i in range(2):\n#     for record in records:\n#         video_file_name = os.path.join(video_dir + '/', record['video_file'])\n#         video_file_label = record['label']\n#         if video_file_label == 0:\n#             randnum = random.randint(1, 100) / 100\n#             if randnum > 0.25:\n#                 continue\n#         video_frame_data = CaptureVideoImage(video_file_name)\n#         pickle_dict['data'].append(video_frame_data)\n#         pickle_dict['label'].append(video_file_label)\n#         cnt += 1\n#         print(cnt)\n\n# output = open('/kaggle/output/kaggle/working/train.pkl', 'wb')\n# pickle.dump(pickle_dict, output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pkl_test_file = open('/kaggle/input/test-data/test.pkl', 'rb')\ntest_data = pickle.load(pkl_test_file)\npkl_test_file.close()\nX_test = test_data[\"data\"]\nfilename = test_data[\"file_name\"]\n\nfor data in X_test:\n    plt.plot(data)\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict = svclassifier_r.predict_proba(X_test)\npredict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_val_index = np.argmax(predict, -1)\nresList = []\nfor i in range(len(predict)):\n    resList.append(predict[i][max_val_index[i]])\n\nfilename = np.array(filename)\ndata = {'filename':filename, 'label':resList}\ndf = pd.DataFrame(data, columns=['filename', 'label'])\ndf.sort_values(by='filename', inplace=True)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# simple_predict = []\n# for data in X_test:\n#     if data[500] > 150:\n#         simple_predict.append(0.85)\n#     else:\n#         simple_predict.append(0.1)\n        \n# data = {'filename':filename, 'label':simple_predict}\n# df = pd.DataFrame(data, columns=['filename', 'label'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sample_submit = pd.read_csv('/kaggle/input/deepfake-detection-challenge/sample_submission.csv')\n# filename = list(filename)\n# final_predict = []\n# for submit_filename in sample_submit['filename']:\n#     index = filename.index(submit_filename)\n#     final_predict.append(simple_predict[index])\n    \n# finaldata = {'filename':sample_submit['filename'], 'label':final_predict}\n# finaldf = pd.DataFrame(finaldata, columns=['filename', 'label'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# finaldf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# try:\n\n#     finaldf.to_csv('submission.csv', index=False)\n# except:\n\n#     finaldf.to_csv('submission.csv', index=False)\n\n#     print(\"Exception writing submission.csv, wrote it again...\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submission = pd.read_csv('/kaggle/input/deepfake-detection-challenge/sample_submission.csv')\n# submission['label'] = 0.5\n# submission.set_index('filename')\n# # for path in paths:\n# #     try:\n# #         # load video and predict\n# #         submission.loc[path] = prediction\n# #     except:\n# #         pass\n# submission.to_csv('submission.csv', index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}