{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install face_recognition","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SKLearn Implemention\nfrom sklearn.metrics import log_loss\nlog_loss([\"REAL\", \"FAKE\", \"FAKE\", \"REAL\"],\n         [[.1, .9], [.9, .1], [.8, .2], [.35, .65]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pylab as plt\nimport cv2\nplt.style.use('ggplot')\nfrom IPython.display import Video\nfrom IPython.display import HTML\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sample_metadata = pd.read_json('/kaggle/input/deepfake-detection-challenge/train_sample_videos/metadata.json').T\ntrain_sample_metadata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sample_metadata.groupby('label')['label'].count().plot(figsize=(15, 5), kind='bar', title='Distribution of Labels in the Training Set')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2 as cv\nimport os\nimport matplotlib.pylab as plt\ntrain_dir = '/kaggle/input/deepfake-detection-challenge/train_sample_videos/'\nfig, ax = plt.subplots(1,1, figsize=(15, 15))\ntrain_video_files = [train_dir + x for x in os.listdir(train_dir)]\n# video_file = train_video_files[30]\nvideo_file = '/kaggle/input/deepfake-detection-challenge/train_sample_videos/akxoopqjqz.mp4'\ncap = cv.VideoCapture(video_file)\nsuccess, image = cap.read()\nimage = cv.cvtColor(image, cv.COLOR_BGR2RGB)\ncap.release()   \nax.imshow(image)\nax.xaxis.set_visible(False)\nax.yaxis.set_visible(False)\nax.title.set_text(f\"FRAME 0: {video_file.split('/')[-1]}\")\nplt.grid(False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import face_recognition\nface_locations = face_recognition.face_locations(image)\n\n# https://github.com/ageitgey/face_recognition/blob/master/examples/find_faces_in_picture.py\nfrom PIL import Image\n\nprint(\"I found {} face(s) in this photograph.\".format(len(face_locations)))\n\nfor face_location in face_locations:\n\n    # Print the location of each face in this image\n    top, right, bottom, left = face_location\n    print(\"A face is located at pixel location Top: {}, Left: {}, Bottom: {}, Right: {}\".format(top, left, bottom, right))\n\n    # You can access the actual face itself like this:\n    face_image = image[top:bottom, left:right]\n    fig, ax = plt.subplots(1,1, figsize=(5, 5))\n    plt.grid(False)\n    ax.xaxis.set_visible(False)\n    ax.yaxis.set_visible(False)\n    ax.imshow(face_image)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"face_landmarks_list = face_recognition.face_landmarks(image)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://github.com/ageitgey/face_recognition/blob/master/examples/find_facial_features_in_picture.py\n# face_landmarks_list\nfrom PIL import Image, ImageDraw\npil_image = Image.fromarray(image)\nd = ImageDraw.Draw(pil_image)\n\nfor face_landmarks in face_landmarks_list:\n\n    # Print the location of each facial feature in this image\n    for facial_feature in face_landmarks.keys():\n        print(\"The {} in this face has the following points: {}\".format(facial_feature, face_landmarks[facial_feature]))\n\n    # Let's trace out each facial feature in the image with a line!\n    for facial_feature in face_landmarks.keys():\n        d.line(face_landmarks[facial_feature], width=3)\n\n# Show the picture\ndisplay(pil_image)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.makedirs('/kaggle/frames')\n\nos.chdir('/kaggle/frames/')\n\nos.makedirs('/kaggle/frames/real')\n\nos.makedirs('/kaggle/frames/fake')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Change the data set here\nfor fn,row in train_sample_metadata.iterrows():\n    video_file = f'/kaggle/input/deepfake-detection-challenge/train_sample_videos/{fn}'\n    cap = cv2.VideoCapture(video_file)\n\n    frames = []\n    while(cap.isOpened()):\n        ret, frame = cap.read()\n        if ret==True:\n            frames.append(frame)\n            if cv2.waitKey(1) & 0xFF == ord('q'):\n                break\n        else:\n            break\n    cap.release()\n    \n    padding = 40\n    for i in [0, 25, 50, 75, 100, 125, 150, 175, 250, 275]:\n        try:\n            frame = frames[i]\n        except:\n            continue\n        #fig, ax = plt.subplots(1,1, figsize=(5, 5))\n        face_locations = face_recognition.face_locations(frame)\n        if len(face_locations) == 0:\n            print(f'Count find face in frame {i}')\n            continue\n        top, right, bottom, left = face_locations[0]\n        frame_face = frame[top-padding:bottom+padding, left-padding:right+padding]\n        face_landmarks_list = face_recognition.face_landmarks(frame_face)\n        if len(face_landmarks_list) == 0:\n            print(f'Could not identify face landmarks for frame {i}')\n            continue\n        face_landmarks = face_landmarks_list[0]\n        pil_image = Image.fromarray(frame_face)\n        d = ImageDraw.Draw(pil_image)\n        landmark_face_array = np.array(pil_image)\n#         image = cv.cvtColor(landmark_face_array, cv.COLOR_BGR2RGB)\n        if row['label'] == \"FAKE\":\n            os.chdir('/kaggle/frames/fake')\n        else:\n            os.chdir('/kaggle/frames/real')\n        cv.imwrite(fn+str(i)+\".jpg\", landmark_face_array)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir('/kaggle/frames/real/'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(os.listdir('/kaggle/frames/fake/')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image.open('/kaggle/frames/real/egghxjjmfg.mp4150.jpg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}