{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ../input/facenet-pytorch-vggface2/facenet_pytorch-2.0.1-py3-none-any.whl\n!mkdir -p /root/.cache/torch/checkpoints/\n!cp ../input/facenet-pytorch-vggface2/20180402-114759-vggface2-logits.pth /root/.cache/torch/checkpoints/vggface2_DG3kwML46X.pt\n!cp ../input/facenet-pytorch-vggface2/20180402-114759-vggface2-features.pth /root/.cache/torch/checkpoints/vggface2_G5aNV2VSMn.pt","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport gc\nfrom glob import glob\nimport json\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\nimport cv2\nfrom albumentations import Compose, Normalize\nfrom PIL import Image, ImageDraw\nfrom tqdm.notebook import tqdm\nfrom collections import defaultdict, deque\nimport sys\nsys.path.append('../input/pretrainedmodels/pretrainedmodels-0.7.4')\n\nimport torch\nfrom torch.nn import Module\n\nfrom torch import nn\nfrom torchvision.models import resnext50_32x4d as resnext50\nfrom torch import optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data import DataLoader, Dataset, Subset\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nfrom facenet_pytorch import MTCNN, InceptionResnetV1\nfrom pretrainedmodels import xception","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_path = '../input/deepfake-detection-challenge/sample_submission.csv'\ntrain_video_path = '../input/deepfake-detection-challenge/train_sample_videos'\ntest_video_path = '../input/deepfake-detection-challenge/test_videos'\n\nnum_frame = 5\ndim_trans = 5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train video"},{"metadata":{"trusted":true},"cell_type":"code","source":"list_train = glob(os.path.join(train_video_path, '*.mp4'))\nprint(f'Sum video in train: {len(list_train)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test video"},{"metadata":{"trusted":true},"cell_type":"code","source":"list_test = glob(os.path.join(test_video_path, '*.mp4'))\nprint(f'Sum video in test: {len(list_test)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train json"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_json = glob(os.path.join(train_video_path, '*.json'))\nwith open(train_json[0], 'rt') as file:\n    train = json.load(file)\n    \ntrain_df = pd.DataFrame()\ntrain_df['file'] = train.keys()\n\nlabel = [i['label'] for i in train.values() if isinstance(i, dict)]\ntrain_df['label'] = label\n\nsplit = [i['split'] for i in train.values() if isinstance(i, dict)]\ntrain_df['split'] = split\n\noriginal = [i['original'] for i in train.values() if isinstance(i, dict)]\ntrain_df['original'] = original\n\ntrain_df['original'] = train_df['original'].fillna(train_df['file'])\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"real = train_df[train_df['label']=='REAL']\nreal.reset_index(inplace=True, drop=True)\nfake = train_df[train_df['label']=='FAKE']\nfake.reset_index(inplace=True, drop=True)\n\nplt.figure(figsize=(15,8))\nax = sns.countplot(y=label, data=train_df)\n\nfor p in ax.patches:\n    ax.annotate('{:.2f}%'.format(100*p.get_width()/train_df.shape[0]), (p.get_x() + p.get_width() + 0.02, p.get_y() + p.get_height()/2))\n    \nplt.title('Distribution of label', size=25, color='b')    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"original_same = train_df.pivot_table(values=['file'], columns=['label'], index=['original'], fill_value=0, aggfunc='count')\noriginal_same = original_same[(original_same[('file', 'FAKE')] != 0) & (original_same[('file', 'REAL')] != 0)]\n\nprint(f'Number of file having both FAKE and REAL: {len(original_same)}')\noriginal_same","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['label'] = train_df['label'].apply(lambda x: 1 if x=='FAKE' else 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission file"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(submission_path)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submission = submission.iloc[:10, :]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## MTCNN with FaceNet"},{"metadata":{"trusted":true},"cell_type":"code","source":"def box_mtcnn(frame, landmarks=True):\n    mtcnn = MTCNN(keep_all=True, device=device)   \n    if landmarks:\n        boxes, scores, landmarks = mtcnn.detect(frame, landmarks=landmarks)\n        return boxes, scores, landmarks\n    else:\n        boxes, scores = mtcnn.detect(frame, landmarks=landmarks)\n        return boxes, scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Optical Flow"},{"metadata":{"trusted":true},"cell_type":"code","source":"def op_display(df, number_frame=5, number_video=3):\n    \n    for index in range(number_video):\n        \n        index_random = random.randint(0, len(df))\n        video = df.loc[index_random, 'file']\n        \n        if video in os.listdir(train_video_path):\n            video_path = os.path.join(train_video_path, video)\n            cap = cv2.VideoCapture(video_path)\n            \n            fig, axes = plt.subplots(number_frame, 2, figsize=(20, 20))\n            \n            frame_index = 0\n            ax_ix = 0\n            previous_crop = ''\n            \n            while True:                    \n                ret, frame = cap.read()\n                \n                if cv2.waitKey(1) & 0xFF == 27:\n                    break\n                \n                if ret:                    \n                    \n                    if frame_index%24==0:\n                        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                        image = Image.fromarray(image)\n                        boxes, scores = box_mtcnn(image, False)\n                        try:\n                            if scores[0]:\n                                if boxes is not None:\n                                    box = boxes[scores.argmax()]\n                                    frame_crop = image.crop(box)\n                                    frame_crop = frame_crop.resize((64,64), Image.ANTIALIAS)\n                                    frame_crop = np.array(frame_crop)\n                                    frame_crop_gray = cv2.cvtColor(frame_crop, cv2.COLOR_BGR2GRAY)\n\n                                    if len(previous_crop) != 0: \n                                        flow = cv2.calcOpticalFlowFarneback(previous_crop, frame_crop_gray,\n                                                                            None, 0.5, 5, 11, 5, 5, 1.1, 0)\n                                        axes[ax_ix, 1].imshow(previous_crop)\n                                        axes[ax_ix, 0].imshow(frame_crop)\n                                        axes[ax_ix, 0].xaxis.set_visible(False)\n                                        axes[ax_ix, 0].yaxis.set_visible(False)\n                                        axes[ax_ix, 0].set_title(f'Frame: {frame_index}')\n                                        ax_ix += 1\n\n                                        fig.tight_layout()\n\n                                        fig.suptitle(video, color='b', size=20, y=1)\n\n                                    previous_crop = frame_crop_gray\n\n                                    if ax_ix == number_frame:\n                                        break\n                        except:\n                            continue\n                else:\n                    break\n                    \n                \n                frame_index += 1          \n        \nop_display(fake)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Display some video"},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_video(df, number_frame=5, number_video=3):\n    \n    color = ['b', 'g', 'r']\n    for index in range(number_video):\n        \n        index_random = random.randint(0, len(df))\n        video = df.loc[index_random, 'file']\n        \n        if video in os.listdir(train_video_path):\n            video_path = os.path.join(train_video_path, video)\n            cap = cv2.VideoCapture(video_path)\n            \n            fig, axes = plt.subplots(number_frame, 2, figsize=(20, 20))\n            \n            frame_index = 0\n            ax_ix = 0\n            while True:\n                    \n                ret, frame = cap.read()\n                \n                if cv2.waitKey(1) & 0xFF == 27:\n                    break\n                \n                if ret:                    \n                    \n                    if frame_index%24==0:\n                        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                        image = Image.fromarray(image)\n                        boxes, scores = box_mtcnn(image, False)\n                        if scores[0]:\n                            if boxes is not None:\n                                box = boxes[scores.argmax()]\n                                frame_crop = image.crop(box)\n                                frame_crop = np.array(frame_crop)\n\n                                for i in range(3):\n                                    hist = cv2.calcHist([frame_crop], [i], None, [256], [0, 256])\n                                    axes[ax_ix, 1].plot(hist, color=color[i])\n\n\n                                axes[ax_ix, 0].imshow(frame_crop)\n                                axes[ax_ix, 0].xaxis.set_visible(False)\n                                axes[ax_ix, 0].yaxis.set_visible(False)\n                                axes[ax_ix, 0].set_title(f'Frame: {frame_index}')\n                                ax_ix += 1\n\n                                fig.tight_layout()\n\n                                fig.suptitle(video, color='b', size=20, y=1)\n\n                                if ax_ix == number_frame:\n                                    break\n                                                                \n                else:\n                    break\n                    \n                \n                frame_index += 1          \n        \ndisplay_video(fake)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Display image with MTCNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_mtcnn(number_frame=3, number_video=2):\n    \n    fake_real = original_same[(original_same[('file', 'FAKE')] == 1) & (original_same[('file', 'REAL')] == 1)].index.tolist()                \n    original_images = random.sample(fake_real, number_video)\n    \n    for original_image in original_images:\n        real_video = train_df[(train_df['label']==0) & (train_df['original']==original_image)]['file'].values[0]\n        fake_video = train_df[(train_df['label']==1) & (train_df['original']==original_image)]['file'].values[0]\n\n        if (real_video in os.listdir(train_video_path)) and (fake_video in os.listdir(train_video_path)):\n            real_path = os.path.join(train_video_path, real_video)\n            fake_path = os.path.join(train_video_path, fake_video)\n\n\n\n            fig, axes = plt.subplots(number_frame, 2, figsize=(40, 30))\n\n            for ind, path in enumerate([real_path, fake_path]):\n\n                cap = cv2.VideoCapture(path)\n                frame_index = 0\n                ax_ix = 0\n                \n                while frame_index < (10*number_frame - 9):\n                    ret, frame = cap.read()\n\n                    if cv2.waitKey(1) & 0xFF == 27:\n                        break\n\n                    if ret:                    \n\n                        if frame_index in 10*np.arange(0, number_frame):\n                            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                            frame = Image.fromarray(frame)\n                            boxes, scores, landmarks = box_mtcnn(frame)\n                            draw = ImageDraw.Draw(frame)\n                            for box, score, landmark in zip(boxes, scores, landmarks):\n                                if score > 0.91:\n                                    draw.rectangle(box.tolist(), outline=(0, 255, 0), width=6)\n                                    axes[ax_ix, ind].scatter(landmark[:, 0], landmark[:, 1], c='red', s=8)\n                                                            \n                            axes[ax_ix, ind].imshow(frame)\n                            axes[ax_ix, ind].xaxis.set_visible(False)\n                            axes[ax_ix, ind].yaxis.set_visible(False)\n                            axes[ax_ix, ind].set_title(f'Frame {frame_index}')\n                            \n                            fig.tight_layout()\n                            ax_ix += 1\n\n                    else:\n                        break\n                    \n                    frame_index+=1\n                    \n            fig.suptitle(original_image, color='b', size=20, y=1)\n\n\ndisplay_mtcnn(number_frame=3, number_video=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creat dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"class VideoDataset(Dataset):\n    \n    def __init__(self, df, path_video, num_frame=20, is_train=True, transforms=None):\n        super(VideoDataset, self).__init__()\n        \n        self.df = df\n        self.num_frame = num_frame\n        self.is_train = is_train\n        self.path_video = path_video\n        self.transforms = transforms\n        \n        index_list = deque()\n        for index in tqdm(range(len(self.df))):\n            \n            video_name = self.df.loc[index, 'filename']\n            video_path = os.path.join(self.path_video, video_name)\n            \n            try:\n                if self.frame_crop(video_path) is not None:\n                    index_list.append(index)\n            except:\n                continue\n                \n        index_list = list(index_list)\n        self.df = self.df[self.df.index.isin(index_list)]\n        self.df.reset_index(inplace=True, drop=True)\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        \n        video_name = self.df.loc[idx, 'filename']\n        video_path = os.path.join(self.path_video, video_name)\n        list_frame = self.frame_crop(video_path)         \n        \n        if self.is_train:\n            label = self.df.loc[idx, 'label']\n            return torch.from_numpy(list_frame), torch.tensor(label, dtype=torch.float)\n        else:\n            return video_name, torch.from_numpy(list_frame)\n        \n        \n    def frame_crop(self, video_path):\n\n        cap = cv2.VideoCapture(video_path)\n        frame_index = 0\n        list_frame = []\n        \n        while True:\n            ret, frame = cap.read()\n            if cv2.waitKey(1) & 0xFF == 27:\n                break\n            if ret:                    \n                if frame_index % 12 == 0:\n                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                    \n                    frame = Image.fromarray(frame)\n                    boxes, scores = box_mtcnn(frame, False)\n                    \n                    try:\n                        if scores[0]:\n                            if boxes is not None:                                \n                                index_max = np.argmax(scores)\n                                box = boxes[index_max]\n                                frame_crop = frame.crop(box)\n                                frame_crop = frame_crop.resize((150,150), Image.ANTIALIAS)\n                                frame_crop = np.array(frame_crop)     \n                                if self.transforms is not None:\n                                    frame_crop = self.transforms(image=frame_crop)['image']\n                                                                \n                                list_frame.append(frame_crop)                                \n                                if len(list_frame) == self.num_frame: \n                                    return np.array(list_frame)\n                    except:\n                        continue\n            else:\n                return None\n\n            frame_index+=1    \n        return None\n    \nnormalize = Compose([\n    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], p=1)\n])\n    \ntest_dataset = VideoDataset(submission, test_video_path, num_frame=num_frame, is_train=False, transforms=normalize)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_ld = DataLoader(test_dataset, batch_size=1, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Module"},{"metadata":{"trusted":true},"cell_type":"code","source":"class EncoderCNN(Module):\n\n  def __init__(self, encoder_output_dim):\n    super(EncoderCNN, self).__init__()\n\n    # self.conv1 = EfficientNet.from_pretrained('efficientnet-b1')    \n    self.conv1 = xception(pretrained=None, num_classes=1000)\n    \n    for param in self.conv1.parameters():\n        param.requires_grad = False\n\n    self.conv2 = nn.Sequential(nn.Linear(1000, encoder_output_dim),\n                               nn.ReLU(inplace=True),\n                               nn.Sigmoid())\n\n\n  def forward(self, x_3d):\n    cnn_embed_seq = []\n\n    for i in range(x_3d.size(1)):\n      \n        x = self.conv1(x_3d[:, i, :, :, :].squeeze(1))\n        x = self.conv2(x)\n        cnn_embed_seq.append(x)\n      \n    cnn_embed_seq = torch.stack(cnn_embed_seq, dim=0).transpose_(0, 1)\n    return cnn_embed_seq\n\n\nclass DecoderRNN(Module):\n\n  def __init__(self, encoder_output_dim, hidden_size=512, num_class=1):\n    super(DecoderRNN, self).__init__()\n\n    self.hidden_size = hidden_size\n    self.lstm = nn.LSTM(input_size=encoder_output_dim, hidden_size=hidden_size, num_layers=1, batch_first=True, bidirectional=True)\n    self.fc2 = nn.Linear(4*hidden_size, num_class)\n\n  def forward(self, x_RNN):\n\n    h0 = torch.zeros(2, x_RNN.size(0), self.hidden_size).requires_grad_()\n    c0 = torch.zeros(2, x_RNN.size(0), self.hidden_size).requires_grad_()\n\n    rnn_out, _ = self.lstm(x_RNN, (h0.to(device), c0.to(device)))\n    rnn_max, _ = torch.max(rnn_out, 1)\n    rnn_mean = torch.mean(rnn_out, 1)\n    out = torch.cat([rnn_max, rnn_mean], axis=1)    \n    out = self.fc2(out)\n\n    return out.squeeze()\n\n  \nclass lrcn(Module):\n\n  def __init__(self, encoder_output_dim, num_class=1):\n    super(lrcn, self).__init__()\n\n    self.cnn = EncoderCNN(encoder_output_dim=encoder_output_dim)\n    self.rnn = DecoderRNN(encoder_output_dim=encoder_output_dim, num_class=num_class)\n\n  def forward(self, x_3d):\n    x = self.rnn(self.cnn(x_3d))\n\n    return x\n\n\ndef load_model(path, encoder_output_dim=dim_trans, load_weight=True):\n\n    model = lrcn(encoder_output_dim=encoder_output_dim, num_class = 1)\n    \n    if load_weight:\n        for weight in sorted(os.listdir(path)):\n            if 'pth' in weight:                \n                weight_path = os.path.join(path, weight)                \n                state = torch.load(weight_path, map_location=lambda storage, loc: storage)\n                return state\n    else:\n        return model.state_dict()\n\nbase_model = load_model('../input/facenet-pretrained', load_weight=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"def focal_loss(pred, expected, alpha=0.25, gamma=2):\n    \n    ce = f.binary_cross_entropy(pred, expected)\n    pt = torch.exp(-ce)\n    fc = alpha*((1-pt)**gamma)*ce\n    \n    return torch.mean(fc)\n\ndef loss_fn(pred, expected):\n    return 0.1*f.mse_loss(torch.sigmoid(pred), expected) + 0.9*focal_loss(pred, expected)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Trainer(object):\n    \n    def __init__(self, base_model):\n        \n        self.base_model = base_model\n        self.model = lrcn(encoder_output_dim=dim_trans, num_class = 1).to(device)\n        self.creation = loss_fn        \n        \n    def train_process(self, folds, dataset, epochs):                \n        for fold, (train_idx, val_idx) in enumerate(KFold(n_splits=5, shuffle=True, random_state=41).split(dataset)):\n            print(f'fold {fold}:')\n            train_dataset = Subset(dataset, train_idx)\n            val_dataset = Subset(dataset, val_idx)\n            train_ld = DataLoader(train_dataset, batch_size=8, shuffle=True)\n            val_ld = DataLoader(val_dataset, batch_size=8, shuffle=True)\n            \n            del train_dataset, val_dataset            \n            self.model.load_state_dict(self.base_model[fold])\n            \n            optimizer = optim.AdamW([      \n            {'params': self.model.conv.parameters(), 'lr': 1e-4},\n            {'params': self.model.fc.parameters(), 'lr': 1e-3}], lr=0.001)\n        \n            scheduler = OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(train_ld), epochs=3)\n            \n            self.model.train()\n            \n            score_max = 0\n            check_step = 0\n            loss_min = 1\n            check_number = 10\n            \n            for epoch in range(epochs):\n                train_loss, val_loss = 0, 0\n                for crop, label in tqdm(train_ld):                \n                    crop = crop.permute(0, 3, 1, 2)\n                    crop, label = crop.float().to(device), label.to(device)\n\n                    optimizer.zero_grad()\n                    output = self.model(crop).squeeze(1)\n                    loss = self.creation(output, label)\n                    loss.backward()\n\n                    optimizer.step()\n                    scheduler.step(train_loss)\n                    train_loss += loss.item()\n\n                    del crop, label\n\n                train_loss = train_loss/len(train_ld)\n                torch.cuda.empty_cache()\n\n                gc.collect()\n\n                self.model.eval()\n\n                val_score = 0\n                with torch.no_grad():\n                    for crop, label in tqdm(val_ld):\n                        crop = crop.permute(0, 3, 1, 2)\n                        crop, label = crop.float().to(device), label.to(device)                    \n\n                        output = self.model(crop).squeeze(1)\n                        loss = self.creation(output, label)\n                        val_loss += loss.item()\n                        val_score += torch.sum((output>0.5).float() == label).item()/len(label)\n\n                    val_loss = val_loss/len(val_ld)\n                    val_score = val_score/len(val_ld)\n\n                scheduler.step(val_loss)\n\n                if val_score > score_max:\n                    print(f'\\tEpoch: {epoch}, train loss: {train_loss:.5f}, val_loss: {val_loss:.5f}.\\n\\tValidation score increased from {score_max:.5f} to {val_score:.5f}')\n                    score_max = val_score\n                    loss_min = val_loss\n                    torch.save(self.model.state_dict(), f'model_{str(fold)}.pth')\n                    print('\\tSaving model!')\n                    check_step = 0\n\n                elif val_score == score_max:\n                    if val_loss < loss_min:\n                        print(f'\\tEpoch: {epoch}, train loss: {train_loss:.5f}, val_loss: {val_loss:.5f}, val_score: {val_score:.5f}.\\n\\tValidation loss decreased from {loss_min:.5f} to {val_loss:.5f}')\n                        loss_min = val_loss\n                        torch.save(self.model.state_dict(), f'model_{str(fold)}.pth')\n                        print('\\tSaving model!')\n                        check_step = 0\n                    else:\n                        check_step += 1\n                        print(f'\\tEpoch: {epoch}, train loss: {train_loss:.5f}, val_loss: {val_loss:.5f}, val_score: {val_score:.5f}.\\n\\tModel not improve in {str(check_step)} step')\n                        if check_step > check_number:\n                            print('\\tStop trainning!')\n                            break\n                else:\n                    check_step += 1\n                    print(f'\\tEpoch: {epoch}, train loss: {train_loss:.5f}, val_loss: {val_loss:.5f}.\\n\\tValidation score not increased from {val_score:.5f} in {str(check_step)} step')\n\n                    if check_step > check_number:\n                        print('\\tStop trainning!')\n                        break\n                        \n            del optimizer, scheduler, train_ld, val_ld\n            torch.cuda.empty_cache()\n            \n            gc.collect()\n            \n                    \n    def predict_process(self, test_ld, submission):\n                        \n        submission['label'] = 0.6\n        self.model.load_state_dict(self.base_model)\n\n        self.model.eval()\n        for filename, crops in tqdm(test_ld):\n\n            crops = crops.permute(0, 1, 4, 2, 3)\n            crops = crops.float().to(device)\n            with torch.no_grad():\n                output = self.model(crops)\n                output = torch.sigmoid(output).cpu().detach().numpy() \n                    \n                submission.loc[(submission[submission['filename']==filename[0]]).index.values[0], 'label'] = np.clip(output, 0.1, 0.9)\n                                \n        return submission\n                    \ntrainer = Trainer(base_model)        \nsubmission = trainer.predict_process(test_ld=test_ld, submission=submission)\nsubmission.to_csv('submission.csv', index=False)\n\nsubmission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"0014f2d4c16044c89edb820756294a1d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b1ecc223f1f745f79cbfc6ec1551da55","placeholder":"​","style":"IPY_MODEL_a1e6461a17ab48668d38d571ced1eb5a","value":" 400/400 [26:02&lt;00:00,  3.91s/it]"}},"071843c29317485c978770f9b05c7dc6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"IntProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_e26db3e4a4c945cda4e8abc9962f4868","max":400,"min":0,"orientation":"horizontal","style":"IPY_MODEL_16a4c6274020493aa9de663999773cc7","value":400}},"16a4c6274020493aa9de663999773cc7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"28e407e075a8436a8cc3b64a63f8e87e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5c9b5e80ef98427a8beccb45115f0623","IPY_MODEL_2e4af5dd593540929228b4838c471061"],"layout":"IPY_MODEL_ff76dc3b437b4857a4eba3738ecdcdab"}},"2e4af5dd593540929228b4838c471061":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bf670ce749d5427c835e5dcdfda8b57b","placeholder":"​","style":"IPY_MODEL_f636b6f5ebb844eeb9cac9fe3333420a","value":" 400/400 [29:29&lt;00:00,  4.42s/it]"}},"5c9b5e80ef98427a8beccb45115f0623":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"IntProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_a97111d12d4843f69644ade186923a69","max":400,"min":0,"orientation":"horizontal","style":"IPY_MODEL_67038b04b012406fbd30b6632f27d352","value":400}},"67038b04b012406fbd30b6632f27d352":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"a1e6461a17ab48668d38d571ced1eb5a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a97111d12d4843f69644ade186923a69":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1ecc223f1f745f79cbfc6ec1551da55":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf670ce749d5427c835e5dcdfda8b57b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2188ade787945dabd031a56fd009c52":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_071843c29317485c978770f9b05c7dc6","IPY_MODEL_0014f2d4c16044c89edb820756294a1d"],"layout":"IPY_MODEL_daca94af41024f1bb3e8fc1f076cf6fa"}},"daca94af41024f1bb3e8fc1f076cf6fa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e26db3e4a4c945cda4e8abc9962f4868":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f636b6f5ebb844eeb9cac9fe3333420a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ff76dc3b437b4857a4eba3738ecdcdab":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":4}