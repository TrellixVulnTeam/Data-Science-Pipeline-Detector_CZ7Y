{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport json\nimport cv2\nimport os\nimport matplotlib.pylab as plt\nfrom random import shuffle\nfrom tqdm import tqdm\nimport pickle\nimport keras\nfrom keras import optimizers\nimport tensorflow\nfrom keras.preprocessing import image\nimport math\nimport matplotlib.pyplot as plt    # for plotting the images\n%matplotlib inline\nfrom keras.utils import np_utils\nfrom keras.applications.vgg16 import preprocess_input\nfrom sklearn.model_selection import train_test_split\nfrom keras.applications.vgg16 import VGG16\nfrom keras.layers import Dense, InputLayer, Dropout","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.applications.vgg19 import VGG19","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from skimage.transform import resize\nimport pickle","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv(\"../input/deepfake-detection-challenge/sample_submission.csv\")\ntrain_sample_metadata = pd.read_json('../input/deepfake-detection-challenge/train_sample_videos/metadata.json').T\n\ntrain_dir = '../input/deepfake-detection-challenge/train_sample_videos/'\ntrain_video_filespath = [train_dir + x for x in tqdm(os.listdir(train_dir))]\ntrain_sample_imgs = [x for x in tqdm(os.listdir(train_dir))]\n\ntest_dir = '../input/deepfake-detection-challenge/test_videos/'\ntest_video_filespath = [test_dir + x for x in tqdm(os.listdir(test_dir))]\ntest_imgs = [x for x in tqdm(os.listdir(test_dir))]\n\ntrain_sample_imgs.remove('metadata.json')\ntrain_video_filespath.remove('../input/deepfake-detection-challenge/train_sample_videos/metadata.json')\n\ntrain_video_filespath = sorted(train_video_filespath)\ntrain_sample_imgs = sorted(train_sample_imgs)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"train_sample_metadata = pd.read_json('../input/deepfake-detection-challenge/train_sample_videos/metadata.json').T\n(train_sample_metadata)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Total = []\n#train_sample_metadata = sorted(train_sample_metadata)\nprint(train_sample_metadata)\nfor i in range(len(train_video_filespath)):\n    Total.append([train_video_filespath[i],train_sample_metadata.iloc[i,0]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"REAL = []\nFAKE = []\nfor i in range(len(Total)):\n    if(Total[i][1] == \"FAKE\"):\n        FAKE.append(Total[i])\n    else:\n        REAL.append(Total[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\n\nFAKE = random.sample(FAKE,len(REAL))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_video_filespath = REAL + FAKE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shuffle(train_video_filespath)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = []\ny = []\ndetector = MTCNN()\ndef detect_face(img):\n    img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    final = []\n    detected_faces_raw = detector.detect_faces(img)\n    if detected_faces_raw==[]:\n        #print('no faces found')\n        return []\n    confidences=[]\n    for n in detected_faces_raw:\n        x,y,w,h=n['box']\n        final.append([x,y,w,h])\n        confidences.append(n['confidence'])\n    if max(confidences)<0.7:\n        return []\n    max_conf_coord=final[confidences.index(max(confidences))]\n    #return final\n    return max_conf_coord\ndef crop(img,x,y,w,h):\n    x-=40\n    y-=40\n    w+=80\n    h+=80\n    if x<0:\n        x=0\n    if y<=0:\n        y=0\n    return cv2.cvtColor(cv2.resize(img[y:y+h,x:x+w],(224,224)),cv2.COLOR_BGR2RGB)\ndef detect_video(video):\n    v_cap = cv2.VideoCapture(video)\n    v_cap.set(1, NUM_FRAME)\n    success, vframe = v_cap.read()\n    vframe = cv2.cvtColor(vframe, cv2.COLOR_BGR2RGB)\n    bounding_box=detect_face(vframe)\n    if bounding_box==[]:\n        count=0\n        current=NUM_FRAME\n        while bounding_box==[] and count<MAX_SKIP:\n            current+=1\n            v_cap.set(1,current)\n            success, vframe = v_cap.read()\n            vframe = cv2.cvtColor(vframe, cv2.COLOR_BGR2RGB)\n            bounding_box=detect_face(vframe)\n            count+=1\n        if bounding_box==[]:\n            print('hi')\n            return None\n    x,y,w,h=bounding_box\n    v_cap.release()\n    return crop(vframe,x,y,w,h)\n\nMAX_SKIP=10\nNUM_FRAME=150\ncount=0\nfor video in tqdm(train_video_filespath):\n    img_file=detect_video(video)\n    X.append(img_file)\n    y.append(train_video_filespath[i][1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = []\ny = []\nfor i in tqdm(range(len(train_video_filespath))):\n    count = 0\n    videoFile = train_video_filespath[i][0]\n    cap = cv2.VideoCapture(videoFile)\n    frameRate = cap.get(5) #frame rate\n    x=1\n    while(cap.isOpened()):\n        frameId = cap.get(1) #current frame number\n        ret, frame = cap.read()\n        if (ret != True):\n            break\n        if (frameId % math.floor(frameRate) == 0):\n            frame = detectface()\n            a = resize(frame, preserve_range=True, output_shape=(224,224)).astype(int)\n            X.append(a)\n            y.append(train_video_filespath[i][1])\n    cap.release()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.array(X)    # converting list to array \n\nfor i in range(len(y)):\n    if(y[i] == 'FAKE'):\n        y[i] = 1\n    else:\n        y[i] = 0\n\ndummy_y = np_utils.to_categorical(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = preprocess_input(X, mode='tf')      # preprocessing the input data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))    # include_top=False to remove the top layer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model19 = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))    # include_top=False to remove the top layer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X1 = base_model.predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install mtcnn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from mtcnn import MTCNN","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X1 = X1.reshape(1694,7*7*512)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = X1/X1.max()      # centering the data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = keras.models.Sequential()\nmodel.add(InputLayer((7*7*512,)))    # input layer\nmodel.add(Dense(units=1024, activation='relu')) # hidden layer\n#model.add(Dense(units=1024, activation='relu')) # hidden layer\nmodel.add(Dense(2, activation='softmax'))    # output layer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#keras.callbacks.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\nhistory = model.fit(train, dummy_y, epochs=100, validation_split = 0.1, batch_size = 32, verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['val_accuracy'])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}