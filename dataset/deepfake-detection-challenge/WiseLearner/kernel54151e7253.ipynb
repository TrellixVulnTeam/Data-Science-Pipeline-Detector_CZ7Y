{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from __future__ import absolute_import, division, print_function, unicode_literals\n\nimport tensorflow as tf\n\nimport pandas as pd\nimport numpy as np\nimport shutil, os\nimport subprocess\nfrom pathlib import Path\nimport glob\nimport cv2\nfrom PIL import Image\nfrom numpy import asarray\nimport matplotlib.pyplot as plt\nimport datetime","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pathCascadeCL = '/kaggle/input/haar-face-detector/haarcascade_frontalface_alt.xml'\nface_cascade = cv2.CascadeClassifier(pathCascadeCL)\nfinalCalculatedThresh = 12.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def detect_face(img,required_size=(256,256)):\n    color = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    detected_faces_raw = face_cascade.detectMultiScale(gray,1.3,5)\n    face_images = []\n    for (x,y,w,h) in detected_faces_raw:\n        face_boundary = color[y:y+h, x:x+w]\n        face_image = Image.fromarray(face_boundary)\n        face_image = face_image.resize(required_size)\n        face_array = asarray(face_image)\n        face_images.append(face_array)\n    return face_images","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def detectVideo(test_path,file):\n    video = test_path + file\n    capture = cv2.VideoCapture(video)\n    v_len = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n    fps = capture.get(cv2.CAP_PROP_FPS)\n    frame_count = int(v_len/fps)*5\n    frame_idxs = np.linspace(0,v_len,frame_count, endpoint=False, dtype=np.int)\n    imgs=[]\n    i=0\n    for frame_idx in range(int(v_len)):\n        ret = capture.grab()\n        if not ret: \n            pass\n        if frame_idx >= frame_idxs[i]:\n            ret, frame = capture.retrieve()\n            if not ret or frame is None:\n                pass\n            else:\n                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                try:\n                    faces=detect_face(frame)\n                except Exception as err:\n                    print(err)\n                    continue\n                for face in faces:\n                    face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n                    imgs.append(face)\n            i += 1\n            if i >= len(frame_idxs):\n                break\n    return imgs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! tar xvf /kaggle/input/ffmpegstatic/ffmpeg-git-amd64-static.tar.xz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"command = f\"/kaggle/working/ffmpeg-git-20191209-amd64-static/ffmpeg -loglevel error -hide_banner -nostats\"\nsubprocess.call(command, shell=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"path = '/kaggle/input/deepfake-detection-challenge/test_videos/'\ntest_files = os.listdir(path)\nfinalvalues = []\naudioThresh = 44300\nfor fileToSave in test_files:\n    diffs = []\n    videoFile = fileToSave\n    output_dir = Path(f\"/kaggle/working/wavs/\")\n    Path(output_dir).mkdir(exist_ok=True, parents=True)\n    value = 0.0\n    try:\n        fileToread = '/kaggle/input/deepfake-detection-challenge/test_videos/'+ fileToSave\n        cap = cv2.VideoCapture(fileToread)\n        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        fps = cap.get(cv2.CAP_PROP_FPS)\n        duration = float(frame_count) / float(fps)\n        duration = int(duration)\n        fileToSave = fileToSave[:-4]\n        file = '/kaggle/input/deepfake-detection-challenge/test_videos/'+ fileToSave +'.mp4'\n        command = f\"/kaggle/working/ffmpeg-git-20191209-amd64-static/ffmpeg -i {file} -ab 192000 -ac 2 -ar 44100 -vn {output_dir/fileToSave}.wav\"\n        subprocess.call(command, shell=True)\n        pathvideo = '/kaggle/working/wavs/' + fileToSave + '.wav'\n        raw_audio = tf.io.read_file(pathvideo)\n        test = tf.audio.decode_wav(raw_audio, desired_channels=-1, desired_samples=-1, name=None)\n        audio = test.audio\n        shutil.rmtree('/kaggle/working/wavs')\n        if(int(audio.shape[0]/duration)>=audioThresh):\n            value = 1.0\n        else:\n            images = detectVideo(path,videoFile)\n            imagescropped = [x[60:220,20:220] for x in images]\n            for image in imagescropped:\n                val1 = np.sum(image)\n                dst1 = cv2.fastNlMeansDenoisingColored(image,None,10,10,7,21)\n                val2 = np.sum(dst1)\n                if val1>val2:\n                    diffs.append(val1-val2)\n                else:\n                    diffs.append(val2-val1)\n            if((min(diffs)/max(diffs)*100)<11.999):\n               value = 1.0\n    except Exception as e:\n        value = 0.5\n        pass\n    finalvalues.append(value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shutil.rmtree('/kaggle/working/ffmpeg-git-20191209-amd64-static')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\"filename\":test_files, \"label\": finalvalues})\nsubmission.to_csv(\"submission.csv\", index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}