{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ../input/efficientnet/efficientnet-1.0.0-py3-none-any.whl","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ef1c1fd7-b763-4a40-a5c0-524bfbc503e0","_uuid":"ffb09dc9-4297-4768-97e3-a9a9a8f37fe0","trusted":true},"cell_type":"code","source":"import pandas as pd\nsample_submission = pd.read_csv(\"../input/deepfake-detection-challenge/sample_submission.csv\")\n\n# %% [code]\nimport tensorflow as tf\nimport cv2\ndetection_graph = tf.Graph()\ndetection_graph = tf.Graph()\nwith detection_graph.as_default():\n    od_graph_def = tf.compat.v1.GraphDef()\n    with tf.io.gfile.GFile('../input/mobilenet-face/frozen_inference_graph_face.pb', 'rb') as fid:\n        serialized_graph = fid.read()\n        od_graph_def.ParseFromString(serialized_graph)\n        tf.import_graph_def(od_graph_def, name='')\n        config = tf.compat.v1.ConfigProto()\n    config.gpu_options.allow_growth = True\n    sess=tf.compat.v1.Session(graph=detection_graph, config=config)\n    image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n    boxes_tensor = detection_graph.get_tensor_by_name('detection_boxes:0')    \n    scores_tensor = detection_graph.get_tensor_by_name('detection_scores:0')\n    num_detections = detection_graph.get_tensor_by_name('num_detections:0')\ndef get_img(images):\n    global boxes,scores,num_detections\n    im_heights,im_widths=[],[]\n    imgs=[]\n    for image in images:\n        (im_height,im_width)=image.shape[:-1]\n        #image_np=cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n        imgs.append(image)\n        im_heights.append(im_height)\n        im_widths.append(im_widths)\n    imgs=np.array(imgs)\n    (boxes, scores_) = sess.run(\n        [boxes_tensor, scores_tensor],\n        feed_dict={image_tensor: imgs})\n    finals=[]\n    #print(num_detections)\n    for x in range(boxes.shape[0]):\n        scores=scores_[x]\n        max_=np.where(scores==scores.max())[0][0]\n        box=boxes[x][max_]\n        ymin, xmin, ymax, xmax = box\n        (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n                                      ymin * im_height, ymax * im_height)\n        left, right, top, bottom = int(left), int(right), int(top), int(bottom)\n        finals.append([left, right, top, bottom])\n    return finals\ndef resize_imgs(bbox,imgs,resize=240,pad=40):\n    finals=[]\n    for image,(left, right, top, bottom) in zip(imgs,bbox):\n        finals.append(cv2.cvtColor(cv2.resize(image[max([0,top-pad]):bottom+(pad*2),max([0,left-pad]):right+(pad*2)],(resize,resize)),cv2.COLOR_BGR2RGB))\n    return np.hstack(finals)\ndef detect_video(video,frame_count=10):\n    capture = cv2.VideoCapture(video)\n    v_len = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n    frame_idxs = np.linspace(0,v_len,frame_count, endpoint=False, dtype=np.int)\n    imgs=[]\n    i=0\n    for frame_idx in range(int(v_len)):\n        ret = capture.grab()\n        if not ret: \n            print(\"Error grabbing frame %d from movie %s\" % (frame_idx, video))\n        if frame_idx >= frame_idxs[i]:\n            if frame_idx-frame_idxs[i]>20:\n                return None\n            ret, frame = capture.retrieve()\n            if not ret or frame is None:\n                print(\"Error retrieving frame %d from movie %s\" % (frame_idx, video))\n            else:\n                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                imgs.append(frame)\n            i += 1\n            if i >= len(frame_idxs):\n                break\n    bbox=get_img(imgs)\n    if len(bbox)<10:\n        return None\n    return imgs,bbox","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport glob\nfrom tqdm.notebook import tqdm\nimport numpy as np\nimport os\nimport torch\nfrom albumentations.pytorch import ToTensorV2\nfrom albumentations import *\nval_transform = Compose([\n    Resize(240, 240),\n    Normalize(mean=(.5, .5, .5), std=(.5, .5, .5)),\n    ToTensorV2(),\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1=torch.jit.load('../input/dfdc-ensemble-final-2/dfdc_mixl_0.pt',map_location='cpu').cuda()\n_=model1.eval()\nmodel2=torch.jit.load('../input/dfdc-ensemble-final-2/dfdc_mixxl_0.pt',map_location='cpu').cuda()\n_=model2.eval()\nmodel3=torch.jit.load('../input/dfdc-ensemble-final-2/dfdc_se101_2.pt',map_location='cpu').cuda()\n_=model3.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import efficientnet.keras as efn \nfrom keras.layers import *\nfrom keras.models import *\nbottleneck = efn.EfficientNetB6(weights=None,include_top=False,pooling='avg')\ninp=Input((240,240,3))\nx=bottleneck(inp)\nx=Dense(256)(x)\nx=Dense(128, activation='relu')(x)\nx=Dense(1,activation='sigmoid')(x)\nmodel_b6=Model(inp,x)\nmodel_b6.load_weights('../input/dfdc-ensemble-final-2/nocrop_effb6_dense256_dense128_e5_wrong_activation_weights.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_34 = load_model(\"../input/dfdc-ensemble-final-2/model_b3_300_4.h5\")\nmodel_35 = load_model(\"../input/dfdc-ensemble-final-2/model_35.h5\")\nmodel_b1_34 = load_model(\"../input/dfdc-ensemble-final-2/model_b1_200_5.h5\")\n\ndef get_birghtness(img):\n    return img/img.max()\ndef process_img(img,flip=False,birghtness=True):\n    imgs=[]\n    for x in range(10):\n        if flip:\n            imgs.append(get_birghtness(cv2.flip(img[:,x*240:(x+1)*240,:],1)))\n        else:\n            imgs.append(get_birghtness(img[:,x*240:(x+1)*240,:]))\n    return np.array(imgs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_SAMPLES=5\ndef aug(img, size):\n    img = A.RandomCrop(size,size)(image=img)['image']\n    return img\ndef aug2(img):\n    img = A.HorizontalFlip(p=0.5)(image=img)['image']\n    img = A.RandomBrightness(p=0.5)(image=img)['image']\n    return img\ndef process_img2(img, size=200, pad=40):\n    imgs=[]\n    for x in range(10):\n        for _ in range(NUM_SAMPLES):\n            imgs.append(aug(img[:,x*size+pad:(x+1)*size+pad,:], size))\n    return np.array(imgs)\ndef process_img3(img):\n    imgs=[]\n    for x in range(10):\n        for _ in range(NUM_SAMPLES):\n            imgs.append(aug2(img[:,x*240:(x+1)*240,:]))\n    return np.array(imgs)\ndef torch_process_img(img):\n    imgs=[]\n    for x in range(10):\n        imgs.append(val_transform(image=img[:,x*240:(x+1)*240,:])['image'])\n    imgs = torch.stack(imgs)\n    return imgs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport glob\nimport albumentations as A\nfrom tqdm.notebook import tqdm\ntest_files=glob.glob('../input/deepfake-detection-challenge/test_videos/*.mp4')\nsubmission=pd.DataFrame()\nsubmission['filename']=os.listdir(('../input/deepfake-detection-challenge/test_videos/'))\nsubmission['label']=0.5\n\n# %% [code]\nfilenames=[]\npreds=[]\nfor x in tqdm(test_files):\n    try:\n        imgs,raw_bbox=detect_video(x)\n        video = resize_imgs(raw_bbox, imgs, resize=240,pad=40)\n        video2 = resize_imgs(raw_bbox, imgs, resize=340, pad=40)\n        img=process_img2(cv2.cvtColor(video,cv2.COLOR_BGR2RGB))\n        img2=process_img2(video2, size=300, pad=40)\n        img3=process_img2(video)\n        img4=process_img3(video)\n        img5=torch_process_img(cv2.cvtColor(video,cv2.COLOR_BGR2RGB)).cuda()\n        pred1 = model_35.predict(img)\n        pred2 = model_34.predict(img2)\n        pred3 = model_b1_34.predict(img3)\n        pred4 = model_b6.predict(img4)\n\n        p = np.concatenate([pred1, pred2, pred3,pred4])\n\n        #HERE IS COMES, THIS TIME IT IS NOT ABOUT JUST HARD MEAN, ITS ABOUT LOSS CONTROL\n\n        l0 = []\n        l1 = []\n        for pr in p:\n            if pr>0.5:\n                l1.append(pr)\n            else:\n                l0.append(pr)\n\n        if len(l0) is 0: l0 = [0]\n        if len(l1) is 0: l1 = [0]\n    \n        confidence_0 = (0.5 - np.mean(l0)) * 2\n        confidence_1 = (np.mean(l1) - 0.5) * 2\n\n        #FORMULA TO SUCCESS meaning with confidence\n        p = np.mean(p) - (np.mean(p) / 100) * (confidence_0 * 100) + (np.mean(p) / 100) * (confidence_1 * 100)\n\n        #BUT AFTER THIS, THE POSSIBILITES OF THE SCORES ARE BEYOND 0 AND 1 SO I AM GONNA CLIP TO MAKE SURE EVERYTHING IS IN PLACE\n\n        p1 = np.clip(p, a_max=0.99, a_min=0.01)\n\n        \n        pred1=torch.sigmoid(model1(img5)).view(10).mean().item()\n        pred2=torch.sigmoid(model2(img5)).view(10).mean().item()\n        pred3=torch.sigmoid(model3(img5)).view(10).mean().item()\n        p2=np.array([pred1,pred2,pred3]).mean()\n        preds.append(np.mean([p1,p2]))\n        filenames.append(x.replace('../input/deepfake-detection-challenge/test_videos/',''))\n    except Exception as err:\n        print(err)\n        pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean([p1,p2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sum(preds)/len(preds))\n# %% [code]\nfor x,y in zip(preds,filenames):\n    submission.loc[submission['filename']==y,'label']=x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.array(submission['label']).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %% [code]\nsubmission.to_csv('submission.csv', index=False)\n!rm -r videos","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"247ff75c56b8487a9bee0e3acb1e2781":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"IntProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_5d16159791224ebabd24f6fc02b7ef96","max":400,"min":0,"orientation":"horizontal","style":"IPY_MODEL_472a361f2eb14f5886bf6943b1589eca","value":400}},"4620c4c8658f41058d46da159047318d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"472a361f2eb14f5886bf6943b1589eca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"5375c930c40c4f06a24b9c3ab972cdbd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5d16159791224ebabd24f6fc02b7ef96":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"76de5f79a928476c85ab06de089fff06":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b26caf329f0b4c12b7629f75faf825e0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_76de5f79a928476c85ab06de089fff06","placeholder":"​","style":"IPY_MODEL_5375c930c40c4f06a24b9c3ab972cdbd","value":" 400/400 [15:14&lt;00:00,  2.29s/it]"}},"d0a202d5e46744728caecd3fb4423fb5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_247ff75c56b8487a9bee0e3acb1e2781","IPY_MODEL_b26caf329f0b4c12b7629f75faf825e0"],"layout":"IPY_MODEL_4620c4c8658f41058d46da159047318d"}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":4}