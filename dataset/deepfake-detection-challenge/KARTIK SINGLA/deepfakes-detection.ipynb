{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install facenet-pytorch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from facenet_pytorch.models.inception_resnet_v1 import get_torch_home\ntorch_home = get_torch_home()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport glob\nimport time\nimport torch\nimport cv2\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom tqdm.notebook import tqdm\n\n# See github.com/timesler/facenet-pytorch:\nfrom facenet_pytorch import MTCNN, InceptionResnetV1, extract_face\n\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\nprint(f'Running on device: {device}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mtcnn = MTCNN(margin=14, keep_all=True, factor=0.5, device=device).eval()\n\n# Load facial recognition model\nresnet = InceptionResnetV1(pretrained='vggface2', device=device).eval()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DetectionPipeline:\n    \"\"\"Pipeline class for detecting faces in the frames of a video file.\"\"\"\n    \n    def __init__(self, detector, n_frames=None, batch_size=60, resize=None):\n        \"\"\"Constructor for DetectionPipeline class.\n        \n        Keyword Arguments:\n            n_frames {int} -- Total number of frames to load. These will be evenly spaced\n                throughout the video. If not specified (i.e., None), all frames will be loaded.\n                (default: {None})\n            batch_size {int} -- Batch size to use with MTCNN face detector. (default: {32})\n            resize {float} -- Fraction by which to resize frames from original prior to face\n                detection. A value less than 1 results in downsampling and a value greater than\n                1 result in upsampling. (default: {None})\n        \"\"\"\n        self.detector = detector\n        self.n_frames = n_frames\n        self.batch_size = batch_size\n        self.resize = resize\n        \n            \n    def __call__(self, filename):\n        \"\"\"Load frames from an MP4 video and detect faces.\n\n        Arguments:\n            filename {str} -- Path to video.\n        \"\"\"\n        # Create video reader and find length\n        v_cap = cv2.VideoCapture(filename)\n        v_len = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        print(v_len)\n        # Pick 'n_frames' evenly spaced frames to sample\n        if self.n_frames is None:\n            sample = np.arange(0, v_len)\n        else:\n            sample = np.linspace(0, v_len - 1, self.n_frames).astype(int)\n\n        # Loop through frames\n        faces = []\n        frames = []\n        for j in range(v_len):\n            success = v_cap.grab()\n            if j in sample:\n                # Load frame\n                success, frame = v_cap.retrieve()\n                if not success:\n                    continue\n                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                frame = Image.fromarray(frame)\n                \n                # Resize frame to desired size\n                if self.resize is not None:\n                    frame = frame.resize([int(d * self.resize) for d in frame.size])\n                frames.append(frame)\n\n                # When batch is full, detect faces and reset frame list\n                if len(frames) % self.batch_size == 0 or j == sample[-1]:\n                    faces.extend(self.detector(frames))\n                    frames = []\n\n        v_cap.release()\n        \n        return faces    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import copy\ndef process_faces(faces, resnet):\n    # Filter out frames without faces\n    faces = [f for f in faces if f is not None]\n    if(len(faces) == 0):\n        return []\n#     faces = np.array(faces)\n#     print(faces[200].shape)\n#     faces = torch.from_numpy(faces)\n#     f = copy.deepcopy(faces)\n#     f = np.array(f[0])\n#     print(type(faces))\n#     print(f.shape)\n    faces = torch.cat(faces).to(device)\n    if(len(faces)<290):\n        return []\n#     print(len(faces))\n    faces = faces[:290]\n    # Generate facial feature vectors using a pretrained model\n    embeddings = resnet(faces)\n#     print(len(embeddings))\n#     print(len(embeddings[0]))\n    \n#     print(len(embeddings))\n#     print(len(embeddings[0]))\n    # Calculate centroid for video and distance of each face's feature vector from centroid\n    centroid = embeddings.mean(dim=0)\n    \n    \n    \n    x = (embeddings - centroid).norm(dim=1).cpu().numpy()\n    \n    return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define face detection pipeline\ndetection_pipeline = DetectionPipeline(detector=mtcnn, batch_size=60, resize=0.25)\nimport json\n\nwith open('../input/deepfake-detection-challenge/train_sample_videos/metadata.json') as f:\n  data = json.load(f)\n# Get all test videos\nfilenames = glob.glob('/kaggle/input/deepfake-detection-challenge/train_sample_videos/*.mp4')\ntotal_files = len(filenames)\n\nX = []\ny = []\nstart = time.time()\nn_processed = 0\n# filename = 'kaggle/input/deepfake-detection-challenge/train_sample_videos/aapnvogymq.mp4'\n# print(filename)\n\n# faces = detection_pipeline(filename)\n# print(faces)\nwith torch.no_grad():\n    for i, filename in tqdm(enumerate(filenames), total=len(filenames)):\n        print(i, filename)\n        try:\n            # Load frames and find faces\n            faces = detection_pipeline(filename)\n           \n            \n#             f = np.array(faces[0])\n#             print(f.shape)\n            \n            \n            \n            # Calculate embeddings\n            \n            z = process_faces(faces, resnet)\n            if (len(z)!=0):\n                X.append(z)\n                if(data[filename[63:]]['label']=='FAKE'):\n                    y.append(1)\n                else:\n                    y.append(0)\n\n        except KeyboardInterrupt:\n            print('\\nStopped.')\n            break\n\n        except Exception as e:\n            print(e)\n            X.append(None)\n        \n        n_processed += len(faces)\n#         print(f'Frames per second (load+detect+embed): {n_processed / (time.time() - start):6.3}\\r', end='')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=X[1:]\ny=y[1:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.85, random_state=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(random_state=0).fit(X_train, y_train)\ny_pred_lr = clf.predict(X_test)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nprint(accuracy_score(y_test,y_pred_lr))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for i in range(len(y_test)):\n#     if y_test[i] == y_pred_lr[i] and y_test[i]==0:\n#         print(y_test[i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\ngnb = GaussianNB()\ny_pred_gnb = gnb.fit(X_train, y_train).predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nprint(accuracy_score(y_test,y_pred_gnb))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import svm\n\nclf = svm.SVC(gamma='auto')\nclf.fit(X_train, y_train)\ny_pred_svm = clf.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nprint(accuracy_score(y_test,y_pred_svm))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}