{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%matplotlib inline\nfrom matplotlib import pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.patches import Rectangle\n\nimport os\nimport json\nimport time\nimport numpy as np\nfrom PIL import Image\nimport torch\nfrom tqdm import tqdm\nimport cv2\nimport sys\n\n# sys.path.append('/kaggle/input/retinaface/RetinaFace')\n# from retinaface import RetinaFace\n\n# sys.path.append('/kaggle/input/yolov2face')\n# from yolov2 import load_mobilenetv2_224_075_detector, FaceDetector_yolo, get_boxes_points\n\n# sys.path.append('/kaggle/input/s3fdface/s3fd')\n# from detection.sfd import FaceDetector\n\nsys.path.append('/kaggle/input/retinafacetorch')\nfrom retina import retinaface_model, detect_images","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nretinaface_model = retinaface_model(model_path='/kaggle/input/retinafacetorch/Resnet50_Final.pth',device=device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"video_path = '/kaggle/input/deepfake-detection-challenge/train_sample_videos/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# n_frames = 10\n# for vi in os.listdir(video_path):\n#     start = time.time()\n#     imgs = []\n    \n#     cap = cv2.VideoCapture(os.path.join(video_path, vi))\n#     v_len = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n#     video_length = 0\n#     for j in range(v_len):\n#         success = cap.grab()\n#         if success:\n#             video_length += 1\n#         else:\n#             break\n#     cap.release()\n\n#     sample = np.linspace(0, video_length-1, n_frames).astype(int)\n#     cap = cv2.VideoCapture(os.path.join(video_path, vi))\n#     for j in range(video_length):\n#         succ, image = cap.read()\n#         if j in sample and succ:\n#             # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n#             imgs.append(np.float32(image))\n#         if len(imgs) == n_frames:\n#             break\n#     if len(imgs) != 0:\n#         detect_images(imgs=imgs, net=retinaface_model, thresh=0.94, device=device)\n#         print(time.time()-start)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_sequence(sequence, num_frames):\n    columns = 3\n    rows = (num_frames + 1) // (columns)\n    fig = plt.figure(figsize = (32,(16 // columns) * rows))\n    gs = gridspec.GridSpec(rows, columns)\n    for j in range(rows*columns):\n        plt.subplot(gs[j])\n        plt.axis(\"off\")\n        plt.imshow(sequence[j])\n\nsample_video = '/kaggle/input/deepfake-detection-challenge/train_sample_videos/apatcsqejh.mp4'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imgs = []\ncap = cv2.VideoCapture(sample_video)\nvideo_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\nsample = np.linspace(0, video_length-1, 9).astype(int)\nfor j in range(video_length):\n    success = cap.grab()\n    if j in sample:\n        success, image = cap.retrieve()\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        if not success:\n            continue\n        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        imgs.append(image)\ncap.release()\nbboxes = detect_images(imgs=[np.float32(img) for img in imgs], net=retinaface_model, thresh=0.94, device=device)\npyretina_final_list = []\nred = (255,0,0)\nfor i in range(len(imgs)):\n    for b in bboxes[i]:\n        lx, ly, rx, ry = b[0], b[1], b[2], b[3]\n        cv2.rectangle(imgs[i], (int(round(lx)),int(round(ly))), (int(round(rx)), int(round(ry))), red, 2)\n    pyretina_final_list.append(imgs[i])\nshow_sequence(pyretina_final_list, 9)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"RetinaFace speed test:"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# retina_detector = RetinaFace('/kaggle/input/retinaface/RetinaFace/models/R50', 0, 0, 'net3')\n# for vi in os.listdir(video_path):\n#     start = time.time()\n#     imgs = []\n#     cap = cv2.VideoCapture(os.path.join(video_path, vi))\n#     video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n#     sample = np.linspace(0, video_length-1, 10).astype(int)\n#     for j in range(video_length):\n#         success = cap.grab()\n#         if j in sample:\n#             success, image = cap.retrieve()\n#             if not success:\n#                 continue\n#             # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n#             imgs.append(image)\n#     cap.release()\n#     thresh = 0.95\n#     scales = [1024, 1980]\n#     im_shape = imgs[0].shape\n#     target_size = scales[0]\n#     max_size = scales[1]\n#     im_size_min = np.min(im_shape[0:2])\n#     im_size_max = np.max(im_shape[0:2])\n#     #im_scale = 1.0\n#     #if im_size_min>target_size or im_size_max>max_size:\n#     im_scale = float(target_size) / float(im_size_min)\n#     # prevent bigger axis from being more than max_size:\n#     if np.round(im_scale * im_size_max) > max_size:\n#         im_scale = float(max_size) / float(im_size_max)\n#     scales = [im_scale]\n#     flip = False\n#     faces, _ = retina_detector.detect(imgs, thresh, scales=scales, do_flip=flip)\n#     print(len(faces))\n#     print(type(faces))\n#     print(faces)\n#     # print(faces)\n#     # print(time.time()-start)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"S3fdFace speed test:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n# sfd_detector = FaceDetector(device=device, path_to_detector='/kaggle/input/s3fdface/s3fd/s3fd-619a316812.pth', verbose=False)\n# for vi in os.listdir(video_path):\n#     start = time.time()\n#     imgs = []\n#     cap = cv2.VideoCapture(os.path.join(video_path, vi))\n#     video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n#     sample = np.linspace(0, video_length-1, 10).astype(int)\n#     for j in range(video_length):\n#         success = cap.grab()\n#         if j in sample:\n#             success, image = cap.retrieve()\n#             image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n#             if not success:\n#                 continue\n#             # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n#             imgs.append(image)\n#     cap.release()\n#     for im in imgs:\n#         detected_faces = sfd_detector.detect_from_image(im, rgb=True)\n#         # print(faces)\n#     print(time.time()-start)\n#     break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Yolo_v2 Face speed test:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# mobilenetv2 = load_mobilenetv2_224_075_detector(\"/kaggle/input/yolov2face/facedetection-mobilenetv2-size224-alpha0.75.h5\")\n# yolo_model = FaceDetector_yolo(mobilenetv2)\n# for vi in os.listdir(video_path):\n#     start = time.time()\n#     imgs = []\n#     cap = cv2.VideoCapture(os.path.join(video_path, vi))\n#     video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n#     sample = np.linspace(0, video_length-1, 10).astype(int)\n#     for j in range(video_length):\n#         success = cap.grab()\n#         if j in sample:\n#             success, image = cap.retrieve()\n#             image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n#             if not success:\n#                 continue\n#             # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n#             imgs.append(image)\n#     cap.release()\n#     for im in imgs:\n#         yolo_boxes = yolo_model.detect(im, 0.9)\n#         # print(faces)\n#     print(time.time()-start)\n#     break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualization of the face detection results:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# def show_sequence(sequence, num_frames):\n#     columns = 3\n#     rows = (num_frames + 1) // (columns)\n#     fig = plt.figure(figsize = (32,(16 // columns) * rows))\n#     gs = gridspec.GridSpec(rows, columns)\n#     for j in range(rows*columns):\n#         plt.subplot(gs[j])\n#         plt.axis(\"off\")\n#         plt.imshow(sequence[j])\n\n# sample_video = '/kaggle/input/deepfake-detection-challenge/train_sample_videos/apatcsqejh.mp4'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# imgs = []\n# cap = cv2.VideoCapture(sample_video)\n# video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n# sample = np.linspace(0, video_length-1, 9).astype(int)\n# for j in range(video_length):\n#     success = cap.grab()\n#     if j in sample:\n#         success, image = cap.retrieve()\n#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n#         if not success:\n#             continue\n#         # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n#         imgs.append(image)\n# cap.release()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# retina_detector = RetinaFace('/kaggle/input/retinaface/RetinaFace/models/R50', 0, -1, 'net3')\n# thresh = 0.94\n# scales = [1024, 1980]\n# im_shape = imgs[0].shape\n# target_size = scales[0]\n# max_size = scales[1]\n# im_size_min = np.min(im_shape[0:2])\n# im_size_max = np.max(im_shape[0:2])\n# #im_scale = 1.0\n# #if im_size_min>target_size or im_size_max>max_size:\n# im_scale = float(target_size) / float(im_size_min)\n# # prevent bigger axis from being more than max_size:\n# if np.round(im_scale * im_size_max) > max_size:\n#     im_scale = float(max_size) / float(im_size_max)\n# scales = [im_scale]\n# flip = False\n# retina_final_list = []\n# red = (255,0,0)\n# for im in imgs:\n#     faces, _ = retina_detector.detect(im, thresh, scales=scales, do_flip=flip)\n#     if faces is not None:\n#         for i in range(faces.shape[0]):\n#             box = faces[i].astype(np.int)\n#             lx, ly, rx, ry = box[0], box[1], box[2], box[3]\n#             cv2.rectangle(im, (int(round(lx)),int(round(ly))), (int(round(rx)), int(round(ry))), red, 2)\n#     retina_final_list.append(im)\n# show_sequence(retina_final_list, 9)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sfd_final_list = []\n# red = (255,0,0)\n# for im in imgs:\n#     detected_faces = sfd_detector.detect_from_image(im, rgb=True)\n#     for b in detected_faces:\n#         lx, ly, rx, ry, _ = b\n#         cv2.rectangle(im, (int(round(lx)),int(round(ly))), (int(round(rx)), int(round(ry))), red, 2)\n#     sfd_final_list.append(im)\n# show_sequence(sfd_final_list, 9)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# mobilenetv2 = load_mobilenetv2_224_075_detector(\"/kaggle/input/yolov2face/facedetection-mobilenetv2-size224-alpha0.75.h5\")\n# yolo_model = FaceDetector_yolo(mobilenetv2)\n# yolo_final_list = []\n# red = (255,0,0)\n# for im in imgs:\n#     yolo_boxes = yolo_model.detect(im, 0.75)\n#     print(yolo_boxes)\n#     yb = get_boxes_points(yolo_boxes, im.shape) \n#     for b in yb:\n#         lx, ly, rx, ry = b\n#         cv2.rectangle(im, (int(round(lx)),int(round(ly))), (int(round(rx)), int(round(ry))), red, 2)\n#     yolo_final_list.append(im)\n# show_sequence(yolo_final_list, 9)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}