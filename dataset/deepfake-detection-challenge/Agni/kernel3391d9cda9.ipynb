{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"'''\n\nAdvantages of my solution: \n\nCPU only\nTotal time taken to process  400  video files is  1667.5433096885681  seconds, that's 28 minutes\nLogloss is  0.21584579832806053  for  400  videos\n\nI have tried a very simple and holistic approach to a complex challenge driven primarily by limited computing resources and\ntime. Thus my focus has been to extract valuable information and simplify the input data and at the same time apply \nproven tested methods to train a NN so that the results can be accurate and fast. \n\nTo that end I have used Transfer Learning leveraging the Inceptionv3 CNN model. Instead of using an RNN/LSTM to deal with the temporal aspects of Video frames,\nI bunched together N frames from a video and coalesced them to include the temporal aspect which I then analyze with the CNN. Instead of the full \nN frames, I extracted relevant sections of frames, faces in this specific case and reduced the input dimensions further. \n\nPre-processing for Training uses exactly the same modules and functions as the ones below except with different set of parameters. I haven't included the trianing module\nhere but it is as simple as an agglomeration of calls like this - in the case of training I store the concatenated extracted sections from the N frames\nin the StoreConcatIn based on this logic :\n\n                            if label.find(\"FAKE\") != -1 : #meaning it  said FAKE\n                                StoreConcatIn = \"TL/data/train/fake\"\n\n                            else:\n                                StoreConcatIn = \"TL/data/train/real\"\n                                \n                            n = ScoreVideo(filename, StoreConcatIn, 0)\n                            \nI loop over training folders in the zip folders and collect this collection of N sub-frames concatenated together. \n\nI feed this to the transfer training NN and train it. The minimum error rate I obtained was around 0.12 and that too with a minor subset of the training\nvideos as I didn't have enough resources.  The training pre-processing module is at the very bottom and as I ran it primarily from my desktop has references to various\nmodules that I just left as is. \n\nI am happy to provide the jupyter notebook for the actual training process as well if needed. \n\nThe testing pre-procerssing part is at the very bottom. \n                         \n'''\n\n# the import modules \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport glob\nimport csv\nimport datetime as dt\nimport platform\nimport os\nimport os.path\nfrom random import seed, random , randint # temporary score generator\nimport cv2\nimport keras\nfrom keras.preprocessing import image\nfrom keras.models import  load_model\nfrom matplotlib import pyplot as plt\nimport time\nimport sys\nfrom matplotlib.patches import Rectangle\n#from mtcnn.mtcnn import MTCNN\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom IPython.display import Image\n#Image(filename='test.png') \nimport math","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#face detection functions\n#for Yolo and dnn stuff\nmin_confidence=0.1 #0.1\nclasses = None\nCOLORS = None\n\n\ncascPath = \"/kaggle/input/haarxml/haarcascade_frontalface_default.xml\"\nsetupName = \"/kaggle/input/yolov3\" # Windows\nmodel = os.path.join(setupName, 'yolo.weights') #https://pjreddie.com/darknet/yolov2/\nconfig = os.path.join(setupName, 'yolo.cfg')\nlabels = os.path.join(setupName, 'labels.txt')\n\nnet = cv2.dnn.readNet(model, config)\n#face detection functions\n#---------------------------------------------------------------------\n#\n#   finds number of people in the image and returns the # found and the co-ordinates using caffee\n#   need to push this into Kaggle\n#---------------------------------------------------------------------\ndef findFaceCaffe (image, debug = 0):\n    #https://github.com/mmilovec/facedetectionOpenCV\n\n    net = cv2.dnn.readNetFromCaffe(\"/kaggle/input/caffestuff/proto.txt\", \"/kaggle/input/caffestuff/res10_300x300_ssd_iter_140000.caffemodel\")\n    #image = cv2.imread(imageName)\n    (h, w) = image.shape[:2]\n    blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0, (300, 300), (103.93, 116.77, 123.68))\n    net.setInput(blob)\n    detections = net.forward()\n    boxes = []\n\n    # loop over the detections\n    for i in range(0, detections.shape[2]):\n        # extract the confidence (i.e., probability) associated with the\n        # prediction\n        confidence = detections[0, 0, i, 2]\n\n        # filter out weak detections by ensuring the `confidence` is\n        # greater than the minimum confidence\n        if confidence > 0.4:\n            # compute the (x, y)-coordinates of the bounding box for the\n            # object\n            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n            (startX, startY, endX, endY) = box.astype(\"int\")\n            boxes.append([startX, startY, endX, endY])\n\n            # draw the bounding box of the face along with the associated\n            # probability\n            if debug:\n                text = \"{:.2f}%\".format(confidence * 100)\n                y = startY - 10 if startY - 10 > 10 else startY + 10\n                cv2.rectangle(image, (startX, startY), (endX, endY),\n                              (0, 0, 255), 2)\n                cv2.putText(image, text, (startX, y),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n\n    # show the output image\n    if debug:\n        cv2.imshow(\"Caffe faces found\", image)\n        cv2.waitKey(0)\n\n    return len(boxes), boxes\n\n'''\n    Function crops an image and does some error checking\n    Returns cropped version of img input \n'''\ndef CropImage (img, rect, debug = 0, name='cropped', dimension = 250):\n    \n    if debug: print('From inside CropImage function of Detectface.py module with dimension = ', dimension)\n    x, y, w, h = rect\n    if debug: print('Rect is  ',rect)\n    w = dimension\n    h = dimension\n    x1 = 0\n    x2 = 0\n    y1 = 0\n    y2 = 0\n\n    # cannot have the left or the top outside the original image\n    if (y - 0.25 * h < 0 ):\n        y1 = 0 #cannot have negative\n        y2 = h #need a square of the dimension = dimension\n    else:\n        y1 = y - 0.25 * h\n        y2 = y + 0.75 * h\n\n    if x - 0.25 * w < 0:\n        x1 = 0\n        x2 =  w #same reason as above for y\n    else:\n        x1 = x - 0.25 * w\n        x2 = x + 0.75 * w\n\n    # cannot have the bottom or right outside the original image\n\n    if y2 > img.shape[0] :\n        y2 = img.shape[0]\n        y1 = y2 - h\n\n    if x2 > img.shape[1] :\n        x2 = img.shape[1]\n        x1 = x2 - w\n\n    if debug: print('parameters for cropping are ' , y1, y2, x1,x2 , ' image shape is ', img.shape)\n    #crop_img = img[int(y - 0.25 * h):int(y + 0.75 * h), int(x - 0.25 * w):int(x + 0.75 * w)]\n    crop_img = img[int(y1):int(y2), int(x1):int(x2)]\n    if debug:\n        cv2.imshow(name, crop_img)\n\n    return crop_img\n\n'''\n    Function returns rectangles that bounds faces found in the image using Haar cascade\n'''\ndef findPersonsInImageHaar( image, debug = 0):\n    if debug: print('From inside findPersonsInImage function of HaarDetectface.py module')\n    # Create the haar cascade\n    #faceCascade = cv2.CascadeClassifier(cascPath)\n    faceCascade = cv2.CascadeClassifier(cascPath)\n\n    # Read the image\n    #image = cv2.imread(img)\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    count = 0\n    starttime = time.time()\n    faces = faceCascade.detectMultiScale(gray, 1.1, 5)\n    ftime = time.time() - starttime\n\n    count = len(faces)\n\n    if debug:\n        # Draw a rectangle around the faces\n        for (x, y, w, h) in faces:\n            #draw bigger rectangles\n            cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n\n\n    #print(count)\n    if debug: \n        print('Haar - it took: ' ,  ftime, \" to find {0} ,  faces!\".format(count))\n        cv2.imshow(\"Haar Faces found\", image)\n        cv2.waitKey(0)\n\n    return count, faces\n\ndef get_output_layers(net):\n    \n    layer_names = net.getLayerNames()\n    \n    output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n\n    return output_layers\n\n'''\n    Function returns rectangles that bounds faces found in the image usingYolo and dnn\n'''\ndef findPersonsInImageYolo( image, debug = 0):\n    global classes, COLORS\n\n    if debug: print('From inside findPersonsInImage function of YoloAIv3Still.py module')\n    \n\n    Width = image.shape[1]\n    Height = image.shape[0]\n    scale = 0.00392\n\n\n    with open(labels, 'r') as f:\n        classes = [line.strip() for line in f.readlines()]\n    #print(classes) # not necessary each time\n\n    COLORS = np.random.uniform(0, 255, size=(len(classes), 3))\n\n    #net = cv2.dnn.readNet(model, config)\n\n    starttime = time.time()\n\n    blob = cv2.dnn.blobFromImage(image, scale, (416,416), (0,0,0), True, crop=False)\n\n    net.setInput(blob)\n\n    outs = net.forward(get_output_layers(net))\n\n    class_ids = []\n    confidences = []\n    boxes = []\n    conf_threshold = 0.5\n    nms_threshold = 0.4\n\n    count = 0\n\n    for out in outs:\n        for detection in out:\n            scores = detection[5:]\n            class_id = np.argmax(scores)\n            confidence = scores[class_id]\n\n            #label = str(classes[class_id])\n            #if confidence > min_confidence: #old\n            if (confidence > min_confidence)  and (str(classes[class_id]) == 'person' ):\n                center_x = int(detection[0] * Width)\n                center_y = int(detection[1] * Height)\n                w = int(detection[2] * Width)\n                h = int(detection[3] * Height)\n                x = center_x - w / 2\n                y = center_y - h / 2\n                class_ids.append(class_id)\n                confidences.append(float(confidence))\n                boxes.append([x, y, w, h])\n                count = count + 1\n\n    totaltime = time.time() - starttime\n    if debug: print( 'YoloAIv3Still/findPersonsInImage ', count, ' persons were found by ',setupName, ' in ', totaltime, ' time')\n\n    if count == 0: return count, boxes\n\n    indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n\n    for i in indices:\n        i = i[0]\n        box = boxes[i]\n        x = box[0]\n        if x < 0: box[0] = 0\n        y = box[1]\n        if y < 0: box[1] = 0\n        w = box[2]\n        h = box[3]\n\n        '''\n        if debug: \n            #draw_prediction(image, class_ids[i], confidences[i], round(x), round(y), round(x+w), round(y+h))\n            #try cropping the relevant sections of the image\n            crop_img = image[round(y):round(y+h), round(x):round(x+w)]\n            cropImage = 'detectPerson' + str(i)\n            cv2.imshow(cropImage, crop_img)\n        '''\n\n    #dont forget why we came here\n    return count, boxes\n\n#####################################################################################\n#\n#   Takes as input a frame of a video and sends back a concatenated set cropped images\n#   that can be used instead of the frame for training\n#\n#   concat = 1 will mean vertical\n#\n#####################################################################################\ndef ConcatImages (frame, debug =0, name='concatenated', concat = 0, dims = 250, numFovea=1):\n\n    if debug: \n        print('1. From inside ConcatImages function of YoloAIv3Still.py module with dimension ', dims, 'debug flag ', debug)\n    img = []\n    boxes = []\n    dimension = dims\n    retcode = 99\n    number = 0\n\n    #cv2.imshow('wierd frame', frame)\n    #cv2.waitKey(0)\n\n    number, boxes = findFaceCaffe(frame, debug)\n    \n    #find the details from the frame by\n    if number == 0 : #lets try Yolo\n        number, boxes = findPersonsInImageYolo(frame, debug)\n    \n    if number == 0 : #lets try Haar\n        number, boxes = findPersonsInImageHaar(frame, debug)\n\n    if debug: print('2. YoloAI/Concat - Boxes are ', boxes, ' Number is ', number)\n\n    if number == 0:\n        if debug: cv2.imshow('YoloAI/Concat - could not find person with Yolo or Haar', frame)\n        return [], 0\n\n    concatenatedImage = np.zeros((dimension,dimension,3)) #initialize\n    \n    for i in range(len(boxes)):\n        if debug : print('3. YoloAI/Concat - In for loop inside ConcatImages in YolAIv3Still.py')\n        \n        #just get 1 people , 2 would mean\n        if i == numFovea:\n            if debug: print('4. ----YoloAI/Concat - More than 2 persons found - leaving with 2')\n            retcode = 4\n            break\n\n\n        #otherwise lets get things done\n        if debug: print('5. ----YoloAI/Concat - Box details are :', boxes[i -1])\n        img = []\n        img = CropImage (frame, boxes[i -1], debug, dimension = dimension)\n        if i == 0: #prior to doubling\n            concatenatedImage = img\n            retcode = 5\n            if debug:\n                print('6. ----YoloAI/Concat - Printing before concatenating images for first fovea sections - shape = ',img.shape  )\n                timee = str(time.time())\n                first = 'first'+ timee\n                cv2.imshow(first, img)\n                second = 'second' + timee\n                cv2.imshow(second, concatenatedImage)\n                #do the concatenation right here\n\n        else:\n            if (concatenatedImage.shape[0] == dimension) and (img.shape[0] == dimension):  # this won't allow concatenation\n                if concat == 0:\n                    #horizontal stack\n                    concatenatedImage = np.hstack((concatenatedImage, img))\n                    retcode = 6\n                    if debug:\n                        print('7.1. ----YoloAI/Concat - concatenating horizontaly')\n                else:\n                    # vertical stack\n                    concatenatedImage = np.vstack((concatenatedImage, img))\n                    retcode = 7\n                    if debug:\n                        print('7.1. ----YoloAI/Concat - concatenating vertically')\n            else:\n                #one of the components is not right - dont return anything\n                return [], 1\n\n\n        imgName = \"img\" + str(i)\n        frameName = name\n        \n\n    #what if there is only one person - then it throws everything off the skelter\n    #double it\n    if (len(boxes) == 1):\n        if debug:\n            extra = 'could this be it ?' + str(time.time())\n            print('8.YoloAI/Concat - One person found - doubling it, and now see the  img')\n            cv2.imshow(extra, concatenatedImage)\n\n        if numFovea == 1: # just ship the image back; no need to double or concatenate\n            if img.shape[0] == dimension :\n                return img, 2\n\n        else: #numFovea != 1\n            if (concatenatedImage.shape[0] == dimension) and (img.shape[0] == dimension):  # this won't allow concatenation\n                if concat == 0: concatenatedImage = np.hstack((concatenatedImage, img))\n                else: concatenatedImage = np.vstack((concatenatedImage, img))\n\n            else: #didn't meet criteria\n                return [], 3\n\n    if debug:\n        cv2.waitKey(0)\n\n    \n    return concatenatedImage, retcode\n\ndef dummyCropImage(): \n    print('Haha')\n    image = cv2.imread(\"/kaggle/input/junkimages/2women.JPG\")\n    faces, n = findPersonsInImageHaar(image, debug = 0)\n    print(n, ' faces found with Haar with following details', faces)\n    \ndef dummyYolov3AITesting (): # this worked well\n    print('hello dumy Yolo')\n    image = cv2.imread(\"/kaggle/input/junkimages1/2women.JPG\")\n    result, retcode = ConcatImages (image, debug=0, dims = 175)\n    \n    if len(result) : print ('Great', result.shape, 'Return code:', retcode)\n    else: print('disaster', 'Return code:', retcode)\n    \n    \n    \n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Frames related functions\n'''\n\n    Function rescales a given frame by a percent \n    \n'''  \ndef rescale_frame(frame, percent=75):\n    width = int(frame.shape[1] * percent/ 100)\n    height = int(frame.shape[0] * percent/ 100)\n    dim = (width, height)\n    return cv2.resize(frame, dim, interpolation =cv2.INTER_AREA)\n\n'''\n\n    Function returns howMany frames from a given Video mp4 with some options\n    returns empty arrays if for any reason VideoCapture, or .get does crazy things\n    \n'''\ndef CaptureNRadomFrames(videoFile, howMany, debug = 0, seedOption = 1, gap=45):\n\n    #frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n    randomFrames = []\n    frameNums = []\n    if seedOption: seed(1)\n    else: seed(time.clock())\n    \n    if debug: print('From inside CaptureNRadomFrames function of VideoFunctions module')\n    \n    cap =  cv2.VideoCapture(videoFile)\n    frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n\n    if debug: print('Frame count for video', videoFile, 'is ',frame_count)\n    #print('Frame count for video', videoFile, 'is ', frame_count)\n    \n    if frame_count == 0: return randomFrames, frameNums\n\n    frame_number = randint(0, frame_count -1)\n    \n    for _ in range(howMany):\n        cap.set(1, frame_number)\n        res, frame = cap.read()\n        if res:\n            frame50 = rescale_frame(frame, percent=50)\n            #YoloAIv3Still.findPersonsInImage(frame50, 2)\n            randomFrames.append(frame50)\n            frameNums.append(frame_number)\n        #frame_number = randint(frame_number, frame_count -1)\n        frame_number = int((frame_number + gap ) % frame_count)\n\n        # When everything done, release the capture\n    cap.release()\n    cv2.destroyAllWindows()\n\n    return randomFrames, frameNums \n\n'''\nThis tested well \n'''\ndef dummyCaptureNRadomFrames ():\n    print('Haha')\n    '''\n        testing for CaptureNRadomFrames\n    '''\n    #lets check out one video and the random frame\n    videoFile = '/kaggle/input/deepfake-detection-challenge/test_videos/uhakqelqri.mp4'\n    RandomFrames, frameNos = CaptureNRadomFrames(videoFile, 5,1,seedOption = 0)\n    print('how many :', len(RandomFrames), ' frame numbers :', frameNos)\n    plt.figure(figsize=(20,10))\n    columns = 5\n    for i in range (len(RandomFrames)):\n        pic = str(i)\n        #cv2.imshow(pic, RandomFrames[i-1] )\n        plt.subplot(len(RandomFrames) / columns + 1, columns, i + 1)\n        plt.imshow(RandomFrames[i-1])\n        plt.title(pic)\n        #plt.figure()\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Analyse video functions - needs ScoreConcat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Video prep and scoring modules\ndef ScoreVideo(videoFile, SavePath ,debugValue = 0):\n\n    # first get random frames from the video\n    debug = debugValue\n\n    if debug:\n        print('From inside ScoreVideo function of AnalyzeVideo.py module')\n        print()\n    \n    RandomFrames = []\n    frameNos = []\n    boxes = []\n    originalWorkingDirectory = os.getcwd()\n    score = 0.99 # unique initialization value\n\n\n    numFrames = 5\n    perFrameMaxFovea = 1 #original default 2\n    heightConcat = 175\n    widthConcat = heightConcat * perFrameMaxFovea\n    ww = heightConcat * perFrameMaxFovea * numFrames\n    masterShape = (heightConcat, ww , 3)\n    masterConcat = []\n\n    if debug:\n        print('AnalyzeVideo/ScoreVideo - the master shape should be ', masterShape)\n        print()\n\n    RandomFrames, frameNos = CaptureNRadomFrames(videoFile, numFrames, debug)\n    if len(RandomFrames) == 0: #no video in container \n        if debug: print('AnalyzeVideo/ScoreVideo - Could not get any frames fom this videofile :' , videoFile)\n        return score\n\n    if len(RandomFrames) != numFrames:  # then lets try again\n        if debug: print('AnalyzeVideo/ScoreVideo Could not get ',numFrames, ' frames fom this videofile :', videoFile, '; trying again')\n        #reinitialise\n        RandomFrames,  frameNos = [], []\n        #and try again\n        RandomFrames, frameNos = CaptureNRadomFrames(videoFile, numFrames, debug, seedOption = 0)\n        if len(RandomFrames) == 0:\n            return score  # no point in going further \n\n    if debug:print('ScoreVideo/AnalyzeVideo ; Frames are :', frameNos) #[223, 228, 233, 238], [180, 185, 190, 195],[180, 185, 190, 195]\n\n    masterConcat = []\n\n    # For each of the RandomFrames, extract fovea regions in the frames and concatenate\n    concatenatedImage = []\n    retcode = 999\n\n    # sometimes we are not getting a full complement of numFrames frames\n\n\n\n    for i in range(len(RandomFrames)):\n\n        concatenatedImage, retcode = ConcatImages (RandomFrames[i], debug, str(frameNos[i]), dims = heightConcat, numFovea=perFrameMaxFovea)\n\n        #sometimes the image returned is no good - (250, 0, 3)\n        if len(concatenatedImage) == 0 :\n            if debug: print('----------------------------> AnalyzeVideo/ScoreVideo Disaster - no good, trying again, Retcode :', retcode)\n            #lets try again\n            concatenatedImage, retcode = ConcatImages(RandomFrames[i], debug, str(frameNos[i]), dims = heightConcat, numFovea=perFrameMaxFovea)\n\n        if len(concatenatedImage) == 0:\n            \n            if debug: \n                print()\n                print(videoFile, ' ----> AnalyzeVideo/ScoreVideo ----------Disaster----No images returned--------------skip this video, Retcode :', retcode)\n                print()\n            return 0.001  # Could not find any faces - most likely real - Change #1\n\n        #change this to just check the shape and not the length - anyways if len is 0, there is no need to check shape\n        #and would be caught before\n        #if len(concatenatedImage) and (concatenatedImage.shape == (heightConcat, widthConcat, 3)): # as if we cannot find any person, we return a null array whose len is 0\n        if (concatenatedImage.shape == (heightConcat, widthConcat, 3)):\n            #masterConcat = masterConcat + concatenatedImage\n            if debug:\n                print(' ----> AnalyzeVideo/ScoreVideo ', frameNos[i], 'image shape is :', concatenatedImage.shape, '  Retcode :', retcode)\n            if i == 0:\n                masterConcat = concatenatedImage\n            else:\n                masterConcat = np.hstack((masterConcat, concatenatedImage))\n\n            if debug:\n                cv2.imshow(' ----> AnalyzeVideo/ScoreVideo  master concat', masterConcat)\n                print(' ----> AnalyzeVideo/ScoreVideo MasterConcat shape is ', masterConcat.shape)\n            if np.array_equal(masterShape , masterConcat.shape) :\n                if debug:\n                    print()\n                    print('------------------------------- >>>AnalyzeVideo/ScoreVideo   --- lets celebrate , Retcode :', retcode)\n                    print()\n                \n\n        else:\n            if debug:\n                print()\n                print('i is ', i, ' image shape is ', concatenatedImage.shape)\n                print(videoFile, ' ----> AnalyzeVideo/ScoreVideo ---Disaster------No person found-------skip this video, Retcode :', retcode)\n                print()\n            return 0.001 # Could not find any faces - most likely real Change #2\n\n\n    cv2.imwrite('temp.jpg', masterConcat)\n    score = ScoreImage('temp.jpg', widthConcat, heightConcat, 0)\n    \n    if debug: print('--------------> AnlyzeVideo/ScoreVideo ', videoFile, ' just scored ', score)\n\n    #lets cleanup\n    os.remove('/kaggle/working/temp.jpg')\n    \n    if debug: cv2.waitKey(0)\n\n    return score\n\ndef TestScoreVideo():\n    print('ha ha testing testing')\n    videoFile = '/kaggle/input/deepfake-detection-challenge/test_videos/uhakqelqri.mp4'\n    score = ScoreVideo(videoFile, \"\", 0)\n\n    print('Score for ', videoFile, ' is ', score)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ScoreConcat functions\n\nfilepath = \"/kaggle/input/mymodel/bestModel.h5\"\nmymodel = load_model(filepath)\n\n\n'''\n    Function prepares image prior to using it for prediction\n'''\ndef prepare_image(file, widthConcat,height):\n    img_path = ''\n    img = image.load_img(img_path + file, target_size=(widthConcat,height))\n    img_array = image.img_to_array(img)\n    img_array_expanded_dims = np.expand_dims(img_array, axis=0)\n    return keras.applications.mobilenet.preprocess_input(img_array_expanded_dims)\n\n'''\n    Function uses the image prepared by function above in conjunction with the\n    model to run a prediction\n'''\ndef ScoreImage (file, widthConcat,height, debug =0):\n    score = 0.5\n    n = []\n    preprocessed_image = prepare_image(file, widthConcat,height)\n    n = mymodel.predict(preprocessed_image)\n    if debug: print('Real prediction is :', predictions)\n\n    score = n[0][0] #fake\n    \n    # Leave the scores alone - Change 3\n    \n    return round(score, 3)\n    \n\ndef testScoreImage (): #works good\n    perFrameMaxFovea = 1\n    height = 175\n    widthConcat = height * perFrameMaxFovea\n    score = 0\n    ImageFilename2 =\"/kaggle/input/realfake/fake.jpg\"\n    score = ScoreImage(ImageFilename2, widthConcat,height, 0)\n    print('Image ', ImageFilename2, ' has a score of ', score)\n    \n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n# added the findfacewithCaffe function - check it out\n\n'''\n    before submission \n        \n        1. remove the if count > 7...\n        2. uncomment #collectdata(fileName, os.path.basename(videoName), score)\n        3. remove the print(os.path.basename(videoName), score)\n\n'''\n    \ndef main():\n    \n    count = 0\n    logloss = 0 \n    fileName = 'submission.csv'\n    filenames = []\n    VideoFilenames = []\n    ScoreArray = []\n    \n    starttime = time.time() # Start the timer\n    \n    for dirname, _, filenames in os.walk('/kaggle/input/deepfake-detection-challenge/test_videos/'):\n        for videoName in filenames:\n            score = 0.5\n            VideoFullPath = os.path.join(dirname, videoName)\n            try :\n\n                #Analyze and score - the ScoreVideo function now checks for the SavePath parameter\n                score = ScoreVideo(VideoFullPath, \"\", 0)\n                print(count, os.path.basename(videoName), score)\n                count = count + 1\n                \n\n            except:\n                # predict something for that videoFile\n                score = 0.99\n                print(videoName, ' Main: Could not Analyze video  ')\n                count = count + 1\n            \n            #store the videoname and the score in arrays to use with pandas\n            VideoFilenames.append(os.path.basename(videoName))\n            ScoreArray.append(score)\n            \n            #if count > 20 : break #testing testing\n            #logloss = logloss + score * math.log(score) + (1 - score) * math.log(1 - score)\n            if score > 0 and score < 1.0: logloss = logloss + score * math.log(score) + (1 - score) * math.log(1 - score)\n    \n    #do the pandas and csv thing now - https://www.kaggle.com/dansbecker/submitting-from-a-kernel -  much easier \n    my_submission = pd.DataFrame({'filename': VideoFilenames, 'label': ScoreArray})\n    # you could use any filename. We choose submission here\n    my_submission.to_csv(fileName, index=False)\n            \n    totalTime = time.time() - starttime # Stop the timer\n    logloss = - 1 / (count ) * logloss\n    \n    \n    print('Total time taken to process ' , count, ' video files is ', totalTime, ' seconds')\n    print ('Logloss is ', logloss , ' for ', count, ' videos')\n    \nif __name__ == '__main__':\n    main()\n    \n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!more *csv | wc -l\n#!more *csv\n#!ls *csv\n#!more submission.csv \n!ls *csv\n#!more submission.csv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nPre-processing for Training  \n\n'''\nimport zipfile\nimport json\nimport pprint\nfrom glob import glob\nimport cv2\nimport numpy as np\nimport os\n\ndirectory = \"/media/saibal/Seagate Backup Plus Drive/DeepFakeData\" # this is where I stored the training videos Kaggle provided\n\ndef ppmain():\n\n    #zipfileName = 'dfdc_train_part_06' + '.zip'\n    #zipfileList = glob('dfdc*.zip') #works\n    zipfileList = glob(directory + '/' + 'dfdc*.zip')  # works\n\n    #zipfileList = glob('dfdc*.zip')  # works\n    \n    try:\n        \n        for zipfileName in zipfileList:\n            print('')\n            print('')\n            print('For zip directory :', zipfileName)\n            count = 0\n            junk = 0\n            json_data = {}\n            with zipfile.ZipFile(zipfileName) as z: # opening the zip file using 'zipfile.ZipFile' class\n\n                listOfiles = z.namelist()\n                \n                #lets get the metadata stuff out of the way first\n                for filename in listOfiles:\n                    if filename.endswith('json'):\n                        print('--->JSON Filename is : ', filename)\n\n                        #extract the path to the filename here for later\n                        #use with the videos\n                        pathVar = os.path.dirname(filename)\n                        # read the file\n                        with z.open(filename) as f:\n                            data = f.read()\n                            json_data = json.loads(data)\n                            #pprint.pprint(json_data)\n\n                                \n                #lets deal with video now\n                for filename in listOfiles:\n                    if filename.endswith('mp4'):\n                        z.extract(filename)\n                        #what is the video label - Fake or real ?\n                        _, tail = os.path.split(filename)\n                        #feed the filename only to get label\n                        label = json_data[tail]['label']\n                        #print('--------------->Filename is : ', tail, ' and label =', label)\n                        StoreConcatIn = ''\n                        if label.find(\"FAKE\") != -1 : #meaning it  said FAKE\n                            StoreConcatIn = \"TL/data/train/fake\"\n                            print('fake - store ',tail,' in ', StoreConcatIn)\n\n                        else:\n                            StoreConcatIn = \"TL/data/train/real\"\n                            print('real - store ',tail,' in ', StoreConcatIn)\n\n                        n = ScoreVideo(filename, StoreConcatIn, 0)\n\n\n                        #now lets do the video reads and representation business - just for debugging\n                        '''\n                        cap =  cv2.VideoCapture(filename)\n                        while True:\n                            ret, frame = cap.read()\n                            #print(ret, frame)\n                            if ret == True:\n                                cv2.imshow('Video', frame)\n                                if cv2.waitKey(1) & 0xFF == ord('q'):\n                                    break\n                            else:\n                                #print('frame is None')\n                                break\n\n                        cap.release()\n                        cv2.destroyAllWindows()\n                        '''\n                        \n                    else:\n                        print('what file is this ?')\n                            \n                                \n                        \n                    \n\n        #list all file names\n        '''\n        for elem in listOfiles:\n            print(elem)\n        '''\n\n            \n\n        #list how many files are there\n        print('')\n        #print(len(listOfiles))\n            \n    except zipfile.BadZipFile: # if the zip file has any errors then it prints the error message which you wrote under the 'except' block\n        print('Error: Zip file is corrupted')\n\n#if __name__ == '__main__':\n    #ppmain()\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}