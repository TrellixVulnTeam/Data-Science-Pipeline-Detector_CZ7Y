{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.simplefilter(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-01T03:36:34.262879Z","iopub.execute_input":"2022-02-01T03:36:34.263313Z","iopub.status.idle":"2022-02-01T03:36:34.292344Z","shell.execute_reply.started":"2022-02-01T03:36:34.263192Z","shell.execute_reply":"2022-02-01T03:36:34.291512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os, gc\nimport random\nimport datetime\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom tqdm.auto import tqdm\nfrom lightgbm import LGBMRegressor\nfrom sklearn.preprocessing import KBinsDiscretizer\n\nimport gresearch_crypto\n\nimport pickle\n\ndef pickle_dump(obj, path):\n    with open(path, mode=\"wb\") as f:\n        pickle.dump(obj,f)\n\ndef pickle_load(path):\n    with open(path, mode=\"rb\") as f:\n        data = pickle.load(f)\n        return data\n\nTRAIN_CSV = '/kaggle/input/g-research-crypto-forecasting/train.csv'\nASSET_DETAILS_CSV = '/kaggle/input/g-research-crypto-forecasting/asset_details.csv'\n\nSEED = 2021\n\nREMOVE_LB_TEST_OVERLAPPING_DATA = True\nTRAIN_FLAG = True","metadata":{"execution":{"iopub.status.busy":"2022-02-01T03:36:34.293697Z","iopub.execute_input":"2022-02-01T03:36:34.294832Z","iopub.status.idle":"2022-02-01T03:36:36.664635Z","shell.execute_reply.started":"2022-02-01T03:36:34.294753Z","shell.execute_reply":"2022-02-01T03:36:36.663531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n#         else:\n#             df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-02-01T03:36:36.666557Z","iopub.execute_input":"2022-02-01T03:36:36.666995Z","iopub.status.idle":"2022-02-01T03:36:36.683614Z","shell.execute_reply.started":"2022-02-01T03:36:36.666961Z","shell.execute_reply":"2022-02-01T03:36:36.682663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if TRAIN_FLAG:\n    df_train = pd.read_csv(TRAIN_CSV)\n    df_train = df_train.drop([\"Count\", \"Open\", \"Volume\", \"VWAP\"], axis=1)\n    df_train['datetime'] = pd.to_datetime(df_train['timestamp'], unit='s')\n    \n    df_test = df_train[df_train['datetime'] >= '2021-06-13 00:00:00']\n    df_train = df_train[df_train['datetime'] < '2021-06-13 00:00:00']\n    \n    df_train = df_train.drop([\"datetime\"], axis=1)\n    df_test  = df_test.drop([\"datetime\"], axis=1)\n    df_train = reduce_mem_usage(df_train)\n    \n    df_train.reset_index(inplace=True, drop=True)\n    df_test.reset_index(inplace=True, drop=True)\n    df_train.head()\n    \n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-01T03:42:32.239851Z","iopub.execute_input":"2022-02-01T03:42:32.241589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_feature(tmp_df, TARGET=True):\n    tmp_df[\"ror1\"]         = tmp_df.groupby(\"Asset_ID\")[\"Close\"].pct_change(1)\n    tmp_df[\"ror1_shift15\"] = tmp_df.groupby(\"Asset_ID\")[\"ror1\"].shift(15)\n    tmp_df[\"ror1_shift30\"] = tmp_df.groupby(\"Asset_ID\")[\"ror1\"].shift(30)\n    tmp_df[\"ror1_shift60\"] = tmp_df.groupby(\"Asset_ID\")[\"ror1\"].shift(60)\n    tmp_df[\"ror4\"] = tmp_df.groupby(\"Asset_ID\")[\"Close\"].pct_change(4)\n    tmp_df[\"ror5\"] = tmp_df.groupby(\"Asset_ID\")[\"ror4\"].shift(1)\n    tmp_df[\"ror10\"] = tmp_df.groupby(\"Asset_ID\")[\"Close\"].pct_change(10)\n    tmp_df[\"ror15\"] = tmp_df.groupby(\"Asset_ID\")[\"ror10\"].shift(5)\n    tmp_df[\"ror15_raw\"] = tmp_df.groupby(\"Asset_ID\")[\"Close\"].pct_change(15)\n    tmp_df[\"ror15_shift60\"] = tmp_df.groupby(\"Asset_ID\")[\"ror15_raw\"].shift(60)\n    tmp_df[\"ror15_shift240\"] = tmp_df.groupby(\"Asset_ID\")[\"ror15_raw\"].shift(240)\n    tmp_df[\"ror15_shift720\"] = tmp_df.groupby(\"Asset_ID\")[\"ror15_raw\"].shift(720)\n    tmp_df[\"ror15_shift1080\"] = tmp_df.groupby(\"Asset_ID\")[\"ror15_raw\"].shift(1080)\n    tmp_df[\"ror15_shift1440\"] = tmp_df.groupby(\"Asset_ID\")[\"ror15_raw\"].shift(1440)\n    tmp_df[\"ror45\"] = tmp_df.groupby(\"Asset_ID\")[\"Close\"].pct_change(45)\n    tmp_df[\"ror60\"] = tmp_df.groupby(\"Asset_ID\")[\"ror45\"].shift(15)\n    tmp_df[\"ror60_raw\"] = tmp_df.groupby(\"Asset_ID\")[\"Close\"].pct_change(60)\n    tmp_df[\"ror60_shift240\"] = tmp_df.groupby(\"Asset_ID\")[\"ror60_raw\"].shift(240)\n    tmp_df[\"ror60_shift720\"] = tmp_df.groupby(\"Asset_ID\")[\"ror60_raw\"].shift(720)\n    tmp_df[\"ror60_shift1080\"] = tmp_df.groupby(\"Asset_ID\")[\"ror60_raw\"].shift(1080)\n    tmp_df[\"ror60_shift1440\"] = tmp_df.groupby(\"Asset_ID\")[\"ror60_raw\"].shift(1440)\n    tmp_df[\"ror180\"] = tmp_df.groupby(\"Asset_ID\")[\"Close\"].pct_change(180)\n    tmp_df[\"ror240\"] = tmp_df.groupby(\"Asset_ID\")[\"ror180\"].shift(60)\n    tmp_df[\"ror240_raw\"] = tmp_df.groupby(\"Asset_ID\")[\"Close\"].pct_change(240)\n    tmp_df[\"ror240_shift720\"] = tmp_df.groupby(\"Asset_ID\")[\"ror240_raw\"].shift(720)\n    tmp_df[\"ror240_shift1080\"] = tmp_df.groupby(\"Asset_ID\")[\"ror240_raw\"].shift(1080)\n    tmp_df[\"ror240_shift1440\"] = tmp_df.groupby(\"Asset_ID\")[\"ror240_raw\"].shift(1440)\n    tmp_df[\"ror480\"] = tmp_df.groupby(\"Asset_ID\")[\"Close\"].pct_change(480)\n    tmp_df[\"ror720\"] = tmp_df.groupby(\"Asset_ID\")[\"ror480\"].shift(240)\n    tmp_df[\"ror720_raw\"] = tmp_df.groupby(\"Asset_ID\")[\"Close\"].pct_change(720)\n    tmp_df[\"ror1440\"] = tmp_df.groupby(\"Asset_ID\")[\"ror720_raw\"].shift(720)\n    tmp_df[\"hh15\"] = tmp_df.groupby(\"Asset_ID\")[\"High\"].rolling(15).max().reset_index().sort_values(\"level_1\").set_index(\"level_1\")[\"High\"]\n    tmp_df[\"hh60\"] = tmp_df.groupby(\"Asset_ID\")[\"High\"].rolling(60).max().reset_index().sort_values(\"level_1\").set_index(\"level_1\")[\"High\"]\n    tmp_df[\"hh240\"] = tmp_df.groupby(\"Asset_ID\")[\"High\"].rolling(240).max().reset_index().sort_values(\"level_1\").set_index(\"level_1\")[\"High\"]\n    tmp_df[\"hh1440\"] = tmp_df.groupby(\"Asset_ID\")[\"High\"].rolling(1440).max().reset_index().sort_values(\"level_1\").set_index(\"level_1\")[\"High\"]\n    tmp_df[\"ll15\"] = tmp_df.groupby(\"Asset_ID\")[\"Low\"].rolling(15).min().reset_index().sort_values(\"level_1\").set_index(\"level_1\")[\"Low\"]\n    tmp_df[\"ll60\"] = tmp_df.groupby(\"Asset_ID\")[\"Low\"].rolling(60).min().reset_index().sort_values(\"level_1\").set_index(\"level_1\")[\"Low\"]\n    tmp_df[\"ll240\"] = tmp_df.groupby(\"Asset_ID\")[\"Low\"].rolling(240).min().reset_index().sort_values(\"level_1\").set_index(\"level_1\")[\"Low\"]\n    tmp_df[\"ll1440\"] = tmp_df.groupby(\"Asset_ID\")[\"Low\"].rolling(1440).min().reset_index().sort_values(\"level_1\").set_index(\"level_1\")[\"Low\"]\n    tmp_df[\"dip15\"] = tmp_df[\"Close\"]/tmp_df[\"hh15\"] - 1\n    tmp_df[\"dip60\"] = tmp_df[\"Close\"]/tmp_df[\"hh60\"] - 1\n    tmp_df[\"dip240\"] = tmp_df[\"Close\"]/tmp_df[\"hh240\"] - 1\n    tmp_df[\"dip1440\"] = tmp_df[\"Close\"]/tmp_df[\"hh1440\"] - 1\n    tmp_df[\"rip15\"] = tmp_df[\"Close\"]/tmp_df[\"ll15\"] - 1\n    tmp_df[\"rip60\"] = tmp_df[\"Close\"]/tmp_df[\"ll60\"] - 1\n    tmp_df[\"rip240\"] = tmp_df[\"Close\"]/tmp_df[\"ll240\"] - 1\n    tmp_df[\"rip1440\"] = tmp_df[\"Close\"]/tmp_df[\"ll1440\"] - 1\n    \n    x_feats = [\"ror1\", \"ror1_shift15\", \"ror1_shift30\", \"ror1_shift60\", \"ror4\", \"ror5\", \"ror10\", \"ror15\", \"ror15_raw\",\n               \"ror15_shift60\", \"ror15_shift240\", \"ror15_shift720\", \"ror15_shift1080\", \"ror15_shift1440\", \"ror45\",\n               \"ror60\", \"ror60_raw\", \"ror60_shift240\", \"ror60_shift720\", \"ror60_shift1080\", \"ror60_shift1440\",\n               \"ror180\", \"ror240\", \"ror240_raw\", \"ror240_shift720\", \"ror240_shift1080\", \"ror240_shift1440\",\n               \"ror480\", \"ror720\", \"ror720_raw\", \"ror1440\",\n               \"dip15\", \"dip60\", \"dip240\", \"dip1440\", \"rip15\", \"rip60\", \"rip240\", \"rip1440\"]\n    \n    if TARGET:\n        feats = [\"Target\"]\n    else:\n        feats = []\n    feats.extend([\"timestamp\", \"Asset_ID\"])\n    feats.extend(x_feats)\n    \n    return tmp_df[feats]","metadata":{"execution":{"iopub.status.busy":"2022-02-01T03:37:59.690448Z","iopub.status.idle":"2022-02-01T03:37:59.690812Z","shell.execute_reply.started":"2022-02-01T03:37:59.69063Z","shell.execute_reply":"2022-02-01T03:37:59.690648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_dataframe_demean(df, TARGET=True, TRAIN=True):\n    # 特徴量\n    df = get_feature(df, TARGET)\n    if TRAIN:\n        gc.collect()\n    \n    x_feats = [\"ror1\", \"ror1_shift15\", \"ror1_shift30\", \"ror1_shift60\", \"ror4\", \"ror5\", \"ror10\", \"ror15\", \"ror15_raw\",\n               \"ror15_shift60\", \"ror15_shift240\", \"ror15_shift720\", \"ror15_shift1080\", \"ror15_shift1440\", \"ror45\",\n               \"ror60\", \"ror60_raw\", \"ror60_shift240\", \"ror60_shift720\", \"ror60_shift1080\", \"ror60_shift1440\",\n               \"ror180\", \"ror240\", \"ror240_raw\", \"ror240_shift720\", \"ror240_shift1080\", \"ror240_shift1440\",\n               \"ror480\", \"ror720\", \"ror720_raw\", \"ror1440\",\n               \"dip15\", \"dip60\", \"dip240\", \"dip1440\", \"rip15\", \"rip60\", \"rip240\", \"rip1440\"]\n    \n    # マーケットリターンを計算（単純平均）\n    tmp = df.groupby(\"timestamp\").mean()\n    tmp = tmp[x_feats]\n    tmp = tmp.add_suffix(\"_market\")\n    tmp.reset_index(inplace=True)\n    \n    if TRAIN:\n        gc.collect()\n    \n    # マーケット項を追加\n    for f in x_feats:\n        df = df.merge(tmp[[\"timestamp\", f+\"_market\"]], on=\"timestamp\", how=\"left\")\n        df[f] = df[f] - df[f+\"_market\"]\n        df = df.drop([f+\"_market\"], axis=1)\n        gc.collect()\n    \n    if TRAIN:\n        del tmp\n        gc.collect()\n    \n    # ビニング\n    if TRAIN:\n        df = df.dropna()\n        for feat in x_feats:\n            bins[feat] = KBinsDiscretizer(n_bins=10, encode=\"ordinal\")\n            bins[feat].fit(df[[feat]])\n            df[feat] = bins[feat].transform(df[[feat]])\n            #pickle_dump(bins[feat], f\"../input/gresearch/bins_model/bins_{feat}.pickle\")\n    else:\n        df = df.replace([-np.inf, np.inf], np.nan).fillna(0)\n        for feat in x_feats:\n            df[feat] = bins[feat].transform(df[[feat]])\n    \n    if TRAIN:\n        gc.collect()\n    \n    # 銘柄数がアンマッチなtimestampを削除\n    #tmp = df.groupby(\"timestamp\").count()\n    #reject_timestamp = [t for t in tmp[tmp[\"Asset_ID\"]!=14].index]\n    #df = df[~df[\"timestamp\"].isin(reject_timestamp)]\n    #\n    #if TRAIN:\n    #    gc.collect()\n    \n    # ピボット\n    df.set_index(\"timestamp\", inplace=True)\n    df2 = df[df[\"Asset_ID\"]==1].copy()  # Bitcoin\n    df2 = df2.drop([\"Asset_ID\"], axis=1)\n    df2 = df2.add_suffix(\"_1\")\n    for i in range(14):\n        if i == 1:\n            continue\n        tmp_df = df[df[\"Asset_ID\"]==i].copy()\n        tmp_df = tmp_df.drop([\"Asset_ID\"], axis=1)\n        tmp_df = tmp_df.add_suffix(\"_\"+str(i))\n        df2 = df2.join(tmp_df)\n    \n    df2.reset_index(inplace=True)\n    df = df2.copy()\n    \n    if TRAIN:\n        del df2, tmp_df\n        gc.collect()\n    \n    # 時刻特徴量\n    df[\"datetime\"] = df[\"timestamp\"].apply(lambda x: datetime.datetime.fromtimestamp(x))\n    df[\"hour\"]   = df[\"datetime\"].dt.hour\n    df[\"minute\"] = df[\"datetime\"].dt.minute\n    \n    df[\"isFunding\"] = df[\"hour\"]%4\n    df[\"is00\"] = 0\n    df[\"is05\"] = 0\n    df[\"is15\"] = 0\n    df[\"is30\"] = 0\n    \n    df.loc[df[\"minute\"]==0, \"is00\"] = 1\n    df.loc[df[\"minute\"].isin([5, 10, 20, 25, 35, 40, 50, 55]), \"is05\"] = 1\n    df.loc[df[\"minute\"].isin([15, 45]), \"is15\"] = 1\n    df.loc[df[\"minute\"]==30, \"is30\"] = 1\n    \n    if TRAIN:\n        gc.collect()\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-02-01T03:37:59.692823Z","iopub.status.idle":"2022-02-01T03:37:59.693411Z","shell.execute_reply.started":"2022-02-01T03:37:59.693205Z","shell.execute_reply":"2022-02-01T03:37:59.693227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 訓練\nbins = {}\nmodels = {}\n\nif TRAIN_FLAG:\n    df_train = get_dataframe_demean(df_train, TARGET=True, TRAIN=True)\n    df_train = df_train.drop([\"datetime\"], axis=1)\n    df_train = reduce_mem_usage(df_train)\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-01T03:37:59.694667Z","iopub.status.idle":"2022-02-01T03:37:59.695563Z","shell.execute_reply.started":"2022-02-01T03:37:59.695337Z","shell.execute_reply":"2022-02-01T03:37:59.695362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targets = [f for f in df_train.columns if \"Target\" in f]\nx_feats = [f for f in df_train.columns if f not in [\"timestamp\", \"datetime\", \"hour\", \"minute\"]]\nx_feats = [f for f in x_feats if f not in targets]\n\nif TRAIN_FLAG:\n    for i in tqdm(range(14)):\n        models[i] = LGBMRegressor(max_depth=4, learning_rate=0.01, num_leaves=20, n_estimators=1000, n_jobs=-1, colsample_bytree=0.1)\n        models[i].fit(df_train[x_feats], df_train[f\"Target_{i}\"])\n        #pickle_dump(models[i], f\"../input/gresearch/main_model/model_{i}.pickle\")\n        gc.collect()","metadata":{"execution":{"iopub.status.idle":"2022-01-31T07:54:28.441583Z","shell.execute_reply.started":"2022-01-31T07:36:09.171182Z","shell.execute_reply":"2022-01-31T07:54:28.440825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if TRAIN_FLAG:\n    df_test = get_dataframe_demean(df_test, TARGET=True, TRAIN=False)\n    gc.collect()\n    for i in tqdm(range(14)):\n        df_test[f\"pred_{i}\"] = models[i].predict(df_test[x_feats])","metadata":{"execution":{"iopub.status.busy":"2022-01-31T07:54:28.443108Z","iopub.execute_input":"2022-01-31T07:54:28.444061Z","iopub.status.idle":"2022-01-31T07:57:32.588675Z","shell.execute_reply.started":"2022-01-31T07:54:28.444015Z","shell.execute_reply":"2022-01-31T07:57:32.587617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if TRAIN_FLAG:\n    sm = 0\n    for i in range(14):\n        sm += df_test[f\"pred_{i}\"].corr(df_test[f\"Target_{i}\"])\n        print(df_test[f\"pred_{i}\"].corr(df_test[f\"Target_{i}\"]))\n    \n    sm = sm/14\n    print(f\"EW_AVE: {sm}\")","metadata":{"execution":{"iopub.status.busy":"2022-01-31T08:01:11.728715Z","iopub.execute_input":"2022-01-31T08:01:11.729101Z","iopub.status.idle":"2022-01-31T08:01:11.829887Z","shell.execute_reply.started":"2022-01-31T08:01:11.72906Z","shell.execute_reply":"2022-01-31T08:01:11.828793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_dataframe_demean_for_loop(tmp_df):\n    assets = [f for f in tmp_df.loc[tmp_df[\"timestamp\"]==tmp_df[\"timestamp\"].max(), \"Asset_ID\"]]\n    x_dict = {}\n    \n    feats = [\"ror1\", \"ror1_shift15\", \"ror1_shift30\", \"ror1_shift60\", \"ror4\", \"ror5\", \"ror10\", \"ror15\", \"ror15_raw\",\n             \"ror15_shift60\", \"ror15_shift240\", \"ror15_shift720\", \"ror15_shift1080\", \"ror15_shift1440\", \"ror45\",\n             \"ror60\", \"ror60_raw\", \"ror60_shift240\", \"ror60_shift720\", \"ror60_shift1080\", \"ror60_shift1440\",\n             \"ror180\", \"ror240\", \"ror240_raw\", \"ror240_shift720\", \"ror240_shift1080\", \"ror240_shift1440\",\n             \"ror480\", \"ror720\", \"ror720_raw\", \"ror1440\",\n             \"dip15\", \"dip60\", \"dip240\", \"dip1440\", \"rip15\", \"rip60\", \"rip240\", \"rip1440\"]\n    \n    params = [[1, 0], [1, 15], [1, 30], [1, 60], [4, 0], [4, 1], [10, 0], [10, 5], [15, 0],\n              [15, 60], [15, 240], [15, 720], [15, 1080], [15, 1440], [45, 0],\n              [45, 15], [60, 0], [60, 240], [60, 720], [60, 1080], [60, 1440],\n              [180, 0], [180, 60], [240, 0], [240, 720], [240, 1080], [240, 1440],\n              [480, 0], [480, 240], [720, 0], [720, 720]]\n    params2 = [15, 60, 240, 1440]\n    \n    # 特徴量作成\n    for i in range(14):\n        if i not in assets:\n            for f in feats:\n                x_dict[f\"{f}_{i}\"] = np.nan\n            continue\n        tmp_close = np.array(tmp_df.loc[tmp_df[\"Asset_ID\"]==i, \"Close\"])\n        tmp_high  = np.array(tmp_df.loc[tmp_df[\"Asset_ID\"]==i, \"High\"])\n        tmp_low   = np.array(tmp_df.loc[tmp_df[\"Asset_ID\"]==i, \"Low\"])\n        for j in range(len(params)):\n            if len(tmp_close)>=(1+params[j][0]+params[j][1]):\n                x_dict[f\"{feats[j]}_{i}\"] = tmp_close[-1-params[j][1]]/tmp_close[-1-params[j][0]-params[j][1]] - 1\n            else:\n                x_dict[f\"{feats[j]}_{i}\"] = np.nan\n        for j in params2:\n            if len(tmp_high)>=j:\n                hh = tmp_high[-j:].max()\n                ll = tmp_low[-j:].min()\n                x_dict[f\"dip{j}_{i}\"] = tmp_close[-1]/hh - 1\n                x_dict[f\"rip{j}_{i}\"] = tmp_close[-1]/ll - 1\n            else:\n                x_dict[f\"dip{j}_{i}\"] = np.nan\n                x_dict[f\"rip{j}_{i}\"] = np.nan\n    \n    # マーケットを控除\n    for f in feats:\n        s = 0\n        n = 0\n        for i in range(14):\n            if ~np.isnan(x_dict[f\"{f}_{i}\"]):\n                s += x_dict[f\"{f}_{i}\"]\n                n += 1\n        if n == 0:\n            m = 0\n        else:\n            m = s/n\n        for i in range(14):\n            if ~np.isnan(x_dict[f\"{f}_{i}\"]):\n                x_dict[f\"{f}_{i}\"] -= m\n    \n    # ビニング\n    for f in feats:\n        for i in range(14):\n            if ~np.isnan(x_dict[f\"{f}_{i}\"]):\n                x_dict[f\"{f}_{i}\"] = bins[f].transform(np.array(x_dict[f\"{f}_{i}\"]).reshape(-1, 1))[0][0]\n    \n    # 時刻特徴量\n    dt = datetime.datetime.fromtimestamp(tmp_df[\"timestamp\"].max())\n    minute = dt.minute\n    \n    x_dict[\"isFunding\"] = dt.hour%4\n    x_dict[\"is00\"] = 0\n    x_dict[\"is05\"] = 0\n    x_dict[\"is15\"] = 0\n    x_dict[\"is30\"] = 0\n    \n    if minute == 0:\n        x_dict[\"is00\"] = 1\n    elif minute==30:\n        x_dict[\"is30\"] = 1\n    elif minute in [15, 45]:\n        x_dict[\"is15\"] = 1\n    elif minute in [5, 10, 20, 25, 35, 40, 50, 55]:\n        x_dict[\"is05\"] = 1\n    return x_dict","metadata":{"execution":{"iopub.status.busy":"2022-01-31T13:49:53.685463Z","iopub.execute_input":"2022-01-31T13:49:53.685843Z","iopub.status.idle":"2022-01-31T13:49:53.713852Z","shell.execute_reply.started":"2022-01-31T13:49:53.685799Z","shell.execute_reply":"2022-01-31T13:49:53.712978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check train and loop\n#targets = [f for f in df_train.columns if \"Target\" in f]\n#x_feats = [f for f in df_train.columns if f not in [\"timestamp\", \"datetime\", \"hour\", \"minute\"]]\n#x_feats = [f for f in x_feats if f not in targets]\n#\n#if TRAIN_FLAG:\n#    df_valid = pd.read_csv(TRAIN_CSV)\n#    df_valid = df_valid.drop([\"Count\", \"Open\", \"Volume\", \"VWAP\"], axis=1)\n#    df_valid['datetime'] = pd.to_datetime(df_valid['timestamp'], unit='s')\n#    df_valid = df_valid[df_valid['datetime'] < '2021-06-13 00:00:00']\n#    df_valid = df_valid[df_valid['datetime'] >= '2020-01-01 00:00:00']\n#    df_valid.reset_index(inplace=True, drop=True)\n#    \n#    gc.collect()\n#\n#def compareResult(timestamp=1597670760):\n#    tmp_index = df_valid[df_valid[\"timestamp\"]==timestamp].index.max()\n#    tmp_df = df_valid.iloc[(tmp_index-14*1440-14*240-2000):(tmp_index+1)]\n#    tmp_dict = get_dataframe_demean_for_loop(tmp_df)\n#    res = pd.DataFrame({\"train\":np.array(df_train.loc[df_train[\"timestamp\"]==timestamp, x_feats]).reshape(-1), \"loop\":pd.Series(tmp_dict)[x_feats]})\n#    return res\n#\n#res = compareResult(timestamp=1597670760)\n#res[res[\"train\"]!=res[\"loop\"]]","metadata":{"execution":{"iopub.status.busy":"2022-01-31T13:49:53.715186Z","iopub.execute_input":"2022-01-31T13:49:53.71542Z","iopub.status.idle":"2022-01-31T13:51:00.778009Z","shell.execute_reply.started":"2022-01-31T13:49:53.715393Z","shell.execute_reply":"2022-01-31T13:51:00.776923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env = gresearch_crypto.make_env()\niter_test = env.iter_test()\n\nhistory = pd.DataFrame()\n\nfor i, (df_test, df_pred) in enumerate(iter_test):\n    history = pd.concat([history, df_test])\n    tmp = history.copy()\n    tmp.reset_index(inplace=True, drop=True)\n    tmp_dict = get_dataframe_demean_for_loop(tmp)\n    for j, row in df_test.iterrows():\n        y_pred = models[row['Asset_ID']].predict(np.array(pd.Series(tmp_dict)[x_feats]).reshape(1, -1))[0]\n        df_pred.loc[df_pred['row_id'] == row['row_id'], 'Target'] = y_pred\n    \n    history = history.sort_values(\"timestamp\")\n    history = history.iloc[(-14*1440-14*240-2000):]\n    \n    env.predict(df_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}