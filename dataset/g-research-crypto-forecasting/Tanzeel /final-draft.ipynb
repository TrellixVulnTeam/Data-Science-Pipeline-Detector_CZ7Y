{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import time\n!pip install NitroFE\nfrom NitroFE import SeriesWeightedMovingFeature, ExponentialMovingFeature, KaufmanEfficiency, TypicalValue, MovingAverageConvergenceDivergence, PercentageValueOscillator\nfrom tqdm import tqdm\n\nimport os\nimport random\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nimport gresearch_crypto\nimport time\nimport datetime\n\nimport pickle\nimport gc\n\nfrom tqdm import tqdm\n\nn_fold = 7\nseed0 = 8586\nuse_supple_for_train = True\n\n# If True, the period used to evaluate Public LB will not be used for training. \n# Set to False on final submission.\nnot_use_overlap_to_train = False\n\nTRAIN_CSV = '/kaggle/input/g-research-crypto-forecasting/train.csv'\nSUPPLE_TRAIN_CSV = '/kaggle/input/g-research-crypto-forecasting/supplemental_train.csv'\nASSET_DETAILS_CSV = '/kaggle/input/g-research-crypto-forecasting/asset_details.csv'\n\npd.set_option('display.max_rows', 6)\npd.set_option('display.max_columns', 350)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-12T13:01:58.624063Z","iopub.execute_input":"2022-05-12T13:01:58.627259Z","iopub.status.idle":"2022-05-12T13:02:19.365554Z","shell.execute_reply.started":"2022-05-12T13:01:58.627014Z","shell.execute_reply":"2022-05-12T13:02:19.364234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lags = [60,300,900]","metadata":{"execution":{"iopub.status.busy":"2022-05-12T13:02:19.368102Z","iopub.execute_input":"2022-05-12T13:02:19.368847Z","iopub.status.idle":"2022-05-12T13:02:19.374837Z","shell.execute_reply.started":"2022-05-12T13:02:19.36876Z","shell.execute_reply":"2022-05-12T13:02:19.373275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {\n    'early_stopping_rounds': 50,\n    'objective': 'regression',\n    'metric': 'rmse',\n#     'metric': 'None',\n    'boosting_type': 'gbdt',\n    'max_depth': 5,\n    'verbose': -1,\n    'max_bin':600,\n    'min_data_in_leaf':50,\n    'learning_rate': 0.03,\n    'subsample': 0.7,\n    'subsample_freq': 1,\n    'feature_fraction': 1,\n    'lambda_l1': 0.5,\n    'lambda_l2': 2,\n    'seed':seed0,\n    'feature_fraction_seed': seed0,\n    'bagging_fraction_seed': seed0,\n    'drop_seed': seed0,\n    'data_random_seed': seed0,\n    'extra_trees': True,\n    'extra_seed': seed0,\n    'zero_as_missing': True,\n    \"first_metric_only\": True\n         }","metadata":{"execution":{"iopub.status.busy":"2022-05-12T13:02:19.3769Z","iopub.execute_input":"2022-05-12T13:02:19.37718Z","iopub.status.idle":"2022-05-12T13:02:19.393942Z","shell.execute_reply.started":"2022-05-12T13:02:19.377145Z","shell.execute_reply":"2022-05-12T13:02:19.39298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-05-12T13:02:19.397619Z","iopub.execute_input":"2022-05-12T13:02:19.398396Z","iopub.status.idle":"2022-05-12T13:02:19.41574Z","shell.execute_reply.started":"2022-05-12T13:02:19.398349Z","shell.execute_reply":"2022-05-12T13:02:19.414562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_asset_details = pd.read_csv(ASSET_DETAILS_CSV).sort_values(\"Asset_ID\")\ndf_asset_details","metadata":{"execution":{"iopub.status.busy":"2022-05-12T13:02:19.420086Z","iopub.execute_input":"2022-05-12T13:02:19.420671Z","iopub.status.idle":"2022-05-12T13:02:19.472469Z","shell.execute_reply.started":"2022-05-12T13:02:19.420635Z","shell.execute_reply":"2022-05-12T13:02:19.471693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndf_train = pd.read_csv(TRAIN_CSV, usecols=['timestamp','Asset_ID', 'Close', 'Target'])\n\nif use_supple_for_train:    \n    df_supple = pd.read_csv(SUPPLE_TRAIN_CSV, usecols=['timestamp','Asset_ID', 'Close', 'Target'])\n#     display(df_supple)\n    df_train = pd.concat([df_train, df_supple])\n    del df_supple\ndf_train = reduce_mem_usage(df_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T13:02:19.473558Z","iopub.execute_input":"2022-05-12T13:02:19.474344Z","iopub.status.idle":"2022-05-12T13:03:26.701343Z","shell.execute_reply.started":"2022-05-12T13:02:19.474305Z","shell.execute_reply":"2022-05-12T13:03:26.699669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain_merged = pd.DataFrame()\ntrain_merged[df_train.columns] = 0\nfor id in tqdm( range(14) ):\n    train_merged = train_merged.merge(df_train.loc[df_train[\"Asset_ID\"] == id, ['timestamp', 'Close','Target']].copy(), on=\"timestamp\", how='outer',suffixes=['', \"_\"+str(id)])\n        \ntrain_merged = train_merged.drop(df_train.columns.drop(\"timestamp\"), axis=1)\ntrain_merged = train_merged.sort_values('timestamp', ascending=True)\ndisplay(train_merged.head())","metadata":{"execution":{"iopub.status.busy":"2022-05-12T13:03:26.704528Z","iopub.execute_input":"2022-05-12T13:03:26.705071Z","iopub.status.idle":"2022-05-12T13:03:57.968716Z","shell.execute_reply.started":"2022-05-12T13:03:26.705002Z","shell.execute_reply":"2022-05-12T13:03:57.967579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_merged.timestamp","metadata":{"execution":{"iopub.status.busy":"2022-05-12T13:03:57.970342Z","iopub.execute_input":"2022-05-12T13:03:57.970876Z","iopub.status.idle":"2022-05-12T13:03:57.980242Z","shell.execute_reply.started":"2022-05-12T13:03:57.970839Z","shell.execute_reply":"2022-05-12T13:03:57.979051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for id in range(14):\n#     print(id, train_merged[f'Close_{id}'].isnull().sum())   # Number of missing before forward fill\n    train_merged[f'Close_{id}'] = train_merged[f'Close_{id}'].fillna(method='ffill', limit=100)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T13:03:57.981906Z","iopub.execute_input":"2022-05-12T13:03:57.982723Z","iopub.status.idle":"2022-05-12T13:03:58.148366Z","shell.execute_reply.started":"2022-05-12T13:03:57.982664Z","shell.execute_reply":"2022-05-12T13:03:58.147034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plug_play_features(s, verbose=1):\n#     df = pd.DataFrame()\n#     ob1 = SeriesWeightedMovingFeature(lookback_period=8, operation= np.mean)\n#     df['SW_Close'] = ob1.fit(s['Close'], s['Volume'],first_fit=True).fillna(s['Close'])\n    if verbose:\n        print(f\"Adding plug n play features\")\n        ss = tqdm(range(14))\n    else: \n        ss = range(14)\n        \n    for i in ss:\n        ob2 = ExponentialMovingFeature(span=8)\n        s[f'EM_Close_{i}'] = ob2.fit(s[f'Close_{i}'], first_fit=True)\n        ob3=PercentageValueOscillator(fast_period=15, slow_period=10,smoothing_period=20)\n        s[f'PVO_Close_{i}']=ob3.fit(s[f'Close_{i}'],first_fit=True)\n        ob4=MovingAverageConvergenceDivergence(fast_period=26, slow_period=12, smoothing_period=9)\n        s[f'MACD_Close_{i}']=ob4.fit(s[f'Close_{i}'],first_fit=True)\n#         ob5 = TypicalValue(lookback_period=32)\n#         s[f'TV_Close_{i}'] = ob5.fit(s[f'Close_{i}'],first_fit=True).fillna(s[f'Close_{i}'])\n#         ob6 = KaufmanEfficiency(lookback_period=12)\n#         s[f'KE_Close_{i}'] = ob6.fit(s[f'Close_{i}'], first_fit=True).fillna(value=1/12)\n    return s","metadata":{"execution":{"iopub.status.busy":"2022-05-12T13:03:58.152482Z","iopub.execute_input":"2022-05-12T13:03:58.152813Z","iopub.status.idle":"2022-05-12T13:03:58.163267Z","shell.execute_reply.started":"2022-05-12T13:03:58.152762Z","shell.execute_reply":"2022-05-12T13:03:58.161957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_features(df, train=True):   \n    if train == True:\n        totimestamp = lambda s: np.int32(time.mktime(datetime.datetime.strptime(s, \"%d/%m/%Y\").timetuple()))\n        valid_window = [totimestamp(\"12/03/2021\")]\n#         valid_window = [totimestamp(\"15/08/2021\")]  #検証用\n        df['train_flg'] = np.where(df['timestamp']>=valid_window[0], 0,1)\n\n        supple_start_window = [totimestamp(\"22/09/2021\")]\n        if use_supple_for_train:\n            df['train_flg'] = np.where(df['timestamp']>=supple_start_window[0], 1 ,df['train_flg']  )\n\n    df = plug_play_features(df)\n#     return df\n\n######################################## TRY\n    for id in range(14):    \n        for lag in lags:\n            df[f'log_close/mean_{lag}_id{id}'] = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.append(np.convolve( np.array(df[f'Close_{id}']), np.ones(lag)/lag, mode=\"valid\"), np.ones(lag-1)), lag-1)  )\n            df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n    for lag in lags:\n        df[f'mean_close/mean_{lag}'] =  np.mean(df.iloc[:,df.columns.str.startswith(f'log_close/mean_{lag}_id')], axis=1)\n        df[f'mean_log_returns_{lag}'] = np.mean(df.iloc[:,df.columns.str.startswith(f'log_return_{lag}_id')] ,    axis=1)\n        for id in range(14):\n            df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n            df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n\n    if train == True:\n        for id in range(14):\n            df = df.drop([f'Close_{id}'], axis=1)\n        oldest_use_window = [totimestamp(\"12/01/2019\")]\n        df = df[  df['timestamp'] >= oldest_use_window[0]   ]\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-05-12T13:03:58.165753Z","iopub.execute_input":"2022-05-12T13:03:58.166035Z","iopub.status.idle":"2022-05-12T13:03:58.18648Z","shell.execute_reply.started":"2022-05-12T13:03:58.166002Z","shell.execute_reply":"2022-05-12T13:03:58.18551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfeat_frag = get_features(train_merged)\nfeat = feat_frag.copy()","metadata":{"execution":{"iopub.status.busy":"2022-05-12T13:03:58.187561Z","iopub.execute_input":"2022-05-12T13:03:58.188994Z","iopub.status.idle":"2022-05-12T13:04:56.507793Z","shell.execute_reply.started":"2022-05-12T13:03:58.18893Z","shell.execute_reply":"2022-05-12T13:04:56.50643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat.columns","metadata":{"execution":{"iopub.status.busy":"2022-05-12T13:04:56.509484Z","iopub.execute_input":"2022-05-12T13:04:56.510455Z","iopub.status.idle":"2022-05-12T13:04:56.519108Z","shell.execute_reply.started":"2022-05-12T13:04:56.510404Z","shell.execute_reply":"2022-05-12T13:04:56.517459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define features for LGBM\nnot_use_features_train = ['timestamp', 'train_flg']\nfor id in range(14):\n    not_use_features_train.append(f'Target_{id}')\n\nfeatures = feat.columns \nfeatures = features.drop(not_use_features_train)\nfeatures = list(features)\n# display(features)  \nlen(features)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T13:04:56.521234Z","iopub.execute_input":"2022-05-12T13:04:56.521627Z","iopub.status.idle":"2022-05-12T13:04:56.536649Z","shell.execute_reply.started":"2022-05-12T13:04:56.521563Z","shell.execute_reply":"2022-05-12T13:04:56.535878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train_merged\ndel df_train\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-12T13:04:56.538638Z","iopub.execute_input":"2022-05-12T13:04:56.539047Z","iopub.status.idle":"2022-05-12T13:04:56.840966Z","shell.execute_reply.started":"2022-05-12T13:04:56.538984Z","shell.execute_reply":"2022-05-12T13:04:56.839806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define the evaluation metric\ndef correlation(a, train_data):\n    \n    b = train_data.get_label()\n    \n    a = np.ravel(a)\n    b = np.ravel(b)\n\n    len_data = len(a)\n    mean_a = np.sum(a) / len_data\n    mean_b = np.sum(b) / len_data\n    var_a = np.sum(np.square(a - mean_a)) / len_data\n    var_b = np.sum(np.square(b - mean_b)) / len_data\n\n    cov = np.sum((a * b))/len_data - mean_a*mean_b\n    corr = cov / np.sqrt(var_a * var_b)\n\n    return 'corr', corr, True\n\n# For CV score calculation\ndef corr_score(pred, valid):\n    len_data = len(pred)\n    mean_pred = np.sum(pred) / len_data\n    mean_valid = np.sum(valid) / len_data\n    var_pred = np.sum(np.square(pred - mean_pred)) / len_data\n    var_valid = np.sum(np.square(valid - mean_valid)) / len_data\n\n    cov = np.sum((pred * valid))/len_data - mean_pred*mean_valid\n    corr = cov / np.sqrt(var_pred * var_valid)\n\n    return corr\n\n# For CV score calculation\ndef wcorr_score(pred, valid, weight):\n    len_data = len(pred)\n    sum_w = np.sum(weight)\n    mean_pred = np.sum(pred * weight) / sum_w\n    mean_valid = np.sum(valid * weight) / sum_w\n    var_pred = np.sum(weight * np.square(pred - mean_pred)) / sum_w\n    var_valid = np.sum(weight * np.square(valid - mean_valid)) / sum_w\n\n    cov = np.sum((pred * valid * weight)) / sum_w - mean_pred*mean_valid\n    corr = cov / np.sqrt(var_pred * var_valid)\n\n    return corr","metadata":{"execution":{"iopub.status.busy":"2022-05-12T13:04:56.842751Z","iopub.execute_input":"2022-05-12T13:04:56.843148Z","iopub.status.idle":"2022-05-12T13:04:56.862028Z","shell.execute_reply.started":"2022-05-12T13:04:56.843098Z","shell.execute_reply":"2022-05-12T13:04:56.860895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from: https://blog.amedama.jp/entry/lightgbm-cv-feature-importance\n# (used in nyanp's Optiver solution)\ndef plot_importance(importances, features_names = features, PLOT_TOP_N = 20, figsize=(10, 10)):\n    importance_df = pd.DataFrame(data=importances, columns=features)\n    sorted_indices = importance_df.median(axis=0).sort_values(ascending=False).index\n    sorted_importance_df = importance_df.loc[:, sorted_indices]\n    plot_cols = sorted_importance_df.columns[:PLOT_TOP_N]\n    _, ax = plt.subplots(figsize=figsize)\n    ax.grid()\n    ax.set_xscale('log')\n    ax.set_ylabel('Feature')\n    ax.set_xlabel('Importance')\n    sns.boxplot(data=sorted_importance_df[plot_cols],\n                orient='h',\n                ax=ax)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-12T13:04:56.864659Z","iopub.execute_input":"2022-05-12T13:04:56.865047Z","iopub.status.idle":"2022-05-12T13:04:56.87617Z","shell.execute_reply.started":"2022-05-12T13:04:56.865008Z","shell.execute_reply":"2022-05-12T13:04:56.874991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from: https://www.kaggle.com/code/nrcjea001/lgbm-embargocv-weightedpearson-lagtarget/\ndef get_time_series_cross_val_splits(data, cv = n_fold, embargo = 3750):\n    all_train_timestamps = data['timestamp'].unique()\n    len_split = len(all_train_timestamps) // cv\n    test_splits = [all_train_timestamps[i * len_split:(i + 1) * len_split] for i in range(cv)]\n    # fix the last test split to have all the last timestamps, in case the number of timestamps wasn't divisible by cv\n    rem = len(all_train_timestamps) - len_split*cv\n    if rem>0:\n        test_splits[-1] = np.append(test_splits[-1], all_train_timestamps[-rem:])\n\n    train_splits = []\n    for test_split in test_splits:\n        test_split_max = int(np.max(test_split))\n        test_split_min = int(np.min(test_split))\n        # get all of the timestamps that aren't in the test split\n        train_split_not_embargoed = [e for e in all_train_timestamps if not (test_split_min <= int(e) <= test_split_max)]\n        # embargo the train split so we have no leakage. Note timestamps are expressed in seconds, so multiply by 60\n        embargo_sec = 60*embargo\n        train_split = [e for e in train_split_not_embargoed if\n                       abs(int(e) - test_split_max) > embargo_sec and abs(int(e) - test_split_min) > embargo_sec]\n        train_splits.append(train_split)\n\n    # convenient way to iterate over train and test splits\n    train_test_zip = zip(train_splits, test_splits)\n    return train_test_zip","metadata":{"execution":{"iopub.status.busy":"2022-05-12T13:04:56.8775Z","iopub.execute_input":"2022-05-12T13:04:56.878045Z","iopub.status.idle":"2022-05-12T13:04:56.896916Z","shell.execute_reply.started":"2022-05-12T13:04:56.878009Z","shell.execute_reply":"2022-05-12T13:04:56.895721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_Xy_and_model_for_asset(df_proc, asset_id):\n    df_proc = df_proc.loc[  (df_proc[f'Target_{asset_id}'] == df_proc[f'Target_{asset_id}'])  ]\n    if not_use_overlap_to_train:\n        df_proc = df_proc.loc[  (df_proc['train_flg'] == 1)  ]\n    \n# EmbargoCV\n    train_test_zip = get_time_series_cross_val_splits(df_proc, cv = n_fold, embargo = 3750)\n    print(\"entering time series cross validation loop\")\n    importances = []\n    oof_pred = []\n    oof_valid = []\n    \n    for split, train_test_split in enumerate(train_test_zip):\n        gc.collect()\n        \n        print(f\"doing split {split+1} out of {n_fold}\")\n        train_split, test_split = train_test_split\n        train_split_index = df_proc['timestamp'].isin(train_split)\n        test_split_index = df_proc['timestamp'].isin(test_split)\n    \n        train_dataset = lgb.Dataset(df_proc.loc[train_split_index, features],\n                                    df_proc.loc[train_split_index, f'Target_{asset_id}'].values, \n                                    feature_name = features, \n                                   )\n        val_dataset = lgb.Dataset(df_proc.loc[test_split_index, features], \n                                  df_proc.loc[test_split_index, f'Target_{asset_id}'].values, \n                                  feature_name = features, \n                                 )\n\n        print(f\"number of train data: {len(df_proc.loc[train_split_index])}\")\n        print(f\"number of val data:   {len(df_proc.loc[test_split_index])}\")\n\n        model = lgb.train(params = params,\n                          train_set = train_dataset, \n                          valid_sets=[train_dataset, val_dataset],\n                          valid_names=['tr', 'vl'],\n                          num_boost_round = 5000,\n                          verbose_eval = 100,     \n                          feval = correlation,\n                         )\n        importances.append(model.feature_importance(importance_type='gain'))\n        \n        file = f'trained_model_id{asset_id}_fold{split}.pkl'\n        pickle.dump(model, open(file, 'wb'))\n        print(f\"Trained model was saved to 'trained_model_id{asset_id}_fold{split}.pkl'\")\n        print(\"\")\n            \n        oof_pred += list(  model.predict(df_proc.loc[test_split_index, features])        )\n        oof_valid += list(   df_proc.loc[test_split_index, f'Target_{asset_id}'].values    )\n    \n    \n    plot_importance(np.array(importances),features, PLOT_TOP_N = 20, figsize=(10, 5))\n\n    return oof_pred, oof_valid","metadata":{"execution":{"iopub.status.busy":"2022-05-12T13:04:56.898874Z","iopub.execute_input":"2022-05-12T13:04:56.8994Z","iopub.status.idle":"2022-05-12T13:04:56.919798Z","shell.execute_reply.started":"2022-05-12T13:04:56.899347Z","shell.execute_reply":"2022-05-12T13:04:56.918568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof = [ [] for id in range(14)   ]\n\nall_oof_pred = []\nall_oof_valid = []\nall_oof_weight = []\n\nfor asset_id, asset_name in zip(df_asset_details['Asset_ID'], df_asset_details['Asset_Name']):\n    print(f\"Training model for {asset_name:<16} (ID={asset_id:<2})\")\n    \n    oof_pred, oof_valid = get_Xy_and_model_for_asset(feat, asset_id)\n    \n    weight_temp = float( df_asset_details.loc[  df_asset_details['Asset_ID'] == asset_id  , 'Weight'   ]  )\n    \n    all_oof_pred += oof_pred\n    all_oof_valid += oof_valid\n    all_oof_weight += [weight_temp] * len(oof_pred)\n    \n    oof[asset_id] = corr_score(     np.array(oof_pred)   ,    np.array(oof_valid)    )\n    \n    print(f'OOF corr score of {asset_name} (ID={asset_id}) is {oof[asset_id]:.5f}. (Weight: {float(weight_temp):.5f})')\n    print('')\n    print('')","metadata":{"execution":{"iopub.status.busy":"2022-05-12T13:04:56.921862Z","iopub.execute_input":"2022-05-12T13:04:56.922233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}