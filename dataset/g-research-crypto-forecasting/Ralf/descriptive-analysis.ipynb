{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-25T17:19:09.053508Z","iopub.execute_input":"2021-11-25T17:19:09.053833Z","iopub.status.idle":"2021-11-25T17:19:09.0872Z","shell.execute_reply.started":"2021-11-25T17:19:09.05375Z","shell.execute_reply":"2021-11-25T17:19:09.086332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us start by taking a look, how the target values evolve over time per currency.","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport gc\n\ndtypes = {\n    'timestamp': np.int64,\n    'Asset_ID': np.int8,\n     'Count': np.int32,\n     'Open': np.float64,\n     'High': np.float64,\n     'Low': np.float64,\n    'Close': np.float64,\n     'Volume': np.float64,\n     'VWAP': np.float64,\n    'Target': np.float64,\n}\ndata = pd.read_csv('/kaggle/input/g-research-crypto-forecasting/train.csv', dtype=dtypes, usecols=list(dtypes.keys()))\ndata['Time'] = pd.to_datetime(data['timestamp'], unit='s')\n\ndetails = pd.read_csv('/kaggle/input/g-research-crypto-forecasting/asset_details.csv')\n\ndata = pd.merge(data, \n                details, \n                on ='Asset_ID', \n                how ='left')\n\nprint(data.head())\nprint(details)","metadata":{"execution":{"iopub.status.busy":"2021-11-25T17:21:25.414994Z","iopub.execute_input":"2021-11-25T17:21:25.415259Z","iopub.status.idle":"2021-11-25T17:22:21.035363Z","shell.execute_reply.started":"2021-11-25T17:21:25.415231Z","shell.execute_reply":"2021-11-25T17:22:21.034434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Subplots are organized in a Rows x Cols Grid\n# Tot and Cols are known\nTot = len(details.Asset_ID)\nCols = 4\n\n\n# Compute Rows required\nRows = Tot // Cols \nRows += Tot % Cols\n\n# Create a Position index\nPosition = range(1,Tot + 1)\n\n# Create main figure\nfig = plt.figure(1)\nfig.set_figheight(20)\nfig.set_figwidth(20)\n\nfor k in range(Tot):\n  # add every single subplot to the figure with a for loop\n\n    tmp_df = data[data.Asset_ID == details.Asset_ID[k]]\n    ax = fig.add_subplot(Rows,Cols,Position[k])\n    ax.plot(tmp_df.Time, tmp_df.Target)\n    ax.set_title(details.Asset_Name[k])\n\nplt.show()\n\ndel tmp_df","metadata":{"execution":{"iopub.status.busy":"2021-11-25T17:22:21.037058Z","iopub.execute_input":"2021-11-25T17:22:21.03786Z","iopub.status.idle":"2021-11-25T17:22:32.833343Z","shell.execute_reply.started":"2021-11-25T17:22:21.037815Z","shell.execute_reply":"2021-11-25T17:22:32.822452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In my opinion, we see very typical things here with respect to financial data. Time series plots of log-returns for assets typically look similar, i.e., the arithmetic mean is close to zero, while we observe volatility clustering which means in certain time periods volatility is higher than in others. Let us take a look at the distributions for the target variable.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Subplots are organized in a Rows x Cols Grid\n# Tot and Cols are known\nTot = len(details.Asset_ID)\nCols = 4\n\n\n# Compute Rows required\nRows = Tot // Cols \nRows += Tot % Cols\n\n# Create a Position index\nPosition = range(1,Tot + 1)\n\n# Create main figure\nfig = plt.figure(1)\nfig.set_figheight(20)\nfig.set_figwidth(20)\n\nfor k in range(Tot):\n  # add every single subplot to the figure with a for loop\n\n    tmp_df = data[data.Asset_ID == details.Asset_ID[k]]\n    ax = fig.add_subplot(Rows,Cols,Position[k])\n    ax.hist(tmp_df.Target, bins = 50)\n    ax.set_xlim(-0.1, 0.1)\n    ax.set_title(details.Asset_Name[k])\n\nplt.show()\n\ndel tmp_df","metadata":{"execution":{"iopub.status.busy":"2021-11-25T17:24:42.763634Z","iopub.execute_input":"2021-11-25T17:24:42.764102Z","iopub.status.idle":"2021-11-25T17:24:50.158824Z","shell.execute_reply.started":"2021-11-25T17:24:42.764065Z","shell.execute_reply":"2021-11-25T17:24:50.158237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most of the currencies exhibit rather symmetric distributions, yet, it looks like all currencies exhibit excess curtosis which means extreme events are more likely than under the assumption of a normal distribution. Now let us take a look if target values are correlated at each point in time.","metadata":{}},{"cell_type":"code","source":"all_timestamps = np.sort(data['timestamp'].unique())\ntargets = pd.DataFrame(index=all_timestamps)\n\nfor i, id_ in enumerate(details.Asset_ID):\n    asset = data[data.Asset_ID == id_].set_index(keys='timestamp')\n    price = pd.Series(index=all_timestamps, data=asset['Close'])\n    targets[details.Asset_Name[i]] = (\n        price.shift(periods=-16) /\n        price.shift(periods=-1)\n    ) - 1\n    \nprint(targets.head())\n\nimport seaborn as sns\n\nsns.heatmap(targets.corr())\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-25T17:26:59.625049Z","iopub.execute_input":"2021-11-25T17:26:59.627266Z","iopub.status.idle":"2021-11-25T17:27:12.542547Z","shell.execute_reply.started":"2021-11-25T17:26:59.627203Z","shell.execute_reply":"2021-11-25T17:27:12.541561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that target values for each currency exhibit high correlations in the cross-section. Unfortunately, this does not support us when making future predictions, as information in the cross-section is contemporaneous and not predictive. Now, let us try to find out, how systematic market movements of currencies behave. To do so, I simply normalize \"Close\"-values for each currency and calculate the naive (equally-weighted) mean for all currencies which are available over a point in time. Finally, I visualize this. Obviously, this is rather a rough quick and dirty method for checking systematic market behavior.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nclosing_prices = pd.DataFrame(index=all_timestamps)\n\nfor i, id_ in enumerate(details.Asset_ID):\n    asset = data[data.Asset_ID == id_].set_index(keys='timestamp')\n    price = pd.Series(index=all_timestamps, data=asset['Close'])\n    closing_prices[details.Asset_Name[i]] = price\n    \nmin_max_scaler = MinMaxScaler()\nclosing_prices_ = min_max_scaler.fit_transform(closing_prices)\nplt.plot(closing_prices.index, np.mean(closing_prices_, axis = 1))","metadata":{"execution":{"iopub.status.busy":"2021-11-25T17:30:44.426448Z","iopub.execute_input":"2021-11-25T17:30:44.426771Z","iopub.status.idle":"2021-11-25T17:30:54.119437Z","shell.execute_reply.started":"2021-11-25T17:30:44.426733Z","shell.execute_reply":"2021-11-25T17:30:54.118654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I would say that we may observe a little systematic behavior which looks similar to economic cycles. This information may be further refined and used when splitting the data. Next, let us use Bitcoin as an example to check the raw data for possible predictive power.","metadata":{}},{"cell_type":"code","source":"btc = data[data.Asset_ID == 1]\nbtc.set_index('timestamp', inplace = True)\nbtc.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-25T17:32:21.359207Z","iopub.execute_input":"2021-11-25T17:32:21.359515Z","iopub.status.idle":"2021-11-25T17:32:21.669681Z","shell.execute_reply.started":"2021-11-25T17:32:21.359485Z","shell.execute_reply":"2021-11-25T17:32:21.668852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Taking a look at the subplots below indicate that alle variables except the target variable are non-stationary which is bad for making models that generalize over time.","metadata":{}},{"cell_type":"code","source":"btc[['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP', 'Target']].plot(subplots = True)","metadata":{"execution":{"iopub.status.busy":"2021-11-25T17:32:44.675668Z","iopub.execute_input":"2021-11-25T17:32:44.676058Z","iopub.status.idle":"2021-11-25T17:33:34.101617Z","shell.execute_reply.started":"2021-11-25T17:32:44.676011Z","shell.execute_reply":"2021-11-25T17:33:34.100479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, I tried first differences, but even those values do not seem to be stationary, so I directly tried the first log-differences below. Strict stationarity means the (unconditional) distribution stays the same over time for a given variable. Weak stationarity means at least the mean, variance and autocovariance are the same over time.","metadata":{}},{"cell_type":"code","source":"first_log_differences = btc[['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP']].apply(np.log).diff()\nfirst_log_differences.plot(subplots = True)","metadata":{"execution":{"iopub.status.busy":"2021-11-25T17:34:41.536798Z","iopub.execute_input":"2021-11-25T17:34:41.537121Z","iopub.status.idle":"2021-11-25T17:35:34.753061Z","shell.execute_reply.started":"2021-11-25T17:34:41.537082Z","shell.execute_reply":"2021-11-25T17:35:34.752193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the plots below, I want to take a look if approximately the mean and the standard deviation are constant over time. For most of the time this seems to be the case, but there are some breaktroughs at some points in time which can really have a negative effect if we would use these variables for prediction.","metadata":{}},{"cell_type":"code","source":"first_log_differences.rolling(7200).mean().plot(subplots = True)","metadata":{"execution":{"iopub.status.busy":"2021-11-25T17:39:00.789583Z","iopub.execute_input":"2021-11-25T17:39:00.789879Z","iopub.status.idle":"2021-11-25T17:39:54.063956Z","shell.execute_reply.started":"2021-11-25T17:39:00.789842Z","shell.execute_reply":"2021-11-25T17:39:54.062989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"first_log_differences.rolling(7200).std().plot(subplots = True)","metadata":{"execution":{"iopub.status.busy":"2021-11-25T17:39:54.06544Z","iopub.execute_input":"2021-11-25T17:39:54.065659Z","iopub.status.idle":"2021-11-25T17:40:46.830561Z","shell.execute_reply.started":"2021-11-25T17:39:54.065634Z","shell.execute_reply":"2021-11-25T17:40:46.829656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us check if the feature variables (here the log-differences) exhibit any correlation to the target values.","metadata":{}},{"cell_type":"code","source":"new_data = btc[['Target']].merge(first_log_differences, left_index = True, right_index = True)\nsns.heatmap(new_data.corr())\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-25T17:42:11.882653Z","iopub.execute_input":"2021-11-25T17:42:11.882945Z","iopub.status.idle":"2021-11-25T17:42:13.170407Z","shell.execute_reply.started":"2021-11-25T17:42:11.882914Z","shell.execute_reply":"2021-11-25T17:42:13.169486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Unfortunately, this does not seem to be the case. So, I further check if there is some correlation between the past of feature variables and the current target variable. To do so, I write a function which approximately calculates auto-cross-correlation. To be honest, I do not know if this is actually defined somewhere in statistics, however, what I want to find out if there is any (linear) relationship between past observations of feature variables and the current target obsevation.","metadata":{}},{"cell_type":"code","source":"def cross_autocorr(df, targetname, varname, lag = 1):\n    df.dropna(inplace = True)\n    x_m = df[targetname].mean()\n    y_m = df[varname].mean()\n\n    x_s = df[targetname].std()\n    y_s = df[varname].std()\n\n    return (x_s * y_s)**(-1) * np.mean((df[targetname].values[lag:] - x_m) * (df[varname].values[:-lag] - y_m))\n\ncross_autocorrs = {}\nfor var in ['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP']:\n    autocorrs_tmp = []\n    for l in range(1, 30, 1):\n        autocorrs_tmp.append(cross_autocorr(new_data, 'Target', var, lag = l))\n        \n    cross_autocorrs[var] = autocorrs_tmp\n    \nsns.heatmap(pd.DataFrame(cross_autocorrs, index = range(1, 30, 1)))","metadata":{"execution":{"iopub.status.busy":"2021-11-25T17:42:52.221824Z","iopub.execute_input":"2021-11-25T17:42:52.222656Z","iopub.status.idle":"2021-11-25T17:43:19.909819Z","shell.execute_reply.started":"2021-11-25T17:42:52.222618Z","shell.execute_reply":"2021-11-25T17:43:19.909022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we all may have guessed, this isn't the case! So, we are forced to create some features which may have at least a little correlation to the target variable. Otherwise, the best and most sophisticated model will fail to learn about making good predictions. So, good luck everyone in finding those features and if you find them let me know;)","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}