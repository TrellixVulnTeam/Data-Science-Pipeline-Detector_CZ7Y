{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Load"},{"metadata":{"trusted":true},"cell_type":"code","source":"!unzip /kaggle/input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip\n!unzip /kaggle/input/word2vec-nlp-tutorial/unlabeledTrainData.tsv.zip\n!unzip /kaggle/input/word2vec-nlp-tutorial/testData.tsv.zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = '/kaggle/working/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(PATH+'labeledTrainData.tsv', delimiter='\\t', quoting=3)\ntest = pd.read_csv(PATH+'testData.tsv', delimiter='\\t', quoting=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe(exclude=[np.number])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['sentiment'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['review'][0][:700]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 전처리 sample\n* BeautifulSoup 으로 html 태그 제거\n* 정규표현식으로 알파벳 이외의 문자 공백 제거\n* NLTK 데이터를 활용 불용어(stopwords) 제거\n* Stemming 을 활용하여 어간 추출"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip show BeautifulSoup4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from bs4 import BeautifulSoup\n\nexample1 = BeautifulSoup(train['review'][0], 'html5lib')\nprint(train['review'][0][:700])\nexample1.get_text()[:700]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\nletters_only = re.sub('[^a-zA-Z]', ' ', example1.get_text())\nletters_only[:700]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lower_case = letters_only.lower()\nwords = lower_case.split()\nprint(len(words))\nwords[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip show nltk","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nnltk.download('stopwords')\nnltk.download('wordnet')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nstopwords.words('english')[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words = [w for w in words if not w in stopwords.words('english')]\nprint(len(words))\nwords[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stemming"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 퍼터 스태머\nstemmer = nltk.stem.PorterStemmer()\nprint(stemmer.stem('maximum'))\nprint('The stemmed form of running is : {}'.format(stemmer.stem('running')))\nprint('The stemmed form of runs is : {}'.format(stemmer.stem('runs')))\nprint('The stemmed form of run is : {}'.format(stemmer.stem('run')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 랭커스터 스태머\nfrom nltk.stem.lancaster import LancasterStemmer\nlancaster_stemmer = LancasterStemmer()\nprint(lancaster_stemmer.stem('maximum'))\nprint('The stemmmed form of running is : {}'.format(lancaster_stemmer.stem('running')))\nprint('The stemmed form of runs is : {}'.format(lancaster_stemmer.stem('runs')))\nprint('The stemmed form of run is {}'.format(lancaster_stemmer.stem('run')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# snowball stemmer\nfrom nltk.stem.snowball import SnowballStemmer\n\nstemmer = SnowballStemmer('english')\nwords = [stemmer.stem(w) for w in words]\n\n# snowball stemm 처리 후\nwords[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Lemmatization"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\n\nprint(wordnet_lemmatizer.lemmatize('fly'))\nprint(wordnet_lemmatizer.lemmatize('flies'))\n\nwords = [wordnet_lemmatizer.lemmatize(w) for w in words]\nwords[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## 전처리 All\n* BeautifulSoup 으로 html 태그 제거\n* 정규표현식으로 알파벳 이외의 문자 공백 제거\n* 소문자 변환\n* NLTK 데이터를 활용 불용어(stopwords) 제거\n* Stemming 을 활용하여 어간 추출"},{"metadata":{"trusted":true},"cell_type":"code","source":"def review_to_words(raw_review):\n    # 1. HTML 제거\n    review_text = BeautifulSoup(raw_review, 'html.parser').get_text()\n    # 2. 영문자가 아닌 문자는 공백으로 변환\n    letters_only = re.sub('[^a-zA-Z]', ' ', review_text)\n    # 3. 소문자 변환\n    words = letters_only.lower().split()\n    # 4. stopwords 를 세트로 변환\n    stops = set(stopwords.words('english'))\n    # 5. stopwords 불용어 제거\n    meaningful_words = [w for w in words if not w in stops]\n    # 6. 어간 추출\n    stemming_words = [stemmer.stem(w) for w in meaningful_words]\n    # 7. 공백으로 구분된 문자열로 결합하여 결과를 반환\n    return (' '.join(stemming_words))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nltk.download('stopwords')\nnltk.download('wordnet')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_review = review_to_words(train['review'][0])\nclean_review","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 첫번째 리뷰를 대상으로 전처리 해줬던 내용을 전체 텍스트 데이터를 대상으로 처리한다.\n# 전체 리뷰 데이터 수 가져오기\nnum_reviews = train['review'].size\nnum_reviews","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from multiprocessing import Pool\nimport numpy as np\n\ndef _apply_df(args):\n    df, func, kwargs = args\n    return df.apply(func, **kwargs)\n\ndef apply_by_multiprocessing(df, func, **kwargs):\n    # 키워드 항목 중 workers 파라미터를 꺼냄\n    workers = kwargs.pop('workers')\n    # 위에서 가져온 workers 수로 프로세스 풀을 정의\n    pool = Pool(processes=workers)\n    # 실행할 함수와 데이터프레임을 워커의 수 만큼 나눠 작업\n    result = pool.map(_apply_df, [(d, func, kwargs)\n                                 for d in np.array_split(df, workers)])\n    pool.close()\n    # 작업 결과를 합쳐서 반환\n    return pd.concat(list(result))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\nclean_train_reviews = apply_by_multiprocessing(\\\n    train['review'], review_to_words, workers=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\nclean_test_reviews = apply_by_multiprocessing(\\\n    test['review'], review_to_words, workers=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## WordCloud"},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndef displayWordCloud(data=None, backgroundcolor='white', width=800, height=600):\n    wordcloud = WordCloud(stopwords=STOPWORDS,\n                         background_color=backgroundcolor,\n                         width=width, height=height).generate(data)\n    plt.figure(figsize=(15,10))\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 학습 데이터의 모든 단어에 대한 워드 클라우드\n%time\ndisplayWordCloud(' '.join(clean_train_reviews))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 단어 수\ntrain['num_words'] = clean_train_reviews.apply(lambda x:len(str(x).split()))\n#  중복을 제거한 단어수\ntrain['num_uniq_words'] = clean_train_reviews.apply(lambda x:len(set(str(x).split())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 테스트 데이터의 모든 단어에 대한 워드 클라우드\n%time\ndisplayWordCloud(' '.join(clean_test_reviews))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = clean_train_reviews[0]\nx = str(x).split()\nprint(len(x))\nx[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n\nfig, axes = plt.subplots(ncols=2)\nfig.set_size_inches(18,6)\nprint('리뷰별 단어 평균 값 : ', train['num_words'].mean())\nprint('리뷰별 단어 중간 값 : ', train['num_words'].median())\nsns.distplot(train['num_words'], bins=100, ax=axes[0])\naxes[0].axvline(train['num_words'].median(), linestyle='dashed')\naxes[0].set_title('review unique word distribution')\n\n\nprint('review unique word mean:', train['num_uniq_words'].mean())\nprint('review unique word meidan', train['num_uniq_words'].median())\nsns.distplot(train['num_uniq_words'], bins=100, color='g', ax=axes[1])\naxes[1].axvline(train['num_uniq_words'].median(), linestyle='dashed')\naxes[1].set_title('review unique word distribution')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bag-of-words model\n### 사이킷런의 CountVenctorizer 를 통해 피처 생성\n* 정규표현식을 사용해 토큰을 추출\n* 모두 소문자로 변환시키기 때문에 good, Good, gOod이 모두 같은 특성이 됨\n* 의미없는 특성을 많이 생성하기 때문에 적어도 두 개의 문서에 나타난 토큰만을 사용\n* min_df로 토큰이 나타날 최소 문서 개수를 지정할 수 있음"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.pipeline import Pipeline\n\nvectorizer = CountVectorizer(analyzer = 'word',\n                            tokenizer = None,\n                            preprocessor = None,\n                            stop_words = None,\n                            min_df = 2,\n                            ngram_range=(1,3),\n                            max_features = 20000)\nvectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = Pipeline([\n    ('vect', vectorizer),\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\ntrain_data_features = pipeline.fit_transform(clean_train_reviews)\ntrain_data_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab = vectorizer.get_feature_names()\nprint(len(vocab))\nvocab[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 벡터화된 피처를 확인\ndist = np.sum(train_data_features, axis=0)\n\nfor tag, count in zip(vocab, dist):\n    print(count, tag)\n\npd.DataFrame(dist, columns=vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(train_data_features[:10].toarray(), columns=vocab).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## RandomForest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nforest = RandomForestClassifier(\n    n_estimators=100, n_jobs=-1, random_state=2018)\nforest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\nforest = forest.fit(train_data_features, train['sentiment'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n%time\nscore = np.mean(cross_val_score(\\\n    forest, train_data_features, \\\n    train['sentiment'], cv=10, scoring='roc_auc'))\nscore","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_test_reviews[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\ntest_data_features = pipeline.transform(clean_test_reviews)\ntest_data_features = test_data_features.toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 벡터화 된 단어로 숫자가 문서에서 등장하는 횟수를 나타낸다\ntest_data_features[5][:100]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 벡터화 하며 만든 사전에서 해당 단어가 무엇인지 찾아볼 수 있다.\nvocab[8], vocab[2558], vocab[2559], vocab[2560]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 테스트 데이터를 넣고 예측한다.\nresult = forest.predict(test_data_features)\nresult[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 캐글 제출을 위해 예측결과 저장"},{"metadata":{"trusted":true},"cell_type":"code","source":"output = pd.DataFrame(data={'id':test['id'], 'sentiment':result})\noutput.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output.to_csv('tutorial1_BOW_{0:.5f}.csv'.format(score), index=False, quoting=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output_sentiment = output['sentiment'].value_counts()\nprint(np.abs(output_sentiment[0] - output_sentiment[1]))\noutput_sentiment","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train, Test 의 감정분류 결과 값 비교"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(ncols=2)\nfig.set_size_inches(12,5)\nsns.countplot(train['sentiment'], ax=axes[0])\nsns.countplot(output['sentiment'], ax=axes[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}