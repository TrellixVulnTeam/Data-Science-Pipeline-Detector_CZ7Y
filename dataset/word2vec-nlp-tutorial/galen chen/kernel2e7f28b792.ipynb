{"cells":[{"metadata":{"trusted":true,"_uuid":"771969fb0a87f3eecd3f83cd4e0b6f1a1c3d2c9e"},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt # 画图常用库\n\nimport pandas as pd\n\n\ntrain = pd.read_csv('../input/labeledTrainData.tsv', delimiter=\"\\t\")\ntest = pd.read_csv('../input/testData.tsv', delimiter=\"\\t\")\ntrain.head() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"621bed79a366a75b7667900764928e576ee19501"},"cell_type":"code","source":"print (train.shape)\nprint (test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"323ca8fbbaa8915bf6052c18aeab91efec3f11a9"},"cell_type":"code","source":"test.head()[\"review\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c0341a223c1808b81bc5007bccde94ac71b30fc6"},"cell_type":"code","source":"import re \n\ndef review_to_wordlist(review):\n    #只保留英文单词\n    review_text = re.sub('[^a-zA-Z]',' ',review) #把非英文字母替换成空格\n    words = review_text.lower()\n    return words\n\ny_train= train['sentiment']\n\ntrain_data=[]\nfor review in train['review']:\n    train_data.append(review_to_wordlist(review))\n\nprint(len(train_data))\ntrain_data\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"790dbcb49c1bd92d5508d71d1eaf022105d08cba"},"cell_type":"code","source":"train_data =np.array(train_data)\ntrain_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cbf5ce841fa932f8c761972fc700824a9eeb99cc"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ecb8f7b323891828e43c10ba08c4e636ba451a8"},"cell_type":"code","source":"#对test文本做提取\ntest_data =[]\nfor review in test['review']:\n    test_data.append(review_to_wordlist(review))\ntest_data=np.array(test_data)\ntest_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2762cc8688c0e5327e60f42fe4c206e1917281ab"},"cell_type":"code","source":"import nltk\nfrom nltk.stem.wordnet import WordNetLemmatizer\nnltk.download('wordnet')\nfrom nltk.corpus import stopwords\n\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cddb46f644f325db8f9c3bd9ef071ae25da0e2fe"},"cell_type":"code","source":"set(stopwords.words('english'))\nstop_words = set(stopwords.words('english'))\nlmtzr = WordNetLemmatizer() \n\ndef GetVocabulary(data):\n    vocab_dict={}\n    wid=0\n    for document in data:\n        words= document.split()\n        \n        for word in words:\n            word=lmtzr.lemmatize(word)\n            if word not in stop_words and word not in vocab_dict:\n                    \n                    vocab_dict[word]=wid\n                    \n                    wid += 1\n            else:\n                continue\n          \n    return vocab_dict\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"052210c0535bb6dfbd0f9ea5a1feb6b3c221238d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7efb92df3993668f987a8500b7771c51312d989b"},"cell_type":"code","source":"vocab_dict = GetVocabulary(train_data)\nprint('Number of all the unique words : '+ str(len(vocab_dict.keys())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"055082d059163982887c08785a81afdb67966b23"},"cell_type":"code","source":"def Document2Vector(vocab_dict,data):\n    word_vector = np.zeros(len(vocab_dict.keys()))\n    words = data.split()\n    for word in words:\n        word = word.lower()\n        if word in vocab_dict:\n            word_vector[vocab_dict[word]]+=1\n    return word_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"abeae27242498f2a23250b17b20247c1fc8b489a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1c756614a4f7094810a01a42691c488c6ff522a"},"cell_type":"code","source":"#把训练集的句子全部变成向量形式，这里面全是数字，每个词汇表里的单词 根据id排序的 出现在 该文章里的次数，即使没有出现 也是有 0\ntrain_matrix =[]\nfor document in train_data:\n#     words= document.split()\n#     for word in words:\n        word_vector = Document2Vector(vocab_dict,document)\n        train_matrix.append(word_vector)\n\nprint(len(train_matrix))  # 有多少个文档\ntrain_matrix[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2cb51b8d57d4a7b1114c2e4ce76b5b9f57a8ac49"},"cell_type":"code","source":"print(len(train_matrix[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f89d34dc733c9575455146cf372d47e0dab8e927"},"cell_type":"code","source":"def NaiveBayes_train(train_matrix,labels_train):\n    num_docs = len(train_matrix)\n    num_words = len(train_matrix[0])  # 对第一个样本取长度\n    \n     # 在每个分类下创建一个与词汇量大小相等的vector(即 numpy array) 用以计算每个单词在该类别下的频率\n    good_word_counter =np.ones(num_words) \n    bad_word_counter = np.ones(num_words) #计算每个word出现的次数，初始化为1. 即使用拉普拉斯平滑\n    \n    good_total_count =0 \n    bad_total_count = 0   #每一个类别 单词总的计数， 所有词出现在good里头的总数 good的总词数 （不去重 ）\n    \n    good_count =0  #good review的总数\n    bad_count = 0\n    \n    for i in range(num_docs):\n        if i%2000==0:\n            print('Train on the doc id:'+ str(i))\n        \n        if y_train[i]==0:   #0 is bad review\n            bad_word_counter += train_matrix[i]\n            bad_total_count += sum(train_matrix[i])\n            bad_count +=1\n        else:\n            good_word_counter += train_matrix[i]\n            good_total_count += sum(train_matrix[i])\n            good_count +=1\n            \n    #以下则是，每个单词 在各类别下出现的概率，并且取了log，为什么取log，就是怕太小变成0～这部分再看看了解下为什么\n    #并且注意 在分母上也要加上平滑部分\n    p_good_vector = np.log(good_word_counter/(good_total_count+num_words))\n    p_bad_vector = np.log(bad_word_counter/(bad_total_count+num_words))\n\n    return p_good_vector, np.log(good_count/num_docs), p_bad_vector,np.log(bad_count/num_docs)\n    \np_good_vector, p_good, p_bad_vector, p_bad = NaiveBayes_train(train_matrix, y_train.values)\n\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"388ddf90e6e69ed34a16c8f3131f803f003d07c7"},"cell_type":"code","source":"print(p_good_vector)\np_bad_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"699dc4c25e40dc6f2914817defe16f2a83c1a78d"},"cell_type":"code","source":"def predict(test_word_vector, p_good_vector,p_good,p_bad_vector,p_bad):\n    \n    good = sum(test_word_vector * p_good_vector)+ p_good\n    bad =  sum(test_word_vector * p_bad_vector )+ p_bad\n    \n    if good > bad:\n        return '1'\n    else:\n        return '0'\n    \npredictions =[]\ni =0\nfor document in test_data:\n    if i%2000 ==0:\n        print('test on the doc id: '+ str(i))\n    i+=1\n    test_word_vector =Document2Vector(vocab_dict,document)\n    ans= predict(test_word_vector,p_good_vector,p_good,p_bad_vector,p_bad)\n    predictions.append(ans)\n    \n  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61a8ecf64d0a8ad154b5bc9e881be1cb8e4898d1"},"cell_type":"code","source":"print(len(predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ad5ebb70939c3251c30c35c7a5ba443771afc29"},"cell_type":"code","source":"df = pd.DataFrame({\"id\": test['id'],\"sentiment\": predictions})\n\ndf.to_csv('submission1.csv',index = False, header=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4892dc239df7617052b5e948f64ea4daecbcf6d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a83659abe80b7c261320137526108407c9d0682"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4426b43cdc600b01d0ab05f66c8231f52a2a1628"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}