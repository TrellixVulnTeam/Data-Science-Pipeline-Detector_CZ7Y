{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from keras import Input\nimport keras\nfrom keras.optimizers import Adam\nimport keras.backend as K\nimport numpy as np\nfrom keras.layers import *\nfrom keras.layers.core import Dense, Dropout\nfrom keras.models import Sequential, Model\nfrom keras.layers.recurrent import LSTM\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom keras.backend.tensorflow_backend import set_session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_SENT_LENGTH = 100\nMAX_SENTS = 15\nMAX_NB_WORDS = 20000\nEMBEDDING_DIM = 100\nVALIDATION_SPLIT = 0.2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train = pd.read_csv('/kaggle/input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip', sep='\\t')\nprint(data_train.shape)\n\nfrom nltk import tokenize\nfrom keras.preprocessing.text import Tokenizer, text_to_word_sequence\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.np_utils import to_categorical\n\nreviews = []\nlabels = []\ntexts = []\n\nfor idx in range(data_train.review.shape[0]):\n    text = data_train.review[idx]\n    texts.append(text)\n    sentences = tokenize.sent_tokenize(text)\n    reviews.append(sentences)\n    \n    labels.append(data_train.sentiment[idx])\n\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS)\ntokenizer.fit_on_texts(texts)\n\ndata = np.zeros((len(texts), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n\nfor i, sentences in enumerate(reviews):\n    for j, sent in enumerate(sentences):\n        if j< MAX_SENTS:\n            wordTokens = text_to_word_sequence(sent)\n            k=0\n            for _, word in enumerate(wordTokens):\n                if k<MAX_SENT_LENGTH and tokenizer.word_index[word]<MAX_NB_WORDS:\n                    data[i,j,k] = tokenizer.word_index[word]\n                    k=k+1                    \n                    \nword_index = tokenizer.word_index\nprint('Total %s unique tokens.' % len(word_index))\n\nlabels = to_categorical(np.asarray(labels))\nprint('Shape of data tensor:', data.shape)\nprint('Shape of label tensor:', labels.shape)\n\nindices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata = data[indices]\nlabels = labels[indices]\nnb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n\nx_train = data[:-nb_validation_samples]\ny_train = labels[:-nb_validation_samples]\nx_val = data[-nb_validation_samples:]\ny_val = labels[-nb_validation_samples:]\n\nprint('Number of positive and negative reviews in traing and validation set')\nprint(y_train.sum(axis=0))\nprint(y_val.sum(axis=0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_layer = Embedding(len(word_index) + 1,\n                            EMBEDDING_DIM,\n                            input_length=MAX_SENT_LENGTH)\n\nsentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\nembedded_sequences = embedding_layer(sentence_input)\nl_lstm = Bidirectional(LSTM(100))(embedded_sequences)\nsentEncoder = Model(sentence_input, l_lstm)\n\nreview_input = Input(shape=(MAX_SENTS,MAX_SENT_LENGTH), dtype='int32')\nreview_encoder = TimeDistributed(sentEncoder)(review_input)\nl_lstm_sent = Bidirectional(LSTM(100))(review_encoder)\npreds = Dense(2, activation='softmax')(l_lstm_sent)\nmodel = Model(review_input, preds)\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['acc'])\n\nprint(\"model fitting - Hierachical LSTM\")\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class AttentionLayer(Layer):\n    def __init__(self, **kwargs):\n        super(AttentionLayer, self).__init__(** kwargs)\n    \n    def build(self, input_shape):\n        assert len(input_shape)==3\n        # W.shape = (time_steps, time_steps)\n        self.W = self.add_weight(name='att_weight', \n                                 shape=(input_shape[1], input_shape[1]),\n                                 initializer='uniform',\n                                 trainable=True)\n        super(AttentionLayer, self).build(input_shape)\n\n    def call(self, inputs, mask=None):\n        # inputs.shape = (batch_size, time_steps, seq_len)\n        x = K.permute_dimensions(inputs, (0, 2, 1))\n        # x.shape = (batch_size, seq_len, time_steps)\n        # general\n        a = K.softmax(K.tanh(K.dot(x, self.W)))\n        a = K.permute_dimensions(a, (0, 2, 1))\n        outputs = a * inputs\n        outputs = K.sum(outputs, axis=1)\n        return outputs\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0], input_shape[2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\nembedded_sequences = embedding_layer(sentence_input)\nl_lstm = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences)\nl_dense = TimeDistributed(Dense(200))(l_lstm)\nl_att = AttentionLayer()(l_dense)\nsentEncoder = Model(sentence_input, l_att)\n\nreview_input = Input(shape=(MAX_SENTS,MAX_SENT_LENGTH), dtype='int32')\nreview_encoder = TimeDistributed(sentEncoder)(review_input)\nl_lstm_sent = Bidirectional(GRU(100, return_sequences=True))(review_encoder)\nl_dense_sent = TimeDistributed(Dense(200))(l_lstm_sent)\nl_att_sent = AttentionLayer()(l_dense_sent)\npreds = Dense(2, activation='softmax')(l_att_sent)\nmodel = Model(review_input, preds)\nmodel.summary()\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['acc'])\n\nprint(\"model fitting - Hierachical attention network\")\nmodel.fit(x_train, y_train, validation_data=(x_val, y_val),\n          epochs=10, batch_size=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_test = pd.read_csv('/kaggle/input/word2vec-nlp-tutorial/testData.tsv.zip', sep='\\t')\nprint(data_test.shape)\n\nfrom nltk import tokenize\nfrom keras.preprocessing.text import Tokenizer, text_to_word_sequence\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.np_utils import to_categorical\n\nreviews = []\ntexts = []\n\nfor idx in range(data_test.review.shape[0]):\n    text = data_test.review[idx]\n    texts.append(text)\n    sentences = tokenize.sent_tokenize(text)\n    reviews.append(sentences)\n    \ntokenizer = Tokenizer(num_words=MAX_NB_WORDS)\ntokenizer.fit_on_texts(texts)\n\ndata = np.zeros((len(texts), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n\nfor i, sentences in enumerate(reviews):\n    for j, sent in enumerate(sentences):\n        if j< MAX_SENTS:\n            wordTokens = text_to_word_sequence(sent)\n            k=0\n            for _, word in enumerate(wordTokens):\n                if k<MAX_SENT_LENGTH and tokenizer.word_index[word]<MAX_NB_WORDS:\n                    data[i,j,k] = tokenizer.word_index[word]\n                    k=k+1                    \n                    \nword_index = tokenizer.word_index\nprint('Total %s unique tokens.' % len(word_index))\nprint('Shape of data tensor:', data.shape)\n\nindices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata = data[indices]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pre=model.predict(data)\nvalue=np.argmax(pre, axis=1)\ndf = pd.read_csv('/kaggle/input/word2vec-nlp-tutorial/testData.tsv.zip', sep='\\t')\ndf.review =value\ndf.to_csv('submit.csv',header=True, index=False, sep=',')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}