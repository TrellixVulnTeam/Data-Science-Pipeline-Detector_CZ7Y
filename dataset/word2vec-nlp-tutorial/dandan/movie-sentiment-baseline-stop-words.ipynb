{"cells":[{"metadata":{"_cell_guid":"9fc86914-828d-4a46-ba7f-c1f4980e1cc9","_uuid":"89d41aa529dd673b83937281bec04f0a75af8b54"},"cell_type":"markdown","source":"# Movie Sentiment Analysis\nhttps://www.kaggle.com/c/word2vec-nlp-tutorial/"},{"metadata":{"_cell_guid":"d6b1ab40-34da-4645-98c4-4b61c97895f6","_uuid":"0259bd59e4fad41ffd78aa5a510a83ef07daab7f","collapsed":true},"cell_type":"markdown","source":" 拿到数据首先读入拿到数据"},{"metadata":{"_cell_guid":"a89760f4-9075-42be-b4c4-c8c87d175c88","_uuid":"4429a082ddabcb23261daec29ee1ae9e68ba2a2e","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt # 画图常用库\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\n\ntrain = pd.read_csv('../input/labeledTrainData.tsv', delimiter=\"\\t\")\ntest = pd.read_csv('../input/testData.tsv', delimiter=\"\\t\")\n\ndata_train, data_vali, labels_train, labels_vali = train_test_split(\n    train,\n    train.sentiment, \n    test_size=0.2, \n    random_state=1)  \nprint(data_train.head())  ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"76be376a-0c66-4001-ac99-9b968ce62781","_uuid":"bc7497787d1d19eee918fec0faabe29669a8b3fb","trusted":true},"cell_type":"code","source":"print (data_train.shape, data_vali.shape, labels_train.shape, labels_vali.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"91a1a92edbc3b5542840fbd266b793096b1f1861"},"cell_type":"code","source":"labels_train=np.array(labels_train)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"45dcacb4-53c6-46ff-b738-36aa68fc896f","_uuid":"f8e259bb2ef260294478c4ec7929cdd10b68d84a"},"cell_type":"markdown","source":"查看一下数据"},{"metadata":{"_cell_guid":"708ea854-0c20-4ac0-97b7-90075d8f63c4","_uuid":"ad1c1cf6085fdf9d83c34c68b324f1c73e710315","trusted":true},"cell_type":"code","source":"import re  #正则表达式\n\ndef review_to_wordlist(review):\n#     print(review)\n\n#   只保留英文单词\n    review_text = re.sub(\"[^a-zA-Z]\",\" \", review)\n#     print (review_text)\n    \n#   变成小写\n    words = review_text.lower()\n    \n    return(words)\n\n#y_train = train['sentiment']\ntrain_data = []\nfor string in data_train['review']:\n    train_data.append(review_to_wordlist(string))        \ntrain_data = np.array(train_data)\n\nvali_data = []\nfor string in data_vali['review']:\n    vali_data.append(review_to_wordlist(string))        \nvali_data = np.array(vali_data)\n\ntest_data = []\nfor string in test['review']:\n    test_data.append(review_to_wordlist(string))    \ntest_data = np.array(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"81b8de134d1657e9457f55e08bedb55ee532b860"},"cell_type":"code","source":"print(train_data.shape,vali_data.shape,test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2514583c-d410-4dd8-9ce2-db907d4e3750","_uuid":"83cd36c185194d6513246af09afb76e20241b380","trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\ndef create_vocab(data):\n    vocab=set([])\n    for item in data:\n        tmp=set(item.split())\n        vocab.update(tmp)\n    vocab.difference_update(stop_words)\n    return {key:value for (key,value) in zip(vocab,range(len(vocab)))}\n\nvocab=create_vocab(np.append(train_data,vali_data))\n\ndef vectorize(vocab,data):\n    res_vector=np.zeros(len(vocab))\n    tmp=data.split()\n    for word in tmp:\n            if word in vocab:res_vector[vocab[word]] +=1\n    return res_vector\n\nprint(\"number of unique words is \",len(vocab))\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"aea2b80a-5aca-4d0a-992f-1585c7a9ef62","_uuid":"87e7ad9872285cf1460afcc8eeae3af9f9438bd4","scrolled":true,"trusted":true},"cell_type":"code","source":"def my_Bayes_Train(train_data,vocab):\n    pos_vector,neg_vector=np.zeros([len(vocab)]),np.zeros([len(vocab)])\n    pos_count,neg_count=0,0\n    for i in range(len(train_data)):\n        if i % 1000 == 0:\n            print ('Train on the doc id:' + str(i))\n        \n        if  labels_train[i]==1:\n            pos_vector += vectorize(vocab,train_data[i])\n            pos_count +=1\n        else:\n            neg_vector += vectorize(vocab,train_data[i])\n            neg_count +=1\n    pos_word_count,neg_word_count=sum(pos_vector),sum(neg_vector)\n#pos_unique_word_count,neg_unique_word_count=np.count_nonzero(pos_vector),np.count_nonzero(neg_vector)\n\n\n    print(pos_count,neg_count,pos_word_count,neg_word_count)\n\n#p_pos_vector=np.log((pos_vector+np.ones([len(vocab)]))/(pos_word_count+pos_unique_word_count))\n#p_neg_vector=np.log((neg_vector+np.ones([len(vocab)]))/(neg_word_count+neg_unique_word_count))\n    p_pos_vector=np.log((pos_vector+np.ones([len(vocab)]))/(pos_word_count+len(vocab)))\n    p_neg_vector=np.log((neg_vector+np.ones([len(vocab)]))/(neg_word_count+len(vocab)))\n    p_pos=np.log(pos_count/(pos_count+neg_count))\n    p_neg=np.log(neg_count/(pos_count+neg_count))\n    return p_pos_vector, p_pos, p_neg_vector, p_neg\n\np_pos_vector, p_pos, p_neg_vector, p_neg = my_Bayes_Train(train_data,vocab)                  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"53ed2b5d9f0f7bfc738ec23d323576ff829bcd7d"},"cell_type":"code","source":"def Predict(test_word_vector,p_pos_vector, p_pos, p_neg_vector, p_neg):\n    \n    pos = sum(test_word_vector * p_pos_vector) + p_pos\n    neg = sum(test_word_vector * p_neg_vector) + p_neg\n   # print (\"pos=\",pos,\"neg=\",neg)\n    if pos > neg:\n        return 1\n    else:\n        return 0\npredictions_baseline=[]\n\nfor review in vali_data:\n    review_vector=vectorize(vocab,review)\n    predictions_baseline.append(Predict(review_vector,p_pos_vector, p_pos, p_neg_vector, p_neg))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8333796fc36b94259f43a3b62a72bd29f612e6b"},"cell_type":"code","source":"predictions_test=[]\n\nfor review in test_data:\n    review_vector=vectorize(vocab,review)\n    predictions_test.append(Predict(review_vector,p_pos_vector, p_pos, p_neg_vector, p_neg))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"80a50d13588b03856c25a1db26a601cf5aaab83d"},"cell_type":"code","source":"df = pd.DataFrame({\"id\": test['id'],\"sentiment\": predictions_test})\ndf.to_csv('dandan_movie_submission_stopwords.csv',index = False, header=True)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}