{"cells":[{"metadata":{"trusted":true,"_uuid":"27c16664d4767a94f78485edf204a55bf9f2fdd3"},"cell_type":"code","source":"#Well lets do this.\n#At first import what we need.\n\nimport os\nprint(os.listdir(\"../input\"))\nimport pandas as pd\nimport numpy as np\nimport re\nfrom bs4 import BeautifulSoup\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import AdaBoostClassifier\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ca2a7ac926f136e7c8b86b50bc031906d68fecb"},"cell_type":"code","source":"#panda does the reading  and saves as DataFrame\ntrain = pd.read_csv(\"../input/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\nprint(train.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96e282625cbd58317d93680ac81ba09b08f0280f"},"cell_type":"code","source":"#Raw Unclean Review Text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"746657793b9fceb7e23cd6c03b6eda67f3a3526c"},"cell_type":"code","source":"\nprint(train.review[0])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0853b5b605eebea8336b08a84625aba12084e9f6"},"cell_type":"code","source":"#Using BeautifulSoup to clean data initially & remove html tags and comments \n\n\nparsedRev = BeautifulSoup(train.review[0],\"html.parser\")\n\n\n#Print the result to compare with Unclean data \nprint(parsedRev.get_text())","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"2afb9a7ad8af10c0785675425b6797bf33445a48"},"cell_type":"code","source":"#We can see the result has Numbers and Symbols in it. Not good for \"Bag of Words\". Lets start removing.\n\n\n\ncleanRev = re.sub(\"[^a-zA-Z]\",\" \",parsedRev.get_text())\nprint(cleanRev)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a54ff259ab944b992c782f9b97c243015b735119"},"cell_type":"code","source":"# changing all the words to lowercase to create a \"bag of words\"\nlcCleanRev = cleanRev.lower()\n\n# Split to create an array from which  \"stop words\" will be removed\nwords = lcCleanRev.split()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"05c9f9f463dd5106d1e8f146759e0dc2c2ebeee3"},"cell_type":"code","source":"# Stopwords from nltk are used in this phase\n#some stopwords in english language are\n## print(stopwords.words(\"english\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a18aa9e73da87c5955aad113b153d19fb4f2bd50"},"cell_type":"code","source":"#removing most common words from split array\nbow = [w for w in words if w not in stopwords.words(\"english\")]\n\n#Bag of Words\nprint(bow)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"037fc878124e818aaabf683b435a2074b43b6f13"},"cell_type":"code","source":"# Function to do the make collection of cleaned text using all the reviews\n\ndef review_to_words(raw_review):\n    review_text = BeautifulSoup(raw_review,\"html.parser\").get_text()\n    letters_only = re.sub(\"[^a-zA-Z]\",\" \",review_text)\n    words = letters_only.lower().split()\n    \n    #create a set of stopwords so that we don't have to access corpus to search for a stopword\n    stop = set(stopwords.words(\"english\"))\n    \n    #removing stopwords from the raw_review\n    meaningful_words = [w for w in words if w not in stop]\n    \n    return(\" \".join(meaningful_words))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"438e0c1a7fa772662c1452debea13ffcef39f437"},"cell_type":"code","source":"# Just Checking\ncheck_review = review_to_words(train.review[0])\nprint(check_review)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"edeca4c82429941927553d7c07aa09ac4e684618"},"cell_type":"code","source":"#number of reviews\nnum_reviews = train.review.size\nprint(\"number of reviews :\",num_reviews)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c6b63a4b666f825bf062872ac6d1395277f89cfc"},"cell_type":"code","source":"#storing all reviews in a list\nclean_train_reviews = []\nfor i in range(num_reviews):\n    clean_train_reviews.append(review_to_words(train.review[i]))\n    if(i%5000==0):\n        print(\"Breathe In... Breathe Out\")\nprint(\"Cleaning Completed\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ceeea0e2d8c9d0e0f6d3c2238f9f363ba69e50a"},"cell_type":"code","source":"print(\"Creating a Bag of Words: \")\n\n# We use CountVectorizer imported from sklearn.feature_extraction.text to create token counts of document\n\n\n# Setting Parameters as None\nvectorizer = CountVectorizer(analyzer=\"word\",\n                            tokenizer=None,\n                            preprocessor=None,\n                            stop_words=None,\n                            max_features=5000)\n\n# We train the classifer using fit_transform() method\ntrain_data_features = vectorizer.fit_transform(clean_train_reviews)\n\n#change the classifier into array\ntrain_data_features = train_data_features.toarray()\nprint(train_data_features.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85464f91cf03fbd3a79fbc14d168ec932996bed1"},"cell_type":"code","source":"#see all the features names\nvocab = vectorizer.get_feature_names()\nprint(\" , \".join(vocab[0:10]),\" . . . . \",\" , \".join(vocab[-10:]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"82298613ecfd35e575df3086de1c1d37043703ff"},"cell_type":"code","source":"#frequency of each word is found using np.sum()\ndist = np.sum(train_data_features,axis=0)\nct = 0\nfor tag,count in zip(vocab,dist):\n    print(tag,\":\",count,end=\"\\n \")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f4c64bfd8592d73b53d4898c5e24a4596e7ff8f"},"cell_type":"code","source":"#Check if words starting with any alphabet is missing or not?\nstartswith = []\nfor val in vocab:\n    if(val[0] not in startswith):\n        startswith.append(val[0])\nprint(startswith)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c0b252b1e9e2ce5a7e49a04b160355e273b2b146"},"cell_type":"code","source":"#counting the total numbers of different words starting with each alphabet\ncounts = np.zeros((len(startswith)),dtype=np.int)\nfor val in vocab:\n    index = startswith.index(val[0])\n    counts[index] += 1\nprint(counts)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d9b53404823e1533392d74dd86bf4e102824c50"},"cell_type":"code","source":"# Lets do some Plotting to visually show above information\nplt.figure(1,figsize=(15,5))\nplt.plot(counts)\nnums = [i for i in range(26)]\nplt.xticks(nums,startswith)\nplt.grid()\nplt.ylabel(\"frequency\")\nprint(plt.show())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d7b375f2f91d0f6b665e68bddc558ecce1702d7"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a89742688f3a301ef63dd7ee86237d2562294c11"},"cell_type":"code","source":"# Using Random Forrest Classifier for classification\nforest = RandomForestClassifier(n_estimators = 100, criterion = \"entropy\")\nprint(\"Fitting RandomForest\")\nforest = forest.fit(train_data_features,train[\"sentiment\"])\nprint(\"RandomForest Done.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b910c3a6ecaf719d19d1effafa9280bc178a9126"},"cell_type":"code","source":"# Using Naive-Bayes\n\nnaive = MultinomialNB()\nprint(\"Fitting NaiveBayes . . . \")\nnaive.fit(train_data_features,train[\"sentiment\"])\nprint(\"NaiveBayes Done.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c6d0477fb7d099d6c00c20a30b7fb8c50c79b51","scrolled":true},"cell_type":"code","source":"adaboost = AdaBoostClassifier(n_estimators = 100)\nprint(\"Fitting AdaBoost . . . \")\nadaboost.fit(train_data_features,train[\"sentiment\"])\nprint(\"Fitting completed.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eeab060a3566b83b0ec01c19a6677496110f1197"},"cell_type":"code","source":"#Now lets check against Test Cases\ntest = pd.read_csv(\"../input/testData.tsv\",header=0,delimiter=\"\\t\",quoting=3)\nprint(\"Shape :\",test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d98f0f7d4c9f1a66401309a59efabffb35b5e34"},"cell_type":"code","source":"num_reviews = len(test[\"review\"])\nclean_test_reviews = []\nprint(\"Cleaning and parsing . . . . \")\nfor i in range(0,num_reviews):\n    if((i+1)%5000 == 0):\n        print(i+1,\" reviews processed . . .\")\n    clean_review = review_to_words(test[\"review\"][i])\n    clean_test_reviews.append(clean_review)\nprint(\"Processing complete.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"00954a8c784870b510820a3739bad3653077b52e"},"cell_type":"code","source":"test_data_features = vectorizer.fit_transform(clean_test_reviews)\ntest_data_features = test_data_features.toarray()\nprint(\"Prediction using RandomForest\")\nresult1 = forest.predict(test_data_features)\nprint(\"Prediction using Naive Bayes\")\nresult2 = naive.predict(test_data_features)\nprint(\"Prediction using AdaBoost\")\nresult3 = adaboost.predict(test_data_features)\nprint(\"Completed\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6fdc603622fe51c9dca01855585006ecc4b33907"},"cell_type":"code","source":"result = result1+result2+result3\nfor i in range(25000):\n    if(result[i]==1):\n        result[i]=0\n    elif(result[i]==2):\n        result[i]=1\n    elif(result[i]==3):\n        result[i]=1\noutput = pd.DataFrame(data = {\"id\":test[\"id\"],\"sentiment\":result})\noutput.to_csv(\"submission.csv\", index=False, quoting=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1f36b3992d60db93821d4f677a1b0a0db969f64b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8e07bd1fa5a2c6cbe6523e5f0a8b7503d99f5c33"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e53e783edb26eca866732013f32fa9eea86b65a3"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"bab325a4308f2b24f11f9869b8f0c2485eb74e97"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}