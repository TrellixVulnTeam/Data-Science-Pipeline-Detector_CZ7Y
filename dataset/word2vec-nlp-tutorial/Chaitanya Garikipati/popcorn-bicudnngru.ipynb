{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom bs4 import BeautifulSoup\nimport re, datetime, functools\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport tensorflow as tf\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.enable_eager_execution()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"TRAIN_DATA_PATH = '/kaggle/input/word2vec-nlp-tutorial/labeledTrainData.tsv'\nUNLABELD_TRAIN_DATA_PATH = '/kaggle/input/word2vec-nlp-tutorial/unlabeledTrainData.tsv'\nTEST_DATA_PATH = '/kaggle/input/word2vec-nlp-tutorial/testData.tsv'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(TRAIN_DATA_PATH, header=0, delimiter='\\t', quoting=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def review_to_words( raw_review ):\n    # Function to convert a raw review to a string of words\n    # The input is a single string (a raw movie review), and \n    # the output is a single string (a preprocessed movie review)\n    #\n    # 1. Remove HTML\n    review_text = BeautifulSoup(raw_review).get_text() \n    #\n    # 2. Remove non-letters        \n    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n    #\n    # 3. Convert to lower case, split into individual words\n    words = letters_only.lower().split()                             \n    #\n    # 4. In Python, searching a set is much faster than searching\n    #   a list, so convert the stop words to a set\n    stops = set(stopwords.words(\"english\"))                  \n    # \n    # 5. Remove stop words\n    meaningful_words = [w for w in words if not w in stops]   \n    #\n    # 6. Join the words back into one string separated by space, \n    # and return the result.\n    return( \" \".join( meaningful_words ))  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_review = review_to_words( train_df[\"review\"][0] )\nprint(clean_review)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['preprocessed_review'] = [review_to_words(r) for r in train_df['review']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_features = 5000\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(train_df['preprocessed_review'])\ntrain_data_features = tokenizer.texts_to_sequences(train_df['preprocessed_review'])\nprint(len(train_data_features))\ntrain_data_features = tf.keras.preprocessing.sequence.pad_sequences(train_data_features, maxlen=150, padding='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data_features.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"Input = tf.keras.layers.Input\nDense = tf.keras.layers.Dense\nGlobalAveragePooling1D = tf.keras.layers.GlobalAveragePooling1D\nDropout = tf.keras.layers.Dropout\nEmbedding = tf.keras.layers.Embedding\nGRU = tf.keras.layers.CuDNNGRU\nBidirectional = tf.keras.layers.Bidirectional","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model():\n    model = tf.keras.Sequential([\n        Embedding(input_dim=max_features, output_dim=128),\n        Bidirectional(GRU(32, return_sequences=True)),\n        GlobalAveragePooling1D(),\n        Dropout(0.03),\n        Dense(20, activation='elu'),\n        Dense(1, activation='sigmoid')\n    ])\n\n    model.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n    model.summary()\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = get_model()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Callbacks"},{"metadata":{"trusted":true},"cell_type":"code","source":"logdir = os.path.join(\"/tmp/logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\ntensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\ncallbacks = [\n    tf.keras.callbacks.ModelCheckpoint(filepath='./weights.hdf5', verbose=1, save_best_only=True),\n    tensorboard_callback,\n    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=8)\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(train_data_features,\n                    train_df['sentiment'], batch_size=100, epochs=20, validation_split=0.2, callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## inferance"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test=pd.read_csv(\"../input/word2vec-nlp-tutorial/testData.tsv\",header=0, delimiter=\"\\t\", quoting=3)\ndf_test.head()\ndf_test[\"review\"]=df_test.review.apply(lambda x: review_to_words(x))\ndf_test[\"sentiment\"] = df_test[\"id\"].map(lambda x: 1 if int(x.strip('\"').split(\"_\")[1]) >= 5 else 0)\ny_test = df_test[\"sentiment\"]\nlist_sentences_test = df_test[\"review\"]\nlist_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\nX_te = tf.keras.preprocessing.sequence.pad_sequences(list_tokenized_test, maxlen=150)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = model.predict(X_te)\ny_pred = (prediction > 0.5)\nfrom sklearn.metrics import f1_score, confusion_matrix\nprint('F1-score: {0}'.format(f1_score(y_pred, y_test)))\nprint('Confusion matrix:')\nconfusion_matrix(y_pred, y_test)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}