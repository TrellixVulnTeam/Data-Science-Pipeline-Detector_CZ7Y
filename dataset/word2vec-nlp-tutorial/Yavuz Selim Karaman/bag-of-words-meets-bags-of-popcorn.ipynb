{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Bag of Words Meets Bags of Popcorn - Sentiment Analysis","metadata":{}},{"cell_type":"markdown","source":"## 1. Importing Necessary Libraries","metadata":{}},{"cell_type":"code","source":"# utilities\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) \nimport re\nimport os\nimport pickle\nimport random\n\n# plotting and visualizing\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS\nimport seaborn as sns\n\n\n# sklearn\n\n # classifiers\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n # metrics\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\n\n # pipeline\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\n\n# nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer","metadata":{"execution":{"iopub.status.busy":"2021-08-27T22:38:06.92776Z","iopub.execute_input":"2021-08-27T22:38:06.928216Z","iopub.status.idle":"2021-08-27T22:38:09.082621Z","shell.execute_reply.started":"2021-08-27T22:38:06.928123Z","shell.execute_reply":"2021-08-27T22:38:09.081597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Preprocessing","metadata":{}},{"cell_type":"markdown","source":"### Extracting zip files","metadata":{}},{"cell_type":"code","source":"from zipfile import ZipFile\n\nwith ZipFile(\"../input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip\",\"r\") as z:\n    z.extractall(\".\")\n    \nwith ZipFile(\"../input/word2vec-nlp-tutorial/testData.tsv.zip\",\"r\") as z:\n    z.extractall(\".\")\n    \nprint(os.listdir(\"/kaggle/working/\"))","metadata":{"execution":{"iopub.status.busy":"2021-08-27T22:38:09.084464Z","iopub.execute_input":"2021-08-27T22:38:09.084912Z","iopub.status.idle":"2021-08-27T22:38:10.84903Z","shell.execute_reply.started":"2021-08-27T22:38:09.084863Z","shell.execute_reply":"2021-08-27T22:38:10.847916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loading the train and test data","metadata":{}},{"cell_type":"code","source":"path_train = \"/kaggle/working/labeledTrainData.tsv\"\npath_test = \"/kaggle/working/testData.tsv\"\n\ndf_train = pd.read_csv(path_train, delimiter = \"\\t\")\ndf_test = pd.read_csv(path_test, delimiter = \"\\t\")","metadata":{"execution":{"iopub.status.busy":"2021-08-27T22:38:10.85072Z","iopub.execute_input":"2021-08-27T22:38:10.851056Z","iopub.status.idle":"2021-08-27T22:38:11.572044Z","shell.execute_reply.started":"2021-08-27T22:38:10.851025Z","shell.execute_reply":"2021-08-27T22:38:11.570928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T22:38:11.575556Z","iopub.execute_input":"2021-08-27T22:38:11.57684Z","iopub.status.idle":"2021-08-27T22:38:11.60803Z","shell.execute_reply.started":"2021-08-27T22:38:11.576761Z","shell.execute_reply":"2021-08-27T22:38:11.607014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T22:38:11.609445Z","iopub.execute_input":"2021-08-27T22:38:11.609882Z","iopub.status.idle":"2021-08-27T22:38:11.62668Z","shell.execute_reply.started":"2021-08-27T22:38:11.609838Z","shell.execute_reply":"2021-08-27T22:38:11.625566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.shape, df_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-27T22:38:11.629548Z","iopub.execute_input":"2021-08-27T22:38:11.629949Z","iopub.status.idle":"2021-08-27T22:38:11.635894Z","shell.execute_reply.started":"2021-08-27T22:38:11.629905Z","shell.execute_reply":"2021-08-27T22:38:11.635122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T22:38:11.636939Z","iopub.execute_input":"2021-08-27T22:38:11.637341Z","iopub.status.idle":"2021-08-27T22:38:11.670857Z","shell.execute_reply.started":"2021-08-27T22:38:11.637312Z","shell.execute_reply":"2021-08-27T22:38:11.669599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Data Visualization","metadata":{}},{"cell_type":"markdown","source":"### Let's look at the each number of sentiment ","metadata":{}},{"cell_type":"code","source":"sns.countplot(df_train[\"sentiment\"])","metadata":{"execution":{"iopub.status.busy":"2021-08-27T22:38:11.67456Z","iopub.execute_input":"2021-08-27T22:38:11.675012Z","iopub.status.idle":"2021-08-27T22:38:11.849576Z","shell.execute_reply.started":"2021-08-27T22:38:11.674974Z","shell.execute_reply":"2021-08-27T22:38:11.848485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### There are same numbers of target. The dataset is balanced.","metadata":{}},{"cell_type":"markdown","source":"## 5. NLP Preprocessing","metadata":{}},{"cell_type":"markdown","source":"### Here we create a 0 to 24999 number list.","metadata":{}},{"cell_type":"code","source":"num_list = list(range(25000))","metadata":{"execution":{"iopub.status.busy":"2021-08-27T22:38:11.851814Z","iopub.execute_input":"2021-08-27T22:38:11.852155Z","iopub.status.idle":"2021-08-27T22:38:11.858057Z","shell.execute_reply.started":"2021-08-27T22:38:11.852122Z","shell.execute_reply":"2021-08-27T22:38:11.856816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Here we chose random number from the list.","metadata":{}},{"cell_type":"code","source":"i = random.choice(num_list)\ndf_train[\"review\"].iloc[i]","metadata":{"execution":{"iopub.status.busy":"2021-08-27T22:38:11.859524Z","iopub.execute_input":"2021-08-27T22:38:11.859913Z","iopub.status.idle":"2021-08-27T22:38:11.872751Z","shell.execute_reply.started":"2021-08-27T22:38:11.85988Z","shell.execute_reply":"2021-08-27T22:38:11.871547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = random.choice(num_list)\ndf_train[\"review\"].iloc[i]","metadata":{"execution":{"iopub.status.busy":"2021-08-27T22:38:11.874598Z","iopub.execute_input":"2021-08-27T22:38:11.875035Z","iopub.status.idle":"2021-08-27T22:38:11.884924Z","shell.execute_reply.started":"2021-08-27T22:38:11.874997Z","shell.execute_reply":"2021-08-27T22:38:11.883821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### I observe some html tags in text, so i clean it.","metadata":{}},{"cell_type":"code","source":"df_train[\"review\"] = df_train[\"review\"].str.replace(\"<br />\",\"\")","metadata":{"execution":{"iopub.status.busy":"2021-08-27T22:38:11.886461Z","iopub.execute_input":"2021-08-27T22:38:11.886989Z","iopub.status.idle":"2021-08-27T22:38:11.97381Z","shell.execute_reply.started":"2021-08-27T22:38:11.886938Z","shell.execute_reply":"2021-08-27T22:38:11.972662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Here we are doing preprocessing, we are cleaning the text. First we define stop_words list, stemmer and cleaning regex expression. After we do the following: first we clean the text with cleaning regex expression and we converted all text to lowercase. After we use strip() function to delete line breaks (\\n). Then we define a empty tokens list. We go over text and tokenize the text. And if token not in stop_words list we take the token and append to the tokens list. And if stem = True is given to us in function calling, we stem the token. Finally we join the tokens and make it a sentence.","metadata":{}},{"cell_type":"code","source":"stop_words = stopwords.words(\"english\")\nstemmer = SnowballStemmer(\"english\")\n\ncleaning = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n\ndef preprocess(text, stem = False):\n    # Remove link,user and special characters\n    text = re.sub(cleaning, ' ', str(text).lower()).strip()\n    tokens = []\n    for token in text.split():\n        if token not in stop_words:\n            if stem:\n                tokens.append(stemmer.stem(token))\n            else:\n                tokens.append(token)\n    return \" \".join(tokens)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T22:38:11.976621Z","iopub.execute_input":"2021-08-27T22:38:11.977119Z","iopub.status.idle":"2021-08-27T22:38:11.991149Z","shell.execute_reply.started":"2021-08-27T22:38:11.977069Z","shell.execute_reply":"2021-08-27T22:38:11.989926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.review = df_train.review.apply(lambda x: preprocess(x, stem = True))","metadata":{"execution":{"iopub.status.busy":"2021-08-27T22:38:11.992672Z","iopub.execute_input":"2021-08-27T22:38:11.993091Z","iopub.status.idle":"2021-08-27T22:39:24.740419Z","shell.execute_reply.started":"2021-08-27T22:38:11.993034Z","shell.execute_reply":"2021-08-27T22:39:24.739336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We've eliminated accents","metadata":{}},{"cell_type":"code","source":"from unicodedata import normalize\ndf_train[\"review\"] = df_train[\"review\"].apply(lambda text: normalize(\"NFKD\", str(text)).encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\"))","metadata":{"execution":{"iopub.status.busy":"2021-08-27T22:39:24.742269Z","iopub.execute_input":"2021-08-27T22:39:24.742594Z","iopub.status.idle":"2021-08-27T22:39:24.838599Z","shell.execute_reply.started":"2021-08-27T22:39:24.742563Z","shell.execute_reply":"2021-08-27T22:39:24.837694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We are deciding x and y and we split it to x_train and x_test","metadata":{}},{"cell_type":"code","source":"x = df_train.review\n\ny = df_train.sentiment\n\nprint(x.shape, y.shape) \n\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T22:39:24.841411Z","iopub.execute_input":"2021-08-27T22:39:24.841736Z","iopub.status.idle":"2021-08-27T22:39:24.853027Z","shell.execute_reply.started":"2021-08-27T22:39:24.841707Z","shell.execute_reply":"2021-08-27T22:39:24.851882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Predicting and Modeling","metadata":{}},{"cell_type":"markdown","source":"### Evaluating Function","metadata":{}},{"cell_type":"code","source":"def evaluate(model, vect):\n    model.fit(x_train_dtm, y_train)\n    y_pred = model.predict(x_test_dtm)\n    print(\"Accuracy: \", metrics.accuracy_score(y_test, y_pred))\n    print(\"F1 Score: \", metrics.f1_score(y_test, y_pred))\n    print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-08-27T22:39:24.85435Z","iopub.execute_input":"2021-08-27T22:39:24.854641Z","iopub.status.idle":"2021-08-27T22:39:24.861843Z","shell.execute_reply.started":"2021-08-27T22:39:24.854601Z","shell.execute_reply":"2021-08-27T22:39:24.860713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Vectorizing and transforming x_train and x_test","metadata":{}},{"cell_type":"code","source":"tfidf_vect = TfidfVectorizer(ngram_range = (1,2), min_df = 2)\n\nx_train_dtm = tfidf_vect.fit_transform(x_train)\nx_test_dtm = tfidf_vect.transform(x_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T22:39:24.863477Z","iopub.execute_input":"2021-08-27T22:39:24.864003Z","iopub.status.idle":"2021-08-27T22:39:35.917458Z","shell.execute_reply.started":"2021-08-27T22:39:24.863954Z","shell.execute_reply":"2021-08-27T22:39:35.916482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Defining the Models and Evaluating","metadata":{}},{"cell_type":"code","source":"MNB = MultinomialNB()\nLSVC = LinearSVC()\nLR = LogisticRegression(C = 2, max_iter = 1000)\nGNB = GaussianNB()\nBNB = BernoulliNB()\nKNC = KNeighborsClassifier()\nSVC = SVC()\nDTC = DecisionTreeClassifier()\nRFC = RandomForestClassifier()\nGBC = GradientBoostingClassifier()\n\nprint(\"Multinomial Classfier: \\n\")\nevaluate(MNB, tfidf_vect)\nprint(\"\\n\")\nprint(\"Linear SVC Classfier: \\n\")\nevaluate(LSVC, tfidf_vect)\nprint(\"\\n\")\nprint(\"Logistic Regression Classfier: \\n\")\nevaluate(LR, tfidf_vect)\nprint(\"\\n\")\nprint(\"Bernoulli Classfier: \\n\")\nevaluate(BNB, tfidf_vect)\nprint(\"\\n\")\nprint(\"K Neighbors Classfier: \\n\")\nevaluate(KNC, tfidf_vect)\nprint(\"\\n\")\nprint(\"SVC Classfier: \\n\")\nevaluate(SVC, tfidf_vect)\nprint(\"\\n\")\nprint(\"Decision Tree Classfier: \\n\")\nevaluate(DTC, tfidf_vect)\nprint(\"\\n\")\nprint(\"Random Forest Classfier: \\n\")\nevaluate(RFC, tfidf_vect)\nprint(\"\\n\")\nprint(\"Gradient Boosting Classfier: \\n\")\nevaluate(GBC, tfidf_vect)\nprint(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-08-27T22:39:35.918733Z","iopub.execute_input":"2021-08-27T22:39:35.919048Z","iopub.status.idle":"2021-08-27T23:05:42.439144Z","shell.execute_reply.started":"2021-08-27T22:39:35.919019Z","shell.execute_reply":"2021-08-27T23:05:42.436727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We have achieved to high accuracy as 90% in Linear SVC Classifier.","metadata":{}},{"cell_type":"markdown","source":"### Creating Wordcloud","metadata":{}},{"cell_type":"code","source":"def create_wordcloud(text):\n    stopwords = set(STOPWORDS)\n    wc = WordCloud(max_words = 25000, stopwords = stopwords)\n    wc.generate(str(text))\n    wc.to_file(\"wordcloud.png\")\n    print(\"Word Cloud saved successfully\")\n    path = \"wordcloud.png\"\n    display(Image.open(path))","metadata":{"execution":{"iopub.status.busy":"2021-08-27T23:05:42.441361Z","iopub.execute_input":"2021-08-27T23:05:42.44181Z","iopub.status.idle":"2021-08-27T23:05:42.449012Z","shell.execute_reply.started":"2021-08-27T23:05:42.441754Z","shell.execute_reply":"2021-08-27T23:05:42.447741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"create_wordcloud(df_train[df_train[\"sentiment\"] == 1].review) # positive","metadata":{"execution":{"iopub.status.busy":"2021-08-27T23:05:42.450234Z","iopub.execute_input":"2021-08-27T23:05:42.450579Z","iopub.status.idle":"2021-08-27T23:05:42.779645Z","shell.execute_reply.started":"2021-08-27T23:05:42.450544Z","shell.execute_reply":"2021-08-27T23:05:42.77874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"create_wordcloud(df_train[df_train[\"sentiment\"] == 0].review) # negative","metadata":{"execution":{"iopub.status.busy":"2021-08-27T23:05:42.780885Z","iopub.execute_input":"2021-08-27T23:05:42.781371Z","iopub.status.idle":"2021-08-27T23:05:43.032851Z","shell.execute_reply.started":"2021-08-27T23:05:42.781337Z","shell.execute_reply":"2021-08-27T23:05:43.031961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7. Creating Pipeline","metadata":{}},{"cell_type":"code","source":"model = Pipeline([(\"vectorizer\", tfidf_vect), (\"classifier\", LSVC)])\nmodel.fit(x_train, y_train)\npred = model.predict(x_test)\nconfusion_matrix(pred, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T23:05:43.035957Z","iopub.execute_input":"2021-08-27T23:05:43.036472Z","iopub.status.idle":"2021-08-27T23:05:56.322542Z","shell.execute_reply.started":"2021-08-27T23:05:43.036433Z","shell.execute_reply":"2021-08-27T23:05:56.321741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(sentence):\n    if model.predict(sentence) == 0:\n        print(\"negative\")\n    else:\n        print(\"positive\")","metadata":{"execution":{"iopub.status.busy":"2021-08-27T23:05:56.323959Z","iopub.execute_input":"2021-08-27T23:05:56.32448Z","iopub.status.idle":"2021-08-27T23:05:56.329291Z","shell.execute_reply.started":"2021-08-27T23:05:56.32443Z","shell.execute_reply":"2021-08-27T23:05:56.328324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ex = [\"It is so nice\"]\npredict(ex)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T23:05:56.330528Z","iopub.execute_input":"2021-08-27T23:05:56.331162Z","iopub.status.idle":"2021-08-27T23:05:56.356111Z","shell.execute_reply.started":"2021-08-27T23:05:56.331111Z","shell.execute_reply":"2021-08-27T23:05:56.354672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ex = [\"This is so bad\"]\npredict(ex)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T23:06:25.203364Z","iopub.execute_input":"2021-08-27T23:06:25.203753Z","iopub.status.idle":"2021-08-27T23:06:25.212798Z","shell.execute_reply.started":"2021-08-27T23:06:25.203717Z","shell.execute_reply":"2021-08-27T23:06:25.211635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Saving the best Model","metadata":{}},{"cell_type":"code","source":"with open(\"model_pickle\", \"wb\") as f:\n    pickle.dump(LSVC, f)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T23:05:56.357881Z","iopub.execute_input":"2021-08-27T23:05:56.358242Z","iopub.status.idle":"2021-08-27T23:05:56.377352Z","shell.execute_reply.started":"2021-08-27T23:05:56.358198Z","shell.execute_reply":"2021-08-27T23:05:56.376092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loading the Model\n","metadata":{}},{"cell_type":"code","source":"with open(\"model_pickle\", \"rb\") as f:\n    loaded_model = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T23:05:56.378894Z","iopub.execute_input":"2021-08-27T23:05:56.379253Z","iopub.status.idle":"2021-08-27T23:05:56.39397Z","shell.execute_reply.started":"2021-08-27T23:05:56.379219Z","shell.execute_reply":"2021-08-27T23:05:56.392628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loaded_model.predict(x_test_dtm)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T23:05:56.395647Z","iopub.execute_input":"2021-08-27T23:05:56.396162Z","iopub.status.idle":"2021-08-27T23:05:56.415083Z","shell.execute_reply.started":"2021-08-27T23:05:56.396112Z","shell.execute_reply":"2021-08-27T23:05:56.41348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### As you can see, we could save the model and load the model and use it","metadata":{}}]}