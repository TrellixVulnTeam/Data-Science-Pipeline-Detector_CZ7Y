{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#import required packages\n#basics\nimport pandas as pd \nimport numpy as np\n#nlp\n\nimport re    #for regex\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer \nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import TweetTokenizer\n\n\n#FeatureEngineering\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\nfrom sklearn.linear_model import LogisticRegression\n\nlem = WordNetLemmatizer()\ntokenizer=TweetTokenizer()\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"81c5a4068ada952972401e06d663260cd9cc974f"},"cell_type":"markdown","source":"## DATA CLEANING AND TEXT PROCESSING\nData collected is  review from imdb as raw html, so it conatins html tags which need to be removed so we will use beautifulsoup library for this task\n \n Removing HTML Markup: The BeautifulSoup Package\n \n Now, we need to decide how to deal with frequently occurring words that don't carry much meaning. Such words are called \"stop words\"; in English they include words such as \"a\", \"and\", \"is\", and \"the\".  We will use NLTK Library to solve this problem\n \n Dealing with Punctuation, Numbers and Stopwords: NLTK and regular expressions\n \n -------------------------------------------------------"},{"metadata":{"trusted":true,"_uuid":"4b5cf3badc55a784660cdffb2758c7bac5d2a113"},"cell_type":"code","source":"train = pd.read_csv(\"../input/labeledTrainData.tsv\", header=0, \\\n                    delimiter=\"\\t\", quoting=3,encoding='utf-8') \ntest = pd.read_csv(\"../input/testData.tsv\", header=0, \\\n                    delimiter=\"\\t\", quoting=3,encoding='utf-8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85672947459e6947897efc558981085b597bee88"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"52aba40f2658da10ed9761c2f773b0022352329e"},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ef122c2b3d695ad51864676651e0455a2fe8e20"},"cell_type":"code","source":"train['review'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad5398991c947f3fdd155d99fdd86705c20903f0"},"cell_type":"code","source":"print (\"number of rows for sentiment 1: {}\".format(len(train[train.sentiment==1])))\nprint ( \"number of rows for sentiment 0: {}\".format(len(train[train.sentiment==0])))\n#sentiments are equally split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9dea2c722acecc745fbbb1fc1d48146e4ed1d74d"},"cell_type":"code","source":"#concat both train and test\nmerge=pd.concat([train[['id','review']],test[['id','review']]])\ndf=merge.reset_index(drop=True)\nprint(df.head())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fda32429d3864ed498ac84189861768e6972217f"},"cell_type":"code","source":"from bs4 import BeautifulSoup\ndef review_to_words( raw_review ):\n    # 1. Remove HTML\n    review_text = BeautifulSoup(raw_review, 'lxml').get_text() \n    \n    # 2. Remove non-letters with regex\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n    \n    # 3. Convert to lower case, split into individual words\n    words = letters_only.lower().split()                           \n    \n    # 4. Create set of stopwords\n    stops = set(stopwords.words(\"english\"))                  \n    \n    # 5. Remove stop words\n    meaningful_words = [w for w in words if not w in stops]   \n    \n    # 6. Join the words back into one string separated by space, \n    # and return the result.\n    return( \" \".join( meaningful_words ))   \n\ndf['review_cleaned']=df['review'].apply(review_to_words)\ndf_justclean = df[['id','review_cleaned']]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eed74c474e1ace5920e09e1ce4fe0842b229d283"},"cell_type":"markdown","source":"## FEATURE GENERATION\n\n-------------------------------"},{"metadata":{"trusted":true,"_uuid":"1ba1d63ac5552e5a23b01cb3a4bb415ea1f00243"},"cell_type":"code","source":"df['count_word']=df[\"review_cleaned\"].apply(lambda x: len(str(x).split()))             #Word count in each comment:\ndf['count_unique_word']=df[\"review_cleaned\"].apply(lambda x: len(set(str(x).split()))) #split creates groups\n\ndf['count_letters']=df[\"review_cleaned\"].apply(lambda x: len(str(x)))                  #Letter count\n                                                                                       \ndf[\"mean_word_len\"] = df[\"review_cleaned\"].apply(                                           \n    lambda x: np.mean([len(w) for w in str(x).split()]))                               #Average length of the words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3e7d33f44a4a4439896ff1a576c0918ecbc396f0"},"cell_type":"code","source":"df['word_unique_percent']=df['count_unique_word']*100/df['count_word']                    #Word count percent in each comment:","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a1016d2c40361062a2c4724e5aa6f7a0eabd4ef"},"cell_type":"code","source":"#serperate train and test features\ntrain_feats=df.iloc[0:len(train),] \ntest_feats=df.iloc[len(train):,]\n\ntrain_tags=train['sentiment']\ntrain_feats=pd.concat([train_feats,train_tags],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e05431a2e0cf3ad95e358f7e9b0ddda427cc8cd"},"cell_type":"code","source":"train_feats.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"889c79ceb70266ad19884b2b877133641898ad28"},"cell_type":"code","source":"train_feats.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9271bc466ddc804029abe1f3048fd5884867f51"},"cell_type":"code","source":"# place bounds \ntrain_feats['count_word'].loc[train_feats['count_word']>150] = 150                   # set columns with count sent longer than 90 to 90\ntrain_feats['count_unique_word'].loc[train_feats['count_unique_word']>136] = 136     # set columns with count sent longer than 90 to 90\ntrain_feats['count_letters'].loc[train_feats['count_letters']>1154] = 1154","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15ba151ea0c0337d89b7022d38e4c1abc71fea2b"},"cell_type":"code","source":"train_feats[train_feats.sentiment==0].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d621b19312a35a3f57c589f5f0e25a056db23e7"},"cell_type":"code","source":"train_feats.rename(columns={\"sentiment\": \"target_sentiment\"},inplace=True)\ntrain_feats.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa576aba095192d8698c3e25f03a0f32df946b99"},"cell_type":"code","source":"\nmerge['cleaned_review']=merge['review'].apply(review_to_words)\ncorpus = merge.cleaned_review\n#dictionary of apostrophe words\nAPPO = {\n\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"that's\" : \"that is\",\n\"there's\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\",\n\"tryin'\":\"trying\"}\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"54ba5d1e9b60551ffe2c47eada7797c60d268ea9"},"cell_type":"markdown","source":"## CLEANING AND GENERATING N-GRAMS USING Tfidvectorizer\nNow that we have our training reviews tidied up, how do we convert them to some kind of numeric representation for machine learning? One common approach is called a Bag of Words. The Bag of Words model learns a vocabulary from all of the documents, then models each document by counting the number of times each word appears\n\nexample:- \n\nSentence 1: \"The cat sat on the hat\"\n\nSentence 2: \"The dog ate the cat and the hat\"\n\nFrom these two sentences, our vocabulary is as follows:\n\n{ the, cat, sat, on, hat, dog, ate, and }\n\nFeature vector for Sentence 1 is:  { 2, 1, 1, 1, 1, 0, 0, 0 }\n\nFeature vector for Sentence 2 is:  { 3, 1, 0, 0, 1, 1, 1, 1}\n\n\n\n------------------------------"},{"metadata":{"trusted":true,"_uuid":"a063130bde9192d88fffd5ee78d69fbc1e826a27"},"cell_type":"code","source":"#Its important to use a clean dataset before creating count features.\n\ndef clean(comment):\n    words=tokenizer.tokenize(comment)                                   #Split the sentences into word\n    words=[APPO[word] if word in APPO else word for word in words]\n    words=[lem.lemmatize(word,\"v\") for word in words]                   #lemmatizes based on position v\n    clean_sent=\" \".join(words)\n    return(clean_sent)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f9a5c3fd7070153ed3f0a5fdb8f58b19d55b74e"},"cell_type":"code","source":"clean_corpus=corpus.apply(lambda x :clean(x))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ffce78e8718b8f99a35c33405e3848ecfeeee370"},"cell_type":"code","source":"# create vectorizer\ntf_vectorizer = TfidfVectorizer(max_df=0.90,min_df=0.001,  max_features=5000, \n            strip_accents='unicode', analyzer='word',ngram_range=(1,2),\n            use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')\ntf = tf_vectorizer.fit_transform(clean_corpus)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d51da6e3b6b1daa7031caf28934594c07986a35"},"cell_type":"code","source":"features=np.array(tf_vectorizer.get_feature_names())\nprint (features)\nprint (len(features))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5be24babfd12d9e432cb58e803e6f2463c7468bb"},"cell_type":"code","source":"df1 = pd.DataFrame(tf.toarray(),columns=features)\ndf1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd3c24dd6be495a5961c9b60e866bab4a1dfe16d"},"cell_type":"code","source":"merged_df=pd.concat([df[['id','count_word','count_unique_word','count_letters','mean_word_len','word_unique_percent']],df1],axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28e357e5caa30ebb2b7dd8e61601c9ab9c7fba5e"},"cell_type":"code","source":"X=merged_df.iloc[:len(train),1:]\nY=train['sentiment']\nunk_features=merged_df.iloc[len(train):,1:]\nunk_ids=merged_df.iloc[len(train):,0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65ffc6e041246e6b1bd4a526a0d8c6caa8a1f62c"},"cell_type":"markdown","source":"## MODEL SELECTION AND PREDICTION\nNow, we have numeric training features from the Bag of Words and the original sentiment labels for each feature vector, so let's do some supervised learning. We will be using LogisticRegression model \nTo select best parameters for our model we will use GridSearch \n\n---------------------------------"},{"metadata":{"trusted":true,"_uuid":"fa011a6c6108e5c2720dfab00a095854863eec20"},"cell_type":"code","source":"# from sklearn.linear_model import LogisticRegression #logistic regression\n# #Logistic Regression has the highest accuracy\n# from sklearn.model_selection import GridSearchCV\n# C=[0.05,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n# penalty=['l1','l2']\n# hyper={'C':C,'penalty':penalty}\n# gd=GridSearchCV(estimator=LogisticRegression(random_state=0),param_grid=hyper,verbose=True,cv=5,scoring='roc_auc')\n# gd.fit(X,Y)\n# print(gd.best_score_)\n# print(gd.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4bfa1b123490844305b0550af100cdab57c68f20"},"cell_type":"markdown","source":"## GENERATE SUBMISSION FILE\n\n\n-------------------------------------------"},{"metadata":{"trusted":true,"_uuid":"f8237df86594c481e0e9e47a4f889b09110c4b39"},"cell_type":"code","source":"clf=LogisticRegression(C=0.9,penalty='l2',n_jobs=-1)\nrounds = 15\nfor i in range(rounds):\n    clf.set_params(random_state = i + 1)\n    clf.fit(X, Y)\n    preds = clf.predict(unk_features)\n    \nsample_sub=pd.read_csv('../input/sampleSubmission.csv')\nresult=pd.DataFrame({'id':sample_sub['id'],'sentiment':preds}).reset_index(drop=True)\nresult.to_csv('result.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}