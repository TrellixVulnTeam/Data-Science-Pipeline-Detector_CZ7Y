{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\nimport pandas as pd  #data processing and data level operation\nimport numpy as np  # Linear Algebra\nimport os, re\nimport string\nimport nltk \nfrom nltk.corpus import stopwords\n\n#Visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud,STOPWORDS\n\n%matplotlib inline\n###For downlaod the nltk\n########nltk.download()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Current working directory\nprint('current workind directory ==== ',os.getcwd())\n\n#Loading data\ntrain = pd.read_csv('/kaggle/input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip',delimiter = '\\t')\ntest = pd.read_csv('/kaggle/input/word2vec-nlp-tutorial/testData.tsv.zip',delimiter = '\\t')\n\ntrain.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (\"number of rows for sentiment 1: {}\".format(len(train[train.sentiment == 1])))\nprint ( \"number of rows for sentiment 0: {}\".format(len(train[train.sentiment == 0])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.groupby('sentiment').describe().transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a new col\ntrain['length'] = train['review'].apply(len)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Histogram of count of letters\ntrain['length'].plot.hist(bins = 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.length.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.hist(column='length', by='sentiment', bins=100,figsize=(12,4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from bs4 import BeautifulSoup\n\n#Creating a function for cleaning of data\ndef clean_text(raw_text):\n    # 1. remove HTML tags\n    raw_text = BeautifulSoup(raw_text).get_text() \n    \n    # 2. removing all non letters from text\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_text) \n    \n    # 3. Convert to lower case, split into individual words\n    words = letters_only.lower().split()                           \n    \n    # 4. Create variable which contain set of stopwords\n    stops = set(stopwords.words(\"english\"))                  \n    \n    # 5. Remove stop word & returning   \n    return [w for w in words if not w in stops]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cleaning review and also adding a new col as its len count of words\ntrain['clean_review'] = train['review'].apply(clean_text)\ntrain['length_clean_review'] = train['clean_review'].apply(len)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking the smallest review\nprint(train[train['length_clean_review'] == 4]['review'].iloc[0])\nprint('------After Cleaning------')\nprint(train[train['length_clean_review'] == 4]['clean_review'].iloc[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Word Cloud**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot wordcloud\nword_cloud = WordCloud(width = 1000, height = 500, stopwords = STOPWORDS, background_color = 'red').generate(\n                        ''.join(train['review']))\n\nplt.figure(figsize = (15,8))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()\n\n#word_cloud.to_file('aa.png')   #for saving file","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Vectorization**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Might take awhile...\nbow_transform = CountVectorizer(analyzer=clean_text).fit(train['review'])  #bow = bag of word\n\n# Print total number of vocab words\nprint(len(bow_transform.vocabulary_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"review1 = train['review'][1]\nprint(review1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bow1 = bow_transform.transform([review1])\nprint(bow1)\nprint(bow1.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(bow_transform.get_feature_names()[71821])\nprint(bow_transform.get_feature_names()[72911])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating bag of words for our review variable\nreview_bow = bow_transform.transform(train['review'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape of Sparse Matrix: ', review_bow.shape)\nprint('Amount of Non-Zero occurences: ', review_bow.nnz)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sparsity = (100.0 * review_bow.nnz / (review_bow.shape[0] * review_bow.shape[1]))\nprint('sparsity: {}'.format(sparsity))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**TF-IDF**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer\n\ntfidf_transformer = TfidfTransformer().fit(review_bow)\ntfidf1 = tfidf_transformer.transform(bow1)\nprint(tfidf1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tfidf_transformer.idf_[bow_transform.vocabulary_['war']])\nprint(tfidf_transformer.idf_[bow_transform.vocabulary_['book']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"review_tfidf = tfidf_transformer.transform(review_bow)\nprint(review_tfidf.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Modeling Part**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(train['review'], train['sentiment'], test_size=0.22, random_state=101)\n\nlen(X_train), len(X_test), len(X_train) + len(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Result Function**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\n#Predicting & Stats Function\ndef pred(predicted,compare):\n    cm = pd.crosstab(compare,predicted)\n    TN = cm.iloc[0,0]\n    FN = cm.iloc[1,0]\n    TP = cm.iloc[1,1]\n    FP = cm.iloc[0,1]\n    print(\"CONFUSION MATRIX ------->> \")\n    print(cm)\n    print()\n    \n    ##check accuracy of model\n    print('Classification paradox :------->>')\n    print('Accuracy :- ', round(((TP+TN)*100)/(TP+TN+FP+FN),2))\n    print()\n    print('False Negative Rate :- ',round((FN*100)/(FN+TP),2))\n    print()\n    print('False Postive Rate :- ',round((FP*100)/(FP+TN),2))\n    print()\n    print(classification_report(compare,predicted))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**\nTraining Model,\nLogistic Regression**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\n\npipeline = Pipeline([\n    ('bow', CountVectorizer(analyzer=clean_text)),  # strings to token integer counts\n    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n    ('classifier', LogisticRegression(random_state=101)),  # train on TF-IDF vectors w/ Naive Bayes classifier\n])\n\npipeline.fit(X_train,y_train)\npredictions = pipeline.predict(X_train)\npred(predictions,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Test Set Result\npredictions = pipeline.predict(X_test)\npred(predictions,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Saving Output\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\n\npipeline_logit = Pipeline([\n    ('bow', CountVectorizer(analyzer=clean_text)),  # strings to token integer counts\n    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n    ('classifier', LogisticRegression(random_state=101)),  # train on TF-IDF vectors w/ Naive Bayes classifier\n])\n\npipeline_logit.fit(train['review'],train['sentiment'])\ntest['sentiment'] = pipeline_logit.predict(test['review'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = test[['id','sentiment']]\nprint(output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output.to_csv( \"sentiment.csv\", index=False, quoting=3 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}