{"cells":[{"metadata":{"_uuid":"36895d2823aa753fe8d8856a491b58d537a20555"},"cell_type":"markdown","source":"# Bag of Words meet Bag of Popcorn\n\n## The Problem Statement : The problem is to predict the sentiment of the reviews obtained from IMDB.\n\n### Solution Steps:\n1. Install and import neccessary library\n2. Import the dataset file into the Notebook\n3. Data Preprocessing\n4. Building the model to predict the sentiment"},{"metadata":{"_uuid":"772902190aec231738747c8054662e93b820425f"},"cell_type":"markdown","source":"### Step 1:   Import necessary library\n\nAll the following packages can be installed from [Anaconda](https://anaconda.org/anaconda/repo \"Visit site to know more\") \n\n1. Pandas             From https://anaconda.org/anaconda/pandas\n2. Numpy              From https://anaconda.org/anaconda/numpy\n3. Matplotlib         From https://anaconda.org/conda-forge/matplotlib\n4. NLTK               From https://anaconda.org/anaconda/nltk\n5. BeautifulSoup      From https://anaconda.org/anaconda/beautifulsoup4\n\n*The packages can also be dowloaded from command line prompt  with*   <b>pip install [package] ex. pip install pandas"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport nltk                            # Cleaning the data\nfrom bs4 import BeautifulSoup\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport re\nimport os","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fad1865c844f015e0132a8932ee124f854271e6a"},"cell_type":"markdown","source":"### Step 2. Loading the dataset"},{"metadata":{"trusted":true,"_uuid":"083e4a25ef602e0185aee2ac9e872c53f839311d"},"cell_type":"code","source":"# Load data file...\n\ntrain_df = pd.read_csv('../input/word2vec-nlp-tutorial/labeledTrainData.tsv',header = 0,delimiter='\\t')\ntest_df = pd.read_csv('../input/word2vec-nlp-tutorial/testData.tsv',header = 0,delimiter='\\t')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1453846e94d890dc090505d6e1bcb04989bcde3d"},"cell_type":"markdown","source":"### Step 3. Data Preprocessing and Cleaning"},{"metadata":{"trusted":true,"_uuid":"f94bb6c49281b7aa1d2d0d61a1c3dfa61bce13bd"},"cell_type":"code","source":"train_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"51c9665701ee93504eeed989a6ce801a74f57915"},"cell_type":"code","source":"test_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f96ad51226d1d98802f5ad01f2bbb996013ed5e"},"cell_type":"code","source":"print(train_df.shape)\nprint(test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"999a06a8f6d2aac221d4dd14e1e836d096df8b59"},"cell_type":"code","source":"train_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9fe42a9cab9568e7819ec623b0748ec20da6bc86"},"cell_type":"code","source":"test_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e89116fccd4fef6587a4f6eb939561eb7fefe7f"},"cell_type":"markdown","source":"#### As suggested in the discussion section of the post https://www.kaggle.com/c/word2vec-nlp-tutorial/discussion/27022 .\n#### The test data labels can be extracted form the test id seperated by ' _ '  "},{"metadata":{"trusted":true,"_uuid":"6e3733fab028363818a5a1d7919baa759d729e7e"},"cell_type":"code","source":"test_df['Sentiment'] = test_df['id'].map(lambda x: 1 if int(x.strip('\"').split('_')[1]) >=5 else 0)\ny_test = test_df['Sentiment']\ny_test.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa8635f33bf91a261c5a86156bdee5da50022f18"},"cell_type":"code","source":"test_df.drop(['Sentiment'],axis = 1,inplace = True)\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a0d6a253bab8c5211678aea84349511b64cf4582"},"cell_type":"code","source":"train_df.sentiment.value_counts()  # balanced data...","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41c71be01fad3455169639d05effc904917590c9"},"cell_type":"code","source":"def clean_review(raw_rev):\n    review_text = BeautifulSoup(raw_rev,'lxml').get_text()          # remove HTML\n    review_text = re.sub('[^a-zA-Z]',\" \",review_text)               # includes only words\n    review_words = review_text.lower().split()              # splits words and converts it to lowercase\n    \n    Stop_words = set(stopwords.words(\"english\"))                        \n    \n    mean_words = [w for w in review_words if not w in Stop_words]    # removes  stopwords..\n    review = ' '.join(mean_words)\n    \n    return review","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84902d1e097d15dc55fc8798a88d490e70cbb4d4"},"cell_type":"code","source":"train_df['clean_review'] = train_df['review'].apply(clean_review)\ntest_df['clean_review'] = test_df['review'].apply(clean_review)\ntest_df.drop(['review'],axis = 1,inplace = True)\ntest_df.rename(columns = {'clean_review':'review'},inplace = True)\ntrain_df['length_review'] = train_df['clean_review'].apply(len)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dbe87b0c2ff1e6bf21b961d495ebe3b8e61305c7"},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e4e1ae066b2865844f36cdc29c28c58c2a24e63"},"cell_type":"markdown","source":"### Step 4. Model building...\nWe will be using Keras for developing Neural Network.  Keras can be downloaded from Anaconda. Visit https://anaconda.org/conda-forge/keras to install"},{"metadata":{"trusted":true,"_uuid":"61af6aed2aec261484a8fecd53087892d353561f"},"cell_type":"code","source":"\n\nfrom keras.preprocessing import sequence,text\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout,Embedding,LSTM,SpatialDropout1D,Bidirectional\nfrom keras.utils import to_categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d74a309a9272711d17cb6fb769cd11beb658c64"},"cell_type":"code","source":"train_X = train_df.iloc[:,3].values\ntarget = train_df.sentiment.values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_val, Y_train, Y_val = train_test_split( train_X, target , test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a1b566ef5c9693b49dace49b2e68a3588798af3"},"cell_type":"code","source":"print(X_train.shape,X_val.shape,Y_train.shape,Y_val.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"51bc3021667f2ba8de207f2b217e8893f573c87a"},"cell_type":"code","source":"# max length of the review\n\nr_len=[]\nfor text in train_df['clean_review']:\n    word=word_tokenize(text)\n    l=len(word)\n    r_len.append(l)\n    \nMAX_REVIEW_LEN=np.max(r_len)\nMAX_REVIEW_LEN","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c0e8f999b5c401e85b517ca1b2d6c0a7f5cab05"},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nmax_features = 6000\nmax_words = 350\nbatch_size = 128\nepochs = 6\nnum_classes=1\n\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(X_train))\nX_train = tokenizer.texts_to_sequences(X_train)\nX_val = tokenizer.texts_to_sequences(X_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a8ea43bad740a361a82c38aabb6e4661ddb40494"},"cell_type":"code","source":"X_train = sequence.pad_sequences(X_train, maxlen=max_words)\nX_val = sequence.pad_sequences(X_val, maxlen=max_words)\nX_test = tokenizer.texts_to_sequences(test_df['review'])\nX_test = sequence.pad_sequences(X_test, maxlen=max_words)\nprint(X_train.shape,X_val.shape,X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c01069cae94bbc58516980178ffd20fb4c280788"},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4274a3e1f6ac0a69fd3d797684fe705c3b2b048"},"cell_type":"code","source":"def get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n    \ndef get_embed_mat(EMBEDDING_FILE, max_features,embed_dim):\n    # word vectors\n    embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE, encoding='utf8'))\n    print('Found %s word vectors.' % len(embeddings_index))\n\n    # embedding matrix\n    word_index = tokenizer.word_index\n    num_words = min(max_features, len(word_index) + 1)\n    all_embs = np.stack(embeddings_index.values()) #for random init\n    embedding_matrix = np.random.normal(all_embs.mean(), all_embs.std(), \n                                        (num_words, embed_dim))\n    for word, i in word_index.items():\n        if i >= max_features:\n            continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n    max_features = embedding_matrix.shape[0]\n    \n    return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f80a8d5cc1c36857fe92f6c86e4c06f66bf32d52"},"cell_type":"code","source":"EMBEDDING_FILE = '../input/glove6b/glove.6B.300d.txt'\nembed_dim = 300 #word vector dim\nembedding_matrix = get_embed_mat(EMBEDDING_FILE,max_features,embed_dim)\nprint(embedding_matrix.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42a8aee543cfae55d248ddc26c56cc8bad58d2e8"},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(max_features, embed_dim, input_length=X_train.shape[1],weights=[embedding_matrix],trainable=True))\nmodel.add(SpatialDropout1D(0.25))\nmodel.add(Bidirectional(LSTM(128,return_sequences=True)))\nmodel.add(Bidirectional(LSTM(64,return_sequences=False)))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3cca88da9d0b76161681c696c6af819ed09c5ed2"},"cell_type":"code","source":"history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val),epochs=epochs, batch_size=batch_size, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"699b6edb92e98d4256c868ab65731962e246e0f6"},"cell_type":"code","source":"prediction = model.predict(X_test)\ny_pred = (prediction > 0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42ada5ebfb5c936d8005d0c2f00d68dfb37e4e25"},"cell_type":"code","source":"from sklearn.metrics import f1_score, confusion_matrix\nprint('F1-score: {0}'.format(f1_score(y_pred, y_test)))\nprint('Confusion matrix:')\nconfusion_matrix(y_pred, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6718d90e7d7f7f02efa2eac546fb377ff28343df"},"cell_type":"code","source":"test_df['Sentiment'] = y_test\ntest_df.drop(['review'],axis = 1,inplace = True)\n\ntest_df.to_csv('Submission.csv',index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}