{"cells":[{"metadata":{"_uuid":"6a5e0ebc7e0082147997009fd7524e7e33e835fe"},"cell_type":"markdown","source":"**# **Project : Bag of words Meets Bags of Popcorn**\n"},{"metadata":{"_uuid":"5f5c7ace5a36f3b94ba0ca2d60093a524debdd3e"},"cell_type":"markdown","source":"## Importing Packages and Data"},{"metadata":{"trusted":false,"_uuid":"4f47523a72afcf73592312be449bca861fa644b3"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport datetime as dt\nimport time\nimport seaborn as sns\nimport os\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\n%matplotlib inline\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"85c81e2afd3b0b6ff806cce56c6b7c413fd7989d"},"cell_type":"code","source":"your_local_path=\"Mcintosh HD/Users/rk/Desktop/UPXTECH/PROJECT/NLP/Bag of world\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"dd5d6bf096a6ee870a15217c18692a6e435736b7"},"cell_type":"code","source":"cd /Users/rk/Desktop/UPXTECH/PROJECT/NLP/Bag of world","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"87a9de4a8c960ec641e3e9697f800f75ee852dfd"},"cell_type":"markdown","source":"# Train Data"},{"metadata":{"trusted":false,"_uuid":"70bf8099e3e46773167886dbb0b157d01817d093"},"cell_type":"code","source":"train = pd.read_csv(\"labeledTrainData.tsv\", delimiter='\\t')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e3a78ed9c1bab4134a16d9930e03cd360f967568"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8726ccda55583b40154e501dbd9c3f85e89edf6"},"cell_type":"markdown","source":"# Test data"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"e1eb9afe772356ca87db00ff64c98ba8ff1005a5"},"cell_type":"code","source":"test = pd.read_csv(\"testData.tsv\", delimiter='\\t')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"77ebf3217cf5bb2e1c1ed13bc65ee4ddf45e1506"},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"eea91132e9812f60ac80e63755168bc4d9465cf4"},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b77d12aea57647f020ff390bfa2a6fa1a648404e"},"cell_type":"markdown","source":"## Data Cleaning and Text Preprocessing "},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"43bd90021c127faf4cc5c729387bf3c18dcc44eb"},"cell_type":"code","source":"from bs4 import BeautifulSoup\nimport re     # to remove Punctuation and numbers","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"712618884516259f97936f7a1b065ef31bfd5a91"},"cell_type":"code","source":"from nltk.corpus import stopwords \nstopset = set(stopwords.words('english'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3e676fd20cd022e8137d2895b8d642f05673d76"},"cell_type":"markdown","source":"## Import the stop word list"},{"metadata":{"trusted":false,"_uuid":"c37bcbbecc86757b66cde5a16d4a9d6e21a3fc23"},"cell_type":"code","source":"from nltk.corpus import stopwords \nprint \nstopwords.words(\"english\")","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"2bf341167bf2187cbc7b75540029d6adc6cc05f8"},"cell_type":"code","source":"def review_to_words( raw_review ):\n    # 1. Remove HTML\n    review_text = BeautifulSoup(raw_review).get_text()\n    \n    # 2. Remove non-letters        \n    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n    \n    # 3. Convert to lower case, split into individual words\n    words = letters_only.lower().split()                             \n    \n    # 4. In Python, searching a set is much faster than searching\n    #   a list, so convert the stop words to a set\n    stops = set(stopwords.words(\"english\"))                  \n    \n    # 5. Remove stop words\n    meaningful_words = [w for w in words if not w in stops]   \n    \n    # 6. Join the words back into one string separated by space, \n    # and return the result.\n    return( \" \".join( meaningful_words ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b7df8aeb50b9790a1fea3bdf4ebdb0f2b1c7435d"},"cell_type":"code","source":"num_reviews = train[\"review\"].size\nclean_train_reviews = []\nfor i in range( 0, num_reviews ):\n   \n    clean_train_reviews.append( review_to_words( train[\"review\"][i] ) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b6dc276bb0cca28d03e071c0456a499ad0b18038"},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer \n\nvectorizer = CountVectorizer(analyzer = \"word\",   \n                             tokenizer = None,    \n                             preprocessor = None, \n                             stop_words = None,   \n                             max_features = 5000) \ntrain_data_features = vectorizer.fit_transform(clean_train_reviews)\ntrain_data_features = train_data_features.toarray()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"691f2d7cdd1724d6c49266cbd1acbb1c827ea276"},"cell_type":"code","source":"vocab = vectorizer.get_feature_names()\nprint(vocab)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0c155350007432e04a3fa936c77355a653dd4d46"},"cell_type":"markdown","source":"## Training the Random forest"},{"metadata":{"trusted":false,"_uuid":"00e6f9f3a3b4297e9358c944c50a0a1353d54114"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nforest = RandomForestClassifier(n_estimators = 500) \nforest = forest.fit( train_data_features, train[\"sentiment\"] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"10c396eb62c3170ce7193d41ad7582e2e0c9bf30"},"cell_type":"code","source":"# Create an empty list and append the clean reviews one by one\nnum_reviews = len(test[\"review\"])\nclean_test_reviews = []","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"6ec98fcfdca470401f95935da96c92cfc6478dd8"},"cell_type":"code","source":"print (\"Cleaning and parsing the test set movie reviews...\\n\")\nfor i in range(0,num_reviews):\n    if( (i+1) % 1000 == 0 ):\n        print (\"Review %d of %d\\n\" % (i+1, num_reviews))\n    clean_review = review_to_words( test[\"review\"][i] )\n    clean_test_reviews.append( clean_review )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc142ee4db5b11849950d975f6acb7281fbde4f7"},"cell_type":"markdown","source":"## Get a bag of words for the test set, and convert to a numpy array"},{"metadata":{"trusted":false,"_uuid":"ae27b4261058504c38225dd112247140ca398d33"},"cell_type":"code","source":"\ntest_data_features = vectorizer.transform(clean_test_reviews)\ntest_data_features = test_data_features.toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3e9d9612af2655ba06ea2e983ff11f217070689e"},"cell_type":"code","source":"result = forest.predict(test_data_features)\nprint (result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0a958a783486813138a047d51e1a305880457a9f"},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"049dae407d6fc8e144eb3efd4128dfc4d3750b03"},"cell_type":"code","source":"output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\nprint (output)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"d524432813c21a0cebf1c7225bc4ec3b5e81b670"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":1}