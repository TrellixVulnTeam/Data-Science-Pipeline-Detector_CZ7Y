{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"../input/word2vec-nlp-tutorial\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#Read the IMDB dataset with 25K reviews for training. \n\ndf = pd.read_csv(\"../input/word2vec-nlp-tutorial/labeledTrainData.tsv\", sep = '\\t', \n                 error_bad_lines=False )\nprint(\"Total no. of reviews are \", df.shape[0])\nprint(\"cols are \", df.columns)\nprint(\"Sample reviews are \")\nprint(df.loc[:5,['review','sentiment']])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0efd7ebd682c02297b60f45d5e04af97eef845b2"},"cell_type":"code","source":"#Import the stopwords (common words) to be removed from the corpus\n\"\"\"\nimport re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\ncorpus = []\ns = set(stopwords.words('english'))\ns.remove('not')\nprint(\"Stopwords length\", len(s))\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08366fa8e92f22301e5465adf96b06408f369f29"},"cell_type":"code","source":"# 3. Remove the puncuation symbols or any other symbols that are not characters [^A-Za-z] and put the text in list Corpus\n#s = set(s)\n#corpus = []\n#for i in range(0, df.shape[0]):\n #   review = re.sub('[^a-zA-Z]', ' ', df['review'][i])\n  #  review = review.lower().split()\n    #ps = PorterStemmer()\n    #review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n   # review = [word for word in review if not word in s]\n    #review = ' '.join(review)\n    #corpus.append(review)\n#print(corpus[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"25ae3199aaf45d2b9b417b5625ecdbbb55955b43"},"cell_type":"code","source":"word2vec = {}\nwith open('../input/glove6b50dtxt/glove.6B.50d.txt', encoding=\"utf8\") as f:\n  # is just a space-separated text file in the format:\n  # word vec[0] vec[1] vec[2] ...\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vec = np.asarray(values[1:], dtype='float32')\n        word2vec[word] = vec\nprint('Found %s word vectors.' % len(word2vec))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6fd3f8196ce72b0200e7b29c5c891820836ebf7"},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer, text_to_word_sequence\nfrom keras.preprocessing.sequence import pad_sequences\nMAX_VCOCAB_SIZE = 5000\nEMBEDDING_DIM = 50\nMAX_SEQUENCE_LENGTH = 1500\n\ntokenizer = Tokenizer( filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True, split=' ')\nsequences = tokenizer.fit_on_texts(df['review'])\nword_index = tokenizer.word_index\ndocuments = tokenizer.texts_to_sequences(df['review'])\n#print(word_index)\ntoken_count = len(word_index)+1\nprint('Found {} unique tokens.'.format(token_count))\n\n#print(t.word_counts)\nprint(\"Total documents \", tokenizer.document_count)\n#print(t.word_index)\n#print(t.word_docs)\nprint(\"max sequence length:\", max(len(s) for s in documents))\nprint(\"min sequence length:\", min(len(s) for s in documents))\n\n# pad sequences so that we get a N x T matrix\ndata = pad_sequences(documents, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\nprint('Shape of data tensor:', data.shape)\nprint(data[1])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84dc4341f96a45cf4c100805476821b48addd671"},"cell_type":"code","source":"print('Filling pre-trained embeddings...')\nembedding_matrix = np.zeros((token_count, EMBEDDING_DIM))\nfor word, i in word_index.items():\n  #if i < MAX_VOCAB_SIZE:\n    embedding_vector = word2vec.get(word) #get(word) is used instead of [word] as it won't give exception in case word is not found\n    if embedding_vector is not None:\n      # words not found in embedding index will be all zeros.\n      embedding_matrix[i,:] = embedding_vector\n\nprint(\"Sample embedded dimension \")\nprint(embedding_matrix[10][:5])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1414c6aa2917a5fcc6bb221e9236905bd2a02799"},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Conv1D, MaxPooling1D, Dropout, Dense, GlobalAveragePooling1D \nfrom keras.layers import Embedding, Conv2D, GlobalMaxPooling1D \nfrom keras import regularizers\n\nembedding_layer = Embedding(\n  token_count,\n  EMBEDDING_DIM,\n  weights=[embedding_matrix],\n  input_length=MAX_SEQUENCE_LENGTH,\n  trainable=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ba4261017a5e015c769dff888b29d01ed869486"},"cell_type":"code","source":"model = Sequential()\nmodel.add(embedding_layer)#, input_shape= (token_count, EMBEDDING_DIM))\nmodel.add(Conv1D(filters = 64, kernel_size = 4, padding = 'same', activation='relu'))\n                 #input_shape=(token_count,EMBEDDING_DIM)))\nmodel.add(MaxPooling1D())#kernel_size=500))\nmodel.add(Conv1D(filters = 128, kernel_size = 3, padding = 'same',  activation='relu', \n                 kernel_regularizer=regularizers.l2(0.01)))\nmodel.add(Dropout(0.25))\nmodel.add(MaxPooling1D())\nmodel.add(Conv1D(filters = 256, kernel_size = 2, padding = 'same', activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(MaxPooling1D())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(64, activation='relu'))\n#model.add(Conv1D(128, 3, activation='relu'))\nmodel.add(GlobalMaxPooling1D())\n\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nprint(model.summary())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d707e75dff855e7564bb48c5dff1c0bba4e2607"},"cell_type":"code","source":"from keras.layers import Input, Dense, Concatenate\nfrom keras.models import Model\n\ninputs = Input(shape=(MAX_SEQUENCE_LENGTH,))\n\nx = embedding_layer(inputs)\nprint(x)\nx1 = Conv1D(filters = 100, kernel_size = 3, padding = 'same', activation='relu'\n           ,kernel_regularizer=regularizers.l1(0.01))(x)\n                 #input_shape=(token_count,EMBEDDING_DIM)))\nx1 = GlobalMaxPooling1D()(x1)\n\nx2 = Conv1D(filters = 100, kernel_size = 4, padding = 'same', activation='relu'\n           ,kernel_regularizer=regularizers.l1(0.01))(x)\n                 #input_shape=(token_count,EMBEDDING_DIM)))\nx2 = GlobalMaxPooling1D()(x2) #pool_size=1500\n\n\nx3 = Conv1D(filters = 100, kernel_size = 5, padding = 'same', activation='relu'\n           ,kernel_regularizer=regularizers.l1(0.01))(x)\n                 #input_shape=(token_count,EMBEDDING_DIM)))\nx3 = GlobalMaxPooling1D()(x3)\n\n# a layer instance is callable on a tensor, and returns a tensor\nprint(x3)\nx = Concatenate()([x1,x2,x3])\nprint(x)\nx = Dense(192)(x)\nx = Dropout(0.5)(x)\nx = Dense(128, activation='relu')(x)\nx = Dropout(0.25)(x)\noutput = Dense(1, activation='sigmoid')(x)\n\n# This creates a model that includes\n# the Input layer and three Dense layers\nFmodel = Model(inputs=inputs, outputs=output)\nFmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nprint(Fmodel.summary())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c78aa181f609051ad6f0929bf124ad642433de15"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(data, df['sentiment'], \n                                                    test_size=0.2, random_state=42)\n\n\nprint(x_train.shape)\n#model.fit(x_train, y_train , batch_size=96, epochs=35, validation_split = 0.25)\n#score = model.evaluate(x_test, y_test, batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5fe848ee6f783f0581a53ad591ef92c0627cee77"},"cell_type":"code","source":"Fmodel.fit(x_train, y_train , batch_size=96, epochs=35, validation_split = 0.1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66bd5bb8cebe5e4b08a976b39a8836fb6e4d7586","trusted":true},"cell_type":"code","source":"print(\"Concatenated CNN Result\")\nprint(\"Loss & accuracty on test set is\", Fmodel.evaluate(x_test, y_test))\n\n#print(\"Traditional CNN Result\")\n#print(\"Loss & accuracty on test set is\", model.evaluate(x_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"afa3ec7c8b81626ca3a9d30c662883ad0ae8c99d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}