{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"../input/word2vec-nlp-tutorial\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#Read the IMDB dataset with 25K reviews for training. \n\ndf = pd.read_csv(\"../input/word2vec-nlp-tutorial/labeledTrainData.tsv\", sep = '\\t', \n                 error_bad_lines=False )\nprint(\"Total no. of reviews are \", df.shape[0])\nprint(\"cols are \", df.columns)\nprint(\"Sample reviews are \")\nprint(df.loc[:5,['review','sentiment']])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"25ae3199aaf45d2b9b417b5625ecdbbb55955b43"},"cell_type":"code","source":"word2vec = {}\nwith open('../input/glove6b50dtxt/glove.6B.50d.txt', encoding=\"utf8\") as f:\n  # is just a space-separated text file in the format:\n  # word vec[0] vec[1] vec[2] ...\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vec = np.asarray(values[1:], dtype='float32')\n        word2vec[word] = vec\nprint('Found %s word vectors.' % len(word2vec))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6fd3f8196ce72b0200e7b29c5c891820836ebf7"},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer, text_to_word_sequence\nfrom keras.preprocessing.sequence import pad_sequences\nMAX_VCOCAB_SIZE = 8000\nEMBEDDING_DIM = 50\nMAX_SEQUENCE_LENGTH = 1500\n\ntokenizer = Tokenizer( filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True, split=' ')\ntokenizer.fit_on_texts(df['review'])\n#print(\"Total Sequences: \", type(sequences))\nword_index = tokenizer.word_index\ndocuments = tokenizer.texts_to_sequences(df['review'])\nprint(list(word_index.items())[:5])#iloc[:10])\ntoken_count = len(word_index)+1\nprint('Found {} unique tokens.'.format(token_count))\n\n#print(t.word_counts)\nprint(\"Total documents \", tokenizer.document_count)\n#print(t.word_index)\n#print(t.word_docs)\nprint(\"max sequence length:\", max(len(s) for s in documents))\nprint(\"min sequence length:\", min(len(s) for s in documents))\n\n# pad sequences so that we get a N x T matrix\ndata = pad_sequences(documents, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\nprint('Shape of data tensor:', data.shape)\nprint(data[1])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84dc4341f96a45cf4c100805476821b48addd671"},"cell_type":"code","source":"print('Filling pre-trained embeddings...')\nembedding_matrix = np.zeros((token_count, EMBEDDING_DIM))\nfor word, i in word_index.items():\n  #if i < MAX_VOCAB_SIZE:\n    embedding_vector = word2vec.get(word) #get(word) is used instead of [word] as it won't give exception in case word is not found\n    if embedding_vector is not None:\n      # words not found in embedding index will be all zeros.\n      embedding_matrix[i,:] = embedding_vector\n\nprint(\"Sample embedded dimension {}\".format(embedding_matrix.shape))\nprint(embedding_matrix[10][:5])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1414c6aa2917a5fcc6bb221e9236905bd2a02799"},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Conv1D, MaxPooling1D, Dropout, Dense, GlobalAveragePooling1D \nfrom keras.layers import Embedding, Conv2D, GlobalMaxPooling1D \nfrom keras import regularizers\n\nembedding_layer = Embedding(\n  token_count,\n  EMBEDDING_DIM,\n  weights=[embedding_matrix],\n  input_length=MAX_SEQUENCE_LENGTH,\n  trainable=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ba4261017a5e015c769dff888b29d01ed869486"},"cell_type":"code","source":"model = Sequential()\nmodel.add(embedding_layer)#, input_shape= (token_count, EMBEDDING_DIM))\nmodel.add(Conv1D(filters = 100, kernel_size = 3, padding = 'same', activation='relu'))\n                 #input_shape=(token_count,EMBEDDING_DIM)))\nmodel.add(Dropout(0.2))\nmodel.add(MaxPooling1D())#kernel_size=500))\nmodel.add(Conv1D(filters = 200, kernel_size = 4, padding = 'same',  activation='relu'))              \nmodel.add(Dropout(0.5))\nmodel.add(MaxPooling1D())\nmodel.add(Conv1D(filters = 300, kernel_size = 5, padding = 'same', activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(MaxPooling1D())\nmodel.add(Dense(192, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(128, activation='relu'))\n#model.add(Conv1D(128, 3, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(GlobalMaxPooling1D())\n\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nprint(model.summary())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c78aa181f609051ad6f0929bf124ad642433de15"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(data, df['sentiment'], \n                                                    test_size=0.2, random_state=42)\nprint(x_train.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28e7559dd0d482f2213177d4adb79af84ada40d9"},"cell_type":"code","source":"from keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='acc', patience=4, mode = 'max')\nmodel.fit(x_train, y_train , batch_size=96, epochs=50, validation_split = 0.25, \n          callbacks=[early_stopping])\n#score = model.evaluate(x_test, y_test, batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66bd5bb8cebe5e4b08a976b39a8836fb6e4d7586","trusted":true},"cell_type":"code","source":"print(\"Standalone CNN Result with dropout\")\nprint(\"Loss & accuracy on test set is\", model.evaluate(x_test, y_test))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"afa3ec7c8b81626ca3a9d30c662883ad0ae8c99d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}