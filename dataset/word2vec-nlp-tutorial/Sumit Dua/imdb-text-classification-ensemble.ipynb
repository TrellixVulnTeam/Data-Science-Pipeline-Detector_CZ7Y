{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"../input/word2vec-nlp-tutorial\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#Read the IMDB dataset with 25K reviews for training. \n\ndf = pd.read_csv(\"../input/word2vec-nlp-tutorial/labeledTrainData.tsv\", sep = '\\t', \n                 error_bad_lines=False )\nprint(\"Total no. of reviews are \", df.shape[0])\nprint(\"cols are \", df.columns)\nprint(\"Sample reviews are \")\nprint(df.loc[:5,['review','sentiment']])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"25ae3199aaf45d2b9b417b5625ecdbbb55955b43"},"cell_type":"code","source":"word2vec = {}\nwith open('../input/glove6b50dtxt/glove.6B.50d.txt', encoding=\"utf8\") as f:\n  # is just a space-separated text file in the format:\n  # word vec[0] vec[1] vec[2] ...\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vec = np.asarray(values[1:], dtype='float32')\n        word2vec[word] = vec\nprint('Found %s word vectors.' % len(word2vec))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6fd3f8196ce72b0200e7b29c5c891820836ebf7"},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer, text_to_word_sequence\nfrom keras.preprocessing.sequence import pad_sequences\nMAX_VCOCAB_SIZE = 8000\nEMBEDDING_DIM = 50\nMAX_SEQUENCE_LENGTH = 1500\n\ntokenizer = Tokenizer( filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True, split=' ')\ntokenizer.fit_on_texts(df['review'])\n#print(\"Total Sequences: \", type(sequences))\nword_index = tokenizer.word_index\ndocuments = tokenizer.texts_to_sequences(df['review'])\nprint(list(word_index.items())[:5])#iloc[:10])\ntoken_count = len(word_index)+1\nprint('Found {} unique tokens.'.format(token_count))\n\n#print(t.word_counts)\nprint(\"Total documents \", tokenizer.document_count)\n#print(t.word_index)\n#print(t.word_docs)\nprint(\"max sequence length:\", max(len(s) for s in documents))\nprint(\"min sequence length:\", min(len(s) for s in documents))\n\n# pad sequences so that we get a N x T matrix\ndata = pad_sequences(documents, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\nprint('Shape of data tensor:', data.shape)\nprint(data[1])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84dc4341f96a45cf4c100805476821b48addd671"},"cell_type":"code","source":"print('Filling pre-trained embeddings...')\nembedding_matrix = np.zeros((token_count, EMBEDDING_DIM))\nfor word, i in word_index.items():\n  #if i < MAX_VOCAB_SIZE:\n    embedding_vector = word2vec.get(word) #get(word) is used instead of [word] as it won't give exception in case word is not found\n    if embedding_vector is not None:\n      # words not found in embedding index will be all zeros.\n      embedding_matrix[i,:] = embedding_vector\n\nprint(\"Sample embedded dimension {}\".format(embedding_matrix.shape))\nprint(embedding_matrix[10][:5])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1414c6aa2917a5fcc6bb221e9236905bd2a02799"},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Conv1D, MaxPooling1D, Dropout, Dense, GlobalAveragePooling1D \nfrom keras.layers import Embedding, Conv2D, GlobalMaxPooling1D \nfrom keras import regularizers\nfrom keras.layers import Input, Dense, Concatenate\nfrom keras.models import Model\n\n\nembedding_layer = Embedding(\n  token_count,\n  EMBEDDING_DIM,\n  weights=[embedding_matrix],\n  input_length=MAX_SEQUENCE_LENGTH,\n  trainable=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef7e40f4bbe92bb94c1464cc455bf7b8c53bf836"},"cell_type":"code","source":"def conv_model(kernel, pool):\n    \n    inputs1 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n    x1 = embedding_layer(inputs1)\n    #print(\"After applying embeeding \", x1)\n    num_filters=64\n    if(kernel==3):\n        num_filters=128\n    x1 = Conv1D(num_filters, kernel_size = kernel, padding = 'same', activation='relu'\n               ,kernel_regularizer=regularizers.l2(0.01))(x1)\n                     #input_shape=(token_count,EMBEDDING_DIM)))\n    #print(\"After applying conv1d on filter size 3 \", x1)\n    x1 = MaxPooling1D(pool_size=pool)(x1)\n    #print(\"After applying global max pooling \", x1)\n\n    x1 = Conv1D(filters = 256, kernel_size = kernel, padding = 'same', activation='relu'\n               ,kernel_regularizer=regularizers.l2(0.01))(x1)\n    #print(x1)\n    x1 = MaxPooling1D(pool_size=pool)(x1)\n\n    x1 = Conv1D(filters = 128, kernel_size = kernel, padding = 'same', activation='relu'\n               ,kernel_regularizer=regularizers.l2(0.01))(x1)\n    x1 = GlobalMaxPooling1D()(x1)\n\n    x1 = Dense(64, activation='relu')(x1)\n\n    x1 = Dense(1, activation='sigmoid')(x1)\n\n    model1 = Model(inputs=inputs1, outputs=x1)\n\n    model1.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n    return model1\n    \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d707e75dff855e7564bb48c5dff1c0bba4e2607"},"cell_type":"code","source":"model1 = conv_model(kernel=3, pool=2)\nprint(model1.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"013b9253e55a6bb4f6f9ae2498d1d4381b00990a"},"cell_type":"code","source":"model2 = conv_model(kernel=4, pool=3)\nprint(model2.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"46c6b361fb6b065069a007fec901de33279d02c6"},"cell_type":"code","source":"model3 = conv_model(kernel=5, pool=4)\nprint(model3.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c78aa181f609051ad6f0929bf124ad642433de15"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(data, df['sentiment'], \n                                                    test_size=0.2, random_state=42)\nprint(x_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28e7559dd0d482f2213177d4adb79af84ada40d9"},"cell_type":"code","source":"from keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='acc', patience=4, mode = 'max')\nmodel1.fit(x_train, y_train , batch_size=96, epochs=50, validation_split = 0.25, \n           callbacks=[early_stopping])\n#score = model.evaluate(x_test, y_test, batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac2f45a3351439d68632babbcb7d2ee1703f5b5f"},"cell_type":"code","source":"model2.fit(x_train, y_train , batch_size=96, epochs=50, validation_split = 0.25,\n           callbacks=[early_stopping])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7df05a3ca3f6d8fdda3920a47c40fdfb4def7f50"},"cell_type":"code","source":"model3.fit(x_train, y_train , batch_size=96, epochs=50, validation_split = 0.25, \n           callbacks=[early_stopping])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66bd5bb8cebe5e4b08a976b39a8836fb6e4d7586","trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n#print(\"Concatenated CNN Result\")\n#print(\"Loss & accuracty on test set is\", Fmodel.evaluate(x_test, y_test))\n\nprint(\"CNN Result\")\nprint(\"Loss & accuracty on test set is\", model1.evaluate(x_test, y_test))\nprint(\"Loss & accuracty on test set is\", model2.evaluate(x_test, y_test))\nprint(\"Loss & accuracty on test set is\", model3.evaluate(x_test, y_test))\n\ny_pred1 = model1.predict(x_test)\ny_pred2 = model2.predict(x_test)\ny_pred3 = model3.predict(x_test)\ny_pred = (y_pred1+y_pred2+y_pred3)/3\nprint(\"Accuracy score on y_test for Ensembel model of 3 is :\")\nprint(accuracy_score(y_test,np.round(y_pred)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"afa3ec7c8b81626ca3a9d30c662883ad0ae8c99d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}