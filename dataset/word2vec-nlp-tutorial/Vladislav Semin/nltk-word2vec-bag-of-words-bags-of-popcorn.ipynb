{"cells":[{"metadata":{},"cell_type":"markdown","source":"# NLTK & Word2vec framework applied to \"Bag of Words Meets Bag of Popcorn\" Kaggle dataset\n\n### Table of contents: \n1. [Import libraries & files](#1)\n    - [1.1. Useful functions](#1.1.)\n2. [Get descriptive info on raw data](#2)\n    - [2.1. Pandas Profiling Report](#2ppr)\n3. [Data cleaning & text preprocessing](#3)\n4. [Creating Features (Bag of Words approach)](#4)\n5. [Train ML models, tune hyperparams & evaluate](#5)\n6. [Export output to csv & submit](#6)\n7. [Important lessons](#7)\n8. [References](#8)\n\n### Frameworks: \n#### Supervised Learning:\n- [Random Forest](#rf)\n- [NLTK](#nltk)\n\n#### Deep Learning:\n- [Word2vec](#w2v)\n\n**Researcher: Vladislav Semin**"},{"metadata":{},"cell_type":"markdown","source":"### Subheader formatting:\n<div class=\"alert alert-block alert-info\">\n<b>Takeaways</b> are highlighted in ***blue***"},{"metadata":{},"cell_type":"markdown","source":"<div class=\"alert alert-block alert-warning\">  \n- ***TO DO's*** to improve predictions are highlighted in ***yellow***"},{"metadata":{},"cell_type":"markdown","source":"<div id=\"1\"></div>\n# 1. Import libraries & files "},{"metadata":{"trusted":true},"cell_type":"code","source":"# import libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pandas_profiling\n\n# data visualization\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style  \n\n# Tools for preprocessing input data\nimport re #reg expressions for find & replace\nfrom bs4 import BeautifulSoup\nimport nltk\nfrom nltk import word_tokenize\nimport nltk.data\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nimport logging\n\n# Tools for creating ngrams and vectorizing input data\nfrom gensim.models import Word2Vec, Phrases\nfrom gensim.models import word2vec\n\n# Tools for building a model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer #for bag of words feature extraction\nfrom sklearn.ensemble import RandomForestClassifier \n\n# ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Input data files are available in the \"../input/\" directory.\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# create data handles\ntrain = pd.read_csv(\"/kaggle/input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip\", header=0, delimiter=\"\\t\", quoting=3)\nunlabeled_train = pd.read_csv(\"/kaggle/input/word2vec-nlp-tutorial/unlabeledTrainData.tsv.zip\", header=0, delimiter=\"\\t\", quoting=3)\ntest = pd.read_csv(\"/kaggle/input/word2vec-nlp-tutorial/testData.tsv.zip\", header=0, delimiter=\"\\t\", quoting=3 )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div id=\"1.1.\"></div>\n### 1.1. Useful functions "},{"metadata":{"trusted":true},"cell_type":"code","source":"# here we create a useful function to display all important confusion matrix metrics\ndef display_confusion_matrix(target, prediction, score=None):\n    cm = metrics.confusion_matrix(target, prediction)\n    plt.figure(figsize=(6,6))\n    sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square=True, cmap='Blues_r')\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label')\n    if score:\n        score_title = 'Accuracy Score: {0}'.format(round(score, 5))\n        plt.title(score_title, size = 14)\n    classification_report = pd.DataFrame.from_dict(metrics.classification_report(target, prediction, output_dict=True))\n    display(classification_report.round(2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div id=\"2\"></div>\n# 2. Get descriptive info on raw data"},{"metadata":{},"cell_type":"markdown","source":"<div id=\"2ppr\"></div>\n### 2.1. Pandas Profiling Report"},{"metadata":{"trusted":true},"cell_type":"code","source":"# In this case we do not need extensive data cleaning & feature engineering. \n#instead of profile report we can simply use head() and info()\n\n#labeled_train.profile_report()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['review'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unlabeled_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div id=\"3\"></div>\n# 3. Data cleaning & text preprocessing"},{"metadata":{},"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\nUse beautiful soup, regex & nltk stopwords to clean word data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize the BeautifulSoup object on a single movie review\n# provides review without tags or markup\nexample1 = BeautifulSoup(train[\"review\"][0])  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"example1.get_text()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use regular expressions to do a find-and-replace\nletters_only = re.sub(\"[^a-zA-Z]\",           # The pattern to search for\n                      \" \",                   # The pattern to replace it with\n                      example1.get_text() )  # The text to search\nletters_only","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lower_case = letters_only.lower()        # Convert to lower case\nlower_case","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words = lower_case.split()               # Split into words\nwords","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div id=\"nltk\"></div>\n### NLTK Stopwords"},{"metadata":{"trusted":true},"cell_type":"code","source":"# stopword list from nltk.corpus\nstopwords.words(\"english\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove stop words from \"words\"\nwords = [w for w in words if not w in stopwords.words(\"english\")]\nwords","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function for reusable code\n\n# example raw_review = train[\"review\"] or raw_review = test[\"review\"]\n\ndef review_to_words(raw_review):\n    # Function to convert a raw review to a string of words\n    # The input is a single string (a raw movie review), and \n    # the output is a single string (a preprocessed movie review)\n    #\n    # 1. Remove HTML\n    review_text = BeautifulSoup(raw_review).get_text() \n    #\n    # 2. Remove non-letters        \n    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n    #\n    # 3. Convert to lower case, split into individual words\n    words = letters_only.lower().split()                             \n    #\n    # 4. In Python, searching a set is much faster than searching\n    #   a list, so convert the stop words to a set\n    stops = set(stopwords.words(\"english\"))                  \n    # \n    # 5. Remove stop words\n    meaningful_words = [w for w in words if not w in stops]   \n    #\n    # 6. Join the words back into one string separated by space, \n    # and return the result.\n    return( \" \".join( meaningful_words ))   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# loop through and clean all of the training set at once\n\n# Get the number of reviews based on the dataframe column size\nnum_reviews = train[\"review\"].size\n\n# Initialize an empty list to hold the clean reviews\nclean_train_reviews = []\n\n# Loop over each review; create an index i that goes from 0 to the length\n# of the movie review list \nfor i in range(0, num_reviews):\n    # Call our function for each one, and add the result to the list of\n    # clean reviews\n    clean_train_reviews.append(review_to_words(train[\"review\"][i]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_train_reviews","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div id=\"4\"></div>\n# 4. Creating Features (Bag of Words approach)"},{"metadata":{},"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\nCreate Features with Bag of Words approach using scikit-learn CountVectorizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"# use CountVectorizer scikit-learn object to create bag of words\nvectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, \n                             preprocessor = None, stop_words = None, max_features = 5000) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit-transform learns the vocabulary and transforms training data into feature vectors\ntrain_data_features = vectorizer.fit_transform(clean_train_reviews)\n\n# transform list of strings to numpy array for more efficiency\ntrain_data_features = train_data_features.toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take a look at the words in the vocabulary\nvocab = vectorizer.get_feature_names()\nvocab","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the counts of each word in vocabulary\n\n# Sum up the counts of each vocabulary word\ndist = np.sum(train_data_features, axis=0)\n\n# For each, print the vocabulary word and the number of times it \n# appears in the training set\nfor tag, count in zip(vocab, dist):\n    print(count, tag)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div id=\"5\"></div>\n# 5. Train ML models, tune hyperparams & evaluate\n<div id=\"rf\"></div>\n### Random forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize a Random Forest classifier with 100 trees\nrf = RandomForestClassifier(n_estimators = 100) \n\n# Fit the forest to the training set, using the bag of words as \n# features and the sentiment labels as the response variable\nrf = rf.fit(train_data_features, train[\"sentiment\"])\n\n# Score\n#rf.score(train_data_features, train[\"sentiment\"])\n#acc_random_forest = round(rf.score(train_data_features, train[\"sentiment\"]) * 100, 2)\n\n#acc_random_forest","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div id=\"w2v\"></div>\n### Word2vec \nWord2vec is an unsupervised learning, deep learning approach.\n\nIt uses unlabeled data and creates clusters of words with similar meanings.\n\nIt is also advised not to remove stopwords with Word2vec"},{"metadata":{"trusted":true},"cell_type":"code","source":"def review_to_wordlist(review, remove_stopwords=False):\n    # Function to convert a document to a sequence of words,\n    # optionally removing stop words. Returns a list of words.\n    \n    # 1. Remove HTML\n    review_text = BeautifulSoup(review).get_text()\n      \n    # 2. Remove non-letters\n    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n    \n    # 3. Convert words to lower case and split them\n    words = review_text.lower().split()\n    \n    # 4. Optionally remove stop words (false by default)\n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))\n        words = [w for w in words if not w in stops]\n\n    # 5. Return a list of words\n    return(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the punkt tokenizer\ntokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n\n# Define a function to split a review into parsed sentences\ndef review_to_sentences( review, tokenizer, remove_stopwords=False ):\n    # Function to split a review into parsed sentences. Returns a \n    # list of sentences, where each sentence is a list of words\n    \n    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n    raw_sentences = tokenizer.tokenize(review.strip())\n    #\n    # 2. Loop over each sentence\n    sentences = []\n    for raw_sentence in raw_sentences:\n        # If a sentence is empty, skip it\n        if len(raw_sentence) > 0:\n            # Otherwise, call review_to_wordlist to get a list of words\n            sentences.append( review_to_wordlist( raw_sentence, \\\n              remove_stopwords ))\n    #\n    # Return the list of sentences (each sentence is a list of words,\n    # so this returns a list of lists\n    return sentences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we can apply this function to prepare our data for input to Word2Vec\n\nsentences = []  # Initialize an empty list of sentences\n\nprint(\"Parsing sentences from training set\")\nfor review in train[\"review\"]:\n    sentences += review_to_sentences(review, tokenizer)\n\nprint(\"Parsing sentences from unlabeled set\")\nfor review in unlabeled_train[\"review\"]:\n    sentences += review_to_sentences(review, tokenizer)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(sentences)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the built-in logging module and configure it so that Word2Vec creates nice output messages\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n\n# Set values for various parameters\nnum_features = 300    # Word vector dimensionality                      \nmin_word_count = 40   # Minimum word count                        \nnum_workers = 4       # Number of threads to run in parallel\ncontext = 10          # Context window size                                                                                    \ndownsampling = 1e-3   # Downsample setting for frequent words\n\n# Initialize and train the model (this will take some time)\nprint (\"Training model...\")\nmodel = word2vec.Word2Vec(sentences, workers=num_workers, \\\n            size=num_features, min_count = min_word_count, \\\n            window = context, sample = downsampling)\n\n# If you don't plan to train the model any further, calling \n# init_sims will make the model much more memory-efficient.\nmodel.init_sims(replace=True)\n\n# It can be helpful to create a meaningful model name and \n# save the model for later use. You can load it later using Word2Vec.load()\nmodel_name = \"300features_40minwords_10context\"\nmodel.save(model_name)\nprint(\"Model saved\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Checking"},{"metadata":{"trusted":true},"cell_type":"code","source":"#The \"doesnt_match\" function will try to deduce which word in a set is most dissimilar from the others:\nmodel.doesnt_match(\"man woman child kitchen\".split())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.most_similar(\"man\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.most_similar(\"queen\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting the minimum word count to 40 gave us a total vocabulary of 16,492 words with 300 features apiece\n# model[\"flower\"] # returns 1*300 numpy array","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### From Words To Paragraphs # 1: Vector Averaging"},{"metadata":{"trusted":true},"cell_type":"code","source":"def makeFeatureVec(words, model, num_features):\n    # Function to average all of the word vectors in a given paragraph\n\n    # Pre-initialize an empty numpy array (for speed)\n    featureVec = np.zeros((num_features,),dtype=\"float32\")\n    nwords = 0\n\n    # Index2word is a list that contains the names of the words in \n    # the model's vocabulary. Convert it to a set, for speed \n    index2word_set = set(model.wv.index2word)\n    \n    # Loop over each word in the review and, if it is in the model's\n    # vocaublary, add its feature vector to the total\n    for word in words:\n        if word in index2word_set: \n            nwords = nwords + 1\n            featureVec = np.add(featureVec,model[word])\n            \n    # Divide the result by the number of words to get the average\n    featureVec = np.divide(featureVec,nwords)\n    return featureVec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getAvgFeatureVecs(reviews, model, num_features):\n    # Given a set of reviews (each one a list of words), calculate \n    # the average feature vector for each one and return a 2D numpy array \n    \n    # Initialize a counter\n    counter = 0\n    \n    # Preallocate a 2D numpy array, for speed\n    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n    \n    # Loop through the reviews\n    for review in reviews:\n        # Print a status message every 1000th review\n        if counter%1000 == 0:\n            print (\"Review %d of %d\" % (counter, len(reviews)))\n        \n       # Call the function (defined above) that makes average feature vectors\n        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n       \n       # Increment the counter\n        counter = counter + 1\n    return reviewFeatureVecs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate average feature vectors for training and testing sets,\n# using the functions we defined above. Notice that we now use stop word removal\n\nclean_train_reviews = []\nfor review in train[\"review\"]:\n    clean_train_reviews.append(review_to_wordlist(review, remove_stopwords=True))\n\ntrainDataVecs = getAvgFeatureVecs(clean_train_reviews, model, num_features)\n\nprint(\"Creating average feature vecs for test reviews\")\nclean_test_reviews = []\nfor review in test[\"review\"]:\n    clean_test_reviews.append(review_to_wordlist(review, remove_stopwords=True))\n\ntestDataVecs = getAvgFeatureVecs(clean_test_reviews, model, num_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit a random forest to the training data, using 100 trees\nfrom sklearn.ensemble import RandomForestClassifier\nforest1 = RandomForestClassifier(n_estimators = 100)\n\nprint(\"Fitting a random forest to labeled training data...\")\nforest1 = forest1.fit(trainDataVecs, train[\"sentiment\"])\n\n# Test & extract results \npred_f1 = forest1.predict(testDataVecs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#accuracy_f1 = round(forest1.score(trainDataVecs, train[\"sentiment\"]) * 100, 2)\n\n#accuracy_f1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\n# Write the test results \noutput = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":pred_f1})\noutput.to_csv(\"Word2Vec_AverageVectors.csv\", index=False, quoting=3)\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### From Words to Paragraphs #2: Clustering "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\nimport time\n\nstart = time.time() # Start time\n\n# Set \"k\" (num_clusters) to be 1/5th of the vocabulary size, or avg of 5 words per cluster\nword_vectors = model.wv.syn0 \nnum_clusters = int(word_vectors.shape[0] / 5)\n\n# Initalize a k-means object and use it to extract centroids\nkmeans_clustering = KMeans(n_clusters = num_clusters)\nidx = kmeans_clustering.fit_predict(word_vectors)\n\n# Get the end time and print how long the process took\nend = time.time()\nelapsed = end - start\nprint(\"Time taken for K Means clustering: \", elapsed, \"seconds.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a Word / Index dictionary, mapping each vocabulary word to\n# a cluster number                                                                                            \nword_centroid_map = dict(zip(model.wv.index2word, idx))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For the first 10 clusters\nfor cluster in range(0,10):\n    # Print the cluster number  \n    print(\"\\nCluster %d\" % cluster)\n    \n    # Find all of the words for that cluster number, and print them out\n    words = [k for k, v in word_centroid_map.items() if v == cluster]\n    print(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_bag_of_centroids(wordlist, word_centroid_map):\n    #\n    # The number of clusters is equal to the highest cluster index\n    # in the word / centroid map\n    num_centroids = max(word_centroid_map.values()) + 1\n    #\n    # Pre-allocate the bag of centroids vector (for speed)\n    bag_of_centroids = np.zeros(num_centroids, dtype=\"float32\")\n    #\n    # Loop over the words in the review. If the word is in the vocabulary,\n    # find which cluster it belongs to, and increment that cluster count \n    # by one\n    for word in wordlist:\n        if word in word_centroid_map:\n            index = word_centroid_map[word]\n            bag_of_centroids[index] += 1\n    #\n    # Return the \"bag of centroids\"\n    return bag_of_centroids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pre-allocate an array for the training set bags of centroids (for speed)\ntrain_centroids = np.zeros((train[\"review\"].size, num_clusters), dtype=\"float32\")\n\n# Transform the training set reviews into bags of centroids\ncounter = 0\nfor review in clean_train_reviews:\n    train_centroids[counter] = create_bag_of_centroids(review, word_centroid_map)\n    counter += 1\n\n# Repeat for test reviews \ntest_centroids = np.zeros((test[\"review\"].size, num_clusters), dtype=\"float32\")\n\ncounter = 0\nfor review in clean_test_reviews:\n    test_centroids[counter] = create_bag_of_centroids(review, word_centroid_map)\n    counter += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit a random forest and extract predictions \nforest2 = RandomForestClassifier(n_estimators = 100)\n\n# Fitting the forest may take a few minutes\nprint(\"Fitting a random forest to labeled training data...\")\nforest2 = forest2.fit(train_centroids,train[\"sentiment\"])\npred_f2 = forest2.predict(test_centroids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\n# Write the test results \noutput = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":pred_f2})\noutput.to_csv(\"BagOfCentroids.csv\", index=False, quoting=3)\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div id=\"6\"></div>\n# 6. Export output to csv & submit"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verify that there are 25,000 rows and 2 columns\ntest.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# EXPORTING PREDICTION FROM FIRST RANDOM FOREST MODEL\n\n#Create an empty list and append the clean reviews one by one\nnum_reviews = len(test[\"review\"])\nclean_test_reviews = [] \n\nfor i in range(0,num_reviews):\n    if((i+1) % 1000 == 0):\n        print(\"Review {} of {}\".format(i+1, num_reviews))\n    clean_review = review_to_words(test[\"review\"][i])\n    clean_test_reviews.append(clean_review)\n\n# Get a bag of words for the test set, and convert to a numpy array\ntest_data_features = vectorizer.transform(clean_test_reviews)\ntest_data_features = test_data_features.toarray()\n\n# Use the random forest to make sentiment label predictions\nresult = rf.predict(test_data_features)\n\n# Copy the results to a pandas dataframe with an \"id\" column and\n# a \"sentiment\" column\noutput = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})\n\n# Use pandas to write the comma-separated output file\noutput.to_csv(\"Bag_of_Words_model.csv\", index=False, quoting=3)\nprint(\"Output exported to csv!\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div id=\"7\"></div>\n# 7. Lessons learned"},{"metadata":{},"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\"> \n#### Key Takeaways \n- Use beautiful soup, regex & nltk stopwords to clean word data\n- Create Features with Bag of Words approach using scikit-learn CountVectorizer"},{"metadata":{},"cell_type":"markdown","source":"<div id=\"8\"></div>\n# 8. References:"},{"metadata":{},"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\">  \n- https://www.kaggle.com/c/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words\n- https://www.kaggle.com/c/word2vec-nlp-tutorial/overview/part-2-word-vectors\n- https://www.kaggle.com/c/word2vec-nlp-tutorial/overview/part-3-more-fun-with-word-vectors\n- https://www.kaggle.com/c/word2vec-nlp-tutorial/overview/part-4-comparing-deep-and-non-deep-learning-methods\n- https://www.kaggle.com/john77eipe/nlp-word2vec-bag-of-words-meets-bags-of-pop\n- https://www.kaggle.com/rohandx1996/google-movie-reviews-sentiment-deep-stack-models\n- https://www.kaggle.com/alexcherniuk/imdb-review-word2vec-bilstm-99-acc\n- https://www.kaggle.com/xchmiao/popcorn-rnn-model\n- https://www.kaggle.com/sameerdev7/93-f-score-bag-of-words-m-bags-of-popcorn-with-rf\n- https://www.kaggle.com/jiaofenx/imdb-review-word2vec-rnn-tutorial\n- https://www.kaggle.com/nichen301/movie-reviews-lstm-with-keras\n- https://www.kaggle.com/yepp2411/baseline-model-using-nn-for-movie-review\n- https://www.kaggle.com/rajspd/sentiment-analysis-in-20-lines-score-0-854\n- https://www.kaggle.com/yutanakamura/nlp-beginner-s-bow-model\n- https://www.kaggle.com/noi031/sentiment-analysis-with-self-attention\n- https://www.kaggle.com/lxyuan0420/imdb-review-bag-of-words-xgboost\n- https://www.kaggle.com/chiranjeevbit/nlp-movies-review\n- https://www.kaggle.com/sumitdua10/cnn-notebook-on-text-classification\n- https://www.kaggle.com/rajmehra03/a-detailed-explanation-of-keras-embedding-layer\n- https://www.kaggle.com/nilanml/imdb-review-deep-model-94-89-accuracy\n- https://www.kaggle.com/c/word2vec-nlp-tutorial/discussion/11261\n- https://www.kaggle.com/c/word2vec-nlp-tutorial/discussion/14966\n- https://sunscrapers.com/blog/8-best-python-natural-language-processing-nlp-libraries/"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}