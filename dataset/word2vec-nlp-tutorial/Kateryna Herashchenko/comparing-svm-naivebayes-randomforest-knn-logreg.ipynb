{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Bag of Words Meets Bags of Popcorn"},{"metadata":{},"cell_type":"markdown","source":"This is my second Kaggle competition. This is tutorial competition to learn about Word2vec neural network implementation but I realized project depending on tips for applying a simple Bag of Words model, mentioned in the tutorial. \n\nLink: https://www.kaggle.com/c/word2vec-nlp-tutorial\n\nProblem description: need to create model for sentiment analysis of movie reviews. The model must distinguish negative and positive movie reviews and mark them as 0 and 1 accordingly. (NLP problem)"},{"metadata":{},"cell_type":"markdown","source":"## 1 - Packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nimport re\nimport seaborn as sns\nfrom bs4 import BeautifulSoup\n\nfrom sklearn import model_selection, ensemble, metrics\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\n\nimport nltk\nnltk.download('stopwords', 'wordnet', 'punkt')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2 - Overview of the Dataset"},{"metadata":{},"cell_type":"markdown","source":"Loading data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"../input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip\", header=0, delimiter=\"\\t\", quoting=3)\ntest_data = pd.read_csv(\"../input/word2vec-nlp-tutorial/unlabeledTrainData.tsv.zip\", header=0, delimiter=\"\\t\", quoting=3)\nsubmit_data = pd.read_csv(\"../input/word2vec-nlp-tutorial/testData.tsv.zip\", header=0, delimiter=\"\\t\", quoting=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Shape of datasets:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape of train data: ', train_data.shape)\nprint('Shape of test data: ', test_data.shape)\nprint('Shape of submit data: ', submit_data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First 5 rows of train data dataset:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check column names:"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print('Column names:', list(train_data.columns))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see review samples."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"train_data.review[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Text has HTML tags, punctuation, stop words(such as “the”, “a”, “an”, “in”) and numbers. So, we need to clean our data and then tokenize it."},{"metadata":{},"cell_type":"markdown","source":"## 3 - Data Cleaning and Text Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"Next functions are created for data cleaning. Preprocessing function prepares a review for following splitting into tokens. Tokenizing function works with every word in review to get the most accurate tokens for analyzing."},{"metadata":{"trusted":true},"cell_type":"code","source":"def my_tokenizer(sample):\n    # Split into words\n    words = nltk.word_tokenize(sample)\n#     print('3',words)\n    \n    # Leave alphabetical tokens\n    tokens = [word for word in words if word.isalnum()]\n    tokens = [word for word in tokens if not word.isdigit()]\n#     print('4',tokens)\n    \n    # Remove stopwords\n    meaningful_words = [w for w in tokens if not w in stops]\n#     print('5',meaningful_words)\n    \n    # Lemmatization \n    word_list = [lemmatizer.lemmatize(w) for w in meaningful_words]\n#     print('6',word_list)\n    \n    return word_list\n\ndef my_preprocessor(sample):\n    # Remove HTML tags\n    no_tags_text = BeautifulSoup(sample).get_text()  \n#     print('1',no_tags_text)\n    \n    # To lowercase\n    review_text = no_tags_text.lower()\n#     print('2',review_text)\n    \n    return review_text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then we will instantiate lemmatizer and load useful set of stopwords. Now we are ready to instantiate `vectorizer` to perform whole text preprocessing and create bag-of-words features."},{"metadata":{"trusted":true},"cell_type":"code","source":"stops = set(stopwords.words(\"english\"))\nlemmatizer = WordNetLemmatizer()\nvectorizer = CountVectorizer(analyzer = \"word\", tokenizer = my_tokenizer, preprocessor = my_preprocessor, \\\n                             stop_words = None, max_features = 5000) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check work of `vectorizer` on the first sample in out train dataset."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"sample = train_data.review[0]\ntrain_data_features = vectorizer.fit_transform([sample])\ntrain_data_features = train_data_features.toarray()\n\ntrain_data_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, at least it works. I don't really know how to check if it's valid. Now fit our model and transform all train dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_features = vectorizer.fit_transform(train_data.review)\ntrain_data_features = train_data_features.toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_data_features = vectorizer.transform(test_data.review)\n# test_data_features = test_data_features.toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit_data_features = vectorizer.transform(submit_data.review)\nsubmit_data_features = submit_data_features.toarray()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The training data array now looks like:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_features.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that the Bag of Words model is trained, let's look at the vocabulary:"},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab = vectorizer.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4 - Model"},{"metadata":{},"cell_type":"markdown","source":"Below we will consider different models and choose that one, which gives the best metric rate. By the terms, submissions are judged on area under the ROC curve. \n\nLet's configure:\n1. SVM\n2. Naive Bayes\n3. RandomForestClassifier\n4. KNN\n5. Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_scorer = metrics.make_scorer(metrics.roc_auc_score)\nX_train = train_data_features[:500]\nY_train = train_data.sentiment[:500]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forest = RandomForestClassifier(n_estimators = 100) \nforest = forest.fit( train_data_features, train_data.sentiment )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = forest.predict(submit_data_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'kernel':['linear', 'rbf'], 'C':[0.1, 1, 5, 10]}\nsvc = SVC(probability = True, random_state = 0)\nclf = GridSearchCV(svc, param_grid = params, scoring = roc_auc_scorer, cv = 5, n_jobs = -1)\nclf.fit(X_train, Y_train)\nprint('Best score: {}'.format(clf.best_score_))\nprint('Best parameters: {}'.format(clf.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_best = SVC(C = clf.best_params_['C'], kernel = clf.best_params_['kernel'], probability = True, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### RandomForestClassifier"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"params = {'n_estimators':[10, 50, 100, 150], 'criterion':['gini', 'entropy'], 'max_depth':[None, 5, 10, 50]}\nrf = RandomForestClassifier(random_state = 0)\nclf = GridSearchCV(rf, param_grid = params, scoring = roc_auc_scorer, cv = 5, n_jobs = -1)\nclf.fit(X_train, Y_train)\nprint('Best score: {}'.format(clf.best_score_))\nprint('Best parameters: {}'.format(clf.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_best = RandomForestClassifier(n_estimators = clf.best_params_['n_estimators'], criterion = clf.best_params_['criterion'], \\\n                                 max_depth = clf.best_params_['max_depth'], random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LogisticRegression"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'penalty':['l1', 'l2'], 'C':[1, 2, 3, 5, 10]}\nlr = LogisticRegression(random_state = 0)\nclf = GridSearchCV(lr, param_grid = params, scoring = roc_auc_scorer, cv = 5, n_jobs = -1)\nclf.fit(X_train, Y_train)\nprint('Best score: {}'.format(clf.best_score_))\nprint('Best parameters: {}'.format(clf.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_best = LogisticRegression(penalty = clf.best_params_['penalty'], C = clf.best_params_['C'], random_state = 0)\n# lr_best = LogisticRegression(penalty = 'l2', C = 10, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\"var_smoothing\" : [1e-8, 1e-7, 1e-6, 1e-5, 1e-4]}\nnb = GaussianNB()\nclf = GridSearchCV(nb, param_grid = params, scoring = roc_auc_scorer, cv = 5, n_jobs = -1)\nclf.fit(X_train, Y_train)\nprint('Best score: {}'.format(clf.best_score_))\nprint('Best parameters: {}'.format(clf.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb_best = GaussianNB(var_smoothing = clf.best_params_['var_smoothing'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### KNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'n_neighbors':[3, 5, 10, 20], 'p':[1, 2, 5], 'weights':['uniform', 'distance']}\nknc = KNeighborsClassifier()\nclf = GridSearchCV(knc, param_grid = params, scoring = roc_auc_scorer, cv = 5, n_jobs = -1)\nclf.fit(X_train, Y_train)\nprint('Best score: {}'.format(clf.best_score_))\nprint('Best parameters: {}'.format(clf.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knc_best = KNeighborsClassifier(n_neighbors = clf.best_params_['n_neighbors'], p=clf.best_params_['p'],\\\n                               weights = clf.best_params_['weights'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# voting_clf = VotingClassifier(estimators=[('svc', svc_best), ('rf', rf_best), ('lr', lr_best), ('nb', nb_best),\\\n#                                           ('knc', knc_best)], voting='hard')\n# voting_clf.fit(train_data_features, train_data.sentiment)\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"lr_best.fit(train_data_features, train_data.sentiment)\ny_pred = lr_best.predict(submit_data_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Downloading results to file. "},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(\"../input/word2vec-nlp-tutorial/sampleSubmission.csv\", header=0, delimiter=\",\", quoting=3)\ncol = submission.columns[1]\nsubmission[col] = y_pred\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Somewhy this file saved symbols \"\"\" instead of \". And I rewrited it in this way:"},{"metadata":{"trusted":true},"cell_type":"code","source":"f = open(\"submission.csv\", \"r\")\nf.readline()\ns = open(\"valid_submission.csv\",\"w+\")\ns.write('\\\"id\\\",\\\"sentiment\\\"\\n')\nfor x in f:\n    x = x.split(',')\n    x[0] = x[0][2:-2]\n    s.write(','.join(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}