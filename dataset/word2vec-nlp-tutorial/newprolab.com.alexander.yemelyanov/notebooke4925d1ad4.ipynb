{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-25T19:36:40.342567Z","iopub.execute_input":"2021-10-25T19:36:40.342889Z","iopub.status.idle":"2021-10-25T19:36:41.55051Z","shell.execute_reply.started":"2021-10-25T19:36:40.342808Z","shell.execute_reply":"2021-10-25T19:36:41.548778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import the pandas package, then use the \"read_csv\" function to read\n# the labeled training data\n      \ntrain = pd.read_csv(\"/kaggle/input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip\",\n                    header=0,\n                    delimiter=\"\\t\",\n                    quoting=3)\n","metadata":{"execution":{"iopub.status.busy":"2021-10-25T19:36:41.552378Z","iopub.execute_input":"2021-10-25T19:36:41.552638Z","iopub.status.idle":"2021-10-25T19:36:42.790571Z","shell.execute_reply.started":"2021-10-25T19:36:41.552604Z","shell.execute_reply":"2021-10-25T19:36:42.789509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T19:36:42.79305Z","iopub.execute_input":"2021-10-25T19:36:42.793357Z","iopub.status.idle":"2021-10-25T19:36:42.93704Z","shell.execute_reply.started":"2021-10-25T19:36:42.793324Z","shell.execute_reply":"2021-10-25T19:36:42.936051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-25T19:36:42.93875Z","iopub.execute_input":"2021-10-25T19:36:42.939051Z","iopub.status.idle":"2021-10-25T19:36:42.946823Z","shell.execute_reply.started":"2021-10-25T19:36:42.939005Z","shell.execute_reply":"2021-10-25T19:36:42.945525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/word2vec-nlp-tutorial/testData.tsv.zip\", \n                   header=0,\n                   delimiter=\"\\t\",\n                   quoting=3 )","metadata":{"execution":{"iopub.status.busy":"2021-10-25T19:36:42.951296Z","iopub.execute_input":"2021-10-25T19:36:42.951639Z","iopub.status.idle":"2021-10-25T19:36:44.019411Z","shell.execute_reply.started":"2021-10-25T19:36:42.951599Z","shell.execute_reply":"2021-10-25T19:36:44.018525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T19:36:44.02089Z","iopub.execute_input":"2021-10-25T19:36:44.021235Z","iopub.status.idle":"2021-10-25T19:36:44.03186Z","shell.execute_reply.started":"2021-10-25T19:36:44.021191Z","shell.execute_reply":"2021-10-25T19:36:44.03101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-25T19:36:44.033536Z","iopub.execute_input":"2021-10-25T19:36:44.034112Z","iopub.status.idle":"2021-10-25T19:36:44.046621Z","shell.execute_reply.started":"2021-10-25T19:36:44.034075Z","shell.execute_reply":"2021-10-25T19:36:44.04569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train[\"review\"][0])","metadata":{"execution":{"iopub.status.busy":"2021-10-25T19:36:44.049362Z","iopub.execute_input":"2021-10-25T19:36:44.04996Z","iopub.status.idle":"2021-10-25T19:36:44.064375Z","shell.execute_reply.started":"2021-10-25T19:36:44.049915Z","shell.execute_reply":"2021-10-25T19:36:44.063325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport nltk\n\nfrom bs4 import BeautifulSoup\nfrom nltk.corpus import stopwords\n\n\nclass KaggleWord2VecUtility(object):\n    \"\"\"KaggleWord2VecUtility is a utility class for processing raw HTML text into segments for further learning\"\"\"\n\n    @staticmethod\n    def review_to_wordlist( review, remove_stopwords=False ):\n        # Function to convert a document to a sequence of words,\n        # optionally removing stop words.  Returns a list of words.\n        #\n        # 1. Remove HTML\n        review_text = BeautifulSoup(review).get_text()\n        #\n        # 2. Remove non-letters\n        review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)  # 'r([^\\w]|[+])'\n        #\n        # 3. Convert words to lower case and split them\n        words = review_text.lower().split()\n        #\n        # 4. Optionally remove stop words (false by default)\n        if remove_stopwords:\n            stops = set(stopwords.words(\"english\"))\n            words = [w for w in words if not w in stops]\n        #\n        # 5. Return a list of words\n        return(words)\n\n    # Define a function to split a review into parsed sentences\n    @staticmethod\n    def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n        # Function to split a review into parsed sentences. Returns a\n        # list of sentences, where each sentence is a list of words\n        #\n        # 1. Use the NLTK tokenizer to split the paragraph into sentences\n        raw_sentences = tokenizer.tokenize(review.strip())\n        #\n        # 2. Loop over each sentence\n        sentences = []\n        for raw_sentence in raw_sentences:\n            # If a sentence is empty, skip it\n            if len(raw_sentence) > 0:\n                # Otherwise, call review_to_wordlist to get a list of words\n                sentences.append( KaggleWord2VecUtility.review_to_wordlist( raw_sentence,\n                                                                           remove_stopwords ))\n        #\n        # Return the list of sentences (each sentence is a list of words,\n        # so this returns a list of lists\n        return sentences\n","metadata":{"execution":{"iopub.status.busy":"2021-10-25T19:36:44.066461Z","iopub.execute_input":"2021-10-25T19:36:44.067081Z","iopub.status.idle":"2021-10-25T19:36:44.769677Z","shell.execute_reply.started":"2021-10-25T19:36:44.067032Z","shell.execute_reply":"2021-10-25T19:36:44.768881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize an empty list to hold the clean reviews\nclean_train_reviews = []\n\n# Loop over each review; create an index i that goes from 0 to the length\n# of the movie review list\n\nprint (\"Cleaning and parsing the training set movie reviews...\\n\")\nfor i in range( 0, len(train[\"review\"])):\n    clean_train_reviews.append(\" \".join(KaggleWord2VecUtility.review_to_wordlist(train[\"review\"][i], True)))\n","metadata":{"execution":{"iopub.status.busy":"2021-10-25T19:36:44.772544Z","iopub.execute_input":"2021-10-25T19:36:44.773713Z","iopub.status.idle":"2021-10-25T19:37:05.843185Z","shell.execute_reply.started":"2021-10-25T19:36:44.77366Z","shell.execute_reply":"2021-10-25T19:37:05.842415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_train_reviews[0]","metadata":{"execution":{"iopub.status.busy":"2021-10-25T19:37:05.844707Z","iopub.execute_input":"2021-10-25T19:37:05.845141Z","iopub.status.idle":"2021-10-25T19:37:05.852622Z","shell.execute_reply.started":"2021-10-25T19:37:05.845077Z","shell.execute_reply":"2021-10-25T19:37:05.851868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(clean_train_reviews)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T19:37:05.853995Z","iopub.execute_input":"2021-10-25T19:37:05.854727Z","iopub.status.idle":"2021-10-25T19:37:05.86673Z","shell.execute_reply.started":"2021-10-25T19:37:05.854677Z","shell.execute_reply":"2021-10-25T19:37:05.865891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"CountVectorization\n\nThe Bag of Words model learns a vocabulary from all of the documents, then models each document by counting the number of times each word appears. For example, consider the following two sentences:\n\nSentence 1: \"The cat sat on the hat\"\n\nSentence 2: \"The dog ate the cat and the hat\"\n\nFrom these two sentences, our vocabulary is as follows:\n\n{ the, cat, sat, on, hat, dog, ate, and }\n\nTo get our bags of words, we count the number of times each word occurs in each sentence. In Sentence 1, \"the\" appears twice, and \"cat\", \"sat\", \"on\", and \"hat\" each appear once, so the feature vector for Sentence 1 is:\n\n{ the, cat, sat, on, hat, dog, ate, and }\n\nSentence 1: { 2, 1, 1, 1, 1, 0, 0, 0 }\n\nSimilarly, the features for Sentence 2 are: { 3, 1, 0, 0, 1, 1, 1, 1}\n\nIn the IMDB data, we have a very large number of reviews, which will give us a large vocabulary. To limit the size of the feature vectors, we should choose some maximum vocabulary size. Below, we use the 5000 most frequent words (remembering that stop words have already been removed).\n","metadata":{"execution":{"iopub.status.busy":"2021-10-25T17:58:59.157471Z","iopub.execute_input":"2021-10-25T17:58:59.158315Z","iopub.status.idle":"2021-10-25T17:58:59.170957Z","shell.execute_reply.started":"2021-10-25T17:58:59.158265Z","shell.execute_reply":"2021-10-25T17:58:59.169078Z"}}},{"cell_type":"code","source":"# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n# bag of words tool.\ncountVectorizer = CountVectorizer(analyzer = \"word\",   \\\n                         tokenizer = None,    \\\n                         preprocessor = None, \\\n                         stop_words = None,   \\\n                         max_features = 5000)\n\n# fit_transform() does two functions: First, it fits the model\n# and learns the vocabulary; second, it transforms our training data\n# into feature vectors. The input to fit_transform should be a list of\n# strings.\ntrain_data_features = countVectorizer.fit_transform(clean_train_reviews)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T19:37:05.868104Z","iopub.execute_input":"2021-10-25T19:37:05.868469Z","iopub.status.idle":"2021-10-25T19:37:10.102966Z","shell.execute_reply.started":"2021-10-25T19:37:05.868432Z","shell.execute_reply":"2021-10-25T19:37:10.102092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(countVectorizer.get_feature_names())","metadata":{"execution":{"iopub.status.busy":"2021-10-25T19:37:10.108126Z","iopub.execute_input":"2021-10-25T19:37:10.108395Z","iopub.status.idle":"2021-10-25T19:37:10.121935Z","shell.execute_reply.started":"2021-10-25T19:37:10.108366Z","shell.execute_reply":"2021-10-25T19:37:10.12124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(train_data_features)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T19:37:10.123389Z","iopub.execute_input":"2021-10-25T19:37:10.123825Z","iopub.status.idle":"2021-10-25T19:37:10.138084Z","shell.execute_reply.started":"2021-10-25T19:37:10.12379Z","shell.execute_reply":"2021-10-25T19:37:10.137442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Numpy arrays are easy to work with, so convert the result to an\n# array\nnp.asarray(train_data_features)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T19:37:10.139396Z","iopub.execute_input":"2021-10-25T19:37:10.140176Z","iopub.status.idle":"2021-10-25T19:37:10.150977Z","shell.execute_reply.started":"2021-10-25T19:37:10.140128Z","shell.execute_reply":"2021-10-25T19:37:10.150049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (\"Training the random forest (this may take a while)...\")\n\n\n# Initialize a Random Forest classifier with 100 trees\nforest = RandomForestClassifier(n_estimators = 200)\n\n# Fit the forest to the training set, using the bag of words as\n# features and the sentiment labels as the response variable\n#\n# This may take a few minutes to run\nforest = forest.fit( train_data_features, train[\"sentiment\"] )\n","metadata":{"execution":{"iopub.status.busy":"2021-10-25T19:37:10.152444Z","iopub.execute_input":"2021-10-25T19:37:10.15451Z","iopub.status.idle":"2021-10-25T19:39:30.565348Z","shell.execute_reply.started":"2021-10-25T19:37:10.154457Z","shell.execute_reply":"2021-10-25T19:39:30.564369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Testing - we pick only the first 2 test reviews alone to save time\n\n\n# Create an empty list and append the clean reviews one by one\nclean_test_reviews = []\n\nprint(\"Cleaning and parsing the test set movie reviews...\\n\")\nfor i in range(0,len(test)):\n    clean_test_reviews.append(\" \".join(KaggleWord2VecUtility.review_to_wordlist(test[\"review\"][i], True)))\n\n# Get a bag of words for the test set, and convert to a numpy array\ntest_data_features = countVectorizer.transform(clean_test_reviews)\nnp.asarray(test_data_features)\n\n# Use the random forest to make sentiment label predictions\nprint (\"Predicting test labels...\\n\")\nresult = forest.predict(test_data_features)\n\n# Write the test results \nprint(\"*****manual verification*******\")\nprint(test[\"review\"][0])\nprint(result[0])\nprint(test[\"review\"][1])\nprint(result[1])","metadata":{"execution":{"iopub.status.busy":"2021-10-25T19:39:30.566846Z","iopub.execute_input":"2021-10-25T19:39:30.567748Z","iopub.status.idle":"2021-10-25T19:39:58.003004Z","shell.execute_reply.started":"2021-10-25T19:39:30.5677Z","shell.execute_reply":"2021-10-25T19:39:58.002074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[\"id\"]","metadata":{"execution":{"iopub.status.busy":"2021-10-25T19:39:58.004283Z","iopub.execute_input":"2021-10-25T19:39:58.005111Z","iopub.status.idle":"2021-10-25T19:39:58.015454Z","shell.execute_reply.started":"2021-10-25T19:39:58.005065Z","shell.execute_reply":"2021-10-25T19:39:58.014404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result","metadata":{"execution":{"iopub.status.busy":"2021-10-25T19:39:58.016636Z","iopub.execute_input":"2021-10-25T19:39:58.016861Z","iopub.status.idle":"2021-10-25T19:39:58.032238Z","shell.execute_reply.started":"2021-10-25T19:39:58.016834Z","shell.execute_reply":"2021-10-25T19:39:58.031349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Copy the results to a pandas dataframe with an \"id\" column and\n# a \"sentiment\" column\noutput = pd.DataFrame( data={\"id\":test.id.apply(lambda x: x.replace('\"', '')), \"sentiment\":result} )","metadata":{"execution":{"iopub.status.busy":"2021-10-25T19:39:58.0346Z","iopub.execute_input":"2021-10-25T19:39:58.0349Z","iopub.status.idle":"2021-10-25T19:39:58.061475Z","shell.execute_reply.started":"2021-10-25T19:39:58.034868Z","shell.execute_reply":"2021-10-25T19:39:58.060542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T19:39:58.06275Z","iopub.execute_input":"2021-10-25T19:39:58.063061Z","iopub.status.idle":"2021-10-25T19:39:58.073884Z","shell.execute_reply.started":"2021-10-25T19:39:58.063021Z","shell.execute_reply":"2021-10-25T19:39:58.072958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use pandas to write the comma-separated output file\noutput.to_csv('./Submission.csv', index=False)\nprint (\"Wrote results to Submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-10-25T19:39:58.075769Z","iopub.execute_input":"2021-10-25T19:39:58.076372Z","iopub.status.idle":"2021-10-25T19:39:58.138821Z","shell.execute_reply.started":"2021-10-25T19:39:58.076327Z","shell.execute_reply":"2021-10-25T19:39:58.138029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(output)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T19:39:58.140343Z","iopub.execute_input":"2021-10-25T19:39:58.140852Z","iopub.status.idle":"2021-10-25T19:39:58.147464Z","shell.execute_reply.started":"2021-10-25T19:39:58.140805Z","shell.execute_reply":"2021-10-25T19:39:58.146565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T19:39:58.149062Z","iopub.execute_input":"2021-10-25T19:39:58.149608Z","iopub.status.idle":"2021-10-25T19:39:58.165561Z","shell.execute_reply.started":"2021-10-25T19:39:58.149562Z","shell.execute_reply":"2021-10-25T19:39:58.16439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}