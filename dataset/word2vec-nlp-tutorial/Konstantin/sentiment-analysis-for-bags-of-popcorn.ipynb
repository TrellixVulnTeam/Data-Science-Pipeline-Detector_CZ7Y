{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca233719bd109a69fa0d0e4517836d5318682d1d"},"cell_type":"code","source":"from bs4 import BeautifulSoup\nfrom gensim.models import word2vec\nfrom nltk.corpus import stopwords\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import make_scorer, roc_auc_score\nfrom sklearn.model_selection import GridSearchCV\nfrom tqdm import tqdm_notebook\nimport re","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"tsvread_params = {\n    \"delimiter\": '\\t',\n    \"quoting\": 3,\n    \"header\": 0\n}\ntrain = pd.read_csv(\"../input/labeledTrainData.tsv\", **tsvread_params)\ntest = pd.read_csv(\"../input/testData.tsv\", **tsvread_params)\nunlabeled_train = pd.read_csv(\"../input/unlabeledTrainData.tsv\", **tsvread_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9a32624ed4e0b015785041f71f31f35118232dc"},"cell_type":"code","source":"print(\"Train data shape:\", train.shape)\nprint(\"Test data shape:\", test.shape)\nprint(\"Unlabeled data shape:\", unlabeled_train.shape)\nprint(\"Columns:\", train.columns.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f80326f7f2de0748bf4a049c0c5e0ec7147a5eb"},"cell_type":"code","source":"def clean_review(raw_text, remove_stops=True, result_as_list=True):\n    # remove HTML\n    text = BeautifulSoup(raw_text, \"lxml\").get_text()\n    \n    # remove non-letters\n    letters_text = re.sub(r\"[^a-zA-Z]\", ' ', text)\n    \n    # lower case and split\n    words = letters_text.lower().split()\n    \n    # remove stop words\n    if remove_stops:\n        stops = set(stopwords.words(\"english\"))\n        words = [w for w in words if not w in stops]\n    if not result_as_list:\n        words = ' '.join(words)\n    return words","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"baf7708c6235504ed6241141406d303f8625c09f"},"cell_type":"markdown","source":"# Word2Vec training"},{"metadata":{"trusted":true,"_uuid":"3b2eb5f41b9fe8bbfbaef5e256b35fbb898a72eb"},"cell_type":"code","source":"sentences = [clean_review(review, remove_stops=False) for review in unlabeled_train[\"review\"].values]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d2f0a93e06e780a3d218c0acfa4de7938ece5bb"},"cell_type":"code","source":"num_features = 3000\nmodel_params = {\n    \"size\": num_features, # Word vector dimensionality\n    \"min_count\": 40,      # Minimum word count\n    \"workers\": 4,         # Number of threads to run in parallel\n    \"window\": 10,         # Context window size\n    \"sample\": 1e-3,       # Downsample setting for frequent words\n}\n\nw2v = word2vec.Word2Vec(sentences, **model_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bab8a36c1e5ac7f8ad2a4fa1df5baed0683f2490"},"cell_type":"code","source":"w2v.doesnt_match(\"king count queen princess\".split())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be8b8c95e314443a84df14e0a4970626db275c34"},"cell_type":"code","source":"w2v.most_similar(\"alien\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"27bef1be826af037180a2190c60fe0db5141417c"},"cell_type":"markdown","source":"# Feature Extraction"},{"metadata":{"trusted":true,"_uuid":"b0e24ad5de2a7b50ebef90350ab05ed94e391385"},"cell_type":"code","source":"def makeFeatureVec(words, model, num_features):\n    # Function to average all of the word vectors in a given\n    # paragraph\n    #\n    # Pre-initialize an empty numpy array (for speed)\n    featureVec = np.zeros((num_features,),dtype=\"float32\")\n    #\n    nwords = 0.\n    # \n    # Index2word is a list that contains the names of the words in \n    # the model's vocabulary. Convert it to a set, for speed \n    index2word_set = set(model.wv.index2word)\n    #\n    # Loop over each word in the review and, if it is in the model's\n    # vocaublary, add its feature vector to the total\n    for word in words:\n        if word in index2word_set: \n            nwords = nwords + 1.\n            featureVec = np.add(featureVec, model[word])\n    # \n    # Divide the result by the number of words to get the average\n    featureVec = np.divide(featureVec,nwords)\n    return featureVec\n\n\ndef getAvgFeatureVecs(reviews, model, num_features):\n    # Given a set of reviews (each one a list of words), calculate \n    # the average feature vector for each one and return a 2D numpy array \n    # \n    # Initialize a counter\n    counter = 0\n    # \n    # Preallocate a 2D numpy array, for speed\n    reviewFeatureVecs = np.zeros((len(reviews), num_features), dtype=\"float32\")\n    # \n    # Loop through the reviews\n    for review in tqdm_notebook(reviews, desc=\"Reviews preprocessed\"):\n        # Call the function (defined above) that makes average feature vectors\n        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n        #\n        # Increment the counter\n        counter += 1\n    return reviewFeatureVecs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"45957d16e511e4f3a5d684ba752f6d7d0b6b7ec6"},"cell_type":"code","source":"print(\"Creating average feature vecs for train reviews\")\nclean_train_reviews = []\nfor review in train[\"review\"]:\n    clean_train_reviews.append(clean_review(review))\n\ntrain_features = getAvgFeatureVecs(clean_train_reviews, w2v, num_features)\n\nprint(\"Creating average feature vecs for test reviews\")\nclean_test_reviews = []\nfor review in test[\"review\"]:\n    clean_test_reviews.append(clean_review(review))\n\ntest_features = getAvgFeatureVecs(clean_test_reviews, w2v, num_features)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1980f1b8c3747ca4ad69ff6883b088fcee827cce"},"cell_type":"markdown","source":"# Learning"},{"metadata":{"trusted":true,"_uuid":"928d7d01e7b4e2f9e19ff5aacec71356110fa847"},"cell_type":"code","source":"model = RandomForestClassifier(n_estimators=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"25e9108186fa3ef93578c513644873437fbd79c0"},"cell_type":"code","source":"model.fit(train_features, train[\"sentiment\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c1a0fbd2835ff6f483960ec69cd4e55b3a21b0e4"},"cell_type":"code","source":"y_pred = model.predict(test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"57277eb41e93ca19b2d12681aaaaf148b11fb7aa"},"cell_type":"code","source":"output = pd.DataFrame(data={\"id\": test[\"id\"], \"sentiment\": y_pred})\noutput.to_csv(\"out.csv\", index=False, quoting=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8756c3707119ce039ddfa7abaec5eec6b0eaab35"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}