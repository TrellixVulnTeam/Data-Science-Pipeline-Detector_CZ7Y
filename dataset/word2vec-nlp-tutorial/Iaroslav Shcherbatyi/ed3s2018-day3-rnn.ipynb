{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Time Series with Recurrent Neural Networks\n\n[Iaroslav Shcherbatyi](http://iaroslav-ai.github.io/), [ED3S 2018](http://iss.uni-saarland.de/de/ds-summerschool/)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Synopsis\n\nOften, real world data comes as time series  - [sequences of data points indexed in time order](https://en.wikipedia.org/wiki/Time_series). Records of vital signs of patients, sequences of physical measurements, video and audio streams, financial market data, corporate historical records are some of many examples of time series data. In this notebook, Recurrent Neural Networks are used to process time series data, as well as \"classical\" methods, that work with inputs being vectors of values.\n\nNote: if you are viewing this notebook on Kaggle, you can download the notebook by clicking the cloud with arrow pointing down in the upper panel, and the necessary data from the panel to the right. "},{"metadata":{"_uuid":"7a7c96b98667163b3396e9119b543bd1ccf6ad1d"},"cell_type":"markdown","source":"# Lumber futures price prediction\n\nIn this task, we will come up with a model, which can forecast change of the price of the lumber futures. The following data is used: [Random Length Lumber Futures, Continuous Contract #1 (LB1) (Front Month)](https://www.quandl.com/data/CHRIS/CME_LB1-Random-Length-Lumber-Futures-Continuous-Contract-1-LB1-Front-Month?utm_medium=graph&utm_source=quandl). A subset of 3000 last historical records is used to reduce computations.\n\nFirst, lets load the data!"},{"metadata":{"trusted":true,"_uuid":"cb89e1bf62dd3997b4b8ddad681e1ba6be41cdb8"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nlumber = pd.read_csv('../input/quandl-lumber-price-history/lumber.csv')\nlumber = lumber[::-1]\nlumber = lumber.drop('Date', axis=1)\ndisplay(lumber[:12])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27e5d3977b5e0d6364b7c32b87141c6cf156b57d"},"cell_type":"code","source":"from skimage.util import view_as_windows\n\nhistory = view_as_windows(lumber.values, (10,7)).squeeze()\nX = history[:, :-1, :]\ny = history[:, -1, 0] > history[:, -2, 3]\n\nprint(X.shape)\nprint(y.shape)\n\nprint(X[0])\nprint(y[0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a858b25e9bf8574bb31d1a1c03431cfee93142f2"},"cell_type":"markdown","source":"# Recurrent Neural Networks in Keras\n\nLets apply RNN first. RNN in Keras is also a layer, that takes as input sequence of vectors, and outputs the final activation of RNN. There are multiple flavors of RNN layers in Keras, such as LSTM or GRU. A few remarks are in place:\n* Depending on random initialization of the weights of the neural network, testing results will be different. Fixing the random initialization can be done using `numpy` package, used internally in Keras, specifically using `numpy.random.seed` function.\n* Standardizing the scale of the feature ranges in Keras is also imoprtant!\n* Different types of RNN can lead to different results."},{"metadata":{"trusted":true,"_uuid":"25930003078f60a101eda0647d6eafff3afdd3d7"},"cell_type":"code","source":"# GRU, SimpleRNN, LSTM are recurrent layers\nfrom keras.layers import Input, GRU, SimpleRNN, LSTM, Dense, LeakyReLU, Softmax\nfrom keras.models import Model\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.losses import sparse_categorical_crossentropy\n\n#np.random.seed(1)\nX_train, X_test, y_train, y_test = X[:1500], X[1500:], y[:1500], y[1500:]\n\nm, s = np.mean(X_train, axis=(0, 1)), np.std(X_train, axis=(0, 1))\nX_train = (X_train - m) / s\nX_test = (X_test - m) / s\n\ninp = Input(shape=(9, 7))\nh = inp\n# Task: try SimpleRNN, GRU, LSTM\nh = GRU(16)(h)  # this takes as input a sequence, and returns last activation of RNN\nh = Dense(2)(h)\nh = Softmax()(h)\n\nmodel = Model(inputs=[inp], outputs=[h])\nmodel.compile(Adam(0.001), sparse_categorical_crossentropy, ['accuracy'])\nmodel.fit(X_train, y_train, epochs=5, verbose=1)\n\n# get the model performance\nloss, acc = model.evaluate(X_test, y_test)\nprint(acc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc671e2911ee0dfe40ea0b78d4e55eef6536fb77"},"cell_type":"markdown","source":"Lets use a trained model to make predictions. Observing how the outputs of the model change by changing inputs might be informative of how the model \"thinks\"."},{"metadata":{"trusted":true,"_uuid":"9eedb687cf5a366efa022f56309416b69b613e5b"},"cell_type":"code","source":"# Task: something missing here?\nx = np.array([\n    [\n        [339., 341., 337.5, 339.3, 339.3, 159., 838.],\n        [338.3, 338.5, 336., 336.5, 336.5, 149., 714.],\n        [337.3, 337.3, 334., 335., 335., 278., 567.],\n        [336., 336., 328.2, 329.1, 329.1, 326., 464.],\n        [332., 336., 331., 331.2, 331.2, 167., 387.],\n        [331.5, 332.4, 330.2, 330.4, 330.4, 97., 302.],\n        [331.3, 334.8, 330., 331.5, 331.5, 243., 209.],\n        [332.2, 334.5, 326.6, 326.6, 326.6, 123., 104.],\n        [332.9, 332.9, 325., 325., 325., 83., 93.]\n    ]\n])\nmodel.predict(x)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c549108bf1883ae9960e72b17da3016833bfb10"},"cell_type":"markdown","source":"# Scikit-Learn for sequence processing\n\nSometimes models which are not explicitly made for sequence processing, can be quite competitive nontheless. Lets try linear SVM below to compare on how it performs to RNN."},{"metadata":{"trusted":true,"_uuid":"9c3745688709169d54382c4c5fbb1ac3e06661e1"},"cell_type":"code","source":"from sklearn.svm import LinearSVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.dummy import DummyClassifier\n\nXf = np.reshape(X, (len(X), -1))\nX_train, X_test, y_train, y_test = Xf[:1500], Xf[1500:], y[:1500], y[1500:]\n\nmodel = make_pipeline(\n    StandardScaler(),\n    LinearSVC('l1', dual=False)  # Task: use L1 regularization\n)\n\nmodel.fit(X_train, y_train)\nscore = model.score(X_test, y_test)\n\ndummy = DummyClassifier()\ndummy.fit(X_train, y_train)\ndummy_score = dummy.score(X_test, y_test)\n\nprint(score)\nprint(dummy_score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13326fffe267b2ebbf3acb1d379b744a69682e40"},"cell_type":"markdown","source":"The code below renders the weights of the linear model."},{"metadata":{"trusted":true,"_uuid":"5d5887759b80386a659916148efa14b4e26e6146"},"cell_type":"code","source":"shape = X[0].shape\nlsvc = model.steps[-1][-1]\n\n# reshape weights to fit the shape of input\nw = np.reshape(lsvc.coef_, shape)\n\n# display what weights are applied to what features of input sequence\ndisplay(pd.DataFrame(w, columns=lumber.columns))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bfa9bf7285f8a8b0b1c4515bbc4869500b673acc"},"cell_type":"markdown","source":"# IMDB data classification\n\nOne particularly successful application of RNN is in sentiment analysis, where the task is to infer whether an excert of a text has a positive tone or not. Below data is used for 25000 movie reviews from the [kaggle nlp challenge](https://www.kaggle.com/c/word2vec-nlp-tutorial)."},{"metadata":{"trusted":true,"_uuid":"f9c1e0ee8bcbabe338815e85e4ace6a891cee4b6"},"cell_type":"code","source":"# first, load the data!\nimport pandas as pd\n\ndata = pd.read_csv('../input/word2vec-nlp-tutorial/labeledTrainData.tsv', sep='\\t')\ndisplay(data.head())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b91224ae92e670176f9eeeeb988694b5e3d679d"},"cell_type":"markdown","source":"Lets split the data into inputs and outputs, and see in more detail what a review looks like. "},{"metadata":{"trusted":true,"_uuid":"cc1cab29c1f97a49d6ad6c5ad22e9c7628c7b5b3"},"cell_type":"code","source":"# split inputs and outputs\nX = data['review'].values\ny = data['sentiment'].values\n\nprint(X[0])\nprint(y[0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"72951584e491f51fb7e0b80a9849695a99457506"},"cell_type":"markdown","source":"One successful approach in working with text is to use word level representation. Firstly, some number $N$ of most frequent words is extracted from training data. Then, all texts are converted to sequences of words. Every word is converted to integer number, which corresponds to the frequency rank of the word. If word does not belong to $N$ most frequent words, it is replaced with 0. For example, consider that $N$ is set to 3. Then the following transformation is done:\n\n['aa aa aa ba aa ab abc'] -> [1, 1, 1, 2, 1, 3, 0]\n\nIn this example, a dedicated `TextToIntSeq` is defined, which also showcases how data transformers are implemented in `sklearn`."},{"metadata":{"trusted":true,"_uuid":"e9e67dc93ed3ac404b43633b24a35bdcdf169f85","collapsed":true},"cell_type":"code","source":"import numpy as np\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/text.py\nfrom keras.preprocessing.text import Tokenizer\n\n# proper sklearn TransformerMixin class\nclass TextToIntSeq(BaseEstimator, TransformerMixin):\n    \"\"\" for a set of text, convert every text to sequence of num_words\n    most frequent words, where a word is represented as integer. Words\n    which are not frequent enough are replaced with 0.\n    \"\"\"\n    def __init__(self, num_words=10000, max_seq_length=80):\n        self.num_words = num_words\n        self.max_seq_length = max_seq_length\n        self._tokenizer = None\n    \n    def fit(self, X, y=None):\n        # X: list of texts\n        self._tokenizer = Tokenizer(self.num_words)\n        self._tokenizer.fit_on_texts(X)\n        return self  # proper sklearn transformer\n    \n    def transform(self, X, y=None):\n        N = self.max_seq_length\n        X = self._tokenizer.texts_to_sequences(X) # convert texts to sequences\n        # trim sequences which are too long\n        X = [x[:min(len(x), N)] for x in X]\n        # add zeros for too small sequences\n        X = [(N - len(x))*[0] + x for x in X]\n        return np.array(X)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0cc3688097fd484c12b976dea6de632d727b0c71"},"cell_type":"markdown","source":"Lets use the newly defined class to convert our dataset to sequence of integers representation."},{"metadata":{"trusted":true,"_uuid":"78d171db62ad8475d9e13d491ad2975908666a88","collapsed":true},"cell_type":"code","source":"tok = TextToIntSeq()\ntok.fit(X)\nXt = tok.transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f026c3bf788a8dc94dd63db8b2a530f98f1a0a0"},"cell_type":"code","source":"print(Xt.shape)\nprint(tok.transform(np.array([\n    'Hello world'\n])))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03aff45ae1cf0843860d4c460c5340aa8150b57a"},"cell_type":"markdown","source":"An integer sequence representation of text data is easy to use with Keras. A dedicated `Embedding` layer can convert sequence of integers to sequence of features, which describe every word in sequence. For more details on `Embedding` layer, [refer here](https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work)."},{"metadata":{"trusted":true,"_uuid":"fc5048ed7129b32fd621cc8a19568bb4948a6e61"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, GRU, Embedding, Softmax\nfrom keras.losses import sparse_categorical_crossentropy\nfrom keras.optimizers import Adam\n\nX_train, X_test, y_train, y_test = train_test_split(Xt, y)\n\n# definition of the network\ninp = Input(shape=X_train[0].shape)\n\nh = Embedding(tok.num_words, 128)(inp)\nh = GRU(128)(h)  # Task: optimize number of neurons!\nh = Dense(2)(h)  # only 2 classes are present\nh = Softmax()(h)\n\nmodel = Model(inputs=[inp], outputs=[h])\n\n# try using different optimizers and different optimizer configs\nmodel.compile(\n    optimizer=Adam(), \n    loss=sparse_categorical_crossentropy,\n    metrics=['accuracy']\n)\n\nmodel.fit(X_train, y_train, batch_size=64, epochs=3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b14c7d4c177a4dab26bebbcf26d6b9d2af4b6385"},"cell_type":"markdown","source":"Lets see how well our model performs."},{"metadata":{"trusted":true,"_uuid":"ab574fb70ed54aaca65cacec1eefb5c3326d629e"},"cell_type":"code","source":"loss, acc = model.evaluate(X_test, y_test, batch_size=64)\nprint('Test score:', score)\nprint('Test accuracy:', acc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e62559dcaef7e949c7b8115a3ba962e0ad007a47"},"cell_type":"markdown","source":"Now, to actually use the model!"},{"metadata":{"trusted":true,"_uuid":"9d7ee1005f864e6e73263139523a6f13d4aaf362"},"cell_type":"code","source":"my_input = tok.transform(np.array([\n    'Best movie EVER!!!',\n    'Worst movie EVER!!!'\n]))\n\nmodel.predict(my_input)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}