{"cells":[{"metadata":{"trusted":true,"_uuid":"b7ade0ac8f0e5f9d50881ecc388f000c77455773"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ntrain = pd.read_csv(\"../input/labeledTrainData.tsv\",sep = '\\t')\ntest = pd.read_csv(\"../input/testData.tsv\", sep = '\\t')\n#unlabeld_test = pd.read_csv(\"unlabeledTrainData.tsv\", sep = '\\t')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb1560073e4e2a1b86b79d740f8429a0cd2cf27c"},"cell_type":"code","source":"print(train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"939989601fbcadcc4822b1c17c20970a38168376"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n#pd.DataFrame(train.loc[:,['id','review']])\n\nX = train.loc[:, 'review']\ny = train.loc[:,'sentiment']\n\nxtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"4cbad1c870b3cdeb4066a8d53020d12504e0b93f"},"cell_type":"code","source":" def GetVocabulary(data):\n    vocab_dict = {}\n    wid = 0\n    for document in data:\n        words = document.split()\n        for word in words:\n            word = word.lower()\n            if word not in vocab_dict:\n                vocab_dict[word] = wid\n                wid += 1\n    return vocab_dict\n\n#print(len(vocab_dict.keys()))\n#truncated_X = xtrain[0:18000]  # rason for this is when I run it on jupyter, it crashes - out of memeory - if it's the entire xtrain matrix\ntruncated_X = xtrain           # but for submitting the homework I set it to the entire xtrain, assuming 老师的 machine can handle it\n#print(len(truncated_X))\n\nvocab_dict = GetVocabulary(truncated_X)\nprint('Number of all the unique words: ' + str(len(vocab_dict.keys())))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"4396a9058946d35d7207f83af7bda6297ff9ca0f"},"cell_type":"code","source":"def Document2Vector(vocab_dict, data):\n    word_vector = np.zeros(len(vocab_dict.keys()))\n    words = data.split()\n    for word in words:\n        word = word.lower()\n        if word in vocab_dict:\n            word_vector[vocab_dict[word]] += 1\n    return word_vector\n\nexample = Document2Vector(vocab_dict, 'we are good good')\nprint(example)\nprint(example[vocab_dict['we']], example[vocab_dict['are']], example[vocab_dict['good']])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"d761c58bfa649471b5b252829fdcd1aba1803bb4"},"cell_type":"code","source":"train_matrix = []\nfor document in truncated_X.values:\n    word_vector = Document2Vector(vocab_dict, document)\n    train_matrix.append(word_vector)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"5662dddf2b7204c4713db3d497f6dc675914d5d9"},"cell_type":"code","source":"def NaiveBayes_train(train_matrix, labels_train):\n    num_docs = len(train_matrix)\n    num_words = len(train_matrix[0])\n    \n    spam_word_counter = np.ones(num_words)\n    ham_word_counter = np.ones(num_words)\n    \n    ham_total_count = 0\n    spam_total_count = 0\n    \n    spam_count = 0 \n    ham_count = 0\n    \n    for i in range(num_docs):\n        if i % 500 == 0:\n            print('Train on the doc id' + str(i))\n            \n        if labels_train[i] == 1:   # 1 is positive, or 'ham'\n            ham_word_counter += train_matrix[i]\n            ham_total_count += sum(train_matrix[i])\n            ham_count += 1\n        else:                      # 0 is negative, or 'spam'\n            spam_word_counter += train_matrix[i]\n            spam_total_count += sum(train_matrix[i])\n            spam_count += 1\n    \n    # spam_word_counter => 没个词的计数\n    # spam_total_count => Spam的总词数\n    # spam_count => spam邮件的计数\n    \n    p_spam_vector = np.log(spam_word_counter / (spam_total_count + num_words))\n    p_ham_vector = np.log(ham_word_counter / (ham_total_count + num_words))\n    \n    return p_spam_vector, np.log(spam_count/num_docs),spam_total_count, p_ham_vector, np.log(ham_count/num_docs), ham_total_count\n\n# p_spam_vetor/p_ham_vector 的每一维分别是一个单词再spam/ham分类下的概率\n# p_spam / p_ham 分别是两个分类的概率\n\np_spam_vector, p_spam, spam_total_count, p_ham_vector, p_ham, ham_total_count = NaiveBayes_train(train_matrix, ytrain.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f028e77a55267a09300a28e15a3c4d6cb5c576af"},"cell_type":"code","source":"def Test2Vector(vocab_dict, data):\n    word_vector = np.zeros(len(vocab_dict.keys()))\n    words = data.split()\n    \n    out_of_voc = 0\n    for word in words:\n        word = word.lower()\n        if word in vocab_dict:\n            word_vector[vocab_dict[word]] += 1\n        #else:\n            #out_of_voc +=\n    return word_vector\n\ndef Predict(test_word_vector, p_spam_vector, p_spam, p_ham_vector, p_ham):\n    spam = sum(test_word_vector * p_spam_vector) + p_spam\n    ham = sum(test_word_vector * p_ham_vector) + p_ham\n    if spam > ham:\n        return 0\n    else:\n        return 1\n\npredictions = []\ni = 0\nfor document in xtest.values:\n    if i % 200 == 0:\n        print('Test on the doc id: ', str(i))\n    i += 1\n    test_word_vector = Document2Vector(vocab_dict, document)\n    ans = Predict(test_word_vector, p_spam_vector, p_spam, p_ham_vector, p_ham)\n    predictions.append(ans)\n    \nprint(len(predictions))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"86addc2e9f99ce6ef3fefc6fa5a7dad302bd824e"},"cell_type":"code","source":"predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"fc0f9cbdb88a84a6277bacb89f44e3cde3594d91"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.model_selection import cross_val_score\n\nprint(accuracy_score(ytest, predictions))\nprint(classification_report(ytest, predictions))\nprint(confusion_matrix(ytest, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c56e577ba7499dfe2c0a52271af945adf51fd315"},"cell_type":"code","source":"test_X = test.loc[:, 'review']\ntest_X_id = test.loc[:,'id']\n\nresults = []\ni = 0\nfor document in test_X.values:\n    if i % 200 == 0:\n        print('Test on the doc id: ', str(i))\n    i += 1\n    result_word_vector = Document2Vector(vocab_dict, document)\n    ans = Predict(result_word_vector, p_spam_vector, p_spam, p_ham_vector, p_ham)\n    results.append(ans)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a00c23b73c81439c7168dd88c96ddac0dfc5d741"},"cell_type":"code","source":"df = pd.DataFrame({\"id\": test.loc[:,'id'], \"sentiment\":results})\ndf.to_csv('submission.csv', header = True, index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}