{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\n# warnings.filterwarnings('ignore')\n%config Completer.use_jedi = False\n\nimport pandas as pd\nimport numpy as np\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **加载数据**","metadata":{}},{"cell_type":"code","source":"#解压缩\n!unzip /kaggle/input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip\n!unzip /kaggle/input/word2vec-nlp-tutorial/unlabeledTrainData.tsv.zip\n!unzip /kaggle/input/word2vec-nlp-tutorial/testData.tsv.zip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train=pd.read_csv('/kaggle/working/labeledTrainData.tsv',delimiter='\\t',quoting=3)\ntest=pd.read_csv('/kaggle/working/testData.tsv',delimiter='\\t',quoting=3)\nunlabeled_train=pd.read_csv(\"/kaggle/working/unlabeledTrainData.tsv\",\n                            header=0,    #设置导入dataframe的列名称，默认为infer\n                            delimiter='\\t',\n                            quoting=3)   #可以如实打印csv中内容，\n                                         #不设置该参数引号内内容会读取错误","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape)\nprint(test.shape)\nprint(unlabeled_train.shape)\nprint(train.info)\n\nprint(train['review'].size)\nprint(test['review'].size)\nprint(unlabeled_train['review'].size) \n# pd.isnull(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#bs4库 是解析、遍历、维护、“标签树“的功能库(bs4库把html源代码重新进行了格式化,\n#                                      从而方便我们对其中的节点、标签、属性等进行操作)\n!pip3 install Beautifulsoup4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile Word2VecUtil.py\nimport re\nimport nltk\n#nltk是一个nlp的工具包，内含数据集、python模块等。\n#可搜索文本，计数词汇\n\nimport pandas as pd\nimport numpy as np\n\nfrom bs4 import BeautifulSoup      #最主要功能就是从网页抓取数据\nfrom nltk.corpus import stopwords  #导入英语停用词，如at of you a that等\nfrom nltk.stem.snowball import SnowballStemmer #一个小的语言转换库 支持15种语言\n\nfrom multiprocessing import Pool\n\nclass KaggleWord2VecUtility(object):\n\n    @staticmethod\n    #删除停用词，返回list\n    def review_to_wordlist(review, remove_stopwords=False): \n        # 1. HTML 删除\n        review_text = BeautifulSoup(review, \"html.parser\").get_text() #创建BS对象并获取其文本\n        # 2. 将特殊字符转为空格 sub(pattern,repl,string)\n        # pattern：正则中的模式字符串 string：被处理的字符串\n        review_text = re.sub('[^a-zA-Z]', ' ', review_text)\n        # 3. 转化为小写+分割\n        words = review_text.lower().split()\n        # 4. 删除停用词\n        if remove_stopwords:\n            #创建一个无序不重复元素集（即停用词集合）\n            stops = set(stopwords.words('english')) \n            words = [w for w in words if not w in stops]\n        # 5. 提取词干\n        stemmer = SnowballStemmer('english')  #选择语言为english\n        words = [stemmer.stem(w) for w in words] #提取每个词语的主干\n        # 6. 返回一个list\n        return(words)\n\n    @staticmethod\n    #用join将word用“ ”连接起来\n    def review_to_join_words( review, remove_stopwords=False ):\n        words = KaggleWord2VecUtility.review_to_wordlist(\\\n            review, remove_stopwords=False)\n        join_words = ' '.join(words)\n        return join_words\n\n    @staticmethod\n    def review_to_sentences( review, remove_stopwords=False ):\n        #下行是一个将英文文本划分成句子的模型，划分依据是每个句子结束后留有空格\n        tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n        # 1. nltk tokenize分割句子为单词，删除空格strip()\n        raw_sentences = tokenizer.tokenize(review.strip())\n        # 2. 遍历每个句子\n        sentences = []\n        for raw_sentence in raw_sentences:\n            # 为空则skip\n            if len(raw_sentence) > 0:\n                # 删除标签，用空格替换非字母字符，删除停用词\n                sentences.append(\\\n                    KaggleWord2VecUtility.review_to_wordlist(\\\n                    raw_sentence, remove_stopwords))\n        return sentences\n\n\n    # 参考 : https://gist.github.com/yong27/7869662\n    # http://www.racketracer.com/2016/07/06/pandas-in-parallel/\n    # 与多个线程一起使用以提高速度\n    @staticmethod\n    def _apply_df(args):\n        df, func, kwargs = args\n        return df.apply(func, **kwargs)\n\n    @staticmethod\n    def apply_by_multiprocessing(df, func, **kwargs):\n        # 获取关键字项中的worker参数\n        workers = kwargs.pop('workers')\n        # 定义Pool进程池\n        pool = Pool(processes=workers)\n        #map()根据提供的函数对指定序列做映射 \n        result = pool.map(KaggleWord2VecUtility._apply_df, [(d, func, kwargs)\n                for d in np.array_split(df, workers)])\n        pool.close()\n        # 작업 결과를 합쳐서 반환\n        return pd.concat(result)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 修改word2vecutil文件\n# %load Word2VecUtil.py","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from Word2VecUtil import KaggleWord2VecUtility","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train['review'][0][:50])\nKaggleWord2VecUtility.review_to_wordlist(train['review'][0][:50])\n#提取词干了 如going-go","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentences = []\nfor review in train['review']:\n    sentences += KaggleWord2VecUtility.review_to_sentences(review,\n                                                           remove_stopwords=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for review in unlabeled_train['review']:\n    sentences += KaggleWord2VecUtility.review_to_sentences(\n        review,remove_stopwords=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# len(sentences)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(sentences[0][:10])\nprint(sentences[1][:10])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Word2Vec算法** \n# 通过预处理解析出的句子列表来训练模型","metadata":{}},{"cell_type":"markdown","source":"体系结构：“体系结构”选项是skip-gram（默认）或CBOW模型。 skip-gram速度较慢，但效果更好\n学习算法：分层softmax（默认）或负采样。默认值在这里工作正常。\n对经常出现的单词进行下采样：Google文档建议在0.00001和0.001之间的值。此处，显示接近0.001的值可提高最终模型的准确性。\n字向量维：使用许多功能并不总是很好，但通常是更好的模型。合理的值可以从数十到数百，此处指定为300。\n上下文/窗口大小：上下文中学习算法应考虑多少个单词？较大的数字对分层softmax有益，但10则很好\n辅助线程：要运行的并行进程数，这在计算机之间是不同的，但在大多数系统上使用的值为4到6之间。\n最小单词数：有助于将词汇表的大小限制为有意义的单词数。在所有文档中没有多次出现的单词将被忽略。适当的范围是10到100，\n因为每个电影都有30条评论，因此最小字数设置为40，以避免过于重视单个电影标题。结果，总词汇量约为15,000个单词。较高的值有助于有限的执行时间。","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import logging\nlogging.basicConfig(  # 为日志配置基本信息\n    format='%(asctime)s : %(levelname)s : %(message)s', #日志输出格式\n    level=logging.INFO) # 设置日志级别 低于该级别的日志消息将被忽略\n                        # CRITICAL>ERROR>WARNNING>INFO>DEBUG>NOTSET\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_features = 300 # 字符数矢量维\nmin_word_count = 40 # 最小字符数\nnum_workers = 4 # 并行处理线程数\ncontext = 10 # 字符串窗口大小\ndownsampling = 1e-3\n\n#初始化和模型训练\nfrom gensim.models import word2vec\n#gensim是个nlp工具包，内含多个常见模型 LSI LDA HDP DTM DIM TF-IDF word2vec paragraph2vec\n\nmodel = word2vec.Word2Vec(sentences,\n                         workers=num_workers,\n#                          size=num_features,\n                         min_count=min_word_count,\n                         window=context,\n                         sample=downsampling) #高频词随机下采样的配置阈值，范围（0，1e-5）\nmodel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  训练完后删除不必要的内存\nmodel.init_sims(replace=True)\n\nmodel_name = '300features_40minwords_10text'\n\nmodel.save(model_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **探索模型结果**","metadata":{}},{"cell_type":"code","source":"#提取没有相似性的单词\nmodel.wv.doesnt_match('man woman child kitchen'.split())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.wv.doesnt_match('france england germany berlin'.split())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#提取相似的词\nmodel.wv.most_similar('man')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.wv.most_similar('queen')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.wv.most_similar('awful')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.wv.most_similar('film')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.wv.most_similar('happy')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.wv.most_similar('happi')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"通过t-SNE可视化Word2Vec矢量化的单词","metadata":{}},{"cell_type":"code","source":"from sklearn.manifold import TSNE  #数据降维和可视化\n#TSNE使高于二维的聚类结果以二维方式展现出来\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport gensim\n#gensim从原始的非结构化文本中，无监督学习到文本隐层的主题向量表达\n#支持TF-IDF，LSA，LDA，word2vec\nimport gensim.models as g","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 用来显示中文标签\n# plt.rcParams['font.sans-serif']=['SimHei']\n#用于显示负号\nmpl.rcParams['axes.unicode_minus'] = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = '300features_40minwords_10text'\nmodel = g.Doc2Vec.load(model_name)\n\n#model.wv.vocab 直接调用生成的词向量\nvocab = list(model.wv.key_to_index)  \n\nX = model[vocab]\n\n\n# rock_idx = model.wv.key_to_index[\"rock\"]\n# vocab = (model.wv.get_vecattr(,))\n\n# \n\nprint(len(X))\n# print(X[0][:10])\ntsne = TSNE(n_components=2)\nX_tsne = tsne.fit_transform(X[:100, :])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(X_tsne, index=vocab[:100], columns=['x', 'y'])\ndf.shape \ndf.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure()\nfig.set_size_inches(40,20)\nax = fig.add_subplot(111)\nax.scatter(df['x'],df['y'])\nfor word,pos in df.iterrows():\n    ax.annotate(word,pos,fontsize=30)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef makeFeatureVec(words, model, num_features):\n    '''\n    在IMDB中每段评论长度不一，因此需要先将每个独立的单词向量转换成等长的特征集合\n    对所有单词向量求平均\n    '''\n    # 初始化一个数组（为了提高速度）\n    featureVec = np.zeros((num_features,), dtype='float32')\n    \n    nwords = 0.\n    # Index2word是模型词汇表中单词名称的列表（为了提速）\n    index2word_set = set(model.wv.index2word)\n    \n    # 循环遍历评论中的每个单词，如果它在模型的词汇表中，\n    # 则将其及其特征向量循环到总和\n    for word in words:\n        if word in index2word_set:\n            nwords = nwords + 1.\n            featureVec = np.add(featureVec, model[word])\n    \n    # 做平均运算\n    featureVec = np.divide(featureVec, nwords)\n    return featureVec\n\ndef getAvgFeatureVecs(reviews, model, num_features):\n    # 给定一组评论（每个评论一个单词列表），计算每个特征向量的平均特征向量，\n    #并返回一个2D numpy数组\n     \n    counter = 0\n\n    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n       \n    for review in reviews:\n       if counter%1000 == 0:\n           print \"Review %d of %d\" % (counter, len(reviews))\n       \n       reviewFeatureVecs[counter] = makeFeatureVec(review, model, \\\n           num_features)\n\n       counter = counter + 1\n    return reviewFeatureVecs\n\ndef getCleanReviews(reviews):\n    clean_reviews = []\n    clean_reviews = KaggleWord2VecUtility.apply_by_multiprocessing(\\\n            reviews['review'], KaggleWord2VecUtility.review_to_wordlist,\\\n            workers=4)\n    return clean_reviews","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%time\ntrainDataVecs = getAvgFeatureVecs(\\\n    getCleanReviews(train), model, num_features)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%time\ntestDataVecs = getAvgFeatureVecs(\\\n    getCleanReviews(test), model, num_features)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **RandomForest**","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nforest = RandomForestClassifier(\n    n_estimators=100, n_jobs=-1, random_state=2018)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%time\nforest = forest.fit(trainDataVecs, train['sentiment'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n%time\nscore = np.mean(cross_val_score(\\\n        forest, trainDataVecs, \\\n        train['sentiment'], cv=10, scoring='roc_auc'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = forest.predict(testDataVecs)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = pd.DataFrame(data={'id':test['id'], 'sentiment':result})\noutput.to_csv('Word2Vec_AverageVEctors_{0:.5f}.csv'.format(score),\n             index=False, quoting=3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_sentiment = output['sentiment'].value_counts()\nprint(output_sentiment[0] - output_sentiment[1])\noutput_sentiment","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\n%matplotlib inline\n\nfig, axes = plt.subplots(ncols=2)\nfig.set_size_inches(12,5)\nsns.countplot(train['sentiment'], ax=axes[0])\nsns.countplot(output['sentiment'], ax=axes[1])","metadata":{},"execution_count":null,"outputs":[]}]}