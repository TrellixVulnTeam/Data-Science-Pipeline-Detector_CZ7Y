{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import the pandas package, then use the \"read_csv\" function to read\n# the labeled training data\nimport pandas as pd\ntrain = pd.read_csv(\"../input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip\", header=0, \\\n                    delimiter=\"\\t\", quoting=3)\n# Import BeautifulSoup into your workspace\nfrom bs4 import BeautifulSoup\n\n# Initialize the BeautifulSoup object on a single movie review\nexample1 = BeautifulSoup(train[\"review\"][0],features=\"lxml\")\n\n# Print the raw review and then the output of get_text(), for\n# comparison\n\nprint(example1.get_text())\nimport re\n# Use regular expressions to do a find-and-replace\nletters_only = re.sub(\"[^a-zA-Z]\",           # The pattern to search for\n                      \" \",                   # The pattern to replace it with\n                      example1.get_text() )  # The text to search\nprint(letters_only)\nlower_case = letters_only.lower()        # Convert to lower case\nwords = lower_case.split()               # Split into words\n#### print(words)\n\nimport nltk\nfrom nltk.corpus import stopwords # Import the stop word list\nprint(stopwords.words(\"english\"))\n\n# Remove stop words from \"words\"\nwords = [w for w in words if not w in stopwords.words(\"english\")]\n#### print(words)","metadata":{"execution":{"iopub.status.busy":"2021-12-25T01:31:30.825448Z","iopub.execute_input":"2021-12-25T01:31:30.825759Z","iopub.status.idle":"2021-12-25T01:31:34.25346Z","shell.execute_reply.started":"2021-12-25T01:31:30.825731Z","shell.execute_reply":"2021-12-25T01:31:34.252141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def review_to_words( raw_review ):\n    # Function to convert a raw review to a string of words\n    # The input is a single string (a raw movie review), and\n    # the output is a single string (a preprocessed movie review)\n    #\n    # 1. Remove HTML\n    review_text = BeautifulSoup(raw_review,features=\"lxml\").get_text()\n    #\n    # 2. Remove non-letters\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n    #\n    # 3. Convert to lower case, split into individual words\n    words = letters_only.lower().split()\n    #\n    # 4. In Python, searching a set is much faster than searching\n    #   a list, so convert the stop words to a set\n    stops = set(stopwords.words(\"english\"))\n    #\n    # 5. Remove stop words\n    meaningful_words = [w for w in words if not w in stops]\n    #\n    # 6. Join the words back into one string separated by space,\n    # and return the result.\n    return( \" \".join( meaningful_words ))\n\nclean_review = review_to_words( train[\"review\"][0] )\nprint(clean_review)\n\n# Get the number of reviews based on the dataframe column size\nnum_reviews = train[\"review\"].size\n\n# Initialize an empty list to hold the clean reviews\nclean_train_reviews = []\n\n# Loop over each review; create an index i that goes from 0 to the length\n# of the movie review list\nfor i in range( 0, num_reviews ):\n    # Call our function for each one, and add the result to the list of\n    # clean reviews\n    if ((i + 1) % 1000 == 0):\n        print(\"Review %d of %d\\n\" % (i + 1, num_reviews))\n    clean_train_reviews.append( review_to_words( train[\"review\"][i] ) )\n\nprint(\"Creating the bag of words...\\n\")\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n# bag of words tool.\nvectorizer = CountVectorizer(analyzer = \"word\",   \\\n                             tokenizer = None,    \\\n                             preprocessor = None, \\\n                             stop_words = None,   \\\n                             max_features = 5000)\n\n# fit_transform() does two functions: First, it fits the model\n# and learns the vocabulary; second, it transforms our training data\n# into feature vectors. The input to fit_transform should be a list of\n# strings.\ntrain_data_features = vectorizer.fit_transform(clean_train_reviews)\n\n# Numpy arrays are easy to work with, so convert the result to an\n# array\ntrain_data_features = train_data_features.toarray()","metadata":{"execution":{"iopub.status.busy":"2021-12-25T01:31:34.256161Z","iopub.execute_input":"2021-12-25T01:31:34.256767Z","iopub.status.idle":"2021-12-25T01:32:01.714444Z","shell.execute_reply.started":"2021-12-25T01:31:34.256714Z","shell.execute_reply":"2021-12-25T01:32:01.713484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_data_features.shape)\nvocab = vectorizer.get_feature_names()\n### print(vocab)\n\nprint(\"Training the random forest...\")\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Initialize a Random Forest classifier with 100 trees\nforest = RandomForestClassifier(n_estimators = 100)\n\n# Fit the forest to the training set, using the bag of words as\n# features and the sentiment labels as the response variable\n#\n# This may take a few minutes to run\nforest = forest.fit( train_data_features, train[\"sentiment\"] )\n","metadata":{"execution":{"iopub.status.busy":"2021-12-25T01:32:01.716062Z","iopub.execute_input":"2021-12-25T01:32:01.716389Z","iopub.status.idle":"2021-12-25T01:32:53.253375Z","shell.execute_reply.started":"2021-12-25T01:32:01.716349Z","shell.execute_reply":"2021-12-25T01:32:53.252411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read the test data\ntest = pd.read_csv(\"../input/word2vec-nlp-tutorial/testData.tsv.zip\", header=0, delimiter=\"\\t\", \\\n                   quoting=3 )\n\n# Verify that there are 25,000 rows and 2 columns\nprint(test.shape)\n\n# Create an empty list and append the clean reviews one by one\nnum_reviews = len(test[\"review\"])\nclean_test_reviews = []\n\nprint(\"Cleaning and parsing the test set movie reviews...\\n\")\nfor i in range(0,num_reviews):\n    if( (i+1) % 1000 == 0 ):\n        print(\"Review %d of %d\\n\" % (i+1, num_reviews))\n    clean_review = review_to_words( test[\"review\"][i] )\n    clean_test_reviews.append( clean_review )\n\n# Get a bag of words for the test set, and convert to a numpy array\ntest_data_features = vectorizer.transform(clean_test_reviews)\ntest_data_features = test_data_features.toarray()\n\n# Use the random forest to make sentiment label predictions\nresult = forest.predict(test_data_features)\n\n# Copy the results to a pandas dataframe with an \"id\" column and\n# a \"sentiment\" column\noutput = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n\n# Use pandas to write the comma-separated output file\noutput.to_csv( \"Bag_of_Words_model.csv\", index=False, quoting=3 )","metadata":{"execution":{"iopub.status.busy":"2021-12-25T01:32:53.256207Z","iopub.execute_input":"2021-12-25T01:32:53.256588Z","iopub.status.idle":"2021-12-25T01:33:24.716853Z","shell.execute_reply.started":"2021-12-25T01:32:53.256532Z","shell.execute_reply":"2021-12-25T01:33:24.715816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tutorial Part 2","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Read data from files \ntrain = pd.read_csv( \"../input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip\", header=0, \n delimiter=\"\\t\", quoting=3 )\ntest = pd.read_csv( \"../input/word2vec-nlp-tutorial/testData.tsv.zip\", header=0, delimiter=\"\\t\", quoting=3 )\nunlabeled_train = pd.read_csv( \"../input/word2vec-nlp-tutorial/unlabeledTrainData.tsv.zip\", header=0, \n delimiter=\"\\t\", quoting=3 )\n\n# Verify the number of reviews that were read (100,000 in total)\nprint(\"Read %d labeled train reviews, %d labeled test reviews, \" \\\n \"and %d unlabeled reviews\\n\" % (train[\"review\"].size,  \n test[\"review\"].size, unlabeled_train[\"review\"].size ))","metadata":{"execution":{"iopub.status.busy":"2021-12-25T01:33:24.718517Z","iopub.execute_input":"2021-12-25T01:33:24.718938Z","iopub.status.idle":"2021-12-25T01:33:27.505597Z","shell.execute_reply.started":"2021-12-25T01:33:24.718898Z","shell.execute_reply":"2021-12-25T01:33:27.504599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import various modules for string cleaning\nfrom bs4 import BeautifulSoup\nimport re\nfrom nltk.corpus import stopwords\n\ndef review_to_wordlist( review, remove_stopwords=False ):\n    # Function to convert a document to a sequence of words,\n    # optionally removing stop words.  Returns a list of words.\n    #\n    # 1. Remove HTML\n    review_text = BeautifulSoup(review).get_text()\n    #  \n    # 2. Remove non-letters\n    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n    #\n    # 3. Convert words to lower case and split them\n    words = review_text.lower().split()\n    #\n    # 4. Optionally remove stop words (false by default)\n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))\n        words = [w for w in words if not w in stops]\n    #\n    # 5. Return a list of words\n    return(words)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-25T01:33:27.507207Z","iopub.execute_input":"2021-12-25T01:33:27.507743Z","iopub.status.idle":"2021-12-25T01:33:27.519156Z","shell.execute_reply.started":"2021-12-25T01:33:27.50764Z","shell.execute_reply":"2021-12-25T01:33:27.517939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Download the punkt tokenizer for sentence splitting\nimport nltk.data\nnltk.download()   \n\n# Load the punkt tokenizer\ntokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n\n# Define a function to split a review into parsed sentences\ndef review_to_sentences( review, tokenizer, remove_stopwords=False ):\n    # Function to split a review into parsed sentences. Returns a \n    # list of sentences, where each sentence is a list of words\n    #\n    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n    raw_sentences = tokenizer.tokenize(review.strip())\n    #\n    # 2. Loop over each sentence\n    sentences = []\n    for raw_sentence in raw_sentences:\n        # If a sentence is empty, skip it\n        if len(raw_sentence) > 0:\n            # Otherwise, call review_to_wordlist to get a list of words\n            sentences.append( review_to_wordlist( raw_sentence, \\\n              remove_stopwords ))\n    #\n    # Return the list of sentences (each sentence is a list of words,\n    # so this returns a list of lists\n    return sentences","metadata":{"execution":{"iopub.status.busy":"2021-12-25T01:33:27.52118Z","iopub.execute_input":"2021-12-25T01:33:27.521549Z","iopub.status.idle":"2021-12-25T01:34:02.795944Z","shell.execute_reply.started":"2021-12-25T01:33:27.521506Z","shell.execute_reply":"2021-12-25T01:34:02.794863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentences = []  # Initialize an empty list of sentences\n\nprint(\"Parsing sentences from training set\")\nfor review in train[\"review\"]:\n    sentences += review_to_sentences(review, tokenizer)\n\nprint(\"Parsing sentences from unlabeled set\")\nfor review in unlabeled_train[\"review\"]:\n    sentences += review_to_sentences(review, tokenizer)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-25T01:34:02.797347Z","iopub.execute_input":"2021-12-25T01:34:02.797807Z","iopub.status.idle":"2021-12-25T01:39:34.970185Z","shell.execute_reply.started":"2021-12-25T01:34:02.797759Z","shell.execute_reply":"2021-12-25T01:39:34.969234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(sentences))\nprint(sentences[0])\nprint(sentences[1])","metadata":{"execution":{"iopub.status.busy":"2021-12-25T01:39:34.971647Z","iopub.execute_input":"2021-12-25T01:39:34.973467Z","iopub.status.idle":"2021-12-25T01:39:34.981298Z","shell.execute_reply.started":"2021-12-25T01:39:34.973423Z","shell.execute_reply":"2021-12-25T01:39:34.980249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import the built-in logging module and configure it so that Word2Vec \n# creates nice output messages\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n    level=logging.INFO)\n\n# Set values for various parameters\nnum_features = 300    # Word vector dimensionality                      \nmin_word_count = 40   # Minimum word count                        \nnum_workers = 4       # Number of threads to run in parallel\ncontext = 10          # Context window size                                                                                    \ndownsampling = 1e-3   # Downsample setting for frequent words\n\n# Initialize and train the model (this will take some time)\nfrom gensim.models import word2vec\nprint(\"Training model...\")\nmodel = word2vec.Word2Vec(sentences, workers=num_workers, \\\n            vector_size=num_features, min_count = min_word_count, \\\n            window = context, sample = downsampling)\n\n# If you don't plan to train the model any further, calling \n# init_sims will make the model much more memory-efficient.\nmodel.init_sims(replace=True)\n\n# It can be helpful to create a meaningful model name and \n# save the model for later use. You can load it later using Word2Vec.load()\nmodel_name = \"300features_40minwords_10context\"\nmodel.save(model_name)","metadata":{"execution":{"iopub.status.busy":"2021-12-25T01:39:34.985224Z","iopub.execute_input":"2021-12-25T01:39:34.985961Z","iopub.status.idle":"2021-12-25T01:43:27.130956Z","shell.execute_reply.started":"2021-12-25T01:39:34.985916Z","shell.execute_reply":"2021-12-25T01:43:27.130036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.wv.doesnt_match(\"man woman child kitchen\".split())","metadata":{"execution":{"iopub.status.busy":"2021-12-25T01:43:27.135477Z","iopub.execute_input":"2021-12-25T01:43:27.137783Z","iopub.status.idle":"2021-12-25T01:43:27.154574Z","shell.execute_reply.started":"2021-12-25T01:43:27.137738Z","shell.execute_reply":"2021-12-25T01:43:27.153742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.wv.doesnt_match(\"france england germany berlin\".split())","metadata":{"execution":{"iopub.status.busy":"2021-12-25T01:43:27.15928Z","iopub.execute_input":"2021-12-25T01:43:27.161846Z","iopub.status.idle":"2021-12-25T01:43:27.174397Z","shell.execute_reply.started":"2021-12-25T01:43:27.161803Z","shell.execute_reply":"2021-12-25T01:43:27.173156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.wv.doesnt_match(\"paris berlin london austria\".split())","metadata":{"execution":{"iopub.status.busy":"2021-12-25T01:43:27.179449Z","iopub.execute_input":"2021-12-25T01:43:27.181969Z","iopub.status.idle":"2021-12-25T01:43:27.193703Z","shell.execute_reply.started":"2021-12-25T01:43:27.181929Z","shell.execute_reply":"2021-12-25T01:43:27.192757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.wv.most_similar(\"man\")","metadata":{"execution":{"iopub.status.busy":"2021-12-25T01:43:27.196807Z","iopub.execute_input":"2021-12-25T01:43:27.199783Z","iopub.status.idle":"2021-12-25T01:43:27.22522Z","shell.execute_reply.started":"2021-12-25T01:43:27.199743Z","shell.execute_reply":"2021-12-25T01:43:27.224318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.wv.most_similar(\"awful\")","metadata":{"execution":{"iopub.status.busy":"2021-12-25T01:43:27.228843Z","iopub.execute_input":"2021-12-25T01:43:27.23188Z","iopub.status.idle":"2021-12-25T01:43:27.254506Z","shell.execute_reply.started":"2021-12-25T01:43:27.231835Z","shell.execute_reply":"2021-12-25T01:43:27.253551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tutorial Three","metadata":{}},{"cell_type":"code","source":"# Load a pre-trained model\nfrom gensim.models import Word2Vec\nfrom sklearn.cluster import KMeans\nimport time\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom bs4 import BeautifulSoup\nimport re\nfrom nltk.corpus import stopwords\nimport numpy as np\nimport os\n\n\n\n# Define a function to create bags of centroids\n#\ndef create_bag_of_centroids( wordlist, word_centroid_map ):\n    #\n    # The number of clusters is equal to the highest cluster index\n    # in the word / centroid map\n    num_centroids = max( word_centroid_map.values() ) + 1\n    #\n    # Pre-allocate the bag of centroids vector (for speed)\n    bag_of_centroids = np.zeros( num_centroids, dtype=\"float32\" )\n    #\n    # Loop over the words in the review. If the word is in the vocabulary,\n    # find which cluster it belongs to, and increment that cluster count\n    # by one\n    for word in wordlist:\n        if word in word_centroid_map:\n            index = word_centroid_map[word]\n            bag_of_centroids[index] += 1\n    #\n    # Return the \"bag of centroids\"\n    return bag_of_centroids\n\n\nif __name__ == '__main__':\n\n    model = Word2Vec.load(\"300features_40minwords_10context\")\n\n\n    # ****** Run k-means on the word vectors and print a few clusters\n    #\n\n    start = time.time() # Start time\n\n    # Set \"k\" (num_clusters) to be 1/5th of the vocabulary size, or an\n    # average of 5 words per cluster\n    word_vectors = model.wv.syn0\n    num_clusters = word_vectors.shape[0] / 5\n\n    # Initalize a k-means object and use it to extract centroids\n    print(\"Running K means\")\n    kmeans_clustering = KMeans( n_clusters = num_clusters )\n    idx = kmeans_clustering.fit_predict( word_vectors )\n\n    # Get the end time and print how long the process took\n    end = time.time()\n    elapsed = end - start\n    print(\"Time taken for K Means clustering: \", elapsed, \"seconds.\")\n\n\n    # Create a Word / Index dictionary, mapping each vocabulary word to\n    # a cluster number\n    word_centroid_map = dict(zip( model.wv.index2word, idx ))\n\n    # Print the first ten clusters\n    for cluster in xrange(0,10):\n        #\n        # Print the cluster number\n        print(\"\\nCluster %d\" % cluster)\n        #\n        # Find all of the words for that cluster number, and print them out\n        words = []\n        for i in xrange(0,len(word_centroid_map.values())):\n            if( word_centroid_map.values()[i] == cluster ):\n                words.append(word_centroid_map.keys()[i])\n        print(words)\n\n\n\n\n    # Create clean_train_reviews and clean_test_reviews as we did before\n    #\n\n    # Read data from files\n    train = pd.read_csv( os.path.join(os.path.dirname(__file__), 'data', 'labeledTrainData.tsv'), header=0, delimiter=\"\\t\", quoting=3 )\n    test = pd.read_csv(os.path.join(os.path.dirname(__file__), 'data', 'testData.tsv'), header=0, delimiter=\"\\t\", quoting=3 )\n\n\n    print(\"Cleaning training reviews\")\n    clean_train_reviews = []\n    for review in train[\"review\"]:\n        clean_train_reviews.append( KaggleWord2VecUtility.review_to_wordlist( review, \\\n            remove_stopwords=True ))\n\n    print(\"Cleaning test reviews\")\n    clean_test_reviews = []\n    for review in test[\"review\"]:\n        clean_test_reviews.append( KaggleWord2VecUtility.review_to_wordlist( review, \\\n            remove_stopwords=True ))\n\n\n    # ****** Create bags of centroids\n    #\n    # Pre-allocate an array for the training set bags of centroids (for speed)\n    train_centroids = np.zeros( (train[\"review\"].size, num_clusters), \\\n        dtype=\"float32\" )\n\n    # Transform the training set reviews into bags of centroids\n    counter = 0\n    for review in clean_train_reviews:\n        train_centroids[counter] = create_bag_of_centroids( review, \\\n            word_centroid_map )\n        counter += 1\n\n    # Repeat for test reviews\n    test_centroids = np.zeros(( test[\"review\"].size, num_clusters), \\\n        dtype=\"float32\" )\n\n    counter = 0\n    for review in clean_test_reviews:\n        test_centroids[counter] = create_bag_of_centroids( review, \\\n            word_centroid_map )\n        counter += 1\n\n\n    # ****** Fit a random forest and extract predictions\n    #\n    forest = RandomForestClassifier(n_estimators = 100)\n\n    # Fitting the forest may take a few minutes\n    print(\"Fitting a random forest to labeled training data...\")\n    forest = forest.fit(train_centroids,train[\"sentiment\"])\n    result = forest.predict(test_centroids)\n\n    # Write the test results\n    output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})\n    output.to_csv(\"BagOfCentroids.csv\", index=False, quoting=3)\n    print(\"Wrote BagOfCentroids.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-12-25T01:53:31.613081Z","iopub.execute_input":"2021-12-25T01:53:31.613965Z","iopub.status.idle":"2021-12-25T01:53:32.233839Z","shell.execute_reply.started":"2021-12-25T01:53:31.613917Z","shell.execute_reply":"2021-12-25T01:53:32.231753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gensim.models import Word2Vec\nmodel = Word2Vec.load(\"300features_40minwords_10context\")\nmodel.wv[\"flower\"]","metadata":{"execution":{"iopub.status.busy":"2021-12-25T02:01:59.051831Z","iopub.execute_input":"2021-12-25T02:01:59.052139Z","iopub.status.idle":"2021-12-25T02:01:59.366554Z","shell.execute_reply.started":"2021-12-25T02:01:59.052109Z","shell.execute_reply":"2021-12-25T02:01:59.365565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np  # Make sure that numpy is imported\n\ndef makeFeatureVec(words, model, num_features):\n    # Function to average all of the word vectors in a given\n    # paragraph\n    #\n    # Pre-initialize an empty numpy array (for speed)\n    featureVec = np.zeros((num_features,),dtype=\"float32\")\n    #\n    nwords = 0.\n    # \n    # Index2word is a list that contains the names of the words in \n    # the model's vocabulary. Convert it to a set, for speed \n    index2word_set = set(model.wv.index_to_key)\n    #\n    # Loop over each word in the review and, if it is in the model's\n    # vocaublary, add its feature vector to the total\n    for word in words:\n        if word in index2word_set: \n            nwords = nwords + 1.\n            featureVec = np.add(featureVec,model.wv[word])\n    # \n    # Divide the result by the number of words to get the average\n    featureVec = np.divide(featureVec,nwords)\n    return featureVec\n\n\ndef getAvgFeatureVecs(reviews, model, num_features):\n    # Given a set of reviews (each one a list of words), calculate \n    # the average feature vector for each one and return a 2D numpy array \n    # \n    # Initialize a counter\n    counter = 0.\n    # \n    # Preallocate a 2D numpy array, for speed\n    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n    # \n    # Loop through the reviews\n    for review in reviews:\n       #\n       # Print a status message every 1000th review\n       if counter%1000. == 0.:\n           print(\"Review %d ofreviewFeatureVecs %d\" % (counter, len(reviews)))\n       # \n       # Call the function (defined above) that makes average feature vectors\n       reviewFeatureVecs[counter] = makeFeatureVec(review, model, \\\n           num_features)\n       #\n       # Increment the counter\n       counter = counter + 1.\n    return reviewFeatureVecs","metadata":{"execution":{"iopub.status.busy":"2021-12-25T02:19:29.661329Z","iopub.execute_input":"2021-12-25T02:19:29.661654Z","iopub.status.idle":"2021-12-25T02:19:29.672883Z","shell.execute_reply.started":"2021-12-25T02:19:29.661624Z","shell.execute_reply":"2021-12-25T02:19:29.671623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(words)","metadata":{"execution":{"iopub.status.busy":"2021-12-25T02:21:55.636524Z","iopub.execute_input":"2021-12-25T02:21:55.63708Z","iopub.status.idle":"2021-12-25T02:21:55.643781Z","shell.execute_reply.started":"2021-12-25T02:21:55.637045Z","shell.execute_reply":"2021-12-25T02:21:55.642805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ****************************************************************\n# Calculate average feature vectors for training and testing sets,\n# using the functions we defined above. Notice that we now use stop word\n# removal.\n\nclean_train_reviews = []\nfor review in train[\"review\"]:\n    clean_train_reviews.append( review_to_wordlist( review, \\\n        remove_stopwords=True ))\n\ntrainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )\n\nprint(\"Creating average feature vecs for test reviews\")\nclean_test_reviews = []\nfor review in test[\"review\"]:\n    clean_test_reviews.append( review_to_wordlist( review, \\\n        remove_stopwords=True ))\n\ntestDataVecs = getAvgFeatureVecs( clean_test_reviews, model, num_features )","metadata":{"execution":{"iopub.status.busy":"2021-12-25T02:19:33.134574Z","iopub.execute_input":"2021-12-25T02:19:33.135709Z","iopub.status.idle":"2021-12-25T02:19:56.40252Z","shell.execute_reply.started":"2021-12-25T02:19:33.135616Z","shell.execute_reply":"2021-12-25T02:19:56.400968Z"},"trusted":true},"execution_count":null,"outputs":[]}]}