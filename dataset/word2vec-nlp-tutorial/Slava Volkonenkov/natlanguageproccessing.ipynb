{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Загрузка и просмотр данных","metadata":{}},{"cell_type":"markdown","source":"Подгрузим необходимые данные","metadata":{}},{"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n   \ntrain = pd.read_csv(\"../input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip\", header=0, delimiter=\"\\t\", quoting=3)\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-17T23:06:31.710957Z","iopub.execute_input":"2021-12-17T23:06:31.711668Z","iopub.status.idle":"2021-12-17T23:06:32.344483Z","shell.execute_reply.started":"2021-12-17T23:06:31.71162Z","shell.execute_reply":"2021-12-17T23:06:32.343528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Посмотрим колонки","metadata":{}},{"cell_type":"code","source":"train.columns.values","metadata":{"execution":{"iopub.status.busy":"2021-12-17T23:06:32.345831Z","iopub.execute_input":"2021-12-17T23:06:32.346628Z","iopub.status.idle":"2021-12-17T23:06:32.353583Z","shell.execute_reply.started":"2021-12-17T23:06:32.346575Z","shell.execute_reply":"2021-12-17T23:06:32.352631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Проверим размер датасета","metadata":{}},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-17T23:06:32.354907Z","iopub.execute_input":"2021-12-17T23:06:32.355188Z","iopub.status.idle":"2021-12-17T23:06:32.37063Z","shell.execute_reply.started":"2021-12-17T23:06:32.355147Z","shell.execute_reply":"2021-12-17T23:06:32.369835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Посмотрим на часть датасета","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-17T23:06:32.37364Z","iopub.execute_input":"2021-12-17T23:06:32.374305Z","iopub.status.idle":"2021-12-17T23:06:32.391853Z","shell.execute_reply.started":"2021-12-17T23:06:32.374249Z","shell.execute_reply":"2021-12-17T23:06:32.390546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Можно посмотреть подробнее на один из обзоров (столбец \"review\")","metadata":{}},{"cell_type":"code","source":"print(train[\"review\"][0])","metadata":{"execution":{"iopub.status.busy":"2021-12-17T23:06:32.393523Z","iopub.execute_input":"2021-12-17T23:06:32.393794Z","iopub.status.idle":"2021-12-17T23:06:32.408173Z","shell.execute_reply.started":"2021-12-17T23:06:32.393754Z","shell.execute_reply":"2021-12-17T23:06:32.407296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Очистка данных и предварительная обработка текста","metadata":{}},{"cell_type":"markdown","source":"В тексте можно заметить некоторого рода конструкции в кавычках <>. Это ничто иное, как HTML-теги, есть мнение, что они могут помешать в будущем, поэтому следуют их убрать. Но убирать подобное вручную было бы безумием. Поэтому воспользуемся предложенной библиотекой Beautiful Soup. Это библиотека для парсинга HTML.","metadata":{}},{"cell_type":"code","source":"from bs4 import BeautifulSoup             # импортируем саму библу\n\n#Теперь давайте создадим пару экспериментальных объектов при помощи данной библиотеки\n\nexample1 = BeautifulSoup(train[\"review\"][0])  #возьмем текст выше\nexample2 = BeautifulSoup(train[\"review\"][1])  #и еще дополнительный\nprint(train[\"review\"][0]) # выведем исходник\n","metadata":{"execution":{"iopub.status.busy":"2021-12-17T23:06:32.409519Z","iopub.execute_input":"2021-12-17T23:06:32.409733Z","iopub.status.idle":"2021-12-17T23:06:32.423879Z","shell.execute_reply.started":"2021-12-17T23:06:32.409707Z","shell.execute_reply":"2021-12-17T23:06:32.422798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(example1.get_text()) # выведем наш \"чистый\" текст","metadata":{"execution":{"iopub.status.busy":"2021-12-17T23:06:32.425321Z","iopub.execute_input":"2021-12-17T23:06:32.425965Z","iopub.status.idle":"2021-12-17T23:06:32.441908Z","shell.execute_reply.started":"2021-12-17T23:06:32.425927Z","shell.execute_reply":"2021-12-17T23:06:32.440894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train[\"review\"][1]) # выведем исходник","metadata":{"execution":{"iopub.status.busy":"2021-12-17T23:06:32.4435Z","iopub.execute_input":"2021-12-17T23:06:32.443829Z","iopub.status.idle":"2021-12-17T23:06:32.456806Z","shell.execute_reply.started":"2021-12-17T23:06:32.443788Z","shell.execute_reply":"2021-12-17T23:06:32.455686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(example2.get_text()) # выведем наш \"чистый\" текст","metadata":{"execution":{"iopub.status.busy":"2021-12-17T23:06:32.457881Z","iopub.execute_input":"2021-12-17T23:06:32.458205Z","iopub.status.idle":"2021-12-17T23:06:32.470164Z","shell.execute_reply.started":"2021-12-17T23:06:32.458161Z","shell.execute_reply":"2021-12-17T23:06:32.469295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Можно заметить, что во втором случае ничего не изменилось. \n\nВ первом же случае разница заметна. Но это еще не все, необходимо далее удалить числа, знаки препинания. Это можно сделать (а точнее, это будет легче всего) это через регулярные выражения.","metadata":{}},{"cell_type":"code","source":"import re #импортируем re (от слова Regular Expression)\n#нам нужны только слова (из букв, без каких либо знаков)\nletters_only = re.sub(\"[^a-zA-Z]\", \" \", example1.get_text())           # Первое - это шаблон или же заданное регулярное выражение, по которому будет отбираться то, что нам НЕ нужно, потому что ^ - перед буквами означает \"не\", то есть будет отбираться все не буквенное\n                          # второе - шаблон, который заметит все то, что нам не нужно, пробелами.\n                       # третье -Tекст, с которым мы будем работать\n\nprint(letters_only)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T23:06:32.472827Z","iopub.execute_input":"2021-12-17T23:06:32.47316Z","iopub.status.idle":"2021-12-17T23:06:32.484684Z","shell.execute_reply.started":"2021-12-17T23:06:32.473126Z","shell.execute_reply":"2021-12-17T23:06:32.483687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"И как можно заметить, все знаки исчезли. Теперь можно преобразовать этот текст в слова, для удобства привести все большие буквы к маленьким, а далее разделить по пробелам.","metadata":{}},{"cell_type":"code","source":"lower_case = letters_only.lower() #к нижнему регистру\nwords =lower_case.split() #разобъем текст на слова\nprint(words)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T23:06:32.485871Z","iopub.execute_input":"2021-12-17T23:06:32.486107Z","iopub.status.idle":"2021-12-17T23:06:32.499619Z","shell.execute_reply.started":"2021-12-17T23:06:32.486078Z","shell.execute_reply":"2021-12-17T23:06:32.498601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Сейчас можно начать разбираться со словами, которые появляются чаще всего и при это не несут никакого более-менее важного значения, так называемые \"шумовые слова\", на английском это обычно зовут Stop Words, как правило, это слова по типу \"а\", \"is\", \"the\". Можно было бы создать свой список таких слов, но благо за меня это уже сделали :) ","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords # Импортируем шумовые слова\nprint(stopwords.words(\"english\")) # посмотрим слова на английском","metadata":{"execution":{"iopub.status.busy":"2021-12-17T23:06:32.500964Z","iopub.execute_input":"2021-12-17T23:06:32.501227Z","iopub.status.idle":"2021-12-17T23:06:32.513541Z","shell.execute_reply.started":"2021-12-17T23:06:32.501195Z","shell.execute_reply":"2021-12-17T23:06:32.512852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(stopwords.words(\"russian\")) # можем еще и на русском глянуть, почему бы и нет","metadata":{"execution":{"iopub.status.busy":"2021-12-17T23:06:32.515159Z","iopub.execute_input":"2021-12-17T23:06:32.51538Z","iopub.status.idle":"2021-12-17T23:06:32.527197Z","shell.execute_reply.started":"2021-12-17T23:06:32.515353Z","shell.execute_reply":"2021-12-17T23:06:32.526182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Но мы все таки имеем дело с английскими обзорами, поэтому будем убирать из words английские шумовые слова","metadata":{}},{"cell_type":"code","source":"words = [w for w in words if not w in stopwords.words(\"english\")]\nprint(words)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T23:06:32.529023Z","iopub.execute_input":"2021-12-17T23:06:32.52937Z","iopub.status.idle":"2021-12-17T23:06:32.603023Z","shell.execute_reply.started":"2021-12-17T23:06:32.529325Z","shell.execute_reply":"2021-12-17T23:06:32.602103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Теперь можно приступать к очистке данных. Напишем функцию, которая будет возвращать нам \"чистый\" текст","metadata":{}},{"cell_type":"code","source":"def review_to_words( raw_review ):\n    #Расправимся с HTML\n    review_text = BeautifulSoup(raw_review).get_text() \n    #Удалим все не буквенное        \n    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n    #Приведем слова в тексте к нижнему регистру и разобъем этот текст на слова (получим массив слов)\n    words = letters_only.lower().split()                             \n    # Работать с множеством быстрее, чем со списком, поэтому используем множество\n    stops = set(stopwords.words(\"english\"))                  \n    # Уберем шумовые слова\n    meaningful_words = [w for w in words if not w in stops]   \n    # Объединим наши слова в текст. Слова будут разделены пробелом\n    return( \" \".join( meaningful_words ))","metadata":{"execution":{"iopub.status.busy":"2021-12-17T23:06:32.604291Z","iopub.execute_input":"2021-12-17T23:06:32.60454Z","iopub.status.idle":"2021-12-17T23:06:32.613291Z","shell.execute_reply.started":"2021-12-17T23:06:32.604511Z","shell.execute_reply":"2021-12-17T23:06:32.612346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Проверим на нашем тексте\nclean_review = review_to_words( train[\"review\"][0] )\nprint(clean_review)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T23:06:32.614331Z","iopub.execute_input":"2021-12-17T23:06:32.614548Z","iopub.status.idle":"2021-12-17T23:06:32.627637Z","shell.execute_reply.started":"2021-12-17T23:06:32.614515Z","shell.execute_reply":"2021-12-17T23:06:32.626821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"А теперь нужно чистить весь датасет от шума","metadata":{}},{"cell_type":"code","source":"# Размер колонки с отзывами\nnum_reviews = train[\"review\"].size\n\n# Новый список, в который будем пихать все, что уже очищено\nclean_train_reviews = []\n\n#Чистим\nfor i in range( 0, num_reviews):\n    clean_train_reviews.append(review_to_words(train[\"review\"][i]))","metadata":{"execution":{"iopub.status.busy":"2021-12-17T23:06:32.629008Z","iopub.execute_input":"2021-12-17T23:06:32.629312Z","iopub.status.idle":"2021-12-17T23:06:53.634923Z","shell.execute_reply.started":"2021-12-17T23:06:32.62927Z","shell.execute_reply":"2021-12-17T23:06:53.633787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Мешок слов","metadata":{}},{"cell_type":"markdown","source":"Мешок слов (англ. bag-of-words) — упрощенное представление текста, которое используется в обработке естественных языков и информационном поиске. В этой модели текст (одно предложение или весь документ) представляется в виде мешка (мультимножества) его слов без какого-либо учета грамматики и порядка слов, но с сохранением информации об их количестве. ","metadata":{}},{"cell_type":"code","source":"#Именно с этой моделью мы и будем дальше работать","metadata":{"execution":{"iopub.status.busy":"2021-12-17T23:06:53.636071Z","iopub.execute_input":"2021-12-17T23:06:53.636309Z","iopub.status.idle":"2021-12-17T23:06:53.639702Z","shell.execute_reply.started":"2021-12-17T23:06:53.63628Z","shell.execute_reply":"2021-12-17T23:06:53.638937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer #В scikit эта штука помогает создать \"мешо слов\"\n\n\nvectorizer = CountVectorizer(analyzer = \"word\",   \\\n                             tokenizer = None,    \\\n                             preprocessor = None, \\\n                             stop_words = None,   \\\n                             max_features = 5000) \n\n#Обучает нашу модель, пополняет словарь, а еще преобразует наши данные в вектор \"фич\" или же особенностей, максимальное кол-во фич мы указали выше (5000)\ntrain_data_features = vectorizer.fit_transform(clean_train_reviews)\n\n#конвертируем результат в массив\ntrain_data_features = train_data_features.toarray()","metadata":{"execution":{"iopub.status.busy":"2021-12-17T23:06:53.640972Z","iopub.execute_input":"2021-12-17T23:06:53.641263Z","iopub.status.idle":"2021-12-17T23:06:58.411311Z","shell.execute_reply.started":"2021-12-17T23:06:53.641232Z","shell.execute_reply":"2021-12-17T23:06:58.410281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_data_features.shape) #теперь наш датасет выглядит вот так","metadata":{"execution":{"iopub.status.busy":"2021-12-17T23:06:58.412823Z","iopub.execute_input":"2021-12-17T23:06:58.41313Z","iopub.status.idle":"2021-12-17T23:06:58.418119Z","shell.execute_reply.started":"2021-12-17T23:06:58.413099Z","shell.execute_reply":"2021-12-17T23:06:58.417288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Можно посмотреть на словарь","metadata":{}},{"cell_type":"code","source":"vocab = vectorizer.get_feature_names()\nprint(vocab)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T23:06:58.419511Z","iopub.execute_input":"2021-12-17T23:06:58.419772Z","iopub.status.idle":"2021-12-17T23:06:58.443118Z","shell.execute_reply.started":"2021-12-17T23:06:58.419719Z","shell.execute_reply":"2021-12-17T23:06:58.441993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\n# просуммируем кол-во каждых словарных слов\ndist = np.sum(train_data_features, axis=0)\n\n#выведем слова из словаря и посмотрим, сколько раз это слово появляется в тренировчном наборе\nfor tag, count in zip(vocab, dist):\n    print(count, tag)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T23:06:58.444719Z","iopub.execute_input":"2021-12-17T23:06:58.446237Z","iopub.status.idle":"2021-12-17T23:06:59.849357Z","shell.execute_reply.started":"2021-12-17T23:06:58.446186Z","shell.execute_reply":"2021-12-17T23:06:59.846827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Модель для классификации","metadata":{}},{"cell_type":"markdown","source":"В качестве модели для классификации предлагается взять случайный лес","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n#Лес со 150-ю деревьями\nforest = RandomForestClassifier(n_estimators = 150) \n\n# Натренируем нашу модель на датасете с особенностями и возьмем столбец sentiment как метку ответа (понравился ли фильм)\nforest = forest.fit( train_data_features, train[\"sentiment\"] )","metadata":{"execution":{"iopub.status.busy":"2021-12-17T23:06:59.851263Z","iopub.execute_input":"2021-12-17T23:06:59.851597Z","iopub.status.idle":"2021-12-17T23:08:31.0782Z","shell.execute_reply.started":"2021-12-17T23:06:59.851555Z","shell.execute_reply":"2021-12-17T23:08:31.077514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Фиксируем результат","metadata":{}},{"cell_type":"code","source":"#берем набор для теста\ntest = pd.read_csv(\"../input/word2vec-nlp-tutorial/testData.tsv.zip\", header=0, delimiter=\"\\t\", \\\n                   quoting=3 )\n\n#очистим далее тестовый дата сет, как делали до этого\nnum_reviews = len(test[\"review\"])\nclean_test_reviews = [] \n\nfor i in range(0,num_reviews):\n    clean_review = review_to_words(test[\"review\"][i] )\n    clean_test_reviews.append( clean_review )\n\n#Мешок слов для тестовой сборки\ntest_data_features = vectorizer.transform(clean_test_reviews)\ntest_data_features = test_data_features.toarray()\n\n# Сделаем прогноз\nresult = forest.predict(test_data_features)\n\n# Запомним ответ\noutput = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n\n# сохраним в файл\noutput.to_csv(\"./submission.csv\", index=False, quoting=3 )","metadata":{"execution":{"iopub.status.busy":"2021-12-17T23:08:31.079312Z","iopub.execute_input":"2021-12-17T23:08:31.080285Z","iopub.status.idle":"2021-12-17T23:09:01.062725Z","shell.execute_reply.started":"2021-12-17T23:08:31.080245Z","shell.execute_reply":"2021-12-17T23:09:01.061915Z"},"trusted":true},"execution_count":null,"outputs":[]}]}