{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this notebook, we train a text classifier to detect the sentiment of movie reviews."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Load dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the pandas package, then use the \"read_csv\" function to get the labeled training data\nimport pandas as pd       \ntraining = pd.read_csv(\"../input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip\", header=0, \\\n                       delimiter=\"\\t\", quoting=3)\nprint(training.shape)\n\n# Get target variable\ny_train = training['sentiment']\nx_train = training.drop([\"sentiment\"], axis=1)\n                      \ntraining.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data exploration"},{"metadata":{},"cell_type":"markdown","source":"We check if the training dataset is balanced."},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.countplot(y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see the number of words averaged for positive and negative reviews."},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt \nimport numpy as np\n# Get mean of positive and negative reviews\navg_pos_reviews = training[training.sentiment==1].review.apply(lambda x: len(x.split())).mean()\navg_neg_reviews = training[training.sentiment==0].review.apply(lambda x: len(x.split())).mean()\n\nplt.figure(figsize=(10, 3))\nplt.barh(['Positive', 'Negative'], [avg_pos_reviews, avg_neg_reviews], height=0.5)\nplt.xticks(np.arange(0, 300, 25))\nplt.xlabel('Average Number of words')\nplt.ylabel('Sentiment')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nimport numpy as np\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Stopwords"},{"metadata":{},"cell_type":"markdown","source":"We load the stopwords list and remove from the list those we do not want to delete from the text. We do not remove negation words because reviews with a lot of these can indicate that the review has a negative sentiment."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import list of stopwords from library NLTK\nfrom nltk.corpus import stopwords\n\nstopwords_list = set(stopwords.words(\"english\"))\nprint(f'List of stopwords:\\n{stopwords_list}\\n')\n\n# We remove negation words in list of stopwords\nno_stopwords = [\"not\",\"don't\",'aren','don','ain',\"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\",\n               'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\",\n               \"won't\", 'wouldn', \"wouldn't\"]\nfor no_stopword in no_stopwords:\n    stopwords_list.remove(no_stopword)\n    \n#print(stopwords_list)\nprint(f'Final list of stopwords:\\n{stopwords_list}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Lemmatize reviews"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import Lemmatizer from NLTK\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\n\n# function that receive a list of words and do lemmatization:\ndef lemma_stem_text(words_list):\n    # Lemmatizer\n    text = [lemmatizer.lemmatize(token.lower()) for token in words_list]# eighties->eight or messages->message or drugs->drug\n    text = [lemmatizer.lemmatize(token.lower(), \"v\") for token in text]# going-> go or started->start or watching->watch\n    return text\n\nword_example = \"feet\"\nprint(f'The word \"{word_example}\" is transformed to \"{lemma_stem_text([word_example])[0]}\"')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Negations"},{"metadata":{},"cell_type":"markdown","source":"We create a function to change negation abbreviate forms to standard using a regular expression."},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nre_negation = re.compile(\"n't \")\n\n# function that receive a sequence of woords and return the same sequence transforming\n# abbreviated negations to the standard form.\ndef negation_abbreviated_to_standard(sent):\n    sent = re_negation.sub(\" not \", sent)\n    return sent\n\nword_example = \"I aren't \"\nprint(f'The sentence \"{word_example}\" is transformed to \"{negation_abbreviated_to_standard(word_example)}\"')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We create a function to clean the text of a review using the functions defined previously."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import function BeautifulSoup to clean text of HTML tags\nfrom bs4 import BeautifulSoup \n\ndef review_to_words(raw_review):\n    # 1. Remove HTML tags\n    review_text = BeautifulSoup(raw_review).get_text() \n    \n    # 2. Transform abbreviated negations to the standard form.\n    review_text = negation_abbreviated_to_standard(review_text)\n    \n    # 3. Remove non-letters and non-numbers   \n    letters_numbers_only = re.sub(\"[^a-zA-Z_0-9]\", \" \", review_text) \n    \n    # 4. Convert to lower case and split into individual words (tokenization)\n    words = np.char.lower(letters_numbers_only.split())                             \n    \n    # 5. Remove stop words\n    meaningful_words = [w for w in words if not w in stopwords_list]   \n    \n    # 6. Apply lemmatization function\n    lemma_words = lemma_stem_text(meaningful_words)\n    \n    # 7. Join the words back into one string separated by space, and return the result.\n    return( \" \".join(lemma_words))   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see how is cleaned the first review in the training set."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Clean first review\nclean_review = review_to_words(x_train[\"review\"][0] )\n\n# Print original review, sentiment and cleaned review\nprint(f'Text of original review:\\n{x_train[\"review\"][0]}\\n')\nprint(f'Sentiment review: {y_train[0]}\\n')\nprint(f'Text of cleaned review:\\n{clean_review}') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We clean the text of all reviews in the training set."},{"metadata":{"trusted":true},"cell_type":"code","source":"# We get the text of reviews in the training set\nreviews = x_train['review']\n\n# We initialize an empty list to add the clean reviews\ncleaned_train_reviews = []\n\n# We loop over each review and clean it  \nfor i in reviews:\n    cleaned_train_reviews.append(review_to_words(i))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Vectorization"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import tf-idf encoding from sklearn library\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Define some hiperparameters of encoded\nvectorizer = TfidfVectorizer(max_features=20000, ngram_range = (1,2))\n\n# Create the training set with the words encoded as features of the reviews\ntrain_data_features = vectorizer.fit_transform(cleaned_train_reviews)\n\nprint(train_data_features.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the logistic regression model from sklearn \nfrom sklearn.linear_model import LogisticRegression\n\n# Define the model\nmodel = LogisticRegression(random_state=0, solver='lbfgs',\n                            multi_class='multinomial')\n# Train model\nmodel.fit(train_data_features, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predictions in Test dataset"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"We predict the sentiment of the reviews in the test dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read the test data\ntest = pd.read_csv(\"../input/word2vec-nlp-tutorial/testData.tsv.zip\", header=0, delimiter=\"\\t\", \\\n                   quoting=3 )\nprint(test.shape)\n\n# Create an empty list and append the clean reviews one by one\nnum_reviews = len(test[\"review\"])\nclean_test_reviews = [] \n\n# Clean the text of all reviews in the training set\nprint(\"Cleaning and parsing the test set movie reviews...\\n\")\nfor i in range(0,num_reviews):\n    clean_review = review_to_words( test[\"review\"][i] )\n    clean_test_reviews.append( clean_review )\n\n# Create the test set with the words encoded as features of the reviews\ntest_data_features = vectorizer.transform(clean_test_reviews)\n\n\n# Use the logistic regression model to make sentiment label predictions\nresult = model.predict(test_data_features)\n\n# Copy the results to a pandas dataframe with an \"id\" column and a \"sentiment\" column\noutput = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we build a dataframe to submission."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use pandas to save the dataframe\noutput.to_csv(\"submission.csv\", index=False, quoting=3 )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Appendix"},{"metadata":{},"cell_type":"markdown","source":"Create training and validation datasets. We train both logistic regression and random forest classifiers and evaluate them in the validation dataset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# We split train dataset\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train_data_features, y_train, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We train two models: random forest and logistic regression\nfrom sklearn.ensemble import RandomForestClassifier\n# Initialize a Random Forest classifier with 500 trees\nforest = RandomForestClassifier(n_estimators = 500, max_depth = None, min_samples_split=2, min_samples_leaf =1,\n                                bootstrap = True, random_state=0)\n# Train the model\nforest = forest.fit(X_train, y_train)\n# Print score of model(using test dataset)\nprint(forest.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize a logistic regression model \nlogistic = LogisticRegression(random_state=0, solver='lbfgs',\n                            multi_class='multinomial')\n# Train the model\nlogistic = logistic.fit(X_train, y_train)\n# Print score of model(using test dataset)\nprint(logistic.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_forest  = forest.predict(X_test)\ny_pred_logistic  = logistic.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nconfusion_matrix_forest = confusion_matrix(y_test, y_pred_forest, labels=[1,0])\nconfusion_matrix_forest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n# plot the confusion matrix\nax = plt.axes()\nsns.heatmap(confusion_matrix_forest, annot=True, fmt=\"d\")\nax.set_title('Confusion matrix Random Forest')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix_logistic = confusion_matrix(y_test, y_pred_logistic, labels=[1,0])\nconfusion_matrix_logistic\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the confusion matrix\nax = plt.axes()\nsns.heatmap(confusion_matrix_logistic, annot=True, fmt=\"d\")\nax.set_title('Confusion matrix Logistic Regression')\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}