{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\nimport re\nimport nltk\nfrom bs4 import BeautifulSoup\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.python.keras.preprocessing.text import Tokenizer\nimport zipfile","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nltk.download('stopwords')\nfrom nltk.corpus import stopwords","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir(\"..\"))\nprint(os.listdir(\"../input\"))\nprint(os.listdir(\"../input/word2vec-nlp-tutorial\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"raw_train_data = pd.read_csv(\"../input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip\", delimiter='\\t')\nraw_train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_train_len = raw_train_data[\"review\"].apply(len)\nraw_train_len","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(raw_train_len, bins=300, color='g')\nplt.yscale('log')\nplt.title('Log-Scale Number of Reviews vs. Length of Reviews')\nplt.xlabel('length of review')\nplt.ylabel('number of review')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_train_len.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axe = plt.subplots(ncols=1)\nfig.set_size_inches(6,3)\nsns.countplot(raw_train_data['sentiment'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('number of + : {}'.format(raw_train_data['sentiment'].value_counts()[1]))\nprint('number of - : {}'.format(raw_train_data['sentiment'].value_counts()[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_word_cnt = raw_train_data['review'].apply(lambda x:len(x.split(' '))) # number of words in each review \ntrain_word_cnt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nplt.hist(train_word_cnt, bins=50, color='g')\nplt.yscale('log')\nplt.title('Log-Scale Number of Reviews vs. Number of Reviews')\nplt.xlabel('number of words')\nplt.ylabel('number of reviews')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_word_cnt.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r_qmarks = np.mean(raw_train_data['review'].apply(lambda x: '?' in x))\nr_fullstops = np.mean(raw_train_data['review'].apply(lambda x: '.' in x))\nr_capitals = np.mean(raw_train_data['review'].apply(lambda x: max(y.isupper() for y in x)))\nr_numbers = np.mean(raw_train_data['review'].apply(lambda x: max(y.isdigit() for y in x)))\n\nprint('물음표가 있는 리뷰 : {:.2f}%'.format(r_qmarks * 100))\nprint('마침표가 있는 리뷰 : {:.2f}%'.format(r_fullstops * 100))\nprint('대문자가 있는 리뷰 : {:.2f}%'.format(r_capitals * 100))\nprint('숫자가 있는 리뷰 : {:.2f}%'.format(r_numbers * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"review = raw_train_data['review'][0]\nreview_text = BeautifulSoup(review,\"html5lib\").get_text() # html 태그를 제거한다.\nreview_text = re.sub(\"[^a-zA-Z]\",\" \",review_text) # 알파벳을 제외하고 모두 공백으로 바꾼다.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(review)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(review_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\n\nreview_text = review_text.lower()\nwords = review_text.split()\nwords = [w for w in words if not w in stop_words]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_review = ' '.join(words)\nprint(clean_review)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(review,remove_stopwords = False):\n    # html 제거\n    review_text = BeautifulSoup(review,\"html5lib\").get_text()\n    \n    # 특수문자 제거\n    review_text = re.sub(\"[^a-zA-Z]\",\" \",review_text)\n    \n    # 소문자로 통일 후 리스트화\n    words = review_text.lower().split()\n    \n    if remove_stopwords:\n        # 불용어 제거\n        stop_words = set(stopwords.words('english'))\n        words = [w for w in words if not w in stop_words]\n \n    clean_review = ' '.join(words)\n    \n    return clean_review","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_train_reviews = []\nfor review in raw_train_data['review']:\n    clean_train_reviews.append(preprocess(review,remove_stopwords = True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_train_reviews[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_train_df = pd.DataFrame({'id':raw_train_data['id'], 'review':clean_train_reviews, 'sentiment':raw_train_data['sentiment']})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(oov_token='<UNK>')\n#oov_token(out of vocab token)은 fitting된 tokenzier가 처음보는 단어를 어떻게 다룰지\n#즉 사전에 없는 단어에 어떤 값을 취할건지 결정한다.\n#본인은 <UNK>으로 설정하였으나 뭐로 하던 크게 상관없다.\ntokenizer.fit_on_texts(clean_train_reviews)\ntext_sequences = tokenizer.texts_to_sequences(clean_train_reviews)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(clean_train_reviews[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(text_sequences[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_vocab = tokenizer.word_index\nprint(word_vocab)\n#<UNK>의 인덱스가 1인것을 확인할 수 있다.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(word_vocab[\"stuff\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"전체 단어 수: \",len(word_vocab))\n#만약 oov_token을 추가하지 않을 경우 사전의 크기는 74066-1=74065가 된다.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_configs = {}\n\ndata_configs['vocab'] = word_vocab\ndata_configs['vocab_size'] = len(word_vocab) + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_SEQUENCE_LENGTH = 174\ntrain_inputs = pad_sequences(text_sequences,maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n\nprint('shape of train data: ', train_inputs.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = np.array(raw_train_data['sentiment'])\nprint('shape of train labels: ',train_labels.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_IN_PATH = './data_in/'\nTRAIN_INPUT_DATA = 'train_input.npy'\nTRAIN_LABEL_DATA = 'train_label.npy'\nTRAIN_CLEAN_DATA = 'train_clean.csv'\nDATA_CONFIGS = 'data_configs.json'\n\nif not os.path.exists(DATA_IN_PATH):\n    os.makedirs(DATA_IN_PATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.save(open(DATA_IN_PATH + TRAIN_INPUT_DATA, 'wb'), train_inputs)\nnp.save(open(DATA_IN_PATH + TRAIN_LABEL_DATA, 'wb'), train_labels)\n\nclean_train_df.to_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA, index = False)\n\njson.dump(data_configs, open(DATA_IN_PATH + DATA_CONFIGS, 'w'), ensure_ascii=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = pd.read_csv(\"../input/word2vec-nlp-tutorial/testData.tsv.zip\", delimiter='\\t')\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_test_reviews = []\n\nfor review in test_data['review']:\n    clean_test_reviews.append(preprocess(review, remove_stopwords = True))\nclean_test_df = pd.DataFrame({'review': clean_test_reviews, 'id': test_data['id']})\ntest_id = np.array(test_data['id'])\n\n#여기서 테스트셋에 대해 tokenizer를 fitting 하지 않는다는 것을 유의하자.\ntext_sequences = tokenizer.texts_to_sequences(clean_test_reviews)\ntest_inputs = pad_sequences(text_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TEST_INPUT_DATA = 'test_input.npy'\nTEST_CLEAN_DATA = 'test_clean.csv'\nTEST_ID_DATA = 'test_id.npy'\n\nnp.save(open(DATA_IN_PATH + TEST_INPUT_DATA, 'wb'), test_inputs)\nnp.save(open(DATA_IN_PATH + TEST_ID_DATA, 'wb'), test_id)\nclean_test_df.to_csv(DATA_IN_PATH + TEST_CLEAN_DATA, index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense, LSTM, Dropout, Bidirectional","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = len(word_vocab)+1\nvocab_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def small_model():\n    model = Sequential()\n    model.add(Embedding(vocab_size, 16))\n    model.add(GlobalAveragePooling1D())\n    model.add(Dense(16, activation = 'relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(1, activation = 'sigmoid'))\n    model.summary()\n    \n    model.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = small_model()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(train_inputs,\n                    train_labels,\n                    epochs=10,\n                    batch_size=256,\n                    validation_split = 0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del model\ndel history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def big_model():\n    model = Sequential()\n    model.add(Embedding(vocab_size, 16))\n    model.add(Bidirectional(LSTM(64, recurrent_dropout=0.1)))\n    model.add(Dropout(0.5))\n    model.add(Dense(32, activation = 'relu'))\n    model.add(Dropout(0.4))\n    model.add(Dense(1, activation = 'sigmoid'))\n    model.summary()\n    \n    model.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = big_model()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(train_inputs,\n                    train_labels,\n                    epochs=10,\n                    batch_size=256,\n                    validation_split = 0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('my_model.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntest_label = model.predict_classes(test_inputs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def submit(predictions):\n    test_data['sentiment'] = predictions\n    test_data.to_csv('submission.csv', index=False, columns=['id','sentiment'])\n\nsubmit(test_label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}