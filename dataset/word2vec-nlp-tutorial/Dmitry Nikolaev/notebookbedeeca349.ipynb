{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\np = '/kaggle/input/word2vec-nlp-tutorial'\ntrain_df = pd.read_csv(f\"{p}/labeledTrainData.tsv.zip\", header=0, delimiter=\"\\t\", quoting=3)\ntest_df = pd.read_csv(f\"{p}/testData.tsv.zip\", header=0, delimiter=\"\\t\", quoting=3 )\nutrain_df = pd.read_csv(f\"{p}/unlabeledTrainData.tsv.zip\", header=0, delimiter=\"\\t\", quoting=3 )","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-02T12:09:00.243645Z","iopub.execute_input":"2022-01-02T12:09:00.243974Z","iopub.status.idle":"2022-01-02T12:09:04.485472Z","shell.execute_reply.started":"2022-01-02T12:09:00.243884Z","shell.execute_reply":"2022-01-02T12:09:04.48457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TEST = False","metadata":{"execution":{"iopub.status.busy":"2022-01-02T12:09:04.487151Z","iopub.execute_input":"2022-01-02T12:09:04.487388Z","iopub.status.idle":"2022-01-02T12:09:04.492371Z","shell.execute_reply.started":"2022-01-02T12:09:04.487354Z","shell.execute_reply":"2022-01-02T12:09:04.491479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Data Cleaning and Text Preprocessing","metadata":{}},{"cell_type":"code","source":"import re\nimport nltk.data\nfrom bs4 import BeautifulSoup\nfrom nltk.corpus import stopwords","metadata":{"execution":{"iopub.status.busy":"2022-01-02T12:09:04.493282Z","iopub.execute_input":"2022-01-02T12:09:04.493488Z","iopub.status.idle":"2022-01-02T12:09:06.114251Z","shell.execute_reply.started":"2022-01-02T12:09:04.493463Z","shell.execute_reply":"2022-01-02T12:09:06.113482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def to_clean_words(review, remove_stopwords):\n    review = re.sub('\\B\\.+?\\B', '', review)\n    review = re.sub('https?:\\/\\/\\S+|www\\.\\S+', '', review)\n    review = BeautifulSoup(review).get_text()\n    review = re.sub('\\b\\d+\\b', 'NUM', review)\n    review = re.sub('[^a-zA-Z]',' ', review)\n\n    words = review.lower().split()\n\n    if remove_stopwords:\n        stops = set(stopwords.words('english'))\n        words = [w for w in words if not w in stops]\n\n    return words\n\ndef to_clean_sentences(review, tr):\n    raw_sentences = tr.tokenize(review.strip())\n\n    sentences = []\n    for raw_sentence in raw_sentences:\n        if len(raw_sentence) > 0:\n            sentences.append(to_clean_words(raw_sentence, False))\n\n    return sentences\n\ndef do_progress_next(step, total):\n    print(f'Processing {step:5} / {total:5}... ', end='\\r')\n    step += 1\n    return step\n\ndef df_to_clean_sentences(df):\n    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n    sentences = []\n    step = 1\n    for review in df['review']:\n        s = to_clean_sentences(review, tokenizer)\n        sentences += s\n        step = do_progress_next(step, df['review'].size)\n    print()\n    return sentences\n\ndef df_to_clean_words(df):\n    cdf = []\n    for r in df['review']:\n        cdf.append(to_clean_words(r, False))\n    return cdf","metadata":{"execution":{"iopub.status.busy":"2022-01-02T12:09:06.117036Z","iopub.execute_input":"2022-01-02T12:09:06.117676Z","iopub.status.idle":"2022-01-02T12:09:06.131053Z","shell.execute_reply.started":"2022-01-02T12:09:06.117627Z","shell.execute_reply":"2022-01-02T12:09:06.130275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nsentences = []\nsentences += df_to_clean_sentences(train_df)\nsentences += df_to_clean_sentences(utrain_df)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T12:09:06.132218Z","iopub.execute_input":"2022-01-02T12:09:06.132469Z","iopub.status.idle":"2022-01-02T12:14:15.992958Z","shell.execute_reply.started":"2022-01-02T12:09:06.132441Z","shell.execute_reply":"2022-01-02T12:14:15.992248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Word2vec Model","metadata":{}},{"cell_type":"code","source":"%%time\nfrom gensim.models import Phrases\nbigrams = Phrases(sentences=sentences)\ntrigrams = Phrases(sentences=bigrams[sentences])","metadata":{"execution":{"iopub.status.busy":"2022-01-02T12:14:15.994738Z","iopub.execute_input":"2022-01-02T12:14:15.994957Z","iopub.status.idle":"2022-01-02T12:16:26.387282Z","shell.execute_reply.started":"2022-01-02T12:14:15.99493Z","shell.execute_reply":"2022-01-02T12:16:26.386434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nimport logging\nfrom gensim.models import word2vec\n\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n    level=logging.INFO)\n\nvector_size = 300\nmin_word_count = 3\nnum_workers = 4\ncontext = 10\ndownsampling = 1e-3\n\nmodel_w2v = word2vec.Word2Vec(trigrams[bigrams[sentences]], workers=num_workers, \\\n            vector_size=vector_size, min_count=min_word_count, \\\n            window=context)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T12:16:26.38926Z","iopub.execute_input":"2022-01-02T12:16:26.389741Z","iopub.status.idle":"2022-01-02T12:29:46.930035Z","shell.execute_reply.started":"2022-01-02T12:16:26.389698Z","shell.execute_reply":"2022-01-02T12:29:46.928912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\nw2v_dict = dict(zip(model_w2v.wv.index_to_key, model_w2v.wv.vectors))\n\ndef vectorize_text(texts):\n    dim = len(next(iter(w2v_dict.values())))\n    return np.array([\n        np.mean([w2v_dict[w] for w in words if w in w2v_dict] \n                or [np.zeros(dim)], axis=0)\n        for words in texts\n    ])","metadata":{"execution":{"iopub.status.busy":"2022-01-02T12:29:46.931994Z","iopub.execute_input":"2022-01-02T12:29:46.932275Z","iopub.status.idle":"2022-01-02T12:29:46.99734Z","shell.execute_reply.started":"2022-01-02T12:29:46.932245Z","shell.execute_reply":"2022-01-02T12:29:46.996349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Main Model","metadata":{}},{"cell_type":"code","source":"def create_x(df):\n    data = trigrams[bigrams[df_to_clean_words(df)]]\n    return vectorize_text(data)\n\ndef create_y(df):\n    return df['sentiment']","metadata":{"execution":{"iopub.status.busy":"2022-01-02T12:29:46.998705Z","iopub.execute_input":"2022-01-02T12:29:46.999005Z","iopub.status.idle":"2022-01-02T12:29:47.008423Z","shell.execute_reply.started":"2022-01-02T12:29:46.998972Z","shell.execute_reply":"2022-01-02T12:29:47.007515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx = create_x(train_df)\ny = create_y(train_df)\n\nif TEST:\n    x_train, x_test, y_train, y_test = train_test_split(\n        x,\n        y,\n        test_size=0.2,\n        shuffle=True,\n        random_state=42)\nelse:\n    x_train = x\n    y_train = y","metadata":{"execution":{"iopub.status.busy":"2022-01-02T12:29:47.009493Z","iopub.execute_input":"2022-01-02T12:29:47.01011Z","iopub.status.idle":"2022-01-02T12:30:52.618956Z","shell.execute_reply.started":"2022-01-02T12:29:47.01005Z","shell.execute_reply":"2022-01-02T12:30:52.618213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if TEST:\n    print(x_train.shape)\n    print(y_train.shape)\n    print(x_test.shape)\n    print(y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T12:30:52.624493Z","iopub.execute_input":"2022-01-02T12:30:52.624735Z","iopub.status.idle":"2022-01-02T12:30:52.629689Z","shell.execute_reply.started":"2022-01-02T12:30:52.624707Z","shell.execute_reply":"2022-01-02T12:30:52.629023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\n\nnum_classes = 2\ny_train = to_categorical(y_train.array, num_classes)\n\nif TEST:\n    y_test = to_categorical(y_test.array, num_classes)\n\ninput_size = x_train.shape[1]\ninput_size","metadata":{"execution":{"iopub.status.busy":"2022-01-02T12:30:52.630724Z","iopub.execute_input":"2022-01-02T12:30:52.631605Z","iopub.status.idle":"2022-01-02T12:30:59.01749Z","shell.execute_reply.started":"2022-01-02T12:30:52.631557Z","shell.execute_reply":"2022-01-02T12:30:59.016474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\nimport tensorflow as tf\n\nhidden_size = 200\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Input(shape=(input_size,)),\n    tf.keras.layers.Dense(hidden_size, activation='relu'),\n    tf.keras.layers.Dense(num_classes, activation='softmax'),\n])\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-01-02T12:30:59.018568Z","iopub.execute_input":"2022-01-02T12:30:59.018787Z","iopub.status.idle":"2022-01-02T12:30:59.165184Z","shell.execute_reply.started":"2022-01-02T12:30:59.018762Z","shell.execute_reply":"2022-01-02T12:30:59.164331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 100\nnum_epochs = 64\n\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs,\n          verbose=1, validation_split=0.1)\nif TEST:\n    model.evaluate(x_test, y_test, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T12:30:59.166422Z","iopub.execute_input":"2022-01-02T12:30:59.167082Z","iopub.status.idle":"2022-01-02T12:31:35.945243Z","shell.execute_reply.started":"2022-01-02T12:30:59.167032Z","shell.execute_reply":"2022-01-02T12:31:35.944362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Results","metadata":{}},{"cell_type":"code","source":"x = create_x(test_df)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T12:31:35.946363Z","iopub.execute_input":"2022-01-02T12:31:35.946578Z","iopub.status.idle":"2022-01-02T12:32:39.621262Z","shell.execute_reply.started":"2022-01-02T12:31:35.946551Z","shell.execute_reply":"2022-01-02T12:32:39.620357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import csv\n\nres = model.predict(x)\npreds = np.argmax(res, axis=1)\n\noutput = pd.DataFrame(data={\"id\": test_df[\"id\"], \"sentiment\": preds})\noutput.to_csv('submission.csv', index=False, columns=['id','sentiment'], quoting=csv.QUOTE_NONE)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T12:32:39.625579Z","iopub.execute_input":"2022-01-02T12:32:39.625841Z","iopub.status.idle":"2022-01-02T12:32:40.4785Z","shell.execute_reply.started":"2022-01-02T12:32:39.625809Z","shell.execute_reply":"2022-01-02T12:32:40.477738Z"},"trusted":true},"execution_count":null,"outputs":[]}]}