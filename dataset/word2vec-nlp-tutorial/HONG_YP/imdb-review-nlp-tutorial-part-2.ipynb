{"cells":[{"metadata":{},"cell_type":"markdown","source":"# IMDb Review Tutorial \n#### ***[코드출처: 오늘코드(박조은 님)](https://github.com/corazzon)***"},{"metadata":{},"cell_type":"markdown","source":"![](https://static.amazon.jobs/teams/53/images/IMDb_Header_Page.jpg?1501027252)"},{"metadata":{},"cell_type":"markdown","source":"# 튜토리얼 PART 2 Word2Vec\n### - 딥러닝 기법인 Word2Vec을 통해 단어를 벡터화 해본다.\n### - t-SNE를 통해 벡터화 한 데이터를 시각화 해본다.\n### - 딥러닝과 지도학습의 랜덤포레스트를 사용하는 하이브리드 방식을 사용한다."},{"metadata":{},"cell_type":"markdown","source":"## Word2Vec (Word Embedding to Vector)\n#### 컴퓨터는 숫자만 인식할 수 있고 한글, 이미지는 바이너리 코드로 저장된다. 튜토리얼 파트1에서는 Bag of Words 라는 개념을 사용해서 머신러닝 알고리즘이 이해할 수 있도록 문자를 벡터화 해주는 작업을 하였다.\n- one hot encoding 혹은 Bag of words에서 vector size가 매우 크고 sparse하므로 neural net 성능이 잘 나오지 않는다.\n- '주위 단어가 비슷하면 해당 단어의 의미는 유사하다'라는 아이디어\n- 단어를 트레이닝 시킬 때 주위 단어를 label로 매치하여 최적화\n- 단어를 의미를 내포한 dense vector로 매칭 시키는 것\n- Word2Vec은 분산 된 텍스트 표현을 사용하여 개념 간 유사성을 본다. 예를 들어, 파리와 프랑스가 베를린과 독일과 같은 방식으로 관련되어 있음을 이해한다 (수도와 국가)"},{"metadata":{},"cell_type":"markdown","source":"![](https://1.bp.blogspot.com/-Q7F8ulD6fC0/UgvnVCSGmXI/AAAAAAAAAbg/MCWLTYBufhs/s1600/image00.gif)\n\n### 출처 : https://opensource.googleblog.com/2013/08/learning-meaning-behind-words.html"},{"metadata":{},"cell_type":"markdown","source":"### Word2Vec은 크게 두가지로 나뉜다.\n\n#### CBOW와 Skip-Gram 기법이 있다.\n1. CBOW(continuous bag of words)는 텍스트로 하나의 단어를 예측하기 때문에 작은 데이터셋일수록 유리하다.\n - 아래 예제에서 ___ 에 들어갈 단어를 예측한다.\n    \n    1) ___ 가 맛있다.\n    \n    2) ___ 를 타는 것이 재미있다.\n    \n    3) 평소보다 두 ___ 로 먹어서 ___ 가 아프다.\n\n2. Skip_Gram은 타겟 단어들로부터 원본 단어를 역으로 예측하는 것이다. CBOW와는 반대로 컨텍스트-타겟 쌍을 새로운 발견으로 처리하고 큰 규모의 데이터셋을 가질 때 유리하다.\n - '배'라는 단어 주변에 올 수 있는 단어를 예측한다.\n    \n    1) *배*가 맛있다.\n    \n    2) *배*를 타는 것이 재미있다.\n    \n    3) 평소보다 두 *배*로 먹어서 *배*가 아프다."},{"metadata":{},"cell_type":"markdown","source":"## Word2Vec 참고자료\n- [word2vec 모델 · 텐서플로우 문서 한글 번역본](https://tensorflowkorea.gitbooks.io/tensorflow-kr/g3doc/tutorials/word2vec/)\n- [Word2Vec으로 문장 분류하기 · ratsgo's blog](https://ratsgo.github.io/natural%20language%20processing/2017/03/08/word2vec/)\n- [Efficient Estimation of Word Representations in\\n\",\"Vector Space](https://arxiv.org/pdf/1301.3781v3.pdf)\n- [Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations- -of-words-and-phrases-and-their-compositionality.pdf)\n- [CS224n: Natural Language Processing with Deep Learning](http://web.stanford.edu/class/cs224n/syllabus.html)\n- [Word2Vec Tutorial - The Skip-Gram Model · Chris McCormick](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n\n## Gensim\n- [gensim: models.word2vec – Deep learning with word2vec](https://radimrehurek.com/gensim/models/word2vec.html)\n- [gensim: Tutorials](https://radimrehurek.com/gensim/tutorial.html)|\n- [한국어와 NLTK, Gensim의 만남 - PyCon Korea 2015](https://www.lucypark.kr/docs/2015-pyconkr/)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\n\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\n\ndf_train = pd.read_csv(\"../input/nlp-dataset/labeledTrainData.tsv\", header = 0,\n                      delimiter = \"\\t\", quoting = 3)\ndf_test = pd.read_csv(\"../input/nlp-dataset/testData.tsv\", header = 0,\n                      delimiter = \"\\t\", quoting = 3)\ndf_unlabeled = pd.read_csv(\"../input/nlp-dataset/unlabeledTrainData.tsv\", header = 0,\n                      delimiter = \"\\t\", quoting = 3)\n\nprint(df_train.shape)\nprint(df_test.shape)\nprint(df_unlabeled.shape)\n\nprint(df_train[\"review\"].size)\nprint(df_test[\"review\"].size)\nprint(df_unlabeled[\"review\"].size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()\n\n# 예측해야하는 sentiment 피쳐가 없다.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport nltk\n\nimport pandas as pd\nimport numpy as np\n\nfrom bs4 import BeautifulSoup\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\n\nfrom multiprocessing import Pool\n\nclass KaggleWord2VecUtility(object):\n\n    @staticmethod\n    def review_to_wordlist(review, remove_stopwords=False):\n        # 1. HTML 제거\n        review_text = BeautifulSoup(review, \"html.parser\").get_text()\n        # 2. 특수문자를 공백으로 바꿔줌\n        review_text = re.sub('[^a-zA-Z]', ' ', review_text)\n        # 3. 소문자로 변환 후 나눈다.\n        words = review_text.lower().split()\n        # 4. 불용어 제거\n        if remove_stopwords:\n            stops = set(stopwords.words('english'))\n            words = [w for w in words if not w in stops]\n        # 5. 어간추출\n        stemmer = SnowballStemmer('english')\n        words = [stemmer.stem(w) for w in words]\n        # 6. 리스트 형태로 반환\n        return(words)\n\n    @staticmethod\n    def review_to_join_words( review, remove_stopwords=False ):\n        words = KaggleWord2VecUtility.review_to_wordlist(\\\n            review, remove_stopwords=False)\n        join_words = ' '.join(words)\n        return join_words\n\n    @staticmethod\n    def review_to_sentences( review, remove_stopwords=False ):\n        # punkt tokenizer를 로드한다.\n        \"\"\"\n        이 때, pickle을 사용하는데\n        pickle을 통해 값을 저장하면 원래 변수에 연결 된 참조값 역시 저장된다.\n        저장된 pickle을 다시 읽으면 변수에 연결되었던\n        모든 레퍼런스가 계속 참조 상태를 유지한다.\n        \"\"\"\n        tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n        # 1. nltk tokenizer를 사용해서 단어로 토큰화 하고 공백 등을 제거한다.\n        raw_sentences = tokenizer.tokenize(review.strip())\n        # 2. 각 문장을 순회한다.\n        sentences = []\n        for raw_sentence in raw_sentences:\n            # 비어있다면 skip\n            if len(raw_sentence) > 0:\n                # 태그제거, 알파벳문자가 아닌 것은 공백으로 치환, 불용어제거\n                sentences.append(\\\n                    KaggleWord2VecUtility.review_to_wordlist(\\\n                    raw_sentence, remove_stopwords))\n        return sentences\n\n\n    # 참고 : https://gist.github.com/yong27/7869662\n    # http://www.racketracer.com/2016/07/06/pandas-in-parallel/\n    # 속도 개선을 위해 멀티 스레드로 작업하도록\n    @staticmethod\n    def _apply_df(args):\n        df, func, kwargs = args\n        return df.apply(func, **kwargs)\n\n    @staticmethod\n    def apply_by_multiprocessing(df, func, **kwargs):\n        # 키워드 항목 중 workers 파라메터를 꺼냄\n        workers = kwargs.pop('workers')\n        # 위에서 가져온 workers 수로 프로세스 풀을 정의\n        pool = Pool(processes=workers)\n        # 실행할 함수와 데이터프레임을 워커의 수 만큼 나눠 작업\n        result = pool.map(KaggleWord2VecUtility._apply_df, [(d, func, kwargs)\n                for d in np.array_split(df, workers)])\n        pool.close()\n        # 작업 결과를 합쳐서 반환\n        return pd.concat(result)\n    \n    \n# KaggleWord2VecUtility를 class로 생성하여 사용 \n# 코드 출처: https://github.com/corazzon/KaggleStruggle/blob/master/word2vec-nlp-tutorial/KaggleWord2VecUtility.py","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"KaggleWord2VecUtility.review_to_wordlist(df_train[\"review\"][0])[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = []\nfor review in df_train[\"review\"]:\n    sentences += KaggleWord2VecUtility.review_to_sentences(\n    review, remove_stopwords = False)\n    \n# KaggleWord2VecUtility을 사용하여 train 데이터를 정제해준다.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for review in df_unlabeled[\"review\"]:\n    sentences += KaggleWord2VecUtility.review_to_sentences(\n    review, remove_stopwords = False)\n    \n# KaggleWord2VecUtility을 사용하여 unlabeled train 데이터를 정제해준다.    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(sentences)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences[0][:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Word2Vec 모델을 학습\n\n### 전처리를 거쳐 파싱된 문장의 목록으로 모델을 학습시킬 준비가 되었다."},{"metadata":{},"cell_type":"markdown","source":"# Gensim\n\n* [gensim: models.word2vec - Deep learning with word2vec](https://radimrehurek.com/gensim/models/word2vec.html)"},{"metadata":{},"cell_type":"markdown","source":"## Word2Vec 모델의 파라메터\n\n* 아키텍처: 아키텍처 옵션은 skip-gram(default) 또는 CBOW 모델이다. skip-gram은 느리지만 더 나은 결과를 낸다.\n* 학습 알고리즘: Hierarchical softmax(default) 또는 negative 샘플링. 여기에서는 기본값이 잘 동작한다.\n* 빈번하게 등장하는 단어에 대한 다운 샘플링: Google 문서는 0.00001에서 0.001 사이의 값을 권장한다. 여기에서는 0.001에 가까운 값이 최종 모델의 정확도를 높이는 것으로 보여진다.\n* 단어 벡터 차원: 많은 feature를 사용한다고 항상 좋은 것은 아니지만 대체적으로 좀 더 나은 모델이 된다. 합리적인 값은 수십에서 수백 개가 될 수 있고 여기에서는 300으로 지정했다.\n* 컨텍스트 / 창크기: 학습 알고리즘이 고려해야 하는 컨텍스트의 단어수는 얼마나 될까? Hierarchical softmax를 위해 좀 더 큰 수가 좋지만 10정도가 적당하다.\n* Worker threads: 실행할 병렬 프로세스의 수로 컴퓨터마다 다르지만 대부분의 시스템에서 4~6의 값을 사용한다.\n* 최소 단어 수: 어휘의 크기를 의미있는 단어로 제한하는 데 도움이 된다. 모든 문서에서 여러 번 발생하지 않는 단어는 무시된다. 10에서 100사이가 적당하며, 이 대회의 데이터는 각 영화가 30개씩의 리뷰가 있기 때문에 개별 영화 제목에 너무 많은 중요성이 붙는 것을 피하기 위해 최소 단어 수를 40으로 설정한다. 그 결과 전체 어휘 크기는 약 15,000 단어가 된다. 높은 값은 제한 된 실행시간에 도움이 된다.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import logging\nlogging.basicConfig(\n    format = \"%(asctime)s : %(levelname)s : %(message)s\",\n    level = logging.INFO)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 파라미터 값을 지정해준다. \n\nnum_features = 300 # 문자 벡터 차원 수 (size)\nmin_word_count = 40 # 최소 문자 수 (min_count)\nnum_workers = 4 # 병렬 처리 스레드 수 (workers)\ncontext = 10 # 문자열 창 크기 (window)\ndownsampling = 1e-3 # 문자 빈도 수 Downsample (sample)\n\n# 초기화 및 모델 학습\nfrom gensim.models import word2vec\n\nmodel = word2vec.Word2Vec(sentences,\n                         workers = num_workers,\n                         size = num_features,\n                         min_count = min_word_count,\n                         window = context,\n                         sample = downsampling)\n\nmodel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 학습이 완료되면 필요없는 메모리를 unload 시킨다.\nmodel.init_sims(replace = True)\n\nmodel_name = \"300features_40minwindows_10text\"\nmodel.save(model_name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 모델 결과 탐색"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 유사도가 없는 단어 추출\nmodel.wv.doesnt_match(\"man woman child kitchen\".split())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.wv.doesnt_match(\"france england germany berlin\".split())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 가장 유사한 단어를 추출\nmodel.wv.most_similar(\"man\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.wv.most_similar(\"queen\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.wv.most_similar(\"film\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.wv.most_similar(\"happi\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Word2Vec으로 벡터화 한 단어를 t-SNE을 통해 시각화"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import TSNE\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport gensim\nimport gensim.models as g\n\n# 그래프에서 마이너스 폰트 깨지는 문제에 대한 대처\nmpl.rcParams[\"axes.unicode_minus\"] = False\n\nmodel_name = \"300features_40minwindows_10text\"\nmodel = g.Doc2Vec.load(model_name)\n\nvocab = list(model.wv.vocab)\nX = model[vocab]\n\nprint(len(X))\nprint(X[0][:10])\ntsne = TSNE(n_components = 2)\n\n# 100개의 단어에 대해서만 시각화\nX_tsne = tsne.fit_transform(X[:100,:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(X_tsne, index = vocab[:100], columns = [\"x\", \"y\"])\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nfig.set_size_inches(40, 20)\nax = fig.add_subplot(1, 1, 1)\n\nax.scatter(df[\"x\"], df[\"y\"])\n\nfor word, pos in df.iterrows():\n    ax.annotate(word, pos, fontsize = 30)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 평균 feature 벡터 계산, 2D numpy 배열로 반환"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\n\n# def를 이용해서 주어진 문장에서 단어 벡터의 평균을 구하는 함수를 만든다.\ndef makeFeatureVec(words, model, num_features):\n    featureVec = np.zeros((num_features,),dtype = \"float32\")\n    \n    # 속도를 위해 0으로 채운 배열로 초기화 한다.\n    nwords = 0.\n    # Index2word는 모델의 사전에 있는 단어명을 담은 리스트이다.\n    # 속도를 위해 set 형태로 초기화 한다.\n    index2word_set = set(model.wv.index2word)\n    # 루프를 돌며 모델 사전에 포함이 되는 단어라면 피처에 추가한다.\n    for word in words:\n        if word in index2word_set:\n            nwords = nwords + 1.\n            featureVec = np.add(featureVec, model[word])\n    # 결과를 단어수로 나누어 평균을 구한다.        \n    featureVec = np.divide(featureVec, nwords)\n    return featureVec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getAvgFeatureVecs(reviews, model, num_features):\n    # 리뷰 단어 목록의 각각에 대한 평균 feature 벡터를 계산하고\n    # 2d Numpy Array로 반환한다.\n    \n    # 카운터를 초기화 한다.\n    counter = 0.\n    # 속도를 위해 2D 넘파이 배열을 미리 할당한다.\n    reviewFeatureVecs = np.zeros(\n        (len(reviews), num_features), dtype = \"float32\")\n    \n    for review in reviews:\n        # 매 1000개 리뷰마다 상태를 출력\n        if counter%1000. == 0.:\n            print(\"Review %d of %d\"%(counter, len(reviews)))\n        # 평균 피처 벡터를 만들기 위해 위에서 정의한 함수를 호출한다.    \n        reviewFeatureVecs[int(counter)] = makeFeatureVec(review,\n                                                         model,\n                                                        num_features)\n        # 카운터를 증가시킨다.\n        counter = counter + 1.\n    return reviewFeatureVecs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 멀티스레드로 4개의 워커를 사용해 처리한다.\n\ndef getCleanReviews(reviews):\n    clean_reviews = []\n    clean_reviews = KaggleWord2VecUtility.apply_by_multiprocessing(\\\n        reviews[\"review\"], KaggleWord2VecUtility.review_to_wordlist,\\\n        workers = 4)\n    return clean_reviews","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time trainDataVecs = getAvgFeatureVecs(\\\n    getCleanReviews(df_train), model, num_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time testDataVecs = getAvgFeatureVecs(\\\n    getCleanReviews(df_test), model, num_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(\n    n_estimators = 100, n_jobs = -1, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time rf.fit(trainDataVecs, df_train[\"sentiment\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n%time score= np.mean(cross_val_score(\\\n    rf, trainDataVecs, df_train[\"sentiment\"], cv = 10, scoring = \"roc_auc\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = rf.predict(testDataVecs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = pd.DataFrame(data = {\"id\": df_test[\"id\"], \"sentiment\": result})\noutput.to_csv(\"./Word2Vec_Tutorial_{:.5f}.csv\".format(score),\n             index = False, quoting = 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output_sentiment = output[\"sentiment\"].value_counts()\nprint(output_sentiment[0] - output_sentiment[1])\noutput_sentiment","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}