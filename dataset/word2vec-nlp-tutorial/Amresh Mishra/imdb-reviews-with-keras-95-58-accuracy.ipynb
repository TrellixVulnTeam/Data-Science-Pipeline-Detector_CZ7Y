{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from importlib import reload\nimport sys\nfrom imp import reload\nimport warnings\nwarnings.filterwarnings('ignore')\nif sys.version[0] == '2':\n    reload(sys)\n    sys.setdefaultencoding(\"utf-8\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"32c8fac968fc038d45327b9bf1b809359bf8802e"},"cell_type":"markdown","source":"**Import panda and the datasets**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\ndf1 = pd.read_csv('../input/word2vec-nlp-tutorial/labeledTrainData.tsv', delimiter=\"\\t\")\ndf1 = df1.drop(['id'], axis=1)\ndf1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"43cbec25f37de8995907c9ae65e9411fca06ee55"},"cell_type":"code","source":"df2 = pd.read_csv('../input/imdb-review-dataset/imdb_master.csv',encoding=\"latin-1\")\ndf2.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"096c1cbe7c619b1c8ab0b7e7b28781dc8a6c8b44"},"cell_type":"markdown","source":"**It is better to drop columns *Unnamed: 0,File* and *type* since these will not contribute to final learning.***"},{"metadata":{"trusted":true,"_uuid":"d9424b01c0ff0c4e0722c5fb1cc73f130c4c0813"},"cell_type":"code","source":"df2 = df2.drop(['Unnamed: 0','type','file'],axis=1)\ndf2.columns = [\"review\",\"sentiment\"]\ndf2.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9384abf2abc01c41793543014a4729b06db67f46"},"cell_type":"markdown","source":"**Let us map the positive sentiments to 1 and negative sentiments to 0.**"},{"metadata":{"trusted":true,"_uuid":"6be14c5fc77cec6e434e0fb258933d31567f6f2c"},"cell_type":"code","source":"df2 = df2[df2.sentiment != 'unsup']\ndf2['sentiment'] = df2['sentiment'].map({'pos': 1, 'neg': 0})\ndf2.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b66d8064a67fcbde9333cdcd0dbd032f9634dfc9"},"cell_type":"markdown","source":"**Merging both datasets into one.**"},{"metadata":{"trusted":true,"_uuid":"1f502985bfb208f282d096cf4eda4df52ba628e3"},"cell_type":"code","source":"df = pd.concat([df1, df2]).reset_index(drop=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0858cfc3c577247c89606cf0cb32da9d926358da"},"cell_type":"markdown","source":"**To do better analysis it is important that we clean the data. We remove unnessecary spaces and stop words like *the, if, of,etc.* Further we process to lemmatize the words to group them.**"},{"metadata":{"trusted":true,"_uuid":"e08a2867d342084bde763daecb65f094273b06c7"},"cell_type":"code","source":"import re\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\nstop_words = set(stopwords.words(\"english\")) \nlemmatizer = WordNetLemmatizer()\n\n\ndef clean_text(text):\n    text = re.sub(r'[^\\w\\s]','',text, re.UNICODE)\n    text = text.lower()\n    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n    text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n    text = [word for word in text if not word in stop_words]\n    text = \" \".join(text)\n    return text\n\ndf['Processed_Reviews'] = df.review.apply(lambda x: clean_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28850b4961bc5ece382efbfe28b997884feaa804"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd16d566ad70485965727a39da32c1c1f5b22c06"},"cell_type":"markdown","source":"**Here we start to build our deep neural network with Keras after tokenizing the words since neural network can only accept numerical values. After the model is created we run the model against our data for 3 epochs.**"},{"metadata":{"trusted":true,"_uuid":"2e2dc8a1cd71d8be15ce4103b7bfccddcbb2cb3b"},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model, Sequential\nfrom keras.layers import Convolution1D\nfrom keras import initializers, regularizers, constraints, optimizers, layers\n\nmax_features = 6000\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(df['Processed_Reviews'])\nlist_tokenized_train = tokenizer.texts_to_sequences(df['Processed_Reviews'])\n\nmaxlen = 130\nX_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\ny = df['sentiment']\n\nembed_size = 128\nmodel = Sequential()\nmodel.add(Embedding(max_features, embed_size))\nmodel.add(Bidirectional(LSTM(32, return_sequences = True)))\nmodel.add(GlobalMaxPool1D())\nmodel.add(Dense(20, activation=\"relu\"))\nmodel.add(Dropout(0.05))\nmodel.add(Dense(10, activation=\"relu\"))\nmodel.add(Dropout(0.05))\nmodel.add(Dense(1, activation=\"sigmoid\"))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n\nbatch_size = 100\nepochs = 3\nmodel.fit(X_t,y, batch_size=batch_size, epochs=epochs, validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a683f1e5f19111319e6367cf3bdc3c48968bdb48"},"cell_type":"markdown","source":"**This is the step where we test how accurate our model is with various metrics.**"},{"metadata":{"trusted":true,"_uuid":"bf093d54044461be3954e535f66636d45abe93da"},"cell_type":"code","source":"df_test=pd.read_csv(\"../input/word2vec-nlp-tutorial/testData.tsv\",header=0, delimiter=\"\\t\", quoting=3)\ndf_test.head()\ndf_test[\"review\"]=df_test.review.apply(lambda x: clean_text(x))\ndf_test[\"sentiment\"] = df_test[\"id\"].map(lambda x: 1 if int(x.strip('\"').split(\"_\")[1]) >= 5 else 0)\ny_test = df_test[\"sentiment\"]\nlist_sentences_test = df_test[\"review\"]\nlist_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\nX_te = pad_sequences(list_tokenized_test, maxlen=maxlen)\nprediction = model.predict(X_te)\ny_pred = (prediction > 0.5)\nfrom sklearn.metrics import f1_score, confusion_matrix\nprint('F1-score: {0}'.format(f1_score(y_pred, y_test)))\nprint('Confusion matrix:')\nconfusion_matrix(y_pred, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"614755d9ffd228e55d6c2c09469bf63379e4a320"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}