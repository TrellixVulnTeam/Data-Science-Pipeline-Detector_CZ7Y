{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n    \nprint(os.listdir(\"../input/word2vec-nlp-tutorial\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import zipfile\n\nfiles=['/kaggle/input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip',\n       '/kaggle/input/word2vec-nlp-tutorial/testData.tsv.zip',\n       '/kaggle/input/word2vec-nlp-tutorial/unlabeledTrainData.tsv.zip']\n\nfor file in files :\n    zip = zipfile.ZipFile(file,'r')\n    zip.extractall()\n    zip.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.read_csv('/kaggle/working/labeledTrainData.tsv', delimiter=\"\\t\")\ntest=pd.read_csv('/kaggle/working/testData.tsv', delimiter=\"\\t\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub=pd.read_csv('/kaggle/input/word2vec-nlp-tutorial/sampleSubmission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('the train data is : {} line'.format(len(train)))\nprint('the test data is : {} line'.format(len(test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_len=train['review'].apply(len)\ntest_len=test['review'].apply(len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['word_n'] = train['review'].apply(lambda x : len(x.split(' ')))\ntest['word_n'] = test['review'].apply(lambda x : len(x.split(' ')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['length']=train['review'].apply(len)\ntrain['length'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['word_n'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\ncloud=WordCloud(width=800, height=600).generate(\" \".join(train['review'])) # join function can help merge all words into one string. \" \" means space can be a sep between words.\nplt.figure(figsize=(15,10))\nplt.imshow(cloud)\nplt.axis('off')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip\",\nsep = '\\t')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import the stopwords (common words) to be removed from the corpus\n\nimport re\nimport json\nfrom bs4 import BeautifulSoup\nfrom nltk.corpus import stopwords\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\n\nfrom nltk.stem.porter import PorterStemmer\ncorpus = []\ns = set(stopwords.words('english'))\ns.remove('not')\nprint(\"Stopwords length\", len(s))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['review']=train['review'].apply(lambda x: BeautifulSoup(x,\"html5lib\").get_text())\ntest['review']=test['review'].apply(lambda x: BeautifulSoup(x,\"html5lib\").get_text())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['review']=train['review'].apply(lambda x: re.sub(\"[^a-zA-Z]\",\" \",x))\ntest['review']=test['review'].apply(lambda x: re.sub(\"[^a-zA-Z]\",\" \",x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\ncloud=WordCloud(width=800, height=600).generate(\" \".join(train['review'])) # join function can help merge all words into one string. \" \" means space can be a sep between words.\nplt.figure(figsize=(15,10))\nplt.imshow(cloud)\nplt.axis('off')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stops = set(stopwords.words(\"english\"))\n\nfor i in range(0,25000) : \n    review = train.iloc[i,2] # review column : 2 \n    review = review.lower().split()\n    words = [r for r in review if not r in stops]\n    clean_review = ' '.join(words)\n    train.iloc[i,2] = clean_review","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(0,25000) : \n    review = test.iloc[i,1] # review column : 1\n    review = review.lower().split()\n    words = [r for r in review if not r in stops]\n    clean_review = ' '.join(words)\n    test.iloc[i,1] = clean_review","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer, text_to_word_sequence\nfrom keras.preprocessing.sequence import pad_sequences\nMAX_VCOCAB_SIZE = 5000\nEMBEDDING_DIM = 50\nMAX_SEQUENCE_LENGTH = 1500\n\ntokenizer = Tokenizer( filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True, split=' ')\nsequences = tokenizer.fit_on_texts(df['review'])\nword_index = tokenizer.word_index\ndocuments = tokenizer.texts_to_sequences(df['review'])\n#print(word_index)\ntoken_count = len(word_index)+1\nprint('Found {} unique tokens.'.format(token_count))\n\n#print(t.word_counts)\nprint(\"Total documents \", tokenizer.document_count)\n#print(t.word_index)\n#print(t.word_docs)\nprint(\"max sequence length:\", max(len(s) for s in documents))\nprint(\"min sequence length:\", min(len(s) for s in documents))\n\n# pad sequences so that we get a N x T matrix\ndata = pad_sequences(documents, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\nprint('Shape of data tensor:', data.shape)\nprint(data[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import Sequential\nfrom keras.layers import Dense, Embedding, Flatten\n\nmodel=Sequential()\nmodel.add(Embedding(101247,65, input_length=400))\nmodel.add(Flatten())\nmodel.add(Dense(2,activation='softmax'))\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word2vec = {}\n\nprint('Filling pre-trained embeddings...')\nembedding_matrix = np.zeros((token_count, EMBEDDING_DIM))\nfor word, i in word_index.items():\n  #if i < MAX_VOCAB_SIZE:\n    embedding_vector = word2vec.get(word) #get(word) is used instead of [word] as it won't give exception in case word is not found\n    if embedding_vector is not None:\n      # words not found in embedding index will be all zeros.\n      embedding_matrix[i,:] = embedding_vector\n\nprint(\"Sample embedded dimension \")\nprint(embedding_matrix[10][:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Conv1D, MaxPooling1D, Dropout, Dense, GlobalAveragePooling1D \nfrom keras.layers import Embedding, Conv2D, GlobalMaxPooling1D \nfrom keras import regularizers\n\nembedding_layer = Embedding(\n  token_count,\n  EMBEDDING_DIM,\n  weights=[embedding_matrix],\n  input_length=MAX_SEQUENCE_LENGTH,\n  trainable=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(embedding_layer)\nmodel.add(Conv1D(filters = 64, kernel_size = 4, padding = 'same', activation='relu'))\nmodel.add(MaxPooling1D())#kernel_size=500))\nmodel.add(Conv1D(filters = 128, kernel_size = 3, padding = 'same',  activation='relu', kernel_regularizer=regularizers.l2(0.01)))\nmodel.add(Dropout(0.25))\nmodel.add(MaxPooling1D())\nmodel.add(Conv1D(filters = 256, kernel_size = 2, padding = 'same', activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(MaxPooling1D())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Conv1D(128, 3, activation='relu'))\nmodel.add(GlobalMaxPooling1D())\n\nmodel.add(Dense(1, activation='softmax'))\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(data, df['sentiment'], test_size=0.2, random_state=42)\nprint(x_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(x_train, y_train , batch_size=32, epochs=10, validation_split = 0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t_loss = model.history.history['loss']\nv_loss = model.history.history['val_loss']\nepochs = range(1,len(t_loss)+1)\nplt.plot(epochs,t_loss, 'bo', label='Training loss')\nplt.plot(epochs,v_loss, 'r--', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t_acc = model.history.history['accuracy']\nv_acc = model.history.history['val_accuracy']\nplt.plot(epochs,t_acc,'bo',label='Training acc')\nplt.plot(epochs,v_acc,'r--', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = model.evaluate(x_test, y_test)\ntest_acc=np.round(results[1]*100,decimals=2)\n#test_loss=np.round(results[2]*100,decimals=2)\nprint('Test accuracy is',test_acc,'%')\n#print('Test loss is',test_loss,'%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('result.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}