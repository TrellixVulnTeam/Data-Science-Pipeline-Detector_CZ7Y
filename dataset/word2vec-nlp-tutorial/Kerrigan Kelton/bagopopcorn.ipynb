{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport nltk                            # Cleaning the data\nfrom bs4 import BeautifulSoup\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport re\nimport os","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import zipfile\n\nfiles=['/kaggle/input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip',\n       '/kaggle/input/word2vec-nlp-tutorial/testData.tsv.zip',\n       '/kaggle/input/word2vec-nlp-tutorial/unlabeledTrainData.tsv.zip']\n\nfor file in files :\n    zip = zipfile.ZipFile(file,'r')\n    zip.extractall()\n    zip.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.read_csv('/kaggle/working/labeledTrainData.tsv', header = 0, delimiter=\"\\t\")\ntest=pd.read_csv('/kaggle/working/testData.tsv', header = 0, delimiter=\"\\t\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_length=train['review'].apply(len)\ntest_length=test['review'].apply(len)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfig=plt.figure(figsize=(15,4))\nfig.add_subplot(1,2,1)\nsns.distplot((train_length),color='red')\n\nfig.add_subplot(1,2,2)\nsns.distplot((test_length),color='blue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['word_n'] = train['review'].apply(lambda x : len(x.split(' ')))\ntest['word_n'] = test['review'].apply(lambda x : len(x.split(' ')))\n\nfig=plt.figure(figsize=(15,4))\nfig.add_subplot(1,2,1)\nsns.distplot(train['word_n'],color='red')\n\nfig.add_subplot(1,2,2)\nsns.distplot(test['word_n'],color='blue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\ncloud=WordCloud(width=800, height=600).generate(\" \".join(train['review'])) # join function can help merge all words into one string. \" \" means space can be a sep between words.\nplt.figure(figsize=(15,10))\nplt.imshow(cloud)\nplt.axis('off')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import the stopwords (common words) to be removed from the corpus\n\nimport re\nimport json\nfrom bs4 import BeautifulSoup\nfrom nltk.corpus import stopwords\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\n\nfrom nltk.stem.porter import PorterStemmer\ncorpus = []\ns = set(stopwords.words('english'))\ns.remove('not')\nprint(\"Stopwords length\", len(s))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['review']=train['review'].apply(lambda x: BeautifulSoup(x,\"html5lib\").get_text())\ntest['review']=test['review'].apply(lambda x: BeautifulSoup(x,\"html5lib\").get_text())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['review']=train['review'].apply(lambda x: re.sub(\"[^a-zA-Z]\",\" \",x))\ntest['review']=test['review'].apply(lambda x: re.sub(\"[^a-zA-Z]\",\" \",x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['Sentiment'] = test['id'].map(lambda x: 1 if int(x.strip('\"').split('_')[1]) >=5 else 0)\ny_test = test_df['Sentiment']\ny_test.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.drop(['Sentiment'],axis = 1,inplace = True)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.sentiment.value_counts()  # balanced data...","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_review(raw_rev):\n    review_text = BeautifulSoup(raw_rev,'lxml').get_text()          # remove HTML\n    review_text = re.sub('[^a-zA-Z]',\" \",review_text)               # includes only words\n    review_words = review_text.lower().split()              # splits words and converts it to lowercase\n    \n    Stop_words = set(stopwords.words(\"english\"))                        \n    \n    mean_words = [w for w in review_words if not w in Stop_words]    # removes  stopwords..\n    review = ' '.join(mean_words)\n    \n    return review","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['clean_review'] = train['review'].apply(clean_review)\ntest['clean_review'] = test['review'].apply(clean_review)\ntest.drop(['review'],axis = 1,inplace = True)\ntest.rename(columns = {'clean_review':'review'},inplace = True)\ntrain['length_review'] = train['clean_review'].apply(len)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\ncloud=WordCloud(width=800, height=600).generate(\" \".join(train['review'])) # join function can help merge all words into one string. \" \" means space can be a sep between words.\nplt.figure(figsize=(15,10))\nplt.imshow(cloud)\nplt.axis('off')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing import sequence,text\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout,Embedding,LSTM,SpatialDropout1D,Bidirectional\nfrom keras.utils import to_categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x = train.iloc[:,3].values\ntarget = train.sentiment.values\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split( train_x, target , test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_train.shape,x_val.shape,y_train.shape,y_val.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# max length of the review\n\nr_len=[]\nfor text in train['clean_review']:\n    word=word_tokenize(text)\n    l=len(word)\n    r_len.append(l)\n    \nMAX_REVIEW_LEN=np.max(r_len)\nMAX_REVIEW_LEN","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nmax_features = 6000\nmax_words = 350\nbatch_size = 128\nepochs = 6\nnum_classes=1\n\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(x_train))\nx_train = tokenizer.texts_to_sequences(x_train)\nx_val = tokenizer.texts_to_sequences(x_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = sequence.pad_sequences(x_train, maxlen=max_words)\nx_val = sequence.pad_sequences(x_val, maxlen=max_words)\nx_test = tokenizer.texts_to_sequences(test['review'])\nx_test = sequence.pad_sequences(x_test, maxlen=max_words)\nprint(x_train.shape,x_val.shape,x_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n    \ndef get_embed_mat(EMBEDDING_FILE, max_features,embed_dim):\n    # word vectors\n    embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE, encoding='utf8'))\n    print('Found %s word vectors.' % len(embeddings_index))\n\n    # embedding matrix\n    word_index = tokenizer.word_index\n    num_words = min(max_features, len(word_index) + 1)\n    all_embs = np.stack(embeddings_index.values()) #for random init\n    embedding_matrix = np.random.normal(all_embs.mean(), all_embs.std(), \n                                        (num_words, embed_dim))\n    for word, i in word_index.items():\n        if i >= max_features:\n            continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n    max_features = embedding_matrix.shape[0]\n    \n    return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_FILE = '../input/glove6b/glove.6B.300d.txt'\nembed_dim = 300 #word vector dim\nembedding_matrix = get_embed_mat(EMBEDDING_FILE,max_features,embed_dim)\nprint(embedding_matrix.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(max_features, embed_dim, input_length=x_train.shape[1],weights=[embedding_matrix],trainable=True))\nmodel.add(SpatialDropout1D(0.25))\nmodel.add(Bidirectional(LSTM(128,return_sequences=True)))\nmodel.add(Bidirectional(LSTM(64,return_sequences=False)))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(x_train, y_train, validation_data=(x_val, y_val),epochs=epochs, batch_size=batch_size, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = model.predict(x_test)\ny_pred = (prediction > 0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score, confusion_matrix\nprint('F1-score: {0}'.format(f1_score(y_pred, y_test)))\nprint('Confusion matrix:')\nconfusion_matrix(y_pred, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t_loss = history.history['loss']\nv_loss = history.history['val_loss']\nepochs = range(1,len(t_loss)+1)\nplt.plot(epochs,t_loss, 'bo', label='Training loss')\nplt.plot(epochs,v_loss, 'r--', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t_acc = history.history['accuracy']\nv_acc = history.history['val_accuracy']\nplt.plot(epochs,t_acc,'bo',label='Training acc')\nplt.plot(epochs,v_acc,'r--', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = model.evaluate(x_val, y_val)\ntest_acc=np.round(results[1]*100,decimals=2)\n#test_loss=np.round(results[2]*100,decimals=2)\nprint('Test accuracy is',test_acc,'%')\n#print('Test loss is',test_loss,'%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.to_csv('result.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_df['Sentiment'] = y_test\n#test_df.drop(['review'],axis = 1,inplace = True)\n\n#test_df.to_csv('Submission.csv',index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}