{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Method 1: Bag of Words\n\n```\nWait Time: 0 seconds\nExecution Time: 0 seconds\nScore: 0.84736\n\n1. Load Data\n2. Clean Data\n3. Create Bag of Words (Feature Vectors)\n4. Train RandomForestClassifier and Predict\n```"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom bs4 import BeautifulSoup\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom tqdm import tqdm\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n\n# Method 2 Libraries\nfrom gensim.models.word2vec import Word2Vec\nfrom sklearn.cluster import KMeans\nimport time\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load Data\ntrain = pd.read_csv('../input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip', quoting=3, delimiter='\\t')\ntest = pd.read_csv('../input/word2vec-nlp-tutorial/testData.tsv.zip', quoting=3, delimiter='\\t')\nprint('Train Shape:', train.shape)\nprint('Test Shape:', test.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Clean Data\ndef review_to_words(raw_review):\n    review_text = BeautifulSoup(raw_review).get_text()           # remove HTML\n    letters_only = re.sub('[^a-zA-Z]', ' ', review_text)          # remove digits\n    words = letters_only.lower().split()                         # lower and split into array \n    stops = set(stopwords.words('english'))                      # get stops, transform into set for speed \n    meaningful_words = [w for w in words if w not in stops]      # get meaningful words without stops\n    return ' '.join(meaningful_words)                            # join meaningful words together\n\nnum_reviews = train.shape[0]\nclean_train_reviews = []\nclean_test_reviews = []\nfor i in tqdm(range(num_reviews)):\n    clean_train_reviews.append(review_to_words(train['review'][i]))\n    clean_test_reviews.append(review_to_words(test['review'][i]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Bag of Words\nvectorizer = CountVectorizer(max_features=5000)                        # initialize CountVectorizer\ntrain_data_features = vectorizer.fit_transform(clean_train_reviews)    # learns vocab & transforms words into feature vectors\ntrain_data_features = train_data_features.toarray()                    # convert to array for ease of use\n\ntest_data_features = vectorizer.transform(clean_test_reviews)          # test data\ntest_data_features = test_data_features.toarray()\n\nvocab = vectorizer.get_feature_names()\nprint('Train Features Shape:', train_data_features.shape)\nprint('Test Features Shape:', test_data_features.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train RandomForestClassifier and Predict\nforest = RandomForestClassifier(n_estimators = 100)           # initialize RandomForestClassifier with 100 trees\nforest = forest.fit(train_data_features, train['sentiment'])  # fit forest\nresult = forest.predict(test_data_features)                   # predict\noutput = pd.DataFrame(data={'id': test['id'], 'sentiment': result})\noutput.to_csv('Bag_of_Words_model.csv', index=False, quoting=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Method 2: Word2vec\n\n```\nWait Time: 0 seconds\nExecution Time: 0 seconds\nScore: \n\n1. Load Data\n2. Preprocess Data\n3. Initialize and Train Word2Vec Model\n4. Use Model to Predict\n```"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load Data\nlabeled_train = pd.read_csv('../input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip', quoting=3, delimiter='\\t')\nunlabeled_train = pd.read_csv('../input/word2vec-nlp-tutorial/unlabeledTrainData.tsv.zip', quoting=3, delimiter='\\t')\ntest = pd.read_csv('../input/word2vec-nlp-tutorial/testData.tsv.zip', quoting=3, delimiter='\\t')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocess Data\ndef review_to_wordlist(review, remove_stopwords=False, remove_numbers=False):   # keep stop words & numbers for Word2vec\n    review_text = BeautifulSoup(review).get_text()\n    if remove_numbers:\n        review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n    else:\n        review_text = re.sub(\"[^a-zA-Z0-9]\",\" \", review_text)\n    words = review_text.lower().split()\n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))\n        words = [w for w in words if not w in stops]\n    return(words)\n\n# Word2Vec expects a list of sentences, where each sentence is a list of words\ntokenizer = nltk.data.load('tokenizers/punkt/english.pickle')            # load punkt tokenizer\n\ndef review_to_sentences(review, tokenizer, remove_stopwords=False):\n    raw_sentences = tokenizer.tokenize(review.strip())     # get list of sentences\n    sentences = []\n    for raw_sentence in raw_sentences:\n        if len(raw_sentence) > 0:                          # get list of words \n            sentences.append(review_to_wordlist(raw_sentence, remove_stopwords))\n    return sentences\n\nsentences = []\nfor review in labeled_train['review']:                         # labeled train set\n    sentences += review_to_sentences(review, tokenizer)\nfor review in unlabeled_train['review']:                       # unlabeled train set\n    sentences += review_to_sentences(review, tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize and Train Model\nnum_features = 300    # Word vector dimensionality                      \nmin_word_count = 40   # Minimum word count                        \nnum_workers = 4       # Number of threads to run in parallel\ncontext = 10          # Context window size                                                                                    \ndownsampling = 1e-3   # Downsample setting for frequent words\n\nmodel = Word2Vec(sentences, workers=num_workers, size=num_features, \n                          min_count=min_word_count, window=context, sample=downsampling)\n\n# init_sims works only when the model will not be trained further, and is more memory-efficient.\nmodel.init_sims(replace=True)\n\n# Save Model\nmodel_name = \"300features_40minwords_10context\"\nmodel.save(model_name)\n\n# Exploring Results\nprint('Most different in france, england, germany, berlin:', model.doesnt_match(\"france england germany berlin\".split()))\nprint('Most similar to awful:', model.most_similar(\"awful\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use Model to Predict: Vector Averaging\n\n# Take word vectors and transform them into a feature set that is the same length for every review.\n# To combine the words in each review, we can average the word vectors in a review (removed stop words to remove extra noise)\n\ndef makeFeatureVec(words, model, num_features):\n    \"\"\"averages all word vectors in a paragraph\"\"\"\n    featureVec = np.zeros((num_features,), dtype=\"float32\")\n    nwords = 0\n    index2word_set = set(model.wv.index2word)           # a set of words in the model's vocabulary\n    for word in words:                               # if a word is in the vocaublary, add its feature vector to the total\n        if word in index2word_set: \n            nwords = nwords + 1\n            featureVec = np.add(featureVec, model[word])\n    featureVec = np.divide(featureVec,nwords)\n    return featureVec\n\ndef getAvgFeatureVecs(reviews, model, num_features):\n    # calculate the average feature vector for n reviews and return a 2D numpy array \n    counter = 0\n    reviewFeatureVecs = np.zeros((len(reviews), num_features), dtype=\"float32\")\n    for review in tqdm(reviews):\n       reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n       counter = counter + 1\n    return reviewFeatureVecs\n\n\n# Calculate average feature vectors for training and testing sets, using stop word removal.\nclean_train_reviews = []\nfor review in train[\"review\"]:\n    clean_train_reviews.append(review_to_wordlist(review, remove_stopwords=True))\ntrainDataVecs = getAvgFeatureVecs(clean_train_reviews, model, num_features)\n\nclean_test_reviews = []\nfor review in test[\"review\"]:\n    clean_test_reviews.append(review_to_wordlist(review, remove_stopwords=True))\ntestDataVecs = getAvgFeatureVecs(clean_test_reviews, model, num_features)\n\n\n# Use average paragraph vectors to train a random forest\nforest = RandomForestClassifier(n_estimators = 100)   # fit RandomForest of 100 trees\nforest = forest.fit(trainDataVecs, train[\"sentiment\"])\nresult = forest.predict(testDataVecs)                   # predict\n\noutput = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})\noutput.to_csv(\"Word2Vec_AverageVectors.csv\", index=False, quoting=3)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}