{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Review Sentiment Prediction\nIn this notebook, we will attempt to predict whether a review has a positive or a negative sentiment.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nfrom bs4 import BeautifulSoup\n\nimport nltk, re, torch\n\nfrom nltk.corpus import stopwords\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.tokenize import sent_tokenize\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split, cross_validate\nfrom sklearn.pipeline import Pipeline\n\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\n\nfrom zipfile import ZipFile","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-11T08:36:13.140658Z","iopub.execute_input":"2021-12-11T08:36:13.141332Z","iopub.status.idle":"2021-12-11T08:36:22.883397Z","shell.execute_reply.started":"2021-12-11T08:36:13.141167Z","shell.execute_reply":"2021-12-11T08:36:22.882538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with ZipFile(\"/kaggle/input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip\",\"r\") as file:\n    file.extractall(\"input\")","metadata":{"execution":{"iopub.status.busy":"2021-12-11T08:36:22.88527Z","iopub.execute_input":"2021-12-11T08:36:22.88557Z","iopub.status.idle":"2021-12-11T08:36:23.687809Z","shell.execute_reply.started":"2021-12-11T08:36:22.885523Z","shell.execute_reply":"2021-12-11T08:36:23.686909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"./input/labeledTrainData.tsv\",delimiter='\\t')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-11T08:36:23.689057Z","iopub.execute_input":"2021-12-11T08:36:23.689352Z","iopub.status.idle":"2021-12-11T08:36:24.083024Z","shell.execute_reply.started":"2021-12-11T08:36:23.689321Z","shell.execute_reply":"2021-12-11T08:36:24.081323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['sentiment'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-12-11T08:36:24.086079Z","iopub.execute_input":"2021-12-11T08:36:24.086596Z","iopub.status.idle":"2021-12-11T08:36:24.100719Z","shell.execute_reply.started":"2021-12-11T08:36:24.086552Z","shell.execute_reply":"2021-12-11T08:36:24.099927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset is perfectly balanced, so no need for data augmentation or oversampling.","metadata":{}},{"cell_type":"markdown","source":"## Vader Baseline\nA first attempt will be to use unsupervised sentiment classification from Vader as provided by nltk, to provide a baseline for our metrics.","metadata":{}},{"cell_type":"code","source":"analyzer = SentimentIntensityAnalyzer()\ndef predict_sentiment(review):\n    sentences = sent_tokenize(review)\n    scores = [analyzer.polarity_scores(sentence)['compound'] for sentence in sentences]\n    return 1 if pd.Series(scores).mean() > 0 else 0\n    \npredictions = df['review'].apply(predict_sentiment)\nprint(classification_report(df['sentiment'], predictions))","metadata":{"execution":{"iopub.status.busy":"2021-12-11T08:36:24.10328Z","iopub.execute_input":"2021-12-11T08:36:24.104045Z","iopub.status.idle":"2021-12-11T08:37:52.943455Z","shell.execute_reply.started":"2021-12-11T08:36:24.104001Z","shell.execute_reply":"2021-12-11T08:37:52.942553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Vader seems to perform pretty well! It seems to struggle a bit with positive reviews, it might be because the dataset is labeled positive on a rating >= 7/10, so there are many non-negative reviews classified as negative. We might be able to account for this by adjusting the classification threshold from 0 to a positive value, e.g. 0.3.","metadata":{}},{"cell_type":"markdown","source":"## Count Vectorization\nAnother attempt might be to use count vectorization for the reviews, with manual preprocessing and tokenization. We'll try Random Forest and Multinomial Naive Bayes","metadata":{}},{"cell_type":"code","source":"alphanum_re = re.compile(r\"\\W+\")\nstemmer =  nltk.stem.snowball.SnowballStemmer(\"english\")\nstop_words = set(stopwords.words(\"english\"))\n\ndef preprocess(review):\n    text = BeautifulSoup(review).get_text()\n    text = alphanum_re.sub(\" \", text)\n    return text\n\ndef tokenize(review):\n    tokens = nltk.tokenize.word_tokenize(review)\n    tokens = [stemmer.stem(token) for token in tokens if token not in stop_words]\n    return tokens\n\ndef evaluate(model):\n    results = cross_validate(\n        model, \n        df['review'], \n        df['sentiment'], \n        scoring=['precision_macro','recall_macro','accuracy']\n    )\n    return results","metadata":{"execution":{"iopub.status.busy":"2021-12-11T08:37:52.944814Z","iopub.execute_input":"2021-12-11T08:37:52.945205Z","iopub.status.idle":"2021-12-11T08:37:52.958437Z","shell.execute_reply.started":"2021-12-11T08:37:52.945143Z","shell.execute_reply":"2021-12-11T08:37:52.957735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will remove the HTML entities from the review text, as well as any non-word characters, and then tokenize and stem the reviews before passing them to our models.","metadata":{}},{"cell_type":"code","source":"vectorizer = CountVectorizer(preprocessor=preprocess, tokenizer=tokenize, min_df=10)\nrf_model = Pipeline([\n    ('bow', vectorizer),\n    ('cls', RandomForestClassifier())\n])\nresults = evaluate(rf_model)\npd.DataFrame(results).mean(axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T08:37:52.959506Z","iopub.execute_input":"2021-12-11T08:37:52.962369Z","iopub.status.idle":"2021-12-11T08:48:03.786766Z","shell.execute_reply.started":"2021-12-11T08:37:52.962318Z","shell.execute_reply":"2021-12-11T08:48:03.785343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer = CountVectorizer(preprocessor=preprocess, tokenizer=tokenize, min_df=10)\nnb_model = Pipeline([\n    ('bow', vectorizer),\n    ('cls', MultinomialNB())\n])\nresults = evaluate(rf_model)\npd.DataFrame(results).mean(axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T08:48:03.788873Z","iopub.execute_input":"2021-12-11T08:48:03.789154Z","iopub.status.idle":"2021-12-11T08:58:23.14568Z","shell.execute_reply.started":"2021-12-11T08:48:03.789109Z","shell.execute_reply":"2021-12-11T08:58:23.144748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can already see an improvement over the unsupervised classification. Random Forest seems to perform slightly better.","metadata":{}},{"cell_type":"markdown","source":"## BERT Classification\nNow we'll try fine-tuning BERT for our classification task","metadata":{}},{"cell_type":"code","source":"class ReviewDataSet(torch.utils.data.Dataset):\n\n    def __init__(self, X, y):\n        self.classes = y\n        reviews = X.apply(preprocess)\n        tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n        \n        self.items = [tokenizer(\n            review, \n            max_length = 128, \n            truncation=True, \n            padding='max_length',\n            return_tensors='pt'\n        ) for review in reviews]\n\n    def classes(self):\n        return self.classes\n\n    def __len__(self):\n        return len(self.items)\n\n    def __getitem__(self, idx):\n        return self.items[idx], self.classes.iloc[idx]","metadata":{"execution":{"iopub.status.busy":"2021-12-11T08:58:23.146765Z","iopub.execute_input":"2021-12-11T08:58:23.146956Z","iopub.status.idle":"2021-12-11T08:58:23.154663Z","shell.execute_reply.started":"2021-12-11T08:58:23.146932Z","shell.execute_reply":"2021-12-11T08:58:23.153768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_x, test_x, train_y, test_y = train_test_split(df['review'],df['sentiment'], shuffle=True, random_state=1)\ndataset = ReviewDataSet(train_x, train_y)\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T12:04:19.599405Z","iopub.execute_input":"2021-12-10T12:04:19.600001Z","iopub.status.idle":"2021-12-10T12:06:24.451632Z","shell.execute_reply.started":"2021-12-10T12:04:19.599961Z","shell.execute_reply":"2021-12-10T12:06:24.450861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is very slow on the CPU, so we'll do the computations on the GPU which is significantly faster","metadata":{}},{"cell_type":"code","source":"cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if cuda else \"cpu\")\n\nlog_interval = 10\nepochs = 3\n\nmodel = BertForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\",\n    num_labels = 2,\n    output_attentions = False,\n    output_hidden_states = False,\n)\nmodel.train()\n\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nif cuda:\n    model.cuda()\n    loss_fn.cuda()\n\nfor epoch in range(epochs):\n    running_loss = 0.0\n    \n    for i, batch in enumerate(dataloader):\n        inputs, labels = batch\n        \n        optimizer.zero_grad()\n        \n        mask = inputs['attention_mask'].to(device)\n        input_id = inputs['input_ids'].squeeze(1).to(device)\n        labels = labels.to(device)\n        \n        outputs = model(input_id,attention_mask=mask, labels=labels)\n        \n        loss = outputs[0]\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        if i % log_interval == log_interval - 1:\n            print(f\"Epoch {epoch+1} Batch {i+1} avg. loss: {running_loss/log_interval}\")\n            running_loss = 0.0","metadata":{"execution":{"iopub.status.busy":"2021-12-10T12:06:24.45582Z","iopub.execute_input":"2021-12-10T12:06:24.456032Z","iopub.status.idle":"2021-12-10T12:06:26.421981Z","shell.execute_reply.started":"2021-12-10T12:06:24.456001Z","shell.execute_reply":"2021-12-10T12:06:26.420695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}