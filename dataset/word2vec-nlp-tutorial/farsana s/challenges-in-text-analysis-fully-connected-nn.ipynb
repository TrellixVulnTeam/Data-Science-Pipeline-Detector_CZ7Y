{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <a>Sentiment Analysis with pre-trained Word2Vec model</a>\n\nContinuation from: https://www.kaggle.com/farsanas/are-you-ready-to-build-your-own-word-embedding\n\nThis session is divided into 2 part\n\n* part 1: Deploy our own Word enbedding\n* Part 2: Lets understand the challenges what happens when we work with Deep Neural Network for Test Analyais"},{"metadata":{},"cell_type":"markdown","source":"## <a>Part1</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import YouTubeVideo      \nYouTubeVideo('8iM5PdxBbWo')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a>Part2</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import YouTubeVideo      \nYouTubeVideo('k2-OkFHsIlk')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"## <a>Overview - Part1</a>\nIn this tutorial we'll do Sentiment analysis based on the concept of Word2Vec using our pre-trained model with unlabelled data where we've applied Word2Vec technique i.e representing a word with a dense vector of 50 numbers. The unlabelled data has 50000 IMDB movie reviews & we extracted some 28000+ unique words after doing some data preprocessing & applying Word2Vec technique with length of 50 numbers."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = pd.read_csv('../input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip',\n                 header=0, delimiter=\"\\t\", quoting=3)\n\nprint(df1.shape)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## About the data\n\nThe labelled data set contains 25000 reviews with label(Sentiment). The output column Sentiment consists of 2 categories[0 & 1].\n\n*0 -- Indicates negative sentiment * \n*1-- Indicates positive sentiment * "},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.iloc[10:15,:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a>Data Preprocessing</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#plit Data into Training and Test Data\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    df1['review'],\n    df1['sentiment'],\n    test_size=0.2, \n    random_state=42\n)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.tolist()[0:2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a>Build Tokenizer to get Number sequences for Each review</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.python.keras.preprocessing.text import Tokenizer\n\n#Vocab size\ntop_words = 10000\n\nt = Tokenizer(num_words=top_words)\nt.fit_on_texts(X_train.tolist())\n\n#Get the word index for each of the word in the review\nX_train = t.texts_to_sequences(X_train.tolist())\nX_test = t.texts_to_sequences(X_test.tolist())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train[0:2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t.word_index.items() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Pad sequences to make each review size equal Get the word index for each of the word in the review\n\nfrom tensorflow.python.keras.preprocessing import sequence\n\n\nmax_review_length = 300 \n\nX_train = sequence.pad_sequences(X_train,maxlen=max_review_length,padding='post') \nX_test = sequence.pad_sequences(X_test, maxlen=max_review_length, padding='post') \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a>Build Embedding Matrix from Pre-Trained Word2Vec model</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Install gensim\n!pip install gensim --quiet\n\n#Load pre-trained model\nimport gensim\nword2vec = gensim.models.Word2Vec.load('../input/w2v-model/word2vec movie-50.model')\n\n#Embedding Length  #our word vec  length is 50\nembedding_vector_length = word2vec.wv.vectors.shape[1]\n\nprint('Loaded word2vec model..')\nprint('Model shape: ', word2vec.wv.vectors.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Build matrix for current data\nembedding_matrix = np.zeros((top_words + 1, # Vocablury size + 1,, we add 1 to vocab size for padding\n                             embedding_vector_length))\nfor word, i in sorted(t.word_index.items(),key=lambda x:x[1]):  #\n    if i > top_words:\n        break\n    if word in word2vec.wv.vocab: #if word is there then quickly extract the embedding\n        embedding_vector = word2vec.wv[word]\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check embeddings for word 'great'\nembedding_matrix[t.word_index['great']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a> Build the Model - Part2 </a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dropout, Dense, Embedding, Flatten\n\n#Build a sequential model\nmodel1 = Sequential()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a>Add Embedding layer</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"model1.add(Embedding(top_words + 1,   \n                    embedding_vector_length,   \n                    input_length=max_review_length, \n                    weights=[embedding_matrix],  \n                    trainable=False)       \n         )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Flatten embedding layer output and flatten layers\nmodel1.add(Flatten())                                                             \nmodel1.add(Dense(200,activation='relu'))                                          \nmodel1.add(Dense(100,activation='relu'))\nmodel1.add(Dropout(0.5))                                                          \nmodel1.add(Dense(60,activation='relu'))\nmodel1.add(Dropout(0.4))\nmodel1.add(Dense(30,activation='relu'))\nmodel1.add(Dropout(0.3))\nmodel1.add(Dense(1,activation='sigmoid'))                                         \n\nmodel1.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#here we r training it \nmodel1.fit(X_train,y_train,\n          epochs=10,\n          batch_size=100,         \n          validation_data=(X_test, y_test))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}