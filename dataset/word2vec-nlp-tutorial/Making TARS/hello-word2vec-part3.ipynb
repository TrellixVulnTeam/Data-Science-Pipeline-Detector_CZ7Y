{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-03T14:32:13.867203Z","iopub.execute_input":"2021-11-03T14:32:13.867548Z","iopub.status.idle":"2021-11-03T14:32:13.90029Z","shell.execute_reply.started":"2021-11-03T14:32:13.867463Z","shell.execute_reply":"2021-11-03T14:32:13.899501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gensim.models import Word2Vec\n\nmodel = Word2Vec.load(\"/kaggle/input/transfer/300features_40minwords_10context\")","metadata":{"execution":{"iopub.status.busy":"2021-11-03T14:32:13.902067Z","iopub.execute_input":"2021-11-03T14:32:13.902397Z","iopub.status.idle":"2021-11-03T14:32:16.442877Z","shell.execute_reply.started":"2021-11-03T14:32:13.902357Z","shell.execute_reply":"2021-11-03T14:32:16.442079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(model.wv[\"queen\"])","metadata":{"execution":{"iopub.status.busy":"2021-11-03T14:32:16.4441Z","iopub.execute_input":"2021-11-03T14:32:16.444553Z","iopub.status.idle":"2021-11-03T14:32:16.454354Z","shell.execute_reply.started":"2021-11-03T14:32:16.444521Z","shell.execute_reply":"2021-11-03T14:32:16.453509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(model.wv)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T14:32:16.455601Z","iopub.execute_input":"2021-11-03T14:32:16.455859Z","iopub.status.idle":"2021-11-03T14:32:16.466603Z","shell.execute_reply.started":"2021-11-03T14:32:16.455828Z","shell.execute_reply":"2021-11-03T14:32:16.465735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_features = len(model.wv['queen'])","metadata":{"execution":{"iopub.status.busy":"2021-11-03T14:32:16.468844Z","iopub.execute_input":"2021-11-03T14:32:16.469215Z","iopub.status.idle":"2021-11-03T14:32:16.476849Z","shell.execute_reply.started":"2021-11-03T14:32:16.469179Z","shell.execute_reply":"2021-11-03T14:32:16.476139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def makeFeatureVec(words, model, num_features):\n    featureVec = np.zeros((num_features,),dtype='float32')\n    nwords = 0\n    index2word_set = set(model.wv.index_to_key)\n    for word in words:\n        if word in index2word_set:\n            nwords += 1\n            featureVec = np.add(featureVec, model.wv[word])\n            \n    featureVec = np.divide(featureVec, nwords)\n    return featureVec\n\ndef getAvgFeatureVecs(reviews, model, num_features):\n    counter = 0\n    reviewFeatureVecs = np.zeros((len(reviews), num_features), dtype=\"float32\")\n    for review in reviews:\n        if counter % 1000. == 0.:\n            print(\"Review %d of %d\" % (counter, len(reviews)))\n        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n        counter += 1\n    return reviewFeatureVecs","metadata":{"execution":{"iopub.status.busy":"2021-11-03T14:32:16.478576Z","iopub.execute_input":"2021-11-03T14:32:16.479206Z","iopub.status.idle":"2021-11-03T14:32:16.489331Z","shell.execute_reply.started":"2021-11-03T14:32:16.479164Z","shell.execute_reply":"2021-11-03T14:32:16.488548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip\",\n                    header=0, delimiter=\"\\t\", quoting=3)\ntest = pd.read_csv(\"/kaggle/input/word2vec-nlp-tutorial/testData.tsv.zip\",\n                   header=0, delimiter=\"\\t\", quoting=3)\nunlabeled_train = pd.read_csv(\"/kaggle/input/word2vec-nlp-tutorial/unlabeledTrainData.tsv.zip\",\n                              header=0, delimiter=\"\\t\", quoting=3)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T14:32:16.490611Z","iopub.execute_input":"2021-11-03T14:32:16.490987Z","iopub.status.idle":"2021-11-03T14:32:19.927817Z","shell.execute_reply.started":"2021-11-03T14:32:16.490947Z","shell.execute_reply":"2021-11-03T14:32:19.926811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from bs4 import BeautifulSoup\nimport re\nfrom nltk.corpus import stopwords\n\ndef review_to_wordlist( review, remove_stopwords=False ):\n    # Function to convert a document to a sequence of words,\n    # optionally removing stop words.  Returns a list of words.\n    #\n    # 1. Remove HTML\n    review_text = BeautifulSoup(review).get_text()\n    #  \n    # 2. Remove non-letters\n    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n    #\n    # 3. Convert words to lower case and split them\n    words = review_text.lower().split()\n    #\n    # 4. Optionally remove stop words (false by default)\n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))\n        words = [w for w in words if not w in stops]\n    #\n    # 5. Return a list of words\n    return(words)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T14:32:19.929017Z","iopub.execute_input":"2021-11-03T14:32:19.929238Z","iopub.status.idle":"2021-11-03T14:32:20.853389Z","shell.execute_reply.started":"2021-11-03T14:32:19.929213Z","shell.execute_reply":"2021-11-03T14:32:20.852506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_train_reviews = []\nfor review in train[\"review\"]:\n    clean_train_reviews.append(review_to_wordlist(review, remove_stopwords=True))\n    \ntrainDataVecs = getAvgFeatureVecs(clean_train_reviews, model, num_features)\nprint(\"Creating average feature vectors for test reviews\")\n    \nclean_test_reviews = []\nfor review in test[\"review\"]:\n    clean_test_reviews.append(review_to_wordlist(review, remove_stopwords=True))\n    \ntestDataVecs = getAvgFeatureVecs(clean_test_reviews, model, num_features)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T14:32:20.854638Z","iopub.execute_input":"2021-11-03T14:32:20.855152Z","iopub.status.idle":"2021-11-03T14:33:44.789131Z","shell.execute_reply.started":"2021-11-03T14:32:20.855096Z","shell.execute_reply":"2021-11-03T14:33:44.788146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nforest = RandomForestClassifier(n_estimators=100)\n\nprint(\"Fitting a random forest to labeled training data...\")\nforest = forest.fit(trainDataVecs, train[\"sentiment\"])\n\nresult = forest.predict(testDataVecs)\n\noutput = pd.DataFrame(data={\"id\": test[\"id\"], \"sentiment\": result})\noutput.to_csv(\"Word2Vec_AverageVectors.csv\", index=False, quoting=3)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T14:33:44.790319Z","iopub.execute_input":"2021-11-03T14:33:44.790556Z","iopub.status.idle":"2021-11-03T14:34:33.905754Z","shell.execute_reply.started":"2021-11-03T14:33:44.790527Z","shell.execute_reply":"2021-11-03T14:34:33.90486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.cluster import KMeans\nimport time\n\nstart = time.time()\n\nword_vectors = model.wv.vectors\nnum_clusters = 3298 # word_vectors.shape[0] / 5\n\nkmeans_clustering = KMeans(n_clusters = num_clusters)\nidx = kmeans_clustering.fit_predict(word_vectors)\n\nend = time.time()\nelapsed = end-start\nprint(\"Time taken for K Means clustering: \", elapsed, \"seconds.\")","metadata":{"execution":{"iopub.status.busy":"2021-11-03T14:34:33.907129Z","iopub.execute_input":"2021-11-03T14:34:33.907455Z","iopub.status.idle":"2021-11-03T14:54:53.121695Z","shell.execute_reply.started":"2021-11-03T14:34:33.907405Z","shell.execute_reply":"2021-11-03T14:54:53.121001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_centroid_map = dict(zip(model.wv.index_to_key, idx))","metadata":{"execution":{"iopub.status.busy":"2021-11-03T14:54:53.125866Z","iopub.execute_input":"2021-11-03T14:54:53.126443Z","iopub.status.idle":"2021-11-03T14:54:53.13351Z","shell.execute_reply.started":"2021-11-03T14:54:53.126411Z","shell.execute_reply":"2021-11-03T14:54:53.132592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(word_centroid_map.values())","metadata":{"execution":{"iopub.status.busy":"2021-11-03T15:10:27.762217Z","iopub.execute_input":"2021-11-03T15:10:27.762855Z","iopub.status.idle":"2021-11-03T15:10:27.76845Z","shell.execute_reply.started":"2021-11-03T15:10:27.762818Z","shell.execute_reply":"2021-11-03T15:10:27.767873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_centroid_map_values = list(word_centroid_map.values())\nword_centroid_map_keys = list(word_centroid_map.keys())\n\nfor cluster in np.arange(0, 3):\n    print(\"\\nCluster %d\" % cluster)\n    words = []\n    for i in np.arange(0, len(word_centroid_map_values)):\n        if(word_centroid_map_values[i] == cluster):\n            words.append(word_centroid_map_keys[i])\n    print(words)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T15:12:04.0404Z","iopub.execute_input":"2021-11-03T15:12:04.040688Z","iopub.status.idle":"2021-11-03T15:12:04.144272Z","shell.execute_reply.started":"2021-11-03T15:12:04.040661Z","shell.execute_reply":"2021-11-03T15:12:04.143433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_bag_of_centroids(wordlist, word_centroid_map):\n    num_centroids = max(word_centroid_map.values()) + 1\n    bag_of_centroids = np.zeros(num_centroids, dtype='float32')\n    \n    for word in wordlist:\n        if word in word_centroid_map:\n            index = word_centroid_map[word]\n            bag_of_centroids[index] += 1\n            \n    return bag_of_centroids","metadata":{"execution":{"iopub.status.busy":"2021-11-03T15:12:09.313811Z","iopub.execute_input":"2021-11-03T15:12:09.314501Z","iopub.status.idle":"2021-11-03T15:12:09.320648Z","shell.execute_reply.started":"2021-11-03T15:12:09.314459Z","shell.execute_reply":"2021-11-03T15:12:09.319813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_centroids = np.zeros((train['review'].size, num_clusters), dtype='float32')\n\ncounter = 0\nfor review in clean_train_reviews:\n    train_centroids[counter] = create_bag_of_centroids(review, word_centroid_map)\n    counter += 1\n    \ntest_centroids = np.zeros((test['review'].size, num_clusters), dtype='float32')\n\ncounter = 0\nfor review in clean_test_reviews:\n    test_centroids[counter] = create_bag_of_centroids(review, word_centroid_map)\n    counter += 1","metadata":{"execution":{"iopub.status.busy":"2021-11-03T15:12:11.181885Z","iopub.execute_input":"2021-11-03T15:12:11.182783Z","iopub.status.idle":"2021-11-03T15:12:56.44845Z","shell.execute_reply.started":"2021-11-03T15:12:11.182724Z","shell.execute_reply":"2021-11-03T15:12:56.4475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"forest = RandomForestClassifier(n_estimators=100)\nprint(\"Fitting a random forest to labeled training data...\")\n\nforest = forest.fit(train_centroids, train[\"sentiment\"])\nresult = forest.predict(test_centroids)\n\noutput = pd.DataFrame(data={'id': test['id'], 'sentiment': result})\noutput.to_csv('BagOfCentroids.csv', index=False, quoting=3)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T15:12:56.449895Z","iopub.execute_input":"2021-11-03T15:12:56.450153Z","iopub.status.idle":"2021-11-03T15:13:47.407986Z","shell.execute_reply.started":"2021-11-03T15:12:56.450123Z","shell.execute_reply":"2021-11-03T15:13:47.407357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}