{"cells":[{"metadata":{"_uuid":"f1b8e004dfdf6bbb6eb6ed32e723316f4091a1f0"},"cell_type":"markdown","source":"## Bag of Words Meets Bags of Popcorn\n\n# 튜토리얼 파트 2 Word Vectors\n\n* 딥러닝 기법인 Word2Vec을 통해 단어를 벡터화 해본다. \n* t-SNE를 통해 벡터화 한 데이터를 시각화 해본다. (차원 축소)\n* 딥러닝과 지도학습의 랜덤포레스트를 사용하는 하이브리드 방식을 사용한다.\n\n##  Word Embedding이란?\n* Word Embedding은 텍스트를 숫자로 변환한 것으로 동일한 텍스트에 대해 다른 숫자로 표현.\n\n## Word Embedding이 필요한 이유는 무엇입니까?\n* 머신러닝 알고리즘과 거의 모든 딥러닝 아키텍처는 원시 형식의 문자열이나 일반 텍스트를 처리 할 수 없음\n* 머신러닝과 딥러닝 아키텍처에서 분류, 회귀 등 넓은 의미의 직업을 수행하기 위해 입력으로 숫자 필요  \n* 텍스트 형식으로 제공되는 방대한 양의 데이터에서 지식을 추출하기 위한 응용 프로그램을 작성하는 것이 중요\n* 텍스트 응용 프로그램의 실제 응용 프로그램에는 Amazon 등의 리뷰 감정 분석, 문서 또는 뉴스 분류 또는 Google의 클러스터링 등이 있음.\n* 기본적으로 컴퓨터가 어떤 단어에 대해 인지할 수 있게 하기 위해서는 수치적인 방식으로 단어를 표현할 수 있어야 한다. 그러나 앞서 말했듯이, 수치화를 통해 단어의 개념적 차이를 나타내기가 근본적으로 힘들었다. 이 한계를 극복하기 전의 NLP는 ‘one-hot encoding’ 방식을 많이 사용했고 Naive Bayes를 이용한 스팸 분류기가 이 방법을 사용하는 전형적인 예시라고 생각할 수 있겠다. 단어 자체를 벡터화한 것은 아니지만, 이 경우 이메일 전체를 보면서 어떤 단어가 있으면 1, 없으면 0으로 나타내는 식으로 이메일 하나에 대한 벡터를 만든다. 이 방식은 당시에는 나름 좋은 성능을 내었고, 지금까지도 사용하는 사람들이 있지만 컴퓨터 자체가 ‘단어가 본질적으로 다른 단어와 어떤 차이점을 가지는 지 이해할 수가 없다’ 는 아주 큰 단점이 존재한다.\n\n ! [WORD EMBEDDING](https://dreamgonfly.github.io/machine/learning,/natural/language/processing/2017/08/16/word2vec_explained.html)\n \n ! [WORD EMBEDDING 활용](https://brunch.co.kr/@goodvc78/7)\n ## Word2Vec(Word Embedding to Vector)\n\n컴퓨터는 숫자만 인식할 수 있고 한글, 이미지는 바이너리 코드로 저장 된다.\n튜토리얼 파트1에서는  Bag of Word라는 개념을 사용해서 문자를 벡터화 하여 머신러닝 알고리즘이 이해할 수 있도록 벡터화 해주는 작업을 하였다.\n\n\n* one hot encoding(예 [0000001000]) 혹은 Bag of Word에서 vector size가 매우 크고 sparse 하므로 neural net 성능이 잘 나오지 않는다.\n* `주위 단어가 비슷하면 해당 단어의 의미는 유사하다` 라는 아이디어\n* 단어를 트레이닝 시킬 때 주위 단어를 label로 매치하여 최적화\n* 단어를 `의미를 내포한 dense vector`로 매칭 시키는 것\n\n* Word2Vec은 분산 된 텍스트 표현을 사용하여 개념 간 유사성을 본다. 예를 들어, 파리와 프랑스가 베를린과 독일이 (수도와 나라) 같은 방식으로 관련되어 있음을 이해한다.\n\n![word2vec](https://1.bp.blogspot.com/-Q7F8ulD6fC0/UgvnVCSGmXI/AAAAAAAAAbg/MCWLTYBufhs/s1600/image00.gif)\n이미지 출처 : https://opensource.googleblog.com/2013/08/learning-meaning-behind-words.html\n\n* 단어의 임베딩과정을 실시간으로 시각화 : [word embedding visual inspector](https://ronxin.github.io/wevi/)\n\n\n![CBOW와 Skip-Gram](https://i.imgur.com/yXY1LxV.png)\n출처 : https://arxiv.org/pdf/1301.3781.pdf\n Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed Representations of Words and Phrases and their Compositionality. In Proceedings of NIPS, 2013.\n\n\n* CBOW와 Skip-Gram기법이 있다.\n\n    * CBOW(continuous bag-of-words)는 전체 텍스트로 하나의 단어를 예측하기 때문에 작은 데이터셋일 수록 유리하다.    \n    * 아래 예제에서 __ 에 들어갈 단어를 예측한다.\n<pre>\n1) __가 맛있다. \n2) __를 타는 것이 재미있다. \n3) 평소보다 두 __로 많이 먹어서 __가 아프다.\n</pre>\n\n    * Skip-Gram은 타겟 단어들로부터 원본 단어를 역으로 예측하는 것이다. CBOW와는 반대로 컨텍스트-타겟 쌍을 새로운 발견으로 처리하고 큰 규모의 데이터셋을 가질 때 유리하다.\n    * `배`라는 단어 주변에 올 수 있는 단어를 예측한다.\n    \n    <pre>\n    1) *배*가 맛있다. \n    2) *배*를 타는 것이 재미있다. \n    3) 평소보다 두 *배*로 많이 먹어서 *배*가 아프다.\n    </pre>\n\n\n\n## Word2Vec 참고자료\n\n* [word2vec 모델 · 텐서플로우 문서 한글 번역본](https://tensorflowkorea.gitbooks.io/tensorflow-kr/g3doc/tutorials/word2vec/)\n* [Word2Vec으로 문장 분류하기 · ratsgo's blog](https://ratsgo.github.io/natural%20language%20processing/2017/03/08/word2vec/)\n\n* [Efficient Estimation of Word Representations in\nVector Space](https://arxiv.org/pdf/1301.3781v3.pdf)\n* [Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n* [CS224n: Natural Language Processing with Deep Learning](http://web.stanford.edu/class/cs224n/syllabus.html)\n* [Word2Vec Tutorial - The Skip-Gram Model · Chris McCormick](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n\n## Gensim\n\n* [gensim: models.word2vec – Deep learning with word2vec](https://radimrehurek.com/gensim/models/word2vec.html)\n* [gensim: Tutorials](https://radimrehurek.com/gensim/tutorial.html)\n* [한국어와 NLTK, Gensim의 만남 - PyCon Korea 2015](https://www.lucypark.kr/docs/2015-pyconkr/)\n\n\n"},{"metadata":{"trusted":true,"_uuid":"c1d2aa05bd1d5694d2d95b197c67c927251c0f0f"},"cell_type":"code","source":"import os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8c850812fc0599766d9f132e93a711f6ea7ad4c"},"cell_type":"code","source":"# 출력이 너무 길어지지 않게하기 위해 찍지 않도록 했으나 \n# 실제 학습 할 때는 아래 두 줄을 주석처리 하는 것을 권장한다.\n# import warnings\n# warnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"807e5939dbab0eb49f6369ee658f2167103ff62a"},"cell_type":"code","source":"import pandas as pd\n\ntrain = pd.read_csv('../input/word2vec-nlp-tutorial/labeledTrainData.tsv', \n                    header=0, delimiter='\\t', quoting=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c783d859dfd270fad1851c4fa1d88687fba8f064"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2c0a3de36502e78f5eb21821578947d2519e318"},"cell_type":"code","source":"train.iloc[0,2][:700]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6de77a0dbaf9bf17665efc0d4ebe0849f2419e41"},"cell_type":"code","source":"train['review'][0][:700]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b2cf88ae4d4461e04b86671e5d6423066b3a15ae"},"cell_type":"code","source":"test = pd.read_csv('../input/word2vec-nlp-tutorial/testData.tsv', \n                   header=0, delimiter='\\t', quoting=3)\nunlabeled_train = pd.read_csv('../input/word2vec-nlp-tutorial/unlabeledTrainData.tsv', \n                              header=0, delimiter='\\t', quoting=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b2cf88ae4d4461e04b86671e5d6423066b3a15ae"},"cell_type":"code","source":"print(train.shape)\nprint(test.shape)\nprint(unlabeled_train.shape)\n\nprint(train['review'].size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49f792d97be58816011194a68a42b9f023fadca7"},"cell_type":"code","source":"import missingno as msno","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aea988877e348749eee10af92af0a7e1078789fc"},"cell_type":"code","source":"train['review'].isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b2cf88ae4d4461e04b86671e5d6423066b3a15ae"},"cell_type":"code","source":"print(train.shape)\nprint(test.shape)\nprint(unlabeled_train.shape)\n\nprint(train['review'].size)\nprint(test['review'].size)\nprint(unlabeled_train['review'].size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1b4d1131366cef97857661819e5bcf1d0230142"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7a2563fb964603fbfaae8dc577e2ebec462d021"},"cell_type":"code","source":"# train에 있는 평점정보인 sentiment가 없다.\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c31bf7b1d83fe6d8074b4fc6e74de551b45e91a"},"cell_type":"code","source":"# import module we'll need to import our custom module\nfrom shutil import copyfile","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c31bf7b1d83fe6d8074b4fc6e74de551b45e91a"},"cell_type":"code","source":"# copy our file into the working directory (make sure it has .py suffix)\ncopyfile(src = \"../input/kaggleword2vecutility/KaggleWord2VecUtility.py\", dst = \"../working/KaggleWord2VecUtility.py\")\n\n# import all our functions\n#from my_functions import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c369c393612323efd537be6fec9c1322ee873f95"},"cell_type":"code","source":"from KaggleWord2VecUtility import KaggleWord2VecUtility","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ebd2b4f7fcfb6445374cd98f170d5a810aa104f"},"cell_type":"code","source":"# train['review'][0]\n# mj 리뷰\n# 'review의 첫번째 데이터'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"31ad08f9f2bed35b5c0b1118d703c02b4591e6ac"},"cell_type":"code","source":"\"\"\"import re\nimport nltk\n\nimport pandas as pd\nimport numpy as np\n\nfrom bs4 import BeautifulSoup\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\n\nfrom multiprocessing import Pool\n\nclass KaggleWord2VecUtility(object):\n\n    @staticmethod\n    def review_to_wordlist(review, remove_stopwords=False):\n        # 1. HTML 제거\n        review_text = BeautifulSoup(review, \"html.parser\").get_text()\n        # 2. 특수문자를 공백으로 바꿔줌\n        review_text = re.sub('[^a-zA-Z]', ' ', review_text)\n        # 3. 소문자로 변환 후 나눈다.\n        words = review_text.lower().split()\n        # 4. 불용어 제거\n        if remove_stopwords:\n            stops = set(stopwords.words('english'))\n            words = [w for w in words if not w in stops]\n        # 5. 어간추출\n        stemmer = SnowballStemmer('english')\n        words = [stemmer.stem(w) for w in words]\n        # 6. 리스트 형태로 반환\n        return(words)\n\n    @staticmethod\n    def review_to_join_words( review, remove_stopwords=False ):\n        words = KaggleWord2VecUtility.review_to_wordlist(\\\n            review, remove_stopwords=False)\n        join_words = ' '.join(words)\n        return join_words\n\n    @staticmethod\n    def review_to_sentences( review, remove_stopwords=False ):\n        # punkt tokenizer를 로드한다.\n        이 때, pickle을 사용하는데\n        pickle을 통해 값을 저장하면 원래 변수에 연결 된 참조값 역시 저장된다.\n        저장된 pickle을 다시 읽으면 변수에 연결되었던\n        모든 레퍼런스가 계속 참조 상태를 유지한다.\n   \n        tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n        # 1. nltk tokenizer를 사용해서 단어로 토큰화 하고 공백 등을 제거한다.\n        raw_sentences = tokenizer.tokenize(review.strip())\n        #review.()을 하면 띄어 쓰기 단위로 자름\n        #그것을 to\n        # 2. 각 문장을 순회한다.\n        sentences = []\n        for raw_sentence in raw_sentences:\n            # 비어있다면 skip\n            if len(raw_sentence) > 0:\n                # 태그제거, 알파벳문자가 아닌 것은 공백으로 치환, 불용어제거\n                sentences.append(\\\n                    KaggleWord2VecUtility.review_to_wordlist(\\\n                    raw_sentence, remove_stopwords))\n        return sentences\n\n\n    # 참고 : https://gist.github.com/yong27/7869662\n    # http://www.racketracer.com/2016/07/06/pandas-in-parallel/\n    # 속도 개선을 위해 멀티 스레드로 작업하도록\n    @staticmethod\n    def _apply_df(args):\n        df, func, kwargs = args\n        return df.apply(func, **kwargs)\n\n    @staticmethod\n    def apply_by_multiprocessing(df, func, **kwargs):\n        # 키워드 항목 중 workers 파라메터를 꺼냄\n        workers = kwargs.pop('workers')\n        # 위에서 가져온 workers 수로 프로세스 풀을 정의\n        pool = Pool(processes=workers)\n        # 실행할 함수와 데이터프레임을 워커의 수 만큼 나눠 작업\n        result = pool.map(KaggleWord2VecUtility._apply_df, [(d, func, kwargs)\n                for d in np.array_split(df, workers)])\n        pool.close()\n        # 작업 결과를 합쳐서 반환\n        return pd.concat(result)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d642f72dfabb28c9163013f26aefec9e8bb05d9f","scrolled":false},"cell_type":"code","source":"KaggleWord2VecUtility.review_to_wordlist(train['review'][0])[:10]\n#html tag 지우고\n#특수문자 지우고\n#소문자로 변환하고\n#기본적인 영어 불용어 지우고\n#어간추출(Stemming)하고\n#리스트 형태로 반환\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"20776499e0971eae32033f3c31cf7e150a5673d0"},"cell_type":"code","source":"sentences = []\nfor review in train[\"review\"]:\n    sentences += KaggleWord2VecUtility.review_to_sentences(\n        review, remove_stopwords=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"69add9939f6e85cb3c1f3f11fbe06b79bc0a4be2"},"cell_type":"code","source":"for review in unlabeled_train[\"review\"]:\n    sentences += KaggleWord2VecUtility.review_to_sentences(\n        review, remove_stopwords=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3f5aaa5f608fcdac10b6d2aec4465e5441bb22af"},"cell_type":"code","source":"len(sentences)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2748eb1089cecf13b5bac41c6a02769402d8e56b"},"cell_type":"code","source":"sentences[0][:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"29a8053e356955be58b96439be825da2e2ba7827"},"cell_type":"code","source":"sentences[1][:10]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"28eab3fc44b1f67392f357021ce3b57852324e2a"},"cell_type":"markdown","source":"### Word2Vec 모델을 학습\n전처리를 거쳐 파싱된 문장의 목록으로 모델을 학습시킬 준비가 되었다.\n\n## Gensim\n* [gensim: models.word2vec – Deep learning with word2vec](https://radimrehurek.com/gensim/models/word2vec.html)\n\n### Word2Vec 모델의 파라메터\n\n* 아키텍처 : 아키텍처 옵션은 skip-gram (default) 또는  CBOW 모델이다. skip-gram (default)은 느리지 만 더 나은 결과를 낸다.\n\n* 학습 알고리즘 : Hierarchical softmax (default) 또는 negative 샘플링. 여기에서는 기본값이 잘 동작한다.\n\n* 빈번하게 등장하는 단어에 대한 다운 샘플링 : Google 문서는 .00001에서 .001 사이의 값을 권장한다. 여기에서는 0.001에 가까운 값이 최종 모델의 정확도를 높이는 것으로 보여진다.\n\n* 단어 벡터 차원 : 많은 feature를 사용한다고 항상 좋은 것은 아니지만 대체적으로 좀 더 나은 모델이 된다. 합리적인 값은 수십에서 수백 개가 될 수 있고 여기에서는 300으로 지정했다.\n\n* 컨텍스트 / 창 크기 : 학습 알고리즘이 고려해야하는 컨텍스트의 단어 수는 얼마나 될까? hierarchical softmax 를 위해 좀 더 큰 수가 좋지만 10 정도가 적당하다. \n\n* Worker threads : 실행할 병렬 프로세스의 수로 컴퓨터마다 다르지만 대부분의 시스템에서 4에서 6 사이의 값을 사용하다.\n\n* 최소 단어 수 : 어휘의 크기를 의미있는 단어로 제한하는 데 도움이 된다. 모든 문서에서이 여러 번 발생하지 않는 단어는 무시된다. 10에서 100 사이가 적당하며, 이 경진대회의 데이터는 각 영화가 30개씩의 리뷰가 있기 때문에 개별 영화 제목에 너무 많은 중요성이 붙는 것을 피하기 위해 최소 단어 수를 40으로 설정한다. 그 결과 전체 어휘 크기는 약 15,000 단어가 된다. 높은 값은 제한 된 실행시간에 도움이 된다."},{"metadata":{"trusted":false,"_uuid":"ed66ca599882bbe4369b138e9099f15ebbdd2c04"},"cell_type":"code","source":"import logging\nlogging.basicConfig(\n    format='%(asctime)s : %(levelname)s : %(message)s', \n    level=logging.INFO)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"71f4676c1e796e70821f5533f0c069e1b9a81b0b"},"cell_type":"code","source":"# 파라메터값 지정\nnum_features = 300 # 문자 벡터 차원 수\nmin_word_count = 40 # 최소 문자 수\nnum_workers = 4 # 병렬 처리 스레드 수\ncontext = 10 # 문자열 창 크기\ndownsampling = 1e-3 # 문자 빈도 수 Downsample\n\n# 초기화 및 모델 학습\nfrom gensim.models import word2vec\n\n# 모델 학습\nmodel = word2vec.Word2Vec(sentences, \n                          workers=num_workers, \n                          size=num_features, \n                          min_count=min_word_count,\n                          window=context,\n                          sample=downsampling)\nmodel","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8231850c15452ea6b6ce26837437bb725979aa03"},"cell_type":"code","source":"# 학습이 완료 되면 필요없는 메모리를 unload 시킨다.\nmodel.init_sims(replace=True)\n\nmodel_name = '300features_40minwords_10text'\n# model_name = '300features_50minwords_20text'\nmodel.save(model_name)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"86b4e85a6166db7c2a8a7b019f6ee75106a9b574"},"cell_type":"markdown","source":"## 모델 결과 탐색 \nExploring the Model Results"},{"metadata":{"trusted":false,"_uuid":"329fb9c048fcaa527a74389b98b9273b6c58050b"},"cell_type":"code","source":"# 유사도가 없는 단어 추출\nmodel.wv.doesnt_match('man woman child kitchen'.split())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a4be3120c9facb76787a8c1e6a61bb60adeddb82"},"cell_type":"code","source":"model.wv.doesnt_match(\"france england germany berlin\".split())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"29400b8a942063e8178de53eb88a7d8bc203f4ad"},"cell_type":"code","source":"# 가장 유사한 단어를 추출\nmodel.wv.most_similar(\"man\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"78258786653a856580884c9dc4af8ed86a5b368a"},"cell_type":"code","source":"model.wv.most_similar(\"queen\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5c7451b09a5eac78087ec8a6709f1ba04ce79b43"},"cell_type":"code","source":"# model.wv.most_similar(\"awful\")","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"7d4f3dcff95ec34805f0f0fd29f52573d00bea1a"},"cell_type":"code","source":"model.wv.most_similar(\"film\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"cd347a3547823001dd1c061dd001a83ac359ce8c"},"cell_type":"code","source":"# model.wv.most_similar(\"happy\")\nmodel.wv.most_similar(\"happi\") # stemming 처리 시 ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5036b966a43e5be06835cf387dc7961a7901aa3e"},"cell_type":"markdown","source":"### Word2Vec으로 벡터화 한 단어를 t-SNE 를 통해 시각화"},{"metadata":{"trusted":false,"_uuid":"b71aad13f34085d4634fad080d2e1fcadabe94a6"},"cell_type":"code","source":"# 참고 https://stackoverflow.com/questions/43776572/visualise-word2vec-generated-from-gensim\nfrom sklearn.manifold import TSNE\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport gensim \nimport gensim.models as g\n\n# 그래프에서 마이너스 폰트 깨지는 문제에 대한 대처\nmpl.rcParams['axes.unicode_minus'] = False\n\nmodel_name = '300features_40minwords_10text'\nmodel = g.Doc2Vec.load(model_name)\n\nvocab = list(model.wv.vocab)\nX = model[vocab]\n\nprint(len(X))\nprint(X[0][:10])\ntsne = TSNE(n_components=2)\n\n# 100개의 단어에 대해서만 시각화\nX_tsne = tsne.fit_transform(X[:100,:])\n# X_tsne = tsne.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"65387cdc3c7c0f595343becff8aa1a056bebaeeb"},"cell_type":"code","source":"df = pd.DataFrame(X_tsne, index=vocab[:100], columns=['x', 'y'])\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0c9a90448ff6146387fe185c2dec597f69ba7213"},"cell_type":"code","source":"df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"173e49a2afa63bae69cd8bfbe0791f2e0e773df4"},"cell_type":"code","source":"fig = plt.figure()\nfig.set_size_inches(40, 20)\nax = fig.add_subplot(1, 1, 1)\n\nax.scatter(df['x'], df['y'])\n\nfor word, pos in df.iterrows():\n    ax.annotate(word, pos, fontsize=30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"49331886c4acdbb3da94e233d0f3886fb58b2460"},"cell_type":"code","source":"import numpy as np\n\ndef makeFeatureVec(words, model, num_features):\n    \"\"\"\n    주어진 문장에서 단어 벡터의 평균을 구하는 함수\n    \"\"\"\n    # 속도를 위해 0으로 채운 배열로 초기화 한다.\n    featureVec = np.zeros((num_features,),dtype=\"float32\")\n\n    nwords = 0.\n    # Index2word는 모델의 사전에 있는 단어명을 담은 리스트이다.\n    # 속도를 위해 set 형태로 초기화 한다.\n    index2word_set = set(model.wv.index2word)\n    # 루프를 돌며 모델 사전에 포함이 되는 단어라면 피처에 추가한다.\n    for word in words:\n        if word in index2word_set:\n            nwords = nwords + 1.\n            featureVec = np.add(featureVec,model[word])\n    # 결과를 단어수로 나누어 평균을 구한다.\n    featureVec = np.divide(featureVec,nwords)\n    return featureVec","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"865dfa8794e5c17381962ae3dd130a71b22136eb"},"cell_type":"code","source":"def getAvgFeatureVecs(reviews, model, num_features):\n    # 리뷰 단어 목록의 각각에 대한 평균 feature 벡터를 계산하고 \n    # 2D numpy 배열을 반환한다.\n    \n    # 카운터를 초기화 한다.\n    counter = 0.\n    # 속도를 위해 2D 넘파이 배열을 미리 할당한다.\n    reviewFeatureVecs = np.zeros(\n        (len(reviews),num_features),dtype=\"float32\")\n    \n    for review in reviews:\n       # 매 1000개 리뷰마다 상태를 출력\n       if counter%1000. == 0.:\n           print(\"Review %d of %d\" % (counter, len(reviews)))\n       # 평균 피처 벡터를 만들기 위해 위에서 정의한 함수를 호출한다.\n       reviewFeatureVecs[int(counter)] = makeFeatureVec(review, model, \\\n           num_features)\n       # 카운터를 증가시킨다.\n       counter = counter + 1.\n    return reviewFeatureVecs","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"028f34abc1323213958dc8d866a8ae642d982c9e"},"cell_type":"code","source":"# 멀티스레드로 4개의 워커를 사용해 처리한다.\ndef getCleanReviews(reviews):\n    clean_reviews = []\n    clean_reviews = KaggleWord2VecUtility.apply_by_multiprocessing(\\\n        reviews[\"review\"], KaggleWord2VecUtility.review_to_wordlist,\\\n        workers=4)\n    return clean_reviews","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1048da5763cb639fd706a0f7bf099d1f2e4f5127"},"cell_type":"code","source":"%time trainDataVecs = getAvgFeatureVecs(\\\n    getCleanReviews(train), model, num_features ) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"41620307fe3c4c798add6765edf38117ede99d28"},"cell_type":"code","source":"%time testDataVecs = getAvgFeatureVecs(\\\n        getCleanReviews(test), model, num_features )","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2c333077da4fd0336530cc30eb150497e5425aa0"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nforest = RandomForestClassifier(\n    n_estimators = 100, n_jobs = -1, random_state=2018)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"89395f3d7fb217ee119af043adabdc85d4919c89"},"cell_type":"code","source":"%time forest = forest.fit( trainDataVecs, train[\"sentiment\"] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b90691cd14b83a054bf0a5da4485c8bbc134fcea"},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n%time score = np.mean(cross_val_score(\\\n    forest, trainDataVecs, \\\n    train['sentiment'], cv=10, scoring='roc_auc'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"249c002db33b3d4e287474cfb186c436b6528e44"},"cell_type":"code","source":"score","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7c626d1e27113a6819ba8286ecdc3c73cf89b9ef"},"cell_type":"code","source":"result = forest.predict( testDataVecs )","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"12980439813a0e4c6e925ad6ba3156e6b75d3248"},"cell_type":"code","source":"output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\noutput.to_csv('data/Word2Vec_AverageVectors_{0:.5f}.csv'.format(score), \n              index=False, quoting=3 )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ef2460bf774ddb1af9ac252df3f5041a5ff0fc6"},"cell_type":"markdown","source":"* 300features_40minwords_10text 일 때 0.90709436799999987\n* 300features_50minwords_20text 일 때 0.86815798399999999"},{"metadata":{"trusted":false,"_uuid":"ec5d48bdc453ae285867b12d671cccd49ffed14d"},"cell_type":"code","source":"output_sentiment = output['sentiment'].value_counts()\nprint(output_sentiment[0] - output_sentiment[1])\noutput_sentiment","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"173d96cab55ac77e9971a3d97df5a59ee4db031d"},"cell_type":"code","source":"import seaborn as sns \n%matplotlib inline\n\nfig, axes = plt.subplots(ncols=2)\nfig.set_size_inches(12,5)\nsns.countplot(train['sentiment'], ax=axes[0])\nsns.countplot(output['sentiment'], ax=axes[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6913f465bc2af6e89e0c46d99521d5d6a3af840a"},"cell_type":"code","source":"544/578","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6383441a7fd18c870ce41e731bc57903de1df161"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}