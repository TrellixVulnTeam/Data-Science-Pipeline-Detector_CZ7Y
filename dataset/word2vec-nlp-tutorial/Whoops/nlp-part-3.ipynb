{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport nltk\nimport time\nfrom nltk.corpus import stopwords\nimport nltk.data\nfrom gensim.models import word2vec\nfrom gensim.models import Word2Vec\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom bs4 import BeautifulSoup\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.cluster import KMeans\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv( \"../input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip\", \\\n                    header=0, delimiter=\"\\t\", quoting=3 )\ntest = pd.read_csv( \"../input/word2vec-nlp-tutorial/testData.tsv.zip\", \\\n                   header=0, delimiter=\"\\t\", quoting=3 )\nunlabeled_train = pd.read_csv( \"../input/word2vec-nlp-tutorial/unlabeledTrainData.tsv.zip\", \\\n                              header=0, delimiter=\"\\t\", quoting=3 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def review_to_wordlist (review, remove_stopwords=False):\n    # 1. Remove HTML\n    review_text=BeautifulSoup(review).get_text()\n    # 2. Remove non-letters\n    review_text=re.sub( \"[^a-zA-Z]\",\" \", review_text)\n    # 3. convert to small letters and split to words    \n    words=review_text.lower().split()\n    # 4. Optionally remove stop words (false by default)\n    if remove_stopwords:\n        stop=set(stopwords.words('english'))\n        words=[w for w in words if not w in stop]\n    return (words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the punkt tokenizer\ntokenizer = nltk.data.load('tokenizers/punkt/english.pickle')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n    # Function to split a review into parsed sentences. Returns a \n    # list of sentences, where each sentence is a list of words\n    #\n    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n    raw_sentences = tokenizer.tokenize(review.strip())\n    #\n    # 2. Loop over each sentence\n    sentences = []\n    for raw_sentence in raw_sentences:\n        # If a sentence is empty, skip it\n        if len(raw_sentence) > 0:\n            # Otherwise, call review_to_wordlist to get a list of words\n            sentences.append( review_to_wordlist( raw_sentence, \\\n              remove_stopwords ))\n    #\n    # Return the list of sentences (each sentence is a list of words,\n    # so this returns a list of lists\n    return sentences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = []  # Initialize an empty list of sentences\n\n\nfor review in train[\"review\"]:\n    sentences += review_to_sentences(review, tokenizer)\n\n\nfor review in unlabeled_train[\"review\"]:\n    sentences += review_to_sentences(review, tokenizer)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A minor detail to note is the difference between the \"+=\" and \"append\" when it comes to Python lists. In many applications the two are interchangeable, but here they are not. If you are appending a list of lists to another list of lists, \"append\" will only append the first list; you need to use \"+=\" in order to join all of the lists at once."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the built-in logging module and configure it so that Word2Vec \n# creates nice output messages\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n    level=logging.INFO)\n\n# Set values for various parameters\nnum_features = 300    # Word vector dimensionality                      \nmin_word_count = 40   # Minimum word count                        \nnum_workers = 4       # Number of threads to run in parallel\ncontext = 10          # Context window size                                                                                    \ndownsampling = 1e-3   # Downsample setting for frequent words\n\n# # Initialize and train the model (this will take some time)\n# from gensim.models import word2vec\n# print (\"Training model...\")\n# model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n#             size=num_features, min_count = min_word_count, \\\n#             window = context, sample = downsampling)\n\n# # If you don't plan to train the model any further, calling \n# # init_sims will make the model much more memory-efficient.\n# model.init_sims(replace=True)\n\n# # It can be helpful to create a meaningful model name and \n# # save the model for later use. You can load it later using Word2Vec.load()\n# model_name = \"300features_40minwords_10context\"\n# model.save(model_name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training and Saving Your Model\nWith the list of nicely parsed sentences, we're ready to train the model. There are a number of parameter choices that affect the run time and the quality of the final model that is produced. For details on the algorithms below, see the word2vec API documentation as well as the Google documentation. \n\n**Architecture**: Architecture options are skip-gram (default) or continuous bag of words. We found that skip-gram was very slightly slower but produced better results.\n\n**Training algorithm**: Hierarchical softmax (default) or negative sampling. For us, the default worked well.\n\n**Downsampling of frequent words**: The Google documentation recommends values between .00001 and .001. For us, values closer 0.001 seemed to improve the accuracy of the final model.\n\n**Word vector dimensionality**: More features result in longer runtimes, and often, but not always, result in better models. Reasonable values can be in the tens to hundreds; we used 300.\n\n**Context / window size**: How many words of context should the training algorithm take into account? 10 seems to work well for hierarchical softmax (more is better, up to a point).\n\n**Worker threads**: Number of parallel processes to run. This is computer-specific, but between 4 and 6 should work on most systems.\n\n**Minimum word count**: This helps limit the size of the vocabulary to meaningful words. Any word that does not occur at least this many times across all documents is ignored. Reasonable values could be between 10 and 100. In this case, since each movie occurs 30 times, we set the minimum word count to 40, to avoid attaching too much importance to individual movie titles. This resulted in an overall vocabulary size of around 15,000 words. Higher values also help limit run time.\n\nChoosing parameters is not easy, but once we have chosen our parameters, creating a Word2Vec model is straightforward:"},{"metadata":{"trusted":true},"cell_type":"code","source":"model=word2vec.Word2Vec(sentences, workers=4, size=300, min_count=40, window=10, sample=1e-3)\nmodel.init_sims(replace=True)\nmodel_name=\"300features_40minwords_10context\"\nmodel.save(model_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Word2Vec.load(\"300features_40minwords_10context\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(model.wv.index2word)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def makeFeatureVec(words, model, num_features):\n      \n    # Pre-initialize an empty numpy array (for speed)\n    featureVec = np.zeros((num_features,),dtype=\"float32\")\n    nwords = 0.\n     \n    # Index2word is a list that contains the names of the words in \n    # the model's vocabulary. Convert it to a set, for speed \n    index2word_set = set(model.wv.index2word)\n    \n    # Loop over each word in the review and, if it is in the model's\n    # vocaublary, add its feature vector to the total\n    for word in words:\n        if word in index2word_set: \n            nwords = nwords + 1.\n            featureVec = np.add(featureVec,model[word])\n     \n    # Divide the result by the number of words to get the average\n    featureVec = np.divide(featureVec,nwords)\n    return featureVec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getAvgFeatureVecs(reviews, model, num_features):\n   \n     \n    # Initialize a counter\n    counter = 0\n    \n    # Preallocate a 2D numpy array, for speed\n    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n    \n    # Loop through the reviews\n    for review in reviews:\n       # Print a status message every 1000th review\n       if counter%1000 == 0:\n            print(f\"Review {counter} of {len(reviews)}\")\n            \n       # Call the function (defined above) that makes average feature vectors\n       reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n        \n       # Increment the counter\n       counter = counter + 1\n        \n    return reviewFeatureVecs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate average feature vectors for training and testing sets,\n# using the functions we defined above. Notice that we now use stop word\n# removal.\n\nclean_train_reviews = []\nfor review in train[\"review\"]:\n    clean_train_reviews.append(review_to_wordlist(review, remove_stopwords=True))\n\ntrainDataVecs = getAvgFeatureVecs(clean_train_reviews, model, num_features)\n\nprint(\"Creating average feature vecs for test reviews\")\nclean_test_reviews = []\nfor review in test[\"review\"]:\n    clean_test_reviews.append( review_to_wordlist(review,remove_stopwords=True))\nprint(\"Finished creating average feature vecs for test reviews\")\n\ntestDataVecs = getAvgFeatureVecs(clean_test_reviews, model, num_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def RandomForest(X_train, y_train, X_test):\n    \n    model=RandomForestClassifier(n_estimators=150)\n    model.fit(X_train, y_train)\n    predictions=model.predict(X_test)\n    \n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub=pd.DataFrame(data={'id':test['id'],\\\n                       'sentiment':predictions})\nsub.to_csv('submission2.csv', index=False, quoting=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def LogisticReg(X_train, y_train, X_test):\n    model=LogisticRegression(C=1)\n    model.fit(X_train, y_train)\n    result=model.predict(X_test)\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub=pd.DataFrame(data={'id':test['id'],\n                         'sentiment':result})\nsub.to_csv('submission_logreg.csv',index=False,quoting=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.wv.syn0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start=time.time()\n\nword_vectors = model.wv.syn0\nnum_clusters = int(word_vectors.shape[0] / 5)\n\nkmeans_clustering=KMeans(n_clusters=num_clusters)\nidx=kmeans_clustering.fit_predict(word_vectors)\n\nprint(f'Time', time.time()-start)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_centroid_map=dict(zip(model.wv.index2word, idx))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a Word / Index dictionary, mapping each vocabulary word to\n# a cluster number                                                                                            \nword_centroid_map = dict(zip(model.wv.index2word, idx))\nword_centroid_map","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For the first 10 clusters\nfor cluster in range(0,10):\n    \n    # Print the cluster number  \n    print(f\"\\nCluster {cluster}\")\n    \n    # Find all of the words for that cluster number, and print them out\n    words = []\n    for i in range(0,len(word_centroid_map.values())):\n        if(list(word_centroid_map.values())[i] == cluster):\n            words.append(list(word_centroid_map.keys())[i])\n    print(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max( word_centroid_map.values() )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_bag_of_centroids (wordlist, word_centroid_map):\n    num_centroid=max(word_centroid_map.values()) + 1\n    \n    bag_of_centroids=np.zeros(num_centroid, dtype='float32')\n    \n    for word in wordlist:\n        if word in word_centroid_map:\n            index=word_centroid_map[word]\n            bag_of_centroids[index]+=1\n    return bag_of_centroids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pre-allocate an array for the training set bags of centroids (for speed)\ntrain_centroids = np.zeros( (train[\"review\"].size, num_clusters), \\\n    dtype=\"float32\" )\n\n# Transform the training set reviews into bags of centroids\ncounter = 0\nfor review in clean_train_reviews:\n    train_centroids[counter] = create_bag_of_centroids( review, \\\n        word_centroid_map )\n    counter += 1\n\n# Repeat for test reviews \ntest_centroids = np.zeros(( test[\"review\"].size, num_clusters), \\\n    dtype=\"float32\" )\n\ncounter = 0\nfor review in clean_test_reviews:\n    test_centroids[counter] = create_bag_of_centroids( review, \\\n        word_centroid_map )\n    counter += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred=RandomForest(train_centroids, train['sentiment'], test_centroids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub=pd.DataFrame(data={'id':test['id'],\n                      'sentiment':pred})\nsub.to_csv('bag_of_words.csv', index=False, quoting=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2=LogisticRegression(C=1)\nmodel2.fit(train_centroids, train['sentiment'])\nresult=model2.predict(test_centroids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub=pd.DataFrame(data={'id':test['id'],\n                      'sentiment':result})\nsub.to_csv('bag_of_words_log.csv', index=False, quoting=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}