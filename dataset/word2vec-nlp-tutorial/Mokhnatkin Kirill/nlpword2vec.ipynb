{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-18T11:48:56.794806Z","iopub.execute_input":"2021-12-18T11:48:56.795248Z","iopub.status.idle":"2021-12-18T11:48:56.827164Z","shell.execute_reply.started":"2021-12-18T11:48:56.795118Z","shell.execute_reply":"2021-12-18T11:48:56.826285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_data = pd.read_csv('/kaggle/input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip',header=0,delimiter='\\t',quoting=3)","metadata":{"execution":{"iopub.status.busy":"2021-12-18T11:49:10.248738Z","iopub.execute_input":"2021-12-18T11:49:10.24909Z","iopub.status.idle":"2021-12-18T11:49:11.434657Z","shell.execute_reply.started":"2021-12-18T11:49:10.249052Z","shell.execute_reply":"2021-12-18T11:49:11.433653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-18T11:51:16.610748Z","iopub.execute_input":"2021-12-18T11:51:16.611145Z","iopub.status.idle":"2021-12-18T11:51:16.636254Z","shell.execute_reply.started":"2021-12-18T11:51:16.611107Z","shell.execute_reply":"2021-12-18T11:51:16.635516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_data['review'][0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#удаляем HTML разметку и тэги, такие как \"<br>\"\nfrom bs4 import BeautifulSoup","metadata":{"execution":{"iopub.status.busy":"2021-12-18T11:51:17.976513Z","iopub.execute_input":"2021-12-18T11:51:17.97686Z","iopub.status.idle":"2021-12-18T11:51:18.315868Z","shell.execute_reply.started":"2021-12-18T11:51:17.976822Z","shell.execute_reply":"2021-12-18T11:51:18.314795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example1 = BeautifulSoup(training_data[\"review\"][0]) \nexample1.get_text()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def review_to_words( raw_review ):\n    # Функция для превращения обзора в строку слов\n    # На вход необработанный обзор фильма \n    # На выходе обработанный\n    #\n    # 1. Удаляем HTML\n    review_text = BeautifulSoup(raw_review).get_text() \n    #\n    # 2. Удаляем не буквы       \n    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n    #\n    # 3. Преобразуем в lower-case, делим на отдельные слова\n    words = letters_only.lower().split()                             \n    #\n    # 4. Для ускорения поиска преобразуем список в set\n    stops = set(stopwords.words(\"english\"))                  \n    # \n    # 5. Удаляем стоповые слова\n    meaningful_words = [w for w in words if not w in stops]   \n    \n    #6.Lemmatization\n    for word in meaningful_words:\n        word = wordnet_lemmatizer.lemmatize(word,'v')\n    \n    # 6. Соединим слова обратно в одну строку с разделителем \" \", \n    # и вернем результат.\n    return( \" \".join( meaningful_words )) ","metadata":{"execution":{"iopub.status.busy":"2021-12-18T11:51:19.290057Z","iopub.execute_input":"2021-12-18T11:51:19.290446Z","iopub.status.idle":"2021-12-18T11:51:19.298505Z","shell.execute_reply.started":"2021-12-18T11:51:19.290401Z","shell.execute_reply":"2021-12-18T11:51:19.297555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()","metadata":{"execution":{"iopub.status.busy":"2021-12-18T11:51:22.004406Z","iopub.execute_input":"2021-12-18T11:51:22.00486Z","iopub.status.idle":"2021-12-18T11:51:23.594037Z","shell.execute_reply.started":"2021-12-18T11:51:22.004805Z","shell.execute_reply":"2021-12-18T11:51:23.592933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Чистим и парсим тренировочный набор данных","metadata":{}},{"cell_type":"code","source":"num_reviews = training_data['review'].size\nclean_train_reviews = []\nfor i in range( 0, num_reviews ):                                                                  \n    clean_train_reviews.append( review_to_words( training_data[\"review\"][i] ))","metadata":{"execution":{"iopub.status.busy":"2021-12-18T11:51:23.595777Z","iopub.execute_input":"2021-12-18T11:51:23.596044Z","iopub.status.idle":"2021-12-18T11:51:58.9256Z","shell.execute_reply.started":"2021-12-18T11:51:23.596014Z","shell.execute_reply":"2021-12-18T11:51:58.924752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_train_reviews[0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Инициализируем объект \"CountVectorizer\", который является инструментом scikit-learn's bag of words.  \nvectorizer = CountVectorizer(analyzer = \"word\",\n                             tokenizer = None, \n                             preprocessor = None,\n                             stop_words = None,  \n                             max_features = 5000)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_features = vectorizer.fit_transform(clean_train_reviews)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_features = train_data_features.toarray()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#25000 обзоров и 5000 уникальных слов(5000 - это число, которое мы назначили \n#в in max_features параметре функции CountVectorizer\n#чтобы ограничить размер словаря уникальных слов до 5000)\ntrain_data_features.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Используем классификатор RandomForest для классификации обзоров\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(train_data_features, training_data[\"sentiment\"], \n                                                    test_size=0.2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RF = RandomForestClassifier(n_estimators = 100)\nRF.fit( X_train, y_train )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = RF.predict(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix,classification_report","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(confusion_matrix(y_test,predictions))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test,predictions))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = pd.read_csv('/kaggle/input/word2vec-nlp-tutorial/testData.tsv.zip',header=0,delimiter='\\t',quoting=3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_reviews = len(test_data[\"review\"])\nclean_test_reviews = [] ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Чистим и парсим тестовый набор данных","metadata":{}},{"cell_type":"code","source":"for i in range(0,num_reviews):\n    clean_review = review_to_words( test_data[\"review\"][i] )\n    clean_test_reviews.append( clean_review )\n\n# Получаем набор слов для тестового набора, и превращаем его в numpy массив\ntest_data_features = vectorizer.transform(clean_test_reviews)\ntest_data_features = test_data_features.toarray()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Используем random forest для предсказания\nresult = RF.predict(test_data_features)\n\noutput = pd.DataFrame( data={\"id\":test_data[\"id\"], \"sentiment\":result} )\n\noutput.to_csv( \"Bag_of_Words_model.csv\", index=False, quoting=3 )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Попытаемся улучшить результат при помощи нейронных сетей.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense\nfrom tensorflow.python import keras","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Строим модель и компилируем ее\nmodel = keras.Sequential([\n    keras.layers.Dense(128,input_shape=(5000,),activation=tf.nn.relu),\n    keras.layers.Dense(128,activation=tf.nn.relu),\n    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n])\n\nmodel.compile(optimizer='adam', \n              loss='binary_crossentropy',\n              metrics=['accuracy'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train,y_train,epochs=10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_ANN = model.predict(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Превращаем результат в 1 и 0\npredictions_ANN = (predictions_ANN>0.9)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_ANN","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(confusion_matrix(y_test,predictions_ANN))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test,predictions_ANN))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_ANN = model.predict(test_data_features)\nresult_ANN = result_ANN>0.9","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_ANN","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_ANN = [1 if res==True else 0 for res in result_ANN]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = pd.DataFrame( data={\"id\":test_data[\"id\"], \"sentiment\":result_ANN} )\n\n#2 вариант предсказания\noutput.to_csv( \"Bag_of_Words_model_ANN.csv\", index=False, quoting=3 )","metadata":{},"execution_count":null,"outputs":[]}]}