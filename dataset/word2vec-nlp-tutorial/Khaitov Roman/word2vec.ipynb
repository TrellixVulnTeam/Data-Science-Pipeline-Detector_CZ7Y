{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import re\nimport logging\nimport time\n\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n    level=logging.INFO)\n\nfrom bs4 import BeautifulSoup \n# удобная библиотека для обработки html-тегов, которые есть в текстах к этой задаче\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier\nfrom sklearn.cluster import KMeans\nfrom sklearn import metrics\nfrom gensim.models import Word2Vec \n# библиотека gensim, в которой реализовано много Deep Learning алгоритмов\n# в том числе есть много алгортмов для обработки текста, в том числе тематическое моделирование\n\nimport nltk\n# nltk.download()  # важно скачать датасеты, в том числе стоп-слова\nfrom nltk.corpus import stopwords # сразу забираем стоп-слова\n\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib notebook\n\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17a4182d5c03683f033ceb099e892575409310c6"},"cell_type":"code","source":"nltk.download('stopwords')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18ea30859f624a65e3a4f16b88b851dd2f71ba81","trusted":true},"cell_type":"code","source":"#set(stopwords.words('english'))\ntrain = pd.read_csv(\"../input/labeledTrainData.tsv\", header=0, \\\n                    delimiter=\"\\t\", quoting=3)\ntest = pd.read_csv(\"../input/testData.tsv\", header=0, delimiter=\"\\t\", \\\n                   quoting=3 )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e594b70ced1a00dc3a5c37a871a602e5df23ba2"},"cell_type":"raw","source":"def formatData(df, fit_transform = True\n               , isTrain = True\n               , max_features = 10\n               , isClearData=False\n               , onlyTopWord = 5000\n               , sprTop = {}\n              ):\n    if isClearData:\n        clearData = []\n        for i in df.index:\n            #print(i)\n            test_text = BeautifulSoup(df.review[i]).get_text()\n     \n            words = re.sub(\"[^a-zA-Z]\", \" \", test_text)\n            words = words.lower().split()\n            \n            stops = set(stopwords.words(\"english\"))\n\n            meaningful_words = [w for w in words if w in sprTop]\n            clearData.append( \" \".join( meaningful_words ))\n\n\n        df['ClearReview'] = clearData\n    \n        \n        #vectorizer = CountVectorizer(\n        #                analyzer='word'\n        #                , tokenizer=None\n        #                , preprocessor = None\n        #                , stop_words = None\n        #                , max_features = max_features\n        #            )\n        #\n        #del_f = vectorizer.fit_transform(df['ClearReview'].values).toarray()\n        #\n        #vocab = vectorizer.get_feature_names()\n        ##print(vocab[1:100])\n        #\n        #dist = np.sum(del_f, axis=0)\n        #top=[]\n        #for count, tag in sorted([(count, tag) for tag, count in zip(vocab, dist)], reverse=True)[1:5000]:\n        #    top.append([count, tag])\n    \n        #top_df[top_df.index<600].plot(title='<'+str(onlyTopWord))\n        \n    #if sprTop != {}:\n    #        \n    #    clearData = []\n    #    k=0\n    #    for i in df.index:\n    #        if k == 1000:\n    #            print(round(i/k),'ok')\n    #            k=0\n    #        k=k+1\n    #        test_text = BeautifulSoup(df.ClearReview[i]).get_text()\n    #        #del_word = \n    #        words = re.sub(\"[^a-zA-Z]\", \" \", test_text)\n    #        meaningful_words = [w for w in words if w in sprTop]\n    #        clearData.append( \" \".join( meaningful_words ))\n#\n#\n    #    df['ClearReview'] = clearData\n    \n    if isTrain:\n        vectorizer = CountVectorizer(\n                        analyzer='word'\n                        , tokenizer=None\n                        , preprocessor = None\n                        , stop_words = None\n                        , max_features = 580 #max_features if onlyTopWord > max_features else onlyTopWord\n                    )\n        \n        if fit_transform:\n            print('transform',df['ClearReview'].shape)\n            features = vectorizer.fit_transform(df['ClearReview'].values).toarray()\n        else:\n            features = vectorizer.transform(df['ClearReview'].values).toarray()\n\n        #features_cumsum = np.cumsum(features,axis=1)\n        #features_div = features/np.sum(features,axis=0)\n        #features_full = np.concatenate([features,features_cumsum,features_div],axis=1)\n        print(features)\n    else:\n        features = []\n        \n    \n    return df, features\n\n#train = formatData(train,isTrain = False)\n#test = formatData(train,isTrain = False)"},{"metadata":{"_uuid":"1124a7cd025c119d16ac2e2ace5477375d4977d8"},"cell_type":"raw","source":"df_clear,features = formatData(train\n                               , fit_transform = False\n                               , isTrain = False\n                               , isClearData = True\n                               , max_features = 5000\n                               , sprTop = [])"},{"metadata":{"collapsed":true,"_uuid":"085346e2f0040122777c437f919d521a3e0792e5"},"cell_type":"raw","source":"df_train,features = formatData(df_clear\n                               , fit_transform = True\n                               , isTrain = True\n                               , isClearData = False\n                               , max_features = 5000\n                               , sprTop = [])"},{"metadata":{"_uuid":"4a797dbe731744e2e1bb735809e5ac6dc7c6485e"},"cell_type":"raw","source":"vocab = vectorizer.get_feature_names()\n#print(vocab[1:100])\n\ndist = np.sum(features, axis=0)\ntop=[]\nfor count, tag in sorted([(count, tag) for tag, count in zip(vocab, dist)], reverse=True)[1:5000]:\n    top.append([count, tag])"},{"metadata":{"trusted":false,"_uuid":"01e8669d0ea5d4d84be12da7b58666c5eeeee2fa"},"cell_type":"code","source":"top_df = pd.DataFrame(top,columns=['cnt','tag'])\ntop_df.plot()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"_uuid":"b9287155c969f80137a751b05bd38d8737915049"},"cell_type":"raw","source":"df_clear,features = formatData(train\n                               , fit_transform = True\n                               , isTrain = False\n                               , isClearData = True\n                               , max_features = 5000\n                               , sprTop = set(top_df[top_df.index<600].tag.values)\n                              )"},{"metadata":{"_uuid":"18db3e2ec389c566214ed605c9065c0823d6cfe3"},"cell_type":"raw","source":"vectorizer = CountVectorizer(\n                analyzer='word'\n                , tokenizer=None\n                , preprocessor = None\n                , stop_words = None\n                , max_features = 580 #max_features if onlyTopWord > max_features else onlyTopWord\n            )\nprint('transform',df_clear['ClearReview'].shape)\nfeatures = vectorizer.fit_transform(df_clear['ClearReview'].values).toarray()\n#else:\n#    features = vectorizer.transform(df['ClearReview'].values).toarray()"},{"metadata":{"_uuid":"edecbe286d41751b1dfae44fd9f5bab923c7d825"},"cell_type":"raw","source":"features.shape"},{"metadata":{"_uuid":"ad72a9c1b09e7879ab728b184c2d7a42fe4e68c1","scrolled":true,"trusted":false},"cell_type":"code","source":"%%time\n\nfrom sklearn.model_selection import train_test_split\n\nresult = []\n\nfor max_word in range(2,16,2):\n\n    #df_train, features_full = formatData(df_clear, fit_transform = True,max_features = max_word\n    X_train, X_test, y_train, y_test = train_test_split(features,df_clear['sentiment'].values,test_size = 0.15)\n    \n    forest = RandomForestClassifier(max_depth=12,random_state=17,n_estimators=60) \n    forest.fit( X_train, y_train )\n    print(max_word,'ok')\n    result.append([max_word, metrics.accuracy_score(forest.predict(X_test),y_test)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8582da80363c2e8ce79aff13dd232671ff38bf07"},"cell_type":"code","source":"result","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"343106b9897a3471e781a1d28289cd6503052347"},"cell_type":"raw","source":"from time import time \nfrom sklearn.metrics import f1_score\n\nX_train, X_test, y_train, y_test = train_test_split(features,df_clear['sentiment'].values,test_size = 0.15)\n    \nforest = RandomForestClassifier(max_depth=12,random_state=17,n_estimators=60) \nforest.fit( X_train, y_train )\nprint(max_word,'ok')\nresult.append([max_word, metrics.accuracy_score(forest.predict(X_test),y_test)])\n\ndef train_classifier(clf, X_train, y_train):\n    ''' Fits a classifier to the training data. '''\n    \n    # Start the clock, train the classifier, then stop the clock\n    start = time()\n    clf.fit(X_train, y_train)\n    end = time()\n    \n    # Print the results\n    print(\"Trained model in {:.4f} seconds\".format(end - start))\n\n    \ndef predict_labels(clf, features, target):\n    ''' Makes predictions using a fit classifier based on F1 score. '''\n    \n    # Start the clock, make predictions, then stop the clock\n    start = time()\n    y_pred = clf.predict(features)\n    \n    end = time()\n    # Print and return results\n    print( \"Made predictions in {:.4f} seconds.\".format(end - start))\n    \n    return f1_score(target, y_pred, pos_label=1), sum(target == y_pred) / float(len(y_pred))\n\n\ndef train_predict(clf, X_train, y_train, X_test, y_test):\n    ''' Train and predict using a classifer based on F1 score. '''\n    \n    # Indicate the classifier and the training set size\n    print (\"Training a {} using a training set size of {}. . .\".format(clf.__class__.__name__, len(X_train)))\n    \n    # Train the classifier\n    train_classifier(clf, X_train, y_train)\n    \n    # Print the results of prediction for both training and testing\n    f1, acc = predict_labels(clf, X_train, y_train)\n    print( f1, acc)\n    print (\"F1 score and accuracy score for training set: {:.4f} , {:.4f}.\".format(f1 , acc))\n    \n    f1, acc = predict_labels(clf, X_test, y_test)\n    print (\"F1 score and accuracy score for test set: {:.4f} , {:.4f}.\".format(f1 , acc))\n\nclf_A = LogisticRegression(random_state = 42)\nclf_B = SVC(random_state = 912, kernel='rbf')\nclf_C = lgb.LGBMClassifier(seed = 82)\n\ntrain_predict(clf_A, x_train, y_train, x_test, y_test)\nprint ('')\ntrain_predict(clf_B, x_train, y_train, x_test, y_test)\nprint ('')\ntrain_predict(clf_C, x_train, y_train, x_test, y_test)\nprint ('')"},{"metadata":{"_uuid":"f603a1906c13476aac7045a953a6330dec080e4f"},"cell_type":"raw","source":"%%time\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nprint(\"Training the random forest...\")\nforest = RandomForestClassifier() \nforest.fit( X_train, y_train )\n\nETR = ExtraTreesClassifier() \nETR.fit(X_train, y_train)\n\nKN = KNeighborsClassifier()\nKN.fit(X_train, y_train)\nprint(\"Ok\")"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"53fe2e540502d6d3c2314df6ee942183a97fdc16"},"cell_type":"code","source":"metrics.accuracy_score(forest.predict(X_test),y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aeb650b9ab3ddc4128889ac594d31bf32983619a","collapsed":true},"cell_type":"raw","source":"print('ETR:',metrics.accuracy_score(ETR.predict(X_test),y_test)\n    , 'n/RFR:',metrics.accuracy_score(forest.predict(X_test),y_test)\n    , 'n/Kneib:',metrics.accuracy_score(KN.predict(X_test),y_test)\n     )\n"},{"metadata":{"_uuid":"c7bf9e4c248117e12de8ff18117c8f2388cf4683","collapsed":true},"cell_type":"raw","source":"#Random KNN class :)\nimport random\n\nclass ScrappyKNN():\n    def fit(self, X_train, y_train):\n        self.X_train = X_train\n        self.y_train = y_train\n    \n    def predict(self, X_test):\n        predictions = []\n        for row in X_test:\n            label = random.choice(self.y_train)\n            predictions.append(label)\n        return predictions\n    "},{"metadata":{"_uuid":"3610611f5172c20b0ef49f321207c18c3248855c","collapsed":true},"cell_type":"raw","source":"#euclidian dist KNN class\nfrom scipy.spatial import distance\n\ndef euc(a,b):\n    return distance.euclidean(a,b)\n\nclass ScrappyKNN():\n    def fit(self, X_train, y_train):\n        self.X_train = X_train\n        self.y_train = y_train\n    \n    def predict(self, X_test):\n        predictions = []\n        for row in X_test:\n            #находим лэйбл от ближайшей точки\n            label = self.closest(row)\n            predictions.append(label)\n        return predictions\n    \n    #находим лэйбл от ближайшей точки\n    def closest(self, row):\n        #фейковая лучшая\n        best_dist = euc(row, self.X_train[0])\n        best_index = 0\n        for i in range(1, len(self.X_train)):\n            dist = euc(row, self.X_train[i])\n            if dist < best_dist:\n                best_dist = dist\n                best_index = i\n        return self.y_train[best_index]"},{"metadata":{"_uuid":"8f7878fa0a2b7164c3b8687ff2fd28d605454df7","collapsed":true},"cell_type":"raw","source":"Sc = ScrappyKNN()\nSc.fit(X_train, y_train)\npred = Sc.predict(X_test)\nprint(metrics.accuracy_score(pred,y_test))"},{"metadata":{"_uuid":"618d457e119c756b20837fdc1c23449ec4391306","collapsed":true},"cell_type":"raw","source":"test['RFR']=forest.predict(features_full)\ntest['ETR']=ETR.predict(features_full)\n\ntest['RFR_proba0'] = pd.DataFrame(forest.predict_proba(features_full),columns=['c0','c1'])['c0']\ntest['RFR_proba1'] = pd.DataFrame(forest.predict_proba(features_full),columns=['c0','c1'])['c1']\n\ntest['ETR_proba0'] = pd.DataFrame(ETR.predict_proba(features_full),columns=['c0','c1'])['c0']\ntest['ETR_proba1'] = pd.DataFrame(ETR.predict_proba(features_full),columns=['c0','c1'])['c1']\n\ntest['mixtrees0'] = test['RFR_proba0']*0.5 + test['ETR_proba0']*0.5\ntest['mixtrees1'] = test['RFR_proba1']*0.5 + test['ETR_proba1']*0.5"},{"metadata":{"_uuid":"218a3820dd06c62841052020ca6fd76c95ebd3b8","collapsed":true},"cell_type":"raw","source":"pipeline = Pipeline([\n    ('vect', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    ('clf', SGDClassifier()),\n])\n\nparameters = {\n    'vect__max_df': (0.5, 0.75, 1.0),\n    # 'vect__max_features': (None, 5000, 10000, 50000),\n    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n    # 'tfidf__use_idf': (True, False),\n    # 'tfidf__norm': ('l1', 'l2'),\n    'clf__max_iter': (5,),\n    'clf__alpha': (0.00001, 0.000001),\n    'clf__penalty': ('l2', 'elasticnet'),\n    # 'clf__max_iter': (10, 50, 80),\n}"},{"metadata":{"_uuid":"f55f687bfeb1561fdf4399287b6d6ed08f7e0aa2"},"cell_type":"raw","source":"X_train.shape, X_test.shape, y_train.shape, y_test.shape"},{"metadata":{"_uuid":"7f00ff3796efdcdf44a10b12dcf7bd806bad0c83"},"cell_type":"raw","source":"grid_search = GridSearchCV(pipeline, parameters, cv=5,\n                           n_jobs=-1, verbose=1)\n\nprint(\"Performing grid search...\")\nprint(\"pipeline:\", [name for name, _ in pipeline.steps])\nprint(\"parameters:\")\nprint(parameters)\ngrid_search.fit(X_train, y_train)\n#print(\"done in %0.3fs\" % (time() - t0))\nprint()\n\nprint(\"Best score: %0.3f\" % grid_search.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = grid_search.best_estimator_.get_params()\nfor param_name in sorted(parameters.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"},{"metadata":{"_uuid":"5949c7adedbf144b4886cf2043d567fc149ca559","trusted":true},"cell_type":"code","source":"clearData = []\ndf=train\nfor i in df.index:\n    #print(i)\n    test_text = BeautifulSoup(df.review[i]).get_text()\n\n    words = re.sub(\"[^a-zA-Z]\", \" \", test_text)\n    words = words.lower().split()\n\n    stops = set(stopwords.words(\"english\"))\n\n    meaningful_words = [w for w in words if not w in stops]\n    clearData.append( \" \".join( meaningful_words ))\n\n\ndf['ClearReview'] = clearData\n\n\nvectorizer = CountVectorizer(\n                analyzer='word'\n                , tokenizer=None\n                , preprocessor = None\n                , stop_words = None\n                , max_features = 5000\n            )\n\nfutures = vectorizer.fit_transform(df['ClearReview'].values).toarray()\n\nvocab = vectorizer.get_feature_names()\n#print(vocab[1:100])\n\ndist = np.sum(futures, axis=0)\ntop=[]\nfor count, tag in sorted([(count, tag) for tag, count in zip(vocab, dist)], reverse=True)[1:5000]:\n    top.append([count, tag])\n\ntop_df = pd.DataFrame(top,columns=['cnt','tag'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"63d3b553bfff9dbbaa94eabd8b85911280ec23f5","trusted":false},"cell_type":"code","source":"from time import time \nfrom sklearn.metrics import f1_score\nimport lightgbm as lgb\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\n\n\nX_train, X_test, y_train, y_test = train_test_split(futures,df['sentiment'].values,test_size = 0.15)\n    \nforest = RandomForestClassifier(max_depth=12,random_state=17,n_estimators=60) \nforest.fit( X_train, y_train )\nprint(metrics.accuracy_score(forest.predict(X_test),y_test))\n#print(max_word,'ok')\n#result.append([max_word, metrics.accuracy_score(forest.predict(X_test),y_test)])\n\ndef train_classifier(clf, X_train, y_train):\n    ''' Fits a classifier to the training data. '''\n    \n    # Start the clock, train the classifier, then stop the clock\n    start = time()\n    clf.fit(X_train, y_train)\n    end = time()\n    \n    # Print the results\n    print(\"Trained model in {:.4f} seconds\".format(end - start))\n\n    \ndef predict_labels(clf, features, target):\n    ''' Makes predictions using a fit classifier based on F1 score. '''\n    \n    # Start the clock, make predictions, then stop the clock\n    start = time()\n    y_pred = clf.predict(features)\n    \n    end = time()\n    # Print and return results\n    print( \"Made predictions in {:.4f} seconds.\".format(end - start))\n    \n    return f1_score(target, y_pred, pos_label=1), sum(target == y_pred) / float(len(y_pred))\n\n\ndef train_predict(clf, X_train, y_train, X_test, y_test):\n    ''' Train and predict using a classifer based on F1 score. '''\n    \n    # Indicate the classifier and the training set size\n    print (\"Training a {} using a training set size of {}. . .\".format(clf.__class__.__name__, len(X_train)))\n    \n    # Train the classifier\n    train_classifier(clf, X_train, y_train)\n    \n    # Print the results of prediction for both training and testing\n    f1, acc = predict_labels(clf, X_train, y_train)\n    print( f1, acc)\n    print (\"F1 score and accuracy score for training set: {:.4f} , {:.4f}.\".format(f1 , acc))\n    \n    f1, acc = predict_labels(clf, X_test, y_test)\n    print (\"F1 score and accuracy score for test set: {:.4f} , {:.4f}.\".format(f1 , acc))\n\nclf_A = LogisticRegression(random_state = 42)\nclf_B = SVC(random_state = 912, kernel='rbf')\nclf_C = lgb.LGBMClassifier(seed = 82)\n\ntrain_predict(clf_A, X_train, y_train, X_test, y_test)\nprint ('')\ntrain_predict(clf_B, X_train, y_train, X_test, y_test)\nprint ('')\ntrain_predict(clf_C, X_train, y_train, X_test, y_test)\nprint ('')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a681843b7c848e87a59c4be4035b949f0687ba85","collapsed":true,"trusted":false},"cell_type":"code","source":"clearData = []\ndf=test\nfor i in df.index:\n    #print(i)\n    test_text = BeautifulSoup(df.review[i]).get_text()\n\n    words = re.sub(\"[^a-zA-Z]\", \" \", test_text)\n    words = words.lower().split()\n\n    stops = set(stopwords.words(\"english\"))\n\n    meaningful_words = [w for w in words if not w in stops]\n    clearData.append( \" \".join( meaningful_words ))\n\n\ndf['ClearReview'] = clearData\n\n\nvectorizer = CountVectorizer(\n                analyzer='word'\n                , tokenizer=None\n                , preprocessor = None\n                , stop_words = None\n                , max_features = 5000\n            )\n\nfutures = vectorizer.fit_transform(df['ClearReview'].values).toarray()\n\nvocab = vectorizer.get_feature_names()\n#print(vocab[1:100])\n\ndist = np.sum(futures, axis=0)\ntop=[]\nfor count, tag in sorted([(count, tag) for tag, count in zip(vocab, dist)], reverse=True)[1:5000]:\n    top.append([count, tag])\n\ntop_df = pd.DataFrame(top,columns=['cnt','tag'])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"a344f689981cde7a4bc44a71ed956f4b88445e86"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"}},"nbformat":4,"nbformat_minor":1}