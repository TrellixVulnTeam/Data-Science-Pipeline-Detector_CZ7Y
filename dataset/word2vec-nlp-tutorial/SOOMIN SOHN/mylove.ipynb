{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import zipfile\n\nDATA_IN_PATH = '../input/word2vec-nlp-tutorial/'\n\nfile_list = ['labeledTrainData.tsv.zip','unlabeledTrainData.tsv.zip','testData.tsv.zip']\n\nfor file in file_list:\n    zipRef = zipfile.ZipFile(DATA_IN_PATH+file,'r')\n    zipRef.extractall(DATA_IN_PATH)\n    zipRef.close()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ntrain_data = pd.read_csv('../input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip',  delimiter=\"\\t\", quoting=3)\ntest_data = pd.read_csv('../input/word2vec-nlp-tutorial/testData.tsv.zip', delimiter = \"\\t\", quoting= 3 )\nsubmission1 = pd.read_csv(\"../input/word2vec-nlp-tutorial/sampleSubmission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_length = train_data['review'].apply(len)\ntrain_length.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install wordcloud","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\ncloud = WordCloud(width=800, height=600).generate(\" \".join(train_data['review']))\nplt.figure(figsize=(20,15))\nplt.imshow(cloud)\nplt.axis(\"off\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install bs4\nimport re\nimport pandas\nimport numpy\nimport json\nfrom bs4 import BeautifulSoup as bs\nfrom nltk.corpus import stopwords\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.python.keras.preprocessing.text import Tokenizer\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data['review'][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"review = train_data['review'][0]\n#HTML tag 제거하기 BeautifulSoup를 이용하여\nreview_text = BeautifulSoup(review,\"html5lib\").get_text()\n#영어 글자를 제외한 나머지 공백으로 바꾸기\nreview_text = re.sub(\"[^a-zA-Z]\",\" \",review_text)\n\nprint(review_text)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#불용어(stopword) 삭제하기\n#NLTK의 불용어 사전 이용하기\n#NLTK 불용어사전은 소문자로 되어있으니 소문자로 바꾸기\n#불용어 set만들기\nstop_words = set(stopwords.words('english'))\n\n\nreview_text = review_text.lower()\nwords = review_text.split()\nwords = [w for w in words if not w in stop_words]\n\nprint(words)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#문자열이 단어 리스트로 변환됨\n#모델에 적용시키기 위해서 다시 하나의 문자열로 합치기\n\nclean_review = ' '.join(words)\nprint(clean_review)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocessing(review,remove_stopwords=False):\n    #불용어 제거는 옵션\n    \n    #HTML 태그 제거하기\n    review_text = BeautifulSoup(review,\"html5lib\").get_text()\n    \n    #영어가 아닌 문자 정규식을 활용하여 공백으로 바꾸기\n    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n    \n    #대문자를 소문자로 바꾸고 공백 단위로 텍스트를 나눠서 리스트로 만든다.\n    \n    words = review_text.lower().split\n    \n    #불용어 제거\n    if remove_stopwords:\n        #불용어사전 불러오기\n        stops = set(stopwords.words(\"english\")) \n        \n        #불용어를 제거한 리스트 생성\n        words = [w for w in words if not w in stops]\n        \n        #단어 리스트를 하나의 글로 합치기\n        clean_review = ' '.join(words)\n        \n    #불용어를 제거하고 싶지 않음\n    else:\n        clean_review = ' '.join(words)\n        \n    return clean_review\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['review'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_train_reviews = []\nfor review in train_data['review']:\n    clean_train_reviews.append(preprocessing(review,remove_stopwords=True))\n\nclean_train_reviews[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocessing( review, remove_stopwords = False ): \n    # 불용어 제거는 옵션으로 선택 가능하다.\n    \n    # 1. HTML 태그 제거\n    review_text = BeautifulSoup(review, \"html5lib\").get_text()\t\n\n    # 2. 영어가 아닌 특수문자들을 공백(\" \")으로 바꾸기\n    review_text = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n\n    # 3. 대문자들을 소문자로 바꾸고 공백단위로 텍스트들 나눠서 리스트로 만든다.\n    words = review_text.lower().split()\n\n    if remove_stopwords: \n        # 4. 불용어들을 제거\n    \n        #영어에 관련된 불용어 불러오기\n        stops = set(stopwords.words(\"english\"))\n        # 불용어가 아닌 단어들로 이루어진 새로운 리스트 생성\n        words = [w for w in words if not w in stops]\n        # 5. 단어 리스트를 공백을 넣어서 하나의 글로 합친다.\t\n        clean_review = ' '.join(words)\n\n    else: # 불용어 제거하지 않을 때\n        clean_review = ' '.join(words)\n\n    return clean_review","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_train_reviews = []\nfor review in train_data['review']:\n    clean_train_reviews.append(preprocessing(review, remove_stopwords = True))\n\n# 전처리한 데이터 출력\nclean_train_reviews[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_train_df = pd.DataFrame({'review':clean_train_reviews,'sentiment':train_data['sentiment']})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#남은 전처리 과정\n#입력값이 텍스트가 아닌 각 단어의 인덱스로 되어있어야함\n#길이는 동일해야 함\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(clean_train_reviews)\ntext_sequences = tokenizer.texts_to_sequences(clean_train_reviews)\n\nprint(text_sequences[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_vocab = tokenizer.word_index\nprint(word_vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(word_vocab))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndata_configs = {}\n\ndata_configs['vocab'] = word_vocab\ndata_configs['vocab_size'] = len(word_vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#0으로 패딩\n\nMAX_SEQUENCE_LENGTH = 174\n\ntrain_inputs = pad_sequences(text_sequences,maxlen=MAX_SEQUENCE_LENGTH,padding='post')\n\nprint(train_inputs.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = np.array(train_data['sentiment'])\nprint('Shape of label tensor:', train_labels.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_INPUT_DATA = 'train_input.npy'\nTRAIN_LABEL_DATA = 'train_label.npy'\nTRAIN_CLEAN_DATA = 'train_clean.csv'\nDATA_CONFIGS = 'data_configs.json'\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 전처리 된 데이터를 넘파이 형태로 저장\nnp.save(open(TRAIN_INPUT_DATA, 'wb'), train_inputs)\nnp.save(open(TRAIN_LABEL_DATA, 'wb'), train_labels)\n\n# 정제된 텍스트를 csv 형태로 저장\nclean_train_df.to_csv(TRAIN_CLEAN_DATA, index = False)\n\n\n# 데이터 사전을 json 형태로 저장\njson.dump(data_configs, open(DATA_CONFIGS, 'w'), ensure_ascii=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_test_reviews = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for review in test_data['review']:\n    clean_test_reviews.append(preprocessing(review, remove_stopwords = True))\n\n\nclean_test_df = pd.DataFrame({'review': clean_test_reviews, 'id': test_data['id']})\ntest_id = np.array(test_data['id'])\n\ntext_sequences = tokenizer.texts_to_sequences(clean_test_reviews)\ntest_inputs = pad_sequences(text_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#토크나이저를 통해 인덱스 벡터로 만들 때 토크나이징 객체로 새롭게 만드는 것이 아니라\n#기존에 학습 데이터에 적용한 토크나이저 객체를 사용하는 것이다.\n#새롭게 만들 경우 학습 데이터와 평가 데이터 대한 각 단어들의 인덱스가 달라져서 모델에\n#정상적으로 적용할 수 없기 때문이다.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TEST_INPUT_DATA = 'test_input.npy'\nTEST_CLEAN_DATA = 'test_clean.csv'\nTEST_ID_DATA = 'test_id.npy'\n\nnp.save(open(TEST_INPUT_DATA, 'wb'), test_inputs)\nnp.save(open(TEST_ID_DATA, 'wb'), test_id)\nclean_test_df.to_csv(TEST_CLEAN_DATA, index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#선형회귀모델","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_using = pd.read_csv('train_clean.csv',header = 0, delimiter='\\t',quoting=3)\n\nreviews = list(train_data['review'])\nsentiments = list(train_data['sentiment'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RANDOM_SEED = 42\nTEST_SPLIT = 0.2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = TfidfVectorizer(min_df = 0.0, analyzer=\"char\", sublinear_tf=True, ngram_range=(1,3), max_features=5000) \n\nX = vectorizer.fit_transform(reviews)\ny = np.array(sentiments)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = vectorizer.get_feature_names()\nX_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RANDOM_SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgs = LogisticRegression(class_weight='balanced') \nmodel = lgs.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\npredicted = lgs.predict(X_eval)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy: %f\" % lgs.score(X_eval, y_eval))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install joblib","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import datasets\nimport pickle\n\nimport joblib\n\nsaved_model = pickle.dumps(lgs)\n\nclf_from_pickle = pickle.loads(saved_model)\n\n# Use the loaded pickled model to make predictions\nclf_from_pickle.predict(X_eval)\n\nprint(\"Accuracy: %f\" % clf_from_pickle.score(X_eval, y_eval))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"joblib.dump(lgs, 'filename.pkl') \nclf_from_joblib = joblib.load('filename.pkl') \nclf_from_joblib.predict(X)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TEST_CLEAN_DATA = 'test_clean.csv'\n\ntest_data = pd.read_csv(TEST_CLEAN_DATA)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntestDataVecs = vectorizer.transform(test_data['review'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predicted = lgs.predict(testDataVecs)\nprint(test_predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"answer_dataset = pd.DataFrame({'id': test_data['id'], 'sentiment': test_predicted})\nanswer_dataset.to_csv('lgs_tfidf_answer.csv', index=False, quoting=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#내 데이터 불러오기","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_data = pd.read_csv('../input/mysettt/myResults.csv', header = 0,delimiter=\",\")\nmy_data.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_my_reviews = []\nfor review in my_data['review']:\n    clean_my_reviews.append(preprocessing(review, remove_stopwords =True))\nclean_my_df = pd.DataFrame({'review': clean_my_reviews,'id': my_data['id']})\nmy_id = np.array(my_data['id'])\n\ntokenizer.fit_on_texts(clean_my_reviews)\ntext_sequences2 = tokenizer.texts_to_sequences(clean_my_reviews)\nmy_inputs = pad_sequences(text_sequences2,maxlen=MAX_SEQUENCE_LENGTH,padding='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.save(open(\"my_input.npy\",\"wb\"),my_inputs)\nnp.save(open(\"my_id.npy\",\"wb\"),my_id)\nclean_my_df.to_csv(\"my_clean.csv\",index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_data2 = pd.read_csv(\"./my_clean.csv\",header = 0,delimiter=\",\")\n\nmyDataVecs = vectorizer.transform(my_data2['review'].values.astype('U'))\n\nmy_predicted = lgs.predict(myDataVecs)\nprint(my_predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"texts = list(my_data2['review'])\nres = pd.DataFrame({'review':texts,'sentiment':my_predicted})\nres.to_csv(\"check_this_one.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}