{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-09T15:04:46.938512Z","iopub.execute_input":"2021-08-09T15:04:46.938854Z","iopub.status.idle":"2021-08-09T15:04:46.952054Z","shell.execute_reply.started":"2021-08-09T15:04:46.938779Z","shell.execute_reply":"2021-08-09T15:04:46.950998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extract zip files\nimport zipfile, os\ninput_dir = '/kaggle/input/word2vec-nlp-tutorial/'\nwork_dir = '/kaggle/working/'\nzip_files = ['labeledTrainData.tsv.zip', 'testData.tsv.zip']\nfor zf in zip_files:\n    zipfile.ZipFile(os.path.join(input_dir, zf), 'r').extractall('./')\n\nos.listdir(work_dir)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T15:04:46.956642Z","iopub.execute_input":"2021-08-09T15:04:46.956882Z","iopub.status.idle":"2021-08-09T15:04:48.522415Z","shell.execute_reply.started":"2021-08-09T15:04:46.956859Z","shell.execute_reply":"2021-08-09T15:04:48.52044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating train dataframes\ntrain_df = pd.read_csv(os.path.join(work_dir, 'labeledTrainData.tsv'), sep='\\t')\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T15:04:48.524158Z","iopub.execute_input":"2021-08-09T15:04:48.52449Z","iopub.status.idle":"2021-08-09T15:04:49.113426Z","shell.execute_reply.started":"2021-08-09T15:04:48.524454Z","shell.execute_reply":"2021-08-09T15:04:49.112496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test dataframe\ntest_df = pd.read_csv(os.path.join(work_dir, 'testData.tsv'), sep='\\t')\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T15:04:49.117771Z","iopub.execute_input":"2021-08-09T15:04:49.118125Z","iopub.status.idle":"2021-08-09T15:04:49.452078Z","shell.execute_reply.started":"2021-08-09T15:04:49.118083Z","shell.execute_reply":"2021-08-09T15:04:49.451238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# shape\nprint(\"Train dataset Shape:\", train_df.shape)\nprint(\"Test dataset Shape:\", test_df.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T15:04:49.454143Z","iopub.execute_input":"2021-08-09T15:04:49.454652Z","iopub.status.idle":"2021-08-09T15:04:49.4603Z","shell.execute_reply.started":"2021-08-09T15:04:49.454606Z","shell.execute_reply":"2021-08-09T15:04:49.45931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train dataframe info\ntrain_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T15:04:49.461828Z","iopub.execute_input":"2021-08-09T15:04:49.46222Z","iopub.status.idle":"2021-08-09T15:04:49.499728Z","shell.execute_reply.started":"2021-08-09T15:04:49.46218Z","shell.execute_reply":"2021-08-09T15:04:49.493019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test dataframe info\ntest_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T15:04:49.50234Z","iopub.execute_input":"2021-08-09T15:04:49.503383Z","iopub.status.idle":"2021-08-09T15:04:49.527391Z","shell.execute_reply.started":"2021-08-09T15:04:49.503309Z","shell.execute_reply":"2021-08-09T15:04:49.526365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sentiment Classes Stats\nprint(train_df['sentiment'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-08-09T15:04:49.528931Z","iopub.execute_input":"2021-08-09T15:04:49.529321Z","iopub.status.idle":"2021-08-09T15:04:49.540134Z","shell.execute_reply.started":"2021-08-09T15:04:49.529283Z","shell.execute_reply":"2021-08-09T15:04:49.538495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# words distribution over sentences \ndef eda_words(sentences):\n    sentences_df = pd.DataFrame({'review': sentences})\n    sentences_df['word_n'] = sentences_df['review'].apply(lambda x : len(x.split(' ')))\n    fig=plt.figure(figsize=(50,4))\n    fig.add_subplot(1,2,1)\n    sns.histplot(data=sentences_df['word_n'], color='blue').set_title('Words Number Distribution')\neda_words(train_df['review'])","metadata":{"execution":{"iopub.status.busy":"2021-08-09T15:04:49.541587Z","iopub.execute_input":"2021-08-09T15:04:49.54225Z","iopub.status.idle":"2021-08-09T15:04:51.331663Z","shell.execute_reply.started":"2021-08-09T15:04:49.542205Z","shell.execute_reply":"2021-08-09T15:04:51.330912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# getting validation dataset 80:20\nsplit_perc = 0.8\nsplit_at = int(len(train_df['review'])*split_perc)\ntrain_sentences = train_df['review'][:split_at]\ntrain_labels = train_df['sentiment'][:split_at]\n\nvalidation_sentences = train_df['review'][split_at:]\nvalidation_labels = train_df['sentiment'][split_at:]\n\ntest_sentences = test_df['review']\n","metadata":{"execution":{"iopub.status.busy":"2021-08-09T15:04:51.334654Z","iopub.execute_input":"2021-08-09T15:04:51.334916Z","iopub.status.idle":"2021-08-09T15:04:51.340181Z","shell.execute_reply.started":"2021-08-09T15:04:51.334882Z","shell.execute_reply":"2021-08-09T15:04:51.33914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom bs4 import BeautifulSoup \nenglish_stopwords = stopwords.words(\"english\")\n\n# cleaning sentences \ndef clean_sentences(sentences):\n    out = []\n    for sentence in sentences:\n        # Lowering\n        sentence = sentence.lower()\n        # Removing html\n        sentence = BeautifulSoup(sentence,).get_text()\n        # Removing Urls\n        sentence = re.sub(\"https?:\\/\\/[\\w+.\\/]+\", \" \", sentence)\n        # Remove non-letters\n        sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence) \n        # Removing stop words\n        for word in english_stopwords:\n            token = \" \" + word + \" \"\n            sentence = sentence.replace(token, \" \").replace(\"  \", \" \")\n        out.append(sentence)\n    return out\n\ntrain_sentences = clean_sentences(train_sentences)\nvalidation_sentences = clean_sentences(validation_sentences)\ntest_sentences = clean_sentences(test_sentences)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T15:04:51.342272Z","iopub.execute_input":"2021-08-09T15:04:51.342693Z","iopub.status.idle":"2021-08-09T15:05:44.379956Z","shell.execute_reply.started":"2021-08-09T15:04:51.342655Z","shell.execute_reply":"2021-08-09T15:05:44.379063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_sentences[:1])","metadata":{"execution":{"iopub.status.busy":"2021-08-09T15:05:44.381384Z","iopub.execute_input":"2021-08-09T15:05:44.381792Z","iopub.status.idle":"2021-08-09T15:05:44.386935Z","shell.execute_reply.started":"2021-08-09T15:05:44.381727Z","shell.execute_reply":"2021-08-09T15:05:44.38595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# words distribution over sentences after cleaning\neda_words(train_sentences)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T15:05:44.38831Z","iopub.execute_input":"2021-08-09T15:05:44.388656Z","iopub.status.idle":"2021-08-09T15:05:45.39201Z","shell.execute_reply.started":"2021-08-09T15:05:44.388621Z","shell.execute_reply":"2021-08-09T15:05:45.390806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# get vocabulary with Tokenizer \ntokenizer = Tokenizer(oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(train_sentences)\nword_index = tokenizer.word_index\nprint(\"TOTAL WORDS:\", len(word_index))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T15:05:45.393651Z","iopub.execute_input":"2021-08-09T15:05:45.394176Z","iopub.status.idle":"2021-08-09T15:05:51.704164Z","shell.execute_reply.started":"2021-08-09T15:05:45.394135Z","shell.execute_reply":"2021-08-09T15:05:51.702291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# words frequency graph\nfrom collections import OrderedDict \nwords_count = tokenizer.word_counts\nordered_words_count = (OrderedDict(sorted(words_count.items(), key=lambda t: t[1], reverse=True))) \nplt.plot(range(len(ordered_words_count)),ordered_words_count.values())\n#plt.axis([0,10000,0,2000])\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-09T15:05:51.70572Z","iopub.execute_input":"2021-08-09T15:05:51.706228Z","iopub.status.idle":"2021-08-09T15:05:51.959349Z","shell.execute_reply.started":"2021-08-09T15:05:51.706181Z","shell.execute_reply":"2021-08-09T15:05:51.958173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# over 5000 word index we have few examples\nvocab_size = 5000\n# over 500 word count we have few sentences\nsequence_length = 500","metadata":{"execution":{"iopub.status.busy":"2021-08-09T15:05:51.960954Z","iopub.execute_input":"2021-08-09T15:05:51.961301Z","iopub.status.idle":"2021-08-09T15:05:51.966499Z","shell.execute_reply.started":"2021-08-09T15:05:51.961265Z","shell.execute_reply":"2021-08-09T15:05:51.9651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get vocabulary with Tokenizer \ntokenizer = Tokenizer(oov_token=\"<OOV>\", num_words=vocab_size)\ntokenizer.fit_on_texts(train_sentences)\nword_index = tokenizer.word_index","metadata":{"execution":{"iopub.status.busy":"2021-08-09T15:05:51.96845Z","iopub.execute_input":"2021-08-09T15:05:51.968809Z","iopub.status.idle":"2021-08-09T15:05:54.667061Z","shell.execute_reply.started":"2021-08-09T15:05:51.968776Z","shell.execute_reply":"2021-08-09T15:05:54.666099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sequencing and padding\n# sequences of numbers from sentences\ntrain_sequences = tokenizer.texts_to_sequences(train_sentences)\nvalidation_sequences = tokenizer.texts_to_sequences(validation_sentences)\ntest_sequences = tokenizer.texts_to_sequences(test_sentences)\n                                               \n# padding\ntrain_padded = pad_sequences(train_sequences, padding='post', maxlen=sequence_length)\nvalidation_padded = pad_sequences(validation_sequences, padding='post', maxlen=sequence_length)\ntest_padded = pad_sequences(test_sequences, padding='post', maxlen=sequence_length)\nprint(sequence_length)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T15:05:54.668495Z","iopub.execute_input":"2021-08-09T15:05:54.669051Z","iopub.status.idle":"2021-08-09T15:05:59.618499Z","shell.execute_reply.started":"2021-08-09T15:05:54.669012Z","shell.execute_reply":"2021-08-09T15:05:59.617485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub ","metadata":{"execution":{"iopub.status.busy":"2021-08-09T15:05:59.619849Z","iopub.execute_input":"2021-08-09T15:05:59.620376Z","iopub.status.idle":"2021-08-09T15:05:59.783773Z","shell.execute_reply.started":"2021-08-09T15:05:59.620338Z","shell.execute_reply":"2021-08-09T15:05:59.782941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for performances\nAUTOTUNE = tf.data.AUTOTUNE\n# converting to TensorFlow Dataset\ntrain_dataset = tf.data.Dataset.from_tensor_slices((train_padded, train_labels.to_numpy())).cache().prefetch(buffer_size=AUTOTUNE)\nvalidation_dataset = tf.data.Dataset.from_tensor_slices((validation_padded, validation_labels.to_numpy())).cache().prefetch(buffer_size=AUTOTUNE)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-09T15:05:59.785057Z","iopub.execute_input":"2021-08-09T15:05:59.785456Z","iopub.status.idle":"2021-08-09T15:06:01.543867Z","shell.execute_reply.started":"2021-08-09T15:05:59.785415Z","shell.execute_reply":"2021-08-09T15:06:01.543004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# how to decode an encoded sequence of numbers\nreversed_word_index = { v: k for k, v in word_index.items() }\ndef decode_review(sequence):\n    return ' '.join([reversed_word_index.get(i, '?') for i in sequence if i != 0])\n\n# verify the decoding of an example sentence\nprint(\"Original Sentence: \", train_sentences[0])\nprint(\"Encoded Sequence of numbers: \", train_padded[0])\nprint(\"Decoded Sequence: \", decode_review(train_padded[0]))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T15:06:01.545092Z","iopub.execute_input":"2021-08-09T15:06:01.545414Z","iopub.status.idle":"2021-08-09T15:06:01.571934Z","shell.execute_reply.started":"2021-08-09T15:06:01.545381Z","shell.execute_reply":"2021-08-09T15:06:01.570823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# GloVe has been trained from Billions words and has several space dimensions\nglove_wikipedia = False\nif glove_wikipedia:\n    # download and load embedding weight from GloVe! https://nlp.stanford.edu/projects/glove/ \n    # Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 50d, 100d, 200d, & 300d vectors, 822 MB download):\n    !wget --no-check-certificate http://nlp.stanford.edu/data/glove.6B.zip -O /tmp/glove.6B.100d.txt.zip\n    !unzip /tmp/glove.6B.100d.txt.zip -d /tmp\n    glove_filename = 'glove.6B.100d.txt'\n    embedding_dim = 100\nelse:\n    # Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased, 25d, 50d, 100d, & 200d vectors, 1.42 GB download)\n    !wget --no-check-certificate https://nlp.stanford.edu/data/glove.twitter.27B.zip  -O /tmp/glove.twitter.27B.zip\n    !unzip /tmp/glove.twitter.27B.zip -d /tmp\n    glove_filename = 'glove.twitter.27B.25d.txt'\n    embedding_dim = 25","metadata":{"execution":{"iopub.status.busy":"2021-08-09T15:06:01.573263Z","iopub.execute_input":"2021-08-09T15:06:01.573607Z","iopub.status.idle":"2021-08-09T15:11:25.986534Z","shell.execute_reply.started":"2021-08-09T15:06:01.573572Z","shell.execute_reply":"2021-08-09T15:11:25.985467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading original embedding matrix\nembeddings_index = {}\nwith open(f\"/tmp/{glove_filename}\") as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n\n# trasforming original embedding weights through our vocabulary\nembeddings_matrix = np.zeros((vocab_size+1, embedding_dim))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None and i < vocab_size+1:\n        embeddings_matrix[i] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2021-08-09T15:11:25.98906Z","iopub.execute_input":"2021-08-09T15:11:25.989357Z","iopub.status.idle":"2021-08-09T15:11:45.608513Z","shell.execute_reply.started":"2021-08-09T15:11:25.989328Z","shell.execute_reply":"2021-08-09T15:11:45.607509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(embedding_dim)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T15:11:45.610238Z","iopub.execute_input":"2021-08-09T15:11:45.610722Z","iopub.status.idle":"2021-08-09T15:11:45.61801Z","shell.execute_reply.started":"2021-08-09T15:11:45.610674Z","shell.execute_reply":"2021-08-09T15:11:45.616606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Our model definition\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, weights=[embeddings_matrix], trainable=True),   \n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim, return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)), \n    tf.keras.layers.Dropout(.2), \n    tf.keras.layers.Dense(64, activation='relu'),     \n    tf.keras.layers.Dense(1)\n])\n\nmodel.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              optimizer=tf.keras.optimizers.Adam() , \n              metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2021-08-09T15:31:00.281247Z","iopub.execute_input":"2021-08-09T15:31:00.281588Z","iopub.status.idle":"2021-08-09T15:31:00.727009Z","shell.execute_reply.started":"2021-08-09T15:31:00.281558Z","shell.execute_reply":"2021-08-09T15:31:00.726126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# some smart callbacks\nearly_stopping = tf.keras.callbacks.EarlyStopping(patience=5, monitor='val_loss')\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n                        patience=5, mode='min',\n                        verbose=1)\ncheckpoint_filepath = './model-best.h5'\nmodel_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath, \n                                                      monitor='val_accuracy',\n                                                      mode='max',\n                                                      save_best_only=True)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-09T15:31:02.823377Z","iopub.execute_input":"2021-08-09T15:31:02.823702Z","iopub.status.idle":"2021-08-09T15:31:02.830414Z","shell.execute_reply.started":"2021-08-09T15:31:02.823672Z","shell.execute_reply":"2021-08-09T15:31:02.829482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training \nepochs_num=50\nhistory = model.fit(train_dataset.shuffle(10000).batch(512),\n                    epochs=epochs_num, \n                    validation_data=validation_dataset.batch(512),\n                    callbacks=[early_stopping, reduce_lr, model_checkpoint]\n                   )","metadata":{"execution":{"iopub.status.busy":"2021-08-09T15:31:05.666534Z","iopub.execute_input":"2021-08-09T15:31:05.66688Z","iopub.status.idle":"2021-08-09T15:33:25.38514Z","shell.execute_reply.started":"2021-08-09T15:31:05.66685Z","shell.execute_reply":"2021-08-09T15:33:25.384313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show loss and accuracy\ndef show_loss_accuracy(history):\n  acc = history.history['accuracy']\n  val_acc = history.history['val_accuracy']\n  loss = history.history['loss']\n  val_loss = history.history['val_loss']\n\n  epochs_range = range(len(acc))\n\n  plt.figure(figsize=(20, 5))\n  plt.subplot(1, 2, 1)\n  plt.plot(epochs_range, acc, label='Training Accuracy')\n  plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n  plt.legend(loc='lower right')\n  plt.title('Training and Validation Accuracy')\n  plt.subplot(1, 2, 2)\n  plt.plot(epochs_range, loss, label='Training Loss')\n  plt.plot(epochs_range, val_loss, label='Validation Loss')\n  plt.legend(loc='upper right')\n  plt.title('Training and Validation Loss')\n  plt.show()\nshow_loss_accuracy(history)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T15:34:26.994206Z","iopub.execute_input":"2021-08-09T15:34:26.994528Z","iopub.status.idle":"2021-08-09T15:34:27.286753Z","shell.execute_reply.started":"2021-08-09T15:34:26.994498Z","shell.execute_reply":"2021-08-09T15:34:27.285722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading best model trained\nmodel.load_weights(checkpoint_filepath)\n# prediction on test data\npredictions = model.predict(test_padded)\n# apply a sigmoid because our model returns logits\npredictions = tf.nn.sigmoid(predictions)\npredictions = tf.where(predictions < 0.5, 0, 1)\ntest_df['sentiment'] = predictions.numpy()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T15:34:35.026273Z","iopub.execute_input":"2021-08-09T15:34:35.02661Z","iopub.status.idle":"2021-08-09T15:34:48.899387Z","shell.execute_reply.started":"2021-08-09T15:34:35.026572Z","shell.execute_reply":"2021-08-09T15:34:48.898341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T15:34:53.079672Z","iopub.execute_input":"2021-08-09T15:34:53.080016Z","iopub.status.idle":"2021-08-09T15:34:53.09137Z","shell.execute_reply.started":"2021-08-09T15:34:53.079984Z","shell.execute_reply":"2021-08-09T15:34:53.09045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission csv creation\nsubmission_df = test_df.copy()\nsubmission_df.drop(['review'], axis=1, inplace=True)\nsubmission_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T14:34:35.547396Z","iopub.execute_input":"2021-08-09T14:34:35.547731Z","iopub.status.idle":"2021-08-09T14:34:35.623603Z","shell.execute_reply.started":"2021-08-09T14:34:35.547701Z","shell.execute_reply":"2021-08-09T14:34:35.622777Z"},"trusted":true},"execution_count":null,"outputs":[]}]}