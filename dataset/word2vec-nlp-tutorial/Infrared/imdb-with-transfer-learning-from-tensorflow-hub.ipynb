{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-09T14:51:12.130685Z","iopub.execute_input":"2021-08-09T14:51:12.130995Z","iopub.status.idle":"2021-08-09T14:51:12.143497Z","shell.execute_reply.started":"2021-08-09T14:51:12.130926Z","shell.execute_reply":"2021-08-09T14:51:12.142645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extract zip files\nimport zipfile, os\ninput_dir = '/kaggle/input/word2vec-nlp-tutorial/'\nwork_dir = '/kaggle/working/'\nzip_files = ['labeledTrainData.tsv.zip', 'testData.tsv.zip']\nfor zf in zip_files:\n    zipfile.ZipFile(os.path.join(input_dir, zf), 'r').extractall('./')\n\nos.listdir(work_dir)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T14:51:12.150163Z","iopub.execute_input":"2021-08-09T14:51:12.15042Z","iopub.status.idle":"2021-08-09T14:51:13.811853Z","shell.execute_reply.started":"2021-08-09T14:51:12.150397Z","shell.execute_reply":"2021-08-09T14:51:13.811049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating train dataframes\ntrain_df = pd.read_csv(os.path.join(work_dir, 'labeledTrainData.tsv'), sep='\\t')\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T14:51:13.813258Z","iopub.execute_input":"2021-08-09T14:51:13.813587Z","iopub.status.idle":"2021-08-09T14:51:14.246927Z","shell.execute_reply.started":"2021-08-09T14:51:13.813553Z","shell.execute_reply":"2021-08-09T14:51:14.245987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test dataframe\ntest_df = pd.read_csv(os.path.join(work_dir, 'testData.tsv'), sep='\\t')\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T14:51:14.252108Z","iopub.execute_input":"2021-08-09T14:51:14.252477Z","iopub.status.idle":"2021-08-09T14:51:14.585905Z","shell.execute_reply.started":"2021-08-09T14:51:14.25244Z","shell.execute_reply":"2021-08-09T14:51:14.584929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# shape\nprint(\"Train dataset Shape:\", train_df.shape)\nprint(\"Test dataset Shape:\", test_df.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T14:51:14.587399Z","iopub.execute_input":"2021-08-09T14:51:14.587756Z","iopub.status.idle":"2021-08-09T14:51:14.595352Z","shell.execute_reply.started":"2021-08-09T14:51:14.587719Z","shell.execute_reply":"2021-08-09T14:51:14.592871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train dataframe info\ntrain_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T14:51:14.596468Z","iopub.execute_input":"2021-08-09T14:51:14.596789Z","iopub.status.idle":"2021-08-09T14:51:14.627958Z","shell.execute_reply.started":"2021-08-09T14:51:14.596754Z","shell.execute_reply":"2021-08-09T14:51:14.627201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test dataframe info\ntest_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T14:51:14.629126Z","iopub.execute_input":"2021-08-09T14:51:14.629655Z","iopub.status.idle":"2021-08-09T14:51:14.649693Z","shell.execute_reply.started":"2021-08-09T14:51:14.629621Z","shell.execute_reply":"2021-08-09T14:51:14.648922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sentiment Classes Stats\nprint(train_df['sentiment'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-08-09T14:51:14.651021Z","iopub.execute_input":"2021-08-09T14:51:14.65158Z","iopub.status.idle":"2021-08-09T14:51:14.658586Z","shell.execute_reply.started":"2021-08-09T14:51:14.651545Z","shell.execute_reply":"2021-08-09T14:51:14.657549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# getting validation dataset 80:20\nsplit_perc = 0.8\nsplit_at = int(len(train_df['review'])*split_perc)\ntrain_sentences = train_df['review'][:split_at]\ntrain_labels = train_df['sentiment'][:split_at]\n\nvalidation_sentences = train_df['review'][split_at:]\nvalidation_labels = train_df['sentiment'][split_at:]\n\ntest_sentences = test_df['review']\n","metadata":{"execution":{"iopub.status.busy":"2021-08-09T14:51:14.662015Z","iopub.execute_input":"2021-08-09T14:51:14.662557Z","iopub.status.idle":"2021-08-09T14:51:14.671375Z","shell.execute_reply.started":"2021-08-09T14:51:14.662522Z","shell.execute_reply":"2021-08-09T14:51:14.669794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom bs4 import BeautifulSoup \nenglish_stopwords = stopwords.words(\"english\")\n\n# cleaning sentences \ndef clean_sentences(sentences):\n    out = []\n    for sentence in sentences:\n        # Lowering\n        sentence = sentence.lower()\n        # Removing html\n        sentence = BeautifulSoup(sentence,).get_text()\n        # Removing Urls\n        sentence = re.sub(\"https?:\\/\\/[\\w+.\\/]+\", \" \", sentence)\n        # Remove non-letters\n        sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence) \n        # Removing stop words\n        for word in english_stopwords:\n            token = \" \" + word + \" \"\n            sentence = sentence.replace(token, \" \").replace(\"  \", \" \")\n        out.append(sentence)\n    return out\n\ntrain_sentences = clean_sentences(train_sentences)\nvalidation_sentences = clean_sentences(validation_sentences)\ntest_sentences = clean_sentences(test_sentences)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T14:51:14.673309Z","iopub.execute_input":"2021-08-09T14:51:14.674074Z","iopub.status.idle":"2021-08-09T14:52:09.091609Z","shell.execute_reply.started":"2021-08-09T14:51:14.674033Z","shell.execute_reply":"2021-08-09T14:52:09.090594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_sentences[:1])","metadata":{"execution":{"iopub.status.busy":"2021-08-09T14:52:09.093227Z","iopub.execute_input":"2021-08-09T14:52:09.093569Z","iopub.status.idle":"2021-08-09T14:52:09.09972Z","shell.execute_reply.started":"2021-08-09T14:52:09.093533Z","shell.execute_reply":"2021-08-09T14:52:09.09869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub ","metadata":{"execution":{"iopub.status.busy":"2021-08-09T14:52:09.10139Z","iopub.execute_input":"2021-08-09T14:52:09.102085Z","iopub.status.idle":"2021-08-09T14:52:13.470652Z","shell.execute_reply.started":"2021-08-09T14:52:09.102044Z","shell.execute_reply":"2021-08-09T14:52:13.469815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for performances\nAUTOTUNE = tf.data.AUTOTUNE\n# converting to TensorFlow Dataset\ntrain_dataset = tf.data.Dataset.from_tensor_slices((train_sentences, train_labels.to_numpy())).cache().prefetch(buffer_size=AUTOTUNE)\nvalidation_dataset = tf.data.Dataset.from_tensor_slices((validation_sentences, validation_labels.to_numpy())).cache().prefetch(buffer_size=AUTOTUNE)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-09T14:52:13.471948Z","iopub.execute_input":"2021-08-09T14:52:13.472276Z","iopub.status.idle":"2021-08-09T14:52:15.323753Z","shell.execute_reply.started":"2021-08-09T14:52:13.472243Z","shell.execute_reply":"2021-08-09T14:52:15.322942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# transfer leraning from hub layer\nhub_layer = hub.KerasLayer(\"https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2\", input_shape=[], dtype=tf.string, trainable=True)\n# Our model definition\nmodel = tf.keras.Sequential([\n    hub_layer,\n    tf.keras.layers.Dense(16, activation='relu', kernel_regularizer = tf.keras.regularizers.l2(0.01)), \n    tf.keras.layers.Dropout(.2),\n    tf.keras.layers.Dense(1)\n])\nmodel.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False), \n              metrics=['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2021-08-09T14:53:50.02973Z","iopub.execute_input":"2021-08-09T14:53:50.030157Z","iopub.status.idle":"2021-08-09T14:53:51.467505Z","shell.execute_reply.started":"2021-08-09T14:53:50.030118Z","shell.execute_reply":"2021-08-09T14:53:51.466726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# some smart callbacks\nearly_stopping = tf.keras.callbacks.EarlyStopping(patience=5, monitor='val_loss')\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n                        patience=5, mode='min',\n                        verbose=1)\ncheckpoint_filepath = './model-best.h5'\nmodel_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath, \n                                                      monitor='val_accuracy',\n                                                      mode='max',\n                                                      save_best_only=True)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-09T14:52:23.201192Z","iopub.execute_input":"2021-08-09T14:52:23.201516Z","iopub.status.idle":"2021-08-09T14:52:23.207041Z","shell.execute_reply.started":"2021-08-09T14:52:23.201483Z","shell.execute_reply":"2021-08-09T14:52:23.205804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training \nepochs_num=100\nhistory = model.fit(train_dataset.shuffle(10000).batch(512),\n                    epochs=epochs_num, \n                    validation_data=validation_dataset.batch(512),\n                    callbacks=[early_stopping, reduce_lr, model_checkpoint]\n                   )","metadata":{"execution":{"iopub.status.busy":"2021-08-09T14:56:12.520581Z","iopub.execute_input":"2021-08-09T14:56:12.520938Z","iopub.status.idle":"2021-08-09T14:57:03.794087Z","shell.execute_reply.started":"2021-08-09T14:56:12.520907Z","shell.execute_reply":"2021-08-09T14:57:03.793228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show loss and accuracy\nimport matplotlib.pyplot as plt\ndef show_loss_accuracy(history):\n  acc = history.history['accuracy']\n  val_acc = history.history['val_accuracy']\n  loss = history.history['loss']\n  val_loss = history.history['val_loss']\n\n  epochs_range = range(len(acc))\n\n  plt.figure(figsize=(20, 5))\n  plt.subplot(1, 2, 1)\n  plt.plot(epochs_range, acc, label='Training Accuracy')\n  plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n  plt.legend(loc='lower right')\n  plt.title('Training and Validation Accuracy')\n  plt.subplot(1, 2, 2)\n  plt.plot(epochs_range, loss, label='Training Loss')\n  plt.plot(epochs_range, val_loss, label='Validation Loss')\n  plt.legend(loc='upper right')\n  plt.title('Training and Validation Loss')\n  plt.show()\nshow_loss_accuracy(history)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T14:57:11.944838Z","iopub.execute_input":"2021-08-09T14:57:11.945172Z","iopub.status.idle":"2021-08-09T14:57:12.202806Z","shell.execute_reply.started":"2021-08-09T14:57:11.945136Z","shell.execute_reply":"2021-08-09T14:57:12.201849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading best model trained\nmodel.load_weights(checkpoint_filepath)\n# prediction on test data\npredictions = model.predict(test_sentences)\n# apply a sigmoid because our model returns logits\npredictions = tf.nn.sigmoid(predictions)\npredictions = tf.where(predictions < 0.5, 0, 1)\ntest_df['sentiment'] = predictions.numpy()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T14:53:43.900794Z","iopub.execute_input":"2021-08-09T14:53:43.901121Z","iopub.status.idle":"2021-08-09T14:53:46.622434Z","shell.execute_reply.started":"2021-08-09T14:53:43.901088Z","shell.execute_reply":"2021-08-09T14:53:46.621605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T14:53:46.625766Z","iopub.execute_input":"2021-08-09T14:53:46.626046Z","iopub.status.idle":"2021-08-09T14:53:46.640174Z","shell.execute_reply.started":"2021-08-09T14:53:46.62602Z","shell.execute_reply":"2021-08-09T14:53:46.639328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission csv creation\nsubmission_df = test_df.copy()\nsubmission_df.drop(['review'], axis=1, inplace=True)\nsubmission_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T14:53:46.643461Z","iopub.execute_input":"2021-08-09T14:53:46.643745Z","iopub.status.idle":"2021-08-09T14:53:46.694937Z","shell.execute_reply.started":"2021-08-09T14:53:46.643696Z","shell.execute_reply":"2021-08-09T14:53:46.694235Z"},"trusted":true},"execution_count":null,"outputs":[]}]}