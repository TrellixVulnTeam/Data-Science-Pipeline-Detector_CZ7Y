{"cells":[{"metadata":{"trusted":true,"_uuid":"964c9bb2d7ce22075f233b86e37d352fc58f1e0f"},"cell_type":"code","source":"#importing pandas\nimport pandas as pd\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8387d2d23a7724f89c9c55adc8e40f72eba19a4c"},"cell_type":"code","source":"#reading the file\ntrain = pd.read_csv(\"../input/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"68a6535f44f0474af875565ab41974c51af23f8b"},"cell_type":"code","source":"#exploration again\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ec1015ae9d16e15a7586c96922163c9589e39e1b"},"cell_type":"code","source":"#exploring\nprint(train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"367229d21c0785247c7123c0c5fe4ac9f3132f74"},"cell_type":"code","source":"#exploring again\nprint(train.columns.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6bd5b5369f7f5853eb911976454c77ced67e3eeb"},"cell_type":"code","source":"#viewing the structure of data we need to work on\nprint(train.review[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"44da0de08a1c6d9af792bb32e10027a5e5277174"},"cell_type":"code","source":"#using BeautifulSoup to clean data initially\nfrom bs4 import BeautifulSoup\n##the html tags and comments etc are reomved and stored as example1\nexample1 = BeautifulSoup(train.review[0],\"html.parser\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c63171fe8ea15fd570c5e5c842c2c86d431a7d7c"},"cell_type":"code","source":"##by using .get_text() method we can see the only texts in the html document\n#it is also better as compared to the raw html doc\nprint(example1.get_text())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"22387c5c18f7e7f3898afd0007ea96bec5f6722c"},"cell_type":"code","source":"#removing numbers\nimport re\n# a '^' within square brackets searches anything other than the one on it\n# hence here it matches everything numbers and punctuations etc , leaving only the words\nletters_only = re.sub(\"[^a-zA-Z]\",\" \",example1.get_text())\nprint(letters_only)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6edeb02664200570fe667452f46c9d7409c5c306"},"cell_type":"code","source":"## changing all the words to lowercase to create a bag of words later\nlower_case = letters_only.lower()\n# the whole doc is now split to create an array from which most common words called \"stop words\" will be removed\nwords = lower_case.split()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b2756801aa6a8063f0e7155bca03f949c985b9d7"},"cell_type":"code","source":"#importing stopwords from nltk\nfrom nltk.corpus import stopwords\n#some stopwords in english language are\nprint(stopwords.words(\"english\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"cc8020edd4ea4750d58786ccb01201f5ce68187f"},"cell_type":"code","source":"##removing most common words from doc\nwords = [w for w in words if w not in stopwords.words(\"english\")]\nprint(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"46cdc3c1265f3c3570e0765fd3147e4d7f8f2121"},"cell_type":"code","source":"# the above code cleans only one review , let's make a function from above code that can clean all the reviews\ndef review_to_words(raw_review):\n    #remove html using BeautifulSoup\n    review_text = BeautifulSoup(raw_review,\"html.parser\").get_text()\n    #removing raw letters,numbers,punctuations\n    letters_only = re.sub(\"[^a-zA-Z]\",\" \",review_text)\n    #creating an array , resolving whitespaces\n    words = letters_only.lower().split()\n    #create an array of stopwords so that we don't have to access corpus to search for a stopword\n    stop = set(stopwords.words(\"english\"))\n    #removing stopwords from the raw_review\n    meaningful_words = [w for w in words if w not in stop]\n    #return a string with only the words that are important\n    return(\" \".join(meaningful_words))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5a456861733f952fc1900b1ac99b0e68544679cb"},"cell_type":"code","source":"#checking if our function works properly\ntrial_review = review_to_words(train.review[0])\nprint(trial_review)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"969c4d5aa5d5d20405ede1b03fa8b1a35aa5100b"},"cell_type":"code","source":"#finding the number of reviews\nnum_reviews = train.review.size\nprint(\"the number of reviews>>>>>>> :\",num_reviews)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7151d0d0c89e2780a0c0ad83c8b58eaa7ef63c8e"},"cell_type":"code","source":"#storing all reviews at one place\nclean_train_reviews = []\nfor i in range(num_reviews):\n    clean_train_reviews.append(review_to_words(train.review[i]))\n    print(\"cleaned review number> \",i,\"Done\")\nprint(\"cleaning is completed\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3ae2e5fada3771f890d854299098b10491896c02"},"cell_type":"code","source":"print(\"we are Creating a bag of words . . . . . \")\n#import CountVectorizer to create token counts of document\nfrom sklearn.feature_extraction.text import CountVectorizer\n#initializing the parameters as None so that we can write and manipulate the processing by our own\nvectorizer = CountVectorizer(analyzer=\"word\",\n                            tokenizer=None,\n                            preprocessor=None,\n                            stop_words=None,\n                            max_features=5000)\n#train the classifer using fit_transform() method\ntrain_data_features = vectorizer.fit_transform(clean_train_reviews)\n#change the classifier into array\ntrain_data_features = train_data_features.toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7fe5e178d584e9dc81cd6850846d24b69b254ad6"},"cell_type":"code","source":"print(train_data_features.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"198b8f1e5f45db1bc8500cae9a1b96ffaf0d8e6e"},"cell_type":"code","source":"#see all the features names\nvocab = vectorizer.get_feature_names()\nprint(\" , \".join(vocab[0:10]),\" . . . . \",\" , \".join(vocab[-10:]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2f015e8a65b005c3344ef619dd5e8ab0d35a668d"},"cell_type":"code","source":"import numpy as np\n#frequency of each word is found using np.sum()\ndist = np.sum(train_data_features,axis=0)\nct = 0\nfor tag,count in zip(vocab,dist):\n    print(tag,\":\",count,end=\" \")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"99f654b4704658064d2ada572e2dfda2f3150569"},"cell_type":"code","source":"startswith = []\nfor val in vocab:\n    if(val[0] not in startswith):\n        startswith.append(val[0])\nprint(startswith)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"f674e814853443b7e089d44ab6cfd4ca9ae403c6"},"cell_type":"code","source":"#counting the total numbers of words starting\ncounts = np.zeros((len(startswith)),dtype=np.int)\nfor val in vocab:\n    index = startswith.index(val[0])\n    counts[index] += 1\nprint(counts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a36f7d5e03f34a8cdd47b910b110bd873ef6ff53"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure(1,figsize=(15,5))\nplt.plot(counts)\nnums = [i for i in range(26)]\nplt.xticks(nums,startswith)\nplt.grid()\nplt.ylabel(\"frequency\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"aff8ad4c31c50b861eadc615452fa8ad0f534735"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nforest = RandomForestClassifier(n_estimators = 100)\nprint(\"fitting RandomForest . . . \")\nforest = forest.fit(train_data_features,train[\"sentiment\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d48eb83dc1cc23f63a90160ceafb9f1f4206cddb"},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nnaive = MultinomialNB()\nprint(\"fitting NaiveBayes . . . \")\nnaive.fit(train_data_features,train[\"sentiment\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e58abc4b5d52884c708c604e4550302281076a2e"},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nadaboost = AdaBoostClassifier(n_estimators = 100)\nprint(\"fitting AdaBoost . . . \")\nadaboost.fit(train_data_features,train[\"sentiment\"])\nprint(\"fitting complete.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3b87b4deba0bb5f30eea65fab1cf06699edf788a"},"cell_type":"code","source":"test = pd.read_csv(\"../input/testData.tsv\",header=0,delimiter=\"\\t\",quoting=3)\nprint(\"shape :\",test.shape)\nprint(test.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1384bc870500e58c02135ebc7edfee76296468a2"},"cell_type":"code","source":"num_reviews = len(test[\"review\"])\nclean_test_reviews = []\nprint(\"Cleaning and parsing . . . . \")\nfor i in range(0,num_reviews):\n    clean_review = review_to_words(test[\"review\"][i])\n    clean_test_reviews.append(clean_review)\nprint(\"processing complete.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"bb6bd86ccfd4fa848bef122965b30954e6bfbd87"},"cell_type":"code","source":"test_data_features = vectorizer.fit_transform(clean_test_reviews)\ntest_data_features = test_data_features.toarray()\nprint(\"predicting using RandomForest . . . ..\")\nresult1 = forest.predict(test_data_features)\nprint(\"predicting using Naive Bayes . . ... \")\nresult2 = naive.predict(test_data_features)\nprint(\"predicting using AdaBoost . . ... \")\nresult3 = adaboost.predict(test_data_features)\nprint(\"process completed :) \")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a62459af276c04d61f0f073f72f17e8a3f4847d0"},"cell_type":"code","source":"result = result1+result2+result3\nfor i in range(25000):\n    if(result[i]==1):\n        result[i]=0\n    elif(result[i]==2):\n        result[i]=1\n    elif(result[i]==3):\n        result[i]=1\noutput = pd.DataFrame(data = {\"id\":test[\"id\"],\"sentiment\":result})\noutput.to_csv(\"Submit_output.csv\", index=False, quoting=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3027beac327606a4e890b865de8a042d1adbdae2"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d6d0a730d06e8e3dc8b2bd881a45ba5a7a468870"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}