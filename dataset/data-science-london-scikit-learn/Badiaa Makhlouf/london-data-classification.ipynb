{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-23T07:19:46.156532Z","iopub.execute_input":"2021-06-23T07:19:46.15695Z","iopub.status.idle":"2021-06-23T07:19:46.168456Z","shell.execute_reply.started":"2021-06-23T07:19:46.156918Z","shell.execute_reply":"2021-06-23T07:19:46.167227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%pwd","metadata":{"execution":{"iopub.status.busy":"2021-06-23T07:19:46.382575Z","iopub.execute_input":"2021-06-23T07:19:46.382967Z","iopub.status.idle":"2021-06-23T07:19:46.392578Z","shell.execute_reply.started":"2021-06-23T07:19:46.382933Z","shell.execute_reply":"2021-06-23T07:19:46.391576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Importing the required libraries ","metadata":{}},{"cell_type":"code","source":"import pandas as pd \nimport numpy as np \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.svm import SVC\nfrom sklearn import tree\nfrom sklearn.metrics import accuracy_score\nimport seaborn as sns\nimport matplotlib \nimport matplotlib.pyplot as plt\nmatplotlib.style.use('ggplot')\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier","metadata":{"execution":{"iopub.status.busy":"2021-06-23T07:19:47.183934Z","iopub.execute_input":"2021-06-23T07:19:47.184629Z","iopub.status.idle":"2021-06-23T07:19:48.471888Z","shell.execute_reply.started":"2021-06-23T07:19:47.184557Z","shell.execute_reply":"2021-06-23T07:19:48.470886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1- Reading the trainin and testing datasets","metadata":{}},{"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/data-science-london-scikit-learn/train.csv', sep=',', header=None)\ntest=pd.read_csv('/kaggle/input/data-science-london-scikit-learn/test.csv', sep=',', header=None)\ntrain_label=pd.read_csv('/kaggle/input/data-science-london-scikit-learn/trainLabels.csv', header=None)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T07:19:48.473421Z","iopub.execute_input":"2021-06-23T07:19:48.473703Z","iopub.status.idle":"2021-06-23T07:19:48.771781Z","shell.execute_reply.started":"2021-06-23T07:19:48.473675Z","shell.execute_reply":"2021-06-23T07:19:48.77077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-23T07:19:37.996674Z","iopub.status.idle":"2021-06-23T07:19:37.997317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-23T07:19:37.99972Z","iopub.status.idle":"2021-06-23T07:19:38.000815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.describe().transpose()","metadata":{"execution":{"iopub.status.busy":"2021-06-23T07:19:38.002752Z","iopub.status.idle":"2021-06-23T07:19:38.003932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe().transpose()","metadata":{"execution":{"iopub.status.busy":"2021-06-23T07:19:38.005179Z","iopub.status.idle":"2021-06-23T07:19:38.006011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Checking if there is any null or NaN value in both training and testing sets","metadata":{}},{"cell_type":"code","source":"print('Train: Does NaN values exist',train.isna().values.any())\nprint('Train: Does Null values exist',train.isnull().values.any())\nprint('Test: Does NaN values exist',test.isna().values.any())\nprint('Test: Does NaN exists',test.isnull().values.any())","metadata":{"execution":{"iopub.status.busy":"2021-06-23T07:19:55.426769Z","iopub.execute_input":"2021-06-23T07:19:55.427192Z","iopub.status.idle":"2021-06-23T07:19:55.43769Z","shell.execute_reply.started":"2021-06-23T07:19:55.427154Z","shell.execute_reply":"2021-06-23T07:19:55.436141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Counting the number of classes ==> To check if classes are well balanced","metadata":{}},{"cell_type":"code","source":"class_0=np.where(train_label==0)\nlen(class_0[0])\nclass_1=np.where(train_label==1)\nlen(class_1[0])\n\nprint('Number of samples for class 0:',len(class_0[0]) )\nprint('Number of samples for class 1:',len(class_1[0]))","metadata":{"execution":{"iopub.status.busy":"2021-06-23T07:19:55.923382Z","iopub.execute_input":"2021-06-23T07:19:55.923815Z","iopub.status.idle":"2021-06-23T07:19:55.94818Z","shell.execute_reply.started":"2021-06-23T07:19:55.923776Z","shell.execute_reply":"2021-06-23T07:19:55.94707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2- Splitting the total dataset into training and testing ","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(train, train_label, test_size=0.2, random_state=30)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T07:19:56.242698Z","iopub.execute_input":"2021-06-23T07:19:56.243132Z","iopub.status.idle":"2021-06-23T07:19:56.25513Z","shell.execute_reply.started":"2021-06-23T07:19:56.243096Z","shell.execute_reply":"2021-06-23T07:19:56.253877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Printing the dimensions of the datasets ","metadata":{}},{"cell_type":"code","source":"print('X train dimension:',X_train.shape)\nprint('X test dimension :', X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T07:19:56.942552Z","iopub.execute_input":"2021-06-23T07:19:56.942974Z","iopub.status.idle":"2021-06-23T07:19:56.948838Z","shell.execute_reply.started":"2021-06-23T07:19:56.942943Z","shell.execute_reply":"2021-06-23T07:19:56.947815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3- Data Normalisation and Standardization\n## 3.1- Data Normalisation ","metadata":{}},{"cell_type":"markdown","source":"The Normalization and the Standardization are called feature scaling techniques. Normalization is also known under the name Min-Max scaling. This technique consists of shifting and rescaling the values, using the minimum and the maximum values of the training dataset, so that they end up ranging between 0 and 1. \n\nStandardization is another scaling technique that consists of scaling the values using the mean and standard deviation of the training dataset. All the values must be centered around the mean value which is null and the standard deviation is 1. ","metadata":{}},{"cell_type":"code","source":"normalizer = MinMaxScaler()\nnormalized_train_X = normalizer.fit_transform(X_train)### we fit the normalizer on training data \n### Testing \nnormalized_test_X = normalizer.transform(X_test)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-23T07:19:57.582809Z","iopub.execute_input":"2021-06-23T07:19:57.583504Z","iopub.status.idle":"2021-06-23T07:19:57.598996Z","shell.execute_reply.started":"2021-06-23T07:19:57.583438Z","shell.execute_reply":"2021-06-23T07:19:57.597637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2- Data Standardization ","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\nscaled_train_X = scaler.fit_transform(X_train)### we fit the normalizer on training data \n### Testing \nscaled_test_X = scaler.transform(X_test)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-23T07:19:58.002624Z","iopub.execute_input":"2021-06-23T07:19:58.003107Z","iopub.status.idle":"2021-06-23T07:19:58.017981Z","shell.execute_reply.started":"2021-06-23T07:19:58.003055Z","shell.execute_reply":"2021-06-23T07:19:58.016767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4- Application of our algorithm \n## 4.1-Decision tree \n### a. Before data Normalization and Standardisation ","metadata":{}},{"cell_type":"markdown","source":"Decision tree is the most powerful and popular algorithm for classification and prediction. It is named decision tree because it builds a structure like the tree. It starts by splitting the total dataset into two subsets based on the most important feature. The most important feature is selected after considering a classification criterion.  \n\nThis process is repeated on each derived subset in a recursive manner called recursive partitioning until all the last nodes are pure means all of its data belongs to a single class. The construction of decision tree classifier does not require any domain knowledge or parameter setting, and therefore is appropriate for exploratory knowledge discovery.  In general decision tree classifier has good accuracy.","metadata":{}},{"cell_type":"code","source":"clf=tree.DecisionTreeClassifier(criterion=\"gini\",random_state = 42) # implementation of the hierarchical/decision trees\nclf.fit(X_train, y_train)\ny_predDT=clf.predict(X_test)\n# The score method returns the accuracy of the model\naccuracy_score(y_test,y_predDT)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T07:19:58.662679Z","iopub.execute_input":"2021-06-23T07:19:58.663149Z","iopub.status.idle":"2021-06-23T07:19:58.705979Z","shell.execute_reply.started":"2021-06-23T07:19:58.663105Z","shell.execute_reply":"2021-06-23T07:19:58.704763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('confusion matrix:')\n# confusion matrix \nprint(confusion_matrix(y_test,y_predDT))","metadata":{"execution":{"iopub.status.busy":"2021-06-23T07:19:58.861575Z","iopub.execute_input":"2021-06-23T07:19:58.86201Z","iopub.status.idle":"2021-06-23T07:19:58.872409Z","shell.execute_reply.started":"2021-06-23T07:19:58.861972Z","shell.execute_reply":"2021-06-23T07:19:58.87142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Classification report:')\nprint(classification_report(y_test, y_predDT))","metadata":{"execution":{"iopub.status.busy":"2021-06-23T07:19:59.04254Z","iopub.execute_input":"2021-06-23T07:19:59.04314Z","iopub.status.idle":"2021-06-23T07:19:59.055583Z","shell.execute_reply.started":"2021-06-23T07:19:59.043103Z","shell.execute_reply":"2021-06-23T07:19:59.054491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### b. After data Normalization","metadata":{}},{"cell_type":"code","source":"clf=tree.DecisionTreeClassifier(criterion=\"gini\",random_state = 42) # implementation of the hierarchical/decision trees\nclf.fit(normalized_train_X, y_train)\ny_predDT1=clf.predict(normalized_test_X)\n# The score method returns the accuracy of the model\naccuracy_score(y_test,y_predDT1)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T07:19:59.728428Z","iopub.execute_input":"2021-06-23T07:19:59.728837Z","iopub.status.idle":"2021-06-23T07:19:59.765103Z","shell.execute_reply.started":"2021-06-23T07:19:59.728804Z","shell.execute_reply":"2021-06-23T07:19:59.764333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### c. After data Standardisation","metadata":{}},{"cell_type":"code","source":"clf=tree.DecisionTreeClassifier(criterion=\"gini\",random_state = 42) # implementation of the hierarchical/decision trees\nclf.fit(scaled_train_X, y_train)\ny_predDT2=clf.predict(scaled_test_X)\n# The score method returns the accuracy of the model\naccuracy_score(y_test,y_predDT2)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T07:20:00.204654Z","iopub.execute_input":"2021-06-23T07:20:00.205236Z","iopub.status.idle":"2021-06-23T07:20:00.241763Z","shell.execute_reply.started":"2021-06-23T07:20:00.205191Z","shell.execute_reply":"2021-06-23T07:20:00.240977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.2-SVM: Support Vector Machine \n### a. Before data Normalization and Standardisation","metadata":{}},{"cell_type":"markdown","source":"It is a supervised machine learning algorithm that can be used for both classification or regression tasks but it is most adopted in classification problems. \n\nThe linear SVM classifier works by drawing a straight line between two classes. It mainly consists of finding the hyperplane equation that distinctly classifies data points or observations.\n\nOur goal is to find a plane that has the maximum margin or maximum distance between the data points of both classes.","metadata":{}},{"cell_type":"code","source":"svclassifier=SVC(kernel='rbf',random_state=30, gamma='auto')\n# train the classifier\nsvclassifier.fit(X_train,y_train)\ny_predSVM=svclassifier.predict(X_test)\nsvclassifier.score(X_test, y_test)\naccuracy_score(y_test, y_predSVM)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T07:20:01.303395Z","iopub.execute_input":"2021-06-23T07:20:01.303944Z","iopub.status.idle":"2021-06-23T07:20:01.398512Z","shell.execute_reply.started":"2021-06-23T07:20:01.303909Z","shell.execute_reply":"2021-06-23T07:20:01.397455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('confusion matrix:')\n# confusion matrix \nprint(confusion_matrix(y_test,y_predSVM))","metadata":{"execution":{"iopub.status.busy":"2021-06-23T07:20:01.462015Z","iopub.execute_input":"2021-06-23T07:20:01.462367Z","iopub.status.idle":"2021-06-23T07:20:01.470109Z","shell.execute_reply.started":"2021-06-23T07:20:01.462337Z","shell.execute_reply":"2021-06-23T07:20:01.469387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Classification report:')\nprint(classification_report(y_test, y_predSVM))","metadata":{"execution":{"iopub.status.busy":"2021-06-23T07:20:01.722804Z","iopub.execute_input":"2021-06-23T07:20:01.723509Z","iopub.status.idle":"2021-06-23T07:20:01.733817Z","shell.execute_reply.started":"2021-06-23T07:20:01.723473Z","shell.execute_reply":"2021-06-23T07:20:01.732839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### b. After data Normalization","metadata":{}},{"cell_type":"code","source":"svclassifier=SVC(kernel='rbf',random_state=30, gamma='auto').fit(normalized_train_X,y_train)\ny_predSVM1=svclassifier.predict(normalized_test_X)\naccuracy_score(y_test, y_predSVM1)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T07:20:02.622834Z","iopub.execute_input":"2021-06-23T07:20:02.623251Z","iopub.status.idle":"2021-06-23T07:20:02.688517Z","shell.execute_reply.started":"2021-06-23T07:20:02.623216Z","shell.execute_reply":"2021-06-23T07:20:02.687451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### c. After data Standardisation ","metadata":{}},{"cell_type":"code","source":"svclassifier=SVC(kernel='rbf',random_state=30, gamma='auto').fit(scaled_train_X,y_train)\ny_predSVM2=svclassifier.predict(scaled_test_X)\naccuracy_score(y_test, y_predSVM2)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T07:20:04.342576Z","iopub.execute_input":"2021-06-23T07:20:04.34301Z","iopub.status.idle":"2021-06-23T07:20:04.403221Z","shell.execute_reply.started":"2021-06-23T07:20:04.342972Z","shell.execute_reply":"2021-06-23T07:20:04.402149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.3- Logistic regression\n### a. Before data Normalization and Standardisation","metadata":{}},{"cell_type":"markdown","source":"Logistic regression is a widely used method in machine learning to solve binary classification problems where two classes are predicted. It is called Logistic Regression because it is based on the logistic function or sigmoid function. The only difference from linear regression is that here our output is 0 or 1 while in linear regression it is a continuous value","metadata":{}},{"cell_type":"code","source":"LR = LogisticRegression(solver='sag').fit(X_train, y_train)\ny_predLR=LR.predict(X_test)\naccuracy_score(y_test, y_predLR)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T07:20:06.202326Z","iopub.execute_input":"2021-06-23T07:20:06.202861Z","iopub.status.idle":"2021-06-23T07:20:06.229692Z","shell.execute_reply.started":"2021-06-23T07:20:06.202827Z","shell.execute_reply":"2021-06-23T07:20:06.228896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('confusion matrix:')\n# confusion matrix \nprint(confusion_matrix(y_test,y_predLR))\n\nprint('Classification report:')\nprint(classification_report(y_test, y_predLR))","metadata":{"execution":{"iopub.status.busy":"2021-06-23T07:20:06.784194Z","iopub.execute_input":"2021-06-23T07:20:06.784759Z","iopub.status.idle":"2021-06-23T07:20:06.797603Z","shell.execute_reply.started":"2021-06-23T07:20:06.78472Z","shell.execute_reply":"2021-06-23T07:20:06.796636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### b. After data Normalization","metadata":{}},{"cell_type":"code","source":"LR1 = LogisticRegression(solver='sag').fit(normalized_train_X, y_train)\ny_predLR1=LR1.predict(normalized_test_X)\naccuracy_score(y_test, y_predLR1)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T07:20:08.303123Z","iopub.execute_input":"2021-06-23T07:20:08.30376Z","iopub.status.idle":"2021-06-23T07:20:08.340245Z","shell.execute_reply.started":"2021-06-23T07:20:08.303701Z","shell.execute_reply":"2021-06-23T07:20:08.339328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### c. After data Standardisation","metadata":{}},{"cell_type":"code","source":"LR2 = LogisticRegression(solver='sag').fit(scaled_train_X, y_train)\ny_predLR2=LR2.predict(scaled_test_X)\naccuracy_score(y_test, y_predLR2)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T07:20:09.762422Z","iopub.execute_input":"2021-06-23T07:20:09.762819Z","iopub.status.idle":"2021-06-23T07:20:09.78792Z","shell.execute_reply.started":"2021-06-23T07:20:09.762786Z","shell.execute_reply":"2021-06-23T07:20:09.786943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.4- K Nearest Neighbors\n### a. Before data Normalization and Standardisation","metadata":{}},{"cell_type":"markdown","source":"It is a supervised machine learning algorithm that can used to solve only classification tasks. This algorithm stores all available cases and classifies new cases based on a similarity measure (e.g., distance functions). The distance could be Euclidean, Manhattan or MinKowski. It is “non-parametric” (only k needs to be set) and is based only on training data.\n\n","metadata":{}},{"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=7).fit(X_train, y_train.values.ravel())\npred_KNN = knn.predict(X_test)\nprint('KNN',accuracy_score(y_test, pred_KNN))","metadata":{"execution":{"iopub.status.busy":"2021-06-23T07:20:12.642776Z","iopub.execute_input":"2021-06-23T07:20:12.643228Z","iopub.status.idle":"2021-06-23T07:20:12.687059Z","shell.execute_reply.started":"2021-06-23T07:20:12.643184Z","shell.execute_reply":"2021-06-23T07:20:12.685839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('confusion matrix:')\n# confusion matrix \nprint(confusion_matrix(y_test,pred_KNN))\n\nprint('Classification report:')\nprint(classification_report(y_test, pred_KNN))","metadata":{"execution":{"iopub.status.busy":"2021-06-23T07:20:13.222334Z","iopub.execute_input":"2021-06-23T07:20:13.222922Z","iopub.status.idle":"2021-06-23T07:20:13.236776Z","shell.execute_reply.started":"2021-06-23T07:20:13.222869Z","shell.execute_reply":"2021-06-23T07:20:13.235763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### b. After data Normalization","metadata":{}},{"cell_type":"code","source":"knn1 = KNeighborsClassifier(n_neighbors=7).fit(normalized_train_X, y_train.values.ravel())\npred_KNN1 = knn.predict(normalized_test_X)\nprint('KNN',accuracy_score(y_test, pred_KNN1))","metadata":{"execution":{"iopub.status.busy":"2021-06-23T07:20:14.622253Z","iopub.execute_input":"2021-06-23T07:20:14.622705Z","iopub.status.idle":"2021-06-23T07:20:14.657072Z","shell.execute_reply.started":"2021-06-23T07:20:14.622664Z","shell.execute_reply":"2021-06-23T07:20:14.655744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### c. After data Standardisation","metadata":{}},{"cell_type":"code","source":"knn2 = KNeighborsClassifier(n_neighbors=7).fit(scaled_train_X, y_train.values.ravel())\npred_KNN2 = knn.predict(scaled_test_X)\nprint('KNN',accuracy_score(y_test, pred_KNN2))","metadata":{"execution":{"iopub.status.busy":"2021-06-23T07:20:16.062325Z","iopub.execute_input":"2021-06-23T07:20:16.062692Z","iopub.status.idle":"2021-06-23T07:20:16.094627Z","shell.execute_reply.started":"2021-06-23T07:20:16.062661Z","shell.execute_reply":"2021-06-23T07:20:16.091873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.5- Random Forest \n### a. Before data Normalization and Standardisation","metadata":{}},{"cell_type":"markdown","source":"Random Forest is a supervised learning algorithm. It is one of the most adopted algorithms that could be used to solve regression or classification tasks. It builds a 'forest' which is an ensemble of decision trees usually trained with the 'bagging' method. \n\nRandom Forest builds multiple decision trees and merges them together to get a more accurate and stable prediction. In addition, instead of searching for the most important feature while splitting a node, it searches for the best feature among a random subset of features. ","metadata":{}},{"cell_type":"code","source":"RF = RandomForestClassifier(criterion='gini',n_estimators=75,random_state =42)\nRF.fit(X_train, y_train.values.ravel())\npred_RF = RF.predict(X_test)\nprint('RF',accuracy_score(pred_RF,y_test))","metadata":{"execution":{"iopub.status.busy":"2021-06-23T07:20:18.542637Z","iopub.execute_input":"2021-06-23T07:20:18.543657Z","iopub.status.idle":"2021-06-23T07:20:18.88056Z","shell.execute_reply.started":"2021-06-23T07:20:18.543615Z","shell.execute_reply":"2021-06-23T07:20:18.879585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### b. After data Normalization","metadata":{}},{"cell_type":"code","source":"RF1 = RandomForestClassifier(criterion='gini',n_estimators=75,random_state =42).fit(normalized_train_X, y_train.values.ravel())\npred_RF1 = RF1.predict(normalized_test_X)\nprint('RF',accuracy_score(pred_RF1,y_test))","metadata":{"execution":{"iopub.status.busy":"2021-06-23T07:20:19.820916Z","iopub.execute_input":"2021-06-23T07:20:19.821262Z","iopub.status.idle":"2021-06-23T07:20:20.157869Z","shell.execute_reply.started":"2021-06-23T07:20:19.821232Z","shell.execute_reply":"2021-06-23T07:20:20.1569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### c. After data Standardisation","metadata":{}},{"cell_type":"code","source":"RF2 = RandomForestClassifier(criterion='gini',n_estimators=75,random_state =42).fit(scaled_train_X, y_train.values.ravel())\npred_RF2 = RF2.predict(scaled_test_X)\nprint('RF',accuracy_score(pred_RF2,y_test))","metadata":{"execution":{"iopub.status.busy":"2021-06-23T07:20:20.962511Z","iopub.execute_input":"2021-06-23T07:20:20.963063Z","iopub.status.idle":"2021-06-23T07:20:21.302843Z","shell.execute_reply.started":"2021-06-23T07:20:20.963028Z","shell.execute_reply":"2021-06-23T07:20:21.302151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We had noticed that data normalization or standardization does not improve the result so we can avoid it. ","metadata":{}},{"cell_type":"markdown","source":"It is clear that the best applied algorithm were KNN and SVM with accuracy equal to 91% and 90.5%, respectively. ","metadata":{}},{"cell_type":"code","source":"prediction=knn.predict(test)\nmysubmission = pd.DataFrame(prediction)\n\nmysubmission.index += 1\n\nmysubmission.columns = ['Solution']\nmysubmission['Id'] = np.arange(1,mysubmission.shape[0]+1)\nmysubmission = mysubmission[['Id', 'Solution']]\n\nmysubmission.to_csv('Submission.csv',index=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-23T07:21:16.462957Z","iopub.execute_input":"2021-06-23T07:21:16.463396Z","iopub.status.idle":"2021-06-23T07:21:17.053188Z","shell.execute_reply.started":"2021-06-23T07:21:16.463357Z","shell.execute_reply":"2021-06-23T07:21:17.052082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}