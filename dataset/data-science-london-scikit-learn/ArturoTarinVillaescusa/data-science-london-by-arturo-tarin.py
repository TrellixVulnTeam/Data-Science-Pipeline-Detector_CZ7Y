# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory

import os
print(os.listdir("../input"))

# Any results you write to the current directory are saved as output.

#####################################################################################
#
# 2. Get the data
#

test_df = pd.read_csv('../input/test.csv', header=None)
train_df = pd.read_csv('../input/train.csv', header=None)
train_labels = pd.read_csv('../input/trainLabels.csv', header=None)

# Take a look at the data. Don't look at the test set!!!!
train_df.head()
"""
         0         1         2     ...           37        38        39
0  0.299403 -1.226624  1.498425    ...    -2.715559 -2.682409  0.101050
1 -1.174176  0.332157  0.949919    ...     1.213219  1.382932 -1.817761
2  1.192222 -0.414371  0.067054    ...     0.656438 -0.932473  2.987436
3  1.573270 -0.580318 -0.866332    ...     0.215748  0.619645  1.883397
4 -0.613071 -0.644204  1.112558    ...     0.870772 -1.894609  0.408332
"""

train_labels.head()
"""
   0
0  1
1  0
2  0
3  1
4  0
"""

# There are 40 attributes
train_df.info()
"""
RangeIndex: 1000 entries, 0 to 999
Data columns (total 40 columns):
0     1000 non-null float64
1     1000 non-null float64
2     1000 non-null float64
3     1000 non-null float64
4     1000 non-null float64
5     1000 non-null float64
6     1000 non-null float64
7     1000 non-null float64
8     1000 non-null float64
9     1000 non-null float64
10    1000 non-null float64
11    1000 non-null float64
12    1000 non-null float64
13    1000 non-null float64
14    1000 non-null float64
15    1000 non-null float64
16    1000 non-null float64
17    1000 non-null float64
18    1000 non-null float64
19    1000 non-null float64
20    1000 non-null float64
21    1000 non-null float64
22    1000 non-null float64
23    1000 non-null float64
24    1000 non-null float64
25    1000 non-null float64
26    1000 non-null float64
27    1000 non-null float64
28    1000 non-null float64
29    1000 non-null float64
30    1000 non-null float64
31    1000 non-null float64
32    1000 non-null float64
33    1000 non-null float64
34    1000 non-null float64
35    1000 non-null float64
36    1000 non-null float64
37    1000 non-null float64
38    1000 non-null float64
39    1000 non-null float64
dtypes: float64(40)
memory usage: 312.6 KB
"""

# All attributes are numerical

# There are 1000 instances in the training dataset, which means that it is fairly small 
# by Machine Learning standards. 
train_df.shape
# (1000, 40)

# The describe() method shows a summary of the 
# numerical attributes
train_df.describe()

"""
                0            1      ...                38           39
count  1000.000000  1000.000000     ...       1000.000000  1000.000000
mean      0.025596    -0.024526     ...         -0.892659     0.609451
std       1.008282     1.016298     ...          2.022022     2.045439
min      -3.365711    -3.492086     ...         -8.051722    -7.799086
25%      -0.669010    -0.693937     ...         -2.220126    -0.565041
50%       0.027895    -0.033194     ...         -0.855470     0.779944
75%       0.762520     0.682753     ...          0.388698     1.992193
max       3.326246     3.583870     ...          5.774120     6.803984

The count , mean , min , and max rows are self-explanatory. 

The std row shows the standard deviation (which measures how dispersed the values are). 

The 25%, 50%, and 75% rows show the corresponding percentiles: a percentile 
indicates the value below which a given percentage of observations in a group 
of observations falls. 

For example, 25% of the rows have the first attribute lower than -0.669010, 
50% are lower than 0.027895 and 75% are lower than 0.762520. These are often
called the 25 th percentile (or 1 st quartile), the median, and the 75 th
percentile (or 3 rd quartile).
"""

# Another quick way to get a feel of the type of data we are dealing with is 
# to plot a histogram for each numerical attribute.

# A histogram shows the number of instances (on the vertical axis) that have a 
# given value range (on the horizontal axis). 

# We can either plot this one attribute at a time, or you can call the hist() 
# method on the whole dataset, and it will plot a histogram for each numerical 
# attribute 

import matplotlib.pyplot as plt
train_df.hist(bins=50, figsize=(20,15))
plt.show()

"""
Notice a few things in these histograms:

1. These attributes have very simmilar scales.
4. Finally, many histograms are centered

Now we have a better understanding of the kind of data we are dealing with.
"""

#####################################################################################
#
# 3. Discover and visualize the data to gain insights.
#
"""
So far we have only taken a quick glance at the data to get a general understanding
of the kind of data we are manipulating.

Now the goal is to go a little bit more in depth.

First, we make sure we have put the test set aside and we are only exploring
the training set.

Also, if the training set is very large, we may want to sample an exploration
set, to make manipulations easy and fast.

In our case, the set is quite small so we can just work directly on the full
set.

"""


# Looking for Correlations

# Since the dataset is not too large, you can easily compute the standard 
# correlation coefficient (also called Pearson’s r) between every pair of 
# attributes using the corr() method:
corr_matrix = train_df.corr()

# Now let’s look at how much each attribute correlates with the median house value:
corr_matrix

"""
          0         1         2     ...           37        38        39
1.0	-0.02872181794159793	0.033054945504337074	0.012392145503154917	-0.022666085203547216	0.0371521899655849	0.018897792917094668	0.015082480344468937	-0.04554886301932045	-0.023961743989544676	-0.037093413314733724	0.012167803577784162	-0.004585341988206861	0.044507756997266654	0.002444152041238683	0.00685579495743153	0.0231041282938347	0.039383455317389966	-0.051758430836456634	0.0036997789788228556	-0.0010249619613190426	0.01644266766081616	0.02109219339610879	-0.02089513067076667	-0.028142220624942503	-0.014641334887758498	0.04757839007310761	-0.06059697594887678	0.026287880416398533	0.031507371208793024	0.012687968127695999	0.02321389164921309
-0.02872181794159793	1.0	0.016075716130688148	0.008241785109396632	0.04590566206393306	-0.03394002713947919	-0.007415695087687753	0.007441772114959892	0.03983108687257769	0.009156436810076041	0.020530225366439527	-0.033614141013052413	-0.06466427753375907	-0.03718651655192131	-0.06740885457442604	-0.0010770796641010887	-0.03347062059217969	0.045274397045504663	0.0194708366600792	0.012494297310779938	0.019869409556435524	0.020463919406684117	0.04458375933281018	0.0520753477981668	0.020366271366322138	0.019152997603747012	0.011046381657566077	-0.001424817316970146	-0.0683826376213547	-0.012136528871980544	-0.001816284778293775	-0.05367734590657238
0.033054945504337074	0.016075716130688148	1.0	0.009481746269456998	0.018246778939113983	-0.013887371291315967	-0.055840654024173486	-0.009979410756900344	0.019588534779745953	-0.01696913651205847	0.014137455423864402	0.021546972989035833	0.026004886837501715	-0.006822039009979962	-0.02768332885467037	0.014775362611895649	0.03764975197726092	0.0541508499337256	0.0011024897020565445	-0.009936599853786372	0.05537347074983476	0.0037961364213867257	-0.010610667024132114	0.050348138950286656	0.0025099588081986413	-0.006311101526217715	-0.018633103104224277	-0.010941915235073423	0.005792658499884998	-0.0036407919650715665	0.03594086449019695	-0.0011735048367429782
0.012392145503154917	0.008241785109396632	0.009481746269456998	1.0	0.024976435589430048	0.021583800435922097	-0.027843419156782998	-0.04356454667865674	0.001158984123161788	-0.049141115859989704	-0.03311531160396746	0.05362845798458287	-0.04820276835677067	-0.028692417220927045	-0.04179830208402601	-0.04627365053068692	0.03901105047669133	0.016228790216592216	-0.016686810017070466	0.02900094383442912	-0.0035994451323580817	-0.008160548366717294	-0.01265002737023151	0.014379162540914956	-0.03526518965796783	-0.017418328579459	0.041353016134738754	0.02279583495434191	-0.03443709204484698	0.03107718716495055	0.0246495428085172	-0.0076672534143655
-0.022666085203547216	0.04590566206393306	0.018246778939113983	0.024976435589430048	1.0	0.02751608454441107	-0.3400009366833171	0.20643022342567968	0.06373768849302645	0.0791646449833138	-0.05031977890633187	-0.020110934542107196	-0.18535000841711377	0.04827227989499304	0.20776579240395712	0.011169610233735071	-0.010587166776889976	-0.00213160002538553	0.07973183543391775	0.022882770695781846	0.005869474913880649	0.009284884128039246	0.4503252918787259	0.568634762143107	-0.05914696663567469	0.024880709888013906	-0.016568038272403197	0.049097823636387254	-0.5222133146372644	-0.464573435948515	-0.03842858074973048	0.008584517919806698
0.0371521899655849	-0.03394002713947919	-0.013887371291315967	0.021583800435922097	0.02751608454441107	1.0	-0.06859211497113289	0.03589278071015155	0.03152740153271976	-0.024041804053344593	-0.004303662382407325	-0.058345664507490395	0.04318874459389796	-0.004467611104270052	0.016589487691868486	0.002010201277986693	-0.004352669397330986	-0.0006861865113342243	0.03932412039880954	-0.043424536482114626	0.04464741649262142	-0.041336863951486516	-0.024619606603582176	0.0034858869079142326	-0.0036545520312501766	-0.022365584907372095	0.029459589998093928	-0.006561331315913037	-0.021470278060848575	0.011030232098589231	0.011602350264691258	-0.04990374894482541
0.018897792917094668	-0.007415695087687753	-0.055840654024173486	-0.027843419156782998	-0.3400009366833171	-0.06859211497113289	1.0	-0.0015757630476234278	-0.031095708659850792	0.06493957747535192	-0.005177591399420358	0.03550030300226415	-0.42047610851012834	-0.0005455305790310598	-0.186587500796059	-0.004623097188686781	0.05957848737792851	-0.043919613418993364	-0.1076826219818336	-0.006538061208718019	0.02607051540766328	-0.031041389974875582	0.1168409099352205	-0.21864209287733213	0.0709361668450396	0.0550302093353773	-0.013885023226287553	-0.015000138394740166	0.048561351687301114	0.07060017410348343	-0.049269303730914225	-0.019649638154812226
0.015082480344468937	0.007441772114959892	-0.009979410756900344	-0.04356454667865674	0.20643022342567968	0.03589278071015155	-0.0015757630476234278	1.0	0.026586401063527824	0.03959775854275112	-0.020982056261015226	-0.004838595705649884	0.055691590624650775	0.0109708178749958	-0.022614299320381885	-0.008393744858415167	-0.011577235614336526	-0.05643018289529148	0.12970860131136677	-0.04616937407373797	0.0572036281616111	-0.002460262131134084	0.07004144732217271	-0.1145688248672532	-0.0581889768406772	0.007113252630031222	0.013424572806147039	-0.03706682739968139	0.0455017212371798	-0.024484442492593655	-0.037181293314343826	0.0012348029513953166
-0.04554886301932045	0.03983108687257769	0.019588534779745953	0.001158984123161788	0.06373768849302645	0.03152740153271976	-0.031095708659850792	0.026586401063527824	1.0	0.01681296552909727	-0.02246396375316077	-0.033220229885066205	0.05163981045211928	0.028365722057508206	0.01725136634560188	0.016591725133903813	0.03605330410192667	-0.004551197415147039	0.008129233608046512	0.0004150326819101847	0.059895389447738465	-0.06278117807866225	0.027914664273241987	0.09024580389737168	0.0013413280884224529	0.004724639187208393	-0.020279004615091287	-0.013451885172216422	0.032806712852962154	-0.03158480734636374	0.016398531277748027	0.011912666337204615
-0.023961743989544676	0.009156436810076041	-0.01696913651205847	-0.049141115859989704	0.0791646449833138	-0.024041804053344593	0.06493957747535192	0.03959775854275112	0.01681296552909727	1.0	0.04788341695031061	0.018688958847100912	-0.07711406818574819	0.012606078879369301	0.0026941115944580326	0.020863197626993013	0.010687214610268515	-0.08815502681122754	-0.011149499217126308	0.009226233581362188	0.031205729659858676	-0.00015180003643886438	0.05957817897016657	0.01517651012858931	-0.013476301281913985	0.02054492887012326	0.05723910965265899	0.002385346212583283	-0.04559546869277992	-0.03388590321617861	-0.038944976905917784	0.05594743180851326
-0.037093413314733724	0.020530225366439527	0.014137455423864402	-0.03311531160396746	-0.05031977890633187	-0.004303662382407325	-0.005177591399420358	-0.020982056261015226	-0.02246396375316077	0.04788341695031061	1.0	-0.012949023040801266	0.030991600982625322	0.014885925478128312	-0.027990180480407686	0.05393551797983293	-0.021133105475974023	0.02791451755180672	0.04022442722942176	0.006596655759071997	-0.023513614389171808	0.03852594861146828	-0.00897992132237035	-0.009760298700790124	0.027802635565223412	-0.02136357251508456	0.00956488847098621	0.011586363193742689	0.022628264906852744	0.00655325143841938	-0.006264869307598833	0.017566602298571803
0.012167803577784162	-0.033614141013052413	0.021546972989035833	0.05362845798458287	-0.020110934542107196	-0.058345664507490395	0.03550030300226415	-0.004838595705649884	-0.033220229885066205	0.018688958847100912	-0.012949023040801266	1.0	-0.056273326717666314	-0.012402548583976264	-0.12043770886377163	-0.015688245656078532	-0.03457106627739806	-0.03764846776860653	-0.002568269932486672	0.029586570984326925	0.06504669201262642	-0.018234025193104988	-0.04576399933420848	-0.020800214227474592	0.031074449045597686	-0.013071199905000748	0.026665587274514697	-0.002109943921949306	-0.03559949161626247	-0.013874567808642081	-0.0176765152806359	0.0007224568022093152
-0.004585341988206861	-0.06466427753375907	0.026004886837501715	-0.04820276835677067	-0.18535000841711377	0.04318874459389796	-0.42047610851012834	0.055691590624650775	0.05163981045211928	-0.07711406818574819	0.030991600982625322	-0.056273326717666314	1.0	-0.01604265209969581	0.3617998924655097	-0.017310995065988845	-0.024775560541440273	0.018676036920466384	0.41214469143122057	0.010943766776393355	-0.041002881880195664	0.02885860839416631	-0.3848679542445067	0.23206489029508454	-0.03618538858690884	-0.004929192808667849	0.02588535997122857	-0.008170439538327792	0.6172126021004793	0.05157293580708636	0.02572836204831177	0.03506869486291684
0.044507756997266654	-0.03718651655192131	-0.006822039009979962	-0.028692417220927045	0.04827227989499304	-0.004467611104270052	-0.0005455305790310598	0.0109708178749958	0.028365722057508206	0.012606078879369301	0.014885925478128312	-0.012402548583976264	-0.01604265209969581	1.0	-0.00904687118645191	-0.012085489641863803	0.004259482541836003	0.01751412014980779	-0.02179178267042453	0.004265643018409276	0.056560020321234014	-0.012932628380922429	0.021266449711225172	0.03678438065689146	0.03625463090270973	-0.00828862497702991	-0.013673911004342768	0.0013593097189974984	-0.021725870078058408	0.005223617980622146	-0.004917213309769997	0.00539780481134203
0.002444152041238683	-0.06740885457442604	-0.02768332885467037	-0.04179830208402601	0.20776579240395712	0.016589487691868486	-0.186587500796059	-0.022614299320381885	0.01725136634560188	0.0026941115944580326	-0.027990180480407686	-0.12043770886377163	0.3617998924655097	-0.00904687118645191	1.0	-0.013227233070166463	-0.027554591340520913	-0.012937502771856211	0.11089835959826735	0.055108344574691465	-0.05578162559622683	0.052368953014415257	0.16718200279403472	0.054091845530803234	-0.029525751119142445	-0.011434451910059979	0.00969871685086178	0.03795160434024175	0.16994042672398801	0.05804406920477856	-0.022093456430124583	0.008780472581554686
0.00685579495743153	-0.0010770796641010887	0.014775362611895649	-0.04627365053068692	0.011169610233735071	0.002010201277986693	-0.004623097188686781	-0.008393744858415167	0.016591725133903813	0.020863197626993013	0.05393551797983293	-0.015688245656078532	-0.017310995065988845	-0.012085489641863803	-0.013227233070166463	1.0	-0.01037010967271147	0.01062511469683328	-0.015626431411468862	0.019312233993109572	0.06849495868179326	-0.0010021371619702822	0.04053583519493408	0.018141082669644766	-0.012503366344953503	0.0031089888224132057	0.04311268268132242	0.018661936234928737	-0.005968259605070836	-0.02575625438759713	-0.04357850861761144	0.012673269857805565
0.0231041282938347	-0.03347062059217969	0.03764975197726092	0.03901105047669133	-0.010587166776889976	-0.004352669397330986	0.05957848737792851	-0.011577235614336526	0.03605330410192667	0.010687214610268515	-0.021133105475974023	-0.03457106627739806	-0.024775560541440273	0.004259482541836003	-0.027554591340520913	-0.01037010967271147	1.0	0.012800861739188192	-0.019523985918376854	-0.0019217455054895125	0.021823564667365687	-0.02294430570760741	-0.001592863598235753	-0.008672691294745344	0.06705978734167883	-0.029013302251523133	-0.004925346708735023	0.02644081483203389	0.003826114883716529	0.01649797220453958	-0.01929183219112051	-0.01133817663050064
0.039383455317389966	0.045274397045504663	0.0541508499337256	0.016228790216592216	-0.00213160002538553	-0.0006861865113342243	-0.043919613418993364	-0.05643018289529148	-0.004551197415147039	-0.08815502681122754	0.02791451755180672	-0.03764846776860653	0.018676036920466384	0.01751412014980779	-0.012937502771856211	0.01062511469683328	0.012800861739188192	1.0	-0.02812694676681654	0.03496876622562991	-0.056842096655889254	-0.004109456675740061	-0.04998173913866476	0.01096341443855977	-0.050381462552779505	-0.011229708891508013	-0.0021066619586324617	-0.03592896608882129	-0.018217269839567986	-0.008308603204153308	0.03755387236812646	-0.01645750568736624
-0.051758430836456634	0.0194708366600792	0.0011024897020565445	-0.016686810017070466	0.07973183543391775	0.03932412039880954	-0.1076826219818336	0.12970860131136677	0.008129233608046512	-0.011149499217126308	0.04022442722942176	-0.002568269932486672	0.41214469143122057	-0.02179178267042453	0.11089835959826735	-0.015626431411468862	-0.019523985918376854	-0.02812694676681654	1.0	-0.02011876005514539	0.00918762057527672	-0.013625994308902673	0.009758904280909338	0.32103526955378103	-0.029539482891043876	0.0045345547397325206	0.04510503384114704	-0.018831438782817417	0.2332918494879824	0.16768235514123422	-0.029216693190828553	0.016949910386554704
0.0036997789788228556	0.012494297310779938	-0.009936599853786372	0.02900094383442912	0.022882770695781846	-0.043424536482114626	-0.006538061208718019	-0.04616937407373797	0.0004150326819101847	0.009226233581362188	0.006596655759071997	0.029586570984326925	0.010943766776393355	0.004265643018409276	0.055108344574691465	0.019312233993109572	-0.0019217455054895125	0.03496876622562991	-0.02011876005514539	1.0	0.014404090102660399	-0.032282267400553885	0.012327683309601038	0.009249550452178807	0.00613984802192646	-0.021210211012088788	-0.007961194769880687	-0.020417651693399378	0.021634253783558755	-0.029451804905802043	0.04545605742175369	-0.06572043516353127
-0.0010249619613190426	0.019869409556435524	0.05537347074983476	-0.0035994451323580817	0.005869474913880649	0.04464741649262142	0.02607051540766328	0.0572036281616111	0.059895389447738465	0.031205729659858676	-0.023513614389171808	0.06504669201262642	-0.041002881880195664	0.056560020321234014	-0.05578162559622683	0.06849495868179326	0.021823564667365687	-0.056842096655889254	0.00918762057527672	0.014404090102660399	1.0	-0.05654255929295351	0.006851574593988099	0.01457597864722828	0.004897361423240856	-0.034407077416182374	-0.013743062525263599	0.02554925589365757	-0.002615964333536537	0.010689845354148282	-0.03193642882835132	-0.05720692022351612
0.01644266766081616	0.020463919406684117	0.0037961364213867257	-0.008160548366717294	0.009284884128039246	-0.041336863951486516	-0.031041389974875582	-0.002460262131134084	-0.06278117807866225	-0.00015180003643886438	0.03852594861146828	-0.018234025193104988	0.02885860839416631	-0.012932628380922429	0.052368953014415257	-0.0010021371619702822	-0.02294430570760741	-0.004109456675740061	-0.013625994308902673	-0.032282267400553885	-0.05654255929295351	1.0	0.015158006415158675	-0.001179979108524694	-0.042421249196654624	0.029838577640370848	-0.024236726464405816	-0.01780803985880261	0.03491961150698561	-0.00240148112613599	-0.006600581216601322	-0.027403328202899657
0.02109219339610879	0.04458375933281018	-0.010610667024132114	-0.01265002737023151	0.4503252918787259	-0.024619606603582176	0.1168409099352205	0.07004144732217271	0.027914664273241987	0.05957817897016657	-0.00897992132237035	-0.04576399933420848	-0.3848679542445067	0.021266449711225172	0.16718200279403472	0.04053583519493408	-0.001592863598235753	-0.04998173913866476	0.009758904280909338	0.012327683309601038	0.006851574593988099	0.015158006415158675	1.0	0.3592200445466627	-0.059319131002664255	-0.009653365028798572	0.007620758206939771	0.02482519708805464	-0.09814109949274506	-0.06565215199133224	-0.0026743692357239844	-0.008124667042084682
-0.02089513067076667	0.0520753477981668	0.050348138950286656	0.014379162540914956	0.568634762143107	0.0034858869079142326	-0.21864209287733213	-0.1145688248672532	0.09024580389737168	0.01517651012858931	-0.009760298700790124	-0.020800214227474592	0.23206489029508454	0.03678438065689146	0.054091845530803234	0.018141082669644766	-0.008672691294745344	0.01096341443855977	0.32103526955378103	0.009249550452178807	0.01457597864722828	-0.001179979108524694	0.3592200445466627	1.0	-0.05500604784827315	0.025752182083250082	-0.01563364702147643	0.01376792402726502	-0.055282765921092984	-0.15939062784412003	-0.021566153796480365	0.03297616112587377
-0.028142220624942503	0.020366271366322138	0.0025099588081986413	-0.03526518965796783	-0.05914696663567469	-0.0036545520312501766	0.0709361668450396	-0.0581889768406772	0.0013413280884224529	-0.013476301281913985	0.027802635565223412	0.031074449045597686	-0.03618538858690884	0.03625463090270973	-0.029525751119142445	-0.012503366344953503	0.06705978734167883	-0.050381462552779505	-0.029539482891043876	0.00613984802192646	0.004897361423240856	-0.042421249196654624	-0.059319131002664255	-0.05500604784827315	1.0	0.018182426895127606	-0.049054967424970744	0.021448544512287913	-0.030237874128577	0.02105485024284597	-0.025009204744630496	0.011388768371649288
-0.014641334887758498	0.019152997603747012	-0.006311101526217715	-0.017418328579459	0.024880709888013906	-0.022365584907372095	0.0550302093353773	0.007113252630031222	0.004724639187208393	0.02054492887012326	-0.02136357251508456	-0.013071199905000748	-0.004929192808667849	-0.00828862497702991	-0.011434451910059979	0.0031089888224132057	-0.029013302251523133	-0.011229708891508013	0.0045345547397325206	-0.021210211012088788	-0.034407077416182374	0.029838577640370848	-0.009653365028798572	0.025752182083250082	0.018182426895127606	1.0	0.02027088247384852	-0.004024781227716225	0.0038710888349462068	-0.039761499428617494	-0.01155597488800321	-0.027183855747916055
0.04757839007310761	0.011046381657566077	-0.018633103104224277	0.041353016134738754	-0.016568038272403197	0.029459589998093928	-0.013885023226287553	0.013424572806147039	-0.020279004615091287	0.05723910965265899	0.00956488847098621	0.026665587274514697	0.02588535997122857	-0.013673911004342768	0.00969871685086178	0.04311268268132242	-0.004925346708735023	-0.0021066619586324617	0.04510503384114704	-0.007961194769880687	-0.013743062525263599	-0.024236726464405816	0.007620758206939771	-0.01563364702147643	-0.049054967424970744	0.02027088247384852	1.0	0.016645329783545885	0.04642898221681092	0.019456061780296997	-0.04876993461057649	-0.0004057360613672874
-0.06059697594887678	-0.001424817316970146	-0.010941915235073423	0.02279583495434191	0.049097823636387254	-0.006561331315913037	-0.015000138394740166	-0.03706682739968139	-0.013451885172216422	0.002385346212583283	0.011586363193742689	-0.002109943921949306	-0.008170439538327792	0.0013593097189974984	0.03795160434024175	0.018661936234928737	0.02644081483203389	-0.03592896608882129	-0.018831438782817417	-0.020417651693399378	0.02554925589365757	-0.01780803985880261	0.02482519708805464	0.01376792402726502	0.021448544512287913	-0.004024781227716225	0.016645329783545885	1.0	-0.047500864310561046	-0.0491619794624762	-0.031257000265321626	-0.03082816474189297
0.026287880416398533	-0.0683826376213547	0.005792658499884998	-0.03443709204484698	-0.5222133146372644	-0.021470278060848575	0.048561351687301114	0.0455017212371798	0.032806712852962154	-0.04559546869277992	0.022628264906852744	-0.03559949161626247	0.6172126021004793	-0.021725870078058408	0.16994042672398801	-0.005968259605070836	0.003826114883716529	-0.018217269839567986	0.2332918494879824	0.021634253783558755	-0.002615964333536537	0.03491961150698561	-0.09814109949274506	-0.055282765921092984	-0.030237874128577	0.0038710888349462068	0.04642898221681092	-0.047500864310561046	1.0	0.32213778481513783	0.02965111161308257	0.018042308226153343
0.031507371208793024	-0.012136528871980544	-0.0036407919650715665	0.03107718716495055	-0.464573435948515	0.011030232098589231	0.07060017410348343	-0.024484442492593655	-0.03158480734636374	-0.03388590321617861	0.00655325143841938	-0.013874567808642081	0.05157293580708636	0.005223617980622146	0.05804406920477856	-0.02575625438759713	0.01649797220453958	-0.008308603204153308	0.16768235514123422	-0.029451804905802043	0.010689845354148282	-0.00240148112613599	-0.06565215199133224	-0.15939062784412003	0.02105485024284597	-0.039761499428617494	0.019456061780296997	-0.0491619794624762	0.32213778481513783	1.0	0.026937916623784395	0.018868567146846378
0.012687968127695999	-0.001816284778293775	0.03594086449019695	0.0246495428085172	-0.03842858074973048	0.011602350264691258	-0.049269303730914225	-0.037181293314343826	0.016398531277748027	-0.038944976905917784	-0.006264869307598833	-0.0176765152806359	0.02572836204831177	-0.004917213309769997	-0.022093456430124583	-0.04357850861761144	-0.01929183219112051	0.03755387236812646	-0.029216693190828553	0.04545605742175369	-0.03193642882835132	-0.006600581216601322	-0.0026743692357239844	-0.021566153796480365	-0.025009204744630496	-0.01155597488800321	-0.04876993461057649	-0.031257000265321626	0.02965111161308257	0.026937916623784395	1.0	-0.025364291341458806
0.02321389164921309	-0.05367734590657238	-0.0011735048367429782	-0.0076672534143655	0.008584517919806698	-0.04990374894482541	-0.019649638154812226	0.0012348029513953166	0.011912666337204615	0.05594743180851326	0.017566602298571803	0.0007224568022093152	0.03506869486291684	0.00539780481134203	0.008780472581554686	0.012673269857805565	-0.01133817663050064	-0.01645750568736624	0.016949910386554704	-0.06572043516353127	-0.05720692022351612	-0.027403328202899657	-0.008124667042084682	0.03297616112587377	0.011388768371649288	-0.027183855747916055	-0.0004057360613672874	-0.03082816474189297	0.018042308226153343	0.018868567146846378	-0.025364291341458806	1.0
-0.004544963468122415	0.004904103547692454	-0.0378567647657568	-0.03168682172680219	-0.3252519582922828	0.016309447878346907	-0.10489334029057046	0.050296691365436595	-0.07704730503449823	-0.03702756445405166	0.03730587151519908	0.02072227644823873	-0.09538439276422932	-0.046572410004242976	-0.03069035486780054	0.043136144718683024	-0.07389750588798712	-0.028103023796657203	-0.003056023404567889	-0.020330707457755344	0.01975824321631602	0.010977516132364067	-0.07582888600529314	-0.452893081495849	-0.014642648524477046	-0.04050650032974544	0.013722169903698438	-0.054947624214738894	-0.08653998788481539	-0.11168566020988359	0.005297822878977246	-0.027660967825123044
0.0050998319091064	0.009485620040856785	0.04620476962899047	-0.01778265459136491	0.030894150400457333	0.016949474454994225	-0.025682936522052172	-0.040835364481840726	-0.012962854876657093	0.01041473492725239	0.01978983911885433	0.019755210077790738	-0.052695381054853885	0.0016809785913384978	-0.0009200751206716614	-0.016461717250198373	0.06539258133610282	0.0029837049402429566	0.005985561287877741	0.037893393788509554	-0.018173645338477772	0.04456759165343849	0.04511246283048389	0.00816500765112864	-0.0026191371833058897	-0.019208044034897825	-0.011614923621658899	-0.051588120792193236	-0.059041501834662884	0.008595463436843306	-0.02658861905555922	-0.0032260250003369423
0.012465942335182457	-0.024422892327345423	0.005191787174275892	-0.05348340320477618	-0.21474191833852976	0.01899876334269746	0.16974663533446463	0.11470442312676472	0.017087834287947135	-0.06033112031960062	0.014498277616551628	-0.03858987165904613	0.4528685085941018	-0.006393140407936686	0.1451784713217935	-0.004295268979906706	0.025083878238753465	0.012547890835683195	-0.03432664904794916	-0.008389851440460875	-0.02452545368533324	-0.017977953797899876	-0.006026667586788515	0.2561218872925343	-0.011841584557480135	0.029576122140404528	-0.012059014115271764	0.029273869527862657	0.295394690257197	0.14027859845448407	-0.014672016124992452	0.007880843277222608
0.04728242596420919	0.05394304876770505	0.013109949760601541	0.025659038806782167	0.014834983214190592	-0.03425759397400323	-0.004401258301692165	-0.014853957338716774	0.005111772120796375	0.0014070616645381444	0.022755892144469703	-0.0027609024091486125	-0.0046690882428916565	0.04666118901491308	-0.010864685043739645	-0.02120832077743152	-0.0641243349172065	-0.015247902406540951	0.02771428322613698	-0.02987328295491777	0.01085270612663634	0.012606609138986136	-0.014283753296641334	0.011835119154928768	0.009221466085232041	-0.015687794974501058	-0.03732258303999062	0.008881659284502741	-0.025154899050168354	-0.022750514135086514	-0.03795001686542754	-0.031148965826617386
-0.026683103470366256	0.04078158093196674	0.018986831635210366	0.04960580874619542	0.015536538819444569	-0.009738399202127835	0.08915272199142156	0.12377964709698605	-0.007374810404830223	-0.00533738067213506	-0.04363623758655662	0.03675493745007846	-0.18362240248557757	-0.0022499468282855253	-0.11688197680384393	0.0012916516343688899	-0.00690600368319696	0.008158561462863048	-0.04310795701737166	-0.0022610518459758823	0.07999052625142385	-0.021476850937769847	-0.13517209810745678	0.20394371302589992	-0.02365586727574572	0.03936862087641153	-0.014025120773611758	-0.04179145908674117	0.019432300563257315	0.24300229334860463	-0.07232999365213211	-0.004552486588304906
0.039736633357061346	0.016375490677840273	0.03454971180542497	0.01304606669212278	0.023760630112327673	-0.04610742574503523	0.07783340517925358	0.01936023286824309	0.015786396263533977	-0.03329896293531549	-0.01573688418066047	-0.016524170399984255	-0.04477759801790338	0.0072940519175941545	0.03515418732398103	0.0022771725060582357	-0.011851123691612052	0.029516075890830346	0.00792861285890709	-0.025016044251758525	-0.02973545496945538	0.03836276889151649	0.04476970378940084	0.005380238359689634	-0.024669368157650882	-0.028076574933345744	0.023122018066947025	-0.0013356954985795488	-0.015066415383383358	0.007028676703856571	-0.06434343919317276	0.0018097186462519093
0.016346152380440668	-0.04739603261066961	-0.003015813950389332	0.04013623487029141	-0.08392520914740037	0.03319042392951839	-0.14065359956647594	-0.04903763538270567	-0.019135702955388245	-0.01987443925964865	-0.014737889309404584	0.04189191057558705	0.09111433865886566	0.019056562175729534	-0.16315434196094059	-0.006288332478264776	0.04195381930944115	0.0410969777025651	-0.2821348130338885	0.029838322088116872	-0.029155682838851695	0.0014467996714430326	-0.2798356634564131	-0.3062068671031013	0.007005499595381814	0.03939661883393439	0.010531560887540606	0.018622174417766594	0.18059125402874765	0.048585735259683745	0.02769283194002841	-0.005490600764377647
-0.04577512960046766	-0.0059654637246400605	0.020513473497168203	0.020931375138666927	-0.22783146624780662	0.013388543883483185	-0.23380577464291125	-0.150825012564106	-0.041376010419123484	-0.07002518842629077	0.026303508018553308	-0.020711103758153684	0.11063233289922061	-0.056717791144279425	0.09342947518910241	-0.04214474882303668	0.01370571550029058	0.02658243639420351	0.3003258359935288	0.01008228670857689	-0.03689864023282337	-0.012037381993889215	-0.10990045201022486	-0.3103999443445203	-0.000887810894585708	-0.03196329710753306	0.04576099605297374	0.052526425242545395	0.2830487275085692	0.28716536913336466	0.018128427118680037	-0.016476673650121034

The correlation coefficient ranges from –1 to 1. When it is close to 1, it means
that there is a strong positive correlation; for example, the median house value
tends to go up when the median income goes up. When the coefficient is close to
–1, it means that there is a strong negative correlation.

Finally, coefficients close to zero mean that there is no linear correlation.
"""

#####################################################################################
#
# 5. Select a model and train it.
#
# Let’s first train a Linear Regression model:

from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()
lin_reg.fit(train_df, train_labels)

# Let’s try it out on a few instances from the training set
a_few_data = train_df.iloc[:10]
a_few_data
"""
         0         1         2     ...           37        38        39
0  0.299403 -1.226624  1.498425    ...    -2.715559 -2.682409  0.101050
1 -1.174176  0.332157  0.949919    ...     1.213219  1.382932 -1.817761
2  1.192222 -0.414371  0.067054    ...     0.656438 -0.932473  2.987436
3  1.573270 -0.580318 -0.866332    ...     0.215748  0.619645  1.883397
4 -0.613071 -0.644204  1.112558    ...     0.870772 -1.894609  0.408332

[5 rows x 40 columns]
"""
a_few_labels = train_labels.iloc[:10]
a_few_labels
"""
   0
0  1
1  0
2  0
3  1
4  0
5  1
6  0
7  1
8  1
9  0
"""

print('Predictions:\t', np.abs(np.round(lin_reg.predict(a_few_data))))
"""
Predictions:     
[[ 1.]
 [ 0.]
 [ 0.]
 [ 0.]
 [ 0.]
 [ 1.]
 [ 0.]
 [ 1.]
 [ 0.]
 [ 0.]]
"""
print('Labels:\t', a_few_labels)
"""
Labels:
0  1
1  0
2  0
3  1 <------ the prediction was wrong
4  0
5  1
6  0
7  1
8  1 <------ the prediction was wrong
9  0
"""

# It works, although the predictions are not exactly accurate (e.g., the third
# and the eight predictions weren't successful, wich means it was 80% of 
# successful predictions!).

# Let’s measure this regression model’s RMSE on the whole training set using
# Scikit-Learn’s mean_squared_error function:
from sklearn.metrics import mean_squared_error
import numpy as np

predictions = lin_reg.predict(train_df)

lin_mse = mean_squared_error(train_labels, predictions)
lin_rmse = np.sqrt(lin_mse)
lin_rmse
# 0.3630995032766908

"""
Okay, this is better than nothing but clearly not a great score:
    
  * 64 % successful predictions  
  * 36 % failures

This is an example of a model underfitting the training data.

When this happens it can mean that the features do not provide enough information
to make good predictions, or that the model is not powerful enough.

The main ways to fix underfitting are:

  * select a more powerful model
  * feed the training algorithm with better features, not possible in our circumstances
  * reduce the constraints on the model.
  
This model is not regularized, so this rules out the last option, but the features
are very homogeneous, so there's not a logical union between them to introduce
new calculated ones

So our only chanc is tryying a more complex model to see how it does.
"""
# Let's compute the cross-validation scores
def display_scores(scores):
    print('Scores: ', scores)
    print('Mean: ', scores.mean())
    print('Standard deviation: ', scores.std())

from sklearn.model_selection import cross_val_score

lin_scores = cross_val_score(lin_reg, train_df, train_labels, scoring='neg_mean_squared_error', cv=10)
lin_rmse_scores = np.sqrt(-lin_scores)
display_scores(lin_rmse_scores)
"""
Scores:  [0.39615626 0.34683562 0.34038119 0.36497862 0.38110545 0.33204426 0.38683085 0.41132961 0.37080227 0.43326716]
Mean:  0.37637312859311745
Standard deviation:  0.030456230787316175
"""

# Let’s try the RandomForestRegressor
from sklearn.ensemble import RandomForestRegressor

forest_reg = RandomForestRegressor()
forest_reg.fit(train_df, train_labels)

# Now that the model is trained, let’s evaluate it on the training set:
forest_predictions = forest_reg.predict(train_df)
forest_mse = mean_squared_error(train_labels, forest_predictions)    
forest_rmse = np.sqrt(forest_mse)
forest_rmse
# 0.14778362561528932

# This means:
#  * 86 % successful predictions  
#  * 14 % failures

# Let’s compute the cross validation scores
forest_scores = cross_val_score(forest_reg, train_df, train_labels, scoring='neg_mean_squared_error', cv=10)
forest_rmse_scores = np.sqrt(-forest_scores)

display_scores(forest_rmse_scores)
"""
Scores:  [0.34785054 0.34438351 0.31874755 0.33436507 0.33451457 0.32557641
 0.33985291 0.38249183 0.33256578 0.38157568]
Mean:  0.34419238571952465
Standard deviation:  0.020532939699467793
"""

# RandomForestRegressor looks better than LinearRegression

#####################################################################################
#
# 6. Fine-tune our model.
#
from sklearn.model_selection import GridSearchCV

# When you have no idea what value a hyperparameter should have, a simple
# approach is to try out consecutive powers of 10 (or a smaller number if you
# want a more fine-grained search, as shown in this example with the n_estimators
# hyperparameter).
param_grid = [
        # This param_grid tells Scikit-Learn to first evaluate all 3 × 4 = 12 
        # combinations of n_estimators and max_features hyperparameter values 
        # specified in the first dict
        {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},
        # in the second dict we are telling to try out all 2 × 3 = 6 combinations 
        # of hyperparameter values, but this time with the bootstrap hyperparameter
        # set to False instead of True
        {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},
        {'n_estimators': [40, 50, 60], 'max_features': [10, 12, 14, 20, 30, 35, 37, 40]},
        {'bootstrap': [False], 'n_estimators': [40, 50, 60], 'max_features': [10, 12, 14, 20, 30, 35, 37, 40]},
    ]

forest_reg = RandomForestRegressor()

# All in all, the grid search will explore 12 + 6 = 18 combinations of
# RandomForestRegressor hyperparameter values, and it will train each model five
# times (since we are using five-fold cross validation).
# In other words, all in all, there will be 18 × 5 = 90 rounds of training!
grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error')

# It may take quite a long time ...
grid_search.fit(train_df, train_labels)

# ... but when it is done we can get the best combination of parameters:
grid_search.best_params_
# {'bootstrap': False, 'max_features': 12, 'n_estimators': 50}

# We can also get the best estimator directly:
grid_search.best_estimator_
"""
RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=None,
           max_features=12, max_leaf_nodes=None, min_impurity_decrease=0.0,
           min_impurity_split=None, min_samples_leaf=1,
           min_samples_split=2, min_weight_fraction_leaf=0.0,
           n_estimators=50, n_jobs=None, oob_score=False,
           random_state=None, verbose=0, warm_start=False)
"""

# And of course the evaluation scores are also available:
cvres = grid_search.cv_results_
for mean_score, params in zip(cvres['mean_test_score'], cvres['params']):
    print(np.sqrt(-mean_score), params)
"""
0.4616395900603751 {'max_features': 2, 'n_estimators': 3}
0.41298910397248983 {'max_features': 2, 'n_estimators': 10}
0.3891829047290404 {'max_features': 2, 'n_estimators': 30}
0.41819718900165853 {'max_features': 4, 'n_estimators': 3}
0.3743661309466977 {'max_features': 4, 'n_estimators': 10}
0.3672676892343725 {'max_features': 4, 'n_estimators': 30}
0.4273952113286562 {'max_features': 6, 'n_estimators': 3}
0.3707020366817534 {'max_features': 6, 'n_estimators': 10}
0.3519832698044358 {'max_features': 6, 'n_estimators': 30}
0.4294699575575042 {'max_features': 8, 'n_estimators': 3}
0.3473182978191619 {'max_features': 8, 'n_estimators': 10}
0.3420558758773517 {'max_features': 8, 'n_estimators': 30}
0.4509249752822894 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3}
0.40162171256046403 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10}
0.4346134936801766 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3}
0.38107741995557803 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10}
0.42882267767562027 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3}
0.3774254893353124 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}
0.3370849299508953 {'max_features': 10, 'n_estimators': 40}
0.3338586527259703 {'max_features': 10, 'n_estimators': 50}
0.3370600012526619 {'max_features': 10, 'n_estimators': 60}
0.33080394495833937 {'max_features': 12, 'n_estimators': 40}
0.32963858997392886 {'max_features': 12, 'n_estimators': 50}
0.32997390469214044 {'max_features': 12, 'n_estimators': 60}
0.33319570075257576 {'max_features': 14, 'n_estimators': 40}
0.3292379079024771 {'max_features': 14, 'n_estimators': 50}
0.33013970106944457 {'max_features': 14, 'n_estimators': 60}
0.33168697592760554 {'max_features': 20, 'n_estimators': 40}
0.33129563836549375 {'max_features': 20, 'n_estimators': 50}
0.326314861581401 {'max_features': 20, 'n_estimators': 60}
0.33057714682052664 {'max_features': 30, 'n_estimators': 40}
0.3288634975183473 {'max_features': 30, 'n_estimators': 50}
0.3330928299038173 {'max_features': 30, 'n_estimators': 60}
0.3312533018703361 {'max_features': 35, 'n_estimators': 40}
0.329197812872443 {'max_features': 35, 'n_estimators': 50}
0.3308965095010825 {'max_features': 35, 'n_estimators': 60}
0.33040978345079314 {'max_features': 37, 'n_estimators': 40}
0.33349722637527285 {'max_features': 37, 'n_estimators': 50}
0.330409258008576 {'max_features': 37, 'n_estimators': 60}
0.32973284640751216 {'max_features': 40, 'n_estimators': 40}
0.3311489091028385 {'max_features': 40, 'n_estimators': 50}
0.32986739760091477 {'max_features': 40, 'n_estimators': 60}
0.3232278608041083 {'bootstrap': False, 'max_features': 10, 'n_estimators': 40}
0.32368379631980343 {'bootstrap': False, 'max_features': 10, 'n_estimators': 50}
0.32048530907567874 {'bootstrap': False, 'max_features': 10, 'n_estimators': 60}
0.31981439930059435 {'bootstrap': False, 'max_features': 12, 'n_estimators': 40}
0.3160803695264861 {'bootstrap': False, 'max_features': 12, 'n_estimators': 50}
0.32052864319918606 {'bootstrap': False, 'max_features': 12, 'n_estimators': 60}
0.32407078702036696 {'bootstrap': False, 'max_features': 14, 'n_estimators': 40}
0.31864211899872874 {'bootstrap': False, 'max_features': 14, 'n_estimators': 50}
0.3183376963051798 {'bootstrap': False, 'max_features': 14, 'n_estimators': 60}
0.3280501105014293 {'bootstrap': False, 'max_features': 20, 'n_estimators': 40}
0.3195822272905676 {'bootstrap': False, 'max_features': 20, 'n_estimators': 50}
0.3222236111081178 {'bootstrap': False, 'max_features': 20, 'n_estimators': 60}
0.3494495671767244 {'bootstrap': False, 'max_features': 30, 'n_estimators': 40}
0.34552973822812993 {'bootstrap': False, 'max_features': 30, 'n_estimators': 50}
0.3458355421616208 {'bootstrap': False, 'max_features': 30, 'n_estimators': 60}
0.3784136295114118 {'bootstrap': False, 'max_features': 35, 'n_estimators': 40}
0.3768718615126367 {'bootstrap': False, 'max_features': 35, 'n_estimators': 50}
0.3743887611086161 {'bootstrap': False, 'max_features': 35, 'n_estimators': 60}
0.4056915392265409 {'bootstrap': False, 'max_features': 37, 'n_estimators': 40}
0.4017451928772764 {'bootstrap': False, 'max_features': 37, 'n_estimators': 50}
0.40198949682243634 {'bootstrap': False, 'max_features': 37, 'n_estimators': 60}
0.45295626168538616 {'bootstrap': False, 'max_features': 40, 'n_estimators': 40}
0.45174683175424707 {'bootstrap': False, 'max_features': 40, 'n_estimators': 50}
0.45350823831791875 {'bootstrap': False, 'max_features': 40, 'n_estimators': 60}

{'bootstrap': False, 'max_features': 12, 'n_estimators': 50}

In this example, we obtain the best solution by setting the max_features
hyperparameter to 12, the n_estimators hyperparameter to 50 and the bootstrap to
false.

The RMSE score for this combination is 0.313, which is slightly better than the
score we got earlier using the default hyperparameter values (which was 0.344).

We have successfully fine-tuned our best model!
"""

# Analyze the Best Model Errors
# -----------------------------

feature_importances = grid_search.best_estimator_.feature_importances_
feature_importances
"""
array([0.00894841, 0.00898077, 0.00759761, 0.00884176, 0.03796187,
       0.00747169, 0.05532201, 0.02091613, 0.00694056, 0.00770392,
       0.00849191, 0.01168478, 0.1132775 , 0.00843988, 0.16727057,
       0.00680161, 0.0061701 , 0.01030992, 0.05425964, 0.00650074,
       0.00919954, 0.00920912, 0.02113551, 0.02467531, 0.01012177,
       0.01013682, 0.01092041, 0.00820194, 0.03121077, 0.05506531,
       0.01001655, 0.0077219 , 0.04041725, 0.00963466, 0.03319729,
       0.00816828, 0.05077126, 0.00833071, 0.01942808, 0.05854615])


Let’s display these importance scores next to their corresponding attribute
names:
"""
sorted(zip(feature_importances, train_df.columns.values), reverse=True)
"""
[(0.16727056512413782, 14),
 (0.11327749820826752, 12),
 (0.058546145157484755, 39),
 (0.055322012727899106, 6),
 (0.05506530573583615, 29),
 (0.05425963888766014, 18),
 (0.0507712556512533, 36),
 (0.04041724678587891, 32),
 (0.03796186607066457, 4),
 (0.033197287059261116, 34),
 (0.031210768719431178, 28),
 (0.024675310678080157, 23),
 (0.021135510009139366, 22),
 (0.020916131333233997, 7),
 (0.019428081034046817, 38),
 (0.011684783838282982, 11),
 (0.010920409601838963, 26),
 (0.010309923522650501, 17),
 (0.01013682488449848, 25),
 (0.01012177374896067, 24),
 (0.010016550908923559, 30),
 (0.00963466487511449, 33),
 (0.009209120729755261, 21),
 (0.009199538797669545, 20),
 (0.008980774002157188, 1),
 (0.008948409404278723, 0),
 (0.008841758762238172, 3),
 (0.008491911301327123, 10),
 (0.008439883294756067, 13),
 (0.008330711279359288, 37),
 (0.00820193807375149, 27),
 (0.008168284264286929, 35),
 (0.007721896193395155, 31),
 (0.007703920136352713, 9),
 (0.0075976080725872405, 2),
 (0.007471686878558326, 5),
 (0.006940558874626506, 8),
 (0.0068016086182903146, 15),
 (0.006500740424027823, 19),
 (0.006170096330037595, 16)]
"""

# With this information, we could try dropping some of the less useful features, starting from the
# bottom of the list

"""
* Evaluate Our System on the Test Set
-------------------------------------

After tweaking our models for a while, we eventually have a system that performs
sufficiently well.

Now is the time to evaluate the final model on the test set.
"""
final_predictions = np.round(np.abs(grid_search.predict(test_df)))
final_predictions
# array([1., 0., 0., ..., 1., 0., 1.])

final_predictions.shape
# (9000,)

result = sorted(zip(test_df.index.values, final_predictions))
print('Id,Solution')
print(*result , sep='\n')



