{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport datetime\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport re\nfrom scipy import stats\n\nmatplotlib.rcParams['figure.figsize'] = (10, 5)\nmatplotlib.rcParams['font.size'] = 12\n\nimport random\nrandom.seed(1)\nimport time\n\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import get_scorer\nfrom sklearn.metrics import f1_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import VotingClassifier\nimport lightgbm as lgb\nfrom sklearn.externals.joblib import Parallel, delayed\nfrom sklearn.base import clone\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input/\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from kaggle.competitions import twosigmanews\n# You can only call make_env() once, so don't lose it!\nenv = twosigmanews.make_env()\nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db5d50c4102023f21edad90992f5348ec67675e4"},"cell_type":"code","source":"(mtrain, ntrain) = env.get_training_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c08f9172b9798cd69d5ea1c802005d4c5967f37f"},"cell_type":"code","source":"market = mtrain.copy()\nnews = ntrain.copy()\nmarket.time = mtrain.time.astype('datetime64[D, UTC]')\n\nnews.time = mtrain.time.astype('datetime64[D, UTC]')\n\nprint(market.shape)\nprint(news.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e81c80f714629299104af92e4b06fbec3309b28b"},"cell_type":"code","source":"market.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af07a62ccb2c7fb6b87119388fbf8dc36d1ad392"},"cell_type":"code","source":"print(\"Total asset count: \" + str(len(market['assetCode'].unique())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"550f136d859bf8a3f02b8f291244c64ab398c0a2"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"517fa64c7c0a6b2c194818c7e7706bde02acf0b4"},"cell_type":"code","source":"def get_asset(df, asset=None):\n    #get an asset, if none specified get random asset\n    ass = asset\n    if ass is None: #get random asset\n        ass = df['assetCode'].unique()[random.randint(0, len(df['assetCode'].unique()))]\n    ass_market = df[df['assetCode'] == ass]\n    ass_market.index = ass_market.time\n    return ass_market","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a690ed1117ca6e9494cc1bfc73b86d6c2eae61b7"},"cell_type":"code","source":"plt.plot(get_asset(market).close) #Plots asset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a073cc16d5f51f6b5aa0821e2891ed8d529304c6"},"cell_type":"code","source":"#gets a sample of assets with all data present after 2009\n#since a lot of companies went bankrupt then lol\ndef market_split(market, sample_size=100000):\n    midx = market[market.time > '2009'][['time', 'assetCode']]\n    midx = midx.sample(sample_size)\n    midx = midx.sort_values(by=['time'])\n    \n    market_train, market_test = train_test_split(midx, shuffle=False, random_state=24)\n    market_train, market_val = train_test_split(market_train, test_size=0.1, shuffle=False, random_state=24)\n    \n    return market_train, market_val, market_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b43fc5123626616a76ff3871c0217213d3ad3824"},"cell_type":"code","source":"mtrain, mval, mtest = market_split(market)\nprint(\"market: \")\nprint(\"    train size: \" + str(len(mtrain)))\nprint(\"    val size:   \" + str(len(mval)))\nprint(\"    test size:  \" + str(len(mtest)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a0d551249028d40d3468f7f32b0112ef98d9cd7"},"cell_type":"code","source":"print(str(len(mtrain.assetCode.unique())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17514e7a3b8a831579c0f6c8651cb37812ac1472"},"cell_type":"code","source":"class MarketPrepro:\n    \n    assetcode_encoded = []\n    assetcode_train_count = 0\n    time_cols = ['year', 'week', 'day', 'dayofweek']\n    num_cols = ['volume', 'close', 'open', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevMktres1',\n                    'returnsOpenPrevMktres1', 'returnsClosePrevRaw10', 'returnsOpenPrevRaw10', 'returnsClosePrevMktres10',\n                    'returnsOpenPrevMktres10']\n    #all features\n    feat_cols = ['assetCode_encoded'] + time_cols + num_cols\n    \n    label_cols = ['returnsOpenNextMktres10']\n    \n    def __init__(self):\n        self.cats = {}\n    \n    def fit(self, mtrain):\n        df = mtrain.copy()\n        #fix/clean data\n        mtrain = self.fix_train(mtrain)\n        \n        #get time cols\n        mtrain = self.prep_time_cols(mtrain)\n        \n        #standardize features by using z = (x - u) / s\n        self.num_scaler = StandardScaler()\n        self.num_scaler.fit(mtrain[self.num_cols + self.time_cols].astype(float))\n    \n        mtrain = self.encode_asset(mtrain, True)\n    \n    def fix_train(self, mtrain):\n        #fix/clean data\n        max_ratio = 2 #removing outliers\n        mtrain = mtrain[((mtrain['close'])/mtrain['open']).abs() <= max_ratio].loc[:]\n        \n        mtrain = self.safe_fix(mtrain)\n        return mtrain\n    \n    def safe_fix(self, mtrain):\n        #fill na and outliers, safe for train, no rows removed\n        \n        #fill na using bfill \n        mtrain[self.num_cols] = mtrain[['assetCode'] + self.num_cols].groupby('assetCode').transform(lambda g: g.fillna(method='bfill'))\n        mtrain[self.num_cols] = mtrain[self.num_cols].fillna(0) #using 0\n        \n        #fix outliers based on quantiles\n        mtrain[self.num_cols] = mtrain[self.num_cols].clip(mtrain[self.num_cols].quantile(0.01), mtrain[self.num_cols].quantile(0.99), axis=1)\n        \n        return mtrain\n    \n    def get_X(self, mtrain):\n        #return x \n        mtrain = mtrain.copy()\n        mtrain = self.safe_fix(mtrain)\n        \n        mtrain = self.prep_time_cols(mtrain)\n        mtrain = self.encode_asset(mtrain, istrain=False)\n        \n        mtrain[self.num_cols + self.time_cols] = self.num_scaler.transform(mtrain[self.num_cols +self.time_cols].astype(float))\n        \n        return mtrain[self.feat_cols]\n    \n    def get_y(self, mtrain):\n        y = (mtrain[self.label_cols]>=0).astype(float)\n        return y\n    \n    def encode_asset(self, df, istrain):\n        def encode(assetcode):\n            try: \n                indx_val = self.assetcode_encoded.index(assetcode) + 1\n            except ValueError: \n                self.assetcode_encoded.append(assetcode)\n                indx_val = len(self.assetcode_encoded)\n            \n            indx_val = indx_val/ (self.assetcode_train_count + 1)\n            return indx_val\n        \n        if istrain:\n            self.assetcode_train_count = len(df['assetCode'].unique()) +1 \n        df['assetCode_encoded'] = df['assetCode'].apply(lambda assetcode: encode(assetcode))\n        return df\n    \n    def prep_time_cols(self, df): \n        #extract time cols, important for time series\n        df = df.copy()\n        df['year'] = df['time'].dt.year\n        df['day'] = df['time'].dt.day\n        df['week'] = df['time'].dt.week\n        df['dayofweek'] = df['time'].dt.dayofweek\n        return df\n    \nmarket_prepro = MarketPrepro()\nprint('market preprocessed lmao')\n        \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"218c24ce27d082736b19c7da3bf1efe0fc894829"},"cell_type":"code","source":"class NewsPrepro():\n    news_cols_agg = {\n        'urgency': ['min', 'count'],\n        'takeSequence': ['max'],\n        'bodySize': ['min', 'max', 'mean', 'std'],\n        'wordCount': ['min', 'max', 'mean', 'std'],\n        'sentenceCount': ['min', 'max', 'mean', 'std'],\n        'companyCount': ['min', 'max', 'mean', 'std'],\n        'marketCommentary': ['min', 'max', 'mean', 'std'],\n        'relevance': ['min', 'max', 'mean', 'std'],\n        'sentimentNegative': ['min', 'max', 'mean', 'std'],\n        'sentimentNeutral': ['min', 'max', 'mean', 'std'],\n        'sentimentPositive': ['min', 'max', 'mean', 'std'],\n        'sentimentWordCount': ['min', 'max', 'mean', 'std'],\n        'noveltyCount12H': ['min', 'max', 'mean', 'std'],\n        'noveltyCount24H': ['min', 'max', 'mean', 'std'],\n        'noveltyCount3D': ['min', 'max', 'mean', 'std'],\n        'noveltyCount5D': ['min', 'max', 'mean', 'std'],\n        'noveltyCount7D': ['min', 'max', 'mean', 'std'],\n        'volumeCounts12H': ['min', 'max', 'mean', 'std'],\n        'volumeCounts24H': ['min', 'max', 'mean', 'std'],\n        'volumeCounts3D': ['min', 'max', 'mean', 'std'],\n        'volumeCounts5D': ['min', 'max', 'mean', 'std'],\n        'volumeCounts7D': ['min', 'max', 'mean', 'std']\n            }\n    news_cols_numeric = set(news_cols_agg.keys()) - set(['assetCode', 'time'])\n    \n    def fit(self, ntrain):\n        ntrain = ntrain.copy()\n        news_train_agg = self.aggregate_news(ntrain)\n        news_train_agg.fillna(0, inplace=True)\n        \n        self.numeric_scaler = StandardScaler()\n        self.numeric_scaler.fit(news_train_agg)\n        self.feat_cols = list(news_train_agg.columns.values)\n        \n    def get_X(self, df):\n        news_df = df.copy()\n        news_df = self.aggregate_news(df)\n        news_df.fillna(0, inplace=True)\n        if not news_df.empty:\n            news_df_numeric = news_df._get_numeric_data().astype(float)\n            news_df[news_df_numeric.columns] = self.numeric_scaler.transform(news_df_numeric)\n        return(news_df)\n        \n        \n    def aggregate_news(self, df):\n        # Fix asset codes (str -> list)\n        df['assetCodes'] = df['assetCodes'].str.findall(f\"'([\\w\\./]+)'\")    \n\n        # Leave only days in time\n        if not df.empty: df.time = df.time.astype('datetime64[D, UTC]') #.tail()\n        \n        #Expand assetCodes\n        assetCodes_expanded = list(chain(*df['assetCodes']))\n        \n        if(not df.empty): assetCodes_index = df.index.repeat(df['assetCodes'].apply(len)) \n        else: assetCodes_index = df.index\n        assert len(assetCodes_index) == len(assetCodes_expanded)\n        df_assetCodes = pd.DataFrame({'level_0': assetCodes_index, 'assetCode': assetCodes_expanded})\n\n        # Create expanded news (will repeat every assetCodes' row)\n        news_cols = ['time', 'assetCodes'] + sorted(list(self.news_cols_agg.keys()))\n        df_expanded = pd.merge(df_assetCodes, df[news_cols], left_on='level_0', right_index=True, suffixes=(['','_old']))\n\n        # Aggregate numerical news features\n        df_aggregated = df_expanded.groupby(['time', 'assetCode']).agg(self.news_cols_agg)\n\n        # Convert to float32 to save memory\n        #df_aggregated = df_aggregated.apply(np.float32)\n\n        # Flat columns\n        df_aggregated.columns = ['_'.join(col).strip() for col in df_aggregated.columns.values]\n\n        return df_aggregated    \n    \nnews_prep = NewsPrepro()\nprint(\"news preprocessed!\")\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ce9d9fab677c5eb1f82af30db01adf3f603369f"},"cell_type":"code","source":"class JoinedPreprocessor:\n    def __init__(self, market_prepro, news_prepro):\n        self.market_prepro = market_prepro\n        self.news_prepro = news_prepro\n        \n    def fit(self, market_train_idx, market, news):\n        # market has index [time, assetCode]\n        market_train_df = market.loc[market_train_idx.index]\n        self.market_prepro.fit(market_train_df)\n        # We select news in train time interval\n        news_train_df = news.merge(market_train_idx, on=['time'])\n        self.news_prepro.fit(news_train_df)\n    \n    def get_X(self, market_df, news_df):\n        # Market should already has index (time, assetCode)\n        # Preprocess market X\n        market_X = market_prepro.get_X(market_df)\n        market_X['time'] = market_df['time']\n        market_X['assetCode'] = market_df['assetCode']\n        \n        #news_X will have index [time, assetCode]\n        news_X = news_prepro.get_X(news_df)\n        # Join by index, which is time, assetCode. Some assets have no news at all, so left join and 0 nans\n        X = market_X.merge(news_X, how='left', left_on=['time', 'assetCode'], right_on=['time','assetCode'],  right_index=True)\n        \n        # Some market data can be without news, fill nans\n        X.fillna(0, inplace=True)\n        # Return features market + news from joined df\n        features = X[market_prepro.feature_cols + news_prepro.feature_cols]\n        return(features)\n\n    def get_y(self, market_df): \n        return(self.market_prepro.get_y(market_df))\n    \n    def get_Xy(self, market_df, news_df):\n        return(self.get_X(market_df, news_df), self.get_y(market_df))\n    \n    def fix_train(self, market_df, news_df):\n        \"\"\"\n        Clean train data. Here we can remove bad rows\n        \"\"\"\n        return(market_prepro.fix_train(market_df), news_df)\n\n    \n# Market and news preprocessor instance\nprepro = JoinedPreprocessor(market_prepro, news_prep)\nprepro.fit(market.loc[mtrain.index], market, news)\nprint('Preprocessor created, it is fit')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d05416842889002f097f54a3b8e6401f6e20590"},"cell_type":"code","source":"def get_merged_Xy(idx):\n    \"\"\"\n    Show min/max and quantiles for given sample\n    \"\"\"\n    market_df = market.loc[idx.index]\n    # Select subset of news for future merge by assetCode and time. \n    news_df = news.merge(idx, on=['time'])\n    X, y = prepro.get_Xy(market_df, news_df)\n    return pd.concat([X,y], axis=1)\n\n# Look at statistics of preprocessed sample\nget_merged_Xy(market_test_idx.sample(10000)).describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"66d86ea3638000a19a3f5602b16d479fda3d0011"},"cell_type":"code","source":"class ModelFactory:\n    \"\"\"\n    Generate different models. Actually only one of them is used in the kernel, \n    this factory is for experiments when debugging.\n    \"\"\"\n    # LSTM look back window size\n    look_back=90\n    # In windows size look back each look_back_step days\n    look_back_step=10\n\n    def lstm_128():\n        model = Sequential()\n        # Add an input layer market + news\n        input_size = len(market_prepro.feature_cols) + len(news_prepro.feature_cols)\n        # input_shape=(timesteps, input features)\n        model.add(LSTM(units=128, return_sequences=True, input_shape=(None,input_size)))\n        model.add(LSTM(units=64, return_sequences=True ))\n        model.add(LSTM(units=32, return_sequences=False))\n        \n        # Add an output layer \n        model.add(Dense(1, activation='sigmoid'))\n        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n        \n        return(model)        \n\nmodel = ModelFactory.lstm_128()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"364dbc22dc05eb49191ce614686cba116aad5a12"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a281a8e87850a9651aa78b6b068ce09a46d3df33"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}