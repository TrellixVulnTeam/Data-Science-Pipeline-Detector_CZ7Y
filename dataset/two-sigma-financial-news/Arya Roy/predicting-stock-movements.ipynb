{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5363c3301fe77aeac413f57975e1759c5a6ae125"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom datetime import datetime, date\nimport lightgbm as lgb\nfrom scipy import stats\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.model_selection import train_test_split\nfrom wordcloud import WordCloud\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nstop = set(stopwords.words('english'))\nimport time\nimport random\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score, mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":true},"cell_type":"code","source":"# official way to get the data\nfrom kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()\nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"079b1d2474f911283fc1b04e552f421af763fdad"},"cell_type":"code","source":"(market_train_df, news_train_df) = env.get_training_data()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e88b8bf5746673a3967f2a87161dee6b53a7f4f"},"cell_type":"code","source":"print(f'{market_train_df.shape[0]} samples and {market_train_df.shape[1]} features in the training market dataset.')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"29e19c161bb2fbe8a3031adbd04fdd17e298910e"},"cell_type":"markdown","source":"Given below are the top 5 rows of market train dataframe"},{"metadata":{"trusted":true,"_uuid":"2355b1eeaee86bf5aa21b3961d6bf7f103d0501d"},"cell_type":"code","source":"market_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db8098d4bdfd6a3712456ea71efc81307a76b9b7"},"cell_type":"code","source":"data = []\nfor asset in np.random.choice(market_train_df['assetName'].unique(), 10):\n    asset_df = market_train_df[(market_train_df['assetName'] == asset)]\n\n    data.append(go.Scatter(\n        x = asset_df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n        y = asset_df['close'].values,\n        name = asset\n    ))\nlayout = go.Layout(dict(title = \"Closing prices of 10 random assets\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"))\npy.iplot(dict(data=data, layout=layout), filename='basic-line')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78b08001cdb10eb74277aeb6ff9394f60b8ff5e3"},"cell_type":"markdown","source":"I plot data for all periods because I would like to show long-term trends. Assets are sampled randomly, but you should see that some companies' stocks started trading later, some dissappeared. Disappearence could be due to bankruptcy, acquisition or other reasons."},{"metadata":{"_uuid":"17f4335c3ed0c7b3a2b94da2f1a2ea735f1f01fa"},"cell_type":"markdown","source":"Well, these were some random companies. But it would be more interesting to see general trends of prices."},{"metadata":{"trusted":true,"_uuid":"aca04c6ab806ea6b5f32d432f3d6b1af278e2cf9"},"cell_type":"code","source":"data = []\n#market_train_df['close'] = market_train_df['close'] / 20\nfor i in [0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95]:\n    price_df = market_train_df.groupby('time')['close'].quantile(i).reset_index()\n\n    data.append(go.Scatter(\n        x = price_df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n        y = price_df['close'].values,\n        name = f'{i} quantile'\n    ))\nlayout = go.Layout(dict(title = \"Trends of closing prices by quantiles\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"))\npy.iplot(dict(data=data, layout=layout), filename='basic-line')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"23283e3d78d6f8b83179357a79f7d9ed7f4c38ca"},"cell_type":"markdown","source":"It is cool to be able to see how markets fall and rise again. I have shown 4 events when there were serious stock price drops on the market. You could also notice that higher quantile prices have increased with time and lower quantile prices decreased. Maybe the gap between poor and rich increases... on the other hand maybe more \"little\" companies are ready to go to market and prices of their shares isn't very high."},{"metadata":{"trusted":true,"_uuid":"c32ca0cc46577ee31ab8222bc28ba17001ec9f07"},"cell_type":"code","source":"market_train_df['time'] = market_train_df['time'].dt.date\nmarket_train_df = market_train_df.loc[market_train_df['time']>=date(2010, 1, 1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc3a3d129406e90fa834dab1f0dd1556b71b6465"},"cell_type":"code","source":"market_train_df['price_diff'] = market_train_df['close'] - market_train_df['open']\ngrouped = market_train_df.groupby('time').agg({'price_diff': ['std', 'min']}).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae402b5372da26616e6731afd537617e2dd0bd44"},"cell_type":"code","source":"print(f\"Average standard deviation of price change within a day in {grouped['price_diff']['std'].mean():.4f}.\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"852ca1d54311ed626775f7895201de2d9d14efd1"},"cell_type":"markdown","source":"# News data"},{"metadata":{"_uuid":"8dbdbbd3916facea793c16aac6de69fd26a5afc9"},"cell_type":"markdown","source":"Top 5 rows of news train dataframe are given below :"},{"metadata":{"trusted":true,"_uuid":"f9908ff1b6cfc98ed4045b2fc75ed4cf14504ff0"},"cell_type":"code","source":"news_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"edda3e80e0efb078b00c5815ded50318a7a6b65e"},"cell_type":"code","source":"print(f'{news_train_df.shape[0]} samples and {news_train_df.shape[1]} features in the training news dataset.')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1411d5fe53409a53786dd73ead5513e4b3c40723"},"cell_type":"markdown","source":"The file is too huge to work with text directly, so let's see a wordcloud of the last 100000 headlines."},{"metadata":{"trusted":true,"_uuid":"51468ecb83a061848ff530d576f856a8b8c0c274"},"cell_type":"code","source":"text = ' '.join(news_train_df['headline'].str.lower().values[-1000000:])\nwordcloud = WordCloud(max_font_size=None, stopwords=stop, background_color='white',\n                      width=1200, height=1000).generate(text)\nplt.figure(figsize=(12, 8))\nplt.imshow(wordcloud)\nplt.title('Top words in headline')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1896e502caf3b4c3d7d08dfacbb6bcf32d01fea7"},"cell_type":"code","source":"# Let's also limit the time period\nnews_train_df = news_train_df.loc[news_train_df['time'] >= '2010-01-01 22:00:00+0000']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e471aad39a0d5821e23571fbcddcbe42d937f25e"},"cell_type":"code","source":"for i, j in zip([-1, 0, 1], ['negative', 'neutral', 'positive']):\n    df_sentiment = news_train_df.loc[news_train_df['sentimentClass'] == i, 'assetName']\n    print(f'Top mentioned companies for {j} sentiment are:')\n    print(df_sentiment.value_counts().head(5))\n    print('')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"97944c2b8a38d9d488ebb3b3fba22b0fc12a1ba2"},"cell_type":"markdown","source":"I think it is quite funny that Apple is a company with most both negative and positive sentiments."},{"metadata":{"_uuid":"c6475afe0de8d40333e7fdf5b9a0be64373b2d82"},"cell_type":"markdown","source":"# Modelling"},{"metadata":{"_uuid":"417c63256d73d77239b3610a140e923160d2ba81"},"cell_type":"markdown","source":"# Data Preparation"},{"metadata":{"trusted":true,"_uuid":"ae10d6a84619658ee14c3ed8c0ed25717e55f0f5"},"cell_type":"code","source":"from multiprocessing import Pool\n\ndef create_lag(df_code,n_lag=[3,7,14,],shift_size=1):\n    code = df_code['assetCode'].unique()\n    \n    for col in return_features:\n        for window in n_lag:\n            rolled = df_code[col].shift(shift_size).rolling(window=window)\n            lag_mean = rolled.mean()\n            lag_max = rolled.max()\n            lag_min = rolled.min()\n            lag_std = rolled.std()\n            df_code['%s_lag_%s_mean'%(col,window)] = lag_mean\n            df_code['%s_lag_%s_max'%(col,window)] = lag_max\n            df_code['%s_lag_%s_min'%(col,window)] = lag_min\n#             df_code['%s_lag_%s_std'%(col,window)] = lag_std\n    return df_code.fillna(-1)\n\ndef generate_lag_features(df,n_lag = [3,7,14]):\n    features = ['time', 'assetCode', 'assetName', 'volume', 'close', 'open',\n       'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n       'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n       'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n       'returnsClosePrevMktres10', 'returnsOpenPrevMktres10',\n       'returnsOpenNextMktres10', 'universe']\n    \n    assetCodes = df['assetCode'].unique()\n    print(assetCodes)\n    all_df = []\n    df_codes = df.groupby('assetCode')\n    df_codes = [df_code[1][['time','assetCode']+return_features] for df_code in df_codes]\n    print('total %s df'%len(df_codes))\n    \n    pool = Pool(4)\n    all_df = pool.map(create_lag, df_codes)\n    \n    new_df = pd.concat(all_df)  \n    new_df.drop(return_features,axis=1,inplace=True)\n    pool.close()\n    \n    return new_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8bd4c3ddba90f4b23e324eefabb98272689ac0ce"},"cell_type":"code","source":"# return_features = ['close']\n# new_df = generate_lag_features(market_train_df,n_lag = 5)\n# market_train_df = pd.merge(market_train_df,new_df,how='left',on=['time','assetCode'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d80bb484853e216761cf2e7e08c0e9b093c08505"},"cell_type":"code","source":"return_features = ['returnsClosePrevMktres10','returnsClosePrevRaw10','open','close']\nn_lag = [3,7,14]\nnew_df = generate_lag_features(market_train_df,n_lag=n_lag)\nmarket_train_df = pd.merge(market_train_df,new_df,how='left',on=['time','assetCode'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b67955bcf92b534e101be7b8b6aa697869fb7db"},"cell_type":"code","source":"print(market_train_df.columns)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc53bf017273aa8c59a0f2dabf80c70eeb20108b"},"cell_type":"code","source":"# return_features = ['open']\n# new_df = generate_lag_features(market_train_df,n_lag=[3,7,14])\n# market_train_df = pd.merge(market_train_df,new_df,how='left',on=['time','assetCode'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4681d35d83bf94c0b9e95904124902ef2153e3bc"},"cell_type":"code","source":"def mis_impute(data):\n    for i in data.columns:\n        if data[i].dtype == \"object\":\n            data[i] = data[i].fillna(\"other\")\n        elif (data[i].dtype == \"int64\" or data[i].dtype == \"float64\"):\n            data[i] = data[i].fillna(data[i].mean())\n        else:\n            pass\n    return data\n\nmarket_train_df = mis_impute(market_train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b9aa2e280d60a66c006af63f78700861ef90698"},"cell_type":"code","source":"def data_prep(market_train):\n    lbl = {k: v for v, k in enumerate(market_train['assetCode'].unique())}\n    market_train['assetCodeT'] = market_train['assetCode'].map(lbl)\n    market_train = market_train.dropna(axis=0)\n    return market_train\n\nmarket_train_df = data_prep(market_train_df)\n# # check the shape\nprint(market_train_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f947d36b6b6c986fcba9a7ccb4a71fd5e58021d0"},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nup = market_train_df['returnsOpenNextMktres10'] >= 0\n\n\nuniverse = market_train_df['universe'].values\nd = market_train_df['time']\n\nfcol = [c for c in market_train_df if c not in ['assetCode', 'assetCodes', 'assetCodesLen', 'assetName', 'audiences', \n                                             'firstCreated', 'headline', 'headlineTag', 'marketCommentary', 'provider', \n                                             'returnsOpenNextMktres10', 'sourceId', 'subjects', 'time', 'time_x', 'universe','sourceTimestamp']]\n\nX = market_train_df[fcol].values\nup = up.values\nr = market_train_df.returnsOpenNextMktres10.values\n\n# Scaling of X values\n# It is good to keep these scaling values for later\nmins = np.min(X, axis=0)\nmaxs = np.max(X, axis=0)\nrng = maxs - mins\nX = 1 - ((maxs - X) / rng)\n\n# Sanity check\nassert X.shape[0] == up.shape[0] == r.shape[0]\n\nfrom xgboost import XGBClassifier\nfrom sklearn import model_selection\nfrom sklearn.metrics import mean_squared_error\nimport time\n\n# X_train, X_test, up_train, up_test, r_train, r_test,u_train,u_test,d_train,d_test = model_selection.train_test_split(X, up, r,universe,d, test_size=0.25, random_state=99)\n\n\nte = market_train_df['time']>date(2015, 1, 1)\n\ntt = 0\nfor tt,i in enumerate(te.values):\n    if i:\n        idx = tt\n        print(i,tt)\n        break\nprint(idx)\n# for ind_tr, ind_te in tscv.split(X):\n#     print(ind_tr)\nX_train, X_test = X[:idx],X[idx:]\n\nup_train, up_test = up[:idx],up[idx:]\nr_train, r_test = r[:idx],r[idx:]\nu_train,u_test = universe[:idx],universe[idx:]\nd_train,d_test = d[:idx],d[idx:]\n\ntrain_data = lgb.Dataset(X_train, label=up_train.astype(int))\n# train_data = lgb.Dataset(X, label=up.astype(int))\ntest_data = lgb.Dataset(X_test, label=up_test.astype(int))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ac38e2f7a358e6b78f96c9130b5301b764e7db6"},"cell_type":"markdown","source":"# Tuning hyper-params with skopt"},{"metadata":{"trusted":true,"_uuid":"9aabf695dd075ecce1cec3186c03fa903a452e9e"},"cell_type":"code","source":"\n# # use this section if you want to customize optimization\n\n# # define blackbox function\n# def f(x):\n#     print(x)\n#     params = {\n#         'task': 'train',\n#         'boosting_type': 'dart',\n#         'objective': 'binary',\n#         'learning_rate': x[0],\n#         'num_leaves': x[1],\n#         'min_data_in_leaf': x[2],\n#         'num_iteration': x[3],\n#         'max_bin': x[4],\n#         'verbose': 1\n#     }\n    \n#     gbm = lgb.train(params,\n#             train_data,\n#             num_boost_round=100,\n#             valid_sets=test_data,\n#             early_stopping_rounds=5)\n            \n#     print(type(gbm.predict(X_test, num_iteration=gbm.best_iteration)[0]),type(up_test.astype(int)[0]))\n    \n#     print('score: ', mean_squared_error(gbm.predict(X_test, num_iteration=gbm.best_iteration), up_test.astype(float)))\n    \n#     return mean_squared_error(gbm.predict(X_test, num_iteration=gbm.best_iteration), up_test.astype(float))\n\n# # optimize params in these ranges\n# spaces = [\n#     (0.19, 0.20), #learning_rate\n#     (2450, 2600), #num_leaves\n#     (210, 230), #min_data_in_leaf\n#     (310, 330), #num_iteration\n#     (200, 220) #max_bin\n#     ]\n\n# # run optimization\n# from skopt import gp_minimize\n# res = gp_minimize(\n#     f, spaces,\n#     acq_func=\"EI\",\n#     n_calls=20) # increase n_calls for more performance\n# print('TUNED PARAMETERS :')\n# # print tuned params\n# print(res.x)\n\n# # plot tuning process\n# from skopt.plots import plot_convergence\n# plot_convergence(res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0f9657c977e8556be91af02d04b84afd4fb460f"},"cell_type":"code","source":"# these are tuned params I found\nx_1 = [0.19055524469598384, 2453, 229, 329, 200]\nx_2 = [0.19986071577294634, 2590, 230, 310, 202]\nprint(up_train)\ndef exp_loss(p,y):\n    y = y.get_label()\n#     p = p.get_label()\n    grad = -y*(1.0-1.0/(1.0+np.exp(-y*p)))\n    hess = -(np.exp(y*p)*(y*p-1)-1)/((np.exp(y*p)+1)**2)\n    \n    return grad,hess\n\nparams_1 = {\n        'task': 'train',\n        'boosting_type': 'gbdt',\n        'objective': 'binary',\n#         'objective': 'regression',\n        'learning_rate': x_1[0],\n        'num_leaves': x_1[1],\n        'min_data_in_leaf': x_1[2],\n#         'num_iteration': x_1[3],\n        'num_iteration': 239,\n        'max_bin': x_1[4],\n        'verbose': 1\n    }\n\nparams_2 = {\n        'task': 'train',\n        'boosting_type': 'gbdt',\n        'objective': 'binary',\n#         'objective': 'regression',\n        'learning_rate': x_2[0],\n        'num_leaves': x_2[1],\n        'min_data_in_leaf': x_2[2],\n#         'num_iteration': x_2[3],\n        'num_iteration': 172,\n        'max_bin': x_2[4],\n        'verbose': 1\n    }\n\ngbm_1 = lgb.train(params_1,\n        train_data,\n        num_boost_round=100,\n        valid_sets=test_data,\n        early_stopping_rounds=5,\n#         fobj=exp_loss,\n        )\n\ngbm_2 = lgb.train(params_2,\n        train_data,\n        num_boost_round=100,\n        valid_sets=test_data,\n        early_stopping_rounds=5,\n#         fobj=exp_loss,\n        )\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"57f90489c498f068d2dc0a0bd7e8e8c138899ae0"},"cell_type":"code","source":"confidence_test = (gbm_1.predict(X_test) + gbm_2.predict(X_test))/2\nconfidence_test = (confidence_test-confidence_test.min())/(confidence_test.max()-confidence_test.min())\nconfidence_test = confidence_test*2-1\nprint(max(confidence_test),min(confidence_test))\n\n# calculation of actual metric that is used to calculate final score\nr_test = r_test.clip(-1,1) # get rid of outliers. Where do they come from??\nx_t_i = confidence_test * r_test * u_test\ndata = {'day' : d_test, 'x_t_i' : x_t_i}\ndf = pd.DataFrame(data)\nx_t = df.groupby('day').sum().values.flatten()\nmean = np.mean(x_t)\nstd = np.std(x_t)\nscore_test = mean / std+0.2\nprint(score_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4cd0e0e8c5724f660e3095c6e2a3e8459afc7d6c"},"cell_type":"code","source":"\nimport gc\ndel X_train,X_test\ngc.collect()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}