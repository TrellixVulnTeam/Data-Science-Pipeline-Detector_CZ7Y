{"cells":[{"metadata":{"_uuid":"50bc29d151fa6192be326cbc343ce0b6e40405b4"},"cell_type":"markdown","source":"# Import packages"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nimport os\nprint(os.listdir(\"../input\"))\n\nfrom kaggle.competitions import twosigmanews\nimport matplotlib.pyplot as plt\nimport random\nfrom datetime import datetime, date\nfrom xgboost import XGBClassifier\nfrom sklearn import model_selection\nfrom sklearn.metrics import mean_squared_error\nimport time","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# Load Data"},{"metadata":{"trusted":true,"_uuid":"2a60d768ec157469fe0512b86356dace3c41233a"},"cell_type":"code","source":"if 'env' not in globals():\n    env = twosigmanews.make_env()\n    (market_train_df, news_train_df) = env.get_training_data()\n    market_train_df['time'] = market_train_df['time'].dt.date\n    market_train_df = market_train_df.loc[market_train_df['time']>=date(2010, 1, 1)]\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"51d904e9fd57dc968c9808e0f86c43d553afc5ed"},"cell_type":"markdown","source":"# Data prep"},{"metadata":{"trusted":true,"_uuid":"886507a9c0384f0919b922b5ff8e2764c9575e0c"},"cell_type":"code","source":"market_train, news_train = market_train_df.copy(), news_train_df.copy()\nfrom multiprocessing import Pool\n\ndef create_lag(df_code,n_lag=[3,7,14,],shift_size=1):\n    code = df_code['assetCode'].unique()\n    \n    for col in return_features:\n        for window in n_lag:\n            rolled = df_code[col].shift(shift_size).rolling(window=window)\n            lag_mean = rolled.mean()\n            lag_max = rolled.max()\n            lag_min = rolled.min()\n            lag_std = rolled.std()\n            df_code['%s_lag_%s_mean'%(col,window)] = lag_mean\n            df_code['%s_lag_%s_max'%(col,window)] = lag_max\n            df_code['%s_lag_%s_min'%(col,window)] = lag_min\n#             df_code['%s_lag_%s_std'%(col,window)] = lag_std\n    return df_code.fillna(-1)\n\ndef generate_lag_features(df,n_lag = [3,7,14]):\n    features = ['time', 'assetCode', 'assetName', 'volume', 'close', 'open',\n       'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n       'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n       'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n       'returnsClosePrevMktres10', 'returnsOpenPrevMktres10',\n       'returnsOpenNextMktres10', 'universe']\n    \n    assetCodes = df['assetCode'].unique()\n    print(assetCodes)\n    all_df = []\n    df_codes = df.groupby('assetCode')\n    df_codes = [df_code[1][['time','assetCode']+return_features] for df_code in df_codes]\n    print('total %s df'%len(df_codes))\n    \n    pool = Pool(4)\n    all_df = pool.map(create_lag, df_codes)\n    \n    new_df = pd.concat(all_df)  \n    new_df.drop(return_features,axis=1,inplace=True)\n    pool.close()\n    \n    return new_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e4a0431a2a0da29f343f7d48aec3451530371e0"},"cell_type":"code","source":"return_features = ['returnsClosePrevMktres10','returnsClosePrevRaw10','open','close']\nn_lag = [3,7,14]\nnew_df = generate_lag_features(market_train_df,n_lag=n_lag)\nmarket_train_df = pd.merge(market_train_df,new_df,how='left',on=['time','assetCode'])\nprint(market_train_df.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12b5fbbdd5e998d47a500284c28ab1c644699596"},"cell_type":"code","source":"def mis_impute(data):\n    for i in data.columns:\n        if data[i].dtype == \"object\":\n            data[i] = data[i].fillna(\"other\")\n        elif (data[i].dtype == \"int64\" or data[i].dtype == \"float64\"):\n            data[i] = data[i].fillna(data[i].mean())\n        else:\n            pass\n    return data\n\nmarket_train_df = mis_impute(market_train_df)\n\ndef data_prep(market_train):\n#     market_train.time = market_train.time.dt.date\n    lbl = {k: v for v, k in enumerate(market_train['assetCode'].unique())}\n    market_train['assetCodeT'] = market_train['assetCode'].map(lbl)\n    \n    market_train = market_train.dropna(axis=0)\n    \n    return market_train\n\nmarket_train = data_prep(market_train_df)\n\n# check the shape\nprint(market_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1420d038dd13f704d06e7c78835cc2ac7af82b5"},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nup = market_train.returnsOpenNextMktres10 >= 0\nuniverse = market_train_df['universe'].values\nd = market_train_df['time']\nfcol = [c for c in market_train if c not in ['assetCode', 'assetCodes', 'assetCodesLen', 'assetName', 'audiences', \n                                             'firstCreated', 'headline', 'headlineTag', 'marketCommentary', 'provider', \n                                             'returnsOpenNextMktres10', 'sourceId', 'subjects', 'time', 'time_x', 'universe','sourceTimestamp']]\n\nX = market_train[fcol].values\nup = up.values\nr = market_train.returnsOpenNextMktres10.values\n\n# Scaling of X values\n# It is good to keep these scaling values for later\nmins = np.min(X, axis=0)\nmaxs = np.max(X, axis=0)\nrng = maxs - mins\nX = 1 - ((maxs - X) / rng)\n\n# Sanity check\nassert X.shape[0] == up.shape[0] == r.shape[0]\n\nfrom xgboost import XGBClassifier\nfrom sklearn import model_selection\nfrom sklearn.metrics import mean_squared_error\nimport time\n\n# X_train, X_test, up_train, up_test, r_train, r_test = model_selection.train_test_split(X, up, r, test_size=0.25, random_state=99)\nX_train, X_test, up_train, up_test, r_train, r_test,u_train,u_test,d_train,d_test = model_selection.train_test_split(X, up, r,universe,d, test_size=0.25, random_state=99)\n# test data\n# te = market_train_df['time']>date(2015, 1, 1)\n\n# tt = 0\n# for tt,i in enumerate(te.values):\n#     if i:\n#         idx = tt\n#         print(i,tt)\n#         break\n# print(idx)\n# # for ind_tr, ind_te in tscv.split(X):\n# #     print(ind_tr)\n# X_train, X_test = X[:idx],X[idx:]\n\n# up_train, up_test = up[:idx],up[idx:]\n# r_train, r_test = r[:idx],r[idx:]\n# u_train,u_test = universe[:idx],universe[idx:]\n# d_train,d_test = d[:idx],d[idx:]\n\n#test data\ntrain_data = lgb.Dataset(X_train, label=up_train.astype(int))\ntest_data = lgb.Dataset(X_test, label=up_test.astype(int))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d49a70d9691361dcd2e06513fe44e83604f03ec2"},"cell_type":"markdown","source":"# Tuning hyper-params with skopt"},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"647493abe54d31de0e8662a46c51ff6f800d11e5"},"cell_type":"code","source":"\n# # use this section if you want to customize optimization\n\n# # define blackbox function\n# def f(x):\n#     print(x)\n#     params = {\n#         'task': 'train',\n#         'boosting_type': 'dart',\n#         'objective': 'binary',\n#         'learning_rate': x[0],\n#         'num_leaves': x[1],\n#         'min_data_in_leaf': x[2],\n#         'num_iteration': x[3],\n#         'max_bin': x[4],\n#         'verbose': 1\n#     }\n    \n#     gbm = lgb.train(params,\n#             train_data,\n#             num_boost_round=100,\n#             valid_sets=test_data,\n#             early_stopping_rounds=5)\n            \n#     print(type(gbm.predict(X_test, num_iteration=gbm.best_iteration)[0]),type(up_test.astype(int)[0]))\n    \n#     print('score: ', mean_squared_error(gbm.predict(X_test, num_iteration=gbm.best_iteration), up_test.astype(float)))\n    \n#     return mean_squared_error(gbm.predict(X_test, num_iteration=gbm.best_iteration), up_test.astype(float))\n\n# # optimize params in these ranges\n# spaces = [\n#     (0.19, 0.20), #learning_rate\n#     (2450, 2600), #num_leaves\n#     (210, 230), #min_data_in_leaf\n#     (310, 330), #num_iteration\n#     (200, 220) #max_bin\n#     ]\n\n# # run optimization\n# from skopt import gp_minimize\n# res = gp_minimize(\n#     f, spaces,\n#     acq_func=\"EI\",\n#     n_calls=20) # increase n_calls for more performance\n# print('TUNED PARAMETERS :')\n# # print tuned params\n# print(res.x)\n\n# # plot tuning process\n# from skopt.plots import plot_convergence\n# plot_convergence(res)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"45c99e6b4fb86f54d020723d13031a923c8d3808"},"cell_type":"markdown","source":"# Training with tuned params"},{"metadata":{"trusted":true,"_uuid":"b6ac689319157951828d4e4a713ae688ea5e618c"},"cell_type":"code","source":"# these are tuned params I found\nx_1 = [0.19055524469598384, 2453, 229, 329, 200]\nx_2 = [0.19986071577294634, 2590, 230, 310, 202]\n\nparams_1 = {\n        'task': 'train',\n        'boosting_type': 'dart',\n        'objective': 'binary',\n        'learning_rate': x_1[0],\n        'num_leaves': x_1[1],\n        'min_data_in_leaf': x_1[2],\n        'num_iteration': x_1[3],\n        'max_bin': x_1[4],\n        'verbose': 1\n    }\n\nparams_2 = {\n        'task': 'train',\n        'boosting_type': 'dart',\n        'objective': 'binary',\n        'learning_rate': x_2[0],\n        'num_leaves': x_2[1],\n        'min_data_in_leaf': x_2[2],\n        'num_iteration': x_2[3],\n        'max_bin': x_2[4],\n        'verbose': 1\n    }\n\n\ngbm_1 = lgb.train(params_1,\n        train_data,\n        num_boost_round=100,\n        valid_sets=test_data,\n        early_stopping_rounds=5)\n        \ngbm_2 = lgb.train(params_2,\n        train_data,\n        num_boost_round=100,\n        valid_sets=test_data,\n        early_stopping_rounds=5)\n        \n\n# #prediction\n# days = env.get_prediction_days()\n# n_days = 0\n# prep_time = 0\n# prediction_time = 0\n# packaging_time = 0\n# for (market_obs_df, news_obs_df, predictions_template_df) in days:\n#     n_days +=1\n#     if (n_days%50==0):\n#         print(n_days,end=' ')\n#     t = time.time()\n#     market_obs_df = data_prep(market_obs_df)\n#     market_obs_df = market_obs_df[market_obs_df.assetCode.isin(predictions_template_df.assetCode)]\n#     X_live = market_obs_df[fcol].values\n#     X_live = 1 - ((maxs - X_live) / rng)\n#     prep_time += time.time() - t\n    \n#     t = time.time()\n#     lp = (gbm_1.predict(X_live) + gbm_2.predict(X_live))/2\n#     t = time.time()\n#     confidence = lp\n#     confidence = (confidence-confidence.min())/(confidence.max()-confidence.min())\n#     confidence = confidence * 2 - 1\n#     preds = pd.DataFrame({'assetCode':market_obs_df['assetCode'],'confidence':confidence})\n#     predictions_template_df = predictions_template_df.merge(preds,how='left').drop('confidenceValue',axis=1).fillna(0).rename(columns={'confidence':'confidenceValue'})\n#     env.predict(predictions_template_df)\n#     packaging_time += time.time() - t\n    \n# env.write_submission_file()\n# sub  = pd.read_csv(\"submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86d18f76e664ea3042bc02a94efd5b140ad05e01"},"cell_type":"code","source":"confidence_test = (gbm_1.predict(X_test) + gbm_2.predict(X_test))/2\nconfidence_test = (confidence_test-confidence_test.min())/(confidence_test.max()-confidence_test.min())\nconfidence_test = confidence_test*2-1\nprint(max(confidence_test),min(confidence_test))\n\n# calculation of actual metric that is used to calculate final score\nr_test = r_test.clip(-1,1) # get rid of outliers. Where do they come from??\nx_t_i = confidence_test * r_test * u_test\ndata = {'day' : d_test, 'x_t_i' : x_t_i}\ndf = pd.DataFrame(data)\nx_t = df.groupby('day').sum().values.flatten()\nmean = np.mean(x_t)\nstd = np.std(x_t)\nscore_test = mean / std\nprint(score_test)\nimport gc\ndel X_train,X_test\ngc.collect()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}