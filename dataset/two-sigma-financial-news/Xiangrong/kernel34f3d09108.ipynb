{"cells":[{"metadata":{"_uuid":"7d4c6a082b77b8a773c326dacf55e26d6d047407"},"cell_type":"markdown","source":" # Load Data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from kaggle.competitions import twosigmanews\n\n# get training data\nenv = twosigmanews.make_env()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba2e1000c23c4e5451953f32833110cfd2dd39b5"},"cell_type":"code","source":"(market_train_df, news_train_df) = env.get_training_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c978e78dbe7d6a899727b2426426c8c9281c51e0"},"cell_type":"code","source":"market_train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ff2dfe92ce19a088d33e0df262a83b21dd6a0131"},"cell_type":"markdown","source":"**Market Data**\n\n> Each asset is identified by an assetCode (note that a single company may have multiple assetCodes). Depending on what you wish to do, you may use the assetCode, assetName, or time as a way to join the market data to news data.\n\nThe marketdata contains a variety of returns calculated over different timespans. All of the returns in this set of marketdata have these properties:\n* Returns are always calculated either open-to-open (from the opening time of one trading day to the open of another) or close-to-close (from the closing time of one trading day to the open of another).\n* Returns are either raw, meaning that the data is not adjusted against any benchmark, or market-residualized (Mktres), meaning that the movement of the market as a whole has been accounted for, leaving only movements inherent to the instrument.\n* Returns can be calculated over any arbitrary interval. Provided here are 1 day and 10 day horizons.\n* Returns are tagged with 'Prev' if they are backwards looking in time, or 'Next' if forwards looking.\nWithin the marketdata, you will find the following columns:\n\n**Data Description**\n* time(datetime64[ns, UTC]) - the current time (in marketdata, all rows are taken at 22:00 UTC)\n* assetCode(object) - a unique id of an asset\n* assetName(category) - the name that corresponds to a group of assetCodes. These may be \"Unknown\" if the corresponding assetCode does not have any rows in the news data.\n* universe(float64) - a boolean indicating whether or not the instrument on that day will be included in scoring. This value is not provided outside of the training data time period. The trading universe on a given date is the set of instruments that are avilable for trading (the scoring function will not consider instruments that are not in the trading universe). The trading universe changes daily.\n* volume(float64) - trading volume in shares for the day\n* close(float64) - the close price for the day (not adjusted for splits or dividends)\n* open(float64) - the open price for the day (not adjusted for splits or dividends)\n* returnsClosePrevRaw1(float64) - see returns explanation above\n* returnsOpenPrevRaw1(float64) - see returns explanation above\n* returnsClosePrevMktres1(float64) - see returns explanation above\n* returnsOpenPrevMktres1(float64) - see returns explanation above\n* returnsClosePrevRaw10(float64) - see returns explanation above\n* returnsOpenPrevRaw10(float64) - see returns explanation above\n* returnsClosePrevMktres10(float64) - see returns explanation above\n* returnsOpenPrevMktres10(float64) - see returns explanation above\n* returnsOpenNextMktres10(float64) - 10 day, market-residualized return. This is the target variable used in competition scoring. The market data has been filtered such that returnsOpenNextMktres10 is always not null."},{"metadata":{"trusted":true,"_uuid":"9fde95f3c79b791f8e058c4e0d132ed0f0d17364","_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"market_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2d039e0f269bb91e5e927bd5e149a6f40c4715c"},"cell_type":"code","source":"market_train_df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dee98164bd61b69dbeb4d39b3770322e33eb8883"},"cell_type":"code","source":"news_train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f725bb254b2a20e85cb1f0fb588cd89ee50c634"},"cell_type":"markdown","source":"**New data**\n> The news data contains information at both the news article level and asset level (in other words, the table is intentionally not normalized).\n\n**Data Description**\n* time(datetime64[ns, UTC]) - UTC timestamp showing when the data was available on the feed (second precision)\n* sourceTimestamp(datetime64[ns, UTC]) - UTC timestamp of this news item when it was created\n* firstCreated(datetime64[ns, UTC]) - UTC timestamp for the first version of the item\n* sourceId(object) - an Id for each news item\n* headline(object) - the item's headline\n* urgency(int8) - differentiates story types (1: alert, 3: article)\n* takeSequence(int16) - the take sequence number of the news item, starting at 1. For a given story, alerts and articles have separate sequences.\n* provider(category) - identifier for the organization which provided the news item (e.g. RTRS for Reuters News, BSW for Business Wire)\n* subjects(category) - topic codes and company identifiers that relate to this news item. Topic codes describe the news item's subject matter. These can cover asset classes, geographies, events, industries/sectors, and other types.\n* audiences(category) - identifies which desktop news product(s) the news item belongs to. They are typically tailored to specific audiences. (e.g. \"M\" for Money International News Service and \"FB\" for French General News Service)\n* bodySize(int32) - the size of the current version of the story body in characters\n* companyCount(int8) - the number of companies explicitly listed in the news item in the subjects field\n* headlineTag(object) - the Thomson Reuters headline tag for the news item\n* marketCommentary(bool) - boolean indicator that the item is discussing general market conditions, such as \"After the Bell\" summaries\n* sentenceCount(int16) - the total number of sentences in the news item. Can be used in conjunction with firstMentionSentence to determine the relative position of the first mention in the item.\n* wordCount(int32) - the total number of lexical tokens (words and punctuation) in the news item\n* assetCodes(category) - list of assets mentioned in the item\n* assetName(category) - name of the asset\n* firstMentionSentence(int16) - the first sentence, starting with the headline, in which the scored asset is mentioned.\n* 1: headline\n* 2: first sentence of the story body\n* 3: second sentence of the body, etc\n* 0: the asset being scored was not found in the news item's headline or body text. As a result, the entire news item's text (headline + body) will be used to determine the sentiment score.\n* relevance(float32) - a decimal number indicating the relevance of the news item to the asset. It ranges from 0 to 1. If the asset is mentioned in the headline, the relevance is set to 1. When the item is an alert (urgency == 1), relevance should be gauged by firstMentionSentence instead.\n* sentimentClass(int8) - indicates the predominant sentiment class for this news item with respect to the asset. The indicated class is the one with the highest probability.\n* sentimentNegative(float32) - probability that the sentiment of the news item was negative for the asset\n* sentimentNeutral(float32) - probability that the sentiment of the news item was neutral for the asset\n* sentimentPositive(float32) - probability that the sentiment of the news item was positive for the asset\nsentimentWordCount(int32) - the number of lexical tokens in the sections of the item text that are deemed relevant to the asset. This can be used in conjunction with wordCount to determine the proportion of the news item discussing the asset.\n* noveltyCount12H(int16) - The 12 hour novelty of the content within a news item on a particular asset. It is calculated by comparing it with the asset-specific text over a cache of previous news items that contain the asset.\n* noveltyCount24H(int16) - same as above, but for 24 hours\n* noveltyCount3D(int16) - same as above, but for 3 days\n* noveltyCount5D(int16) - same as above, but for 5 days\n* noveltyCount7D(int16) - same as above, but for 7 days\n* volumeCounts12H(int16) - the 12 hour volume of news for each asset. A cache of previous news items is maintained and the number of news items that mention the asset within each of five historical periods is calculated.\n* volumeCounts24H(int16) - same as above, but for 24 hours\n* volumeCounts3D(int16) - same as above, but for 3 days\n* volumeCounts5D(int16) - same as above, but for 5 days\n* volumeCounts7D(int16) - same as above, but for 7 days"},{"metadata":{"trusted":true,"_uuid":"c53455c7d28376c13ccc8084dcaf5d7e492986dc","_kg_hide-input":true,"_kg_hide-output":true,"scrolled":true},"cell_type":"code","source":"news_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bed63b9663bd43ab5fd70121faa72c08d6f25d80"},"cell_type":"code","source":"news_train_df.tail()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"72843c9c4a0aed23a7c4bbb8c8158f1ba3a6718b"},"cell_type":"markdown","source":" # EDA"},{"metadata":{"_uuid":"157c346adab83eea5cc7b6abc95067b9816586cb"},"cell_type":"markdown","source":" **Data Overview**\n>\n我们本次使用了两个真实数据集：\n* 由Intrinio提供的**市场数据**（2007年至今）包含金融市场信息，如开盘价，收盘价，交易量，计算回报等。\n* 由汤森路透提供的**新闻数据**（2007年至今）包含有关资产的新闻文章/警报信息，如文章详情，情绪和其他评论。\n\n在做正式的分析之前，我们可以在数据中发现一些**有趣的事实**。"},{"metadata":{"trusted":true,"_uuid":"82ad85e31ec2bdbe9dbdf9b380f7b794f1487d2c","_kg_hide-input":true,"scrolled":true},"cell_type":"code","source":"import plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n%matplotlib inline\n\ndata = []\nfor asset in np.random.choice(market_train_df['assetName'].unique(), 10):\n    asset_df = market_train_df[(market_train_df['assetName'] == asset)]\n\n    data.append(go.Scatter(\n        x = asset_df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n        y = asset_df['open'].values,\n        name = asset\n    ))\nlayout = go.Layout(dict(title = \"Closing prices of 10 random assets\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"))\npy.iplot(dict(data=data, layout=layout), filename='basic-line')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a241e95bc0a47f83616752c1e6f68302bd4a6e7"},"cell_type":"markdown","source":"上图中，我们对资产进行随机抽样，得到了十家不同的公司并画出他们的股票价格，有的线突然出现可能代表的一家公司刚刚成立，而线的消失可能表示了这家公司倒闭或是被收购了。\n\n我们将数据通过对分位数进行分组来得到**市场的总体趋势**。"},{"metadata":{"trusted":true,"_uuid":"40c05696e912746a5b44dbb91cabf8579ad29cb6","scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"data = []\nfor i in [0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95]:\n    price_df = market_train_df.groupby('time')['close'].quantile(i).reset_index()\n\n    data.append(go.Scatter(\n        x = price_df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n        y = price_df['close'].values,\n        name = f'{i} quantile'\n    ))\nlayout = go.Layout(dict(title = \"Trends of closing prices by quantiles\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"),\n    annotations=[\n        dict(\n            x='2008-09-01 22:00:00+0000',\n            y=82,\n            xref='x',\n            yref='y',\n            text='Collapse of Lehman Brothers',\n            showarrow=True,\n            font=dict(\n                family='Courier New, monospace',\n                size=16,\n                color='#ffffff'\n            ),\n            align='center',\n            arrowhead=2,\n            arrowsize=1,\n            arrowwidth=2,\n            arrowcolor='#636363',\n            ax=20,\n            ay=-30,\n            bordercolor='#c7c7c7',\n            borderwidth=2,\n            borderpad=4,\n            bgcolor='#ff7f0e',\n            opacity=0.8\n        ),\n        dict(\n            x='2011-08-01 22:00:00+0000',\n            y=85,\n            xref='x',\n            yref='y',\n            text='Black Monday',\n            showarrow=True,\n            font=dict(\n                family='Courier New, monospace',\n                size=16,\n                color='#ffffff'\n            ),\n            align='center',\n            arrowhead=2,\n            arrowsize=1,\n            arrowwidth=2,\n            arrowcolor='#636363',\n            ax=20,\n            ay=-30,\n            bordercolor='#c7c7c7',\n            borderwidth=2,\n            borderpad=4,\n            bgcolor='#ff7f0e',\n            opacity=0.8\n        ),\n        dict(\n            x='2014-10-01 22:00:00+0000',\n            y=120,\n            xref='x',\n            yref='y',\n            text='Another crisis',\n            showarrow=True,\n            font=dict(\n                family='Courier New, monospace',\n                size=16,\n                color='#ffffff'\n            ),\n            align='center',\n            arrowhead=2,\n            arrowsize=1,\n            arrowwidth=2,\n            arrowcolor='#636363',\n            ax=-20,\n            ay=-30,\n            bordercolor='#c7c7c7',\n            borderwidth=2,\n            borderpad=4,\n            bgcolor='#ff7f0e',\n            opacity=0.8\n        ),\n        dict(\n            x='2016-01-01 22:00:00+0000',\n            y=120,\n            xref='x',\n            yref='y',\n            text='Oil prices crash',\n            showarrow=True,\n            font=dict(\n                family='Courier New, monospace',\n                size=16,\n                color='#ffffff'\n            ),\n            align='center',\n            arrowhead=2,\n            arrowsize=1,\n            arrowwidth=2,\n            arrowcolor='#636363',\n            ax=20,\n            ay=-30,\n            bordercolor='#c7c7c7',\n            borderwidth=2,\n            borderpad=4,\n            bgcolor='#ff7f0e',\n            opacity=0.8\n        )\n    ])\npy.iplot(dict(data=data, layout=layout), filename='basic-line')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fbf0d016bba8f4e8432a7390302c39491ed1190c"},"cell_type":"markdown","source":" 从中我们可以看到市场在经历某些危机后股价严重下跌又再次上涨。还可以注意到，较高的分位数价格随着时间的推移而增加，较低的分位数价格也会下降。 也许穷人和富人之间的差距会增加......另一方面，也许更多的“小”公司准备进入市场，他们的股票价格也不是很高。"},{"metadata":{"trusted":true,"_uuid":"9839e1cb8ff1e9925fae750b8bf4cacfa1b90f5f"},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_random_asset(market):\n    \"\"\"\n    Get random asset, show price, volatility and volume\n    \"\"\"\n    # Get any asset\n    ass = market_train_df.assetCode.sample(1, random_state=24).iloc[0]\n    ass_market = market_train_df[market_train_df['assetCode'] == ass]\n    ass_market.index = ass_market.time\n\n    # Plotting\n    f, axs = plt.subplots(3,1, sharex=True, figsize=(12,8))\n    # Close price \n    ass_market.close.plot(ax=axs[0])\n    axs[0].set_ylabel(\"Price\")\n\n    # Volatility (close-open)\n    volat_df = (ass_market.close - ass_market.open)\n    (ass_market.close - ass_market.open).plot(color='green', ax = axs[1])\n    axs[1].set_ylabel(\"Volatility\")\n\n    # Volume\n    ass_market.volume.plot(ax=axs[2], color='darkred')\n    axs[2].set_ylabel(\"Volume\")\n\n    # Show the plot\n    f.suptitle(\"Asset: %s\" % ass, fontsize=22)\n    plt.tight_layout()\n    plt.subplots_adjust(top=0.93)\n    plt.show()\n\nplot_random_asset(market_train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"34b7e8a184b615b0160d51e67eca818340b5bbbb"},"cell_type":"code","source":"market_train_df['price_diff'] = market_train_df['close'] - market_train_df['open']\ngrouped = market_train_df.groupby('time').agg({'price_diff': ['std', 'min']}).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3e2f544c4085e3f892b7043e3b3c77009f666846"},"cell_type":"code","source":"g = grouped.sort_values(('price_diff', 'std'), ascending=False)[:10]\ng['min_text'] = 'Maximum price drop: ' + (-1 * g['price_diff']['min']).astype(str)\ntrace = go.Scatter(\n    x = g['time'].dt.strftime(date_format='%Y-%m-%d').values,\n    y = g['price_diff']['std'].values,\n    mode='markers',\n    marker=dict(\n        size = g['price_diff']['std'].values,\n        color = g['price_diff']['std'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = g['min_text'].values\n    #text = f\"Maximum price drop: {g['price_diff']['min'].values}\"\n    #g['time'].dt.strftime(date_format='%Y-%m-%d').values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Top 10 months by standard deviation of price change within a day',\n    hovermode= 'closest',\n    yaxis=dict(\n        title= 'price_diff',\n        ticklen= 5,\n        gridwidth= 2,\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e44b5e4ad02923dc7ee63d57474f690560199f7"},"cell_type":"code","source":"market_train_df.sort_values('price_diff')[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa2203bb55ee66bd9736e64fa9a4e255a1f81204"},"cell_type":"markdown","source":"从上图和上表的price_diff中我们可以看到2010年的时候出现了巨大的价格波动（但事实并没有这样）…\n\n可见我们的数据中应该存在着较大的误差，所以我们接下来进行数据的预处理。"},{"metadata":{"_uuid":"b8a9ba64c9e99957eb61f6d061337f932f8a61e9"},"cell_type":"markdown","source":"# **Possible data errors**"},{"metadata":{"_uuid":"fff68d2f73c62710dbc8f41e658cda97883456b1"},"cell_type":"markdown","source":"**market data**\n\nmarket data部分的数据预处理。"},{"metadata":{"trusted":true,"_uuid":"53032a1eefde2ab1f7dc49aa3554602217b6d8e5"},"cell_type":"code","source":"# sorted by time\nmarket_train_orig = market_train_df.sort_values('time')\nnews_train_orig = news_train_df.sort_values('time')\nmarket_train_df = market_train_orig.copy()\nnews_train_df = news_train_orig.copy()\ndel market_train_orig\ndel news_train_orig","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c2d054b3101d3255c5e9676d64d83e9c347bcf6"},"cell_type":"markdown","source":"统计数据显示，大多数数据在2009年后表现均匀（成交量增加，价格上涨等）。然而，在2009年之前，由于2008年金融危机导致房地产泡沫破灭，数据表现不同。我们选择了2009年之后的数据。"},{"metadata":{"trusted":true,"_uuid":"72680a5355be711218d6417ec56a72cdfe069e77"},"cell_type":"code","source":"import datetime\n\nmarket_train_df = market_train_df.loc[market_train_df['time'].dt.date>=datetime.date(2009,1,1)]\nnews_train_df = news_train_df.loc[news_train_df['time'].dt.date>=datetime.date(2009,1,1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2aebbcb20f598fd2c07ab6acdb2fbe1628d3e62d"},"cell_type":"markdown","source":"**缺失值处理**\n\n对数据进行分析的时候要注意其中是否有缺失值。\n\n一些机器学习算法能够处理缺失值，比如神经网络，一些则不能。对于缺失值，一般有以下几种处理方法：\n\n（1）如果数据集很多，但有很少的缺失值，可以删掉带缺失值的行；\n\n（2）如果该属性相对学习来说不是很重要，可以对缺失值赋均值或者众数.\n\n（3）对于标称属性，可以赋一个代表缺失的值，比如‘U0’。因为缺失本身也可能代表着一些隐含信息。比如船舱号Cabin这一属性，缺失可能代表并没有船舱。\n\n（4）使用回归 随机森林等模型来预测缺失属性的值。因为Age在该数据集里是一个相当重要的特征（先对Age进行分析即可得知），所以保证一定的缺失值填充准确率是非常重要的，对结果也会产生较大影响。一般情况下，会使用数据完整的条目作为模型的训练集，以此来预测缺失值。对于当前的这个数据，可以使用随机森林来预测也可以使用线性回归预测。这里使用随机森林预测模型，选取数据集中的数值属性作为特征（因为sklearn的模型只能处理数值属性，所以这里先仅选取数值特征，但在实际的应用中需要将非数值特征转换为数值特征）\n"},{"metadata":{"trusted":true,"_uuid":"9d963e2fb6cd1fa71c5d8c1037dbbec61527ecf4"},"cell_type":"code","source":"print('Check null data:')\nmarket_train_df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a2eacd336bf8f0b3d1956968427861e5feef693"},"cell_type":"markdown","source":"检查到数据中returnsClosePrevMktres1和returnsOpenPrevMktres1有15980个空值，returnsClosePrevMktres10和returnsOpenPrevMktres10分别由93010和93054个空置。"},{"metadata":{"trusted":true,"_uuid":"a492b924c81be89ba7931f5347b864749b160d76"},"cell_type":"code","source":"column_market = ['returnsClosePrevMktres1','returnsOpenPrevMktres1','returnsClosePrevMktres10', 'returnsOpenPrevMktres10']\ncolumn_raw = ['returnsClosePrevRaw1', 'returnsOpenPrevRaw1','returnsClosePrevRaw10', 'returnsOpenPrevRaw10']\n#for i in range(len(column_raw)):\n#    market_train_df[column_market[i]] = market_train_df[column_market[i]].fillna(market_train_df[column_raw[i]])\n# 这个地方raw应该是不能直接移到mktres中的 之后有空考虑用随机森林填数据","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b4932e4712a93f105a93813190c888bff57488e"},"cell_type":"markdown","source":"**异常值处理**\n\n（1）删去单日价格增幅/降幅超过50%的数据。我们认为这类数据要么是数据错误，要么是极端情况。由于我们本身数据集就比较大，而此类异常数量较少，就选择直接删除了。\n\n（2）删去奇怪的数据？\n\n（3）删除极端值？\n"},{"metadata":{"trusted":true,"_uuid":"c5e673d1f1befe5d88c90595e5356e4c370228f0"},"cell_type":"code","source":"market_train_df['close_open_ratio'] = np.abs(market_train_df['close']/market_train_df['open'])\nthreshold = 0.5\nprint('In %i lines price increases by 50%% or more in a day' %(market_train_df['close_open_ratio']>=1+threshold).sum())\nprint('In %i lines price decreases by 50%% or more in a day' %(market_train_df['close_open_ratio']<=1-+threshold).sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b121be864794ac24d24f7f7cf7e771b4c51297be"},"cell_type":"code","source":"market_train_df = market_train_df.loc[market_train_df['close_open_ratio'] < 1.5]\nmarket_train_df = market_train_df.loc[market_train_df['close_open_ratio'] > 0.5]\nmarket_train_df = market_train_df.drop(columns=['close_open_ratio'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d52da1bfc565f7aaf91022546459a056bd1eacf"},"cell_type":"code","source":"print('Removing strange data ...')\norig_len = market_train_df.shape[0]\nmarket_train_df = market_train_df[~market_train_df['assetCode'].isin(['PGN.N','EBRYY.OB'])]\n#market_train_df = market_train_df[~market_train_df['assetName'].isin(['Unknown'])]\nnew_len = market_train_df.shape[0]\nrmv_len = np.abs(orig_len-new_len)\nprint('There were %i lines removed' %rmv_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e67b30b4bc89733f43788ebdc0574ece68fbc8d3"},"cell_type":"code","source":"print('Removing outliers ...')\ncolumn_return = column_market + column_raw + ['returnsOpenNextMktres10']\norig_len = market_train_df.shape[0]\nfor column in column_return:\n    market_train_df = market_train_df.loc[market_train_df[column]>=-2]\n    market_train_df = market_train_df.loc[market_train_df[column]<=2]\nnew_len = market_train_df.shape[0]\nrmv_len = np.abs(orig_len-new_len)\nprint('There were %i lines removed' %rmv_len)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1534c436a7c54d99a08a4fbe1705149ed64b186e"},"cell_type":"markdown","source":"**news data**\n\nnews data部分的数据预处理。\n\nnews data部分没有空值，所以我们仅删去了较为极端的数据。"},{"metadata":{"trusted":true,"_uuid":"564e16698d76d9a18b9525238641e9cb10ee5b78"},"cell_type":"code","source":"print('Check null data:')\nnews_train_df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b8d040f0a70a2fe67b1bb12c28cb24d3f189b090"},"cell_type":"code","source":"from wordcloud import WordCloud \nimport matplotlib.pyplot as plt\n\ntext = ' '.join(news_train_df['headline'].str.lower().values[-1000000:])\nwordcloud = WordCloud(max_font_size=None,stopwords=None, background_color='white',\n                      width=1200, height=1000).generate(text)\nplt.figure(figsize=(12, 8))\nplt.imshow(wordcloud)\nplt.title('Top words in headline')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4dc63136d796460010ddf6fde5d40d9dd05b172c"},"cell_type":"code","source":"def remove_outliers(data_frame, column_list, low=0.02, high=0.98):\n    for column in column_list:\n        this_column = data_frame[column]\n        quant_df = this_column.quantile([low,high])\n        low_limit = quant_df[low]\n        high_limit = quant_df[high]\n        data_frame[column] = data_frame[column].clip(lower=low_limit, upper=high_limit)\n    return data_frame","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0321cddb3187aff6515287e0dd113708f94fe6e6"},"cell_type":"code","source":"columns_outlier = ['takeSequence', 'bodySize', 'sentenceCount', 'wordCount', 'sentimentWordCount', 'firstMentionSentence','noveltyCount12H',\\\n                  'noveltyCount24H', 'noveltyCount3D', 'noveltyCount5D', 'noveltyCount7D', 'volumeCounts12H', 'volumeCounts24H',\\\n                  'volumeCounts3D','volumeCounts5D','volumeCounts7D']\nprint('Clipping news outliers ...')\nnews_train_df = remove_outliers(news_train_df, columns_outlier)\n\nprint(f'{news_train_df.shape[0]} samples and {news_train_df.shape[1]} features in the training news dataset.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a91d1a12b6c2f3e835f5fa84301ece8ba53076d3"},"cell_type":"markdown","source":"# **Feature Selection**"},{"metadata":{"trusted":true,"_uuid":"7f2a00390a075f1d94d01e8e211ab908f7b145d5"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ncolumns_corr_market = ['volume', 'open', 'close','returnsClosePrevRaw1','returnsOpenPrevRaw1',\\\n           'returnsClosePrevMktres1','returnsOpenPrevMktres1','returnsClosePrevMktres10','returnsOpenPrevRaw10',\\\n           'returnsClosePrevMktres10', 'returnsOpenPrevMktres10', 'returnsOpenNextMktres10']\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(18,15))\nsns.heatmap(market_train_df[columns_corr_market].astype(float).corr(), linewidths=0.1, vmax=1.0, vmin=-1., square=True, cmap=colormap, linecolor='white', annot=True)\nplt.title('Pair-wise correlation')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8290d6830a27faad85be443eb548aefeb0ea0b5a"},"cell_type":"code","source":"# Plot correlation\ncolumns_corr = ['urgency', 'takeSequence', 'companyCount','marketCommentary','sentenceCount',\\\n           'firstMentionSentence','relevance','sentimentClass','sentimentWordCount','noveltyCount24H',\\\n           'noveltyCount3D', 'noveltyCount5D', 'noveltyCount7D','volumeCounts24H','volumeCounts3D','volumeCounts5D','volumeCounts7D']\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(18,15))\nsns.heatmap(news_train_df[columns_corr].astype(float).corr(), linewidths=0.1, vmax=1.0, vmin=-1., square=True, cmap=colormap, linecolor='white', annot=True)\nplt.title('Pair-wise correlation')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb2241905137bd09f6d15a1a8c8037bfa64c0ca7"},"cell_type":"code","source":"\"\"\"\ng = sns.pairplot(market_train_df[[u'volume', 'open', 'close','returnsClosePrevRaw1','returnsOpenPrevRaw1',\\\n           'returnsClosePrevMktres1','returnsOpenPrevMktres1','returnsClosePrevMktres10','returnsOpenPrevRaw10',\\\n           'returnsClosePrevMktres10', 'returnsOpenPrevMktres10', 'returnsOpenNextMktres10']], hue='open', palette = 'seismic',size=1.2,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\ng.set(xticklabels=[])\n\"\"\"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"015d5603faf5a8d7d4feefbb2a47c8024a9cf3da"},"cell_type":"code","source":"\"\"\"\nasset_code_dict = {k: v for v, k in enumerate(market_train_df['assetCode'].unique())}\ndrop_columns = [col for col in news_train_df.columns if col not in ['sourceTimestamp', 'urgency', 'takeSequence', 'bodySize', 'companyCount', \n               'sentenceCount', 'firstMentionSentence', 'relevance','firstCreated', 'assetCodes']]\ncolumns_news = ['firstCreated','relevance','sentimentClass','sentimentNegative','sentimentNeutral',\n               'sentimentPositive','noveltyCount24H','noveltyCount7D','volumeCounts24H','volumeCounts7D','assetCodes','sourceTimestamp',\n               'assetName','audiences', 'urgency', 'takeSequence', 'bodySize', 'companyCount', \n               'sentenceCount', 'firstMentionSentence','time']\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7422aea84c4ad918af1c3110a762bd7711310f4"},"cell_type":"code","source":"\"\"\"\n# Data processing function\ndef data_prep(market_df,news_df):\n    market_df['date'] = market_df.time.dt.date\n    market_df['close_to_open'] = market_df['close'] / market_df['open']\n    market_df.drop(['time'], axis=1, inplace=True)\n    \n    news_df = news_df[columns_news]\n    news_df['sourceTimestamp']= news_df.sourceTimestamp.dt.hour\n    news_df['firstCreated'] = news_df.firstCreated.dt.date\n    news_df['assetCodesLen'] = news_df['assetCodes'].map(lambda x: len(eval(x)))\n    news_df['assetCodes'] = news_df['assetCodes'].map(lambda x: list(eval(x))[0])\n    news_df['asset_sentiment_count'] = news_df.groupby(['assetName', 'sentimentClass'])['time'].transform('count')\n    news_df['len_audiences'] = news_train_df['audiences'].map(lambda x: len(eval(x)))\n    kcol = ['firstCreated', 'assetCodes']\n    news_df = news_df.groupby(kcol, as_index=False).mean()\n    market_df = pd.merge(market_df, news_df, how='left', left_on=['date', 'assetCode'], \n                            right_on=['firstCreated', 'assetCodes'])\n    del news_df\n    market_df['assetCodeT'] = market_df['assetCode'].map(asset_code_dict)\n    market_df = market_df.drop(columns = ['firstCreated','assetCodes','assetName']).fillna(0) \n    return market_df\n\nprint('Merging Data ...')\nmarket_train_df = data_prep(market_train_df,news_train_df)\nmarket_train_df.head()\n    \n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab6c4a3829a0815d98676ac4bbc1ace0df514877"},"cell_type":"markdown","source":"# Simple LSTM"},{"metadata":{"trusted":true,"_uuid":"c85c714b9a5fa70959463e7401723cc78f6018cf"},"cell_type":"code","source":"print(len(market_train_df.assetName.unique().categories))\nprint(market_train_df.count())\n\n# How many total records and assets are in the data\nprint(\"Total count: %d records of %d assets\", len(market_train_df.assetName.unique().categories), market_train_df.count())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cf9275bc07a5971dc63a7fd762bebf1129ad0e88"},"cell_type":"code","source":"# Common libs\nimport pandas as pd\nimport numpy as np\nimport sys\nimport os\nimport os.path\nimport random\nfrom pathlib import Path\nfrom time import time\nfrom itertools import chain\nimport gc\n\n# Image processing\nimport imageio\nimport skimage\nimport skimage.io\nimport skimage.transform\n#from skimage.transform import rescale, resize, downscale_local_mean\n\n# Charts\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\nimport seaborn as sns\n\n# ML\nimport scipy\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\n#from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n#from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import QuantileTransformer,StandardScaler, MinMaxScaler,OneHotEncoder, LabelEncoder, RobustScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n#from sklearn.preprocessing import OneHotEncoder\nfrom keras.preprocessing.sequence import TimeseriesGenerator\nfrom keras import optimizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout, BatchNormalization, LSTM, Embedding\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau\nfrom keras.utils import to_categorical\nimport tensorflow","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"81cd40f48b3b7c29f0a4430b692a8b78b83de3bb"},"cell_type":"code","source":"def learning_rate_power(current_round):\n    base_learning_rate = 0.19000424246380565\n    min_learning_rate = 0.01\n    lr = base_learning_rate * np.power(0.995,current_round)\n    return max(lr, min_learning_rate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"307252e3492ea734bb7f2c3df5b702c65cf75d82"},"cell_type":"code","source":"class MarketPrepro:\n    # features\n    assetcode_encoded = []\n    time_cols = ['year', 'week', 'day', 'dayofweek']\n    numeric_cols = ['volume', 'close', 'open', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n                    'returnsClosePrevMktres1', 'returnsOpenPrevMktres1', 'returnsClosePrevRaw10',\n                    'returnsOpenPrevRaw10', 'returnsClosePrevMktres10', 'returnsOpenPrevMktres10']\n    feature_cols = ['assetCode_encoded'] + time_cols + numeric_cols\n\n    # labels\n    label_cols = ['returnsOpenNextMktres10']\n\n    def __init__(self):\n        self.cats = {}\n        self.numeric_scaler = StandardScaler()\n\n    def fit(self, market_train_idx, market):\n        \"\"\"\n        Fit preprocessing scalers, encoders on given train df.\n        Store given indices to generate batches_from.\n        @param market_train_df: train data to fit on\n        \"\"\"\n        market_train_df = market.loc[market_train_idx.index].copy()\n        # Clean bad data. We fit on train dataset and it's ok to remove bad data\n        market_train_df = self.fix_train(market_train_df)\n\n        # Extract day, week, year from time\n        market_train_df = self.prepare_time_cols(market_train_df)\n        # Fit for numeric and time\n        # self.numeric_scaler = QuantileTransformer()\n        self.numeric_scaler.fit(market_train_df[self.numeric_cols + self.time_cols])\n\n        # Fit asset encoding\n        self.encode_asset(market_train_df, is_train=True)\n\n    def fix_train(self, train_df):\n        \"\"\"\n        Remove bad data. For train dataset only\n        \"\"\"\n        # Remove strange cases with close/open ratio > 2\n        max_ratio = 2\n        train_df = train_df[(train_df['close'] / train_df['open']).abs() <= max_ratio].loc[:]\n        # Fix outliers etc like for test set\n        train_df = self.safe_fix(train_df)\n        return train_df\n\n    def safe_fix(self, df):\n        \"\"\"\n        Fill na, fix outliers. Safe for test dataset, no rows removed.\n        \"\"\"\n        # Fill nans\n        df[self.numeric_cols] = df[self.numeric_cols].fillna(0)\n        # Fix outliers\n        df[self.numeric_cols] = df[self.numeric_cols].clip(df[self.numeric_cols].quantile(0.01),\n                                                           df[self.numeric_cols].quantile(0.99), axis=1)\n        return df\n\n    def get_X(self, df):\n        \"\"\"\n        Preprocess and return X without y\n        \"\"\"\n        df = df.copy()\n        df = self.safe_fix(df)\n\n        # Add day, week, year\n        df = self.prepare_time_cols(df)\n        # Encode assetCode\n        df = self.encode_asset(df)\n        # Scale numeric features and labels\n\n        df = df.set_index(['assetCode', 'time'], drop=False)\n        df[self.numeric_cols + self.time_cols] = self.numeric_scaler.transform(\n            df[self.numeric_cols + self.time_cols].astype(float))\n\n        # print(df.head())\n        # Return X\n        return df[self.feature_cols]\n\n    def get_y(self, df, is_raw_y=False):\n        if is_raw_y:\n            return df[self.label_cols]\n        else:\n            return (df[self.label_cols] >= 0).astype(float)\n\n    def encode_asset(self, df, is_train=False):\n        def encode(assetcode):\n            \"\"\"\n            Encode categorical features to numbers\n            \"\"\"\n            try:\n                # Transform to index of name in stored names list\n                index_value = self.assetcode_encoded.index(assetcode) + 1\n            except ValueError:\n                # If new value, add it to the list and return new index\n                self.assetcode_encoded.append(assetcode)\n                index_value = len(self.assetcode_encoded)\n\n            # index_value = 1.0/(index_value)\n            index_value = index_value / (self.assetcode_train_count + 1)\n            return (index_value)\n\n        # Store train assetcode_train_count for use as a delimiter for test data encoding\n        if is_train:\n            self.assetcode_train_count = len(df['assetCode'].unique()) + 1\n\n        df['assetCode_encoded'] = df['assetCode'].apply(lambda assetcode: encode(assetcode))\n        return (df)\n\n    @staticmethod\n    def prepare_time_cols(df):\n        \"\"\"\n        Extract time parts, they are important for time series\n        \"\"\"\n        df['year'] = pd.to_datetime(df['time']).dt.year\n        # Maybe remove month because week of year can handle the same info\n        df['day'] = pd.to_datetime(df['time']).dt.day\n        # Week of year\n        df['week'] = pd.to_datetime(df['time']).dt.week\n        df['dayofweek'] = pd.to_datetime(df['time']).dt.dayofweek\n        return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af75076729ed6a59aa567b08f21df792a49f560d"},"cell_type":"code","source":"class NewsPrepro:\n    \"\"\"\n    Aggregate news by day and asset. Normalize numeric values.\n    \"\"\"\n    news_cols_numeric = ['urgency', 'takeSequence', 'wordCount', 'sentenceCount', 'companyCount',\n                         'marketCommentary', 'relevance', 'sentimentNegative', 'sentimentNeutral',\n                         'sentimentPositive', 'sentimentWordCount', 'noveltyCount12H', 'noveltyCount24H',\n                         'noveltyCount3D', 'noveltyCount5D', 'noveltyCount7D', 'volumeCounts12H',\n                         'volumeCounts24H', 'volumeCounts3D', 'volumeCounts5D', 'volumeCounts7D']\n\n    feature_cols = news_cols_numeric\n\n    def fit(self, idx, news):\n        \"\"\"\n        Fit preprocessing scalers, encoders on given train df.\n        @param idx: index with time, assetCode\n        \"\"\"\n        # Save indices[assetCode, time, news_index] for all news\n        self.all_news_idx = self.news_idx(news)\n\n        # Get news only related to market idx\n        news_idx = idx.merge(self.all_news_idx, on=['assetCode', 'time'], suffixes=['_idx', ''])[\n            ['news_index', 'assetCode', 'time']]\n        news_train_df = news_idx.merge(news, left_on='news_index', right_index=True, suffixes=['_idx', ''])[\n            self.news_cols_numeric]\n\n        # Numeric data normalization\n        self.numeric_scaler = StandardScaler()\n        news_train_df.fillna(0, inplace=True)\n\n        # Fit scaler\n        self.numeric_scaler.fit(news_train_df)\n\n    def get_X(self, idx, news):\n        \"\"\"\n        Preprocess news for asset code and time from given index\n        \"\"\"\n        news_idx = idx.merge(self.all_news_idx, on=['assetCode', 'time'], suffixes=['_idx', ''])[\n            ['news_index', 'assetCode', 'time']]\n        news_df = news_idx.merge(news, left_on='news_index', right_index=True, suffixes=['_idx', ''])[\n            ['time', 'assetCode'] + self.news_cols_numeric]\n        news_df = self.aggregate_news(news_df)\n\n        return self.safe_fix(news_df)\n\n    def safe_fix(self, news_df):\n        \"\"\"\n        Scale, fillna\n        \"\"\"\n        # Normalize, fillna etc without removing rows.\n        news_df.fillna(0, inplace=True)\n        if not news_df.empty:\n            news_df[self.news_cols_numeric] = self.numeric_scaler.transform(news_df[self.news_cols_numeric])\n        return news_df\n\n    def news_idx(self, news):\n        \"\"\"\n        Get asset code, time -> news id\n        :param news:\n        :return:\n        \"\"\"\n\n        # Fix asset codes (str -> list)\n        asset_codes_list = news['assetCodes'].str.findall(f\"'([\\w\\./]+)'\")\n\n        # Expand assetCodes\n        assetCodes_expanded = list(chain(*asset_codes_list))\n\n        assetCodes_index = news.index.repeat(asset_codes_list.apply(len))\n        assert len(assetCodes_index) == len(assetCodes_expanded)\n        df_assetCodes = pd.DataFrame({'news_index': assetCodes_index, 'assetCode': assetCodes_expanded})\n\n        # Create expanded news (will repeat every assetCodes' row)\n        #        df_expanded = pd.merge(df_assetCodes, news, left_on='level_0', right_index=True)\n        df_expanded = pd.merge(df_assetCodes, news[['time']], left_on='news_index', right_index=True)\n        # df_expanded = df_expanded[['time', 'assetCode'] + self.news_cols_numeric].groupby(['time', 'assetCode']).mean()\n\n        return df_expanded\n\n    def with_asset_code(self, news):\n        \"\"\"\n        Update news index to be time, assetCode\n        :param news:\n        :return:\n        \"\"\"\n        if news.empty:\n            if 'assetCode' not in news.columns:\n                news.columns = news.columns + 'assetCode'\n            return news\n\n        # Fix asset codes (str -> list)\n        news['assetCodesList'] = news['assetCodes'].str.findall(f\"'([\\w\\./]+)'\")\n\n        # Expand assetCodes\n        assetCodes_expanded = list(chain(*news['assetCodesList']))\n\n        assetCodes_index = news.index.repeat(news['assetCodesList'].apply(len))\n        assert len(assetCodes_index) == len(assetCodes_expanded)\n        df_assetCodes = pd.DataFrame({'level_0': assetCodes_index, 'assetCode': assetCodes_expanded})\n\n        # Create expanded news (will repeat every assetCodes' row)\n        #        df_expanded = pd.merge(df_assetCodes, news, left_on='level_0', right_index=True)\n        df_expanded = pd.merge(df_assetCodes, news, left_on='level_0', right_index=True)\n        df_expanded = df_expanded[['time', 'assetCode'] + self.news_cols_numeric].groupby(['time', 'assetCode']).mean()\n\n        return df_expanded\n\n    def aggregate_news(self, df):\n        \"\"\"\n        News are rare for an asset. We get mean value for 10 days\n        :param df:\n        :return:\n        \"\"\"\n        if df.empty:\n            return df\n\n        # News are rare for the asset, so aggregate them by rolling period say 10 days\n        rolling_days = 10\n        df_aggregated = df.groupby(['assetCode', 'time']).mean().reset_index(['assetCode', 'time'])\n        df_aggregated = df_aggregated.groupby('assetCode') \\\n            .rolling(rolling_days, on='time') \\\n            .apply(np.mean, raw=False) \\\n            .reset_index('assetCode')\n        #df_aggregated.set_index(['time', 'assetCode'], inplace=True)\n        return df_aggregated","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0902b9de4757feadd2ae3996ac0e00bc52c0656d"},"cell_type":"code","source":"class JoinedPreprocessor:\n    def __init__(self, market_prepro, news_prepro):\n        self.market_prepro = market_prepro\n        self.news_prepro = news_prepro\n\n    def get_X(self, market, news):\n        \"\"\"\n        Returns preprocessed market + news\n        :return: X\n        \"\"\"\n        # Market row\n        market_X = self.market_prepro.get_X(market)\n        news_idx = self.news_prepro.news_idx(news)\n        news_X = self.news_prepro.get_X(news_idx, news)\n        #news_X.time = news_X.time.astype('datetime64')\n        # X = market X + news X\n        X = market_X.merge(news_X, how='left', on=['time', 'assetCode'], left_index=True)\n        X = X.fillna(0)\n        X = X[self.market_prepro.feature_cols + self.news_prepro.feature_cols]\n        return X\n\n    def get_Xy(self, idx, market, news, is_train=False, is_raw_y=False):\n        \"\"\"\n        Returns preprocessed features and labels for given indices\n        \"\"\"\n        # Get market data for index\n        market_df = market.loc[idx.index]\n        # We can remove bad data in train\n        if is_train:\n            market_df = self.market_prepro.fix_train(market_df)\n        market_Xy = self.market_prepro.get_X(market_df)\n        # Get news data for index\n        news_X = self.news_prepro.get_X(idx, news)\n        #news_X.time = pd.to_datetime(news_X.time, utc=True)\n        #news_X.time = news_X.time.astype('datetime64')\n        # Merge and return\n        Xy = market_Xy.merge(news_X, how='left', on=['time', 'assetCode'], left_index=True)\n        Xy = Xy.fillna(0)\n        X = Xy[self.market_prepro.feature_cols + self.news_prepro.feature_cols]\n        y = self.market_prepro.get_y(market_df, is_raw_y)\n\n        return X, y\n\n    def with_look_back(self, X, y, look_back, look_back_step):\n        \"\"\"\n        Add look back window values to prepare dataset for LSTM\n        \"\"\"\n        look_back_fixed = look_back_step * (look_back // look_back_step)\n        # Fill look_back rows before first\n        first_xrow = X.values[0]\n        first_xrow.shape = [1, X.values.shape[1]]\n        first_xrows = np.repeat(first_xrow, look_back_fixed, axis=0)\n        X_values = np.append(first_xrows, X.values, axis=0)\n\n        if y is not None:\n            first_yrow = y.values[0]\n            first_yrow.shape = [1, y.values.shape[1]]\n            first_yrows = np.repeat(first_yrow, look_back_fixed, axis=0)\n            y_values = np.append(first_yrows, y.values, axis=0)\n\n        # for i in range(0, len(X) - look_back + 1):\n        X_processed = []\n        y_processed = []\n        for i in range(look_back_fixed , len(X_values)):\n            # Add lookback to X\n            x_window = X_values[i - (look_back_fixed//look_back_step)*look_back_step:i+1:look_back_step, :]\n            X_processed.append(x_window)\n            # If input is X only, we'll not output y\n            if y is None:\n                continue\n            # Add lookback to y\n            y_window = y_values[i - (look_back_fixed//look_back_step)*look_back_step:i+1:look_back_step, :]\n            y_processed.append(y_window)\n        # Return Xy for train/test or X for prediction\n        if y is not None:\n            #return np.array(X_processed), np.array(y_processed)\n            return np.array(X_processed), y.values\n        else:\n            return np.array(X_processed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3988cde319cfbb7b494b0aec3a2341064614676e"},"cell_type":"code","source":"class JoinedGenerator:\n    \"\"\"\n    Keras standard approach to generage batches for model.fit_generator() call.\n    \"\"\"\n\n    def __init__(self, prepro, idx, market, news):\n        \"\"\"\n        @param preprocessor: market and news join preprocessor\n        @param market: full loaded market df\n        @param news: full loaded news df\n        @param index_df: df with assetCode and time of train or validation market data. Batches will be taken from them.\n        \"\"\"\n        self.market = market\n        self.prepro = prepro\n        self.news = news\n        self.idx = idx\n\n    def flow_lstm(self, batch_size, is_train, look_back, look_back_step):\n        \"\"\"\n        Generate batch data for LSTM NN\n        Each cycle in a loop we yield a batch for one training step in epoch.\n        \"\"\"\n        while True:\n            # Get market indices of random assets, sorted by assetCode, time.\n            batch_idx = self.get_random_assets_idx(batch_size)\n\n            # Get X, y data for this batch, containing market and news, but without look back yet\n            X, y = self.prepro.get_Xy(batch_idx, self.market, self.news, is_train)\n            # Add look back data to X, y\n            X, y = self.prepro.with_look_back(X, y, look_back, look_back_step)\n            yield X, y\n\n    def get_random_assets_idx(self, batch_size):\n        \"\"\"\n        Get random asset and it's last market data indices.\n        Repeat for next asset until we reach batch_size.\n        \"\"\"\n        asset_codes = self.idx['assetCode'].unique().tolist()\n\n        # Insert first asset\n        asset = np.random.choice(asset_codes)\n        asset_codes.remove(asset)\n        #asset = 'ADBE.O'\n        batch_index_df = self.idx[self.idx.assetCode == asset].tail(batch_size)\n\n        return batch_index_df.sort_values(by=['assetCode', 'time'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d0915222e8db24a9a8aadff2c31e8c6f7443b97"},"cell_type":"code","source":"class ModelFactory:\n    # LSTM look back window size\n    look_back = 90\n    # In windows size look back each look_back_step days\n    look_back_step = 10\n\n    def lstm_128(input_size):\n        model = Sequential()\n        # Add an input layer market + news\n        model.add(LSTM(units=128, return_sequences=True, input_shape=(None, input_size)))\n        model.add(LSTM(units=64, return_sequences=True))\n        model.add(LSTM(units=32, return_sequences=False))\n\n        # Add an output layer\n        model.add(Dense(1, activation='sigmoid'))\n        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n\n        return (model)\n\n    def train(model, toy, join_generator, val_generator):\n        weights_file = 'best_weights.h5'\n\n        earlystopper = EarlyStopping(patience=5, verbose=1)\n\n        checkpointer = ModelCheckpoint(weights_file\n                                       # ,monitor='val_acc'\n                                       , verbose=1\n                                       , save_best_only=True\n                                       , save_weights_only=True)\n\n        reduce_lr = ReduceLROnPlateau(factor=0.1, patience=2, min_lr=0.001)\n\n        # Set fit parameters\n        # Rule of thumb: steps_per_epoch = TotalTrainingSamples / TrainingBatchSize\n        #                validation_steps = TotalvalidationSamples / ValidationBatchSize\n        if toy:\n            batch_size = 1000\n            validation_batch_size = 1000\n            steps_per_epoch = 5\n            validation_steps = 2\n            epochs = 5\n            look_back = 10\n            look_back_step = 2\n        else:\n            batch_size = 1000\n            validation_batch_size = 1000\n            steps_per_epoch = 20\n            validation_steps = 5\n            epochs = 20\n            look_back = 90\n            look_back_step = 10\n\n        print(f'Toy:{toy}, epochs:{epochs}, steps per epoch: {steps_per_epoch}, validation steps:{validation_steps}')\n        print(f'Batch_size:{batch_size}, validation batch size:{validation_batch_size}')\n\n        # Fit\n        training = model.fit_generator(join_generator.flow_lstm(batch_size=batch_size\n                                                                , is_train=True\n                                                                , look_back=look_back\n                                                                , look_back_step=look_back_step)\n                                       , epochs=epochs\n                                       , validation_data=val_generator.flow_lstm(batch_size=validation_batch_size\n                                                                                 , is_train=False\n                                                                                 , look_back=look_back\n                                                                                 , look_back_step=look_back_step)\n                                       , steps_per_epoch=steps_per_epoch\n                                       , validation_steps=validation_steps\n                                       , callbacks=[earlystopper, checkpointer, reduce_lr])\n        # Load best weights saved\n        model.load_weights(weights_file)\n        return training","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"193a17f551e53f72ba515cd170a92a866299dd4b"},"cell_type":"code","source":"class TrainValTestSplit:\n\n    @staticmethod\n    def train_val_test_split(market, size):\n        \"\"\"\n        Get train, validation, test sample indices - time, assetCode, market index in original market df\n        @return: train, validation, test df.  Columns - time, assetCode, market_index, news_index\n        \"\"\"\n        market_idx = market[['assetCode', 'time']]\n        start_date = pd.datetime(2000, 1, 1).date()\n        market_idx = market_idx.loc[market_idx.time >= start_date] \\\n            .sort_values(by=['time', 'assetCode']) \\\n            .tail(size).copy()\n\n        # Split to train, validation and test\n        train_idx, test_idx = train_test_split(market_idx, shuffle=False, random_state=24)\n        train_idx, val_idx = train_test_split(train_idx, test_size=0.1, shuffle=False, random_state=24)\n\n        return train_idx, val_idx, test_idx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"58861140c35b4cc5bdf2dc66cd5edb77a7739215"},"cell_type":"code","source":"# general mode\ndef train_test_val_split(market):\n    \"\"\"\n    Get sample of assets but each asset has full market data after 2009\n    Split to time sorted train, validation and test.\n    @return: train, validation, test df. Short variant - time and asset columns only\n    \"\"\"\n    # Work with data after 2009\n    market_idx = market[market.time > '2009'][['time', 'assetCode']]\n    if toy: market_idx = market_idx.sample(100000)\n    else: market_idx = market_idx.sample(1000000)\n    # Split to train, validation and test\n    market_idx = market_idx.sort_values(by=['time'])\n    market_train_idx, market_test_idx = train_test_split(market_idx, shuffle=False, random_state=24)\n    market_train_idx, market_val_idx = train_test_split(market_train_idx, test_size=0.1, shuffle=False, random_state=24)\n    return(market_train_idx, market_val_idx, market_test_idx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df00cae1bbb8ccc3e8b1c7656819fa81eee21d13"},"cell_type":"code","source":"class Predictor:\n    def __init__(self, prepro, market_prepro, news_prepro, model, look_back, look_back_step):\n        self.prepro = prepro\n        self.market_prepro = market_prepro\n        self.news_prepro = news_prepro\n        self.model = model\n        self.look_back = look_back\n        self.look_back_step = look_back_step\n\n    def predict(self, market, news):\n        X = self.prepro.get_X(market, news)\n        X = self.prepro.with_look_back(X, None, self.look_back, self.look_back_step)\n        y = self.model.predict(X) # * 2 - 1\n        return y\n\n    def predict_idx(self, pred_idx, market, news):\n        # Get preprocessed X, y\n        X_test, y_test = self.prepro.get_Xy(pred_idx, market, news, is_train=False, is_raw_y=True)\n        # look back\n        X_test, y_test = self.prepro.with_look_back(X_test, y_test, \n                                                    look_back=self.look_back,\n                                                    look_back_step=self.look_back_step)\n        y_pred = self.model.predict(X_test) # * 2 - 1\n        return y_pred, y_test\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f493da6cc530047b019689d660e990cd8aa7a649"},"cell_type":"code","source":"plt.style.use('seaborn')\n# Set random seed to make results reproducable\nnp.random.seed(42)\ntensorflow.set_random_seed(42)\nos.environ['PYTHONHASHSEED'] = '42'\npd.set_option('display.max_columns', 100)\npd.set_option('display.width', 200)\n\ninput_dir = '../input'\nprint(os.listdir(\"../input\"))\nmarket = pd.read_csv(input_dir + '/marketdata_sample.csv')\nnews = pd.read_csv(input_dir + '/news_sample.csv')\n\n# !!! Hack\nnews.time = pd.to_datetime('2007-02-01 23:35')\n# Restrict datetime to date\n#news.time = pd.to_datetime(news.time.astype('datetime64').dt.date, utc=True)\nnews.time = news.time.astype('datetime64').dt.date\nmarket.time = market.time.astype('datetime64').dt.date\n\n# Split to train, validation and test\ntoy = True\nif toy:\n    sample_size = 10000\nelse:\n    sample_size = 500000\ntrain_idx, val_idx, test_idx = TrainValTestSplit.train_val_test_split(market, sample_size)\n\n# print(market)\n# print(news)\n# print(train_idx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b922b7f2399b171952ff6d0515a602254557526"},"cell_type":"code","source":"plt.style.use('seaborn')\n# Set random seed to make results reproducable\nnp.random.seed(42)\ntensorflow.set_random_seed(42)\nos.environ['PYTHONHASHSEED'] = '42'\n# pd.set_option('display.max_columns', 100)\n# pd.set_option('display.width', 200)\n\nmarket, news = env.get_training_data()\n\nmarket = pd.DataFrame(market)\nnews = pd.DataFrame(news)\n\n\n# Restrict datetime to date\nnews.time = pd.to_datetime(news.time.astype('datetime64').dt.date)\nmarket.time = pd.to_datetime(market.time.astype('datetime64').dt.date)\n\n\n# Split to train, validation and test\ntoy = True\nif toy:\n    sample_size = 10000\nelse:\n    sample_size = 500000\n#train_idx, val_idx, test_idx = TrainValTestSplit.train_val_test_split(market, sample_size)\ntrain_idx, val_idx, test_idx = train_test_val_split(market)\n\n# print(market)\n# print(news)\n# print(train_idx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92cd5a5241d27877dcfbf8acb8bf9f51c312445f"},"cell_type":"code","source":"print(train_idx.shape)\nprint(val_idx.shape)\nprint(test_idx.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff6398bdf7938d89cbdef4f5740bde57799a39f2","scrolled":true},"cell_type":"code","source":"# Create preprocessors\nmarket_prepro = MarketPrepro()\nmarket_prepro.fit(train_idx, market)\n\nnews_prepro = NewsPrepro()\nnews_prepro.fit(train_idx, news)\n\nprepro = JoinedPreprocessor(market_prepro, news_prepro)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f35427cc8ea26a781e08970a3d2b72225f8794c0","scrolled":false},"cell_type":"code","source":"# prediction_prepro = PredictionPreprocessor(prepro, market_prepro, news_prepro)\n# x = prediction_prepro.get_X_with_lookback(market, news, 4,2)\n\n\n# Train data generator instance\njoin_generator = JoinedGenerator(prepro, train_idx, market, news)\nval_generator = JoinedGenerator(prepro, val_idx, market, news)\nprint('Generators created')\n\n# Create and train model\n# model = ModelFactory.lstm_128(len(market_prepro.feature_cols) + len(news_prepro.feature_cols))\nmodel.load_weights(\"best_weights.h5\")\n\nprint(model.summary())\ntraining = ModelFactory.train(model, toy, join_generator, val_generator)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f234b30e2e52134fabd0cb806c3faa249036b1d"},"cell_type":"code","source":"\"\"\"\n# Predict\n\ny_pred, y_test = predictor.predict_idx(test_idx, market, news)\n\ny_pred = predictor.predict(market, news)\n\nplt.plot(y_pred)\nplt.plot(y_test)\nplt.legend([\"pred\", \"test\"])\nplt.show()\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4587e3cc623135361e4d4491f7aff41f8d18bf74"},"cell_type":"code","source":"# # Plotting\n# f, axs = plt.subplots(3,1, sharex=True, figsize=(12,8))\n# # Close price \n# ass_market.close.plot(ax=axs[0])\n# axs[0].set_ylabel(\"Price\")\n\nplt.figure(1, figsize=(8,3))\nplt.subplot(121)\nplt.plot(training.history['loss'])\nplt.plot(training.history['val_loss'])\nplt.title(\"Loss and validation loss\")\nplt.legend([\"Loss\", \"Validation loss\"])\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\n\nplt.subplot(122)\nplt.plot(training.history['acc'])\nplt.plot(training.history['val_acc'])\nplt.title(\"Acc and validation acc\")\nplt.legend([\"Acc\", \"Validation acc\"])\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.suptitle('Training history', fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"afe65acb8b2704a034d283503b4155e61a39987e"},"cell_type":"code","source":"def predict_on_test():\n    # Predict on last test data\n    pred_size=1000\n    pred_idx = test_idx.tail(pred_size + ModelFactory.look_back)\n    y_pred, y_test = predictor.predict_idx(pred_idx, market, news)\n    #market_df = market.loc[pred_idx.index]\n    #y_test = market_df['returnsOpenNextMktres10'].values\n    # Plot\n    ax1 = plt.subplot2grid((2, 2), (0, 0), rowspan=2)\n    ax1.plot(y_test, linestyle='none', marker='.', color='darkblue')\n    ax1.plot(y_pred, linestyle='none', marker='.', color='darkorange')\n    ax1.legend([\"Ground truth\",\"Predicted\"])\n    ax1.set_title(\"Both\")\n    ax1.set_xlabel(\"Epoch\")\n    ax2 = plt.subplot2grid((2, 2), (0, 1), colspan=1,rowspan=1)\n    ax2.plot(y_test, linestyle='none', marker='.', color='darkblue')\n    ax2.set_title(\"Ground truth\")\n    ax3 = plt.subplot2grid((2, 2), (1, 1), colspan=1,rowspan=1)\n    ax3.plot(y_pred, linestyle='none', marker='.', color='darkorange')\n    ax3.set_title(\"Predicted\")\n    plt.tight_layout()\n    plt.show()\n\npredictor = Predictor( prepro, market_prepro, news_prepro, model, ModelFactory.look_back, ModelFactory.look_back_step)  \npredict_on_test()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"503222a471cdc56d5d5ca461ca725f0d5fbbb927"},"cell_type":"code","source":"def predict_random_asset():\n    \"\"\"\n    Get random asset from test set, predict on it, plot ground truth and predicted value\n    \"\"\"\n    # Get any asset\n    asset = test_idx['assetCode'].sample(1, random_state=66).values[0]\n    pred_idx = test_idx[test_idx.assetCode == asset]\n    y_pred, y_test = predictor.predict_idx(pred_idx, market, news)\n    # Plot\n    plt.plot(y_test, linestyle='none', marker='.', color='darkblue')\n    plt.plot(y_pred, linestyle='none', marker='.', color='darkorange')\n    plt.xticks(rotation=45)\n    plt.title(asset)\n    plt.legend([\"Ground truth\", \"predicted\"])\n    plt.show()\n    \npredict_random_asset()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9e291b5fe1d66160f90121f0ace2299f01ff554"},"cell_type":"code","source":"def get_score():\n    \"\"\"\n    Calculation of actual metric that is used to calculate final score\n    @param r: returnsOpenNextMktres10\n    @param u: universe\n    where rti is the 10-day market-adjusted leading return for day t for instrument i, and uti is a 0/1 universe variable (see the data description for details) that controls whether a particular asset is included in scoring on a particular day.    \n    \"\"\"\n    # Get test sample to calculate score on\n    pred_idx = test_idx #.sample(10000, random_state=24)\n    y_pred, y_test = predictor.predict_idx(pred_idx, market, news)    \n    look_back=ModelFactory.look_back\n    market_df = market.loc[pred_idx.index]\n    r=market_df['returnsOpenNextMktres10'].values#.values[look_back:]\n    u=market_df['universe'].values#.values[look_back:]\n    confidence=y_pred\n    # calculation of actual metric that is used to calculate final score\n    r = r.clip(-1,1) # get rid of outliers. Where do they come from??\n    x_t_i = confidence.reshape(r.shape) * r * u\n\n    #print(x_t_i.iloc[0])\n    d = (market_df['time'].dt.day).values #[look_back:]\n    data = {'day' : d, 'x_t_i' : x_t_i}\n    df = pd.DataFrame(data)\n    x_t = df.groupby('day').sum().values.flatten()\n    mean = np.mean(x_t)\n    std = np.std(x_t)\n    score = mean / std\n    return score\n    \nprint(f\"Sigma score: {get_score()}\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1b47d8f51329cf7acd81f6e6c164c6b34bcfbad"},"cell_type":"code","source":"def calc_acc():\n    # Get X_test, y_test with look back for LSTM\n    pred_idx = test_idx.sample(10000)\n    y_pred, y_test = predictor.predict_idx(pred_idx, market, news)\n    \n    #y_pred = pd.DataFrame(market_prepro.y_scaler.inverse_transform(model.predict(X_test)))\n    print(\"Accuracy: %f\" % accuracy_score(y_test >= 0, y_pred >= 0))\n    #score = get_score(market_df, confidence, market_df.returnsOpenNextMktres10, market_df.universe)\n    print('Predictions size: ', len(y_pred))\n    print('y_test size:', len(y_test))\n     # Show distribution of confidence that will be used as submission\n    plt.hist(y_test, bins='auto', alpha=0.3)\n    plt.hist(y_pred, bins='auto', alpha=0.3, color='darkorange')\n    plt.legend(['Ground truth', 'Predicted'])\n    plt.xlabel(\"Confidence\")\n    plt.ylabel(\"Count\")\n    plt.title(\"predicted confidence\")\n    plt.show()\n\n# Call accuracy calculation and plot    \ncalc_acc()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5793d3be36c1c06494226d4d70be912808c26b1d"},"cell_type":"code","source":"# market['time']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59ee06d12b9be09451276cbf0e73638d9af6a3df"},"cell_type":"code","source":"def make_predictions(market_obs_df, news_obs_df, predictions_template_df):\n    \"\"\"\n    Predict confidence for one day and update predictions_template_df['confidenceValue']\n    @param market_obs_df: market_obs_df returned from env\n    @param predictions_template_df: predictions_template_df returned from env.\n    @return: None. prediction_template_df updated instead. \n    \"\"\"\n    # Predict\n    market_obs_df = pd.DataFrame(market_obs_df)\n    news_obs_df = pd.DataFrame(news_obs_df)\n    \n    # Restrict datetime to date\n    news_obs_df['time'] = news_obs_df['time'].astype('datetime64')\n    market_obs_df['time'] = market_obs_df['time'].astype('datetime64')\n    \n    y_pred = predictor.predict(market_obs_df, news_obs_df)\n    confidence_df=pd.DataFrame(y_pred, columns=['confidence'])\n\n    # Merge predicted confidence to predictions template\n    pred_df = pd.concat([predictions_template_df, confidence_df], axis=1).fillna(0)\n    predictions_template_df.confidenceValue = pred_df.confidence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c0ec12b43470d7cdd40f431264b92215675ec8f9"},"cell_type":"code","source":"##########################\n# Submission code\n\n# Save data here for later debugging on it\ndays_saved_data = []\n\n# Store execution info for plotting later\npredicted_days=[]\npredicted_times=[]\nlast_predictions_template_df = None\n\n# Predict day by day\ndays = env.get_prediction_days()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1357741240d49d70334b56cd34028e23d950a620"},"cell_type":"code","source":"# market['time'].astype('datetime64')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4385e129b204cd0c04e5a68156a486507988c0a6"},"cell_type":"code","source":"last_year=None\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    # Store the data for later debugging on it\n    days_saved_data.append((market_obs_df, news_obs_df, predictions_template_df))\n   \n    # For later plotting\n    predicted_days.append(market_obs_df.iloc[0].time.strftime('%Y-%m-%d'))\n    time_start = time()\n    # For logging\n    cur_year = market_obs_df.iloc[0].time.strftime('%Y')\n    if cur_year != last_year:\n        print(f'Predicting {cur_year}...')\n        last_year = cur_year\n    # Call prediction func\n    make_predictions(market_obs_df, news_obs_df, predictions_template_df)\n    #!!!\n    env.predict(predictions_template_df)\n    \n    # For later plotting\n    last_predictions_template_df = predictions_template_df\n    predicted_times.append(time()-time_start)\n    #print(\"Prediction completed for \", predicted_days[-1])\n    \nprint(f\"Prediction for {len(predicted_days)} days completed\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77d14d31cb87164a057fa6b313adcba83e928e6c"},"cell_type":"code","source":"# Plot execution time \nsns.barplot(np.array(predicted_days), np.array(predicted_times))\nplt.title(\"Execution time per day\")\nplt.xlabel(\"Day\")\nplt.ylabel(\"Execution time, seconds\")\nplt.show()\n\n# Plot predicted confidence for last day\nlast_predictions_template_df.plot(linestyle='none', marker='.', color='darkorange')\nplt.title(\"Predicted confidence for last observed day: %s\" % predicted_days[-1])\nplt.xlabel(\"Observation No.\")\nplt.ylabel(\"Confidence\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be769dbd351eb5efe86be9c8a11af771bcdb0225"},"cell_type":"markdown","source":"录入结果"},{"metadata":{"trusted":true,"_uuid":"493a95abfee6ef3b899bd2baad1192ebf8e9aee3"},"cell_type":"code","source":"env.write_submission_file()\nprint([filename for filename in os.listdir('.') if '.csv' in filename])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11659ce12d89887aef5d5a8025661993f1f680ab"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"36f6863a4f850c6dbba4abf7c0ad4171d88e6e3f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}