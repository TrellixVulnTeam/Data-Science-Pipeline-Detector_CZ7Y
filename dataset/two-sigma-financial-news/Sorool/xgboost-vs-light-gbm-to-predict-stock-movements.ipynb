{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n\nimport os\nimport time\nfrom datetime import date, datetime\nimport matplotlib.patches as mpatches\nfrom math import log\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport shap\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nimport lightgbm as lgb \nimport xgboost as xgb \n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from kaggle.competitions import twosigmanews\n# You can only call make_env() once, so don't lose it!\nenv = twosigmanews.make_env()\n(market_train_df, news_train_df) = env.get_training_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3df74311cdabec4439b7a6f958cd23258b1981ab"},"cell_type":"code","source":"from multiprocessing import Pool\n\ndef create_lag(df_code, n_lag=[3, 7, 14, ], shift_size=1):\n    code = df_code['assetCode'].unique()\n\n    for col in return_features:\n        for window in n_lag:\n            rolled = df_code[col].shift(shift_size).rolling(window=window)\n            lag_mean = rolled.mean()\n            lag_max = rolled.max()\n            lag_min = rolled.min()\n            lag_std = rolled.std()\n            df_code['%s_lag_%s_mean' % (col, window)] = lag_mean\n            df_code['%s_lag_%s_max' % (col, window)] = lag_max\n            df_code['%s_lag_%s_min' % (col, window)] = lag_min\n\n    return df_code.fillna(-1)\n\n\n\ndef generate_lag_features(df, n_lag=[3, 7, 14]):\n    \n    \n    features = ['time', 'assetCode', 'assetName', 'volume', 'close', 'open',\n                'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n                'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n                'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n                'returnsClosePrevMktres10', 'returnsOpenPrevMktres10',\n                'returnsOpenNextMktres10', 'universe']\n\n    assetCodes = df['assetCode'].unique()\n    print(assetCodes)\n    all_df = []\n    df_codes = df.groupby('assetCode')\n    df_codes = [df_code[1][['time', 'assetCode'] + return_features]\n                for df_code in df_codes]\n    print('total %s df' % len(df_codes))\n\n    pool = Pool(4)\n    all_df = pool.map(create_lag, df_codes)\n\n    new_df = pd.concat(all_df)\n    new_df.drop(return_features, axis=1, inplace=True)\n    pool.close()\n\n    return new_df\n\ndef mis_impute(data):\n    for i in data.columns:\n        if data[i].dtype == \"object\":\n            data[i] = data[i].fillna(\"other\")\n        elif (data[i].dtype == \"int64\" or data[i].dtype == \"float64\"):\n            data[i] = data[i].fillna(data[i].mean())\n        else:\n            pass\n    return data\n\ndef data_prep(market_train):\n    lbl = {k: v for v, k in enumerate(market_train['assetCode'].unique())}\n    market_train['assetCodeT'] = market_train['assetCode'].map(lbl)\n    market_train = market_train.dropna(axis=0)\n    return market_train\n\ndef exp_loss(p, y):\n    y = y.get_label()\n    grad = -y * (1.0 - 1.0 / (1.0 + np.exp(-y * p)))\n    hess = -(np.exp(y * p) * (y * p - 1) - 1) / ((np.exp(y * p) + 1)**2)\n    return grad, hess","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"997bf5c8a6b681c554571f9478f9a6d31d798859"},"cell_type":"code","source":"market_train_df['time'] = market_train_df['time'].dt.date\nmarket_train_df = market_train_df.loc[market_train_df['time'] >= date(\n    2010, 1, 1)] #recession data is anomalous\n\nreturn_features = ['returnsClosePrevMktres10',\n                   'returnsClosePrevRaw10', 'open', 'close']\n#making lag features\nn_lag = [3, 7, 14]\nnew_df = generate_lag_features(market_train_df, n_lag=n_lag)\nmarket_train_df = pd.merge(market_train_df, new_df,\n                           how='left', on=['time', 'assetCode'])\n\n#fixing null values\nmarket_train_df = mis_impute(market_train_df)\n#encoding assetCode\nmarket_train_df = data_prep(market_train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7575f417801b987f54b99615108f799d48f64152"},"cell_type":"code","source":"market_train_df['price_diff'] = market_train_df['close'] - market_train_df['open']\ngrouped = market_train_df.groupby('time').agg({'price_diff': ['std', 'min']}).reset_index()\nmarket_train_df.sort_values('price_diff')[:10]\n#looking at big stock changes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"88f47493040979f4f2d4e93aa558a700680885de"},"cell_type":"code","source":"#changing outlier features to mean\nmarket_train_df['close_to_open'] =  np.abs(market_train_df['close'] / market_train_df['open'])\nmarket_train_df['assetName_mean_open'] = market_train_df.groupby('assetName')['open'].transform('mean')\nmarket_train_df['assetName_mean_close'] = market_train_df.groupby('assetName')['close'].transform('mean')\n\nfor i, row in market_train_df.loc[market_train_df['close_to_open'] >= 2].iterrows():\n    if np.abs(row['assetName_mean_open'] - row['open']) > np.abs(row['assetName_mean_close'] - row['close']):\n        market_train_df.iloc[i,5] = row['assetName_mean_open']\n    else:\n        market_train_df.iloc[i,4] = row['assetName_mean_close']\n        \nfor i, row in market_train_df.loc[market_train_df['close_to_open'] <= 0.5].iterrows():\n    if np.abs(row['assetName_mean_open'] - row['open']) > np.abs(row['assetName_mean_close'] - row['close']):\n        market_train_df.iloc[i,5] = row['assetName_mean_open']\n    else:\n        market_train_df.iloc[i,4] = row['assetName_mean_close']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"146717870579fe59082fb706a0aae9600dbfc92b"},"cell_type":"code","source":"#more feature engineering\nmarket_train_df['returnsOpenPrevRaw1_to_volume'] = market_train_df['returnsOpenPrevRaw1'] / market_train_df['volume']\nmarket_train_df['close_to_open'] = market_train_df['close'] / market_train_df['open']\nnews_train_df['sentence_word_count'] =  news_train_df['wordCount'] / news_train_df['sentenceCount']\nnews_train_df['time'] = news_train_df.time.dt.hour\nnews_train_df['sourceTimestamp']= news_train_df.sourceTimestamp.dt.hour\nnews_train_df['firstCreated'] = news_train_df.firstCreated.dt.date\nnews_train_df['assetCodesLen'] = news_train_df['assetCodes'].map(lambda x: len(eval(x)))\nnews_train_df['assetCodes'] = news_train_df['assetCodes'].map(lambda x: list(eval(x))[0])\nnews_train_df['headlineLen'] = news_train_df['headline'].apply(lambda x: len(x))\nnews_train_df['assetCodesLen'] = news_train_df['assetCodes'].apply(lambda x: len(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"736c416430f81d1bfd4a59ce6d9adde41925eeb8"},"cell_type":"code","source":"lbl = {k: v for v, k in enumerate(news_train_df['headlineTag'].unique())}\nnews_train_df['headlineTagT'] = news_train_df['headlineTag'].map(lbl)\nkcol = ['firstCreated', 'assetCodes']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4ffb06da19c218fba10e90ab442820830472f21"},"cell_type":"code","source":"#merging market and news\nnews_train_df = news_train_df.groupby(kcol, as_index=False).mean()\nagg_market_df = pd.merge(market_train_df, news_train_df, how='left', left_on=['time', 'assetCode'], \n                            right_on=['firstCreated', 'assetCodes'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d72c8d798958a32aabec57d10792a8723a5f028"},"cell_type":"code","source":"#freeing memory and dropping na values\ndel market_train_df\ndel news_train_df\n#lbl = {k: v for v, k in enumerate(market_df['assetCode'].unique())}\n#market_df['assetCodeT'] = market_df['assetCode'].map(lbl)\nagg_market_df = agg_market_df.dropna(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86d143107d6818abaaf22c3ae4fc51f2ec8f1c14"},"cell_type":"code","source":"#dropping features\nagg_market_df.drop(['price_diff', 'assetName_mean_open', 'assetName_mean_close'], axis=1, inplace=True)\nup = agg_market_df.returnsOpenNextMktres10 >= 0\n\nfcol = [c for c in agg_market_df.columns if c not in ['assetCode', 'assetCodes', 'assetCodesLen', 'assetName', 'assetCodeT',\n                                             'firstCreated', 'headline', 'headlineTag', 'marketCommentary', 'provider',\n                                             'returnsOpenNextMktres10', 'sourceId', 'subjects', 'time', 'time_x', 'universe','sourceTimestamp']]\n\n#making labels\nX = agg_market_df[fcol].values\nup = up.values\nr = agg_market_df.returnsOpenNextMktres10.values\n\n# Scaling of X values\nmins = np.min(X, axis=0)\nmaxs = np.max(X, axis=0)\nrng = maxs - mins\nX = 1 - ((maxs - X) / rng)\nassert X.shape[0] == up.shape[0] == r.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86b4a9a630324593dd532e17566c5fbf79903605"},"cell_type":"code","source":"#creates train test split with nsamples and specified random state\ndef sample_ttsplit(nsamples, rng):\n    r_sample = r[:nsamples]\n    X_sample = X[:nsamples]\n    up_sample = up[:nsamples]\n\n    X_train, X_test, up_train, up_test, r_train, r_test = model_selection.train_test_split(X_sample, up_sample, r_sample, random_state=rng)\n    return X_train, X_test, up_train, up_test, r_train, r_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7283200629b4fc99fc0b918eed981b273851b14"},"cell_type":"code","source":"#returns execution time, accuracy, roc, and the model itself for either lgbm or xgbm\ndef boost_test(X_train, X_test, y_train, y_test, xg, model, params):\n    #The data is stored in a DMatrix object \n    #label is used to define our outcome variable\n    if xg == True:\n        dtrain=model.DMatrix(X_train,label=y_train, feature_names=agg_market_df[fcol].columns)\n        dtest=model.DMatrix(X_test, feature_names=agg_market_df[fcol].columns)\n    else:\n        dtrain=model.Dataset(X_train,label=y_train, feature_name=list(agg_market_df[fcol].columns))\n\n    num_round=50\n    start = datetime.now() \n    fit=model.train(params,dtrain,num_round) \n    stop = datetime.now()\n    #Execution time of the model \n    execution_time = stop-start \n    \n    if xg == True:\n        ypred=fit.predict(dtest) \n    else:\n        ypred=fit.predict(X_test)\n    \n    max = len(ypred)\n    for i in range(0,max): \n        if ypred[i]>=.5:       # setting threshold to .5 \n           ypred[i]= True \n        else: \n           ypred[i]= False \n\n    accuracy = accuracy_score(y_test,ypred) \n    auc =  roc_auc_score(y_test,ypred)\n    return execution_time, accuracy, auc, fit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"57e90de8b0dfaba248e473b2546adb425f838fe9"},"cell_type":"code","source":"X_train, X_test, up_train, up_test, r_train, r_test = sample_ttsplit(100, 99)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"58de8cef8ce863d146941399610a20d05905ad18"},"cell_type":"code","source":"#small model for plotting\nparams = {'max_depth':7, 'eta':1, 'silent':1,'objective':'binary:logistic','eval_metric':'auc','learning_rate':.05}\na, b, c, xgbfit = boost_test(X_train, X_test, up_train, up_test, True, xgb, params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61d1f3e8be1ea02eb0000813a33baac626bf9fee"},"cell_type":"code","source":"#small model for plotting\nparam = {'num_leaves':150, 'objective':'binary','max_depth':7,'learning_rate':.05,'max_bin':200}\nparam['metric'] = ['auc', 'binary_logloss']\na, b, c, lgbfit = boost_test(X_train, X_test, up_train, up_test, False, lgb, param)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ce6551564f3f5c680b6746869d377d87891cdb9"},"cell_type":"code","source":"#lgb tree\nplt.figure()\nlgb.plotting.plot_tree(lgbfit, figsize=(100, 100))\nfig = plt.gcf()\nfig.set_size_inches(150, 100)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c2e026b9a7dd1166871c7ab4955943d3ec627767"},"cell_type":"code","source":"#xgb tree\nplt.figure()\nxgb.plot_tree(xgbfit)\nfig = plt.gcf()\nfig.set_size_inches(150, 100)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12eba049d5324baf1b348c88fadcd5f22b0f65ea"},"cell_type":"code","source":"#computing accuracy, roc, execution time for several max_depths, cross validation included\nscore_dict = {}\nfor md in [4, 7, 11, 15, 19]:\n    xgb_time, xgb_acc, xgb_auc = 0, 0, 0\n    lgb_time, lgb_acc, lgb_auc = 0, 0, 0\n    for i in [14, 50, 80]:\n        X_train, X_test, up_train, up_test, r_train, r_test = sample_ttsplit(10000, i)\n        lgbparams = {'num_leaves':2**(4*log(md)), 'objective':'binary','max_depth':md,'learning_rate':.05,'max_bin':200}\n        lgbparams['metric'] = ['auc', 'binary_logloss']\n        xgbparams = {'max_depth':md, 'eta':1, 'silent':1,'objective':'binary:logistic','eval_metric':'auc','learning_rate':.05}\n        xgbtime, xgbacc, xgbauc, xgbfit = boost_test(X_train, X_test, up_train, up_test, True, xgb, params)\n        lgbtime, lgbacc, lgbauc, lgbfit = boost_test(X_train, X_test, up_train, up_test, False, lgb, param)\n        xgb_time += xgbtime.total_seconds()\n        xgb_acc += xgbacc\n        xgb_auc += xgbauc\n        lgb_time += lgbtime.total_seconds()\n        lgb_acc += lgbacc\n        lgb_auc += lgbauc\n    xgb_time /= 3\n    xgb_acc /= 3\n    xgb_auc /= 3\n    lgb_time /= 3\n    lgb_acc /= 3\n    lgb_auc /= 3\n    score_dict[md] = {}\n    score_dict[md]['xgb'] = [xgb_time, xgb_acc, xgb_auc]\n    score_dict[md]['lgb'] = [lgb_time, lgb_acc, lgb_auc]\n\nscore_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8445edd952dfe45735ce6292df7fd71c44c08e15"},"cell_type":"code","source":"plt.figure()\nfor (i, col) in enumerate(['r', 'g']):\n    xgb_vec = [score_dict[4]['xgb'][i + 1], score_dict[7]['xgb'][i + 1], score_dict[11]['xgb'][i + 1], score_dict[15]['xgb'][i+1], score_dict[19]['xgb'][i+1]]\n    lgb_vec = [score_dict[4]['lgb'][i + 1], score_dict[7]['lgb'][i + 1], score_dict[11]['lgb'][i + 1], score_dict[15]['lgb'][i+1], score_dict[19]['lgb'][i+1]]\n    plt.plot([4, 7, 11, 15, 19], xgb_vec, color = col)\n    plt.plot([4, 7, 11, 15, 19], lgb_vec, color = col, linestyle = '--')\nplt.xlabel(\"max depth\")\nplt.title(\"max depth vs accuracy and roc_auc score (10000 samples)\")\nplt.ylabel(\"score\")\nplt.legend(['xgb roc', 'lgb roc', 'xgb accuracy', 'lgb accuracy'])\nplt.show()\n\nplt.figure()\nxgb_vec = [score_dict[4]['xgb'][0], score_dict[7]['xgb'][0], score_dict[11]['xgb'][0], score_dict[15]['xgb'][0], score_dict[19]['xgb'][0]]\nlgb_vec = [score_dict[4]['lgb'][0], score_dict[7]['lgb'][0], score_dict[11]['lgb'][0], score_dict[15]['lgb'][0], score_dict[19]['lgb'][0]]\nplt.plot([4, 7, 11, 15, 19], xgb_vec, color = 'r')\nplt.plot([4, 7, 11, 15, 19], lgb_vec, color = 'g')\nplt.xlabel(\"max depth\")\nplt.title(\"max depth vs computational time (10000 samples)\")\nplt.ylabel(\"seconds\")\nplt.legend(['xgb', 'lgb'])\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8a831a0f40d69cee2a35d6845d1e159bf61ab48"},"cell_type":"code","source":"#a larger sample split\nX_train, X_test, up_train, up_test, r_train, r_test = sample_ttsplit(200000, i)\nmd = 10\nlgbparams = {'num_leaves':2**(4*log(md)), 'objective':'binary','max_depth':md,'learning_rate':.05,'max_bin':200}\nlgbparams['metric'] = ['auc', 'binary_logloss']\nxgbparams = {'max_depth':md, 'eta':1, 'silent':1,'objective':'binary:logistic','eval_metric':'auc','learning_rate':.05}\nxgbtime, xgbacc, xgbauc, xgbfit = boost_test(X_train, X_test, up_train, up_test, True, xgb, params)\nlgbtime, lgbacc, lgbauc, lgbfit = boost_test(X_train, X_test, up_train, up_test, False, lgb, param)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1320d1a6093ae7c3598df758efad50aa292fd895"},"cell_type":"code","source":"#xgb feature importances\nX_importance = X_test\n\n# Explain model predictions using shap library:\nexplainer = shap.TreeExplainer(xgbfit)\nshap_values = explainer.shap_values(pd.DataFrame(X_importance, columns = agg_market_df[fcol].columns))\nshap.summary_plot(shap_values, X_importance, agg_market_df[fcol].columns)\nshap.summary_plot(shap_values, X_importance, agg_market_df[fcol].columns, plot_type='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"934bc8cd157186355ccae847eb17aeee8e658143"},"cell_type":"code","source":"#lgb feature importances\nX_importance = X_test\n\n# Explain model predictions using shap library:\nexplainer = shap.TreeExplainer(lgbfit)\nshap_values = explainer.shap_values(X_importance)\nshap.summary_plot(shap_values, X_importance, agg_market_df[fcol].columns)\nshap.summary_plot(shap_values, X_importance, agg_market_df[fcol].columns, plot_type='bar')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}