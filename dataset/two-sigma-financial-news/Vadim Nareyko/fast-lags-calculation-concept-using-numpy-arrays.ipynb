{"cells":[{"metadata":{"_uuid":"ab78c671331f03bca78e8b09af34cab9adedf6fe"},"cell_type":"markdown","source":"This kernel shows how to prepare lags separately for train and test phases"},{"metadata":{"trusted":true,"_uuid":"fe2fd9c897a1b59e4fd7908bcebe07916f78dce1"},"cell_type":"code","source":"# Locally I created classes with a similar behaviour that we have in this competition\n# It slowed me down during validation phase and I decided to refactor code for lags creation\n\n# from twosigmanews import *\nfrom kaggle.competitions import twosigmanews","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport gc\nfrom resource import getrusage, RUSAGE_SELF\nfrom datetime import date, datetime\n\nimport multiprocessing\nfrom multiprocessing import Pool, cpu_count\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"global STARTED_TIME\nSTARTED_TIME = datetime.now()\n\n# It's better to use cpu_count from the system - who knows what happens during test phase\nglobal N_THREADS\nN_THREADS=multiprocessing.cpu_count()\n\nprint(f'N_THREADS: {N_THREADS}')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# FILTERDATE - start date for the train data\nFILTERDATE = date(2007, 1, 1)\n\n# SAMPLEDATE - I use it for sampling and fast sanity check of scripts\nSAMPLEDATE = None\n# SAMPLEDATE = date(2007, 1, 30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0bea66838a06e3a7a9036e0ca13e5790d2102e01"},"cell_type":"code","source":"global N_LAG, RETURN_FEATURES\n\n# Let's try how it works for 1-year lags\nN_LAG = np.sort([5, 10, 20, 252])\n\n# Features for lags calculation\nRETURN_FEATURES = [\n    'returnsOpenPrevMktres10',\n    'returnsOpenPrevRaw10',\n    'open',\n    'close']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e48594ab7071aa960560b10b83cba87291b0e995"},"cell_type":"code","source":"# Tracking time and memory usage\nglobal MAXRSS\nMAXRSS = getrusage(RUSAGE_SELF).ru_maxrss\ndef using(point=\"\"):\n    global MAXRSS, STARTED_TIME\n    print(str(datetime.now()-STARTED_TIME).split('.')[0], point, end=' ')\n    max_rss = getrusage(RUSAGE_SELF).ru_maxrss\n    if max_rss > MAXRSS:\n        MAXRSS = max_rss\n    print(f'max RSS {MAXRSS/1024/1024:.1f}Gib')\n    gc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4cd8ce7429b04098a560006b4cedbc5df9c506fb"},"cell_type":"code","source":"# I've slightly optimized this function from https://www.kaggle.com/qqgeogor/eda-script-67\n# It uses just one loop ofr N_LAG\n\nglobal FILLNA\nFILLNA = -1\n\ndef create_lag(df_code):\n    prevlag = 1    \n    for window in N_LAG:\n        rolled = df_code[RETURN_FEATURES].shift(prevlag).rolling(window=window)\n        # Mean is not so stable as median if you have assets with very high/low beta risk factor\n        # df_code = df_code.join(rolled.mean().add_suffix(f'_lag_{window}_mean'))\n        df_code = df_code.join(rolled.median().add_suffix(f'_lag_{window}_median'))\n        df_code = df_code.join(rolled.max().add_suffix(f'_lag_{window}_max'))\n        df_code = df_code.join(rolled.min().add_suffix(f'_lag_{window}_min'))\n\n        # We also have an idea to make lags uncorrelated - in this case you may uncomment\n        # the following line. N_LAG have to be sorted\n\n        # prevlag = window\n    return df_code.fillna(FILLNA)\n\ndef generate_lag_features(df):\n    global RETURN_FEATURES, N_THREADS\n    all_df = []\n    df_codes = df.groupby('assetCode')\n    df_codes = [df_code[1][['time','assetCode']+RETURN_FEATURES] for df_code in df_codes]\n    \n    pool = Pool(N_THREADS)\n    all_df = pool.map(create_lag, df_codes)\n    \n    new_df = pd.concat(all_df)  \n    new_df.drop(RETURN_FEATURES,axis=1,inplace=True)\n    pool.close()\n    \n    return new_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd23eef7934857d8b025ab68cd0b2a87e97b4b75"},"cell_type":"code","source":"# Pre-processing of dataframe, this functions is the same for train and test periods\n# In production we had more calculations\n\ndef preparedf(df):\n    df['time'] = df['time'].dt.date\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e4d25bc67d59846444c0e43874caa64d0c7a151"},"cell_type":"code","source":"# The following functions are used for initialization and expanding of numpy arrays\n# for storing historical data of all assets.\n\n# It helps to have very fast lags creation.\n\n# Initialization of history array\ndef initialize_values(items=5000, features=4, history=15):\n    return np.ones((items, features, history))*np.nan\n\n# Expanding of history array for new assets\ndef add_values(a, items=100):\n    return np.concatenate([a, initialize_values(items, a.shape[1], a.shape[2])])\n\n# codes dictionary maps assetCode to the index in the history array\n# if we found new code - we have to store it and expand history\ndef get_code(a):\n    global codes, history\n    try: \n        return codes[a]\n    except KeyError:\n        codes[a] = len(codes)\n        if len(codes) > history.shape[0]:\n            history = add_values(history, 100)\n        return codes[a]\n\n# list2codes returns numpy array of indexes of assetCodes (for each day)\ndef list2codes(l):\n    return np.array([get_code(a) for a in l])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5d51b46657369f8eebe53b235ab2e616fddda4b"},"cell_type":"markdown","source":"Let's start"},{"metadata":{"trusted":true,"_uuid":"6081b1fd43a5d854a2768154f23fd2f91c1425d6"},"cell_type":"code","source":"env = twosigmanews.make_env()\n(market_train_df, news_train_df) = env.get_training_data()\nusing('Done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eef75e58b7c48232101b83c1090ac7feaf56e398"},"cell_type":"code","source":"print('Dataframe pre-processing')\nmarket_train_df = preparedf(market_train_df)\nusing('Done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1763239aac5612acbe20cbc5aeaee00ca3bcde8"},"cell_type":"code","source":"# Dataframe filtering\nprint('DF Filtering')\nmarket_train_df = market_train_df.loc[market_train_df['time']>=FILTERDATE]\n\nif SAMPLEDATE is not None:\n    market_train_df = market_train_df.loc[market_train_df['time']<=SAMPLEDATE]  \nusing('Done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a88652b9b21b2ab8081c651163f4c76840c6fa2"},"cell_type":"code","source":"print('Lag features generation')\nnew_df = generate_lag_features(market_train_df)\nusing('Done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41dae4ef181050b76edee6bc0bffc3eb7034d587"},"cell_type":"code","source":"print('DF Merging')\nmarket_train_df = pd.merge(market_train_df,new_df,how='left',on=['time','assetCode'])\ndel new_df\nusing('Done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b5e34a28d9aecf22a955acebdc8f79690060be3"},"cell_type":"code","source":"print('Preparation for the prediction')\n# codes maps assetCodes with indexes into history array\ncodes = dict(\n    zip(market_train_df.assetCode.unique(), np.arange(market_train_df.assetCode.nunique()))\n)\n# history stores information for all assets, required features\n# np.max(LAG)+1 - to store information for maximum beriod and the current day (+1)\nhistory = initialize_values(len(codes), len(RETURN_FEATURES), np.max(N_LAG)+1)\n\n# Get the latest information for assets\nlatest_events = market_train_df.groupby('assetCode').tail(np.max(N_LAG)+1)\n# but we may have different informations size for different assets\nlatest_events_size = latest_events.groupby('assetCode').size()\n\n# Filling the history array\nfor s in latest_events_size.unique():\n    for i in range(len(RETURN_FEATURES)):\n        # l is a Dataframe with assets with the same history size for each asset\n        l = latest_events[\n            latest_events.assetCode.isin(latest_events_size[latest_events_size==s].index.values)\n        ].groupby('assetCode')[RETURN_FEATURES[i]].apply(list)\n\n        # v is a 2D array contains history information of feature RETURN_FEATURES[i] \n        v = np.array([k for k in l.values])\n\n        # r contains indexes (in the history array) of all assets\n        r = list2codes(l.index.values)\n\n        # Finally, filling history array\n        history[r, i, -s:] = v\n        del l, v, r\n\ndel latest_events, latest_events_size\nusing('Done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11245043acf408ee4869995c7dcd49c3826c789e"},"cell_type":"code","source":"print('Prediction')\n#prediction\ndays = env.get_prediction_days()\nn_days = 0\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    n_days +=1\n    if n_days % 100 == 0:\n        using(f'{n_days}')\n    # Test data preprocessing    \n    market_obs_df = preparedf(market_obs_df)\n\n    # Getting indexes of asses for the current day\n    r = list2codes(market_obs_df.assetCode.values)\n\n    # Shifting history by 1 for assets of the current day\n    history[r, :, :-1] = history[r, :, 1:] \n\n    # Filling history with a new data\n    history[r, :, -1] = market_obs_df[RETURN_FEATURES].values\n\n    prevlag = 1    \n    for lag in np.sort(N_LAG):\n        lag_median = np.median(history[r, : , -lag:-prevlag], axis=2)\n        lag_median[np.isnan(lag_median)] = FILLNA\n        lag_max = history[r, : , -lag-1:-prevlag].max(axis=2)\n        lag_max[np.isnan(lag_max)] = FILLNA\n        lag_min = history[r, : , -lag-1:-prevlag].min(axis=2)\n        lag_min[np.isnan(lag_min)] = FILLNA\n\n        for ix in range(len(RETURN_FEATURES)):\n            market_obs_df[f'{RETURN_FEATURES[ix]}_lag_{lag}_median'] = lag_median[:, ix]\n            market_obs_df[f'{RETURN_FEATURES[ix]}_lag_{lag}_min'] = lag_min[:, ix]\n            market_obs_df[f'{RETURN_FEATURES[ix]}_lag_{lag}_max'] = lag_max[:, ix]\n            \n#         prevlag=lag\n\n    confidence = 0\n    \n    preds = pd.DataFrame({'assetCode':market_obs_df['assetCode'],'confidence':confidence})\n\n    predictions_template_df = predictions_template_df.merge(preds,how='left')\\\n    .drop('confidenceValue',axis=1)\\\n    .fillna(0)\\\n    .rename(columns={'confidence':'confidenceValue'})\n    \n    env.predict(predictions_template_df)\n    \nusing('Prediction done')\n\n# env.write_submission_file()\n# using('Done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb7ac24b1402c8b4ad96d57c07d7e16e89b01fe1"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}