{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n%matplotlib inline\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom itertools import chain\nimport gc\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n#import os\n#print(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":true},"cell_type":"code","source":"from kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ad95c5db771aded6b2cdbd029e1d32acd040bbf"},"cell_type":"code","source":"(market_train_df, news_train_df) = env.get_training_data()\n#del env","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e493a6b4ab4c5a1b720a527fc47fab273d8bcd11"},"cell_type":"code","source":"# increase the number of displayed columns in dataframes\npd.set_option('display.max_columns', 100)\n# Check if the run is for testing the code or final results.\n# if checking the code then reduce the size of datasets for memory saving.\ntest_code = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ddd6f1503e5a21083467b845ca497f253db451f2"},"cell_type":"code","source":"if test_code:\n    market_train_df = market_train_df.head(100000)\n    news_train_df = news_train_df.head(300000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"abc2c60b91a85e83e393e0f9bd5e56584bb683fc"},"cell_type":"code","source":"market_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"98ba4f461c70a00bc71a1c5bcd7a8999cea1ecf5"},"cell_type":"code","source":"# Categorizing and encoding the object type columns\ndef categorize(df):\n    obj_df = df.select_dtypes(exclude=[np.int8, np.int16, np.int32, np.float32]).drop(['assetCode', 'time'], axis=1).astype('category')\n    for col in obj_df.columns:\n        df[col] = obj_df[col].cat.codes\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5a2c5f6c9a73b8fcfbc13723cc3bfe1b2e75540"},"cell_type":"code","source":"# Scaler for numerical numbers\nfrom sklearn.preprocessing import MinMaxScaler\nstd = MinMaxScaler()\ndef scaler(df):\n    arr = df.select_dtypes(exclude=[object])\n    temp = pd.DataFrame(std.fit_transform(arr), columns=arr.columns)\n    temp = temp.astype(np.float16)\n    df[arr.columns] = temp\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2dfcc7ca9d6b91ed8cc30f153be5e37fe56d7e7"},"cell_type":"code","source":"## Functions for preparing the data. The code should run in 1-2 minutes for the full dataset.\n\ndef prepare_news(news_df):   \n    agg_dict = {\n        'sourceId' : ['count'],\n        'urgency': ['min'],\n        'takeSequence': ['max'],\n        'provider' : ['count'],\n        'bodySize': ['mean'],\n        'wordCount': ['mean'],\n        'sentenceCount': ['mean'],\n        'companyCount': ['mean'],\n        'relevance': ['mean'],\n        'sentimentClass' : ['mean'],\n        'sentimentNegative': ['mean'],\n        'sentimentNeutral': ['mean'],\n        'sentimentPositive': ['mean'],\n        'sentimentWordCount': ['mean'],\n        'noveltyCount12H': ['mean'],\n        'noveltyCount24H': ['mean'],\n        'noveltyCount3D': ['mean'],\n        'noveltyCount5D': ['mean'],\n        'noveltyCount7D': ['mean'],\n        'volumeCounts12H': ['mean'],\n        'volumeCounts24H': ['mean'],\n        'volumeCounts3D': ['mean'],\n        'volumeCounts5D': ['mean'],\n        'volumeCounts7D': ['mean']\n    }\n\n    print('Preparing \"news_train_df\".')\n    #code_list = news_df['assetCodes'].apply(lambda x: x.strip('{}').replace(',', ' ').replace('\\'','').split())\n    code_list = news_df['assetCodes'].str.findall(f\"'([\\w\\./]+)'\")\n    code_list_ext = list(chain(*code_list))\n    code_name_ext = news_df['assetName'].repeat(code_list.apply(len))\n    code_time_ext = news_df['time'].repeat(code_list.apply(len))\n    code_list_ext = pd.Series(code_list_ext, name='assetCode')\n    code_df_ext = pd.DataFrame(code_list_ext).reset_index(drop=True)\n    name_df_ext = pd.DataFrame(code_name_ext).reset_index(drop=True)\n    time_df_ext = pd.DataFrame(code_time_ext).reset_index(drop=True)\n    code_df_ext = code_df_ext.join(name_df_ext).join(time_df_ext)\n    \n    del time_df_ext, name_df_ext, code_list, code_name_ext, code_time_ext, code_list_ext \n    gc.collect()\n\n    print('Extending the \"news_train_df\".')\n    \n    news_df = news_df.drop('assetCodes', axis=1)  \n    news_df = news_df.merge(code_df_ext, on=['assetName', 'time'], how='inner')\n    \n    del code_df_ext\n    print('\"news_train_df\" extended.')\n\n    news_df['time'] = news_df['time'].dt.date\n\n    print('Converted date_time to date only.')\n    \n    list_to_drop = ['headline', 'subjects', 'audiences', 'assetName', 'headlineTag', 'marketCommentary', \n                    'firstMentionSentence', 'firstCreated', 'sourceTimestamp']\n    for col in list_to_drop:\n        if col in news_df.columns:\n            news_df = news_df.drop(col, axis=1)\n    print('Cleaned the \"news_train_df\".')\n    \n    news_df = news_df.groupby(['assetCode', 'time']).agg(agg_dict).reset_index()\n    news_df.columns = news_df.columns.get_level_values(0)\n\n    print('Combined the \"news_train_df\" different hours of a day into one day.')\n    print('Finished preparing \"news_train_df\".')\n    return news_df\n\ndef prepare_market(market_df):\n    print('Preparing \"market_train_df\".')\n    market_df = market_df.drop('assetName', axis=1)\n    \n    print('Cleaning ...')\n    market_df = market_df.dropna(axis = 1, how='all')\n    market_df = market_df[market_df['universe'] == 1]\n\n    print('Cleaning \"market_train_df\" finished.')\n    market_df['time'] =  market_df['time'].dt.date\n    \n    print('Finished preparing  market_train_df')\n    return market_df\n\ndef create_full_dataframe(news_df, market_df):\n    \n    news = prepare_news(news_df)\n    market = prepare_market(market_df)\n    \n    m = pd.DataFrame(market['assetCode'].unique()).rename(columns={0 : 'assetCode'})\n    n = pd.DataFrame(news['assetCode'].unique()).rename(columns={0 : 'assetCode'})\n    shared_assets_pd = pd.DataFrame(m.merge(n, on='assetCode', how='inner')['assetCode'])\n    \n    print('Created a dataframe of shared assets between the market and news dataframes')\n    \n    news = news.merge(shared_assets_pd.reset_index(), on='assetCode', how='inner')\n    \n    market = market.merge(shared_assets_pd.reset_index(), on='assetCode', how='inner')\n    \n    print('Created a news and market dataframe only with shared assets')\n    \n    full_df = news.merge(market, on=['assetCode', 'time'], how='outer')\n    \n    del market, news, shared_assets_pd, m, n\n    gc.collect()\n    \n    full_df = full_df.sort_values(by=['assetCode', 'time']).reset_index(drop=True)\n    \n    print('Merged the news and market datframes into one full dataframe with shared assets, retaining the times from both')\n    print('Finished preparing data.')\n    return full_df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"b11f6beb3ff92eaaef9462c5bcb7b620d36fff3b"},"cell_type":"code","source":"assets_df = create_full_dataframe(news_train_df,market_train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7bdfd565f00edc07138e1783734d7e3ec8502b21"},"cell_type":"code","source":"plt.figure(1, figsize=(15, 8))\nplt.xticks(rotation='75')\nplt.xlabel('Time')\nplt.ylabel('Price')\nplt.plot(assets_df[assets_df['assetCode'] == 'A.N']['time'], \n         scaler(pd.DataFrame(assets_df[assets_df['assetCode'] == 'A.N']['close'])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d4406e653b5256b784eed601038341b482dbd82"},"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nclass model(object):\n    def __init__(self, asset_df = None,\n                 status='close', \n                 roll_window_size = 10,\n                 ewm_half_life = 1,\n                 decomp_model = 'multiplicative',\n                 trend_deg = 3,\n                 l=0,\n                 u=None):\n        \n        self.asset_df = asset_df\n        self.status = status\n        self.roll_window_size = roll_window_size\n        self.ewm_half_life = ewm_half_life\n        self.decomp_model = decomp_model\n        self.trend_deg = trend_deg\n        self.coef, self.t_coef, self.price = self.get_rolling_coef(l=l, u=u)\n        \n    ## Performs rolling window averaging on the time series and decomposes the data into its trend, seasonality and residuals.\n    ## Residuals are the data we want to explore and predict abd then we add back the trend and seasonality\n        \n    def get_rolling_price(self):\n        price = self.asset_df[['time', self.status]].dropna()\n        price[self.status] = price[self.status].map(lambda x: np.log(x))\n        price[self.status] = std.fit_transform(np.reshape(np.array(price[self.status]), (-1, 1)))\n        #price[self.status] = price[self.status] - price[self.status].shift()\n        price[self.status]  = price[self.status].ewm(halflife=self.ewm_half_life).mean().rolling(window=self.roll_window_size, win_type='blackman').mean()\n        price['time'] = pd.to_datetime(price['time'])\n        price = price.dropna().set_index('time').asfreq('d').fillna(method='ffill')\n        decomposition = seasonal_decompose(price, model=self.decomp_model)\n        price['resid'] = decomposition.resid\n        price['trend'] = decomposition.trend\n        price['seasonal'] = decomposition.seasonal\n        price = price.dropna()\n        return price.reset_index()\n    \n    ## Adfuller test for stationary time series, Time series should be stationary to ensure that the series distribution parameters are consistent.\n    def test_stationary(self, time_series):\n        print('Results of Dickey-Fuller test:')\n        dftest = adfuller(time_series, autolag='AIC')\n        dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n        for key,value in dftest[4].items():\n            dfoutput['Critical Value (%s)'%key] = value\n        print(dfoutput)\n        \n    ## Rolling window regression for the residual and the trend. This is used to predict the future trend and the asset price.\n    def get_rolling_coef(self, l, u):\n        time_series = self.get_rolling_price()\n        time_series = time_series.reset_index()[l:u]\n        self.test_stationary(time_series.resid)\n        p_splits = [time_series['resid'].shift(x).values[::-1][:self.roll_window_size] for x in range(len(time_series['resid']))[::-1]]\n        t_splits = [time_series['index'].shift(x).values[::-1][:self.roll_window_size] for x in range(len(time_series['index']))[::-1]]\n        size = p_splits[0].shape[0]\n        rolling_coef = []\n        for c in range(len(p_splits)):\n            if c >= size - 1:\n                coef = np.polyfit(t_splits[c], p_splits[c], 1)\n                rolling_coef.append(coef[0])\n        price_coef = np.polyfit(np.arange(0, len(rolling_coef)), rolling_coef, 1)\n        trend_coef = np.polyfit(np.arange(0, len(time_series.trend)), time_series.trend, self.trend_deg)\n        return price_coef, trend_coef, time_series\n    \n    ## Estimates the price at each time point. (for trainig and prediction) \n    ## Seasonality is periodical so we already know the value for it at each time point and there is no need for prediction.\n    def estimate_price(self):\n        coef, t_coef, price = self.coef, self.t_coef, self.price\n        season = price.seasonal.where(price.seasonal.duplicated() == False).dropna()\n        fit = np.poly1d(coef)\n        split_coef = pd.Series(fit(price['index']))\n        eprice = (split_coef * price['index']).rename('estimated_price')\n        fit = np.poly1d(t_coef)\n        trend = pd.Series(fit(price['index']))\n        new_index = pd.Series(price.index).apply(lambda x: x % len(season))\n        time = price.time\n        eprice = eprice + new_index.apply(lambda x: season[x])\n        eprice = eprice + trend - 1\n        eprice = pd.concat((time, eprice.rename('estimated_price')), axis=1)\n        return eprice\n    \n    def predict(self, time_points=None):\n        coef, t_coef, price = self.coef, self.t_coef, self.price       \n        season = price.seasonal.where(price.seasonal.duplicated() == False).dropna()\n        fit = np.poly1d(coef)\n        split_coef = pd.Series(fit(time_points))\n        eprice = (split_coef * time_points).rename('estimated_price')\n        fit = np.poly1d(t_coef)\n        trend = pd.Series(fit(time_points))\n        new_index = pd.Series(time_points).apply(lambda x: x % len(season))\n        time = pd.Series(pd.date_range(price.time[len(price.time) - 1], periods=len(time_points))).rename('time')\n        eprice = eprice + new_index.apply(lambda x: season[x])\n        eprice = eprice + trend - 1\n        eprice = pd.concat((time, eprice.rename('estimated_price')), axis=1)  \n        return eprice","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a89d3021ee5107193e1b766b7b7d6de0d5fed5ea","scrolled":true},"cell_type":"code","source":"status = 'close'\nroll_size = 30\nhalflife = 1\ndeg = 3\n\nmo = model(assets_df[assets_df['assetCode'] == 'A.N'], \n           status=status,\n          roll_window_size=roll_size,\n          ewm_half_life=halflife,\n          trend_deg=deg)\n\nprice = mo.get_rolling_price()\neprice = mo.estimate_price()\ntarget = pd.Series(np.arange(len(eprice) - 1, len(eprice) + 15), name='time_points')\ntarget_price = mo.predict(time_points=target)\n\nplt.figure(2, figsize=(8, 4))\nplt.xticks(rotation='75')\nplt.xlabel('Time')\nplt.ylabel('Price')\nplt.plot(price.time, price.close, label='Real data')\nplt.plot(eprice.time, eprice.estimated_price, label='Estimated price for the training set')\nplt.plot(target_price.time, target_price.estimated_price, label='Predicted price for the future')\nplt.legend()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d45222c39d9f5d7da01ea37c7d3aeb4edd155fa"},"cell_type":"code","source":"#new_index = pd.Series(price.index).apply(lambda x: x % np.floor(len(price.seasonal)/len(season)).astype(np.int8))\n#eprice + new_index.apply(lambda x: x + price.seasonal[x])\n#print(price.seasonal + eprice)\n#print(new_index.apply(lambda x: x + price.seasonal[x]) + eprice)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}