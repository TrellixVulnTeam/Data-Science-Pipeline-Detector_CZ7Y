{"cells":[{"metadata":{"_uuid":"ac799c39c0b8381025181dd7a1bf8d0bb9c42658"},"cell_type":"markdown","source":"<h2>Introduction and loading data</h2>\n\nThere are two sets of data in this 'kernels only' competition: News and Prices/Returns. The ideia is to use both sets to predict the movement of a given financial asset in the next 10 days. We have data from 2007 to 2017 for training and must predict the movement of assets from Jan 2017 to July 2019."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# load data from twosigma\nfrom kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()\n(marketdf, newsdf) = env.get_training_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c257b0e69b89c35a244c70661e8876790937c06f"},"cell_type":"code","source":"import lightgbm as lgb\nfrom datetime import datetime, date\nfrom sklearn import model_selection\nfrom sklearn.metrics import mean_squared_error\nimport time\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport datetime\nfrom scipy import stats\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.model_selection import train_test_split\nfrom wordcloud import WordCloud\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nstop = set(stopwords.words('english'))\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3761abd8a700facb98a0a5aba064d80772caf8e8"},"cell_type":"code","source":"marketdf.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cbf1279bdfefd022fda993d5d1447a5cc3561e18"},"cell_type":"code","source":"newsdf.head().T","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"464ae05a756ffe237662fc1bdcddcf6869a746a0"},"cell_type":"markdown","source":"<h2>Exploratory Data Analysis (EDA) on Market data and News data</h2>"},{"metadata":{"_uuid":"3e487619d2179da6054cd5e65413bc0ccdd98f2e"},"cell_type":"markdown","source":"## Market data\nWe can see long-term trends, appearing and declining companies and many other things.\n\nAt first let's take 10 random assets and plot them."},{"metadata":{"_kg_hide-output":false,"trusted":true,"_uuid":"c262631947f3e9d41fff8502dcc79e93b53286af"},"cell_type":"code","source":"data = []\nfor asset in np.random.choice(marketdf['assetName'].unique(), 10):\n    asset_df = marketdf[(marketdf['assetName'] == asset)]\n\n    data.append(go.Scatter(\n        x = asset_df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n        y = asset_df['close'].values,\n        name = asset\n    ))\nlayout = go.Layout(dict(title = \"Closing prices of 10 random assets\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"))\npy.iplot(dict(data=data, layout=layout), filename='basic-line')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5e351b6bd556ea02d0025e0f5648e434c13e84ad"},"cell_type":"markdown","source":"It would be more interesting to see general trends of prices."},{"metadata":{"trusted":true,"_uuid":"676450eb9874da398e5d8bdfedfe64d9ef5575b0"},"cell_type":"code","source":"data = []\nfor i in [0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95]:\n    price_df = marketdf.groupby('time')['close'].quantile(i).reset_index()\n\n    data.append(go.Scatter(\n        x = price_df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n        y = price_df['close'].values,\n        name = f'{i} quantile'\n    ))\nlayout = go.Layout(dict(title = \"Trends of closing prices by quantiles\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"),\n    annotations=[\n        dict(\n            x='2008-09-01 22:00:00+0000',\n            y=82,\n            xref='x',\n            yref='y',\n            text='Collapse of Lehman Brothers',\n            showarrow=True,\n            font=dict(\n                family='Courier New, monospace',\n                size=16,\n                color='#ffffff'\n            ),\n            align='center',\n            arrowhead=2,\n            arrowsize=1,\n            arrowwidth=2,\n            arrowcolor='#636363',\n            ax=20,\n            ay=-30,\n            bordercolor='#c7c7c7',\n            borderwidth=2,\n            borderpad=4,\n            bgcolor='#ff7f0e',\n            opacity=0.8\n        ),\n        dict(\n            x='2011-08-01 22:00:00+0000',\n            y=85,\n            xref='x',\n            yref='y',\n            text='Black Monday',\n            showarrow=True,\n            font=dict(\n                family='Courier New, monospace',\n                size=16,\n                color='#ffffff'\n            ),\n            align='center',\n            arrowhead=2,\n            arrowsize=1,\n            arrowwidth=2,\n            arrowcolor='#636363',\n            ax=20,\n            ay=-30,\n            bordercolor='#c7c7c7',\n            borderwidth=2,\n            borderpad=4,\n            bgcolor='#ff7f0e',\n            opacity=0.8\n        ),\n        dict(\n            x='2014-10-01 22:00:00+0000',\n            y=120,\n            xref='x',\n            yref='y',\n            text='Another crisis',\n            showarrow=True,\n            font=dict(\n                family='Courier New, monospace',\n                size=16,\n                color='#ffffff'\n            ),\n            align='center',\n            arrowhead=2,\n            arrowsize=1,\n            arrowwidth=2,\n            arrowcolor='#636363',\n            ax=-20,\n            ay=-30,\n            bordercolor='#c7c7c7',\n            borderwidth=2,\n            borderpad=4,\n            bgcolor='#ff7f0e',\n            opacity=0.8\n        ),\n        dict(\n            x='2016-01-01 22:00:00+0000',\n            y=120,\n            xref='x',\n            yref='y',\n            text='Oil prices crash',\n            showarrow=True,\n            font=dict(\n                family='Courier New, monospace',\n                size=16,\n                color='#ffffff'\n            ),\n            align='center',\n            arrowhead=2,\n            arrowsize=1,\n            arrowwidth=2,\n            arrowcolor='#636363',\n            ax=20,\n            ay=-30,\n            bordercolor='#c7c7c7',\n            borderwidth=2,\n            borderpad=4,\n            bgcolor='#ff7f0e',\n            opacity=0.8\n        )\n    ])\npy.iplot(dict(data=data, layout=layout), filename='basic-line')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad2f0e06b62c41f17f7818f9f5c1570834061cf6"},"cell_type":"code","source":"marketdf['price_diff'] = marketdf['close'] - marketdf['open']\ngrouped = marketdf.groupby('time').agg({'price_diff': ['std', 'min']}).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6ff905e4ee3e552dac002cff774636ca342e0e4"},"cell_type":"code","source":"print(f\"Average standard deviation of price change within a day in {grouped['price_diff']['std'].mean():.4f}.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05211611d6bed9c0b126b059311850e79fbf5156"},"cell_type":"code","source":"g = grouped.sort_values(('price_diff', 'std'), ascending=False)[:10]\ng['min_text'] = 'Maximum price drop: ' + (-1 * g['price_diff']['min']).astype(str)\ntrace = go.Scatter(\n    x = g['time'].dt.strftime(date_format='%Y-%m-%d').values,\n    y = g['price_diff']['std'].values,\n    mode='markers',\n    marker=dict(\n        size = g['price_diff']['std'].values,\n        color = g['price_diff']['std'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = g['min_text'].values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Top 10 months by standard deviation of price change within a day',\n    hovermode= 'closest',\n    yaxis=dict(\n        title= 'price_diff',\n        ticklen= 5,\n        gridwidth= 2,\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64f0e5b66268f3a01d7daf37d88480e73175981a"},"cell_type":"markdown","source":"We can see huge price fluctiations when market crashed. But this is wrong... There was no huge crash on January 2010... Let's dive into the data."},{"metadata":{"_uuid":"54884e52cc4c99156dd2e3d2c674a43a8ba3f37b"},"cell_type":"markdown","source":"### Possible data errors\n\nAt first let's simply sort data by the difference between open and close prices."},{"metadata":{"trusted":true,"_uuid":"328ac0026692de5eb0ecf5db22ef209d2ea985f8"},"cell_type":"code","source":"marketdf.sort_values('price_diff')[:10]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c1e20e5b617473fb6c36e65bfccf59f598e8cec"},"cell_type":"markdown","source":"So, let's try to find strange cases."},{"metadata":{"trusted":true,"_uuid":"89b9b042c8bcb96851ff1361869b4124e1fa08f2"},"cell_type":"code","source":"marketdf['close_to_open'] =  np.abs(marketdf['close'] / marketdf['open'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e40c43b6efbcf68691c4eb475d04cd348ae5a3f"},"cell_type":"code","source":"print(f\"In {(marketdf['close_to_open'] >= 1.2).sum()} lines price increased by 20% or more.\")\nprint(f\"In {(marketdf['close_to_open'] <= 0.8).sum()} lines price decreased by 20% or more.\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"86fe7c9d8e387d5ce444accec9e3e8598d42377d"},"cell_type":"markdown","source":"Well, this isn't much considering we have more than 4 million lines and a lot of these cases are due to price falls during market crash. Well just need to deal with outliers."},{"metadata":{"trusted":true,"_uuid":"bdc23441daecdae2866ad470eac62d2c7cfbec82"},"cell_type":"code","source":"print(f\"In {(marketdf['close_to_open'] >= 2).sum()} lines price increased by 100% or more.\")\nprint(f\"In {(marketdf['close_to_open'] <= 0.5).sum()} lines price decreased by 100% or more.\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fbe2e1860c66a62eddbbac9514852aa4b00f996e"},"cell_type":"markdown","source":"For a quick fix I'll replace outliers in these lines with mean open or close price of this company."},{"metadata":{"trusted":true,"_uuid":"a90b8295681645986ec7663e7a7e6cea173a2a75"},"cell_type":"code","source":"marketdf['assetName_mean_open'] = marketdf.groupby('assetName')['open'].transform('mean')\nmarketdf['assetName_mean_close'] = marketdf.groupby('assetName')['close'].transform('mean')\n\n# if open price is too far from mean open price for this company, replace it. Otherwise replace close price.\nfor i, row in marketdf.loc[marketdf['close_to_open'] >= 2].iterrows():\n    if np.abs(row['assetName_mean_open'] - row['open']) > np.abs(row['assetName_mean_close'] - row['close']):\n        marketdf.iloc[i,5] = row['assetName_mean_open']\n    else:\n        marketdf.iloc[i,4] = row['assetName_mean_close']\n        \nfor i, row in marketdf.loc[marketdf['close_to_open'] <= 0.5].iterrows():\n    if np.abs(row['assetName_mean_open'] - row['open']) > np.abs(row['assetName_mean_close'] - row['close']):\n        marketdf.iloc[i,5] = row['assetName_mean_open']\n    else:\n        marketdf.iloc[i,4] = row['assetName_mean_close']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7eaae2ab340121e83581a4af9107c0a9df3ea57f"},"cell_type":"code","source":"marketdf['price_diff'] = marketdf['close'] - marketdf['open']\ngrouped = marketdf.groupby(['time']).agg({'price_diff': ['std', 'min']}).reset_index()\ng = grouped.sort_values(('price_diff', 'std'), ascending=False)[:10]\ng['min_text'] = 'Maximum price drop: ' + (-1 * np.round(g['price_diff']['min'], 2)).astype(str)\ntrace = go.Scatter(\n    x = g['time'].dt.strftime(date_format='%Y-%m-%d').values,\n    y = g['price_diff']['std'].values,\n    mode='markers',\n    marker=dict(\n        size = g['price_diff']['std'].values * 5,\n        color = g['price_diff']['std'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = g['min_text'].values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Top 10 months by standard deviation of price change within a day',\n    hovermode= 'closest',\n    yaxis=dict(\n        title= 'price_diff',\n        ticklen= 5,\n        gridwidth= 2,\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c003f3dbf053a4d3dc885f536da0af239389779e"},"cell_type":"markdown","source":"Now the graph is much more reasonable."},{"metadata":{"trusted":true,"_uuid":"14623cb1e854fac42d74be7f013d1f36fc1bc27d"},"cell_type":"code","source":"data = []\nfor i in [0.1, 0.25, 0.5, 0.75, 0.9, 0.95]:\n    price_df = marketdf.groupby('time')['returnsOpenNextMktres10'].quantile(i).reset_index()\n\n    data.append(go.Scatter(\n        x = price_df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n        y = price_df['returnsOpenNextMktres10'].values,\n        name = f'{i} quantile'\n    ))\nlayout = go.Layout(dict(title = \"Trends of returnsOpenNextMktres10 by quantiles\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"),)\npy.iplot(dict(data=data, layout=layout), filename='basic-line')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"775691966542b459ef1537ec23e3a184ae6ede97"},"cell_type":"markdown","source":"### Let's see some trends of additional features like MACD, Z score, MA, EMA..."},{"metadata":{"trusted":true,"_uuid":"bbe023713b399266fa82051bc3f683794bc4717f"},"cell_type":"code","source":"marketdf['close_to_open'] = marketdf['close'] / marketdf['open']\n# DIF-MACD\nma_12 = lambda x: x.rolling(12).mean()\nma_26 = lambda x: x.rolling(26).mean()\nmarketdf['DIF'] = marketdf.groupby('assetCode')['close'].transform(lambda x : pd.Series.ewm(x, span=12).mean())-marketdf.groupby('assetCode')['close'].transform(lambda x : pd.Series.ewm(x, span=26).mean())\nmarketdf['MACD'] = marketdf['DIF'].transform(lambda x : pd.Series.ewm(x, span=9).mean())\nmarketdf['OSC'] = marketdf['DIF']-marketdf['MACD']\n# Z score: 200, 20\nzscore_fun_improved = lambda x:(x - x.rolling(window=15, min_periods=7).mean())/x.rolling(window=15, min_periods=7).std()\nmarketdf['zscore'] = marketdf.groupby('assetCode')['close'].apply(zscore_fun_improved)\nmarketdf['average'] = (marketdf['close'] + marketdf['open'])/2\nmarketdf['price_volume'] = marketdf['volume'] * marketdf['close']\nmarketdf['volume_to_mean'] = marketdf['volume'] / marketdf['volume'].mean()\n# time series rolling based features: mean, std, ewm\nwindows = [7,14]\n#f = ['open','close','returnsOpenPrevMktres10', 'returnsClosePrevMktres10']\nf = ['returnsOpenPrevMktres10']\nfor ff in f:\n    for d in windows:\n        marketdf['%s_%s_mean'%(ff,d)] = marketdf.groupby('assetCode')[ff].apply(lambda x: x.rolling(d).mean())\n        #marketdf['%s_%s_std'%(ff,d)] = marketdf.groupby('assetCode')[ff].apply(lambda x: x.rolling(d).std())\n        marketdf['%s_%s_ewm'%(ff,d)] = marketdf.groupby('assetCode')[ff].transform(lambda x : pd.Series.ewm(x, span=d).mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11fa34b81c7cd3486a68266b9a99928d37d6f37a"},"cell_type":"code","source":"marketdf.head().T","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d160b583a33a5b441fdd31244530e64d31e9b4e6"},"cell_type":"markdown","source":"We can see that quantiles have a high deviation, but mean value doesn't change much.\n\nNow I think it is time to throw an old part of dataset. Let's leave only data since 2010 year, this way we will get rid of the data of the biggest crisis."},{"metadata":{"trusted":true,"_uuid":"c6b4dea35541ba4cc78361fa273daf03f50c84a8"},"cell_type":"code","source":"data = []\nmarketdf = marketdf.loc[marketdf['assetCode'] == 'AAPL.O']\nmarketdf = marketdf.loc[marketdf['time'] >= '2014-01-01 22:00:00+0000']\nfeatures = ['close']\nfor f in features:\n    price_df = marketdf.groupby('time')[f].mean().reset_index()\n\n    data.append(go.Scatter(\n        x = price_df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n        y = price_df[f].values,\n        name = f'%s'%f\n    ))\nlayout = go.Layout(dict(title = \"Treand of z-score of Apple.Inc\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"),)\npy.iplot(dict(data=data, layout=layout), filename='basic-line')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a86d0cd06f2128449bfbe8ad440c7a22a437288b"},"cell_type":"markdown","source":"Let's have a look at means of the return variables."},{"metadata":{"trusted":true,"_uuid":"d3120cf7a8e90747887c0ede67e0b03af95c9520"},"cell_type":"code","source":"data = []\nfor col in ['returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n       'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n       'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n       'returnsClosePrevMktres10', 'returnsOpenPrevMktres10',\n       'returnsOpenNextMktres10']:\n    df = marketdf.groupby('time')[col].mean().reset_index()\n    data.append(go.Scatter(\n        x = df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n        y = df[col].values,\n        name = col\n    ))\n    \nlayout = go.Layout(dict(title = \"Treand of mean values\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"),)\npy.iplot(dict(data=data, layout=layout), filename='basic-line')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"29999767541f35587925fb4fb685ec3d7101ba37"},"cell_type":"markdown","source":"See some correlation between prices."},{"metadata":{"trusted":true,"_uuid":"30df90531bd10337c9b30677ac4a329bc7b6da88"},"cell_type":"code","source":"from sklearn.preprocessing import scale\n\ndf = marketdf.groupby('time')['returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n       'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n       'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n       'returnsClosePrevMktres10', 'returnsOpenPrevMktres10'].mean().reset_index()\ncorr = df.corr()\nsns.heatmap(corr, \n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b1a85c6e6dc5f37220938e80f30f4e53cfb277b"},"cell_type":"markdown","source":"### EDA on News data"},{"metadata":{"trusted":true,"_uuid":"e4f2962194ba8a78aec57e8f7bc1948bbee055c1"},"cell_type":"code","source":"print(f'{newsdf.shape[0]} samples and {newsdf.shape[1]} features in the training news dataset.')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e1138d6ec9eef769ef4617fee5474e49a92bf581"},"cell_type":"markdown","source":"The file is too huge to work with text directly, so let's see a wordcloud of the last 100000 headlines."},{"metadata":{"trusted":true,"_uuid":"1db377e590c4001509df2443c4ba1a47276033ce"},"cell_type":"code","source":"text = ' '.join(newsdf['headline'].str.lower().values[-1000000:])\nwordcloud = WordCloud(max_font_size=None, stopwords=stop, background_color='white',\n                      width=1200, height=1000).generate(text)\nplt.figure(figsize=(12, 8))\nplt.imshow(wordcloud)\nplt.title('Top words in headline')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a20e80e421ca5129a92960881e5fdc198f976360"},"cell_type":"code","source":"# Let's also limit the time period\nnewsdf = newsdf.loc[newsdf['time'] >= '2010-01-01 22:00:00+0000']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5312aee46b37b181bca18186674b59d81f6175f7"},"cell_type":"code","source":"news_train_df = newsdf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"446669c1f9347707649d4f13603de083594a28b1"},"cell_type":"code","source":"# Sort values by time then extract date\nnews_train_df = news_train_df.sort_values(by='time')\nnews_train_df['date'] = news_train_df['time'].dt.date","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7532d0f820c544768daea73dd89e15a47027d173"},"cell_type":"code","source":"# Function to plot time series data\ndef plot_vs_time(data_frame, column, calculation='mean', span=10):\n    if calculation == 'mean':\n        group_temp = data_frame.groupby('date')[column].mean().reset_index()\n    if calculation == 'count':\n        group_temp = data_frame.groupby('date')[column].count().reset_index()\n    if calculation == 'nunique':\n        group_temp = data_frame.groupby('date')[column].nunique().reset_index()\n    group_temp = group_temp.ewm(span=span).mean()\n    fig = plt.figure(figsize=(10,3))\n    plt.plot(group_temp['date'], group_temp[column])\n    plt.xlabel('Time')\n    plt.ylabel(column)\n    plt.title('%s versus time' %column)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fbf1abb823605ba7f4e76636cbb27ef01aea77c9"},"cell_type":"code","source":"plot_vs_time(news_train_df, 'sourceId', calculation='count', span=10)\nplt.title('News count vs time')\nplt.ylabel('Count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc43874f59f174f5cc55c56ab66475a9590d85ad"},"cell_type":"code","source":"# Plot time evolution of several parameters\n\ncolumns = ['urgency', 'takeSequence', 'companyCount','marketCommentary','sentenceCount',\\\n           'firstMentionSentence','relevance','sentimentClass','sentimentWordCount','noveltyCount24H', 'volumeCounts24H']\n\nfor column in columns:\n    plot_vs_time(news_train_df, column)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a3c723b6b57bae3781696a6bea73d95cbe6a7c49"},"cell_type":"code","source":"time_delay = (pd.to_datetime(news_train_df['time']) - pd.to_datetime(news_train_df['firstCreated']))\ntime_delay_log10 = np.log10(time_delay.dt.total_seconds()/60+1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"58cb554cc3fc732671dd5ce29eadbfa59667d5ed"},"cell_type":"code","source":"plt.hist(time_delay_log10, bins=np.arange(0,2.5,0.25), rwidth=0.7)\nplt.xlabel('$Log_{10}$(Time delay in minutes +1)')\nplt.ylabel('Counts')\nplt.title('Delay time distribution')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a65348f1205e764e2ae3180ac50f2b6c0d6f6d5a"},"cell_type":"code","source":"time_delay_min = time_delay.dt.total_seconds()/60\ntime_delay_df = time_delay_min.to_frame().join(news_train_df['date'].to_frame())\ntime_delay_df.columns = ['delay','date']\nplot_vs_time(time_delay_df, 'delay')\nplt.ylabel('Delay (minutes)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc6b139e88cb593aa19a2028e6d375b2b70c204b"},"cell_type":"code","source":"urgency_count = news_train_df.groupby('urgency')['sourceId'].count()\nurgency_count = urgency_count/urgency_count.sum()\nprint('Urgency ratio')\nurgency_count.sort_values(ascending=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"548f121c085c8d0822b55b4cc2ad6bb58715fd2f"},"cell_type":"code","source":"take_sequence = news_train_df.groupby('takeSequence')['sourceId'].count()\ntake_sequence = take_sequence.sort_values(ascending= False)\ntake_sequence[:10].plot.barh()\nplt.xlabel('Count')\nplt.ylabel('Take sequence')\nplt.title('Top 10 take sequence')\nplt.gca().invert_yaxis()\ndel take_sequence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a005d1f961f8e169e2e75948e29e8989059be0f"},"cell_type":"code","source":"provider_count = news_train_df.groupby('provider')['sourceId'].count()\nprovider_sort = provider_count.sort_values(ascending= False)\nprovider_sort[:10].plot.barh()\nplt.xlabel('Count')\nplt.ylabel('Provider')\nplt.title('Top 10 news provider')\nplt.gca().invert_yaxis()\ndel provider_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4734d241e9b4a357160439e56982ce52ac71688"},"cell_type":"code","source":"import re\n# Extract data from a single cell\ndef contents_to_list(contents):\n    text = contents[1:-1]\n    text = re.sub(r\",\",' ',text)\n    text = re.sub(r\"'\",\"\", text)\n    text_list = text.split('  ')\n    return text_list\n\n# Put data from columns into dict\ndef get_content_dict(content_column):\n    content_dict = {}\n    for i in range(len(content_column)):\n        this_cell = content_column[i]\n        content_list = contents_to_list(this_cell)        \n        for content in content_list:\n            if content in content_dict.keys():\n                content_dict[content] += 1\n            else:\n                content_dict[content] = 1\n    return content_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"48b92a46980023506c441a29f6978b84649e6f74"},"cell_type":"code","source":"subjects = news_train_df.sample(n=10000, random_state=1)['subjects']\nsubjects_dict = get_content_dict(subjects)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ab4220f3cc65aad10849588c8583d03b8dcf6b15"},"cell_type":"code","source":"subjects_df = pd.Series(subjects_dict).sort_values(ascending=False)\nsubjects_df[:15].plot.barh()\nplt.ylabel('Subjects')\nplt.xlabel('Counts')\nplt.title('Top subjects for 10k data')\nplt.gca().invert_yaxis()\ndel subjects_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d872e57805a231e70b030938171153471b5297d"},"cell_type":"code","source":"audiences = news_train_df.sample(n=10000, random_state=1)['audiences']\naudiences_dict = get_content_dict(audiences)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c914a2f1344a2ba824399e97a83a6d4025e22b6"},"cell_type":"code","source":"audiences_df = pd.Series(audiences_dict).sort_values(ascending=False)\naudiences_df[:15].plot.barh()\nplt.ylabel('Audiences')\nplt.xlabel('Counts')\nplt.title('Top audiences for 10k data')\nplt.gca().invert_yaxis()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f13072c8745757842914debaa7b423d71f932626"},"cell_type":"code","source":"news_train_df['companyCount'].hist(bins=np.arange(0,30,1))\nplt.xlabel('Company count')\nplt.title('Company count distribution')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef24f76df3f734518833cadf5a5d8ebdd7030e79"},"cell_type":"code","source":"head_line = news_train_df.groupby('headlineTag')['sourceId'].count()\nhead_line_sort = head_line.sort_values(ascending= False)\nhead_line_sort[:10].plot.barh()\nplt.xlabel('Count')\nplt.ylabel('Head line')\nplt.title('Top 10 head lines')\nplt.gca().invert_yaxis()\ndel head_line","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d2e60b65af19f6ea602ccd5389c06928d4567f07"},"cell_type":"code","source":"news_train_df['firstMentionSentence'].hist(bins=np.arange(0,20,1))\nplt.xlabel('First mention sentence')\nplt.ylabel('Count')\nplt.title('First mention sentence distribution')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df3a0563cabb3404b2bacc63593a02b24023728c"},"cell_type":"code","source":"sentence_urgency = news_train_df.groupby('firstMentionSentence')['urgency'].mean()\nsentence_urgency.head(5)\ndel sentence_urgency","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae03e30914918cd2e8de1d9e18a8016b33d17a9f"},"cell_type":"code","source":"news_train_df['relevance'].hist(bins=np.arange(0,1.01,0.05))\nplt.xlabel('Relevance')\nplt.ylabel('Count')\nplt.title('Relevance distribution')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"577114230a1a28bbebfa96de71dcb250b6a8e073"},"cell_type":"code","source":"sentence_relevance = news_train_df.groupby('firstMentionSentence')['relevance'].mean()\nsentence_relevance[:15].plot.barh()\nplt.xlabel('Relevance')\nplt.title('Relevance by sentence')\nplt.gca().invert_yaxis()\ndel sentence_relevance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e62d5fa8078bae7919bada9150348353036dae1"},"cell_type":"code","source":"sentimentWordCount = news_train_df.groupby('sentimentWordCount')['sourceId'].count().reset_index()\nplt.plot(sentimentWordCount['sentimentWordCount'], sentimentWordCount['sourceId'])\nplt.xlim(0,300)\nplt.xlabel('Sentiment words count')\nplt.ylabel('Count')\nplt.title('Sentiment words count distribution')\ndel sentimentWordCount","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c0cf196d64ba5ba0e8a5ba2702314671fa383b17"},"cell_type":"code","source":"sentimentWordRatio = news_train_df.groupby('sentimentWordCount')['relevance'].mean()\nplt.plot(sentimentWordRatio)\nplt.xlim(0,2000)\nplt.ylabel('Relevance')\nplt.xlabel('Sentiment word count')\nplt.title('Sentiment word count and relevance')\ndel sentimentWordRatio","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8bb778b329a61a7517208f32558e3b99e9bdc8f"},"cell_type":"code","source":"news_train_df['sentimentRatio'] = news_train_df['sentimentWordCount']/news_train_df['wordCount']\nnews_train_df['sentimentRatio'].hist(bins=np.linspace(0,1.001,40))\nplt.xlabel('Sentiment ratio')\nplt.ylabel('Count')\nplt.title('Sentiment ratio distribution')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ec088a4d04f6ca7f0296ae36b7f8ee49851d509"},"cell_type":"code","source":"news_train_df.sample(n=10000, random_state=1).plot.scatter('sentimentRatio', 'relevance')\nplt.title('Relevance vs sentiment ratio of 10k samples')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1356622bb2983a07aa9192dc0ae647de5fb56c6"},"cell_type":"code","source":"asset_name = news_train_df.groupby('assetName')['sourceId'].count()\nprint('Total number of assets: ',news_train_df['assetName'].nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c25daffb47987f71b7bd3978090df9402d31c92a"},"cell_type":"code","source":"asset_name = asset_name.sort_values(ascending=False)\nasset_name[:10].plot.barh()\nplt.gca().invert_yaxis()\nplt.xlabel('Count')\nplt.title('Top 10 assets news')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b10b3340568f477e3530f2afa7c850db1d22ceb6"},"cell_type":"code","source":"for i, j in zip([-1, 0, 1], ['negative', 'neutral', 'positive']):\n    df_sentiment = news_train_df.loc[news_train_df['sentimentClass'] == i, 'assetName']\n    print(f'Top mentioned companies for {j} sentiment are:')\n    print(df_sentiment.value_counts().head(5))\n    print('')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9980778a6b49024ef20040fe02b86e6b28c81b0"},"cell_type":"code","source":"# Function to remove outliers\ndef remove_outliers(data_frame, column_list, low=0.02, high=0.98):\n    temp_frame = data_frame\n    for column in column_list:\n        this_column = data_frame[column]\n        quant_df = this_column.quantile([low,high])\n        low_limit = quant_df[low]\n        high_limit = quant_df[high]\n        temp_frame[column] = data_frame[column].clip(lower=low_limit, upper=high_limit)\n    return temp_frame","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"64b1b79a232daa9847e0af375767cdad0d2a33d6"},"cell_type":"code","source":"# Remove outlier\ncolumns_outlier = ['takeSequence', 'bodySize', 'sentenceCount', 'wordCount', 'sentimentWordCount', 'firstMentionSentence','noveltyCount12H',\\\n                  'noveltyCount24H', 'noveltyCount3D', 'noveltyCount5D', 'noveltyCount7D', 'volumeCounts12H', 'volumeCounts24H',\\\n                  'volumeCounts3D','volumeCounts5D','volumeCounts7D']\nnews_rmv_outlier = remove_outliers(news_train_df, columns_outlier)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"996dc1caf9cd98a87e175e11733d15ec99d74d68"},"cell_type":"code","source":"# Plot correlation\ncolumns_corr = ['urgency', 'takeSequence', 'companyCount','marketCommentary','sentenceCount',\\\n           'firstMentionSentence','relevance','sentimentClass','sentimentWordCount','noveltyCount24H',\\\n           'noveltyCount3D', 'noveltyCount5D', 'noveltyCount7D','volumeCounts24H','volumeCounts3D','volumeCounts5D','volumeCounts7D']\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(18,15))\nsns.heatmap(news_rmv_outlier[columns_corr].astype(float).corr(), linewidths=0.1, vmax=1.0, vmin=-1., square=True, cmap=colormap, linecolor='white', annot=True)\nplt.title('Pair-wise correlation')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e0af451f007cb8f469c4fc81cc823ff9178709c0"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}