{"cells":[{"metadata":{"_uuid":"512d64e04ad0533dd72ca6bcbca4882a7f289956"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#%env KERAS_BACKEND=theano\n%env PYTHONHASHSEED=0\nfrom tensorflow import set_random_seed\nset_random_seed(1234)\nimport numpy as np\nnp.random.seed(1234)\nimport random as rn\nrn.seed(1234)\n\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport time\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import log_loss\nfrom kaggle.competitions import twosigmanews\n\n\n#***********************************import keras\nfrom keras import initializers\nfrom keras import regularizers\nfrom keras import constraints\nfrom keras.utils import conv_utils\nfrom keras.utils.data_utils import get_file\nfrom keras.engine.topology import get_source_inputs\nfrom keras.engine import InputSpec\nfrom keras import backend as K\nfrom keras.layers import LeakyReLU\nfrom keras.layers import ZeroPadding2D\nfrom keras.losses import binary_crossentropy\nimport keras.callbacks as callbacks\nfrom keras.callbacks import Callback\nfrom keras.applications.xception import Xception\nfrom keras.layers import multiply\n\nimport keras\nfrom keras import optimizers\nfrom keras.legacy import interfaces\nfrom keras.utils.generic_utils import get_custom_objects\nfrom keras.engine.topology import Input\nfrom keras.engine.training import Model\nfrom keras.layers.convolutional import Conv2D, UpSampling2D, Conv2DTranspose\nfrom keras.layers.core import Activation, SpatialDropout2D\nfrom keras.layers.merge import concatenate\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers import Input,Dropout,BatchNormalization,Activation,Add\nfrom keras.regularizers import l2\nfrom keras.layers.core import Dense, Lambda\nfrom keras.layers.merge import concatenate, add\nfrom keras.layers import GlobalAveragePooling2D, Reshape, Dense, multiply, Permute\nfrom keras.optimizers import SGD","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":true},"cell_type":"code","source":"env = twosigmanews.make_env()\n(market_train, _) = env.get_training_data()\nprint(market_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"48194844bdb10254d573c08f042b4315cb6b03c8"},"cell_type":"code","source":"currentValDicts = {}\nimport math\n\njj = 0\n\n \ndef getEMWA(assetCode, alpha, val, name):\n    if math.isnan(val):\n        val = 0\n    \n    try:\n        currentValDict = currentValDicts[str(alpha) + name]\n    except KeyError:\n        currentValDict = {}\n        currentValDicts[str(alpha) + name] = currentValDict\n        \n    try:\n        currentValue = currentValDict[assetCode]\n    except KeyError:\n        currentValue = 0\n        \n    result = alpha * val + (1-alpha) * currentValue\n    currentValDict[assetCode] = result\n    return result\n\nassetCodeStats = {}\ndef getStats(assetCode, next10mktresVal):\n    if math.isnan(next10mktresVal):\n        next10mktresVal = 0\n        \n    try:\n        currentStats = assetCodeStats[assetCode]\n    except KeyError:\n        currentStats = {\"totalCount\": 0, \"positives\" : 0}\n        assetCodeStats[assetCode] = currentStats\n        \n    currentStats['totalCount'] = currentStats['totalCount'] + 1\n    currentStats['positives'] = currentStats['positives'] + (1 if next10mktresVal > 0 else 0)\n    \n    return currentStats\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db8eda4b3b4e36f7fc4a365fd0231c8317a2fbc3"},"cell_type":"code","source":"plt.hist(market_train[market_train.time == '2007-02-01 22:00:00+00:00'].returnsOpenNextMktres10, bins=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26fad7cc514b1ece228623b55c1872bb1ec3f02c"},"cell_type":"code","source":"plt.hist(market_train[market_train.time == '2015-12-03 22:00:00+00:00'].returnsOpenNextMktres10, bins=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0186a20edab3d94afdab669f2af4bd5831b4155c"},"cell_type":"code","source":"market_train.groupby('time')['returnsOpenNextMktres10'].agg(lambda x: (x>0).sum() / ((x>0).sum() + (x<0).sum()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b230b685aba19dc9164b4f777e4a7c38c8748fd"},"cell_type":"code","source":"mytimestep=20\n\ntrain_indices=np.array([i for i in range(800, 1800)]) \ntrain_indices_timestep=np.array([i for i in range(800 + mytimestep - 1, 1800)]) \nval_indices=np.array([i for i in range(1810, 2490)])\nval_indices_timestep=np.array([i for i in range(1810 + mytimestep - 1, 2490)])\nlatest100_indices = np.array([i for i in range(2460, 2490)]) \nlatest_indices = np.array([i for i in range(0, 2498)]) \nall_indices=np.array([i for i in range(1, 4072955)]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b364626bf46e1f04fc9b97ba9cb71dcf52286b0","scrolled":false},"cell_type":"code","source":"from sklearn.preprocessing import scale\ndef enrich_df(_df):\n    _df.loc[_df['returnsOpenPrevMktres10']>1, 'returnsOpenPrevMktres10'] = 1\n    _df.loc[_df['returnsOpenPrevMktres10']<-1, 'returnsOpenPrevMktres10'] = -1\n    _df.loc[_df['returnsClosePrevMktres10']>1, 'returnsClosePrevMktres10'] = 1\n    _df.loc[_df['returnsClosePrevMktres10']<-1, 'returnsClosePrevMktres10'] = -1\n    _df.loc[_df['returnsOpenPrevMktres1']>0.3, 'returnsOpenPrevMktres1'] = 0.3\n    _df.loc[_df['returnsOpenPrevMktres1']<-0.3, 'returnsOpenPrevMktres1'] = -0.3\n    _df.loc[_df['returnsClosePrevMktres1']>0.3, 'returnsClosePrevMktres1'] = 0.3\n    _df.loc[_df['returnsClosePrevMktres1']<-0.3, 'returnsClosePrevMktres1'] = -0.3\n    _df.loc[_df['returnsOpenPrevRaw10']>0.7, 'returnsOpenPrevRaw10'] = 0.7\n    _df.loc[_df['returnsOpenPrevRaw10']<-0.7, 'returnsOpenPrevRaw10'] = -0.7\n    _df.loc[_df['returnsOpenPrevRaw1']>0.5, 'returnsOpenPrevRaw1'] = 0.5\n    _df.loc[_df['returnsOpenPrevRaw1']<-0.5, 'returnsOpenPrevRaw1'] = -0.5\n\n    _df['market_code'] = _df.assetCode.str.split('.', expand=True)[1]\n\n    _df['returnsOpenPrevMktres10SO'] = _df['returnsOpenPrevMktres10'].transform(lambda x: 1 if x > 0 else 0)\n    _df['returnsClosePrevMktres10SO'] = _df['returnsClosePrevMktres10'].transform(lambda x: 1 if x > 0 else 0)\n    _df['returnsOpenPrevMktres1SO'] = _df['returnsOpenPrevMktres1'].transform(lambda x: 1 if x > 0 else 0)\n\n    # for i in _df.index:\n    #     assetCode = _df.at[i, 'assetCode']\n    # #     _df.at[i, 'returnsOpenPrevMktres10_emwa033_custom'] = getEMWA(assetCode, 0.33,  _df.at[i, 'returnsOpenPrevMktres10'], 'returnsOpenPrevMktres10')\n    # #     _df.at[i, 'returnsOpenPrevMktres10_emwa014_custom'] = getEMWA(assetCode, 0.14,  _df.at[i, 'returnsOpenPrevMktres10'], 'returnsOpenPrevMktres10')\n    # #     _df.at[i, 'returnsOpenPrevMktres10_emwa007_custom'] = getEMWA(assetCode, 0.07,  _df.at[i, 'returnsOpenPrevMktres10'], 'returnsOpenPrevMktres10')\n    #     assetStats =  getStats(assetCode,  _df.at[i, 'returnsOpenPrevMktres10'])\n    #     _df.at[i, 'assetCodePerfC'] = assetStats['positives'] /  assetStats['totalCount']\n    #     _df.at[i, 'assetCodePerfCountC'] = assetStats['totalCount']\n\n    _df['returnsOpenPrevMktres10_dailyMean'] = _df.groupby('time')['returnsOpenPrevMktres10'].mean()\n    _df['returnsClosePrevMktres10_dailyMean'] = _df.groupby('time')['returnsClosePrevMktres10'].mean()\n    _df['returnsOpenPrevMktres1_dailyMean'] = _df.groupby('time')['returnsOpenPrevMktres1'].mean()\n    _df['returnsClosePrevMktres1_dailyMean'] = _df.groupby('time')['returnsClosePrevMktres1'].mean()\n    _df['returnsOpenPrevRaw10_dailyMean'] = _df.groupby('time')['returnsOpenPrevRaw10'].mean()\n    _df['returnsOpenPrevRaw1_dailyMean'] = _df.groupby('time')['returnsOpenPrevRaw1'].mean()\n\n    #_df['returnsOpenPrevMktres10_dailyMean_market'] =  _df.groupby(['time', 'market_code'])['returnsOpenPrevMktres10'].transform('mean')\n    _df['returnsClosePrevMktres10_dailyMean_market'] =  _df.groupby(['time', 'market_code'])['returnsClosePrevMktres10'].transform('mean')\n    #_df['returnsOpenPrevMktres1_dailyMean_market'] =  _df.groupby(['time', 'market_code'])['returnsOpenPrevMktres1'].transform('mean')\n    _df['returnsClosePrevMktres1_dailyMean_market'] =  _df.groupby(['time', 'market_code'])['returnsClosePrevMktres1'].transform('mean')\n    #_df['returnsOpenPrevRaw10_dailyMean_market'] =  _df.groupby(['time', 'market_code'])['returnsOpenPrevRaw10'].transform('mean')\n    _df['returnsClosePrevRaw10_dailyMean_market'] =  _df.groupby(['time', 'market_code'])['returnsClosePrevRaw10'].transform('mean')\n    _df['returnsOpenPrevRaw1_dailyMean_market'] =  _df.groupby(['time', 'market_code'])['returnsOpenPrevRaw1'].transform('mean')\n\n    _df['returnsOpenPrevMktres10SO_dailyMean'] =  _df.groupby('time')['returnsOpenPrevMktres10SO'].transform('mean')\n    _df['returnsOpenPrevMktres10SO_dailyMean_market'] =  _df.groupby(['time', 'market_code'])['returnsOpenPrevMktres10SO'].transform('mean')\n    _df['returnsClosePrevMktres10SO_dailyMean_market'] =  _df.groupby(['time', 'market_code'])['returnsClosePrevMktres10SO'].transform('mean')\n    _df['returnsOpenPrevMktres1SO_dailyMean_market'] =  _df.groupby(['time', 'market_code'])['returnsOpenPrevMktres1SO'].transform('mean')\n    \n    print('unique time', _df['time'].nunique())\n    print('unique market_code', _df['market_code'].unique())\n    return _df\n\nmarket_train = enrich_df(market_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0bf3bee51ff22fbce943cc4b25c28de686f22a79"},"cell_type":"code","source":"pd.options.display.max_rows = 300\npd.DataFrame(market_train.groupby(['time', 'market_code'])['returnsOpenPrevMktres1SO'].agg('mean'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39305154aee09e96c2f0ca444bafd5c75a982f46"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n#volumeScaler = StandardScaler()\n#market_train['volume'] = market_train.groupby('assetCode')['volume'].transform(lambda x: scale(x.astype(float)))\n#market_train[['returnsOpenPrevMktres10', 'returnsOpenPrevMktres1', 'volume' ,'returnsOpenPrevRaw10', 'returnsOpenPrevMktres10_emwa033_custom', 'returnsOpenPrevMktres10_emwa014_custom', 'returnsOpenPrevMktres10_emwa007_custom']] = scaler.fit_transform(market_train[['returnsOpenPrevMktres10', 'returnsOpenPrevMktres1', 'volume', 'returnsOpenPrevRaw10', 'returnsOpenPrevMktres10_emwa033_custom', 'returnsOpenPrevMktres10_emwa014_custom', 'returnsOpenPrevMktres10_emwa007_custom']])\n#market_train[['returnsOpenPrevMktres10', 'returnsOpenPrevMktres1', 'volume' ,'returnsOpenPrevRaw10', 'assetCodePerf' , 'assetCodePerfCount', 'assetCodePerfC' , 'assetCodePerfCountC']] = scaler.fit_transform(market_train[['returnsOpenPrevMktres10', 'returnsOpenPrevMktres1', 'volume', 'returnsOpenPrevRaw10', 'assetCodePerf', 'assetCodePerfCount', 'assetCodePerfC' , 'assetCodePerfCountC']])\n\n\n#train_X_reshaped[:,:,0] = scaler.fit_transform(train_X_reshaped)\n#val_X_reshaped = scaler.transform(val_X_reshaped)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5dfa1843dfece6fccfca91896ef85a332b55e3e6"},"cell_type":"code","source":"cat_cols = ['market_code'] \nnum_cols = [\n              'volume', \n              'returnsOpenPrevMktres1', 'returnsClosePrevMktres1', \n              'returnsOpenPrevMktres10',  'returnsClosePrevMktres10',\n       \n              'returnsOpenPrevRaw10', 'returnsClosePrevRaw10', \n              'returnsOpenPrevRaw1', 'returnsClosePrevRaw1',\n#              'close', 'open',\n#               'returnsOpenPrevMktres10_dailyMean', #'returnsClosePrevMktres10_dailyMean', \n#               'returnsOpenPrevMktres1_dailyMean', #'returnsClosePrevMktres1_dailyMean', \n#               'returnsOpenPrevRaw10_dailyMean', #'returnsOpenPrevRaw1_dailyMean',\n#               'returnsOpenPrevMktres10SO_dailyMean',\n    \n              #'returnsOpenPrevMktres10_dailyMean_market', \n               'returnsClosePrevMktres10_dailyMean_market', \n              #'returnsOpenPrevMktres1_dailyMean_market', \n              'returnsClosePrevMktres1_dailyMean_market', \n              #'returnsOpenPrevRaw10_dailyMean_market', \n               'returnsClosePrevRaw10_dailyMean_market',\n#              'returnsOpenPrevRaw1_dailyMean_market',\n              #'returnsOpenPrevMktres10SO_dailyMean_market',\n               'returnsOpenPrevMktres10SO_dailyMean_market',\n#              'returnsOpenPrevMktres1SO_dailyMean_market',\n#              'assetCodePerfC', 'assetCodePerfCountC',\n              #'returnsOpenPrevMktres10SO'\n              'returnsClosePrevMktres10SO'\n\n           ]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"184ec665b87ba0a11ac19c7d95d5ae545d0106e1"},"cell_type":"code","source":"print('reindex data...')\nall_assetCodes = market_train.assetCode.unique()\nmarket_train = market_train.set_index(['time', 'assetCode'], drop=False)\n(time_index, asset_index) = market_train.index.levels\nnew_index = pd.MultiIndex.from_product([time_index, all_assetCodes], names=['time', 'assetCode'])\nmarket_train = market_train.reindex(new_index)\nprint(market_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"adce27de1f08e4eedfaa600140a1326fab45cda1","scrolled":true},"cell_type":"code","source":"market_train[market_train.assetCode=='AAPL.O'].plot(x='time', y=['returnsOpenPrevMktres10','returnsOpenPrevMktres1', 'volume'] , figsize=(150,10))\nmarket_train[market_train.assetCode=='AAPL.O'].plot(x='time', y=['returnsOpenPrevMktres10','returnsOpenNextMktres10'] , figsize=(150,10))\n\n# market_train[market_train.assetCode=='AAPL.O'].plot(x='time', y=['assetCodePerfC'] , figsize=(150,10))\n# market_train[market_train.assetCode=='AAP.N'].plot(x='time', y=['assetCodePerfC'] , figsize=(150,10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"558777e7bcd72661b15a356aa3b00fcc5b2ea26a"},"cell_type":"code","source":"#market_train = market_train[market_train.assetCode=='AAPL.O']\n#market_train = market_train[market_train.assetCode=='AAP.N']\n\nfrom pandas.tools.plotting import autocorrelation_plot\nautocorrelation_plot(market_train[market_train.assetCode=='AAPL.O']['returnsOpenPrevMktres10'])\nplt.show()\n\nfrom statsmodels.tsa.arima_model import ARIMA\nmodel = ARIMA(market_train[market_train.assetCode=='AAPL.O']['returnsOpenPrevMktres10'].iloc[10:], order=(7,0,0))\nmodel_fit = model.fit(disp=0)\nprint(model_fit.summary())\nmodel_fit.predict(start=market_train[market_train.assetCode=='AAPL.O']['returnsOpenPrevMktres10'].iloc[10:].shape[0], end=market_train[market_train.assetCode=='AAPL.O']['returnsOpenPrevMktres10'].iloc[10:].shape[0]+10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"039e4f376ce13d28156ff3a0251f7d64f2b4abb5"},"cell_type":"code","source":"def encode(encoder, x):\n    len_encoder = len(encoder)\n    try:\n        id = encoder[x]\n    except KeyError:\n        id = len_encoder\n    return id\n\nencoders = [{} for cat in cat_cols]\n\n\nfor i, cat in enumerate(cat_cols):\n    print('encoding %s ...' % cat, end=' ')\n    #encoders[i] = {l: id for id, l in enumerate(market_train.loc[train_indices, cat].astype(str).unique())}\n    encoders[i] = {l: id for id, l in enumerate(market_train.loc[:, cat].astype(str).unique())}\n    market_train[cat+\"_encoded\"] = market_train[cat].astype(str).apply(lambda x: encode(encoders[i], x))\n    print('Done')\n\nembed_sizes = [len(encoder) + 1 for encoder in encoders] #+1 for possible unknown assets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"79ae8d2e6e4edcfbb40b87d80fd931077ffa1543"},"cell_type":"code","source":"market_train[market_train.index.get_level_values(1)=='AAPL.O'][['returnsOpenPrevMktres10', 'returnsOpenPrevMktres1', 'returnsOpenNextMktres10']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c2a4a203674c18d7b395149cce74028467aede9"},"cell_type":"code","source":"def getMyData(_df, _days=2498):\n    mydata = np.zeros((_days, _df.index.get_level_values(1).nunique(), len(num_cols)))\n    print('mydata.shape', mydata.shape)\n    \n    nn = 0\n    for num_col in num_cols:\n        #print('num_col', num_col)\n        _mylist =_df.groupby([_df.index.get_level_values(0)])[num_col].apply(lambda x : x.values.tolist())\n        _myarr = np.array(_mylist)\n        _myarr = np.array([i for i in _myarr])\n        mydata[:,:,nn] = _myarr\n        nn = nn + 1\n\n    #mydata = np.reshape(mydata, (_days, -1)) #2498 days\n    print('mydata', mydata.shape)\n    return mydata\nmydata = getMyData(market_train)\n\n_myylist =market_train.groupby([market_train.index.get_level_values(0)])['returnsOpenNextMktres10'].apply(lambda x : x.values.tolist())\n_myyarr = np.array(_myylist)\n_myyarr = np.array([i for i in _myyarr])\nmyy = np.expand_dims(_myyarr, axis=2)\nprint('myy', myy.shape)\n\n_myulist =market_train.groupby([market_train.index.get_level_values(0)])['universe'].apply(lambda x : x.values.tolist())\n_myuarr = np.array(_myulist)\n_myuarr = np.array([i for i in _myuarr])\nmyu = np.expand_dims(_myuarr, axis=2)\nprint('myu', myu.shape)\n\ndays = market_train.index.get_level_values(0)\ndays = days.unique()\nmyd = np.array([days for d in range(0, 3780)])\nmyd = myd.T\nmyd = np.expand_dims(myd, axis=2)\n\ndel _myylist, _myyarr, _myulist, _myuarr, days\nimport gc\nn = gc.collect()\nprint('gc collect called ', n)\n\nprint('myd', myd.shape)\nprint('done reindexing')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8911e8a2e72268f37e8ea7bf293bf2f381c602ec"},"cell_type":"code","source":"asset_corr = np.corrcoef(np.nan_to_num(mydata[:,:,4]).T)\npd.DataFrame(data=asset_corr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35a2f267a799ff22f7d99ad60d6261632f35a2f2"},"cell_type":"code","source":"asset_corr[asset_corr>0.999] = 0\npd.DataFrame(data=[np.max(np.nan_to_num(asset_corr), axis=0), np.argmax(np.nan_to_num(asset_corr), axis=0)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a033e1605811daef833b4d1656c9b3b95c532fe1"},"cell_type":"code","source":"market_train[market_train.assetCode=='AAPL.O'].plot(x='time', y=['returnsOpenPrevMktres10','returnsOpenPrevMktres1', 'volume'] , figsize=(150,10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be7f5db2f17a3d423f9e3223ddc4e4676bb5d7c8"},"cell_type":"code","source":"np.set_printoptions(edgeitems=20)\nprint('mydata AAPL.O', mydata[:,3]) #3 is AAPL.O\nnp.set_printoptions(edgeitems=3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f51d00dc43857b446ae4a24b3718753f09040fd5"},"cell_type":"markdown","source":"# Handling categorical variables"},{"metadata":{"_uuid":"eae09951757845ad5bdf08e004e6477513ed739a"},"cell_type":"markdown","source":"# Define NN Architecture"},{"metadata":{"_uuid":"68473e82a5e1ca3a97d0d89fbb8ac0bd5ca6ea57"},"cell_type":"markdown","source":"Todo: add explanaition of architecture"},{"metadata":{"trusted":true,"_uuid":"f8661c00b3363e610d2ed00fdb86b507a71a41c3"},"cell_type":"code","source":"from keras.models import Model, Sequential\nfrom keras.layers import Input, Dense, Embedding, Concatenate, Flatten, BatchNormalization, LSTM, GRU, Masking, RepeatVector\nfrom keras.losses import binary_crossentropy, mse\nfrom keras import regularizers\n\n\n\ndef getLSTMModel():\n    inputs = Input(shape=(mytimestep, len(num_cols),))\n\n    #assetCodeInput = Input(shape=(mytimestep, 1,), name=\"assetCodeInput\")\n    #assetCodeEmbedding = Embedding(3444, 7, embeddings_regularizer=regularizers.L1L2(l1=0.1, l2=0.1))(assetCodeInput)\n    #assetCodeEmbedding = Reshape((mytimestep, 7))(assetCodeEmbedding)\n\n    #concatInputs = Concatenate()([inputs, assetCodeEmbedding])\n\n#    masked = Masking()(inputs)\n    #x = LSTM(10, stateful=False,  return_sequences=True,   recurrent_dropout=0., kernel_regularizer=regularizers.L1L2(l1=0.0, l2=0.0))(inputs)\n    #x = Dropout(0.5)(x)\n#     x = LSTM(10,  stateful=False,  return_sequences=True,  recurrent_dropout=0., kernel_regularizer=regularizers.L1L2(l1=0.0, l2=0.0))(x)\n#     x = Dropout(0.5)(x)\n    x = LSTM(10, stateful=False,  return_sequences=False,  recurrent_dropout=0., kernel_regularizer=regularizers.L1L2(l1=0.0, l2=0.0))(inputs)\n    x = Dropout(0.8)(x)\n    \n#     x = LSTM(10, stateful=False,  return_sequences=False,  recurrent_dropout=0., kernel_regularizer=regularizers.L1L2(l1=0.0, l2=0.0))(x)\n#     x = Dropout(0.8)(x)\n    \n    #assetCodeEmbedding = Embedding(3444, 10, embeddings_regularizer=regularizers.L1L2(l1=0.1, l2=0.1))(assetCodeInput)\n    #assetCodeEmbedding = Reshape((10,)) (assetCodeEmbedding)\n    #assetCodeEmbedding = Dense(10, activation='relu', kernel_regularizer=regularizers.L1L2(l1=0.01, l2=0.01))(assetCodeEmbedding)\n    #x = Concatenate()([assetCodeEmbedding, x])\n\n    x = Dense(5, activation=\"relu\",  kernel_regularizer=regularizers.L1L2(l1=0.00, l2=0.00))(x)\n    #x = Dense(10, activation=\"relu\")(x)\n    predictions = Dense(1, activation='sigmoid')(x)\n    #predictions = Dense(1, activation='linear')(x)\n\n    model = Model(inputs=[inputs, ], outputs=predictions)\n\n    # model = Sequential()\n    # model.add(LSTM(20, stateful=False,  return_sequences=True, batch_input_shape=(10, mytimestep, 1), kernel_regularizer=regularizers.L1L2(l1=0.0, l2=0.0)))\n    # model.add(LSTM(20, stateful=False,  return_sequences=True, kernel_regularizer=regularizers.L1L2(l1=0.0, l2=0.0)))\n    # model.add(LSTM(15, stateful=False,   kernel_regularizer=regularizers.L1L2(l1=0.0, l2=0.0)))\n    # model.add(Dense(10))\n    # #model.add(Dense(1, activation='linear'))\n    # model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', metrics=['binary_crossentropy', 'accuracy'], optimizer='adam')\n    #model.compile(loss='mse', metrics=['mse'], optimizer='sgd')\n    return model\n\nmodel1 = getLSTMModel()\nmodel2 = getLSTMModel()\nmodel3 = getLSTMModel()\n\nmodel1.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c7b30d0413edbe3f527ebfbf7f667d344c9c613"},"cell_type":"code","source":"# def get3dinput(indices, timestep, _mydata=mydata):\n#     X = _mydata[indices]\n#     y = myy[indices]\n#     u = myu[indices]\n#     d = myd[indices]\n#     a = mya[indices]\n    \n    \n#     XXlist = []\n#     yylist= []\n#     uulist = []\n#     ddlist = []\n#     aalist = []\n#     for ii in range(timestep, X.shape[0]):\n#         part = X[ii - timestep : ii]\n#         XXlist.append(part)\n#         yylist.append(y[ii])\n#         uulist.append(u[ii])\n#         ddlist.append(d[ii])\n#         aalist.append(a[ii])\n#     XX = np.array(XXlist)\n#     yy = np.array(yylist)\n#     uu = np.array(uulist)\n#     dd = np.array(ddlist)\n#     aa = np.array(aalist)\n#     return XX,yy,uu,dd,aa\n\n# # mydataoriginal = mydata.copy()\n# # myyoriginal = myy.copy()\n\n# # mydata = mydata[:,0,:]\n# # myy = myy[:,0,:]\n# print('getting 3d input train')\n# train_X, train_y, train_u, train_d, train_a =  get3dinput(train_indices, mytimestep)\n# print('getting 3d input val')\n# val_X, val_y, val_u, val_d, val_a =  get3dinput(val_indices, mytimestep)\n\n# # train_X = np.nan_to_num(train_X)\n# # train_y = np.nan_to_num(train_y)\n\n# # val_X = np.nan_to_num(val_X)\n# # val_y = np.nan_to_num(val_y)\n\n# # train_y = train_y.squeeze()\n# # val_y = val_y.squeeze()\n# # val_u = val_u.squeeze()\n\n# #train_X = np.nan_to_num(train_X)\n# # train_mask = np.isnan(train_y)\n# # #train_y = np.nan_to_num(train_y)\n# # train_u = np.nan_to_num(train_u)\n\n# #val_X = np.nan_to_num(val_X)\n# # val_mask = np.isnan(val_y)\n# # #val_y = np.nan_to_num(val_y)\n# # val_u = np.nan_to_num(val_u)\n\n# train_y_bin = np.empty_like(train_y)\n# val_y_bin = np.empty_like(val_y)\n# train_y_bin[:] = 2\n# val_y_bin[:] = 2\n\n# train_y_bin[train_y > 0] = 1\n# val_y_bin[val_y > 0] = 1\n\n# train_y_bin[train_y < 0] = 0\n# val_y_bin[val_y < 0] = 0\n\n# train_y_bin_nn_input = train_y_bin.astype(float)\n# #train_y_bin_nn_input[np.isnan(train_y)] = 0.5\n# train_y_bin_nn_input = train_y_bin_nn_input.reshape(train_y_bin_nn_input.shape[0], train_y_bin_nn_input.shape[1])\n\n# val_y_bin_nn_input = val_y_bin.astype(float)\n# #val_y_bin_nn_input[np.isnan(val_y)] = 0.5\n# val_y_bin_nn_input = val_y_bin_nn_input.reshape(val_y_bin_nn_input.shape[0], val_y_bin_nn_input.shape[1])\n\n# print('done get3dinput train_X.shape', train_X.shape)\n# print('train_a.shape', train_a.shape)\n# print('val_a.shape', val_a.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bcb69bde4c10e25b89dc574e1bf6f3866ca25404"},"cell_type":"code","source":"# train_X = train_X[:,::2,:,:]\n# print('train_X.shape', train_X.shape)\n# val_X = val_X[:,::2,:,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"739db6766c9b884a6f5c9744141197e5b8e73712"},"cell_type":"code","source":"import gc\nn = gc.collect()\nprint('gc collect called ', n)\n\n# np.set_printoptions(edgeitems=20)\n# print('train_X AAPL.O', train_X[0,:,3]) #3 is AAPL.O\n# print('train_X AAPL.O', train_X[1,:,3]) #3 is AAPL.O\n# print('train_X AAPL.O', train_X[2,:,3]) #3 is AAPL.O\n# np.set_printoptions(edgeitems=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8dbd9b938633b061cfa7d53901f76737bfa95873"},"cell_type":"code","source":"# Lets print our model\n# model1 = getModelAssetCode()\n# model1.summary()\n\n# model2 = getModelNoAssetCode()\n# model2.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe146ed72bffea0e6f4921866248a180214c9d0b"},"cell_type":"code","source":"needed_cols = num_cols.copy()\nneeded_cols.extend(['time', 'assetCode'])\nmarket_train = market_train[needed_cols]\nhistory = market_train.iloc[-(3780 * (mytimestep)):,:]\n# print('history intial shape', history.shape)\n# print('history AA.N returnsOpenPrevMktres10', history[history.assetCode=='AA.N']['returnsOpenPrevMktres10'])\n# print('history AA.N returnsOpenPrevMktres1', history[history.assetCode=='AA.N']['returnsOpenPrevMktres1'])\n# print('history AA.N volume', history[history.assetCode=='AA.N']['volume'])\nprint('history', history.shape)\ndel market_train \n#del mydata, myy, myd, myu,\nn = gc.collect()\nprint('gc collect called ', n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ecd18fe6d1a0ecf20678f94d7ba4746a7103b407"},"cell_type":"markdown","source":"[](http://) # Train NN model"},{"metadata":{"trusted":true,"_uuid":"1d28e29e9b39ea4638234e8c6bf2c77cac95615a"},"cell_type":"code","source":"# print('reshaping')\n# train_X_reshaped = np.vstack(np.split(train_X, 3780, axis=2))\n# print('train_X_reshaped', train_X_reshaped.shape)\n# del train_X\n# train_X_reshaped = np.squeeze(train_X_reshaped)\n# print('train_X_reshaped', train_X_reshaped.shape)\n\n# train_a_reshaped = np.vstack(np.split(train_a, 3780, axis=1))\n# train_a_reshaped = np.squeeLSTM...\n# print('train_a_reshaped', train_a_reshaped.shape)\n\n# train_y_reshaped = np.vstack(np.split(train_y_bin_nn_input, 3780, axis=1))\n# print('train_y_reshaped', train_y_reshaped.shape)\n# del train_y_bin_nn_input, train_a\n# train_y_reshaped = np.squeeze(train_y_reshaped)\n# print('train_y_reshaped', train_y_reshaped.shape)\n\n# n = gc.collect()\n# print('gc collect called ', n)\n\n# train_non_nan_indexes = train_y_reshaped!=2\n# print('train_non_nan_indexes', train_non_nan_indexes.shape)\n# train_X_reshaped = train_X_reshaped[train_non_nan_indexes]\n# train_y_reshaped = train_y_reshaped[train_non_nan_indexes]\n# train_a_reshaped = train_a_reshaped[train_non_nan_indexes]\n\n# del train_non_nan_indexes\n# n = gc.collect()\n# print('gc collect called ', n)\n\n# val_X_reshaped =  np.vstack(np.split(val_X, 3780, axis=2))\n# print('val_X_reshaped', val_X_reshaped.shape)\n# del val_X\n# val_X_reshaped = np.squeeze(val_X_reshaped)\n# print('val_X_reshaped', val_X_reshaped.shape)\n\n# val_a_reshaped = np.vstack(np.split(val_a, 3780, axis=1))\n# val_a_reshaped = np.squeeze(val_a_reshaped)\n# print('val_a_reshaped', val_a_reshaped.shape)\n\n# val_y_reshaped = np.vstack(np.split(val_y_bin_nn_input, 3780, axis=1))\n# del val_y_bin_nn_input, val_y_bin, val_a\n# val_y_reshaped = np.squeeze(val_y_reshaped)\n# val_non_nan_indexes = val_y_reshaped!=2\n# val_X_reshaped = val_X_reshaped[val_non_nan_indexes]\n# val_y_reshaped = val_y_reshaped[val_non_nan_indexes]\n# val_a_reshaped = val_a_reshaped[val_non_nan_indexes]\n\n# n = gc.collect()\n# print('gc collect called ', n)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"33226421dab2ba12413072762e963a60fd0fe747"},"cell_type":"code","source":"# print('trim to be dividable by 1024')\n# train_X_reshaped = train_X_reshaped[:((train_X_reshaped.shape[0]//1024) * 1024)]\n# train_y_reshaped = train_y_reshaped[:((train_y_reshaped.shape[0]//1024) * 1024)]\n# train_a_reshaped = train_a_reshaped[:((train_a_reshaped.shape[0]//1024) * 1024)]\n# val_X_reshaped = val_X_reshaped[:((val_X_reshaped.shape[0]//1024) * 1024)]\n# val_y_reshaped = val_y_reshaped[:((val_y_reshaped.shape[0]//1024) * 1024)]\n# val_a_reshaped = val_a_reshaped[:((val_a_reshaped.shape[0]//1024) * 1024)]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4c05b711120c68976da2fc2cc5011b9d038085a"},"cell_type":"code","source":"# print('np.nan_to_num')\n# train_X_reshaped = np.nan_to_num(train_X_reshaped)\n# val_X_reshaped = np.nan_to_num(val_X_reshaped)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5ead9d058594a464852d5b6293b3355702896fa1"},"cell_type":"code","source":"# train_unique_asset_ids = np.unique(train_a_reshaped)\n# val_with_train_asset_ids = np.isin(val_a_reshaped, train_unique_asset_ids )\n# print('val_with_train_asset_ids', val_with_train_asset_ids.shape)\n# print('before val shapes', val_X_reshaped.shape, val_y_reshaped.shape, val_a_reshaped.shape)\n# val_X_reshaped = val_X_reshaped[val_with_train_asset_ids]\n# val_y_reshaped = val_y_reshaped[val_with_train_asset_ids]\n# val_a_reshaped = val_a_reshaped[val_with_train_asset_ids]\n# print('after val shapes', val_X_reshaped.shape, val_y_reshaped.shape, val_a_reshaped.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e08de7489bfa250f08d3f34b9a7ca4bac9bc9d57"},"cell_type":"code","source":"mydata[train_indices] = np.reshape(scaler.fit_transform(np.reshape(mydata[train_indices], (len(train_indices) * 3780, len(num_cols)))), (len(train_indices), 3780, len(num_cols)))\nmydata[val_indices]   = np.reshape(scaler.transform    (np.reshape(mydata[val_indices],   (len(val_indices)   * 3780, len(num_cols)))), (len(val_indices),   3780, len(num_cols)))\nhistory[num_cols] = scaler.transform(history[num_cols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e180d2f02afc0651188e9160784fcc784f3ef812"},"cell_type":"code","source":"my_batch_size=1024\n\ndef batch_generator(_mydata, _myy, batch_size=1024):\n    day_index = mytimestep\n    company_index = 0\n    iii = 0\n    \n    while True:\n        #print('day_index2', day_index)\n        batch_X = np.zeros((batch_size, mytimestep, _mydata.shape[2]))\n        batch_a = np.zeros((batch_size, mytimestep, 1))\n        batch_y = np.zeros((batch_size, 1))\n      \n        ii = 0\n        kk = 0\n        while ii < batch_size:\n           \n            X_entrty = _mydata[day_index-mytimestep:day_index,company_index,:]\n            y_entry = _myy[day_index, company_index]\n            #print('day_index3', day_index, 'ii', ii, 'y_entry', y_entry, '(y_entry>0)', (y_entry>0))\n            #print('y', y_entry[0], (y_entry[0] > 0 or  y_entry[0] < 0))\n            #if (y_entry[0] > 0 or  y_entry[0] < 0) and np.isin(a_entry[0,0], train_unique_asset_ids):\n            if (y_entry[0] > 0 or  y_entry[0] < 0):    \n                X_entrty = np.nan_to_num(X_entrty)\n                batch_X[ii,:,:] = X_entrty\n                #batch_a[ii,:] = a_entry\n                batch_y[ii,:] = 1 if y_entry>0 else 0\n#                 if y_entry>0.2:\n#                     #print('resetting y_entry from x to 1',y_entry)\n#                     y_entry = 0.2\n#                 if y_entry<-0.2:\n#                     #print('resetting y_entry from x to -1',y_entry)\n#                     y_entry = -0.2\n#                 batch_y[ii,:] = y_entry\n                ii = ii + 1\n                iii = iii + 1\n                \n                \n                #if kk % 1111 == 0:\n                    #print('X_entrty', X_entrty)\n                    #print('(y_entry>0)', (y_entry>0), y_entry, y_entry.shape)\n            \n            day_index = day_index + 1\n            if day_index >= _mydata.shape[0]:\n                day_index = mytimestep\n                company_index = company_index + 1\n            if company_index >= _mydata.shape[1]:\n                #print('setting company_index = 0. iii=', iii)\n                iii = 0\n                company_index = 0\n            \n        yield [batch_X, ], batch_y\n\n# train_X_reshaped = np.nan_to_num(train_X_reshaped)\n# val_X_reshaped = np.nan_to_num(val_X_reshaped) \n    \ntrain_gen = batch_generator(mydata[train_indices],  myy[train_indices], batch_size=my_batch_size)\nvalid_gen = batch_generator(mydata[val_indices],   myy[val_indices] , batch_size=my_batch_size)\nlatest_gen = batch_generator(mydata[latest_indices], myy[latest_indices], batch_size=my_batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"542f77f1f5675067d810614da9add097266b85a5","scrolled":true},"cell_type":"code","source":"print('LSTM...')\nfinal_model = True\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\n\n# check_point = ModelCheckpoint('model.hdf5',verbose=True, save_best_only=True, monitor='val_binary_crossentropy')\n# early_stop = EarlyStopping(patience=2,verbose=True, monitor='val_binary_crossentropy')\n\ncheck_point2 = ModelCheckpoint('model.hdf5',verbose=True, save_best_only=True, monitor='val_binary_crossentropy')\nearly_stop2 = EarlyStopping(patience=4,verbose=True, monitor='val_binary_crossentropy')\n\n# company_id1 = 3\n# company_id2 = 3\n# model.fit(np.expand_dims(train_X[:,:,company_id1], axis=2), train_y_bin_nn_input[:,company_id1], \n#            validation_data=(np.expand_dims(val_X[:,:,company_id2], axis=2), val_y_bin_nn_input[:,company_id2]),\n#          train_unique_asset_ids  epochs=20, batrain_a_reshapedtch_size=10, verbose=2, shuffle=False)\n    \n\n# model.fit(train_X_reshaped, train_y_reshaped, \n#            validation_data=(print('train_X AAPL.O', train_X[0,:,3]) #3 is AAPL.O, val_y_reshaped),\n#            epochs=10, batch_size=1024, verbose=2, shuffle=False,\n#            callbacks=[check_point2, early_stop2])\n\ntrain_ex_n = np.count_nonzero(~np.isnan(myy[train_indices_timestep]))\nval_ex_n = np.count_nonzero(~np.isnan(myy[val_indices_timestep]))\nlatest_ex_n = np.count_nonzero(~np.isnan(myy[latest_indices]))\n\nif final_model:\n    print('training final model')\n    model1.fit_generator(\n        generator=latest_gen,\n        epochs=10,\n        verbose=2,\n        steps_per_epoch=latest_ex_n // my_batch_size)\nelse:\n    print('trainig test model')\n    model1.fit_generator(\n        generator=train_gen,\n        epochs=20,\n        verbose=2,\n        steps_per_epoch=train_ex_n // my_batch_size,\n        validation_data=valid_gen,\n        validation_steps=val_ex_n // my_batch_size,\n        callbacks=[check_point2, early_stop2])\n    model1.load_weights('model.hdf5')\n\n# model2.fit_generator(\n#     generator=train_gen,\n#     epochs=10,\n#     verbose=2,\n#     steps_per_epoch=train_ex_n // my_batch_size,\n#     validation_data=valid_gen,\n#     validation_steps=val_ex_n // my_batch_size,\n#     callbacks=[check_point2, early_stop2])\n# model2.load_weights('model.hdf5')\n\n# model3.fit_generator(\n#     generator=train_gen,\n#     epochs=10,\n#     verbose=2,\n#     steps_per_epoch=train_ex_n // my_batch_size,\n#     validation_data=valid_gen,\n#     validation_steps=val_ex_n // my_batch_size,\n#     callbacks=[check_point2, early_stop2])\n# model3.load_weights('model.hdf5')\n\n\n\n# model1.fit_generator(\n#     generator=latest_gen,\n#     epochs=4,\n#     verbose=2,\n#     steps_per_epoch=latest_ex_n // my_batch_size)\n\n# model2.fit_generator(\n#     generator=latest_gen,\n#     epochs=4,\n#     verbose=2,\n#     steps_per_epoch=latest_ex_n // my_batch_size)\n\n# model3.fit_generator(\n#     generator=latest_gen,\n#     epochs=4,\n#     verbose=2,\n#     steps_per_epoch=latest_ex_n // my_batch_size)\n\n\n# code you want to evaluate\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e65abfa28901cb62229fecaa06c45f88d16f1917"},"cell_type":"code","source":"gc.collect()\nvalid_gen = batch_generator(mydata[latest_indices],   myy[latest_indices], batch_size=my_batch_size)\n\nvalid_predictions = model1.predict_generator(valid_gen, steps=latest_ex_n // my_batch_size)\nvalid_predictions = valid_predictions\nprint('valid_predictions', valid_predictions.shape)\n\nvalid_gen2 = batch_generator(mydata[latest_indices],   myy[latest_indices], batch_size=my_batch_size)\nval_y_raveled = np.zeros((valid_predictions.shape[0]))\nfor vvv in range(latest_ex_n // my_batch_size):\n    _next = next(valid_gen2)\n#     print('_next[1]', _next[1])\n#     print('_next[1].shape', _next[1].shape)\n    val_y_raveled[vvv * my_batch_size:(vvv + 1) * my_batch_size] = np.squeeze(_next[1])\nprint('-->', val_y_raveled.shape)\n    \n\nval_y_raveled_train = val_y_raveled[:800000]\nval_y_raveled_val = val_y_raveled[800000:]\nvalid_predictions_train = valid_predictions[:800000]\nvalid_predictions_val = valid_predictions[800000:]\n\nfrom sklearn.calibration import CalibratedClassifierCV\ncalibrator = CalibratedClassifierCV(method='isotonic', cv=10)\ncalibrator.fit(valid_predictions, val_y_raveled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ccda537accb7db58c3c8d3f4183c71f52e3020e6","scrolled":false},"cell_type":"code","source":"\n# preds_list = []\n\n# for _compId in range(mydata.shape[1]):\n#     preds = model.predict(np.expand_dims(val_X[:,:,_compId], axis=2), batch_size=1)\n#     preds_list.append(preds)\n#     if _compId % 100 == 0:\n#         print(_compId, 'preds.shape', preds.shape)\n    \n# endtime = timeit.default_timer()\n# elapsed = endtime - start_time\n# print('start_time', start_time, 'endtime', endtime, 'elapsed', elapsed)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"460d68972755dff7c94d8f91fba7da2177c48ae7"},"cell_type":"markdown","source":"# Evaluation of Validation Set"},{"metadata":{"trusted":true,"_uuid":"c4e6ce521ba5ef75fa704551f87e2c30f7fd879a"},"cell_type":"code","source":"# confidence_valid = np.concatenate(preds_list, axis=1)\n# confidence_valid = confidence_valid * 2 -1\n# val_y_bin_nn_input[val_y_bin_nn_input==0.5]=0\n# print('accuracy_score', accuracy_score((confidence_valid>0).reshape((-1,)), val_y_bin_nn_input.reshape((-1,))))\n# plt.hist(confidence_valid.reshape((-1,)), bins=20)\n# plt.title(\"predicted confidence\")\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ddfb2d560b0e1849cf261baf353c17558fc4da1d"},"cell_type":"code","source":"# # calculation of actual metric that is used to calculate final score\n# val_y = np.nan_to_num(val_y)\n# val_y = val_y.clip(-1,1) # get rid of outliers. Where do they come from??\n# x_t_i = confidence_valid.reshape((-1,)) * val_y.reshape((-1,)) * val_u.reshape((-1,))\n# data = {'day' : val_d.reshape((-1,)), 'x_t_i' : x_t_i}\n# df = pd.DataFrame(data)\n# x_t = df.groupby('day').sum().values.flatten()\n# mean = np.mean(x_t)\n# std = np.std(x_t)\n# score_valid = mean / std\n# print('mean', mean)\n# print('std', std)\n# print(score_valid)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a556452e64bef6bee50b7e281ef65f2923c554a7"},"cell_type":"markdown","source":"# Prediction"},{"metadata":{"trusted":true,"_uuid":"f57d1b1d8cf8bea0594e0d699f1b849566ebad86"},"cell_type":"code","source":"days = env.get_prediction_days()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0cea4e659153962bdf2062b0ca10943927549a26"},"cell_type":"code","source":"import gc\nfrom sklearn.preprocessing import quantile_transform\nfrom sklearn.preprocessing import power_transform\nfrom sklearn.preprocessing import MinMaxScaler\nminMaxScaler = MinMaxScaler(feature_range=(-1, 1))\nn = gc.collect()\nprint('gc collect called ', n)\n\nn_days = 0\nprep_time = 0\nprediction_time = 0\npackaging_time = 0\npredicted_confidences = np.array([])\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    n_days +=1\n    print('******************************************')\n    print('n_days', n_days, 'market_obs_df.shape', market_obs_df.shape)\n    market_obs_df_rows = market_obs_df.shape[0]\n    print('AA.N')\n    print(market_obs_df[market_obs_df.assetCode=='AA.N'][['returnsOpenPrevMktres10', 'returnsOpenPrevMktres1', 'volume']])\n    t = time.time()\n        \n    market_obs_df = enrich_df(market_obs_df)\n    \n    market_obs_df[num_cols] = scaler.transform(market_obs_df[num_cols])\n    market_obs_df[num_cols] = market_obs_df[num_cols].fillna(0)\n   \n    market_obs_df = market_obs_df.set_index(['time', 'assetCode'], drop=False)\n    market_obs_df = market_obs_df[needed_cols]\n    \n    print('market_obs_df', market_obs_df.head(5))\n    print('concating')\n    history = pd.concat([history, market_obs_df], ignore_index=False, sort=False)\n    print('concated')\n    \n    (time_index, asset_index) = history.index.levels\n    all_assetCodes = list(history.index.get_level_values(1).unique())\n    new_index = pd.MultiIndex.from_product([time_index, all_assetCodes])\n    history = history.reindex(new_index)\n    \n    history = history.iloc[-(len(all_assetCodes) * (mytimestep)):,:]\n\n    (time_index, asset_index) = history.index.levels\n    mytestdata = getMyData(history, _days=(mytimestep))\n    print('mytestdata', mytestdata.shape)\n    mytestdata = np.swapaxes(mytestdata, 0, 1)\n    #print('mytestdata2:', mytestdata.shape)\n    \n    prep_time += time.time() - t\n    \n    t = time.time()\n    mytestdata = np.nan_to_num(mytestdata)\n    \n#     AAN_index = list(history.index.get_level_values(1)).index('AA.N')\n#     print('AAN mytestdata')\n#     print(mytestdata[AAN_index, :, :])\n    #if n_days == 1:\n    #    firstAAN = mytestdata[AAN_index, :, :]\n    #    firstAAN = np.expand_dims(firstAAN, axis=0)\n    \n    \n    market_prediction = ((model1.predict(mytestdata, batch_size=1024))) # + (model2.predict(mytestdata, batch_size=1024) * 2 - 1) + (model3.predict(mytestdata, batch_size=1024) * 2 - 1)) /  3\n    # market_prediction = np.expand_dims(ir.transform(np.squeeze(market_prediction)), axis=1)\n    market_prediction = calibrator.predict_proba(market_prediction)[:,1]\n    market_prediction = market_prediction * 2 - 1\n    print('market_prediction', market_prediction.shape)\n    print(market_prediction[:5])\n    print('max market_prediction', np.max(market_prediction), 'min market_prediction', np.min(market_prediction))\n    print('market_prediction>0', (market_prediction>0).sum(), 'market_prediction<0', (market_prediction<0).sum())\n    prediction_time += time.time() - t\n    market_prediction = np.squeeze(market_prediction)\n    market_prediction = np.clip(market_prediction, -0.999, 0.999)\n    # plt.hist(market_prediction, bins=100)\n    predicted_confidences = np.concatenate((predicted_confidences, market_prediction))\n    \n    t = time.time()\n    preds = pd.DataFrame({'assetCode':history.index.get_level_values(1).unique(),'confidence':market_prediction})\n    # insert predictions to template\n    #print('preds AA.N')\n    #print( preds[preds.assetCode=='AA.N'])\n    predictions_template_df = predictions_template_df.merge(preds,how='left').drop('confidenceValue',axis=1).fillna(0).rename(columns={'confidence':'confidenceValue'})\n    #print('predictions_template_df:', predictions_template_df.head(5))\n    #print('predictions_template_df.shape', predictions_template_df.shape)\n    predictions_template_df['confidenceValue'] = np.squeeze(power_transform(np.expand_dims(predictions_template_df['confidenceValue'], axis=1), method='yeo-johnson'))\n    \n    \n    ptd_mean = predictions_template_df['confidenceValue'].mean()\n    print('ptd_mean', ptd_mean)\n    predictions_template_df['confidenceValue'] = predictions_template_df['confidenceValue'] - ptd_mean\n    \n    predictions_template_sum = predictions_template_df['confidenceValue'].abs().sum()\n    print('predictions_template_sum sum', predictions_template_sum)\n    predictions_template_df['confidenceValue'] = predictions_template_df['confidenceValue'] / (predictions_template_sum / 100)\n    #predictions_template_df['confidenceValue']  = np.squeeze(minMaxScaler.fit_transform(predictions_template_df['confidenceValue'].values.reshape((-1, 1))))\n    \n    print('predictions_template_df before submit', predictions_template_df.shape)\n    print(predictions_template_df['confidenceValue'].head(5))\n    print('predictions_template_dfmean', predictions_template_df['confidenceValue'].mean(), 'predictions_template_df sum', predictions_template_df['confidenceValue'].sum())\n    env.predict(predictions_template_df)\n    packaging_time += time.time() - t\n10\nenv.write_submission_file()\ntotal = prep_time + prediction_time + packaging_time\nprint(f'Preparing Data: {prep_time:.2f}s')\nprint(f'Making Predictions: {prediction_time:.2f}s')\nprint(f'Packing: {packaging_time:.2f}s')\nprint(f'Total: {total:.2f}s')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c0b10b1ee21cfa2ff48b7ad2b7351382c03daeaa"},"cell_type":"code","source":"# distribution of confidence as a sanity check: they should be distributed as above\nplt.hist(predicted_confidences, bins='auto')\nplt.title(\"predicted confidence\")\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}