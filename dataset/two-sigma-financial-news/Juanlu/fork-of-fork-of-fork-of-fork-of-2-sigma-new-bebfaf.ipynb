{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"cell_type":"markdown","source":"Importar librerías"},{"metadata":{"_uuid":"e2524e66838667e802b1fe2a999502b6554f4b8f","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport time\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime as dt\nfrom datetime import datetime, date, time, timedelta\nimport calendar\n\nplt.style.use('seaborn')\nsns.set(font_scale=2)\n\nimport warnings \nwarnings.filterwarnings('ignore')\nimport os","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fdd83f326e1787df94e1a11e7a3bfa3e23dd32ac"},"cell_type":"markdown","source":"Crear entorno"},{"metadata":{"_uuid":"a99c0e803afb4d179d0c33f7bb73b4458fe3b43d","trusted":true},"cell_type":"code","source":"from kaggle.competitions import twosigmanews\n# You can only call make_env() once, so don't lose it!\nenv = twosigmanews.make_env()\nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e08403c881b7963f4ad31a9a1ac31daca547125b"},"cell_type":"markdown","source":"Cargar datos de training"},{"metadata":{"_uuid":"048583ec640b32ca30a238ca81b5660fe0dbff59","trusted":true},"cell_type":"code","source":"(market_train_df, news_train_df) = env.get_training_data()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cargamos datos de test, para tener a mano la plantilla que en principio habría que rellenar para submitir la competicion...\nNosotros no submitiremos, pero... vamos a basarnos en ella para realizar el cálculo de los rendimientos que se obtendrían aplicando el modelo"},{"metadata":{"trusted":true},"cell_type":"code","source":"days = env.get_prediction_days()\n(market_test_df, news_test_df, predictions_template_df) = next(days)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d9aff50dda8a73c529c3001f42cc0d508ec0f7aa"},"cell_type":"markdown","source":"Dadas las características del proyecto, de cara a poder medir la accuracy, simplemente consultamos las características de los ficheros de test... pero no vamos a usarlos en lo sucesivo, sino que vamos a coger un subconjunto de los ficheros de train, que tengan las mismas características: los datos de mercados del último día y los datos del fichero de noticias posteriores a las 22h del penúltimo día (más todos los del último día).\n\nPara poder simular un seguimiento durante varios días en lugar del último día, cogeremos los días de la última semana para reservarlos como test."},{"metadata":{"_uuid":"08c536a8702144f8b7cd44ac0ca45586e340d072","trusted":true},"cell_type":"code","source":"del market_test_df, news_test_df\n\nstart = '2016-06-30 22:00:00+00:00'\n\nmarket_test_df = market_train_df.loc[market_train_df['time'] > start]\nnews_test_df = news_train_df.loc[news_train_df['time'] > start]\n\nmarket_train_df = market_train_df.loc[market_train_df['time'] <= start].reset_index(drop=True)\nnews_train_df = news_train_df.loc[news_train_df['time'] <= start].reset_index(drop=True)\n\nmarket_train_df = market_train_df.loc[market_train_df['time'] >= '2012-01-03 22:00:00+0000']\nnews_train_df = news_train_df.loc[news_train_df['time'] >= '2011-12-30 22:00:00+0000']\n#to make it fast\n#market_train_df = market_train_df.loc[market_train_df['time'] >= '2016-01-01 22:00:00+0000']\n#news_train_df = news_train_df.loc[news_train_df['time'] >= '2015-12-31 22:00:00+0000']\n\nmarket_train_df['close_to_open'] =  np.abs(market_train_df['close'] / market_train_df['open'])\nmarket_train_df = market_train_df.loc[market_train_df['close_to_open'] > 0.5]\nmarket_train_df = market_train_df.loc[market_train_df['close_to_open'] < 2]\n\nmarket_test_df['close_to_open'] =  np.abs(market_test_df['close'] / market_test_df['open'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Para reproducir las condiciones reales, en las que el fichero de mercados de test no tendría las variables de los rendimientos en los siguientes 10 días y la de 'universe' (si la acción entraría en cotización), vamos a eliminar estas dos variables del fichero de test...\npero, para poder medir al final el resultado de la predicción, guardaremos estos datos en un fichero auxiliar."},{"metadata":{"trusted":true},"cell_type":"code","source":"aux_columns = ['assetCode', 'time', 'returnsOpenNextMktres10', 'universe']\nmarket_test_aux = market_test_df[aux_columns]\nmarket_test_aux.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_test = ['returnsOpenNextMktres10', 'universe']\nmarket_test_df.drop(drop_test, axis=1, inplace=True)\nmarket_test_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"market_train_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"De cara a ahorrar pasos, una vez preparado el fichero de mercados de test, volvemos a juntar los ficheros de training y test en uno sólo, para hacer las transformaciones una única vez"},{"metadata":{"trusted":true},"cell_type":"code","source":"market_dfs = [market_train_df, market_test_df]\n\nmarket_train_df = pd.concat(market_dfs)\n\nmarket_train_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Y hacemos los mismo con los ficheros de noticias"},{"metadata":{"trusted":true},"cell_type":"code","source":"news_dfs = [news_train_df, news_test_df]\n\nnews_train_df = pd.concat(news_dfs)\n\nnews_train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del market_test_df, news_test_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preparamos los datos para hacer el merge que nos permitirá comenzar a ejecutar modelos"},{"metadata":{"_uuid":"431da71f2607c3b604f63435e9c4275ebaf618a1","trusted":true},"cell_type":"code","source":"def preprocess_news(news_train):\n    drop_list = [\n        'audiences', 'subjects', 'assetName',\n        'headline', 'firstCreated', 'sourceTimestamp',\n    ]\n    news_train.drop(drop_list, axis=1, inplace=True)\n    \n    # Factorize categorical columns\n    for col in ['headlineTag', 'provider', 'sourceId']:\n        news_train[col], uniques = pd.factorize(news_train[col])\n        del uniques\n    \n    # Remove {} and '' from assetCodes column\n    news_train['assetCodes'] = news_train['assetCodes'].apply(lambda x: x[1:-1].replace(\"'\", \"\"))\n    return news_train\n\nnews_train_df = preprocess_news(news_train_df)\n#news_test_df = preprocess_news(news_test_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a260ae107acfbcbb332e0879243ebeab6e4d973","trusted":true},"cell_type":"code","source":"def unstack_asset_codes(news_train_df):\n    codes = []\n    indexes = []\n    for i, values in news_train_df['assetCodes'].iteritems():\n        explode = values.split(\", \")\n        codes.extend(explode)\n        repeat_index = [int(i)]*len(explode)\n        indexes.extend(repeat_index)\n    index_df = pd.DataFrame({'news_index': indexes, 'assetCode': codes})\n    del codes, indexes\n#    gc.collect()\n    return index_df\n\nindex_df = unstack_asset_codes(news_train_df)\n#index_df2 = unstack_asset_codes(news_test_df)\n\ndef merge_news_on_index(news_train_df, index_df):\n    news_train_df['news_index'] = news_train_df.index.copy()\n\n    # Merge news on unstacked assets\n    news_unstack = index_df.merge(news_train_df, how='left', on='news_index')\n    news_unstack.drop(['news_index', 'assetCodes'], axis=1, inplace=True)\n    return news_unstack\n\nnews_unstack = merge_news_on_index(news_train_df, index_df)\n#news_unstack2 = merge_news_on_index(news_test_df, index_df2)\n#del news_train_df, index_df, news_test_df, index_df2\ndel news_train_df, index_df\n#gc.collect()\n#news_unstack.head(3)\n\n\ndef group_news(news_frame):\n#    news_frame['date'] = news_frame.time.dt.date  # Add date column\n    news_frame['date'] = np.where(news_frame.time.dt.time < dt.time(22, 0, 0), news_frame.time.dt.date, news_frame.time.dt.date + timedelta(days=1))   \n#    news_frame['date'] = np.where(datetime.weekday(news_frame['date'])==5, news_frame['date'] + timedelta(days=2), news_frame['date'])\n#    news_frame['date'] = np.where(datetime.weekday(news_frame['date'])==6, news_frame['date'] + timedelta(days=1), news_frame['date'])\n    news_frame['weekday'] = np.where(news_frame.time.dt.time < dt.time(22, 0, 0), news_frame.time.dt.dayofweek, news_frame.time.dt.dayofweek + 1)\n    aggregations = ['mean']\n    gp = news_frame.groupby(['assetCode', 'date', 'weekday']).agg(aggregations)\n#    gp.columns = pd.Index([\"{}_{}\".format(e[0], e[1]) for e in gp.columns.tolist()])\n    gp.columns = pd.Index([e[0] for e in gp.columns.tolist()])\n    gp.reset_index(inplace=True)\n    # Set datatype to float32\n    float_cols = {c: 'float32' for c in gp.columns if c not in ['assetCode', 'date', 'weekday']}\n    return gp.astype(float_cols)\n\nnews_agg = group_news(news_unstack)\ndel news_unstack\n#news_agg2 = group_news(news_unstack2)\n#del news_unstack2\n#; gc.collect()\n#news_agg.head(3)\n\nnews_agg['weekday'][news_agg.weekday == 7] = 0\n#news_agg2['weekday'][news_agg2.weekday == 7] = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tratamiento sobre los festivos y sobre fines de semana"},{"metadata":{"trusted":true},"cell_type":"code","source":"news_agg['date'][news_agg.weekday == 5] = news_agg['date'][news_agg.weekday == 5] + timedelta(days=2)\nnews_agg['date'][news_agg.weekday == 6] = news_agg['date'][news_agg.weekday == 6] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.weekday == 5] = 0\nnews_agg['weekday'][news_agg.weekday == 6] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"news_agg2['date'][news_agg2.weekday == 5] = news_agg2['date'][news_agg2.weekday == 5] + timedelta(days=2)\nnews_agg2['date'][news_agg2.weekday == 6] = news_agg2['date'][news_agg2.weekday == 6] + timedelta(days=1)\n\nnews_agg2['weekday'][news_agg2.weekday == 5] = 0\nnews_agg2['weekday'][news_agg2.weekday == 6] = 0"},{"metadata":{"_uuid":"ee4d34cd51a2990af3db9552f507a11c80314bda","trusted":true},"cell_type":"code","source":"news_agg['weekday'][news_agg.date == date(2010,1,1)] = news_agg['weekday'][news_agg.date == date(2010,1,1)] + 1\nnews_agg['date'][news_agg.date == date(2010,1,1)] = news_agg['date'][news_agg.date == date(2010,1,1)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2012,1,2)] = news_agg['weekday'][news_agg.date == date(2012,1,2)] + 1\nnews_agg['date'][news_agg.date == date(2012,1,2)] = news_agg['date'][news_agg.date == date(2012,1,2)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2013,1,1)] = news_agg['weekday'][news_agg.date == date(2013,1,1)] + 1\nnews_agg['date'][news_agg.date == date(2013,1,1)] = news_agg['date'][news_agg.date == date(2013,1,1)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2014,1,1)] = news_agg['weekday'][news_agg.date == date(2014,1,1)] + 1\nnews_agg['date'][news_agg.date == date(2014,1,1)] = news_agg['date'][news_agg.date == date(2014,1,1)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2015,1,1)] = news_agg['weekday'][news_agg.date == date(2015,1,1)] + 1\nnews_agg['date'][news_agg.date == date(2015,1,1)] = news_agg['date'][news_agg.date == date(2015,1,1)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2016,1,1)] = news_agg['weekday'][news_agg.date == date(2016,1,1)] + 1\nnews_agg['date'][news_agg.date == date(2016,1,1)] = news_agg['date'][news_agg.date == date(2016,1,1)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2010,1,18)] = news_agg['weekday'][news_agg.date == date(2010,1,18)] + 1\nnews_agg['date'][news_agg.date == date(2010,1,18)] = news_agg['date'][news_agg.date == date(2010,1,18)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2011,1,17)] = news_agg['weekday'][news_agg.date == date(2011,1,17)] + 1\nnews_agg['date'][news_agg.date == date(2011,1,17)] = news_agg['date'][news_agg.date == date(2011,1,17)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2012,1,16)] = news_agg['weekday'][news_agg.date == date(2012,1,16)] + 1\nnews_agg['date'][news_agg.date == date(2012,1,16)] = news_agg['date'][news_agg.date == date(2012,1,16)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2013,1,21)] = news_agg['weekday'][news_agg.date == date(2013,1,21)] + 1\nnews_agg['date'][news_agg.date == date(2013,1,21)] = news_agg['date'][news_agg.date == date(2013,1,21)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2014,1,20)] = news_agg['weekday'][news_agg.date == date(2014,1,20)] + 1\nnews_agg['date'][news_agg.date == date(2014,1,20)] = news_agg['date'][news_agg.date == date(2014,1,20)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2015,1,19)] = news_agg['weekday'][news_agg.date == date(2015,1,19)] + 1\nnews_agg['date'][news_agg.date == date(2015,1,19)] = news_agg['date'][news_agg.date == date(2015,1,19)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2016,1,18)] = news_agg['weekday'][news_agg.date == date(2016,1,18)] + 1\nnews_agg['date'][news_agg.date == date(2016,1,18)] = news_agg['date'][news_agg.date == date(2016,1,18)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2010,2,15)] = news_agg['weekday'][news_agg.date == date(2010,2,15)] + 1\nnews_agg['date'][news_agg.date == date(2010,2,15)] = news_agg['date'][news_agg.date == date(2010,2,15)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2011,2,21)] = news_agg['weekday'][news_agg.date == date(2011,2,21)] + 1\nnews_agg['date'][news_agg.date == date(2011,2,21)] = news_agg['date'][news_agg.date == date(2011,2,21)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2012,2,20)] = news_agg['weekday'][news_agg.date == date(2012,2,20)] + 1\nnews_agg['date'][news_agg.date == date(2012,2,20)] = news_agg['date'][news_agg.date == date(2012,2,20)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2013,2,18)] = news_agg['weekday'][news_agg.date == date(2013,2,18)] + 1\nnews_agg['date'][news_agg.date == date(2013,2,18)] = news_agg['date'][news_agg.date == date(2013,2,18)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2014,2,17)] = news_agg['weekday'][news_agg.date == date(2014,2,17)] + 1\nnews_agg['date'][news_agg.date == date(2014,2,17)] = news_agg['date'][news_agg.date == date(2014,2,17)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2015,2,16)] = news_agg['weekday'][news_agg.date == date(2015,2,16)] + 1\nnews_agg['date'][news_agg.date == date(2015,2,16)] = news_agg['date'][news_agg.date == date(2015,2,16)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2016,2,15)] = news_agg['weekday'][news_agg.date == date(2016,2,15)] + 1\nnews_agg['date'][news_agg.date == date(2016,2,15)] = news_agg['date'][news_agg.date == date(2016,2,15)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2010,4,2)] = news_agg['weekday'][news_agg.date == date(2010,4,2)] + 1\nnews_agg['date'][news_agg.date == date(2010,4,2)] = news_agg['date'][news_agg.date == date(2010,4,2)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2011,4,22)] = news_agg['weekday'][news_agg.date == date(2011,4,22)] + 1\nnews_agg['date'][news_agg.date == date(2011,4,22)] = news_agg['date'][news_agg.date == date(2011,4,22)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2012,4,6)] = news_agg['weekday'][news_agg.date == date(2012,4,6)] + 1\nnews_agg['date'][news_agg.date == date(2012,4,6)] = news_agg['date'][news_agg.date == date(2012,4,6)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2013,3,29)] = news_agg['weekday'][news_agg.date == date(2013,3,29)] + 1\nnews_agg['date'][news_agg.date == date(2013,3,29)] = news_agg['date'][news_agg.date == date(2013,3,29)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2014,4,18)] = news_agg['weekday'][news_agg.date == date(2014,4,18)] + 1\nnews_agg['date'][news_agg.date == date(2014,4,18)] = news_agg['date'][news_agg.date == date(2014,4,18)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2015,4,3)] = news_agg['weekday'][news_agg.date == date(2015,4,3)] + 1\nnews_agg['date'][news_agg.date == date(2015,4,3)] = news_agg['date'][news_agg.date == date(2015,4,3)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2016,3,25)] = news_agg['weekday'][news_agg.date == date(2016,3,25)] + 1\nnews_agg['date'][news_agg.date == date(2016,3,25)] = news_agg['date'][news_agg.date == date(2016,3,25)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2010,5,31)] = news_agg['weekday'][news_agg.date == date(2010,5,31)] + 1\nnews_agg['date'][news_agg.date == date(2010,5,31)] = news_agg['date'][news_agg.date == date(2010,5,31)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2011,5,30)] = news_agg['weekday'][news_agg.date == date(2011,5,30)] + 1\nnews_agg['date'][news_agg.date == date(2011,5,30)] = news_agg['date'][news_agg.date == date(2011,5,30)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2012,5,28)] = news_agg['weekday'][news_agg.date == date(2012,5,28)] + 1\nnews_agg['date'][news_agg.date == date(2012,5,28)] = news_agg['date'][news_agg.date == date(2012,5,28)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2013,5,27)] = news_agg['weekday'][news_agg.date == date(2013,5,27)] + 1\nnews_agg['date'][news_agg.date == date(2013,5,27)] = news_agg['date'][news_agg.date == date(2013,5,27)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2014,5,26)] = news_agg['weekday'][news_agg.date == date(2014,5,26)] + 1\nnews_agg['date'][news_agg.date == date(2014,5,26)] = news_agg['date'][news_agg.date == date(2014,5,26)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2015,5,25)] = news_agg['weekday'][news_agg.date == date(2015,5,25)] + 1\nnews_agg['date'][news_agg.date == date(2015,5,25)] = news_agg['date'][news_agg.date == date(2015,5,25)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2016,5,30)] = news_agg['weekday'][news_agg.date == date(2016,5,30)] + 1\nnews_agg['date'][news_agg.date == date(2016,5,30)] = news_agg['date'][news_agg.date == date(2016,5,30)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2010,7,5)] = news_agg['weekday'][news_agg.date == date(2010,7,5)] + 1\nnews_agg['date'][news_agg.date == date(2010,7,5)] = news_agg['date'][news_agg.date == date(2010,7,5)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2011,7,4)] = news_agg['weekday'][news_agg.date == date(2011,7,4)] + 1\nnews_agg['date'][news_agg.date == date(2011,7,4)] = news_agg['date'][news_agg.date == date(2011,7,4)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2012,7,4)] = news_agg['weekday'][news_agg.date == date(2012,7,4)] + 1\nnews_agg['date'][news_agg.date == date(2012,7,4)] = news_agg['date'][news_agg.date == date(2012,7,4)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2013,7,4)] = news_agg['weekday'][news_agg.date == date(2013,7,4)] + 1\nnews_agg['date'][news_agg.date == date(2013,7,4)] = news_agg['date'][news_agg.date == date(2013,7,4)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2014,7,4)] = news_agg['weekday'][news_agg.date == date(2014,7,4)] + 1\nnews_agg['date'][news_agg.date == date(2014,7,4)] = news_agg['date'][news_agg.date == date(2014,7,4)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2015,7,4)] = news_agg['weekday'][news_agg.date == date(2015,7,4)] + 1\nnews_agg['date'][news_agg.date == date(2015,7,4)] = news_agg['date'][news_agg.date == date(2015,7,4)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2016,7,4)] = news_agg['weekday'][news_agg.date == date(2016,7,4)] + 1\nnews_agg['date'][news_agg.date == date(2016,7,4)] = news_agg['date'][news_agg.date == date(2016,7,4)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2010,9,6)] = news_agg['weekday'][news_agg.date == date(2010,9,6)] + 1\nnews_agg['date'][news_agg.date == date(2010,9,6)] = news_agg['date'][news_agg.date == date(2010,9,6)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2011,9,5)] = news_agg['weekday'][news_agg.date == date(2011,9,5)] + 1\nnews_agg['date'][news_agg.date == date(2011,9,5)] = news_agg['date'][news_agg.date == date(2011,9,5)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2012,9,3)] = news_agg['weekday'][news_agg.date == date(2012,9,3)] + 1\nnews_agg['date'][news_agg.date == date(2012,9,3)] = news_agg['date'][news_agg.date == date(2012,9,3)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2013,9,2)] = news_agg['weekday'][news_agg.date == date(2013,9,2)] + 1\nnews_agg['date'][news_agg.date == date(2013,9,2)] = news_agg['date'][news_agg.date == date(2013,9,2)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2014,9,1)] = news_agg['weekday'][news_agg.date == date(2014,9,1)] + 1\nnews_agg['date'][news_agg.date == date(2014,9,1)] = news_agg['date'][news_agg.date == date(2014,9,1)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2015,9,7)] = news_agg['weekday'][news_agg.date == date(2015,9,7)] + 1\nnews_agg['date'][news_agg.date == date(2015,9,7)] = news_agg['date'][news_agg.date == date(2015,9,7)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2016,9,5)] = news_agg['weekday'][news_agg.date == date(2016,9,5)] + 1\nnews_agg['date'][news_agg.date == date(2016,9,5)] = news_agg['date'][news_agg.date == date(2016,9,5)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2010,11,25)] = news_agg['weekday'][news_agg.date == date(2010,11,25)] + 1\nnews_agg['date'][news_agg.date == date(2010,11,25)] = news_agg['date'][news_agg.date == date(2010,11,25)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2011,11,24)] = news_agg['weekday'][news_agg.date == date(2011,11,24)] + 1\nnews_agg['date'][news_agg.date == date(2011,11,24)] = news_agg['date'][news_agg.date == date(2011,11,24)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2012,11,22)] = news_agg['weekday'][news_agg.date == date(2012,11,22)] + 1\nnews_agg['date'][news_agg.date == date(2012,11,22)] = news_agg['date'][news_agg.date == date(2012,11,22)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2013,11,28)] = news_agg['weekday'][news_agg.date == date(2013,11,28)] + 1\nnews_agg['date'][news_agg.date == date(2013,11,28)] = news_agg['date'][news_agg.date == date(2013,11,28)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2014,11,27)] = news_agg['weekday'][news_agg.date == date(2014,11,27)] + 1\nnews_agg['date'][news_agg.date == date(2014,11,27)] = news_agg['date'][news_agg.date == date(2014,11,27)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2015,11,26)] = news_agg['weekday'][news_agg.date == date(2015,11,26)] + 1\nnews_agg['date'][news_agg.date == date(2015,11,26)] = news_agg['date'][news_agg.date == date(2015,11,26)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2016,11,24)] = news_agg['weekday'][news_agg.date == date(2016,11,24)] + 1\nnews_agg['date'][news_agg.date == date(2016,11,24)] = news_agg['date'][news_agg.date == date(2016,11,24)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2010,12,24)] = news_agg['weekday'][news_agg.date == date(2010,12,24)] + 1\nnews_agg['date'][news_agg.date == date(2010,12,24)] = news_agg['date'][news_agg.date == date(2010,12,24)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2011,12,26)] = news_agg['weekday'][news_agg.date == date(2011,12,26)] + 1\nnews_agg['date'][news_agg.date == date(2011,12,26)] = news_agg['date'][news_agg.date == date(2011,12,26)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2012,12,25)] = news_agg['weekday'][news_agg.date == date(2012,12,25)] + 1\nnews_agg['date'][news_agg.date == date(2012,12,25)] = news_agg['date'][news_agg.date == date(2012,12,25)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2013,12,25)] = news_agg['weekday'][news_agg.date == date(2013,12,25)] + 1\nnews_agg['date'][news_agg.date == date(2013,12,25)] = news_agg['date'][news_agg.date == date(2013,12,25)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2014,12,25)] = news_agg['weekday'][news_agg.date == date(2014,12,25)] + 1\nnews_agg['date'][news_agg.date == date(2014,12,25)] = news_agg['date'][news_agg.date == date(2014,12,25)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2015,12,25)] = news_agg['weekday'][news_agg.date == date(2015,12,25)] + 1\nnews_agg['date'][news_agg.date == date(2015,12,25)] = news_agg['date'][news_agg.date == date(2015,12,25)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2016,12,26)] = news_agg['weekday'][news_agg.date == date(2016,12,26)] + 1\nnews_agg['date'][news_agg.date == date(2016,12,26)] = news_agg['date'][news_agg.date == date(2016,12,26)] + timedelta(days=1)\n\n#news_agg['weekday'][news_agg.weekday == 7] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"news_agg2['weekday'][news_agg2.date == date(2016,12,26)] = news_agg2['weekday'][news_agg2.date == date(2016,12,26)] + 1\nnews_agg2['date'][news_agg2.date == date(2016,12,26)] = news_agg2['date'][news_agg2.date == date(2016,12,26)] + timedelta(days=1)\n\n#news_agg2['weekday'][news_agg2.weekday == 7] = 0"},{"metadata":{"_uuid":"4fce005f4edf5f343ab37632dce6eab4d65c66e5","trusted":true},"cell_type":"code","source":"news_agg['date'][news_agg.weekday == 5] = news_agg['date'][news_agg.weekday == 5] + timedelta(days=2)\nnews_agg['date'][news_agg.weekday == 6] = news_agg['date'][news_agg.weekday == 6] + timedelta(days=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"news_agg2['date'][news_agg2.weekday == 5] = news_agg2['date'][news_agg2.weekday == 5] + timedelta(days=2)\nnews_agg2['date'][news_agg2.weekday == 6] = news_agg2['date'][news_agg2.weekday == 6] + timedelta(days=1)"},{"metadata":{"_uuid":"ea32c7ba7a64c589f4b3db94a430a8418af5c998","trusted":true},"cell_type":"code","source":"def group_news(news_frame):\n    aggregations = ['mean']\n    gp = news_frame.groupby(['assetCode', 'date']).agg(aggregations)\n#    gp.columns = pd.Index([\"{}_{}\".format(e[0], e[1]) for e in gp.columns.tolist()])\n    gp.columns = pd.Index([e[0] for e in gp.columns.tolist()])\n    gp.reset_index(inplace=True)\n    # Set datatype to float32\n    float_cols = {c: 'float32' for c in gp.columns if c not in ['assetCode', 'date']}\n    return gp.astype(float_cols)\n\nnews_aggr = group_news(news_agg)\ndel news_agg\n#news_aggr2 = group_news(news_agg2)\n#del news_agg2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Generamos el fichero con el que vamos a trabajar que resulta del merge de los ficheros de mercados y de noticias"},{"metadata":{"trusted":true},"cell_type":"code","source":"market_train_df['date'] = market_train_df.time.dt.date\nfull_train_df = market_train_df.merge(news_aggr, how='left', on=['assetCode', 'date'])\ndel market_train_df, news_aggr\nfull_train_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"market_test_df['date'] = market_test_df.time.dt.date\nfull_test_df = market_test_df.merge(news_aggr2, how='left', on=['assetCode', 'date'])\ndel market_test_df, news_aggr2\nfull_test_df.head(5)"},{"metadata":{"_uuid":"320b63b310cda36592324ac9d6453a71c16e84b9"},"cell_type":"markdown","source":"## 5.2 Preparación de los datos"},{"metadata":{},"cell_type":"markdown","source":"## pensar justificación en elección de variables... basada en el EDA... Y ver si quitar las variables calculadas y poner las que hay a pelo\n"},{"metadata":{},"cell_type":"markdown","source":"Antes de nada, vamos a hacer encoding de los asset codes para poder incluirlos en nuestro modelo"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nfull_train_df[\"assetToken\"] = LabelEncoder().fit_transform(full_train_df[\"assetCode\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A continuación, dividimos el fichero en train y test"},{"metadata":{"trusted":true},"cell_type":"code","source":"start = '2016-06-30 22:00:00+00:00'\n\nfull_test_df = full_train_df.loc[full_train_df['time'] > start]\nfull_train_df = full_train_df.loc[full_train_df['time'] <= start].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Eliminamos del fichero de test las columnas de los rendimientos en los siguientes 10 días y de la variable 'universe' (que, por otro lado, están rellenas por nulos, ya que antes de juntar los ficheros, habíamos eliminado esas variables del fichero de test)"},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_test = ['returnsOpenNextMktres10', 'universe']\nfull_test_df.drop(drop_test, axis=1, inplace=True)\nfull_test_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Definimos una función para:\n\n- eliminar variables que no consideramos relevantes\n- generar variables nuevas que vemos que pueden resumir la información de las variables existentes... \n\nY aplicamos la función sobre el fichero de train"},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_data(full_train_df):\n#    full_train_df = full_train_df[pd.notnull(full_train_df['urgency'])]\n    full_train_df['returnsClosePrevMktres1'] = np.where(np.isnan(full_train_df['returnsClosePrevMktres1']), full_train_df['returnsClosePrevRaw1'], full_train_df['returnsClosePrevMktres1'])\n    full_train_df['returnsOpenPrevMktres1'] = np.where(np.isnan(full_train_df['returnsOpenPrevMktres1']), full_train_df['returnsOpenPrevRaw1'], full_train_df['returnsOpenPrevMktres1'])\n    full_train_df['returnsClosePrevMktres10'] = np.where(np.isnan(full_train_df['returnsClosePrevMktres10']), full_train_df['returnsClosePrevRaw10'], full_train_df['returnsClosePrevMktres10']) \n    full_train_df['returnsOpenPrevMktres10'] = np.where(np.isnan(full_train_df['returnsOpenPrevMktres10']), full_train_df['returnsOpenPrevRaw10'], full_train_df['returnsOpenPrevMktres10'])\n#    full_train_df['time'] = full_train_df.time.dt.strftime(\"%Y%m%d\").astype(int)\n#    full_train_df['bartrend'] = full_train_df['close'] / full_train_df['open']\n    full_train_df['average'] = (full_train_df['close'] + full_train_df['open'])/2\n    full_train_df['pricevolume'] = full_train_df['volume'] * full_train_df['close']\n#    full_train_df['position'] = full_train_df['firstMentionSentence'] / full_train_df['sentenceCount']\n#    full_train_df['coverage'] = full_train_df['sentimentWordCount'] / full_train_df['wordCount']\n\n    # eliminar variables prescindibles\n#    droplist = ['sourceId','takeSequence','provider','firstMentionSentence',\n#                'sentenceCount','bodySize','headlineTag','marketCommentary',\n#                'sentimentClass','urgency','wordCount','sentimentWordCount',\n#                'weekday','assetName','volume','time']\n    droplist = ['open', 'close', \n                'takeSequence', \n                'bodySize', 'companyCount',                   \n                'sentenceCount', 'wordCount', \n                'firstMentionSentence',\n                                       \n                                      'noveltyCount12H','noveltyCount24H',\n                'noveltyCount3D','noveltyCount5D', 'noveltyCount7D',\n                'weekday','assetName','time',\n                'sourceId', 'urgency', 'provider', 'marketCommentary',          \n                'relevance', 'sentimentClass',                                                   \n                'volumeCounts12H', 'volumeCounts24H', 'volumeCounts3D', 'volumeCounts5D',            \n                'volumeCounts7D']  \n    full_train_df.drop(droplist, axis=1, inplace=True)\n    return full_train_df\n\ncdf = prepare_data(full_train_df)    \ndel full_train_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creamos una variable adicional, la variable \"return\", que es la que vamos a tomar como variable objetivo, a la que le asignaremos un 1 para rendimientos por encima de 0.03 (percentil 75 de los rendimientos a 10 días en el futuro) y un 0 para los demás"},{"metadata":{"trusted":true},"cell_type":"code","source":"cdf['return'] = np.where(cdf.returnsOpenNextMktres10 > 0, 1, 0)   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dividimos en:\n\n- training\n- validation\n\nPero... inicialmente únicamente preparamos la variable objetivo... y la variable de rendimientos que nos servirá para calcular más adelante la bondad del modelo en validación"},{"metadata":{"_uuid":"b4ff110e696aa8e9209eaf0bd6fcc4eb85ef1df0","trusted":true},"cell_type":"code","source":"targetcols = ['return']\ntraincols = [col for col in cdf.columns if col not in ['date', 'assetCode', 'universe','returnsOpenNextMktres10'] + targetcols]\n\ndates = cdf['date'].unique()\ntrain = range(len(dates))[:int(0.85*len(dates))]\nval = range(len(dates))[int(0.85*len(dates)):]\n\n# we be classifyin\n#cdf[targetcols[0]] = (cdf[targetcols[0]] > 0).astype(int)\nY0 = cdf[targetcols].fillna(0).values\nZ0 = cdf['returnsOpenNextMktres10'].fillna(0).values\n\n# train data\nYt = cdf[targetcols].fillna(0).loc[cdf['date'].isin(dates[train])].values\nZt = cdf['returnsOpenNextMktres10'].fillna(0).loc[cdf['date'].isin(dates[train])].values\n\n# validation data\nYv = cdf[targetcols].fillna(0).loc[cdf['date'].isin(dates[val])].values\nZv = cdf['returnsOpenNextMktres10'].fillna(0).loc[cdf['date'].isin(dates[val])].values\n\nprint(Y0.shape)\nprint(Yt.shape, Yv.shape)\nprint(Zt.shape, Zv.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comprobamos los tipos de las variables del fichero"},{"metadata":{"trusted":true},"cell_type":"code","source":"cdf.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Y hacemos la normalización de las variables numéricas para preparar los ficheros con las variables que nos van a servir para entrenar el modelo y para introducirlas como input para la predicción en validación"},{"metadata":{"trusted":true},"cell_type":"code","source":"numcols = [col for col in cdf.columns if col not in ['date', 'assetCode', 'assetToken', 'returnsOpenNextMktres10', 'return', 'universe']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\ncdf[numcols] = StandardScaler().fit_transform(cdf[numcols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X0 = cdf[traincols].fillna(0).values\n\n# train data\nXt = cdf[traincols].fillna(0).loc[cdf['date'].isin(dates[train])].values\n\n# validation data\nXv = cdf[traincols].fillna(0).loc[cdf['date'].isin(dates[val])].values\n\nprint(Xt.shape, Xv.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Utilizamos sklearn para entrenar un modelo con los datos de training"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"from sklearn.ensemble import GradientBoostingClassifier\nclf = GradientBoostingClassifier().fit(Xt, Yt)"},{"metadata":{},"cell_type":"markdown","source":"Definimos la score function, que predecirá la categoría para cada registro"},{"metadata":{"trusted":true},"cell_type":"code","source":"def score_function(data, model):\n    predicted = model.predict(data)\n    return predicted","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hacemos la predicción para training... medimos la precisión y comprobamos la matriz de confusión"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"from sklearn.metrics import accuracy_score, confusion_matrix\n\npred_trn = score_function(Xt, clf)\nprint('Accuracy train: ', accuracy_score(Yt, pred_trn))\n\npd.DataFrame(confusion_matrix(Yt, pred_trn))"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"import numpy as np\nx = pred_trn\nunique, counts = np.unique(x, return_counts=True)\n\nnp.asarray((unique, counts)).T"},{"metadata":{},"cell_type":"markdown","source":"y, a continuación, medimos la precisión en validation"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"pred_val = score_function(Xv, clf)\nprint('Accuracy valid: ', accuracy_score(Yv, pred_val))\n\npd.DataFrame(confusion_matrix(Yv, pred_val))"},{"metadata":{},"cell_type":"markdown","source":"Usamos la función de evaluación de resultados propuesta en la competición para ver cómo de rentable hubiera resultado la inversión en el conjunto de validación"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# calculation of actual metric that is used to calculate final score\nprod_val = pred_val * Zv.ravel()\nu_val = cdf['universe'].loc[cdf['date'].isin(dates[val])].values\nx_t_i = prod_val * u_val\ndata = {'day' : cdf['date'].loc[cdf['date'].isin(dates[val])], 'x_t_i' : x_t_i}\ndf = pd.DataFrame(data)\nx_t = df.groupby('day').sum().values.flatten()\nmean = np.mean(x_t)\nstd = np.std(x_t)\nscore_valid = mean / std\nprint('Validation score', score_valid)"},{"metadata":{},"cell_type":"markdown","source":"Ahora realizaremos la predicción sobre el conjunto de datos de test...\npero, antes, juntamos training y validation en \"train\" y volvemos a entrenar el modelo"},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtrain = X0\nYtrain = Y0\ndel X0, Y0\n\n#from sklearn.ensemble import GradientBoostingClassifier\n#clf1 = GradientBoostingClassifier().fit(Xtrain, Ytrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\nfrom scipy.stats import randint\nfrom sklearn.model_selection import GridSearchCV\n\nparam_test1 = {'learning_rate':[0.1,0.11], 'loss':['deviance', 'exponential'], 'n_estimators':[20,30]}\n#learning_rate=0.1, min_samples_split=500,min_samples_leaf=50,max_depth=8,max_features='sqrt',subsample=0.8,random_state=10\ngsearch1 = GridSearchCV(estimator = GradientBoostingClassifier(), \nparam_grid = param_test1, scoring='recall',n_jobs=4,iid=False, cv=5)\ngsearch1.fit(Xtrain,Ytrain)\n\ngsearch1.best_params_","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}