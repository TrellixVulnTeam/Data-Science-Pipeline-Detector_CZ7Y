{"cells":[{"metadata":{},"cell_type":"markdown","source":"Thanks to all the contributors with kernels to this competition!\n\nSpecially, to the ones that provided code and comments that inspired us making this and other kernels we developed in the context of this competition:\nhttps://www.kaggle.com/dmdm02/complete-eda-voting-lightgbm\nhttps://www.kaggle.com/chocozzz/two-sigma-news-simple-eda-prophet-nlp\nhttps://www.kaggle.com/ashishpatel26/bird-eye-view-of-two-sigma-nn-approach\nhttps://www.kaggle.com/jsaguiar/baseline-with-news\nhttps://www.kaggle.com/artgor/eda-feature-engineering-and-everything\nhttps://www.kaggle.com/christofhenkel/market-data-nn-baseline\nhttps://www.kaggle.com/smasar/tutorial-timeseriesapproach\nhttps://www.kaggle.com/rabaman/0-64-in-100-lines\nhttps://www.kaggle.com/guowenrui/market-nn-if-you-like-you-can-use-it-and-upvote/notebook\nhttps://www.kaggle.com/smasar/eda-preprocessing-processing-evaluation"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"cell_type":"markdown","source":"Importar librerías"},{"metadata":{"_uuid":"e2524e66838667e802b1fe2a999502b6554f4b8f","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport time\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime as dt\nfrom datetime import datetime, date, time, timedelta\nimport calendar\n\nplt.style.use('seaborn')\nsns.set(font_scale=2)\n\nimport warnings \nwarnings.filterwarnings('ignore')\nimport os","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fdd83f326e1787df94e1a11e7a3bfa3e23dd32ac"},"cell_type":"markdown","source":"Crear entorno"},{"metadata":{"_uuid":"a99c0e803afb4d179d0c33f7bb73b4458fe3b43d","trusted":true},"cell_type":"code","source":"from kaggle.competitions import twosigmanews\n# You can only call make_env() once, so don't lose it!\nenv = twosigmanews.make_env()\nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e08403c881b7963f4ad31a9a1ac31daca547125b"},"cell_type":"markdown","source":"Cargar datos de training"},{"metadata":{"_uuid":"048583ec640b32ca30a238ca81b5660fe0dbff59","trusted":true},"cell_type":"code","source":"(market_train_df, news_train_df) = env.get_training_data()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cargamos datos de test, para tener a mano la plantilla que en principio habría que rellenar para submitir la competicion...\nNosotros no submitiremos, pero... vamos a basarnos en ella para realizar el cálculo de los rendimientos que se obtendrían aplicando el modelo"},{"metadata":{"trusted":true},"cell_type":"code","source":"days = env.get_prediction_days()\n(market_test_df, news_test_df, predictions_template_df) = next(days)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d9aff50dda8a73c529c3001f42cc0d508ec0f7aa"},"cell_type":"markdown","source":"Dadas las características del proyecto, de cara a poder medir la accuracy, simplemente consultamos las características de los ficheros de test... pero no vamos a usarlos en lo sucesivo, sino que vamos a coger un subconjunto de los ficheros de train, que tengan las mismas características: los datos de mercados del último día y los datos del fichero de noticias posteriores a las 22h del penúltimo día (más todos los del último día).\n\nPara poder simular un seguimiento durante varios días en lugar del último día, cogeremos los días de la última semana para reservarlos como test."},{"metadata":{"_uuid":"08c536a8702144f8b7cd44ac0ca45586e340d072","trusted":true},"cell_type":"code","source":"del market_test_df, news_test_df\n\nstart = '2016-06-30 22:00:00+00:00'\n\nmarket_test_df = market_train_df.loc[market_train_df['time'] > start]\nnews_test_df = news_train_df.loc[news_train_df['time'] > start]\n\nmarket_train_df = market_train_df.loc[market_train_df['time'] <= start].reset_index(drop=True)\nnews_train_df = news_train_df.loc[news_train_df['time'] <= start].reset_index(drop=True)\n\nmarket_train_df = market_train_df.loc[market_train_df['time'] >= '2010-01-01 22:00:00+0000']\nnews_train_df = news_train_df.loc[news_train_df['time'] >= '2009-12-31 22:00:00+0000']\n#to make it fast\n#market_train_df = market_train_df.loc[market_train_df['time'] >= '2016-01-01 22:00:00+0000']\n#news_train_df = news_train_df.loc[news_train_df['time'] >= '2015-12-31 22:00:00+0000']\n\nmarket_train_df['close_to_open'] =  np.abs(market_train_df['close'] / market_train_df['open'])\nmarket_train_df = market_train_df.loc[market_train_df['close_to_open'] > 0.5]\nmarket_train_df = market_train_df.loc[market_train_df['close_to_open'] < 2]\n\nmarket_test_df['close_to_open'] =  np.abs(market_test_df['close'] / market_test_df['open'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Para reproducir las condiciones reales, en las que el fichero de mercados de test no tendría las variables de los rendimientos en los siguientes 10 días y la de 'universe' (si la acción entraría en cotización), vamos a eliminar estas dos variables del fichero de test...\npero, para poder medir al final el resultado de la predicción, guardaremos estos datos en un fichero auxiliar."},{"metadata":{"trusted":true},"cell_type":"code","source":"aux_columns = ['assetCode', 'time', 'returnsOpenNextMktres10', 'universe']\nmarket_test_aux = market_test_df[aux_columns]\nmarket_test_aux.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_test = ['returnsOpenNextMktres10', 'universe']\nmarket_test_df.drop(drop_test, axis=1, inplace=True)\nmarket_test_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"market_train_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"De cara a ahorrar pasos, una vez preparado el fichero de mercados de test, volvemos a juntar los ficheros de training y test en uno sólo, para hacer las transformaciones una única vez"},{"metadata":{"trusted":true},"cell_type":"code","source":"market_dfs = [market_train_df, market_test_df]\n\nmarket_train_df = pd.concat(market_dfs)\n\nmarket_train_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Y hacemos los mismo con los ficheros de noticias"},{"metadata":{"trusted":true},"cell_type":"code","source":"news_dfs = [news_train_df, news_test_df]\n\nnews_train_df = pd.concat(news_dfs)\n\nnews_train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del market_test_df, news_test_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preparamos los datos para hacer el merge que nos permitirá comenzar a ejecutar modelos"},{"metadata":{"_uuid":"431da71f2607c3b604f63435e9c4275ebaf618a1","trusted":true},"cell_type":"code","source":"def preprocess_news(news_train):\n    drop_list = [\n        'audiences', 'subjects', 'assetName',\n        'headline', 'firstCreated', 'sourceTimestamp',\n    ]\n    news_train.drop(drop_list, axis=1, inplace=True)\n    \n    # Factorize categorical columns\n    for col in ['headlineTag', 'provider', 'sourceId']:\n        news_train[col], uniques = pd.factorize(news_train[col])\n        del uniques\n    \n    # Remove {} and '' from assetCodes column\n    news_train['assetCodes'] = news_train['assetCodes'].apply(lambda x: x[1:-1].replace(\"'\", \"\"))\n    return news_train\n\nnews_train_df = preprocess_news(news_train_df)\n#news_test_df = preprocess_news(news_test_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a260ae107acfbcbb332e0879243ebeab6e4d973","trusted":true},"cell_type":"code","source":"def unstack_asset_codes(news_train_df):\n    codes = []\n    indexes = []\n    for i, values in news_train_df['assetCodes'].iteritems():\n        explode = values.split(\", \")\n        codes.extend(explode)\n        repeat_index = [int(i)]*len(explode)\n        indexes.extend(repeat_index)\n    index_df = pd.DataFrame({'news_index': indexes, 'assetCode': codes})\n    del codes, indexes\n#    gc.collect()\n    return index_df\n\nindex_df = unstack_asset_codes(news_train_df)\n#index_df2 = unstack_asset_codes(news_test_df)\n\ndef merge_news_on_index(news_train_df, index_df):\n    news_train_df['news_index'] = news_train_df.index.copy()\n\n    # Merge news on unstacked assets\n    news_unstack = index_df.merge(news_train_df, how='left', on='news_index')\n    news_unstack.drop(['news_index', 'assetCodes'], axis=1, inplace=True)\n    return news_unstack\n\nnews_unstack = merge_news_on_index(news_train_df, index_df)\n#news_unstack2 = merge_news_on_index(news_test_df, index_df2)\n#del news_train_df, index_df, news_test_df, index_df2\ndel news_train_df, index_df\n#gc.collect()\n#news_unstack.head(3)\n\n\ndef group_news(news_frame):\n#    news_frame['date'] = news_frame.time.dt.date  # Add date column\n    news_frame['date'] = np.where(news_frame.time.dt.time < dt.time(22, 0, 0), news_frame.time.dt.date, news_frame.time.dt.date + timedelta(days=1))   \n#    news_frame['date'] = np.where(datetime.weekday(news_frame['date'])==5, news_frame['date'] + timedelta(days=2), news_frame['date'])\n#    news_frame['date'] = np.where(datetime.weekday(news_frame['date'])==6, news_frame['date'] + timedelta(days=1), news_frame['date'])\n    news_frame['weekday'] = np.where(news_frame.time.dt.time < dt.time(22, 0, 0), news_frame.time.dt.dayofweek, news_frame.time.dt.dayofweek + 1)\n    aggregations = ['mean']\n    gp = news_frame.groupby(['assetCode', 'date', 'weekday']).agg(aggregations)\n#    gp.columns = pd.Index([\"{}_{}\".format(e[0], e[1]) for e in gp.columns.tolist()])\n    gp.columns = pd.Index([e[0] for e in gp.columns.tolist()])\n    gp.reset_index(inplace=True)\n    # Set datatype to float32\n    float_cols = {c: 'float32' for c in gp.columns if c not in ['assetCode', 'date', 'weekday']}\n    return gp.astype(float_cols)\n\nnews_agg = group_news(news_unstack)\ndel news_unstack\n#news_agg2 = group_news(news_unstack2)\n#del news_unstack2\n#; gc.collect()\n#news_agg.head(3)\n\nnews_agg['weekday'][news_agg.weekday == 7] = 0\n#news_agg2['weekday'][news_agg2.weekday == 7] = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tratamiento sobre los festivos y sobre fines de semana"},{"metadata":{"trusted":true},"cell_type":"code","source":"news_agg['date'][news_agg.weekday == 5] = news_agg['date'][news_agg.weekday == 5] + timedelta(days=2)\nnews_agg['date'][news_agg.weekday == 6] = news_agg['date'][news_agg.weekday == 6] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.weekday == 5] = 0\nnews_agg['weekday'][news_agg.weekday == 6] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"news_agg2['date'][news_agg2.weekday == 5] = news_agg2['date'][news_agg2.weekday == 5] + timedelta(days=2)\nnews_agg2['date'][news_agg2.weekday == 6] = news_agg2['date'][news_agg2.weekday == 6] + timedelta(days=1)\n\nnews_agg2['weekday'][news_agg2.weekday == 5] = 0\nnews_agg2['weekday'][news_agg2.weekday == 6] = 0"},{"metadata":{"_uuid":"ee4d34cd51a2990af3db9552f507a11c80314bda","trusted":true},"cell_type":"code","source":"news_agg['weekday'][news_agg.date == date(2010,1,1)] = news_agg['weekday'][news_agg.date == date(2010,1,1)] + 1\nnews_agg['date'][news_agg.date == date(2010,1,1)] = news_agg['date'][news_agg.date == date(2010,1,1)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2012,1,2)] = news_agg['weekday'][news_agg.date == date(2012,1,2)] + 1\nnews_agg['date'][news_agg.date == date(2012,1,2)] = news_agg['date'][news_agg.date == date(2012,1,2)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2013,1,1)] = news_agg['weekday'][news_agg.date == date(2013,1,1)] + 1\nnews_agg['date'][news_agg.date == date(2013,1,1)] = news_agg['date'][news_agg.date == date(2013,1,1)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2014,1,1)] = news_agg['weekday'][news_agg.date == date(2014,1,1)] + 1\nnews_agg['date'][news_agg.date == date(2014,1,1)] = news_agg['date'][news_agg.date == date(2014,1,1)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2015,1,1)] = news_agg['weekday'][news_agg.date == date(2015,1,1)] + 1\nnews_agg['date'][news_agg.date == date(2015,1,1)] = news_agg['date'][news_agg.date == date(2015,1,1)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2016,1,1)] = news_agg['weekday'][news_agg.date == date(2016,1,1)] + 1\nnews_agg['date'][news_agg.date == date(2016,1,1)] = news_agg['date'][news_agg.date == date(2016,1,1)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2010,1,18)] = news_agg['weekday'][news_agg.date == date(2010,1,18)] + 1\nnews_agg['date'][news_agg.date == date(2010,1,18)] = news_agg['date'][news_agg.date == date(2010,1,18)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2011,1,17)] = news_agg['weekday'][news_agg.date == date(2011,1,17)] + 1\nnews_agg['date'][news_agg.date == date(2011,1,17)] = news_agg['date'][news_agg.date == date(2011,1,17)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2012,1,16)] = news_agg['weekday'][news_agg.date == date(2012,1,16)] + 1\nnews_agg['date'][news_agg.date == date(2012,1,16)] = news_agg['date'][news_agg.date == date(2012,1,16)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2013,1,21)] = news_agg['weekday'][news_agg.date == date(2013,1,21)] + 1\nnews_agg['date'][news_agg.date == date(2013,1,21)] = news_agg['date'][news_agg.date == date(2013,1,21)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2014,1,20)] = news_agg['weekday'][news_agg.date == date(2014,1,20)] + 1\nnews_agg['date'][news_agg.date == date(2014,1,20)] = news_agg['date'][news_agg.date == date(2014,1,20)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2015,1,19)] = news_agg['weekday'][news_agg.date == date(2015,1,19)] + 1\nnews_agg['date'][news_agg.date == date(2015,1,19)] = news_agg['date'][news_agg.date == date(2015,1,19)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2016,1,18)] = news_agg['weekday'][news_agg.date == date(2016,1,18)] + 1\nnews_agg['date'][news_agg.date == date(2016,1,18)] = news_agg['date'][news_agg.date == date(2016,1,18)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2010,2,15)] = news_agg['weekday'][news_agg.date == date(2010,2,15)] + 1\nnews_agg['date'][news_agg.date == date(2010,2,15)] = news_agg['date'][news_agg.date == date(2010,2,15)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2011,2,21)] = news_agg['weekday'][news_agg.date == date(2011,2,21)] + 1\nnews_agg['date'][news_agg.date == date(2011,2,21)] = news_agg['date'][news_agg.date == date(2011,2,21)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2012,2,20)] = news_agg['weekday'][news_agg.date == date(2012,2,20)] + 1\nnews_agg['date'][news_agg.date == date(2012,2,20)] = news_agg['date'][news_agg.date == date(2012,2,20)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2013,2,18)] = news_agg['weekday'][news_agg.date == date(2013,2,18)] + 1\nnews_agg['date'][news_agg.date == date(2013,2,18)] = news_agg['date'][news_agg.date == date(2013,2,18)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2014,2,17)] = news_agg['weekday'][news_agg.date == date(2014,2,17)] + 1\nnews_agg['date'][news_agg.date == date(2014,2,17)] = news_agg['date'][news_agg.date == date(2014,2,17)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2015,2,16)] = news_agg['weekday'][news_agg.date == date(2015,2,16)] + 1\nnews_agg['date'][news_agg.date == date(2015,2,16)] = news_agg['date'][news_agg.date == date(2015,2,16)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2016,2,15)] = news_agg['weekday'][news_agg.date == date(2016,2,15)] + 1\nnews_agg['date'][news_agg.date == date(2016,2,15)] = news_agg['date'][news_agg.date == date(2016,2,15)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2010,4,2)] = news_agg['weekday'][news_agg.date == date(2010,4,2)] + 1\nnews_agg['date'][news_agg.date == date(2010,4,2)] = news_agg['date'][news_agg.date == date(2010,4,2)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2011,4,22)] = news_agg['weekday'][news_agg.date == date(2011,4,22)] + 1\nnews_agg['date'][news_agg.date == date(2011,4,22)] = news_agg['date'][news_agg.date == date(2011,4,22)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2012,4,6)] = news_agg['weekday'][news_agg.date == date(2012,4,6)] + 1\nnews_agg['date'][news_agg.date == date(2012,4,6)] = news_agg['date'][news_agg.date == date(2012,4,6)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2013,3,29)] = news_agg['weekday'][news_agg.date == date(2013,3,29)] + 1\nnews_agg['date'][news_agg.date == date(2013,3,29)] = news_agg['date'][news_agg.date == date(2013,3,29)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2014,4,18)] = news_agg['weekday'][news_agg.date == date(2014,4,18)] + 1\nnews_agg['date'][news_agg.date == date(2014,4,18)] = news_agg['date'][news_agg.date == date(2014,4,18)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2015,4,3)] = news_agg['weekday'][news_agg.date == date(2015,4,3)] + 1\nnews_agg['date'][news_agg.date == date(2015,4,3)] = news_agg['date'][news_agg.date == date(2015,4,3)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2016,3,25)] = news_agg['weekday'][news_agg.date == date(2016,3,25)] + 1\nnews_agg['date'][news_agg.date == date(2016,3,25)] = news_agg['date'][news_agg.date == date(2016,3,25)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2010,5,31)] = news_agg['weekday'][news_agg.date == date(2010,5,31)] + 1\nnews_agg['date'][news_agg.date == date(2010,5,31)] = news_agg['date'][news_agg.date == date(2010,5,31)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2011,5,30)] = news_agg['weekday'][news_agg.date == date(2011,5,30)] + 1\nnews_agg['date'][news_agg.date == date(2011,5,30)] = news_agg['date'][news_agg.date == date(2011,5,30)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2012,5,28)] = news_agg['weekday'][news_agg.date == date(2012,5,28)] + 1\nnews_agg['date'][news_agg.date == date(2012,5,28)] = news_agg['date'][news_agg.date == date(2012,5,28)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2013,5,27)] = news_agg['weekday'][news_agg.date == date(2013,5,27)] + 1\nnews_agg['date'][news_agg.date == date(2013,5,27)] = news_agg['date'][news_agg.date == date(2013,5,27)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2014,5,26)] = news_agg['weekday'][news_agg.date == date(2014,5,26)] + 1\nnews_agg['date'][news_agg.date == date(2014,5,26)] = news_agg['date'][news_agg.date == date(2014,5,26)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2015,5,25)] = news_agg['weekday'][news_agg.date == date(2015,5,25)] + 1\nnews_agg['date'][news_agg.date == date(2015,5,25)] = news_agg['date'][news_agg.date == date(2015,5,25)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2016,5,30)] = news_agg['weekday'][news_agg.date == date(2016,5,30)] + 1\nnews_agg['date'][news_agg.date == date(2016,5,30)] = news_agg['date'][news_agg.date == date(2016,5,30)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2010,7,5)] = news_agg['weekday'][news_agg.date == date(2010,7,5)] + 1\nnews_agg['date'][news_agg.date == date(2010,7,5)] = news_agg['date'][news_agg.date == date(2010,7,5)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2011,7,4)] = news_agg['weekday'][news_agg.date == date(2011,7,4)] + 1\nnews_agg['date'][news_agg.date == date(2011,7,4)] = news_agg['date'][news_agg.date == date(2011,7,4)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2012,7,4)] = news_agg['weekday'][news_agg.date == date(2012,7,4)] + 1\nnews_agg['date'][news_agg.date == date(2012,7,4)] = news_agg['date'][news_agg.date == date(2012,7,4)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2013,7,4)] = news_agg['weekday'][news_agg.date == date(2013,7,4)] + 1\nnews_agg['date'][news_agg.date == date(2013,7,4)] = news_agg['date'][news_agg.date == date(2013,7,4)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2014,7,4)] = news_agg['weekday'][news_agg.date == date(2014,7,4)] + 1\nnews_agg['date'][news_agg.date == date(2014,7,4)] = news_agg['date'][news_agg.date == date(2014,7,4)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2015,7,4)] = news_agg['weekday'][news_agg.date == date(2015,7,4)] + 1\nnews_agg['date'][news_agg.date == date(2015,7,4)] = news_agg['date'][news_agg.date == date(2015,7,4)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2016,7,4)] = news_agg['weekday'][news_agg.date == date(2016,7,4)] + 1\nnews_agg['date'][news_agg.date == date(2016,7,4)] = news_agg['date'][news_agg.date == date(2016,7,4)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2010,9,6)] = news_agg['weekday'][news_agg.date == date(2010,9,6)] + 1\nnews_agg['date'][news_agg.date == date(2010,9,6)] = news_agg['date'][news_agg.date == date(2010,9,6)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2011,9,5)] = news_agg['weekday'][news_agg.date == date(2011,9,5)] + 1\nnews_agg['date'][news_agg.date == date(2011,9,5)] = news_agg['date'][news_agg.date == date(2011,9,5)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2012,9,3)] = news_agg['weekday'][news_agg.date == date(2012,9,3)] + 1\nnews_agg['date'][news_agg.date == date(2012,9,3)] = news_agg['date'][news_agg.date == date(2012,9,3)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2013,9,2)] = news_agg['weekday'][news_agg.date == date(2013,9,2)] + 1\nnews_agg['date'][news_agg.date == date(2013,9,2)] = news_agg['date'][news_agg.date == date(2013,9,2)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2014,9,1)] = news_agg['weekday'][news_agg.date == date(2014,9,1)] + 1\nnews_agg['date'][news_agg.date == date(2014,9,1)] = news_agg['date'][news_agg.date == date(2014,9,1)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2015,9,7)] = news_agg['weekday'][news_agg.date == date(2015,9,7)] + 1\nnews_agg['date'][news_agg.date == date(2015,9,7)] = news_agg['date'][news_agg.date == date(2015,9,7)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2016,9,5)] = news_agg['weekday'][news_agg.date == date(2016,9,5)] + 1\nnews_agg['date'][news_agg.date == date(2016,9,5)] = news_agg['date'][news_agg.date == date(2016,9,5)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2010,11,25)] = news_agg['weekday'][news_agg.date == date(2010,11,25)] + 1\nnews_agg['date'][news_agg.date == date(2010,11,25)] = news_agg['date'][news_agg.date == date(2010,11,25)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2011,11,24)] = news_agg['weekday'][news_agg.date == date(2011,11,24)] + 1\nnews_agg['date'][news_agg.date == date(2011,11,24)] = news_agg['date'][news_agg.date == date(2011,11,24)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2012,11,22)] = news_agg['weekday'][news_agg.date == date(2012,11,22)] + 1\nnews_agg['date'][news_agg.date == date(2012,11,22)] = news_agg['date'][news_agg.date == date(2012,11,22)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2013,11,28)] = news_agg['weekday'][news_agg.date == date(2013,11,28)] + 1\nnews_agg['date'][news_agg.date == date(2013,11,28)] = news_agg['date'][news_agg.date == date(2013,11,28)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2014,11,27)] = news_agg['weekday'][news_agg.date == date(2014,11,27)] + 1\nnews_agg['date'][news_agg.date == date(2014,11,27)] = news_agg['date'][news_agg.date == date(2014,11,27)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2015,11,26)] = news_agg['weekday'][news_agg.date == date(2015,11,26)] + 1\nnews_agg['date'][news_agg.date == date(2015,11,26)] = news_agg['date'][news_agg.date == date(2015,11,26)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2016,11,24)] = news_agg['weekday'][news_agg.date == date(2016,11,24)] + 1\nnews_agg['date'][news_agg.date == date(2016,11,24)] = news_agg['date'][news_agg.date == date(2016,11,24)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2010,12,24)] = news_agg['weekday'][news_agg.date == date(2010,12,24)] + 1\nnews_agg['date'][news_agg.date == date(2010,12,24)] = news_agg['date'][news_agg.date == date(2010,12,24)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2011,12,26)] = news_agg['weekday'][news_agg.date == date(2011,12,26)] + 1\nnews_agg['date'][news_agg.date == date(2011,12,26)] = news_agg['date'][news_agg.date == date(2011,12,26)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2012,12,25)] = news_agg['weekday'][news_agg.date == date(2012,12,25)] + 1\nnews_agg['date'][news_agg.date == date(2012,12,25)] = news_agg['date'][news_agg.date == date(2012,12,25)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2013,12,25)] = news_agg['weekday'][news_agg.date == date(2013,12,25)] + 1\nnews_agg['date'][news_agg.date == date(2013,12,25)] = news_agg['date'][news_agg.date == date(2013,12,25)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2014,12,25)] = news_agg['weekday'][news_agg.date == date(2014,12,25)] + 1\nnews_agg['date'][news_agg.date == date(2014,12,25)] = news_agg['date'][news_agg.date == date(2014,12,25)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2015,12,25)] = news_agg['weekday'][news_agg.date == date(2015,12,25)] + 1\nnews_agg['date'][news_agg.date == date(2015,12,25)] = news_agg['date'][news_agg.date == date(2015,12,25)] + timedelta(days=1)\n\nnews_agg['weekday'][news_agg.date == date(2016,12,26)] = news_agg['weekday'][news_agg.date == date(2016,12,26)] + 1\nnews_agg['date'][news_agg.date == date(2016,12,26)] = news_agg['date'][news_agg.date == date(2016,12,26)] + timedelta(days=1)\n\n#news_agg['weekday'][news_agg.weekday == 7] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"news_agg2['weekday'][news_agg2.date == date(2016,12,26)] = news_agg2['weekday'][news_agg2.date == date(2016,12,26)] + 1\nnews_agg2['date'][news_agg2.date == date(2016,12,26)] = news_agg2['date'][news_agg2.date == date(2016,12,26)] + timedelta(days=1)\n\n#news_agg2['weekday'][news_agg2.weekday == 7] = 0"},{"metadata":{"_uuid":"4fce005f4edf5f343ab37632dce6eab4d65c66e5","trusted":true},"cell_type":"code","source":"news_agg['date'][news_agg.weekday == 5] = news_agg['date'][news_agg.weekday == 5] + timedelta(days=2)\nnews_agg['date'][news_agg.weekday == 6] = news_agg['date'][news_agg.weekday == 6] + timedelta(days=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"news_agg2['date'][news_agg2.weekday == 5] = news_agg2['date'][news_agg2.weekday == 5] + timedelta(days=2)\nnews_agg2['date'][news_agg2.weekday == 6] = news_agg2['date'][news_agg2.weekday == 6] + timedelta(days=1)"},{"metadata":{"_uuid":"ea32c7ba7a64c589f4b3db94a430a8418af5c998","trusted":true},"cell_type":"code","source":"def group_news(news_frame):\n    aggregations = ['mean']\n    gp = news_frame.groupby(['assetCode', 'date']).agg(aggregations)\n#    gp.columns = pd.Index([\"{}_{}\".format(e[0], e[1]) for e in gp.columns.tolist()])\n    gp.columns = pd.Index([e[0] for e in gp.columns.tolist()])\n    gp.reset_index(inplace=True)\n    # Set datatype to float32\n    float_cols = {c: 'float32' for c in gp.columns if c not in ['assetCode', 'date']}\n    return gp.astype(float_cols)\n\nnews_aggr = group_news(news_agg)\ndel news_agg\n#news_aggr2 = group_news(news_agg2)\n#del news_agg2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Generamos el fichero con el que vamos a trabajar que resulta del merge de los ficheros de mercados y de noticias"},{"metadata":{"trusted":true},"cell_type":"code","source":"market_train_df['date'] = market_train_df.time.dt.date\nfull_train_df = market_train_df.merge(news_aggr, how='left', on=['assetCode', 'date'])\ndel market_train_df, news_aggr\nfull_train_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"market_test_df['date'] = market_test_df.time.dt.date\nfull_test_df = market_test_df.merge(news_aggr2, how='left', on=['assetCode', 'date'])\ndel market_test_df, news_aggr2\nfull_test_df.head(5)"},{"metadata":{"_uuid":"320b63b310cda36592324ac9d6453a71c16e84b9"},"cell_type":"markdown","source":"## 5.2 Preparación de los datos"},{"metadata":{},"cell_type":"markdown","source":"## pensar justificación en elección de variables... basada en el EDA... Y ver si quitar las variables calculadas y poner las que hay a pelo\n"},{"metadata":{},"cell_type":"markdown","source":"Antes de nada, vamos a hacer encoding de los asset codes para poder incluirlos en nuestro modelo"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nfull_train_df[\"assetToken\"] = LabelEncoder().fit_transform(full_train_df[\"assetCode\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A continuación, dividimos el fichero en train y test"},{"metadata":{"trusted":true},"cell_type":"code","source":"start = '2016-06-30 22:00:00+00:00'\n\nfull_test_df = full_train_df.loc[full_train_df['time'] > start]\nfull_train_df = full_train_df.loc[full_train_df['time'] <= start].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Eliminamos del fichero de test las columnas de los rendimientos en los siguientes 10 días y de la variable 'universe' (que, por otro lado, están rellenas por nulos, ya que antes de juntar los ficheros, habíamos eliminado esas variables del fichero de test)"},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_test = ['returnsOpenNextMktres10', 'universe']\nfull_test_df.drop(drop_test, axis=1, inplace=True)\nfull_test_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Definimos una función para:\n\n- eliminar variables que no consideramos relevantes\n- generar variables nuevas que vemos que pueden resumir la información de las variables existentes... \n\nY aplicamos la función sobre el fichero de train"},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_data(full_train_df):\n#    full_train_df = full_train_df[pd.notnull(full_train_df['urgency'])]\n    full_train_df['returnsClosePrevMktres1'] = np.where(np.isnan(full_train_df['returnsClosePrevMktres1']), full_train_df['returnsClosePrevRaw1'], full_train_df['returnsClosePrevMktres1'])\n    full_train_df['returnsOpenPrevMktres1'] = np.where(np.isnan(full_train_df['returnsOpenPrevMktres1']), full_train_df['returnsOpenPrevRaw1'], full_train_df['returnsOpenPrevMktres1'])\n    full_train_df['returnsClosePrevMktres10'] = np.where(np.isnan(full_train_df['returnsClosePrevMktres10']), full_train_df['returnsClosePrevRaw10'], full_train_df['returnsClosePrevMktres10']) \n    full_train_df['returnsOpenPrevMktres10'] = np.where(np.isnan(full_train_df['returnsOpenPrevMktres10']), full_train_df['returnsOpenPrevRaw10'], full_train_df['returnsOpenPrevMktres10'])\n#    full_train_df['time'] = full_train_df.time.dt.strftime(\"%Y%m%d\").astype(int)\n#    full_train_df['bartrend'] = full_train_df['close'] / full_train_df['open']\n    full_train_df['average'] = (full_train_df['close'] + full_train_df['open'])/2\n    full_train_df['pricevolume'] = full_train_df['volume'] * full_train_df['close']\n#    full_train_df['position'] = full_train_df['firstMentionSentence'] / full_train_df['sentenceCount']\n#    full_train_df['coverage'] = full_train_df['sentimentWordCount'] / full_train_df['wordCount']\n\n    # eliminar variables prescindibles\n#    droplist = ['sourceId','takeSequence','provider','firstMentionSentence',\n#                'sentenceCount','bodySize','headlineTag','marketCommentary',\n#                'sentimentClass','urgency','wordCount','sentimentWordCount',\n#                'weekday','assetName','volume','time']\n    droplist = ['open', 'close',\n                'takeSequence', \n                'bodySize', 'companyCount',                   \n                'sentenceCount', 'wordCount',\n                'firstMentionSentence',\n                                       \n                                      'noveltyCount12H','noveltyCount24H',\n                'noveltyCount3D','noveltyCount5D', 'noveltyCount7D',\n                'weekday','assetName','time',\n                'sourceId', 'urgency', 'provider', 'marketCommentary',          \n                'relevance', 'sentimentClass',                                                   \n                'volumeCounts12H', 'volumeCounts24H', 'volumeCounts3D', 'volumeCounts5D',            \n                'volumeCounts7D']  \n    full_train_df.drop(droplist, axis=1, inplace=True)\n    return full_train_df\n\ncdf = prepare_data(full_train_df)    \ndel full_train_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creamos una variable adicional, la variable \"return\", que es la que vamos a tomar como variable objetivo, a la que le asignaremos un 1 para rendimientos por encima de 0.03 (percentil 75 de los rendimientos a 10 días en el futuro) y un 0 para los demás"},{"metadata":{"trusted":true},"cell_type":"code","source":"cdf['return'] = np.where(cdf.returnsOpenNextMktres10 > 0, 1, 0)   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dividimos en:\n\n- training\n- validation\n\nPero... inicialmente únicamente preparamos la variable objetivo... y la variable de rendimientos que nos servirá para calcular más adelante la bondad del modelo en validación"},{"metadata":{"_uuid":"b4ff110e696aa8e9209eaf0bd6fcc4eb85ef1df0","trusted":true},"cell_type":"code","source":"targetcols = ['return']\ntraincols = [col for col in cdf.columns if col not in ['date', 'assetCode', 'universe','returnsOpenNextMktres10'] + targetcols]\n\ndates = cdf['date'].unique()\ntrain = range(len(dates))[:int(0.85*len(dates))]\nval = range(len(dates))[int(0.85*len(dates)):]\n\n# we be classifyin\n#cdf[targetcols[0]] = (cdf[targetcols[0]] > 0).astype(int)\nY0 = cdf[targetcols].fillna(0).values\nZ0 = cdf['returnsOpenNextMktres10'].fillna(0).values\n\n# train data\nYt = cdf[targetcols].fillna(0).loc[cdf['date'].isin(dates[train])].values\nZt = cdf['returnsOpenNextMktres10'].fillna(0).loc[cdf['date'].isin(dates[train])].values\n\n# validation data\nYv = cdf[targetcols].fillna(0).loc[cdf['date'].isin(dates[val])].values\nZv = cdf['returnsOpenNextMktres10'].fillna(0).loc[cdf['date'].isin(dates[val])].values\n\nprint(Y0.shape)\nprint(Yt.shape, Yv.shape)\nprint(Zt.shape, Zv.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comprobamos los tipos de las variables del fichero"},{"metadata":{"trusted":true},"cell_type":"code","source":"cdf.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Y hacemos la normalización de las variables numéricas para preparar los ficheros con las variables que nos van a servir para entrenar el modelo y para introducirlas como input para la predicción en validación"},{"metadata":{"trusted":true},"cell_type":"code","source":"numcols = [col for col in cdf.columns if col not in ['date', 'assetCode', 'assetToken', 'returnsOpenNextMktres10', 'return', 'universe']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\ncdf[numcols] = StandardScaler().fit_transform(cdf[numcols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X0 = cdf[traincols].fillna(0).values\n\n# train data\nXt = cdf[traincols].fillna(0).loc[cdf['date'].isin(dates[train])].values\n\n# validation data\nXv = cdf[traincols].fillna(0).loc[cdf['date'].isin(dates[val])].values\n\nprint(Xt.shape, Xv.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Utilizamos sklearn para entrenar un modelo con los datos de training"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import BernoulliNB\nclf = BernoulliNB().fit(Xt, Yt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Definimos la score function, que predecirá la categoría para cada registro"},{"metadata":{"trusted":true},"cell_type":"code","source":"def score_function(data, model):\n    predicted = model.predict(data)\n    return predicted","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hacemos la predicción para training... medimos la precisión y comprobamos la matriz de confusión"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix\n\npred_trn = score_function(Xt, clf)\nprint('Accuracy train: ', accuracy_score(Yt, pred_trn))\n\npd.DataFrame(confusion_matrix(Yt, pred_trn))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nx = pred_trn\nunique, counts = np.unique(x, return_counts=True)\n\nnp.asarray((unique, counts)).T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"y, a continuación, medimos la precisión en validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_val = score_function(Xv, clf)\nprint('Accuracy valid: ', accuracy_score(Yv, pred_val))\n\npd.DataFrame(confusion_matrix(Yv, pred_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Usamos la función de evaluación de resultados propuesta en la competición para ver cómo de rentable hubiera resultado la inversión en el conjunto de validación"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# calculation of actual metric that is used to calculate final score\nprod_val = pred_val * Zv.ravel()\nu_val = cdf['universe'].loc[cdf['date'].isin(dates[val])].values\nx_t_i = prod_val * u_val\ndata = {'day' : cdf['date'].loc[cdf['date'].isin(dates[val])], 'x_t_i' : x_t_i}\ndf = pd.DataFrame(data)\nx_t = df.groupby('day').sum().values.flatten()\nmean = np.mean(x_t)\nstd = np.std(x_t)\nscore_valid = mean / std\nprint('Validation score', score_valid)"},{"metadata":{},"cell_type":"markdown","source":"Ahora realizaremos la predicción sobre el conjunto de datos de test...\npero, antes, juntamos training y validation en \"train\" y volvemos a entrenar el modelo"},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtrain = X0\nYtrain = Y0\ndel X0, Y0\n\nfrom sklearn.naive_bayes import BernoulliNB\nclf1 = BernoulliNB().fit(Xtrain, Ytrain)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preparamos los datos del conjunto de test aplicando la función de transformación definida anteriormente"},{"metadata":{"trusted":true},"cell_type":"code","source":"cdf2 = prepare_data(full_test_df)    \ndel full_test_df\n\ncdf2.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nx = cdf2['date']\nunique, counts = np.unique(x, return_counts=True)\n\nnp.asarray((unique, counts)).T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creamos varios conjuntos de datos de test:\n\n- para el primero de los días\n- para el segundo\n- para el tercero\n- y para el cuarto día\n\nDe cara a simular que vamos obteniendo ficheros en distintos días y que vamos haciendo lo que habría que hacer en la realidad"},{"metadata":{"trusted":true},"cell_type":"code","source":"dtest1 = date(2016,7,1)\ntest1 = cdf2.loc[cdf2['date']==dtest1]\n\n#dtest2 = date(2016,12,28)\n#test2 = cdf2.loc[cdf2['date']==dtest2]\n\n#dtest3 = date(2016,12,29)\n#test3 = cdf2.loc[cdf2['date']==dtest3]\n\n#dtest4 = date(2016,12,30)\n#test4 = cdf2.loc[cdf2['date']==dtest4]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cogemos únicamente el fichero correspondiente al primer día de test... y normalizamos las variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"numcols = [col for col in cdf2.columns if col not in ['date', 'assetCode', 'assetToken']]\ntest1[numcols] = StandardScaler().fit_transform(test1[numcols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtest1 = test1[traincols].fillna(0).values\n#Ytest1 = test1[targetcols].fillna(0).values\n#Ztest1 = test1['returnsOpenNextMktres10'].fillna(0).values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Aplicamos el modelo entrenado con todos los datos de training para hacer las predicciones y preparar la plantilla que habría que submitir (en caso de participar en la competición)"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_test1 = score_function(Xtest1, clf1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Ztrain1 = cdf['returnsOpenNextMktres10'].fillna(0).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\ndef score_function2(data, model):\n    pred_proba = model.predict_proba(data)\n    predicted = pred_proba[:,1] - pred_proba[:,0]\n#    scaler1 =  StandardScaler()\n#    scaler1.fit(reference.reshape(-1,1))\n#    scaler2 = StandardScaler()\n#    data2 = scaler2.fit_transform(predicted.reshape(-1,1))\n#    data2 = scaler1.inverse_transform(data2)[:,0]\n    scaler3 = MinMaxScaler(feature_range=(-1, 1))\n    data3 = scaler3.fit_transform(predicted.reshape(-1,1))\n#    data4 = data3.reshape(-1,1)\n    pred_df = pd.DataFrame(data3)\n    pred_df['score'] = np.where(pred_df[0] < 0, 0, pred_df[0])\n    data4 = pred_df['score'].values\n\n    return data4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_conf1 = score_function2(Xtest1, clf1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.percentile(pred_conf1, [0,10,25,50,75,90,100])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Usaremos la template de submisión como base para almacenar las variables que nos harán falta para medir lo rentable que nos resulta el modelo:\n\n- Y para la predicción del valor de confianza en ese día, por cada uno de los activos\n- R para los rendimientos reales del día, por cada uno de los activos\n- U para la variable 'universe' que dice si un activo en concreto entra en la cotización de ese día"},{"metadata":{"trusted":true},"cell_type":"code","source":"#predictions1 = pd.DataFrame({'assetCode':test1['assetCode'],'confidence':pred_test1})\npredictions1 = pd.DataFrame({'assetCode':test1['assetCode'],'confidence':pred_conf1})\n\npredictions_template_df = predictions_template_df.merge(predictions1,how='left').drop('confidenceValue',axis=1).fillna(0).rename(columns={'confidence':'Y1'})\n\npredictions_template_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ahora, para poder comprobar cómo habría resultado la predicción...\nrecuperamos el fichero auxiliar que habíamos generado al principio, con las columnas de rendimientos obtenidos en los siguientes 10 días y con la variable 'universe' del conjunto de test\n\nY volvemos a unirlos con el fichero de test"},{"metadata":{"trusted":true},"cell_type":"code","source":"market_test_aux['date'] = market_test_aux.time.dt.date\nmarket_test_aux.drop('time', axis=1, inplace=True)\n\ntest12 = test1.merge(market_test_aux,how='left', on=['assetCode', 'date'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preparamos la variable objetivo, y la variable con los rendimientos"},{"metadata":{"trusted":true},"cell_type":"code","source":"test12['return'] = np.where(test12.returnsOpenNextMktres10 > 0, 1, 0)\n\nYtest1 = test12['return'].fillna(0).values\nZtest1 = test12['returnsOpenNextMktres10'].fillna(0).values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculamos la precisión y la matriz de confusión"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Accuracy test: ', accuracy_score(Ytest1, pred_test1))\n\npd.DataFrame(confusion_matrix(Ytest1, pred_test1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Y añadimos a la template de predicciones las columnas correspondientes a los parámetros que nos harían falta para calcular la rentabilidad de la inversión"},{"metadata":{"trusted":true},"cell_type":"code","source":"param1 = pd.DataFrame({'assetCode':test12['assetCode'],'R1':Ztest1, 'U1':test12['universe']})\n\npredictions_template_df = predictions_template_df.merge(param1,how='left')\n\npredictions_template_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_template_df['X1'] = predictions_template_df['Y1'] * predictions_template_df['R1'] * predictions_template_df['U1']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ahora crearemos un dataframe con los datos de train (de ese primer día) y los de test (de ese primer día), para usarlos como datos de train para el segundo día"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\n\nunique","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cdf0 = cdf\ntestB = test12\n\nx = cdf2['date'].loc[cdf2['date']>dtest1]\nlist_dates, counts = np.unique(x, return_counts=True)\n\nnumcols = [col for col in cdf2.columns if col not in ['date', 'assetCode', 'assetToken']]\nj = 1\n\n#para no reentrenar\nclf = clf1\n\nfor i in list_dates:\n    j = j+1\n\n    frames = [cdf0, testB]\n    cdf0 = pd.concat(frames)\n\n    Xtrain = cdf0[traincols].fillna(0).values\n    Ytrain = cdf0[targetcols].fillna(0).values\n    Ztrain = cdf0['returnsOpenNextMktres10'].fillna(0).values\n    \n#para no reentrenar\n#    clf = BernoulliNB().fit(Xtrain, Ytrain)\n    \n    testA = cdf2.loc[cdf2['date']==i]\n    testA[numcols] = StandardScaler().fit_transform(testA[numcols])\n    Xtest = testA[traincols].fillna(0).values\n    \n    pred_test = score_function(Xtest, clf)\n    pred_conf = score_function2(Xtest, clf)\n    \n#    predictions = pd.DataFrame({'assetCode':testA['assetCode'], 'Y' + str(j):pred_test})\n    predictions = pd.DataFrame({'assetCode':testA['assetCode'], 'Y' + str(j):pred_conf})\n    \n    predictions_template_df = predictions_template_df.merge(predictions,how='left')\n    \n    testB = testA.merge(market_test_aux,how='left', on=['assetCode', 'date'])\n    testB['return'] = np.where(testB.returnsOpenNextMktres10 > 0, 1, 0)\n    \n    Ytest = testB['return'].fillna(0).values\n    Ztest = testB['returnsOpenNextMktres10'].fillna(0).values\n\n    print('Accuracy test: ', accuracy_score(Ytest, pred_test))\n\n    print(pd.DataFrame(confusion_matrix(Ytest, pred_test)))\n    \n    print(np.percentile(pred_conf, [0,10,25,50,75,90,100]))\n\n    params = pd.DataFrame({'assetCode':testB['assetCode'],'R' + str(j):Ztest, 'U' + str(j):testB['universe']})\n\n    predictions_template_df = predictions_template_df.merge(params,how='left')\n\n    predictions_template_df['X' + str(j)] = predictions_template_df['Y' + str(j)] * predictions_template_df['R' + str(j)] * predictions_template_df['U' + str(j)]\n\npredictions_template_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A partir de todas las variables calculadas, añadimos las columnas correspondientes a los productos de las tres variables, para cada activo para cada uno de los días"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"predictions_template_df['X1'] = predictions_template_df['Y1'] * predictions_template_df['R1'] * predictions_template_df['U1']\npredictions_template_df['X2'] = predictions_template_df['Y2'] * predictions_template_df['R2'] * predictions_template_df['U2']\npredictions_template_df['X3'] = predictions_template_df['Y3'] * predictions_template_df['R3'] * predictions_template_df['U3']\npredictions_template_df['X4'] = predictions_template_df['Y4'] * predictions_template_df['R4'] * predictions_template_df['U4']\n\npredictions_template_df.head(10)\n#env.write_submission_file()"},{"metadata":{},"cell_type":"markdown","source":"Calculamos la suma de los productos de todos los activos para cada uno de los días"},{"metadata":{"trusted":true},"cell_type":"code","source":"Xcolumns = [col for col in predictions_template_df.columns if 'X' in col]\nx_t = predictions_template_df[Xcolumns].sum().values.flatten()\nx_t","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Y, por último, calculamos media y desviación estándar, para calcular el resultado de nuestro modelo"},{"metadata":{"trusted":true},"cell_type":"code","source":"mean = np.mean(x_t)\nstd = np.std(x_t)\nscore_test = mean / std\nprint('Xt mean', mean)\nprint('Xt std', std)\nprint('Test score', score_test)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}